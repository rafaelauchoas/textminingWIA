Artiﬁcial Intelligence 210 (2014) 78–122

Contents lists available at ScienceDirect

Artiﬁcial Intelligence

www.elsevier.com/locate/artint

The dropout learning algorithm

Pierre Baldi

∗

, Peter Sadowski

Department of Computer Science, University of California, Irvine, CA 92697-3435, United States

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 11 June 2013
Received in revised form 18 February 2014
Accepted 18 February 2014
Available online 24 February 2014

Keywords:
Machine learning
Neural networks
Ensemble
Regularization
Stochastic neurons
Stochastic gradient descent
Backpropagation
Geometric mean
Variance minimization
Sparse representations

Dropout is a recently introduced algorithm for training neural networks by randomly
dropping units during training to prevent their co-adaptation. A mathematical analysis
of some of the static and dynamic properties of dropout is provided using Bernoulli
gating variables, general enough to accommodate dropout on units or connections, and
with variable rates. The framework allows a complete analysis of the ensemble averaging
properties of dropout in linear networks, which is useful to understand the non-linear
case. The ensemble averaging properties of dropout in non-linear logistic networks result
from three fundamental equations: (1) the approximation of the expectations of logistic
functions by normalized geometric means, for which bounds and estimates are derived;
(2) the algebraic equality between normalized geometric means of logistic functions with
the logistic of the means, which mathematically characterizes logistic functions; and (3) the
linearity of the means with respect to sums, as well as products of independent variables.
The results are also extended to other classes of transfer functions, including rectiﬁed
linear functions. Approximation errors tend to cancel each other and do not accumulate.
Dropout can also be connected to stochastic neurons and used to predict ﬁring rates,
and to backpropagation by viewing the backward propagation as ensemble averaging
in a dropout linear network. Moreover, the convergence properties of dropout can be
understood in terms of stochastic gradient descent. Finally, for the regularization properties
of dropout, the expectation of the dropout gradient is the gradient of the corresponding
approximation ensemble, regularized by an adaptive weight decay term with a propensity
for self-consistent variance minimization and sparse representations.
© 2014 The Authors. Published by Elsevier B.V.

Open access under CC BY-NC-ND license.

1. Introduction

Dropout is a recently introduced algorithm for training neural networks [27]. In its simplest form, on each presentation
of each training example, each feature detector unit is deleted randomly with probability q = 1 − p = 0.5. The remaining
weights are trained by backpropagation [40]. The procedure is repeated for each example and each training epoch, shar-
ing the weights at each iteration (Fig. 1.1). After the training phase is completed, predictions are produced by halving all
the weights (Fig. 1.2). The dropout procedure can also be applied to the input layer by randomly deleting some of the
input-vector components—typically an input component is deleted with a smaller probability (i.e. q = 0.2).

The motivation and intuition behind the algorithm is to prevent overﬁtting associated with the co-adaptation of feature
detectors. By randomly dropping out neurons, the procedure prevents any neuron from relying excessively on the output of
any other neuron, forcing it instead to rely on the population behavior of its inputs. It can be viewed as an extreme form of

* Corresponding author.

E-mail address: pfbaldici@uci.edu (P. Baldi).

http://dx.doi.org/10.1016/j.artint.2014.02.004
0004-3702 © 2014 The Authors. Published by Elsevier B.V.

Open access under CC BY-NC-ND license.

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

79

Fig. 1.1. Dropout training in a simple network. For each training example, feature detector units are dropped with probability 0.5. The weights are trained
by backpropagation (BP) and shared with all the other examples.

Fig. 1.2. Dropout prediction in a simple network. At prediction time, all the weights from the feature detectors to the output units are halved.

bagging [17], or as a generalization of naive Bayes [23], as well as denoising autoencoders [42]. Dropout has been reported
to yield remarkable improvements on several diﬃcult problems, for instance in speech and image recognition, using well
known benchmark datasets, such as MNIST, TIMIT, CIFAR-10, and ImageNet [27].

In [27], it is noted that for a single unit dropout performs a kind of “geometric” ensemble averaging and this property
is conjectured to extend somehow to deep multilayer neural networks. Thus dropout is an intriguing new algorithm for
shallow and deep learning, which seems to be effective, but comes with little formal understanding and raises several
interesting questions. For instance:

1. What kind of model averaging is dropout implementing, exactly or in approximation, when applied to multiple layers?
2. How crucial are its parameters? For instance, is q = 0.5 necessary and what happens when other values are used? What

happens when other transfer functions are used?

3. What are the effects of different deletion randomization procedures, or different values of q for different layers? What

happens if dropout is applied to connections rather than units?

4. What are precisely the regularization and averaging properties of dropout?
5. What are the convergence properties of dropout?

To answer these questions, it is useful to distinguish the static and dynamic aspects of dropout. By static we refer to
properties of the network for a ﬁxed set of weights, and by dynamic to properties related to the temporal learning process.
We begin by focusing on static properties, in particular on understanding what kind of model averaging is implemented
by rules like “halving all the weights”. To some extent this question can be asked for any set of weights, regardless of the
learning stage or procedure. Furthermore, it is useful to ﬁrst study the effects of droupout in simple networks, in particular
in linear networks. As is often the case [8,9], understanding dropout in linear networks is essential for understanding
dropout in non-linear networks.

Related work. Here we point out a few connections between dropout and previous literature, without any attempt at being
exhaustive, since this would require a review paper by itself. First of all, dropout is a randomization algorithm and as such
it is connected to the vast literature in computer science and mathematics, sometimes a few centuries old, on the use
of randomness to derive new algorithms, improve existing ones, or prove interesting mathematical results (e.g. [22,3,33]).

80

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Second, and more speciﬁcally, the idea of injecting randomness into a neural network is hardly new. A simple Google search
yields dozen of references, many dating back to the 1980s (e.g. [24,25,30,34,12,6,37]). In these references, noise is typically
injected either in the input data or in the synaptic weights to increase robustness or regularize the network in an empirical
way. Injecting noise into the data is precisely the idea behind denoising autoencoders [42], perhaps the closest predecessor
to dropout, as well as more recent variations, such as the marginalized-corrupted-features learning approach described in
[29]. Finally, since the posting of [27], three articles with dropout in their title were presented at the NIPS 2013 conference:
a training method based on overlaying a dropout binary belief network on top of a neural network [7]; an analysis of the
adaptive regularizing properties of dropout in the shallow linear case suggesting some possible improvements [43]; and a
subset of the averaging and regularization properties of dropout described primarily in Sections 8 and 11 of this article [10].

2. Dropout for shallow linear networks

In order to compute expectations, we must associate well deﬁned random variables with unit activities or connection
weights when these are dropped. Here and everywhere else we will consider that a unit activity or connection is set to 0
when the unit or connection is dropped.

2.1. Dropout for a single linear unit (combinatorial approach)

We begin by considering a single linear unit computing a weighted sum of n inputs of the form

S = S(I) =

n(cid:2)

i=1

w i Ii

(1)

where I = (I1, . . . , In) is the input vector. If we delete inputs with a uniform distribution over all possible subsets of inputs,
or equivalently with a probability q = 0.5 of deletion, then there are 2n possible networks, including the empty network.
For a ﬁxed I , the average output over all these networks can be written as:

E(S) = 1
2n

(cid:2)

N

S(N , I)

(2)

where N is used to index all possible sub-networks, i.e. all possible edge deletions. Note that in this simple case, deletion
of input units or of edges are the same thing. The sum above can be expanded using networks of size 0, 1, 2, . . . , n in the
form

(cid:3)

(cid:4)

(cid:5)

(cid:6) (cid:2)

(cid:7)

w i Ii + w j I j

+ · · ·

(cid:8)

w i Ii

+

E(S) = 1
2n

0 +

n(cid:2)

i=1

1(cid:2)i< j(cid:2)n

(cid:7)

(cid:6)

n − 1
n − 1

= 2n−1

In this expansion, the term w i Ii occurs
(cid:7)

(cid:7)

(cid:6)

(cid:6)

1 +

n − 1
1

n − 1
2

+ · · · +

times. So ﬁnally the average output is

E(S) = 2n−1
2n

(cid:5)

w i Ii

=

n(cid:2)

i=1

n(cid:2)

i=1

w i
2

Ii

+

(cid:4)

(3)

(4)

(5)

Thus in the case of a single linear unit, for any ﬁxed input I the output obtained by halving all the weights is equal to the
arithmetic mean of the outputs produced by all the possible sub-networks. This combinatorial approach can be applied to
other cases (e.g. p (cid:3)= 0.5) but it is much easier to work directly with a probabilistic approach.

2.2. Dropout for a single linear unit (probabilistic approach)

Here we simply consider that the output is a random variable of the form

S =

n(cid:2)

i=1

w iδi Ii

(6)

where δi is a Bernoulli selector random variable, which deletes the weight w i (equivalently the input Ii ) with probability
P (δi = 0) = qi . The Bernoulli random variables are assumed to be independent of each other (in fact pairwise independence,
as opposed to global independence, is suﬃcient for all the results to be presented here). Thus P (δi = 1) = 1 − qi = pi . Using
the linearity of the expectation we have immediately

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

E(S) =

n(cid:2)

i=1

w i E(δi)Ii =

n(cid:2)

i=1

w i pi Ii

81

(7)

This formula allows one to handle different pi for each connection, as well as values of pi that deviate from 0.5. If all the
connections are associated with independent but identical Bernoulli selector random variables with pi = p, then

E(S) =

n(cid:2)

i=1

w i E(δ)Ii =

n(cid:2)

i=1

w i p Ii

(8)

(cid:9)

Thus note, for instance, that if the inputs are deleted with probability 0.2 then the expected output is given by 0.8
i w i Ii .
Thus the weights must be multiplied by 0.8. The key property behind Eq. (8) is the linearity of the expectation with respect
to sums and multiplications by scalar values, and more generally for what follows the linearity of the expectation with
respect to the product of independent random variables. Note also that the same approach could be applied for estimating
expectations over the input variables, i.e. over training examples, or both (training examples and subnetworks). This remains
true even when the distribution over examples is not uniform.

If the unit has a ﬁxed bias b (aﬃne unit), the random output variable has the form

S =

n(cid:2)

i=1

w iδi Ii + bδb

(9)

The case where the bias is always present, i.e. when δb = 1 always, is just a special case. And again, by linearity of the
expectation

E(S) =

n(cid:2)

i=1

w i pi Ii + bpb

(10)

where P (δb = 1) = pb. Under the natural assumption that the Bernoulli random variables are independent of each other, the
variance is linear with respect to the sum and can easily be calculated in all the previous cases. For instance, starting from
the most general case of Eq. (9) we have

Var(S) =

n(cid:2)

i=1

w 2

i Var(δi)I 2

i

+ b2 Var(δb) =

n(cid:2)

i=1

w 2

i piqi I 2

i

+ b2 pbqb

(11)

with qi = 1 − pi . S can be viewed as a weighted sum of independent Bernoulli random variables, which can be approximated
by a Gaussian random variable under reasonable assumptions.

2.3. Dropout for a single layer of linear units

We now consider a single linear layer with k output units

S i(I) =

n(cid:2)

j=1

w i j I j

for i = 1, . . . , k

(12)

In this case, dropout applied to input units is slightly different from dropout applied to the connections. Dropout applied to
the input units leads to the random variables

S i(I) =

n(cid:2)

j=1

w i jδ j I j

for i = 1, . . . , k

whereas dropout applied to the connections leads to the random variables

S i(I) =

n(cid:2)

j=1

δi j w i j I j

for i = 1, . . . , k

(13)

(14)

In either case, the expectations, variances, and covariances can easily be computed using the linearity of the expectation
and the independence assumption. When dropout is applied to the input units, we get:

E(S i) =

n(cid:2)

j=1

w i j p j I j

for i = 1, . . . , k

(15)

82

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Var(S i) =

n(cid:2)

j=1

w 2

i j p jq j I 2

j

for i = 1, . . . , k

Cov(S i, Sl) =

n(cid:2)

j=1

w i j wlj p jq j I 2
j

for 1 (cid:2) i < l (cid:2) k

When dropout is applied to the connections, we get:

E(S i) =

n(cid:2)

j=1

w i j pi j I j

for i = 1, . . . , k

Var(S i) =

n(cid:2)

j=1

w 2

i j pi jqi j I 2

j

for i = 1, . . . , k

Cov(S i, Sl) = 0 for 1 (cid:2) i < l (cid:2) k

(16)

(17)

(18)

(19)

(20)

Note the difference in covariance between the two models. When dropout is applied to the connections, S i and Sl are
entirely independent.

3. Dropout for deep linear networks

In a general feedforward linear network described by an underlying directed acyclic graph, units can be organized into
layers using the shortest path from the input units to the unit under consideration. The activity in unit i of layer h can be
expressed as:

i (I) =
Sh

(cid:2)

(cid:2)

l<h

j

whl

i j Sl

j with S 0

j

= I j

(21)

Again, in the general case, dropout applied to the units is slightly different from dropout applied to the connections. Dropout
applied to the units leads to the random variables

=

Sh
i

(cid:2)

(cid:2)

l<h

j

whl

i j δl

j Sl

j with S 0

j

= I j

whereas dropout applied to the connections leads to the random variables

=

Sh
i

(cid:2)

(cid:2)

l<h

j

i j whl
δhl

i j Sl

j with S 0

j

= I j

(22)

(23)

When dropout is applied to the units, assuming that the dropout process is independent of the unit activities or the

weights, we get:
(cid:10)

(cid:11)

(cid:2)

E

Sh
i

=

(cid:2)

whl

i j pl

j E

(cid:11)

(cid:10)

Sl
j

for h > 0

(24)

l<h

j

j ) = I j in the input layer. This formula can be applied recursively across the entire network, starting from the
with E(S 0
input layer. Note that the recursion of Eq. (24) is formally identical to the recursion of backpropagation suggesting the use
of dropout during the backward pass. This point is elaborated further at the end of Section 10. Note also that although the
expectation E(Sh
i ) is taken over all possible subnetworks of the original network, only the Bernoulli gating variables in the
previous layers (l < h) matter. Therefore it coincides also with the expectation taken over only all the induced subnetworks
of node i (comprising only nodes that are ancestors of node i).

Remarkably, using these expectations, all the covariances can also be computed recursively from the input layer to the

output layer, by writing Cov(Sh

(cid:4)

i , Sh

i(cid:4) ) = E(Sh
(cid:2)

i Sh
(cid:2)

(cid:4)

i(cid:4) ) − E(Sh

i )E(Sh
(cid:13)

i(cid:4) ) and computing
(cid:2)
(cid:2)

(cid:2)

(cid:2)

(cid:12)(cid:2)

(cid:2)

(cid:11)

(cid:10)

E

(cid:4)
i Sh
Sh
i(cid:4)

= E

(cid:4)

(cid:4)

l

wh

i(cid:4) j(cid:4) δl

(cid:4)

(cid:4)
j(cid:4) Sl
j(cid:4)

=

whl

i j wh

(cid:4)
(cid:4)
l
i(cid:4) j(cid:4) E

(cid:10)
(cid:4)
jδl
δl
j(cid:4)

(cid:11)

(cid:10)

E

(cid:4)
j Sl
Sl
j(cid:4)

(cid:11)

(25)

l<h

j

l(cid:4)<h(cid:4)

j(cid:4)

l<h

l(cid:4)<h(cid:4)

j

j(cid:4)

under the usual assumption that δl
independent when l (cid:3)= l
the usual independence assumptions, E(Sh

j(cid:4) is independent of Sl
jδl
, we have in this case E(δl
(cid:4)

j(cid:4) . Furthermore, under the usual assumption that δl
j(cid:4) ) = pl

j) = pl
i(cid:4) ) can be computed recursively from the values of E(Sl

j(cid:4) , with furthermore E(δl

j(cid:4) are
j . Thus in short under
j Sl
j(cid:4) ) in lower layers,

j and δl

or j (cid:3)= j

j Sl
jδl

i Sh

j pl

jδl

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

(cid:4)

whl

i j δl

j Sl

j

(cid:4)

(cid:4)

(cid:4)

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

83

with the boundary conditions E(Ii I j) = Ii I j for a ﬁxed input vector (layer 0). The recursion proceeds layer by layer, from
the input to the output layer. When a new layer is reached, the covariances to all the previously visited layers must be
computed, as well as all the intralayer covariances.

When dropout is applied to the connections, under similar independence assumptions, we get:

(cid:2)

(cid:2)

(cid:11)

(cid:10)

E

Sh
i

=

i j whl
phl
i j E

(cid:11)

(cid:10)

Sl
j

for h > 0

l<h

j

(26)

j ) = I j

with E(S 0
in the input layer. This formula can be applied recursively across the entire network. Note again that
although the expectation E(Sh
i ) is taken over all possible subnetworks of the original network, only the Bernoulli gating
variables in the previous layers (l < h) matter. Therefore it is also the expectation taken over only all the induced subnet-
works of node i (corresponding to all the ancestors of node i). Furthermore, using these expectations, all the covariances
can also be computed recursively from the input layer to the output layer using a similar analysis to the one given above
for the case of dropout applied to the units of a general linear network.

In summary, for linear feedforward networks the static properties of dropout applied to the units or the connections using Bernoulli
gating variables that are independent of the weights, of the activities, and of each other (but not necessarily identically distributed) can
be fully understood. For any input, the expectation of the outputs over all possible networks induced by the Bernoulli gating variables
is computed using the recurrence equations (24) and (26), by simple feedforward propagation in the same network where each weight
is multiplied by the appropriate probability associated with the corresponding Bernoulli gating variable. The variances and covariances
can also be computed recursively in a similar way.

4. Dropout for shallow neural networks

We now consider dropout in non-linear networks that are shallow, in fact with a single layer of weights.

4.1. Dropout for a single non-linear unit (logistic)

Here we consider that the output of a single unit with total linear input S is given by the logistic sigmoidal function

O = σ (S) =

1
1 + ce−λS

(27)

Here and everywhere else, we must have c (cid:3) 0 There are 2n possible sub-networks indexed by N and, for a ﬁxed input I ,
each sub-network produces a linear value S(N , I) and a ﬁnal output value O N = σ (N ) = σ (S(N , I)). Since I is ﬁxed, we
omit the dependence on I in all the following calculations. In the uniform case, the geometric mean of the outputs is given
by

G =

(cid:14)

N

1/2n
N

O

Likewise, the geometric mean of the complementary outputs (1 − O N ) is given by

(cid:14)

(1 − O N )1/2n

(cid:4) =

G

N

The normalized geometric mean (NGM) is deﬁned by

NGM = G

G + G(cid:4)

The NGM of the outputs is given by

(cid:10)

(cid:11)
O (N )

NGM

=

(cid:15)
[

(cid:15)
[
N σ (S(N ))]1/2n + [

N σ (S(N ))]1/2n
(cid:15)

N (1 − σ (S(N )))]1/2n

=

(cid:15)

1 + [

1

1−σ (S(N ))
σ (S(N ))

N

]1/2n

Now for the logistic function σ , we have

1 − σ (x)
σ (x)

= ce

−λx

Applying this identity to Eq. (31) yields

(cid:10)

(cid:11)
O (N )

=

NGM

(cid:15)

1 + [

1

N ce−λS(N )]1/2n

=

1 + c[e

−λ

1
(cid:9)

N S(N )/2n ]

(cid:10)

(cid:11)
E(S)

= σ

(28)

(29)

(30)

(31)

(32)

(33)

(35)

(36)

(37)

(38)

(39)

84

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

(cid:9)

where here E(S) =
(cid:11)
σ (S)

(cid:10)

(cid:11)
E(S)

= σ

N S(N )/2n. Or, in more compact form,
(cid:10)

NGM

(34)
Thus with a uniform distribution over all possible sub-networks N , equivalent to having i.i.d. input unit selector variables
δ = δi with probability pi = 0.5, the NGM is simply obtained by keeping the same overall network but dividing all the
(cid:9)
weights by two and applying σ to the expectation E(S) =
n
i=1
It is essential to observe that this result remains true in the case of a non-uniform distribution over the subnetworks N ,
such as the distribution generated by Bernoulli gating variables that are not identically distributed, or with p (cid:3)= 0.5. For
this we consider a general distribution P (N ). This is of course even more general than assuming the P is the product of n
independent Bernoulli selector variables. In this case, the weighted geometric means are deﬁned by:

w i
2 Ii .

(cid:14)

G =

O P (N )
N

N

and

(cid:14)

(1 − O N )P (N )

(cid:4) =

G

N

and similarly for the normalized weighted geometric mean (NWGM)

NWGM = G

G + G(cid:4)

Using the same calculation as above in the uniform case, we can then compute the normalized weighted geometric mean
NWGM in the form

(cid:10)

(cid:11)
O (N )

=

(cid:15)

NWGM

(cid:10)

(cid:11)
O (N )

=

NWGM

(cid:9)

(cid:15)

N σ (S(N ))P (N )
(cid:15)

N σ (S(N ))P (N ) +
1
N ( 1−σ (S(N ))
σ (S(N )

(cid:15)

1 +

)P (N )

N (1 − σ (S(N )))P (N )

=

1 + ce

−λ

1
(cid:9)
N P (N )S(N )

(cid:10)

(cid:11)
E(S)

= σ

N P (N )S(N ). Thus in summary with any distribution P (N ) over all possible sub-networks N , including the
where here E(S) =
case of independent but not identically distributed input unit selector variables δi with probability pi , the NWGM is simply obtained
by applying the logistic function to the expectation of the linear input S. In the case of independent but not necessarily identically
distributed selector variables δi , each with a probability pi of being equal to one, the expectation of S can be computed simply by
keeping the same overall network but multiplying each weight w i by pi so that E(S) =

(cid:9)
n
i=1 pi w i Ii .

Note that as in the linear case, this property of logistic units is even more general. That is for any set of S 1, . . . , Sm and
i=1 P i = 1) and associated outputs O 1, . . . , O m (with O = σ (S)), we
any associated probability distribution P 1, . . . , P m (
have NWGM(O ) = σ (E) = σ (
i P i S i). Thus the NVGM can be computed over inputs, over inputs and subnetworks, or over
other distributions than the one associated with subnetworks, even when the distribution is not uniform. For instance, if
we add Gaussian or other noise to the weights, the same formula can be applied. Likewise, we can approximate the average
activity of an entire neuronal layer, by applying the logistic function to the average input of the neurons in that layer, as
long as all the neurons in the layer use the same logistic function. Note also that the property is true for any c and λ
and therefore, using the analyses provided in the next sections, it will be applicable to each of the units, in a network
where different units have different values of c and λ. Finally, the property is even more general in the sense that the same
calculation as above shows that for any function f

(cid:9)

(cid:9)

m

(cid:10)

(cid:10)

σ

(cid:11)(cid:11)

(cid:10)

(cid:10)

(cid:11)(cid:11)

f (S)

= σ

E

f (S)

NWGM

and in particular, for any k
(cid:10)
(cid:10)

(cid:11)(cid:11)

(cid:10)

NWGM

σ

Sk

= σ

(cid:11)(cid:11)

(cid:10)

E

Sk

4.2. Dropout for a single layer of logistic units

In the case of a single output layer of k logistic functions, the network computes k linear sums S i =

i = 1, . . . , k and then k outputs of the form

(40)

(41)

(cid:9)
n
j=1 w i j I j for

O i = σi(S i)

(42)
The dropout procedure produces a subnetwork M = (N1, . . . , Nk) where Ni here represents the corresponding sub-network
associated with the i-th output unit. For each i, there are 2n possible sub-networks for unit i, so there are 2kn possible
subnetworks M. In this case, Eq. (39) holds for each unit individually. If dropout uses independent Bernoulli selector
variables δi j on the edges, or more generally, if the sub-networks (N1, . . . , Nk) are selected independently of each other,
then the covariance between any two output units is 0. If dropout is applied to the input units, then the covariance between
two sigmoidal outputs may be small but non-zero.

4.3. Dropout for a set of normalized exponential units

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

85

We now consider the case of one layer of normalized exponential units. In this case, we can think of the network as
j=1 w i j I j for i = 1, . . . , k and then k outputs

having k outputs obtained by ﬁrst computing k linear sums of the form S i =
of the form

(cid:9)
n

O i =

eλS i
(cid:9)
k
j=1 eλS j

=

(cid:9)

1 + (

1

j(cid:3)=i eλS j )e−λS i

(43)

Thus O i is a logistic output but the coeﬃcients of the logistic function depend on the values of S j for j (cid:3)= i. The dropout
procedure produces a subnetwork M = (N1, . . . , Nk) where Ni represents the corresponding sub-network associated with
the i-th output unit. For each i, there are 2n possible sub-networks for unit i, so there are 2kn possible subnetworks M.
We assume ﬁrst that the distribution P (M) is factorial, that is P (M) = P (N1) . . . P (Nk), equivalent to assuming that the
subnetworks associated with the individual units are chosen independently of each other. This is the case when using
independent Bernoulli selector applied to the connections. The normalized weighted geometric average of output unit i is
given by

NWGM(O i) =

(cid:15)

M(
(cid:15)

(cid:9)
k
l=1

eλSi (N
(cid:9)
k
j=1 e

i )
λS j (N

j ) )P (M)

M(

eλSl (N
l )
(cid:9)
λS j (N
k
j=1 e

j ) )P (M)

Simplifying by the numerator

NWGM(O i) =

i ) )P (M)
Factoring and collecting the exponential terms gives

1 +

(cid:9)
k
l=1,l(cid:3)=i

(cid:15)

1
M( eλSl (N
l )
eλSi (N

NWGM(O i) =

(cid:9)

1 + e

−

M λP (M)S i(Ni )

1
(cid:9)
k
l=1,l(cid:3)=i e

(cid:9)

M λP (M)Sl(Nl)

(44)

(45)

(46)

1 + e−λE(S i )

NWGM(O i) =

1
(cid:9)
k
l=1,l(cid:3)=i eλE(Sl)

= eλE(S i )
(cid:9)
k
l=1 eλE(Sl)
Thus with any distribution P (N ) over all possible sub-networks N , including the case of independent but not identically distributed
input unit selector variables δi with probability pi , the NWGM of a normalized exponential unit is obtained by applying the normalized
exponential to the expectations of the underlying linear sums S i . In the case of independent but not necessarily identically distributed
selector variables δi , each with a probability pi of being equal to one, the expectation of S i can be computed simply by keeping the
same overall network but multiplying each weight w i by pi so that E(S i) =

(cid:9)
n
j=1 p j w i I j .

(47)

5. Dropout for deep neural networks

Finally, we can deal with the most interesting case of deep feedforward networks of sigmoidal units,1 described by a set

of equations of the form

O h
i

= σ h
i

(cid:11)

(cid:10)

Sh
i

= σ

(cid:6)(cid:2)

(cid:2)

l<h

j

(cid:7)

with O 0
j

= I j

whl

i j O l

j

Dropout on the units can be described by

O h
i

= σ h
i

(cid:11)

(cid:10)

Sh
i

= σ

(cid:6)(cid:2)

(cid:2)

l<h

j

(cid:7)

with O 0
j

= I j

whl

i j δl

j O l

j

using the selector variables δl

j and similarly for dropout on the connections. For each sigmoidal unit

NWGM

(cid:10)

(cid:11)

O h
i

=

(cid:15)

(cid:15)

i )P (N )
N (O h
(cid:15)
N (1 − O h

i )P (N )

N (O h

i )P (N ) +

1 Given the results of the previous sections, the network can also include linear units or normalized exponential units.

(48)

(49)

(50)

86

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

and the basic idea is to approximate expectations by the corresponding NWGMs, allowing the propagation of the expectation
symbols from outside the sigmoid symbols to inside.

(cid:10)

(cid:16)
σ

E

(cid:11)(cid:17)

S(N , I)

≈ NWGM

(cid:16)

(cid:17)
O (N , I)

(cid:10)

(cid:16)

E

= σ

(cid:17)(cid:11)

S(N , I)

More precisely, we have the following recursion:

(cid:11)

(cid:10)

E

O h
i

NWGM
(cid:11)

(cid:10)

E

Sh
i

=

≈ NWGM
(cid:11)
(cid:10)
= σ h
O h
i
i
(cid:2)
(cid:2)

(cid:10)

(cid:11)

O h
i
(cid:10)
(cid:16)

(cid:11)(cid:17)

Sh
E
i
i j pl

j E

whl

(cid:11)

(cid:10)

O l
j

(51)

(52)

(53)

(54)

l<h

j

Eqs. (52), (53), and (54) are the fundamental equations underlying the recursive dropout ensemble approximation in deep neural
networks. The only direct approximation in these equations is of course Eq. (52) which will be discussed in more depth in Sections
i are identical over all possible subnetworks N . However, even when
8 and 9. This equation is exact if and only if the numbers O h
the numbers O h
i are not identical, the normalized weighted geometric mean often provides a good approximation. If the network
contains linear units, then Eq. (52) is not necessary for those units and their average can be computed exactly. The only
fundamental assumption for Eq. (54) is independence of the selector variables from the activity of the units or the value of
the weights so that the expectation of the product is equal to the product of the expectations. Under the same conditions,
the same analysis can be applied to dropout gating variables applied to the connections or, for instance, to Gaussian noise
added to the unit activities.

Finally, we measure the consistency C(O h

i , I) of neuron i in layer h for input I by the variance Var[O h

i (I)] taken over all
subnetworks N and their distribution when the input I is ﬁxed. The larger the variance is, the less consistent the neuron is,
and the worse we can expect the approximation in Eq. (52) to be. Note that for a random variable O in [0, 1] the variance
is bound to be small anyway, and cannot exceed 1/4. This is because Var(O ) = E(O 2) − (E(O ))2 (cid:2) E(O ) − (E(O ))2 =
E(O )(1 − E(O )) (cid:2) 1/4. The overall input consistency of such a neuron can be deﬁned as the average of C(O h
i , I) taken over
all training inputs I , and similar deﬁnitions can be made for the generalization consistency by averaging C(O h
i , I) over a
generalization set.

Before examining the quality of the approximation in Eq. (52), we study the properties of the NWGM for aver-
aging ensembles of predictors, as well as the classes of transfer functions satisfying the key dropout NWGM relation
(NWGM( f (x)) = f (E(x))) exactly, or approximately.

6. Ensemble optimization properties

The weights of a neural network are typically trained by gradient descent on the error function computed using the
outputs and the corresponding targets. The error functions typically used are the squared error in regression and the relative
entropy in classiﬁcation. Considering a single example and a single output O with a target t, these errors functions can be
written as:

Error(O , t) = 1
2

(t − O )2

and Error(O , t) = −t log O − (1 − t) log(1 − O )

(55)

Extension to multiple outputs, including classiﬁcation with multiple classes using normalized exponential transfer functions,
is immediate. These error terms can be summed over examples or over predictors in the case of an ensemble. Both error
functions are convex up (∪) and thus a simple application of Jensen’s theorem shows immediately that the error of any
ensemble average is less than the average error of the ensemble components. Thus in the case of any ensemble producing
outputs O 1, . . . , O m and any convex error function we have
(cid:2)

(cid:6)(cid:2)

(cid:7)

Error

pi O i, t

(cid:2)

pi Error(O i, t) or Error(E) (cid:2) E(Error)

i

i

Note that this is true for any individual example and thus it is also true over any set of examples, even when these are not
identically distributed. Eq. (56) is the key equation for using ensembles and for averaging them arithmetically.

In the case of dropout with a logistic output unit the previous analyses show that the NWGM is an approximation to
E and on this basis alone it is a reasonable way of combining the predictors in the ensemble of all possible subnetworks.
However the following stronger result holds. For any convex error function, both the weighted geometric mean WGM and
its normalized version NWGM of an ensemble possess the same qualities as the expectation. In other words:

(cid:6)(cid:14)

(cid:7)

Error

i

(cid:6)

Error

(cid:15)

O

pi
i , t

(cid:2)

(cid:15)

i O
(cid:15)

(cid:2)

i
pi
i

i O

pi
i

+

i(1 − O i)pi

pi Error(O i, t) or Error(WGM) (cid:2) E(Error)

(cid:7)

, t

(cid:2)

(cid:2)

i

pi Error(O i, t) or Error(NWGM) (cid:2) E(Error)

(56)

(57)

(58)

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

87

In short, for any convex error function, the error of the expectation, weighted geometric mean, and normalized weighted geometric
mean of an ensemble of predictors is always less than the expected error.

f

is convex and g is increasing, then the composition f (g) is convex. This is easily shown by di-
Proof. Recall that if
rectly applying the deﬁnition of convexity (see [39,16] for additional background on convexity). Eq. (57) is obtained by
applying Jensen’s inequality to the convex function Error(g), where g is the increasing function g(x) = ex, using the points
log O 1, . . . , log O m. Eq.(58)
is obtained by applying Jensen’s inequality to the convex function Error(g), where g is the in-
creasing function g(x) = ex/(1 + ex), using the points log O 1 − log(1 − O 1), . . . , log O m − log(1 − O m). The cases where some
of the O i are equal to 0 or 1 can be handled directly, although these are irrelevant for our purposes since the logistic output
can never be exactly equal to 0 or 1.

Thus in circumstances where the ﬁnal output is equal to the weighted mean, weighted geometric mean, or normal-
ized weighted geometric mean of an underlying ensemble, Eqs. (56), (57), or (58) apply exactly. This is the case, for
instance, of linear networks, or non-linear networks where dropout is applied only to the output layer with linear, logistic,
or normalized-exponential units.

Since dropout approximates expectations using NWGMs, one may be concerned by the errors introduced by such ap-
proximations, especially in a deep architecture when dropout is applied to multiple layers. It is worth noting that the result
above can be used at least to “shave off” one layer of approximations by legitimizing the use of NWGMs to combine models
in the output layer, instead of the expectation. Similarly, in the case of a regression problem, if the output units are linear
then the expectations can be computed exactly at the level of the output layer using the results above on linear networks,
thus reducing by one the number of layers where the approximation of expectations by NWGMs must be carried. Finally,
as shown below, the expectation, the WGM, and the NWGM are relatively close to each other and thus there is some ﬂex-
ibility, hence some robustness in how predictors are combined in an ensemble, in the sense that combining models with
approximations to these quantities may still outperform the expectation of the error of the individual models.

Finally, it must also be pointed out that in the prediction phase once can also use expected values, estimated at some
computational cost using Monte Carlo methods, rather than approximate values obtained by forward propagation in the
network with modiﬁed weights.

7. Dropout functional classes and transfer functions

7.1. Dropout functional classes

Dropout seems to rely on the fundamental property of the logistic sigmoidal function NWGM(σ ) = σ (E). Thus it is
natural to wonder what is the class of functions f satisfying this property. Here we show that the class of functions f deﬁned
on the real line with range in [0, 1] and satisfying

G

G + G(cid:4) ( f ) = f (E)

(59)

for any set of points and any distribution, consists exactly of the union of all constant functions f (x) = K with 0 (cid:2) K (cid:2) 1 and all
−λx). As a reminder, G denotes the geometric mean and G
logistic functions f (x) = 1/(1 + ce
denotes the geometric mean
of the complements. Note also that all the constant functions with f (x) = K with 0 (cid:2) K (cid:2) 1 can also be viewed as logistic
functions by taking λ = 0 and c = (1 − K )/K (K = 0 is a limiting case corresponding to c → ∞).

(cid:4)

Proof. To prove this result, note ﬁrst that the [0, 1] range is required by the deﬁnitions of G and G
, since these impose
that f (x) and 1 − f (x) be positive. In addition, any function f (x) = K with 0 (cid:2) K (cid:2) 1 is in the class and we have shown
that the logistic functions satisfy the property. Thus we need only to show these are the only solutions.

By applying Eq. (59) to pairs of arguments, for any real numbers u and v with u (cid:2) v and any real number 0 (cid:2) p (cid:2) 1,

(cid:4)

any function in the class must satisfy:

f (u)p f (v)1−p
f (u)p f (v)1−p + (1 − f (u))p(1 − f (v))1−p

(cid:10)

= f

pu + (1 − p)v

(cid:11)

(60)

Note that if f (u) = f (v) then the function f must be constant over the entire interval [u, v]. Note also that if f (u) = 0 and
f (v) > 0 then f = 0 in [u, v). As a result, it is impossible for a non-zero function in the class to satisfy f (u) = 0, f (v 1) > 0,
and f (v 2) > 0. Thus if a function f
in the class is not constantly equal to 0, then f > 0 everywhere. Similarly (and by
symmetry), if a function f

in the class is not constantly equal to 1, then f < 1 everywhere.

Consider now a function f

where. Eq. (60) shows that on any interval [u, v] f
this interval, by letting x = pu + (1 − p)v or equivalently p = (v − x)/(v − u) the function is given by

in the class, different from the constant 0 or constant 1 function so that 0 < f < 1 every-
is completely deﬁned by at most two parameters f (u) and f (v). On

f (x) =

(cid:10)

1 +

1− f (u)
f (u)

1
(cid:11) v−x
v−u

(cid:10)

(cid:11) x−u
v−u

1− f (v)
f (v)

(61)

88

or

with

and

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

f (x) =

1
1 + ce−λx

(cid:6)

c =

1 − f (u)
f (u)

(cid:7) v
v−u

(cid:6)

1 − f (v)
f (v)

(cid:7) −u
v−u

(cid:6)

λ = 1

v − u

log

1 − f (u)
f (u)

f (v)
1 − f (v)

(cid:7)

Note that a particular simple parameterization is given in terms of

f (0) = 1
1 + c

and

f (x) = 1
2

for x = log c
λ

(62)

(63)

(64)

(65)

[As a side note, another elegant formula is obtained from Eq. (60) for f (0) by taking u = −v and p = 0.5. Simple algebraic
manipulations give:

1 − f (0)
f (0)

=

(cid:6)

1 − f (−v)
f (−v)

(cid:6)

(cid:7) 1
2

(cid:7) 1
2

1 − f (v)
f (v)

(66)

.] As a result, on any interval [u, v] the function f must be: (1) continuous, hence uniformly continuous; (2) differentiable,
in fact inﬁnitely differentiable; (3) monotone increasing or decreasing, and strictly so if f
is constant; (4) and therefore f
must have well deﬁned limits at −∞ and +∞. It is easy to see that the limits can only be 0 or 1. For instance, for the limit
at +∞, let u = 0 and v

(cid:4) = α v, with 0 < α < 1 so that v

(cid:4) → ∞ as v → ∞. Then

(cid:11)

(cid:4)

(cid:10)

f

v

=

(cid:10)

1 +

1− f (0)
f (0)

1
1−α(cid:10)
(cid:11)

(cid:11)α

1− f (v)
f (v)

(67)

As v

(cid:4) → ∞ the limit must be independent of α and therefore the limit f (v) must be 0 or 1.
Finally, consider u1 < u2 < u3. By the above results, the quantities f (u1) and f (u2) deﬁne a unique logistic function
on [u1, u2], and similarly f (u2) and f (u3) deﬁne a unique logistic function on [u2, u3]. It is easy to see that these two
logistic functions must be identical either because of the analycity or just by taking two new points v 1 and v 2 with
u1 < v 1 < u2 < v 2 < u3. Again f (v 1) and f (v 2) deﬁne a unique logistic function on [v 1, v 2] which must be identical to the
other two logistic functions on [v 1, u2] and [u2, v 2] respectively. Thus the three logistic functions above must be identical.
In short, f (u) and f (v) deﬁne a unique logistic function inside [u, v], with the same unique continuation outside of [u, v].
From this result, one may incorrectly infer that dropout is brittle and overly sensitive to the use of logistic non-linear
functions. This conclusion is erroneous for several reasons. First, the logistic function is one of the most important and
widely used transfer functions in neural networks. Second, regarding the alternative sigmoidal function tanh(x), if we
translate it upwards and normalize it so that its range is the [0, 1] interval, then it reduces to a logistic function since
−2x). This leads to the formula: NWGM((1 + tanh(x))/2) = (1 + tanh(E(x)))/2. Note also that the
(1 + tanh(x))/2 = 1/(1 + e
NWGM approach cannot be applied directly to tanh, or any other transfer function which assumes negative values, since G
and NWGM are deﬁned for positive numbers only. Third, even if one were to use a different sigmoidal function, such as
1 + x2, when rescaled to [0, 1] its deviations from the logistic function may be small and lead to ﬂuctu-
arctan(x) or x/
ations that are in the same range as the ﬂuctuations introduced by the approximation of E by NWGM. Fourth and most
importantly, dropout has been shown to work empirically with several transfer functions besides the logistic, including for
instance tanh and rectiﬁed linear functions. This point is addressed in more detail in the next section. In any case, for
all these reasons one should not be overly concerned by the superﬁcially fragile algebraic association between dropout,
NWGMs, and logistic functions.

√

7.2. Dropout transfer functions

In deep learning, one is often interested in using alternative transfer functions, in particular rectiﬁed linear functions
which can alleviate the problem of vanishing gradients during backpropagation. As pointed out above, for any transfer
function it is always possible to compute the ensemble average at prediction time using sampling. However, we can show
that the ensemble averaging property of dropout is preserved to some extent also for rectiﬁed linear transfer functions, as
well for broader classes of transfer functions.

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

89

To see this, we ﬁrst note that, while the properties of the NWGM are useful for logistic transfer functions, the NWGM
is not needed to enable the approximation of the ensemble average by deterministic forward propagation. For any transfer
function f , what is really needed is the relation

(cid:10)

(cid:11)
f (S)

E

(cid:10)

(cid:11)
E(S)

≈ f

(68)

Any transfer function satisfying this property can be used with dropout and allow the estimation of the ensemble at
prediction time by forward propagation. Obviously linear functions satisfy Eq. (68) and this was used in the previous sections
on linear networks. A rectiﬁed linear function RL(S) with threshold t and slope λ has the form

(cid:18)

RL(S) =

if S (cid:2) t

0
λS − λt otherwise

(69)

and is a special case of a piece-wise linear function. Eq. (68) is satisﬁed within each linear portion and will be satisﬁed
around the threshold if the variance of S is small. Everything else being equal, smaller value of λ will also help the approx-
imation. To see this more formally, assume without any loss of generality that t = 0. It is also reasonable to assume that S
is approximately normal with mean μS and variance σ 2
S —a treatment without this assumption is given in Appendix A. In
this case,
(cid:10)
RL

(cid:11)
E(S)

= RL(μS ) =

(70)

(cid:18)

if μS (cid:2) 0
0
λμS otherwise

On the other hand,

(cid:10)
(cid:11)
RL(S)

E

=

+∞(cid:19)

λS

0

1√

2πσS

− (S−μS )2
2σ 2
S

e

+∞(cid:19)

dS = λ

(σS u + μS )

− μS
σS

1√
2π

e

− u2

2 du

and thus

(cid:10)
(cid:11)
RL(S)

E

= λμS Φ

(cid:7)

(cid:6)

μS
σS

+ λσ√
2π

− μ2
S
2σ 2
S

e

where Φ is the cumulative distribution of the standard normal distribution. It is well known that Φ satisﬁes

1 − Φ(x) ≈ 1√
2π

1

x

− x2
2

e

when x is large. This allows us to estimate the error in all the cases. If μS = 0 we have

(cid:20)
(cid:20)E

(cid:10)

(cid:11)
RL(S)

(cid:10)
− RL

E(S)

(cid:11)(cid:20)
(cid:20) = λσ√
2π

(71)

(72)

(73)

(74)

and the error in the approximation is small and directly proportional to λ and σ . If μS < 0 and σS is small, so that |μS |/σS
is large, then Φ(μS /σS ) ≈ 1√
2π
(cid:11)(cid:20)
(cid:11)
(cid:20) ≈ 0
RL(S)

(cid:10)
− RL

σS
|μS | e

S and

S /2σ 2

E(S)

(cid:20)
(cid:20)E

−μ2

(75)

(cid:10)

And similarly for the case when μS > 0 and σS is small, so that μS /σS is large. Thus in all these cases Eq. (68) holds. As we
shall see in Section 11, dropout tends to minimize the variance σS and thus the assumption that σ be small is reasonable.
Together, these results show that the dropout ensemble approximation can be used with rectiﬁed linear transfer functions. It is also
possible to model a population of RL neurons using a hierarchical model where the mean μS is itself a Gaussian random variable. In
this case, the error E(RL(S)) − RL(E(S)) is approximately Gaussian distributed around 0. [This last point will become relevant in
Section 9.]

More generally, the same line of reasoning shows that the dropout ensemble approximation can be used with piece-wise linear
transfer functions as long as the standard deviation of S is small relative to the length of the linear pieces. Having small angles between
subsequent linear pieces also helps strengthen the quality of the approximation.

Furthermore any continuous twice-differentiable function with small second derivative (curvature) can be robustly approximated

by a linear function locally and therefore will tend to satisfy Eq. (68), provided the variance of S is small relative to the curvature.

In this respect, a rectiﬁed linear transfer function can be very closely approximated by a twice-differentiable function by

using the integral of a logistic function. For the standard rectiﬁed linear transfer function, we have

S(cid:19)

S(cid:19)

RL(S) ≈=

σ (x) dx =

−∞

−∞

1
1 + e−λx

dx

(76)

With this approximation, the second derivative is given by σ (cid:4)(S) = λσ (S)(1 − σ (S)) which is always bounded by λ/4.

90

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Finally, for the most general case, the same line of reasoning, shows that the dropout ensemble approximation can be used with
any continuous, piece-wise twice differentiable, transfer function provided the following properties are satisﬁed: (1) the curvature of
each piece must be small; (2) σS must be small relative to the curvature of each piece. Having small angles between the left and right
tangents at each junction point also helps strengthen the quality of the approximation. Note that the goal of dropout training is
precisely to make σS small, that is to make the output of each unit robust, independent of the details of the activities of
the other units, and thus roughly constant over all possible dropout subnetworks.

8. Weighted arithmetic, geometric, and normalized geometric means and their approximation properties

m

(cid:9)

To further understand dropout, one must better understand the properties and relationships of the weighted arithmetic,
geometric, and normalized geometric means and speciﬁcally how well the NWGM of a sigmoidal unit approximates its
expectation (E(σ ) ≈ NWGMS(σ )). Thus consider that we have m numbers O 1, . . . , O m with corresponding probabilities
i=1 P i = 1). We typically assume that the m numbers satisfy 0 < O i < 1 although this is not always necessary
P 1, . . . , P m (
for the results below. Cases where some of the O i are equal to 0 or 1 are trivial and can be examined separately. The case of
interest of course is when the m numbers are the outputs of a sigmoidal unit of the form O (N ) = σ (S(N )) for a given input
I = (I1, . . . , In). We let E be the expectation (weighted arithmetic mean) E =
m
i=1 P i O i and G be the weighted geometric
i=1 P i(1 − O i) be the expectation of the complements, and
mean G =
. When 0 (cid:2) O i (cid:2) 1 we also let E
(cid:15)
(cid:4) = 1 − E. The normalized
G
(cid:4)). We also let V = Var(O ). We then have the following properties.
weighted geometric mean is given by NWGM = G/(G + G

i=1(1 − O i)P i be the weighted geometric mean of the complements. Obviously we have E

i=1 O P i

(cid:4) =

(cid:4) =

(cid:9)

(cid:9)

(cid:15)

m

m

m

i

1. The weighted geometric mean is always less or equal to the weighted arithmetic mean

G (cid:2) E

and G

(cid:4) (cid:2) E

(cid:4)

(77)

with equality if and only if all the numbers O i are equal. This is true regardless of whether the number O i are bounded
by one or not. This results immediately from Jensen’s inequality applied to the logarithmic function. Although not
directly used here, there are interesting bounds for the approximation of E by G, often involving the variance, such as:

1

2 maxi O i

Var(O ) (cid:2) E − G (cid:2)

1

2 mini O i

Var(O )

(78)

with equality only if the O i are all equal. This inequality was originally proved by Cartwright and Field [20]. Several
reﬁnements, such as
maxi O i − G
2 maxi O i

Var(O ) (cid:2) E − G (cid:2)

Var(O )

(79)

pi(O i − G)2 (cid:2) E − G (cid:2)

pi(O i − G)2

mini O i − G
2 mini O i(mini O i − E)
1

(cid:2)

2 mini O i

i

(cid:2)

1

2 maxi O i

i

as well as other interesting bounds can be found in [4,5,31,32,1,2].

2. Since G (cid:2) E and G

(cid:4)) with equality if and only if all the
numbers O i are equal. Thus the weighted geometric mean is always less or equal to the normalized weighted geometric
mean.

(cid:4) (cid:2) 1, and thus G (cid:2) G/(G + G

(cid:4) = 1 − E, we have G + G

(cid:4) (cid:2) E

3. If the numbers O i satisfy 0 < O i (cid:2) 0.5 (consistently low), then
and therefore G (cid:2) G

(cid:2) E

G
G(cid:4)

(cid:2) E
E(cid:4)

G + G(cid:4)

(80)

(81)

[Note that if O i = 0 for some i with pi (cid:3)= 0, then G = 0 and the result is still true.] This is easily proved using Jensen’s
inequality and applying it to the function ln x − ln(1 − x) for x ∈ (0, 0.5]. It is also known as the Ky Fan inequality
[11,35,36] which can also be viewed as a special case of the Levinson’s inequality [28]. In short, in the consistently low
case, the normalized weighted geometric mean is always less or equal to the expectation and provides a better approximation
of the expectation than the geometric mean. We will see in a later section why the consistently low case is particularly
signiﬁcant for dropout.

4. If the numbers O i satisfy 0.5 (cid:2) O i < 1 (consistently high), then

(cid:4)

G

G

(cid:4)

(cid:2) E
E

and therefore

G
G + G(cid:4)

(cid:3) E

(82)

(cid:4) = 0 and the result is still true. In short, the normalized weighted
Note that if O i = 1 for some i with pi (cid:3)= 0, then G
geometric mean is greater or equal to the expectation. The proof is similar to the previous case, interchanging x and
1 − x.

5. Note that if G/(G + G

(cid:4)) underestimates E then G

(cid:4)/(G + G

(cid:4)) overestimates 1 − E, and vice versa.

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

91

Fig. 8.1. The curve associated with the approximate bound |E − NWGM| (cid:3) E(1 − E)|1 − 2E|/[1 − 2E(1 − E)] (Eq. (87)).

6. This is the most important set of properties. When the numbers O i satisfy 0 < O i < 1, to a ﬁrst order of approximation

we have

G ≈ E

and

G
G + G(cid:4)

≈ E

and E − G ≈

(cid:20)
(cid:20)
(cid:20)E − G
(cid:20)

G + G(cid:4)

(cid:20)
(cid:20)
(cid:20)
(cid:20)

(83)

Thus to a ﬁrst order of approximation the WGM and the NWGM are equally good approximations of the expectation.
However the results above, in particular property 3, lead one to suspect that the NWGM may be a better approximation,
and that bounds or estimates ought to be derivable in terms of the variance. This can be seen by taking a second order
approximation, which gives

G ≈ E − V and G

(cid:4) ≈ 1 − E − V and

G
G + G(cid:4)

≈ E − V
1 − 2V

and

(cid:4)

G
G + G(cid:4)

≈ 1 − E − V
1 − 2V

with the differences

E − G ≈ V ,

1 − E − G

(cid:4) ≈ V ,

E − G

G + G(cid:4)

≈ V (1 − 2E)
1 − 2V

,

and

V (1 − 2E)
1 − 2V

(cid:2) V

and 1 − E − G

(cid:4)

G + G(cid:4)

≈ V (2E − 1)
1 − 2V

(84)

(85)

(86)

The difference |E − NWGM| is small to a second order of approximation and over the entire range of values of E. This
is because either E is close to 0.5 and then the term 1 − 2E is small, or E is close to 0 or 1 and then the term V is
small. Before we provide speciﬁc bounds for the difference, note also that if E < 0.5 the second order approximation to
the NWGM is below E, and vice versa when E > 0.5.
Since V (cid:2) E(1 − E), with equality achieved only for 0–1 Bernoulli variables, we have

(cid:20)
(cid:20)
(cid:20)E − G
(cid:20)

G + G(cid:4)

(cid:20)
(cid:20)
(cid:20)

(cid:20) ≈ V |1 − 2E|
1 − 2V

(cid:2) E(1 − E)|1 − 2E|
1 − 2V

(cid:2) E(1 − E)|1 − 2E|
1 − 2E(1 − E)

(cid:2) 2E(1 − E)|1 − 2E|

(87)

√

(cid:21)√

The inequalities are optimal in the sense that they are attained in the case of a Bernoulli variable with expectation E.
The function E(1 − E)|1 − 2E|/[1 − 2E(1 − E)] is zero for E = 0, 0.5, or 1, and symmetric with respect to E = 0.5. It is
convex down and its maximum over the interval [0, 0.5] is achieved for E = 0.5 −
5 − 2/2 (Fig. 8.1). The function
2E(1 − E)|1 − 2E| is zero for E = 0, 0.5, or 1, and symmetric with respect to E = 0.5. It is convex down and its maximum
over the interval [0, 0.5] is achieved for E = 0.5 −
3/6 (Fig. 8.2). Note that at the beginning of learning, with small
random weights initialization, typically E is close to 0.5. Towards the end of learning, E is often close to 0 or 1. In all
these cases, the bounds are close to 0 and the NWGM is close to E.
Note also that it is possible to have E = NWGM even when the numbers O i are not identical. For instance, if O 1 = 0.25,
and thus: E = NWGM = 0.5.
O 2 = 0.75, and P 1 = P 2 = 0.5 we have G = G
In short, in general the NWGM is a better approximation to the expectation E than the geometric mean G. The property is always
true to a second order of approximation. Furthermore, it is always exact when NWGM (cid:2) E since we must have G (cid:2) NWGM (cid:2) E.
Furthermore, in general the NWGM is a better approximation to the mean than a random sample. Using a randomly
chosen O i as an estimate of the mean E, leads to an error that scales like the standard deviation σ =
V , whereas the
NWGM leads to an error that scales like V .

√

(cid:4)

92

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Fig. 8.2. The curve associated with the approximate bound |E − NWGM| (cid:3) 2E(1 − E)|1 − 2E| (Eq. (87)).

When NWGM > E, “third order” cases can be found where

G
G + G(cid:4)

− E ≈ E − G with

G
G + G(cid:4)

− E (cid:3) E − G

(88)

An example is provided by: O 1 = 0.622459, O 2 = 0.731059 with a uniform distribution (p1 = p2 = 0.5). In this case,
E = 0.676759, G = 0.674577, G

(cid:4) = 0.318648, NWGM = 0.679179, E − G = 0.002182 and NWGM − E = 0.002420.

(cid:4) = 0. In this case, NWGM = 1, unless
Extreme cases. Note also that if for some i, O i = 1 with non-zero probability, then G
there is a j (cid:3)= i such that O j = 0 with non-zero probability. Likewise if for some i, O i = 0 with non-zero probability, then
G = 0. In this case, NWGM = 0, unless there is a j (cid:3)= i such that O j = 1 with non-zero probability. If both O i = 1 and O j = 0
are achieved with non-zero probability, then NWGM = 0/0 is undeﬁned. In principle, in a sigmoidal neuron, the extreme
output values 0 and 1 are never achieved, although in simulations this could happen due to machine precision. In all these
extreme cases, where the NWGM is a good approximation of E or not depends on the exact distribution of the values. For
instance, if for some i, O i = 1 with non-zero probability, and all the other O j ’s are also close to 1, then NWGM = 1 ≈ E. On
the other hand, if O i = 1 with small but non-zero probability, and all the other O j ’s are close to 0, then NWGM = 1 is not
a good approximation of E.

Higher order moments. It would be useful to be able to derive estimates also for the variance V , as well as other higher
order moments of the numbers O , especially when O = σ (S). While the NWGM can easily be generalized to higher order
moments, it does not seem to yield simple estimates as for the mean (see Appendix C). However higher order moments in
a deep network trained with dropout can easily be approximated, as in the linear case (see Section 9).

Proof. To prove these results, we compute ﬁrst and second order approximations. Depending on the case of interest, the
numbers 0 < O i < 1 can be expanded around E, around G, or around 0.5 (or around 0 or 1 when they are consistently
close to these boundaries). Without assuming that they are consistently low or high, we expand them around 0.5 by writing
O i = 0.5 + (cid:8)i where 0 (cid:2) |(cid:8)i| (cid:2) 0.5. [Estimates obtained by expanding around E are given in Appendix B]. For any distribution
i(0.5 +
P 1, . . . , P m over the m subnetworks, we have E(O ) = 0.5 + E((cid:8)) and Var(O ) = Var((cid:8)). As usual, let G =
(cid:8)i)P i = 0.5

i O P i

(cid:15)

(cid:15)

(cid:15)

=

i

i(1 + 2(cid:8)i)P i . To a ﬁrst order of approximation,
(cid:6)
m(cid:2)

m(cid:14)

(cid:7)

P i

m(cid:14)

G =

1

+ (cid:8)i

2

i=1

= 1
2

(1 + 2(cid:8)i)P i ≈ 1
2

+

i=1

i=1

P i(cid:8)i = E

The approximation is obtained using a Taylor expansion and the fact that 2|(cid:8)i| < 1. In a similar way, we have G
and G/(G + G

(cid:4) ≈ 1 − E
(cid:4)) ≈ E. These approximations become more accurate as (cid:8)i → 0. To a second order of approximation, we have
(cid:14)

∞(cid:2)

(cid:14)

(cid:7)

(cid:6)

(cid:12)

G = 1
2

P i
n

(2(cid:8)i)n = 1
2

i

1 + P i2(cid:8)i + P i(P i − 1)

2

(cid:13)
(2(cid:8)i)2 + R3((cid:8)i)

i

n=0

(cid:6)

(cid:7)

where R3((cid:8)i) is the remainder of order three

R3((cid:8)i) =

P i
3

(2(cid:8)i)3
(1 + ui)3−P i

(cid:11)

(cid:10)

(cid:8)2
i

= o

(89)

(90)

(91)

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

and |ui| (cid:2) 2|(cid:8)i|. Expanding the product gives
(cid:6)

(cid:7)

(cid:12)

G = 1
2

(cid:14)

∞(cid:2)

i

n=0

P i
n

(2(cid:8)i)n = 1
2

1 +

(cid:2)

i

P i2(cid:8)i +

(cid:2)

i

P i(P i − 1)
2

(2(cid:8)i)2 +

(cid:2)

i< j

(cid:13)
4P i P j(cid:8)i(cid:8) j + R3((cid:8))

93

(92)

which reduces to

G = 1
2

+

(cid:2)

i

(cid:6)(cid:2)

(cid:7)

2

P i(cid:8)i

−

(cid:2)

P i(cid:8)i +

i

P i(cid:8)2
i

+ o

(cid:11)

(cid:10)

(cid:8)2

= 1
2

+ E((cid:8)) − Var((cid:8)) + o

(cid:11)

(cid:10)

(cid:8)2

= E(O ) − Var(O ) + R3((cid:8)) (93)

By symmetry, we also have
(cid:14)

(cid:4) =

G

(1 − O i)P i = 1 − E(O ) − Var(O ) + R3((cid:8))

(94)

i

where again R3((cid:8)) is the higher order remainder. Neglecting the remainder and writing E = E(O ) and V = Var(O ) we have

G
G + G(cid:4)

≈ E − V
1 − 2V

and

(cid:4)

G
G + G(cid:4)

≈ 1 − E − V
1 − 2V

(95)

Thus the differences between the mean on one hand, and the geometric mean and the normalized geometric means on the
other, satisfy

E − G ≈ V and E − G

G + G(cid:4)

≈ V (1 − 2E)
1 − 2V

and

1 − E − G

(cid:4) ≈ V and (1 − E) − G

(cid:4)

G + G(cid:4)

≈ V (1 − 2E)
1 − 2V

(96)

(97)

To know when the NWGM is a better approximation to E than the WGM, we consider when the factor |(1 − 2E)/(1 − 2V )|
is less or equal to one. There are four cases:

(1) E (cid:2) 0.5 and V (cid:2) 0.5 and E (cid:3) V .
(2) E (cid:2) 0.5 and V (cid:3) 0.5 and E + V (cid:3) 1.
(3) E (cid:3) 0.5 and V (cid:2) 0.5 and E + V (cid:2) 1.
(4) E (cid:3) 0.5 and V (cid:3) 0.5 and E (cid:2) V .

However, since 0 < O i < 1, we have V (cid:2) E − E 2 = E(1 − E) (cid:2) 0.25. So only cases 1 and 3 are possible and in both cases the
relationship is trivially satisﬁed. Thus in all cases, to a second order of approximation, the NWGM is closer to E than the
WGM.

9. Dropout distributions and approximation properties

Throughout the rest of this article, we let W l
i

= σ (U l

i) denote the deterministic variables of the dropout approximation

(or ensemble network) with

(cid:6)(cid:2)

(cid:2)

wlh

i j ph

j W h

j

W l
i

= σ

(cid:7)

(98)

h<l

j

in the case of dropout applied to the nodes. The main question we wish to consider is whether W l
to E(O l

i) for every input, every layer l, and any unit i.

i is a good approximation

9.1. Dropout induction

Dropout relies on the correctness of the approximation of the expectation of the activity of each unit over all its dropout

subnetworks by the corresponding deterministic variable in the form

W l
i

≈ E

(cid:11)

(cid:10)

O l
i

(99)

for each input, each layer l, and each unit i. The correctness of this approximation can be seen by induction. For the ﬁrst
layer, the property is obvious since W 1
i ), using the results of Section 8. Now assume that the property
i
is true up to layer l. Again, by the results in Section 8,

= NWGM(O 1

i ) ≈ E(O 1

94

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

(cid:11)

(cid:10)

E

O l+1

i

≈ NWGM

(cid:10)

O l+1

i

(cid:11)

(cid:10)

(cid:10)

E

= σ

Sl+1

i

(cid:11)(cid:11)

which can be computed by

(cid:10)

(cid:10)

E

σ

Sl+1

i

(cid:11)(cid:11)

= σ

(cid:6) (cid:2)

(cid:2)

wl+1h

i j

ph

j E

(cid:7)

(cid:11)

(cid:10)

O h
j

≈ σ

(cid:6) (cid:2)

(cid:2)

(cid:7)

wl+1h

i j

j W h
ph

j

= W l+1

i

h<l+1

j

h<l+1

j

(100)

(101)

The approximation in Eq. (101) uses of course the induction hypothesis. This induction, however, does not provide any sense
of the errors being made, and whether these errors increase signiﬁcantly with the depth of the networks. The error can be
decomposed into two terms
(cid:10)
(cid:16)

(cid:11)(cid:17)

(cid:10)

(cid:11)

(cid:10)

(cid:11)

(cid:10)

(cid:11)

(cid:17)

(cid:16)

(cid:8)l
i

= E

O l
i

− W l
i

=

E

O l
i

− NWGM

O l
i

+

NWGM

O l
i

− W l
i

= αl
i

+ βl
i

(102)

Thus in what follows we study each term.

9.2. Sampling distributions

In Section 8, we have shown that in general NWGM(O ) provides a good approximation to E(O ). To further understand
the dropout approximation and its behavior in deep networks, we must look at the distribution of the difference α =
E(O ) − NWGM(O ). Since both E and NWGM are deterministic functions of a set of O values, a distribution can only be
deﬁned if we look at different samples of O values taken from a more general distribution. These samples could correspond
to dropout samples of the output of a given neuron. Note that the number of dropout subnetworks of a neuron being
exponentially large, only a sample can be accessed during simulations of large networks. However, we can also consider
that these samples are associated with a population of neurons, for instance the neurons in a given layer. While we cannot
expect the neurons in a layer to behave homogeneously for a given input, they can in general be separated in a small
number of populations, such as neurons that have low activity, medium activity, and high activity and the analysis below
can be applied to each one of these populations separately. Letting O S denote a sample of m values O i, . . . , O m, we are
going to show through simulations and more formal arguments that in general E(O S ) − NWGM(O S ) has a mean close to 0,
a small standard deviation, and in many cases is approximately normally distributed. For instance, if the values O originate
from a uniform distribution over [0, 1], it is easy to see that both E and NWGM are approximately normally distributed,
with mean 0.5, and a small variance decreasing as 1/m.

9.3. Mean and standard deviation of the normalized weighted geometric mean

More generally, assume that the variables O i are i.i.d. with mean μO and variance σ 2

O . Then the variables S i satisfying
O i = σ (S i) are also i.i.d. with mean μS and variance σ 2
S . Densities for S when O has a Beta distribution, or for O when
S has a Gaussian distribution, are derived in Appendices A–F. These could be used to model in more detail non-uniform
distributions, and distributions corresponding to low or high activity. For m suﬃciently large, by the central limit theorem2
the means of these quantities are approximately normal with:
(cid:7)

(cid:6)

(cid:6)

(cid:7)

E(O S ) ∼ N

μO ,

and E(SS ) ∼ N

μS ,

σ 2
O
m

σ 2
S
m

If these standard deviations are small enough, which is the case for instance when m is large, then σ can be well approx-
imated by a linear function with slope t over the corresponding small range. In this case, NWGM(O S ) = σ (E(SS )) is also
approximately normal with
(cid:6)

(cid:7)

(103)

(104)

NWGM(O S ) ∼ N

σ (μS ),

t2σ 2
S
m

Note that |t| (cid:2) λ/4 since σ (cid:4) = λσ (1 − σ ). Very often, σ (μS ) ≈ μO . This is particularly true if μO = 0.5. Away from 0.5,
a bias can appear—for instance we know that if all the O i < 0.5 thenNWGM < E—but this bias is relatively small. This is
conﬁrmed by simulations, as shown in Fig. 9.1 using Gaussian or uniform distributions to generate the values O i . Finally,
note that the variance of E(O S ) and NWGM(O S ) are of the same order and behave like C1/m and C2/m respectively as
m → ∞. Furthermore σ 2
O

= C1 ≈ C2 if σ 2

O is small.

If necessary, it is also possible to derive better and more general estimates of E(O ), under the assumption that S is
Gaussian by approximating the logistic function with the cumulative distribution of a Gaussian, as described in Appendix F
(see also [41]).

If we sample from many neurons whose activities come from the same distribution, the sample mean and the sample
NWGM will be normally distributed and have roughly the same mean. The difference will have approximately zero mean.
To show that the difference is approximately normal we need to show that E and NWGM are uncorrelated.

2 Note that here all the weights P i are identical and equal to 1/m. However the central limit theorem can be applied also in the non-uniform case, as

long as the weights do not deviate too much from the uniform distribution.

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

95

Fig. 9.1. Histogram of NWGM values for a random sample of 100 values O taken from: (1) the uniform distribution over [0, 1] (upper left); (2) the uniform
distribution over [0, 0.5] (lower left); (3) the normal distribution with mean 0.5 and standard deviation 0.1 (upper right); and (4) the normal distribution
with mean 0.25 and standard deviation 0.05 (lower right). All probability weights are equal to 1/100. Each sampling experiment is repeated 5000 times to
build the histogram.

9.4. Correlation between the mean and the normalized weighted geometric mean

We have
(cid:16)

Var

(cid:17)
E(O S )

− NWGM[O S )] = Var

(cid:16)

(cid:17)
E(O S )

(cid:16)
(cid:17)
NWGM(O S )

(cid:16)
+ 2 Cov

(cid:17)
E(O S ), NWGM(O S )

+ Var

(105)

Thus to estimate the variance of the difference, we must estimate the covariance between E(O S ) and NWGM(O S ). As we
shall see, this covariance is close to null.

In this section, we assume again samples of size m from a distribution on O with mean E = μO and variance V = σ 2
O .
To simplify the notation, we use ES , V S , and NWGMS to denote the random variables corresponding to the mean, variance,
and normalized weighted geometric mean of the sample. We have seen, by doing a Taylor expansion around 0.5, that
NWGMS ≈ (ES − V S )/(1 − 2V S ).

We ﬁrst consider the case where E = NWGM = 0.5. In this case, the covariance of NWGMS and ES can be estimated as

Cov(NWGMS , ES ) ≈ E

(cid:12)(cid:6)

ES − V S
1 − 2V S

− 1
2

(cid:7)(cid:6)

ES − 1
2

(cid:7)(cid:13)

(cid:12)

= E

(cid:13)

(E − 1
2 )2
1 − 2V S

(106)

We have 0.5 (cid:2) 1 − 2V S (cid:2) 1 and E(ES − 1
2 )2 = Var(ES ) = V /m. Thus in short the covariance is of order V /m and goes to 0
as the sample size m goes to inﬁnity. For the Pearson correlation, the denominator is the product of two similar standard
deviations and scales also like V /m. Thus the correlation should be roughly constant and close to 1. More generally, even
when the mean E is not equal to 0.5, we still have the approximations
(cid:12)

(cid:12)(cid:6)

(cid:7)

(cid:13)

ES − V S
1 − 2V S

− E − V
1 − 2V

(cid:13)
(ES − E)

= E

(E − ES )2 + (V − V S )(ES − E)
(1 − 2V S )(1 − 2V )

Cov(NWGMS , ES ) ≈ E

(107)

96

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Fig. 9.2. Behavior of the Pearson correlation coeﬃcient (left) and the covariance (right) between the empirical expectation E and the empirical NWGM as a
function of the number of samples and sample distribution. For each number of samples, the sampling procedure is repeated 10,000 times to estimate the
Pearson correlation and covariance. The distributions are the uniform distribution over [0, 1], the uniform distribution over [0, 0.5], the normal distribution
with mean 0.5 and standard deviation 0.1, and the normal distribution with mean 0.25 and standard deviation 0.05.

And the leading term is still of order V /m. [Similar results are also obtained by using the expansions around 0 or 1 given in
Appendices A–F to model populations of neurons with low or high activity.] Thus again the covariance between NWGM and
E goes to 0, and the Pearson correlation is constant and close to 1. These results are conﬁrmed by simulations in Fig. 9.2.

Combining the previous results we have

Var(ES − NWGMS ) ≈ Var(ES ) + Var(NWGMS ) ≈ C1
m

+ C2
m

(108)

Thus in general E(O S ) and NWGM(O S ) are random variables with: (1) similar, if not identical, means; (2) variances and covariance
that decrease to 0 inversely to the sample size; (3) approximately normal distributions. Thus E − NWGM is approximately normally
distributed around zero. The NWGM behaves like a random variable with small ﬂuctuations above and below the mean. [Of course
contrived examples can be constructed (for instance with small m or small networks) which deviate from this general
behavior.]

9.5. Dropout approximations: the cancellation effects

To complete the analysis of the dropout approximation of E(O l
= E(O l

i where in general the error term (cid:8)l

= αl
i

+ βl

i

i) − (cid:8)l
W l
i
0. Furthermore the error (cid:8)l

i) by W l

i , we show by induction over the layers that
i is small and approximately normally distributed with mean
= E(O l

First, the property is true for l = 1 since W 1
i

i is uncorrelated with the error αl
i
= NWGM(O 1
i ) and the results of the previous sections apply immediately to

i) for l > 1.

i) − NWGM(O l

this case. For the induction step, we assume that the property is true up to layer l. At the following layer, we have

(cid:6)(cid:2)

(cid:2)

W l+1

i

= σ

wl+1h

i j

j W h
ph

j

= σ

h(cid:2)l

j

h(cid:2)l

j

(cid:7)

(cid:6)(cid:2)

(cid:2)

wl+1h

i j

ph
j

(cid:11)

(cid:16)

(cid:10)

E

O h
j

− (cid:8)h
j

(cid:17)

(cid:7)

Using a ﬁrst order Taylor expansion

W l+1

i

≈ NWGM

or more compactly

W l+1

i

≈ NWGM

(cid:10)

(cid:10)

thus

(cid:11)

O l+1

i

+ σ (cid:4)

(cid:6)(cid:2)

(cid:2)

wl+1h

i j

ph

j E

(cid:7)(cid:12)

(cid:11)

−

(cid:10)

O h
j

(cid:2)

(cid:2)

wl+1h

i j

j (cid:8)h
ph

j

(cid:13)

h(cid:2)l

j

O l+1

i

(cid:11)

(cid:10)

(cid:10)

E

− σ (cid:4)

Sl+1

i

(cid:12)(cid:2)

(cid:11)(cid:11)

(cid:2)

h(cid:2)l

j

h(cid:2)l

j

(cid:13)

wl+1h

i j

j (cid:8)h
ph

j

βl+1

i

= NWGM

(cid:10)

(cid:11)

O l+1

i

− W l+1

i

≈ σ (cid:4)

(cid:10)

(cid:10)

E

Sl+1

i

(cid:12)(cid:2)

(cid:11)(cid:11)

(cid:2)

(cid:13)

wl+1h

i j

j (cid:8)h
ph

j

h(cid:2)l

j

(109)

(110)

(111)

(112)

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

As a sum of many linear small terms, βl+1

i

(cid:11)

(cid:10)
βl+1

i

E

≈ 0

is approximately normally distributed. By linearity of the expectation

By linearity of the variance with respect to sums of independent random variables
(cid:2)
(cid:10)

(cid:2)

(cid:11)

(cid:11)

(cid:11)

(cid:10)

(cid:10)

(cid:10)

(cid:10)

(cid:10)

Var

βl+1

i

(cid:16)
σ (cid:4)

≈

(cid:11)(cid:11)(cid:17)
2

E

Sl+1

i

wl+1h

i j

2

ph
j

Var

(cid:8)h
j

(cid:11)
2

97

(113)

(114)

j

h(cid:2)l
This variance is small since [σ (cid:4)(E(Sl+1
))]2 (cid:2) 1/16 for the standard logistic function (and much smaller than 1/16 at the
j ) is small by induction. The weights wl+1h
j )2 (cid:2) 1, and Var((cid:8)h
end of learning), (ph
are small at the beginning of learning
and as we shall see in Section 11 dropout performs weight regularization automatically. While this is not observed in the
simulations used here, one concern is that with very large layers the sum could become large. We leave a more detailed
study of this issue for future work. Finally, we need to show that αl+1
are uncorrelated. Since both terms have
approximately mean 0, we compute the mean of their product

and βl+1

i j

i

i

i

(cid:12)

(cid:10)
i βl+1
αl+1

i

(cid:11)

E

≈ E

(cid:11)

(cid:10)

(cid:10)

E

O l+1

i

− NWGM

(cid:10)

O l+1

i

(cid:11)(cid:11)

(cid:10)

(cid:10)

E

σ (cid:4)

Sl+1

i

(cid:11)(cid:11)(cid:2)

(cid:2)

h(cid:2)l

j

wl+1h

i j

j (cid:8)h
ph

j

(cid:13)

By linearity of the expectation
(cid:10)
i βl+1
αl+1

≈ σ (cid:4)

E

E

(cid:11)

(cid:10)

(cid:10)

i

i

Sl+1

(cid:11)(cid:11)(cid:2)

(cid:2)

wl+1h

i j

ph

j E

(cid:16)(cid:10)

(cid:10)

E

(cid:11)

O l+1

i

− NWGM

(cid:10)

(cid:11)(cid:11)

(cid:17)

(cid:8)h
j

O l+1

i

≈ 0

(115)

(116)

since E(E(O l+1

i

) − NWGM(O l+1

i

h(cid:2)l
j
= E[E(O l+1
))(cid:8)h
j
i and NGWM(O l

i

) − NWGM(O l+1
j ) ≈ 0.
i) can be viewed as good approximations to E(O l

)]E((cid:8)h

i

In summary, in general both W l

i) with small deviations that are
approximately Gaussians with mean zero and small standard deviations. These deviations act like noise and cancel each other to some
extent preventing the accumulation of errors across layers.

These results and those of the previous section are conﬁrmed by simulation results given by Figs. 9.3, 9.4, 9.5, 9.6, and
9.7. The simulations are based on training a deep neural network classiﬁer on the MNIST handwritten characters dataset
with layers of size 784-1200-1200-1200-1200-10 replicating the results described in [27], using p = 0.8 for the input layer
and p = 0.5 for the hidden layers. The raster plots accumulate the results obtained for 10 randomly selected input vectors.
For ﬁxed weights and a ﬁxed input vector, 10,000 Monte Carlo simulations are used to sample the dropout subnetworks
and estimate the distribution of activities O of each neuron in each layer. These simulations use the weights obtained at
the end of learning, except in the cases were the beginning and end of learning are compared (Figs. 9.6 and 9.7). In general,
the results show how well the NWGM(O l
i) in each
layer, both at the beginning and the end of learning, and how the deviations can roughly be viewed as small, approximately
Gaussian, ﬂuctuations well within the bounds derived in Section 8.

i approximate the true expectation E(O l

i) and the deterministic values W l

9.6. Dropout approximations: estimation of variances and covariances

We have seen that the deterministic values W s can be used to provide very simple but effective estimates of the values
E(O )s across an entire network under dropout. Perhaps surprisingly, the W s can also be used to derive approximations of
the variances and covariances of the units as follows.

First, for the dropout variance of a neuron, we can use
(cid:10)

(cid:11)

(cid:11)

(cid:10)

(cid:10)

O l

i O l

i

≈ W l

i or equivalently Var

O l
i

≈ W l
i

1 − W l
i

(cid:11)

O l

i O l

i

≈ W l

i W l

i or equivalently Var

(cid:11)

(cid:10)

O l
i

≈ 0

or

E

(cid:10)

E

(cid:11)

(117)

(118)

These two approximations can be viewed respectively as rough upperbounds and lower bounds to the variance. For neurons
whose activities are close to 0 or 1, and thus in general for neurons towards the end of learning, these two bounds are
similar to each other. This is not the case at the beginning of learning when, with very small weights and a standard logistic
i) ≈ 0 (Figs. 9.8 and 9.9). At the beginning and the end of learning, the variances are
transfer function, W l
i
small and so “0” is the better approximation. However, during learning, variances can be expected to be larger and closer
to their approximate upper bound W (1 − W ) (Figs. 9.10 and 9.11).

= 0.5 and Var(O l

For the covariances of two different neurons, we use

(cid:10)

E

(cid:11)

(cid:10)

(cid:11)

(cid:10)

E

O l
i

O h
j

(cid:11)

= E

O l

i O h

j

≈ W l

i W h

j

(119)

This independence approximation is accurate for neurons that are truly independent of each other, such as pairs of neurons
in the ﬁrst layer. However it can be expected to remain approximately true for pairs of neurons that are only loosely coupled,

98

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Fig. 9.3. Each row corresponds to a scatter plot for all the neurons in each one of the four hidden layers of a deep classiﬁer trained on the MNIST dataset
(see text) after learning. Scatter plots are derived by cumulating the results for 10 random chosen inputs. Dropout expectations are estimated using 10,000
dropout samples. The second order approximation in the left column (blue dots) correspond to |E − NWGM| ≈ V |1 − 2E|/(1 − 2V ) (Eq. (87)). Bound 1 is
the variance-dependent bound given by E(1 − E)|1 − 2E|/(1 − 2V ) (Eq. (87)). Bound 2 is the variance-independent bound given by E(1 − E)|1 − 2E|/(1 −
2E(1 − E)) (Eq. (87)). In the right column, W represent the neuron activations in the deterministic ensemble network with the weights scaled appropriately
and corresponding to the “propagated” NWGMs. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version
of this article.)

i.e. for most pairs of neurons in a large neural networks at all times during learning. This is conﬁrmed by simulations
(Fig. 9.12) conducted using the same network trained on the MNIST dataset. The approximation is much better than simply
using 0 (Fig. 9.13).

For neurons that are directly connected to each other, this approximation still holds but one can try to improve it by
j feeding directly into the neuron with output

introducing a slight correction. Consider the case of a neuron with output O h
O l

i j . By isolating the contribution of O h

i (h < l) through a weight wlh
(cid:6)(cid:2)

(cid:2)

O l
i

= σ

w

f <l

k(cid:3)= j

lf

ikδ f

k O

f
k

+ wlh

i j δh

j O h

j

≈ σ

(cid:7)

(cid:6)(cid:2)

j , we have
(cid:7)

lf

ikδ f

k O

f
k

+ σ (cid:4)

(cid:2)

w

f <l

k(cid:3)= j

(cid:6)(cid:2)

(cid:2)

w

f <l

k(cid:3)= j

(cid:7)

f
k

lf

ikδ f

k O

wlh

i j δh

j O h

j

(120)

with a ﬁrst order Taylor approximation which is more accurate when wlh
well satisﬁed at the beginning of learning or with sparse coding). In this expansion, the ﬁrst term is independent of O h

j are small (conditions that are particularly
j and

i j or O h

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

99

Fig. 9.4. Similar to Fig. 9.3, using the sharper but potentially more restricted second order approximation to the NWGM obtained by using a Taylor expansion
around the mean (see Appendix B, Eq. (202)).

its expectation can easily be computed as
(cid:6)

(cid:7)(cid:7)

(cid:6)

(cid:6)(cid:2)

(cid:2)

E

σ

w

f <l

k(cid:3)= j

lf

ikδ f

k O

f
k

≈ σ

E

(cid:6)(cid:2)

(cid:2)

w

f <l

k(cid:3)= j

lf

ikδ f

k O

(cid:6)(cid:2)

(cid:2)

(cid:7)(cid:7)

f
k

= σ

f <l

k(cid:3)= j

(cid:7)

w

lf
ik p

f
k W

f
k

= W lh
i j

(121)

Thus here W lh
is simply the deterministic activation of neuron i in layer l in the ensemble network when neuron j in layer
i j
h is removed from its inputs. Thus it can easily be computed by forward propagation in the deterministic network. Using a
ﬁrst-order Taylor expansion it can be estimated by

W lh
i j

≈ W l
i

− σ (cid:4)

(cid:11)

(cid:10)

U l
i

In any case,

wlh

i j ph

j W h

j

(cid:11)

(cid:10)

E

O l

i O h

j

≈ W lh

i j W h

j

(cid:6)(cid:2)

(cid:2)

(cid:6)
σ (cid:4)

+ E

w

f <l

k(cid:3)= j

(cid:7)(cid:7)

f
k

wlh

i j ph
j E

(cid:10)

lf

ikδ f

k O

O h

j O h

j

(cid:11)

(122)

(123)

100

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Fig. 9.5. Similar to Figs. 9.3 and 9.4. Approximation 1 corresponds to the second order Taylor approximation around 0.5: |E − NWGM| ≈ V |1 − 2E|/(1 − 2V )
1−([0.5V ]/[E(1−E)]) (see Appendix B, Eq. (202)).
(Eq. (87)). Approximation 2 is the sharper but more restrictive second order Taylor approximation around E:

E−(V /2E)

Histograms for the two approximations are interleaved in each ﬁgure of the right column.

Towards the end of learning, σ (cid:4) ≈ 0 and so the second term can be neglected. A slightly more precise estimate can be
obtained by writing σ (cid:4) ≈ λσ when σ is close to 0, and σ (cid:4) ≈ λ(1 − σ ) when σ is close to 1, replacing the corresponding
expectation by W lh

i j . In any case, to a leading term approximation, we have

i j or 1 − W lh

(cid:11)

(cid:10)

E

O l

i O h

j

≈ W lh

i j W h

j

(124)

The accuracy of these formula for pairs of connected neurons is demonstrated in Fig. 9.14 at the beginning and end of
learning, where it is also compared to the approximation E(O l
j . The correction provides a small improvement
at the end of learning but not at the beginning. This is because it neglects a term in σ (cid:4)
which presumably is close to 0 at
the end of learning. The improvement is small enough that for most purposes the simpler approximation W l
j may be
used in all cases, connected or unconnected.

j ) ≈ W l

i W h

i W h

i O h

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

101

Fig. 9.6. Empirical distribution of NWGM − E is approximately Gaussian at each layer, both before and after training. This was performed with Monte Carlo
simulations over dropout subnetworks with 10,000 samples for each of 10 ﬁxed inputs. After training, the distribution is slightly asymmetric because the
activation of the neurons is asymmetric. The distribution in layer one before training is particularly tight simply because the input to the network (MNIST
data) is relatively sparse.

10. The duality with spiking neurons and with backpropagation

10.1. Spiking neurons

There is a long-standing debate on the importance of spikes in biological neurons, and also in artiﬁcial neural networks,
in particular as to whether the precise timing of spikes is used to carry information or not. In biological systems, there are
many examples, for instance in the visual and motor systems, where information seems to be carried by the short term
average ﬁring rate of neurons rather than the exact timing of their spikes. However, other experiments have shown that
in some cases the timing of the spikes are highly reproducible and there are also known examples where the timing of
the spikes is crucial, for instance in the auditory location systems of bats and barn owls, where brain regions can detect
very small interaural differences, considerably smaller than 1 ms [26,19,18]. However these seem to be relatively rare and
specialized cases. On the engineering side the question of course is whether having spiking neurons is helpful for learning

102

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Fig. 9.7. Empirical distribution of W − E is approximately Gaussian at each layer, both before and after training. This was performed with Monte Carlo
simulations over dropout subnetworks with 10,000 samples for each of 10 ﬁxed inputs. After training, the distribution is slightly asymmetric because the
activation of the neurons is asymmetric. The distribution in layer one before training is particularly tight simply because the input to the network (MNIST
data) is relatively sparse.

or any other purposes, and if so whether the precise timing of the spikes matters or not. There is a connection between
dropout and spiking neurons which might shed some, at the moment faint, light on these questions.

A sigmoidal neuron with output O = σ (S) can be converted into a stochastic spiking neuron by letting the neuron
“ﬂip a coin” and produce a spike with probability O . Thus in a network of spiking neurons, each neuron computes three
random variables: an input sum S, a spiking probability O , and a stochastic output (cid:10) (Fig. 10.1). Two spiking mechanisms
can be considered: (1) global: when a neuron spikes it sends the same quantity r along all its outgoing connections; and
(2) local or connection-speciﬁc: when a neuron spikes with respect to a speciﬁc connection, it sends a quantity r along
that connection. In the latter case, a different coin must be ﬂipped for each connection. Intuitively, one can see that the
ﬁrst case corresponds to dropout on the units, and the second case to droupout on the connections. When a spike is not
produced, the corresponding unit is dropped in the ﬁrst case, and the corresponding connection is dropped in the second
case.

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

103

Fig. 9.8. Approximation of E(O l
i ) and for the variance for neurons in a
MNIST classiﬁer network before and after training. Histograms are obtained by taking all non-input neurons and aggregating the results over 10 random
input vectors.

i corresponding respectively to the estimates W l

i and by W l

i ) by W l

i W l

i O l

i (1 − W l

Fig. 9.9. Histogram of the difference between the dropout variance of O l
i ) in a MNIST classiﬁer network before
and after training. Histograms are obtained by taking all non-input neurons and aggregating the results over 10 random input vectors. Note that at the
beginning of learning, with random small weights, E(O l

i and its approximate upperbound W l

≈ 0.5, and thus Var(O l

i (W l

i ) ≈ 0 whereas W l

i (1 − W l

i ) ≈ 0.25.

i ) ≈ W l

i

Fig. 9.10. Temporal evolution of the dropout variance V (O ) during training averaged over all hidden units.

104

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Fig. 9.11. Temporal evolution of the difference W (1 − W ) − V during training averaged over all hidden units.

Fig. 9.12. Approximation of E(O l
j for pairs of non-input neurons that are not directly connected to each other in a MNIST classiﬁer network,
before and after training. Histograms are obtained by taking 100,000 pairs of unconnected neurons, uniformly at random, and aggregating the results over
10 random input vectors.

j ) by W l

i W h

i O h

Fig. 9.13. Comparison of E(O l
after training. As shown in the previous ﬁgure, W l
neurons, uniformly at random, and aggregating the results over 10 random input vectors.

j ) to 0 for pairs of non-input neurons that are not directly connected to each other in a MNIST classiﬁer network, before and
j provides a better approximation. Histograms are obtained by taking 100,000 pairs of unconnected

i W h

i O h

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

105

Fig. 9.14. Approximation of E(O l
j for pairs of connected non-input neurons, with a directed connection from j to i in a MNIST
classiﬁer network, before and after training. Histograms are obtained by taking 100,000 pairs of connected neurons, uniformly at random, and aggregating
the results over 10 random input vectors.

i j and W l

j ) by W l

i W lh

i W h

i O h

Fig. 9.15. Histogram of the difference between E(σ (cid:4)(S)) and σ (cid:4)(E(S)) (see Eq. (220)) over all non-input neurons, in a MNIST classiﬁer network, before and
after training. Histograms are obtained by taking all non-input neurons and aggregating the results over 10 random input vectors. The nodes in the ﬁrst
hidden layer have 784 sparse inputs, while the nodes in the upper three hidden layers have 1200 non-sparse inputs. The distribution of the initial weights
are also slightly different for the ﬁrst hidden layer. The differences between the ﬁrst hidden layer and all the other hidden layers are responsible for the
initial bimodal distribution.

106

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Fig. 10.1. A spiking neuron formally operates in 3 steps by computing ﬁrst a linear sum S, then a probability O = σ (S), then a stochastic output (cid:10) of size
r with probability O (and 0 otherwise).

To be more precise, a multi-layer network is described by the following equations. First for the spiking of each unit:

(cid:18)

=

(cid:10)h
i

i with probability O h
rh
0

otherwise

i

in the global ﬁring case, and
(cid:18)

(cid:10)h
ji

=

ji with probability O h
rh
0

otherwise

i

in the connection-speciﬁc case. Here we allow the “size” of the spikes to vary with the neurons or the connections, with
spikes of ﬁxed-size being an easy special case. While the spike sizes could in principle be greater than one, the connection
to dropout requires spike sizes of size at most one. The spiking probability is computed as usual in the form

O h
i

= σ

(cid:11)

(cid:10)

Sh
i

and the sum term is given by
(cid:2)

(cid:2)

=

Sh
i

whl

i j (cid:10)l

j

l<h

j

in the global ﬁring case, and
(cid:2)

(cid:2)

=

Sh
i

whl

i j (cid:10)l

i j

(127)

(128)

(129)

l<h

j

in the connection-speciﬁc case. The equations can be applied to all the layers, including the output layer and the input
layer if these layers consist of spiking neurons. Obviously non-spiking neurons (e.g. in the input or output layers) can be
combined with spiking neurons in the same network.

In this formalism, the issue of the exact timing of each spike is not really addressed. However some information about
the coin ﬂips must be given in order to deﬁne the behavior of the network. Two common models are to assume complete
asynchrony, or to assume synchrony within each layer. As spikes propagate through the network, the average output E((cid:10))
of a spiking neuron over all spiking conﬁgurations is equal to r times the size its average ﬁring probability E(O ). As we have
seen, the average ﬁring probability can be approximated by the NWGM over all possible inputs S, leading to the following
recursive equations:
(cid:10)

(cid:11)

(cid:11)

= rh

i E

O h
i

in the global ﬁring case, or
(cid:11)

(cid:10)

(cid:11)

= rh

ji E

O h
i

(cid:10)
(cid:10)h
i

E

(cid:10)
(cid:10)h
ji

E

in the connection-speciﬁc case. Then
(cid:10)

(cid:10)

(cid:10)

(cid:11)

(cid:11)

(cid:10)

E

O h
i

≈ NWGM

O h
i

= σ

E

(cid:11)(cid:11)

Sh
i

with

(cid:2)

(cid:2)

(cid:11)

(cid:10)

E

Sh
i

=

(cid:11)

(cid:10)
(cid:10)l
j

=

whl

i j E

(cid:2)

(cid:2)

i j rl
whl

j E

(cid:11)

(cid:10)

O l
j

l<h

j

l<h

j

(125)

(126)

(130)

(131)

(132)

(133)

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

107

Fig. 10.2. Three closely related networks. The ﬁrst network operates stochastically and consists of spiking neurons: a neuron sends a spike of size r with
probability O . The second network operates stochastically and consists of logistic dropout neurons: a neurons sends an activation O with a dropout
probability r. The connection weights in the ﬁrst and second networks are identical. The third network operates in a deterministic way and consists of
logistic neurons. Its weights are equal to the weights of the second network multiplied by the corresponding probability r.

in the global ﬁring case, or

(cid:11)

(cid:10)

E

Sh
i

=

(cid:2)

(cid:2)

(cid:11)

(cid:10)
(cid:10)l
i j

=

whl

i j E

(cid:2)

(cid:2)

i j rl
whl

i j E

(cid:11)

(cid:10)

O l
j

(134)

l<h

j

l<h

j

in the connection-speciﬁc case.

In short, the expectation of the stochastic outputs of the stochastic neurons in a feedforward stochastic network can be approxi-
mated by a dropout-like deterministic feedforward propagation, proceeding from the input layer to the output layer, and multiplying
each weight whl
i j )—which acts as a dropout probability parameter—of the corresponding pre-
synaptic neuron. [Operating a neuron in stochastic mode is also equivalent to setting all its inputs to 1 and using dropout on
its connections with different Bernoulli probabilities associated with the sigmoidal outputs of the previous layer.]

i j by the corresponding spike size rl

j (or rl

In particular, this shows that given any feedforward network of spiking neurons, with all spikes of size 1, we can ap-
proximate the average ﬁring rate of any neuron simply by using deterministic forward propagation in the corresponding
identical network of sigmoidal neurons. The quality of the approximation is determined by the quality of the approxima-
tions of the expectations by the NWGMs. More generally, consider three feedforward networks (Fig. 10.2) with the same
identical topology, and almost identical weights. The ﬁrst network is stochastic, has weights whl
i j , and consists of spiking
neurons: a neuron with activity O h
i , and 0 otherwise (a similar argument can
be made with connection-speciﬁc spikes of size rh
ji ). Thus, in this network neuron i in layer h sends out a signal that has
instantaneous mean and variance given by

i sends a spike of size rh

i with probability O h

E = rh

i O h

i

and Var =

(cid:10)

(cid:11)

2

(cid:10)
rh
i

O h
i

1 − O h
i

(cid:11)

for ﬁxed O h

i , and short-term mean and variance given by
(cid:10)
rh
i

and Var =

1 − E

O h
i

O h
i

O h
i

i E

(cid:11)(cid:10)

E

(cid:10)

(cid:10)

(cid:11)

(cid:11)

(cid:10)

2

(cid:11)(cid:11)

E = rh

(135)

(136)

when averaged over all spiking conﬁgurations, for a ﬁxed input.

The second network is also stochastic, has identical weights to the ﬁrst network, and consists of dropout sigmoidal
i with probability rh
i , and 0 otherwise (a similar argument can be made
ji ). Thus neuron i in layer h sends out a signal that has instantaneous

neurons: a neuron with activity O h
i sends a value O h
with connection-speciﬁc dropout with probability rh
expectation and variance given by

E = rh

i O h

i

and Var =

(cid:10)

(cid:11)

2

(cid:10)

rh
i

O h
i

1 − rh
i

(cid:11)

for a ﬁxed O h

i , and short-term expectation and variance given by
(cid:11)
(cid:10)

(cid:10)

(cid:11)

(cid:11)

(cid:11)

(cid:10)

E = rh

i E

O h
i

and Var = r Var

O h
i

+ E

O h
i

2

rh
i

(cid:10)
1 − rh
i

(137)

(138)

when averaged over all dropout conﬁgurations, for a ﬁxed input.

The third network is deterministic and consists of logistic units. Its weights are identical to those of the previous two
networks except they are rescaled in the form whl
j . Then, remarkably, feedforward deterministic propagation in the third
i j
network can be used to approximate both the average output of the neurons in the ﬁrst network over all possible spiking
conﬁgurations, and the average output of the neurons in the second network over all possible dropout conﬁgurations. In

× rl

108

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

particular, this shows that using stochastic neurons in the forward pass of a neural network of sigmoidal units may be
similar to using dropout.

Note that the ﬁrst and second network are quite different in their details. In particular the variances of the signals sent
i , then the variance is greater in the dropout
(cid:3) 0.5, then the variance is greater in the

i < O h
by a neuron to the following layer are equal only when O h
i
network. When rh
i , which is the typical case with sparse encoding and rh
i
spiking network. This corresponds to the Poisson regime of relatively rare spikes.

i . Whenr h

i > O h

= rh

In summary, a simple deterministic feedforward propagation allows one to estimate the average ﬁring rates in stochastic,
even asynchronous, networks without the need for knowing the exact timing of the ﬁring events. Stochastic neurons can be
used instead of dropout during learning. Whether stochastic neurons are preferable to dropout, for instance because of the
differences in variance described above, requires further investigations. There is however one more aspect to the connection
between dropout, stochastic neurons, and backpropagation.

10.2. Backpropagation and backpercolation

Another important observation is that the backward propagation used in the backpropagation algorithm can itself be
viewed as closely related to dropout. Starting from the errors at the output layer, backpropagation uses an orderly alternat-
ing sequence of multiplications by the transpose of the forward weight matrices and by the derivatives of the activation
functions. Thus backpropagation is essentially a form of linear propagation in the reverse linear network combined with
multiplication by the derivatives of the activation functions at each node, and thus formally looks like the recursion of
Eq. (24). If these derivatives are between 0 and 1, they can be interpreted as probabilities. [In the case of logistic activation
functions, σ (cid:4)(x) = λσ (x)(1 − σ (x)) and thus σ (cid:4)(x) (cid:2) 1 for every value of x when λ (cid:2) 4.] Thus backpropagation is computing the
dropout ensemble average in the reverse linear network where the dropout probability p of each node is given by the derivative of the
corresponding activation. This suggests the possibility of using dropout (or stochastic spikes, or addition of Gaussian noise),
during the backward pass, with or without dropout (or stochastic spikes, or addition of Gaussian noise) in the forward pass,
and with different amounts of coordination between the forward and backward pass when dropout is used in both.

Using dropout in the backward pass is still faced with the problem of vanishing gradients since units with activities close
to 0 or 1, hence derivatives close to 0, lead to rare sampling. However, imagine for instance six layers of 1000 units each,
fully connected, with derivatives that are all equal to 0.1 everywhere. Standard backpropagation produces an error signal
−6 by the time the ﬁrst layer is reached. Using dropout in the backpropagation instead selects
that contains a factor of 10
on average 100 units per layer and propagates a full signal through them, with no attenuation. Thus a strong error signal
is propagated but through a narrow channel, hence the name of backpercolation. Backpropagation can be thought of as a
special case of backpercolation, because with a very small learning rate backpercolation is essentially identical to backpropa-
gation, since backpropagation corresponds to the ensemble average of many backpercolation passes. This approach of course
would be slow on a computer since a lot of time would be spent sampling to compute an average signal that is provided
in one pass by backpropagation. However it shows that exact gradients are not always necessary and that backpropagation
can tolerate noise, alleviating at least some of the concerns with the biological plausibility of backpropagation. Furthermore,
aside from speed issue, noise in the backward pass might help avoiding certain local minima. Finally, we note that sev-
eral variations on these ideas are possible, such as using backpercolation with a ﬁxed value of p (e.g. p = 0.5), or using
backpropagation for the top layers followed by backpercolation for the lower layers and vice versa. Detailed investigation of
these issues is beyond the scope of this paper and left for future work.

11. Dropout dynamics

So far, we have concentrated on the static properties of dropout, i.e. properties of dropout for a ﬁxed set of weights.
In this section we look at more dynamic properties of dropout, related to the training procedure and the evolution of the
weights.

11.1. Dropout convergence

With properly decreasing learning rates, dropout is almost sure to converge to a small neighborhood of a local minimum
(or global minimum in the case of a strictly convex error function) in a way similar to stochastic gradient descent in
standard neural networks [38,13,14]. This is because it can be viewed as a form of on-line gradient descent with respect to
the error function

Error = E TENS =

(cid:2)

(cid:2)

I

N

P (N ) f w

(cid:10)

(cid:11)
O N , t(I)

=

P (N ) f w

(cid:10)

(cid:11)
O N , t(I)

(cid:2)

I×N

(139)

of the true ensemble, where t(I) is the target value for input I and f w is the elementary error function, typically the
squared error in regression, or the relative entropy error in classiﬁcation, which depends on the weights w. In the case of
dropout, the probability P (N ) of the network N is factorial and associated with the product of the underlying Bernoulli
selector variables.

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

109

Thus dropout is “on-line” with respect to both the input examples I and the networks N , or alternatively one can form a
new set of training examples, where the examples are formed by taking the cartesian product of the set of original examples
with the set of all possible subnetworks. In the next section, we show that dropout is also performing a form of stochastic
gradient descent with respect to a regularized ensemble error.
Finally, we can write the gradient of the error above as:

∂ E TENS
∂ wlh
i j

=

(cid:2)

(cid:2)

I

N :δh
j

=1

P (N )

=

∂ f w
∂ whl
i j

(cid:2)

(cid:2)

I

N :δh
j

=1

P (N )

∂ f w
∂ Sl
i

O h

j (N , I)

(140)

If the backpropagated error does not vary too much around its mean from one network to the next, which seems reasonable
in a large network, then we can replace it by its mean, and similarly for the activity O h
j . Thus the gradient of the true
ensemble can be approximated by the product of the expected backpropagated error (post-synaptic terms) and the expected
pre-synaptic activity
(cid:6)

(cid:7)

(cid:6)

(cid:7)

j W h
ph

j

(141)

∂ E TENS
∂ wlh
i j

≈ E

∂ f w
∂ Sl
i

(cid:10)

(cid:11)

ph

j E

O h
j

≈ E

∂ f w
∂ Sl
i

11.2. Dropout gradient and adaptive regularization: single linear unit

As for the static properties, it is instructive to ﬁrst consider the simplest case of a single linear unit. In the case of a single
linear unit trained with dropout with an input I , an output O = S, and a target t, the error is typically quadratic of the
form Error = 1
2 (t − O )2. Let us consider the two error functions E ENS and E D associated with the ensemble of all possible
subnetworks and the network with dropout. In the linear case, the ensemble network is identical to the deterministic
network obtained by scaling the connections by the dropout probabilities. For a single input I , these error functions are
deﬁned by:

(cid:4)

E ENS = 1
2

(t − O ENS)2 = 1
2

t −

(cid:5)2

n(cid:2)

i=1

pi w i Ii

and

(cid:4)

E D = 1
2

(t − O D )2 = 1
2

t −

(cid:5)2

n(cid:2)

i=1

δi w i Ii

(142)

(143)

Here δi are the Bernoulli selector random variables with P (δi = 1) = pi , hence E D is a random variable, whereas E ENS is
a deterministic function. We use a single training input I for notational simplicity, otherwise the errors of each training
example can be combined additively. The learning gradients are of the form ∂ E
∂ w

= −(t − O ) ∂ O

∂ w , yielding:

= ∂ E
∂ O

∂ O
∂ w

∂ E ENS
∂ w i

= −(t − O ENS)pi Ii

and

∂ E D
∂ w i

= −(t − O D )δi Ii = −tδi Ii + w iδ2

i I 2

i

+

(cid:2)

j(cid:3)=i

w jδiδ j Ii I j

(144)

(145)

The last vector is a random vector variable and we can take its expectation. Assuming as usual that the random variables
δi ’s are pairwise independent, we have

(cid:6)

(cid:7)

E

∂ E D
∂ w i

(cid:10)
(cid:11)
= −
t − E(O D |δi = 1)

pi Ii

= −tpi Ii + w i pi I 2
i

+

(cid:2)

j(cid:3)=i

w i pi p j Ii I j = −(t − O ENS)pi Ii + w i I 2

i (pi)(1 − pi)

which yields

(cid:6)

E

(cid:7)

= ∂ E ENS
∂ w i

+ w i I 2

i Var δi = ∂ E ENS
∂ w i

+ w i Var(δi Ii)

∂ E D
∂ w i

Thus, in general the dropout gradient is well aligned with the ensemble gradient. Remarkably, the expectation of the gradient with
dropout is the gradient of the regularized ensemble error

E = E ENS + 1
2

n(cid:2)

i=1

w 2

i I 2

i Var δi

(148)

(146)

(147)

110

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

The regularization term is the usual weight decay or Gaussian prior term based on the square of the weights and ensuring that the
weights do not become too large and overﬁt the data. Dropout provides immediately the magnitude of the regularization term which
is adaptively scaled by the square of the input terms and by the variance of the dropout variables. Note that here pi = 0.5 is the value
that provides the highest level of regularization and the regularization term depends only on the inputs, and not on the target outputs.
Furthermore, the expected dropout gradient is on-line also with respect to the regularization term since there is one term
for each training example. Obviously, the same result holds for an entire layer of linear units. The regularization effect of
dropout in the case of generalized linear models is also discussed in [43] where it is also used to derive other regularizers.

11.3. Dropout gradient and adaptive regularization: deep linear networks

Similar calculations can be made for deep linear networks. For instance, the previous calculation can be adapted imme-

diately to the top layer of a linear network with T layers with

(cid:10)
ti − O T
= −
i

(cid:11)

j O l
δl

j

∂ E D
∂ w T
i jl

and

(cid:7)

(cid:6)

E

∂ E D
∂ w T l
i j

= ∂ E ENS
∂ w T l
i j

+ w T l

i j Var

(cid:11)

(cid:10)

j O l
δl

j

(149)

(150)

which corresponds again to an adaptive quadratic regularization term in w T l
i j , with a coeﬃcient associated for each input
j O l
with the corresponding variance of the dropout pre-synaptic neuron Var(δl
j).

To study the gradient of any weight w in the network, let us assume without any loss of generality that the deep
network has a single output unit. Let us denote its activity by S in the dropout network, and by U in the deterministic
ensemble network. Since the network is linear, for a given input the output is a linear function of w

S = α w + β and U = E(S) = E(α)w + E(β)

(151)

The output is obtained by summing the contributions provided by all possible paths from inputs to output. Here α and β
are random variables. α corresponds to the sum of all the contributions associated with paths from the input layer to the
output layer that contain the edge associated with w. β corresponds to the sum of all the contributions associated with
paths from the input layer to the output layer that do not contain the edge associated with w. Thus the gradients are given
by

∂ E D
∂ w

= −(t − S)

∂ S
∂ w

= (α w + β − t)α

and

∂ E ENS
∂ w

= −(t − U )

(cid:10)

=

∂ U
∂ w

E(α)w + E(β) − t

(cid:11)

E(α)

The expectation of the dropout gradient is given by

(cid:7)

(cid:6)

E

∂ E D
∂ w

= (α w + β − t)α = E

(cid:11)

(cid:10)

α2

w + E(αβ) − t E(α)

This yields the remarkable expression

(cid:7)

(cid:6)

E

∂ E D
∂ w

= ∂ E ENS
∂ w

+ w Var(α) + Cov(α, β)

(152)

(153)

(154)

(155)

Thus again the expectation of the dropout gradient is the gradient of the ensemble plus an adaptive regularization term which has
two components. The component wVar(α) corresponds to a weight decay, or quadratic regularization term in the error function. The
adaptive coeﬃcient Var(α) measures the dropout variance of the contribution to the ﬁnal output associated with all the input-to-output
paths which contain w. The component Cov(α, β) measures the dropout covariance between the contribution associated with all the
paths that contain w and the contribution associated with all the paths that do not contain w. In general, this covariance is small and
equal to zero for a single layer linear network. Both α and β depend on the training inputs, but not on the target outputs.

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

111

11.4. Dropout gradient and adaptive regularization: single sigmoidal unit

For a single sigmoidal unit something quite similar, but not identical holds. With a sigmoidal unit O = σ (S) = 1/(1 +
−λS ), one typically uses the relative entropy error

ce

(cid:11)
(cid:10)
t log O + (1 − t) log(1 − O )
E = −

(156)

We can again consider two error functions E ENS and E D . Note that while in the linear case E ENS is exactly equal to the
ensemble error, in the non-linear case we use E ENS to denote the error of deterministic network which approximates the
ensemble network.

By the chain rule, we have ∂ E
∂ w

= ∂ E
∂ O

∂ O
∂ S

∂ S
∂ w with

∂ E
∂ O

= −t

1

O

+ (1 − t)

1
1 − O

and

∂ O
∂ S

= λO (1 − O )

Thus ﬁnally grouping terms together

∂ E
∂ w

= −λ(t − O )

∂ S
∂ w

(157)

(158)

Thus the overall form of the derivative is similar to the linear case up to multiplication by the positive factor λ which
is often ﬁxed to one. However the outputs are non-linear which complicates the comparison of the derivatives. We use
O = σ (S) in the dropout network and W = σ (U ) in the deterministic ensemble approximation. For the ensemble network

= −λ(t − W )pi Ii = −λ

(cid:11)
(cid:10)
t − σ (U )

∂ E ENS
∂ w i

pi Ii = λ

t − σ

(cid:6)

(cid:6)(cid:2)

(cid:7)(cid:7)

For the dropout network

∂ E D
∂ w i

= −λ(t − O )δi Ii = λ

t − σ

(cid:6)

(cid:7)(cid:7)

w jδ j I j

δi Ii

(cid:6)(cid:2)

j

Taking the expectation of the gradient gives

(cid:6)

(cid:7)

(cid:6)

(cid:7)(cid:13)(cid:7)

= −λ

t − E

w jδ j I j|δi = 1

pi Ii

w j p j I j

pi Ii

j

(159)

(160)

(161)

Using the NWGM approximation to the expectation allows one to take the expectation inside the sigmoidal function so that
(cid:7)(cid:7)

(cid:6)

(cid:6)

(cid:7)

≈ −λ

t − σ

w j p j I j − w i pi Ii + w i Ii

pi Ii = −λ

U + Ii w i(1 − pi)

pi Ii

(162)

(cid:10)

(cid:10)
t − σ

(cid:11)(cid:11)

The logistic function is continuously differentiable everywhere so that one can take its ﬁrst-order Taylor expansion
around U :
(cid:6)

(cid:7)

≈ −λ

(cid:10)
t − σ (SENS) − σ (cid:4)

(cid:11)
(SENS)Ii w i(1 − pi)

pi Ii

where σ (cid:4)(x) = σ (x)(1 − σ (x)) denotes the derivative of σ . So ﬁnally we obtain a result similar to the linear case

∂ E D
∂ w i

≈ ∂ E ENS
∂ w i

+ λσ (cid:4)

(U )w i I 2

i Var(δi) = ∂ E ENS
∂ w i

+ λσ (cid:4)

(U )w i Var(δi Ii)

The dropout gradient is well aligned with the ensemble approximation gradient. Remarkably, and up to simple approximations, the
expectation of the gradient with dropout is the gradient of the regularized ensemble error

E = E ENS + 1
2

λσ (cid:4)

(U )

n(cid:2)

i=1

w 2

i I 2

i Var(δi)

(165)

The regularization term is the usual weight decay or Gaussian prior term based on the square of the weights and ensuring that the
weights do not become too large and overﬁt the data. Dropout provides immediately the magnitude of the regularization term which
is adaptively scaled by the square of the input terms, the gain λ of the sigmoidal function, by the variance of the dropout variables, and
the instantaneous derivative of the sigmoidal function. This derivative is bounded and approaches zero when SENS is small or large.
Thus regularization is maximal at the beginning of learning and decreases as learning progresses. Note again that pi = 0.5 is the value
that provides the highest level of regularization. Furthermore, the expected dropout gradient is on-line also with respect to

(163)

(164)

∂ E D
∂ w i

∂ E D
∂ w i

∂ E D
∂ w i

E

E

E

E

(cid:6)

(cid:7)

(cid:6)(cid:2)

(cid:12)
σ

j

(cid:6)(cid:2)

j

112

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

the regularization term since there is one term for each training example. Note again that the regularization term depends
only on the inputs, and not on the target outputs. A similar analysis, with identical results, can be carried also for a set
of normalized exponential units or for an entire layer of sigmoidal units. A similar result can be derived in a similar way
for other suitable transfer functions, for instance for rectiﬁed linear functions by expressing them as integrals of logistic
functions to ensure differentiability.

11.5. Dropout gradient and adaptive regularization: deep neural networks

In deep neural networks with logistic transfer functions at all the nodes, the basic idea remains the same. In fact, for
a ﬁxed set of weights and a ﬁxed input, we can linearize the network around any weight w and thus Eq. (155) applies
“instantaneously”.

To derive more speciﬁc approximations, consider a deep dropout network described by

O h
i

= σ h
i

(cid:11)

(cid:10)

Sh
i

= σ

(cid:6)(cid:2)

(cid:2)

(cid:7)

l<h

j

whl

i j δl

j O l

j

with O 0
j

= I j

(166)

with layers ranging from h = 0 for the inputs to h = T for the output layer, using the selector random variables δl
corresponding approximation ensemble network is described by

j . The

W h
i

= σ h
i

(cid:11)

(cid:10)

U h
i

= σ

(cid:6)(cid:2)

(cid:2)

(cid:7)

whl

i j pl

j W l

j

with W 0
j

= I j

(167)

l<h

j

using a new set of U and W distinct variables to avoid any confusion. In principle each node could use a different logistic
function, with different c and λ parameters, but to simplify the notation we assume that the same logistic function is used
by all neurons. Then the gradient in the ensemble network can be computed by

∂ E ENS
∂ whl
i j

= ∂ E ENS
∂ U h
i

∂ U h
i
∂ whl
i j

where the backpropagated error can be computed recursively using

∂ E ENS
∂ U h
i

=

(cid:2)

(cid:2)

l>h

k

∂ E ENS
∂ U l
k

wlh

ki ph

i σ (cid:4)

(cid:11)

(cid:10)

U h
i

with the initial values at the top of the network

= −λ

(cid:10)
ti − W T
i

(cid:11)

∂ E ENS
∂ U T
i

(168)

(169)

(170)

Here ti is the i-th component of the target vector for the example under consideration. In addition, for the pre-synaptic
term, we have

∂ U h
i
∂ whl
i j

= pl

j W l

j

Likewise, for the dropout network,

with

∂ E D
∂ whl
i j

= ∂ E D
∂ Sh
i

∂ Sh
i
∂ whl
i j

=

∂ E D
∂ Sh
i

(cid:2)

(cid:2)

l>h

k

∂ E D
∂ Sl
k

wlh

kiδh

i σ (cid:4)

(cid:11)

(cid:10)

Sh
i

and the initial values at the top of the network

= −λ

(cid:11)

(cid:10)
ti − O T
i

∂ E D
∂ S T
i

and the pre-synaptic term

∂ Sh
i
∂ whl
i j

= δl

j O l

j

(171)

(172)

(173)

(174)

(175)

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

113

Consider unit i in the output layer T receiving a connection from unit j in a layer l (typically l = T − 1) with weight w T l
i j .

The gradient of the error function in the dropout network is given by

= −λ

(cid:11)

(cid:10)
ti − O T
i

j O l
δl

j

= −λ

(cid:10)

(cid:10)
ti − σ

S T
i

∂ E D
∂ w T l
i j

(cid:11)(cid:11)

j O l
δl

j

= −λ

(cid:10)

(cid:10)
ti − σ

S T l
i j

+ w T l

i j δl

j O l

j

(cid:11)(cid:11)

j O l
δl

j

(176)

using the notation of Section 9.5: S T l
i j
terms gives:

= Sl
i

− w T l

i j δl

j O l

j . Using a ﬁrst order Taylor expansion to separate out independent

≈ −λ

(cid:10)

(cid:10)
ti − σ

S T l
i j

(cid:11)

− σ (cid:4)

(cid:11)

(cid:10)

S T l
i j

∂ E D
∂ w T l
i j

w T l

i j δl

j O l

j

(cid:11)

j O l
δl

j

We can now take the expectation of the gradient

(cid:7)

(cid:6)

E

∂ E D
∂ w T l
i j

≈ −λ

(cid:10)
ti − E

(cid:10)
σ

(cid:10)

S T l
i j

(cid:11)(cid:11)(cid:11)

j W l
pl

j

+ λE

(cid:11)(cid:11)

(cid:10)

(cid:10)
σ (cid:4)

S T l
i j

(cid:10)

w T l

i j pl

j E

(cid:11)

O l

j O l

j

i j )) = σ (U T l
i j )) ≈ σ (E(S T l
(cid:10)
(cid:11)
(cid:10)
σ (cid:4)

O l

pl

i j λ(E

S T l
i j

j O l

j E

(cid:10)

j

i j ) = W T l
(cid:10)

(cid:11)

i j

− σ (cid:4)

(cid:11)

U T
i

+ w T l

≈ W T
i

− σ (cid:4)(U T

i j pl

j W l

j

i )w T l
(cid:11)

j W l
pl

j pl

j W l

j

Now, using the NWGM approximation E(σ (S T l

(cid:7)

(cid:6)

E

∂ E D
∂ w T l
i j

≈ −λ

(cid:10)
ti − W T
i

(cid:11)

j W l
pl

j

which has the form
(cid:6)

(cid:7)

E

∂ E D
∂ w T l
i j

≈ ∂ E ENS
∂ w T l
i j

+ w T l

i j A

(177)

(178)

(179)

(180)

where A has the complex expression given by Eq. (179). Thus we see again that the expectation of the dropout gradient in the
top layer is approximately the gradient of the ensemble network regularized by a quadratic weight decay with an adaptive coeﬃcient.
Towards the end of learning, if the sigmoidal functions are saturated, then the derivatives are close to 0 and A ≈ 0.

Using the dropout approximation E(O l

j) ≈ W l

j together with E(σ (cid:4)(S T l

i j )) ≈ σ (cid:4)(U T

i ) (Fig. 9.15, Eq. (220)) produces the

more compact approximation

(cid:7)

(cid:6)

E

∂ E D
∂ w T l
i j

≈ −λ

(cid:10)
ti − W T
i

(cid:11)

j W l
pl

j

+ w T l

i j λσ (cid:4)

(cid:10)

U T
i

(cid:11)

(cid:10)

Var

j O l
δl

j

(cid:11)

(181)

similar to the single layer-case and showing that dropout tends to minimize the variance Var(δl
j thus A can be further approximated as A ≈ σ (cid:4)(U T
mation of Section 9.5 E(O l
j W l
can also write the expected gradient as a product of a post-synaptic backpropagated error and a pre-synaptic expectation

j). Also with the approxi-
j). In this case, we

j O l
j(1 − pl

j) ≈ W l

i )pl

j W l

j O l

(cid:7)

(cid:6)

E

∂ E D
∂ w T l
i j

(cid:10)

≈

−λ

(cid:10)
ti − W T
i

(cid:11)

+ λw T l

i j σ (cid:4)

(cid:11)(cid:10)

(cid:10)

U T
i

1 − pl

j W l

j

(cid:11)(cid:11)

j W l
pl

j

(182)

With approximations, similar results appear to be true for deeper layers. To see this, the ﬁrst approximation we make is
j of the immediate pre- and post-synaptic

to assume that the backpropagated error is independent of the product σ (cid:4)(Sh
terms, so that

i )δl

j P l

(cid:6)

E

∂ E D
∂ wh
i jl

(cid:7)

(cid:6)(cid:2)

(cid:2)

= E

l>h

k

(cid:7)

∂ E D
∂ Sl
k

wlh

kiδh

i

(cid:10)

E

i σ (cid:4)
δh

(cid:11)

(cid:10)

Sh
i

j O l
δl

j

(cid:11)

=

(cid:2)

(cid:2)

E

l>h

k

(cid:6)

∂ E D
∂ Sl
k

(cid:7)

wlh

ki ph
i E

(cid:11)

(cid:10)

(cid:10)
i σ (cid:4)
δh

Sh
i

j O l
δl

j

(cid:11)

(183)

(cid:10)

This approximation should be reasonable and increasingly accurate for units closer to the input layer, as the presence and
activity of these units bears vanishingly less inﬂuence on the output error. As in the case of the top layer, we can use a
ﬁrst-order Taylor approximation to separate the dependent terms in Eq. (183) so that E(δh
j) is approximately
equal to
(cid:10)
δh
i

Shl
i j
We can approximate E(σ (cid:4)(Shl
σ (cid:4)(cid:4)(U h
i j W l
j whl
(cid:10)
σ (cid:4)

≈ σ (cid:4)(U h
i ) − σ (cid:4)(cid:4)(U h
(cid:11)
(cid:10)
(cid:11)(cid:11)
(cid:10)

j O l
δl
i j δ j O l
i j )) by σ (cid:4)(U hl
i j ) and use a similar Taylor expansion in reverse to get E(σ (cid:4)(Shl
i j E(O l
j whl
i )pl
(cid:11)(cid:16)
(cid:10)
σ (cid:4)

j) so that
(cid:11)
(cid:10)

i j )) ≈ σ (cid:4)(U h

i σ (cid:4)(Sh

+ σ (cid:4)(cid:4)

− σ (cid:4)(cid:4)

(cid:10)
σ (cid:4)

(cid:16)
σ (cid:4)

i ) −

= ph

≈ ph

+ ph

i )pl

(184)

(185)

i )δl

j O l

i pl

i pl

σ (cid:4)(cid:4)

j O l

whl

whl

Shl
i j

Shl
i j

Shl
i j

O l
j

i j E

O l

j E

j E

(cid:11)(cid:11)

(cid:11)(cid:11)

(cid:11)(cid:17)

E

E

E

(cid:11)

(cid:10)

(cid:10)

(cid:11)

(cid:11)

(cid:11)

(cid:10)

(cid:10)

(cid:11)

(cid:10)

(cid:10)

(cid:10)

(cid:11)

(cid:10)

(cid:17)

j

j

j

j

U h
i

j whl
pl
i j E

O l
j

i pl

j E

O l
j

i pl
ph

j E

U h
i

Shl
i j

O l
j

114

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

Collecting terms, ﬁnally gives
(cid:10)

(cid:11)

(cid:11)

(cid:10)
i σ (cid:4)
δh

E

Sh
i

j O l
δl

j

≈ ph

i pl

j

(cid:10)

(cid:16)

σ (cid:4)

U h
i

(cid:11)

(cid:10)

E

O l
j

(cid:11)

− σ (cid:4)(cid:4)

(cid:11)

(cid:10)

U h
i

j whl
pl
i j E

(cid:10)

(cid:11)

(cid:11)

(cid:10)

E

O l
j

O l
j

(cid:10)

(cid:10)
σ (cid:4)(cid:4)

+ E

Shl
i j

(cid:11)(cid:11)

whl

i j E

(cid:10)

O l

j O l

j

or, by extracting the variance term,
(cid:10)
(cid:11)

(cid:11)

(cid:10)

(cid:10)
i σ (cid:4)
δh

E

Sh
i

j O l
δl

j

≈ ph

i pl

j E

O l
j

(cid:11)

(cid:10)

(cid:11)

σ (cid:4)

U h
i

+ ph

i σ (cid:4)(cid:4)

(cid:11)

(cid:10)

U h
i

whl

i j Var

(cid:11)

(cid:10)

j O l
δl

j

Combining this result with Eq. (183) gives

(cid:7)

(cid:6)

E

∂ E D
∂ whl
i j

≈ ∂ E ENS
∂ whl
i j

+ whl

i j A

(cid:11)(cid:17)

(186)

(187)

(188)

where A is an adaptive coeﬃcient, proportional to σ (cid:4)(cid:4)(U h
i )Var(δl
j). Note that it is not obvious that A is always positive—
a requirement for being a form of weight decay—especially since σ (cid:4)(cid:4)(x) is negative for x > 0.5 in the case of the standard
sigmoid. Further analyses and simulations of these issues and the underlying approximations are left for future work.

j O l

In conclusion, the approximations suggest that the gradient of the dropout approximation ensemble ∂ E ENS/∂ whl

i j and the expecta-
tion of the gradient E(∂ E D /∂ whl
i j ) of the dropout network are similar. The difference is approximately a (weight decay) term linear in
whl
i j with a complex, adaptive coeﬃcient, that varies during learning and depends on the variance of the pre-synaptic unit and on the
input. Thus dropout has a built in regularization effect that keeps the weights small. Furthermore, this regularization tends also to keep
the dropout variance of each unit small. This is a form of self-consistency since small variances ensure higher accuracy in the dropout
ensemble approximations. Furthermore, since the dropout variance of a unit is minimized when all its inputs are 0, dropout has also a
built-in propensity towards sparse representations.

11.6. Dropin

It is instructive to think about the apparently symmetric algorithm we call dropin where units are randomly and inde-
pendently set to 1, rather than 0 as in dropout. Although superﬁcially symmetric to dropout, simulations show that dropin
behaves very differently and in fact does not work. The reason can be understood in terms of the previous analyses since
setting units to 1 tends to maximize variances, rather then minimizing them.

11.7. Learning phases and sparse coding

Finally, in light of these results, we can expect roughly three phases during dropout learning:

1. At the beginning of learning, when the weights are random and very small, the total input to each unit is close to 0
for all the units and the consistency is high: the output of the units remains roughly constant across subnetworks (and
equal to 0.5 if the logistic coeﬃcient is c = 1.0).

2. As learning progresses, the sizes of the weights increase, activities tend to move towards 0 or 1, and the consistencies
decreases, i.e. for a given input the dropout variance of the units across subnetworks increases, and more so for units
that move towards 1 than units that move towards 0. However, overall the regularization effect of dropout keeps the
weights and variances small. To keep variances small, sparse representations tend to emerge.

3. As learning converges, the consistency of the units stabilizes, i.e. for a given input the variance of the units across
subnetworks becomes roughly constant and small for units that have converged towards 1, and very small for units that
have converged towards 0. This is a consequence of the convergence of stochastic gradient.

σ (Sh

(cid:9)

For simplicity, let us assume that dropout is carried only in layer h where the units have an output of the form O h
i
i ) and Sh
(cid:11)
(cid:10)

j is a constant since dropout is not applied to layer l. Thus

j . For a ﬁxed input, O l
(cid:11)

j whl
i j δl
(cid:10)
(cid:11)

l<h
(cid:2)
(cid:10)

j O l
(cid:11)

(cid:9)

=

(cid:10)

i

=

Var

Sh
i

=

whl
i j

2

O l
j

2

pl
j

1 − pl
j

(189)

l<h

under the usual assumption that the selector variables δl
j are independent of each other. A similar expression is obtained if
dropout is applied in the same way to the connections. Thus Var(Sh
i ), which ultimately inﬂuences the consistency of unit i
in layer h, depends on three factors. Everything else being equal, it is reduced by: (1) Small weights which goes together
with the regularizing effect of dropout, or the random initial condition; (2) Small activities, which show that dropout is
not symmetric with respect to small or large activities, hence the failure of dropin. Overall, dropout tends to favor small
activities and thus sparse coding; and (3) Small (close to 0) or large (close to 1) values of the dropout probabilities pl
j . The
sparsity and learning phases of dropout are demonstrated through simulations in Figs. 11.1, 11.2, and 11.3.

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

115

Fig. 11.1. Empirical distribution of ﬁnal neuron activations in each layer of the trained MNIST classiﬁer demonstrating the sparsity. The empirical distribu-
tions are combined over 1000 different input examples.

Fig. 11.2. The three phases of learning. For a particular input, a typical active neuron (red) starts out with low dropout variance, experiences an increase in
variance during learning, and eventually settles to some steady constant consistency value. A typical inactive neuron (blue) quickly learns to stay silent. Its
dropout variance grows only minimally from the low initial value. Curves correspond to mean activation with 5% and 95% percentiles. This is for a single
ﬁxed input, and 1000 dropout Monte Carlo simulations. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web
version of this article.)

Fig. 11.3. Consistency of active neurons does not noticeably decline in the upper layers. ‘Active’ neurons are deﬁned as those with activation greater than
0.1 at the end of training. There were at least 100 active neurons in each layer. For these neurons, 1000 dropout simulations were performed at each time
step of 100 training epochs. The plot represents the dropout mean standard deviation and 5%, 95% percentiles computed over all the active neurons in each
layer. Note that the standard deviation does not increase for the higher layers.

116

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

12. Conclusion

We have developed a general framework that has enabled the understanding of several aspects of dropout with good
mathematical precision. Dropout is an eﬃcient approximation to training all possible sub-models of a given architecture and
taking their average. While several theoretical questions regarding both the static and dynamic properties of dropout require
further investigations, for instance its generalization properties, the existing framework clariﬁes the ensemble averaging
properties of dropout, as well as its regularization properties. In particular, it shows that the three standard approaches to
regularizing large models and avoiding overﬁtting: (1) ensemble averaging; (2) adding noise; and (3) adding regularization
terms (equivalent to Bayesian priors) to the error functions, are all present in dropout and thus may be viewed in a more
uniﬁed manner.

Dropout wants to produce robust units that do not depend on the details of the activation of other individual units. As
a result, it seeks to produce units with activities that have small dropout variance, across dropout subnetworks. This partial
variance minimization is achieved by keeping the weights small and using sparse encoding, which in turn increases the
accuracy of the dropout approximation and the degree of self-consistency. Thus, in some sense, by using small weights and
sparse coding, dropout leads to large but energy eﬃcient networks, which could potentially have some biological relevance
as it is well known that carbon-based computing is orders of magnitude more eﬃcient than silicon-based computing.

It is worth to consider which other classes of models, besides, linear and non-linear feedforward networks, may beneﬁt
from dropout. Some form of dropout ought to work, for instance, with Boltzmann machines or Hopﬁeld networks. Further-
more, while dropout has already been successfully applied to several real-life problems, many more remain to be tested.
Among these, the problem of predicting quantitative phenotypic traits, such as height, from genetic data, such as single
nucleotide polymorphisms (SNPs), is worth mentioning. While genomic data is growing rapidly, for many complex traits
we are still in the ill-posed regime where typically the number of loci where genetic variation occurs exceeds the number
of training examples. Thus the best current models are typically highly (L1) regularized linear models, and these have had
limited success. With its strong regularization properties, dropout is a promising algorithm that could be applied to these
questions, using both simple linear or logistic regression models, as well as more complex models, with the potential for
also capturing epistatic interactions.

Finally, at ﬁrst sight dropout seems like another clever hack. More careful analysis, however reveals an underlying web
of elegant mathematical properties. This mathematical structure is unlikely to be the result of chance alone and leads one
to suspect that dropout is more than a clever hack and that over time it may become an important concept for AI and
machine learning.

Acknowledgements

Work supported in part by grants NSF IIS-0513376, NSF-IIS-1321053, NIH LM010235, and NIH NLM T15 LM07443. We

wish also to acknowledge a hardware grant from NVIDIA. We thank Julian Yarkony for feedback on the manuscript.

Appendix A. Rectiﬁed linear transfer function without Gaussian assumption

Here we consider a rectiﬁed linear transfer function RE with threshold 0 and slope λ. If we assume that S is uniformly
distributed over the interval [−a, a] (similar considerations hold for intervals that are not symmetric), then μS = 0 and
σS = a/3. We have RL(E(S)) = 0 and E(RL(S)) =

(cid:22)
0 λx(1/2a) dx = λa/4. In this case

a

(cid:20)
(cid:10)
(cid:20)RL

(cid:11)
E(S)

− E

(cid:10)

RL(S)

(cid:11)(cid:20)
(cid:20) = λa
4

(190)

This difference is small when the standard deviation is small, i.e. when a is small, and proportional to λ as in the Gaussian
case. Alternatively, one can also consider m input (dropout) values S 1, . . . , Sm with probabilities P 1, . . . , P m. We then have

(cid:9)

i P i S i (cid:2) 0
i P i S i > 0

(cid:18)

(cid:10)
RL

(cid:11)
E(S)

=

(cid:10)
(cid:11)
RL(S)

E

= λ

(cid:9)

if

if

i P i S i

(cid:9)

0

λ

(cid:2)

P i S i

i:S i>0

and

Thus

(cid:20)
(cid:10)
(cid:20)RL

(cid:11)
E(S)

− E

(cid:10)

(cid:11)(cid:20)
(cid:20) =

RL(S)

(cid:23)

(cid:9)

(cid:9)

λ

λ

i:S i>0 P i S i
i:S i(cid:2)0 P i|S i|

In the usual case where P i = 1/m this yields
(cid:10)

(cid:9)

(cid:23)

(cid:20)
(cid:10)
(cid:20)RL

(cid:11)
E(S)

− E

RL(S)

(cid:11)(cid:20)
(cid:20) =

λ 1
m
λ 1
m

(cid:9)

i:S i >0 S i

i:S i (cid:2)0

|S i|

(cid:9)

(cid:9)

i P i S i (cid:2) 0
i P i S i > 0

(cid:9)

(cid:9)

i S i (cid:2) 0
i S i > 0

if

if

if

if

(191)

(192)

(193)

(194)

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

117

Again these differences are proportional to λ and it is easy to show they are small if the standard deviation is small using,
for instance, Tchebycheff’s inequality.

Appendix B. Expansion around the mean and around zero or one

B.1. Expansion around the mean

Using the same notation as in Section 8, we consider the outputs O 1, . . . , O m of a sigmoidal neuron with associated
i P i = 1) and O i = σ (S i). The difference here is that we expand around the mean and write

(cid:9)

probabilities P 1, . . . , P m (
O i = E+(cid:8)i . As a result
(cid:14)

(cid:14)

G =

O P i
i

= E

i

i

(cid:6)

(cid:7)

P i

1 − (cid:8)i
E

(195)

(196)

(197)

(198)

(199)

(200)

(201)

and

(cid:4) =

G

(cid:14)

(1 − O i)P i = (1 − E)

i

(cid:7)

P i

(cid:6)

(cid:14)

i

1 − (cid:8)i
1 − E

In order to use the Binomial expansion, we must further assume that for every i, |(cid:8)i| < min(E, 1 − E). In this case,
+ P i(P i − 1)
2

(cid:13)
+ R3((cid:8)i)

1 + P i

G = E

= E

P i
n

(cid:8)i
E

(cid:8)i
E

(cid:8)i
E

∞(cid:2)

(cid:7)
n

(cid:7)(cid:6)

(cid:14)

(cid:14)

(cid:6)

(cid:7)

(cid:6)

(cid:12)

2

i

n=0

i

where R3((cid:8)i) is the remainder of order three. Expanding and collecting terms gives

(cid:12)

G = E

1 +

(cid:2)

i

P i

(cid:8)i
E

+

(cid:2)

i

P i(P i − 1)
2

(cid:7)

2

(cid:6)

(cid:8)i
E

+

(cid:2)

i(cid:3)= j

P i P j

(cid:8)i
E

(cid:8) j
E

(cid:13)
+ R3((cid:8))

(cid:9)

Noting that

(cid:12)

G = E

i P i(cid:8)i = 0, we ﬁnally have
≈ E − V
1 − V
E 2
2E

(cid:13)
+ R3((cid:8))

and similarly by symmetry

(cid:4) ≈ (1 − E) −

G

V
2(1 − E)

As a result,

G + G

(cid:4) ≈ 1 − 1
2

V
E(1 − E)

V
E(1−E)

where
bining the results above yields

(cid:2) 1 is a measure of how much the distribution deviates from the binomial case with the same mean. Com-

NWGM = G

G + G(cid:4)

≈

E − V
2E
V
E(1−E)

1 − 1
2

(202)

In general, this approximation is slightly more accurate than the approximation obtained in Section 8 by expanding around
0.5 (Eq. (87)), as shown by Figs. 9.4 and 9.5, however its range of validity may be slightly narrower.

B.2. Expansion around zero or one

Consider the expansion around one with O i = 1 − (cid:8)i , G =

requires (cid:8)i < 1, which is satisﬁed for every O i . We have

(cid:15)

i(1 − (cid:8)i)P i , and G

(cid:4) =

(cid:15)

i((cid:8)i)P i . The binomial expansion

G =

(cid:14)

∞(cid:2)

(cid:6)

i

n=0

(cid:7)

P i
n

(−1)n(cid:8)n
i

=

i

(cid:12)

(cid:14)

1 − P i(cid:8)i + P i(P i − 1)

2

(cid:13)
+ R3((cid:8)i)

(cid:8)2
i

where R3((cid:8)i) is the remainder of order three. Expanding and collecting terms gives

G = E − 1
2

V + R3((cid:8)) ≈ E − 1
2

V

(203)

(204)

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

118

and

G

(cid:4) ≈ 1 − E − 1
2

V

As a result,

G + G

(cid:4) ≈ 1 − V

Thus

and

NWGM = G

G + G(cid:4)

≈ 2E − V
2 − 2V

(cid:6)

E − NWGM ≈ −

(cid:7)

V
1 − V

E − 1
2

This yields various approximate bounds

|E − NWGM| (cid:4) 1
2

V
1 − V

(cid:2) 1
2

E(1 − E)
1 − E(1 − E

(cid:2) 1
6

and

|E − NWGM| (cid:4)

(cid:20)
(cid:20)
(cid:20)E − 1
(cid:20)
2

(cid:20)
(cid:20)
(cid:20)
(cid:20)

E(1 − E)
1 − E(1 − E)

(cid:2) 1
2

E(1 − E)
1 − E(1 − E

(cid:2) 1
6

(205)

(206)

(207)

(208)

(209)

(210)

Over the interval [0, 1], the function f (E) = E(1−E)
1−E(1−E)
E = 1, and reaches its maximum for E = 0.5 with f (0.5) = 1
and G

and yields

(cid:4)

is positive and concave down. It satisﬁes f (E) = 0 for E = 0 and
3 . Expansion around 0 is similar, interchanging the role of G

NWGM ≈ 1 − E − 0.5V

1 − V

from which similar bounds on |E − NWGM| can be derived.

Appendix C. Higher order moments

(211)

It would be useful to have better estimates of the variance V and potentially also of higher order moments. We have

seen

0 (cid:2) V (cid:2) E(1 − E) (cid:2) 0.25

(212)

Since V = E(O 2) − E(O )2 = E(O 2) − E 2, one would like to estimate E(O 2) or, more generally, E(O k) and it is tempting to
use the NWGM approach, since we already know from the general theory that E(O k) ≈ NWGM(O k). This leads to

(cid:10)

(cid:11)

O k

=

(cid:15)

NWGM

(cid:15)

i(O k
i )P i
(cid:15)
i(1 − O k

i )P i

i(O k

i )P i +

=

(cid:15)

1 +

1
(cid:10) (1−O i )(1+O i +···O k−1
O k
i

i

i

(cid:11)

)

P i

For k = 2 this gives
(cid:10)

(cid:11)

E

O 2

≈ NWGM

(cid:10)

(cid:11)

=

O 2

(cid:15)

(cid:10)

i

1
(1−O i )
O i

1 +

(1+O i )
O i

=

(cid:11)

P i

1 + ce−λE(S)

1
(cid:15)
i(2 + ce−λS i )P i

(213)

(214)

However one would have to calculate exactly or approximately the last term in the denominator above. More or less equiv-
alently, one can use the general fact that NWGM(σ ( f (S))) = σ (E( f (S))), which leads in particular to

(cid:11)(cid:11)

(cid:10)

(cid:10)

σ

Sk

(cid:11)(cid:11)

(cid:10)

(cid:10)

E

Sk

= σ

NWGM

By inverting the sigmoidal function, we have

S = 1
λ

c O
1 − O

which can be expanded around E or around 0.5 using log(1 + u) =
letting O = 05 + (cid:8), gives

(215)

(216)

(cid:9)∞

n=1(−1)n+1un/n for |u| < 1. Expanding around 0.5,

S = 1
λ

log c + 1
λ

(cid:3)

∞(cid:2)

n=0

(2(cid:8))2n+1
2n + 1

2

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122
(cid:8)

≈ 1
λ

log c + 4
λ

(cid:8)

119

(217)

where the last approximation is obtained by retaining only up to second order terms in the expansion. Thus with this
approximation, we have

(cid:6)

(cid:11)

(cid:10)

E

S 2

≈ E

1
λ

log c + 4
λ

(cid:8)

(cid:7)

2

(cid:6)

= E

1
λ

log c + 4
λ

(cid:7)

2

(O − 0.5)

(218)

We already have an estimate for E = E(O ) provided by NWGM(O ). Thus any estimate of E(S 2) obtained directly, or through
NWGM(σ (S 2)) by inverting Eq. (215), leads to an estimate of (O 2) through Eq. (218), and hence to an estimate of the
variance V . And similarly for all higher order moments.

However, in all these cases, additional costly information seem to be required, in order to get estimates of V that are

sharper than those in Eq. (212), and one might as well directly sample the values O i .

Appendix D. Derivatives of the logistic function and their expectations

For σ (x) = 1/(1 + ce

−λx)2 and the
−λx), the ﬁrst order derivative is given by σ (cid:4)(x) = λσ (x)(1 − σ (x)) = λce
second order derivative by σ (cid:4)(cid:4)(x) = λσ (x)(1 − σ (x))(1 − 2σ (x)). As expected, when λ > 0 the maximum of σ (cid:4)(x) is reached
when σ (x) = 0.5 and is equal to λ/4.

As usual, let O i = σ (S i) for i = 1, . . . , m with corresponding probabilities P 1, . . . , P m. To approximate E(σ (cid:4)(S)), we can

−λx/(1 + ce

apply the deﬁnition of the derivative

(cid:10)
σ (cid:4)

(cid:11)
(S)

E

= E

(cid:6)

lim
h→0

σ (S + h) − σ (S)
h

(cid:7)

= lim
h→0

E(σ (S + h)) − E(σ (S))
h

≈ lim
h→0

σ (E(S) + h) − σ (E(S))
h

(219)

using the NWGM approximation to the expectation. Note that the NWGM approximation requires 0 (cid:2) σ (cid:4)(S i) (cid:2) 1 for every i,
which is always satisﬁed if λ (cid:2) 4. Using a ﬁrst order Taylor expansion, we ﬁnally get:

(cid:10)
σ (cid:4)

(cid:11)
(S)

E

≈ lim
h→0

σ (cid:4)(E(S))h
h

= σ (cid:4)

(cid:10)

(cid:11)
E(S)

To derive another approximation to E(σ (cid:4)(S)), we have

(cid:10)
σ (cid:4)

(cid:11)
(S)

E

≈ NWGM

(cid:10)

(cid:11)
(S)

σ (cid:4)

=

1 +

(cid:15)

i

1
(1−σ (cid:4)(S i ))P i
(σ (cid:4)(S i))P i

=

(cid:15)

(cid:10)

i

1
λc eλS i + 2
λ

1

1 +

− 1 + c

λ e−λS i

(cid:11)

P i

(220)

(221)

As in most applications, we assume now that c = λ = 1 to slightly simplify the calculations since the odd terms in the
Taylor expansion of the two exponential functions in the denominator cancel each other. In this case

(cid:10)
σ (cid:4)

(cid:11)
(S)

E

≈ NWGM

(cid:10)

(cid:11)
(S)

σ (cid:4)

=

1 +

(cid:15)

(cid:10)

i

3 +

1
(cid:9)∞
n=1

=

(cid:11)

P i

1 + 3

(cid:15)

(cid:10)

i

1 +

1
(cid:9)∞
n=1

(cid:11)

P i

2(λS i)2n
3(2n)!

2(λS i)2n
(2n)!

(222)

Now different approximations can be derived by truncating the denominator. For instance, by retaining only the term cor-
responding to n = 1 in the sum and using (1 + x)α ≈ 1 + αx for x small, we ﬁnally have the approximation

(cid:10)
σ (cid:4)

(cid:11)
(S)

E

≈

1
4 + λ2 E(S 2
i )

=

1
4 + λ2(Var(S) + (E(S))2)

Appendix E. Distributions

(223)

Here we look at the distribution of O and S, where O = σ (S) under some simple assumptions.

E.1. Assuming S has a Gaussian distribution

Under various probabilistic assumptions, it is natural to assume that the incoming sum S into a neuron has a Gaussian

distribution with mean μ and variance σ 2 with the density

fS (s) = 1√

2π σ

− (s−μ)2
2σ 2

e

(224)

120

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

In this case, the distribution of O is given by

F O (o) = P (O (cid:2) o) = P

S (cid:2)

(cid:6)

−1
λ

log

1 − o
co

(cid:7)

λ log 1−o
−1
(cid:19)

co

=

0

−s−μ)2

e

2σ 2 ds

1√

2π σ

which yields the density

f O (o) = 1√

2π σ

λ log 1−o
−1
co
2σ 2

− (

e

−μ)2

1
λ

1
o(1 − o)

(225)

(226)

In general this density is bell-shaped, similar but not identical to a beta density. For instance, if μ = 0 and λ = c = 1 = σ

f O (o) = 1√
2π

(1 − o)

−1− 1

2 log 1−o

o o

−1+ 1

2 log 1−o

o

(227)

E.2. The mean and variance of S

Consider a sum of the form S =

have mean μO and variance σ 2
approximately Gaussian by the central limit theorem, with

(cid:9)
n
i=1 w i O i . Assume that the weights have mean μw and variance σ 2

w , the activities
O , and the weights and the activities are independent of each other. Then, for n large, S is

E(S) = nμwμO

and

Var(S) = nVar(w i O i) = n

(cid:16)

(cid:10)

E

(cid:11)

(cid:11)

(cid:10)

E

O 2
i

w 2
i

− E(w i)2 E(O i)2

(cid:16)(cid:10)

(cid:17)

= n

σ 2
w

+ μ2
w

(cid:11)(cid:10)

σ 2
O

+ μ2
O

(cid:11)

− μ2

wmu2
O

(cid:17)

In a typical case where μw = 0, the variance reduces to

Var(S) = n

(cid:10)

(cid:16)

σ 2
w

σ 2
O

+ μ2
O

(cid:11)(cid:17)

E.3. Assuming O has a Beta distribution

(228)

(229)

(230)

The variable O is between 0 and 1 and thus it is natural to assume a Beta distribution with parameters a (cid:3) 0 and b (cid:3) 0

with the density

f O (o) = B(a, b)oa−1(1 − o)b−1

with the normalizing constant B(a, b) = Γ (a + b)/Γ (a)Γ (b). In this case, the distribution of S is given by

F S (s) = P (S (cid:2) s) = P

(cid:10)

(cid:11)
O (cid:2) σ (s)

=

σ (s)(cid:19)

B(a, b)oa−1(1 − o)b−1 do

0

which yields the density

f S (s) = B(a, b)σ (s)a−1

(cid:10)

(cid:10)
(cid:11)
b−1λσ (s)
1 − σ (s)

(cid:11)
1 − σ (s)

= λB(a, b)σ (s)a

(cid:10)

(cid:11)
b
1 − σ (s)

(231)

(232)

(233)

In general this density is bell-shaped, similar but not identical to a Gaussian density. For instance, in the balanced case
where a = b,

f S (s) = λB(a, b)σ (s)a

(cid:10)

(cid:11)
a = λB(a, b)
1 − σ (s)

(cid:6)

(cid:7)
a

ce

−λs
(1 + ce−λs)2

(234)

Note, for instance, how this density at +∞ decays exponentially like e
a quadratic one as in the exact Gaussian case.

−λas with a linear term in the exponent, rather than

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

121

Appendix F. Alternative estimate of the expectation

Here we describe an alternative way for obtaining a closed form estimate of E(O ) when O = σ (S) and S has a Gaussian
S , which is a reasonable assumption in the case of dropout applied to large

distribution with mean μS and variance σ 2
networks. It is known that the logistic function can be approximated by a cumulative Gaussian distribution in the form

1
1 + e−S

≈ Φ0,1(α S)

(235)

(cid:22)

1√

x
−∞

−t2/2 dt for a suitable value of α. Depending on the optimization criterion, different but rea-
where Φμ,σ 2 (x) =
sonably close values of α can be found in the literature such as α = 0.607 [21] or α = 1/1.702 = 0.587 [15]. Just equating
√
the ﬁrst derivatives of the two functions at S = 0 gives α =
2π /4 ≈ 0.626. In what follows, we will use α = 0.607. In any
case, for the more general logistic case, we have

2π σ

e

1
1 + ce−λS

≈ Φ0,1

(cid:10)

(cid:11)
α(λS − log c)

As a result, in the general case,

+∞(cid:19)

E(O ) ≈

Φ0,1

−∞

(cid:10)

(cid:11)
α(λS − log c)

− (S−μS )2
2σ 2
s

e

dS

1√

2πσS

It is easy to check that
(cid:6)

(cid:7)

−μ
σ

= Φμ,σ 2 (0)

Φ0,1

Thus

+∞(cid:19)

E(O ) ≈

P (Z < 0|S)

−∞

− (S−μS )2
2σ 2
s

e

1√

2πσS

dS = P (Z < 0)

(236)

(237)

(238)

(239)

where Z |S is normally distributed with mean −λS + log c and variance 1/α2. Thus Z is normally distributed with mean
−λμs + log c and variance σ 2
S

+ α−2, and the expectation can be estimated by

E(O ) ≈ P (Z < 0) = Φ−λμs+log c,σ 2

S

+α−2 (0) = Φ0,1

(cid:7)

(cid:6)

λμS − log c
+ α−2
σ 2
S

Finally, using in reverse the logistic approximation to the cumulative Gaussian distribution, we have

(cid:6)

E(O ) ≈ Φ0,1

(cid:7)

≈

λμS − log c
+ α−2
σ 2
S

1 + e
In the usual case where c = λ = 1 this gives

1

− 1
α

λμS
(cid:24)
σ 2
S

−log c
+α−2

=

1 + e

1
− λμS
(cid:24)

−log c
S α2

1+σ 2

E(O ) ≈

1
S α2)−1/2μS
−(1+σ 2

≈

1
−(1+0.368σ 2

S )−1/2μS

1 + e

1 + e

(240)

(241)

(242)

using α = 0.607 in the last approximation. In some cases this approximation to E(O ) may be more accurate than the
NWGMS approximation but there is a tradeoff. This approximation requires a normal assumption on S, as well as knowing
both the mean and the variance of S, whereas the NWGM approximation uses only the mean of S in the form E(O ) ≈
NWGM(O ) = σ (E(S)). For small values of σ 2
S , the estimate
in Eq. (242) converges to 0.5 whereas the NWGM could be arbitrarily close to 0 or 1 depending on the values of E(S)μS . In
practice this is not observed because the size of the weights remains limited due to the dropout regularization effect, and
thus the variance of S is also bounded.

S the two approximations are similar. For very large values of σ 2

Note that for non-Gaussian distributions, artiﬁcial cases can be constructed where the discrepancy between E and the
NWGM is even larger and goes all the way to 1. For example there is a large discrepancy for S = −1/(cid:8) with probability
1 − (cid:8) and S = 1/(cid:8)3 with probability (cid:8), and (cid:8) close to 0. In this case E(O ) ≈ 0 and NWGM ≈ 1.

122

P. Baldi, P. Sadowski / Artiﬁcial Intelligence 210 (2014) 78–122

References

[1] J. Aldaz, Self improvement of the inequality between arithmetic and geometric means, J. Math. Inequal. 3 (2) (2009) 213–216.
[2] J. Aldaz, Sharp bounds for the difference between the arithmetic and geometric means, arXiv preprint, arXiv:1203.4454, 2012.
[3] N. Alon, J.H. Spencer, The Probabilistic Method, John Wiley & Sons, 2004.
[4] H. Alzer, A new reﬁnement of the arithmetic mean geometric mean inequality, J. Math. 27 (3) (1997).
[5] H. Alzer, Some inequalities for arithmetic and geometric means, Proc. R. Soc. Edinb., Sect. A, Math. 129 (02) (1999) 221–228.
[6] G. An, The effects of adding noise during backpropagation training on a generalization performance, Neural Comput. 8 (3) (1996) 643–674.
[7] J. Ba, B. Frey, Adaptive dropout for training deep neural networks, in: C. Burges, L. Bottou, M. Welling, Z. Ghahramani, K. Weinberger (Eds.), Advances

in Neural Information Processing Systems, vol. 26, 2013, pp. 3084–3092.

[8] P. Baldi, K. Hornik, Neural networks and principal component analysis: Learning from examples without local minima, Neural Netw. 2 (1) (1988) 53–58.
[9] P. Baldi, K. Hornik, Learning in linear networks: a survey, IEEE Trans. Neural Netw. 6 (4) (1994) 837–858.

[10] P. Baldi, P.J. Sadowski, Understanding dropout, in: Advances in Neural Information Processing Systems, vol. 26, 2013, pp. 2814–2822.
[11] E.F. Beckenbach, R. Bellman, Inequalities, Springer-Verlag, Berlin, 1965.
[12] C.M. Bishop, Training with noise is equivalent to Tikhonov regularization, Neural Comput. 7 (1) (1995) 108–116.
[13] L. Bottou, Online algorithms and stochastic approximations, in: D. Saad (Ed.), Online Learning and Neural Networks, Cambridge University Press,

Cambridge, UK, 1998.

[14] L. Bottou, Stochastic learning, in: O. Bousquet, U. von Luxburg (Eds.), Advanced Lectures on Machine Learning, in: Lecture Notes in Artiﬁcial Intelligence,

vol. 3176, Springer Verlag, Berlin, 2004, pp. 146–168.

[15] S.R. Bowling, M.T. Khasawneh, S. Kaewkuekool, B.R. Cho, A logistic approximation to the cumulative normal distribution, J. Ind. Eng. Manag. 2 (1)

(2009) 114–127.

[16] S. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.
[17] L. Breiman, Bagging predictors, Mach. Learn. 24 (2) (1996) 123–140.
[18] C. Carr, M. Konishi, A circuit for detection of interaural time differences in the brain stem of the barn owl, J. Neurosci. 10 (10) (1990) 3227–3246.
[19] C.E. Carr, M. Konishi, Axonal delay lines for time measurement in the owl’s brainstem, Proc. Natl. Acad. Sci. 85 (21) (1988) 8311–8315.
[20] D. Cartwright, M. Field, A reﬁnement of the arithmetic mean-geometric mean inequality, Proc. Am. Math. Soc. (1978) 36–38.
[21] D.D.R. Cox, The Analysis of Binary Data, vol. 32, CRC Press, 1989.
[22] P. Diaconis, Bayesian numerical analysis, in: Statistical Decision Theory and Related Topics IV, vol. 1, 1988, pp. 163–175.
[23] R.O. Duda, P.E. Hart, D.G. Stork, Pattern Classiﬁcation, second ed., Wiley, New York, NY, 2000.
[24] D. Gardner, Noise modulation of synaptic weights in a biological neural network, Neural Netw. 2 (1) (1989) 69–76.
[25] S.J. Hanson, A stochastic version of the delta rule, Physica D 42 (1) (1990) 265–272.
[26] G. Harnischfeger, G. Neuweiler, P. Schlegel, Interaural time and intensity coding in superior olivary complex and inferior colliculus of the echolocating

bat molossus ater, J. Neurophysiol. 53 (1) (1985) 89–109.

[27] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, R.R. Salakhutdinov, Improving neural networks by preventing co-adaptation of feature detectors,

http://arxiv.org/abs/1207.0580, 2012.

[28] N. Levinson, Generalization of an inequality of Ky Fan, J. Math. Anal. Appl. 8 (1) (1964) 133–134.
[29] L. Maaten, M. Chen, S. Tyree, K.Q. Weinberger, Learning with marginalized corrupted features, in: Proceedings of the 30th International Conference on

Machine Learning (ICML-13), 2013, pp. 410–418.

[30] K. Matsuoka, Noise injection into inputs in back-propagation learning, IEEE Trans. Syst. Man Cybern. 22 (3) (1992) 436–440.
[31] A.M. Mercer, Improved upper and lower bounds for the difference an-gn, J. Math. 31 (2) (2001).
[32] P.R. Mercer, Reﬁned arithmetic, geometric and harmonic mean inequalities, J. Math. 33 (4) (2003).
[33] M. Mitzenmacher, E. Upfal, Probability and Computing: Randomized Algorithms and Probabilistic Analysis, Cambridge University Press, 2005.
[34] A.F. Murray, P.J. Edwards, Enhanced mlp performance and fault tolerance resulting from synaptic weight noise during training, IEEE Trans. Neural Netw.

5 (5) (1994) 792–802.

[35] E. Neuman, J. Sándor, On the Ky Fan inequality and related inequalities i, Math. Inequal. Appl 5 (2002) 49–56.
[36] E. Neuman, J. Sandor, On the Ky Fan inequality and related inequalities ii, Bull. Aust. Math. Soc. 72 (1) (2005) 87–108.
[37] Y. Raviv, N. Intrator, Bootstrapping with noise: An effective regularization technique, Connect. Sci. 8 (3–4) (1996) 355–372.
[38] H. Robbins, D. Siegmund, A convergence theorem for non negative almost supermartingales and some applications, in: Optimizing Methods in Statistics,

1971, pp. 233–257.

[39] R.T. Rockafellar, Convex Analysis, vol. 28, Princeton University Press, 1997.
[40] D. Rumelhart, G. Hintont, R. Williams, Learning representations by back-propagating errors, Nature 323 (6088) (1986) 533–536.
[41] D.J. Spiegelhalter, S.L. Lauritzen, Sequential updating of conditional probabilities on directed graphical structures, Networks 20 (5) (1990) 579–605.
[42] P. Vincent, H. Larochelle, Y. Bengio, P. Manzagol, Extracting and composing robust features with denoising autoencoders, in: Proceedings of the 25th

International Conference on Machine Learning, ACM, 2008, pp. 1096–1103.

[43] S. Wager, S. Wang, P. Liang, Dropout training as adaptive regularization, in: C. Burges, L. Bottou, M. Welling, Z. Ghahramani, K. Weinberger (Eds.),

Advances in Neural Information Processing Systems, vol. 26, 2013, pp. 351–359.

