Artiﬁcial Intelligence 313 (2022) 103784

Contents lists available at ScienceDirect

Artiﬁcial  Intelligence

www.elsevier.com/locate/artint

Two’s  company,  three’s  a  crowd:  Consensus-halving  for  a 
constant  number  of  agents ✩
Argyrios Deligkas a,∗
a Royal Holloway University of London, United Kingdom
b University of Edinburgh, United Kingdom
c University of Oxford, United Kingdom

,  Alexandros Hollender c,∗

,  Aris Filos-Ratsikas b,∗

a  r  t  i  c  l  e 

i  n  f  o

a  b  s  t  r  a  c  t

Article history:
Received 29 July 2021
Received in revised form 19 April 2022
Accepted 7 September 2022
Available online 9 September 2022

Keywords:
Consensus-halving
Fair division
Computational complexity
Query complexity
Robertson-Webb

We consider the ε-Consensus-Halving problem, in which a set of heterogeneous agents 
aim at dividing a continuous resource into two (not necessarily contiguous) portions that 
all of them simultaneously consider to be of approximately the same value (up to ε). This 
problem was recently shown to be PPA-complete, for n agents and n cuts, even for very 
simple  valuation  functions.  In  a  quest  to  understand  the  root  of  the  complexity  of  the 
problem, we consider the setting where there is only a constant number of agents, and we 
consider both the computational complexity and the query complexity of the problem.
For agents with monotone valuation functions, we show a dichotomy: for two agents the 
problem is polynomial-time solvable, whereas for three or more agents it becomes PPA-
complete.  Similarly,  we  show  that  for  two  monotone  agents  the  problem  can  be  solved 
with polynomially-many queries, whereas for three or more agents, we provide exponential 
query complexity lower bounds. These results are enabled via an interesting connection to 
a monotone Borsuk-Ulam problem, which may be of independent interest. For agents with 
general  valuations,  we  show  that  the  problem  is PPA-complete  and  admits  exponential 
query complexity lower bounds, even for two agents.
© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the 
CC BY license (http://creativecommons.org/licenses/by/4.0/).

1.  Introduction

The topic of fair division, founded in the work of Steinhaus [64], has been in the centre of the literature in economics, 
mathematics, and more recently computer science and artiﬁcial intelligence. Classic examples include the well-known fair 
cake  cutting  problem  for  the  division  of  divisible  resources  (e.g.,  see  [43],  [20] or  [58]),  as  well  as  the  fair  division  of 
indivisible  resources  (e.g.,  see  [19]).  The  earlier  works  in  economics  and  mathematics  were  mainly  concerned  with  the 
questions  of  whether  fair  solutions  exist,  and  whether  they  can  be  found  constructively,  i.e.,  via  ﬁnite-time  protocols.  In 
the more recent literature in computer science, a plethora of works has been concerned with the computational complexity 
of  ﬁnding  such  solutions,  either  by  providing  polynomial-time  algorithms,  or  by  proving  computational  hardness  results. 
Currently,  it  would  be  no  exaggeration  to  say  that  fair  division  is  one  of  the  most  vibrant  and  important  topics  in  the 
intersection of these areas.

✩

This paper is a participant in the 2021 ACM Conference on Economics and Computation (EC) Forward-to-Journal Program.

* Corresponding authors.

E-mail addresses: Argyrios.Deligkas@rhul.ac.uk (A. Deligkas), Aris.Filos-Ratsikas@ed.ac.uk (A. Filos-Ratsikas), Alexandros.Hollender@cs.ox.ac.uk

(A. Hollender).

https://doi.org/10.1016/j.artint.2022.103784
0004-3702/© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license 
(http://creativecommons.org/licenses/by/4.0/).

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Besides the classic fair division settings mentioned above, another well-known problem is the Consensus-Halving problem, 
whose origins date back to the 1940s and the work of Neyman [56]. In this problem, a set of n agents with different and 
heterogeneous valuation functions over the unit interval [0, 1] aim at ﬁnding a partition of the interval into pieces labelled 
either “+” or “−” using at most n cuts, such that the total value of every agent for the portion labelled “+” and for the 
portion labelled “−” is the same. Very much like other well-known problems in fair division, the existence of a solution can 
be proven via ﬁxed-point theorems, in particular the Borsuk-Ulam theorem [18], and can also be seen as a generalisation of 
the Hobby-Rice Theorem [48].

The  problem  has  applications  in  the  context  of  the  well-known  Necklace Splitting problem of  Alon [3] and  was  studied 
in conjunction with this latter problem [5,44]. Other applications were highlighted by Simmons and Su [63], who were the 
ﬁrst to study the problem in isolation. For example, consider two families that are dividing a piece of land into two regions, 
such  that  every  member  of  each  family  considers  the  split  to  be  equal.  Another  example  is  the  1994  Convention  of  the 
Law of the Sea (see [20]), which regards the protection of developing countries in the event that an industrialised nation is 
planning to mine resources in international waters. In such cases, a representative of the developing nations reserves half 
of the seabed for future use by them, and a consensus-halving solution would correspond to a partition of the seabed into 
two portions that all the developing nations consider to be of equal value.

Simmons  and  Su [63] in  fact  studied  the  approximate  version  of  Consensus-Halving,  coined  the ε-Consensus-Halving
problem,  where  the  requirement  is  that  the  total  value  of  every  agent  for  the  portion  labelled  “+”  and  for  the  portion 
labelled “−” is approximately the same, up to an additive parameter ε. For this version, Simmons and Su [63] provided a 
constructive solution via an exponential-time algorithm. The ε-Consensus-Halving problem received considerable attention 
in  the  literature  of  computer  science  over  the  past  few  years,  as  it  was  proven  to  be  the  ﬁrst  “natural” PPA-complete 
problem [38], i.e., a problem that does not have a polynomial-sized circuit explicitly in its deﬁnition, answering a decade-
old open question [47,57]. Additionally, Filos-Ratsikas and Goldberg [39], reduced from this problem to establish the PPA-
completeness of Necklace Splitting with two thieves; these PPA-completeness results provided the ﬁrst deﬁnitive evidence 
of intractability for these two classic problems, establishing for instance that solving them is at least as hard as ﬁnding a 
Nash equilibrium of a strategic game [26,29]. Filos-Ratsikas et al. [41] improved on the results for the ε-Consensus-Halving
problem,  by  showing  that  the  problem  remains PPA-complete,  even  if  one  restricts  the  attention  to  very  small  classes  of 
agents’  valuations,  namely  piecewise  uniform  valuations  with  only  two  valuation  blocks.  Very  recently,  the ε-Consensus-
Halving problem was shown to be PPA-complete even for constant ε, namely any ε < 1/5 [32].

This latter result falls under the general umbrella of imposing restrictions on the structure of the problem, to explore if 
the computational hardness persists or whether we can obtain polynomial-time algorithms. Filos-Ratsikas et al. [41] applied 
this approach along the axis of the valuation functions, while considering a general number of agents, similarly to [38,39]. 
In this paper, we take a different approach, and we restrict the number of agents to be constant. This is in line with most 
of the theoretical work on fair division, which is also concerned with solutions for a small number of agents1 and it is also 
quite relevant from a practical standpoint, as fair division among a few participants is quite common. We believe that such 
investigations are necessary in order to truly understand the computational complexity of the problem. To this end, we state 
our ﬁrst main question:

What is the computational complexity of ε-Consensus-Halving for a constant number of agents?

Since the number of agents is now ﬁxed, any type of computational hardness must originate from the structure of the 
valuation functions. We remark that the existence results for ε-Consensus-Halving are fairly general, and in particular do 
not require assumptions like additivity or monotonicity of the valuation functions. For this reason, the sensible approach is 
to start from valuations that are as general as possible (for which hardness is easier to establish), and gradually constrain 
the domain to more speciﬁc classes, until eventually polynomial-time solvability becomes possible. Indeed, in a paper that 
is conceptually similar to ours, Deng et al. [34] studied the computational complexity of the contiguous envy-free cake-cutting 
problem2 and proved that the problem is PPAD-complete, even for three agents, when agents have ordinal preferences over 
the possible pieces. These types of preferences induce no structure on the valuation functions and are therefore as general as 
possible. In contrast, the authors showed that for three agents and monotone valuations, the problem becomes polynomial-
time solvable, leaving the case of four or more agents as an open problem. We adopt a similar approach in this paper for 
ε-Consensus-Halving, and we manage to completely settle the complexity of the problem when the agents have monotone 
valuations, among other results, which are highlighted in Section 1.1.

Another relevant question that has been surprisingly overlooked in the related literature is the query complexity of the 
problem. In this regime, the algorithm interacts with the agents via queries, asking them to provide their values for different 
parts  of  the  interval  [0, 1],  and  the  complexity  is  measured  by  the  number  of  queries  required  to  ﬁnd  an  ε-approximate 
solution. This brings us to our second main question:

1 For example, the existence of bounded protocols for cake-cutting for more than 3 agents was not known until very recently [8,9], but such protocols 
for 2 and 3 agents were known since the 1960s (see [20,61]).
2 This version of the classic envy-free cake-cutting problem requires that every agent receives a single, connected piece.

2

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Fig. 1. A classiﬁcation of ε-Consensus-Halving for a constant number n of agents, in terms of increasing generality of the valuation functions.

What is the query complexity of ε-Consensus-Halving for a constant number of agents?

We develop appropriate machinery that allows us to answer both of our main questions at the same time. In a nutshell, 
for  the  positive  results,  we  design  algorithms  that  run  in  polynomial  time  and  can  be  recast  as  query-based  algorithms 
that only use a polynomial number of queries. For the negative results, we construct reductions from “hard” computational 
problems which allow us to simultaneously obtain computational hardness results and query complexity lower bounds.

1.1.  Our results

In  this  section,  we  list  our  main  results  regarding  the  computational  complexity  and  the  query  complexity  of  the 

ε-Consensus-Halving problem.

Computational Complexity: We  start  from  the  computational complexity of  the  problem  for  a  constant  number  of  agents. 
We  prove  the  following  main  results,  parameterised  by  (a)  the  number  of  agents  and  (b)  the  structure  of  the  valuation 
functions.

- For a single agent and general valuations, the problem is polynomial-time solvable. The same result applies to the case 

of any number of agents with identical general valuations.

- For two or more agents and general valuations, the problem is PPA-complete.
- For two agents and monotone valuations, the problem is polynomial-time solvable. This result holds even if one of the 

two agents has a general valuation.

- For three or more agents and monotone valuations, the problem is PPA-complete.

Finally, the ε-Consensus-Halving problem with  2 agents coincides with the well-known ε-Perfect Division problem for 
cake-cutting (e.g., see [21,22]), and thus naturally our results imply that ε-Perfect Division with  2 agents with monotone 
valuations can be done in polynomial time, whereas it becomes PPA-complete for 2 agents with general valuations.

Before we proceed, we offer a brief discussion on the different cases that are covered by our results. The distinction on 
the number of agents is straightforward. For the valuation functions, we consider mainly general valuations and monotone 
valuations. Note that neither of these functions is additive, meaning that the value that an agent has for the union of two 
disjoint intervals [a, b] and [c, d] is not necessarily the sum of her values for the two intervals. For monotone valuations, the 
requirement is that for any two subsets I and I
at least as much as I , whereas 
for general valuations there is no such requirement.

of [0, 1] such that I ⊆ I

, the agent values I

(cid:3)

(cid:3)

(cid:3)

We  remark  that  for  agents  with  piecewise  constant  valuations  (i.e.,  the  valuations  used  in  [38] to  obtain  the PPA-
completeness  of  the  problem  for  many  agents),  the  problem  can  be  solved  rather  straightforwardly  in  polynomial  time 
for a constant number of agents, using linear programming (see Appendix A). In terms of the classiﬁcation of the complex-
ity of the problem in order of increasing generality of the valuation functions, this observation provides the “lower bound” 
whereas our PPA-hardness results for monotone valuations provide the “upper bound”, see Fig. 1. While the precise point 
of the phase transition has not yet been identiﬁed, our results make considerable progress towards this goal.

Query Complexity: Besides  the  computational  complexity  of  the  problem,  we  are  also  interested  in  its  query  complexity. 
In this setting, one can envision an algorithm which interacts with the agents via a set of queries, and aims to compute a 
solution to ε-Consensus-Halving using the minimum number of queries possible. In particular, a query is a question from 
the algorithm to an agent about a subset of [0, 1], who then responds with her value for that set. We provide the following 
results, where L denotes the Lipschitz parameter of the valuation functions:

for any number of agents with identical general valuations.

(cid:2)
log L
- For a single agent and general valuations, the query complexity of the problem is (cid:3) 
ε
(cid:5)
(cid:4)(cid:2)
(cid:3)n−1
L
ε
(cid:4)
log2 L
- For two agents and monotone valuations, the query complexity of the problem is  O  
ε

- For n ≥ 2 agents and general valuations, the query complexity of the problem is (cid:3) 

.
(cid:5)

(cid:3)

. The same result applies 

. This result holds even if 

one of the two agents has a general valuation.

3

- For  n ≥ 3 agents and  monotone  valuations,  the  query  complexity  of  the  problem  is  between  (cid:4) 

Artiﬁcial Intelligence 313 (2022) 103784
(cid:4)(cid:2)

(cid:5)

(cid:3)n−2

L
ε

and 

A. Deligkas, A. Filos-Ratsikas and A. Hollender

(cid:4)(cid:2)

(cid:5)

(cid:3)n−1

O  

L
ε

.

To put these results into context, we remark that when studying the query complexity of the problem, the input consists 
of the error parameter ε and the Lipschitz parameter L, given by their binary representation. In that sense, for some constant 
k, a (cid:3)(logk(L/ε)) number of queries is polynomial in the size of the input. On the contrary, a (cid:3)(L/ε) number of queries 
is exponential in the size of the input. Not surprisingly, our PPA-hardness results give rise to exponential query complexity 
lower  bounds,  whereas  our  algorithms  can  be  transformed  into  query-based  algorithms  of  polynomial  query  complexity. 
We remark however that beyond this connection, our query complexity analysis is in fact quantitative, as we provide tight 
or almost tight bounds on the query complexity as a function of the number of agents n, for both general and monotone 
valuation functions.

Finally, for the case of monotone valuations, we consider a more expressive query model, which is an appropriate ex-
tension  of  the  well-known  Robertson-Webb query  model  [61,67],  the  predominant  query  model  in  the  literature  of  fair 
cake-cutting  [20];  we  refer  to  this  extension  as  the  Generalised Robertson-Webb (GRW) query  model.  We  show  that  our 
bounds extend to this model as well, up to logarithmic factors.

1.2.  Related work

As  we  mentioned  in  the  introduction,  the  origins  of  the  Consensus-Halving  problem  can  be  traced  back  to  the  1940s 
and  the  work  of  Neyman [56],  who  studied  a  generalisation  of  the  problem  with k labels  instead  of  two  (“+”,  “−”),  and 
proved the existence of a solution when the valuation functions are probability measures and there is no constraint on the 
number of cuts used to obtain the solution. The existence theorem for two labels is known as the Hobby-Rice Theorem [48]
and has been studied rather extensively in the context of the famous Necklace Splitting problem [3,5,44]. In fact, most of the 
proofs  of  existence  for  Necklace  Splitting  (with  two  thieves)  were  established  via  the  Consensus-Halving  problem,  which 
was at the time referred to as Continuous Necklace Splitting [3]. The term “Consensus-Halving” was coined by Simmons and 
Su [63], who studied the continuous problem in isolation, and provided a constructive proof of existence which holds for 
very  general  valuation  functions,  including  all  of  the  valuation  functions  that  we  consider  in  this  paper.  Interestingly,  the 
proof of Simmons and Su [63] reduces the problem to ﬁnding edges of complementary labels on a triangulated n-sphere, 
labelled as prescribed by Tucker’s lemma, a fundamental result in topology.

While  not  strictly  a  reduction  in  the  sense  of  computational  complexity,  the  ideas  of  [63] certainly  paved  the  way 
for subsequent work on the problem in computer science. The ﬁrst computational results were obtained by Filos-Ratsikas 
et al. [40],  who  proved  that  the  associated  computational  problem,  ε-Consensus-Halving,  lies  in PPA (adapting  the  con-
structive proof of Simmons and Su [63]) and that the problem is PPAD-hard, for n agents with piecewise constant valuation 
functions. Filos-Ratsikas and Goldberg [38] proved that the problem is in fact PPA-complete, establishing for the ﬁrst time 
the existence of a “natural” problem complete for PPA, i.e., a problem that does not contain a polynomial-sized circuit ex-
plicitly in its deﬁnition, answering a long-standing open question of Papadimitriou [57]. In a follow-up paper, [39] used the
PPA-completeness of Consensus-Halving to prove that the Necklace Splitting problem with two thieves is also PPA-complete. 
Very recently, Filos-Ratsikas et al. [41] strengthened the PPA-hardness result to the case of very simple valuation functions, 
namely piecewise constant valuations with at most two blocks of value. Deligkas et al. [31] studied the computational com-
plexity of the exact version of the problem, and obtained among other results its membership in a newly introduced class 
BU (for “Borsuk-Ulam” [18]) and its computational hardness for the well-known class FIXP of Etessami and Yannakakis [36]. 
Batziou et al. [12] showed that the corresponding strong approximation problem (with measures represented by algebraic 
circuits) is complete for BU. A version of Consensus-Halving with divisible items was studied by Goldberg et al. [46], who 
proved that the problem is polynomial-time solvable for additive utilities, but PPAD-hard for slightly more general utilities. 
Very recently, Deligkas et al. [30] showed the PPA-completeness of the related Pizza Sharing problem [50], via a reduction 
from Consensus-Halving.

Importantly, none of the aforementioned results apply to the case of a constant number of agents, which was prior to 
this paper completely unexplored. Additionally, none of these works consider the query complexity of the problem. A recent 
work [4] studies ε-Consensus-Halving in a hybrid computational model (see the full version of their paper) which includes 
query access to the valuations, but contrary to our paper, their investigations are not targeted towards a constant number 
of agents, and the agents have additive valuation functions.

A relevant line of work is concerned with the query complexity of fair cake-cutting [20,59], a related but markedly differ-
ent fair-division problem. Conceptually closest to ours is the paper by Deng et al. [34], who study both the computational 
complexity and the query complexity of contiguous envy-free cake-cutting, for agents with either general3 or monotone valu-
ations. For the latter case, the authors obtain a polynomial-time algorithm for three agents, and leave open the complexity 

3 More accurately, Deng et al. [34] prove their impossibility results for ordinal preferences, where for each possible division of the cake, the agent speciﬁes 
the piece that she prefers. In particular, if one were to deﬁne valuation functions consistent with these preferences, the value of an agent for a piece would 
have to depend also on the way the rest of the cake is divided among the agents.

4

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

of  the  problem  for  four  or  more  agents.  In  our  case,  for  ε-Consensus-Halving,  we  completely  settle  the  computational 
complexity of the problem for agents with monotone valuations.

In the literature of fair cake-cutting, most of the related research (e.g., see [6,8,9,20,21]) has focused on the well-known 
Robertson-Webb (RW) query  model,  in  which  agents  interact  with  the  protocol  via  two  types  of  queries,  evaluation queries
(eval) and cut queries (cut). As the name suggests, this query model is due to Robertson and Webb [60,61], but the work 
of Woeginger and Sgall [67] has been rather instrumental in formalising it in the form that it is being used today. Given 
the conceptual similarity of fair cake-cutting with Consensus-Halving, it certainly makes sense to study the latter problem 
under this query model as well, and in fact, the queries used by Alon and Graur [4] are essentially RW queries. As we show 
in Section 6, our bounds are qualitatively robust when migrating to this more expressive model, i.e., they are preserved up 
to logarithmic factors.

Related to our investigation is also the work of Brânzei and Nisan [21], who among other settings study the problem of 
ε-Perfect Division, which stipulates a partition of the cake into n pieces, such that each of the n agents interprets all pieces 
to be of approximate equal value (up to ε). For the case of n = 2, this problem coincides with ε-Consensus-Halving, and 
thus one can interpret our results for n = 2 as extensions of the results in [21] (which are only for additive valuations) to 
the  case  of  monotone  valuations  (for  which  the  problem  is  solvable  with  polynomially-many  queries)  and  to  the  case  of 
general valuations (for which the problem admits exponential query complexity lower bounds). Besides the aforementioned 
results, there is a plethora of works in computer science and artiﬁcial intelligence related to computational aspects of fair 
cake cutting and fair division in general, e.g., see [2,10,14–17,23,27,35,45,49,52,62].

2.  Preliminaries

In the ε-Consensus-Halving problem, there is a set of n agents with valuation functions v i (or simply valuations) over 
the interval [0, 1], and the goal is to ﬁnd a partition of the interval into subintervals labelled either “+” or “−”, using at 
most n cuts. This partition should satisfy that for every agent  i, the total value for the union of subintervals  I+
labelled 
“+” and the total value for the union of subintervals I−
labelled “−” is the same up to ε, i.e., |v i(I+) − v i(I−)| ≤ ε. In this 
paper we will assume n to be a constant and therefore the inputs to the problem will only be ε and the valuation functions 
v i .

We will be interested in fairly general valuation functions; intuitively, these will be functions mapping measurable subsets
A ⊆ [0, 1] to non-negative real numbers. Formally, let (cid:5)([0, 1]) denote the set of Lebesgue-measurable subsets of the interval 
[0, 1] and  λ : (cid:5)([0, 1]) → [0, 1] the  Lebesgue  measure.  We  consider  valuation  functions  v i : (cid:5)([0, 1]) → R≥0,  with  the 
interpretation that agent i has value  v i( A) for the subset  A ∈ (cid:5)([0, 1]) of the resource. Similarly to [4,11,21,22,34], we also 
require that the valuation functions be Lipschitz-continuous. Following [34], a valuation function  v i is said to be Lipschitz-
continuous  with  Lipschitz  parameter  L ≥ 0,  if  for  all  A, B ∈ (cid:5)([0, 1]),  it  holds  that  |v i( A) − v i(B)| ≤ L · λ( A(cid:9)B).  Here  (cid:9)
denotes the symmetric difference, i.e.,  A(cid:9)B = ( A \ B) ∪ (B \ A).

Valuation Classes: We will be particularly interested in the following three valuation classes, in terms of decreasing gener-
ality:

- General valuations, in which there is no further restriction to the functions v i .
- Monotone valuations,  in  which  v i( A) ≤ v i( A

. 
Intuitively, for this type of function, when comparing two sets such that one is a subset of the other, an agent cannot 
have a smaller value for the set that contains the other.

(cid:3)) for  any  two  Lebesgue-measurable  subsets  A and  A

such  that  A ⊆ A

- Additive valuations, in which  v i is a function from individual intervals in [0, 1] to R≥0 and for a set of intervals I, it 
holds that v i(I) =
I∈I v i(I). Note that if v i is an additive valuation function, then it is in fact a measure. We will not 
prove  any  results  for  this  type  of  valuation  function  in  this  paper,  but  we  deﬁne  them  for  reference  and  comparison 
(e.g., see Fig. 1).

(cid:6)

(cid:3)

(cid:3)

Normalisation: We  will  also  be  interested  in  valuation  functions  that  satisfy  some  standard  normalisation  properties.  A 
valuation function v i is normalised, if the following properties hold:

1. v i( A) ∈ [0, 1] for all  A ∈ (cid:5)([0, 1]),
2. v i(∅) = 0 and v i([0, 1]) = 1.

In  other  words,  we  require  the  agents’  values  to  lie  in  [0, 1] and  that  their  value  for  the  whole  interval  is  normalised  to 
1. These are the standard assumptions in the literature of the problem for additive valuations [3], as well as in the related 
problem of fair cake-cutting [59]. We will only consider normalised valuation functions for our lower bounds and hardness 
results,  whereas  for  the  upper  bounds  and  polynomial-time  algorithms  we  will  not  impose  any  normalisation;  this  only 
makes both sets of results even stronger.

5

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

With regard to the valuation classes deﬁned above, we will often be referring to their normalised versions as well, e.g., 

normalised general valuations or normalised monotone valuations.

Input models: Given the fairly general nature of the valuation functions, we need to specify the manner in which they will 
be accessed by an algorithm for ε-Consensus-Halving. Since we are interested in both the computational complexity and 
the query complexity of the problem, we will assume the following standard two ways of accessing these functions.

- In the black-box model, the valuation functions  v i can be arbitrary functions, and are accessed via queries (sometimes 
also referred to as oracle calls). A query to the function  v i inputs a Lebesgue-measurable subset  A (intuitively a set of 
subintervals) of [0, 1] and outputs v i( A) ∈ R≥0. This input model is appropriate for studying the query complexity of the 
problem, where the complexity is measured as the number of queries to the valuation function v i .
We will also consider the following weaker version of the black-box model, which we will use in our query complexity 
upper bounds, thus making them stronger: In the weak black-box model the input to a valuation function  v i is some 
set I of intervals, obtained by using at most n cuts, where n is the number of agents.

- In  the white-box model,  the  valuation  functions  v i are  polynomial-time algorithms,  mapping  sets  of  intervals  to  non-
negative  rational  numbers.  These  polynomial-time  algorithms  are  given  explicitly  as  part  of  the  input,  including  the 
Lipschitz parameter L.4 This input model is appropriate for studying the computational complexity of the problem, where 
the complexity is measured as usual by the running time of the algorithm.

We now provide the formal deﬁnitions of the problem in the black-box and the white-box model. Note that the Lipschitz-
parameter L is part of the input of the problem and thus will appear in the bounds we obtain. Some of the previous works 
take L to be bounded by a constant, and as a result it does not appear in their bounds.

Deﬁnition 1 (ε-Consensus-Halving (black-box model)). For any constant n ≥ 1, the problem ε-Consensus-Halving with n
agents is deﬁned as follows:

- Input: ε > 0, the Lipschitz parameter L, query access to the functions v i .
- Output: A partition of [0, 1] into two sets of intervals I+

and I−

such that for each agent i, it holds that |v i(I+) −

v i(I−)| ≤ ε, using at most n cuts.

Deﬁnition 2 (ε-Consensus-Halving (white-box model)). For any constant n ≥ 1, the problem ε-Consensus-Halving with 
n agents is deﬁned as follows:

- Input: ε > 0, the Lipschitz parameter L, polynomial-time algorithms v i .
- Output: A partition of [0, 1] into two sets of intervals I+

and I−

such that for each agent i, it holds that |v i(I+) −

v i(I−)| ≤ ε, using at most n cuts.

Terminology: When the valuation functions are normalised, we will refer to the problem as ε-Consensus-Halving with n
normalised agents. When the valuation functions are monotone, we will refer to the problem as ε-Consensus-Halving with n
monotone agents. If both conditions are true, we will use the term ε-Consensus-Halving with n normalised monotone agents.

2.1.  Borsuk-Ulam and Tucker

For  our PPA-hardness  results  and  query  complexity  lower  bounds,  we  will  reduce  from  a  well-known  problem,  the 
computational version of the Borsuk-Ulam Theorem [18], which states that for any continuous function  F from  Sn to Rn
there is a pair of antipodal points (i.e., x, −x) which are mapped to the same point. There are various equivalent versions of 
the problem (e.g., see [53]); we will provide a deﬁnition that is most appropriate for our purposes. In fact, we will include 
several  “optional”  properties  of  the  function  F in  our  deﬁnition,  which  will  map  to  properties  of  the  valuation  functions 
v i when we construct our reductions in subsequent sections. Speciﬁcally, we will impose conditions for normalisation and 
monotonicity, which will correspond to normalised valuation functions for our lower bounds/hardness results of Section 4, 
and to normalised monotone valuation functions for our lower bounds/hardness results of Section 5. Let  Bn = [−1, 1]n and 
let  ∂(Bn) denote its boundary. As before, we will require that the functions we consider be Lipschitz-continuous. We say 
that  F : Bn+1 → Bn is Lipschitz-continuous with parameter  L, if (cid:12)F (x) − F ( y)(cid:12)∞ ≤ L · (cid:12)x − y(cid:12)∞ for all  x, y ∈ Bn+1, where 
(cid:12)x(cid:12)∞ = maxi |xi|.

4 To avoid introducing too many technical details here, we refer to Appendix B for the fully formal deﬁnition.

6

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Deﬁnition 3 (nD-Borsuk-Ulam). For any constant n ≥ 1, the problem nD-Borsuk-Ulam is deﬁned as follows:

- Input: ε > 0, the Lipschitz parameter L, a function  F : Bn+1 → Bn.
- Output: A point x ∈ ∂(Bn+1) such that (cid:12)F (x) − F (−x)(cid:12)∞ ≤ ε.
- Optional Properties:
Normalisation:

(cid:2) F (1, 1, . . . , 1) = (1, 1, . . . , 1).
(cid:2) F (−x) = −F (x), for all x ∈ Bn+1.

Monotonicity:

(cid:2) If x ≤ y, then  F (x) ≤ F ( y), for all x, y ∈ Bn+1, where “≤” denotes coordinate-wise comparison.

In  the  normalised nD-Borsuk-Ulam problem,  where  F is  normalised,  we  instead  ask  for  a  point  x ∈ ∂(Bn+1) such  that 
(cid:12)F (x)(cid:12)∞ ≤ ε.  By  using  the  fact  that  F is  an  odd  function  (F (−x) = −F (x),  for  all  x ∈ Bn+1),  it  is  easy  to  see  that  this 
is equivalent to (cid:12)F (x) − F (−x)(cid:12)∞ ≤ ε/2. We will also use the term normalised monotone nD-Borsuk-Ulam to refer to the 
problem when both the normalisation and monotonicity properties are satisﬁed for  F .

In  the  black-box  version  of nD-Borsuk-Ulam,  we  can  query  the  value  of  the  function  F at  any  point  x ∈ Bn+1.  In  the 
white-box version of this problem, we are given a polynomial-time algorithm that computes  F . Since the number of inputs 
of  F is  ﬁxed,  we  can  assume  that  we  are  given  an  arithmetic  circuit  with  n + 1 inputs  and  n outputs  that  computes 
F .  Following  the  related  literature  [28],  we  will  consider  circuits  that  use  the  arithmetic  gates  +, −, ×, max, min, < and 
rational constants.5

Another  related  problem  that  will  be  of  interest  to  us  is  the  computational  version  of  Tucker’s  Lemma  [66].  Tucker’s 
lemma is a discrete analogue of the Borsuk-Ulam theorem, and its computational counterpart, nD-Tucker, is deﬁned below.

Deﬁnition 4 (nD-Tucker). For any constant n ≥ 1, the problem nD-Tucker is deﬁned as follows:

- Input: grid size  N ≥ 2, a labelling function (cid:8) : [N]n → {±1, ±2, . . . , ±n} that is antipodally anti-symmetric (i.e., for 

any point  p on the boundary of [N]n, we have (cid:8)(p) = −(cid:8)(p), where  pi
- Output: Two points  p, q ∈ [N]n with (cid:8)(p) = −(cid:8)(q) and (cid:12)p − q(cid:12)∞ ≤ 1.

= N + 1 − pi for all i).

In the black-box version of this problem, we can query the labelling function for any point p ∈ [N]n and retrieve its label. 

In the white-box version, (cid:8) is given in the form of a Boolean circuit with the usual gates ∧, ∨, ¬.

In  the  white-box  model,  nD-Tucker was  recently  proven  to  be PPA-hard  for  any  n ≥ 2,  by  Aisenberg  et al. [1];  the 

membership of the problem in PPA was known by [57].

Theorem 2.1 ([1,57]). For any constant n ≥ 2, nD-Tucker is PPA-complete.

The  computational  class PPA was  deﬁned  by  Papadimitriou [57],  among  several  subclasses  of  the  class  TFNP  [54],  the 
class of problems with a guaranteed solution which is veriﬁable in polynomial time. PPA is deﬁned with respect to a graph 
of exponential size, which is given implicitly as input, via the use of a circuit that outputs the neighbours of a given vertex, 
and the goal is to ﬁnd a vertex of odd degree, given another such vertex as input.

Given the close connection between Tucker’s Lemma and the Borsuk-Ulam Theorem, the PPA-hardness of some appropri-
ate computational version of Borsuk-Ulam follows as well [1]. However, this does not apply to the version of nD-Borsuk-Ulam
deﬁned  above,  especially  when  one  considers  the  additional  properties  of  the  function  F required  for  normalisation  and 
monotonicity, as discussed earlier. We will reduce from nD-Tucker to our version of nD-Borsuk-Ulam, to obtain its PPA-
hardness  (even  for  the  normalised  monotone  version),  which  will  then  imply  the PPA-hardness  of  ε-Consensus-Halving, 
via our main reduction in Section 3.

In the black-box model, Deng et al. [33], building on the results of Chen and Deng [25], proved both query complexity 

lower bounds and upper bounds for nD-Tucker.

(cid:2)
Theorem 2.2 ([33]). For any constant n ≥ 2, the query complexity of nD-Tucker is (cid:3) 

Nn−1

(cid:3)

.

We remark that Deng et al. [33] use a version of nD-Tucker that is slightly different from the one that we deﬁned above, 

but their results apply to this version as well.

5 Formally speaking these circuits also need to be well-behaved in a certain sense (see [37]). It is easy to check that the circuits we construct in this 
paper all have this property.

7

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Remark 1. Some connections between the aforementioned problems, namely nD-Borsuk-Ulam, nD-Tucker, and ε-Consensus-
Halving are known from the previous literature. First, nD-Borsuk-Ulam and nD-Tucker are known to be computationally 
equivalent due to Papadimitriou [57] and Aisenberg et al. [1], although, technically speaking, none of the aforementioned 
papers  proved  this  result  formally,  or  even  deﬁned  nD-Borsuk-Ulam formally.  Yet,  even  the  implicit  reduction  between 
those problems is insuﬃcient for our purposes, because, in order to achieve our results for  Consensus-Halving, we need 
a version of nD-Borsuk-Ulam that exhibits several properties, as described in Deﬁnition 3 (namely, normalisation and pri-
marily monotonicity). Indeed, proving the PPA-completeness of monotone nD-Borsuk-Ulam is the main technical result of 
our work.

In terms of the completeness of Consensus-Halving, the works of Filos-Ratsikas and Goldberg [38,39] indeed establish 
reductions from nD-Tucker, but crucially, these reductions are white-box, i.e., they have access to the Boolean circuit that 
encodes the labelling function. On the contrary, our reductions are black-box (see Section 2.2 below), which allows us to 
obtain both computational complexity results and query complexity bounds at the same time. Also, quite importantly, all 
the previous reductions do not work when there is a constant number of agents.

2.2.  Eﬃcient black-box reductions

The reductions that we will construct (from nD-Tucker to nD-Borsuk-Ulam to ε-Consensus-Halving) will be black-box 
reductions, and therefore they will also allow us to obtain query complexity lower bounds for ε-Consensus-Halving in the 
black-box model, given the corresponding lower bounds of Theorem 2.2. For the upper bounds, we will reduce directly from 
ε-Consensus-Halving to nD-Tucker, again via a black-box reduction.

Roughly speaking,6 a black-box reduction from Problem A to Problem B is a procedure by which we can answer oracle 
calls (queries) for an instance of Problem B by using an oracle for some instance of Problem A, such that a solution to the 
instance of Problem B yields a solution to the instance of Problem A. For example, a black-box reduction from nD-Borsuk-
Ulam to ε-Consensus-Halving is a procedure that simulates an instance for the latter problem by accessing the function  F
of the former problem a number of times, and such that a solution of ε-Consensus-Halving can easily be translated to a 
solution of nD-Borsuk-Ulam. The name “black-box” comes from the fact that this type of reduction does not need to know 
the structure of the functions v i of ε-Consensus-Halving or  F of nD-Borsuk-Ulam.

In order to prove lower bounds on the query complexity of some Problem B, it suﬃces to construct a black-box reduction 
from  some  Problem  A,  for  which  query  complexity  lower  bounds  are  known;  the  obtained  bounds  will  depend  on  the 
number k of oracle calls to the input of Problem A that are needed to answer an oracle call to the input of Problem B. A 
black-box  reduction  is  eﬃcient if k is  a  constant,  and  therefore  the  query  complexity  lower  bounds  of  Problems  A  and  B 
are  of  the  same  asymptotic  order.  To  obtain  upper  bounds  on  the  query  complexity,  we  can  construct  a  reduction  in  the 
opposite direction (from Problem B to Problem A), assuming that query complexity upper bounds for Problem A are known.
Ideally,  we  would  like  to  use  the  same  reduction  to  also  obtain  computational  complexity  results  in  the  white-box 
model.  For  this  to  be  possible,  the  procedure  described  above  should  actually  be  a  polynomial-time  algorithm.  Slightly 
abusing terminology, we will use the term “eﬃcient” to describe such a reduction in the white-box model as well.

Deﬁnition 5. We say that a black-box reduction from Problem A to Problem B is eﬃcient if:

- in  the  black-box  model,  it  uses  a  constant  number  of  queries  (oracle  calls)  to  the  function  (oracle)  of  Problem  A,  for 

each query (oracle call) to the function of Problem B;

- in the white-box model, the condition above holds, and the reduction is also a polynomial-time algorithm.

Concretely for our case, all of our reductions will be eﬃcient black-box reductions, thus allowing us to obtain both PPA-
completeness results and query complexity bounds matching those of the problems that we reduce from/to. We remark that 
the reductions constructed for proving the PPA-hardness of the problem in previous works (for a non-constant number of 
agents) [38,39,41] are not black-box reductions, and therefore have no implications on the query complexity of the problem.

3.  Black-box reductions to and from consensus-halving

In this section we develop our main machinery for proving both PPA-completeness results and query complexity upper 
and lower bounds for ε-Consensus-Halving. We summarise our general approach for obtaining positive and negative results 
below.

For  our  impossibility results (i.e.,  computational  hardness  results  in  the  white-box  model  and  query  complexity  lower 
bounds in the black-box model), we will construct an eﬃcient black-box reduction from nD-Borsuk-Ulam to ε-Consensus-
Halving with n agents (Proposition 3.1). This reduction will preserve the optional properties of Deﬁnition 3, meaning that 
if  the  instance  of  nD-Borsuk-Ulam is  normalised  (respectively  monotone),  the  valuation  functions  of  the  corresponding 

6 To keep the exposition clean, we do not provide formal deﬁnitions of these concepts here, as they are rather standard; we refer the reader to the 
related works of [13,51] for more details.

8

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

instance of ε-Consensus-Halving will be normalised (respectively monotone) as well. This will allow us in subsequent sec-
tions  to  reduce  the  problem  of  proving  impossibility  results  for ε-Consensus-Halving to  proving  impossibility  results  for 
the versions of nD-Borsuk-Ulam with those properties. We will obtain these latter results via reductions from nD-Tucker, 
which  for  n ≥ 2 is  known  to  be PPA-hard  (Theorem 2.1)  and  admit  exponential  query  complexity  lower  bounds  (Theo-
rem 2.2).

For our positive results (i.e., membership in PPA in the white-box model and query complexity upper bounds in the black-
box model), we will construct an eﬃcient black-box reduction from ε-Consensus-Halving to nD-Tucker (Proposition 3.2). 
We remark here that a similar reduction already exists in the related literature [42], but only applied to the case of additive 
valuation functions. The extension to the case of general valuations follows along the same lines, and we provide it here for 
completeness. We also note that some of our positive results, namely the results for one general agent and two monotone 
agents, will not be obtained via reductions, but rather directly via the design of polynomial-time algorithms in the white-box 
model or algorithms of polynomial query complexity in the black-box model.

Related  to  the  discussion  above,  we  have  the  following  two  propositions.  Their  proofs  are  presented  in  the  sections 

below.

Proposition 3.1. There is an eﬃcient black-box reduction from (normalised, monotone) nD-Borsuk-Ulam to (normalised, monotone) 
ε-Consensus-Halving.

Proposition 3.2. There is an eﬃcient black-box reduction from ε-Consensus-Halving to nD-Tucker.

3.1.  Proof of Proposition 3.1

Description of the reduction. Let n ≥ 1 be a ﬁxed integer. Let ε > 0 and let F : Bn+1 → Bn be a Lipschitz-continuous function 
with Lipschitz parameter L. We now construct valuation functions v 1, . . . , vn for a Consensus-Halving instance.

Let  R1, R2, . . . , Rn+1 denote the partition of interval [0, 1] into n + 1 subintervals of equal length, i.e.,  R j = [ j−1

]

n+1 ,  j
n+1

for  j ∈ [n + 1]. For any  A ∈ (cid:5)([0, 1]), we deﬁne x( A) ∈ Bn+1 = [−1, 1]n+1 by

[x( A)] j = 2(n + 1) · λ( A ∩ R j) − 1

for all  j ∈ [n + 1]. Recall that λ denotes the Lebesgue measure on the interval [0, 1]. Note that since λ( A ∩ R j) ∈ [0,  1
n+1
we indeed have [x( A)] j ∈ [−1, 1]; see Fig. 2 for a visualisation.

For i ∈ [n], the valuation function v i of the ith agent is deﬁned as

], 

v i( A) = F i(x( A)) + 1

2

for any  A ∈ (cid:5)([0, 1]), where  F i : Bn+1 → [−1, 1] is the ith output of  F . Note that v i( A) ∈ [0, 1], since  F i(x( A)) ∈ [−1, 1].

Lipschitz-continuity. For any  A, B ∈ (cid:5)([0, 1]) it holds that

|v i( A) − v i(B)| = 1
2
≤ L
2

|F i(x( A)) − F i(x(B))| ≤ 1
2

(cid:12)F (x( A)) − F (x(B))(cid:12)∞

(cid:12)x( A) − x(B)(cid:12)∞ ≤ (n + 1) · L · max
j∈[n+1]

|λ( A ∩ R j) − λ(B ∩ R j)|

≤ (n + 1) · L · max
j∈[n+1]

λ(( A(cid:9)B) ∩ R j) ≤ (n + 1) · L · λ( A(cid:9)B).

Thus, v i is Lipschitz-continuous with Lipschitz parameter (n + 1) · L.

Correctness. Now  consider  an  ε/2-Consensus-Halving  of  v 1, . . . , vn.  Namely,  let  I+, I−
be  a  partition  of  [0, 1] using  at 
most n cuts, such that |v i(I+) − v i(I−)| ≤ ε/2 for all i ∈ [n]. Since I+
is obtained by using at most n cuts, it follows that 
there exists (cid:8) ∈ [n + 1] such that R(cid:8) does not contain a cut. As a result, I+ ∩ R(cid:8) is either empty or equal to R(cid:8). This implies 
that λ(I+ ∩ R(cid:8)) ∈ {0,  1
n+1

} and thus [x(I+)](cid:8) ∈ {±1}, i.e., x(I+) ∈ ∂(Bn+1). Furthermore, for any  j ∈ [n + 1], we have

[x(I+

)] j = 2(n + 1) · λ(I+ ∩ R j) − 1 = 2(n + 1) ·

(cid:7)

1
n + 1

(cid:8)
− λ(I− ∩ R j)

− 1

= −2(n + 1) · λ(I− ∩ R j) + 1 = −[x(I−
Letting  y = x(I+) ∈ ∂(Bn+1), we have that for any i ∈ [n]

)] j.

|F i( y) − F i(− y)| = |F i(x(I+

)) − F i(x(I−

))| = 2|v i(I+

) − v i(I−

)| ≤ ε.

9

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Fig. 2. The partition of [0, 1] into n + 1 subintervals of equal length and a set  A, coloured by the green region, as it is deﬁned by the red cuts. The ﬁrst 
three cuts on the left are located at positions a ≤ b ≤ c, where a ∈ R1 and b, c ∈ R2. Here, since λ( A ∩ R1) = 1/(n + 1) − a, we would obtain [x( A)]1 =
2(n + 1)(1/(n + 1) − a) − 1 = 1 − 2(n + 1)a. Similarly, λ( A ∩ R2) = 1/(n + 1) − (c − b), and thus [x( A)]2 = 1 − 2(n + 1)(c − b). (For interpretation of the 
colours in the ﬁgure(s), the reader is referred to the web version of this article.)

Thus,  y is a solution to the original nD-Borsuk-Ulam instance.

White-box model. This  reduction  yields  a  polynomial-time  many-one  reduction  from  nD-Borsuk-Ulam to  ε-Consensus-
Halving with n agents. Thus, if we show that nD-Borsuk-Ulam is PPA-hard for some n, then we immediately obtain that 
ε-Consensus-Halving with n agents is also PPA-hard.

Black-box model. It  is  easy  to  see  that  this  is  a  black-box  reduction.  It  can  be  formulated  as  follows:  given  access  to  an 
oracle for an instance of nD-Borsuk-Ulam with parameters (ε, L) we can simulate an oracle for an instance of ε-Consensus-
Halving (with n agents) with parameters (ε/2, (n + 1)L) such that any solution of the latter yields a solution to the former. 
Furthermore,  in  order  to  answer  a  query  to  some  v i ,  we  only  need  to  perform  a  single  query  to  F .  Thus,  we  obtain  the 
following query lower bound: solving an instance of ε-Consensus-Halving (with n agents) with parameters (ε, L) requires 
at least as many queries as solving an instance of nD-Borsuk-Ulam with parameters (ε(cid:3), L
n+1 ). This means that 
ε(cid:3) )n−1) for some n, then ε-Consensus-Halving (with n agents) has a 
if nD-Borsuk-Ulam has a query lower bound of (cid:4)(( L
query lower bound of (cid:4)((

(cid:3)) = (2ε,  L

ε )n−1), since n is constant.

2ε(n+1) )n−1) = (cid:4)(( L

L

(cid:3)

Additional properties of the reduction. Some properties of the Borsuk-Ulam function F carry over to the valuation functions 
v 1, . . . , vn. In particular, the following properties are of interest to us:

- If  F is monotone, then  v 1, . . . , vn are monotone. Indeed,  consider  A, B ∈ (cid:5)([0, 1]) with  A ⊆ B.  Then,  it  holds  that 
λ( A ∩ R j) ≤ λ(B ∩ R j) for all  j ∈ [n + 1], and as a result x( A) ≤ x(B) (coordinate-wise). By monotonicity of  F , it follows 
that  F i(x( A)) ≤ F i(x(B)), and thus v i( A) ≤ v i(B) for all i ∈ [n].

- If  F is normalised, then v 1, . . . , vn are normalised. As  noted  earlier,  we  already  have  that  v i( A) ∈ [0, 1] for  all  A ∈
(cid:5)([0, 1]).  Thus,  it  remains  to  prove  that  v i(∅) = 0 and  v i([0, 1]) = 1.  It  is  easy  to  see  that  x([0, 1]) = (1, 1, . . . , 1)
and  thus  F (x([0, 1])) = (1, 1, . . . , 1) since  F is  normalised,  which  yields  v i([0, 1]) = 1.  On  the  other  hand,  we  have 
x(∅) = (−1, −1, . . . , −1) and  thus  F (x(∅)) = −F (−x(∅)) = −F (1, 1, . . . , 1) = (−1, −1, . . . , −1),  which  yields  v i(∅) = 0. 
Here  we  also  used  the  fact  F is  an  odd  function,  since  it  is  normalised.  In  fact,  since  F is  odd,  we  also  obtain  that 
v i( A) + v i( Ac) = 1 for all A ∈ (cid:5)([0, 1]), where Ac = [0, 1] \ A denotes the complement of A. This can be shown by noting 
that  x( Ac) = −x( A) (by  using  the  same  argument  as  for  I+
above)  and  then  using  the  fact  that  F (x( Ac)) =
−F (x( A)).

and  I−

This means that if we are able to show (white- or black-box) hardness of nD-Borsuk-Ulam where  F has additional proper-
ties, then the hardness will also hold for ε-Consensus-Halving with n agents that have the corresponding properties.

Furthermore,  note  that  if  F is  a  normalised  nD-Borsuk-Ulam function,  then  an  ε-approximate  Consensus-Halving  for 
v 1, . . . , vn (i.e., |v i(I+) − v i(I−)| ≤ ε), yields an ε-approximate solution to  F , in the sense that (cid:12)F (x(I+))(cid:12)∞ ≤ ε. This is 
due to the fact that, by deﬁnition,  F is an odd function, if it is normalised.

3.2.  Proof of Proposition 3.2

The reduction presented in this section is based on a proof of existence for a generalization of Consensus-Halving with 
general valuations given in [42]. This existence proof was also used in [42] to provide a reduction for additive valuations. It 
can easily be extended to work for general valuations as well. We include the full reduction here for completeness.

Description of the reduction. Consider  an  instance  of  ε-Consensus-Halving with  n agents  with  parameters  ε,  L.  Let 
m, where  Km = {−1, −(m − 1)/m, . . . , −1/m, 0,
v 1, . . . , vn denote the valuations of the agents. We consider the domain  K n
1/m, 2/m, . . . , (m − 1)/m, 1}, for m = (cid:17)2nL/ε(cid:18). A point in  K n
m corresponds to a way to partition the interval [0, 1] into two 
sets I+, I−
using at most n cuts. A very similar encoding was also used by Meunier [55] for the Necklace Splitting problem. 
m corresponds to the partition I+(x), I−(x) obtained as follows.
A point x ∈ K n

1. Provisionally put the label “+” on the whole interval [0, 1]
2. For (cid:8) = 1, 2, . . . , n:

- if x(cid:8) > 0, then put label “+” on the interval [0, x(cid:8)];

10

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

- if x(cid:8) < 0, then put label “−” on the interval [0, −x(cid:8)].

Note that subsequent assignments of a label to an interval, “overwrite” previous assignments. One way of thinking about 
it,  is  that  we  are  applying  a  coat  of  paint  on  the  interval  [0, 1].  Initially  the  whole  interval  is  painted  with  colour  “+”, 
and  as  the  procedure  is  executed,  various  subintervals  will  be  painted  over  with  colour  “−”  or  “+”.  It  is  easy  to  check 
that the ﬁnal partition into I+(x), I−(x) that is obtained, uses at most n cuts. Furthermore, for any x ∈ ∂ K n
m, the partition 
I+(−x), I−(−x) obtained from −x corresponds to the partition I+(x), I−(x) with labels “+” and “−” switched. In other 
words, I+(−x) = I−(x) and I−(−x) = I+(x). For a more formal deﬁnition of this encoding, see [42].

We deﬁne a labelling (cid:8) : K n
m

→ {±1, ±2, . . . , ±n} as follows. For any x ∈ K n
m:

1. Let i ∈ [n] be the agent that sees the largest difference between v i(I+(x)) and v i(I−(x)), i.e., i = arg maxi∈[n] |v i(I+(x)) −

v i(I−(x))|, where we break ties by picking the smallest such i.

2. Pick a sign  s ∈ {+, −} as follows. If  v i(I+(x)) > v i(I−(x)), then let  s = +. If  v i(I+(x)) < v i(I−(x)), then let  s = −. If 

v i(I+(x)) = v i(I−(x)), then pick s such that I s contains the left end of the interval [0, 1].

3. Set (cid:8)(x) = +i if s = +, and (cid:8)(x) = −i otherwise.

With this deﬁnition, it is easy to check that (cid:8)(−x) = −(cid:8)(x) for all x ∈ ∂(K n

m as a grid [N]n with 
N = 2m + 1, we thus obtain an instance (cid:9)(cid:8) : [N]n → {±1, ±2, . . . , ±n} of nD-Tucker. In particular, note that (cid:9)(cid:8) is antipodally 
anti-symmetric on the boundary, as required.

m). By re-interpreting  K n

Correctness. Any solution to the nD-Tucker instance (cid:9)(cid:8) yields  x, y ∈ K n
loss of generality, assume that (cid:8)(x) = +i for some i ∈ [n]. Since (cid:12)x − y(cid:12)∞ ≤ 1/m, we obtain that

m with (cid:12)x − y(cid:12)∞ ≤ 1/m and (cid:8)(x) = −(cid:8)( y). Without 

λ(I+

(x)(cid:9) I+

( y)) ≤

n(cid:10)

j=1

|x j − y j| = (cid:12)x − y(cid:12)1 ≤ n(cid:12)x − y(cid:12)∞ ≤ n/m

and the same bound also holds for λ(I−(x)(cid:9) I−( y)). Since v i is Lipschitz-continuous with parameter L, it follows that

|v i(I+

( y))| ≤ L · λ(I+
(x)) − v i(I+
and similarly for |v i(I−(x)) − v i(I−( y))|.

(x)(cid:9) I+

( y)) ≤ nL/m

Since  (cid:8)(x) = +i,  it  follows  that  v i(I+(x)) ≥ v i(I−(x)).  For  the  sake  of  contradiction,  let  us  assume  that  v i(I+(x)) >

v i(I−(x)) + ε. Then, it follows that
( y)) − v i(I−

v i(I+

( y)) ≥ v i(I+

(x)) − v i(I−

(x)) − 2nL/m > ε − 2nL/m ≥ 0

since  m ≥ 2nL/ε.  But  this  contradicts  the  fact  that  (cid:8)( y) = −i.  Thus,  it  must  hold  that  |v i(I+(x)) − v i(I−(x))| ≤ ε.  Since 
(cid:8)(x) = +i, it follows that for all  j ∈ [n]
(x)) − v j(I−

(x)) − v i(I−
This means that I+(x), I−(x) yields a solution to the original ε-Consensus-Halving instance.

(x))| ≤ |v i(I+

(x))| ≤ ε.

|v j(I+

Note that the reduction uses N = 2m + 1 ≤ 4nL/ε + 3 for the nD-Tucker instance. Furthermore, any query to the labelling 

function (cid:9)(cid:8) can be answered by performing 2n queries to the valuation functions v 1, . . . , vn.

4.  General valuations

We are now ready to prove our main results for the ε-Consensus-Halving problem, starting from the case of general 
valuations.  First,  for  a  single  agent  with  a  general  valuation  function,  a  simple  binary  search  procedure  is  suﬃcient  to 
solve ε-Consensus-Halving with a polynomial number of queries and in polynomial time, therefore obtaining an eﬃcient 
algorithm both in the white-box and in the black-box model. We have the following theorem.

Theorem 4.1. For one agent with a general valuation function (or multiple agents with identical general valuations), ε-Consensus-
(cid:2)
log L
Halving is solvable in polynomial time and has query complexity (cid:3) 
ε

(cid:3)

.

Proof. We will prove the theorem for the case of n = 1, as a solution to ε-Consensus-Halving for this case is also straight-
forwardly  a  solution  to  the  problem  with  multiple  agents  with  identical  valuations.  Our  algorithm  essentially  simulates 
binary search. We say that the label of a cut x ∈ [0, 1] is “+”, if  v([0, x]) > v([x, 1]) + ε. Respectively, the label of cut  x is 
“−”,  if  v([x, 1]) > v([0, x]) + ε.  In  any  other  case,  the  label  of  the  cut  is  0;  if  a  cut  has  label  0,  then  it  is  a  solution  to 
ε-Consensus-Halving. Observe that in order for an interval [a, b] ⊆ [0, 1] to contain a solution, it suﬃces that the label of a
be “−” and the label of b be “+” (or vice-versa); then there is deﬁnitely a point x ∈ [a, b] where the label is 0 (by continuity 
of v).

11

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Now let a = 0 and b = 1. If a or b has label 0, then we have immediately found a solution. Otherwise, note that if a has 
label “−”, then b must have label “+”, and vice-versa. For convenience, in what follows, we assume that a has label “−” 
and b has label “+”. Our algorithm proceeds as follows in every iteration. Given an interval [a, b] with label “−” for a and 
label “+” for b, it computes the label of  a+b
is “+”, it sets 
2 ; if the label is “−”, it sets a = a+b
b = a+b

2 . This can be done via two eval queries. Then, if the label of  a+b
2 ; and if the label is 0 it outputs this cut.

We claim that the algorithm will always ﬁnd a cut with label 0 after at most  log L

ε iterations. For the sake of contra-
diction, assume that there is no such cut after log L
L . In 
addition,  we  know  the  labels  of a and  b.  Cut a has  label  “−”,  thus  v([a, 1]) > v([0, a]) + ε,  and  cut  b has  label  “+”,  i.e., 
v([0, b]) > v([b, 1]) + ε. Since |b − a| ≤ ε

ε iterations. Observe that the length of [a, b] in this case will be  ε

L and v is L-Lipschitz-continuous, it follows that

2

|v([a, 1]) − v([b, 1])| ≤ L · λ([a, b)) ≤ L · ε
L

= ε

and similarly |v([0, a]) − v([0, b])| ≤ ε. Putting everything together, we obtain that

v([0, b]) ≤ v([0, a]) + ε < v([a, 1]) ≤ v([b, 1]) + ε
which contradicts the assumption that cut b has label “+”.

(cid:2)
log L
Since  a  polynomial-time  algorithm  which  queries  the  polynomial-time  algorithm  of  the  input  O  
ε
polynomial-time algorithm, we immediately obtain the polynomial-time upper bound for the white-box model.

(cid:3)

times  is  a 

For the black-box model, the algorithm immediately gives us the upper bound, whereas the lower bound follows from 
our general reduction from nD-Borsuk-Ulam (Proposition 3.1), and the query lower bounds for the latter problem obtained 
through Lemma 4.2 below. In more detail, 1D-Borsuk-Ulam (and thus ε-Consensus-Halving with a single agent) inherits 
its query complexity lower bounds from  1D-Tucker, which can be easily seen to require at least (cid:4)(log N) queries in the 
worst-case. The latter bound naturally translates to a (cid:4)(log(L/ε)) bound for ε-Consensus-Halving. We also remark that the 
upper bound holds for any version of the problem with general valuations, even in the weak black-box model, whereas the 
lower bound holds even for normalised general valuations and for the standard black-box model. (cid:2)

We now move to our results for two or more agents with general valuations. Here we obtain a PPA-completeness result 
for ε-Consensus-Halving, as well as exponential bounds on the query complexity of the problem. Our results demonstrate 
that for general valuations, even in the case of two agents, the problem is intractable in both the black-box and the white-
box model. The main technical result of the section is the following pivotal lemma, proved at the end of this section.

Lemma 4.2. For any constant n ≥ 1, nD-Tucker reduces to normalised nD-Borsuk-Ulam, via an eﬃcient black-box reduction.

Now  we  state  our  main  theorem  about  the  computational/query  complexity  of  the  ε-Consensus-Halving problem,  as 
well  as  a  corresponding  theorem  for  nD-Borsuk-Ulam.  The  proofs  follow  from  Theorems 2.1 and 2.2 characterising  the 
complexity of nD-Tucker and the following chain of reductions (where “≤” denotes an eﬃcient black-box reduction from 
the problem on the left-hand side to the problem on the right-hand side).

nD-Tucker ≤

nD-Borsuk-Ulam ≤

Lem. 4.2

P rop. 3.1

ε-Consensus-Halving ≤

P rop. 3.2

nD-Tucker

The speciﬁc parameters that appear in the bounds below follow from the proof of Lemma 4.2.

Theorem 4.3. Let n ≥ 2 be any constant. Then,

- ε-Consensus-Halving with n normalised general agents is PPA-complete. This remains the case, even if (a) we ﬁx ε ∈ (0, 1), or 

(b) we ﬁx L ≥ 3(n + 1);

- there exists a constant c > 0 such that for any ε ∈ (0, 1) and any L ≥ 3(n +1) with L/ε ≥ c, the query complexity of ε-Consensus-

Halving with n normalised general agents is (cid:3)((L/ε)n−1).

Theorem 4.4. Let n ≥ 2 be any constant. Then,

- normalised nD-Borsuk-Ulam is PPA-complete. This remains the case, even if (a) we ﬁx ε ∈ (0, 1), or (b) we ﬁx L ≥ 3;
- there exists a constant c > 0 such that for any ε ∈ (0, 1) and any  L ≥ 3 with  L/ε ≥ c, the query complexity of normalised 

nD-Borsuk-Ulam is (cid:3)((L/ε)n−1).

In both cases, the lower bounds hold even for the normalised versions of the problems, while the upper bounds hold 

even for the more general, non-normalised, versions.

12

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

4.1.  Reducing nD-tucker to normalised nD-Borsuk Ulam (Proof of Lemma 4.2)

Let  n ≥ 1 be  any  constant.  Consider  an  instance  (cid:8) : [N]n → {±1, ±2, . . . , ±n} of  nD-Tucker.  Let  ε ∈ (0, 1).  We  will 
construct  a  normalised  nD-Borsuk-Ulam function  F : Bn+1 → Bn that  is  Lipschitz-continuous  with  Lipschitz  parameter 
L = max{3, 4n2(N − 1)ε + 1} and such that any x ∈ ∂(Bn+1) with (cid:12)F (x)(cid:12)∞ ≤ ε yields a solution to the nD-Tucker instance.
Let  δ = min{2ε, 1}.  Note  that  δ ∈ (0, 1] and  ε < δ < 2ε.  Without  loss  of  generality,  we  can  assume  that  for  p =
(N, N, . . . , N) it  holds  (cid:8)(p) = +1.  Indeed,  it  is  easy  to  see  that  we  can  rename  the  labels  to  achieve  this,  without  intro-
ducing any new solutions.

The remainder of the proof will proceed in three steps: In Step 1, we will interpolate the nD-Tucker instance, to obtain 
a continuous function on [−1/2, 1/2]n, in Step 2, we extend this function to the whole domain [−1, 1]n , and in Step 3 we 
further extend to [−1, 1]n+1, to obtain an instance of the normalised nD-Borsuk-Ulam problem.

Step 1: Interpolating the nD-TUCKER instance. The ﬁrst step is to embed the nD-Tucker grid [N]n in [−1/2, 1/2]n, deﬁne 
the value of the function at every grid point according to the labelling function (cid:8) and then interpolate to obtain a continuous 
function  f : [−1/2, 1/2]n → [−δ · n, δ · n].

We embed the grid [N]n in [−1/2, 1/2]n in a straightforward way, namely p ∈ [N]n corresponds to (cid:9)p ∈ [−1/2, 1/2]n such 
that (cid:9)p j = −1/2 + (p j − 1)/(N − 1) for all  j ∈ [n]. Note that antipodal grid points exactly correspond to antipodal points in 
[−1/2, 1/2]n. In other words,  p and q are antipodal on the grid, if and only if (cid:9)p = −(cid:9)q.

Next we deﬁne the value of the function  f : [−1/2, 1/2]n → [−δ · n, δ · n] at the embedded grid points as follows

f ((cid:9)p) = δ · n · e(cid:8)(p)

for all p ∈ [N]n. For i ∈ [n], e+i denotes the ith unit vector in Rn, and e−i := −e+i . We then use Kuhn’s triangulation on the 
embedded grid to interpolate between these values and obtain a function  f : [−1/2, 1/2]n → [−δ · n, δ · n] (see Appendix C
for more details). We obtain:

• f
is antipodally anti-symmetric on the boundary of [−1/2, 1/2]n , i.e.,  f (−x) = − f (x) for all x ∈ ∂([−1/2, 1/2]n).
• f is Lipschitz-continuous with Lipschitz parameter 2n2(N − 1)δ, since the grid size is 1/(N − 1) and (cid:12) f ((cid:9)p)(cid:12)∞ ≤ δ · n for 

all  p ∈ [N]n.

• Any x ∈ [−1/2, 1/2]n such that (cid:12) f (x)(cid:12)∞ ≤ ε must lie in a Kuhn simplex that contains two grid points  p, q ∈ [N]n such 
that  (cid:8)(p) = −(cid:8)(q),  i.e.,  a  solution  to  the  nD-Tucker instance.  Indeed,  let  p0, p1, . . . , pn ∈ [N]n be  the  grid  points  of 
the Kuhn simplex containing x. If {(cid:8)(p0), (cid:8)(p1), . . . , (cid:8)(pn)} does not contain two opposite labels, then all the points in 
(cid:9)
V = { f (
pi)(cid:12)∞ = δ · n for all i, it follows that any convex 
combination v of vectors in V must be such that (cid:12)v(cid:12)1 ≥ δ · n, and thus (cid:12)v(cid:12)∞ ≥ δ. As a result, if (cid:12) f (x)(cid:12)∞ ≤ ε < δ, then 
{(cid:8)(p0), (cid:8)(p1), . . . , (cid:8)(pn)} must contain two opposite labels.

(cid:9)
p1), . . . , f ( (cid:9)pn)} lie in the same orthant of Rn. Since (cid:12) f (

(cid:9)
p0), f (

• f (1/2, 1/2, . . . , 1/2) = δ · n · e+1 = (δ · n, 0, 0, . . . , 0).

Now, we deﬁne  g : [−1/2, 1/2]n → [−δ, δ]n to be the truncation of  f to [−δ, δ]n, namely

gi(x) = min{δ, max{−δ, f i(x)}}.

It is not hard to see that  g is also antipodally anti-symmetric, Lipschitz-continuous with Lipschitz parameter 2n2(N − 1)δ
and g(1/2, 1/2, . . . , 1/2) = δ · e+1. Furthermore, if x ∈ [−1/2, 1/2]n is such that (cid:12)g(x)(cid:12)∞ ≤ ε, then, since ε < δ, (cid:12) f (x)(cid:12)∞ ≤ ε, 
and thus x again yields a solution to the nD-Tucker instance.

Step 2: Extending to [−1, 1]n. The goal of the next step is to deﬁne a function h : [−1, 1]n → [−1, 1]n that extends  g and 
ensures that h(1, 1, . . . , 1) = (1, 1, . . . , 1), while maintaining its other properties. For x ∈ [−1, 1]n we let T (x) ∈ [−1/2, 1/2]n
denote  its  truncation  to  [−1/2, 1/2]n,  i.e.,  [T (x)]i = min{1/2, max{−1/2, xi}} for  all  i ∈ [n].  The  function  h : [−1, 1]n →
[−1, 1]n is deﬁned as

⎧
⎨

h(x) =

⎩

(2 min j x j − 1) · 1 + (2 − 2 min j x j) · δ · e+1
(2 min j(−x j) − 1) · (−1) + (2 − 2 min j(−x j)) · δ · e−1
g(T (x))

if xi ≥ 1/2 for all i
if xi ≤ −1/2 for all i
otherwise

where 1 ∈ Rn denotes the all-ones vector, i.e., 1 = (1, 1, . . . , 1). Clearly, it holds that h(1, 1, . . . , 1) = 1. It is also easy to see 
that h(−x) = −h(x) for all x ∈ ∂([−1, 1]n), in particular because  T (−x) = −T (x). Furthermore, if (cid:12)h(x)(cid:12)∞ ≤ ε, then it must 
be that (cid:12)g(T (x))(cid:12)∞ ≤ ε, which yields a solution to the nD-Tucker instance. Indeed, if xi ≥ 1/2 for all i, then

h1(x) = (2 min

j

x j − 1) · 1 + (2 − 2 min

j

x j) · δ ≥ δ > ε

so (cid:12)h(x)(cid:12)∞ > ε. By the same argument, if xi < −1/2 for all i, then we also have (cid:12)h(x)(cid:12)∞ > ε.

13

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Since  g(1/2, 1/2, . . . , 1/2) = δ · e+1 and  g(−1/2, −1/2, . . . , −1/2) = δ · e−1,  it  is  easy  to  see  that  h is  continu-
ous.  Furthermore,  since  for  any  x, y ∈ [−1, 1]n it  holds  that  (cid:12)T (x) − T ( y)(cid:12)∞ ≤ (cid:12)x − y(cid:12)∞,  it  is  easy  to  see  that  h is 
2n2(N − 1)δ-Lipschitz-continuous  outside  of  {x ∈ [−1, 1]n|xi ≥ 1/2 for all i ∈ [n]} ∪ {x ∈ [−1, 1]n|xi ≤ −1/2 for all i ∈ [n]}. 
For  any  y, z ∈ {x ∈ [−1, 1]n|xi ≥ 1/2 for all i ∈ [n]},  it  holds  that  |hi( y) − hi(z)| = 2| min j y j − min j z j| ≤ 2(cid:12) y − z(cid:12)∞ for 
i > 1, and |h1( y) − h1(z)| = 2(1 − δ)| min j y j − min j z j| ≤ 2(cid:12) y − z(cid:12)∞. Thus, h is 2-Lipschitz-continuous on {x ∈ [−1, 1]n|xi ≥
1/2 for all i ∈ [n]} and, by the same argument, also on {x ∈ [−1, 1]n|xi ≤ −1/2 for all i ∈ [n]}.

As  a  result,  h is  Lipschitz-continuous  on  [−1, 1]n with  Lipschitz  parameter  max{2, 2n2(N − 1)δ}.  Indeed,  consider  any 
x, y ∈ [−1, 1]n.  If  xi ≥ 1/2 and  yi ≤ −1/2 for  all  i,  then  (cid:12)x − y(cid:12)∞ ≥ 1,  and  thus  (cid:12)h(x) − h( y)(cid:12)∞ ≤ 2 ≤ 2(cid:12)x − y(cid:12)∞.  By 
symmetry,  the  only  remaining  case  that  we  need  to  check  is  when  xi ≥ 1/2 for  all  i,  and  y is  such  that  there  exists 
i with  yi < 1/2 and  there  exists  i with  yi > −1/2.  In  that  case,  we  consider  the  segment  [x, y] from  x to  y,  and  let 
z ∈ [x, y] be the point that is the furthest away from  x but such that  zi ≥ 1/2 for all  i. Note that there must exist  i such 
that  zi = 1/2.  This  means  h(z) = g(T (z)) and  thus  (cid:12)h(z) − h( y)(cid:12)∞ ≤ 2n2(N − 1)δ(cid:12)z − y(cid:12)∞.  On  the  other  hand,  we  have 
(cid:12)h(x) − h(z)(cid:12)∞ ≤ 2(cid:12)x − z(cid:12)∞.  Putting  these  two  expressions  together,  we  obtain  that  (cid:12)h(x) − h( y)(cid:12)∞ ≤ max{2, 2n2(N −
1)δ}((cid:12)x − z(cid:12)∞ + (cid:12)z − y(cid:12)∞) = max{2, 2n2(N − 1)δ}(cid:12)x − y(cid:12)∞. Here we used the fact that z ∈ [x, y], which means that there 
exists t ∈ [0, 1] such that z = x + t( y − x) and thus

(cid:12)x − z(cid:12)∞ + (cid:12)z − y(cid:12)∞ = t(cid:12)x − y(cid:12)∞ + (1 − t)(cid:12)x − y(cid:12)∞ = (cid:12)x − y(cid:12)∞.

Step 3: Extending to [−1, 1]n+1. The ﬁnal step is to deﬁne a normalised nD-Borsuk-Ulam function F : [−1, 1]n+1 → [−1, 1]n
such that any x ∈ ∂([−1, 1]n+1) with (cid:12)F (x)(cid:12)∞ ≤ ε yields a solution to the nD-Tucker instance. For x ∈ [−1, 1]n+1 we write 
x = (x

(cid:3) ∈ [−1, 1]n. We deﬁne

(cid:3), xn+1), where x

(cid:3)
F (x) = F (x

, xn+1) = 1 + xn+1
(cid:3)) ∈ [−1, 1] and  F (x) is a convex combination of these two, it follows that  F (x) ∈ [−1, 1]n. Furthermore, 

) + 1 − xn+1

(cid:3)
(−h(−x

(cid:3)
h(x

)).

2

2

(cid:3)), −h(−x

Since h(x
we have  F (1, 1, . . . , 1) = h(1, 1, . . . , 1) = 1.  F is an odd function, since

(cid:3)
F (−x) = F (−x

, −xn+1) = 1 − xn+1

2

(cid:3)
h(−x

) + 1 + xn+1

2

(cid:3)
(−h(x

(cid:3)
)) = −F (x

, xn+1) = −F (x).

Consider  any  x = (x
(otherwise just use −x instead of x). If xn+1 = 1, then  F (x
the nD-Tucker instance. If xn+1 ∈ [0, 1), then x
in this case too.

(cid:3), xn+1) ∈ ∂([−1, 1]n+1) with  (cid:12)F (x)(cid:12)∞ ≤ ε.  Since  F is  an  odd  function,  we  can  assume  that  xn+1 ≥ 0
(cid:3))(cid:12)∞ ≤ ε, which yields a solution to 
(cid:3))
(cid:3), xn+1) = h(x

(cid:3)), and thus (cid:12)h(x
(cid:3)) = −h(−x

(cid:3) ∈ ∂([−1, 1]n) and thus h(x

(cid:3)). This implies that  F (x

(cid:3), xn+1) = h(x

Finally, let us determine the Lipschitz parameter of  F . Let x, y ∈ [−1, 1]n+1. We have

(cid:3)
(cid:12)F (x

, xn+1) − F ( y

(cid:3)

, xn+1)(cid:12)∞ ≤ 1 + xn+1

2

(cid:3)
(cid:12)h(x

) − h( y

(cid:3)

≤ max{2, 2n2(N − 1)δ}(cid:12)x

)(cid:12)∞ + 1 − xn+1
(cid:3)(cid:12)∞
(cid:3) − y

2

(cid:3)
(cid:12)h(−x

) − h(− y

(cid:3)

)(cid:12)∞

and also

(cid:3)

(cid:12)F ( y

, xn+1) − F ( y

(cid:3)

, yn+1)(cid:12)∞ ≤

|xn+1 − yn+1|
2

Putting these two expressions together, it follows that

(cid:3)

((cid:12)h( y

)(cid:12)∞ + (cid:12)h(− y

(cid:3)

)(cid:12)∞) ≤ |xn+1 − yn+1|.

(cid:12)F (x) − F ( y)(cid:12)∞ ≤ max{3, 2n2(N − 1)δ + 1}(cid:12)x − y(cid:12)∞.

Note that max{3, 2n2(N − 1)δ + 1} ≤ max{3, 4n2(N − 1)ε + 1}.

In the black-box model, in order to answer one query to  F , we have to answer two queries to h, i.e., two queries to  g. 
In order to answer a query to  g, we have to answer one query to  f , i.e., n + 1 queries to the labelling function (cid:8) (in order 
to  interpolate).  Thus,  one  query  to  F requires  2(n + 1) queries  to  (cid:8).  Since  n is  a  constant,  the  query  lower  bounds  from 
nD-Tucker carry over to normalised nD-Borsuk-Ulam.

In  the  white-box  model,  the  reduction  actually  gives  us  a  way  to  construct  an  arithmetic  circuit  that  computes  F ,  if 
we  are  given  a  Boolean  circuit  that  computes  (cid:8).  Indeed,  using  standard  techniques  [26,29],  the  execution  of  the  Boolean 
circuit  on  some  input  can  be  simulated  by  the  arithmetic  circuit.  Furthermore,  the  input  bits  for  the  Boolean  circuit  can 
be obtained by using the < gate. All the other operations that we used to construct  F can be computed by the arithmetic 
gates +, −, ×, max, min, < and rational constants. Thus, we obtain a polynomial-time many-one reduction from nD-Tucker
to normalised nD-Borsuk-Ulam for all n ≥ 1.

14

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Fig. 3. Visualisation of Algorithm 1. (a) depicts our initial assumptions. The red line shows where Agent 1 is indifferent. The blue signs on (0, p(cid:10)) and 
(p(cid:10), 1) show the (weak) preferences of Agent 2 under these pairs of cuts. (b) shows a possible position for the cuts xr − x(cid:8) ≤ ε
8L . The arrows show how the 
difference between the values of the positive piece and the negative piece change between the four possible combinations of pairs of cuts. (c) depicts the 
actual cuts on the cake: the green parts have label “+” and the yellow parts have label “−”.

5.  Monotone valuations

In this section, we present our results for agents with monotone valuations. In contrast to the results of Section 4, here 
we prove that for two agents with monotone valuations, the problem is solvable in polynomial time and with a polynomial 
number of queries, and in fact this result holds even if only one of the two agent has a monotone valuation and the other 
has a general valuation. For three or more agents however, the problem becomes PPA-complete once again, and we obtain 
a corresponding exponential lower bound on its query complexity.

5.1.  An eﬃcient algorithm for two monotone agents

We start with our eﬃcient algorithm for the case of two agents, which is a polynomial-time algorithm in the white-box 
model, as well as an algorithm of polynomial query complexity in the black-box model; see Algorithm 1. The algorithm is 
based on a nested binary search procedure. At the higher level, we are performing a binary search on the position of the left 
cut of a solution. At the lower level, for any ﬁxed position for the left cut, we perform another binary search in order to 
ﬁnd a right cut such that the pair of cuts forms a solution for the ﬁrst agent; as we have already seen this can be eﬃciently 
done if the agent has monotone valuation. Intuitively, we are moving on the “indifference curve” of the valuation function 
of the agent with the monotone valuation (see the red zig-zag line in Fig. 3) until we reach a solution. We decide how to 
move on this curve by checking the preferences of the second agent.

Before we proceed, we draw an interesting connection with Austin’s moving knife procedure [7], an Exact-Division proce-
dure  for  two  agents  with  general  valuations.  The  procedure  is  based  on  two  moving  knifes  which  one  of  the  two  agents 
simultaneously and continuously slides across the cake, maintaining that the corresponding cut positions ensure that she is 
satisﬁed with the partition. At some point during this process, the other agent becomes satisﬁed, which is guaranteed by 
the intermediate value theorem. Our algorithm can be interpreted as a discrete time implementation of this idea and quite 
interestingly, it results in a polynomial-time algorithm when one of the two agents has a monotone valuation, whereas it is 
computationally hard when both agents have general valuations, as shown in Section 4. On a more fundamental level, this 
demonstrates the intricacies of transforming moving-knife fair division protocols into discrete algorithms.

The main theorem of this section is the following.

Theorem 5.1. For two agents with monotone valuation functions, ε-Consensus-Halving is solvable in polynomial time and has query 
(cid:4)
log2 L
complexity O  
ε

, even in the weak black-box model. This result holds even if one of the two agents has a general valuation.

(cid:5)

Before  we  proceed  with  the  description  of  our  algorithm  and  its  analysis,  let  us  begin  with  some  conventions  that 
will  make  the  presentation  easier.  Since  we  have  to  make  two  cuts,  we  denote  x the  position  of  the  leftmost  cut  and  y
the  position  of  the  rightmost  cut.  So,  0 ≤ x ≤ y ≤ 1.  In  addition,  the  labels  of  the  corresponding  intervals  are  as  follows: 
intervals [0, x] and [ y, 1] have label “+”, forming the positive piece, and interval [x, y] has label “−”, forming the negative 
piece. Given a pair of cuts (x, y), we say that agent i:

• weakly prefers the positive piece, if v i([0, x] ∪ [ y, 1]) ≥ v i([x, y]) − ε/2;
• weakly prefers the negative piece, if v i([x, y]) ≥ v i([0, x] ∪ [ y, 1]) − ε/2;
• is indifferent if |v i([x, y]) − v i([0, x] ∪ [ y, 1])| ≤ ε/2.

In  addition,  let  p(cid:10) ∈ [0, 1] be  such  that  |v 1([0, p(cid:10)]) − v 1([p(cid:10), 1])| ≤ ε/2.  Note  that  we  can  eﬃciently  compute  p(cid:10) using 
Theorem 4.1.  The  ﬁnal  assumption  we  need  to  make  is  regarding  the  preferences  of  Agent  2  for  the  two  special  pairs 
of  (0, p(cid:10)) and  (p(cid:10), 1).  Observe  that  both  pairs  of  cuts  create  the  same  pieces  over  [0, 1] and  only  change  the  labels  of 
the pieces. Hence, if Agent 2 weakly prefers the positive piece under the pair of cuts (0, p(cid:10)), he has to weakly prefer the 
negative piece under the pair of cuts (p(cid:10), 1). In the description of the algorithm we will assume that this is indeed the case, 

15

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

i.e., he weakly prefers the positive piece under (0, p(cid:10)) and the negative piece under (p(cid:10), 1). (The other case can be handled 
analogously.) Using the above notation and assumptions we can now state Algorithm 1.

ALGORITHM 1: ε-Consensus-Halving for two agents with monotone valuations.
1 Set x(cid:8) ← 0 and xr ← p(cid:10)
2 Set y(cid:8) ← p(cid:10) and yr ← 1
3 while xr − x(cid:8) > ε
4

Find y ∈ [ y(cid:8), yr ] such that Agent 1 is indifferent under the pair of cuts ( x(cid:8)+xr
if Agent 2 weakly prefers the positive piece under ( x(cid:8)+xr
and y(cid:8) ← y

Set x(cid:8) ← x(cid:8)+xr

, y) then

8L do

, y)

2

2

else if Agent 2 weakly prefers the negative piece under ( x(cid:8)+xr

, y) then

2

Set xr ← x(cid:8)+xr

2

and yr ← y

end

10 end
11 while yr − y(cid:8) > ε
12

8L do

Find x ∈ [x(cid:8), xr ] such that Agent 1 is indifferent under the pair of cuts (x, y(cid:8)+ yr
if Agent 2 weakly prefers the positive piece under (x, y(cid:8) + yr
and x(cid:8) ← x

Set y(cid:8) ← y(cid:8)+ yr

) then

2

2

)

else if Agent 2 weakly prefers the negative piece under (x, y(cid:8) + yr

) then

2

Set yr ← y(cid:8)+ yr

and xr ← x

end

2

2

2

5
6

7
8
9

13
14

15
16
17

18 end
19 Output (x(cid:8), y(cid:8))

Proof of Theorem 5.1. To prove the correctness of Algorithm 1 we will prove that the following invariants are maintained 
through all iterations of the algorithm.

1. Agent 1 is indifferent under the pair of cuts (x(cid:8), y(cid:8)), and also under the pair of cuts (xr , yr).
2. Agent 2 weakly prefers the positive piece under the pair of cuts (x(cid:8), y(cid:8)), and weakly prefers the negative piece under 

the pair of cuts (xr, yr).

Assuming  that  Invariants  1  and  2  hold,  it  follows  that  the  algorithm  outputs  a  correct  solution.  Indeed,  Agent  1  is 
indifferent  under  the  pair  of  cuts  (x(cid:8), y(cid:8)) by  Invariant  1.  By  Invariant  2,  Agent  2  weakly  prefers  the  positive  piece  un-
der  the  pair  of  cuts  (x(cid:8), y(cid:8)),  i.e.,  v 2([0, x(cid:8)] ∪ [ y(cid:8), 1]) ≥ v 2([x(cid:8), y(cid:8)]) − ε/2 ≥ v 2([x(cid:8), y(cid:8)]) − ε.  Thus,  it  suﬃces  to  show  that 
v 2([x(cid:8), y(cid:8)]) ≥ v 2([0, x(cid:8)] ∪ [ y(cid:8), 1]) − ε.  By  Invariant  2,  Agent  2  weakly  prefers  the  negative  piece  under  the  pair  of  cuts 
(xr, yr), i.e., v 2([0, xr] ∪ [ yr, 1]) ≥ v 2([xr, yr]) − ε/2. Since |xr − x(cid:8)| ≤ ε/8L and | yr − y(cid:8)| ≤ ε/8L, by the L-Lipschitz continu-
ity of  v 2 it follows that |v 2([0, xr] ∪ [ yr, 1]) − v 2([0, x(cid:8)] ∪ [ y(cid:8), 1])| ≤ ε/4 and |v 2([xr, yr]) − v 2([x(cid:8), y(cid:8)])| ≤ ε/4. As a result, 
v 2([0, x(cid:8)] ∪ [ y(cid:8), 1]) ≥ v 2([x(cid:8), y(cid:8)]) − ε.

Next, we prove that Invariants 1 and 2 hold. First of all, note that they hold at the start of the algorithm by the choice of 
p(cid:10) and from the fact that Agent 2 weakly prefers the positive piece under (0, p(cid:10)) and the negative piece under (p(cid:10), 1). Both 
invariants are then automatically maintained by construction of the algorithm. We just have to argue that Steps 4 and 12 are 
possible, i.e., that such  y (respectively x) exists. Consider Step 4 ﬁrst. We apply the intermediate value theorem as follows: 
for the pair of cuts ( x(cid:8)+xr
, y), Agent 1 weakly prefers the positive piece when  y = y(cid:8), and weakly prefers the negative piece 
when  y = yr .  Thus,  by  continuity  of  the  valuation,  there  exists  y ∈ [ y(cid:8), yr] such  that  Agent  1  is  indifferent  between  the 
two pieces. For the ﬁrst point, note that Agent 1 is indifferent for the pair of cuts (x(cid:8), y(cid:8)) (by Invariant 1 in the previous 
iteration),  and  thus  weakly  prefers  the  positive  piece  for  the  pair  of  cuts  ( x(cid:8)+xr
, y(cid:8)) by  monotonicity,  since  the  positive 
piece has increased. For the second point, note that Agent 1 is indifferent for the pair of cuts (xr , yr) (again by Invariant 1 
in the previous iteration), and thus weakly prefers the negative piece for the pair of cuts ( x(cid:8)+xr
, yr) by monotonicity, since 
the negative piece has increased. The same argument also applies to Step 12. Finally, observe that we have not made use 
of the monotonicity of Agent 2’s valuation anywhere in our proof. Thus, the algorithm also works if Agent 2 has a general 
valuation.

2

2

2

In  order  to  bound  the  running  time  of  Algorithm 1,  we  need  to  bound  the  running  time  of:  the  number  of  iterations 
the algorithm needs in each while loop, and the time we need to perform Step 4 and Step 12. Firstly, observe that Steps 4
and 12 can be tackled using the algorithm from Theorem 4.1. This is because we can view the problem as a special case 
where we have one agent and we need to place a single cut in a speciﬁc subinterval where we know that a solution exists. 
Thus, each of these steps requires  O (log L
ε ) time. In addition, observe in every while loop we essentially perform a binary 
search.  Thus,  after  O (log L
8L ,  since  in  every  iteration  the  distance  between  xr and  x(cid:8)
decreases by a factor of 2. Similarly, after  O (log L
8L as well. Hence, every while loop 
requires  O (log2 L
ε ) time. Finally, observe that our algorithm just requires the 
preferences of Agent 2 for speciﬁc pairs of cuts which can be done via two evaluations of v 2.

ε ) time. So, Algorithm 1 terminates in  O (log2 L

ε ) iterations we get that | yr − y(cid:8)| ≤ ε

ε ) iterations  we  get  that  xr − x(cid:8) ≤ ε

16

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Next we argue that we can implement Algorithm 1 using O (log2 L

and 12 can be simulated with O (log L
every time we ask Agent 2 for his preferences we only need two evaluation queries. Since we need  O (log L
ε ) queries in total. (cid:2)
every while loop, Algorithm 1 needs  O (log2 L

ε ) queries in the black-box model. Observe that Steps 4
ε ) queries each, as we have already explained in Theorem 4.1. In addition, observe that 
ε ) iterations in 

5.2.  Results for three or more monotone agents

We  now  move  on  to  the  case  of  three  or  more  monotone  agents,  for  which  we  manage  to  show  that  the  problem 
becomes  computationally  hard  and  has  exponential  query  complexity.  Our  results  thus  show  a  clear  dichotomy  on  the 
complexity of ε-Consensus-Halving with monotone agents, between the case of two agents and the case of three or more 
agents.

Again we employ our general approach, but this time we need to prove computational and query-complexity hardness 
of the monotone nD-Borsuk-Ulam problem; the corresponding impossibility results for ε-Consensus-Halving with agents 
with  monotone  valuations  then  follow  from  our  property-preserving  reduction  (Proposition 3.1).  To  this  end,  we  in  fact 
construct  an  eﬃcient  black-box  reduction  from  (n − 1)D-Tucker to  monotone nD-Borsuk-Ulam,  i.e.,  we  reduce  from  the 
corresponding version of nD-Tucker of one lower dimension. In order to achieve this, we once again interpolate the (n −
1)D-Tucker instance  to  obtain  a  continuous  function,  but,  this  time,  we  embed  it  in  a  very  speciﬁc  lower  dimensional 
subset of the nD-Borsuk-Ulam domain. We then show that the function can be extended to a monotone function on the 
whole domain.

The “drop in dimension” which is featured in our reduction has the following effects:

- Since 1D-Tucker is solvable in polynomial-time, we can only obtain the PPA-hardness of monotone nD-Borsuk-Ulam

for n ≥ 3, and therefore the PPA-hardness of ε-Consensus-Halving for three or more monotone agents.

- The query complexity lower bounds that we “inherit” from (n − 1)D-Tucker do not exactly match our upper bounds, 

obtained via the reduction from ε-Consensus-Halving to nD-Tucker (Proposition 3.2).

The main technical contribution of this section is the following lemma, proved at the end of this section.

Lemma 5.2. For any constant n ≥ 2, (n − 1)D-Tucker reduces to normalised monotone nD-Borsuk-Ulam in polynomial time, via an 
eﬃcient black-box reduction.

Similarly to Section 4, we then obtain the following two theorems.

Theorem 5.3. Let n ≥ 3 be any constant. Then,

- ε-Consensus-Halving with n monotone agents is PPA-complete. This remains the case, even if (a) we ﬁx ε ∈ (0, 1), or (b) we ﬁx 

L ≥ 3(n + 1);

- for any constant t ∈ (0, 1), there exists a constant c > 0 such that for any ε ∈ (0, t) and any L ≥ 3(n + 1) with L/ε ≥ c, the query 

complexity of ε-Consensus-Halving with n monotone agents is between (cid:4)((L/ε)n−2) and O ((L/ε)n−1).

Theorem 5.4. Let n ≥ 3 be any constant. Then,

- monotone nD-Borsuk-Ulam is PPA-complete. This remains the case, even if (a) we ﬁx ε ∈ (0, 1), or (b) we ﬁx L ≥ 3;
- for any constant t ∈ (0, 1), there exists a constant c > 0 such that for any ε ∈ (0, t) and any L ≥ 3 with L/ε ≥ c, the query 

complexity of monotone nD-Borsuk-Ulam is between (cid:4)((L/ε)n−2) and O ((L/ε)n−1).

The proofs of the theorems follow from Theorems 2.1 and 2.2 and the following chain of reductions, which now crucially 

involve the monotone version of nD-Borsuk-Ulam and ε-Consensus-Halving.

(n − 1)D-Tucker ≤

monotone nD-Borsuk-Ulam ≤

Lem. 5.2

P rop. 3.1

monotone ε-Consensus-Halving ≤

P rop. 3.2

nD-Tucker

The  speciﬁc  parameters  that  appear  in  the  bounds  follow  from  the  proof  of  Lemma 5.2.  The  lower  bounds  again  hold 
even for the normalised versions of the problems. There is a small gap between our lower and upper bounds; we conjecture 
that it should be possible to prove upper bounds that match the lower bounds of Theorem 5.3, at least up to logarithmic 
factors, but we leave this for future work.

17

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

5.3.  Reducing (n − 1)D-Tucker to monotone nD-Borsuk-Ulam (Proof of Lemma 5.2)

Let n ≥ 2 be any ﬁxed constant. Consider an instance (cid:8) : [N]n−1 → {±1, ±2, . . . , ±(n − 1)} of (n − 1)D-Tucker. Let ε ∈
(0, 1).  We  will  construct  a  monotone  normalised  nD-Borsuk-Ulam function  F : [−1, 1]n+1 → [−1, 1]n that  is  Lipschitz-
continuous with Lipschitz parameter L = (4(n + 1)4 Nε + 2)/(min{2, 1/ε} − 1) and such that any x ∈ ∂(Bn+1) with (cid:12)F (x)(cid:12)∞ ≤
ε yields a solution to the (n − 1)D-Tucker instance.

Step 1: From (n − 1)D-TUCKER to non-normalised (n − 1)D-BORSUK-ULAM. Let δ = min{2ε, 1}. Note that δ ∈ (0, 1] and ε <
δ < 2ε. The ﬁrst step of the proof is very similar to the proof of Lemma 4.2. Namely, using Step 1 of the proof of Lemma 4.2, 
we  construct  g : [−1/2, 1/2]n−1 → [−δ, δ]n−1 such  that  g is  antipodally  anti-symmetric  and  4(n − 1)2(N − 1)ε-Lipschitz-
continuous.  Furthermore,  any  x ∈ [−1/2, 1/2]n−1 with  (cid:12)g(x)(cid:12)∞ < δ yields  a  solution  to  the  original  (n − 1)D-Tucker
instance.  Then,  we  deﬁne  h : [−1, 1]n−1 → [−δ, δ]n−1 by  h(x) = g(x/2).  h has  the  same  properties  as  g,  except  that  it  is 
2(n − 1)2(N − 1)ε-Lipschitz-continuous. Note that here the construction of h differs from Step 2 of the proof of Lemma 4.2, 
since we do not want h to be normalised.

Next,  we  extend  h to  a  (non-normalised)  (n − 1)D-Borsuk-Ulam function  G : [−1, 1]n → [−δ, δ]n−1 using  the  same 
construction as in Step 3 of the proof of Lemma 4.2. By the same arguments, it holds that  G is an odd function and it is 
Lipschitz-continuous  with  parameter  2(n − 1)2(N − 1)ε + δ ≤ 2n2 Nε.  Furthermore,  any  x ∈ ∂([−1, 1]n) with  (cid:12)G(x)(cid:12)∞ < δ
yields a solution to the original (n − 1)D-Tucker instance.

On a high-level, the rest of the reduction, which is the most interesting part, works by embedding G in an n-dimensional 

subspace of Rn+1 and then carefully extending it to all of [−1, 1]n+1 in a monotonic way. This n-dimensional subspace is

(cid:14)

D :=

x ∈ Rn+1

(cid:16)

xi = 0

.

(cid:15)
(cid:15)
(cid:15)
(cid:15)
(cid:15)

n+1(cid:10)

i=1

It has the nice property that for any x, y ∈ D, if x ≤ y, then x = y.

Step 2: Embedding into a function D → [−δ, δ]n−1. We  begin  by  deﬁning  a  slightly  modiﬁed  version  of  G.  The  function 
(cid:9)G : Rn → [−δ, δ]n−1 is deﬁned as follows

(cid:4)

(cid:9)G(x) = G

(cid:5)

(n + 1) · T 1
n+1

(x)

where  T r : [−1, 1]n → [−r, r]n denotes  truncation  to  [−r, r] in  every  coordinate.  It  is  easy  to  see  that (cid:9)G remains  an  odd 
function and that it is Lipschitz-continuous with parameter (n + 1) · 2n2 Nε ≤ 2(n + 1)3 Nε. Furthermore, it holds that any 
x ∈ Rn \ (− 1

n+1 )n with (cid:12)(cid:9)G(x)(cid:12)∞ < δ yields a solution to the (n − 1)D-Borsuk-Ulam instance G.

Next, we embed (cid:9)G into D. Let H : D → [−δ, δ]n−1 be deﬁned by H(x) = H(x

(cid:3)). Note that H remains an odd 
function and is also 2(n + 1)3 Nε-Lipschitz-continuous. Any x ∈ D with (cid:12)H(x)(cid:12)∞ < δ and such that there exists i ∈ [n] with 
|xi| ≥ 1/(n + 1), yields a solution to (cid:9)G and thus to the original (n − 1)D-Tucker instance.

(cid:3), xn+1) = (cid:9)G(x

n+1 ,  1

Step 3: Extending to a function [−1, 1]n+1 → Rn−1. In the next step, we extend H to a function F
(cid:3) : [−1, 1]n+1 → Rn−1. For 
n+1
x ∈ [−1, 1]n+1, we let S(x) =
i=1 xi ∈ [−(n + 1), n + 1] and (cid:11)(x) = x − (cid:20)x, 1n+1(cid:21) · 1n+1/(n + 1) = x − S(x) · 1n+1/(n + 1) ∈ D
denote the orthogonal projection onto D. Here (cid:20)·, ·(cid:21) denotes the scalar product in Rn+1, and 1n+1 ∈ Rn+1 is the all-ones 
vector.  F

is deﬁned as follows:

(cid:6)

(cid:3)

(cid:7)

(cid:3)

F

(x) =

1 −

(cid:8)

|S(x)|
n + 1

· H((cid:11)(x)) + C · S(x)
n + 1

· 1n−1

(cid:3)

is an odd function by using the fact that  S(−x) = −S(x), (cid:11)(−x) =

is  continuous.  Let  us  determine  an  upper  bound  on  its  Lipschitz  parameter.  For  any  x, y ∈

where C = 1 + 2(n + 1)4 Nε. It is easy to check that  F
−(cid:11)(x) and  H(−x) = −H(x).
(cid:3)
It  is  easy  to  see  that  F

[−1, 1]n+1 we have

(cid:3)

(cid:12)F

(x) − F

(cid:3)

( y)(cid:12)∞ ≤

(cid:15)
(cid:15)
(cid:15)|S(x)| − |S( y)|
(cid:15)
n + 1

(cid:12)H((cid:11)(x))(cid:12)∞ +

(cid:2)

(cid:3)
1 − |S( y)|/(n + 1)

(cid:12)H((cid:11)(x)) − H((cid:11)( y))(cid:12)∞

+ C|S(x) − S( y)|/(n + 1)
√

≤ (cid:12)x − y(cid:12)∞ + 2(n + 1)3 Nε
≤ (2(n + 1)4 Nε + C + 1)(cid:12)x − y(cid:12)∞

n + 1(cid:12)x − y(cid:12)∞ + C(cid:12)x − y(cid:12)∞

where we used |S(x) − S( y)| ≤ (n + 1)(cid:12)x − y(cid:12)∞ and (cid:12)(cid:11)(x) − (cid:11)( y)(cid:12)∞ ≤ (cid:12)x − y(cid:12)2 ≤
continuous with Lipschitz parameter 2(n + 1)4 Nε + C + 1 = 4(n + 1)4 Nε + 2.

18

√

n + 1(cid:12)x − y(cid:12)∞. Thus,  F

(cid:3)

is Lipschitz-

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Let  us  now  show  that  F

x ≤ y and  S(x) ≥ 0,  S( y) ≥ 0. Indeed, since  F
Finally, if  S(x) ≤ 0 and  S( y) ≥ 0, then there exists z with x ≤ z ≤ y and  S(z) = 0, which implies that  F

is  monotone.  For  this,  it  is  enough  to  show  that  F

(cid:3)( y) for  all  x, y ∈ [−1, 1]n+1 with 
(cid:3)(x) ≤ F
is odd, this implies that the statement also holds if  S(x) ≤ 0 and  S( y) ≤ 0. 

(cid:3)(x) ≤ F

(cid:3)(z) ≤ F

(cid:3)( y).

(cid:3)

(cid:3)

Let x ∈ [−1, 1]n+1 be such that  S(x) ≥ 0. For any  j ∈ [n + 1], i ∈ [n − 1] and t ≥ 0 with x j + t ≤ 1, it holds that

(cid:3)
i (x + t · e j) − F

(cid:3)
i (x)

F
= − t

n + 1

H i((cid:11)(x + t · e j)) +

(cid:2)

1 − S(x)/(n + 1)

(cid:3)(cid:2)

(cid:3)
H i((cid:11)(x + t · e j)) − H i((cid:11)(x))

+ tC
n + 1

≥ − t

n + 1

− 2(n + 1)3 Nε(cid:12)(cid:11)(x + t · e j) − (cid:11)(x)(cid:12)∞ + tC
n + 1

≥ (C − 1 − 2(n + 1)4 Nε) ·

t
n + 1

≥ 0

since C = 1 + 2(n + 1)4 Nε. Here we used the fact that (cid:12)(cid:11)(x + t · e j) − (cid:11)(x)(cid:12)∞ ≤ (cid:12)(cid:11)(x + t · e j) − (cid:11)(x)(cid:12)2 ≤ (cid:12)x + t · e j − x(cid:12)2 = t. 
is monotone, since for any x, y with x ≤ y and S(x) ≥ 0, S( y) ≥ 0, we can decompose  y = (. . . ((x + ( y1 −
We obtain that  F
x1) · e1) + ( y2 − x2) · e2) . . . ) + ( yn+1 − xn+1) · en+1.

(cid:3)

Step 4: Extending to a function [−1, 1]n+1 → [−1, 1]n. Deﬁne  F
(cid:3)(cid:3)(x) = Cn · S(x)/(n + 1),  where 
Cn := (4(n + 1)4 Nε + 2)/(min{2, 1/ε} − 1). Clearly, F
is an odd function, monotone and Cn-Lipschitz-continuous. Finally, we 
deﬁne  F : [−1, 1]n+1 → [−1, 1]n by  F (x) = T 1((F
(cid:3)(cid:3)(x))). It is easy to check that  F remains monotone and odd, because 
are monotone and odd. Furthermore, since C ≥ 1 and Cn ≥ 1, we have that F (1, 1, . . . , 1) = (1, 1, . . . , 1). Thus, 
T 1, F
F is a monotone normalised nD-Borsuk-Ulam function with Lipschitz parameter max{4(n + 1)4 Nε + 2, Cn} = (4(n + 1)4 Nε +
2)/(min{2, 1/ε} − 1).

(cid:3)(cid:3) : [−1, 1]n+1 → R by  F

(cid:3)(cid:3)
(cid:3)(x), F

and F

(cid:3)(cid:3)

(cid:3)

Consider  any  x ∈ ∂([−1, 1]n+1) with  (cid:12)F (x)(cid:12)∞ ≤ ε.  Since  |F
there exists i ∈ [n − 1] such that |H i((cid:11)(x))| ≥ δ. Then, we have

(cid:3)(cid:3)(x)| ≤ ε,  it  follows  that  |S(x)| ≤ (n + 1)ε/Cn.  Assume  that 

|F i(x)| ≥ (1 − ε/Cn)δ − Cε/Cn ≥ δ − (C + 1)ε/Cn > δ − (min{2, 1/ε} − 1)ε ≥ δ − (δ − ε) ≥ ε

where we used  Cn > (C + 1)/(min{2, 1/ε} − 1). But this would mean that (cid:12)F (x)(cid:12)∞ > ε, a contradiction. Thus, it must be 
that (cid:12)H((cid:11)(x))(cid:12)∞ < δ.

In order to show that (cid:11)(x) is a solution to H , it remains to prove that there exists i ∈ [n] such that |[(cid:11)(x)]i| ≥ 1/(n + 1). 
Since  x ∈ ∂([−1, 1]n+1),  there  exists  j ∈ [n + 1] such  that  |x j| = 1.  As  a  result,  |[(cid:11)(x)] j| = |x j − S(x)/(n + 1)| ≥ 1 − ε/Cn. 
If  j < n + 1,  then  let  i := j.  Otherwise,  if  j = n + 1,  then,  since  S((cid:11)(x)) = 0,  there  necessarily  exists  i ∈ [n] such  that 
|[(cid:11)(x)]i| ≥ 1/n − ε/(nCn). In both cases we have found  i ∈ [n] such that |[(cid:11)(x)]i| ≥ 1/n − ε/(nCn). Since  Cn ≥ (n + 1)ε, it 
follows that |[(cid:11)(x)]i| ≥ 1/(n + 1).

Thus, from any x ∈ ∂([−1, 1]n+1) with (cid:12)F (x)(cid:12)∞ ≤ ε, we can obtain a solution to the original (n − 1)D-Tucker instance. 
Note that the reduction is polynomial-time and we have only used operations allowed by the gates of the arithmetic circuit. 
In  particular,  we  have  only  used  division  by  constants,  which  can  be  performed  by  multiplying  by  the  inverse  of  that 
constant.

The reduction is black-box and any query to  F can be answered by at most 2n queries to the labelling function (cid:8) of the 

original (n − 1)D-Tucker instance. Note that this is a constant, since n is constant.

6.  Relations to the Robertson-Webb query model

The black-box query model that we used in the previous sections is the standard model used in the related literature 
of query complexity, where the nature of the input functions depends on the speciﬁc problems at hand. For example, for 
nD-Borsuk-Ulam the function  F inputs points on the domain and returns other points, whereas in nD-Tucker the function 
(cid:8) inputs points and outputs their labels.

At the same time, in the literature of the cake-cutting problem, the predominant query model is in fact a more expressive 
query model, known as the Robertson-Webb model (RW) [61,67]. The RW model has been deﬁned only for the case of additive 
valuations, and consists of the following two types of queries:

- eval queries, where the agent is given an interval [a, b] and she returns her value for that interval, and
- cut queries, where the agent is given a point x ∈ [0, 1] and a real number α, and they designate the smallest interval 

[x, y], for which their value is exactly α.

In fact, in the literature of envy-free cake-cutting, the query complexity in the RW model has been one of the most important 
open problems [20,59], with breakthrough results coming from the literature of computer science fairly recently [8,9]. Since 

19

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

Fig. 4. Visualisation of cut and eval queries. (a) The input A to an eval query is denoted by the green intervals. (b) The inputs A1 and A2 to the cut query 
are denoted by the green and yellow intervals respectively, and the interval I = [a, b] is denoted in blue. The agent places a cut (if possible) at a position 
x ∈ I such that her value for A1 ∪ [a, x] and her value for A2 ∪ [x, b] are in a speciﬁed proportion.

ε-Consensus-Halving and  ε-fair cake-cutting  [21] are  conceptually  closely  related,  it  would  make  sense  to  consider  the 
query complexity of the former problem in the RW model as well.7

A potential hurdle in this investigation is that the RW model has not been deﬁned for valuation functions beyond the 
additive  case.  To  this  end,  we  propose  the  following  generalisation  of  the  RW  model  that  we  call  Generalised Robertson-
Webb model (GRW)), which is appropriate for monotone valuation functions that are not necessarily additive. Intuitively, in 
the  GRW  model  the  agent  essentially  is  given  sets of intervals A rather  than  single  intervals,  and  the  queries  are  deﬁned 
accordingly (see also Fig. 4).

Deﬁnition 6 (Generalised  Robertson-Webb  (GRW)  Query  Model). In  the  GRW  query  model,  there  are  two  types  of 
queries:

- eval queries, where agent  i is given any Lebesgue-measurable subset  A of [0, 1] and she returns her value  v i( A)

for that set, and

- cut queries, where agent i is given two disjoint Lebesgue-measurable subsets  A1 and  A2 of [0, 1], an interval  I =
= γ , 

[a, b], disjoint from  A1 and  A2, and a real number γ ≥ 0, and she designates some x ∈ I such that  v i ( A1∪[a,x])
v i ( A2∪([x,b])
if such a point exists.

Let  us  discuss  why  this  model  is  the  most  appropriate  generalisation  of  the  RW  model.  First,  the  deﬁnition  of eval
queries is in fact the natural extension, as the agent needs to specify her value for sets of intervals; note that in the additive 
case, it suﬃces to elicit an agent’s value for only single intervals, as her value for unions of intervals is then simply the sum 
of  the  elicited  values.  This  is  not  the  case  in  general  for  monotone  valuations,  and  therefore  we  need  a  more  expressive
eval query.  We  also  remark  that  the eval query  is  exactly  the  same  as  a  query  in  the  black-box  model,  as  deﬁned  in 
Section 2, and therefore the GRW model is stronger than the black-box query model. Brânzei and Nisan [21] in fact studied 
the restriction of the RW model (for the cake-cutting problem and for additive valuations), for which only eval queries are 
allowed, and they coined this the R W
query model. To put our results into context, we offer the following deﬁnition of the 
query model, which is, as discussed, equivalent to the black-box query model of Section 2. By the discussion above, 
G R W
all of our query complexity bounds in Section 4 and Section 5 apply verbatim to the GRW

query model.

−

−

−

Deﬁnition 7 (Generalised  Robertson-Webb
lowed; there agent i is given a Lebesgue-measurable subset  A of [0, 1] and she returns her value v i( A) for that set.

)  query  model). In  the  GRW

query  model,  only eval queries  are  al-

(GRW

−

−

−

While the extension of eval queries from the RW model to the GRW model is relatively straightforward, the generali-
sation of cut queries is somewhat more intricate. Upon closer inspection of a cut query in the (standard) RW model for 
additive valuations, it is clear that one can equivalently deﬁne this query as

Given an interval I = [a, b] and a real number γ , place a cut at x ∈ [a, b] such that  v i ([a,x])
v i ([x,b])

= γ .

This is because one can easily ﬁnd the value of the agent for [a, b] with one eval query, and then for any value of α used 
in the standard deﬁnition of a cut query, there is an appropriate value of γ in the modiﬁed deﬁnition above, which will 
result in exactly the same position x of the cut and vice-versa.

The simplicity of the cut queries in the RW model is enabled by the fact that for additive valuations, the value of any 
agent  i for an interval  I does not depend on how the remainder of the interval [0, 1] has been cut. This is no longer the 
case for monotone valuations, as now the agent needs to specify a different value for sets of intervals. We believe that our 
deﬁnition of the cut query in the GRW model is the appropriate generalisation, which captures the essence of the original 
cut  queries  in  RW,  but  also  allows  for  enough  expressiveness  to  make  this  type  of  query  useful  for  monotone  valuations 
beyond the additive case.

Finally, we remark that for general valuations (beyond monotone), any sensible deﬁnition of cut queries seems to be too 
strong, in the sense that it conveys unrealistically too much information (in contrast to the RW and GRW models, where 

7 The ε-Consensus-Halving halving problem has in fact recently been studied under this query model as well, but in a somewhat different direction, 
and for agents with additive valuations [4]. Note that the authors do not refer to their query model as the RW model, but the queries that they use are 
essentially RW queries.

20

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

the cut queries are intuitively “shortcuts” for binary search). For example, assume that the agent is asked to place a cut at 
some point  x in an interval [a, b], for which (a) if the cut is placed at a the agent “sees” an excess of “+” and (b) if the 
cut is placed at b, the agent still “sees” an excess of “+”. By the boundary conditions of the interval, there is no guarantee 
that a cut that “satisﬁes” the agent exists within that interval, and we would need to exhaustively search through the whole 
interval  to  ﬁnd  such  a  cut  position,  if  it  exists,  meaning  that  binary  search  does  not  help  us  here.  On  the  other  hand,  a 
single cut query would either ﬁnd the position or return that there is no such x within the interval.

We are now ready to state our results for the section, which we summarise in the following theorem. Qualitatively, we 
prove that ε-Consensus-Halving with three normalised monotone agents still has an exponential query complexity in the 
GRW  model  (with  logarithmic  savings  compared  to  the  black-box  model),  whereas  for  two  normalised  monotone  agents, 
the problem becomes “easier” by a logarithmic factor.

Theorem 6.1. In the Generalised Robertson-Webb model:

(cid:4)
- ε-Consensus-Halving with n ≥ 3 normalised monotone agents requires (cid:4) 
queries.
- ε-Consensus-Halving with n = 2 monotone agents can be solved with O (log(L/ε)) queries.

(L/ε)n−2
log(L/ε)

(cid:5)

2

2

v i ( A2)

v i ( A1)
v i ( A2∪[a,b])

Proof. The upper bound for n = 2 can be obtained relatively easily, by observing that in the proof of Theorem 5.1, Step 4 and 
Step 12 of Algorithm 1 were obtained via binary search, using  O (log(L/ε)) queries, which resulted in a query complexity 
of  O (log2(L/ε)),  since  these  steps  were  executed  O (log(L/ε)) times.  In  the  GRW  model,  we  can  simply  replace  each  of 
those binary searches by a single cut query (as these only apply to the monotone agents) and obtain a query complexity of 
O (log(L/ε)). For example, Step 4 can be simulated by a cut query where γ = 1,  A1 = [ x(cid:8)+xr
] ∪ [ yr, 1], 
and  I = [ y(cid:8), yr].

, y(cid:8)],  A2 = [0, x(cid:8)+xr

∗) = γ and  that  φ can  be  evaluated  by  using  two eval queries.  Furthermore,  since  v i

For the lower bound when n ≥ 3, we will show how to construct an instance of ε-Consensus-Halving with n normalised 
monotone agents, such that the (cid:4)((L/ε)n−2) lower bound for eval queries still holds, but we can additionally answer any
cut query  by  performing  at  most  O (log(L/ε)) eval queries.  We  will  again  use  our  reduction  from  normalised  monotone 
nD-Borsuk-Ulam to ε-Consensus-Halving with n normalised monotone agents (Proposition 3.1).
Given a cut query ( A1, A2, I = [a, b], γ ), we deﬁne φ : [a, b] → R≥0 by φ(t) = v i ( A1∪[a,t])

v i ( A2∪[t,b]) . Note that we are looking for 
∗ ∈ I such  that  φ(t
is  monotone, 
t
≥ γ .  If  one  of  these  two  conditions  does 
φ is  non-decreasing.  We  begin  by  checking  that 
not  hold,  then  we  can  immediately  answer  that  there  is  no  t ∈ I that  satisﬁes  the  query.  In  what  follows,  we  assume 
that these two conditions hold.  In that case, we can query  φ(t) for some t ∈ I to determine whether the solution t
lies 
∗ ∈ J .  At  the  beginning,  we  have 
in  [a, t] or  in  [t, b].  We  denote  by  J ⊆ I the  current  interval  for  which  we  know  that  t
J := I .  Using  at  most  (cid:17)log2(n + 1)(cid:18) + 2 queries  to  φ we  can  shrink  J such  that  J ⊆ R j for  some  j ∈ [n + 1].  Recall  that 
[x( A)]i = 2(n + 1) · λ( A ∩ R i) − 1 for  any  i ∈ [n + 1] and  A ∈ (cid:5)([0, 1]).  It  follows  that  for  any  t ∈ J ,  [x( A1 ∪ [a, t])]i and 
[x( A2 ∪ [t, b])]i are ﬁxed for all  i ∈ [n + 1] \ { j}. Furthermore, with an additional (cid:17)log2 m(cid:18) + 2 queries, we can ensure that 
for  all  t ∈ J ,  [x( A1 ∪ [a, t])] j ∈ [k/m, (k + 1)/m] and  [x( A2 ∪ [t, b])] j ∈ [(cid:8)/m, ((cid:8) + 1)/m] for  some  k, (cid:8) ∈ Z.  Next,  with  an 
additional 2(cid:17)log2(n + 1)(cid:18) queries, we can shrink  J , so that for all t ∈ J , x( A1 ∪ [a, t]) and x( A2 ∪ [t, b]) each lie in some ﬁxed 
simplex of Kuhn’s triangulation of the domain K n+1
(deﬁned below). In that case, by our construction below, it will hold that 
v i(x( A1 ∪ [a, t])) and v i(x( A2 ∪ [t, b])) can be expressed as an aﬃne function of t ∈ J and thus we can exactly determine the 
. In order for this to hold, we will ensure that our normalised monotone nD-Borsuk-Ulam function is piecewise 
value of t
linear.  Furthermore,  we  will  pick m = (cid:17)2nL/ε(cid:18),  and  thus  we  have  used  2(3(cid:17)log2(n + 1)(cid:18) + 2 + (cid:17)log2((cid:17)2nL/ε(cid:18))(cid:18) + 2) eval
queries to answer one cut query. Note that this expression is  O (log(L/ε)), since n is constant.

≤ γ and  v i ( A1∪[a,b])

Consider  a  normalised  monotone  nD-Borsuk-Ulam function  F : [−1, 1]n+1 → [−1, 1]n with  Lipschitz  parameter  L ≥ 3
and  some  ε ∈ (0, 1).  We  ﬁrst  discretize  the  domain  to  be  K n+1
:= {−1, −(m − 1)/m, . . . , −1/m, 0, 1/m, 2/m, . . . , (m −
1)/m, 1}n+1 where  m = (cid:17)2nL/ε(cid:18).  We  let  f : K n+1
is  antipodally  anti-
symmetric  ( f (−x) = − f (x) for  all  x ∈ K n+1
m ),  monotone  ( f (x) ≤ f ( y) whenever  x ≤ y)  and  f (1, 1, . . . , 1) = (1, 1, . . . , 1). 
Furthermore, any x ∈ ∂(K n+1
m ) with (cid:12) f (x)(cid:12)∞ ≤ ε yields a solution to the original instance  F . We extend  f back to a func-
tion  (cid:9)f : [−1, 1]n+1 → [−1, 1]n by  using  Kuhn’s  triangulation  on  the  grid  K n+1
and  interpolating  (see  Appendix C for  a 
description of the triangulation and interpolation). By the arguments presented in Appendix C, it holds that (cid:9)f
is a contin-
uous, monotone, odd function, and (cid:9)f (1, 1, . . . , 1) = (1, 1, . . . , 1). Furthermore, if xi is ﬁxed for all i ∈ [n + 1] \ { j} and x j is 
constrained such that x lies in some ﬁxed simplex σ of Kuhn’s triangulation, then (cid:9)f (x) can be expressed as a linear aﬃne 
function of x j .

→ [−1, 1]n be  deﬁned  by  f (x) = F (x).  Note  that  f

Let us now determine the Lipschitz parameter of (cid:9)f . Consider any simplex σ = { y0,  y1, . . . ,  yn+1} of the Kuhn triangula-
m . Consider any x ∈ [0, 1]n+1 that lies in σ and any  j ∈ [n + 1] and t ∈ [−1/m, 1/m] such that x + t · e j also lies 

tion of  K n+1
in σ . Then, the interpolation (as deﬁned in Appendix C) yields

m

m

m

m

∗

∗

(cid:12)(cid:9)f (x) − (cid:9)f (x + t · e j)(cid:12)∞ = (cid:12)t · m · f ( y j) − t · m · f ( y j−1)(cid:12)∞ ≤ |t| · m · (cid:12)F ( y j) − F ( y j−1)(cid:12)∞

≤ |t| · m · L · (cid:12) y j − y j−1(cid:12)∞
≤ L · |t|.

21

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

It  is  easy  to  check  that  this  implies  that (cid:9)f
follows that (cid:9)f

is (n + 1)L-Lipschitz-continuous on [−1, 1]n+1 (see e.g., the proof of Lemma 4.2).

is  (n + 1)L-Lipschitz-continuous  on  the  simplex  σ .  By  a  simple  argument,  it 

Now  consider  any  simplex  σ = { y0, y1, . . . , yn+1} such  that  there  exists  i ∈ [n + 1] ∪ {0} with  (cid:12) f ( yi)(cid:12)∞ > ε.  Since 

(cid:9)f ( yi) = f ( yi), and (cid:9)f

is (n + 1)L-Lipschitz-continuous, it follows that for any x ∈ [−1, 1]n+1 that lies in σ we have

(cid:12)(cid:9)f (x)(cid:12)∞ ≥ (cid:12)(cid:9)f ( yi)(cid:12)∞ − nL(cid:12)x − y0(cid:12)∞ > ε − nL/m ≥ ε/2

where we used m ≥ 2nL/ε. Thus, for any x ∈ ∂([−1, 1]n+1) with (cid:12)(cid:9)f (x)(cid:12)∞ ≤ ε/2, it must hold that both (cid:12) f ( y0)(cid:12)∞ ≤ ε and 
(cid:12) f ( yn+1)(cid:12)∞ ≤ ε, where σ = { y0, y1, . . . , yn+1} is the Kuhn simplex containing x. However, since x ∈ ∂([−1, 1]n+1), it follows 
that  y0 or  yn+1 lies on the boundary of K n+1
m . This means that we have obtained a solution to the original nD-Borsuk-Ulam
function  F .

Since  the  parameters  for (cid:9)f are  L

(cid:3) = nL and  ε(cid:3) = ε/2,  and  n is  constant,  the  query  lower  bound  for  F carries  over  to 

(cid:9)f . (cid:2)

7.  Conclusion and future directions

In this paper, we managed to completely settle the computational complexity of the ε-Consensus-Halving problem for a 
constant number of agents with either general or monotone valuation functions. We also studied the query complexity of the 
problem and we provided exponential lower bounds corresponding to our hardness results, and polynomial upper bounds 
corresponding  to  our  polynomial-time  algorithms.  We  also  deﬁned  an  appropriate  generalisation  of  the  Robertson-Webb 
query model for monotone valuations and we showed that our bounds are qualitatively robust to the added expressiveness 
of this model. The main open problem associated with our work is the following.

What  is  the  computational  complexity  and  the  query  complexity  of  ε-Consensus-Halving with  a  constant number  of 
agents and additive valuations?

Our approach in this paper prescribes a formula for answering this question: One can construct a black-box reduction to 
this version of ε-Consensus-Halving from a computationally-hard problem like nD-Tucker, for which we also know query 
complexity lower bounds, and obtain answers to both questions at the same time. Alternatively, one might be able to con-
struct polynomial-time algorithms for solving this problem; concretely, attempting to do that for three agents with additive 
valuations would be the natural starting step, as this is the ﬁrst case for which the problem becomes computationally hard 
for  agents  with  monotone  valuations.  It  is  unclear  whether  one  should  expect  the  problem  to  remain  hard  for  additive 
valuations.

Another line of work would be to study the query complexity of the related fair cake-cutting problem using the GRW 
model  that  we  propose.  In  fact,  while  the  fundamental  existence  results  for  the  problem  (e.g.,  see  [65])  actually  apply  to 
quite general valuation functions, most of the work in computer science has restricted its attention to the case of additive 
valuations only, with a few notable exceptions [24,34]. We believe that our work can therefore spark some interest in the 
study of the query complexity of fair cake-cutting with valuations beyond the additive case.

Declaration of competing interest

The  authors  declare  that  they  have  no  known  competing  ﬁnancial  interests  or  personal  relationships  that  could  have 

appeared to inﬂuence the work reported in this paper.

Acknowledgements

Alexandros Hollender was supported by an EPSRC doctoral studentship (Reference 1892947).

Appendix A.  ε-CONSENSUS-HALVING for piecewise constant valuations

As we mentioned in the introduction, the valuation functions studied in previous work [38,39] are piecewise constant 
valuations (i.e., step functions) which are given explicitly as part of the input, with each agent specifying the end-points and 
the height of each step. For a constant number of agents n, this case is solvable in polynomial-time, as illustrated in Fig. 1.

Lemma A.1. ε-Consensus-Halving with a constant number n of agents and piecewise constant valuations (given explicitly as part of 
the input) is solvable in polynomial time.

Proof. The proof follows closely that of Lemma 15 in [41]; here we provide the main proof idea, and we refer the reader 
to that paper for a fully detailed proof.

First,  we  partition  the  interval  [0, 1] into  regions,  via  a  set  of  points  t1, . . . , tm such  that  the  density  of  the  valuation 
function of each agent is constant within each region. Let T = [t j, t j+1] denote an arbitrary region, and let v i(T ) denote the 

22

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

constant value of the density of agent i in region T (i.e., the height of the step). Since the valuations are provided explicitly 
(or  equivalently,  are  polynomial  in  the  other  input  parameters),  this  process  will  result  in  a  polynomial  number  of  such 
regions.

We will be interested in the regions that will contain cuts, and then we will ﬁnd the appropriate positions of those cuts 
using linear programming. First, it is not hard to see that a solution in which some region contains more than 1 cut can be 
transformed into a solution in which every region contains at most 1 cut, by appropriately “merging” and “shifting” sets of 
cuts within the region. Therefore, we can assume that each region either contains a cut or it doesn’t. For any k = 1, . . . , n, 
we can consider all the possible ways of distributing the k cuts to the regions, such that no region receives more than one 
cut; since n is a constant, this can be done in polynomial time.

Let  xT be  the  position  of  the  cut  in  region  T = [ti, ti+1].  This  means  that  for  agent  i,  the  “left  sub-region”  [ti, xT ]
receives one label (say “+”), whereas the “right sub-region” [xT , ti+1] receives the other label (say “−”), resulting in values 
v i(T ) · [ti, xT ] and v i(T ) · [xT , ti+1] for the agent respectively. If there is not cut in region T , then the whole interval receives 
the  same  label.  Now,  we  can  consider  the  set  of  cuts  at  positions  xT in  each  of  the  regions  that  contain  cuts,  and  the 
corresponding partitioning of [0, 1] into intervals, labelled “+” and “−”; without loss of generality we can assume that this 
labelling is alternating. From there, we can devise a linear program for each value of k, where we aim to minimise z, subject 
to |v i(I+) − v i(I−)| ≤ z (which can be transformed into a set of linear constraints), plus additional linear constraints for 
the  positions  of  the  cuts.  Since  a  solution  always  exists,  and  since  we  consider  all  values  of  k ∈ [1, n],  one  of  the  linear 
programs will terminate with z = 0, and therefore we can ﬁnd an exact solution to the problem. (cid:2)

Appendix B.  Formal deﬁnition of the input model

In  Section 2,  we  mentioned  that  in  the  white-box  model,  the  agents’  valuations  v i are  accessed  via  polynomial-time 
algorithms,  which  are  given  explicitly  as  part  of  the  input.  We  provide  the  precise  deﬁnition  of  the  input  model  in  this 
section. Formally, we assume that valuations are computed by Turing machines. The input to the Turing machine is a list of 
intervals, and the Turing machine outputs the value of the union of these intervals.

Deﬁnition 8 (ε-Consensus-Halving (white-box model), formal deﬁnition). For any constant n ≥ 1, and polynomial  p, the 
problem ε-Consensus-Halving with n agents is deﬁned as follows:

• Input: ε > 0, the Lipschitz parameter L, Turing machines v 1, . . . , vn
• Output: Any of the following:

– A  partition  of  [0, 1] into  two  sets  of  intervals  I+

and  I−

v i(I−)| ≤ ε, using at most n cuts.

– A violation of L-Lipschitz-continuity for one of the valuations.
– An input  X on which one of the Turing machines does not terminate in at most  p(| X|) steps.
– (Optional, for monotone valuations only). A violation of monotonicity for one of the valuations.

such  that  for  each  agent  i,  it  holds  that  |v i(I+) −

Note that for all solution types except the ﬁrst one, a solution can be a union of an arbitrary number of intervals, i.e., 
not necessarily obtained using at most n cuts. The violation solutions are only there to ensure that the problem is contained 
in TFNP. They are irrelevant for our hardness results, which also hold for the promise version of the problem where we are 
promised that the input does not violate any of the conditions.

Appendix C.  The Kuhn triangulation

Let  Dm := {0, 1/m, 2/m, . . . , m/m}. Kuhn’s triangulation is a standard way to triangulate a grid  Dn

m. Every x ∈ (Dm \ {1})n
is the base of the cube containing all vertices { y : yi ∈ {xi, xi + 1/m}}. Every such cube is subdivided into n! n-dimensional 
simplices as follows: for every permutation π of [n], σ = { y0, y1, . . . , yn} is a simplex, where  y0 = x and  yi = yi−1 + 1
m eπ (i)
for all i ∈ [n] (where ei is the ith unit vector).

It is easy to see that Kuhn’s triangulation has the following properties:

• For any simplex σ = {z1, . . . , zk} it holds that (cid:12)zi − z j(cid:12)∞ ≤ 1/m for all i, j, and there exists a permutation π of [k] such 

that zπ (1) ≤ · · · ≤ zπ (k) (component-wise).

• Given a point  x ∈ [0, 1]n, we can eﬃciently determine the simplex that contains it as follows. First ﬁnd the base  y of 
m that contains x. Next, ﬁnd a permutation π such that xπ (1) − yπ (1) ≥ · · · ≥ xπ (n) − yπ (n). Then, it follows 

a cube of  Dn
that ( y, π ) is the simplex containing x.

• The triangulation is antipodally anti-symmetric: if σ = { y0, y1, . . . , yn} is a Kuhn simplex of  Dn

m, then σ = { y0, . . . , yn}

is also a simplex, where  yi

= 1 − yi

j for all i, j.

j

Using  Kuhn’s  triangulation,  a  function  f
[0, 1]n → [−M, M].  (cid:9)f

:
is  constructed  in  each  Kuhn  simplex  σ = { y0, y1, . . . , yn} by  interpolating  over  the  values 

→ [−M, M] can  be  extended  to  a  Lipschitz-continuous  function  (cid:9)f

: Dn
m

23

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

{ f ( y0), f ( y1), . . . , f ( yn)}. In more detail, for any x ∈ [0, 1]n that lies in simplex σ = { y0, y1, . . . , yn}, we let z := m · (x − y0). 
Let π denote the permutation used to obtain σ . Note that since x lies in σ , it must be that zπ (1) ≥ zπ (2) ≥ · · · ≥ zπ (n). Then, 
it holds that x =

i=0 αi yi , where α0 = 1 − zπ (1), αn = zπ (n), and αi = zπ (i) − zπ (i+1) for i ∈ [n − 1]. We deﬁne

(cid:6)
n

(cid:9)f (x) :=

n(cid:10)

i=0

αi f ( yi).

It is easy to check that the function (cid:9)f : [0, 1]n → [−M, M] thus obtained is continuous. Indeed, if x lies on a common face 
of two Kuhn simplices, then the value (cid:9)f (x) obtained by interpolating in either simplex is the same. It can be shown that 
(cid:9)f
is antipodally 
anti-symmetric, i.e.,  f (x) = − f (x) for all x ∈ Dn

is Lipschitz-continuous with Lipschitz parameter 2M · n · m with respect to the (cid:8)∞-norm. Furthermore, if  f

m, then so is (cid:9)f , i.e., (cid:9)f (x) = −(cid:9)f (x) for all x ∈ [0, 1]n.

is monotone, i.e.,  f (x) ≤ f ( y) for all x, y ∈ Dn

m with x ≤ y, then so is (cid:9)f , i.e., (cid:9)f (x) ≤ (cid:9)f ( y) for all x, y ∈ [0, 1]n
with x ≤ y. Consider any point x ∈ [0, 1]n that lies in some simplex σ = { y0, y1, . . . , yn}. Then for any  j ∈ [n] and t ≥ 0 such 
that x + t · e j lies in the simplex σ , we have

Finally, if  f

(cid:9)f (x + t · e j) − (cid:9)f (x) = tmf ( y j) − tmf ( y j−1) ≥ 0

is monotone. Using this it is easy to show that (cid:9)f

since  y j ≥ y j−1 and  f
is monotone within any simplex σ , since for any 
x ≤ y in σ we can construct a path that goes from  x to  y (and lies in σ ) that only uses the positive cardinal directions. 
Since monotonicity holds for any segment of the path, it also holds for x and  y. Finally, for any x ≤ y that lie in different 
simplices, we can just use the straight path that goes from x to  y, and the fact that (cid:9)f is monotone in each simplex that we 
traverse.

References

gence, 2014.

[1] James Aisenberg, Maria Luisa Bonet, Sam Buss, 2-D Tucker is PPA complete, J. Comput. Syst. Sci. 108 (2020) 92–103, https://doi .org /10 .1016 /j .jcss .2019 .

[2] Reza Alijani, Majid Farhadi, Mohammad Ghodsi, Masoud Seddighin, Ahmad Tajik, Envy-free mechanisms with minimum number of cuts, in: Proceedings 

of the AAAI Conference on Artiﬁcial Intelligence, vol. 31, 2017.

[3] Alon Noga, Splitting necklaces, Adv. Math. 63 (3) (1987) 247–253, https://doi .org /10 .1016 /0001 -8708(87 )90055 -7.
[4] Alon Noga, Andrei Graur, Eﬃcient splitting of necklaces, in: Proceedings of the 48th International Colloquium on Automata, Languages, and Program-

ming (ICALP), 2021, pp. 14:1–14:17, https://doi .org /10 .4230 /LIPIcs .ICALP.2021.14.

[5] Alon Noga, Douglas B. West, The Borsuk-Ulam theorem and bisection of necklaces, Proc. Am. Math. Soc. 98 (4) (1986) 623–628, https://doi .org /10 .

09 .002.

2307 /2045739.

[6] Georgios Amanatidis, George Christodoulou, John Fearnley, Evangelos Markakis, Christos-Alexandros Psomas, Eftychia Vakaliou, An improved envy-free 
cake cutting protocol for four agents, in: Proceedings of the 11th International Symposium on Algorithmic Game Theory (SAGT), 2018, pp. 87–99, 
https://doi .org /10 .1007 /978 -3 -319 -99660 -8 _9.

[7] A.K. Austin, Sharing a cake, Math. Gaz. 66 (437) (1982) 212–215, https://doi .org /10 .2307 /3616548.
[8] Haris Aziz, Simon Mackenzie, A discrete and bounded envy-free cake cutting protocol for any number of agents, in: Proceedings of the 57th Annual 

IEEE Symposium on Foundations of Computer Science (FOCS), 2016, pp. 416–427, https://doi .org /10 .1109 /FOCS .2016 .52.

[9] Haris Aziz, Simon Mackenzie, A discrete and bounded envy-free cake cutting protocol for four agents, in: Proceedings of the 48th Annual ACM Sym-

posium on Theory of Computing (STOC), 2016, pp. 454–464, https://doi .org /10 .1145 /2897518 .2897522.

[10] Eric Balkanski, Simina Brânzei, David Kurokawa, Ariel D. Procaccia, Simultaneous cake cutting, in: Twenty-Eighth AAAI Conference on Artiﬁcial Intelli-

[11] Siddharth Barman, Nidhi Rathi, Fair cake division under monotone likelihood ratios, in: Proceedings of the 21st ACM Conference on Economics and 

Computation (EC), 2020, pp. 401–437, https://doi .org /10 .1145 /3391403 .3399512.

[12] Eleni Batziou, Kristoffer Arnsfelt Hansen, Kasper Høgh, Strong approximate consensus halving and the Borsuk-Ulam theorem, in: Proceedings of the 
48th International Colloquium on Automata, Languages, and Programming (ICALP), 2021, pp. 24:1–24:20, https://doi .org /10 .4230 /LIPIcs .ICALP.2021.24.
[13] Paul Beame, Stephen Cook, Jeff Edmonds, Russell Impagliazzo, Toniann Pitassi, The relative complexity of NP search problems, J. Comput. Syst. Sci. 

57 (1) (1998) 3–19, https://doi .org /10 .1145 /225058 .225147.

[14] Xiaohui Bei, Warut Suksompong, Dividing a graphical cake, in: Proceedings of the 35th AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021.
[15] Xiaohui Bei, Ning Chen, Xia Hua, Biaoshuai Tao, Endong Yang, Optimal proportional cake cutting with connected pieces, in: Twenty-Sixth AAAI Con-

ference on Artiﬁcial Intelligence, 2012.

[16] Xiaohui Bei, Ning Chen, Guangda Huzhang, Biaoshuai Tao, Jiajun Wu, Cake cutting: envy and truth, in: IJCAI, 2017, pp. 3625–3631.
[17] Xiaohui Bei, Ayumi Igarashi, Xinhang Lu, Warut Suksompong, The price of connectivity in fair division, in: Proceedings of the AAAI Conference on 

Artiﬁcial Intelligence, vol. 35, 2021, pp. 5151–5158.

[18] Karol Borsuk, Drei Sätze über die n-dimensionale euklidische Sphäre, Fundam. Math. 20 (1933) 177–190, https://doi .org /10 .4064 /fm -20 -1 -177 -190.
[19] Sylvain  Bouveret,  Yann  Chevaleyre,  Nicolas  Maudet,  Fair  allocation  of  indivisible  goods,  in:  Handbook  of  Computational  Social  Choice,  Cambridge 

University Press, 2016, pp. 284–310, https://doi .org /10 .1017 /CBO9781107446984 .013.

[20] Steven J. Brams, Alan D. Taylor, Fair Division: From Cake-Cutting to Dispute Resolution, Cambridge University Press, 1996.
[21] Simina Brânzei, Noam Nisan, The query complexity of cake cutting, https://arxiv.org /abs /1705 .02946, 2017.
[22] Simina Brânzei, Noam Nisan, Communication complexity of cake cutting, in: Proceedings of the 20th ACM Conference on Economics and Computation 

[23] Simina Brânzei, Ioannis Caragiannis, David Kurokawa, Ariel D. Procaccia, An algorithmic framework for strategic fair division, in: Thirtieth AAAI Con-

(EC), 2019, p. 525, https://doi .org /10 .1145 /3328526 .3329644.

ference on Artiﬁcial Intelligence, 2016.

[24] Ioannis Caragiannis, John K. Lai, Ariel D. Procaccia, Towards more expressive cake cutting, in: Proceedings of the 22nd International Joint Conference 

on Artiﬁcial Intelligence (IJCAI), 2011, pp. 127–132, https://doi .org /10 .5591 /978 -1 -57735 -516 -8 /IJCAI11 -033.

[25] Xi Chen, Xiaotie Deng, Matching algorithmic bounds for ﬁnding a Brouwer ﬁxed point, J. ACM 55 (3) (2008) 13:1–13:26, https://doi .org /10 .1145 /

1379759 .1379761.

24

A. Deligkas, A. Filos-Ratsikas and A. Hollender

Artiﬁcial Intelligence 313 (2022) 103784

https://doi .org /10 .1287 /moor.1110 .0511.

1120 .1116.

Artiﬁcial Intelligence (AAAI), 2021.

doi .org /10 .1137 /080720826.

[26] Xi Chen, Xiaotie Deng, Shang-Hua Teng, Settling the complexity of computing two-player Nash equilibria, J. ACM 56 (3) (2009) 14:1–14:57, https://

doi .org /10 .1145 /1516512 .1516516.

2011.

[27] Yuga J. Cohler, John K. Lai, David C. Parkes, Ariel D. Procaccia, Optimal envy-free cake cutting, in: Twenty-Fifth AAAI Conference on Artiﬁcial Intelligence, 

[28] Constantinos Daskalakis, Christos Papadimitriou, Continuous local search, in: Proceedings of the 22nd Annual ACM-SIAM Symposium on Discrete 

Algorithms (SODA), SIAM, 2011, pp. 790–804, https://doi .org /10 .1137 /1.9781611973082 .62.

[29] Constantinos Daskalakis, Paul W. Goldberg, Christos H. Papadimitriou, The complexity of computing a Nash equilibrium, SIAM J. Comput. 39 (1) (2009) 

[30] Argyrios Deligkas, John Fearnley, Themistoklis Melissourgos, Pizza sharing is PPA-hard, in: Proceedings of the Thirty-Sixth AAAI Conference on Artiﬁcial 

195–259, https://doi .org /10 .1137 /070699652.

Intelligence, 2022.

[31] Argyrios Deligkas, John Fearnley, Themistoklis Melissourgos, Paul G. Spirakis, Computing exact solutions of consensus halving and the Borsuk-Ulam 

theorem, J. Comput. Syst. Sci. 117 (2021) 75–98, https://doi .org /10 .1016 /j .jcss .2020 .10 .006.

[32] Argyrios Deligkas, John Fearnley, Alexandros Hollender, Themistoklis Melissourgos, Constant inapproximability for PPA, in: Proceedings of the 54th 

ACM Symposium on Theory of Computing (STOC), pp. 1010–1023, 2022.

[33] Xiaotie Deng, Qi Qi, Amin Saberi, Jie Zhang, Discrete ﬁxed points: models, complexities, and applications, Math. Oper. Res. 36 (4) (2011) 636–652, 

[34] Xiaotie Deng, Qi Qi, Amin Saberi, Algorithmic solutions for envy-free cake cutting, Oper. Res. 60 (6) (2012) 1461–1476, https://doi .org /10 .1287 /opre .

[35] Edith Elkind, Erel Segal-Halevi, Warut Suksompong, Mind the gap: cake cutting with separation, in: Proceedings of the 35th AAAI Conference on 

[36] Kousha Etessami, Mihalis Yannakakis, On the complexity of Nash equilibria and other ﬁxed points, SIAM J. Comput. 39 (6) (2010) 2531–2597, https://

[37] John Fearnley, Paul W. Goldberg, Alexandros Hollender, Rahul Savani, The complexity of gradient descent: CLS = PPAD ∩ PLS, in: Proceedings of the 

53rd ACM Symposium on Theory of Computing (STOC), 2021, pp. 46–59, https://doi .org /10 .1145 /3406325 .3451052.

[38] Aris Filos-Ratsikas, Paul W. Goldberg, Consensus halving is PPA-complete, in: Proceedings of the 50th Annual ACM Symposium on Theory of Computing 

(STOC), 2018, pp. 51–64, https://doi .org /10 .1145 /3188745 .3188880.

[39] Aris Filos-Ratsikas, Paul W. Goldberg, The complexity of splitting necklaces and bisecting ham sandwiches, in: Proceedings of the 51st Annual ACM 

Symposium on Theory of Computing (STOC), 2019, pp. 638–649, https://doi .org /10 .1145 /3313276 .3316334.

[40] Aris Filos-Ratsikas, Søren Kristoffer Still Frederiksen, Paul W. Goldberg, Jie Zhang, Hardness results for consensus-halving, in: Proceedings of the 43rd 
International Symposium on Mathematical Foundations of Computer Science (MFCS), 2018, pp. 24:1–24:16, https://doi .org /10 .4230 /LIPIcs .MFCS .2018 .
24.

[41] Aris Filos-Ratsikas, Alexandros Hollender, Katerina Sotiraki, Manolis Zampetakis Consensus-Halving, Does it ever get easier?, in: Proceedings of the 

21st ACM Conference on Economics and Computation (EC), 2020, pp. 381–399, https://doi .org /10 .1145 /3391403 .3399527.

[42] Aris Filos-Ratsikas, Alexandros Hollender, Katerina Sotiraki, Manolis Zampetakis, A topological characterization of modulo-p arguments and implications 
for necklace splitting, in: Proceedings of the 32nd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), 2021, pp. 2615–2634, https://doi .org /
10 .1137 /1.9781611976465 .155.

[43] George Gamow, Marvin Stern, Puzzle-Math, Viking, 1958.
[44] Charles H. Goldberg, Douglas B. West, Bisection of circle colorings, SIAM J. Algebraic Discrete Methods 6 (1) (1985) 93–106, https://doi .org /10 .1137 /

[45] Paul Goldberg, Alexandros Hollender, Warut Suksompong, Contiguous cake cutting: hardness results and approximation algorithms, J. Artif. Intell. Res. 

0606010.

69 (2020) 109–141.

[46] Paul W. Goldberg, Alexandros Hollender, Ayumi Igarashi, Pasin Manurangsi, Warut Suksompong, Consensus halving for sets of items, in: Proceedings 
of the 16th International Conference on Web and Internet Economics (WINE), 2020, pp. 384–397, https://doi .org /10 .1007 /978 -3 -030 -64946 -3 _27.
[47] Michelangelo Grigni, A Sperner lemma complete for PPA, Inf. Process. Lett. 77 (5–6) (2001) 255–259, https://doi .org /10 .1016 /S0020 -0190(00 )00152 -6.
[48] Charles R. Hobby, John R. Rice, A moment problem in L1 approximation, Proc. Am. Math. Soc. 16 (4) (1965) 665–670, https://doi .org /10 .2307 /2033900.
[49] Hadi Hosseini, Ayumi Igarashi, Andrew Searns, Fair division of time: multi-layered cake cutting, in: 29th International Joint Conference on Artiﬁcial 

Intelligence, IJCAI 2020, in: International Joint Conferences on Artiﬁcial Intelligence, 2020, pp. 182–188.

[50] Roman N. Karasev, Edgardo Roldán-Pensado, Pablo Soberón, Measure partitions using hyperplanes with ﬁxed directions, Isr. J. Math. 212 (2) (2016) 

[51] Ilan Komargodski, Moni Naor, Eylon Yogev, White-box vs. black-box complexity of search problems: Ramsey and graph property testing, J. ACM 66 (5) 

[52] David Kurokawa, John Lai, Ariel Procaccia, How to cut a cake before the party ends, in: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 

[53] Jiˇrí Matoušek, Using the Borsuk-Ulam Theorem: Lectures on Topological Methods in Combinatorics and Geometry, Springer Science & Business Media, 

[54] Nimrod Megiddo, Christos H. Papadimitriou, On total functions, existence theorems and computational complexity, Theor. Comput. Sci. 81 (2) (1991) 

[55] Frédéric Meunier, Simplotopal maps and necklace splitting, Discrete Math. 323 (28) (2014) 14–26, https://doi .org /10 .1016 /j .disc .2014 .01.008.
[56] Jerzy Neyman, Un théorème d’existence, C. R. Acad. Sci. Paris 222 (1946) 843–845.
[57] Christos H. Papadimitriou, On the complexity of the parity argument and other ineﬃcient proofs of existence, J. Comput. Syst. Sci. 48 (3) (1994) 

498–532, https://doi .org /10 .1016 /S0022 -0000(05 )80063 -7.

[58] Ariel D. Procaccia, Cake cutting: not just child’s play, Commun. ACM 56 (7) (2013) 78–87, https://doi .org /10 .1145 /2483852 .2483870.
[59] Ariel D. Procaccia, Cake cutting algorithms, in: Handbook of Computational Social Choice, Cambridge University Press, 2016, pp. 311–330, https://

doi .org /10 .1017 /CBO9781107446984 .014.

doi .org /10 .1016 /0097 -3165(95 )90073 -X.

[60] Jack M. Robertson, William A. Webb, Approximating fair division with a limited number of cuts, J. Comb. Theory, Ser. A 72 (2) (1995) 340–344, https://

[61] Jack M. Robertson, William A. Webb, Cake-Cutting Algorithms: Be Fair If You Can, CRC Press, ISBN 9781568810768, 1998.
[62] Erel Segal-Halevi, Redividing the cake, in: Proceedings of the 27th International Joint Conference on Artiﬁcial Intelligence, 2018, pp. 498–504.
[63] Forest W. Simmons, Francis E. Su, Consensus-halving via theorems of Borsuk-Ulam and Tucker, Math. Soc. Sci. 45 (1) (2003) 15–25, https://doi .org /10 .

[64] Hugo Steinhaus, Sur la division pragmatique, Econometrica 17 (1949) 315–319, https://doi .org /10 .2307 /1907319.
[65] Francis E. Su, Rental harmony: Sperner’s lemma in fair division, Am. Math. Mon. 106 (10) (1999) 930–942, https://doi .org /10 .1080 /00029890 .1999 .

[66] Albert W. Tucker, Some topological properties of disk and sphere, in: Proceedings of the First Canadian Math, Congress, Montreal, University of Toronto 

1016 /S0165 -4896(02 )00087 -2.

12005142.

Press, 1945, pp. 286–309.

[67] Gerhard J. Woeginger, JiˇríSgall, On the complexity of cake cutting, Discrete Optim. 4 (2) (2007) 213–220, https://doi .org /10 .1016 /j .disopt .2006 .07.003.

705–728, https://doi .org /10 .1007 /s11856 -016 -1303 -z.

(2019) 1–28, https://doi .org /10 .1109 /FOCS .2017.63.

vol. 27, 2013.

2008, https://doi .org /10 .1007 /978 -3 -540 -76649 -0.

317–324, https://doi .org /10 .1016 /0304 -3975(91 )90200 -L.

25

