Artiﬁcial Intelligence 175 (2011) 1498–1527

Contents lists available at ScienceDirect

Artiﬁcial Intelligence

www.elsevier.com/locate/artint

Eﬃcient solutions to factored MDPs with imprecise transition
probabilities
Karina Valdivia Delgado a,∗

, Scott Sanner b, Leliane Nunes de Barros a,c

a EACH, Universidade de São Paulo, Av. Arlindo Béttio, 1000 – Ermelino Matarazzo São Paulo – SP, Brazil
b NICTA and the Australian National University, Canberra, ACT 2601, Australia
c IME, Universidade de São Paulo, Rua de Matão, 1010 – Cidade Universitária São Paulo – SP, Brazil

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 3 December 2009
Received in revised form 31 December 2010
Accepted 31 December 2010
Available online 4 January 2011

Keywords:
Probabilistic planning
Markov Decision Process
Robust planning

When modeling real-world decision-theoretic planning problems in the Markov Decision
Process (MDP) framework, it is often impossible to obtain a completely accurate estimate
of
transition probabilities. For example, natural uncertainty arises in the transition
speciﬁcation due to elicitation of MDP transition models from an expert or estimation from
data, or non-stationary transition distributions arising from insuﬃcient state knowledge. In
the interest of obtaining the most robust policy under transition uncertainty, the Markov
Decision Process with Imprecise Transition Probabilities (MDP-IPs) has been introduced to
model such scenarios. Unfortunately, while various solution algorithms exist for MDP-IPs,
they often require external calls to optimization routines and thus can be extremely time-
consuming in practice. To address this deﬁciency, we introduce the factored MDP-IP and
propose eﬃcient dynamic programming methods to exploit its structure. Noting that the
key computational bottleneck in the solution of factored MDP-IPs is the need to repeatedly
solve nonlinear constrained optimization problems, we show how to target approximation
techniques to drastically reduce the computational overhead of the nonlinear solver
while producing bounded, approximately optimal solutions. Our results show up to two
orders of magnitude speedup in comparison to traditional “ﬂat” dynamic programming
approaches and up to an order of magnitude speedup over the extension of factored MDP
approximate value iteration techniques to MDP-IPs while producing the lowest error of any
approximation algorithm evaluated.

© 2011 Elsevier B.V. All rights reserved.

1. Introduction

Markov Decision Processes (MDP) [1] have become the de facto standard model for decision-theoretic planning problems
and a great deal of research in recent years has aimed to exploit structure in order to compactly represent and eﬃciently
solve factored MDPs [2–5]. However, in many real-world problems, it is simply impossible to obtain a precise representation
of the transition probabilities in an MDP. This may occur for many reasons, including (a) imprecise or conﬂicting elicitations
from experts, (b) insuﬃcient data from which to estimate reliable precise transition models, or (c) non-stationary transition
probabilities due to insuﬃcient state information.

For example, in an MDP for traﬃc light control, it is diﬃcult to estimate the turn probabilities for each traﬃc lane that
has the option of going straight or turning. These lane-turning probabilities may change during the day or throughout the
year, as a function of traﬃc at other intersections, and based on holidays and special events; in general it is impossible to

* Corresponding author. Tels.: +55 11 73326036, +55 11 30919878; fax: +55 11 30916134.

E-mail addresses: kvd@usp.br (K.V. Delgado), ssanner@nicta.com.au (S. Sanner), leliane@ime.usp.br (L.N. de Barros).

0004-3702/$ – see front matter © 2011 Elsevier B.V. All rights reserved.
doi:10.1016/j.artint.2011.01.001

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1499

accurately model all of these complex dependencies. In this case it would be ideal to have a traﬃc control policy optimized
over a range of turn probabilities in order to be robust to inherent non-stationarity in the turn probabilities.

To accommodate optimal models of sequential decision-making in the presence of strict uncertainty over the transition
model, the MDP with imprecise transition probabilities (MDP-IP) was introduced [6,7]. While the MDP-IP poses a robust
framework for the real-world application of decision-theoretic planning, its general solution requires the use of computa-
tionally expensive optimization routines that are extremely time-consuming in practice.

To address this computational deﬁciency, we extend the factored MDP model to MDP-IPs by proposing to replace the
usual Dynamic Bayes Net (DBN) [8] used in factored MDPs with Dynamic Credal Nets (DCNs) [9] to support compact factored
structure in the imprecise transition model of factored MDP-IPs. Then we propose eﬃcient, scalable algorithms for solving
these factored MDP-IPs. This leads to the following novel contributions in this work:

• We introduce the parameterized ADD (PADD) with polynomial expressions at its leaves and explain how to extend ADD

properties and operations to PADDs.

• We extend the decision-diagram based SPUDD and APRICODD algorithms for MDPs [3,4] to MDP-IP algorithms that

exploit DCN structure via PADDs.

• As shown in our experimental evaluation, the generalization of SPUDD and APRICODD to MDP-IPs using PADDs is
just the ﬁrst step in obtaining eﬃcient solutions. Observing that the key computational bottleneck in the solution of
MDP-IPs is the need to repeatedly solve nonlinear constrained optimization problems, we show how to target our
approximations to drastically reduce the computational overhead of the nonlinear solver while producing provably
bounded, approximately optimal solutions.

As our results will demonstrate, using the above contributions we can obtain up to two orders of magnitude speedup
in comparison to traditional “ﬂat” dynamic programming approaches [6]. In addition, our best approximate factored MDP-
IP solver yields an order of magnitude speedup over a direct generalization of state-of-the-art approximate factored MDP
solvers [4] for factored MDP-IPs (also implemented in this work) and consistently produces the lowest error of all approxi-
mate solution algorithms evaluated.

2. Markov decision processes

Formally, an MDP is deﬁned by the tuple M = (cid:3)S, A, P , R, T , γ (cid:4), where [1,10]:

(cid:5)|s, a) is the conditional probability of reaching state s

• S is a ﬁnite set of fully observable states;
• A is a ﬁnite set of actions;
• P (s
• R : S × A → R is a ﬁxed reward function associated with every state and action;
• T is the time horizon (number of decision stages remaining) for decision-making;
• γ = [0, 1) is a discount factor (the reward obtained t stages into the future is discounted in the sense that it is multiplied

(cid:5) ∈ S when action a ∈ A is taken from state s ∈ S;

by γ t ).

A stationary policy π : S → A indicates the action a = π (s) to take in each state s (regardless of stage). The value of a
stationary policy π is deﬁned as the expected sum of discounted rewards over an inﬁnite horizon (|T | = ∞) starting in
state s0 at stage 0 and following π

V π (s) = Eπ

(cid:4)
(cid:4) s0 = s

γ t Rt

(cid:5)

,

(cid:2)

∞(cid:3)

t=0

(1)

where Rt (abbreviation of Rt (st, π (st)) is the reward obtained at stage t when the agent is in state st and takes action π (st ).
(1) can be decomposed and rewritten recursively based on the values of the possible successor states s
(cid:6)

(cid:5) ∈ S as follows:

(cid:3)

(cid:6)

(cid:7)

(cid:6)

.

(2)

V π (s) = R

(cid:7)
s, π (s)

+ γ

(cid:7)
(cid:5)|s, π (s)

(cid:5)

V π

s

P

s

s(cid:5)∈S

Our objective is to ﬁnd an optimal policy π ∗

that yields the maximal value in each state, i.e., ∀s, π (cid:5)

V π ∗ (s) (cid:2) V π (cid:5) (s).

A well-known algorithm to solve an MDP is value iteration [1]. For t > 0, it constructs a series of t-stage-to-go value
functions V t . Starting with arbitrary V 0, value iteration performs value updates for all states s, computing V t based on
V t−1. The Q-value for state s and action a is:

Q t(s, a) = R(s, a) + γ

(cid:6)

P

(cid:5)|s, a

s

(cid:7)

V t−1

(cid:7)

(cid:6)

(cid:5)

s

(cid:3)

s(cid:5)∈S

where the best value attainable at decision stage t and state s is

(3)

1500

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Fig. 1. A credal set example represented by the gray region. The credal set is deﬁned by the triplets {P (x1), P (x2), P (x3)} that belong to this region.

V t(s) = max
a∈ A

Q t(s, a).

We deﬁne the greedy policy πV w.r.t. some V as follows:

(cid:8)

πV (s) = arg max

a∈ A

R(s, a) + γ

(cid:6)

P

(cid:5)|s, a

s

(cid:7)

(cid:6)

V

(cid:5)

s

(cid:9)

(cid:7)

.

(cid:3)

s(cid:5)∈S

At the inﬁnite horizon, the value function provably converges
(cid:4)
(cid:4)
(cid:4)V t(s) − V t−1(s)
(cid:4) = 0
max
s

lim
t→∞

(4)

(5)

(6)

leading to a stationary, deterministic optimal policy π ∗ = πV ∞ [1]. For practical MDP solutions, we are often only concerned
with (cid:4)-optimality. If we terminate the MDP when the following condition is met:

(cid:4)
(cid:4)
(cid:4)V t(s) − V t−1(s)
(cid:4) <

max
s

(cid:4)(1 − γ )
2γ

then we guarantee that the greedy policy πV t
π ∗

[1].

3. MDPs with imprecise transitions

loses no more than (cid:4) in value over an inﬁnite horizon in comparison to

(7)

As described in our introductory traﬃc example, it is often necessary to work with imprecise probabilities in order to
represent incomplete, ambiguous or conﬂicting expert beliefs about transition probabilities. An MDP with imprecise transition
probabilities (MDP-IP)1 is speciﬁcally designed for this setting and is simply an extension of the MDP where the transition
probabilities can be imprecisely speciﬁed. That is, instead of a probability measure P (·|s, a) over the state space S, we have
a set of probability measures. For example, let P ( X) be the probability density function for X = {x1, x2, x3} deﬁned with the
following constraint set:
(cid:10)

C =

P (x1) (cid:3) 2/3,
P (x3) (cid:3) 2/3,
2P (x1) (cid:2) P (x2),
P (x1) + P (x2) + P (x3) = 1

(cid:11)

.

(8)

The two-dimensional region of all probability measures that satisfy C is shown as the gray region in Fig. 1. This is referred
to as a credal set, i.e., a set of probability measures (or a set of distributions for a random variable) [11]. We denote a credal
set of distributions for variable X by K ( X).

Next we slightly specialize the deﬁnition of credal set to specify uncertainty in MDP-IP transition probabilities:

1 The term MDP-IP was proposed by White III and Eldeib [7], while Satia and Lave Jr. [6] adopt instead the term MDP with Uncertain Transition Probabilities.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1501

Deﬁnition 3.1 (Transition credal set). A credal set containing conditional distributions over the next state s
and an action a, is referred to as a transition credal sets [11] and denoted by K (s
deﬁne imprecisely speciﬁed transition probabilities.

, given a state s
(cid:5)|s, a). Thus, we have P (·|s, a) ∈ K (·|s, a) to

(cid:5)

We assume that all credal sets are closed and convex, an assumption that is often used in the literature, and that
(cid:5)|s, a); that
(cid:5)|s, a) to be
(cid:5)|s, a) may be selected from the corresponding credal sets in a time-dependent

encompasses most practical applications [12]. We further assume stationarity for the transition credal sets K (s
(cid:5)|s, a) is non-stationary, we note that this does not require P (s
is, they do not depend on the stage t. While K (s
stationary in an MDP-IP: distributions P (s
manner [13].

Formally, an MDP-IP is deﬁned by MIP = (S, A, K , R, T , γ ). This deﬁnition is identical to the MDP M, except that the
transition distribution P is replaced with a transition credal set K . We will represent K implicitly as the set of transition
probabilities consistent with a set of side linear inequality constraints C , like (8), over the probability parameters.

There are several optimization criteria that can be used to deﬁne the value of a policy in an MDP-IP. In the context of
the discounted inﬁnite horizon setting that we focus on in this work, there is always a deterministic stationary policy that
(cid:5)|s, a)
is maximin optimal [6] (i.e., no other policy could achieve greater value under the assumption that Nature’s selects P (s
adversarially to minimize value); moreover, given the assumption that A is ﬁnite and the credal set K is closed, this policy
induces an optimal value function that is the unique ﬁxed-point solution of

(cid:12)

∗

V

(s) = max
a∈ A

min
P ∈K

R(s, a) + γ

(cid:6)

P

(cid:5)|s, a

s

(cid:7)

(cid:6)

∗

(cid:5)

s

V

(cid:13)

(cid:7)

.

(cid:3)

s(cid:5)∈S

(9)

There are various algorithms for solving ﬂat (i.e., enumerated state) MDP-IPs based on dynamic programming [6,7]. In this
work, we build on a ﬂat value iteration solution to MDP-IPs [6]:

(cid:12)

V t(s) = max
a∈ A

min
P ∈K

R(s, a) + γ

(cid:6)

P

(cid:5)|s, a

s

(cid:7)

V t−1

(cid:6)

(cid:5)

s

(cid:13)

(cid:7)

.

(cid:3)

s(cid:5)∈S

(10)

Value iteration for MDP-IPs is the same as that given in (3) and (4) for MDPs except that now for every state s, we optimize
our action choice a ∈ A w.r.t. the worst-case distribution P ∈ K that minimizes the future expected value. Thus we ensure
that the resulting value function and policy are robust to the worst outcome that Nature could choose in light of the future
value V t−1(s

(cid:5)) that we expect to achieve.

As we noted before, Nature’s true transition function P may be non-stationary; Nature can choose a different P ∈ K for
every action a and every state s and every decision stage t. As an example of such non-stationarity that may occur in practice,
in the previously discussed traﬃc scenario, we observed that traﬃc turn probabilities may differ on holidays versus normal
weekdays even though the embedded traﬃc controller may not be explicitly aware of the holiday in its state description.
However, as long as such transition non-stationarity can be bounded by P ∈ K , convergence properties of MDP-IP value
iteration in (10) still hold [13].

In [14,9] we have shown how MDP-IP solutions can be formulated as a bilevel or multilinear programming problem. In
this paper we are interested in extending the dynamic programming solution for MDP-IPs [6,7] outlined above to eﬃciently
solve problems with a factored state description, which we discuss next.

4. Factored MDP and MDP-IPs

4.1. Factored MDP

In many MDPs, it is often natural to think of the state as an assignment to multiple state variables and a transition
function that compactly speciﬁes the probabilistic dependence of variables in the next state on a subset of variables in the
current state. Such an approach naturally leads us to deﬁne a Factored MDP [2], where S = {(cid:10)x}. Here, (cid:10)x = (x1, . . . , xn) where
each state variable xi ∈ {0, 1}.2

The deﬁnition of actions a ∈ A is unchanged between MDPs and factored MDPs, so the reward can simply be speciﬁed
as R((cid:10)x, a). The transition probabilities in a factored MDP are encoded using Dynamic Bayesian Networks (DBNs) [8]. A DBN
is a directed acyclic graph (DAG) with two layers: one layer represents the variables in the current state and the other layer
(cid:5)
represents the next state (Fig. 2a). Nodes xi and x
i refer to the respective current and next state variables. The connection
between these two layers deﬁnes the dependences between state variables w.r.t. the execution of an action a ∈ A. Directed
edges are allowed from nodes in the ﬁrst layer into the second layer, and also between nodes in the second layer (these
(cid:5)
(cid:5)
i) the parents of x
latter edges are termed synchronic arcs). We denote by paa(x
in the graph for action a. The graph
i
(cid:5)
encodes the standard Bayes net conditional independence assumption that a variable x
i is conditionally independent of its
nondescendants given its parents, which incidentally for a DBN also encodes the Markov assumption (the current state is

2 While our extensions are not necessarily restricted to binary state variables, we make this restriction here for simplicity of notation.

1502

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

(cid:5)
Fig. 2. a) A Dynamic Bayesian Network (DBN) for an action a; b) conditional probability table for x
2

(cid:5)
= 1; c) conditional probability table for x
2

= 0.

independent of the history given the previous state). The use of a DBN leads to the following factorization of transition
probabilities:

(cid:6)

P

(cid:5)|(cid:10)x, a
(cid:10)x

(cid:7)

=

n(cid:14)

i=1

(cid:6)

P

(cid:5)
x
i

|paa

(cid:6)

(cid:5)
x
i

(cid:7)

(cid:7)

, a

.

Fig. 2(b) shows the conditional probability table (CPT) for P (x

The tables show all combinations of variable assignments for the parents of x
row in Fig. 2(b) and Fig. 2(c) must be 1, which can be easily veriﬁed.

(cid:5)
2

(cid:5)
(cid:5)
= 1|paa(x
2), a); Fig. 2(c) shows the same CPT for x
2

= 0.
(cid:5)
(cid:5)
2); by deﬁnition, the sum of each
2, i.e., pa(x

(11)

4.2. Factored MDP-IP

As our ﬁrst major contribution, we extend the factored MDP representation [2] to compactly represent MDP-IPs. This
simply requires modifying the DBN transition representation to account for uncertainty over the exact transition probabili-
ties. Before we formally describe this transition function though, we ﬁrst introduce one possible extension of the SysAdmin
factored MDP to allow for imprecise transition probabilities, which we use from here out as a running example of a factored
MDP-IP.

SysAdmin domain [5].
In the SysAdmin domain we have n computers c1, . . . , cn connected via different directed graph
topologies: (a) unidirectional ring, (b) bidirectional ring and (c) independent bidirectional rings of pairs of computers (Fig. 3).
Let state variable xi denote whether computer ci is up and running (xi = 1) or not (xi = 0). Let Conn(c j, ci) denote a

⎧

(i)

connection from c j to ci . Formally, the CPTs for this domain have the following form:
if a = reboot(ci): then 1
if a (cid:11)= reboot(ci) ∧ xi = 1: then
pi1 · |{x j | j(cid:11)=i∧x j=1∧Conn(c j,ci )}|+1
if a (cid:11)= reboot(ci) ∧ xi = 0: then
pi2 · |{x j | j(cid:11)=i∧x j=1∧Conn(c j,ci )}|+1

|{x j | j(cid:11)=i∧Conn(c j,ci )}|+1

⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩

= 1|(cid:10)x, a

(iii)

(ii)

(cid:5)
x
i

=

P

(cid:6)

(cid:7)

|{x j | j(cid:11)=i∧Conn(c j,ci )}|+1

(12)

and the constraints C on the probabilities variables are

C = {0.85+ pi2 (cid:3) pi1 (cid:3) 0.95}.

We have n + 1 actions: reboot(c1), . . . , reboot(cn) and notreboot, the latter of which indicates that no machine is rebooted.
The intuition behind Eq. (12) is that if a computer is rebooted then its probability of running in the next time step is 1
(situation i); if a computer is not rebooted and its current state is running (situation ii) or not running (situation iii), the
probability depends on the fraction of computers with incoming connections that are also currently running. The probability
parameters pi1, pi2 and the constraint C over them deﬁne the credal sets K (·|(cid:10)x, a).

The reward for SysAdmin is simply 1 if all computers are running at any time step otherwise the reward is 0, i.e.,
(cid:19)
n
i=1 xi . An optimal policy in this problem will reboot the computer that has the most impact on the expected

R((cid:10)x) =
future discounted reward given the network conﬁguration.

Like the previous deﬁnition of an enumerated state MDP-IP, the set of all legal transition distributions for a factored
MDP-IP is deﬁned as a credal set K . The challenge then is to specify such transition credal sets in a factored manner that is

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1503

Fig. 3. Connection topologies for the SysAdmin example: a) unidirectional-ring, b) bidirectional ring and c) independent bidirectional rings of pairs of
computers [5].

Fig. 4. a) Dynamic Credal Network for action notreboot for an unidirectional-ring topology of SysAdmin domain with 2 computers. b) Conditional prob-
(cid:5)
= 1 and the constraints related to the probabilities. c) The Parameterized ADD representation for
ability table for the state variables x
1
x
|x1, x2, notreboot) that we call CPT
notreboot . A solid line indicates the true (1) branch of a variable test and a dashed line indicates the false (0) branch.

(cid:5)
= 1 and x
2

(cid:5)
P (x
1

(cid:5)
1

itself compact. For this, we propose to use dynamic credal networks (DCNs), a special case of credal networks [11,15], as an
appropriate language to express factored transition credal sets.

Deﬁnition 4.1 (Factored transition credal set). A credal set containing conditional distributions over the values of a variable xi ,
given the values of paa(xi) (the parents of xi in the graph for action a), is referred to as a factored transition credal set and
denoted by Ka(xi|paa(xi)).

Deﬁnition 4.2 (Dynamic credal network). A Dynamic credal network (DCN) is a generalization of a DBN. Different from the
deﬁnition of a DBN, in a DCN each variable xi is associated with factored transition credal sets Ka(xi|paa(xi)) for each value
of paa(xi). We assume that a DCN represents a joint credal set [15,11] over all of its variables consisting of all distributions
(cid:5)
|paa(x
that satisfy the factorization in Eq. (11), where each CPT distribution P (x
i), a) is an element of the transition credal
(cid:5)
set Ka(x
i

(cid:5)
|paa(x
i)) associated with the DCN.

(cid:5)
i

A DCN example is shown in Fig. 4(a). For each variable x

(cid:5)
i in a DCN, we have a conditional probability table (CPT) with
imprecise probabilities. If we examine the CPTs in Fig. 4(b), we note that entries are speciﬁed by probability parameters pi j
(cid:5)
(cid:5)
(i for variable x
i ). Furthermore, we note that we have a set of side linear
i and j for the jth parameter in the CPT for x
constraints on these pi j (shown in the boxes below the CPT, collectively call this constraint set C ). We use (cid:10)p to denote a
vector containing all parameter values that are free to vary within the given credal sets (i.e., that satisfy the probability
constraints C of the DCN).

We note that the joint transition probability may be nonlinear in the probability parameters (cid:10)p. However, we explicitly

introduce the following restriction to prevent exponents exceeding 1:

(cid:5)
i .
Restriction 4.3 (DCN parameter restriction for factored MDP-IP CPTs). a parameter pi j may only appear in the CPT for x

(cid:5)
= 1, x
2

(cid:5)
i are multiplied together to determine
This restriction prevents the multiplication of pi j by itself when CPTs for each x
the joint transition distribution in the DCN. This subset of nonlinear expressions, where the exponent of each pi j is either
0 or 1, is referred to as a multilinear expression. To see the multilinearity of the transition probability in Fig. 4, we observe
(cid:5)
P (x
1
When combined with a set of constraints C on the pi j , there are eﬃcient implementations that we can use in practice
to solve the resulting multilinear program. Interestingly, because there are no additional restrictions on the linear constraints
C deﬁned over the pi j in a multilinear program, Restriction 4.3 actually turns out to be a minor limitation in practice as we
demonstrate in the experimental domains of Section 8.

= 1|x1 = 1, x2 = 1, notreboot) = p11 p21.

1504

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Fig. 5. An example reward function R(x1, x2, x3) =

(cid:20)

3
i=1 xi represented as an ADD.

Even though we can qualitatively represent the conditional independence properties of a distribution using DCNs, there
are certain independences that we cannot represent with the Credal network structure, e.g., independences that hold for
speciﬁc contexts (assignments of values to certain variables) known as context-speciﬁc independence (CSI) [16]. In order to
compactly represent CSI and shared function structure in the CPTs for an MDP-IP, we propose a novel extension of algebraic
decision diagrams (ADDs) [17] called parameterized ADDs (PADDs) since the leaves are parameterized expressions as shown
in Fig. 4(c). PADDs will not only allow us to compactly represent the CPTs for factored MDP-IPs, but they will also enable
eﬃcient computations for factored MDP-IP value iteration operations as we outline next.

5. Parameterized algebraic decision diagrams

Algebraic decision diagrams (ADDs) [17] are a generalization of ordered binary decision diagrams (BDDs) that represent
boolean functions {0, 1}n → {0, 1} [18]. A BDD is a data structure that has decision nodes, each node labeled with a boolean
test variable with two successor nodes: l (low) and h (high). The arc from a node to its successor l (h) represents an
assignment 0(1) to the test variable. BDDs are DAGs whose variable tests on any path from root to leaf follow a ﬁxed total
variable ordering. BDDs are used to generate the value of a boolean function as follows: given assignments to the boolean
test variables in a BDD, we follow branches l or h, until we get to a leaf, which is the boolean value returned by the function.
The only difference between an ADD and a BDD is that terminal nodes in an ADD are real values, i.e., ADDs permit the
compact representation of functions {0, 1}n → R. BDDs and ADDs often provide an eﬃcient representation of functions with
(cid:20)
context-speciﬁc independence [16] and shared function structure. For example, the reward function R(x1, x2, x3) =
3
i=1 xi
represented in Fig. 5 as an ADD exploits the redundant structure of sub-diagrams through its DAG representation.

Operations on ADDs can be performed eﬃciently by exploiting their DAG structure and ﬁxed variable ordering. Examples
of eﬃcient ADD operations are unary operations such as min, max (return the minimum or maximum value in the leaves
) that eliminates a variable xi of an ADD; binary operations such as
of a given ADD), marginalization over variables (
addition (⊕), subtraction ((cid:14)), multiplication (⊗), division ((cid:16)), and even min(·, ·) and max(·,·) (return an ADD with min/max
values in the leaves). We refer the reader to [17] for details.

(cid:20)

xi

Parameterized ADDs (PADDs) are an extension of ADDs that allow for a compact representation of functions from
{0, 1}n → E, where E is the space of expressions parameterized by (cid:10)p (in our case, we further restrict this to the space
of multilinear expressions of (cid:10)p). For example, the CPT in Fig. 6 represented as a PADD contains leaves consisting of single
parameters while Fig. 8(d) shows a PADD with a leaf containing a more complex parameterized expression.

In the following, we formally deﬁne PADDs and their basic operations needed to construct eﬃcient solutions for MDP-
IPs. Because PADDs are introduced to solve MDP-IPs, we make the following restrictive assumptions: (a) we allow only
multilinear expressions in the leaves; (b) we only deﬁne a subset of PADD operations that could be inherited from ADDs;
and (c) we only show these operations are closed (i.e., yield a resulting PADD with multilinear leaves) for the operations
needed in MDP-IPs. Finally, we contribute a new unary operation MinParameterOut (min(cid:10)p ) speciﬁc to PADDs.

5.1. PADD: Formal deﬁnition, properties and operations

PADDs generalize the constant real-valued leaves of ADDs to polynomials (Poly) expressed in a sum-of-products canonical

form:

d0 +

(cid:3)

(cid:14)

di

pi j

i

j

(13)

where the di are constants and the pi j are parameters. Formally, we can deﬁne a PADD by the following BNF grammar3:

3 We will adopt lowercase ( f ) to refer to a mathematical function, and uppercase (F ) to refer to the function represented structurally as a PADD.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1505

(cid:5)
(cid:5)
2 for action a1. b) The Parameterized ADD representation for P (x
Fig. 6. a) Conditional probability table for the state variable x
2

= 1|x1, x2, x3, x4, a1).

F ::= Poly|if(F var) then Fh else Fl
(cid:3)
Poly ::= d0 +

pi j.

(cid:14)

di

i

j

This grammar is notationally overloaded, so we brieﬂy explain: a PADD node F can either be a terminal node with an
expression of type Poly or a decision node with variable test F var (e.g., x1 or xn) and two branches Fh and Fl (both of
grammar non-terminal type F ), where Fh is taken when F var = 1 and Fl is taken when F var = 0.

The value returned by a function f represented as a PADD F containing (a subset of) the variables {x1, . . . , xn} with

variable assignment ρ ∈ {0, 1}n can be deﬁned recursively by:

⎧
⎨

V al(F , ρ) =

⎩

if F = Poly :
if F (cid:11)= Poly ∧ ρ(F var) = true : Val(Fh, ρ)
if F (cid:11)= Poly ∧ ρ(F var) = false : Val(Fl, ρ).

Poly

This recursive deﬁnition of Val(F , ρ) reﬂects the structural evaluation of a PADD F by starting at its root node and following
the branch at each decision node corresponding to the variable assignment in ρ — this continuing until a leaf node is
reached, which is then returned as V al(F , ρ). As an example, for the PADD represented in Fig. 6, assigning ρ = {1, 0, 1, 0}
for variables {x1, x2, x3, x4} yields Val(F , ρ) = p21.

Like ADDs, for any function f (x1, . . . , xn) and a ﬁxed variable ordering over x1, . . . , xn, a reduced PADD is deﬁned as the

minimally sized ordered decision diagram representation of a function f .

Lemma 5.1. There exists a unique reduced PADD F (the canonical PADD representation of f ) satisfying the given variable ordering
such that for all ρ ∈ {0, 1}n we have f (ρ) = Val(F , ρ).

The proof of this lemma for BDDs was provided by [19] and can be trivially generalized to ADDs and PADDs. Since PADDs
allow polynomial leaves, the only change for demonstrating this lemma is that we need to ensure that there exists a way to
identify when two leaf expressions are identical, which can be easily done by (a) sorting the parameters in each multilinear
term, (b) factoring out (grouping terms with the same ordered set of parameters) and summing constants in identical
multilinear terms, and (c) sorting the list of terms according to the lowest variable index and number of parameters. With
such a unique leaf identiﬁcation method, the proof of [19] generalizes to PADDs and shows that there is a unique canonical
PADD representation for every function from {0, 1}n to polynomials in the form of (13).

In fact, not only does such a minimal, reduced PADD always exist for a function f that can be represented as a PADD,
but there is a straightforward algorithm for computing it called ReducePADD, which we present in Section 5.2.1. Before
we present formal PADD algorithms though, we ﬁrst discuss extensions of the unary and binary operations from ADDs to
PADDs. Fortunately, this only requires that operations on the leaves of ADDs are modiﬁed to accept and produce resulting
polynomials in the form of (13).

5.1.1. Binary operations on PADDs

The binary operations ⊕ (sum) and (cid:14) (subtraction) as deﬁned for ADDs [17] can be extended for PADDs and are always
closed since these operations yield PADDs with leaves in the form of (13). However, the binary operation ⊗ (product) can

1506

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Fig. 7. An example application of Restrict operation and Marginalization operation on a PADD.

only yield a PADD with leaves in the form of (13) if the set of parameters (cid:10)p in the leaves of each operand are disjoint.
Fortunately, for factored MDP-IPs, we note that the only place ⊗ is used is to compute the product of the DCN CPTs;
because of Restriction 4.3 on the usage of parameters pi j in these CPTs, we note that the condition for closed ⊗ operations
on PADDs is always satisﬁed for the required factored MDP-IP operations.

However, not all PADD binary operations have simple conditions under which they are closed. We note that PADDs are
not closed under (cid:16) (binary division), i.e., the resulting leaves could be a polynomial fraction and hence cannot be expressed
as (13). Similarly, the binary min(·,·) and max(·,·) operations deﬁned for ADDs [17] cannot generally be computed in closed
form unless the actual assignment to the parameters (cid:10)p is known. Fortunately, (cid:16), min(·,·), and max(·,·) will not be needed
in our proposed solution to factored MDP-IPs.

5.1.2. Unary operations on PADDs

The two important classical unary operations for ADDs are restriction (F |xi ) and marginalization (

extended to PADDs as follows:

(cid:20)

xi

) and can be easily

• Restriction of a variable xi to either true (F |xi =1) or false (F |xi =0) can be calculated by replacing all decision nodes for
variable xi with either the high or low branch, respectively. This operation can be used to do marginalization as we
show next. This operation does not affect the leaves of the decision diagram, so its extension from ADDs to PADDs is
straightforward.
(cid:20)

• The marginalization or sum_out operation (represented as

) eliminates a variable xi from an ADD. It is computed as
xi
the sum of the true and false restricted functions, i.e., (F |xi =1 ⊕ F |xi =0). Since ⊕ is closed for PADDs, marginalization is
also closed for PADDs. An example is shown in Fig. 7.

The classical unary min(·) and max(·) operations for ADDs cannot generally be computed for PADDs unless the actual
assignment to the parameters (cid:10)p is known. However, we will not need this particular PADD operation for factored MDP-IPs,
but rather a new unary operation for PADDs called MinParameterOut, which in our case will make the choices of Nature in
Eq. (9).

Deﬁnition 5.2 (MinParameterOut operation). Represented as min(cid:10)p(F ), this operation takes as input (1) a PADD F and (2) a
set C of global constraints over the PADD’s parameters, and returns an ADD. We note that an ADD is a special case of a
PADD with constant expressions at its leaves, which implies that min(cid:10)p(F ) is closed for PADDs. This unary operation calls a
nonlinear solver for each leaf expression e in the form of (13) to compute c = min(cid:10)p(e) w.r.t. constraints C and replaces the
leaf e with the constant c.

Because the set of variable assignments that can reach each PADD leaf are disjoint, each leaf can be minimized indepen-
dently of the others. This is precisely the operation we’ll need for factored MDP-IPs, since we note that Nature performs it’s
minimization independently per state s in (9), and every path in the PADD will correspond to a different state assignment.
An example of min(cid:10)p(F ) is shown in Fig. 12.

5.2. PADD algorithms

Previously we have discussed PADD algorithms conceptually, in this subsection, we discuss how to implement eﬃcient
operations for PADDs. In the following algorithms we use four hash tables: ReduceCache, NodeCache, ApplyCache and Min-
ParCache. We use the notation key → value to represent key/value pairs in the hash table. The table NodeCache stores a
unique identiﬁcation for each node (representing sub-diagrams by unique identiﬁers), the hash table ReduceCache stores
the reduced canonical nodes (the results of previous Reduce operations), the table ApplyCache stores the results of previous
Apply operations (so we can avoid redundant calculations) and the hash table MinParCache stores the results of previous
MinParameterOut operations.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1507

Algorithm 1: ReducePADD(F)

input : F (root node id for an arbitrary ordered decision diagram)
output: F r (root node id for reduced PADD)

begin

//if terminal node, return canonical terminal node
if F is terminal node then

return canonical terminal node for polynomial of F ;

//use recursion to reduce sub diagrams
if F → F r is not in ReduceCache then

Fh = ReducePADD(Fh );
Fl = ReducePADD(Fl );
//get a canonical internal node id
F r = GetNode(F var , Fh , Fl);
insert F → F r in ReduceCache;

return F r ;

end

1
2
3
4
5
6
7
8
9
10
11

12

13

Algorithm 2: GetNode((cid:3)var, Fh, Fl(cid:4))

input : (cid:3)var, Fh, Fl(cid:4) (variable and true and false branches node ids for internal node)
output: F r (canonical internal node id)

1
2
3
4
5
6
7
8

9

begin

//redundant branches
if Fl = Fh then
return Fl ;

//check if the node exists previously
if (cid:3)var, Fh, Fl(cid:4) → id is not in NodeCache then

id = new unallocated id;
insert (cid:3)var, Fh, Fl(cid:4) → id in NodeCache;

return id;

10

end

5.2.1. Reduce algorithm for PADDs

While we know there exists a unique canonical form for every function expressible as a PADD (Lemma 5.1), the algo-
rithm ReducePADD actually allows the eﬃcient construction of this unique canonical PADD representation from an arbitrary
ordered decision diagram with polynomial leaves of type (13).

Algorithm 1 recursively constructs such a reduced PADD from the bottom up. In this algorithm, an internal node is repre-
sented as (cid:3)F var, Fh, Fl(cid:4), where F var is the variable name, and Fh and Fl are the true and false branch node ids, respectively.
Additionally, the input F refers to an arbitrary node, while the returned value Fr refers to a canonical node id. Reduced
canonical nodes are stored in the hash table ReduceCache and the helper function GetNode (Algorithm 2) ensures that re-
dundant decision tests at internal nodes are removed. The table NodeCache used in the function GetNode stores a unique
identiﬁcation for each node.

An example of the application of the ReducePADD algorithm is shown in Fig. 8. The hollow arrow points to the internal
node F that is being evaluated by ReducePADD after the two recursive calls to ReducePADD (lines 7 and 8) but before line
10. Fig. 8(a) shows the input diagram for the algorithm where x3 is being evaluated by ReducePADD creating two canonical
terminal nodes for 0.3 + 5p12 and 0. Note that while evaluating node x3 (on the left), the execution of line 10 will result in
the insertion of (cid:3)x3, 0.3 + 5p12, 0(cid:4) in the NodeCache hash table. Fig. 8(b) shows the resulting evaluation of node x3 on the
right, which returns the same previous canonical terminal nodes for 0.3 + 5p12 and 0. And again, after executing line 10,
the GetNode algorithm will return the same id for (cid:3)x3, 0.3 + 5p12, 0(cid:4), previously inserted in the NodeCache. Fig. 8(c) shows
the evaluation of x2. Note that Fh and Fl are equal, thus after getNode is called, Fl is returned and as a consequence x2
disappears. Finally, Fig. 8(d) shows the canonical PADD representation of the input. Note that ReducePADD (Fl) returned the
same canonical terminal node that exists previously for the node 0.

The running time and space of ReducePADD are linear in the size of the input diagram since the use of the ReduceCache
guarantees that each node is visited only once and at most one unique reduced node is generated in the canonical diagram
for every node visited.

5.2.2. Apply algorithm for binary operations for PADDs

The notation we will use in this paper for PADDs is shown in Fig. 9. Any operation with two PADDs, F 1 and F 2, results
and two new sub-diagrams Fh and Fl. Note that F i,h

in a new canonical PADD Fr , with eventually a new root node F var
and F i,l represent sub-diagrams.

r

For all binary operations, we use the generic function Apply(F 1, F 2, op) (Algorithm 3) and the result computation table in
the helper function ComputeResult (Table 1) that supports operations between arbitrary PADD nodes and polynomial leaves.

1508

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Fig. 8. A step-by-step illustration of the application of ReducePADD algorithm (Algorithm 1) where a) and d) are the input and output PADDs, respectively.

Fig. 9. The notation used in the Apply and ChooseVarBranch algorithms.

Fig. 10. An example of PADDs multiplication.

Table 1 is implemented as a method named ComputeResult, which is simply a case structure for each line of Table 1. Notice
that lines 2–9 of Table 1 deﬁne the result of PADD operations in special cases that avoid unnecessary computation in Apply.
The Apply algorithm (Algorithm 3) has as input two operands represented as canonical PADDs, F 1 and F 2, and a binary
operator op ∈ {⊕, (cid:14), ⊗}; the output is the result of the function application represented as a canonical PADD F r . Apply
(F 1, F 2, op) ﬁrst checks if the result can be immediately computed by calling the method ComputeResult (line 3). If the
result is null, it then checks whether the result was previously computed by checking in the ApplyCache, which stores the
results of previous Apply operations (line 6). If there is not a cache hit, Apply chooses the earliest variable in the ordering
to branch on by calling the auxiliary function ChooseVarBranch (Algorithm 4) and then branches on this variable with two
recursive Apply calls, one to compute Fl and other to compute Fh. After that, the results of these two operations are checked
for redundancy elimination throughout GetNode function. An example of PADD multiplication via Apply algorithm is shown
in Fig. 10.

5.2.3. MinParameterOut algorithm for PADDs

The MinParameterOut algorithm (Algorithm 5) has as input a canonical PADD F and a set of constraints C over the PADD’s
parameters; the output is the result of calling the nonlinear solver for each PADD leaf, represented as a canonical ADD F r .
MinParameterOut ﬁrst checks if F is a constant terminal node, in this case it is not necessary to call the nonlinear solver
for this leaf. If the terminal node is not a constant then we need to make a call to the nonlinear solver passing the leaf
expression as an objective to minimize subject to C (line 7). If F is not a terminal node, Algorithm 5 recursively traverses
the PADD. Similar to ReducePADD, an internal node is represented as (cid:3)F var, Fh, Fl(cid:4) and previously computed canonical nodes
are stored in the hash table MinParCache. The helper function GetNode (Algorithm 2) ensures again that redundant decision
tests at internal nodes are removed.

With this last speciﬁcation of MinParameterOut, we have formally described almost all of the PADD algorithms we will
need in our factored MDP-IP solution. We omit the restriction and marginalization algorithms for PADDs since they are
identical to the same operations for ADDs (i.e., these operations don’t modify the leaves, which is the only place that PADDs
and ADDs differ).

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1509

Algorithm 3: Apply(F 1, F 2, op)

input : F 1 (root node id for operand 1),
F 2 (root node id for operand 2),
op (binary operator, op ∈ {⊕, (cid:14), ⊗})

output: F r (root node id for the resulting reduced PADD)
begin

//check if the result can be immediately computed
if ComputeResult(F 1, F 2, op) → F r (cid:11)= null then

return F r ;

//check if we previously computed the same operation
if (cid:3)F 1, F 2, op(cid:4) → F r is not in ApplyCache then

//choose variable to branch
var = ChooseVarBranch(F 1, F 2);
//set up nodes for recursion
if F 1 is non-terminal ∧ var = F var
1

then

else

F v1
l
F v1
h

= F 1,l ;
= F 1,h ;

F v1
l,h

= F 1;

F v2
l
F v2
h

= F 2,l ;
= F 2,h ;

if F 2 is non-terminal ∧var = F var
2

then

else

F v2
l,h

= F 2;

l

, F v2
l
h , F v2

//use recursion to compute true and false branches for resulting PADD
Fl = Apply(F v1
, op);
Fh = Apply(F v1
h , op);
F r = GetNode(var, Fh, Fl);
//save the result to reuse in the future
insert (cid:3)F 1, F 2, op(cid:4) → F r into ApplyCache;

return F r ;

end

1
2
3
4
5
6
7
8
9
10
11

12

13
14

15
16

17

18
19

20

21

22
23
24
25

26

27

Algorithm 4: ChooseVarBranch(F 1, F 2)

input : F 1 (root node id for operand 1),
F 2 (root node id for operand 2)
output: var (selected variable to branch)
begin

//select the variable to branch based on the order criterion
if F 1 is a non-terminal node then

if F 2 is a non-terminal node then
then

comes before F var

2

if F var
1
var = F var
1 ;

else

var = F var
2 ;

else

var = F var
1 ;

else

var = F var
2 ;

return var;

end

1
2
3
4
5
6
7
8
9

10
11
12

13
14
15

16

6. Factored MDP-IP value iteration

In the two previous sections, we showed a compact representation for factored MDP-IPs based on dynamic credal net-
works (DCNs) and parameterized ADDs (PADDs) and the respective algorithms needed to manipulate DCNs and PADDs. In
this section, we will present our ﬁrst exact value iteration solution that exploits both of these representations. This solution
is an extension of the SPUDD [3] algorithm. First, we give a mathematical description of the proposed solution and then
proceed to formally specify the algorithm that computes it.

6.1. SPUDD-IP description

We extend the SPUDD [3] algorithm for exploiting DBN and ADD structure in the solution of factored MDPs to a novel
algorithm SPUDD-IP for exploiting DCN and PADD structure in the solution of factored MDP-IPs. We begin by expressing

1510

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Table 1
Input case and result for the method ComputeResult for binary operations ⊕, (cid:14) and ⊗ for PADDs.

Case number

1
2
3
4
5
6
7
8
9

; F 2 = Poly2

Case operation
F 1 op F 2; F 1 = Poly1
F 1 ⊕ F 2; F 2 = 0
F 1 ⊕ F 2; F 1 = 0
F 1 (cid:14) F 2; F 2 = 0
F 1 ⊗ F 2; F 2 = 1
F 1 ⊗ F 2; F 1 = 1
F 1 ⊗ F 2; F 2 = 0
F 1 ⊗ F 2; F 1 = 0
other

Return

Poly1 op Poly2
F 1
F 2
F 1
F 1
F 2
0
0
null

Algorithm 5: MinParameterOut(F, C )

input : F (root node id for a PADD),
C (set of constraints)
output: F r (root node id for an ADD)

begin

//if terminal node, call the solver and return the value
if F is terminal node then

node = canonical terminal node for polynomial of F ;
if node is a constant then

return node;

c = CallNonLinearSolver(node,C );
return canonical terminal node for the constant c;

//use recursion to compute sub diagrams
if F → F r is not in ReduceCacheMinPar then

Fh = MinParameterOut(Fh );
Fl = MinParameterOut(Fl );
//get a canonical internal node id
F r = GetNode(F var , Fh , Fl);
insert F → F r in ReduceCacheMinPar;

return F r ;

end

1
2
3
4
5
6
7
8

9
10
11
12
13
14
15

16

17

MDP-IP value iteration from (10) in the following factored form using the transition representation of (11) and operations
on decision diagrams4:
⎧
⎪⎨

⎫
⎪⎬

D D ((cid:10)x) = max
V t
a∈ A

R D D ((cid:10)x, a) ⊕ γ min
(cid:10)p

⎪⎩

(cid:3)

n(cid:21)

P D D

(cid:10)x(cid:5)

i=1

(cid:6)

(cid:6)

(cid:7)

(cid:7)

(cid:5)
x
i

|paa

(cid:5)
x
i

, a

V t−1

D D

(cid:6)

(cid:7)

(cid:5)
(cid:10)x

.

⎪⎭

(14)

(cid:5)
|paa(x

Because the transition CPTs in the MDP-IP DCN contain parameters (cid:10)p, these CPTs must be represented in decision diagram
(cid:5)
i), a)). On the other hand, the reward R D D ((cid:10)x, a) can be represented as an ADD since it
format as PADDs ( P D D (x
i
contains only constants (for the purpose of operations, recall that ADDs are special cases of PADDs). Although it may appear
D D ((cid:10)x) is a PADD, we note that the parameters (cid:10)p are “minimized”-out w.r.t. the side constraints on (cid:10)p
that the form of V t
(cid:2) is the MinParameterOut operation on PADDs, that performs the
during the min(cid:10)p
minimization over the parameters by calling a nonlinear solver for each leaf and returns an ADD). This is crucial, because
D D ((cid:10)x)
the maxa∈ A can only be performed on ADDs (recall that max is not a closed operation on PADDs). Thus the resulting V t
computed from the maxa∈ A has constant leaves and can be expressed as the ADD special case of PADDs.

(cid:2) operation in (14) (remember that min(cid:10)p

To explain the eﬃcient evaluation of (14) in more detail, we can exploit the variable elimination algorithm [20] in the
(cid:5)
(cid:5)
i for i (cid:11)= 1, we can “push” the
1 is not dependent on any other x
(cid:10)x(cid:5) . For example, if x

(cid:20)

marginalization over all next states
(cid:5)
1 inwards to obtain:
sum over x
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩

D D ((cid:10)x) = max
V t
a∈ A

R D D ((cid:10)x, a) ⊕ γ min
(cid:10)p

(cid:3)

n(cid:21)

(cid:6)

(cid:5)
x
i

|paa

(cid:7)

(cid:6)

(cid:5)
x
i

, a

P D D

(cid:7)(cid:3)

(cid:5)
i (i(cid:11)=1)
x

i=1(i(cid:11)=1)

(cid:5)
x
1

(cid:6)

(cid:5)
x
1

|paa

(cid:6)

(cid:7)

(cid:7)

, a

(cid:5)
x
1

V t−1

D D

(cid:7)

(cid:6)

(cid:5)
(cid:10)x

P D D

⎫
⎪⎪⎪⎬
⎪⎪⎪⎭

.

(15)

We show this graphically in the example of Fig. 11. Here, we have the ADD representation for the ﬁrst value function V 0
(cid:5)
R D D (Fig. 11(a)), which we multiply by P D D (x
1

=
(cid:5)
|paa(x
1), a) (Fig. 11(b)) yielding the result (Fig. 11(c)) and sum this out over

D D

4 We use D D for the functions represented by ADDs or PADDs, since the ﬁrst is a special case of the second.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1511

Fig. 11. a) We show V 0

(cid:5)
representation for P (x
1
which is a PADD.

A D D
|x1, x2, notreboot) (CPT

= R(x1, x2) for the unidirectional-ring topology of SysAdmin domain with 2 computers represented as an ADD. b) The PADD
x
(cid:5)
1,
notreboot resulting in a PADD. d) The result of summing out over x

notreboot ). c) The multiplication V 0

⊗ CPT

A D D

(cid:5)
1

(cid:5)
1

x

(cid:5)
(cid:5)
|paa(x
2), a),
1 to obtain the ﬁnal result (Fig. 11(d)). Then we can continue with x
x
(cid:5)
(cid:5)
(cid:5)
i to compute (cid:2). After this (cid:2) does not contain anymore the variables x
summing out over x
2, and repeating for all x
i , but
only the variables xi .

(cid:5)
(cid:5)
2, multiplying this result by the P D D (x
2

Representing the contents of (cid:2) as f ((cid:10)x, a, (cid:10)p), we obtain

(cid:25)

D D ((cid:10)x) = max
V t
a∈ A

R D D ((cid:10)x, a) ⊕ γ min
(cid:10)p

f ((cid:10)x, a, (cid:10)p)

(cid:26)
.

(16)

Note that min(cid:10)p f ((cid:10)x, a, (cid:10)p) leads to a separate nonlinear expression minimization for every (cid:10)x and every a subject to the set
C of side linear constraints on (cid:10)p (given with the DCN speciﬁcation) since this follows from the deﬁnition of the MDP-IP
Bellman equation — every state gets its own minimizer and each PADD leaf corresponds to a set of states with exactly
the same minimization objective. This optimization problem may be represented as a simple multilinear program due to
(cid:5)
Restriction 4.3 that guarantees each pi j only appears in the DCN CPT for x
i (this prevents multiplication of pi j by itself,
thereby preventing exponents exceeding 1). This restriction is important to guarantee the existence of exact solutions and
the existence of eﬃcient implementations that we can use in practice to solve multilinear programs. We note that this is
only a restriction on the factored MDP-IP models themselves.

To demonstrate how the min(cid:10)p f ((cid:10)x, a, (cid:10)p) is performed on PADDs, we refer to Fig. 12. Here, each leaf expression in
f ((cid:10)x, a, (cid:10)p) (Fig. 12(a)) given by the PADD corresponds to the function that Nature must minimize in each state. We cru-
cially note that the PADD aggregates states with the same minimization objective, thus saving time-consuming calls to the
multilinear solver. We will observe this time savings in our experiments. Now, we need to make a call to the multilinear
solver for each leaf, passing the leaf expression as the objective to minimize subject to the side constraints C of our DCN
that specify the legal (cid:10)p — after the minimization, we can replace this leaf with a constant for the optimal objective value
returned by the multilinear solver (Fig. 12(b)). We can see that after the min(cid:10)p operation, all PADDs are simpliﬁed to the
special case of ADDs with leaf nodes that are constants.

To complete one step of factored MDP-IP value iteration, we take the ADD resulting from the min(cid:10)p operation, multiply it
by the scalar γ , add in the reward R D D ((cid:10)x, a), and ﬁnally perform a sequence of binary ADD max(·,·) operations to compute
the maxa, thus yielding the ADD V t

D D ((cid:10)x) and completing one step of value iteration from (14).

D D ((cid:10)x) from the ADD for V t−1

6.2. SPUDD-IP algorithm

Factored MDP-IP value iteration is formally speciﬁed in the following two procedures:
Solve (Algorithm 6) constructs a series of t-stage-to-go value functions V t

D D that are represented as ADDs. First we
create the PADD representation of all DCN CPTs in the MDP-IP and initialize the ﬁrst value function to 0 (line 3). The loop
is repeated until a maximum number of iterations or until a Bellman error BE termination condition (BE < tol) is met.

1512

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Fig. 12. The MinParameterOut operation example. a) The PADD before minimization and a multilinear program for the ﬁrst leaf, the solution for this leaf is
the constant value c1. b) The resulting ADD after the minimization at all leaves.

Fig. 13. a) The value function V t
within error of each other have been merged and averaged and the resulting ADD simpliﬁed.

D D represented as an ADD. b) the result of ApproxADD applied to V t

D D with approximation error = 1; note that the leaves

We note that setting the tolerance tol according to (7) guarantees (cid:4)-optimality for MDP-IPs since the same termination
conditions used for MDPs directly generalize to MDP-IPs in the discounted case (γ < 1). At each iteration the Regress
algorithm is called (line 13) and V t
D D for each action a). Af-
|V t((cid:10)x) − V t−1((cid:10)x)| is computed and tested for termination. We observe in Algorithm 6 the parameters
ter this, BE = max(cid:10)x
δ, APRICODD, Objective, and Vmax play no role now; they are used for approximation as we explain in the next section (in
particular we use δ = 0 to obtain an exact solution by the SPUDD-IP).

D D is updated with the max over all Q t

D D (there is a Q t

D D , i.e, it regresses V t−1

Regress (Algorithm 7) computes Q t

D D through action a that provides the values Q t

D D that could
be obtained if executing a and acting so as to obtain V t−1
D D thereafter. During regression we “prime” the variables using
(cid:5)
i (since the V i
the function convertToPrimes that converts each xi to x
D D is now part of the “next” state) and the CPTs for
(cid:5)
i ,
action a are multiplied in and summed out (lines 4–6). We assume here there are no synchronic arcs among variables x
(cid:5)
j for i (cid:11)= j in the DCN. If synchronic arcs are present, the algorithm can be simply modiﬁed to multiply in all relevant
x
CPTs. After this, the MinParameterOut function is performed that calls the multilinear solver to ﬁnd the minimizing (cid:10)p for
each leaf in the PADD w.r.t. the side linear constraints C on the DCN (line 11), resulting in an ADD. We note that if a leaf
is already a constant, then the multilinear solver call can be avoided altogether; this observation will prove important later
when we introduce objective pruning. Finally, the future value is discounted and the reward ADD is added in to complete
the regression. Objective and error are used for approximate value iteration and will be discussed later.

7. Factored MDP-IP approximate value iteration

The previous SPUDD-IP exact value iteration solution to factored MDP-IPs often yields an improvement over ﬂat value
iteration as we will demonstrate in our experiments. But as the number of state variables in a problem grows larger, it often
becomes impossible to obtain an exact solution due to time and space limitations.

Approximate value iteration (AVI) is one way to trade off time and space with error by approximating the value func-
tion after each iteration. In this section, we propose two (bounded) AVI extensions of SPUDD-IP: the APRICODD-IP and the
Objective-IP algorithms. Each method uses a different way to approximate the value, but both methods incur a maximum
of δ · Vmax error per iteration where Vmax as computed in Solve represents the maximum possible value at each step
of value iteration (with 0 < δ (cid:3) 1). By making the approximation error sensitive to Vmax we prevent over-aggressive value
approximation in the initial stages of AVI when values are relatively small as suggested in [4]. Even with this value ap-

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1513

Algorithm 6: Solve (MDP-IP, tol, maxIter, δ, APRICODD, Objective)

input : MDP-IP (given by (cid:3)S, A, R, K , γ (cid:4)),

tol (tolerance that guarantees (cid:4)-optimality),
maxIter (maximum number of iterations),
//variables used for approximate value iteration
δ (fraction of the maximum possible value, with 0 < δ (cid:2) 1),
APRICODD (APRICODD = true to execute APRICODD-IP),
Objective (Objective = true to execute Objective-IP)

output: V t

DD (t-state-to-go value function)

1
2

3
4
5
6
7
8
9
10
11
12
13
14

15

16
17
18
19
20
21
22
23

24

25

1
2
3
4
5

6

7
8
9
10
11
12
13

14

DD until termination condition is met

begin

ADD

= 0;

(cid:5)
|pa(x
i ), a) for MDP-IP;

(cid:5)
Create PADD: P D D (x
i
V 0
//Vmax is the maximum possible value at each iteration
Vmax = max(RDD);
t = 0;
//construct t-stage-to-go value functions V t
while i < maxIter do

t = t + 1;
= −∞;
V t
//update V t
foreach a ∈ A do

DD

DD with the max over all Q t

DD

DD=Regress(V t−1
Q t
DD, Q t
DD=max(V t
V t

DD , a, δ · Vmax, Objective);
DD);

//compute Bellman Error (BE) and check for termination

DD

= V t

(cid:14) V t−1
DD ;
Diff DD
BE = max(max(Diff DD),− min(Diff DD));
if BE < tol then
break;

//approximate value iteration: APRICODD-IP
if APRICODD pruning then
=ApproxADD (V t

DD, δ · Vmax);

V t

DD

Vmax = max(RDD) + γ Vmax;

return V t

DD ;

end

Algorithm 7: Regress(V DD, a, error, Objective)

input : V DD (value function),

a (action),
error (maximum error),
Objective (Objective = true to execute Objective-IP)

output: Q DD (the value function obtained if executing a and acting so as obtain V DD thereafter)

begin

(cid:5)
Q DD = convertToPrimes(V DD); //convert variables xi to x
i
//CPTs are multiplied in and summed out
(cid:5)
i in Q DD do
for all x
(cid:5)
Q DD = Q DD ⊗ P D D (x
(cid:20)
i
Q DD =

(cid:5)
|pa(x
i ), a);

Q DD;

x

(cid:5)
i

//approximate value iteration: Objective-IP
if Objective pruning then

Q DD =approxPADDLeaves ( Q DD, error);

//call the nonlinear solver for each PADD leaf — returns an ADD
Q DD = MinParameterOut ( Q DD,C );
Q DD = RDD ⊕ (γ ⊗ Q DD) ;
return Q DD;

end

proximation at every iteration, satisfying the termination condition BE < tol for some tol still yields strict guarantees on the
overall approximation error given by (7) as discussed previously for SPUDD-IP.

7.1. APRICODD-IP algorithm

The APRICODD algorithm [4] provides an eﬃcient way of approximating the ADD value representation for a factored MDP,
reducing its size and thus reducing computation time per iteration. This approach immediately generalizes to MDP-IPs since
the value function V t
D D is also an ADD. To execute APRICODD-IP AVI for MDP-IPs, we simply call Solve (Algorithm 6) with
APRICODD = true and set δ (0 < δ (cid:3) 1) to some fraction of the maximum possible value Vmax with which to approximate
calling the algorithm ApproxADD (line 22 Algorithm 6).

1514

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Algorithm 8: ApproxADD(valuei

D D ,error)

input : valuei

D D (an ADD),
error (maximum error)

output: a new ADD

begin

//collect all leaves of the ADD
leavesold=collectLeavesADD (valuei
//group the leaves that can be merged within maximum error
{leavesold → leavesnew} = mergeLeaves (leavesold, error);
//return a simpliﬁed ADD
return createNewDD (valuei

D D , {leavesold → leavesnew});

D D );

end

1
2

3
4
5
6

7

8

ApproxADD (Algorithm 8) has two inputs: (1) a value function represented as an ADD and (2) an approximation error
to merge the leaves. The output is a new ADD with the merged leaves. The algorithm ﬁrst collects all leaves of the ADD
and determines which can be merged to form new values without approximating more than error. The old values are then
replaced with these new values creating a new (minimally reduced) ADD that represents the approximated value function.
An illustrative example is shown in Fig. 13.

collectLeavesADD compiles the leaves of the ADD and puts them in a set (leavesold). In the example in Fig. 13, the

set of old leaves is {9, 0, 10, 1}.

mergeLeaves groups together the leaves that can be merged within error and computes the average for each group,
creating a new set of leaves (leavesnew). In the example of Fig. 13, the groups than can be merged within an error = 1 are
{9, 10} and {0, 1}; and the new leaves are 9.5 and 0.5.

createNewDD creates a simpliﬁed ADD, replacing the old leaves by the new ones. The result of this operation in the

example is shown in Fig. 13(b)).

7.2. Objective-IP algorithm

APRICODD is an effective extension of SPUDD for factored MDPs (not MDP-IPs) because it reduces the size of the value
function ADDs, which largely dictate the time complexity of the SPUDD algorithm. However, in solving (factored) MDP-IPs,
the time is dictated less by the size of the value function ADD and more by the number of calls to the multilinear optimizer
(cid:2). SPUDD-IP started to attack this source of time complexity by aggregating states with the same
to compute the min(cid:10)p
(cid:2). Our goal with the Objective-IP pruning algorithm will be to more closely target the source of time
objective for the min(cid:10)p
complexity in an AVI version of SPUDD-IP by approximating the objectives in an attempt to avoid calling the solver altogether.
To execute Objective-IP for MDP-IPs, we simply call Solve (Algorithm 6) with APRICODD = false, Objective = true and set
δ (0 < δ (cid:3) 1) to some fraction of the maximum possible value Vmax. Noting that each PADD leaf in Regress function is
a multilinear objective, we simplify it by calling ApproxPADDLeaves (line 9 Algorithm 7) just prior to carrying out the
multilinear optimization at the leaves of that PADD (line 11 Algorithm 7).

ApproxPADDLeaves (Algorithm 9) is called for a PADD by Regress when Objective = true. It takes as input a PADD
and the maximum error, and the output is a new PADD with approximated leaves using the upper and lower bounds of the
parameters. The main loop of the algorithm attempts to approximate each leaf in a PADD (lines 3–17). To approximate the
multilinear term i, Algorithm 9 ﬁrst computes the average of their maximum and minimum values (line 8), this requires
knowing the absolute upper pU
i j and lower bounds p L
i j for any pi j , which can be easily precomputed once for the entire
= min pi j subject to the side linear constraints C
= max pi j and p L
MDP-IP by calling the nonlinear solver to compute pU
i j
i j
on all CPTs. After that, Algorithm 9 computes the error incurred by using these maximum and minimum values (line 10).
If the actual accumulated error for the leaf (curError + termErrori ) is less than the maximum error (error), the term i is
removed (line 13) and replaced by the average (line 14). In some cases the complexity of the leaf expression may be
reduced, in others, it may actually be reduced to a constant. Note that the leaves are each approximated independently, this
can be done since each leaf corresponds to a different state (or set of states) and the system can only be in one state at a
time. Furthermore, we can guarantee that no objective pruning at the leaves of the PADD incurs more than error after the
multilinear optimization is performed:

Theorem (ApproxPADDLeavesError Bound). Given an MDP-IP, its precomputed constants p L
mum approximation error, then whenever ApproxPADDLeaves (Algorithm 9) reduces a leaf d0 +
expression, the approximation error in the objective minimization (min(cid:10)p) of that leaf is bounded by error.

i j and pU
i j for all pi j , and the maxi-
(cid:20)
#terms
j pi j to a simpler
i=1

(cid:19)

di

Proof. We begin by showing that the approximation error induced by removing a single term i from the objective is
bounded by termErrori . To do this, we ﬁrst ﬁnd upper and lower bounds on term i (di
j pi j ) based on the legal values of
i j). Thus for any possible legal values of (cid:10)p the term
(cid:10)p. We know the maximal (minimal) possible value for each pi j is pU
i must be bounded in the interval [Li, U i] with Li and U i deﬁned as follows:

i j (p L

(cid:19)

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1515

Algorithm 9: ApproxPADDLeaves(DD, error)

1
2

3
4
5
6
7

8

9

10

11
12
13

14
15

16

17

18

19

input : DD (parameterized ADD),

error (maximum error)

output: DD (simpliﬁed parameterized ADD)

begin

(cid:19)

//approximate each leaf independently
(cid:20)
foreach leaf : d0 +

#terms
i=1
i = 1, curError = 0;
//for all terms of the leaf, prune them if possible
while curError < error ∧ i (cid:2) #terms do

j pi j ∈ DD do

di

+

(cid:19)

(cid:19)

j pU
i j

//compute the average of max and min values for term i
newValue = di
j p L
i j ) ;
2 (
//compute the error of using max and min values for term i
termErrori = | di
j pU
j p L
2 (
i j
//if within error, prune term i from leaf
if curError + termErrori < error then
j pi j from leaf ;

i j)| ;

(cid:19)

(cid:19)

(cid:19)

−

remove term di
d0 = d0 + newValue;
curError = curError + termErrori ;

i = i + 1;

return DD;

end

(cid:27)

Li =

di > 0 di

di < 0 di

(cid:19)

(cid:19)

j p L
i j
j pU
i j

,

(cid:27)

U i =

di > 0 di

di < 0 di

(cid:19)

(cid:19)

j pU
i j
j p L
i j.

2

Let g be a value for term i and (cid:28)g = Li +U i
|) = | Li −U i
(cid:28)g|, |U i − (cid:28)g|) = max(| Li −U i
|, | U i −Li
(cid:20)
(cid:19)
2
Now, let OBJ1 = d0 +
di

2
#terms
i=1
mal objective value using (cid:10)p = (cid:10)p1. Let OBJ2 = d0 + (cid:28)g +
after replacing term 1 with L1+U 1
We want to prove that −| L1−U 1

2

2

, then maxg|g − (cid:28)g| occurs at g = Li or g = U i . So the max termErrori = max(|Li −

| as computed in Algorithm 9.

j pi j be the original non-approximated objective expression to minimize and v 1 the opti-
j pi j be the approximated objective expression to minimize

#terms
i=2

(cid:20)

(cid:19)

di

and v 2 the optimal objective using (cid:10)p = (cid:10)p2.
| < v 1 − v 2 < | L1−U 1
(cid:5)
1

(cid:19)

|. First we prove the second part of this inequality. Using (cid:10)p2 in OBJ1
j p1 j, (cid:10)p2) + v 2 − d0 − (cid:28)g (where eval is a function to
= d0 + eval(d1

2
and the approximated objective expression we obtain v
evaluate the term with the assigned values). Because v 1 is optimal v 1 (cid:3) v
(cid:9)

(cid:8)

2

(cid:5)
1 then:

v 1 − v 2 (cid:3) eval

d1

p1 j, (cid:10)p2

−(cid:28)g.

(17)

(cid:14)

j

|, i.e., −| L1−U 1
Additionally for any possible legal values of (cid:10)p and for (cid:10)p2, |eval(d1
j p1 j,
(cid:10)p2) −(cid:28)g < | L1−U 1
|. The proof of the ﬁrst inequality follows by the
same reasoning, but this time substituting (cid:10)p1 into OBJ2 and using the non-approximated objective expression. Thus, we obtain
(cid:5)
v
2

|. From this equation and (17) we obtain v1 − v2 < | L1−U 1

j p1 j, (cid:10)p1). Because v 2 is optimal v 2 (cid:3) v

= d0 + (cid:28)g + v 1 − d0 − eval(d1

j p1 j, (cid:10)p2) −(cid:28)g| < | L1−U 1

| < eval(d1

(cid:5)
2 then:

(cid:19)

2

2

2

2

(cid:19)

(cid:19)

(cid:8)

v 2 − v 1 (cid:3) (cid:28)g − eval

d1

(cid:9)

p1 j, (cid:10)p1

.

(cid:14)

j

(cid:19)

| < eval(d1

j p1 j, (cid:10)p1) − (cid:28)g < | L1−U 1

2

|. From this equation

(18)

Additionally for any possible legal values of (cid:10)p and for (cid:10)p1, −| L1−U 1
and (18) we obtain v1 − v2 > −| L1−U 1

|.

2

2

This bounds the objective approximation error for one term approximation and by simple induction, we can additively

bound the accumulated error for multiple approximations as calculated using curError in Algorithm 9. (cid:2)

8. Experimental results

Before we delve into experimental results involving the SPUDD-IP, APRICODD-IP, and Objective-IP algorithms contributed

in the previous sections, we begin by describing the factored MDP-IP domains used in our experiments.

1516

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

8.1. Domains

We perform experiments with three factored MDP-IP domains: Factory [4], SysAdmin [5] and Traﬃc (a new domain).
In the following, we review Factory and introduce the new Traﬃc domain; SysAdmin was already introduced in Section 4.

8.1.1. Factory domain

The Factory domain [4] is based on a manufacturing problem in which connected, ﬁnished parts are produced. The
parts must be shaped, polished, painted and connected by bolting, welding or gluing them. In particular in Factory do-
main the agent’s task is connect two objects A and B. The agent can choose between the following actions: shape(x),
handPaint(x), polish(x), drill(x), weld(x, y), dip(x) (paint x by dipping it), bolt(x, y) (connect objects x and y by bolting them)
and glue(x, y) (connect objects x and y by gluing them) and sprayPaint(x). sprayPaint(x) yields a lower quality of painting
than handPaint(x).

The main variables in this domain are:

• connected and connectedWell, that represent if objects A and B are simply connected (e.g. by gluing) or are well con-
nected (e.g. by welding them). The only reason for the objects well connected became not connected is when the agent
shapes one of them.

• apainted, bpainted, apaintedWell and bpaintedWell are variables to represent the painted state of the object respectively.

Painted object remains painted if it is not shaped, polished or drilled.

• ashaped bshaped represent object A shaped and B shaped respectively. Shaped part remains shaped if it is not drilled.
• asmooth and bsmooth, an object becomes smoothed if the agent execute the action polish and it succeeds. Smoothed

object remains smoothed if it is not shaped or drilled.

• adrilled and bdrilled, an object becomes drilled if the action drill is apply and it succeeds.

There are other variables that describe the things that are available in the environment to be used by the agent such as:

spraygun, glue, bolts, drill and clamps. Additionally, the variable skilledlab represents the existence of skilled labor.

The quality required for the ﬁnished product is represented by the variable typeneeded and can be high-quality or low-
quality. The process and the reward depend directly on the quality required. For example, when high-quality is required,
hand-painted, drilled and bolted objects will have more reward while spray-painted and glued objects will obtain little
reward. Additional variables can be included in the problem to generate different instances.

To obtain a factored MDP-IP, we introduce uncertainty in the bolt action for the variable connected as follows. The success
probability of the bolt action for two objects that are not connected before, when there are bolts, A is drilled and B is drilled
is p1. In the case when the two objects are not drilled but there are bolts, the success probability is p2. These probabilities
are constrained by 0.2 + p2 (cid:3) p1 (cid:3) 1 and 0.5 (cid:3) p2 (cid:3) 1. Note that p1 should always be an equal or higher probability than
p2 (since the process associated with p1 is more likely to succeed), hence the implied constraint p2 (cid:3) p1.

8.1.2. A new domain: Traﬃc

We introduce Traﬃc, a factored MDP-IP domain motivated by a real traﬃc intersection control problem modeled using
cellular transition model dynamics [21]. While this is not meant to be an accurate large-scale traﬃc model over long stretches
of road, it should still approximately model local traﬃc propagation at busy intersections where speeds are necessarily
limited by queuing and traﬃc turn delays.

A graphical representation with examples of state variables are given in Fig. 14. We encode our traﬃc state as (cid:10)x =

(x1, . . . , xn) where (cid:10)x ∈ {O , U }n indicating that each traﬃc cell xi (1 (cid:3) i (cid:3) n) is either occupied O or unoccupied U .

Our basic traﬃc model for intermediate road cells is that a car will move forward into the next cell as long as it is

unoccupied, otherwise it stops in its current cell and waits.

For each intersection road cell x j (i.e., leading into an intersection), we deﬁne a state variable t j ∈ {turn, no-turn} indicating
whether a car in xi will intend to turn into oncoming traﬃc or not. The state variable t j is drawn randomly with probability
pt that a car will turn when a new car arrives. When determining the update for x j , we note that it can always go straight
or turn left on a green, but whether it can cross the opposing lane to make a right turn depends on the opposing traﬃc
light state and the opposing traﬃc cell states to and xo (two opposing right-turning cars may safely turn though and this is
allowed by conditioning on to).

We refer to a boundary traﬃc cell xk as a feeder road cell since new cars are introduced at these points. We assume that

when the cell is not occupied, new cars arrive on a time step with probability pa.

Finally, we have state variables (cid:10)c encoding the current state of the light cycle. The action set is simply to remain in the
same state or advance in the sequence: A = {advance, no-change}. In Fig. 14, we have (cid:10)c = (c1, c2, c3, c4), where one may
interpret each binary ci as indicating whether the intended light is green (or not). However, each ci need not be binary, it
could have an additional state for the period between green lights before advancing to the next cycle. We need not commit
to a particular state sequence here, rather we simply rely on a model-speciﬁc function next-state((cid:10)c) to generate the next
state from the current when the lights advance.

With this high-level description, we now proceed to deﬁne the DCN, reward, and speciﬁc Traﬃc instance conﬁgurations

used in this article.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1517

Fig. 14. Diagram showing a 4-way single-lane intersection with cells (dotted boxes) and various state variables used in our state description. Note that
we do not model road cells that exit the intersection as we assume that cars freely exit the boundaries of the model once they have passed through the
intersection.

Traﬃc DCN transition model. Based on the above description, the transition model is provided in a compact factored format
as a dynamic credal network (DCN) [11,15], subdivided into different functional subcomponents as follows.

Light cycle transition. Here we simply model the effect of a no-change or advance action on the light state:

(cid:7)

(cid:6)
(cid:5)|(cid:10)c, a
(cid:10)c

P

=

⎧
⎨

⎩

1.0 a = no-change ∧ (cid:10)c
1.0 a = advance ∧ (cid:10)c
0.0 otherwise.

(cid:5) = (cid:10)c

(cid:5) = next-state((cid:10)c)

Lane turning indicator. Here we assume that the probability of a car at the head of the queue making a right turn is pt and
that while a car is waiting, its turn decision does not change:

(cid:6)
t

(cid:5)
j

P

= turn|t j, x j

⎧
⎨

⎩

(cid:7)

=

1.0 x j = O ∧ t j = turn
0.0 x j = O ∧ t j = no-turn
pt

x j = U .

It is diﬃcult in traﬃc models to obtain an accurate estimate of pt over all hours of the day, so using our DCN, we allow the
(cid:3) 1 (to be deﬁned for speciﬁc
turn probability to ﬂuctuate over time and thus model pmin
problem instances).

(cid:3) pt (cid:3) pmax

for 0 (cid:3) pmin

(cid:3) pmax
t

t

t

t

Intermediate road cell. The occupancy of a car in an intermediate road cell xi depends on whether an occupying car can
move forward into the next cell xi+1 and if so, whether there is a car in the previous cell xi−1 that can move forward to
take its place:

1518

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

(cid:6)

(cid:5)
x
i

P

= O |xi−1, xi, xi+1

⎧
⎨

⎩

(cid:7)

=

1.0 xi = O ∧ xi+1 = O
1.0 xi = U ∧ xi−1 = O
0.0 xi = otherwise.

Feeder road cell. A feeder road cell simply serves as an input to the traﬃc network with cars arriving at each unoccupied
feeder cell with probability pa:

(cid:6)

(cid:5)
x
k

P

= O |xk

(cid:12)

(cid:7)

=

1.0 xk = O ∧ xk+1 = O
otherwise.
pa

It is diﬃcult in traﬃc models to obtain an accurate estimate of arrival probabilities pa over all hours of the day, so using
(cid:3) 1
our DCN, we allow the arrival probability to ﬂuctuate over time and thus model pmin
(to be deﬁned for speciﬁc problem instances).

(cid:3) pa (cid:3) pmax

for 0 (cid:3) pmin

(cid:3) pmax
a

a

a

a

Intersection road cell. The intersection road cells are the most complex cells to model in a traﬃc network as traﬃc behavior
depends on the light state, the occupancy of all cells with green access to the intersection, and the state of turning traﬃc.
Here we attempt to implement a basic model of traﬃc behavior taking into account all of these contingencies:

(cid:6)

(cid:5)
x
j

P

(cid:7)
= O |x j, t j, xo, to, (cid:10)c

=

⎧

⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩

0.0 x j = U ∧ x j−1 = U
1.0 x j = U ∧ x j−1 = O
0.0 x j = O ∧ t j = no-turn
0.0 x j = O ∧ t j = turn ∧ xo = U
0.0 x j = O ∧ t j = turn ∧ xo = O ∧ ¬green((cid:10)c, o)
1.0 x j = O ∧ t j = turn ∧ xo = O ∧ green((cid:10)c, o) ∧ to = no-turn
0.0 x j = O ∧ t j = turn ∧ xo = O ∧ green((cid:10)c, o) ∧ to = turn.

Here we assume there are user-deﬁned helper functions green((cid:10)c, j) that extract the part of the state (cid:10)c indicating whether
(cid:5)
the intersection cell j has a green light (or not). We also assume that when ¬green((cid:10)c, o) holds, then x
= x j (thereby making
j
a simplifying assumption of no turns on red).

Traﬃc reward model. Because our goal is to reduce traﬃc congestion in the intersection, our objective is to minimize
the count of occupied road cells around an intersection. Thus, an appropriate reward to maximize would be the count of
unoccupied cells5:

5 We use I[·] as an indicator function taking the value 1 when its argument is true and 0 otherwise.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1519

Fig. 15. Time performance comparison for Traﬃc, SysAdmin and Factory problems using SPUDD-IP and Flat Value Iteration. The name includes the number
of variables in each problem, so the corresponding number of states is 2#variables.

R((cid:10)x) =

n(cid:3)

i=1

I[xi = U ].

Here, we get +1 reward for every cell that is unoccupied.

In this article we solve instances of Traﬃc domain with two opposing lanes. In these particular
Traﬃc problem instances.
= 1 and furthermore constrain the turn
instances, we set the turn probability minimum as pmin
probabilities p1 and p2 of the two different lanes to be highly correlated using the constraint |p1 − p2| (cid:3) 0.1. Additionally,
= 0.4
the probabilities p3 and p4 of a car arriving at either of the feeder cells for each lane use the probability bounds pmin
and pmax

= 0.6 and are constrained by |p3 − p4| (cid:3) 0.1.

= 0 and maximum as pmax

a

t

t

a

8.2. Evaluation

In this section, we empirically evaluate four algorithms: Flat Value Iteration from (10) and our three contributions from
the previous section for solving factored MDP-IPs: (i) SPUDD-IP that offers an exact solution; (ii) APRICODD-IP and (iii)
Objective-IP that offer bounded approximate solutions.

As an additional point of comparison, we note that recent years have seen the emergence of very fast approximate
factored MDP solvers based on Approximate Linear Programming (ALP) [5]. Recently, such techniques have been extended to
factored MDP-IPs [9]. Thus, we also compare the approximate solutions APRICODD-IP and Objective-IP based on approximate
value iteration with an Approximate Multilinear Programming (AMP) algorithm from [9]. AMP performs linear-value function
approximation using a ﬁxed set of basis functions and a compact constraint encoding for multilinear optimization problems
that exploits structure in the DCN.

For all algorithms, we set maxIter = 50 for SysAdmin and maxIter = 75 for the other domains with γ = 0.9. In the next

subsections we present our main results.

8.2.1. Flat value iteration vs. SPUDD-IP

In Fig. 15 we compare the running time of the two exact solution methods: SPUDD-IP and Flat Value Iteration which
∗((cid:10)x).6 Solutions not completing in ﬁve hours are marked Did Not Finish (DNF). We note that SPUDD-IP did not
compute V
outperform Flat Value Iteration on the SysAdmin domains because the exact value function has little structure as an ADD.
However, both Traﬃc and Factory had highly structured value functions and up to two orders magnitude time improvement is
demonstrated by SPUDD-IP, largely due to the ability of the PADDs to aggregate common nonlinear objectives, thus saving
a substantial number of calls to the nonlinear solver and therefore time.

8.2.2. APRICODD-IP vs. Objective-IP

In order to see the scalability of our approximate solutions, in Fig. 16 we compare the running time for APRICODD-IP and
Objective-IP vs. the number of state variables using δ = 0.1 for Factory, Traﬃc, and the three conﬁgurations of SysAdmin.

6 We note that to do this comparison, we need to slightly extend Flat Value Iteration algorithm from (10) to allow for multilinear expressions in the

transition probability table.

1520

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Fig. 16. Time performance of APRICODD-IP and Objective-IP for Traﬃc, SysAdmin and Factory problems for δ = 0.1.

We note that Objective-IP runs faster than APRICODD-IP in all domains when running with a ﬁxed bound on maximum
error per iteration (i.e., δ = 0.1).

In order to evaluate the policy returned by our AVI solutions, we compute for each ﬁxed value of δ (δ is the maximum

error per iteration w.r.t. V max), the True Approximation Error (TAE) given by:

(cid:4)
(cid:4)V

∗

(cid:4)
(cid:4)
((cid:10)x) − V approx((cid:10)x)

max
(cid:10)x

(19)

where V approx((cid:10)x) is the value returned by APRICODD-IP or Objective-IP and V
IP.

∗((cid:10)x) is the optimal value computed by SPUDD-

In the following plots we ran Solve for a range of δ. In Figs. 17 and 18 we present a detailed comparison of the time,
size (number of nodes in the ADD of the last iteration), and number of nonlinear solver calls required by APRICODD-IP and
Objective-IP plotted vs. the TAE for traﬃc-10, respectively. We note little relationship between the space required by the
ADD value representation (number of nodes) and the running times of the two algorithms (space actually increases slightly
for Objective-IP while running time decreases, see Fig. 18). But what is striking about these plots is that the running time of
each algorithm is directly correlated with the number of nonlinear solver calls made by the algorithm (taking up to 100 ms
in some cases), reﬂecting our intuitions that the time complexity of solving MDP-IPs is governed by the computational
overhead of nonlinear optimization.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1521

Fig. 17. Time, nonlinear solver calls and ADD size of APRICODD-IP pruning for the traﬃc problem with 10 variables. Results are plotted for δ ∈
{0.0, 0.025, 0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.

Fig. 18. Time, nonlinear solver calls and ADD size of Objective-IP pruning for the traﬃc problem with 10 variables. Results are plotted for δ ∈
{0.0, 0.025, 0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.

Fig. 19 shows the advantage of Objective-IP pruning that uses the upper and lower values to approximate the leaves
in PADDs. For all problems, as the number of nodes reduced to a constant grows, we see that the True Approximation
Error increases, but also the number of calls to the multilinear solver decreases. These ﬁgures also show cases where the
Objective-IP approach to PADD reduction occurs with great success, since the original PADD sizes for the exact cases are
very large, but can be reduced by orders of magnitude in exchange for a reasonable amount of approximation error.

In Figs. 20, 21, 22, 23 and 24 we show a comparison of the True Approximation Error (TAE) vs. running times for
three problems and three different sizes of each problem (varying δ). The results here echo one conclusion: Objective-IP
consistently takes less time than APRICODD-IP to achieve the same approximation error and up to one order of magnitude less
time than APRICODD-IP. This time reduction can be explained by the decreased number of calls to the multilinear solver.

8.2.3. Approximate value iteration vs. approximate multilinear programming

In Figs. 20, 21, 22, 23 and 24 we compare the two approximate solution methods, APRICODD-IP and Objective-IP, with
our implementation of approximate multilinear programming (AMP) [9] for MDP-IPs. We used simple basis functions (one
for each variable in the problem description) and pairwise basis functions (one for each pair of variables that have a common
child variable in the DCN).

When it does ﬁnish within a limit of ten hours, AMP takes only a few seconds to produce an approximate solution for
each problem (except for the Factory domain for which it did not return a solution). Comparing the algorithms in terms
of their true approximation error, we observe that: (a) in the SysAdmin problem (Figs. 22, 23, 24), AMP with pair basis
functions outperforms APRICODD-IP and obtains a solution 2–3× larger than the error of Objective-IP, but in signiﬁcantly
less time; (b) for the Traﬃc problem (Fig. 21), AMP with the simple basis solution obtains a solution with 2–3× more error

1522

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Fig. 19. Number of nonlinear solver calls and number of nodes reduced to a constant vs. True Approximation Error for Objective-IP pruning for three
different problems. Results are plotted for δ ∈ {0.0, 0.025, 0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.

Fig. 20. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis functions (MPA pairwise did not ﬁnish in a
ten hour time limit and MPA with simple basis functions did not ﬁnish for two problems) for three Factory problems.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1523

Fig. 21. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis and pairwise basis functions for Traﬃc
problem.

Fig. 22. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis and pairwise basis functions for SysAdmin
problem with unidirectional-ring topology.

1524

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

Fig. 23. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis and pairwise basis functions for SysAdmin
problem with bidirectional-ring topology.

Fig. 24. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis and pairwise basis functions for SysAdmin
problem with independent bidirectional topology.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1525

than Objective-IP, still in signiﬁcantly less time; and (c) in the case of the Factory problem (Fig. 20), AMP only can solve
one instance, while Objective-IP can solve the rest within the time limit with much lower error. These results lead us to
conclude that Objective-IP consistently gives an error at least 2–3× lower than AMP and sometimes runs as fast as the AMP
solution, while in other cases running slower.

8.2.4. Results summary

Over all problems, given the unpredictable performance of AMP (which has no error guarantees and often does not
ﬁnish within the time limit) and the consistently worse performance of APRICODD-IP compared to Objective-IP, Objective-IP
stands out as the more reliable option: it offers guaranteed error bounds and empirically it offers consistently lower error
rates (the lowest of any algorithm) with overall reasonable running times (if not the fastest).

9. Related work

The Bounded-parameter Markov Decision Process (BMDP) [22] is a special case of an MDP-IP, where the probabilities and
rewards are speciﬁed by constant intervals. Exploiting the speciﬁc structure available in a BMDP given by the intervals,
the algorithm in [22] can directly derive the solution without requiring expensive optimization techniques. Recent solutions
to BMDPs include extensions of real-time dynamic programming (RTDP) [23] and LAO* [24,25] that search for the best
policy under the worst model. The Markov Decision Process with Set-valued Transitions (MDPSTs) [26] is another subclass
of MDP-IPs where probability distributions are given over ﬁnite sets of states. Since BMDP and MDPST are special cases
of MDP-IPs, we can represent both by “ﬂat” MDP-IPs. Then the algorithms deﬁned in this paper clearly apply to both
BMDPs and MDPSTs, however their solutions do not generalize to the factored MDP-IPs we examined in this paper, which allow
a multilinear probability representation resulting from the use of a DCN. Furthermore, MDP-IPs allow for general linear
constraints between probabilities, which are prohibited in interval bounded probability settings like BMDPs. This use of
general linear constraints is particularly useful when we do not know the probabilities, but only relative constraints between
them (e.g., two probabilities in the Traﬃc problem are unknown but highly correlated).

Previous work on “ﬂat” MDP-IPs [6,7,27] focused on credal sets (represented as polytopes) proposed algorithms based
on dynamic programming, but they only solved very small problems. It is important to notice that our factored MDP-IP
model is more expressive than the simple “ﬂat” MDP-IPs referred to in those papers; as we saw in Section 4, the joint DCN
transition probabilities in factored MDP-IPs may be nonlinear, while for ﬂat MDP-IPs, the transition probability for any next
state, given a previous state and action, can only be trivially linear (a single parameter pi ).

As we have discussed in Section 8, it is interesting to note that if we allow only interval bounds on the parameters in
the CPTs of the DCN of the factored MDP-IP then the result is still a more expressive model than a “ﬂat” MDP-IP or BMDP,
i.e., the transition expression for any next state given a previous state and action can be a multilinear expression of (cid:10)p.
Consequently, to deﬁne Flat Value Iteration for the comparative analysis from the previous section, we note that we already
needed to slightly extend previous work to allow for multilinear expressions in the transition probability tables required to
match the expressivity of factored MDP-IPs.

A ﬁnal piece of work that is related with MDP-IPs is a two-player zero-sum alternating Markov Game [28] (a.k.a. a Stochastic
Game [29]). This is a subset of “ﬂat” MDP-IPs if intermediate state variables are introduced to represent opponent actions
and the parameters specify the distribution over opponent actions is allowed to vary in the full interval [0, 1]. However,
it might be computationally wasteful to use a “ﬂat” or factored MDP-IP algorithm to solve a Stochastic Game since a
minimization over a ﬁnite set of opponent actions would likely be computationally cheaper than a (nonlinear) optimization
over the probability parameters required in an MDP-IP solution. Hence, it seems more computationally advantageous to use
specialized algorithms for the solution of ﬁnite action Stochastic Games to exploit the speciﬁc structure found there than to
attempt to use any of the more general-purpose MDP-IP algorithms presented here.

Finally, probability trees were also used to represent convex sets of probabilities associated to intervals to obtain posterior
intervals of probability [30]. Probability trees can compactly represent context-speciﬁc independence (CSI), but as we saw in
Section 5, our parameterized ADDs are DAGs that not only exploit CSI but also shared function structure. Additionally, we
used PADDs to represent general probability expressions (multilinear for the case of factored MDP-IPs), not just probability
intervals.

10. Concluding remarks

Motivated by the real-world need to solve MDPs with uncertainty in the transition model, we made a number of novel
contributions to the literature in this article. In Section 4, we introduced the factored MDP-IP model based on Dynamic
Credal Networks (DCNs). In Section 5, we contributed the novel parameterized ADD (PADD) data structure containing leaves
with parameterized expressions; we showed how to eﬃciently obtain a minimal canonical representation of a PADD; and
we showed how to eﬃciently perform a variety of unary and binary operations on PADDs. In Section 6, we contributed
the exact factored MDP-IP solution algorithm SPUDD-IP and showed how to eﬃciently make use of the PADD in all steps
of this factored MDP-IP value iteration algorithm. The resulting SPUDD-IP algorithm yielded up to two orders of magnitude
speedup over existing value iteration techniques for MDP-IPs.

1526

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

To further enhance the SPUDD-IP algorithm, in Section 7, we contributed two novel approximate value iteration ex-
tensions: APRICODD-IP and Objective-IP. While APRICODD-IP is the obvious extension based on previous work, it did not
speciﬁcally target the main source of time complexity for solving MDP-IPs — calls to the nonlinear solver during MDP-IP
value iteration. Based on this observation, we developed an alternate and novel approximation method that directly ap-
proximated the objective of multilinear solver calls, proving the theoretical correctness of this innovative bounded error
approximation approach and substantially reducing the number of nonlinear solver calls and thus running time of approx-
imate value iteration. In Section 8, we performed comparisons of the above algorithms to a previously existing “ﬂat” value
iteration algorithm as well as a state-of-the-art approximate multilinear programming (AMP) solver for MDP-IPs.

Altogether these novel contributions — and particularly their culmination in the Objective-IP algorithm — enable the
(bounded approximate) solution of factored MDP-IPs that can scale orders of magnitude beyond existing ﬂat value iteration
approaches to MDP-IPs and yield substantially lower errors than other existing approximate MDP-IP solvers like approximate
multilinear programming (AMP) that have no a priori error guarantees and depend on appropriate basis function generation
algorithms.

For future work, we note that PADDs represent the tip of the iceberg in the use of advanced decision diagram techniques
for solving factored MDP-IPs. Following the success of the Aﬃne extension of ADDs for solving factored MDPs [31] with
additive and multiplicative structure, it would be interesting to extend this technique to PADDs to exploit the same structure
in MDP-IPs. Such advances would ideally reduce the running time of solutions for factored MDP-IP problems like Traﬃc
that contains signiﬁcant additive structure in their reward deﬁnition and might be amenable to even further exploitation of
factored MDP-IP problem structure.

Finally, we note that the exploration of objectives other than maximin optimality for factored MDP-IPs would also be
interesting. Although the maximin criteria works ﬁne in a domain with many imprecise parameters (like in the SysAdmin
domain we have used in our experiments), we observe that for a problem with large imprecision in terms of very loose
constraints (e.g. 0.1 (cid:2) pi j (cid:2) 0.9), the maximin criterion may be too adversarial — it may reﬂect a worst-case that would
be extremely unlikely in practice. Hence, future work might also examine other methods of handling transition uncertainty,
such as a Bayesian approach [32], and determine whether factored MDP-IPs and PADDs could enhance solution approaches
for those alternate criteria.

Acknowledgements

This work was performed while the ﬁrst author was visiting NICTA. NICTA is funded by the Australian Government as
represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council
through the ICT Centre of Excellence program. This work has also been supported by the Brazilian agencies FAPESP (under
grant 2008/03995-5) and CAPES.

References

[1] M.L. Puterman, Markov Decision Processes, Wiley Series in Probability and Mathematical Statistics, John Wiley and Sons, New York, 1994.
[2] C. Boutilier, S. Hanks, T. Dean, Decision-theoretic planning: Structural assumptions and computational leverage, JAIR 11 (1999) 1–94.
[3] J. Hoey, R. St-Aubin, A. Hu, C. Boutilier, SPUDD: Stochastic planning using decision diagrams, in: Proceedings UAI, Morgan Kaufmann, 1999, pp. 279–

288.

[4] R. St-Aubin, J. Hoey, C. Boutilier, APRICODD: Approximate policy construction using decision diagrams, in: Proceedings NIPS, MIT Press, 2000, pp. 1089–

1095.

[5] C. Guestrin, D. Koller, R. Parr, S. Venkataraman, Eﬃcient solution algorithms for factored MDPs, JAIR 19 (2003) 399–468.
[6] J.K. Satia, R.E. Lave Jr., Markovian decision processes with uncertain transition probabilities, Oper. Res. 21 (1970) 728–740.
[7] C.C. White III, H.K. El-Deib, Markov decision processes with imprecise transition probabilities, Oper. Res. 42 (4) (1994) 739–749.
[8] T. Dean, K. Kanazawa, A model for reasoning about persistence and causation, Comput. Intell. 5 (3) (1990) 142–150.
[9] K.V. Delgado, L.N. de Barros, F.G. Cozman, R. Shirota, Representing and solving factored Markov decision processes with imprecise probabilities, in:

Proceedings ISIPTA, Durham, United Kingdom, 2009.

[10] D.P. Bertsekas, J.N. Tsitsiklis, An analysis of stochastic shortest path problems, Math. Oper. Res. 16 (3) (1991) 580–595.
[11] F.G. Cozman, Credal networks, Artiﬁcial Intelligence 120 (2000) 199–233.
[12] P. Walley, Statistical Reasoning with Imprecise Probabilities, Chapman and Hall, London, 1991.
[13] A. Nilim, L. El Ghaoui, Robust control of Markov decision processes with uncertain transition matrices, Oper. Res. 53 (5) (2005) 780–798.
[14] R. Shirota, F.G. Cozman, F.W. Trevizan, C.P. de Campos, L.N. de Barros, Multilinear and integer programming for Markov decision processes with impre-

cise probabilities, in: Proceedings ISIPTA, Prague, Czech Republic, 2007, pp. 395–404.

[15] F.G. Cozman, Graphical models for imprecise probabilities, Internat. J. Approx. Reason. 39 (2–3) (2005) 167–184.
[16] C. Boutilier, N. Friedman, M. Goldszmidt, D. Koller, Context-speciﬁc independence in Bayesian networks, in: Proceedings UAI, 1996, pp. 115–123.
[17] R.I. Bahar, E.A. Frohm, C.M. Gaona, G.D. Hachtel, E. Macii, A. Pardo, F. Somenzi, Algebraic decision diagrams and their applications, in: Proceedings

ICCAD, IEEE Computer Society Press, Los Alamitos, CA, USA, 1993, pp. 188–191.

[18] R.E. Bryant, Symbolic Boolean manipulation with ordered binary-decision diagrams, ACM Comput. Surv. 24 (3) (1992) 293–318.
[19] R.E. Bryant, Graph-based algorithms for Boolean function manipulation, IEEE Trans. Comput. 35 (8) (1986) 677–691.
[20] N.L. Zhang, D. Poole, A simple approach to Bayesian network computations, in: Proceedings of the Tenth Canadian Conference on Artiﬁcial Intelligence,

1994, pp. 171–178.

[21] C.F. Daganzo, The cell transmission model: a dynamic representation of highway traﬃc consistent with the hydrodynamic theory, Transport. Res.

B 28 (4) (1994) 269–287.

[22] R. Givan, S. Leach, T. Dean, Bounded-parameter Markov decision processes, Artiﬁcial Intelligence 122 (2000) 71–109, (39).
[23] O. Buffet, D. Aberdeen, Robust planning with LRTDP, in: Proceedings IJCAI, 2005, pp. 1214–1219.
[24] S. Cui, J. Sun, M. Yin, S. Lu, Solving uncertain Markov decision problems: an interval-based method, in: Proceedings ICNC (2), 2006, pp. 948–957.

K.V. Delgado et al. / Artiﬁcial Intelligence 175 (2011) 1498–1527

1527

[25] M. Yin, J. Wang, W. Gu, Solving planning under uncertainty: quantitative and qualitative approach, in: Proceedings IFSA (2), 2007, pp. 612–620.
[26] F.W. Trevizan, F.G. Cozman, L.N. de Barros, Planning under risk and knightian uncertainty, in: Proceedings IJCAI, 2007, pp. 2023–2028.
[27] J.A. Bagnell, A.Y. Ng, J.G. Schneider, Solving uncertain Markov decision processes, Tech. rep., Carnegie Mellon University, 2001.
[28] M.L. Littman, Markov games as a framework for multi-agent reinforcement learning, in: Proceedings ICML, Morgan Kaufmann, 1994, pp. 157–163.
[29] L.S. Shapley, Stochastic games, Proc. Natl. Acad. Sci. USA 39 (1953) 327–332.
[30] A. Cano, S. Moral, Using probability trees to compute marginals with imprecise probabilities, Internat. J. Approx. Reason. 29 (1) (2002) 1–46.
[31] S. Sanner, D. McAllester, Aﬃne algebraic decision diagrams (AADDs) and their application to structured probabilistic inference, in: Proceedings IJCAI,

2005, pp. 1384–1390.

[32] M.O. Duff, Optimal learning: Computational procedures for Bayes-adaptive Markov decision processes, Ph.D. thesis, University of Massachusetts,

Amherst, January 2002.

