Artiﬁcial Intelligence 172 (2008) 1731–1751

Contents lists available at ScienceDirect

Artiﬁcial Intelligence

www.elsevier.com/locate/artint

The combination of multiple classiﬁers using an evidential reasoning
approach
Yaxin Bi a,∗

, Jiwen Guan b, David Bell b

a School of Computing and Mathematics, University of Ulster at Jordanstown, Co Antrim, BT37 0QB, UK
b School of Computer Science, The Queen’s University of Belfast, Belfast, BT7 1NN, UK

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 4 June 2007
Received in revised form 12 June 2008
Accepted 12 June 2008
Available online 19 June 2008

Keywords:
Ensemble methods
Dempster’s rule of combination
Evidential reasoning
Evidential structures
Combination functions

1. Introduction

In many domains when we have several competing classiﬁers available we want to
synthesize them or some of them to get a more accurate classiﬁer by a combination
function. In this paper we propose a ‘class-indifferent’ method for combining classiﬁer
decisions represented by evidential structures called triplet and quartet, using Dempster’s
rule of combination. This method is unique in that it distinguishes important elements
from the trivial ones in representing classiﬁer decisions, makes use of more information
than others in calculating the support for class labels and provides a practical way to
apply the theoretically appealing Dempster–Shafer theory of evidence to the problem of
ensemble learning. We present a formalism for modelling classiﬁer decisions as triplet
mass functions and we establish a range of formulae for combining these mass functions
in order to arrive at a consensus decision.
In addition we carry out a comparative
simplet and dichotomous structure and also compare two
study with the alternatives of
combination methods, Dempster’s rule and majority voting, over the UCI benchmark data,
to demonstrate the advantage our approach offers.

© 2008 Elsevier B.V. All rights reserved.

The idea characterizing ensemble learning is to learn and retain multiple classiﬁers and combine their decisions in some
way in order to classify new instances [36]. The attraction of this approach in supervised machine learning is based on
the premise that a combination of classiﬁers is often more accurate than an individual classiﬁer. A theoretical explanation
of its success is that different classiﬁers offer complementary information about instances to be classiﬁed which could be
harnessed to improve the performance of the individual classiﬁers [27].

Generally speaking a successful ensemble method depends on two components: a set of appropriate classiﬁers and a
combination method, function or scheme [30]. Classiﬁers assign single classes or sets of classes to a new instance along
with respective numeric values as decisions, and a combination function merges these decisions in some way to determine
a ﬁnal decision—usually by voting among the decisions.

Ensemble classiﬁers can be generated in different ways. A typical approach is to use a single learning algorithm to
operate on different subsets of attributes or instances of the training data, as done in bagging [9] and boosting [11,21],
and in derivatives such as random forests [10] or the random subspace method for constructing decision tree forests [26].
Another approach is to use different learning algorithms to operate on a single data set [4,8,32]. Among any set of individual
classiﬁers, some are more accurate for a given task and others are less accurate. However there is often not a dominant

* Corresponding author.

E-mail addresses: y.bi@ulster.ac.uk (Y. Bi), j.guan@qub.ac.uk (J. Guan), da.bell@qub.ac.uk (D. Bell).

0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.
doi:10.1016/j.artint.2008.06.002

1732

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

one for the complete data distribution. By taking account of the strengths of classiﬁers through combination functions, the
performance of the best individual classiﬁer can be improved [12].

Kuncheva [28] roughly characterizes combination methods, based on the forms of classiﬁer outputs, into two categories.
In the ﬁrst category, the combination of decisions is performed on single class labels, including majority voting and Bayesian
probability, which have been extensively examined in the ensemble literature [18,27,40,49]. The second category is con-
cerned with the utilization of continuous values corresponding to class labels. One set of methods, often called class-aligned
methods, is based on using the same class labels from different classiﬁers in calculating the support for class labels, regard-
less of what the support for the other classes is. This method includes linear sum and order statistics, to which considerable
effort has been devoted [25,27,47,48,51]. Another method, called stacked generalization or meta-learning, is to use continu-
ous values of class labels as a set of features to learn a combination function in addition to a set of classiﬁers [19,46,54]. An
alternative group of methods, which are called class-indifferent methods, is to make use of as much information as possible
obtained from both single classes and sets of classes in calculating the support for each class [28].

Class-aligned methods and class-indifferent methods are both based on continuous values of class labels in calculating
the support for class labels. A distinction between them is, however, that the latter takes impacts from different classes
into account in determining the support for a class that permits the presence of uncertainty information—as happens when
an instance is classiﬁed into different classes by different classiﬁers. Several related studies are presented in the literature,
where class-indifferent methods utilize single classes and sets of classes [16,39,49]. Class-indifferent methods for combining
decisions in the form of a list of ordered decisions have not been intensively studied and are poorly understood. In particular,
little is known about the value of evidential reasoning methods for combining truncated lists of ordered decisions [5,8].

In this study we consider a class-indifferent approach to combining classiﬁers using Dempster’s rule of combination. Our
focus is on generating classiﬁers using different learning methods to manipulate a single data set, and the combination
of classiﬁers is modeled as a process of reasoning under uncertainty. We model each output given by classiﬁers on new
instances as a list of contender decisions and reduce it to subsets of 2 and 3 decisions, respectively, which are then repre-
sented by the evidential structures of
triplet and quartet [5–8]. We ﬁrst establish a formalism for combining triplets and
quartets using Dempster’s rule of combination to constrain the ﬁnal decision, and then we empirically and analytically ex-
amine the effect of different sizes of decision lists on the combination of classiﬁers. Furthermore we justify the assumption
we make that modelling classiﬁer results as independent bodies of evidence is sensible.

The advantages of our approach are summarized as follows. The ﬁrst advantage is that our method makes use of a wide
range of evidence items in classiﬁcation to make the ﬁnal decision. The idea is inspired by the observation that if only
the ‘best’ single class labels are selected on the basis of their corresponding values, valuable information contained in the
discarded labels may be lost. Arguably, the potential loss of support from the other classes should be avoided by utilizing
this support information in the decision making process. The evidence structures, such as the triplet, are able to distinguish
the important classes from the trivial ones and incorporate the best-supported class, the second best-supported class, and
the rest of the classes which are treated in terms of ignorance within the process of decision making. The second advantage
is that these evidence structures provide an eﬃcient way for combining many pieces of evidence since they break down
a large list of contender decisions into smaller, more tractable subsets. Like the simplet and dichotomous structures [2,44],
our method deals well with a long-standing criticism saying that the evidence theory does not translate easily into practical
applications due to the computational complexity of combining multiple pieces of evidence.

To validate our method and illustrate its power, we have carried out numerous comparative experiments over the UCI
data sets [3]. We experimentally compare the triplet and quartet with the alternatives of simplet, dichotomous structure
and the full list of decisions. We also make a comparison between Dempster’s rule and majority voting in combining
classiﬁers. During the course of classiﬁer combination, another important issue, namely the extent of agreement reached
on classiﬁcation decisions among classiﬁers, is assessed by means of κ statistics, and the associative property of combining
triplets is also experimentally examined. Finally to explain our empirical ﬁndings, we present an investigation into the
calculation process of Dempster’s rule which provides an insight into the reason for superiority of our method.

The rest of the paper is organized as follows. Section 2 presents the representation of classiﬁer outputs and the idea
of class-independent methods. Section 3 reviews the Dempster–Shafer theory of evidence. Section 4 presents a review of
several related studies with a focus on the alternative structures of simplet and dichotomous structure previously used in
combining classiﬁers. The rationale of the evidential structure of the triplet, and the associated formulae, are presented
in Section 5. The combination functions of Dempster’s rule with different evidential structures and majority voting for
combining classiﬁers are evaluated and the experimental settings and results are detailed in Section 6. Section 7 presents
a discussion about the advantage of the triplet and the quartet over the alternatives. Section 8 gives a justiﬁcation for the
independence of evidence derived from classiﬁer outputs. The concluding summary is given in Section 9.

2. Representation of classiﬁer outputs and combination methods

In supervised machine learning, a learning algorithm is provided with training instances of the form {(cid:3)d1, c1(cid:4), . . . ,
(cid:3)d|D|, cq(cid:4)} (di ∈ D, ci ∈ C, 1 (cid:2) q (cid:2) |C|) for inducing some unknown function f such that f (d) = c. D is the space of at-
tribute vectors and each vector di is in the form (w i1 , . . . , w in ) whose components are symbolic or numeric values; C is
a set of categorical classes and each class ci is in the form of class label. Given a set of training data, a learning algorithm

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1733

Fig. 1. A decision proﬁle for instance d generated by ϕ1(d), ϕ2(d), . . . , ϕM (d).

is aimed at learning a function ϕ—a classiﬁer from the training data. The classiﬁer ϕ is an approximation to the unknown
function f .

Given a new instance d, a classiﬁcation task is to decide, using ϕ, whether instance d belongs to class ci . In a multi-class

assignment, we denote such a process as a mapping:

ϕ : D → C × [0, 1],

(1)
where C × [0, 1] = {(ci, si) | ci ∈ C, 0 (cid:2) si (cid:2) 1}, si is a numeric value that can be in different forms, such as a similarity score,
a class-conditional probability (prior posterior probability) or other measure, depending on the types of learning algorithms
used. It represents the degree of support or conﬁdence in the proposition that instance d is assigned to class ci . The greater
the value for class ci , the greater the conﬁdence we have that the instance belongs to that class. Without loss of generality,
we denote the classiﬁer output by ϕ(d) = {s1, . . . , s|C|}—a general representation of classiﬁer outputs.

From the representation ϕ(d), alternative forms of outputs can be derived. For example, we could rank all class labels
according to their continuous values in descending order. By choosing a single label at the top of the ranked list—the single
label with maximal value in the classiﬁer output ϕ(d), we can assign a unique label or a label subset to instance d as a
classiﬁcation decision. Alternatively, we rearrange the process of assigning a unique label to instance d as a mapping of each
pair (cid:3)d, ci(cid:4) to a Boolean value true (T ) or false (F ) in terms of the oracle output [29]. If value T is assigned to (cid:3)d, ci(cid:4), that
means a decision is made that the proposition of instance d belonging to class ci is true, whereas value F indicates the
proposition is false. These alternatives can be seen as the output information at the ﬁnal stage of classiﬁcation.

Given an ensemble of classiﬁers, ϕ1, ϕ2, . . . , ϕM , if each classiﬁer outputs only a single class label for instance d, the
results of classiﬁers can be combined using majority voting (weighted) [31] or Naive Bayes [49], for example. More generally,
if each classiﬁer produces multiple classes as output for instance d—a numeric score vector (list) that is represented as ϕi(d)
below, all these vectors can then be organized into a matrix called a decision proﬁle (DP) as depicted in Fig. 1 [28]. There
are several different ways that the combination of classiﬁer outputs can be carried out.

ϕi(d) =

si j

(cid:2)

(cid:3)
(cid:3) 1 (cid:2) j (cid:2) |C|

(cid:4)

, 1 (cid:2) i (cid:2) M.

(2)

One of the most commonly used combination methods is to calculate the support for class c j using only the DP’s jth
column, i.e. s1 j, s2 j, . . . , sM j , regardless of what the support for the other classes is. We call such a method a class-aligned
method. Some examples of this are linear sum [51], order statistics: minimum, maximum and median [48], and probabilistic
product and sum rules [27]. Alternatively, the combination of classiﬁer outputs can be performed on an entire decision
proﬁle, or on the selected information in order to constrain a class decision. We refer to this alternative group of methods
as class-indifferent methods.

There exist several related contributions to this subject, including the combination of neural network classiﬁers using
Dempster’s rule [41] and the combination of neural network classiﬁers derived from different feature sets using Dempster’s
rule [1]. In a broad sense, these studies take a similar approach to calculating the support for classes. That is, for each clas-
siﬁer against each class, a mean vector (called a reference vector) is generated and organized into a matrix called a decision
template, denoted by DT i . For M classiﬁers and |C| classes, |C| decision templates with M × |C| dimensions are formed.
Given a decision proﬁle DP(d), the closeness between DP(d) and DT i , 1 (cid:2) i (cid:2) |C|, is computed using different proximity
measures such as Euclidean distance and cosine function, the class with the largest support is assigned to instance d.

In our work, the concept of class-indifferent methods is slightly different from the one aforementioned. We neither
generate decision templates nor use an entire decision proﬁle to compute the degrees of support for every class. Instead we
select 2 and 3 classes from each ϕ(d) according to their numeric values, and restructure these into a new list composed of
three and four subsets of classes of C respectively, which are represented by the evidential structures of triplet and quartet.
With a triplet, for example, the ﬁrst subset contains the class with the largest value of conﬁdence, and the second contains
the second largest valued class, and the third one is the whole set of classes C . In this way, the decision proﬁle as illustrated
in Fig. 1 will be restructured into a triplet or quartet decision proﬁle and each column no longer corresponds to the same
class. The degree of support for each class is computed through combining all triplets or quartets in a decision proﬁle. We
will detail our method in later sections.

1734

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

3. Dempster–Shafer (DS) theory of evidence

Fig. 2. Representation of belief interval.

The DS theory of evidence has been recognized as an effective method for coping with uncertainty or imprecision
embedded in evidence used in a reasoning process. It is often viewed as a generalization of Bayesian probability theory, by
providing a coherent representation for ignorance (lack of evidence) and also by discarding the insuﬃcient reasoning principle.
The DS theory is well suited to a range of decision making activities. It formulates a reasoning process as pieces of evidence
and hypotheses, and subjects these to a strict formal process in order to infer conclusions from the given uncertain evidence,
avoiding human subjective intervention to some extent.

In the DS theory, which is also referred to as evidence theory, evidence is represented in terms of evidential functions
and ignorance. These functions include mass functions, belief functions, and plausibility functions [44]. Any one of these conveys
the same information as any of the others.

Deﬁnition 1. Let Θ be a ﬁnite nonempty set, called the frame of discernment. Let [0, 1] be an interval of numeric values.
A mapping function m : 2Θ → [0, 1] is called a mass function if it satisﬁes:

1) m(∅) = 0,

2)

(cid:5)

X⊆Θ

m( X) = 1.

A mass function is a basic probability assignment (bpa) to all subsets X of Θ . A subset A of a frame Θ is called a focal

element or focus of a mass function m over Θ if m( A) > 0 and A is called a singleton if it is a one-element subset.

Deﬁnition 2. A function bel : 2Θ → [0, 1] is called a belief function if it satisﬁes:

1) bel(∅) = 0,

2) bel(Θ) = 1

and for any collection A1, A2, . . . , An (n (cid:3) 1) of subsets of Θ ,
(cid:6) (cid:7)

(cid:5)

bel( A1 ∪ A2 ∪ · · · ∪ An) (cid:3)

(−1)

|I|+1bel

(cid:8)

Ai

.

I⊆{1,2,...,n},I(cid:10)=∅

i∈I

One of the attractive features of the DS theory is that the belief function is in contrast to conventional probability, where
the inequality is replaced by an equality. With this function, a plausibility function can be deﬁned as pls( A) = 1 − bel( A).
Notice that a belief function gathers all of the support that a subset A gets from all of the mass functions of its subsets,
whereas a plausibility function is the difference between 1 and all of the support of A’s complement subsets. The difference
pls( A) − bel( A) represents the residual ignorance, denoted by ignorance( A) (or ign( A)). This gives another important feature
of DS—the representation of what is precisely known and what remains unknown. Fig. 2 presents an intuitive representation
of supporting a subset A by its belief function and plausibility function along with ignorance.

Deﬁnition 3. Let m1 and m2 be two mass functions on the frame of discernment Θ , and for any subset A ⊆ Θ , the orthogonal
sum ⊕ of two mass functions on A is deﬁned as

m( A) = (1/N)

(cid:5)

m1( X)m2(Y ),

X,Y ⊆Θ, X∩Y = A

(3)

(cid:9)

where N = 1 −
X∩Y =∅ m1( X)m2(Y ) and K = 1/N is called the normalization constant of the orthogonal sum m1 ⊕ m2. The
orthogonal sum is a fundamental operation of evidential reasoning and it is often called Dempster’s rule of combination
(Dempster’s rule, for short). There are two conditions governing when it is used to combine mass functions.

The ﬁrst condition is that N (cid:10)= 0, i.e. if N = 0, the orthogonal sum does not exist.
The second is that two mass functions must be independent of each other; that is they must represent independent
opinions or evidence sources relative to the same frame of discernment. Notice that Dempster’s rule simply involves two
mass functions, it does not tell us whether one mass function is independent of another, nor is it able to rule out the
dependence between two mass functions. Therefore, the context of applying Dempster’s rule and the effect of combining
two mass functions depend very much on how evidence sources are treated or modelled as mass functions, rather than on
accounting for their relation alone [24,43].

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1735

3.1. Simple support function

In some situations, when the structure of evidence is taken into account mass functions can be signiﬁcantly simpliﬁed.
The simplest form of a mass function is called a simple support function which is limited to providing a degree of support
for a single proposition A ⊆ Θ , which is referred to as a simplet structure or simplet. This structure provides no support at
all for any other propositions discerned by a frame of discernment Θ [44].

Formally, a mass function is said to be simple if there is a non-empty subset A ⊆ Θ such that

m( A) = s,

m(Θ) = 1 − s,

m(Θ − A) = 0,

where 0 < s (cid:2) 1, the subset A is a focus of m and s is called the degree of support and the mass function in this form is
called a simple support function focused on A.

A simple support function is the belief function bel of a simple mass function m. Thus,

(cid:5)

bel( A) =

m( X) = m( A)

X⊆ A

for all A ⊆ Θ .

Let m1, m2, . . . , mn be simple mass functions with the common focus A and respective degrees of support s1, s2, . . . , sn.
Then m1 ⊕ m2 ⊕ · · · ⊕ mn is still a simple mass function with the same focus A, and support degrees for A and others are
as follows:

(m1 ⊕ m2 ⊕ · · · ⊕ mn)( A) = 1 −

(cid:10)

(1 − si),

(m1 ⊕ m2 ⊕ · · · ⊕ mn)(Θ) =

i=1,2,...,n
(cid:10)

(1 − si),

i=1,2,...,n

(m1 ⊕ m2 ⊕ · · · ⊕ mn)(Θ − A) = 0.

Separable mass functions include both simple mass functions and orthogonal sums of simple mass functions. They are
based on both homogeneous (with the same focuses) and heterogeneous evidence, where different subsets of the frame of
discernment can be referenced from different evidence sources.

3.2. Dichotomous function

A mass function m is said to be a dichotomous function if the only possible focal elements of m are A, Θ − A, Θ for
some A ⊆ Θ . A special case occurs when A is a singleton {x} ⊆ Θ . In such a situation, a dichotomous mass function m has
no focuses other than {x}, Θ − {x}, Θ for some x which is referred to as a dichotomous structure [2].

Let Θ = {x1, x2, . . . , x|Θ|}. Suppose that

for every i = 1, 2, . . . , |Θ|,

there is a dichotomous mass function mi :

mi({xi}), mi(Θ − {xi}), mi(Θ) and mi({xi}) + mi(Θ − {xi}) + mi(Θ) = 1. We view these quantities as follows:

• mi({xi}) is the degree of support for {xi };
• mi(Θ − {xi}) is the degree of support for the refutation of {xi }; and
• mi(Θ) is the degree of the support not assigned for or against the proposition {xi }.

Barnett has proposed a technique based on dichotomous mass functions instead of general mass functions. It means
that instead of potentially exponential time complexity function in deriving evidence combination, the computation of
dichotomous mass functions involves only the 3 particular subsets {x}, Θ − {x}, Θ for each x ∈ Θ ; the general mass functions
have to enumerate all 2

subsets of Θ .

Barnett’s approach is to consider the entire orthogonal sum for evidence bodies which have the structure m1 ⊕ m2 ⊕
· · · ⊕ m|Θ|. These are precisely those evidence spaces which are separable into exactly |Θ| dichotomous mass functions
m1, m2, . . . , m|Θ|, and the time complexity of combining these mass functions is linear. Guan and Bell [22,23] generalized
this to consider the general orthogonal sum as explained below.

|Θ|

Let Θ = {x1, . . . , x|Θ|}. Suppose that for each xi ∈ Θ , there are li dichotomous mass functions of repeated focus:
i (Θ) = 1; where i = 1, 2, . . . , |Θ|; j = 1, 2, . . . , ls; s = l1, . . . , lk;
i ({xi}), m

m
1 (cid:2) k (cid:2) |Θ|; 0 (cid:2) l1, . . . , lk. The task now is to calculate quantities associated with

i (Θ − {xi}) + m

i (Θ − {xi}), m

i ({xi}) + m

i (Θ); m

j

j

j

j

j

j

m = m1
1

(cid:11)

⊕ · · · ⊕ ml1
(cid:14)
1

(cid:12)(cid:13)

· · · ⊕ m1
k

(cid:11)

lk
⊕ · · · ⊕ m
,
(cid:14)
(cid:12)(cid:13)
k

l1 items

lk items

(4)

1736

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

where l1 + · · · + lk = n, and n is the number of masses to be summed which may be greater than |Θ|. In Eq. (4), the calcu-
lation of combining n dichotomous mass functions is divided into two parts. The ﬁrst part is to combine the mass functions
with repeated focal elements. The second part is to combine the mass functions for all focuses. A method of combining
these mass functions was studied in [22,23], and it has been shown that the computational complexity of combining such
mass functions is linear.

4. Existing methods

In this section we ﬁrst develop a model to unify the tasks of combining the outputs of multiple classiﬁers in the con-

ceptual framework of Dempster–Shafer theory and then look at several DS-based studies.

In the supervised learning domain, classiﬁers can assign one or more classes to each instance. The key assumption
underlying our approach here is a single class assignment where each instance belongs to one and only one class. This
assumption suggests that only the one-element subsets in 2C will be of semantic interest and they will be used to represent
propositions. By using the DS terminology, given a frame of discernment C = {c1, . . . , c|C|}, evidence derived from classiﬁers
concerns speciﬁc individual classes, tending to support singleton classes of C , and the other subsets of 2C are not particularly
meaningful. Therefore the powerset 2C for all the propositions could be reduced to the subsets that contain only individual
classes making up a frame of discernment C itself.

Formally let M be the number of classiﬁers ϕ1, . . . , ϕM and let C = {c1, . . . , c|C|} be a set of classes. For any instance
d ∈ D, each classiﬁer produces an output vector ϕi(d). Classifying d means assigning it into one class in C , i.e., deciding
among a set of |C| hypotheses: d belongs to ck, k = 1, . . . , |C|, according to ϕi(d). In DS terms, C is referred to as a frame of
discernment, and the classifying process is regarded as one which decides the true value the proposition of that instance d
belongs to ck according to the knowledge ϕ(d). ϕ(d) can be regarded as a piece of evidence that represents the degrees of
our support or belief for the proposition. Instead of 100% certainty, it only expresses some part of our belief committed to
{ck} ∈ 2C and the rest of our belief which cannot be directly derived from ϕ(d) and the negation of the proposition remains
unknown or indiscernible. In the DS formalism, such a situation is regarded as ignorance, and belief (mass) functions provide
us with an effective way to express it. This is one of the attractive features of DS-based methods.

The merit of assuming the single class assignment is that it signiﬁcantly eases the application of DS theory to practical
evidential functions in a general context are reduced to |C|. The following sections

problems since the computations of 2
review four DS-based methods for representing evidence and deﬁning mass (belief) functions based on ϕ(d).

|C|

4.1. Xu’s method

Xu et al. [49] discussed several approaches for combining multiple classiﬁers, including majority voting (weighted) and
the Bayesian formalism, and proposed a DS model for combining the results of multiple classiﬁers. Their method treated
classiﬁer outputs as single class labels and deﬁned the sources of evidence for the propositions of interest on the basis of the
performance of classiﬁers in terms of recognition, substitution and rejection rates. The items of evidence were represented
by dichotomous mass functions.

Let D be a training data set, and suppose that the recognition and substitution rates of classiﬁers are denoted by
s. For a new instance d, a piece of evidence ϕi(d) is

s(ϕi(D)), and the rejection rates are given by 1 − (cid:5)i

− (cid:5)i

r

(cid:5)i
r(ϕi(D)) and (cid:5)i
represented by the following mass function:

(cid:16)

(cid:16)

(cid:15)
{ck}
(cid:15)
{¯ck}

mi
k

mi
k

= (cid:5)i
r
= (cid:5)i
s

mi

k(C) = 1 − mi

(cid:16)
(cid:15)
ϕi(d)
(cid:15)

, 1 (cid:2) i (cid:2) M, ∃k ∈
(cid:16)
ϕi(d)
(cid:15)
{ck}

, 1 (cid:2) i (cid:2) M, ∃k ∈
(cid:16)

(cid:15)
{¯ck}

− mi
k

(cid:16)
,

k

(cid:4)

(cid:2)
1, . . . , |C|
(cid:2)

1, . . . , |C|

,
(cid:4)
,

(5)

(6)

(7)

where {¯ck} = C − {ck}. With M pieces of evidence existing, represented by M dichotomous mass functions, the degrees of
support for classes can be calculated through combining these mass functions by formula (3). A ﬁnal class decision for
a given instance is made on selecting the class with the largest degree of support. Notice that such a combination is an
class-indifferent method since the calculation of support for a class ck is not only based on the mass functions which have
the same focus ck, but the support is also impacted by the mass values of the second subset {¯ck} = C − {ck} and the whole
set C , i.e. the ignorance.

In the above deﬁnition, however, the dichotomous mass functions are not directly deﬁned on the basis of the numeric
values of ϕi(d) (see formula (2)). Instead they are deﬁned based on the overall performance of classiﬁers. Therefore for
different instances, the basic probability assignments derived from the classiﬁer outputs—dichotomous mass functions—
are the same. This method ignores the fact that normally a classiﬁer does not have the same performance on different
classes, this might consequently degrade the combined performance of classiﬁers. Nevertheless the proposed method lays
groundwork for formalizing the problem of combining classiﬁer outputs using the dichotomous evidence structure and it
achieved a considerable performance improvement when applied to handwriting recognition.

4.2. Rogova’s method

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1737

Rogova [41] proposed a model for combining the results of neural network classiﬁers using the DS theory. This work
accounted for the relation between classiﬁer outputs and reference vectors as pieces of evidence and developed a general
way to measure the relation using proximity measures. The pieces of evidence are represented by simple support functions.
Formally, let Dk be a subset of a training data set, in which all instances belong to class ck ∈ C and let {ϕi(x)} (x ∈ Dk) be a
set of classiﬁer outputs. A mean vector of {ϕi(x)} is denoted by E i
k called a reference vector for class k. For any instance d,
a general proximity measure used for E i

k and ϕi(d) is deﬁned as follows:

i (cid:2) M, k (cid:2) |C|.

(8)

μi
k

= φ

(cid:15)

(cid:16)
E i
k, ϕi(d)

,

The measure φ can be in different forms—cosine function, Euclidean distance, and so forth. It provides us with evidence
about how likely it is that instance d belongs to class ck. If the output ϕi(d) is far from E i
k, ϕi(d) is considered as providing
very little information regarding the proposition of d being under ck; in that case, φ must therefore take on a ‘small’ value.
On the contrary, if ϕi(d) is close to E i
k belong to the same class.
Simple support functions are given below:

k, one will be much more inclined to believe that d and E i

(cid:16)

(cid:15)
= μi
{ck}
k,
(cid:16)
(cid:15)
{¯ck}
= 1 −

j( j(cid:10)=k)

mi
k
mi

mi

k(C) = 1 − μi
k,
(cid:10)
(cid:16)
(cid:15)
1 − μi
,
r

r=1,r(cid:10)=k

mi

j( j(cid:10)=k)(C) = 1 − mi

j

(cid:16)

(cid:15)
{¯ck}

=

(cid:10)

(cid:15)
1 − μi
r

(cid:16)
.

r=1,r(cid:10)=k

(9)

(10)

The interpretation of the above formulation is that formulae (9) and (10) quantify the relation of ϕi(d) with each class
of reference vector E i
k—pieces of evidence. However, each piece of evidence represents only some part of our belief which is
committed to {ck} and it does not point to any other particular hypotheses. Thus the rest of our belief cannot be distributed
to anything else other than C —the whole frame of discernment. Based on this formulation, for a unseen instance, a decision
proﬁle is remodeled by formula (11) and the kth (1 (cid:2) k (cid:2) |C|) column of DP corresponds to M simple support functions
with two focal elements, viz. {ck} and the whole set C .

DP(d) =

(cid:2)
mi
j

(cid:3)
(cid:3) 1 (cid:2) j (cid:2) |C|

(cid:4)
, 1 (cid:2) i (cid:2) M.

For such a decision proﬁle, the combination will be performed on each column k using Dempster’s rule, resulting in a

degree of support for class ck as follows:

(cid:16)

(cid:15)
{ck}

=

(cid:15)
m1
k

⊕ · · · ⊕ mM
k

(cid:16)(cid:15)

(cid:16)
.

{ck}

m

(12)
The ﬁnal class c = argmax{m({ck}) | 1 (cid:2) k (cid:2) |C|} is assigned to instance d. The combination performed in this way using

Dempster’s rule can be seen as a class-aligned method.

Rogova’s work is based on an original idea proposed by Mandler and Schurmann [35] and it extended that work in two
aspects regardless of how reference vectors are generated. The ﬁrst aspect was to introduce a generic form of proximity
measure φ, allowing different distance measures to be applied to compute class-conditional probabilities—basic probability
assignments. The second was to obtain more support for ck by combining two simple mass functions given by formulae (9)
and (10). However, the issue with this extension is the use of the opponent of ck, i.e., ¯ck, which was not explicitly speciﬁed.
If {¯ck} = C − {ck}, then formula (10) seems not to make sense and the combination mi
j (Rogova used the notation of
k
mk ⊕ m¯k) is questionable.

⊕ mi

4.3. Al-Ani’s method

A similar attempt to apply the DS theory of evidence to combining neural network outputs was carried out by Al-Ani
et al. [1]. This method also treated the distance between a reference vector and a classiﬁer output as a piece of evidence. But
the difference from the previous work is in the way it obtains reference vectors. It ﬁrst initializes reference vectors for each
class, and then iteratively uses training instances to optimize reference vectors through minimizing the mean square errors
between combined classiﬁer outputs and the target outputs, ensuring the optimized reference vectors can be achieved.
Finally the distance between the optimized reference vectors and classiﬁer outputs is deﬁned as a piece of evidence and
is represented by a simple support function. Let E i
k be an optimized reference vector. For any instance d, each classiﬁer
produces an output vector ϕi(d) (1 (cid:2) i (cid:2) M), a simple support function is deﬁned below:

mi
k

(cid:15)
{ck}

j

(cid:16)

=

μi
k(cid:9)|C|
j=1 μi
gi
j=1 μi
= exp(−(cid:14)E i
k

(cid:9)|C|

mi

k(C) =

,

+ gi

,

+ gi

j
− ϕi(d)(cid:14)2) and gi is a coeﬃcient to be tuned. The combination method is the same as Rogova’s,
where μi
k
which is static. However, the way of obtaining reference vectors through minimizing the overall mean square error makes

(11)

(13)

(14)

1738

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

the process of combining classiﬁers trainable, which may lead to better performance than a static combination scheme, but
with the additional cost for training as well as additional training data.

4.4. Denoeux’s method

Denoeux [17] proposed an evidence theoretic k-nearest neighbors (kNN) method for classiﬁcation problems based on
the DS theory. Unlike the methods above which were designed for combining classiﬁers in ensemble learning, this method
focuses on a single classiﬁer ϕ in classifying new instances, by accounting for distances from their neighbors to determine
class labels. Formally, let D be a training data set, for instance, d ∈ D and let Φ be a set of the k-nearest neighbors of
d according to some distance measures (e.g., Euclidean distance). Classifying d means assigning it to one of the classes
ck ∈ C based on the weights of representative classes of its neighbors. Thus the distance between d and neighbor di ∈ Φ is
considered as a piece of evidence to support a proposition about the class membership of d. The evidence is represented by
a simple support function as follows:

(cid:16)

(cid:15)
{ck}

= φ(d, di),

mi
mi(C) = 1 − φ(d, di),
mi( A) = 0, ∀ A ∈ 2C \

di ∈ Φ,

(cid:2)

(cid:4)

,

{ck}, C

(15)

(16)

(17)

where φ was suggested to be exp(−γ (μi)2) and μi = (cid:14)d − di(cid:14). This formulation appears to be similar to Rogova’s method
without considering what speciﬁc distance measures are used. However, Denoeux’s method is quite different from Rogova’s
in the sense that the former computes distances in feature space, whereas the latter works in the decision (classiﬁer output)
space. Therefore the decision proﬁle of this method is slightly different from the one given in Fig. 1. For the decision matrix
each neighbor is treated as a row and the column corresponds to classes, and the combination using Dempster’s rule is
carried out on the column. In order to improve the classiﬁcation accuracy, an effective decision procedure was proposed to
determine the optimal or near-optimal parameter values from the data by minimizing an error function [50].

Denoeux’s method shows the advantage of permitting a clear distinction between the presence of conﬂicting
information—as happens when an instance is close to several neighbors from different classes—and incomplete information—
when an instance is far away from any instances in its neighborhood. It proves to be very competitive with the standard
kNN methods. A similar idea has been adapted to a neural network classiﬁer by Denoeux in [16].

5. Triplet mass function

In this section we describe the development of a key evidence structure—the triplet and its formulation.
Starting by analyzing the computational complexity of combining multiple pieces of evidence, we consider how a more
eﬃcient method for combining evidence can be established. Given M pieces of evidence represented by formula (2), the
computational complexity of combining these pieces of evidence using Eq. (3) is dominated by the number of elements in C
and the number of classiﬁers M. In the worst case, the time complexity of combining M pieces of evidence is O (|C|M−1).
One way of reducing the computational complexity is to reduce the number of pieces of evidence being combined, so that
the combination of evidence is carried by a partition of the frame of discernment C , with fewer focal elements than C , but
including possible answers to the question of interest. The partition can thus be used in place of C when the computations
of the orthogonal sum are carried out [42]. For example, a dichotomous structure can be used to partition the frame
of discernment C into two subsets ϑ1 and ϑ2, where there are a number of mass functions that represent evidence in
favor of ϑ1 and against ϑ2, along with the lack of evidence— ignorance. It has been shown that Dempster’s rule can be
implemented in such a way that the number of computations increases only linearly with the number of elements in C
if the mass functions being combined are focused on the subsets where ϑ1 is singleton and ϑ2 is the complement of ϑ1,
i.e., O (|C|) [22]. Another approach to reducing the computational complexity of Dempster’s rule is to approximate the
calculation of belief functions in a coarsened frame of discernment, which is detailed in [15].

The partitioning technique enables a large problem to be broken up into several smaller and more tractable problems.
However, a fundamental issue in applying this technique is how to select elements that contain the possibly correct answers
to the propositions corresponding to C .

An intuitive way is to select the element with the highest degree of conﬁdence. Indeed, since the classiﬁer outputs
approximate class posteriori probabilities, selecting the maximum probability reduces to selecting the output that is the
most ‘certain’ of the decisions. This could be justiﬁed from two perspectives. First, the probability assignments given in for-
mula (2) give quantitative representation of judgments made by classiﬁers on the propositions; the greater their values, the
more likely these decisions are correct. Thus selecting the maximum distinguishes the trivial decisions from the important
ones. Second, the combination of decisions with the lower degrees of conﬁdence may not contribute to the performance in-
crease of combined classiﬁers, but only make the combination of classiﬁer’s decisions more complicated [48]. The drawback
of selecting the maximum, however, is that the combined performance can be reduced by a single dominant classiﬁer that
repeatedly provides high conﬁdence values. Contenders with the higher values are always chosen as the ﬁnal classiﬁcation
decisions, but some of these may not be correct.

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1739

To cope with the deﬁciency resulting from the maximal selection, we propose to take the second maximum decision into
account in combining classiﬁers. Its inclusion not only provides valuable information contained in the discarded class labels
by the maximal selection for combining classiﬁers, but this also to some extent avoids the deterioration of the combined
performance caused by the errors resulting from a single dominant classiﬁer that repeatedly produces high conﬁdence
values. We propose a novel structure—a triplet—partitioning a list of decisions ϕ(d) into three subsets.

Deﬁnition 4. Let C be a frame of discernment, where each choice ci ∈ C is a proposition that instance d is classiﬁed in
category ci . Let ϕ(d) = {s1, s2, . . . , s|C|} be a list of scores, a localized mass function is deﬁned by a mapping function,
m : 2C → [0, 1], i.e. a bpa to ci ∈ C for 1 (cid:2) i (cid:2) |C| as follows:

(cid:16)

(cid:15)
{ci}

m

=

si(cid:9)|C|

j=1 s j

,

where 1 (cid:2) i (cid:2) |C|.

(18)

This mass function expresses the degrees of belief with regard to the choices of classes to which a given instance could

belong. With this deﬁnition, we rewrite formula (2) as ϕ(d) = m({c1}), m({c2}), . . . , m({c|C|}).

Deﬁnition 5. Let C be a frame of discernment, and let {u}, {v} be focal singletons and m be a respective mass function. An
expression of the form Y = (cid:3){u}, {v}, C(cid:4) is deﬁned as a triplet, it satisﬁes

(cid:16)

(cid:15)
{u}

m

(cid:16)

(cid:15)
{v}

+ m

+ m(C) = 1

and mass function m is called a triplet mass function.

To obtain triplet mass functions, we deﬁne an outstanding rule below.

Deﬁnition 6. Let C be a frame of discernment and ϕ(d) = {m({x1}), m({x2}), . . . , m({xn})}, where |n| (cid:3) 2, an outstanding rule
is a focusing operator, denoted by mσ , which breaks up ϕ(d) and makes
(cid:16)
, . . . , m

{u} = argmax m

(cid:16)
, m

(cid:16)(cid:4)(cid:16)
,

(19)

(cid:15)(cid:2)(cid:15)

(cid:15)(cid:2)

(cid:15)
{x2}

(cid:15)
{xn}
{x1}
(cid:3)
(cid:4)(cid:16)
(cid:3) x ∈ {x1, . . . , xn} − {u}
{x}
,
(cid:16)
+ mσ (C) = 1.

{v} = argmax m
(cid:16)
(cid:15)
mσ
{v}

(cid:15)
{u}

+ mσ

(20)

(21)

Clearly mσ is a triplet mass function and it is also referred as a two-point mass function. Based on this notation, for-

mula (2) is simply rewritten as formula (22)
(cid:16)
, mσ

(cid:4)
(cid:16)
, mσ (C)

(cid:15)
{v}

(cid:15)
{u}

(cid:2)
mσ

ϕi(d) =

1 (cid:2) i (cid:2) M.

(22)

For simplicity, we write ϕi(d) = {m({u}), m({v}), m(C)}.
With the above deﬁnition of a triplet, it is easy to illustrate that it meets the two conditions given in Deﬁnition 1. We

now show that the mass m(C) indeed represents ignorance.

Given a triplet (cid:3){u}, {v}, C(cid:4), and m({u}) + m({v}) + m(C) = 1, we let A = {u} and A = {v}, then we have

bel( A) =

(cid:5)

m( X) = m

(cid:15)
{u}

(cid:16)
,

pls( A) =

X⊆{u}
(cid:5)

X∩{u}(cid:10)=∅

m( X) = m

(cid:16)

(cid:15)
{u}

+ m(C),

ignorance( A) = pls( A) − bel( A) = m

(cid:16)

(cid:15)
{u}

+ m(C) − m

(cid:16)

(cid:15)
{u}

= m(C).

From the above formulation it can be seen that u gives the maximum of our quantitative judgments, representing the
class with the highest degree of conﬁdence (support) in a list of decisions. It implies that the decision has a strong possibility
of being correct. v represents the class with the second highest degree of conﬁdence in the decision list. This decision is
less likely to be correct than u. However, its support is important in combining decisions since making a maximal selection
may lose valuable information contained in the discarded class labels. Moreover, including v could avoid deterioration of
the combined performance caused by a single error of a classiﬁer. Apart from the support for u and v, a certain amount
of conﬁdence still remains unassigned, which is assigned to the entire set of classes—it is committed to the frame of
discernment C . The triplet evidence structure can be intuitively interpreted as follows. If a classiﬁer cannot successfully
assign an instance to the correct class in two occasions, then it is not likely for the classiﬁer to classify this instance
correctly at all. We use the ignorance concept to capture important information in such a situation.

1740

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

To develop formulae for combining two triplet mass functions, we need to consider the relation between two pairs of

singletons in any two triplets.

Suppose we are given two triplets (cid:3){x1}, { y1}, C(cid:4) and (cid:3){x2}, { y2}, C(cid:4) where xi, yi ∈ C (i = 1, 2), and the associated triplet
mass functions m1 and m2. The enumerative relations between any two pairs of focal points {x1}, { y1} and {x2}, { y2} are
illustrated below:

1. Two focal points equal: 1) if {x1} = {x2} and { y1} = { y2}, then {x1} ∩ { y2} = ∅ and { y1} ∩ {x2} = ∅; 2) {x1} = { y2} and
{ y1} = {x2}, then {x1} ∩ {x2} = ∅ and { y1} ∩ { y2} = ∅. In this case, the combination of two triplet functions involves three
different focal elements.

2. One focal point equal: 1) if {x1} = {x2} and { y1} (cid:10)= { y2} then {x1} ∩ { y2} = ∅, { y1} ∩ {x2} = ∅ and { y1} ∩ { y2} = ∅; 2) if
{x1} (cid:10)= {x2} and { y1} = { y2}, then {x1} ∩ { y2} = ∅, {x2} ∩ { y1} = ∅ and {x1} ∩ {x2} = ∅; 3) if {x1} = { y2} and { y1} (cid:10)=
{x2} then {x1} ∩ {x2} = ∅, { y1} ∩ {x2} = ∅ and { y1} ∩ { y2} = ∅; 4) if { y1} = {x2} and {x1} (cid:10)= { y2} then {x1} ∩ {x2} = ∅,
{x1} ∩ { y2} = ∅ and { y1} ∩ { y2} = ∅. In this case, the combination of two triplet functions involves four different focal
elements.

3. Totally different focal points: if {x1} (cid:10)= {x2}, { y1} (cid:10)= { y2}, {x1} (cid:10)= { y2} and { y1} (cid:10)= {x2}, then {x1} ∩ {x2} = ∅, { y1} ∩ { y2} = ∅,

{x1} ∩ { y2} = ∅ and { y1} ∩ {x2} = ∅, so the combination involves ﬁve different focal elements.

The above three different cases require different formulae for combination. In cases 2) and 3), the combinations of
multiple triplet functions cannot be iteratively performed since we are interested only in combining two-point focuses.
However, by applying the outstanding rule (see Deﬁnition 6), the combined results of any two triplets can be transformed
to a triplet mass function. In the following sections, we seek general formulae for combining triplet mass functions based
on the three different cases and present our combination algorithm.

5.1. Two focal points equal

Considering the case where two focal singletons {x1}, { y1} in one triplet are equal to {x2}, { y2} in another triplet, i.e.,

x1 = x2, y1 = y2 (x1 (cid:10)= y1) and xi, yi ∈ C (i = 1, 2), we have two triplet mass function m1 and m2 along with

(cid:16)

(cid:16)

(cid:15)
{x}
(cid:15)
{x}

m1

m2

(cid:16)

(cid:16)

(cid:15)
{ y}
(cid:15)
{ y}

+ m1
+ m2

+ m1(C) = 1,
+ m2(C) = 1.

First, we need to show the combination of m1 ⊕ m2 does exist and then establish formulae to compute their combination. To
ensure the combinability of triplet mass functions, we need only show under what conditions two triplet mass functions m1
−1 (cid:10)= 0. By using formula (3) to combine m1 and m2, we can
and m2 are not in conﬂict, i.e., the normalization factor K
−1 must be greater than zero, i.e.,
obtain K
0 < 1 − m1({x})m2({ y}) − m1({ y})m2({x}), thus m1({x})m2({ y}) + m1({ y})m2({x}) < 1. Under this condition, we establish the
formulae for computing the combination of two triplet mass functions below:

−1 = 1 − m1({x})m2({ y}) − m1({ y})m2({x}), to make K

−1 (cid:10)= 0 to be true, K

(cid:16)

{x}

= K

(cid:15)
(cid:16)
(m1 ⊕ m2)
m2
(cid:15)
(cid:16)
(m1 ⊕ m2)
m2
(m1 ⊕ m2)(C) = K m1(C)m2(C),

(cid:15)
{x}
(cid:15)
{ y}

(cid:17)
m1
(cid:17)
m1

= K

{ y}

(cid:16)

(cid:16)

(cid:15)
{x}
(cid:15)
{ y}

(cid:15)
{x}
(cid:15)
{ y}

(cid:16)
m2(C) + m1(C)m2
(cid:16)
m2(C) + m1(C)m2

(cid:15)
{x}
(cid:15)
{ y}

(cid:16)(cid:18)

+ m1
(cid:16)
+ m1

,
(cid:16)(cid:18)

,

where

−1 = 1 −

K

(cid:5)

X∩Y =∅

m1( X)m2(Y ) = 1 − m1

(cid:15)
{x}

(cid:16)
m2

(cid:15)
{ y}

(cid:16)

(cid:15)
{ y}

(cid:16)
m2

(cid:15)
{x}

(cid:16)
.

− m1

5.2. One focal point equal

(23)

(24)

(25)

(26)

We now consider the case where given two triplet mass functions m1 and m2 and two pairs of singletons {x}, { y} and
{x}, {z} ( y (cid:10)= z and x, y, z ∈ C), a focal element in one triplet is equal to one in another triplet. Following the procedure
given in Section 5.1, we can show that m1, m2 are combinable if and only if the following condition is held:
(cid:15)
{x}

(cid:15)
{ y}

(cid:15)
{ y}

(cid:15)
{x}

(cid:15)
{z}

(cid:15)
{z}

< 1.

(cid:16)

(cid:16)

(cid:16)

(cid:16)
m2

(cid:16)
m2

(cid:16)
m2

+ m1

+ m1

m1

Thus by the orthogonal sum operation, the general formulae for computing the combination of any two triplet mass

(cid:16)

{x}

functions are given below:
(cid:17)
= K
m1
(cid:15)
= K m1
{ y}
= K m1(C)m2

(cid:15)
(cid:15)
(m1 ⊕ m2)
{x}
(cid:15)
(cid:16)
(m1 ⊕ m2)
m2(C),
(cid:15)
(cid:15)
{z}
(m1 ⊕ m2)
(m1 ⊕ m2)(C) = K m1(C)m2(C),

(cid:16)
m2

(cid:15)
{x}

{ y}
(cid:16)

(cid:16)
,

{z}

(cid:16)

(cid:16)

(cid:15)
{x}

(cid:16)
m2(C) + m1(C)m2

(cid:15)
{x}

(cid:16)(cid:18)

,

+ m1

(27)

(28)

(29)

(30)

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1741

where

(cid:16)

(cid:15)

(cid:16)

K

− m1

(cid:15)
{z}

(cid:15)
{x}

(cid:16)
m2

(cid:16)
m2

(cid:15)
{ y}

−1 = 1 − m1

(cid:16)
m2
(31)
It is noted that the new mass function m1 ⊕ m2 is no longer a triplet mass function. It now involves four different focal
elements {x}, { y}, {z} and C . For more than two triplet functions, the combining process cannot proceed iteratively since we
are interested only in two-point focuses. However, by applying the outstanding rule, the combined result can be transformed
to a new triplet mass function. We detail the computational steps below.
By Deﬁnition 6, we have a new function (m1 ⊕ m2)σ as follows:

(cid:15)
{ y}

(cid:15)
{x}

− m1

{z}

(cid:16)
.

(m1 ⊕ m2)σ

+ (m1 ⊕ m2)σ

+ (m1 ⊕ m2)σ (C) = 1.

(cid:16)

(cid:15)
{x

(cid:16)}

(cid:16)

(cid:15)
{ y

(cid:16)}

(cid:15)
{ y}

(cid:16)

= f ( y); m1 ⊕ m2

(cid:16)

(cid:15)
{z}

= f (x).

(cid:16)

m1 ⊕ m2

To obtain (m1 ⊕ m2)σ , we assume
(cid:15)
{x}
Then for focal element {x
(cid:15)
{x

= f (x); m1 ⊕ m2
(cid:16)} we have

(cid:16)
= f (x
(cid:16)} = argmax( f (x), f ( y), f (z)).

m1 ⊕ m2

where {x

(cid:16)}

),

(cid:16)

For focal element { y

(cid:16)} we have
(cid:16)

(cid:16)

(cid:15)
{ y

(cid:16)}

m1 ⊕ m2

= f ( y
(cid:16) = argmax( f (t) | t ∈ ({x, y, z} − {x

),

(cid:16)})).

where y

For focal element C we have
(cid:16)
(m1 ⊕ m2)σ (C) = 1 − f (x

) − f ( y

(cid:16)

).

5.3. Completely different focal points

(32)

(33)

(34)

Finally, let us examine the case where no focal element is common to two triplets. As indicated previously, the combi-
nation of two such triplet mass functions will involve ﬁve different focal elements. Let m1, m2 be two triplet functions, and
{x}, { y} and {u}, {v} (x (cid:10)= y, x (cid:10)= u and y (cid:10)= v, and x, y, u, v, y ∈ C) be two pairs of focal elements along with the following
conditions:
(cid:15)
{x}
(cid:15)
{u}

+ m1(C) = 1,
+ m2(C) = 1.

(cid:15)
{ y}
(cid:15)
{v}

+ m1
+ m2

m1

m2

(cid:16)

(cid:16)

(cid:16)

(cid:16)

Following the patten of the previous sections, it can be shown that m1, m2 are combinable if and only if the following

constraint holds:

(cid:15)
{x}

(cid:16)
m2

(cid:16)

(cid:15)
{u}

(cid:15)
{x}

(cid:16)
m2

(cid:15)
{v}

(cid:16)

+ m1

(cid:15)
{ y}

(cid:16)
m2

(cid:15)
{u}

(cid:16)

(cid:15)
{ y}

(cid:16)
m2

(cid:15)
{v}

(cid:16)

+ m1

+ m1

< 1.

m1

Given the above condition we derive the formulae for computing each focal element below:

(cid:16)

(cid:16)

(cid:15)
(m1 ⊕ m2)
{x}
(cid:15)
{ y}
(m1 ⊕ m2)
(cid:16)
(cid:15)
(m1 ⊕ m2)
{u}
(cid:15)
{v}
(m1 ⊕ m2)

(cid:16)

(cid:15)
{ y}
(cid:15)
{ y}

= K m1
= K m1
= K m1(C)m2
= K m1(C)m2

(cid:16)
m2(C) = f (x),
(cid:16)
m2(C) = f ( y),
(cid:15)
= f (u),
{z}
(cid:15)
= f (v),
{z}

(cid:16)

(cid:16)

where

(cid:5)

−1 = 1 −

K

m1( X)m2(Y )

X∩Y =∅
(cid:15)
{x}

= 1 − m1

(cid:16)
m2

(cid:15)
{u}

(cid:16)

(cid:15)

(cid:16)
m2

(cid:15)
{v}

(cid:16)

{x}

− m1

(cid:15)
{ y}

(cid:16)
m2

(cid:15)
{u}

(cid:16)

(cid:15)
{ y}

(cid:16)
m2

(cid:15)
{v}

(cid:16)

,

− m1

− m1

(35)

(36)

(37)

(38)

(39)

However, the same situation occurs as in Section 5.2, the combination of m1, m2 is no longer a triplet mass function, it
now involves ﬁve focal elements {x}, { y}, {u}, {v}, C , therefore further combinations with more triplet functions are invalid
in this context. To obtain a new triplet mass function, there is a need to apply the outstanding rule to the combined results.

More speciﬁcally, by Deﬁnition 6, we can obtain a new function (m1 ⊕ m2)σ as follows:

(m1 ⊕ m2)σ

+ (m1 ⊕ m2)σ

+ (m1 ⊕ m2)σ (C) = 1.

(cid:16)

(cid:15)
{x

(cid:16)}

(cid:16)

(cid:15)
{ y

(cid:16)}

Then for focal element {x
(cid:15)
{x

m1 ⊕ m2

(cid:16)
= f (x

(cid:16)}

(cid:16)

),
(cid:16)} = argmax( f (x), f ( y), f (u), f (v)).

where {x

(cid:16)} we have

(40)

1742

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

//combine them by formulae (23)–(26)

(cid:16) ∈ T

(cid:16)} do

ct ← ct ⊕ t

if (two focuses equal in t and ct) then {

endif
else if (one focus equal in t and ct) then

1 set T a set of triplet mass functions
2 ct: holds the combined results of triplet mass functions
3 set ct ← t
4 for each t ∈ T \ {t
5
6
7
8
9
10
11
12
13
14
15
16 endfor
17 return ct

ct ← ct ⊕ t
ct ← ctσ

ct ← ct ⊕ t
ct ← ctσ

endelseif
else

endelse

//combine them by formulae (27)–(31)
//transform it a new triplet by formulae (32)–(34)

//combine them by formulae (35)–(39)
//transform the combined results to a new triplet by formulae (40)–(42)

Algorithm 1. Combine multiple triplet mass functions.

For focal element { y

(cid:16)} we have
(cid:16)

(cid:16)

(cid:15)
{ y

(cid:16)}

m1 ⊕ m2

= f ( y

),
(cid:16) = argmax({ f (t) | t ∈ ({x, y, u, v} − {x

(cid:16)})}).

where y

Finally, for focal element C we have
) − f ( y

(cid:16)
(m1 ⊕ m2)σ (C) = 1 − f (x

(cid:16)

).

(41)

(42)

We have shown that any two triplet mass functions are combinable if the conditions hold and we have established the
formulae for computing the combination of two triplet mass functions. These formulae provide a way to not only compute
the combinations of two triplet mass functions eﬃciently but also help us develop a general algorithm for combining
multiple triplet mass functions in a complex situation where the evidence sources of triplets are independent of each other.
Suppose we are given M triplet mass functions m1, m2, . . . , mM , by using the algorithm below they can be combined in
any order due to Dempster’s rule being both commutative and associative. That means we can arrange these triplet mass
functions into a certain order based on three cases mentioned above. By repeatedly applying the outstanding rule at each
computational step of combining two triplet mass functions, the results can be transformed to a new triplet mass function.
Formula (43) is a pairwise orthogonal sum calculation for combining any number of triplets which can be performed by the
above algorithm. Its time complexity is approximately O (2 × |C|) and the ﬁnal decision is a class with the largest degree of
support among all the classes.

m = m1 ⊕ m2 ⊕ · · · ⊕ mM =

[m1 ⊕ m2] ⊕ · · · ⊕ mM

.

(43)

(cid:17)

(cid:17)
· · ·

(cid:18)(cid:18)

5.4. Focusing on more points—quartet

In the previous sections we have presented the formulation of triplet functions and developed formulae for combining
them. Similarly, we can consider three-point focuses, four-point focuses, or more focuses. In general, when a mass function
has more than four focal singletons, we can use the outstanding rule σ to focus on 3 points in terms of quartet. Let m be a
mass function with focal singletons {x1}, {x2}, . . . , {xn}, n (cid:3) 4, and n (cid:2) |C|. Then the focusing operator σ is used to obtain m
as follows:
(cid:15)
mσ
{u}

+ mσ (C) = 1,

(cid:15)
{v}

(cid:15)
{z}

+ mσ

+ mσ

(cid:16)

(cid:16)

(cid:16)

where

{u} = argmax m

(cid:15)(cid:2)(cid:15)

(cid:15)(cid:2)

(cid:15)(cid:2)

(cid:16)(cid:4)(cid:16)
,

(cid:16)
, m

(cid:16)
, . . . , m

(cid:15)
(cid:15)
{x1}
{xn}
{x2}
(cid:4)(cid:16)
{x} | x ∈ {x1, . . . , xn} − {u}
,
(cid:4)(cid:16)
{x} | x ∈ {x1, . . . , xn} − {u, v}
,
(cid:16)
(cid:15)
(cid:15)
{v}
{u}

(cid:15)
{z}

− mσ

− mσ

(cid:16)

(cid:16)

{v} = argmax m

{z} = argmax m
mσ (C) = 1 − mσ

(47)
and {u}, {v}, {z} are three-point focuses. The structure of the quartet is conceptually simple, but it has the added advantage
that it can be used to handle the case where the classiﬁer results are diverse. For this case, the theoretical analysis is
clearly more complicated than that of the triplet structure. In addition to the properties of mass function and ignorance,
the corresponding analysis also needs to address, in each case, the situation where the focuses are ordered by their masses
for each of the two pieces of evidence being combined. For example, we need to consider the cases where all three focuses
are the same, two focuses are identical, where just one focus is shared, and where there are no focuses in common. More
details about the extension from triplet to quartet can be found in [6].

(44)

(45)

(46)

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1743

Table 1
General description of the datasets

Dataset

Instance

Class

anneal
audiology
balance
car
glass
autos
iris
letter
heart
segment
soybean
wine
Zoo

798
200
625
1728
214
205
150
20 000
303
1500
683
178
101

6
23
3
4
7
6
3
26
5
7
19
3
7

Attribute

Categorical

Numerical

32
69
4
6
0
10
0
0
8
0
35
0
15

6
0
0
0
9
15
4
16
5
19
0
13
2

Table 2
General description of the thirteen learning algorithms

No

Classiﬁer

Description

0

1

2
3

4
5
6
7
8
9
10

11
12

AOD

NaiveBayes

SMO
IBk

IB1
KStar
DecisionStump
J48
RandomForest
DecisionTable
JRip

NNge
PART

Perform classiﬁcation by averaging over all of a small space of alternative naive-Bayes-like models that have weaker
(and hence less detrimental) independence assumptions than naive Bayes
The Naive Bayes classiﬁer using kernel density estimation over multiple values for continuous attributes, instead of
assuming a simple normal distribution
Sequential minimal optimization algorithm for training a support vector classiﬁer using polynomial or RBF kernels
A instance-based learning algorithm. It uses a simple distance measure to ﬁnd the training instance closest to the
given test instance, and predicts the same class as this training instance
The IBk instance-based learner with K = 1 nearest neighbors, in order to offset KStar with a maximally local learner
The K instance-based learner using all nearest neighbors and an entropy-based distance
Building and using a decision stump, but it is not used in conjunction with a boosting algorithm
Decision tree induction, a Java implementation of C4.5
Constructing random forests for classiﬁcation
A decision table learner
A propositional rule learner—a Java implementation of Ripper. It repeats incremental pruning to produce error re-
duction
Nearest neighbor-like algorithm using non-nested generalized exemplars
Generating a PART decision list for classiﬁcation

6. Experimental evaluation

6.1. Experimental settings

In our experiments, we used thirteen data sets downloaded from the UCI machine learning repository [3]. All the selected
data sets have at least three or more classes as required by the evidential structures. The details about these data sets can
be found in Table 1.

For generating individual (base) classiﬁers, we used thirteen learning algorithms which are taken from the Waikato
Environment for Knowledge Analysis (Weka) version 3.4 (see Table 2). These algorithms were simply chosen on the basis
of the performance over three data sets which were randomly picked. They can make up various ensembles of classiﬁers.
Parameters used for each algorithm were set at the default settings. Detailed description of these algorithms can be found
in [55].

To reﬂect the ensemble performance faithfully and to avoid overﬁtting to some extent, the experiments were performed
on a three partition scheme using a ten-fold cross validation. We therefore divided each of the data sets into 10 mutually
exclusive sets. For each fold, after excluding the test set, the training set was further subdivided into 70% for a new training
set and 30% for a validation set. Apart from evaluating the performance of individual classiﬁers, the validation set was
used to select the best combination of classiﬁers. The performance of combining selected classiﬁers using the DS and MV
(majority voting) combining schemes was evaluated on the testing set.

Seven groups of experiments are reported here, which were carried out individually and in combination across all the

thirteen data sets. These are:

• evaluating the performance of the 13 algorithms shown in Table 2,
• experimenting with various combinations of individual classiﬁers using DS, in which the classiﬁer outputs are repre-

sented by triplet and quartet functions, respectively,

1744

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

Fig. 3. An example: the combination of two individual classiﬁers ϕ1 and ϕ2 with the triplet structure in a fold of a ten-fold cross validation, T1 is a number
of instances within a fold of training data.

• examining the performance of combining 13 individual classiﬁers in the three different orders of decreasing, corre-

sponding increasing and increasing using DS,

• experimenting with combinations of individual classiﬁers represented by the dichotomous structure using DS, where
the dichotomous mass functions were deﬁned on the basis of the performance of classiﬁers in terms of recognition,
substitution and rejection rates [49],

• conducting experiments on combining the individual classiﬁers represented by the simplet structure using DS. The
simplet mass functions were deﬁned based on Rogova’s method (see Section 4.2; here classiﬁers were generated by the
learning algorithms shown in Table 2 rather than neural networks),

• experimenting with combinations of classiﬁers using DS, where classiﬁer outputs are represented by the full list of

decisions (contenders),

• experimenting with combinations of individual classiﬁers using MV, in which the individual outputs are single class

labels.

It is noted that the ensemble construction involves 213 combinations of 13 individual classiﬁers for one evidential struc-
ture with one data set in one fold. The computational cost for all the structures using a ten-fold cross validation requires
213 × 13 × 5 × 10 combinations in total. Instead of exhausting all the combinations of classiﬁers, we ranked all the individual
classiﬁers based on their performance and then combined them in decreasing order as suggested in [1,8]. For example, we
took the best individual classiﬁer, and then combined it with the second best, the third best, and so forth, until we achieved
the best accuracy of the combined classiﬁers. During the course of combination, a hybrid order (and non consecutive or-
dering) was also involved in combining classiﬁers in order to ﬁnd out the best combination of classiﬁers. Fig. 3 presents an
example of combining two individual classiﬁers where the classiﬁer outputs are represented by triplets.

To compare the classiﬁcation accuracies between the individual classiﬁers and the ensemble classiﬁers across all the data
sets, we employed ranking statistics in terms of the win/ draw/ loss (W/D/L) record used by Webb [53]. The win/draw/loss
record presents three values, the number of data sets for which classiﬁer A obtained better, equal, or worse than classiﬁer B
with respect to classiﬁcation accuracy. All collected classiﬁcation accuracies were measured by the averaged F -measure
[55]. A paired t-test across all these domains was also carried out to determine whether the differences between the base
classiﬁers and combined classiﬁers (ensembles of classiﬁers) are statistically signiﬁcant at the 0.05 level.

6.2. The basics of Kappa (κ ) statistics

To examine the reliability of the ensemble performance, we performed a κ (Kappa) statistic analysis on the extent (level)
of agreement between the combined classiﬁers under the different evidential structures using DS, and also the extent of the
agreement between majority voting and Dempster’s rule in combining the base classiﬁers.

The κ statistic is the most widely used pairwise method to measure the level of agreement (or disagreement) between
raters/classiﬁers [29]. It can be thought of as chance-corrected proportional agreement [20]. Formally, given two classi-
ﬁers ϕ1 and ϕ2 and a testing data set T , we can construct a global contingency table where entry E i j contains the number
of instances x ∈ T for which ϕ1 = i and ϕ2 = j. If ϕ1 and ϕ2 are identical on the data set, then all non-zero counts will
appear along the diagonal of the table, otherwise there will be a number of counts off the diagonal. Now we deﬁne

μ1 =

μ2 =

(cid:9)

L
i=1 E ii
|T |
(cid:19)

L(cid:5)

L(cid:5)

i=1

j=1

,

(cid:20)

,

E i j
|T |

×

L(cid:5)

j=1

E ji
|T |

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1745

Fig. 4. Comparison of running time among simplet, triplet quartet and dichotomous structures.

where μ1 is the probability that two classiﬁers agree and μ2 is a correction term for μ1, estimating the probability that
the two classiﬁers agree simply by chance. Then the κ statistic is deﬁned as follows:

κ = μ1 − μ2
1 − μ2

κ = 0 when the agreement of two classiﬁers equals that expected by chance, κ = 1 when two classiﬁers agree on all the
testing instances, and negative values of κ mean that an agreement is less than expected by chance.

6.3. Special cases for triplets and quartets

In our experiments, to ensure that Dempster’s rule is properly applied to combine triplet mass functions (and quartet
mass functions), i.e., making sure N (cid:10)= 0 as required in Deﬁnition 3, adjustments have been made for the following three
special cases.

The ﬁrst one is that given two mass functions m1 and m2 along with two pairs of singletons {x1}, { y1} and {x2}, { y2}, and
the whole set C , if x1 (cid:10)= x2, y1 (cid:10)= y2, xi (cid:10)= y j (i, j = 1, 2), and m1({x1}) = 1 or m1({ y1}) = 1 and m2({x2}) = 1 or m2({ y2}) = 1,
then the intersection of m1 and m2 is committed to the empty set ∅, and consequently the condition of N (cid:10)= 0 does not
hold. Thus it is necessary to redistribute masses over the triplets where the uncertainties lie. In this situation, we discount
the masses of xi and y j (i, j = 1, 2) to mi(C) (i = 1, 2) by a small value α which has been defaulted as 0.001 in our
experiments.

The addition of α represents the uncertainty involved in a classiﬁcation process, which can be justiﬁed in two ways:
1) generating classiﬁers is an approximation process, so that the class conditional probabilities estimated by classiﬁers is
not of 100% certainty; 2) α will play a role in improving the combined performance (or at least, in not deteriorating the
combined effect).

The second case is where, as usual, m is a mass function derived from a classiﬁer output, {x} and { y} are a pair of
singletons and C is the whole set of classes. If the classiﬁer output for an instance is a nil one, this means the classiﬁer
is not able to assign a class label to the instance, thus m({x}) = m({ y}) = 0. For such a situation, we reallocate 1 to m(C).
Making m(C) = 1 can be regarded as the representation of uncertainty in classifying the instance by the classiﬁer; in fact, it
only ensures N (cid:10)= 0 when m is combined with another mass function m

and it does not affect the value of m ⊕ m

The last case is where we are given a resulting triplet mass function m together with singletons {x}, { y}, {z}, and m({x}) =
m({ y}) = m({z}). To approximate m as a new triplet mass function or determine the best focus, we have no criterion for
identifying the best choice. All we can do is pick two focuses up at random for constructing a triplet, or randomly take one
as a ﬁnal decision.

.

(cid:16)

(cid:16)

Similar treatments have also been administered for the special cases of quartet mass functions.

6.4. Experimental results

6.4.1. Run time

The ﬁrst experimental result is the comparison of running time required in combining different evidential structures
using DS. As expected, combining simplets was more eﬃcient than the others as there is less computation involved. The
empirical results show the time required for combining triplet functions is 3.5% longer than that of combining simplets.
The time required for combining dichotomous functions is 119.6% longer than that of simplets on average, and the time for
combining quartets is 169.1% longer than that for simplets on average. The comparative results are illustrated in Fig. 4.

From the above results, it can be seen that although the triplet and dichotomous structures both have three focal ele-
ments, the computational time required for combining triplet functions is signiﬁcantly less than that required for combining

1746

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

Table 3
The classiﬁcation accuracy of the best base classiﬁer, the best combined classiﬁers based on the structures of simplet, triplet, quartet, dichotomous structure
and fullist using DS and MV over 13 data sets

Datasets

Anneal
Audiology
Balance
Car
Glass
Autos
Iris
Letter
Heart
Segment
Soybean
Wine
Zoo

Average
W/D/L
Sig win

Individual

Simplet

80.23
48.67
65.67
89.62
65.36
77.59
95.33
92.05
35.48
96.69
95.89
98.90
90.62

79.39

81.57
53.67
63.17
95.40
64.91
78.46
96.67
92.41
36.02
97.48
96.92
100.00
93.61

80.79
11/0/2
7

Triplet

81.57
57.44
63.17
94.29
66.81
79.17
96.67
92.91
37.09
97.28
96.69
100.00
93.61

81.30
12/0/1
7

Quartet

Dichotomous

79.77
57.44
64.44
96.96
64.91
78.08
96.67
92.54
37.03
96.68
96.88
100.00
93.61

81.15
10/0/3
5

80.68
51.97
65.67
91.92
66.26
78.78
96.67
93.41
35.37
97.74
96.85
98.90
93.61

80.60
10/2/1
5

MV

81.14
54.30
62.72
91.75
66.69
77.94
96.67
92.77
34.37
96.55
96.17
98.97
93.61

80.28
10/0/3
4

Fullist

69.88
49.32
21.68
90.03
62.75
66.80
60.71
68.38
34.26
88.40
91.11
96.70
64.39

66.49
–
–

Table 4
The extent of agreement among the best combined classiﬁers under the four structures along with the agree-
ment between combination methods DS and MV

Triplet (DS)
MV

Simplet

0.9325
0.9427

Quartet

0.9396
0.9273

Dichotomous

0.9349
0.9432

Triplet

–
0.9437

Average

0.9357
0.9392

dichotomous functions. This is mainly due to the second element of the dichotomous structure, i.e., it is a subset of the
whole set C which contains only one class less than the whole set of classes C . Examining the calculation process of
Dempster’s rule, it is not diﬃcult to ﬁnd that for the optimized case, combining two dichotomous mass functions requires
(|C| − 2)2 more computations than combining two triplet mass functions using DS.

6.4.2. Performance summary

The seven groups of experimental results are summarized in Table 3. The ﬁrst column lists all the data sets used, the
second column gives the classiﬁcation accuracies of the best individual classiﬁers and the rest of the columns represent the
accuracies of the best combined classiﬁers using DS or MV over the data sets. If the difference between the best combined
classiﬁer and the best individual or base classiﬁer on the same data set is statistically signiﬁcant, then the larger of the two
is shown in bold.

The bottom of the table provides summary statistics comparing the performance of the best base classiﬁers with the
best combined classiﬁers across the data sets. From this summary, it can be observed that the accuracy of the combined
classiﬁers based on the triplet structure using DS is better than any of the ﬁve others on average. It has more wins than
loses over the simplet, quartet, dichotomous structure and the best combined classiﬁers using MV compared to the best
individual classiﬁers. This conspicuous superiority is further supported by the number of statistically signiﬁcant wins—the
triplet has two more than the quartet and dichotomous structure, and three more than MV.

6.4.3. κ statistics

For examining the reliability of the ensemble performance, we selected six data sets (Anneal, Audio, Car, Iris, Wine,
Zoo) where the ensemble performance is statistically signiﬁcant and better than that of the best individuals, and carried
out a pairwise analysis on the level of agreement between the best combined classiﬁers along with the level of agreement
between DS and MV on the testing data. Table 4 shows the statistical results averaged on the six data sets. Within the table,
the ﬁrst row consists of the four best combined classiﬁers, denoted by Simplet, Quartet, Dichotomous structure and Triplet,
each of which corresponds to the six data sets. The ﬁrst column names the two best combined classiﬁers with DS and MV,
simply denoted by Triplet (DS) and MV, respectively. They are associated with the six data sets as well. Each remaining cell
of the table is an averaged κ value, which represents the level of agreement between a pair of the classiﬁers on classifying
the instances of the six data sets. For example, in the cell of (Triplet (DS), Simplet), 0.9325 is the averaged κ value of the
best combined classiﬁers of Triplet and Simplet which agree on classifying the instances across the six data sets. The κ
values presented in Table 4 indicate that the agreement on classifying the instances by the pairs of classiﬁers is substantial
according to the rough guide provided in [20]. This establishes that the performance of these combined classiﬁers is reliable.

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1747

Fig. 5. Performance of different numbers of classiﬁers in combination with different orders: decreasing, corresponding increasing and increasing.

6.4.4. Order of combining classiﬁers under the triplet

As explained in Sections 5.2 and 5.3, combining two triplet mass functions does not in general result in a triplet mass
function. The combined results may need to be approximated to a new triplet using the focusing operator. However such
an approximation may make the combination of triplets no longer meet the associative property of Dempster’s rule. Thus
different orders of combination of classiﬁers may lead to the different combined performance.

To better understand how the chosen order of classiﬁers affects the combined performance in ensemble construction,
we have conducted a further experiment on combining classiﬁers with three different orders of decreasing, corresponding
increasing ( c-increasing) and increasing. The combination process is as follows. We ranked the 13 best individual classiﬁers
in decreasing and increasing orders, and then consecutively combined them one by one using Dempster’s rule. With respect
to the c-increasing order, it is a reverse process of each of the corresponding decreasing combinations. For instance, the ﬁrst
decreasing combination is to combine the best classiﬁer with the second best, the second is to combine the best classiﬁer
with the second best and then with the third best; this process will be repeated until all decreasing combinations are
completed. The reverse process of the decreasing combination is, correspondingly, to combine the second best classiﬁer
with the ﬁrst, to combine the third best classiﬁer with the second and then with the best, and so forth.

Fig. 5 illustrates an effect comparison of three orders in combining classiﬁers under the triplet. It can be observed that
the combined classiﬁers with the c-increasing order outperforms the other two, and the performance of the increasing
combination is worst among the three orders of combination. This suggests that combining the better classiﬁers would out-
perform combining the worse classiﬁers. Consider the c-increasing order. It should be noted that although its performance
is better that of the decreasing combination, this mainly occurs on the combination of 9 classiﬁers and more; for the com-
bination of 3 to 8 classiﬁers, the decreasing performance tends to be the same as that of the c-increasing. This experimental
result leads to the conclusion that the order of the classiﬁers in combination has an impact on the performance of the
combined classiﬁers, but it is not dramatic. For both the decreasing and the c-increasing orders, the smaller the number of
classiﬁers to be combined, the less the impact of the combination order.

6.4.5. Ensemble size

Fig. 6 presents the sizes of the best combined classiﬁers across all the data sets. With the different structures, the
construction of these ensembles involve 2–7 classiﬁers, and most of the ensembles are composed of only two classiﬁers.
This result is consistent with some previous studies [1,8,19], but different from others [36,38] where experiments showed
that ensemble accuracy increased with ensemble size and the performance levels out with ensemble sizes of 10–25. The
sizes of ensembles of classiﬁers which are generated using different learning algorithms to operate on a single data set are
not necessarily the same as those of the combined classiﬁers constructed using a single learning method to manipulate
different portions of features or instances.

6.4.6. Contribution to the ensembles

Fig. 7 presents the contribution to the ensemble construction of each individual classiﬁer with the structures of simplet
triplet, quartet and dichotomous structure across all the domains. There are eleven aggregated columns with four different
colors in the ﬁgure. Each column represents the number of a classiﬁer which contributes to the construction of the thirteen
best combined classiﬁers (corresponding to thirteen data sets), and each color presents an evidential structure. For example,
the ﬁrst column indicates that AOD occurs six times over the thirteen ensembles (thirteen data sets) with the simplet and
triplet structures, and seven times over the thirteen ensembles with the quartet and dichotomous structure. It is observed
that the classiﬁers that contribute to four or more ensembles with four structures are AOD, SMO, IBk and KStar. Of these,
AOD is not the best performing individually, but it plays a more important role than the others in constructing the effective
ensembles.

1748

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

Fig. 6. Ensemble size: number of individual classiﬁers involved in the best combined classiﬁers across all the data sets with the evidential structures of
simplet, triplet, quartet and dichotomous structure.

Fig. 7. Number of the individual classiﬁers (0: AOD; 1: NaiveBayes; etc. see Table 2) occurring in all thirteen combined classiﬁers across all the thirteen
data sets with different structures of simplet, triplet, quartet and dichotomous structure.

7. Discussion

The structure of simplet consists of two elements. The ﬁrst is a decision in a list of decisions and the simplet mass
represents the degree of support for that decision, which is derived from a decision proﬁle using a cosine function. The
second is the whole list of decisions C , its mass representing the uncertainty of the singleton element. With this structure,
a list of decisions is associated with a list of simplet mass functions where each of the simplet functions corresponds to
one of the decisions, and both the lists share the same order after ranking. It is easy to illustrate that the combined effect
of two simplet functions will be highly affected by the larger of the simplet masses.

Consider the combination of two simplet mass functions m1 and m2 along with two singletons {x} and { y} in the
case where x and y are not equal. By using formula (3) to combine m1 and m2, x is the better supported decision if
m1({x}) > m2({ y}), otherwise y will be better, and m1(Θ) and m2(Θ) do not affect the combination effect. Thus, we ﬁnd
that which of {x} and { y} becomes the better supported decision depends merely on the mass values of {x} and { y}. This
effect can be generalized to combining two simplet classiﬁers ϕ1 ⊕ ϕ2; that is, the performance of ϕ1 ⊕ ϕ2 is dominated by
a single simplet classiﬁer ϕ1 or ϕ2 which repeatedly provides larger mass functions m1 or m2.

Although the way of deriving mass values for simplets is different from that for triplets and quartets, in the broadest
sense, the triplet and quartet can be regarded as extensions of the simplet structure. The key difference between both the
triplet and the quartet and the simplet is that the former makes use of a wide range of information that is contained in the
second and third best decisions in combining classiﬁers. The performance of combining two triplet classiﬁers, for example,
will be determined by the ﬁrst and second elements along with ignorance. We conjecture that the use of more information
plays an important role in overcoming the problem we identiﬁed earlier—that a single error produced by a single classiﬁer
which repeatedly provides high conﬁdence values of classes can occur when combining simplets.

Now we look at a theoretical justiﬁcation of our claim. We state formally the conditions for the ﬁrst or second decision
in either of two triplets to become the better supported decision. Assume that two triplet functions m1 and m2 fall into the

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1749

category where a pair of singletons {x1}, { y1} is equal to a pair of {x2}, { y2}, i.e., x1 = x2 and y1 = y2 (see Section 5.1). By
using formulae (23)–(26), we have the following inequality when x is the better choice:

(cid:15)
{x}

(cid:16)
m2

(cid:15)
{x}

(cid:16)

m1

(cid:15)
{x}

(cid:16)
m2(C) + m1(C)m2

(cid:15)
{x}

(cid:16)

+ m1

(cid:15)
{ y}

(cid:16)
m2

(cid:15)
{ y}

(cid:16)

(cid:15)
{ y}

(cid:16)
m2(C) + m1(C)m2

(cid:15)
{ y}

(cid:16)
.

+ m1

> m1

Substituting for C in formula (48) and rearranging it, we have the condition for the better support for x:

(cid:16)

(cid:15)
{x}

m1

> 1 −

[1 − m1({ y})][1 − m2({ y})]
[1 − m2({x})]

.

Likewise we can derive the condition for y being the better supported decision:

(cid:16)

(cid:15)
{ y}

m2

> 1 −

[1 − m1({x})][1 − m2({x})]
[1 − m1({ y})]

.

(48)

(49)

(50)

We can obtain the conditions for the other two cases in two triplets in a similar manner.
Formulae (49) and (50) present two conditions for determining which of x and y is the better choice. The ﬁrst condition
indicates that, for example, when x is in the second position of a list of decisions, it can be ranked as the ﬁrst decision when
two triplets are combined as long as this condition is met. This effect provides an insight into the process of combining
triplet classiﬁers where a single classiﬁer cannot dominate the combined performance and the ignorance derived also plays
an important role in deciding the best supported decisions. This is one explanation of the superior performance of the
triplet over the simplet. A similar analysis for the case of combining quartets can be carried out in the same way, obtaining
a possible account of when the quartet outperforms the simplet.

With respect to the dichotomous structure, its drawback is in its way of measuring evidence, where it ignores the fact
that classiﬁers normally do not have the same performance on different classes, which could cause a deterioration in the
performance of the combined classiﬁers (see Section 4.1).

It is not a surprise that the performance of combining classiﬁers in the form of triplets and quartets is better than that
of combining classiﬁers with a full list of decisions. The reason for this is that different individual classiﬁers produce various
distributions of class-conditional probabilities for all the classes. The classes can have various predicted values in the range
between zero and one. When two lists are combined using the orthogonal sum operation, if ϕ1 and ϕ2 produce two lists of
decisions with a similar distribution, then many non-zero values will appear along the diagonal of an intersection table (for
the orthogonal sum calculation). Otherwise there will be a large number of values off the diagonal. In the latter situation,
the combination of the two lists is committed to more conﬂicting class labels. Examining the calculation of orthogonal
sums closely, we ﬁnd that the conﬂict incurred in combining two classiﬁers in the form of a full list is larger than that in
combining two triplet classiﬁers. Such conﬂict could thus result in poor combined performance of classiﬁers, using a full
list. This ﬁnding is also consistent with the result illustrated in Fig. 5 and the previous studies where the combination of
decisions with the lower degrees of conﬁdence may not contribute to an increase of combined performance of classiﬁers,
but only make the combination of decisions more complicated [48].

8. Independence assumption

Making an independence assumption for a set of variables or a set of attribute values is a common practice in many
learning tasks, such as Bayesian belief networks and naive Bayes classiﬁer [37]. Such an assumption dramatically reduces the
complexity of learning classiﬁers and makes the computational process more tractable. This is also true in other applications,
for instance, in the present context of modelling classiﬁer outputs as independent bodies of evidence. The independence
assumption is of practical value, but there is little information available about whether it has substantially deteriorated the
effectiveness of classiﬁers in many applications.

As mentioned in Deﬁnition 3, one condition of using Dempster’s rule is that pieces of evidence to be combined are
independent. However the precise meaning of independence in practice is diﬃcult to specify. In an effort to clarify this,
Dempster [13] explained that “opinions of different people based on overlapping experiences could not be regarded as inde-
pendent sources”. This was subsequently complemented by a statement by Denoeux [14], who said that, “non-overlapping
random samples from a population are clearly distinct items of evidence”. In the present context, a possible argument could
be used against independence of two pieces of evidence derived from two classiﬁer outputs due to the fact that the two
classiﬁer outputs arise from the same sample instance. However, at the same time it can also be argued that two classiﬁer
outputs are distinct, because it is not clear that two classiﬁers have ‘overlapping experiences’ in determining class labels for
an instance.

Somewhat less philosophically, the argument in favor of the independence assumption is that the classiﬁers involved in
combination are generated by the distinct learning algorithms as shown in Table 2. These algorithms are built on different
theories and they use their own mechanisms to search for a characterization of the data. So they do not share “experiences”
in generating classiﬁers, nor is there correlative dependence between the internal structures of the classiﬁers. The inherent
processes of producing outputs by classiﬁers are entirely distinct. This distinctness can be formally interpreted as follows.
Let ϕ1 and ϕ2 be two distinct classiﬁers, for any instance d, ϕ1(d) does not logically imply ϕ2(d), and vice versa, hence
they are mutually unrelated. Consequently as a natural case, denoting e1 as the proposition corresponding to ϕ1(d) and e2

1750

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

to ϕ2(d) (see Section 4), then e1 is independent of e2 and the probability P (e1 ∧ e2) = P (e1) · P (e2) [52]. Therefore the
assumption made for modelling classiﬁer outputs as independent pieces of evidence is sensible.

There is a great deal of debate about the conditions for validly applying Dempster’s rule and the precise meaning of
distinct bodies of evidence in the literature [13,14,33,43,52], and so far there is no conclusive study to these issues. Shafer
[43] pointed out that the task of sorting evidence into independent items is not always easy, and “a theory that directs
us to this task is grappling with the real problems in the assessment of evidence”. Over the past decades progress in
several applications has been suﬃcient to show that although the independent condition is not explicitly speciﬁed in the
applications, with an independence assumption Dempster’s rule still demonstrates its effectiveness [1,6,17,39,49].

It should be emphasized that this study is focused on developing a more eﬃcient and effective computational method
for more convincing practical applications based on the DS theory, rather than on investigating alternative combination rules
for dependent evidence sources and conﬂict management. More detailed discussions of these aspects can be found in recent
studies [14,34].

9. Conclusion and future work

We have described an effective framework built on the Dempster–Shafer theory of evidence for combining multiple
classiﬁers and expert opinions in classiﬁcation and decision making systems. We have also developed a formalism for
triplets and quartets and the formulae for combining classiﬁers represented in
modelling classiﬁer outputs in terms of
the form of triplets that can be extended to the quartet. The distinguishing aspect of our class-indifferent method from
class-aligned methods is in selecting the prioritized class decisions to be combined and in making use of ignorance in
representing unknown and uncertain class decisions.

A range of experiments have been carried out over the thirteen UCI benchmark data sets. Our results show that the
performance of the best combined classiﬁers is better than that of the best individuals on most of the data sets and the
corresponding ensemble sizes are 2–7 where the ensemble sizes of 2 and 3 take 61.5% of the thirteen best ensembles.
The comparative analysis among the structures of simplet, triplet, quartet and dichotomous structure identiﬁes the triplet as
best, and the comparison made between DS and MV shows that DS is better than MV in combining the individual classiﬁers.
We have used the κ statistic to examine the extent of agreement of the different combination methods in deciding
classes for testing instances. The statistics reveal that the classiﬁcation performance achieved by using our DS combination
method under our evidential structures is reliable. However, in this work we did not touch the issue of classiﬁer diversity.
Although most successful ensemble methods encourage diversity to some extent, a recent study on ten diversity measures
raised the question of the what role diversity plays in constructing effective ensembles of classiﬁers and how it could be
measured [29]. To our knowledge there has not been a conclusive study showing which measure of diversity is best for use
in evaluating ensembles. We are working on this and we will discuss our ﬁndings in a future paper. Moreover, we would
like to carry out a comparative study with alternative combination rules in the future. These include a new combination
rule for combining non distinct items of evidence more recently introduced in [14] and the unnormalized combination rule
introduced in [45].

Acknowledgements

The authors would like thank the anonymous reviewers for their detailed constructive comments which have helped us

improve the paper considerably.

References

[1] A. Al-Ani, M. Deriche, A new technique for combining multiple classiﬁers using the Dempster–Shafer theory of evidence, J. Artif. Intell. Res. 17 (2002)

333–361.

[2] J.A. Barnett, Computational methods for a mathematical theory of evidence, in: Proc. of 17th Joint Conference of Artiﬁcial Intelligence, 1981, pp. 868–

875.

[3] C.L. Blake, C.J.E. Keogh, UCI repository of machine learning databases, http://www.ics.uci.edu/mlearn/MLRepository.html.
[4] Y. Bi, D. Bell, J.W. Guan, Combining evidence from classiﬁers in text categorization, in: Proc. of KES04, 2004, pp. 521–528.
[5] Y. Bi, Combining multiple classiﬁers for text categorization using the Dempster–Shafer theory of evidence, PhD thesis, University of Ulster, UK, 2004.
[6] D. Bell, J.W. Guan, Y. Bi, On combining classiﬁers mass functions for text categorization, IEEE Trans. Knowledge Data Engrg. 17 (10) (2005) 1307–1319.
[7] Y. Bi, J.W. Guan, An eﬃcient triplet-based algorithm for evidential reasoning, in: Proc. of the 22nd Conference on Uncertainty in Artiﬁcial Intelligence,

2006, pp. 31–38.

[8] Y. Bi, S.I. McClean, T. Anderson, On combining multiple classiﬁers using an evidential approach, in: Proc. of the Twenty-First National Conference on

Artiﬁcial Intelligence (AAAI’06), 2006, pp. 324–329.

[9] L. Breiman, Bagging predictors, Machine Learning 24 (2) (1996) 123–140.

[10] L. Breiman, Random forests, Machine Learning 45 (1) (2001) 5–32;

T. Dietterich, Machine learning research: Four current directions, AI Magazine 18 (4) (1997) 97–136.

[11] T. Dietterich, An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization,

Machine Learning 1 (22) (1998).

[12] T. Dietterich, Ensemble methods in machine learning, in: Proc. 2nd Int. Workshop on Multiple Classiﬁer Systems MCS2000, LNCS, vol. 1857, 2000,

pp. 1–15.

[13] A.P. Dempster, Upper and lower probabilities induced by a multivalued mapping, Ann. Math. Stat. 38 (1967) 325–339.

Y. Bi et al. / Artiﬁcial Intelligence 172 (2008) 1731–1751

1751

[14] T. Denoeux, Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence, Artif. Intell. 172 (2–3) (2008)

234–264.

[15] T. Denoeux, A. Ben Yaghlane, Approximating the combination of belief functions using the fast Moebius transform in a coarsened frame, Internat. J.

Approx. Reason. 31 (1–2) (2002) 77–101.

[16] T. Denoeux, A neural network classiﬁer based on Dempster–Shafer theory, IEEE Trans. Systems Man Cybernet. A 30 (2) (2000) 131–150.
[17] T. Denoeux, A k-nearest neighbor classiﬁcation rule based on Dempster–Shafer theory, IEEE Trans. Systems Man Cybern. 25 (5) (1995) 804–813.
[18] R.P.W. Duin, D.M.J. Tax, Experiments with classiﬁer combining rules, in: J. Kittler, F. Roli (Eds.), Multiple Classiﬁer Systems, 2000, pp. 16–29.
[19] S. Dzeroski, B. Zenko, Is combining classiﬁers with stacking better than selecting the best one? Machine Learning 54 (3) (2004) 255–273.
[20] J.L. Fleiss, J. Cuzick, The reliability of dichotomous judgments: unequal numbers of judgments per subject, Appl. Psycholog. Meas. 3 (1979) 537–542.
[21] Y. Freund, R. Schapire, Experiments with a new boosting algorithm, in: Machine Learning: Proceedings of the Thirteenth International Conference,

1996, pp. 148–156.

[22] J.W. Guan, D.A. Bell, Evidence Theory and Its Applications, vols. 1–2, Studies in Computer Science and Artiﬁcial Intelligence, vols. 7–8, Elsevier, North-

Holland, 1991–1992.

[23] J.W. Guan, D.A. Bell, Eﬃcient algorithms for automated reasoning in expert systems, in: The 3rd IASTED International Conference on Robotics and

Manufacturing, 1995, pp. 336–339.

[24] R. Haenni, Are alternatives to Dempster’s rule of combination real alternatives?: Comments on “about the belief function combination and the conﬂict

management problem” Lefèvre et al., Information Fusion 3 (3) (2002) 237–239.

[25] L.K. Hansen, P. Salamon, Neural network ensembles, IEEE Trans. Pattern Anal. Machine Intell. 12 (10) (1990) 993–1001.
[26] T.K. Ho, The random subspace method for constructing decision forests, IEEE Trans. Pattern Anal. Machine Intell. 20 (8) (1998) 832–844.
[27] J. Kittler, M. Hatef, R.P.W. Duin, J. Matas, On combining classiﬁers, IEEE Trans. Pattern Anal. Machine Intell. 20 (3) (1998) 226–239.
[28] L. Kuncheva, Combining classiﬁers: Soft computing solutions, in: S.K. Pal, A. Pal (Eds.), Pattern Recognition: From Classical to Modern Approaches, 2001,

pp. 427–451.

[29] L. Kuncheva, C.J. Whitaker, Measures of diversity in classiﬁer ensembles, Machine Learning 51 (2003) 181–207.
[30] A.K. Jain, R.P.W. Duin, J. Mao, Statistical pattern recognition: A review, IEEE Trans. Pattern Anal. Machine Intell. 22 (1) (2000) 4–37.
[31] L. Lam, C.Y. Suen, Application of majority voting to pattern recognition: An analysis of its behavior and performance, IEEE Trans. Systems Man Cyber-

net. 27 (5) (1997) 553–568.

[32] L.S. Larkey, W.B. Croft, Combining classiﬁers in text categorization, in: Proceedings of SIGIR-96, 19th ACM International Conference on Research and

Development in Information Retrieval, 1996, pp. 289–297.

[33] W. Liu, J. Hong, Reinvestigating Dempster’s idea on evidence combination, Knowledge Inform. Syst. 2 (2) (2000) 223–241.
[34] W. Liu, Analyzing the degree of conﬂict among belief functions, Artif. Intell. 170 (11) (2006) 909–924.
[35] E.J. Mandler, J. Schurmann, Combining the classiﬁcation results of independent classiﬁers based on Dempster–Shafer theory of evidence, Pattern Recogn.

Artif. Intell. X (1988) 381–393.

[36] P. Melville, R.J. Mooney, Constructing diverse classiﬁer ensembles using artiﬁcial training examples, in: Proc. of IJCAI-03, 2003, pp. 505–510.
[37] T. Mitchell, Machine Learning, McGraw Hill, 1997.
[38] D. Opitz, Feature selection for ensembles, in: Proc. of AAAI-99, AAAI Press, 1999, pp. 379–384.
[39] B. Quost, T. Denoeux, M.-H. Masson, Pairwise classiﬁer combination using belief functions, Pattern Recogn. Lett. 28 (5) (2007) 644–653.
[40] F. Sebastiani, Machine learning in automated text categorization, ACM Comput. Surv. 34 (1) (2002) 1–47.
[41] G. Rogova, Combining the results of several neural network classiﬁers, Neural Networks 7 (5) (1994) 777–781.
[42] G. Shafer, R. Logan, Implementing Dempster’s rule for hierarchical evidence, Artif. Intell. 33 (3) (1987) 271–298.
[43] G. Shafer, Belief functions and possibility measures, in: J.C. Bezdek (Ed.), The Analysis of Fuzzy Information, vol. 1: Mathematics and Logic, CRC Press,

1987, pp. 51–84.

[44] G. Shafer, A Mathematical Theory of Evidence, Princeton University Press, Princeton, NJ, 1976.
[45] Ph. Smets, The combination of evidence in the Transferable Belief Model, IEEE Trans. Pattern Anal. Machine Intell. 12 (5) 447–458.
[46] K.M. Ting, I.H. Witten, Issues in stacked generalization, J. Artif. Intell. Res. (JAIR) 10 (1999) 271–289.
[47] D.M.J. Tax, M. van Breukelen, R.P.W. Duin, J. Kittler, Combining multiple classiﬁers by averaging or by multiplying, Pattern Recognition 33 (9) (2000)

1475–1485.

[48] K. Tumer, G.J. Robust, Combining of disparate classiﬁers through order statistics, Pattern Anal. Appl. 6 (1) (2002) 41–46.
[49] L. Xu, A. Krzyzak, C.Y. Suen, Several methods for combining multiple classiﬁers and their applications in handwritten character recognition, IEEE Trans.

System Man Cybernet. 2 (3) (1992) 418–435.

[50] L.M. Zouhal, T. Denoeux, An evidence-theoretic k-NN rule with parameter optimization, IEEE Trans. Systems Man Cybernet. C 28 (2) (1998) 263–271.
[51] Y. Yang, T. Ault, T. Pierce, Combining multiple learning strategies for effective cross validation, in: Proc. of ICML’00, 2000, pp. 1167–1182.
[52] F. Voorbraak, On the justiﬁcation of Dempster’s rule of combination, Artif. Intell. 48 (2) (1991) 171–197.
[53] G.I. Webb, MultiBoosting: A technique for combining boosting and wagging, Machine Learning 40 (2) (2000) 159–196.
[54] D. Wolpert, Stacked generalization, Neural Networks 5 (2) (1992) 241–259.
[55] I.H. Witten, E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, second ed., Morgan Kaufmann, San Francisco, 2005.

