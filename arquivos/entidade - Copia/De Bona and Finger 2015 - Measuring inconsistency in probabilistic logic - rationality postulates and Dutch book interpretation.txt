Artiﬁcial Intelligence 227 (2015) 140–164

Contents lists available at ScienceDirect

Artiﬁcial  Intelligence

www.elsevier.com/locate/artint

Measuring  inconsistency  in  probabilistic  logic:  rationality 
postulates  and  Dutch  book  interpretation
∗,1,  Marcelo Finger 2

Glauber De Bona

Department of Computer Science, Institute of Mathematics and Statistics, University of São Paulo, Brazil

a  r  t  i  c  l  e 

i  n  f  o

a  b  s  t  r  a  c  t

Article history:
Received 25 October 2014
Received in revised form 13 June 2015
Accepted 15 June 2015
Available online 19 June 2015

Keywords:
Probabilistic reasoning
Probabilistic logic
Inconsistency measures

Inconsistency measures have been proposed as a way to manage inconsistent knowledge 
bases  in  the  AI  community.  To  deal  with  inconsistencies  in  the  context  of  conditional 
probabilistic  logics,  rationality  postulates  and  computational  eﬃciency  have  driven  the 
formulation of inconsistency measures. Independently, investigations in formal epistemol-
ogy  have  used  the  betting  concept  of  Dutch  book  to  measure  an  agent’s  degree  of 
incoherence. In this paper, we show the impossibility of joint satisﬁability of the proposed 
postulates,  proposing  to  replace  them  by  more  suitable  ones.  Thus  we  reconcile  the 
rationality  postulates  for  inconsistency  measures  in  probabilistic  bases  and  show  that 
several  inconsistency  measures  suggested  in  the  literature  and  computable  with  linear 
programs satisfy the reconciled postulates. Additionally, we give an interpretation for these 
feasible measures based on the formal epistemology concept of Dutch book, bridging the 
views of two so far separate communities in AI and Philosophy. In particular, we show that 
incoherence degrees in formal epistemology may lead to novel approaches to inconsistency 
measures in the AI view.

© 2015 Elsevier B.V. All rights reserved.

1.  Introduction

“when you can measure what you are speaking about, you know something about it; but when you cannot [. . .] your knowledge is 
of a meagre and unsatisfactory kind;”

— Lord Kelvin [45]

Measuring  has  been  a  prominent  activity  in  advancing  scientiﬁc  and  technological  development.  Not  all  measures  are 
alike and good measures express intuitive notions in a useful way. In the ﬁeld of deductive logical reasoning, one usually 
has  an  intuition  expressing  that  one  theory  is  more inconsistent than  other,  capturing  the  idea  that  the  “effort”  to  restore 
consistency is greater in one case than the other. Also, no effort is required to restore the consistency of a consistent theory.
Based on those intuitions, there are several proposals for measuring inconsistency in knowledge bases over purely logical 
languages [19]. Some of these proposals involved attaching probabilities to formulas [29], or the combination of inconsis-
tency  factors [20].  Some  of  these  measures  are  discrete  or  even  qualitative,  while  others  are  more  like  distances,  but  all 
these  measures  have  to  behave  like  an  information measure [6].  And  to  adhere  to  certain  intuitions,  a  series  of  postulates 

* Corresponding author.

E-mail addresses: debona@ime.usp.br (G. De Bona), mﬁnger@ime.usp.br (M. Finger).

1 Supported by CAPES grant.
2 Partially supported by CNPq grant PQ 306582/2014-7.

http://dx.doi.org/10.1016/j.artint.2015.06.006
0004-3702/© 2015 Elsevier B.V. All rights reserved.

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

141

for inconsistency measures for purely logical knowledge bases were proposed [21,22]; for example, the consistency postulate
states that the inconsistency measure of a consistent base is 0.

Purely logical bases are known to be expressively limited in representing uncertainty required for real-world applications. 
In  this  work,  we  are  interested  in  measuring  the  inconsistency  of  knowledge  bases  over  logical  probabilistic  languages, 
which combine the deductive power of logical systems with the well-founded theory of probability. This kind of extension 
of purely logical systems can be traced back to the work of Boole [2], but has gained attention of AI researchers since the 
work of Nilsson [33], and has been extended to conditional probabilistic logic [37].

In AI, one of the main uses of measuring inconsistency in a knowledge base is to guide the consolidation of inconsistent 
pieces of information. Within propositional logic, Grant and Hunter [13] showed how inconsistency measures can be used 
to direct the stepwise resolution of conﬂicts via the weakening or the discarding of formulas.

In probabilistic bases, inconsistencies are rather common, specially when knowledge is gathered from different sources. 
To ﬁx these probabilistic knowledge bases, one can, for instance, delete pieces of information, or change the probabilities’ 
numeric values (or intervals). In this case, an inconsistency measure helps one to detect if a change approximates consis-
tency  or  not.  In  other  areas,  inconsistency  measures  for  probabilistic  logic  have  found  applications  in  merging  conﬂicting 
opinions, leading to an increased predictive power [47,25], and in quantifying the incoherence of procedures from classical 
statistical hypothesis testing [41].

Example 1.1.  Consider  we  are  devising  an  expert  system  to  assist  medical  diagnosis.  Suppose  a  group  of  experts  on  a 
disease  D is required to quantify the relationship between  D and its symptoms. Suppose three conditional probabilities are 
presented:

• the probability of a patient exhibiting symptom  S1 given he/she has disease  D is 50%;
• the probability of a patient exhibiting symptom  S2 given he/she exhibits symptom  S1 and has disease  D is 80%;
• the probability of a patient exhibiting symptom  S2 given he/she has disease  D is 30%.

A  knowledge  engineer,  while  checking  those  facts,  ﬁnds  that  they  are  inconsistent:  according  to  the  ﬁrst  two  items,  the 
probability of symptom  S2, given disease  D, should be at least 50% × 80% = 40%, instead of 30%. He does not even know 
where each probability came from, but plans to change the probabilities, since consistency is a requirement. How should he 
proceed? Which probabilities is the degree of inconsistency most sensitive to? Once chosen which number to change, should 
it be raised or lowered in order to approximate consistency? These are the kind of questions an inconsistency measure can 
help to answer.

The issue of measuring inconsistency in probabilistic bases has more recently been tackled by Thimm [44], Muiño [31]
and Potyka [34], who developed measures based on distance minimization, tailored to the probabilistic case. Potyka focused 
on computational aspects, looking for eﬃciently computable measures [34]. Muiño was driven by the CADIAG-2 knowledge 
base,  presenting  its  inﬁnitesimal  inconsistency  degree,  however  based  on  a  different  semantics  [31].  Thimm [44] adapted 
Hunter  and  Konieczny’s  [22] desirable  properties  for  inconsistency  measures  to  the  probabilistic  setting,  developing  mea-
sures that satisfy a set of rationality postulates.

It was Thimm [44] who realized the importance of continuity as a Postulate for the probabilistic case, namely the property 
that  a  small  change  in  the  probability  associated  to  formula  (absent  in  the  purely  logical  case)  should  lead  only  to  small 
changes  in  the  inconsistency  measure.  It  was  just  natural  that,  (conditional)  probabilistic  logic  being  an  extension  of  the 
classical cases, the continuity postulate was simply added to the postulates deﬁning classical inconsistency measures.

In this work, we argue that continuity cannot hold together with classical postulates such as consistency and indepen-
dence, and some of these postulates must be abandoned or exchanged for other ones that restore joint satisﬁability. So the 
ﬁrst contribution of this work is that we identify and ﬁx the possible problem with the postulates proposed by Thimm [44].
Another contribution lies in showing that these measures of inconsistency have a direct counterpart in formal epistemol-
ogy research over the coherence of an agent’s degrees of belief. It is known that inconsistent probabilistic beliefs correspond 
to a set of bets with guaranteed loss to the agent, which is called a “Dutch Book”  [8,27]. This agent’s incoherence has been 
measured by formalizing the intuition that the greater the inconsistency the greater the corresponding sure loss, and vice 
versa  [40,43]. Thus we interpret these incoherence measures via guaranteed losses as inconsistency measures, showing that 
existing measures based on distance minimization correspond to guaranteed losses that quantify an agent’s incoherence. To 
the best of our knowledge, no clear link has been shown between these two areas.

Here is a bird’s-eye view of how we achieve these goals.
After  introducing  probabilistic  knowledge  bases  in  Section 2,  this  paper  develops  three  main  contributions,  in  three 
different sections, dealing closely with three other works. In the following, we overview such contributions, together with 
the organization of the paper and their relation to the existing literature.

Inconsistency  measures  for  probabilistic  knowledge  bases  were  analyzed  via  rationality  postulates  by  Thimm  [44].  In 
Section 3,  we  argue  for  the  incompatibility  of  such  desirable  properties.  Firstly,  we  introduce  the  problematic  postulates: 
consistency, independence and continuity. The independence postulate claims that a free conditional — a (conditional) prob-
ability assignment that does not belong to any minimal inconsistent set — can be rule out without changing the degree of 

142

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

Table 1
Inconsistency measures, where they are deﬁned and a brief description.

Notation
Ip

Iε
p

Isum
SSK

Imax
SSK

Ia,sum
SSK

, Ib,sum
SSK

Ia,max
SSK

, Ib,max
SSK

Section

4.3

5.1

5.2

5.2

5.3

5.3

Explanation
Minimum p-norm of the vector composed by the adjustments on the probability bounds to reach consistency.
Minimum p-norm of the vector composed by the violations of each restriction corresponding to a probability bound. In the 
unconditional case, Iε
Maximum sure loss in a Dutch book if the sum of the stakes’ absolute values is at most 1.
Maximum sure loss in a Dutch book if the maximum of the stakes’ absolute values is at most 1.

= Ip .

p

Maximum sure loss in a Dutch book if the sum of the agent’s or the bettor’s possible losses is at most 1, respectively.

Maximum sure loss in a Dutch book if the maximum of the agent’s or the bettor’s possible losses is at most 1, respectively.

inconsistency. We also present the MIS-separability property, which deals with decomposability (through Minimal Inconsis-
tent Sets) and implies independence. As Thimm’s work regards precise probabilities, these four concepts are then introduced 
in this way. In a second step, Section 3 brings the ﬁrst contribution of this work: the presented postulates are shown to be 
incompatible.

In  Section 4,  we  search  for  a  reasonable  way  to  reconcile  the  incompatible  postulates.  First  of  all,  we  argue  that  in-
dependence  is  the  requirement  to  be  weakened,  together  with  the  stronger  property  of  MIS-separability.  Afterwards,  the 
concept of free conditional is analyzed, for independence is based on it. We ﬁnd that free conditional is a notion linked to 
classical consolidation and contraction (i.e., discarding formulas to reach consistency), and it is not suitable for probabilistic 
bases. The innocuous conditional concept is introduced, by investigating a natural consolidation procedure for probabilities: 
through widening their intervals, instead of ruling them out. The i-independence postulate is put forward based on innocu-
ous conditionals. In a similar way, we observe that MIS (minimal inconsistent set) is a notion that fails to capture all causes 
of inconsistency in the probabilistic knowledge bases. We deﬁne the alternative concept of inescapable conﬂict, which yields 
the  IC-separability  property.  We  show  that  innocuous  conditionals  are  to  inescapable  conﬂicts  as  free  conditionals  are  to 
minimal inconsistent sets. At the end of Section 4, the second main contribution of this paper emerges when i-independence 
and IC-separability are shown to be compatible with consistency and continuity, besides other desirable properties from the 
literature that are then presented.

Once a consistent package of postulates is laid out, a myriad of inconsistency measures can still be considered rational. 
Hence, in Section 5, further criteria to evaluate inconsistency measures are discussed: computational eﬃciency and mean-
ingful  interpretation.  On  the  one  hand,  two  inconsistency  measures  computable  through  linear  programs  are  adapted  to 
imprecise  probabilities  from  the  work  of  Potyka  [34].  It  is  shown  that  both  measures  satisfy  the  core  postulates,  and  we 
present the additional desirable properties each one satisﬁes. On the other hand, we review two measures from Schervish, 
Kadane and Seidenfeld that quantify the incoherence of an agent using the betting concept of Dutch book [40], under the 
operational interpretation that her degrees of belief (probabilities) determine her gambling behavior. The third main contri-
bution of this paper lies in showing the connection between inconsistency measures for probabilistic knowledge bases and 
incoherence measures for agents from formal epistemology. We prove that the two measures adapted from Potyka that are 
computable through linear programs are equivalent to two measures from Schervish et al. — that is, these two measures are 
rather eﬃcient and have a meaningful interpretation. The ﬁnal part of the section reviews other measures from the work of 
Schervish et al., showing that they also satisfy the postulates and are computationally feasible.

As several families of inconsistency measures are discussed throughout the paper, Table 1 compiles their notation, the 

section in which they are deﬁned and a short explanation of each.

2.  Preliminaries

A propositional logical language is a set of formulas formed by atomic propositions combined with logical connectives, 
possibly with punctuation elements (parentheses). We assume a ﬁnite set of symbols  Xn = {x1, x2, x3, . . . , xn} corresponding 
to atomic propositions (atoms). Formulas are constructed inductively with connectives (¬, ∧, ∨, →) and atomic propositions 
as usual. The set of all these well-formed formulas is the propositional language over  Xn , denoted by L Xn . Additionally, (cid:6)
denotes xi ∨ ¬xi for some xi ∈ Xn, and ⊥ denotes ¬(cid:6).

Given a signature  Xn, a possible world w is a conjunction of | Xn| = n atoms containing either xi or ¬xi for each xi ∈ Xn. 
= {w 1, . . . , w 2n } the set of all possible worlds over  Xn and say a  w ∈ W Xn entails a xi ∈ Xn (w |(cid:9) xi ) iff 

We denote by W Xn
xi is not negated in  w. This entailment relation can be extended to all ϕ ∈ L Xn as usual.

A probabilistic conditional (or simply conditional) is a statement of the form (ϕ|ψ)[q, ¯q], with the underlying meaning “the 
probability that ϕ is true given that ψ is true lies within the interval [q, ¯q]”, where ϕ, ψ ∈ L Xn are propositional formulas 
and q, ¯q ∈ [0, 1] are real numbers. Note that we do not assume q ≤ ¯q, since we are going to measure inconsistency. If ψ is 
a tautology, a conditional like (ϕ|ψ)[q, ¯q] is called an unconditional probabilistic assessment, usually denoted by (ϕ)[q, ¯q]. We 
say a conditional in the format (.)[q, q] is precise and denote it by (.)[q].

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

143

(cid:2)

→ [0, 1], with 

A probabilistic interpretation π : W Xn

j π (w j) = 1, is a probability mass over the set of possible worlds, 
{π (w j)|w j |(cid:9) ϕ}. A conditional (ϕ|ψ)[q, ¯q]
which induces a probability measure  P π : L Xn
→ [0, 1] by means of  P π (ϕ) =
is satisﬁed by π iff  P π (ϕ ∧ ψ) ≥ q P π (ψ) and  P π (ϕ ∧ ψ) ≤ ¯q P π (ψ). Note that when  P π (ψ) > 0, a probabilistic conditional 
(ϕ|ψ)[q, ¯q] is constraining the conditional probability of ϕ given ψ ; but any π with  P π (ψ) = 0 trivially satisﬁes the condi-
tional (ϕ|ψ)[q, ¯q] (this semantics is adopted by Halpern [15], Frisch and Haddawy [11] and Lukasiewicz [30], for instance). 
(cid:12)]. 
A knowledge base is a ﬁnite set (cid:6) of probabilistic conditionals such that, if (ϕ|ψ)[q, ¯q], (ϕ|ψ)[q
That  is,  for  each  pair  ϕ, ψ ,  only  one  probability  interval  can  be  assigned  to  (ϕ|ψ) in  a  knowledge  base.3 A  knowledge 
base (cid:6) is consistent (or satisﬁable) if there is a probability mass satisfying all conditionals (ϕ|ψ)[q, ¯q] ∈ (cid:6). It is precise if all 
intervals are singletons.

(cid:12)] ∈ (cid:6), then [q, ¯q] = [q

(cid:12), ¯q

(cid:12), ¯q

(cid:2)

The problem of verifying the consistency of a knowledge base is called probabilistic satisﬁability (or PSAT) [12]. Probabilis-
tic satisﬁability has been rediscovered several times, and an analytical and unconditional version was actually proposed by 
Boole [2]. Hailperin [14], Bruno and Gilio [3], and Nilsson [33] suggested solutions via linear programs. This linear program-
ming  approach  can  be  easily  extended  to  handle  conditional  probabilities  under  the  semantics  we  are  using  [16].  Recent 
advances in algorithms for PSAT solving can be found in [17,9,28].

If  any  probability  mass  π satisfying  (ϕ|ψ)[q, ¯q] implied  P π (ψ) > 0,  in  an  alternative  semantics,  the  latter  restriction 
could  be  added  to  the  program,  although  losing  the  linear  program  standard  format;  this  is  the  semantics  adopted  by 
Muiño [31], for instance. De Finetti proposed an alternative setting in which the conditional probability is fundamental [8]
and the satisfaction of probabilistic conditionals does not trivialize when the conditioning event has null probability. In such 
scenario, the consistency is called coherence, and its checking demands solving a sequence of linear programs [5].

When  all  interval  bounds  are  rational  numbers,  PSAT  is  an  NP-complete  problem  [12];  if  there  is  a  solution,  there 
is  a  solution  with  only  m + 1 possible  worlds  receiving  positive  probability  mass,  where  m is  the  knowledge  base  size. 
Nevertheless, column generation methods can handle large problems [26,24], and several approaches have recently appeared 
[28,9,17,7].  Note  that  this  linear  programming  approach  can  be  applied  to  other  probabilistic  logics  (see,  for  instance,  [1]
and [23]).

3.  Proposed postulates for inconsistency measures

Approaches to measuring inconsistency in probabilistic knowledge bases have been put forward by Muiño [31], Thimm 
[44] and Potyka [34], with different semantics for the conditionals. We follow the one adopted by Thimm and Potyka, in 
which  a  conditional  is  also  satisﬁed  by  any  measure  assigning  null  probability  to  the  conditioning  formula.  Thimm  has 
done a groundlaying work [44], extending Hunter’s postulates for inconsistency measures to the probabilistic case, which 
is our starting point. Potyka suggests feasible measures [34] we will review in Section 5.1, after investigating carefully the 
postulates. In this section, we begin with some desirable properties proposed by Thimm and then argue against their joint 
satisﬁability.

3.1.  Postulates

Let K (Kprec) be the set of all (precise) knowledge bases. An inconsistency measure for knowledge bases is a function 
I : K → [0, ∞).  Thimm’s  investigation  is  restricted  to  measures  I : Kprec → [0, ∞) over  knowledge  bases  with  precise 
probabilities, to what we narrow our focus in this section. The author proposes some desirable properties such a function 
should  satisfy,  following  Hunter  and  Konieczny’s  work  for  classical  logic  [20].  Although  Thimm  investigates  a  total  of  ten 
postulates, we describe in this section only four of these properties that we consider problematic. The ﬁrst one claims that 
an inconsistency measure must at least discriminate between consistent and inconsistent bases:

Postulate 3.1 (Consistency). I((cid:6)) = 0 iff (cid:6) is consistent.

A  second  desirable  property  has  to  do  with  probabilistic  conditionals  one  can  ignore  while  measuring  inconsistency, 

since they are not involved with the unsatisﬁability, in some sense. Some notation is needed to formalize it.

Deﬁnition 3.2. A set (cid:6) of probabilistic conditionals is a minimal inconsistent set (MIS) if (cid:6) is inconsistent and every set (cid:6)(cid:12) (cid:2) (cid:6)
is consistent.

Minimal  inconsistent  sets  can  be  considered  the  purest  form  of  inconsistency  [21],  capturing  its  causes.  The  focus  on 
MISes is derived from the seminal work of Reiter [36] on the diagnosis problem. Reiter investigated how formulas from a 
base could be ruled out in order to restore consistency, by choosing at least one element from each MIS, computing thusly 
a hitting set of their collection.

3 Note that this requirement is not too restrictive. Since nothing was said about logically equivalent propositions, a knowledge base may contain different 
probability intervals assigned to ϕ and ϕ ∧ (cid:6), for instance.

144

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

Let MIS((cid:6)) denote the collection of all MISes in (cid:6). Now we can deﬁne the central concept of free probabilistic conditional, 

following Thimm [44]:

Deﬁnition 3.3. A free probabilistic conditional of (cid:6) is a probabilistic conditional α ∈ (cid:6) such that, for all (cid:8) ∈ MIS((cid:6)), α /∈ (cid:8).

Analogously, a free probabilistic conditional of (cid:6) is in all its maximal consistent subsets. The postulate of independence 
then  claims  that  ruling  out  a  free  probabilistic  conditional  from  a  knowledge  base  should  not  change  its  inconsistency 
degree [44].

Postulate 3.4 (Independence). If α is a free probabilistic conditional of (cid:6), then I((cid:6)) = I((cid:6) \ {α}).

A stronger condition, also introduced by Hunter and Konieczny and adopted by Thimm, deals with a sort of decompos-
ability of the inconsistency measure through its minimal inconsistent sets. We call it a property, saving the name “postulate” 
to the most basic properties required from every measure. The version we present is tailored from Hunter and Konieczny’s
work [20]:

Property 3.5 (MIS-separability). If (cid:6) = (cid:8) ∪ (cid:9), (cid:8) ∩ (cid:9) = ∅ and MIS((cid:6)) = MIS((cid:8)) ∪ MIS((cid:9)), then I((cid:6)) = I((cid:8)) + I((cid:9)).

The  idea  behind  this  property  is  that  the  inconsistency  of  the  whole  knowledge  base  should  be  the  sum  of  the 
inconsistency  of  its  parts,  whenever  the  partition  does  not  break  any  minimal  inconsistent  set.  For  instance,  consider 
(cid:8) = {(x1)[0.5], (¬x1)[0.6]},  (cid:9) = {(x2)[0.7], (x2 ∧ x3)[0.8]} and  (cid:6) = (cid:8) ∪ (cid:9).  It  is  clear  that  (cid:8) and  (cid:9) are  the  only  minimal 
inconsistent sets in (cid:6). MIS-separability posits that the measure of inconsistency of (cid:6) is obtained by summing the measures 
of (cid:8) and (cid:9); formally, I((cid:6)) = I((cid:8)) + I((cid:9)). MIS-separability is stronger than independence [44]:

Proposition 3.6. If I satisﬁes MIS-separability, then I satisﬁes independence.

These  properties  can  be  found  in  Hunter  and  Konieczny’s  work  [21],  in  the  deﬁnition  of  a  “MinInc”  separable  basic 
inconsistency  measure  for  knowledge  bases  over  classical  propositional  logic.  The  measures  they  introduce  are  shown  to 
ﬁt  such  desiderata.  Thimm  revises  the  adaptation  of  these  classical  inconsistency  measures  to  the  probabilistic  case  and 
convincingly argues that they are not suitable to the quantitative nature of probabilities, since classical logic is qualitative.

To motivate the search for new inconsistency measures for probabilistic knowledge bases, while dispensing with mea-
sures from classical logic, Thimm puts forward the postulate of continuity. Intuitively, one expects that small changes in the 
probabilities of a knowledge base yield small changes in its degree of inconsistency. To formalize the continuity concept in 
precise knowledge bases, we introduce some notation, following Thimm [44].

That work studies precise knowledge bases of the form (cid:6) = {(ϕi|ψi)[qi]|1 ≤ i ≤ m}. For each precise knowledge base (cid:6), 
there  is  a  characteristic function (cid:10)(cid:6) : [0, 1]|(cid:6)| → Kprec that,  roughly  speaking,  changes  the  probabilities qi in  the  base;  i.e., 
(cid:10)(cid:6)((cid:16)q
]|1 ≤ i ≤ m}.  To  handle  the  (consistent)  empty  knowledge  base,  we  deﬁne  (cid:10)∅ : {∅} →
{∅}. Thimm imposes some order on the set (cid:6), building a sequence, for the function (cid:10)(cid:6) be unique and well-deﬁned. For 
simplicity,  we  just  suppose  there  is  some  order  (say,  lexicographic)  over  the  probabilistic  conditionals  used  to  uniquely 
specify (cid:10)(cid:6) .4 Now the continuity postulate can be enunciated, with ◦ denoting function composition:

(cid:17)) = {(ϕi|ψi)[q

(cid:12)
2, . . . , q

(cid:12)
1, q

(cid:12)
m

(cid:12)
i

Postulate 3.7 (Continuity (for precise probabilities)). For all (cid:6) ∈ Kprec, the function I ◦ (cid:10)(cid:6) : [0, 1]|(cid:6)| → [0, ∞) is continuous.

To  ﬁnd  inconsistency  measures  holding  the  desirable  properties,  including  continuity,  Thimm  introduces  a  family  of 
measures based on distance minimization, taking into account the numerical value of the probabilities. The basic idea is to 
quantify the inconsistency through the minimum changes, according to some distance, one has to apply on the probabilities 
to make the base consistent. The compatibility of consistency, independence and continuity is implicitly stated when it is 
proved that this whole family of inconsistency measures based on distance minimization satisﬁes them; and another family 
is proved to hold MIS-separability as well [44].

3.2.  The postulates’ incompatibility

The work done by Thimm [44] has carefully analyzed the problem of measuring inconsistency in knowledge bases over 
probabilistic logic. Desirable properties were borrowed from classical logic [20], and the crucial postulate of continuity was 
added. To attend these properties, measures based on distance minimization were introduced and some important results 
were proved. However, under a closer examination, the proposed postulates are incompatible.

4 Technically, we could use the lexicographic order over the pairs (ϕi|ψi ) to construct a function Lex taking each set (cid:6) to the corresponding sequence 
(cid:9) = Lex((cid:6)), uniquely specifying a function (cid:10)(cid:12)

(cid:9) that changes the probabilities of the sequence (cid:9). Then it could be deﬁned (cid:10)(cid:6)(q) = Lex

−1((cid:10)(cid:12)

(cid:9) (q)).

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

145

Theorem 3.8. There is no inconsistency measure I : Kprec → [0, ∞) that satisﬁes consistency, independence and continuity.

Proof. To prove by contradiction, suppose there is a measure I satisfying consistency, independence and continuity. Con-
sider the following knowledge bases:

(cid:6) = {(x1 ∧ x2)[0.5 + ε], (x1 ∧ ¬x2)[0.5]} for some 0 < ε ≤ 0.1
(cid:8) = (cid:6) ∪ {α}, α = (x1)[0.8]

(2)
We  are  going  to  use  I to  measure  the  inconsistency  of  (cid:8) when  ε → 0.  To  apply  independence,  we  are  going  to  show 
that  α is  free  in  (cid:8);  we  prove  that  (cid:6) is  the  only  MIS  in  (cid:8).  Note  that  {(x1 ∧ x2)[0.5 + ε], (x1)[0.8]} is  consistent  for  any 
ε ∈ (0, 0.1],  for  such  set  is  satisﬁed  by  the  probability  measure  induced  by  the  following  probability  mass: π1(x1 ∧ x2) =
0.5 + ε, π1(x1 ∧ ¬x2) = 0.3 − ε, π1(¬x1 ∧ x2) = π1(¬x1 ∧ ¬x2) = 0.1. To prove that {(x1 ∧ ¬x2)[0.5], (x1)[0.8]} is consistent, 
consider the following probability mass: π2(x1 ∧ x2) = 0.3, π2(x1 ∧ ¬x2) = 0.5, π2(¬x1 ∧ x2) = π2(¬x1 ∧ ¬x2) = 0.1. Hence, 
all MISes of (cid:8) must contain (cid:6) = {(x1 ∧ x2)[0.5 + ε], (x1 ∧ ¬x2)[0.5]}, for other subsets are all consistent. Furthermore, note 
that (cid:6) is inconsistent and minimal, so it is a MIS. We can conclude that (cid:6) is the only MIS in (cid:8), for any value of 0 < ε ≤ 0.1. 
As α is a free probabilistic conditional of (cid:8), we can apply independence:

(1)

I((cid:8)) = I((cid:6)) ,
for any 0 < ε ≤ 0.1.

To  exploit  the  continuity  of  I,  we  need  the  characteristic  function  of  (cid:8),  (cid:10)(cid:8) : [0, 1]3 → Kprec,  to  be  well-deﬁned;  so, 
we need an order over the probabilistic conditionals. Suppose that (cid:6) and (cid:8) are ordered as they were deﬁned in (1) and 
∗) differs from (cid:8) only in its ﬁrst conditional, which becomes 
(2). Let q
∗) is inconsistent. For any probability measure  P π ,  P π (x1 ∧ x2) = P π (x1 ∧ ¬x2) = 0.5
(x1 ∧ x2)[0.5]. Now we prove that (cid:10)(cid:8)(q
implies  P π (x1) = 1, contradicting α = {(x1)[0.8]}. As I satisﬁes consistency,

be the vector (cid:16)0.5, 0.5, 0.8(cid:17). It follows that (cid:10)(cid:8)(q

∗

I ◦ (cid:10)(cid:8)(q

∗

) > 0.

(3)

By the continuity of I, the function I ◦ (cid:10)(cid:8) : [0, 1]3 → [0, ∞) must be continuous, so there must be a limit at the point q
and such limit must be unique for any path approaching q
:

∗

∗

, 

lim
q→q∗

I ◦ (cid:10)(cid:8)(q) = lim
ε→0+

I ◦ (cid:10)(cid:8)((cid:16)0.5 + ε, 0.5, 0.8(cid:17)) = lim
ε→0+

I((cid:8)) .

By independence, we also have:
I((cid:6)) .

I((cid:8)) = lim
ε→0+

lim
ε→0+

As I satisﬁes continuity and {(x1 ∧ x2)[0.5], (x1 ∧ ¬x2)[0.5]} is satisﬁable, the consistency of I implies

lim
ε→0+

I((cid:6)) = I({(x1 ∧ x2)[0.5], (x1 ∧ ¬x2)[0.5]}) = 0 = lim
q→q∗

I ◦ (cid:10)(cid:8)(q) .

(4)

The  continuity  of  I requires  that  I ◦ (cid:10)(cid:6)(q
proof. (cid:2)

∗) = limq→q∗ I ◦ (cid:10)(cid:6)(q),  which  by  (3) and  (4) is  a  contradiction,  ﬁnishing  the 

Corollary 3.9. There is no inconsistency measure I : Kprec → [0, ∞) that satisﬁes consistency, MIS-separability and continuity.

Looking  at  the  counterexample  given  in  the  proof  of  Theorem 3.8 may  shed  some  light  on  what  is  the  cause  of  such 
conﬂict among the desirable properties. The only minimal inconsistent set in (cid:8) is (cid:6), and so independence forces the degree 
of inconsistency of (cid:8) to be the same as that of (cid:6), but this is not generally the case when inconsistency is measured via 
probability changing. This happens due to the fact that changing the probabilities in (cid:6) to some consistent setting does not 
in general imply that  (cid:8) becomes consistent. Although  (cid:6) is the only minimal inconsistent set of  (cid:8), there is another way 
to  prove  the  contradiction.  Note  that  (cid:6) implies  (x1)[1 + ε],  with  ε > 0,  which  contradicts  a  probability  axiom,  but  also 
contradicts α = (x1)[0.8].  While ε = 0 consolidates  (cid:6),  consolidating  (cid:8) requires  a  bigger  change  in  probabilities,  which  is 
ignored by independence. By demanding I((cid:8)) = I((cid:6)) for ε > 0, the postulate of consistency forces a discontinuity on ε = 0. 
When ε → 0, the inconsistency degree of (cid:6) tends to zero (by continuity), and independence requires the same from (cid:8). But 
this  contradicts  continuity,  given  consistency,  for  {(x1 ∧ x2)[0.5], (x1 ∧ ¬x2)[0.5]} would  still  contradict  (x1)[0.8],  and  (cid:8)
would be inconsistent.

4.  Reconciling the postulates

The ﬁndings from the previous section suggest that in order to drive the rational choice of an inconsistency measure for 
knowledge bases, we must abandon at least one postulate among consistency, independence and continuity. We claim that 
a  weakening  of  the  desired  properties  can  restore  their  compatibility,  and  in  this  section  we  investigate  paths  to  achieve 

146

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

that goal. After reconciling the problematic postulates, we review other proposed properties for inconsistency measures and 
extend them to the general case of knowledge bases with imprecise probabilities, showing some measures to satisfy them.

The consistency postulate seems to be indisputable, since the least one can expect from an inconsistency measure is that 
it separates inconsistent from consistent cases, or some inconsistency from none. The answer to the question of which prop-
erty we should relax to restore compatibility is thus reduced to either independence or continuity. Hunter and Konieczny 
have  already  noted  problems  with  independence  in  knowledge  bases  over  classical  logic,  proposing  to  relax  it  [22].  Intu-
ition shall be inclined towards keeping continuity, for it reﬂects the particular quantitative nature of probabilistic reasoning. 
A pragmatic  reason  to  give  up  independence  (and  so  MIS-separability)  is  simply  to  keep  continuity,  given  consistency,  to 
save inconsistency measures based on distance minimization. In the sequel, the withdrawal of independence within proba-
bilistic logic is argued for in a more compelling way.

The notion of free conditional and the postulate of independence are strongly related to the idea that minimal inconsis-
tent sets are the causes of inconsistencies, as suggested by Hunter and Konieczny [20]. Thimm says that free conditionals are 
“harmless”, in some sense, to the consistency of a knowledge base [44]. What is behind these notions is the classical way 
of handling inconsistency through ruling out formulas, as Reiter proposed in his diagnosis problem [36] and as the standard 
AGM paradigm of belief revision deﬁnes base contraction (see [18] for a general view of the AGM paradigm). Reiter’s hitting 
sets technique views a repair of some inconsistency set of formulas as giving up of at least one element from each minimal 
inconsistent set. For such repair to be minimal, no free formula should be discarded. In the AGM paradigm, the consolidation 
process of a belief base can be interpreted as the contraction of ⊥, the contradiction. The inclusion postulate claims that the 
result of a contraction is a subset of the belief base in question, and the relevance postulate forces the contraction of ⊥ to 
contain all free formulas of the base.

When we move from classical to probabilistic logic, there is a natural way to relax formulas without completely losing 
their  information.  Note  that  ruling  out  a  probabilistic  conditional  (ϕ|ψ)[q, ¯q] is  semantically  equivalent  to  changing  it  to 
(ϕ|ψ)[0, 1], so it is a particular (and extreme) case of widening the probability interval. If we need to give up the belief on 
(cid:12)] can still be consistently 
(ϕ|ψ)[q, ¯q] to restore consistency, perhaps there are some q
believed.  When  inconsistency  is  measured  continuously,  through  changes  in  probabilities,  it  is  this  more  general  kind  of 
consolidation  process  that  is  being  suggested.  As  it  is  indicated  in  the  proof  of  Theorem 3.8,  consolidating  all  minimal 
inconsistent sets ((cid:6)) through probability changing does not imply consolidating the whole base ((cid:8)). We can conclude that 
the concepts of free conditional and minimal inconsistent set are not suitable to analyze continuous inconsistency measures 
based on distance minimization.

(cid:12) ≥ ¯q such that (ϕ|ψ)[q

(cid:12) ≤ q and  ¯q

(cid:12), ¯q

Furthermore,  it  seems  that  the  deﬁnition  of  free  conditional,  and  so  independence,  can  be  reﬁned  to  be  suitable  for 
analyzing continuous measures, while continuity is a harder deﬁnition to be contrived to be compatible with independence. 
Hence,  we  can  try  to  weaken  independence,  and  perhaps  MIS-separability,  by  modifying  the  notion  of  free  conditional, 
instead of fully forgetting this postulate.

As both independence and MIS-separability are deﬁned via minimal inconsistent sets, in order to weaken these properties 
to reach compatibility with consistency and continuity, it seems reasonable to replace MIS by an alternative concept that 
could reconcile the desirable properties altogether. However, to do it in a principled way, we ﬁrst analyze the concept of 
free  probabilistic  conditional  as  to  the  corresponding  consolidation  procedure  and  then  modify  it  to  save  independence. 
Afterwards, a related notion of conﬂict that also ﬁxes MIS-separability is introduced.

4.1.  Reﬁning the free probabilistic conditional concept

A weaker form of independence has already been suggested in the literature. Thimm [44] deﬁnes a safe conditional as 
one  whose  atomic  propositions  are  disjoint  from  those  in  the  rest  of  the  base.  We  also  demand  that  the  conditional  be 
satisﬁable in order to be safe.5 The weak independence postulate then posits that ruling a (satisﬁable) safe conditional out 
should not change the inconsistency measure of a base. Hunter and Konieczny have suggested the same weakening for inde-
pendence, in the classical setting, when they acknowledge that independence may be too strong a property to require [22]. 
Weak independence is compatible with consistency and continuity, since Potyka’s measures satisfy them [34]. Although safe 
conditionals are easily recognizable, we expect that they be rare in practice, due to the natural logical dependencies among 
propositions within a base. We are looking for a stronger, more useful notion of independence, between the safe-based and 
the free-based ones, hence we look for a concept between safe and free.

Besides  deﬁning  free  probabilistic  conditional  through  minimal  inconsistent  sets,  one  could  equivalently  do  it  via  the 

notion of consolidation as giving up conditionals to restore consistency. Let us formalize this concept.

Deﬁnition 4.1. Let (cid:6) be a knowledge base in K. An abrupt repair of (cid:6) is any set (cid:8) ⊆ (cid:6) such that (cid:6)(cid:12) = (cid:6) \ (cid:8) is consistent — 
we call (cid:6)(cid:12)
an abrupt consolidation. If an abrupt repair (cid:8) is such that, for every (cid:9) (cid:2) (cid:8), (cid:6) \ (cid:9) is inconsistent, (cid:8) is a minimal 
abrupt repair — and (cid:6)(cid:12) = (cid:6) \ (cid:8) is a maximal abrupt consolidation.

5 Thimm [44] only considers conditionals (ϕ|ψ)[q, ¯q] such that ϕ ∧ ψ and ¬ϕ ∧ ψ are (classically) satisﬁable, so the conditional is also satisﬁable.

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

147

We  can  now  prove6 a  result  that  states  different  ways  to  deﬁne  a  free  probabilistic  conditional,  as  being  part  of  no 
minimal abrupt repairs (of all maximal consistent sets) or being consistent with any abrupt repair. We say a conditional α
is consistent with a knowledge base (cid:6) if there is a probability mass π that satisﬁes α and (cid:6).

Theorem 4.2. Consider a knowledge base (cid:6) ∈ K and a probabilistic conditional α ∈ (cid:6). The following statements are equivalent:

1. There is no minimal abrupt repair (cid:8) of (cid:6) such that α ∈ (cid:8).
2. For all maximal abrupt consolidation (cid:6)(cid:12)
3. If (cid:6)(cid:12) = (cid:6) \ (cid:8) is an abrupt consolidation of (cid:6) (equivalently, (cid:8) is an abrupt repair of (cid:6)), then α is consistent with (cid:6)(cid:12)
4. There is no minimal inconsistent set (cid:8) ⊆ (cid:6) such that α ∈ (cid:8).

of (cid:6), α ∈ (cid:6)(cid:12)

.

.

Note that the fourth statement above is the deﬁnition of free probabilistic conditional given in Section 3.1. The ﬁrst and 
the second statements are clearly dual to each other, so we have presented two new ways of equivalently deﬁning a free 
probabilistic conditional without mentioning minimal inconsistent sets, but using abrupt repair and abrupt consolidation. As 
it is suggested in the previous section, ruling a conditional out is equivalent to widening the corresponding interval to [0, 1]
— that is why we call it an abrupt repair. However, a probabilistic logic allows for a more general notion of consolidation, 
(cid:12) ≥ ¯q;  and  (cid:2) is  deﬁned  from  ⊆ as 
formalized  below.  To  save  notation,  we  write  (ϕ|ψ)[q, ¯q] ⊆ (ϕ|ψ)[q
usual.

(cid:12) ≤ q and  ¯q

(cid:12)] if q

(cid:12), ¯q

Deﬁnition 4.3.  Let  (cid:6) be  a  knowledge  base  in  K.  (cid:6)(cid:12) ∈ K is  a  widening of  (cid:6) if  there  is  a  bijection  f : (cid:6) → (cid:6)(cid:12)
α ⊆ f (α) for all α ∈ (cid:6); furthermore, if a widening (cid:6)(cid:12)

is consistent, we say it is a consolidation of (cid:6).

such  that 

In other words, a consolidation of (cid:6) is the result of widening the probability intervals of its conditionals to a consistent 
setting.  Analogously  to  the  maximal  abrupt  consolidation,  related  to  a  minimal  abrupt  repair,  we  can  deﬁne  a  sort  of 
consolidation with minimal changes, we call dominant.

Deﬁnition 4.4. A consolidation (cid:6)(cid:12)
(cid:9) of (cid:6), if (cid:6)(cid:12)

is a widening of (cid:9), then (cid:6)(cid:12) = (cid:9).

of (cid:6) is a dominant consolidation (or simply a d-consolidation) of (cid:6) if, for all consolidations 

A d-consolidation (cid:6)(cid:12)

of (cid:6) is such that if some probability interval of (cid:6) were less widened, ﬁxing the others, the resulting 
base would not be consistent. In other words, it is not possible to give up strictly less information than a d-consolidation 
while restoring consistency; for an interval to be less widened, another must be more enlarged. In these sense, the changes 
in the probability bounds are minimal, and the consolidation is maximal.

From these concepts, two new deﬁnitions for free probabilistic conditional could be derived: a conditional is free if it is 
in any d-consolidation; or a conditional is free if it is consistent with any consolidation. We can prove these deﬁnitions are 
actually equivalent:

Lemma 4.5. Consider a knowledge base (cid:6) ∈ K and a probabilistic conditional α ∈ (cid:6). The following statements are equivalent:

1. For all d-consolidations (cid:6)(cid:12)
2. If (cid:6)(cid:12)

is a consolidation of (cid:6), then α is consistent with (cid:6)(cid:12)

.

of (cid:6), α ∈ (cid:6)(cid:12)

.

A  modiﬁcation  of  the  free  probabilistic  conditional  concept  is  suggested  by  the  comparison  of  Lemma 4.5 with  Theo-
rem 4.2, which would yield a different postulate of independence. To not overload the concept of free conditional, we say 
these probabilistic conditionals are innocuous, for they are consistent with any consolidation of the knowledge base.

Deﬁnition 4.6. An innocuous probabilistic conditional of (cid:6) is a probabilistic conditional α ∈ (cid:6) such that, for every dominant 
consolidation (cid:6)(cid:12)

of (cid:6), α ∈ (cid:6)(cid:12)

.

The  difference  between  free  and  innocuous  conditionals  can  be  seen  in  the  knowledge  base  from  the  proof  of  Theo-

rem 3.8, as the following example shows.

Example 4.7. Consider the following knowledge base:

(cid:8) = {(x1 ∧ x2)[0.6], (x1 ∧ ¬x2)[0.5], (x1)[0.8]} .

6 Long proofs of technical results are in a separate Appendix A.

148

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

As it was claimed in the proof of Theorem 3.8, {(x1 ∧ x2)[0.6], (x1 ∧ ¬x2)[0.5]} is the only minimal inconsistent set of (cid:8); 
so α = (x1)[0.8] is a free probabilistic conditional. Nonetheless, (cid:8) has no innocuous probabilistic conditional. This can be 
noted through the following dominant consolidation of (cid:8):

(cid:12) = {(x1 ∧ x2)[0.55, 0.6], (x1 ∧ ¬x2)[0.45, 0.5], (x1)[0.8, 1]} .

(cid:8)

is consistent and any consolidation (cid:9) (cid:20)= (cid:8)(cid:12)

has at least one wider probability interval; so (cid:8)(cid:12)
(cid:8)(cid:12)
is dominant. But no original 
conditional of (cid:8) is in (cid:8)(cid:12)
, so none is innocuous. Equivalently, any β ∈ (cid:8) is inconsistent with (cid:8)(cid:12)
. An example of innocuous 
conditional can be given in the knowledge base (cid:9) = (cid:8) ∪ {(x2)[0.3, 0.8]}, since (x2)[0.3, 0.8] would be consistent with any 
consolidation of (cid:9).

An  innocuous  probabilistic  conditional  of  (cid:6) is  consistent  with  any  abrupt  consolidation  of  (cid:6),  since  it  is  semantically 
equivalent to a consolidation with [0, 1] probability intervals; furthermore, a safe conditional of (cid:6) is clearly consistent with 
any consolidation of (cid:6):

Proposition 4.8. Consider a probabilistic conditional α ∈ (cid:6). If α is safe, it is innocuous; if α is innocuous, it is free.

As to the independence postulate, we modify it in a corresponding way:

Postulate 4.9 (i-Independence). If α is an innocuous probabilistic conditional of (cid:6), then I((cid:6)) = I((cid:6) \ {α}).

From Proposition 4.8 follows the relation among weak independence, i-independence and independence:

Corollary 4.10. If I satisﬁes independence, then I satisﬁes i-independence. If I satisﬁes i-independence, then I satisﬁes weak inde-
pendence.

4.2.  Reﬁning the minimal conﬂict concept

To redeﬁne MIS-separability, we need a new notion of minimal conﬂict, related to the consolidation we introduced. Note 
that the union of minimal inconsistent sets is equal to the union of minimal abrupt repairs of a knowledge base, so that 
it  forms  the  complement  of  the  set  of  free  probabilistic  conditionals.  To  be  consistent,  we  should  provide  a  deﬁnition  of 
conﬂicting sets such that their union is complementary to the set of innocuous conditionals. A set with all probabilistic con-
ditionals that are not innocuous would be inconsistent when not empty, but would not have the minimality we are looking 
for.  Such  a  set  would  be  analogous  to  the  union  of  all  minimal  inconsistent  sets,  but  we  search  for  a  more  fundamental 
notion of conﬂict, that can be derived by analyzing the consolidation properties of minimal inconsistent sets.

A minimal inconsistent set is minimal regarding set inclusion, and this is related to the abrupt consolidation:

Proposition 4.11. A knowledge base (cid:6) is a minimal inconsistent set iff (cid:6) is inconsistent and there are no (cid:8)1, . . . , (cid:8)k (cid:2) (cid:6), with k ≥ 1, 
such that:
(cid:3)
k

i=1 (cid:8)i = (cid:6);

1.
2. For every (cid:6)(cid:12) ⊆ (cid:6), if (cid:6)(cid:12) ∩ (cid:8)i is an abrupt consolidation of (cid:8)i for all 1 ≤ i ≤ k, then (cid:6)(cid:12)

is an abrupt consolidation of (cid:6).

Intuitively,  a  minimal  inconsistent  set  (cid:6) is  a  conﬂict  that  cannot  be  analyzed  in  smaller  subsets  such  that  abruptly 
consolidating them implies abruptly consolidating (cid:6). Starting with a single inconsistent base (cid:6), we can ﬁnd smaller subsets 
(cid:8)i satisfying both items of 4.11. We can do this recursively on the inconsistent sets (cid:8)i until we reach unanalyzable conﬂicts, 
which happens to be minimal inconsistent sets. So, abruptly consolidating these sets is abruptly consolidating (cid:6). Substituting 
consolidation for abrupt consolidation, we have an analogous deﬁnition of conﬂict:

Deﬁnition 4.12.  A  knowledge  base  (cid:6) is  an  inescapable conﬂict if  (cid:6) is  inconsistent  and  there  are  no  (cid:8)1, . . . , (cid:8)k (cid:2) (cid:6),  with 
k ≥ 1, such that:

(cid:3)
k
1.
2. If (cid:8)(cid:12)

i=1 (cid:8)i = (cid:6);

i is a consolidation of (cid:8)i for all 1 ≤ i ≤ k and 

(cid:3)
k

i=1 (cid:8)(cid:12)

i is a widening of (cid:6), then 

(cid:3)
k

i=1 (cid:8)(cid:12)

i is a consolidation of (cid:6).

The extra condition in the second item of Deﬁnition 4.12 forces consolidations of different knowledge bases (cid:8)i , (cid:8) j (cid:2) (cid:6)
with  some  probabilistic  conditional  in  common  to  agree  in  that  probability  interval;  otherwise, 
i would  not  be  a 
knowledge  base.  In  other  words,  the  second  item  says  that  if  we  widen  the  probability  intervals  of  (cid:6) making  each  (cid:8)i
consistent, then (cid:6) becomes consistent. Inescapable conﬂicts could equivalently be deﬁned in an alternative way:

i=1 (cid:8)(cid:12)

(cid:3)
k

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

149

Lemma 4.13. A knowledge base (cid:6) is an inescapable conﬂict iff there is a widening (cid:6)(cid:12)

of (cid:6) such that (cid:6)(cid:12)

is a minimal inconsistent set.

Lemma 4.13 captures  the  intuition  behind  the  proof  of  Theorem 3.8,  where  there  is  a  widening  that  consolidates  any 
proper subset of the knowledge base without consolidating the whole base. As it happens with abrupt consolidation and 
MISes, to consolidate  (cid:6), one only needs to widen its probability intervals in such a way that each inescapable conﬂict is 
solved.

Corollary 4.14. Consider two knowledge bases (cid:6), (cid:6)(cid:12) ∈ K such that (cid:6)(cid:12)
widening {β ∈ (cid:6)(cid:12) | α ∈ (cid:8) and α ⊆ β} is consistent, then (cid:6)(cid:12)

is a consolidation of (cid:6).

is a widening of (cid:6). If for every inescapable conﬂict (cid:8) ⊆ (cid:6) its 

As  all  abrupt  consolidations  can  be  viewed  as  consolidations  and  each  knowledge  base  is  a  widening  of  itself,  an  in-

escapable conﬂict is something weaker than a minimal inconsistent set:

Corollary 4.15. If (cid:8) is a minimal inconsistent set, then (cid:8) is an inescapable conﬂict.

Example 4.16. Consider again the knowledge base from Example 4.7:

(cid:8) = {(x1 ∧ x2)[0.6], (x1 ∧ ¬x2)[0.5], (x1)[0.8]} .

(cid:3)
k

As it was already shown, {(x1 ∧ x2)[0.6], (x1 ∧ ¬x2)[0.5]} is the only minimal inconsistent set of (cid:8) — and, by Corollary 4.15, 
it is an inescapable conﬂict. Nevertheless, it can be proved that the whole (cid:8) is an inescapable conﬂict as well.

i=1 (cid:8)(cid:12)

Suppose, by contradiction, there are (cid:8)1, . . . , (cid:8)k (cid:2) (cid:8) such that 
i is a widening of (cid:8), then 

1 ≤ i ≤ k and 
(cid:8)(cid:12)
i for each (cid:8)i (cid:2) (cid:8). There are two cases: (a) (x1 ∧ x2)[0.6] ∈ (cid:8)i ; and (b) (x1 ∧ x2)[0.6] /∈ (cid:8)i . In case (a), we construct (cid:8)(cid:12)
widening the probability interval of the conditional (x1 ∧ x2)[0.6] to (x1 ∧ x2)[0.5, 0.6]; formally, (cid:8)(cid:12)
{(x1 ∧ x2)[0.5, 0.6]}. In case (b), we choose the trivial consolidation (cid:8)(cid:12)
that each (cid:8)(cid:12)
i is consistent. Consider then the following knowledge base:

i=1 (cid:8)i = (cid:8) and, if (cid:8)(cid:12)
i is a consolidation of (cid:8)i for all 
(cid:3)
k
i , we pick a consolidation 
i is a consolidation of (cid:8). To build 
i by 
= ((cid:8)i \ {(x1 ∧ x2)[0.6]}) ∪
i
= (cid:8)i . Even though the proof is omitted, we claim 

i=1 (cid:8)(cid:12)

i=1 (cid:8)(cid:12)

(cid:3)
k

(cid:3)
k

i

(cid:12) =

(cid:8)

k(cid:4)

i=1

(cid:8)

(cid:12)
i

= {(x1 ∧ x2)[0.5], (x1 ∧ ¬x2)[0.5], (x1)[0.8]} .

By  the  premises,  (cid:8)(cid:12)
Section 3.2). Finally, there cannot exist such (cid:8)1, . . . , (cid:8)k (cid:2) (cid:8), and (cid:8) is an inescapable conﬂict.

is  a  consolidation  of  (cid:8),  but  it  is  inconsistent,  since  (cid:8)(cid:12) \ {(x1)[0.8]} implies  (x1)[1] (as  shown  in 

We can now change MIS-separability to respect inescapable conﬂicts (IC) instead of minimal inconsistent sets. Let IC((cid:6))

denote the collection of all inescapable conﬂicts of (cid:6).

Property 4.17 (IC-separability). If (cid:6) = (cid:8) ∪ (cid:9), (cid:8) ∩ (cid:9) = ∅ and IC((cid:6)) = IC((cid:8)) ∪ IC((cid:9)), then I((cid:6)) = I((cid:8)) + I((cid:9)).

As inescapable conﬂict is a weaker concept than MIS, MIS-separability is stronger than IC-separability.

Corollary 4.18. If I satisﬁes MIS-separability, then I satisﬁes IC-separability.

Recall that a free probabilistic conditional is deﬁned in the standard way as not belonging to any minimal inconsistent 
set. We prove the analogous result for innocuous conditionals and inescapable conﬂicts, linking all concepts introduced in 
this section.

Theorem 4.19. The following statements are equivalent:

of (cid:6), α ∈ (cid:6)(cid:12)
1. For all d-consolidation (cid:6)(cid:12)
2. If (cid:6)(cid:12)
is a consolidation of (cid:6), then α is consistent with (cid:6)(cid:12)
3. There is no inescapable conﬂict (cid:8) in (cid:6) such that α ∈ (cid:8).
4. α is an innocuous probabilistic conditional in (cid:6).

.

.

A result analogous to Proposition 3.6 follows:

Corollary 4.20. If I satisﬁes IC-separability, then I satisﬁes i-independence.

150

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

As  already  mentioned,  inescapable  conﬂicts  are  to  consolidations  as  minimal  inconsistent  sets  are  to  abrupt  consoli-
dations.  If  consolidation  via  conditionals  withdrawal,  as  in  Reiter’s  and  AGM  approaches,  can  focus  on  the  collection  of 
minimal inconsistent sets (ignoring free conditionals), consolidation through widening probability intervals can be done by 
watching only for the inescapable conﬂicts (ignoring innocuous conditionals). All these relations among free and innocuous 
probabilistic conditionals, minimal inconsistent sets and inescapable conﬂicts argue in favor of the new proposed postulates, 
whose compatibility with consistency and continuity we will prove.

4.3.  Compatible postulates for imprecise probabilities

To  replace  the  postulate  of  independence  and  the  property  of  MIS-separability,  we  propose  the  weaker  pair  of 
i-independence and IC-separability towards building a compatible package together with consistency and continuity. Before 
proving such compatibility, the postulates have to be generalized to imprecise knowledge bases. To generalize consistency, 
i-independence and IC-separability is straightforward, we just enlarge their intended scope from knowledge bases in Kprec
to bases in K, but the continuity postulate demands some notation.

Let  (cid:6) = {(ϕi|ψi)[q

, ¯qi]|1 ≤ i ≤ m} be  a  knowledge  base.  The  characteristic  function  of  (cid:6) can  be  generalized  as  a 
function  (cid:10)(cid:6) : [0, 1]2m → K that  changes  both  upper  and  lower  bounds  of  each  probabilistic  conditional  in  (cid:6);  formally, 
(cid:12)
, ¯q
(cid:10)(cid:6)((cid:16)q
] |1 ≤ i ≤ m}.  Now  the  continuity  postulate  can  be  generalized,  with  ◦ denoting 
m
function composition:

(cid:17)) = {(ϕi|ψi)[q

(cid:12)
1, . . . , q

, ¯q

, ¯q

(cid:12)
m

(cid:12)
1

(cid:12)
i

(cid:12)
i

i

Postulate 4.21 (Continuity). For all (cid:6) ∈ K, the function I ◦ (cid:10)(cid:6) : [0, 1]2|(cid:6)| → [0, ∞) is continuous.

Note  that  the  postulate  above  implies  Postulate  3.7,  which  deﬁnes  continuity  for  precise  probabilities.  Given  a  base  (cid:6)
of size m, Postulate 3.7 considers a function  f : [0, 1]m → Kprec (the characteristic function when probabilities are precise) 
(cid:17)) and requires that I ◦ f be continuous. But note that, if I ◦ (cid:10)(cid:6)
such that  f ((cid:16)q
is continuous, so is I ◦ f . Therefore, Theorem 3.8 and Corollary 3.9 also hold within the imprecise probability framework.

(cid:17)) = (cid:10)(cid:6)((cid:16)q

(cid:12)
2, . . . , q

(cid:12)
2, . . . , q

(cid:12)
m, q

(cid:12)
1, q

(cid:12)
1, q

(cid:12)
2, q

(cid:12)
1, q

(cid:12)
m

(cid:12)
m

Hunter and Konieczny proposed another basic postulate for inconsistency measures [20] that was also adopted by Thimm 

[44].

Postulate 4.22 (Monotonicity). For any knowledge bases (cid:6), ((cid:6) ∪ {α}) ∈ K, I((cid:6) ∪ {α}) ≥ I((cid:6)).

Thimm actually suggests a stronger principle, super-additivity, which implies monotonicity. Since super-additivity is in-

compatible with normalization [44] — as also is IC-separability —, we state them as properties, and not postulates.

Property 4.23 (Super-additivity). For any knowledge base (cid:6) ∪ (cid:8) ∈ K, if (cid:6) ∩ (cid:8) = ∅, then I((cid:6) ∪ (cid:8)) ≥ I((cid:6)) + I((cid:8)).

Property 4.24 (Normalization). For any knowledge base (cid:6) ∈ K, I((cid:6)) ∈ [0, 1].

To attend the desirable properties, we generalize the inconsistency measures based on distance minimization proposed by 
Thimm [44] to the case of imprecise probabilities. Muiño introduced similar ideas under a different semantics for conditional 
probabilities [31]. Firstly, we deﬁne a family of  p-norms.

Deﬁnition 4.25.  Consider  a  (positive)  m ∈ N>0 and  a  p ∈ N>0 ∪ {∞}.  Given  a  vector  q = (cid:16)q1, q2, . . . , qm(cid:17) over  the  real 

(cid:5)

numbers, the  p-norm of q is (cid:21)q(cid:21)p = p

m(cid:2)

i=1

|qi|p if  p is ﬁnite; otherwise it is (cid:21)q(cid:21)∞ = maxi |qi|.

Thimm deﬁnes a family Ip of inconsistency measures based on the  p-norms, which we modify to also consider  p = ∞

and handle the empty base.

Deﬁnition 4.26.  Consider  a  p ∈ N>0 ∪ {∞} and  a  (cid:6) ∈ K.  The  function  Ip : K → [0, ∞) is  the  dp -inconsistency measure, 
deﬁned as

Ip((cid:6)) = min{(cid:21)q − q

(cid:12)(cid:21)p | (cid:10)(cid:6)(q) = (cid:6) and (cid:10)(cid:6)(q

(cid:12)

) is consistent} ,

for any non-empty (cid:6), and Ip(∅) = 0.

Finally,  we  are  in  a  position  to  show  inconsistency  measures  satisfying  the  wanted  properties.  We  extend  Thimm’s 
results to prove that all dp -inconsistency measures satisfy the reconciled postulates and that some of them hold additional 
properties. Muiño has similar results, though under a different semantics [31].

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

151

Theorem 4.27. For any p ∈ N>0 ∪ {∞}, Ip is well-deﬁned and satisﬁes the postulates of consistency, continuity, i-independence and 
monotonicity.

The compatibility of IC-separability and super-additivity with consistency, continuity, monotonicity and i-independence 

is conﬁrmed by the I1 measure:

Lemma 4.28. Ip satisﬁes super-additivity and IC-separability iff p = 1.

If normalization is required, we can use the following result due to Muiño [31]:

Lemma 4.29. Ip satisﬁes normalization iff p = ∞.

5.  Feasible principled inconsistency measures

Although we have compatible postulates to drive the rational choice of inconsistency measures, these desirable properties 
are satisﬁed by a myriad of functions. We may use other arguments to pick some particular inconsistency measures among 
those obeying the postulates. This section investigates computational aspects of measuring inconsistency through distance 
minimization, reviewing and generalizing measures proposed by Potyka [34] that can be handled via linear programming. 
In a second moment, we show how the concrete measures introduced can be justiﬁed by means of Dutch books, displaying 
the maximum guaranteed loss an agent would be exposed to, if stakes are limited. We also show that Dutch books offer 
other interesting measures.

5.1.  Measuring inconsistency with linear programs

In order to better understand the connection between Potyka’s inconsistency measures and incoherence measures based 
on Dutch books, it is worth detailing the construction of the corresponding linear programs. Furthermore, due to such link, 
every  property  we  prove  for  Potyka’s  inconsistency  measures  shall  be  inherited  by  the  equivalent  Dutch  book  measures 
presented in the next section.

To check the consistency of a knowledge base, one can use the well-known formulation of PSAT as a linear program [16]. 
, ¯qi] is
, ¯qi]|1 ≤ i ≤ m}. Under the semantics adopted, each assessment (ϕi|ψi)[q
Consider a knowledge base (cid:6) = {(ϕi|ψi)[q
P π (ψi) ≥ 0 and P π (ϕi ∧ ψi) − ¯qi P π (ψi) ≤ 0 of restrictions on P π . The knowledge base 
equivalent to the pair P π (ϕi ∧ ψi) − q
is consistent iff these 2m restrictions can be jointly satisﬁed by a probability measure  P π induced by a probability mass π . 
I w j (ψi) and bi j = I w j (ϕi ∧ ψi) − ¯qi I w j (ψi), 
Consider two (m × 2n)-matrices,  A = [ai j] and B = [bi j], with ai j = I w j (ϕi ∧ ψi) − q
in  which  I w j
is  the  valuation  relative  to  the 
possible world  w j . The knowledge base (cid:6) is satisﬁable iff there is a (2n × 1)-vector π satisfying the system:

→ {0, 1} is  the  indicator  function  of  the  set  {ϕ ∈ L Xn

|w j |(cid:9) ϕ} —  I w j

: L Xn

i

i

i

i

Aπ ≥ 0
Bπ ≤ 0
(cid:6)

π = 1

(5)

(6)

(7)

π ≥ 0 .

i

Restrictions in (5) correspond to  P π (ϕi|ψi) ≥ q

(8)
, and those in (6) codify  P π (ϕi|ψi) ≤ ¯qi ; Constraints (7) and (8) force π
to be a probability mass over the possible worlds w 1, w 2, . . . , w 2n . As all constraints are linear, this system can be solved by 
linear programming techniques as Simplex. Despite the exponential number of columns, column generation methods can be 
used to handle them implicitly [26,24], keeping the computation eﬃcient enough to solve large knowledge bases (thousands 
of probabilities in [17,10]).

To measure inconsistency using distance minimization with Ip , we can add to the system variables εi

≥ 0 ( ¯εi ≥ 0) cor-
, ¯qi]
responding to decrements (increments) in lower (upper) bounds of each probability interval. Any conditional (ϕi|ψi)[q
P π (ψi) ≥ −εi P π (ψi) and  P π (ϕi ∧ ψi) − ¯qi P π (ψi) ≤ ¯εi P π (ψi). Computing the Ip
yields a pair of restrictions  P π (ϕi ∧ ψi) − q
measure is thus reduced to minimizing the  p-norm of the vector (cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17).7 Nonetheless, the constraints contain 
non-linear  terms  (from  εi P π (ψi) and  ¯εi P π (ψi)),  and  Potyka  points  out  that  these  programs  have  (non-global)  local  op-
tima [34], so convex optimization techniques cannot be directly applied. Thus, computing Ip is typically less eﬃcient than 
deciding PSAT, as empirical results indicate [34].

i

i

Potyka emphasizes this impracticability and suggests a new family of inconsistency measures, the minimal violation mea-
sures [34],  which  we  adapt  here  to  the  case  of  imprecise  probabilities.  In  order  to  keep  constraints  linear,  “violation” 

7 Note that if we allow εi < 0 (and ¯εi < 0), it would represent the tightening of a bound, useless when searching for consistency, and the minimization 
would avoid it anyway.

152

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

variables  εi, ¯εi ≥ 0 are  inserted  in  the  right-hand  side  of  P π (ϕi ∧ ψi) − q
yielding  P π (ϕi ∧ ψi) − q
when the  p-norm of (cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17) is minimized with such constraints. We denote by Iε
following program, where ε = [εi

P π (ψi) ≥ 0 and  P π (ϕi ∧ ψi) − ¯qi P π (ψi) ≤ 0, 
P π (ψi) ≥ −εi and  P π (ϕi ∧ ψi) − ¯qi P π (ψi) ≤ ¯εi . Potyka’s minimal violation measures are obtained 
p the optimal value from the 

] and  ¯ε = [¯εi] are (m × 1)-vectors:

i

i

min (cid:21)(cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17)(cid:21)p subject to:
Aπ ≥ −ε
Bπ ≤ ¯ε
(cid:6)

π = 1
π , ε, ¯ε ≥ 0 .

(9)

(10)

(11)

(12)

(13)

The restrictions are all linear, and non-linear terms may appear only within the objective function. We can ignore the 
. within the p-norm deﬁnition, applying it only after the minimization stops. The degree of each term 
+ ¯εi . 
1 with  practically  the  same 

√
monotone function  p
in the new objective function is  p, and for  p = 1 a linear program is recovered, since (cid:21)(cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17)(cid:21)1 =
Hence,  one  can  apply  the  standard  Simplex  and  column  generation  methods  to  compute  Iε
eﬃciency as deciding PSAT [34].

m
i=1 εi

(cid:2)

For any ﬁnite  p different from 1, the system (9)–(13) has non-linear terms in its objective function, but this is not the 
case when we consider  p = ∞. The ∞-norm is equivalent to take the maximum of the vector (cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17), but this 
is the same as considering all εi, ¯εi equal to a single scalar ε ≥ 0. The measure Iε
∞ is the solution of the following program 
[34], in which ε = ¯ε = [ε ε . . . ε]T are (m × 1)-vectors:

min ε subject to:
Aπ ≥ −ε
Bπ ≤ ¯ε
(cid:6)

π = 1
π , ε ≥ 0 .

(14)

(15)

(16)

(17)

(18)

The system (14)–(18) is also a linear program, like (9)–(13) when  p = 1, but has a lesser number of variables. However, 
Potyka remarks that the variable ε in (14)–(18) is involved in 2m restrictions, while each variable εi, ¯εi appears in only one 
constraint in (9)–(13); therefore, the computation of Iε

1 [34].
For  sets  of  unconditional  probabilistic  assessments,  when  all  conditioning  events  ψi are  equivalent  to  (cid:6),  the  incon-
p are  extensionally  identical  for  all  p.  The  reason  is  that  the  restriction  on  P π and  εi, ¯εi
sistency  measures  Ip and  Iε
corresponding  to  a  conditional  is  the  same  when  computing  both  measures.  For  instance,  any  constraint  P π (ϕi ∧ ψi) −
¯qi P π (ψi) ≤ 0 becomes equivalent to  P π (ϕi) − ¯qi ≤ 0 when ψi is a tautology, and inserting an error to the probability bound, 
P π (ϕi) − (¯qi + ¯εi) ≤ 0, is the same as placing it in the right-hand side.

∞ may in practice be slightly less eﬃcient than computing Iε

Potyka  has  proved  that  these  measures,  Iε

p with  p ∈ N>0 ∪ {∞},  besides  being  computable  via  linear  programming, 

satisfy the postulates of consistency, continuity and monotonicity for the case of precise probabilities [34]:

Proposition 5.1. For any p ∈ N>0 ∪ {∞}, Iε
p
and monotonicity. Iε

1 also satisﬁes super-additivity.

: Kprec → [0, ∞) is well-deﬁned and satisﬁes consistency, continuity, weak independence 

We can generalize the result above to encompass probability intervals and the new postulates we introduced:

Theorem 5.2. For any p ∈ N>0 ∪ {∞}, Iε
p
monotonicity. Iε

1 also satisﬁes super-additivity and IC-separability; and Iε

∞ satisﬁes normalization.

: K → [0, ∞) is well-deﬁned and satisﬁes consistency, continuity, i-independence and 

Now we have a set of compatible postulates for inconsistency measures and two particular measures satisfying them that 
can be computed rather eﬃciently using linear programming techniques. On the one hand, Iε
1 also satisﬁes super-additivity 
and IC-separability; on the other hand, Iε
∞ additionally satisﬁes normalization. Nonetheless, one can argue that these mea-
sures lack some proper justiﬁcation, despite satisfying some postulates and being feasible, as Capotorti, Regoli and Vattari 
did [4]. They claim that distances between conditional probabilities are meaningless, being only geometrical measures. This 
might be the case, but it would only undermine the Ip family. For Iε
p measures, distances between probabilities are com-
puted weighting by the probabilities of the conditioning formulas, so to speak, allowing some operational interpretation. In 
the next section, we provide a rationale for Iε

∞ based on Dutch books.

1 and Iε

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

153

Table 2
Prizes earned by Alice (in $) for each bet and each possible world.

Possible world

Bet on Hot (x1)
Bet on Sunny (x2)
Bet on Hot and Sunny (x1 ∧ x2)
Total

x1 ∧ x2

10
10
−10
10

x1 ∧ ¬x2

¬x1 ∧ x2

¬x1 ∧ ¬x2

10
0
0
10

0
10
0
10

0
0
0
0

5.2.  Inconsistency measures and Dutch books

In formal epistemology, there is an interest in measuring the incoherence of an agent whose beliefs are given as proba-
bilities or lower previsions over propositions or random variables — a Bayesian agent. If we have propositions from classical 
logic, the formalized problem at hand is exactly the one we are investigating. When the agent’s degrees of belief are repre-
sented by a knowledge base, to measure the agent’s incoherence is to measure the inconsistency of such knowledge base. 
Schervish, Kadane and Seidenfeld [39,40,38] have proposed ways to measure incoherence of an agent based on Dutch books.
Dutch book arguments are based on the agent’s betting behavior induced by her degrees of belief, typically used to show 
their irrationality. To introduce the concept of Dutch book, we start with the unconditional case. Dutch book arguments rely 
on  an  operational  interpretation  of  (imprecise)  degrees  of  belief,  in  which  their  lower/upper  bounds  are  deﬁned  through 
, ¯qi], for 
bet buying/selling prices. Suppose an agent believes that the probability of proposition ϕi being true lies within [q
1 ≤ i ≤ m. Consider a bet ticket on the proposition ϕi that returns a prize (from the ticket seller) of λi ≥ 0 if ϕi is the case, 
and is worthless if ϕi is not the case. Dutch book arguments generally use a willingness-to-bet assumption that this agent is 
λi ≥ 0 and to sell it for  ¯qiλi ≥ 0, for any λi ≥ 0. Then, if a bettor can buy and/or 
willing to buy such a bet ticket on ϕi for q
sell a set of bet tickets from/to the agent that will cause her a sure loss no matter which possible world is the case, we say 
she is exposed to a Dutch book. This set of bet tickets that causes a guaranteed loss to the agent is called a Dutch book.

i

i

Example 5.3. Alice (the agent) and Bob (the bettor) are ﬂying to the beach. To spend the time on the plane, they discuss 
and gamble on the destination weather, to be checked on arrival — will it be sunny and/or hot (say, at least 20
C )? Alice 
assigns probability intervals for three propositions, formed by the atoms x1 =“the weather is sunny” and x2 =“the weather 
is hot”:

◦

• she believes the probability of the weather being hot is between 70% and 80%; which we represent by the conditional 

• she thinks the probability of the weather being sunny is between 50% and 60%; which is represented by (x2)[0.5, 0.6];
• she  also  says  that  the  probability  of  the  weather  being  both  hot  and  sunny  is  at  most  10%;  formalized  into 

(x1)[0.7, 0.8];

(x1 ∧ x2)[0, 0.1].

Now Bob can choose which bet tickets he wants to buy from Alice, and which ones he wants to sell, under the willingness-
to-bet assumption. He can also set the prize λi each ticket will return if the corresponding proposition turns out to be the 
case. So Bob decides to trade the following bet tickets with Alice:

• He sells to Alice a bet ticket that pays back $10 if the weather is hot (x1 is true); and 0 otherwise. Alice pays $10 × 0.7 =

$7 for it, which she considers fair;

• Bob also makes Alice buy for $10 × 0.5 = $5 a bet ticket that will return $10 to her only if the weather is sunny (x2 is 

true);

• Finally, Bob buys from Alice a bet ticket whose prize $10 is paid back only in case the weather is hot and sunny (x1 ∧ x2

is true); and Alice sells it for $10 × 0.1 = $1.

Combining the three bet tickets, Bob received $7 + $5 = $12 from Alice and returned $1 to her, so that before they arrive 
and prizes are paid Alice is losing $11. Table 2 shows the prizes Alice can earn from each bet ticket traded for each possible 
weather; a negative quantity means Alice has to pay it to Bob.

Note that, no matter how is the weather when they arrive and pay the prizes, the total quantity Alice can receive from 
Bob is at most $10. Since she was losing $11 before landing and checking the weather, she will lose at least $1 in the end. 
Given this sure loss scenario, this set of three bets is said to be a Dutch book.

Instead of the agent (the bettor) paying for a bet ticket and eventually getting its prize back from the bettor (the agent), 

we can view this whole operation as a single contract between these two players.

Deﬁnition 5.4.  A  gamble on  ϕ ∈ L Xn is  an  agreement  between  the  agent  and  the  bettor  with  two  parameters,  the  stake
λ ∈ R and the relative price q ∈ [0, 1], stating that:

154

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

• the agent pays λ × q to the bettor if ϕ is false;
• the bettor pays λ × (1 − q) to the agent if ϕ is true.

A gamble on ϕ with stake λ ≥ 0 and relative price q is equivalent to the agent buying from the bettor a bet ticket for 
λ × q that returns λ only if ϕ is the case; if the stake λ is negative, the gamble is equivalent to the bettor buying the same 
ticket  from  the  agent.  The  willingness-to-bet  assumption  translates  to  gambles  in  the  following  way:  if  an  agent  believes 
that the probability of a proposition ϕ being true lies within [q, ¯q], she ﬁnds acceptable gambles on ϕ with any stake λ ≥ 0
and relative price q and gambles with any stake λ ≤ 0 and relative price  ¯q. In Example 5.3, the tickets trading is equivalent 
to a set of three gambles: a gamble on x1 with stake $10 and relative price 0.7; a gamble on x2 with stake $10 and relative 
price 0.5; and a gamble on x1 ∧ x2 with stake $−10 and relative price 0.1.

A gamble on ϕ can be generalized to consider a conditioning event ψ . Consider a bet ticket that, when ψ is true, pays a 
prize of λ if ϕ is the case and returns 0 if ϕ is false. In other words, this bet ticket works as a gamble on ϕ when ψ is the 
case. However, suppose this bet ticket pays back to the agent the same amount that was spent in its buying if ψ is false — 
that is, the gamble is canceled. The following generalization of gambles capture these “conditional bets”:

Deﬁnition 5.5. A (conditional) gamble on ϕ|ψ ∈ L Xn
rameters, the stake λ ∈ R and the relative price q ∈ [0, 1], stating that:

|L Xn is an agreement between the agent and the bettor with two pa-

• the agent pays λ × q to the bettor if ψ is true and ϕ is false;
• the bettor pays λ × (1 − q) to the agent if ψ is true and ϕ is true;
• the gamble is called off, causing neither proﬁt nor loss to the involved parts, if ψ is false.

Accordingly, we generalize the willingness-to-bet assumption: if an agent believes that the probability of a proposition ϕ
being true given that another proposition ψ is true lies within [q, ¯q], she ﬁnds acceptable gambles on ϕ|ψ with stake λ ≥ 0
and relative price q and gambles with stake λ ≤ 0 and relative price  ¯q. A Dutch book is a set of (conditional) gambles that 
the agent sees as fair, under the willingness-to-bet assumption, that causes her a guaranteed loss no matter which possible 
, ¯qi] ∈ (cid:6), 
world is the case. We assume Dutch books contain exactly two gambles on (ϕi |ψi) per each conditional (ϕi|ψi)[q
≥ 0 and  the  other  with  stake  −¯λi ≤ 0.  This  is  not  restrictive, 
the  base  formalizing  the  agent’s  beliefs:  one  with  stake  λi
since gambles on the same (ϕi|ψi) with the same relative price can be merged by summing the stakes, and the absence of 
a gamble is equivalent to a stake equal to zero. We can thus denote a Dutch book simply by the absolute value of its stakes 
λ1, ¯λ1, . . . , λm, ¯λm ≥ 0, where m = |(cid:6)|. Actually, any set of gambles involving an agent whose epistemic state is represented 
by (cid:6) can be represented by these 2m (absolute value of) stakes, since the relative prices are set in (cid:6).

i

If the set of probabilistic conditionals that represents an agent’s epistemic state turns out to be inconsistent, then she 
is  exposed  to  a  Dutch  book,  and  vice-versa  [32].  In  other  words,  an  agent  sees  as  fair  a  set  of  gambles  that  causes  her 
a guaranteed loss if, and only if, the knowledge base codifying her (conditional) degrees of belief is inconsistent. We can 
check this connection in Example 5.3: (x1)[0.7, 0.8] and (x2)[0.5, 0.6] imply a probability of at least 0.2 for x1 ∧ x2, which 
Alice violates. Consequently, she is exposed to a Dutch book, for her three probability interval assignments are not jointly 
satisﬁable.  In  this  way,  Dutch  book  arguments  were  introduced  to  show  that  degrees  of  belief  must  obey  the  axioms  of 
probability and are a standard proof of incoherence (introductions to Dutch books and their relation to incoherence can be 
found in [42] and [8]). Hence, a natural approach to measuring an agent’s degree of incoherence is through the magnitude 
of the sure loss she is vulnerable to. The intuition says that, the more incoherent an agent is, the greater the guaranteed 
loss that can be imposed on her through a Dutch book. Nevertheless, with no bounds on the stakes, such loss would also 
be unlimited for incoherent agents. For instance, in Example 5.3, if stakes were  $100,  $100 and  $−100, Alice would have 
a net loss of at least  $10, instead of  $1, regardless of the weather on arrival. To better understand the loss a Dutch book 
causes to an agent, we formalize it in the following.
, ¯qi]|1 ≤ i ≤ m} representing an agent’s epistemic state. Let λi, ¯λi ≥ 0 denote 
Consider the knowledge base (cid:6) = {(ϕi|ψi)[q
i
≥ 0, the second with relative price  ¯qi and stake −¯λi ≤ 0, for 
gambles on (ϕi|ψi), the ﬁrst with relative price q
1 ≤ i ≤ m. A set of gambles can then be represented by the vector (cid:16)λ1, ¯λ1, . . . , λm, ¯λm(cid:17). If a possible world  w j is the case, 
the net proﬁt for the agent regarding a bet on ϕ|ψ with stake λ and relative price q can be computed via

and stake λi

i

λ(I w j (ϕ ∧ ψ) − qI w j (ψ)),

: L Xn

→ {0, 1} is the indicator function of the set {ϕ ∈ L Xn
|w j |(cid:9) ϕ} — a valuation. For a gamble on (ϕi|ψi)
in which  I w j
I w j (ψi)) (or −¯λi(I w j (ϕi ∧ ψi) −
with stake λi (or −¯λi ), the agent’s net proﬁt in a possible word w j is λi(I w j (ϕi ∧ ψi) − q
¯qi I w j (ψi))). Recall (from (5)–(8)) that ai j = I w j (ϕi ∧ ψi) − q
I w j (ψi) and bi j = I w j (ϕi ∧ ψi) − ¯qi I w j (ψi). If a given possible 
(cid:2)
¯λi . Let (cid:13) be 
world  w j is the case, the set of gambles (cid:16)λ1, ¯λ1, . . . , λm, ¯λm(cid:17) gives the agent a proﬁt of 
the sure loss (−(cid:13) is proﬁt) a set of gambles yields to the agent; i.e., no matter which possible world is the case, the agent 
¯λi ≤ −(cid:13) for all possible worlds  w j . When there is no restriction 
loses at least (cid:13). Thus, (cid:13) is such that 
on the stakes, to ﬁnd the set of gambles (cid:16)λ1, ¯λ1, . . . , λm, ¯λm(cid:17) that maximizes the sure loss is to solve the following linear 
program:

m
i=1 ai jλi

m
i=1 ai jλi

−bi j

−bi j

m
i=1

m
i=1

(cid:2)

(cid:2)

(cid:2)

+

+

i

i

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

max (cid:13) subject to:

⎡

⎢
⎢
⎣

1
1

a11
a12
...
...
1 a12n

am1 −b11
. . .
am2 −b12
. . .
...
...
. . .
. . . am2n −b12n

⎤

⎥
⎥
⎦

. . . −bm1
. . . −bm2
. . .
. . . −bm2n

...

λ1, ¯λ1, . . . , λm, ¯λm ≥ 0 .

⎤

⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

⎡

⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣

(cid:13)
λ1
...
λm¯λ1
...
¯λm

⎡

⎢
⎢
⎣

⎤

⎥
⎥
⎦

0
0

...

0

≤

155

(19)

(20)

(21)

The linear program above can be viewed as the dual of that in lines (5)–(8), which checks the consistency of (cid:6), if we 
consider that 0 is the function being minimized in the latter, since we are interested only in its feasibility (for duality theory 
in linear programing, see, for instance, [46]). Note that, in (5)–(8),  Bπ ≤ 0 is equivalent to −Bπ ≥ 0 and 
π = 1 can be 
inserted into  A as a line of 1’s. By duality theory, as the program above is feasible, it is unbounded iff (5)–(8) is infeasible. 
That is, if (cid:6) is inconsistent, sure loss via Dutch book is unlimited.

(cid:2)

Different strategies to circumvent this in order to measure incoherence as a ﬁnite loss are found in the formal epistemol-
ogy literature. Schervish et al. propose a ﬂexible formal approach to limiting these stakes generating a family of incoherence 
measures  for  upper  and  lower  previsions  on  bounded  random  variables  [40].  In  this  section,  we  are  interested  in  two  of 
them, which we simplify to our case.

Their  whole  family  of  incoherence  measures  is  based  on  the  maximum  guaranteed  loss  an  agent  is  exposed  to  via  a 
Dutch book, varying only on how stakes are limited. The ﬁrst incoherence measure Schervish et al. introduce that concerns 
+ ¯λi ≤ 1. The second incoherence 
us is when the sum of the absolute values of the stakes is lesser than or equal to one, 
measure we investigate is deﬁned as the maximum guaranteed loss when each stake have absolute value no greater than 
one,  or  λi, ¯λi ≤ 1.8 We  deﬁne  the  inconsistency  measures  I sum
: K → [0, ∞) on  knowledge  bases 
SSK
as these two incoherence measures on the corresponding agents represented by these knowledge bases. That is, we equate 
SSK ((cid:6)) (and  I max
I sum
SSK ((cid:6))),  for  any  (cid:6) ∈ K,  to  the  maximum  sure  loss  an  agent  whose  epistemic  state  is  represented  by  (cid:6) is 
exposed to through a Dutch book when the sum (maximum) of the stakes’ absolute values is at most one.

: K → [0, ∞) and  I max
SSK

i λi

(cid:2)

Example 5.6. Recall Example 5.3, in which there are three gambles, with stakes $10, $10 and $−10. These gambles guarantee 
a loss of at least $1 to Alice. But now suppose that Bob, while choosing the gambles, must do it so that the absolute values 
of the stakes sum up to one. He could so arrange the same gambles but changing the stakes to 1/3, 1/3 and −1/3. In this 
new scenario, Alice would have a sure loss of 1/30. Similarly, if the absolute value of each stake is limited to the interval 
[0, 1], stakes could be 1, 1 and −1, yielding a guaranteed loss of 1/10 to Alice. In fact, it can be checked (by solving the 
linear programs) that 1/30 and 1/10 are the greatest amount one can take for sure from Alice via Dutch book if stakes have 
absolute values summing up to one or are all in [0, 1], respectively. Formalizing, with (cid:6) = {(x1)[0.7, 0.8], (x2)[0.5, 0.6], (x1 ∧
x2)[0, 0.1]} codifying Alice’s epistemic state, we have I sum

SSK ((cid:6)) = 1/30 and I max

SSK ((cid:6)) = 1/10.

Even though incoherence measures based on Dutch books from the formal epistemology community and inconsistency 
measures  based  on  distance  minimization  from  Artiﬁcial  Intelligence  researchers  may  seem  unrelated  at  ﬁrst,  they  are 
actually  two  sides  of  the  same  coin.  The  programs  that  compute  the  maximum  guaranteed  loss  an  agent  is  exposed  to 
are  technically  dual  to  those  that  minimize  distances  to  measure  inconsistency.  Nau  has  already  investigated  this  matter, 
mentioning results similar to the following [32]:

Theorem 5.7. For any (cid:6) ∈ K, I sum

SSK ((cid:6)) = Iε

∞((cid:6)).

Proof. Just add the constraint λ1
would  become  the  program  (14)–(18),  which  computes  Iε
both programs are always feasible. (cid:2)

+ ¯λ1 + · · · + λm

+ ¯λm ≤ 1 to the linear program (19)–(21). The dual of this new program 
∞((cid:6)),  for 

∞((cid:6)).  So,  by  the  strong  duality  theorem,  I sum

SSK ((cid:6)) = Iε

Recall that Iε

∞ is exactly one of the two feasible measures proposed by Potyka [34]. Far from meaningless, such mea-
sure  quantiﬁes  the  maximum  sure  loss  an  agent  is  exposed  to  when  the  sum  of  the  stakes  is  no  greater  than  one  —  or, 
equivalently, ﬁxed at one.

As  to  Potyka’s  other  feasible  proposal,  Iε

1 ,  duality  in  linear  programming  provides  a  correspondence  with  the  second 

incoherence measure we presented from Schervish et al.:

8 Schervish et al. [40] actually measure the incoherence as maximum rates between the guaranteed loss and the sum (the maximum) of the stakes’ 
absolute values. Clearly, this is equivalent to maximizing the guaranteed loss when the sum of the stakes’ absolute values is no greater than 1 (or these 
absolute values are in [0, 1]).

156

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

Theorem 5.8. For any (cid:6) ∈ K, I max

SSK ((cid:6)) = Iε

1 ((cid:6)).

Proof. Similarly  to  the  proof  of  Theorem 5.7,  insert  the  constraints  λi, ¯λi ≤ 1,  for  1 ≤ i ≤ m,  into  the  linear  program 
(19)–(21). The dual of this new program would become the program (9)–(13), with  p = 1, which computes Iε
1 ((cid:6)). Again, 
by the strong duality theorem, I max

1 ((cid:6)), since both programs are always feasible. (cid:2)

SSK ((cid:6)) = Iε

Theorem 5.8 states the extensional identity between Iε

SSK . Within the unconditional probabilities scenario, this 
means that the Manhattan distance between the agent’s probabilities and the closest consistent probabilities is equal to the 
maximum sure loss she is exposed to when stakes’ absolute values are not higher than one.

1 and I max

Theorem 5.7 and  Theorem 5.8 give  an  operational  interpretation  for  the  inconsistency  measures  Iε

betting  behavior.  It  was  remarked  in  Section 5.1 that  Iε
knowledge bases. Thus, Dutch books with limited stakes (λi, ¯λi ≤ 1 or 
I∞ in the unconditional setting. However, when we take into account conditional probabilities, only Iε
the  maximum  guaranteed  loss  an  agent  would  be  exposed  to,  when  stakes  are  limited  via  λi, ¯λi ≤ 1 or 
respectively.

1 based  on 
p and  Ip give  the  same  inconsistency  degrees  to  unconditional 
+ ¯λi ≤ 1) can be used to rationalize also I1 and 
∞ measure 
+ ¯λi ≤ 1, 

1 and Iε
(cid:2)
i λi

∞ and  Iε

i λi

(cid:2)

Different strategies for bounding stakes can lead to different inconsistency measures, but our motivation in this section 
was not to use Dutch books to determine which measures should be adopted — that is the reason of the postulates. The 
point here is that these two measures (Iε
∞), besides satisfying the postulates and being computable through linear 
programs, have a meaningful interpretation. In the next section, we show that other measures based on Dutch books have 
these qualities as well.

1 and Iε

5.3.  Other feasible principled measures

In order to measure incoherence as the greatest guaranteed loss in a Dutch book, Schervish et al. have ﬁrstly proposed 
two  different  ways  of  normalizing  such  loss:  by  limiting  either  the  agent’s  or  the  bettor’s  resources  [39].  The  authors 
introduce the concept of escrow as the amount committed into a gamble by the agent (or the bettor). For instance, consider 
a gamble on ϕi|ψi with stake λi
λi with this gamble, while the bettor is 
)λi . Now consider a gamble on the same conditional with stake −¯λi ≤ 0 and relative price  ¯qi . 
exposed to a loss of (1 − q
¯λi ≥ 0 to the agent. Schervish et al. call 
The agent might have to pay (1 − ¯qi)¯λi to the bettor, whilst the bettor might lose  ¯qi
these quantities the agent’s and the bettor’s escrows. Equivalently, the agent’s (or bettor’s) escrow for a gamble is how much 
she (he) has to commit from her (his) resources to cover an eventual loss.

≥ 0 and relative price q

. The agent might lose q

i

i

i

Instead of bounding the sum of the stakes, an agent’s degree of incoherence can be measured, as the maximum guar-
anteed loss in a Dutch book, by limiting the agent’s (or the bettor’s) total escrow to one.9 In other words, we are limiting 
how  much  the  agent  (or  the  bettor)  could  lose  in  case  that  every  gamble  resolves  unfavorably,  inﬂicting  a  loss  to  her 
(him).  Schervish  et  al.  give  market  situations  that  justiﬁes  these  choices  [39].  We  denote  by  Ia,sum
: K → [0, ∞) and 
Ib,sum
K → [0, ∞) ∪ {∞}10 the inconsistency measures corresponding to these two incoherence measures, when the agent’s 
SSK
or the bettor’s total escrow is at most one, respectively.

SSK

Formally, starting with the linear program of lines (19)–(21), Ia,sum

((cid:6)) and Ib,sum

SSK

of (cid:13) by adding further constraints. Let (cid:6) = {(ϕi|ψi)[q
(cid:2)
m
to insert the restriction 
i=1 q
(19)–(21) incremented with the constraint 

+ (1 − ¯qi)¯λi ≤ 1 into (19)–(21). Similarly, I g,sum
)λi

i=1(1 − q

¯λi ≤ 1.

+ ¯qi

(cid:2)

λi

SSK

m

i

i

i

, ¯qi]|1 ≤ i ≤ m} be a knowledge base. To compute Ia,sum

SSK

((cid:6)) are obtained via the maximization 
((cid:6)), one need 
((cid:6)) is the solution (on (cid:13)) of the program 

SSK

The fact that I g,sum

SSK may be unbounded is acknowledged by Schervish et al. [39]. For instance, consider an agent whose 
belief  state  is  given  by  (cid:6) = {(ϕ)[1], (¬ϕ)[1]}.  The  agent  ﬁnds  acceptable  pairs  of  gambles  on  (ϕ and  ¬ϕ)  in  which  the 
bettor has escrows equal to zero (λi(1 − q
= 1), and sure loss can be scaled arbitrarily up. In such cases, we 
deﬁne Ib,sum

) = 0, for q

((cid:6)) = ∞.

i

i

SSK

Example 5.9.  Recall  Example 5.3,  its  three  gambles,  with  stakes  $10,  $10 and  $−10,  and  the  implied  loss  of  at  least  $1
to  Alice.  But  now  suppose  that  Bob  has  to  choose  gambles  in  such  a  way  that  his  (or  Alice’s)  total  escrow  sum  up  to  1. 
Note that, with stakes $10, $10 and $−10, his total escrow is $10 × (1 − 0.7) + $10 × (1 − 0.5) + $10 × 0.1 = $9 (Alice’s is 
$10 × 0.7 + $10 × 0.5 + $10 × (1 − 0.1) = $21). He could then arrange the same gambles but changing the stakes to 10/9, 
10/9 and −10/9 (or 10/21, 10/21 and −10/21) in order to his (Alice’s) total escrow be equal to one. In this new scenario, 
Alice would have a sure loss of 1/9 (or 1/21). Once again, one could verify (by solving the linear programs) that 1/9 and 
1/21 are the greatest amount one can take for sure from Alice via Dutch book if Bob’s or Alice’s total escrow is no greater 
than  1,  respectively.  Formalizing,  with  (cid:6) = {(x1)[0.7, 0.8], (x2)[0.5, 0.6], (x1 ∧ x2)[0, 0.1]} codifying  Alice’s  epistemic  state, 
we have Ib,sum

((cid:6)) = 1/9 and Ia,sum

((cid:6)) = 1/21.

SSK

SSK

9 Again, this is equivalent to measuring incoherence as the maximum ratio between the sure loss and the agent’s (the bettor’s) total escrow.
10 For reasons that will be clear soon, we relax in this section the deﬁnition of inconsistency measures, allowing their range to include ∞.

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

157

Schervish et al. contemplate in detail a whole spectrum of ways to bound the agent’s escrows, the bettor’s, or their sum 
in order to measure the maximum sure loss [40]. For each of these three quantities, the author note that the two extreme 
functions in their framework used to normalize the guaranteed loss are the maximum and the sum, from which six different 
inconsistency measures arise [38]. Note that the sum of the agent’s and the bettor’s escrows for a single gamble is equal to 
the absolute value of its stake, so I sum
SSK are two inconsistency measures from this same framework. To build the 
remaining two measures, escrows could be bounded via their maximum, instead of their total. Intuitively, this corresponds 
to limiting the quantity the agent (or the bettor) accepts to eventually lose in each individual gamble.

SSK and I max

SSK

: K → [0, ∞) (and  Ib,max

We  deﬁne  the  inconsistency  measure  Ia,max

K → [0, ∞) ∪ {∞})  on  knowledge  bases  as 
the  degree  of  incoherence  of  the  corresponding  agents  measured  via  the  maximum  sure  loss  she  is  exposed  through 
a  Dutch  book  if  the  agent’s  (the  bettor’s)  escrow  for  each  gamble  in  no  greater  than  one.  To  compute  Ia,max
((cid:6)),  for 
, ¯qi]|1 ≤ i ≤ m},  we  may  again  use  the  linear  program  (19)–(21) and  compute  the  maximum  value  of  (cid:13)
(cid:6) = {(ϕi|ψi)[q
((cid:6)) is the solution (on (cid:13)) to the program 
with extra constraints q
formed  by  inserting  the  restrictions  (1 − q
,  we  deﬁne 
Ib,max
SSK

≤ 1 and (1 − ¯qi)¯λi ≤ 1, for 1 ≤ i ≤ m. Similarly, Ib,max
≤ 1 and  ¯qi

¯λi ≤ 1,  for  1 ≤ i ≤ m,  into  (19)–(21).  As  with  Ib,sum

((cid:6)) = ∞ when such program is unbounded.

)λi

λi

SSK

SSK

SSK

SSK

i

i

i

Example 5.10. Remember the scenario from Example 5.3, in which three gambles are considered, with stakes $10, $10 and 
$−10, and Alice has a guaranteed loss of at least $1. Now suppose that Bob can only choose gambles in which his (Alice’s) 
eventual  loss  —  the  escrow  —  is  lesser  than  or  equal  to  one.  In  other  words,  his  (her)  maximum  escrow  is  no  greater 
than  1.  With  these  constraints,  Bob  can  choose  the  same  three  gambles,  but  with  stakes  2,  2 and  −2:  his  escrows  are 
2 × (1 − 0.7) = 0.6, 2 × (1 − 0.5) = 1 and 2 × 0.1 = 0.2 (with stakes 10/9, 10/9 and −10/9, Alice’s escrow are (10/9) × 0.7 =
7/9,  10/9 × 0.5 = 5/9 and  10/9 × (1 − 0.1) = 1). Note that Bob (Alice) can eventually lose at most  1 in a single gamble. 
In this new setting, Alice would have a guaranteed loss of 1/5 (or 1/9). By solving the corresponding linear programs, we 
would ﬁnd that  1/5 and  1/9 are the greatest amount one can take for sure from Alice via Dutch book if Bob’s or Alice’s 
maximum  escrow  is  no  greater  than  1,  respectively.  Formalizing,  with  (cid:6) = {(x1)[0.7, 0.8], (x2)[0.5, 0.6], (x1 ∧ x2)[0, 0.1]}
codifying Alice’s epistemic state, we have Ib,max

((cid:6)) = 1/5 and Ia,max

((cid:6)) = 1/9.

SSK

SSK

These  four  inconsistency  measures  (Ia,sum

,  Ib,sum
SSK

,  Ia,max
SSK

and  Ib,max

SSK

)  based  on  limiting  the  escrows  have  most  of  the 

SSK

desirable properties we presented.

Theorem 5.11. Ia,sum
and Ib,max

, Ia,max
SSK
also satisfy super-additivity and IC-separability.

and Ib,max

, Ib,sum
SSK

SSK

SSK

SSK

are well-deﬁned and satisfy consistency, i-independence and monotonicity. Ia,max

SSK

Lemma 5.12. Ia,sum

SSK

, Ia,max
SSK

, Ib,sum
SSK

and Ib,max

SSK

are continuous for probabilities within (0, 1).

Lemma 5.13. Ia,sum

SSK

satisﬁes normalization.

, Ib,sum
SSK

Altogether, Ia,sum

SSK

are all computable through linear programs, have the core desirable proper-
and Ib,max
also satisfy super-additivity and IC-separability, while 
SSK
is normalized. These measures can be good alternatives, depending on the context, as the market scenarios described 

ties and can be given an operational interpretation. Ia,max
Ia,sum
SSK
by Schervish et al. [39].

SSK

SSK

and Ib,max

, Ia,max
SSK

6.  Conclusions and future work

Handling inconsistency has been receiving increased attention in the AI community since most inference methods rely 
on the consistency of the premises; and such requirement is commonly violated in large bases of probabilistic knowledge. 
A reasonable  start  point  to  deal  with  the  inconsistency  in  probabilistic  bases  is  to  know  how  severe  it  is,  and  how  this 
severity changes with the probabilities. In this work, we studied different ways of measuring inconsistency in probabilistic 
knowledge bases. Three aspects were discussed: postulates the measures should satisfy, the eﬃciency of the methods used 
to compute the measures, and possible meaningful interpretations for them. As it was argued for, the independence pos-
tulate shall be abandoned in favor of continuity. The causes of such incompatibility were analyzed, and a modiﬁcation of 
independence was proposed to restore compatibility. Inconsistency measures that can be computed using linear programs 
were  reviewed  and  proved  to  satisfy  the  postulates,  and  we  gave  them  a  rational  by  means  of  Dutch  books.  Finally,  we 
showed  that  other  measures  in  the  literature  based  on  Dutch  books,  and  computable  through  linear  programming,  also 
satisfy the postulates.

By  restoring  the  compatibility  of  the  postulates  for  measuring  inconsistency  in  probabilistic  knowledge  bases,  we  put 
forward a new pair of properties one can use to formulate or evaluate measures: i-independence and IC-separability. These 
desirable  properties  are  based  on  two  new  concepts:  innocuous  conditional  and  inescapable  conﬂicts.  Besides  measuring 

158

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

inconsistency,  these  concepts  may  be  useful  for  formalizing  inference  from  inconsistent  bases  or  performing  probabilistic 
belief  revision/update,  for  instance.  Both  concepts  are  derived  from  a  speciﬁc  consolidation  procedure  —  the  widening  of 
probability intervals. If other consolidation methods are considered, one can deﬁne analogous concepts, even in a different 
logical formalism.

In  AI,  inconsistency  measures  for  probabilistic  knowledge  bases  have  been  based  on  distance  minimization,  while  in 
Formal Epistemology incoherence measures for Bayesian agents were focused on Dutch books vulnerability. The connections 
here established can help both communities to investigate their corresponding problems under a different angle.

The  introduced  concepts  of  innocuous  conditional  and  inescapable  conﬂict  might  have  practical  use  in  measuring  in-
consistency  only  if  their  instances  are  recognizable  in  a  reasonable  time.  Nothing  was  said  here  about  the  complexity  of 
the computational task of ﬁnding innocuous conditionals and inescapable conﬂicts within a knowledge base, but they are 
clearly very hard problems. Thus, future work includes investigating these problems aiming at devising algorithms to solve 
them. It would also be interesting to propose concrete procedures to consolidate knowledge bases, as done in [35] for in-
stance. To achieve that, one could rely on the same triplet: rationality postulates, eﬃciency of computation and meaningful 
interpretation. Another intended continuation of this work is to study principled ways of inferring probabilistic conclusions 
from  inconsistent  bases,  using  the  ideas  here  presented.  For  instance,  this  could  be  done  by  deﬁning  the  set  of  models 
of  an  inconsistent  base  as  the  set  containing  all  models  of  each  closest  consistent  base,  construed  as  the  consolidations 
corresponding to some inconsistency measure here studied.

Appendix A.  Proofs of technical results

Proposition 3.6. If I satisﬁes MIS-separability, then I satisﬁes independence.

Proof. Let (cid:6) be a knowledge base and α ∈ (cid:6) a free conditional. By MIS-separability, as α is free and all MISes of (cid:6) are in 
(cid:6) \ {α}, we have I((cid:6)) = I((cid:6) \ {α}) + I(α). (cid:2)

Corollary 3.9. There is no inconsistency measure I : Kprec → [0, ∞) that satisﬁes consistency, MIS-separability and continuity.

Proof. It follows directly from Theorem 3.8 and Proposition 3.6. (cid:2)

Theorem 4.2. Consider a knowledge base (cid:6) ∈ K and a probabilistic conditional α ∈ (cid:6). The following statements are equivalent:

1. There is no minimal abrupt repair (cid:8) of (cid:6) such that α ∈ (cid:8).
2. For all maximal abrupt consolidation (cid:6)(cid:12)
3. If (cid:6)(cid:12) = (cid:6) \ (cid:8) is an abrupt consolidation of (cid:6) (equivalently, (cid:8) is an abrupt repair of (cid:6)), then α is consistent with (cid:6)(cid:12)
4. There is no minimal inconsistent set (cid:8) ⊆ (cid:6) such that α ∈ (cid:8).

of (cid:6), α ∈ (cid:6)(cid:12)

.

.

Proof. The ﬁrst two items are clearly dual, and the fourth one is the deﬁnition of free conditional. Suppose α is free in (cid:6). 
Note that all abrupt consolidations (cid:6)(cid:12)
is consistent, it has no MIS, and adding α cannot 
create a MIS, for it is free. Thus, if an abrupt consolidation does not contain α, it is not maximal. Now suppose there is a 
maximal abrupt consolidation (cid:6)(cid:12)
is consistent, it 
has no MIS, and adding α creates a MIS (that contains α), which also is a MIS of (cid:6) — hence, α cannot be free. (cid:2)

is maximal, α cannot be consistent with it. As (cid:6)(cid:12)

of (cid:6) are consistent with α. As (cid:6)(cid:12)

such that α /∈ (cid:6)(cid:12)

. For (cid:6)(cid:12)

Lemma 4.5. Consider a knowledge base (cid:6) ∈ K and a probabilistic conditional α ∈ (cid:6). The following statements are equivalent:

1. For all d-consolidation (cid:6)(cid:12)
2. If (cid:6)(cid:12)

is a consolidation, then α is consistent with (cid:6)(cid:12)

.

of (cid:6), α ∈ (cid:6)(cid:12)

.

Proof. Suppose  all  d-consolidations  of  (cid:6) contain α.  For  any  consolidation  (cid:9),  there  is  a  d-consolidation  (cid:9)(cid:12)
such  that,  for 
each β(cid:12) ∈ (cid:9)(cid:12)
, there is a β ∈ (cid:9) such that β(cid:12) ⊆ β. Therefore, any probability mass π satisfying (cid:9)(cid:12)
must also satisﬁes (cid:9), and 
α ∈ (cid:9)(cid:12)
implies π satisﬁes α as well. Now suppose there is a d-consolidation (cid:9) that does not contain α. As α ∈ (cid:6), there is 
a β ∈ (cid:9) such that α (cid:2) β. For (cid:9) is dominant, ((cid:9) \ {β}) ∪ {α} cannot be a consolidation and thus is inconsistent. Finally, α is 
not consistent with (cid:9). (cid:2)

Proposition 4.8. Consider a probabilistic conditional α ∈ (cid:6). If α is safe, it is innocuous; if α is innocuous, it is free.

Proof. If  (cid:6) = {α},  then  α is  safe,  innocuous  and  free  iff  it  is  satisﬁable,  thus  we  focus  on  (cid:6) (cid:20)= {α}.  Let  (cid:6) be  built  over 
the  set  of  atoms  Xn = {x1, . . . , xn}.  Suppose  α is  safe  and,  without  loss  of  generality,  the  set  of  atoms  appearing  in  α is 
Xα = {x1, . . . , xm},  for  some  m < n.  As  α is  satisﬁable,  there  is  a  probability  mass  πα : W Xα
→ [0, 1] satisfying  it,  where 
W Xα is the set containing the 2m possible worlds with atoms from  Xα . The base (cid:6)(cid:12) = (cid:6) \ {α} is built over the set of atoms 
X(cid:6)(cid:12) = Xn \ Xα .  Any  consolidation  (cid:9) of  (cid:6)(cid:12)
must  also  be  formed  by  atoms  in  X(cid:6)(cid:12) .  If  (cid:8) is  a  consolidation  of  (cid:6),  there  is  a 

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

159

consolidation (cid:9) of (cid:6)(cid:12)
such that (cid:8) = (cid:9) ∪ {β}, for some β such that α ⊆ β. Let π(cid:9) : W X(cid:6)(cid:12) → [0, 1] be the probability mass 
is  the  set  containing  the  2n−m possible  worlds  with  atoms  from  X(cid:6)(cid:12) .  Consider  the  probability 
satisfying  (cid:9),  where  W X(cid:6)(cid:12)
mass π : W Xn
× W X(cid:6)(cid:12) . Note that π satisﬁes 
(cid:9) and α, thus π satisﬁes (cid:9) and β. Therefore, α is consistent with any consolidation (cid:8) = (cid:9) ∪ {β} of (cid:6) and is innocuous by 
Lemma 4.5.

→ [0, 1] such that π (w i ∧ w j) = πα(w i) × π(cid:9) (w j) for any pair (w i, w j) ∈ W Xα

Now  suppose  α is  innocuous.  Any  abrupt  consolidation  (cid:8) ⊆ (cid:6) is  equivalent  (and  equisatisﬁable)  to  a  consolidation 
(cid:8)(cid:12) ∈ (cid:6) such that (cid:8)(cid:12) = (cid:8) ∪ {(ϕ|ψ)[0, 1]|(ϕ|ψ)[q, ¯q] ∈ (cid:6) \ (cid:8)}. As α is innocuous, it is consistent with any consolidation (cid:8)(cid:12)
and, consequently, any abrupt consolidation (cid:8). Finally, by Theorem 4.2, α is free. (cid:2)

Corollary 4.10. If I satisﬁes independence, then I satisﬁes i-independence. If I satisﬁes i-independence, then I satisﬁes weak inde-
pendence.

Proof. It follows directly from the deﬁnitions and Proposition 4.8. (cid:2)

Proposition 4.11. A knowledge base (cid:6) is a minimal inconsistent set iff (cid:6) is inconsistent and there are no (cid:8)1, . . . , (cid:8)k (cid:2) (cid:6), with k ≥ 1, 
such that:
(cid:3)
k

i=1 (cid:8)i = (cid:6);

1.
2. For every (cid:6)(cid:12) ⊆ (cid:6) if (cid:6)(cid:12) ∩ (cid:8)i is an abrupt consolidation of (cid:8)i for all 1 ≤ i ≤ k, then (cid:6)(cid:12)

is an abrupt consolidation of (cid:6).

Proof. (→) Suppose (cid:6) is a MIS and there are (cid:8)1, . . . , (cid:8)k (cid:2) (cid:6) satisfying both items. For any  1 ≤ i ≤ k, as (cid:8)i (cid:2) (cid:6) is con-
sistent,  (cid:6) ∩ (cid:8)i is  an  abrupt  consolidation  of  (cid:8)i .  Thus,  by  the  second  item,  (cid:6) is  an  abrupt  consolidation  of  itself,  which 
contradicts the fact that (cid:6) is inconsistent.

(←)  Now  suppose  (cid:6) is  inconsistent  but  not  a  MIS.  Let  MIS((cid:6)) = {(cid:8)1, . . . , (cid:8)m} be  the  set  of  MISes  in  (cid:6),  for  some 
(cid:3)
m+1
i=1 (cid:8)i = (cid:6). Now consider a set (cid:6)(cid:12) ⊆ (cid:6) such that (cid:6)(cid:12) ∩ (cid:8)i
was  inconsistent,  it  would  contain  a  MIS  (cid:8)i ∈ MIS((cid:6)) and  (cid:6)(cid:12) ∩ (cid:8)i would  be 

m ≥ 1. Let (cid:8)m+1 denote the set of free formulas in (cid:6). Clearly, 
is  consistent  for  any  1 ≤ i ≤ m + 1.  If  (cid:6)(cid:12)
inconsistent — a contradiction. Thus (cid:6)(cid:12)

is an abrupt consolidation of (cid:6). (cid:2)

Lemma 4.13. A knowledge base (cid:6) is an inescapable conﬂict iff there is a widening (cid:6)(cid:12)

of (cid:6) such that (cid:6)(cid:12)

is a minimal inconsistent set.

(cid:3)
k

Proof. (←)  Consider  a  minimal  inconsistent  set  (cid:6)(cid:12)
that  is  a  widening  of  (cid:6).  To  prove  by  contradiction,  suppose  (cid:6) is  not 
an inescapable conﬂict. As its widening (cid:6)(cid:12)
is inconsistent, (cid:6) also is, for each conditional in (cid:6)(cid:12)
is implied by a conditional 
(cid:3)
i=1 (cid:8)i = (cid:6) and,  if  (cid:8)(cid:12)
in  (cid:6).  Hence,  as  (cid:6) is  not  an  inescapable  conﬂict,  there  must  be  (cid:8)1, . . . , (cid:8)k (cid:2) (cid:6) such  that 
k
is  a 
i=1 (cid:8)(cid:12)
i=1 (cid:8)(cid:12)
consolidation of (cid:8)i , for all 1 ≤ i ≤ k, and 
i is a consolidation of (cid:6). Consider such 
i is a widening of (cid:6), then 
, deﬁned via (cid:9)i = {β ∈ (cid:6)(cid:12)|α ∈ (cid:8)i and α ⊆ β}, 
collection (cid:8)1, . . . , (cid:8)k (cid:2) (cid:6). Note that, for each (cid:8)i , there is a widening (cid:9)i (cid:2) (cid:6)(cid:12)
for 1 ≤ i ≤ k. Furthermore any (cid:9)i (cid:2) (cid:6)(cid:12)
i=1 (cid:9)i is equal to (cid:6)(cid:12)
, it is a 
widening of (cid:6). As each (cid:9)i is consistent, (cid:6)(cid:12)
is consistent. This is a contradiction, 
which proves that (cid:6) is an inescapable conﬂict.

must be a consolidation of (cid:6) and, thus, (cid:6)(cid:12)

is a minimal inconsistent set. As 

is consistent, for (cid:6)(cid:12)

(cid:3)
| 

1, (cid:8)(cid:12)

2, . . . , (cid:8)(cid:12)

i=1 (cid:8)i = (cid:6).  Let  (cid:8)(cid:12)
i is a widening of |(cid:6)} is such that 

(→)  Let  (cid:6) = {α1, α2, . . . , αm} be  an  inescapable  conﬂict  and  deﬁne  (cid:8)i = (cid:6) \ {αi},  for  1 ≤ i ≤ m.  Note  that 
i denote  an  arbitrary  consolidation  of  (cid:8)i ,  for  1 ≤ i ≤ m.  If  every  set 
i is a consolidation of (cid:6), (cid:6) would not be an inescapable 
is widening of (cid:6) but is not consis-
}, for 
is 

i=1 (cid:8)(cid:12)
m
(cid:12) ≤ i ≤ m) such that 
} for some αi ⊆ α(cid:12)
m are the maximal proper subsets of (cid:6), and every proper subset of (cid:6)(cid:12)

(cid:8)1, . . . , (cid:8)m (cid:2) (cid:6) such  that 
{(cid:8)(cid:12)
m
conﬂict. So, there are consolidations (cid:8)(cid:12)
is a widening of (cid:6), (cid:6)(cid:12) = {α(cid:12)
tent. As (cid:6)(cid:12)
1 ≤ i ≤ m. Hence, (cid:8)(cid:12)
a minimal inconsistent set. (cid:2)

i for  1 ≤ i ≤ m. As (cid:8)i = (cid:6) \ {αi}, (cid:8)(cid:12)

i for each (cid:8)i (
1, α(cid:12)
2, . . . , α(cid:12)

is consistent. Thus, (cid:6)(cid:12)

= (cid:6)(cid:12) \ {α(cid:12)
i

1, . . . , (cid:8)(cid:12)

i=1 (cid:8)(cid:12)

i=1 (cid:8)(cid:12)

= (cid:6)(cid:12)

(cid:3)
k

(cid:3)
k

(cid:3)

(cid:3)

(cid:3)

m

m

m

m

i

i

i

Corollary 4.14. Consider two knowledge bases (cid:6), (cid:6)(cid:12) ∈ K such that (cid:6)(cid:12)
widening {β ∈ (cid:6)(cid:12) | α ∈ (cid:8) and α ⊆ β} is consistent, then (cid:6)(cid:12)

is a consolidation of (cid:6).

is a widening of (cid:6). If for every inescapable conﬂict (cid:8) ⊆ (cid:6) its 

Proof. We will prove via the contrapositive: given (cid:6) and its widening (cid:6)(cid:12)
inescapable conﬂict (cid:8) ⊆ (cid:6) such that the set {β ∈ (cid:6)(cid:12) | α ∈ (cid:8) and α ⊆ β} is inconsistent.

, if (cid:6)(cid:12)

is not a consolidation of (cid:6), then there is an 

If  (cid:6)(cid:12)
denote by (cid:8)(cid:12)
an inescapable conﬂict. (cid:2)

is  not  a  consolidation  of  (cid:6),  (cid:6)(cid:12)

. Let (cid:8) be the set {α ∈ (cid:6)|β ∈ (cid:8)(cid:12)

is  inconsistent  and  must  contain  at  least  one  minimal  inconsistent  set,  that  we 
is a widening of (cid:8) ⊆ (cid:6). By Lemma 4.13, (cid:8) is 

and α ⊆ β} — that is, (cid:8)(cid:12) ⊆ (cid:6)(cid:12)

Corollary 4.15. If (cid:8) is a minimal inconsistent set, then (cid:8) is an inescapable conﬂict.

Proof. Just note that (cid:8) is a widening of itself. By Lemma 4.13, (cid:8) is an inescapable conﬂict. (cid:2)

160

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

Corollary 4.18. If I satisﬁes MIS-separability, then I satisﬁes IC-separability.

Proof. If follows directly from the deﬁnitions and Corollary 4.15. (cid:2)

Theorem 4.19. The following statements are equivalent:

of (cid:6), α ∈ (cid:6)(cid:12)
1. For all d-consolidation (cid:6)(cid:12)
is a consolidation of (cid:6), then α is consistent with (cid:6)(cid:12)
2. If (cid:6)(cid:12)
3. There is no inescapable conﬂict (cid:8) in (cid:6) such that α ∈ (cid:8).
4. α is an innocuous probabilistic conditional in (cid:6).

.

.

Proof. By  the  deﬁnition  of  innocuous  conditionals  and  Lemma 4.5,  the  ﬁrst,  the  second  and  the  fourth  statements  are 
equivalent. It remains to prove that α is innocuous iff there is no inescapable conﬂict (cid:8) in (cid:6) such that α ∈ (cid:8).

(→)  Let  α be  innocuous  in  (cid:6).  Suppose  there  is  an  inescapable  conﬂict  (cid:8) ⊆ (cid:6) such  that  α ∈ (cid:8).  Consider  the  base 
be  a  consolidation  of  (cid:9).  Thus,  (cid:6)(cid:12) = (cid:9)(cid:12) ∪ {(ϕ|ψ)[0, 1]|(ϕ|ψ)[q, ¯q] ∈ (cid:6) \ (cid:9)} is  consistent  and  it  is  a 
(by  Lemma 4.5)  and,  therefore,  with  (cid:9)(cid:12)
. 
of (cid:9). Furthermore, if {β} is a consolidation of {α}
and {β}

(cid:9) = (cid:8) \ {α}.  Let  (cid:9)(cid:12)
consolidation  of  (cid:6).  Due  to  the  fact  that α is  innocuous, α is  consistent  with  (cid:6)(cid:12)
Consequently, (cid:9)(cid:12) ∪ {α} is a consolidation of (cid:8) for any consolidation (cid:9)(cid:12)
(i.e., α ⊆ β), (cid:9)(cid:12) ∪ {β} is a consolidation of (cid:8). As (cid:9), {α} (cid:2) (cid:8) are such that (cid:9) ∪ {α} = (cid:8), and any consolidations (cid:9)(cid:12)
of theirs are such that (cid:9)(cid:12) ∪ {β} is a consolidation of (cid:8), (cid:8) is not an inescapable conﬂict, which is a contradiction.

of (cid:6) can be written as (cid:6)(cid:12) = (cid:9)(cid:12) ∪ {β}, where (cid:9)(cid:12)

(←) Suppose there is no inescapable conﬂict (cid:8) in (cid:6) such that α ∈ (cid:8). Consider the base (cid:9) = (cid:6) \ {α}. Every consolida-
tion (cid:6)(cid:12)
is a consolidation of (cid:9) and α ⊆ β. As all inescapable conﬂicts of (cid:6)
are  in (cid:9),  by  Corollary 4.14,  (cid:9)(cid:12) ∪ {α} is  consistent.  Hence,  α is  consistent  with  any  consolidation  (cid:6)(cid:12) = (cid:9)(cid:12) ∪ {β} and  α is 
innocuous by Lemma 4.5. (cid:2)

Corollary 4.20. If I satisﬁes IC-separability, then I satisﬁes i-independence.

Proof. Let (cid:6) be a knowledge base and α ∈ (cid:6) an innocuous conditional. As α is innocuous, all inescapable conﬂicts of (cid:6) are 
in (cid:6) \ {α} by Lemma 4.19. By IC-separability, we have I((cid:6)) = I((cid:6) \ {α}) + I(α). (cid:2)

Theorem 4.27. For any p ∈ N>0 ∪ {∞}, Ip is well-deﬁned and satisﬁes the postulates of consistency, continuity, i-independence and 
monotonicity.

, ¯qi]|1 ≤
Proof. To show that Ip is well-deﬁned, we use results from the proof of Theorem 1 in [44]. For any (cid:6) = {(ϕi|ψi)[q
i ≤ m},  Thimm  shows  that  the  set  Q (cid:6) = {(cid:16)q1, . . . , qm(cid:17) ∈ Rm|(cid:10)(cid:6)((cid:16)q1, q1, . . . , qm, qm(cid:17)) is consistent} is  compact  and  closed, 
where (cid:10)(cid:6) : [0, 1]2m → K is the characteristic function of (cid:6). Let h : R2 → R be a function such that h(a, b) = max(0, a − b)
for any a, b ∈ R. The measure Ip is the minimum of (cid:21) fq,¯q(q)(cid:21)p with q ∈ Q (cid:6) , where  fq,¯q : Rm → R2m is a function such that 
− qm), h(qm − ¯qm)(cid:17). Intuitively,  fq,¯q(q) measures, for each point qi , how 
− q1), h(q1 − ¯q1), . . . , h(q
fq,¯q((cid:16)q1, . . . , qm(cid:17)) = (cid:16)h(q
, ¯qi]. Finally, Ip is well deﬁned, for  Q (cid:6) is closed 
much the lower and the upper bounds have to change for we have qi ∈ [q
and compact [44].

m

1

i

i

Consistency: By deﬁnition, a p-norm is never negative, thus Ip((cid:6)) ≥ 0. Suppose (cid:6) = (cid:10)(cid:6)(q) is consistent. A vector q

is such that (cid:21)q
(cid:12) (cid:20)= q, then (cid:21)q
q

(cid:12) − q(cid:21)p = 0 for any p ∈ N>0 ∪ {∞}, thus Ip((cid:6)) = 0. Now suppose (cid:6) = (cid:10)(cid:6)(q) is inconsistent. For every q
(cid:12) − q(cid:21)p > 0 and Ip((cid:6)) > 0 for any  p ∈ N>0 ∪ {∞}.

(cid:12) = q
(cid:12) ∈ Q (cid:6) , 

Continuity: Given a base (cid:6) = {(ϕi|ψi)[q

deﬁne the function  gq : R2m → R such that  gq((cid:16)q
, ¯q1, . . . , q
computed  as  the  minimum  of  {gq((cid:16)q
functions is continuous, hence Ip ◦ (cid:10)(cid:6) is continuous.

m

1

i

, ¯qi]|1 ≤ i ≤ m}, its characteristic function (cid:10)(cid:6) : [0, 1]2m → K and a ﬁxed q ∈ Q (cid:6) , 
, ¯qm(cid:17)) is 
, ¯qm(cid:17)) = (cid:21) fq,¯q(q)(cid:21)p . Note that Ip ◦ (cid:10)(cid:6)((cid:16)q
, ¯q1, . . . , q
1
, ¯qm(cid:17))|q ∈ Q (cid:6)}.  Each  gq is  continuous,  and  the  minimum  of  continuous 

, ¯q1, . . . , q

m

m

1

Monotonicity: Let  (cid:10)(cid:6)(q

and  Ip((cid:6)) = (cid:21)q
is  a  consolidation  (cid:9)(cid:12) = (cid:10)(cid:9) (r
α ⊆ β.  As  (cid:9)(cid:12)
in  sense)  of  r and  r
Ip((cid:6)) ≤ (cid:21)q

,  q
(cid:12)(cid:12) − q(cid:21)p < (cid:21)q

(cid:12)

(cid:12)) be  a  consolidation  of  (cid:6) = (cid:10)(cid:6)(q) such  that  (cid:21)q

(cid:12) − q(cid:21)p is  minimized,  for  a  p ∈ N>0 ∪ {∞}, 
(cid:12) − q(cid:21)p .  To  prove  by  contradiction,  suppose  I((cid:6) ∪ {α}) < Ip((cid:6)),  for  some  (cid:9) = (cid:6) ∪ {α} ∈ K.  Hence,  there 
(cid:12) − q(cid:21)p .  Consider  the  base  (cid:6)(cid:12) = (cid:9)(cid:12) \ {β},  such  that 
(cid:12)(cid:12)
are  projections  (subsets, 
(cid:12) − q(cid:21)p .  Finally,  it  would  follow  that 

(cid:12)(cid:12)) also  is,  and  it  is  a  consolidation  of  (cid:6).  Since  q and  q
(cid:12)(cid:12) − q(cid:21)p ≤ (cid:21)r

(cid:12)) of  (cid:9) = (cid:10)(cid:9) (r) such  that  (cid:21)r

is  consistent,  (cid:6)(cid:12) = (cid:10)(cid:6)(q

(cid:12) − r(cid:21)p < (cid:21)q

(cid:12) − r(cid:21)p < (cid:21)q

(cid:12) − r and  (cid:21)q

(cid:12)(cid:12) − q is  a  projection  of  r
(cid:12) − q(cid:21)p = Ip((cid:6)), which is a contradiction.

i-Independence: Consider the bases (cid:6) = (cid:10)(cid:6)(r) and (cid:9) = (cid:6) \ {α} in K, where α = (ϕ|ψ)[q, ¯q] is innocuous in (cid:6). We are 
(cid:12)) be a consolidation of 
(cid:12) − q(cid:21)p . Note that (cid:6)(cid:12) = (cid:9)(cid:12) ∪ {(ϕ|ψ)[0, 1]}
(cid:12)) is  a 
.  Hence,  (cid:9)(cid:12) ∪ {α} = (cid:10)(cid:6)(r
and  (cid:9)(cid:12)
(cid:12) − q(cid:21)p =

going to prove that Ip((cid:6)) ≤ Ip((cid:9)), and the desired result follows from monotonicity. Let (cid:9)(cid:12) = (cid:10)(cid:9) (q
(cid:12) − q(cid:21)p is minimized, for a p ∈ N>0 ∪ {∞}, and Ip((cid:9)) = (cid:21)q
(cid:9) = (cid:10)(cid:9) (q) such that (cid:21)q
is  a  consolidation  of  (cid:6).  As  α = (ϕ|ψ)[q, ¯q] is  innocuous,  α is  consistent  with  (cid:6)(cid:12)
consolidation  of  (cid:6).  Note  that  r
Ip((cid:9)). (cid:2)

(cid:12) − q with  two  extra  0’s  (from  alpha).  Finally,  Ip((cid:6)) ≤ (cid:21)r

(cid:12) − r(cid:21)p = (cid:21)q

(cid:12) − r is  q

Lemma 4.28. Ip satisﬁes super-additivity and IC-separability iff p = 1.

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

161

Proof. (→) To note that super-additivity and IC-separability do not hold if  p > 1, consider the bases (cid:9) = {((cid:6))[0.9]}, (cid:8) =
(cid:12)) of (cid:6) = (cid:10)(cid:6)(q)
{(⊥)[0.1]}, (cid:6) = (cid:9) ∪ (cid:8). By the deﬁnition of d-consolidation, if Ip((cid:6)) = d, then there is d-consolidation (cid:10)(cid:6)(q
(cid:12) − q(cid:21)p = d.  The  only  d-consolidations  of  (cid:9), (cid:8), (cid:6) are  (cid:9)(cid:12) = {((cid:6))[0.9, 1]}, (cid:8)(cid:12) = {(⊥)[0, 0.1]}, (cid:6)(cid:12) = (cid:9)(cid:12) ∪ (cid:8)(cid:12)
such  that  (cid:21)q
, 
for  changing  the  lower  bound  in  (cid:9) and  the  upper  bound  in  (cid:8) is  useless  to  reach  consistency.  For  any  ﬁnite  p, 
√
√
Ip((cid:9)) = Ip((cid:8)) = p
2.  For  p = ∞,  Ip((cid:9)) = Ip((cid:8)) = max(cid:16)0.1(cid:17) = 0.1 and 
0.1p + 0.1p = 0.1 p
Ip((cid:6)) = max(cid:16)0.1, 0.1(cid:17) = 0.1. Therefore, for any  p > 1 ∈ N ∪ {∞}, Ip((cid:6)) < 0.2 = Ip((cid:9)) + Ip((cid:8)), and both super-additivity 
and IC-separability fail.

√
0.1p = 0.1,  and  Ip((cid:6)) = p

(cid:12) − r(cid:21)1, (cid:21)s

(cid:12)), (cid:6)(cid:12) = (cid:10)(cid:6)(s

(cid:12)), (cid:8)(cid:12) = (cid:10)(cid:8)(r

(cid:9) ), (cid:8)(cid:12)(cid:12) = (cid:10)(cid:8)(s

− r(cid:21)1. Hence, for I1((cid:9)) ≤ (cid:21)s

(cid:12) − s(cid:21)1,  corresponding  to  I1((cid:9)), I1((cid:8)), I1((cid:6)).  Clearly,  (cid:6)(cid:12)

(cid:9) ∩ (cid:8) = ∅. Let (cid:9)(cid:12) = (cid:10)(cid:9) (q
(cid:12) − q(cid:21)1, (cid:21)r
imize  (cid:21)q
in such a way that (cid:9)(cid:12)(cid:12) = (cid:10)(cid:9) (s
(cid:12)
(cid:21)s
(cid:8)

(←)  Now  ﬁx  p = 1.  To  prove  that  super-additivity  holds,  suppose  there  are  bases  (cid:9), (cid:8), (cid:6) = (cid:9) ∪ (cid:8) in  K such  that 
(cid:12)) be d-consolidations of (cid:9) = (cid:10)(cid:9) (q), (cid:8) = (cid:10)(cid:8)(r), (cid:6) = (cid:10)(cid:6)(s) that min-
can  be  partitioned  into  (cid:9)(cid:12)(cid:12) ∪ (cid:8)(cid:12)(cid:12)
(cid:12)
(cid:12)
(cid:12) − s(cid:21)1 =
(cid:8), (cid:21)s
(cid:9) and  s
(cid:12)
(cid:12) − s(cid:21)1 ≥ I1((cid:9)) + I1((cid:8)).
− q(cid:21)1 + (cid:21)s
(cid:9)
To  prove  that  IC-separability  holds,  suppose  there  are  bases  (cid:9), (cid:8), (cid:6) = (cid:9) ∪ (cid:8) in  K such  that  (cid:9) ∩ (cid:8) = ∅,  IC((cid:6)) =
(cid:12) − r(cid:21)1, 
(cid:12)) is  a  widening  of  (cid:6) = (cid:10)(cid:6)(s) such  that,  for  each  (cid:14) ∈ IC((cid:6)) =
is a consolidation 
(cid:12) − r(cid:21)1 = I1((cid:9)) + I1((cid:8)),  it  follows  that  I1((cid:6)) ≤ I1((cid:9)) + I1((cid:8)).  By 

IC((cid:9)) ∪ IC((cid:8)). Let (cid:9)(cid:12) = (cid:10)(cid:9) (q
corresponding  to  I1((cid:9)), I1((cid:8)).  As  (cid:6)(cid:12) = (cid:9)(cid:12) ∪ (cid:8)(cid:12) = (cid:10)(cid:6)(s
IC((cid:9)) ∪ IC((cid:8)), the base {β ∈ (cid:6)(cid:12) | α ∈ (cid:14) and α ⊆ β} is consistent (all inescapable conﬂicts are solved), (cid:6)(cid:12)
(cid:12) − s(cid:21)1 = (cid:21)q
of  (cid:6) by  Corollary 4.14.  As  (cid:21)s
super-additivity, I1((cid:6)) ≥ I1((cid:9)) + I1((cid:8)), thus I1((cid:6)) = I1((cid:9)) + I1((cid:8)). (cid:2)

(cid:12)
(cid:8)) are consolidations of (cid:9), (cid:8). By the construction of  s
− q(cid:21)1 and I1((cid:8)) ≤ (cid:21)s

(cid:12)), be consolidations of (cid:9) = (cid:10)(cid:9) (q), (cid:8) = (cid:10)(cid:8)(r) that minimize (cid:21)q

− r(cid:21)1, it follows that I((cid:6)) = (cid:21)s

(cid:12)), (cid:8)(cid:12) = (cid:10)(cid:8)(r

(cid:12) − q(cid:21)1 + (cid:21)r

(cid:12) − q(cid:21)1, (cid:21)r

(cid:12)
(cid:8)

(cid:12)
(cid:9)

(cid:12)

Lemma 4.29. Ip satisﬁes normalization iff p = ∞.

Proof. (→)  To  note  that  normalization  does  not  hold  if  p is  ﬁnite,  consider  the  base  (cid:6) = {((cid:6))[0], (⊥)[1]}.  The  only  d-
consolidation of (cid:6) is (cid:6)(cid:12) = {((cid:6))[0, 1], (⊥)[0, 1]}, for changing the lower bound in ((cid:6))[0] and the upper bound in (⊥)[1] is 
√
√
useless to reach consistency. For any ﬁnite  p, Ip((cid:6)) = p
1p + 1p = p
(←) By deﬁnition, I∞((cid:6)) is the minimum of (cid:21)q
(cid:12)

(cid:12)) being consistent. As the vectors 
(cid:12) − q(cid:21)∞ subject to (cid:6) = (cid:10)(cid:6)(q) and (cid:10)(cid:6)(q
(cid:12)
(cid:12)
. (cid:2)
i of q, q

− qi| ∈ [0, 1] for all elements qi, q

2 > 1, and normalization fails.

(cid:12) − q(cid:21)∞ ∈ [0, 1], since |q

are in [0, 1]2|(cid:6)|

, (cid:21)q

q, q

(cid:12)
i

Proposition 5.1. For any p ∈ N>0 ∪ {∞}, Iε
p
and monotonicity. Iε

1 also satisﬁes super-additivity.

: K → [0, ∞) is well-deﬁned and satisﬁes consistency, continuity, weak independence 

Proof. See Section 4 in [34]. (cid:2)

Theorem 5.2. For any p ∈ N>0 ∪ {∞}, Iε
p
monotonicity. Iε

1 also satisﬁes super-additivity and IC-separability; and Iε

∞ satisﬁes normalization.

: K → [0, ∞) is well-deﬁned and satisﬁes consistency, continuity, i-independence and 

Proof. For  well-deﬁnedness,  consistency,  i-independence,  monotonicity,  super-additivity  and  IC-separability,  see  the  proof 
of Theorem 5.11. For continuity, see Lemma 5.12.

For  normalization,  we  note  that  Iε

SSK ,  we 
limit  the  sum  of  the  absolute  values  of  the  stakes  to  one.  As  the  agent  cannot  lose  more  than  the  absolute  value  of  the 
stake in each gamble in a Dutch book, and they sum up to one, I sum
SSK

SSK ((cid:6)) for  any  (cid:6) ∈ K,  by  Theorem 5.7.  When  we  are  computing  I sum

∞ satisfy normalization. (cid:2)

∞((cid:6)) = I sum

= Iε

Theorem 5.11. Ia,sum
and Ib,max

, Ia,max
SSK
also satisfy super-additivity and IC-separability.

and Ib,max

, Ib,sum
SSK

SSK

SSK

SSK

are well-deﬁned and satisfy consistency, i-independence and monotonicity. Ia,max

SSK

, ¯γi be non-negative real parameters, 
Proof. Let (cid:6) = {(ϕi|ψi)[q
for 1 ≤ i ≤ m. Consider the following program, with  p ∈ N>0 ∪ {∞}, where εγ ( ¯εγ ) is a (m × 1)-vector whose elements are 
εiγ

, ¯qi]|1 ≤ i ≤ m} be an arbitrary knowledge base in K and γ

i

i

i

( ¯εi ¯γi ), for 1 ≤ i ≤ m:
min (cid:21)(cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17)(cid:21)p subject to:
Aπ ≥ −εγ
Bπ ≤ ¯εγ
(cid:6)

π = 1

π , ε1, ¯ε1, . . . , εm, ¯εm ≥ 0 .

(A.1)

(A.2)

(A.3)

(A.4)

(A.5)

Deﬁne  the  inconsistency  measure  Iγ

p

: K → [0, ∞) ∪ ∞ in  such  a  way  that  Iγ

function of the program above; or ∞ if it is infeasible. If γ

i

= ¯γi = 1 for all 1 ≤ i ≤ m, Iγ

p ((cid:6)) is  the  minimum  of  the  objective 
p((cid:6)), for any p ∈ N>0 ∪{∞}.

p ((cid:6)) = Iε

162

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

SSK

SSK

i

i

i

i

i

i

= q

When  p = 1,  if  γ

λi
and  ¯γi = ¯qi for 1 ≤ i ≤ m, Iγ

and  ¯γi = 1 − ¯qi for  1 ≤ i ≤ m,  the  program  above  is  the  dual  of  that  formed  by  adding  the 
((cid:6)). Analogously, if 

≤ 1 and (1 − ¯qi)¯λi ≤ 1, for 1 ≤ i ≤ m, into the program (19)–(21). That is, Iγ
((cid:6)).

constraints q
= 1 − q
γ
When  p = ∞,  in  all  restrictions  we  can  replace  εi, ¯εi by  a  single  scalar  ε,  as  it  was  done  in  (14)–(18),  creating  an 
and  ¯γi = 1 − ¯qi for 1 ≤ i ≤ m, the program 
+ (1 − ¯qi)¯λi ≤ 1 into  (19)–(21).  That  is,  P computes 

equivalent new linear program P that minimizes ε, computing Iγ
P is  the  dual  of  that  formed  by  adding  the  constraints 
m
i=1 q
Iγ
∞((cid:6)) = Ia,sum

λi
and  ¯δi = ¯qi for 1 ≤ i ≤ m,  P computes Iγ

((cid:6)). Analogously, if γ

1 ((cid:6)) = I g,max

1 ((cid:6)) = Ia,max

∞((cid:6)) = I g,sum

∞((cid:6)). If γ

= 1 − q

= q

((cid:6)).

(cid:2)

SSK

SSK

i

i

i

i

Note that the linear restrictions in the program (A.1)–(A.5), when it is feasible, deﬁne a convex, closed region of feasible 
points (a simplex). The  p-norm is a continuous function, so the minimum of the objective function in (A.1) is well-deﬁned 
for any  p ∈ N>0 ∪ {∞}. If the program (A.1)–(A.5) is infeasible for some (cid:6) ∈ K, Iγ

p ((cid:6)) is (well-)deﬁned as ∞.

Consistency: Note that a  p-norm is never negative. The base (cid:6) is consistent iff the program (5)–(8) is feasible; and such 
program is feasible iff the program (A.1)–(A.5) has a feasible solution with (cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17) = (cid:16)0, 0, . . . , 0(cid:17); which is the 
case iff (cid:21)(cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17)(cid:21)p = 0 is the minimum of the objective function in (A.1).

Monotonicity: Consider  the  program  P from  lines  (A.1)–(A.5),  corresponding  to  the  computation  of  Iγ

p ((cid:6)),  for  some 
, ¯γm ≥ 0, the program 
(cid:6) ∈ K. Let (cid:9) = (cid:6) ∪ {α} be a knowledge base. For any  p ∈ N>0 ∪ {∞} and parameters γ
(A.1)–(A.5) whose solution gives Iγ
p ((cid:9)) has two extra constraints in comparison with P . Thus, the program that computes 
Iγ
p ((cid:9)) cannot reach a smaller value for (cid:21)(cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17)(cid:21)p , the objective function being minimized by P . Furthermore, 
, ¯γm ≥ 0. Hence, 
(cid:21)(cid:16)ε1, ¯ε1, . . . , εm+1, ¯εm+1(cid:17)(cid:21)p ≥ (cid:21)(cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17)(cid:21)p , for any p ∈ N>0 ∪{∞} and parameters γ
p ((cid:6) ∪ {α}) ≥ Iγ
Iγ
i-independence: Let (cid:6) = {(ϕi|ψi)[q

, ¯qm] be an innocuous 
conditional in (cid:6), and deﬁne (cid:9) = (cid:6) \ {α}. Suppose Iγ
p ((cid:9)) is ﬁnite. The solution on (cid:16)ε1, ¯ε1, . . . , εm−1, ¯εm−1(cid:17) to the program 
(A.1)–(A.5) that  computes  Iγ
p ((cid:9)) corresponds  to  a  consolidation  of  (cid:9) given  by  (cid:9)(cid:12) = {(ϕi|ψi)[q
εi, ¯qi + ¯γi ¯εi]|1 ≤ i ≤
m − 1}. For α is innocuous in (cid:6), it is consistent with (cid:9)(cid:12) ∪ (ϕm|ψm)[0, 1] (a consolidation of (cid:6)) and (cid:9)(cid:12) ∪ {α} is a consolidation 
of (cid:6). Hence, (cid:16)ε1, ¯ε1, . . . , εm−1, ¯εm−1, 0, 0(cid:17) corresponds to a feasible solution to the program (A.1)–(A.5) computing Iγ
p ((cid:6)). 
As  (cid:21)(cid:16)ε1, ¯ε1, . . . , εm−1, ¯εm−1(cid:17)(cid:21)p is  equal  to  (cid:21)(cid:16)ε1, ¯ε1, . . . , εm−1, ¯εm−1, 0, 0(cid:17)(cid:21)p for  any  p ∈ N>0 ∪ {∞},  Iγ
p ((cid:9)).  By 
monotonicity, Iγ

, ¯qi]|1 ≤ i ≤ m} be a knowledge base in K and α = (ϕm|ψm)[q

p ((cid:6)), for any  p ∈ N>0 ∪ {∞}.

p ((cid:6)) ≤ Iγ

, ¯γ1, . . . , γ

, ¯γ1, . . . , γ

− γ

p ((cid:6)) = Iγ

p ((cid:9)).

m

m

m

1

1

i

i

i

Now  suppose  Iγ

p ((cid:9)) is  inﬁnite.  Thus,  the  program  (A.1)–(A.5) that  computes  Iγ

p ((cid:6)) = Iγ

p ((cid:9) ∪ {α}) together with the infeasibility, hence Iγ

p ((cid:9)) is  infeasible.  Constraints  in  such 
p ((cid:6)) = ∞

program are inherited by the program that computes Iγ
by deﬁnition.

i

i

i

i

i

− εiγ

, ¯qi]|1 ≤ i ≤ m}.  If  Iγ

, ¯qi]|1 ≤ i ≤ k},  (cid:8) = {(ϕi|ψi)[q

, ¯qi]|k + 1 ≤ i ≤ m} and  (cid:6) = {(ϕi|ψi)[q

1 ((cid:6)), minimizing the objective function. As (cid:6)(cid:12) = {(ϕi|ψi)[q
, ¯qi + ¯εi ¯γi]|1 ≤ i ≤ k} and  (cid:8)(cid:12) = {(ϕi|ψi)[q
− εiγ

Super-additivity: Suppose there are bases (cid:9), (cid:8), (cid:6) = (cid:9) ∪ (cid:8) in K such that (cid:9) ∩ (cid:8) = ∅. Without loss of generality, let 
1 ((cid:6)) = ∞,  super-
(cid:9) = {(ϕi|ψi)[q
additivity trivially holds, then consider Iγ
1 ((cid:6)) is ﬁnite. Let (cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17) be part of a solution (that includes π ) to the 
program (A.1)–(A.5) that computes Iγ
, ¯qi + ¯εi ¯γi]|1 ≤ i ≤ m}
− εiγ
is  consistent,  so  are  (cid:9)(cid:12) = {(ϕi|ψi)[q
, ¯qi + ¯εi ¯γi]|k + 1 ≤ i ≤ m}, 
which  are  consolidations  of  (cid:9) and  (cid:8).  Thus,  (cid:16)ε1, ¯ε1, . . . , εk, ¯εk(cid:17) and  (cid:16)εk+1, ¯εk+1, . . . , εm, ¯εm(cid:17) correspond  to  feasible  solu-
tions  to  the  programs  that  compute  Iγ
1 ((cid:9)) ≤ (cid:21)(cid:16)ε1, ¯ε1, . . . , εk, ¯εk(cid:17)(cid:21)1 and 
1 ((cid:8)) ≤ (cid:21)(cid:16)εk+1, ¯εk+1, . . . , εm, ¯εm(cid:17)(cid:21)1. Finally, Iγ
Iγ
m
i=1 εi
IC-separability: To prove that IC-separability holds, suppose there are bases (cid:9), (cid:8), (cid:6) = (cid:9) ∪ (cid:8) in K such that (cid:9) ∩ (cid:8) = ∅, 
, ¯qi]|1 ≤ i ≤ k}, (cid:8) = {(ϕi|ψi)[q
, ¯qi]|k + 1 ≤ i ≤ m} and 
IC((cid:6)) = IC((cid:9)) ∪ IC((cid:8)). Without loss of generality, let (cid:9) = {(ϕi|ψi)[q
1 ((cid:8)) = ∞, then Iγ
1 ((cid:9)) = ∞ or Iγ
1 ((cid:6)) = ∞ by monotonicity, and IC-separability holds, 
(cid:6) = {(ϕi|ψi)[q
considering that ∞ plus any non-negative number yields ∞; thus, we assume Iγ
1 ((cid:8)) < ∞. Let (cid:16)ε1, ¯ε1, . . . , εk, ¯εk(cid:17)
and (cid:16)εk+1, ¯εk+1, . . . , εm, ¯εm(cid:17) be solutions (on ε, ¯ε) to the programs in the form (A.1)–(A.5) that compute Iγ
1 ((cid:8)), 
respectively,  minimizing  their  objective  functions.  As  all  inescapable  conﬂicts  of  (cid:6) are  either  in  (cid:9) or  in  (cid:8),  the  union  of 
consolidations of (cid:9) and (cid:8) is a consolidation of (cid:6), by Corollary 4.14. Hence, (cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17) correspond to a feasible 
solution  to  the  program  in  the  form  (A.1)–(A.5) that  computes  Iγ
+
¯εi) + (

1 ((cid:8)),  respectively.  It  follows  that  Iγ
(cid:2)
k
i=1 εi

1 ((cid:6)) ≤ (cid:21)(cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17)(cid:21)1 = (

, ¯qi]|1 ≤ i ≤ m}. If Iγ

1 ((cid:8)). By super-additivity, Iγ

1 ((cid:9)) and  Iγ

1 ((cid:6)) and  Iγ

1 ((cid:9)) and Iγ

1 ((cid:9)) + Iγ

1 ((cid:9)) + Iγ

1 ((cid:8)) + Iγ

1 ((cid:6)) = Iγ

(cid:2)
k
i=1 εi

+ ¯εi) = Iγ

1 ((cid:9)), Iγ

1 ((cid:8)). (cid:2)

+ ¯εi = Iγ

1 ((cid:9)) ≤ (

+ ¯εi) + (

m
i=k+1 εi

m
i=k+1 εi

+ ¯εi) =

1 ((cid:6)).

(cid:2)

(cid:2)

(cid:2)

i

i

i

i

i

i

i

Lemma 5.12. Ia,sum

SSK

, Ia,max
SSK

, Ib,sum
SSK

and Ib,max

SSK

are continuous for probabilities within (0, 1).

(cid:12)
i

(cid:12)
, ¯q
i
, ¯γ1, . . . , γ

Proof. Consider  the  inconsistency  measure  Iγ
{(ϕi|ψi)[q
]|1 ≤ i ≤ m} and  the  vector  q = (cid:16)q
, ¯γm are  positive  (for  Ia,sum
γ
rameters  are  positive,  every  probability  mass  π : W Xn
in  the  following  way:  εi

p deﬁned  in  the  proof  of  Theorem 5.11,  the  knowledge  base  (cid:6) =
, ¯qm(cid:17).  Note  that,  for  any  measure  Iε
p ,  the  parameters 
,  they  are  positive  if  q ∈ (0, 1)2m).  When  these  pa-
→ [0, 1] deﬁnes  a  vector  επ (q) = (cid:16)ε1, ¯ε1, . . . , εm, ¯εm(cid:17) for  each  q
P π (ψi))} and  ¯εi = max{0, (1/ ¯γi)(P π (ϕi ∧ ψi) − q
P π (ψi))}

)(P π (ϕi ∧ ψi) − q

m
and  Ib,max

= − min{0, (1/γ

, ¯q1, . . . , q

,  Ia,max
SSK

,  Ib,sum
SSK

SSK

SSK

m

1

1

i

i

i

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

163

p ((cid:10)(cid:6)(q)) for  any  q ∈ [0, 1]2m (or  q ∈ (0, 1)2m),  since  P π (ϕi ∧ ψi) − q

for  every  1 ≤ i ≤ m and  q ∈ [0, 1]2m (or  q ∈ (0, 1)2m).  Note  that  the  pair  π , επ (q) is  a  feasible  solution  to  the  program 
(A.1)–(A.5) that  computes  Iγ
and 
P π (ϕi ∧ ψi) − ¯qi P π (ψi) ≤ ¯εi ¯γi for all 1 ≤ i ≤ m. Thus, every π yields a value for the objective function hπ (q) = (cid:21)επ (q)(cid:21) of 
p and any q ∈ [0, 1]2m, 
the program (A.1)–(A.5), for any q ∈ [0, 1]2m (or q ∈ (0, 1)2m). As γ
επ (q) is continuous on q ∈ [0, 1]2m, and as any p-norm is a continuous function, hπ : [0, 1]2m → [0, ∞) also is for any π . (For 
Ia,sum
, ¯γi ; furthermore, such parameters change con-
SSK
tinuously — linearly — with q ∈ (0, 1)2m. Thus, hπ : (0, 1)2m → [0, ∞) is continuous on q ∈ (0, 1)2m). To compute Iγ
p ((cid:10)(cid:6)(q))
→ [0, 1] is a probability mass}.  As  the  mini-
for  a  particular  q,  one  needs  to  take  the  minimum  in  π of  {hπ (q)|π : W Xn
mum  of  continuous  functions  is  continuous,  Iγ
◦ (cid:10)(cid:6) : (0, 1)2m → [0, ∞) ∪ {∞})  is 
continuous for any  p ∈ N>0 ∪ {∞}. (cid:2)

◦ (cid:10)(cid:6) : [0, 1]2m → [0, ∞) ∪ {∞} (or  Iγ

, q ∈ (0, 1)2m implies positive parameters γ

P π (ψi) ≥ −εiγ

= ¯γm = 1 for Iε

= ¯γ1 = · · · = γ

and Ib,max

, Ia,max
SSK

, Ib,sum
SSK

SSK

m

1

p

p

i

i

i

Lemma 5.13. Ia,sum

SSK

satisfy normalization.

Proof. When we are computing Ia,sum
cannot lose more her total escrow in a Dutch book, Ia,sum

SSK

SSK

is trivially normalized. (cid:2)

, the maximum sure loss when limiting the agent’s total escrows to one. As the agent 

References

[1] K.A. Andersen, J.N. Hooker, A linear programming framework for logics of uncertainty* 1, Decis. Support Syst. 16 (1) (1996) 39–53.
[2] G. Boole, An Investigation of the Laws of Thought: On Which Are Founded the Mathematical Theories of Logic and Probabilities, Walton and Maberly, 

[3] G. Bruno, A. Gilio, Applicazione del metodo del simplesso al teorema fondamentale per le probabilita nella concezione soggettivistica, Statistica 40 (3) 

1854.

(1980) 337–344.

[4] A. Capotorti, G. Regoli, F. Vattari, Correction of incoherent conditional probability assessments, Int. J. Approx. Reason. 51 (6) (2010) 718–727.
[5] G. Coletti, R. Scozzafava, Probabilistic Logic in a Coherent Setting, Kluwer Academic Pub, 2002.
[6] Thomas M. Cover, Joy A. Thomas, Elements of Information Theory, Wiley Series in Telecommunications and Signal Processing, Wiley-Interscience, 2006.
[7] F.G. Cozman, L.F. di Ianni, Probabilistic satisﬁability and coherence checking through integer programming, in: Symbolic and Quantitative Approaches 

to Reasoning with Uncertainty, Springer, 2013, pp. 145–156.

[8] B. De Finetti, Theory of Probability, 1974.
[9] M. Finger, G. De Bona, Probabilistic satisﬁability: logic-based algorithms and phase transition, in: Proceedings of IJCAI’11, 2011.
[10] M. Finger, R. Le Bras, C.P. Gomes, B. Selman, Solutions for hard and soft constraints using optimized probabilistic satisﬁability, in: Proceedings of SAT, 

2013.

[11] A.M. Frisch, P. Haddawy, Anytime deduction for probabilistic logic, Artif. Intell. 69 (1) (1994) 93–122.
[12] G. Georgakopoulos, D. Kavvadias, C.H. Papadimitriou, Probabilistic satisﬁability, J. Complex. 4 (1) (1988) 1–11.
[13] J. Grant, A. Hunter, Measuring the good and the bad in inconsistent information, in: IJCAI Proceedings-International Joint Conference on Artiﬁcial 

Intelligence, vol. 22, Citeseer, 2011, pp. 2632–2637.

[14] T. Hailperin, Best possible inequalities for the probability of a logical function of events, Am. Math. Mon. 72 (4) (1965) 343–359.
[15] J.Y. Halpern, An analysis of ﬁrst-order logics of probability, Artif. Intell. 46 (3) (1990) 311–350.
[16] P. Hansen, B. Jaumard, Probabilistic satisﬁability, in: Handbook of Defeasible Reasoning and Uncertainty Management Systems: Algorithms for Uncer-

tainty and Defeasible Reasoning, 2000, p. 321.

[17] P. Hansen, S. Perron, Merging the local and global approaches to probabilistic satisﬁability, Int. J. Approx. Reason. 47 (2) (2008) 125–140.
[18] S.O. Hansson, A Textbook of Belief Dynamics, Vol. 1, Springer, 1999.
[19] A.  Hunter,  S.  Konieczny,  Approaches  to  measuring  inconsistent  information,  in:  Inconsistency  Tolerance,  in:  Lecture  Notes  in  Computer  Science, 

[20] A. Hunter, S. Konieczny, Shapley inconsistency values, in: 10th International Conference on Principles of Knowledge Representation and Reasoning (KR), 

[21] A. Hunter, S. Konieczny, Measuring inconsistency through minimal inconsistent sets, in: 11th International Conference on Principles of Knowledge 

[22] A. Hunter, S. Konieczny, On the measure of conﬂicts: Shapley inconsistency values, Artif. Intell. 174 (14) (2010) 1007–1026.
[23] B. Jaumard, A. Fortin, I. Shahriar, R. Sultana, First order probabilistic logic, in: Fuzzy Information Processing Society, 2006. NAFIPS 2006. Annual Meeting 

of the North American, IEEE, 2006, pp. 341–346.

[24] B. Jaumard, P. Hansen, M. Poggi de Aragao, Column generation methods for probabilistic logic, INFORMS J. Comput. 3 (2) (1991) 135.
[25] C.W. Karvetski, K.C. Olson, D.R. Mandel, C.R. Twardy, Probabilistic coherence weighting for optimizing expert forecasts, Decis. Anal. 10 (4) (2013) 

305–326.

[26] D. Kavvadias, C.H. Papadimitriou, A linear programming approach to reasoning about probabilities, Ann. Math. Artif. Intell. 1 (1) (1990) 189–205.
[27] J.G. Kemeny, Fair bets and inductive probabilities, J. Symb. Log. 20 (3) (1955) 263–273.
[28] P. Klinov, B. Parsia, A hybrid method for probabilistic satisﬁability, in: Automated Deduction, CADE-23, Springer, 2011, pp. 354–368.
[29] K. Knight, Measuring inconsistency, J. Philos. Log. 31 (1) (2002) 77–98.
[30] T. Lukasiewicz, Probabilistic deduction with conditional constraints over basic events, J. Artif. Intell. Res. 10 (1999) 199–241.
[31] D.P. Muiño, Measuring and repairing inconsistency in probabilistic knowledge bases, Int. J. Approx. Reason. 52 (6) (2011) 828–840.
[32] R.F. Nau, Coherent assessment of subjective probability, Technical report, DTIC Document, 1981.
[33] N.J. Nilsson, Probabilistic logic* 1, Artif. Intell. 28 (1) (1986) 71–87.
[34] N. Potyka, Linear programs for measuring inconsistency in probabilistic logics, in: Fourteenth International Conference on Principles of Knowledge 

Representation and Reasoning, KR-14, AAAI, 2014.

[35] N. Potyka, M. Thimm, Consolidation of probabilistic knowledge bases by inconsistency minimization, in: ECAI 2014, 2014.
[36] R. Reiter, A theory of diagnosis from ﬁrst principles, Artif. Intell. 32 (1) (1987) 57–95.
[37] W. Rödder, Conditional logic and the principle of entropy, Artif. Intell. 117 (1) (2000) 83–106.
[38] M.J. Schervish, J.B. Kadane, T. Seidenfeld, Measures of incoherence: how not to gamble if you must, in: Bayesian Statistics 7: Proceedings of the 7th 

Valencia Conference on Bayesian Statistics, 2003, pp. 385–402.

vol. 3300, Springer, Berlin Heidelberg, 2005, pp. 191–236.

2006, pp. 249–259.

Representation and Reasoning (KR), 2008, pp. 358–366.

164

G. De Bona, M. Finger / Artiﬁcial Intelligence 227 (2015) 140–164

[39] M.J. Schervish, T. Seidenfeld, J.B. Kadane, Two measures of incoherence: how not to gamble if you must, Technical report, Department of Statistics, 

Carnegie Mellon University, 1998.

[40] M.J. Schervish, T. Seidenfeld, J.B. Kadane, Measuring incoherence, Sankhy ¯a, Indian J. Stat., Ser. A (2002) 561–587.
[41] M.J. Schervish, T. Seidenfeld, J.B. Kadane, A rate of incoherence applied to ﬁxed-level testing, Proc. Philos. Sci. Assoc. 2002 (3) (2002) 248–264.
[42] A. Shimony, Coherence and the axioms of conﬁrmation, J. Symb. Log. 20 (01) (1955) 1–28.
[43] J. Staffel, Measuring the overall incoherence of credence functions, Synthese (2015) 1–27.
[44] M. Thimm, Inconsistency measures for probabilistic logics, Artif. Intell. 197 (2013) 1–24.
[45] William Thomson, Popular Lectures and Addresses, vol. 1, Macmillan and Co., 1891, available at https://archive.org/details/popularlectures10kelvgoog.
[46] R.J. Vanderbei, Linear Programming: Foundations and Extensions, 1996.
[47] G. Wang, S.R. Kulkarni, H.V. Poor, D.N. Osherson, Aggregating large sets of probabilistic forecasts by weighted coherent adjustment, Decis. Anal. 8 (2) 

(2011) 128–144.

