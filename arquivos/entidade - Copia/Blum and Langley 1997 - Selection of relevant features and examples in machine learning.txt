ELSEVIER 

Amficial  Intelligence  97  ( 1997)  245-271 

Artificial 
Intelligence 

Selection  of  relevant  features  and 

examples  in  machine 

Avrim  L.  Bluma**, Pat  Langley  b~1 
a School of  Computer Science,  Carnegie Mellon  University Pittsburgh, PA  15213-3891, USA 
h Institute for  the  Study of Learning  and Expertise,  2164 Staunton Court,  Palo Alto,  CA  94306,  USA 

Received  October  1995; revised  May  1996 

Abstract 

In  this  survey,  we  review  work  in  machine 

large  amounts  of  irrelevant 
relevant  features,  and  the  problem  of  selecting 
have  been  made  on  these  topics 
we  present  a  general 
framework 
challenges for future work in this area. @  1997 Elsevier Science  B.V. 

learning  on  methods  for  handling  data  sets  containing 
the  problem  of  selecting 
that 
in  both  empirical  and  theoretical  work  in  machine 
learning,  and 
that  we  use  to  compare  different  methods. We close with some 

information.  We  focus  on  two  key  issues: 

relevant  examples.  We describe 

the  advances 

Keywords: Relevant  features;  Relevant  examples;  Machine  learning 

1.  Introduction 

As  machine  learning  aims  to  address  larger,  more  complex  tasks,  the  problem  of 
focusing  on  the  most  relevant  information  in a potentially  overwhelming  quantity  of  data 
has  become  increasingly  important.  For  instance,  data  mining  of  corporate  or  scientific 
records  often  involves  dealing  with  both  many  features  and  many  examples,  and  the 
internet  and  World  Wide  Web  have  put  a huge  volume  of  low-quality  information  at  the 
easy  access  of  a  learning  system.  Similar  issues  arise  in  the  personalization  of  filtering 
systems  for  information  retrieval,  electronic  mail,  netnews,  and  the  like. 

In this paper,  we address  two  specific  aspects  of  this “focusing”  task that have  received 
significant  attention  in  the  AI  literature:  the  problem  of  focusing  on  the  most  relevant 

* Corresponding author.  Email:  avrim@cs.cmu.cdu. 
’ Also  affiliated  with  the  Intelligent  Systems  Laboratory,  Dahnler-Benz  Research  and  Technology  Center, 
1510 Page  Mill  Road,  Palo  Alto,  CA  94304, USA.  Email:  laagleyOisle.org. 

0004-3702/97/%17.00  @  1997 Blsevier  Science  B.V. All  rights  reserved 
PII  SOOO4-3702(97)00063-5 

246 

A.L.  Blum,  F! Langley/Artificial  intelligence  97  (1997)  245-271 

features 
examples 
general 

for  use  in  representing 

the  most  relevant 
to  drive  the  learning  process.  We  review  recent  work  on  these  topics,  presenting 
that  we  use  to  compare  and  contrast  different  approaches. 

the  data,  and  the  problem  of  selecting 

frameworks 

important 

for  this  problem, 

features. 
for  this 

notions  of  “relevance” 

We  begin  with  the  problem  of  focusing  on  relevant 

In  Section  2  we  present 
some 
task  and  describe 
that  have  been 
“filter”,  or  “wrapper” 
to  those  based  on 
schemes.  We  then  turn  (in  Section  3)  to  the  problem  of  focusing  on  relevant 
for  filtering  both  labeled  and  unlabeled  data.  We  conclude 
for  future  work,  on  both  the  empirical 

and  relate  several 
general  goals  of  feature-selection 
developed 
approaches, 
weighting 
examples,  describing  methods 
(in  Section  4)  with  open  problems  and  challenges 
and  theoretical 

algorithms.  We  report  on  methods 
them  as  “embedded”, 

characterizing 
explicit 

and  we  compare 

feature-selection 

techniques 

fronts. 

Before  proceeding,  we  should  clarify 

learning 

from  computational 

ods  and  results 
There  has  been  substantial  work  on  feature 
recognition 
theory,  and  the  philosophy 
work 
approaches  we  will  discuss. 

in  these  areas,  readers 

and  statistics,  and  on  data  selection 

should  be  aware 

the  scope  of  our  survey,  which  focuses  on  meth- 
learning. 
fields  such  as  pattern 
information 

theory  and  experimental  machine 
in  other 

in  fields  such  as  statistics, 

selection 

of  science.  Although  we  do  not  have  the  space  to  cover  the 
to  the 

that  there  are  many  similarities 

2.  The  problem  of  irrelevant 

features 

At  a  conceptual 

level,  one  can  divide 

the  task  of  concept 

into  two  subtasks: 
the  concept  and  deciding  how  to  combine 

learning 

the  selection  of  relevant 

features,  and  the  elimination 

of 

to  use  in  describing 

deciding  which  features 
those  features. 
irrelevant  ones, 
algorithms 

incorporate 

In  this  view, 
is  one  of  the  central  problems 
some  approach 

to  addressing 

At  a  practical 

level,  we  would 

like  induction 
features.  More  specifically, 

irrelevant 

to  reach  a  desired 

to  achieve  good  performance. 

with  many 
of  training  examples  needed 
complexity,  to  grow  slowly  with  the  number  of  features  present, 
are  needed 
classification 
that  only  a  small  fraction  of  these  are  crucial 
of  work  in  machine 
on  developing 
Induction 

algorithms  with  such  desirable  properties. 

task  to  represent  examples  using 

differ  considerably 

and  theoretical 

For  instance, 

learning-both 

experimental 

algorithms 

[ 62,631. 

in  machine 

learning,  and  many 

induction 

it. 
algorithms 
that  scale  well  to  domains 
as  one  goal  we  would  like  the  number 
level  of  accuracy,  often  called  the  sample 
if  indeed  not  all  these 
in  a  text 

it  is  not  uncommon 
lo4  to  lo7  attributes,  with  the  expectation 
In  recent  years,  a  growing  amount 
focused 
in  nature-has 

in  their  emphasis  on  focusing  on  relevant 
test 
lies  the  simple  nearest-neighbor  method,  which  classifies 
features.  At  one  extreme 
instances  by  retrieving 
the  nearest  stored  training  example,  using  all  available  attributes 
in  its  distance  computations.  Although  Cover  and  Hart  [25]  showed  that  this  approach 
accuracy,  a  little  thought  reveals  that  the  presence  of  irrelevant 
has  excellent  asymptotic 
In  fact,  Langley  and  Iba’s  [58] 
attributes 
average-case 
that  number  of  training  exam- 
ples  needed 

analysis  of  simple  nearest-neighbor 
(similar 
to  reach  a  given  accuracy 

to  the  PAC  notion  of  sample  complexity) 

slow  the  rate  of  learning. 

should  considerably 

indicates 

A.L.  Blunt,  R  L.angley/Art$cial 

Intelligence 97  (1997)  245-271 

247 

grows  exponentially  with  the  number  of  irrelevant 
get  concepts.  Experimental 
discouraging 

studies  of  nearest-neighbor 

conclusion. 
At  the  other  extreme 

lie  induction  methods 

that  explicitly 

attempt 

attributes, 

tar- 
even  for  conjunctive 
[ 1,611  are  consistent  with  this 

logical  descriptions 

example  of  this  approach,  and  there  are  more  sophisticated  methods 

relevant 

attributes 

for  learning 

nearest-neighbor. 

that  can  augment 

features  and  reject  irrelevant  ones.  Techniques 
the  simplest 
identifying 
including 
much  more  encouraging. 
only  a  small  subset  of  features,  an  algorithm 
hypotheses  under  consideration, 
size  sufficient 
two  extremes  are  feature-weighting  methods 
above 
of  features,  but  still  aim  to  achieve  good  scaling  behavior. 

and  experimental 
theoretical 

Theoretical 
For  instance, 

good  generalization 

to  guarantee 

and 

any 

results 

improve 

for 
induction  method, 
for  these  methods  are 
results  show  that  if,  by  focusing  on 
the  number  of 
in  the  sample 
in  the  middle  of  the 
select  subsets 

that  do  not  explicitly 

reduce 
reduction 

[ 131.  Somewhat 

then  there  is  a  corresponding 

can  significantly 

to  select  relevant 
constitute 

learning. 

terminology, 

We  structure 

in  the  context  of  supervised 

formal  notions  of  ‘relevance’ 

for  this  problem,  characterizing 

algorithms.  We  then  turn  to  discussing 

the  remainder  of  this  section  as  follows.  We  begin  by  describing 

important 
to  introducing 
of  feature-selection 
been  developed 
“wrapper”  approaches,  based  on  the  relation  between 
algorithm.  This  decomposition 
induction 
helps  for  comparing 
approaches 
belong 
also  compare  explicit 
tackle 
which 

several 
In  addition 
these  definitions  help  to  illustrate  some  of  the  general  goals 
that  have 
“filter”,  or 
the  selection  scheme  and  the  basic 
trends,  but  it  also 
that  may  seem  to  be  very  different,  but  can  be  seen  to 
in  certain  ways  have  similar  motivations.  We 
schemes, 

from  a  somewhat  different  perspective. 

to  the  same  category  and  therefore 

to  those  based  on  weighting 

them  as  either  “embedded”, 

in  part  reflects  historical 

some  of  the  methods 

the  same  problem 

feature-selection 

techniques 

2.1.  De@itions  of  “relevance” 

for  features 

in  the  machine 

to  be  “relevant”.  The  reason 

There  are  a  number  of  different  definitions 

literature 
for  what 
is  that  it  generally 
it  means 
depends  on  the  question: 
to  the  point,  different  definitions 
may  be  more  appropriate  depending  on  one’s  goals.  Here,  we  describe  several  important 
In  doing  so,  we  hope  to  illustrate 
definitions  of  relevance,  and  discuss 
taken 
some  of  the  issues  involved  and  some  of  the  variety  of  motivations 
in  the  literature. 

learning 
for  this  variety 

their  significance. 

to  what?’  More 

and  approaches 

“relevant 

let  us  consider  a  setting 

For  concreteness, 
to  describe 

examples 

and  each  feature 

used 
feature  may  be  Boolean 
or  continuous 
F,  x  F2  x  ’ . . x  F,.  The  learning  algorithm 
data  point 
also  be  Boolean,  multiple  valued,  or  continuous). 

is  an  example  paired  with  an  associated 

(what-wavelength?). 

(isred?), 

discrete  with  multiple 
An  example  is  a  point 

i  has  some  domain  Fi.  For 

in  which  there  are  n  features  or  attributes 
a 

instance, 
(what-color?), 
values 
the  instance  space 
in 
is  given  a  set  S  of  training  data,  where  each 
label  or  classification  (which  might 

Although 

the  learning 
two  additional 

algorithm 
quantities, 

sees  only 
as  is  done 

postulate 
[46]  ):  a  probability 

the  fixed  sample  S,  it  is  often  helpful 

in  the  PAC  learning  model 

to 
(see,  e.g., 
c 

distribution  D  over 

the  instance 

space,  and  a  target  function 

248 

A.L.  Blunt,  f?  L.angley/Artijicial  Intelligence  97  (1997)  245-271 

from  examples  to  labels.  We  then  model  the  sample  S  as  having  been  produced  by 
repeatedly  selecting  examples  from  D  and  then  labeling  them  according  to  the  function 
c.  The  target  function  c  may  be  deterministic  or  probabilistic:  in  the  latter  case,  for 
some  example  A,  c(A)  would  be  a  probability  distribution  over  labels  rather  than  just 
a  single  label.  Note  that  we  can  use  the  distribution  D  to  model  “integrity  constraints” 
in  the  data.  For  instance,  suppose  we  are  representing  a  decimal  digit  by  nine  boolean 
features  such  that  feature  i  is  1 if  the  digit  is  greater  than  or  equal  to  i. We  can  model 
this  by  having  D  assign  examples  such  as  101010101  the  probability  zero  (even  though 
the  target  function  c  is  still  defined  on  such  examples). 

Given  this  setup, perhaps  the  simplest  notion  of relevance  is a notion  of  being  “relevant 

to  the  target  concept”. 

Definition  1  (Relevant  to  the  target).  A  feature  xi  is  relevant  to  a  target  concept  c  if 
there  exists  a pair  of  examples  A  and  B  in  the  instance  space  such  that  A  and  B  differ 
only  in  their  assignment  to  Xi and  c(A)  #  c(B). 

Another  way  of  stating  this  definition  is  that  feature  xi  is  relevant  if  there  exists 
some  example  in  the  instance  space  for  which  twiddling  the  value  of  xi  affects  the 
classification  given  by  the  target  concept. 

Notice  that  this  notion  has  the  drawback  that  the  learning  algorithm,  given  access 
to  only  the  sample  S,  cannot  necessarily  determine  whether  or  not  some  feature  xi  is 
relevant.  Even  worse,  if  the  encoding  of  features  is  redundant  (say  every  feature  is 
repeated  twice), 
it  may  not  even  be  possible  to  see  two  examples  that  differ  in  only 
one  feature,  since  at  least  one  of  those  examples  would  have  probability  zero  under 
D.  On  the  other  hand,  this  is  often  the  definition  of  choice  for  theoretical  analyses  of 
learning  algorithms,  where  the  notion  of  relevance  is  used  to  prove  some  convergence 
properties  of  an  algorithm,  rather  than  in  the  algorithm  itself.  The  definition  also  is 
useful  in  situations  where  the  target  function  c  is  a  real  object  that  the  learning  algo- 
rithm  can  actively  query  at  inputs  of  its  own  choosing  (e.g.,  if  the  learning  algorithm 
is  trying  to  reverse  engineer  some  piece  of  hardware)  rather  than  just  a  convenient 
fiction. 

To  remedy  some  of  the  drawbacks  of  the  above  definition,  John,  Kohavi  and  Pfleger 
[42]  define  two  notions  of  what  might  be  termed  “relevance  with  respect  to  a  distribu- 
tion,”  which  also  has  a  nice  interpretation  as  a  notion  of  “relevance  with  respect  to  a 
sample”. 

Definition  2  (Strongly  relevant  to  the  sample/distribution).  A  feature  Xi  is  strongly 
relevant  to  sample  S  if  there  exist  examples  A  and  B  in  S  that  differ  only  in  their 
assignment  to  Xi and  have  different  labels  (or  have  different  distributions  of  labels  if 
they  appear  in  S multiple  times).  Similarly,  xi  is  strongly  relevant  to  target  c  and  dis- 
tribution  D  if  there  exist  examples  A  and  B  having  non-zero  probability  over  D  that 
differ  only  in  their  assignment  to  xi  and  satisfy  c(A)  #  c(B). 

In  other  words,  this  is just  like  Definition  1 except  A  and  B  are now  required  to  be 

in  S  (or  have  non-zero  probability). 

A.L.  Blum,  P  Lungley/Artijicial  Intelligence  97  (1997)  245-271 

249 

Definition  3  (Weakly  relevant 
vant  to  sample  S  (or  to  target  c  and  distribution  D) 
relevant. 
of  the  features  so  that  xi  becomes 

to  the  sample/distribution). 

strongly 

A  feature  Xi is  weakly  rele- 
to  remove  a  subset 

if  it  is  possible 

from 

features 

important 

are  useful 

to  ignore.  Features 

to  keep  and  which 

These  notions  of  relevance 

the  viewpoint  of  a  learning 

to  decide  which 
relevant  are  generally 
a  strongly 

algorithm 
that  are 
attempting 
to  keep  no  matter  what,  at  least  in  the  sense 
strongly 
that 
that  removing 
on  which  other 
are  weakly 
features  are  ignored. 
to  account 
for  statistical  variations.  For  instance,  a  special  case  of  Definition  3  is  that  feature  X, is 
relevant 
weakly 
relevant 
to 
when  all  other 
account 

if  it  is  correlated  with  the  target  function 
features 

so  given  a  finite  sample,  one  would  want 

relevant  may  or  may  not  be  important 

In  practice,  one  may  wish  to  adjust 

for  variance  and  statistical 

to  the  sample.  Features 

feature  adds  ambiguity 

(i.e.,  Xi  is  strongly 

to  keep  depending 

these  definitions 

are  removed), 

significance. 

relevant 

In  a  somewhat  different  vein 

than  the  above  definitions, 

in  many  cases  rather 

features  are  relevant,  we  simply  want 

caring  about  exactly  which 
a  measure  of  complexity.  That  is,  we  want  to  use  relevance 
a  function 
features,  we  just  want 
another  notion  of  relevance  as  a  complexity  measure  with  respect  to  a  sample  of  data  S 
and  a  set  of  concepts  C  is  useful: 

select  a  subset  of 
is  low.  For  this  purpose, 

to  say  how  “complicated” 

it  to  perform  well  when 

to  use  relevance 

is,  and  rather 

than  requiring 

our  algorithm 

this  quantity 

to  explicitly 

than 
as 

Definition  4  (Relevance  as  a  complexity  measure). 
set  of  concepts  C,  let  r(  S, C)  be  the  number  of  features 
a  concept 
features. 

Given  a  sample  of  data  S  and  a 
1  to 
in  C  that,  out  of  all  those  whose  error  over  S  is  least,  has  the  fewest  relevant 

relevant  using  Definition 

over  S  via  a  concept 

In  other  words,  we  are  asking 

for  the  smallest  number  of  features  needed 

to  achieve 
the  concept 
that  is 
from  the  point  of  view  of  the  information  contained,  but  that  is  useless 
this 
error 

optimal  performance 
class  C  is  that  there  may  be  a  feature,  such  as  a  person’s  social-security 
highly 
with  respect 
is  sometimes  modified 
definition 
over  S,  if  this  produces  a  smaller 

to  the  sorts  of  concepts  under  consideration. 

to  allow  concepts 
relevant  set. 

in  C.  The  reason  for  specifying 

in  C  with  “nearly”  minimal 

For  additional 

robustness, 

number, 

relevant 

The  above  notions  of  relevance 

are  independent 

of  the  specific 

being  used.  There  is  no  guarantee 
be  useful 
with  a  notion  of  what  we  might 
call  “usefulness”) 

to  an  algorithm 

: 

that  just  because  a  feature  is  relevant, 

(or  vice  versa).  Caruana  and  Freitag 

term  “incremental 

usefulness” 

learning 

algorithm 
it  will  necessarily 
[ 191  make  this  explicit 
(and  which  they  simply 

Definition  5  (Incremental 
L,  and  a  feature  set  A, feature  Xi  is  incrementally 
accuracy  of  the  hypothesis 
that  L  produces  using 
the  accuracy  achieved  using  just  the  feature  set  A. 

usefulness). 

Given  a  sample  of  data  S,  a  learning  algorithm 
useful  to  L  with  respect  to  A if  the 
the  feature  set  {xi}  U A  is  better  than 

250 

A.L.  Hum,  P. Langley/Artificial  Intelligence  97  (1997)  24.5-271 

This  notion 

is  especially  natural 
of  feature  subsets  by  incrementally 
instance,  many 
To  make 

that  follow 

the  general 

disjunctions 
these  five  examples: 

for  feature-selection 
adding  or  removing 

algorithms 
features 

that  search  the  space 

to  their  current  set-for 

these  definitions  more  clear,  consider 
of  features 

(e.g.,  xt  V x3 V x7),  and  suppose 

that  can  be  expressed 

that  the  learning  algorithm 

as 
sees 

framework  described 
concepts 

in  Section  2.2. 

+ 
100000000000000000000000000000 
111111111100000000000000000000  + 

000000000011111111110000000000  + 

000000000000000000001111111111  + 

000000000000000000000000000000  - 

(note 

The  relevant 

relevant  because 

that  x:!  is  weakly 

1  would  depend  on  the  true  target  concept 
features  using  Definition 
the  first  feature).  Using  Def- 
c  must  include 
(though  any  consistent 
target  disjunction 
relevant  and  the  rest  are  weakly 
is  strongly 
initions  2  and  3,  we  would  say  that  xi 
relevant  by 
relevant 
removing  xi  and  x3,.  . . ,x10).  Using  Definition  4  we  would  say  simply 
that  there  are 
relevant 
to 
three  relevant 
the  smallest  consistent  disjunction.  The  notion  of  incremental 
in  Definition  5 
depends  on  the  learning  algorithm  but,  presumably,  given  the  feature  set  { 1,2},  the  third 
the 
feature  would  not  be  useful  but  any  of  features  xii 
question  of  how  Definition  5  is  related 
to  the  others  at  the  end  of  Section  2.2  when  we 
discuss  a  simple  specific  algorithm. 

to  x30  would  be.  We  will  revisit 

this  is  the  number  of  features 

it  can  be  made  strongly 

(r(  S, C)  = 3),  since 

usefulness 

features 

There  are  a  variety  of  natural  extensions  one  can  make  to  the  above  definitions.  For 
than  just 
to  Definition  4  above,  one  could 
in  S 

one  can  consider 
features. 
individual 
is  the  lowest-dimensional 

relevant 
In  this  case,  in  analogy 

linear  combinations  of  features, 

space  such  that  projecting 

rather 

the  existence  of  a  good  function 
for  statistical  approaches 

all  the  examples 
in  the  class  C?”  This  notion 
Indeed,  methods 
to  learning. 
for  finding 

[44]  are  commonly  used  as  heuristics 

instance, 
relevant 
ask:  “What 
onto  that  space  preserves 
of  relevance 
such  as  principal 
these  low-dimensional 

is  often  most  natural 
component 

analysis 

subspaces. 

2.2.  Feature  selection  as  heuristic  search 

feature-selection 

We  now  turn  to  discussing 

for  viewing  many  of  these  approaches 

algorithms  and,  more  generally,  algorithms 
large  numbers  of  irrelevant  attributes.  A  convenient 
those  that  perform  explicit 
is  that  of  heuristic  search,  with  each  state  in  the  search  space  specifying 
to  this  view,  we  can  characterize  any  feature- 
the  nature  of 

in  terms  of  its  stance  on  four  basic  issues  that  determine 

for  dealing  with  data  sets  that  contain 
paradigm 
feature  selection) 
a  subset  of  the  possible 
selection  method 
the  heuristic 

features.  According 

(especially 

influences 
As  Fig.  1  depicts, 
there 
having  exactly  one  more 

the  direction  of  search  and  the  operators  used  to  generate 

(or  points) 

the  starting  point 

in  turn 
states. 
is  a  natural  partial  ordering  on  this  space,  with  each  child 
that  one  might  start 
feature 

in  the  space,  which 
successor 

its  parents.  This  suggests 

than 

search  process. 
First,  one  must  determine 

A.L.  Blum,  P  Langley/Artificial  Intelligence 97 (1997) 245-271 

251 

Fig.  1, Each  state  in  the  space  of  feature  subsets  specifies 
states 
(to  the  right) 

including  one  more  attribute 

(in  this  case  involving 

in  the  space 

four  features) 
(dark  circles) than  its  parents. 

induction.  Note  that  the 
the  attributes 
are  partially  ordered,  with  each  of  a  state’s  children 

to  use  during 

[27] 

remove 

involves 

the  latter 

is  sometimes 

the  organization 

them.  The  former  approach 

with  nothing  and  successively 
successively 
whereas 
this  partial  ordering:  Devijver  and  Kittler 
and  takes  one  away,  and  genetic  operators 
types  of  connectivity. 
A  second  decision 

add  attributes,  or  one  might  start  with  all  attributes  and 
called  forward  selection, 
is  known  as  backward  elimination.  One  can  also  use  variations  on 
that  adds  k  features 
somewhat  different 

report  an  operator 
like  crossover  produce 

of  the  search.  Clearly,  an  exhaustive 
subsets  of  a  attributes.  A 
the  space.  At  each  point 
selects  one, 
approach  known  as  stepwise  selection 
features  at  each  decision  point, 
track  of  the  search 
these  options,  one  can  consider  all  states  generated  by  the  operators  and 
accuracy 
the  greedy  scheme  with  more  sophisticated 

search  of  the  space  is  impractical, 
more  realistic  approach 
in  the  search,  one  considers 
and  then  iterates.  For  instance, 
or  elimination 
which 
path.  Within 
then  select 
over  the  current 
methods, 
domains. 

the  best,  or  one  can  simply  choose 
set.  One  can  also  replace 

such  as  best-first  search,  which  are  more  expensive  but  still  tractable 

the  hill-climbing 
both  adding  and  removing 

lets  one  retract  an  earlier  decision  without  keeping  explicit 

to  the  current  set  of  attributes, 

the  first  state  that  improves 

relies  on  a  greedy  method 

as  there  exist  2’  possible 

local  changes 

to  traverse 

considers 

in  some 

used  metric 

issue  concerns 

A  third 
One  commonly 
that  occur 
on  information 
a  separate  evaluation 
interacts  with  the  basic 

the  strategy  used  to  evaluate  alternative 

in  the  training  data.  Many 

involves  an  attribute’s  ability 
algorithms 
induction 

to  discriminate 
incorporate 

theory,  but  others  directly  measure  accuracy  on  the  training 
issue  concerns  how  the  feature-selection 

set.  A  broader 

induction 

algorithm,  as  we  discuss  shortly 

in  more  detail. 

subsets  of  attributes. 
among  classes 
a  criterion  based 
set  or  on 
strategy 

252 

A.L.  Blum,  l?  Langley/Art@cial  Intelligence  97  (1997)  245-271 

accuracy;  one  might  continue 

Finally,  one  must  decide  on  some  criterion 

for  halting 
attributes  when  none  of  the  alternatives 

the  search.  For  example,  one 
the 
to  revise  the  feature  set  as  long 
sets  until 
candidate 
the  other  end  of  the  search  space  and  then  select  the  best.  One  simple  halting 
for  the  selected  attributes  maps 
is  to  stop  when  each  combination 
robust 
then  uses  a 

might  stop  adding  or  removing 
estimate  of  classification 
as  accuracy  does  not  degrade;  or,  one  might  continue  generating 
reaching 
criterion 
onto  a  single  class  value,  but 
alternative 
system  parameter 

this  assumes  noise-free 

training  data.  A  more 

the  features  according 

to  some  relevancy 

the  breakpoint. 

simply  orders 

to  determine 

of  values 

improves 

score, 

Note  that  the  above  design  decisions  must  be  made  for  any  induction 

carries  out  feature 
techniques  developed 

selection.  Thus, 
to  address 

To  make  this  more  concrete, 

algorithm 
for  describing 
this  problem,  and  we  will  refer  to  them  repeatedly. 

they  provide  useful  dimensions 

that 
the 

let  us  revisit  the  scenario  given  at  the  end  of  Section  2.1 
features)  with  a 

of  Boolean 

(we  are  considering 
as  a  disjunction 
simple  strategy  known  as  the  greedy  set-cover  algorithm: 

concepts  expressible 

of  zero  features 

Begin  with  a  disjunction 
(which  by  convention 
tive”  on  every  example).  Then,  out  of  those  features  not  present 
example 
choose 
inclusion 
classified 
no  more  “safe” 
positives, 

outputs  “nega- 
in  any  negative 
the  one  whose 
the  number  of  correctly 
there  are 
the  number  of  correctly  classified 

thus  are  “safe” 
the  current  hypothesis  most 

features 
and  then  halt. 

ties  arbitrarily).  Repeat  until 

to  add  into  the  hypothesis) 

(and 
into 

that  would 

(breaking 

examples 

increases 

increase 

positive 

With  respect 

to  our  framework, 

this  algorithm  begins  at  the  leftmost  point 

incrementally  moves 
training 
set  with  an 
when  it  can  take  no  further  step  that  strictly 

rightward  only,  evaluates 
infinite  penalty 

for  misclassifying 

negative 

examples, 

improves 

its  evaluated  performance. 

subsets  based  on  performance 

in  Fig.  1, 
on  the 
and  halts 

Given 

the  five  data  points 
put  in  xi,  then  perhaps  xii, 
that  if  there  exists  a  disjunction 
find  one.  In  fact,  the  number  of  features  selected  by  this  method 
times  larger 

listed  at  the  end  of  Section  2.1,  this  algorithm  would  first 
then  perhaps  xpt,  and  then  would  halt.  It  is  not  hard  to  see 
set,  then  this  method  will 
is  at  most  O(log  ISI) 

features  using  Definition  4  [ 39,451.  2 

than  the  number  of  relevant 

consistent  with  the  training 

to  illustrate 

relationships 

section.  For  instance, 

We  can  also  use  this  algorithm 

in  the  previous 
(Definition 

nitions 
algorithm 
not  necessarily 
strongly 
that  may  cause  it  to  misclassify 
(it  ignores  any  feature 
algorithm’s 
a  negative  example).  On  the  other  hand,  if  the  data  is consistent  with  some  disjunction, 

some  of  the  defi- 
for  this 
is 
then  even 
2)  may  be  ignored  by  the  algorithm  due  to  the 

true.  In  fact,  if  the  data  is  not  consistent  with  any  disjunction, 

features 
useful 
3),  but  the  converse 

5)  will  also  be  weakly  relevant 

the  incrementally 

conservative 

(Definition 

(Definition 

between 

features 

relevant 

nature 

2 This  is  not  too  hard  to  see,  and  follows  from  the  fact that  there  must  always  exist  some  feature  to  add  that 
captures  at least  a  1 /r(  S, C)  fraction  of  the  still-misclassified  positive  examples.  In the  other  direction,  finding 
the  smallest  disjunction  consistent  with  a  given  set  of  data  is  NP-hard  [ 351; a  polynomial-time  algorithm  to 
find disjunctions  only  clog  n times  larger than the  smallest  for  c  <  l/4  would  place  NP into quasi-polynomial 
time  1711. 

A.L.  Blum,  I?  Langley/Artijicial  Intelligence  97  (1997)  245-271 

253 

then  all  strongly 
placed  in  the  algorithm’s  hypothesis), 
feature 

to  a  strongly 

relevant 

features  are  incrementally 

be 
though  the  algorithm  may  prefer  a  weakly  relevant 

(and  all  will  eventually 

useful 

relevant  one  due  to  its  evaluation 

criterion. 

We  now  review  some  specific  feature-selection  methods,  which  we  have  grouped 

those  that  embed  the  selection  within 

the  basic  induction 

three  classes: 
that  use  feature  selection 
selection  as  a  wrapper  around 

tofilter 

features  passed  to  induction, 

the  induction  process. 

into 
those 
and  those  that  treat  feature 

algorithm, 

2.3.  Embedded  approaches 

to feature 

selection 

notes  that  one  can  achieve  slightly  better  bounds 

provide 

logical  conjunctions 

Methods 

for  inducing 

logical  descriptions 

selection  methods  embedded  within  a basic  induction  algorithm. 
for  inducing 
given  above)  do  little  more  than  add  or  remove  features  from  the  concept  description 
response 
in  Fig.  1  also  describes 
ordering 

the  clearest  example  of  feature- 
In  fact,  many  algorithms 
; and  the  greedy  set-cover  algorithm 
in 
the  partial  ordering 
typically  use  this 

errors  on  new  instances.  For  these  methods, 

the  space  of  hypotheses, 

and  the  algorithms 

to  prediction 

[ 75,99,102] 

(e.g., 

to  organize 
results 

their  search  for  concept  descriptions. 
for  learning  pure  conjunctive 

Theoretical 

(or  pure  disjunctive) 

above, 

factor  larger 

of  halfspaces 

encouraging.  As  mentioned 
most  a  logarithmic 
communication) 
halting  earlier  so  that  some 
hypothesis 
is  guaranteed 
cally  with  the  number  of  irrelevant 
in  which 
a  list  of  functions  produced  by  the  induction 
learning 
intersections 
for  learning  DNF  formulas 
above  results 
Pazzani  and  Sarrett 
learning 
conjunctive 
Similar  operations 
inducing  more  complex 
combining 
for  induction, 
a  greedy  search 
function 
They  partition 
subset,  extending 

to  select  the  attribute 

such  as  Quinlan’s 

features 

through 

concepts  are 
at 

the  greedy  set-cover  approach 

finds  a  hypothesis 

than  the  smallest  possible. 

In  fact,  Warmuth 

(personal 

training 

features.  These  results  apply  directly 

in  the  PAC  setting  by 
the  resulting 
examples  are  misclassified.  Because 
to  be  fairly  small,  the  sample  complexity  grows  only  logarithmi- 
to  other  settings 
of 
algorithm.  Situations  of  this  form  include 
[ 14 1,  and  algorithms 
[98].  The 
free  and  worst  case,  but 
for 

spaces 
the  uniform  distribution 

in  constant-dimensional 
time  under 

as  a  conjunction 

(or  disjunction) 

in  n ‘(“gn) 

for  the  greedy  set-cover  method  are  distribution 
report  an  average-case 
logarithmic 

[78] 
that  imply 
for  adding  and  removing 

logical  concepts,  but  these  methods  also  involve 

into  richer  descriptions.  For  example, 

features 

analysis  of  even  simpler  methods 
growth  for  certain  product  distributions. 
the  core  of  methods 
routines 

for 
for 
recursive  partitioning  methods 
[ 151,  carry  out 

[ 8 11,  and  CART 

form 

ID3 
the  space  of  decision 

[ 801  and  C4.5 

to  discriminate 
that  has  the  best  ability 
the  training  data  based  on  this  attribute  and  repeat 
the  tree  downward  until  no  further  discrimination 

trees,  at  each  stage  using  an  evaluation 
the  classes. 
the  process  on  each 
is  possible. 

among 

the  target  concept  can  be  characterized 

to  apply  to  more  complex 

Dhagat  and  Hellerstein 
fashion 

recursive 
k-alternation 
the  set  of  all  attributes 
reasonably 
documents, 

decision 

functions 

techniques 

[28]  have  also  extended 

for  greedy  set  cover  in  a 
such  as  k-term  DNF  formulas  and 
that  can  be  used  even  when 
satisfies  a 
example 
this  is  often  a  good  model  when  dealing  with  text 
that  may  each  contain  only  a  small  number  of  the  possible 

so  long  as  each  individual 

[8]  describes  methods 

is  unbounded, 

small  number  of  them; 
for  instance, 

lists.  Blum 

254 

A.L.  Blum,  P  Lungley/Arti&ial 

Intelligence  97  (1997)  245-271 

in 

words 
embedded  within  another,  more  complex  algorithm. 

the  dictionary.  For  all 

these  cases, 

the  feature-selection 

process 

is  clearly 

Separate-and-conquer 

methods 

in  a  similar  manner.  These 

selection 
feature  that  helps  distinguish 
conjunctive 
other  classes, 
on  the  remaining 

then  remove 

Clearly,  both  partitioning 

training  cases. 

rule  for  C.  They  repeat 

lists  [ 22,73,79] 

for  learning  decision 

techniques  use  an  evaluation 

feature 
to  select  a 
test  to  a  single 
the  rule  excludes  all  members  of 
the  members  of  C  that  the  rule  covers  and  repeat  the  process 

a  class  C  from  others,  then  add  the  resulting 

this  process  until 

function 

embed 

to  other  features 

methods  explicitly 

and  separate-and-conquer 

this  reason,  one  might  expect 

in  a  branch  or  rule,  in  preference 

select  features 
that  appear  less  relevant 
to  scale  well 
that 
irrelevant 
results  exist  for  these  methods, 
studies  by  Langley  and  Sage  [ 611  suggest  that  decision-tree  methods  scale 
target  concepts,  such  as  logical 
they 

the  same  studies  also  show  that,  for  other  targets  concepts, 

them 
few  theoretical 

features  for  certain 

features.  Although 

to  domains 

growth  as  does  nearest-neighbor.  Experiments  by  Almuallim 

[ 31  and  by  Kira  and  Rendell 

[ 471  also  show  substantial 
features  are  introduced 

in 
decreases 
into  selected 

irrelevant 

for  inclusion 
or  irrelevant.  For 
involve  many 
experimental 
linearly  with  the  number  of  irrelevant 
conjunctions.  However, 
exhibit 
and  Dietterich 
accuracy, 
Boolean 

for  a  given  sample  size,  when 
target  concepts. 

the  same  exponential 

involves 

The  standard 

explanation 

of  this  effect 
to  discriminate 

greedy  selection  of  attributes 
in  domains  where  there  is  little  interaction 
tive  concepts.  However, 
feature 
in  isolation 
nificant  problems 
of  this  situation,  but  it  also  arises  with  other  target  concepts.3 

the  presence  of  attribute 
to  look  no  more  discriminating 

for  this  scheme.  Parity  concepts  constitute 

among 

the  reliance  of  such  algorithms 

on 
among  classes.  This  approach  works  well 
the  relevant  attributes,  as  in  conjunc- 
interactions,  which  can  lead  a  relevant 
than  an  irrelevant  one,  can  cause  sig- 
the  most  extreme  example 

lookahead 

to  remedy 

researchers 

these  problems 

search  carries  with 

(e.g., 
it  a  significant 

have  attempted 
techniques 

Some 
search  with 
extensive 
have  responded  by  selectively  defining  new  features  as  combinations 
so  as  to  make  greedy  search  more  powerful  by  letting 
791).  However,  neither  approach  has  been  directly  evaluated 
handle 
analysis. 

greedy 
[ 771))  with  some  success.  Of  course,  more 
cost.  Others 
in  computational 
of  existing  ones, 
[72, 
to 

it  take  larger  steps  (e.g., 

in  terms  of  its  ability 

through  experiment 

large  numbers 

or  theoretical 

by  replacing 

of  irrelevant 

features, 

increase 

either 

2.4.  Filter  approaches 

to feature  selection 

A  second  general 

this  purpose 
and  Pfleger 
attributes  before 

approach 
that  occurs  before 
[ 421  have 
termed 

selection 
to  feature 
the  basic 
induction 
them  jilter  methods,  because 

a  separate  process 

for 
introduces 
step.  For  this  reason,  John,  Kohavi 
they  filter  out  irrelevant 
step  uses  general  characteristics 

occurs.  The  preprocessing 

induction 

3 Note  that  this  problem  does  not  disappear  with  increasing 

sample  size.  Embedded 

rely  on  greedy 
even  when  the  entire  instance 

search  cannot  distinguish  between 
space 

is  available. 

relevant  and  irrelevant 

features  early 

selection  methods 

that 
in  the  search  process 

A.L.  Blum,  R  L.angley/Art@cial  Intelligence  97  (1997)  245-271 

255 

set  to  select  some  features  and  exclude  others.  Thus,  filtering  methods 
that  will  use  their  output,  and  they  can  be 

algorithm 

of  the  training 
are  independent 
of  the  induction 
combined  with  any  such  method. 
the  simplest 

Perhaps 

filtering 

scheme 

is  to  evaluate  each  feature 
(e.g.,  using  a  mutual 

individually 

based 
information  measure) 

then 

on  its  correlation  with  the  target  function 
and 
to  select 
then  be  determined 
categorization 
nearest-neighbor 

the  k  features  with 
by  testing  on  a  holdout  set.  This  method 
[62,63], 
classification 

often 
scheme,  and  has  achieved  good  empirical 

the  highest  value.  The  best  choice  of  k  can 
is  commonly  used  in  text 
in  combination  with  either  a  “naive  Bayes”  or  a 

success. 

tasks 

Kira  and  Rendell’s 
rates  a  more  complex 
a  decision 
reports 

[ 471  RELIEF  algorithm 
feature-evaluation 

function.  Their  system 

follows  this  general  paradigm  but  incorpo- 
then  uses  ID3  to  induce 
[55] 

Almuallim 

for  minimal 

and  Dietterich 

two  extensions 

to  this  method 

tree  from  the  training  data  using  only  the  selected  features.  Kononenko 
types  of  features. 
to  feature  selection 

that  handle  more  general 
[3]  describe  a  filtering  approach 

through 
of  attributes 

involves  a  greater  degree  of  search 
looks 
classes.  This  method  begins  by  looking  at  each  feature 
of  features, 
pure  partitions 
FOCUS then  passes  on  the  original 
to  an  algorithm 
features, 

that 
the  feature  space.  Their  FOCUS  algorithm 
the 
to  pairs 
that  generates 
in  which  no  instances  have  different  classes). 
training  examples,  described  using  only  the  selected 

that  perfectly  discriminate 
in  isolation, 

triples,  and  so  forth,  halting  only  when  it  finds  a  combination 

for  decision-tree 

of  the  training 

combinations 

then  turns 

induction. 

set  (i.e., 

among 

Comparative 

studies  with  a  regular  decision-tree  method 

examples  on  randomly 

number  of  training 
was  almost  unaffected  by  the  introduction 
of  the  decision-tree  method  degraded  significantly.  Schlimmer 
approach 
space  of  feature  sets,  again  starting  with  the  empty  set  and  adding  features  until 
a  combination 

consistent  with  the  training  data. 

of  irrelevant  attributes,  whereas 

that  carries  out  a  systematic 

(to  avoid  revisiting 

selected  Boolean 

that,  for  a  given 
target  concepts,  FOCUS 
the  accuracy 
[ 871  describes  a  related 
the 
it  finds 

through 

showed 

search 

states) 

Although  Focus 

and  RELIEF  follow  feature  selection  with  decision-tree 

features 

for  nearest-neighbor 

for  use  with  a  naive  Bayesian 
that  relies  on  an  embedded 

one  can  of  course  use  other  induction  methods.  For  instance,  Cardie 
as  a  preprocessor 
retrieval,  and  Kubat,  Flotzinger 
[56] 
filter 
a  decision-tree  method 
produce  a  reduced 
information-theoretic 
while  Koller  and  Sahami 
“Markov  blankets”  of  features, 
In  a  somewhat  different  vein,  Greiner,  Grove  and  Kogan 
settings  where  a  helpful 
Table  1  characterizes 

tutor  filters  out  conditionally 
the  recent  work  on  filter  methods 

set  of  attributes.  More  recently,  Singh  and  Provan 

classifier. 
selection 

for  inclusion 

to  filter 

features 

metrics 

(in  this  issue 

[ 541  have  employed  a  cross-entropy  measure,  designed 
for  use  in  both  naive  Bayes  and  decision-tree 

irrelevant  attributes. 

in  a  Bayesian 

Interestingly, 
scheme  as  the  filter 

both  used 
to 
[93]  have  used 
network, 
to  find 

induction. 
[ 371)  consider 

in  the  section,  along  with  the  induction 

algorithm 

feature  set.  The  typical 

described  earlier 
of  the  reduced 
selection  methods.  Most  experiments 
an  unknown 
experimentally 

number  of  irrelevant 

the  effect  of  artificially 

results  show  some  improvement 
have 

focused  on  natural  domains 

features,  but  a  few  researchers 
such  features. 

introducing 

in  terms  of  the  dimensions 
that  takes  advantage 
over  embedded 
that  contain 
[ 3,471  have  studied 

construction, 
[ 171  uses  filtering 
and  Pfurtscheller 

256 

A.L.  Blum,  P: L.angley/Artijicial 

Intelligence  97  (1997)  245-271 

Table  1 
Characterization 
the  space  of  feature  sets 

of  recent  work  on  filter  approaches 

to  feature  selection 

in  terms  of  heuristic 

search 

through 

Authors 

(system) 

Starting 

point 

Search 

control 

Almuallim 

(FOCUS) 

Cardie 

Keller  and  Sahami 

Kira  and  Rendell 

(RELIEF) 

Kubat et  al. 

Schlimmer 

Singh  and  Provan 

None 

None 

All 

- 

None 

None 

None 

Breadth 

first 

Greedy 

Greedy 

Ordering 

Greedy 

Systematic 

Greedy 

Halting 

criterion 

Consistency 

Consistency 

Threshold 

Threshold 

Consistency 

Consistency 

Induction 

algorithm 

Dec.  tree 

Near.  neigh. 

Tree/Bayes 

Dec.  tree 

Naive  Bayes 

None 

No  info.  gain 

Bayes  net 

Another  class  of  filter  methods  actually  constructs  higher-order 
them  in  terms  of  the  variance 

features  from  the  orig- 
the  best  such 

in  the  original 

inal  ones,  orders 
features.  The  statistical 
example  of  this  approach,  generates 
orthogonal 
reduced  dimensionality 
scribe 
theoretical 
intersection 
tribution.  The 
similar 
nal. 

of  halfspaces 

guarantees 

technique  of  principal  components 
linear  combinations 
principal 

space.  Empirically, 
on  a  variety  of  learning 

they  explain,  and  selects 
analysis 
of  features  whose  vectors  are 
components 
tasks.  Blum  and  Kannan 

[ 441,  the  best-known 

has  successfully 

for  methods  of  this  form,  when 

the  target  function 

from  a  sufficiently 
[24] 

analysis 

[ 121  de- 
is  an 
benign  dis- 
incorporates 
rather  than  orthogo- 

and  the  examples 

are  chosen 

related  method  of  independent 

component 

ideas,  but  insists  only  that  the  new  features  be  independent 

2.5.  Wrapper  approaches  to feature  selection 

A  third  generic  approach 

for  feature  selection  also  occurs  outside 

the  basic  induction 

that  method  as  a  subroutine, 

refer  to  these  as  wrapper  approaches 

[ 5 11)  . The  typical  wrapper  algorithm 

(see  Fig.  1)  as  embedded 
induction 

some 

rather 

searches 

than  as  a  postprocessor.  For  this 
(see,  also,  the  paper  by 
the  same 
and  filter  methods,  but  it  evaluates 
algorithm  on  the  training  data  and  using 
the  wrapper 
(e.g., 
topic, 

accuracy  of  the  resulting 

classifier  as  its  metric.4  Actually, 
the  literature  on  statistics  and  pattern  recognition 
the  problem  of  feature  selection  has  long  been  an  active  research 

method  but  uses 
reason,  John  et  al.  [42] 
Kohavi  and  John  in  this  issue 
space  of  feature  subsets 
alternative 
the  estimated 
scheme  has  a  long  history  within 
[ 271))  where 
but  its  use  within  machine 
The  general  argument 

sets  by  running 

learning 

is  relatively 

recent. 

for  wrapper  approaches 

is  that  the  induction  method 

use  the  feature 

subset 

should  provide 

a  better  estimate  of  accuracy 

that  will 
than  a  separate 

4 One  natural  metric 

then  measuring 

running 

involves 

the  induction  algorithm  over  the  entire  training  data  using  a  given  set 
John  et  al. 
that  a  cross-validation  method  provides  a  better  measure  of  expected  accuracy  on  novel 

the  accuracy  of  the  learned  structure  on  the  training  data.  However, 

of  features, 
argue  convincingly 
test  cases. 

A.L.  Blunt.  P: Langley/Artificial 

Intelligence  97  (1997)  24.5-271 

257 

in  favor  of  using  a  wrapper  method 

inductive  bias.  For  example,  both  Doak 
the 
of  forward 

comparisons 

to  improve 

induction.  Doak  reports  experimental 
elimination, 
John  et  al.  present  similar  comparative 

as  well  as  the  impact  of  different 
studies, 

including 

[ 181  report  a  third  set  of  empirical 

search-control 

the  effect  of  using 
studies, 

that  may  have  an  entirely  different 

measure 
[ 291  and  John  et  al.  [42]  argue 
behavior  of  decision-tree 
selection 
and  backward 
techniques. 
wrappers  versus  filters.  Caruana  and  Freitag 
also  focusing  on  decision 
The  major  disadvantage 

trees,  that  explore  variations  on  wrapper  methods. 
of  wrapper  methods  over  filter  methods 
algorithm 

from  calling 

the  induction 
to  invent 

ingenious 

is  the  former’s  com- 
for  each  feature  set 
for  speed- 
for 
time. 
that  instead  speeds  feature  selection 

and  Freitag  describe  a  scheme 

techniques 

search  larger  spaces  in  reasonable 

scheme 

the  evaluation 

cost,  which  results 

putational 
considered.  This  cost  has  led  some  researchers 
ing 
process. 
caching  decision 
Moore  and  Lee  [ 761  describe  an  alternative 
by  reducing 
Certainly 

trees  that  lets  their  algorithms 

In  particular,  Caruana 

the  percentage  of  training  cases  used  during  evaluation. 
the  wrapper 
not  all  work  within 
Indeed,  one  might  expect  methods 

induction. 
take  into  account  all  attributes,  would  benefit  more  from  feature-selection  wrappers 
algorithms 
a  substantial 
learning. 

framework  has  focused  on  decision-tree 
which  by  default 
like  nearest-neighbor, 
than 
schemes.  This  expectation  has  led  to 
for  nearest-neighbor 

embedded 
body  of  work  on  wrapper  methods 

that  themselves 

and  case-based 

incorporate 

Let  us  consider  one  such  approach  and  its  behavior 

in  some  detail.  Langley  and  Sage’s 

[ 591  OBLIWON  algorithm  combines 
method,  which  assigns 
during 
in  these  decisions, 
others. 

learning.  The  feature-selection 

taking 

to  new  instances 

into  account 

the  wrapper 

idea  with  the  simple  nearest-neighbor 

the  class  of  the  nearest  case  stored  in  memory 
process  effectively  alters  the  distance  metric  used 
the 

and  ignoring 

the  features 

relevant 

judged 

search 

through 

elimination 

in  estimated 

OBLIVION  carries  out  a  backward 

sets,  starting  with  all  features  and  iteratively 
est  improvement 
estimated 
method  because 
training  data 
to  measure 
system  uses  leave-one-out 
on  novel 

the  space  of  feature 
the  one  that  leads  to  the  great- 
the 
actually  declines.  We  characterize  OBLIVION  as  using  a  wrapper 
itself  on  the 
the 
the  accuracy  of  each  feature  set 

running 
the  accuracy  with  alternative 
cross-validation 

nearest-neighbor 
feature  sets.  In  particular, 

accuracy.  The  system  continues 

its  evaluation  metric 

this  process  until 

to  estimate 

test  cases. 

removing 

accuracy 

involves 

Although 

[ 761  to  make 

this  approach  may  seem  computationally 

expensive,  OBLIVION  uses  an  in- 
technique 
accuracy  on  N  training  cases  by  holding  out  each  case  in  turn,  constructing 
the  classifier  correctly 

sight  from  Moore  and  Lee 
estimates 
a  classifier  based  on  the  remaining  N  -  1  cases,  seeing  whether 
predicts 
simply  stores 
cessively 
is  no  more  expensive 

the  remaining  ones  to  classify 
accuracy  on  the  training  set  itself. 

the  case,  and  averaging 
the  training 

leave  one  out  by  suc- 
it.  This  scheme 

the  results  over  all  N  cases.  Because  nearest-neighbor 

each  case  and  using 
than  estimating 

cases  in  memory,  one  can  implement 

it  tractable.  5  The 

leave-one-out 

removing 

5 Kohavi 

many  similarities 

[ 501  has  incorporated 
to  OBLIVION. 

the  same 

idea 

into  his  technique 

for  inducing  decision 

tables,  which  has 

258 

A.L.  Blunt,  l?  L.an@ey/Arttjkial 

Intelligence  97  (1997)  245-271 

Table 2 
Characterization  of  recent  work  on wrapper  approaches  to feature selection  in terms  of  heuristic  search  through 
the  space  of  feature  sets 

Authors  (system) 

Starting 
point 

Search 
control 

Halting 
criterion 

Aha  and  Bankert  (Beam) 
Caruana  and  Freitag  (CAP) 
Doak 
John,  Kohavi  and  Pfleger 
Langley  and  Sage  (OBLIVION) 
Langley  and  Sage  (Sel.  Bayes) 
Moore  and  Lee  (Race) 
Singh  and  Provan  (K2-AS) 
Skalak 
Townsend-Weber  and  Kibler 

Comparison 
Comparison 
Comparison 
All 
None 
Comparison 
None 
Random 
All 

Comparison 
Greedy 
Comparison 
Greedy 
Greedy 
Greedy 
Greedy 
Greedy 
Mutation 
Comparison 

No  better 
All  used 
Not  enough  better 
No better 
Worse 
Worse 
No  better 
Worse 
Enough  times 
No better 

Induction 
algorithm 

Near. neigh. 
Dec.  tree 
Tre.e/Bayes 
Dec.  tree 
Near.  neigh. 
Naive  Bayes 
Near. neigh. 
Bayes  net 
Near. neigh. 
Near. neigh. 

Langley  and  Sage  designed  a  number  of  experiments 
suggest 

that,  when  some 

to  evaluate 

their  system.  Results 
are  irrelevant,  OBLIVION 

features 

domains 

classifiers 

with  synthetic 
learns  high-accuracy 
they  also  found 
However, 
suggesting 
that  Holte’s 
due  to  highly  correlated 
than  completely 
chess  end  games  and  predicting 
domains  do  contain 

[40] 
features 

irrelevant 

from  many  fewer  instances 

than  simple  nearest-neighbor. 

that  this  effect  was  absent 

finding  about 

the  accuracy  of  one-level  decision 

(which  cause  no  difficulty 

from  many  of  the  UC1  data  sets, 
trees  was 
rather 
better  on  classifying 
that  these 

class,  giving  evidence 

for  nearest-neighbor) 

irrelevant  ones.  OBLIVION  did  fare  significantly 

a  word’s  semantic 

features. 

Other  researchers  have  also  developed  wrapper  methods  for  use  with  nearest-neighbor. 

Most 

research  on  wrapper  methods  has  focused  on  classification, 

For  instance,  Aha  and  Bankert 
system 
starts  with  a  randomly 
beam  search  rather 
cloud  classification 
feature  selection 
greedy  search  with  random  hill  climbing 

for  nearest-neighbor 

an  option 

subset  of  features  and  includes 

[ 21  report  an  a  technique  much  like  OBLIVION,  but  their 
for 
selected 
on  a 
[94]  work  on 
feature  set,  but  replaces 
for  a  specified  number  of  cycles. 
but  both  Moore 
this  idea  with  k-nearest- 

impressive 
features.  Skalak’s 

also  starts  with  a  random 

[97]  combine 

that  continues 

improvements 

than  greedy  decisions.  They  report 
task  that  involves  over  200  numeric 

and  Kibler 

for  induction  methods 

that  are  highly  sensitive 

for  numeric  prediction.  Also,  most  work  has  emphasized 

to  redundant  features,  can  benefit  from  the  same  basic  approach 

and  Lee  [76]  and  Townsend-Weber 
of 
neighbor 
feature  selection 
features. 
However,  Langley  and  Sage  [60]  have  shown  that  the  naive  Bayesian  classifier,  which  is 
sensitive 
(as  did  Doak’s 
this  idea  to  learning  more  complex 
earlier  work).  Singh  and  Provan 
Bayesian  networks.  This  suggests 
the 
in  the  presence 
algorithms 
behavior  of  induction 
for  feature 
of  irrelevant 
selection 
the  sense  of 
Definition  5),  rather 

[ 921  have  extended 
that  techniques 
in  a  variety  of  situations, 

for  feature  selection  can  improve 
not  only 

focus  on  finding  attributes 
than  necessarily 

attributes.  As  Caruana  and  Freitag 

that  are  useful 
finding 

[ 191  argue,  most  methods 

the  relevant  ones. 

for  performance 

the  advantages 

to  irrelevant 

(in 

A.L.  Blum, P  L.angley/Artificial Intelligence 97 (1997) 245-271 

259 

Table  2  characterizes 

the  recent  efforts  on  wrapper  methods 

in  terms  of  the  dimensions 

discussed  earlier,  as  well  as  the  induction  method  used  in  each  case  to  direct  the  search 
that  researchers  have  developed,  and 
process.  The  table  shows  the  diversity  of  techniques 
of  variant  methods.  Unfortunately, 
comparison 
the  heavy 
reliance  on  the  experimental 
to  deal  with  increasing 
the  algorithms’ 
few  of  these  experiments 
for  them. 
numbers  of  irrelevant 

ability 
results  are  available 

features,  and  few  theoretical 

directly 

study 

2.6.  Feature-weighting  methods 

So  far,  we  have  discussed  algorithms 

that  explicitly  attempt  to  select  a  “most  relevant” 

in  effect  assigning 

to  features, 
this  from  the  explicit 

subset  of  features.  However,  another  approach,  especially 
to  apply  a  weighting 
function 
relevance.  We  have  separated 
the  motivations 
selection 
humans,  or  fed  into  another  algorithm.  Weighting  schemes 
in  on-line 
considerations. 
Weighting 

and  uses  for  these 
is  generally  most  natural  when 

schemes  can  be  viewed 

two  methods 

incremental 

the  result 

tend 

for  embedded 

algorithms, 

is 

feature-selection 

them  degrees  of  perceived 
approach  because 
feature 
by 

to  be  understood 

to  be  different.  Explicit 

is  intended 

tend  to  be  easier  to  implement 
settings,  and  are  generally  more  purely  motivated  by  performance 

feature-selection  methods.  However,  because 
of  feature  sets,  most  approaches 
search.  For 
instance, 
training 

lead  to  simultaneous 

the  most  common 

instances 

changes 

Perhaps 

the  best-known 

attribute-weighting 

in  all  weights. 
method 

in  terms  of  heuristic  search,  as  we  viewed  explicit 
the  weight  space  lacks  the  partial  ordering 
forms  of 
rely  on  quite  different 
in  which 
is  some  form  of  gradient  descent, 

to  feature  weighting 

instances.  The 

[ 741,  which  adds  or  subtracts  weights  on  a  linear 
on 
training 
backpropagation 
additive  changes 
Pomerleau 
whose  features  have  time-varying 

[ 841,  its  generalization 
to  a  set  of  weights 

least-mean 

this  issue 

squares 

(in 

algorithm 

is  the  perceptron 
threshold  unit  in  response 

updating 

rule 
to  errors 
11011  for  linear  units  and 
involve 
set.  6  Baluja  and 
in  domains 

also 

for  multilayer 

neural  networks, 

to  reduce  error  on  the  training 

[ 7]),  discuss  using  a  neural  network  approach 

by  truly 
the  paper  by  Kivinen,  Warmuth  and  Auer  in  this 
that  up- 
[ 661  developed  WINNOW, 
rule. 
of  Y 

rather  than  additively  as  in  the  perceptron 
that,  on  any  on-line  stream  of  data  consistent  with  a  disjunction 

in  settings  dominated 

an  algorithm 

degrees  of  relevance. 
can  have  difficulty 

Perceptron-weighting 

techniques 

showed 

features 

in  a  multiplicative  manner, 

(see,  for  instance, 
[49]  ).  In  response,  Littlestone 

irrelevant 
issue 
dates  weights 
Littlestone 
features,  WINNOW  makes  at  most  0(  rlogn)  mistakes. 
of  relevance  given 
with  the  number  of  irrelevant 
achieves 
formulas, 
negative  examples. 

this  logarithmic 

in  Definition 

degradation 

4.)  Thus, 

threshold 

features 

linear 

and 

(This  effectively  uses  the  notion 

its  behavior  degrades  only 

logarithmically 
in  the  target  concept.  More  generally,  WINNOW 
k-DNF 
for  concept  classes  such  as  conjunctions, 
and 

between  positive 

functions  with  good  separation 

h While  most  work  on  embedded  weighting 

schemes  has  a  neural-network 

driven  method, 
weights. 

embedded  within  a  nearest-neighbor 

learner, 

that  modifies 

fiavor, Aha  [ 11 reports  an  error- 
its  distance  metric  by  altering 

260 

A.L.  Blum,  P. Lungley/Ar?ijicial  Intelligence  97  (1997)  245-271 

For  concreteness,  we  present  a version  of  the  WINNOW algorithm  for  the  disjunction- 
learning  scenario  discussed  in  Sections  2.1  and  2.2,  along  with  a  proof  of  Littlestone’s 
theorem: 

The  Winnow  algorithm  (a  simple  version). 

I.  Initialize  the  weights  WI,.  . . , w,  of  the  features  to  1. 
2.  Given  an  example  (xi,. 

. . ,x,>,  output  1 if  ~1x1  +  . . . +  w,x,  >  n,  and  output  0 

otherwise. 

3.  If  the  algorithm  makes  a mistake: 

(a)  If  the  algorithm  predicts  negative  on  a  positive  example,  then  for  each  xi 

equal  to  1, double  the  value  of  wi. 

(b)  If  the  algorithm  predicts  positive  on  a  negative  example,  then  for  each  xi 

equal  to  1, cut  the  value  of  Wi in  half. 

4.  Go  to  2. 

Theorem  6.  WINNOW  makes  at  most  2 +  3r( 1 +  lg n)  mistakes  on  any  sequence  of 
examples  consistent  with  a  disjunction  of  r features. 

Proof.  Let  us first bound  the number  of  mistakes  that  will be  made  on positive  examples. 
Any  mistake  made  on  a  positive  example  must  double  at  least  one  of  the  weights  in 
the  target  function  (the  relevant  weights),  and  a  mistake  made  on  a  negative  example 
will  not  halve  any  of  these  weights,  by  definition  of  a disjunction.  Furthermore,  each  of 
relevant  weights  can  be  doubled  at  most  1 +  lgn  times,  since  only  weights  that  are  less 
than  n  can  ever  be  doubled.  Therefore,  WINNOW makes  at  most  r( 1 +  lg n)  mistakes 
on  positive  examples. 

Now  we  bound  the  number  of  mistakes  made  on  negative  examples.  The  total  weight 
summed  over  all  features  is  initially  n.  Each  mistake  made  on  a  positive  example 
increases  the  total  weight  by  at  most  n  (since  before  doubling,  we  must  have  had 
. . w,x,  <  n).  On  the  other  hand,  each  mistake  made  on  a  negative  example 
WlXl +. 
decreases  the  total  weight  by  at  least  n/2  (since  before  halving,  we  must  have  had 
wixi  +  ...  +  w,x,  2  n).  The  total  weight  never  drops  below  zero.  Therefore, 
the 
number  of  mistakes  made  on  negative  examples  is at most  twice  the  number  of  mistakes 
made  on  positive  examples,  plus  2;  that  is,  2 +  2r(  1 +  lg n).  Adding  this  to  the  bound 
on  the  number  of  mistakes  on  positive  examples  yields  the  theorem. 

0 

The  same  general  approach  of  WINNOW has  been  used  in  algorithms  developed  by 
Littlestone  and  Warmuth  [ 691,  Vovk  [ 1001, Littlestone,  Long  and  Warmuth  [ 671,  and 
Cesa-Bianchi  et  al.  [21].  Kivinen  and  Warmuth  [48]  describe  relations  between  these 
approaches  and  additive  updating  methods  such  as  the  least  mean  squares  algorithm. 
In  fact,  these  multiplicative  updating  schemes  are  very  similar  to  the  kind  of  multi- 
plicative  probability  updates  that  occur  in  Bayesian  methods,  and  several  of  the  results 
provide  bounds  on  the  performance  of  Bayesian  updating,  even  when  the  probabilistic 
assumptions  of  that  approach  are  not  met.  Experimental  tests  of  WINNOW and  related 
multiplicative  methods  on  natural  domains  have  revealed  good  behavior  [ 6,93,  and stud- 
ies  with  synthetic  data  show  that  they  scale  very  well  to  domains  with  even  thousands 
of  irrelevant  features  [ 681. 

A.L.  Blum,  P  L.angley/Art$cial  Intelligence 97 (1997) 245-271 

261 

More  generally,  weighting  methods  are  often  cast  as  ways  of  merging  advice 

sources 

that  may 

themselves 

knowledge 
through 
the  weighting  process  plays  an  interesting  dual  role  with  respect 

different 
this  light, 
methods  discussed  earlier.  Filter  approaches  pass  their  output 
to  a  black-box 
classifiers 
combine 

generated  by  black-box 

their  predictions. 

and  determine 

be  generated 

algorithms 

learning 

learning 

algorithm,  whereas  weighting  approaches  can  take  as  input 

the 
the  best  way  to 

from 
learning. 
In 
to  the  filter 
(a  set  of  selected  features) 

On  the  other  hand,  direct  analogs 

to  the  filter  and  wrapper  approaches  do  exist  for 
that  use 

probability 

distributions 

[96]  describe 

filter-like  methods 

determining  weights.  Stanfill  1951  and  Ting 
conditional 
features  based  on  an 
et  al.  [26]  present  a  different  weighting 
information-theoretic 
to 
the  same  end.  Finally,  Kohavi,  Langley  and  Yun  [52]  have  adapted  the  wrapper  method 
in  much  the  same  way 
to  search  through  a  discretized  weight  space  that  can  be  explored 
as  feature  sets.  Each  of  these  approaches 
over  use  of  all  features, 
but  only 

the  latter  reports  comparisons  with  a  simple  selection  of  attributes. 

metric,  and  one  could  use  the  scores  produced  by  RELIEF  [47] 

to  weight  attributes 
scheme 

for  nearest-neighbor.  Daelemans 

shows  improvement 

that  normalizes 

3.  The  problem  of  irrelevant  examples 

than  others.  This  suggests  a  second  broad 

are  more  useful 

Just  as  some  attributes 
process 
aid  the  learning 
that  concerns 
themselves, 
the  examples 
their  selection.  Some  work  has  assumed 
informative 
instances, 
However,  a  more  robust  approach 
training  examples  by  itself. 

such  as  near  misses,  or  provides 
involves 

letting 

and  here  we  briefly  consider 
the  presence  of  a  benevolent 

than  others,  so  may  some  examples  better 
type  of  relevance 
for 
techniques 
tutor  who  gives 
[ 1021. 
the  learning  system  select  or  focus  on 

ideal  training 

sequences 

intensive; 

algorithm 

is  computationally 

efficiency.  Another 

labels  must  be  obtained 

training  data  is  available, 

it  makes  sense  to  learn  only  from  some  examples 

have  proposed  at  least  three  reasons 
is  if  the  learning 

Researchers 
learning.  One 
sufficient 
purposes  of  computational 
(e.g.,  when 
from  experts)  but  many  unlabeled 
available  or  are  easy  to  generate.  Yet  a  third  reason  for  example  selection 
the  rate  of  learning  by  focusing 
the  space  of  hypotheses.  Here  we  should  distinguish  between  examples 
through 
relevant 
from  the  viewpoint  of  infomtation 
of  one’s  algorithm.  Most  work  emphasizes 
are  sometimes  used  for  this  purpose. 

for  selecting  examples  used  during 
in  this  case,  if 
for 
is  high 
examples 
are 
is  to  increase 
thus  aiding  search 
that  are 

and  ones  that  are  relevant  from  the  viewpoint 
information-based  measures 
the  latter,  though 

is  if  the  cost  of  labeling 

attention  on  informative 

examples, 

reason 

As  with  feature-selection 

schemes,  we  can  separate  example-selection  methods 

the  selection  process  within 
to  the  induction 

them 

those 
that  embed 
examples  before  passing 
around 
selection 
this  dimension 
between  methods 
that  select  from  unlabeled 

to  the  learning 

successive 

calls 
below,  we  will  instead  organize 
that  select  relevant  examples 
instances. 

into 
that  filter 
that  wrap  example 

algorithm, 

the  learning 
process,  and  those 
technique.  Although  we  will  refer 
the  section  around  another  distinction: 

those 

to 

from  labeled  training 

instances  and  ones 

262 

A.L.  Blum,  I!  Langley/Artificial  Intelligence  97  (1997)  245-271 

3.1.  Selecting 

labeled  data 

The  first  generic  approach  assumes 

for 
system,  but  that  not  all  of  these  examples  are  equally  useful.  As  we 

training  data  is  available 

that  a  set  of  labeled 

the  process  of  example  selection  within 

use  by  the  learning 
noted  above,  one  can  embed 
algorithm,  and  many  simple 
ceptron  algorithm, 
methods  only  learn  from  an  example  when  their  current  hypothesis  misclassifies 
embedded  methods, 
which  their  hypothesis 

edited  nearest-neighbor  methods,  and  some  incremental 

take  this  approach.  For  instance, 

called  conservative 

is  correct.  ’ 

algorithms, 

sometimes 

induction 

schemes 

it.  Such 
ignore  all  examples  on 

the  basic  learning 
the  per- 

conjunctive 

If  one  assumes 

then  one  can  guarantee 

that  with  high  probability, 
to  the  success  criteria  used  for  testing 

bution, 
overall  be  relevant 
however, 
examples 
when  a  conservative 
cases,  and  when  it  achieves  10%  error,  it  will  ignore  90%  of  the  data. 

that  training  data  and  test  data  are  both  taken  from  a  single  fixed  distri- 
the  data  used  for  training  will 
[ 141.  As  learning  progresses, 
and 
less  useful.  For  instance, 
algorithm  has  a  20%  error  rate,  it  will  ignore  80%  of  the  training 

about  certain  parts  of  the  input  space  increases, 
portion  of  the  space  become 

the  learner’s  knowledge 
in  the  “well-understood” 

(roughly) 

Although 

it  wishes 

each  time 

is  proportional 

constant.  Thus, 

In  the  PAC  model, 

in  l/e 
algorithms 

one  can  use  explicit  example 

learning, 
induction  methods. 

seen  in  order  to  halve  their  error  rate  [ 14,34,86].  However,  for  conservative 
since  the  number  of  examples  actually  used  for  learning 
the  number  of  new  examples  used  by  the  algorithm 
error  rate  remains 
achieve  some  error  rate  E  is  really  just  logarithmic 
this  result  holds  only  for  conservative 

learning  algorithms  need  to  roughly  double  the  number  of  examples 
algorithms, 
to  the  error  rate, 
its 
to  halve 
the  number  of  examples  actually  used  to 
rather  than  linear. 
that  embed 

the  example- 
to  achieve 
a 
and  adjusts 
some  training  data)  based  on  the  algorithm’s 
the  input 
to  keep  the  accuracy  of  the  learner’s  current  hypothesis  near  to  that  of  random 
focuses  on  the  currently  hard  data.  Schapire 
use  of  examples  described 
the  logarithmic 
improved  on 
that 
the  accuracy  of  neural  network  methods  on  tasks  involving  optical 
like 
This  approach 

selection  process  within 
similar 
for  other 
effects 
wrapper  method 
the  distribution 
behavior.  The  basic 
distribution 
guessing.  As  a  result, 
has  shown 
above  under  quite  general 
this 
boosting  can  improve 
character 
recognition. 
backpropagation, 

training 

is  much  more  expensive 

than  prediction.’ 

the  learning  process 
lets  one  achieve 

idea  is  that,  as  learning  progresses, 

In  particular,  Schapire 

front,  Drucker  et  al. 

[ 30,311  have  shown 

[ 33,341  has  further 

the  booster  samples 

to  it  (by  removing 

seems  especially 

the  experimental 

takes  a  generic 

called  boosting 

technique.  On 

[86]  describes 

for  techniques 

that  boosting 

and  Freund 

appropriate 

conditions, 

for  which 

algorithm 

selection 

learning 

given 

that 

7 Littlestone 

and  Mesterharm 
can  deal  better  with  irrelevant 
This  shows  there  exist  interactions 

[68]  have  shown 

that  a  variant  of  naive  Bayes 
than  the  standard  version,  which  updates 

features 

that  learns  only  from  errors 
its  statistics  on  each  example. 

between 

the  problems  of  feature  selection  and  example  selection. 

x Although  boosting  has  clear  empirical  uses,  it  was  originally  developed 

learning 

that  “weak 
will  perform  somewhat  better  than  guessing  over  every  distribution, 
function  being 

learned,  and  one  can  boost  performance 

implies  strong 

learning” 

in  the  PAC  model.  In  other  words, 

to  produce  high-quality 

for  the  theoretical  goal  of  showing 
that 
if  one  has  an  algorithm 
to  the 

then  there  cannot  be  a  hard  “core” 
predictions. 

A.L.  Blunt,  P  Langley/Artificial  Intelligence  97  (1997)  245-271 

263 

the  time  needed 

selects  a  random 

Another  class  of  wrapper  methods 

induction.  Quinlan 
to  construct 

study  of  decision-tree 
to  reduce 
Windowing 
tree,  then  uses  that  tree  to  classify  all  the  remaining 
cases,  the  method  selects  another 
a  new  decision 
classifies 
reduction 
describes 
sets.  John  and  Langley 
proper  size  of  a  randomly 

in  the  experimental 
for  example  selection  originated 
technique  designed 
[SO]  reports  a  windowing 
sets. 
training 
large 
trees  from  very 
decision 
sample  of  the  training  data  to  induce  an  initial  decision 
the  misclassified 
the  original  sample,  constructs 
the  process  until  it  has  a  tree  that  correctly 
led  to  substantial 
reports 
[20] 
training 
the 

all  of  the  training 
in  processing 
another  wrapper  method  called  peepholing 

tree,  and  so  forth,  repeating 
data.  Quinlan 

time  on  a  large  collection  of  chess  endgames, 

report  a  much  simpler  use  of  wrappers 

random  set  to  augment 

examples.  From 

for  even  larger 

that  windowing 

to  determine 

and  Catlett 

designed 

selected 

training 

[43] 

sample. 
[ 641  describe  a  filter  approach 
learning 
in  the  machine 

are  less  common 

Lewis  and  Catlett 

techniques 
methods.  One  can  imagine  simple  techniques 
inconsistent 
examples 
widely  used.  One-pass 
again  research  has  leaned 
and  windowing. 

to  selection  of  labeled  data,  but  such 
than  embedded  or  wrapper 
literature 
training  data,  say  by  removing 
that  are  identical  except  for  their  class,  but  such  methods  are  not 
filtering,  but 
sampling  of  the  training  data  would  also  constitute 
like  those  in  boosting 

iterative  versions  of  sampling 

for  cleaning 

towards 

3.2,  Selecting  unlabeled  data 

[ 891.  Given  an  unlabeled 

The  learner  can  also  select  data  even  before  it  has  been  labeled.  This  can  be  useful 

in 
is  expensive. 
scenarios  where  unlabeled  data  is  plentiful,  but  where  the  labeling  process 
to  this  problem,  which  can  be  embedded  within  an  induction  algo- 
One  generic  approach 
a  set  of  hypotheses  consistent  with  the  training  data,  is  called  query 
rithm  that  maintains 
at 
by  committee 
random 
the  label 
for  the  instance.  The  basic  idea  is  that  informative  or  relevant  examples  are  more  likely 
to 
the  same  way.  Unfortunately, 
to  pass  the  test  than  those  that  most  hypotheses  classify 
requires  much  stronger  constraints  on 
obtain 
theoretical 
this  method  requires  an  ability 
the  space  of  hypotheses 
to  sample  random  consistent  hypotheses,  which  can  be  quite  difficult,  although 
it  is  also 
a  major 

the  method  selects 
set  and,  if  they  make  different  predictions, 

for  query  by  committee 
than  does  boosting.  Specifically, 

two  hypotheses 
requests 

topic  of  algorithmic 

from  the  consistent 

) . 
There  has  been  a  larger  body  of  work  on  algorithms 

[ 32,70,91] 

instance, 

research 

results 

(e.g., 

of  this  sort  is  to  take  a  known  example 
the  effect  on  its  classification. 
then  “walk” 

them 

that  generate 

examples 
query  algorithms  within 

of 
the 
the  empirical  community.  A  common 
and  slightly 
For  instance,  one 
towards  each 
is 
in  turn, 

changes 

(this, 

the  heading  of  membership 

to  determine 

its  feature  values 

used  by  algorithms 

under 
and  experimentation  within 

their  own  choosing, 
theoretical  community 
technique 
alter 
take 
might 
other 
to  determine 
often  used  to  determine 
class  of  methods  effectively  designs  critical  experiments 
them  eliminate 
hypotheses, 
[75] 
learning 

two  examples  with  different 
at  what  point 

letting 
task.  Mitchell 

relevant  features, 

competitors 

suggested 

tying 

labels  and 
the  desired  classification 

and  thus  reduce 

an  information-theoretic 

the  complexity 
approach 

of  the 
to  example 

in  with  our  earlier  discussion).  Another 
among  competing 

to  distinguish 

264 

A.L.  Blum,  I? L.angley/Artijicial  Intelligence  97 (1997) 245-271 

selection,  whereas  Sammut  and  Banerji 
but  demonstrated 
has  continued 
successful 
variance. 
to  generate  queries  greatly  enlarges 
guarantee  polynomial-time 

this  tradition; 
results  with  a  system 

In  parallel, 

theoretical 

learning. 

[ 851  and  Gross 
their  advantage  empirically.  More  recently,  work  on  “active 

for  instance,  Cohn,  Ghahramani 

that  selects  examples  designed 

researchers 

[4,5,16,41,83] 

the  types  of  concept  classes 

[ 381  used  less  formal  methods 
learning” 
[23] 
report 
the  learner’s 
have  shown  that  the  ability 
for  which  one  can 

and  Jordan 
to  reduce 

Although  much  work  on  queries  and  experimentation 

learning,  other  efforts  have  addressed  more  complex 

has  emphasized 
learning 

simple  classifi- 
tasks.  For  example, 
the 

[53] 

let  their  grammar-induction 

system  query  an  oracle  about 

strings 

to  distinguish 

[ 571  KEKADA  and  Rajamoney’s 
hypotheses 

among  competing 

among  competing  hypotheses, 

and  Kulkarni 
[ 821  COAST  design  critical  experiments 
in  scientific  domains.  Finally,  Shen  and 
in  learning  action 

the  uses  of  experimentation 

cation 
Knobe  and  Knobe 
legality  of  candidate 
and  Simon’s 
to  distinguish 
Simon 
models 

[ 901  and  Gil 
for  planning 
learning 

Other 

[ 361  have  explored 
tasks. 

systems 

incorporate 

strategies 

that  have  not  yet  been  encountered 
the  domain.  For  example,  Scott  and  Markovitch 
learning 

and  many  methods 

space 
about 
vised 
toward  exploring  unfamiliar 
considerably 

situations, 

increase 

learning 

parts  of  the  state  space  (e.g., 

rates  over  random  presentations. 

to  obtain  more 

for  reinforcement 

Most  work  on  selecting 

and  querying  unlabeled 

for  exploring  portions  of  the  instance 
information 
representative 
[ 881  adapt  this  idea  to  unsuper- 
include  a  bias 
[65]).  Both  approaches  can 

learning 

but  Angluin 
query  method 
membership 
learning 
in  which 
the  number  of  irrelevant 
features  known 
to  determine 
queries 
place  a  new  relevant 

et  al.  [5]  and  Blum  et  al.  [Ill 

that  can  be  applied 
queries  are  available, 

theoretical 
describe 
to  any  algorithm.  Specifically, 
any  algorithm  with  a  polynomial  mistake  bound 

data  has  used  embedded  methods, 
results 
for  a  wrapper 
they  show  that  when 
for 
in  an  automated  way  into  one 
on 
idea  is  to  gradually  grow  a  set  of 
to  use 
feature  and,  if  so,  to 

dependence 

relevant 

the  algorithm  makes  a  mistake, 

to  be  relevant, 

features  present.  The  basic 
and  whenever 
results 
into  the  set. 

if  the  mistake 
feature 

from  a  missing 

a  “reasonable” 
concept  class  can  be  converted 
the  number  of  mistakes  plus  queries  has  only  a  logarithmic 

4.  Challenges 

for  future  relevance  research 

Despite 

relevant 
can 
challenges 

the  recent  activity, 
features  and  examples, 

the  associated 

and 
progress, 
there  remain  many  directions 

improve 

its  study  of  these 

for  the  theoretical 

important 
and  empirical 

problems.  Here  we  outline 
learning  communities. 

in  methods 
in  which  machine 
some 

for  selecting 
learning 
research 

4.1.  Theoretical  challenges 

We  claim 

that,  in  a  sense,  many  of  the  central  open  theoretical  problems 

learning 
well-known 

revolve  around  questions  of  finding 

relevant 

features.  For  instance,  consider 

question  of  whether 

there  are  polynomial-time 

algorithms 

that  can  guarantee 

in  machine 
the 

A.L.  Blum.  l? Langley/Art$cial 

Intelligence 97 (1997) 245-271 

265 

DNF  formulas 
the  similar  question  of  whether  polynomial-size 

learning  of  polynomial-size 
Or,  consider 
in  either  model.  These  questions  both  include 
case: 

in  the  PAC 

or  uniform 

models. 
trees  are  learnable 
the  following  open  problem  as  a  special 

decision 

distribution 

Does 
there  exist  a  polynomial-time 
functions  over  (0,  l}n  that  have  log*(n) 
distribution  models? 

algorithm 

for  learning 
relevant  features, 

the  class  of  Boolean 
in  the  PAC  or  uniform 

This  is  a  special  case  because  any  function 

tree  and  a  small  DNF  representation 
if  we  knew  a  priori  which 

that  has  only  log,  n  relevant 
by  definition,  be  written  as  a  truth  table  having  only  n  entries,  and  therefore 
a  small  decision 
would  be  trivial 
other  hand, 
algorithm 
has  been  proven 
issues  of  finding 
hard. 

features  can, 
it  must  have 
that  the  learning  problem 
log,  n  variables  were  relevant).  9  On  the 
any 
in  the  sense  that  the  class 
[ lo].  Thus, 
those  classes 

to  be  a  quite  difficult  special  case.  For  instance, 

features  seem  to  be  at  the  core  of  what  makes 

to  learn  in  the  statistical  query  model  of  Kearns 

this  problem  would  need  to  be  “unusual” 

impossible 
relevant 

this  problem 

to  solve 

appears 

(note 

As  a  practical  matter, 

test  a  proposed  algorithm 
is  given.  In  fact,  functions 
truth  tables  in  this  class  are  generally  easy.  To  allow  for  easier  experimental 

it  is  unclear  how  to  experimentally 
on  the  targetfunctions 

since  no  distribution 

the  following 
that  seems  quite  hard  even  for  uniform 

for  this  problem, 

is  a  speci$c  distribution  on  the  target 
random  examples 

(for  convenience, 

for  this  problem, 
with  random 
testing  of  algorithms 
functions 
the  number  of  relevant 

features 

is  2 log,  n): 

the  parity  of  the  bits  indexed  by  S  (that 

Select  at  random 
X,  compute 
number  of  ones?) 
does  T  contain  more  ones  than  zeroes?), 
results. 

two  disjoint  sets  S, T  c  { 1,.  . . , n}  each  of  size  log,  n.  On  input 
is,  does  S  contain  an  odd 
is, 
the  exclusive-or  of  the  two 

function  of  the  bits  indexed  by  T  (that 

and  the  majority 

and  output 

lo 

ability 
lists,  parity 
the  class  of 

challenge 
to  more  complex 

A  second 

theoretical 

is  to  develop  algorithms  with 

the  focusing 

of  WINNOW  that  apply 
functions, 
problems 

target  classes  such  as  decision 
functions.  This  would  greatly  extend 
in  on-line  settings. 
In  the  framework  of  example  selection,  one  important  direction 

or  general 
for  which 

there  exist  positive 

threshold 

results 

linear 

is  to  connect 

the  work 

query  models,  which  have  the  advantage  of  generally  being  algorithmic 

on  membership 
but  assume 
filtering  unlabeled 
often  require  solving  a  computationally 

that  arbitrary  points 

in  the  input  space  may  be  probed,  with  the  work  on 
instances,  which  apply  when  only  a  fixed  data  stream  is  available,  but 
is  to  further 

hard  subproblem.  Another  challenge 

‘) In  fact, 

this  class 

the  algorithm 

to  learn  when 
Indeed, 
in  the  exact 

is  easy 
examples  of  its  own  choosing. 
with  membership 
larger  class  of  general  DNF  formulas  using  membership  queries,  with  respect 
I” For  instance, 
be  positive,  since  the  first  three  bits  have  an  even  number  of  ones  (making 
bits  have  more  ones  than  zeros  (so  the  majority 

the  algorithm  of  Bshouty 
leaning  model,  and  a  recent  algorithm  of  Jackson 

then  the  classification 

can  make  active 

and  T  =  {4,5,6} 

if  S  =  { 1,2,3} 

function 

queries 

learns 

[41] 
to  the  uniform  distribution. 
of  the  example  011101001010  would 
their  parity  0).  and  the  next  three 

is  I),  and  the  XOR  of  those  two  quantities 

is  I 

queries  about 
trees 
the  even 

[ 161  learns  the  larger  class  of  decision 

(membership) 

266 

A.L.  Blum,  l?  Langley/Art@cial 

Intelligence  97  (1997)  245-271 

theoretically 
process. 

analyze 

the  ways  in  which  example  selection  can  aid  the  feature-selection 

4.2.  Empirical 

challenges 

(204  attributes) 

( 1675  attributes), 

data  sets.  For  instance, 

Considerable  work  also  remains  on  the  empirical 

front,  with  one  of  the  most  urgent 
needs  being  studies  on  more  challenging 
few  of  the  domains  used 
to  date  have  involved  more  than  40  features.  Two  exceptions  are  Aha  and  Bankert’s  study 
of  cloud  classification 
retrieval 
Moreover,  Langley 
that  many  of  the  widely-used  UC1  data  sets  have  few  completely 
In  hindsight, 
about 
relevant 
world  domains 
substantial 
feature  selection. 

and  Koller  and  Sahami’s  work  on  information 
have  dealt  with  far  fewer  features; 
results  with  the  nearest-neighbor  method  suggest 
attributes. 
to  ask 
that  many 
real- 
find  data  sets  with  a 
to  test  adequately  our  ideas  on 

ignore  other  ones.  However,  we  believe 

features 
and 
do  not  have 

this  character, 
attributes 

for  diagnostic  domains, 

but  typical  experiments 

fraction  of  irrelevant 

this  seems  natural 

in  which  experts 

that  we  must 

if  we  want 

and  Sage’s 

irrelevant 

tend 

[61] 

and 

roles 

important 

methods.  Such  data  sets  can 

such  as  the  number  of  relevant 

Experiments  with  synthetic  data  also  have 

feature-selection 
interest, 
factors  constant. 
gorithms  as  a  function  of  these  factors,  showing 
features.  However,  we  distinguish 
many 
irrelevant 
such  systematic 
experiments 
Monks  problems),  which  seem  much 

in  the  study  of 
vary  factors  of 
attributes,  while  holding  other 
of  al- 
the  sample  complexity 
to  scale  to  domains  with 
the  use  of  synthetic  data  for 
and  reliance  on  isolated  artificial  data  sets  (such  as  the 

and  irrelevant 
In  this  way,  one  can  directly  measure 

their  ability 
between 

let  one  systematically 

less  useful. 

to  play 

improvements 

More  challenging 

in  efficiency  would 

for  feature  selection.  Although 

increase 
cannot  eliminate 

the  number  of  states  examined, 
caused  by  exponential 
problems 

domains,  with  more  features  and  a  higher  proportion  of  irrelevant 
further 
such  constant- 
growth 
in  terms  of  heuristic 

ones,  will  require  more  sophisticated  methods 
increases 
in  the 
factor 
search 
these  problems 
number  of  feature  sets.  However,  viewing 
suggests  some  places 
In  general,  we  must  invent  better  techniques 
to  look  for  solutions. 
for  selecting  an  initial  feature  set  from  which  to  start  the  search,  formulate  search-control 
methods 
improved 
feature  sets,  and  design  better 
frameworks 
that  will  improve  efficiency  without  sacrificing  accuracy.  Future  research 
halting  criteria 
and 
in  the  area  should  also  compare  more  carefully 
leaving 
attribute-weighting 
each  approach  has  some  advantages, 
informed 
an  open  question 
by 
by  experiment, 
to  relevance. 
experiments 

but  preferably 
to  test  specific  hypotheses  about  these  two  approaches 

in  the  space  of  feature  sets,  devise 
of  alternative 

that  take  advantage  of  structure 
the  usefulness 

schemes.  Presumably, 
is  best  answered 
that 

the  behavior  of  feature-selection 

for  evaluating 

designed 

feature 

and  example 

More  generally, 

selection 
related  and  we  need  more  studies  designed 

intimately 
this  relationship.  Much  of  the  empirical  work  on  example 
has  dealt  with 
this  approach 
potential 
sort  promises 

to  keep  the  field  of  machine 

involving  many 

learning  occupied 

low-dimensional 

for  domains 

spaces,  yet 

irrelevant 

selection 

are  tasks 

that  seem 

to  help  understand 

to  be 
and  quantify 
[23,38] 

) 
(e.g., 
selection 
clearly  holds  even  greater 
issues  of  this 

for  many  years  to  come. 

features.  Resolving  basic 

A.L.  Blunt,  II  Langley/Artificial 

Intelligence  97  (1997)  245-271 

267 

Acknowledgements 

This  research  was  supported 

in  part  by  Grant  No.  CCR-9357793 
by  a  Sloan  Foundation  Research  Fellowship, 

from  the  National 
and  by  Grant  No. 
active 
from  the  Office  of  Naval  Research.  Many  of  the  researchers 
directly  or  indirectly, 
to  the 
the  referees  and  the  editors 

in  this  paper.  We  would  also  like  to  thank 

Science  Foundation, 
l-0505 
N00014-94- 
in  the  area  of  feature  and  example 
ideas  presented 
of  this  issue  for  their  helpful  comments 

and  suggestions. 

contributed, 

selection 

References 

I 11  D.  Aha,  A  study  of  instance-based 

algorithms 

for  supervised 

learning 

evaluations,  Doctoral  Dissertation,  Department  of  Information 

tasks:  mathematical, 

empirical 
and  Computer  Science, 

and  psychological 
University  of  California, 

Irvine,  CA  ( 1990). 
[ 21  D.W.  Aha  and  R.L.  Bankert,  A  comparative 
D.  Fisher  and  J.-H.  Lenz,  eds.,  Arttficial 

evaluation  of  sequential 

feature  selection  algorithms, 
Intelligence  and  Statistics  V  (Springer,  New  York,  1996). 

in: 

]3]  H.  Almuallim 

and  T.G.  Dietterich,  Learning  with  many 

irrelevant 

features, 

in:  Proceedings  AAAI-91, 

Anaheim,  CA  (AAAI  Press,  1991)  547-552. 

141  D.  Angluin,  Learning 

regular 

sets  from  queries  and  counterexamples, 

Inform.  and  Compuf.  75  ( 1987) 

87-106. 

[ 5 1 D.  Angluin,  L.  Hellerstein 

and  M.  Karpinski,  Learning 

read-once 

formulas  with  queries, 

J.  ACM  40 

(1993) 

185-210. 

[ 61  R.  Armstrong,  D.  Freitag,  T.  Joachims  and  T.  Mitchell,  Webwatcher: 

Wide  Web, 
Distributed  Environments 

( 1993). 

in:  Proceedings  AAAI  Spring  Symposium  on  Information  Gathering 

a  learning  apprentice 

for  the  World 
from  Heterogeneous 

networks 

[ 71  S.  Baluja  and  D.  Pomerleau,  Dynamic 
(Technical  Note),  Artificial 
functions 
for  WINNOW  and  weighted-majority 

Intelhgence  97  ( 1997)  38 l-395 
[ 81  A.  Blum,  Learning  Boolean 
in  an  infinite  attribute  space,  Muchine  Learning  9  ( 1992)  373-386. 
support 
]9]  A.  Blum,  Empirical 
results  on  a  calendar 
in:  Proceedings  12th  International  Conference  on  Machine  Learning,  Lake  Tahoe, 
domain, 

focus  of  attention  using  artificial  neural 

relevance:  vision-based 

based  algorithms: 

(this  issue). 

scheduling 
CA  (Morgan  Kaufmann,  San  Mateo,  CA,  1995)  64-72. 

] IO]  A.  Blum,  M.  Fur&,  J.  Jackson,  M.  Kearns,  Y.  Mansour 

characterizing 
Symposium  on  Theory  of  Computing,  Montreal,  Que.  (1994)  253-262. 

using  Fourier  analysis, 

statistical 

learning 

query 

and  S.  Rudic,  Weakly 
in:  Proceedings 

learning  DNF  and 
26th  Annual  ACM 

[ 111  A.  Blum,  L.  Hellerstein 
irrelevant  attributes, 

and  N.  Littlestone,  Learning 
J.  Comput.  System  Sci.  50  ( 1995)  32-40. 

in  the  presence  of  finitely  or  infinitely  many 

I 121  A.  Blum  and  R.  Kannan,  Learning 

an  intersection 

of  k  halfspaces 

over  a  uniform  distribution, 

J.  ACM  36  ( 1989)  929-965. 

in: 
Proceedings  34th  Annual  IEEE  Symposium  on  Foundations  of  Computer  Science,  Palo  Alto,  CA  (IEEE, 
1993)  312-320. 
A.  Blumer,  A.  Ehrenfeucht,  D.  Haussler  and  M.K.  Warmuth,  Occam’s 
(1987)  377-380. 
A.  Blumer,  A.  Ehrenfeucht,  D.  Haussler  and  M.K.  Warmuth,  Learnability 
dimension, 
L.  Breiman,  J.H.  Friedman,  R.A.  Olshen  and  C.J.  Stone,  Classification  and  Regression  Trees  (Wadsworth, 
Belmont,  CA  1984). 
N.H.  Bshouty,  Exact  learning  via  the  monotone 
of  Computer  Science,  Palo  Alto,  CA  (IEEE,  1993)  302-3  11. 
trees 
C.  Cardie,  Using  decision 
Conference  on  Machine  Learning,  Amherst,  MA  (Morgan  Kaufmann,  San  Mateo,  CA,  1993)  25-32. 
R.A.  Caruana  and  D.  Freitag,  Greedy  attribute  selection, 
on  Machine  Learning,  New  Brunswick,  NJ  (Morgan  Kaufmann,  San  Mateo,  CA,  1994)  28-36. 

IEEE  Symposium  on  Foundations 

razor,  Inform.  Process.  Lett.  24 

and  the  Vapnik-Chervonenkis 

International  Conference 

to  improve  case-based 

theory,  in:  Proceedings 

10th  International 

in:  Proceedings 

in:  Proceedings 

learning, 

Ilth 

I131 

I141 

1151 

1161 

I171 

1181 

268 

A.L.  Blum,  P  L.angley/Artificial 

Intelligence  97  (1997)  245-271 

I 19 1 R.A.  Caruana  and  D.  Freitag,  How  useful  is  relevance?, 

in:  Working Motes of  the AAAI  Fall  Symposium 

on  Relevance,  New  Orleans,  LA  (AAAI  Press,  1994)  25-29. 

[ 201  J.  Catlett,  Peepholing: 

choosing  attributes  efficiently 

Conference  on  Machine  Learning,  Aberdeen,  Scotland 
54. 

for  megainduction, 

in:  Proceedings  9th btternntionnl 
(Morgan  Kaufmann,  San  Mateo,  CA,  1992)  49- 

1211  N.  Cesa-Bianchi,  Y. Freund,  D.P  Helmbold,  D.  Haussler,  R.E.  Schapire  and  M.K.  Warmuth,  How  to  use 
expert  advice,  Proceedings  Annual  ACM  Symposium  on  the  Theory  of  Computing  (1993)  382-391. 

[ 221  P.  Clark  and  T.  Niblett,  The  CN2  induction  algorithm,  Machine  Learning  3  ( 1989)  261-284. 
[23]  D.A.  Cohn,  Z.  Ghahramani 

learning  with  statistical  models, 

and  M.I.  Jordan,  Active 

J.  Artif  Intell. 

Research  4  (1996)  129-145. 

1241  P. Comon, 
component 
[2S  I  T.M.  Cover  and  PE.  Hart,  Nearest  neighbor  pattern  classification, 

Independent 

analysis:  a  new  concept,  Signal  Process.  36  ( 1994)  287-314. 

IEEE  Trans.  Inform.  Theory  13  ( 1967) 

21-27. 
[ 261  W.  Daelemans, 

S.  Gillis  and  G.  Durieux,  The  acquisition 

of  stress:  a  data-oriented 

approach,  Comput. 

Linguistics  20  (3) 

(1994)  421-451. 

[ 271  PA.  Devijver 

and  J.  Kittler,  Pattern  Recognition:  A  Sfatistical  Approach 

(Prentice-Hall, 

Englewood 

Cliffs,  NJ,  1982). 

[28]  A.  Dhagat  and  L.  Hellerstein,  PAC  learning  with  irrelevant  attributes, 

in:  Proceedings  IEEE  Symposium 

on  Foundations  of  Computer  Science  (IEEE,  1994)  64-74. 

I29  1 J.  Doak,  An  evaluation  of  feature-selection  methods  and  their  application 

to  computer 

security,  Tech. 

Rept.  CSE-92-18,  Department  of  Computer  Science,  University  of  California  at  Davis  ( 1992). 

1301  H.  Drucker,  R.  Schapire 

Improving  performance 
in:  Advances  in  Neural  Information  Processing  Systems,  Vol.  4  (Morgan  Kaufmann, 

in  neural  networks  using  a  boosting 
San 

and  P  Simard, 

algorithm, 
Mateo,  CA,  1992). 

[31]  H.  Drucker,  C.  Cortes,  L.D.  Jackel,  Y.  LcCun  and  V.  Vapnik,  Boosting 

learning 
in:  Proceedings  11th  International  Conference  on  Machine  Learning,  New  Brunswick,  NJ 

and  other  machine 

algorithms, 
(Morgan  Kaufmann,  San  Mateo,  CA,  1994)  53-61. 
[ 32  1 M.  Dyer,  A.  Frieze  and  R.  Kannan,  A  random  polynomial 

the  volume 
in:  Proceedings  Annual  ACM  Symposium  on  the  Theory  of  Computing  (1989)  375- 

for  approximating 

time  algorithm 

of  convex  bodies, 

381. 

[ 331  Y.  Freund,  Boosting 

a  weak 

learning 

algorithm 

by  majority, 

in:  Proceedings  3rd  Annunl  Workshop 

on  Compututional  Learning  Theory,  San  Francisco,  CA  (Morgan  Kaufmann,  San  Mateo,  CA,  1990) 
202-216. 

[ 34  ]  Y. Freund,  An  improved  boosting  algorithm  and  its  implications  on  learning  complexity, 

in:  Proceedings 
5th  Annual  ACM  Workshop  on  Computational  Learning  Theory,  Pittsburgh,  PA  (ACM  Press,  1992) 
391-398. 

1351  M.  Garey  and  D.  Johnson,  Computers  and  Intractability:  A  Guide  to  the  Theory  of  NP-Completeness 

(W.H.  Freeman,  San  Francisco,  CA,  1979). 

1361  Y. Gil,  Efficient  domain-independent 

experimentation, 
Machine  Learning,  Amherst,  MA,  (Morgan  Kaufmann,  San  Mateo,  CA,  1993)  128-134. 

in:  Proceedings  J&h  International  Conference  on 

137  I  R.  Greiner,  A.J.  Grove  and  A.  Kogan,  Knowing  what  doesn’t  matter:  exploiting 

the  omission  of  irrelevant 

data,  Artificial  Intelligence  97  ( 1997)  345-380 

(this  issue). 

[ 38  1 K.P  Gross,  Concept 

acquisition 

through 

attribute 

evolution 

and  experiment 

selection,  Doctoral 

Dissertation,  School  of  Computer  Science,  Carnegie  Mellon  University,  Pittsburgh,  PA  ( 199 1). 

[ 391  D.  Haussler,  Quantifying 

the  inductive  bias  in  concept 

learning, 

in:  Proceedings  AAAI-86,  Philadelphia, 

PA  (AAAI  Press,  1986)  485-489. 

[40]  R.  Hohe,  Very  simple  classification 
Learning  I1  (1993)  63-91. 

rules  perform  well  on  most  commonly 

used  domains,  Machine 

[ 411  J.  Jackson,  An  efficient  membership-query 

algorithm 

for  learning  DNF  with  respect 

to  the  uniform 

distribution, 

in:  Proceedings  IEEE  Symposium  on  Foundations  of  Computer  Science  (IEEE,  1994). 

[ 421  G.H.  John,  R.  Kohavi  and  K.  Pfleger,  Irrelevant 

features  and  the  subset  selection  problem, 

I1 rh  International  Conference  on  Machine  Learning,  New  Brunswick,  NJ  (Morgan  Kaufmann, 
Mateo,  CA,  1994)  121-129. 

in:  Proceedings 
San 

A.L.  Blum,  P  Langley/Artificial  Intelligence  97  (1997)  245-271 

269 

[43]  G.H.  John  and  P  Langley,  Static  VS.  dynamic 

sampling 
Conference  of  Knowledge  Discovery  and  Data  Mining,  Portland,  OR  ( AAAI  Press,  1996)  367-370. 

in:  Proceedings  2nd  International 

for  data  mining, 

1441 LT.  Jolliffe,  Principal  Component  Analysis  (Springer,  New  York,  1986). 
[ 45  I  D.S.  Johnson,  Approximation 

for  combinatorial 

algorithms 

problems, 

J.  Comput.  System  Sci.  9  ( 1974) 

256-278. 
[46]  M.J.  Keams 

and  U.V.  Vazirani,  An  Introduction  to  Computational  Learning  Theory  (MIT  Press, 

Cambridge,  MA,  1994). 

1471 K.  Kira  and  L.  Rendell,  A  practical 

approach 

to  feature  selection, 

Conference  on  Machine  Learning,  Aberdeen,  Scotland 
249-256. 

(Morgan  Kaufmann, 

in:  Proceedings  9th  international 
San  Mateo,  CA,  1992) 

[ 48  ]  J.  Kivinen  and  M.K.  Warmuth,  Additive  versus  exponentiated 

in: 
Proceedings  27th  Annual  ACM  Symposium  on  Theory  of  Computing,  New  York  (ACM  Press,  1995) 
209-2  18. 

for  linear  prediction, 

gradient  updates 

[ 491  J.  Kivinen,  M.K.  Warmuth 

linear  versus 
logarithmic  mistake  bounds  when  few  input  variables  are  relevant  (Technical  Note),  Artificial Intelligence 
97  ( 1997)  325-343  (this  issue). 
[ 50  1 R.  Kohavi,  The  power  of  decision 

tables,  in:  Proceedings  8th European  Conference  on Machine  Learning 

and  P  Auer,  The  Perceptron 

versus  Winnow: 

algorithm 

(1995). 

1511  R.  Kohavi  and  C.H.  John,  Wrappers 
(this  issue). 

273-324 

for  feature 

subset 

selection,  Artificial  Intelligence  97  ( 1997) 

[ 521  R.  Kohavi,  P.  Langley  and  Y.  Yun,  The  utility  of  feature  weighting 

in  nearest-neighbor 

algorithms, 

in: 

Proceedings  9th  European  Conference  on  Machine  Learning,  Prague 

(Springer,  Berlin,  1997). 

[ 531  B.  Knobe  and  K.  Knobe,  A  method  for  inferring  context-free  grammars, 

Inform.  and  Control  3 I  ( 1977) 

129-146. 

1541  D.  Koller  and  M.  Sahami,  Toward  optimal 

feature 

selection, 

in:  Proceedings  13th  International 

Conference  on  Machine  Learning,  Bari,  Italy  (Morgan  Kaufmann,  San  Mateo,  CA,  1996). 

[ 551  1.  Kononenko,  Estimating 

attributes: 

analysis  and  extensions  of  RELIEF,  in:  Proceedings  7th European 

Conference  on  Machine  Learning  ( 1994). 

[ 561  M.  Kubat,  D.  Flotzinger 
of  a  few  methods, 
Berlin,  1993)  367-371. 

and  G.  Pfurtscheller,  Discovering 

study 
in:  Proceedings  1993  European  Conference  on  Machine  Learning,  Vienna  (Springer, 

in  EEG  signals:  comparative 

patterns 

[ 57  1 D.  Kulkami  and  H.A.  Simon,  Experimentation 

in:  J.  Shrager  and  P  Langley,  eds., 
Computational  Models  of  Scient$c  Discovery  and  Theory  Formation  (Morgan  Kaufmann,  San  Mateo, 
CA,  1990). 

in  machine  discovery, 

[ 581  P. Langley  and  W.  Iba,  Average-case 
(1993)  889-894. 

Chambery,  France 

analysis  of  a  nearest  neighbor  algorithm, 

in:  Proceedings  IJCAI-93, 

I59  ]  P.  Langley  and  S.  Sage,  Oblivious  decision 

trees  and  abstract  cases, 

in:  Working Notes  af  the  AAAI-94 

Workshop on  Case-Based  Reasoning,  Seattle,  WA  (AAAI  Press,  1994)  113-l  17. 

( 601  P. Langley  and  S.  Sage,  Induction  of  selective  Bayesian  classifiers, 

in:  Proceedings  10th  Conference  on 

Uncertainty  in Artificial Intelligence,  Seattle,  WA  (Morgan  Kaufmann,  San  Mateo,  CA,  1994)  399-406. 

161  1 F’. Langley 

and  S.  Sage,  Scaling 

ed., 
Computational  Learning  Theory  and  Natural  Learning  Systems,  Vol.  4  (MIT  Press,  Cambridge,  MA, 
1997). 

to  domains  with  many 

in:  R.  Greiner, 

irrelevant 

features, 

1621  D.D.  Lewis,  Representation 

and  learning 

in  information 

retrieval,  Doctoral  Dissertation,  Tech.  Rept.  UM- 

CS-1991-093,  Department  of  Computer  Science,  University  of  Massachusetts,  Amherst,  MA  ( 1992). 

1631  D.D.  Lewis,  Feature  selection  and  feature  extraction 

for  text  categorization, 

in:  Proceedings  Speech  and 

Natural  Language  Workshop,  San  Francisco 
[64]  D.D.  Lewis  and  J.  Catlett,  Heterogeneous 

(Morgan  Kaufmann,  San  Mateo,  CA,  1992)  2 12-2  17. 
uncertainty 

sampling, 

in:  Proceedings  Zlth  International 
San  Mateo,  CA,  1994) 

Conference  on  Machine  Learning,  New  Brunswick,  NJ  (Morgan  Kaufmann, 
148-156. 

I65  1 L.J.  Lin,  Self-improving 

reactive  agents  based  on  reinforcement 

learning,  planning  and  teaching,  Machine 

Learning  8  ( 1992)  293-321. 

I66  1 N.  Limestone,  Learning  quickly  when 

irrelevant  attributes  abound:  a  new 

linear 

threshold 

algorithm, 

Machine  Learning  2  (1988)  285-318. 

270 

A.L.  Blum,  P  Langley/Artificial  Intelligence  97  (1997)  245-271 

[ 671  N.  Littlestone,  PM.  Long  and  M.K.  Warmuth, On-line  learning  of  linear  functions,  in: Proceedings  23rd 
Annual  ACM  Symposium  on  Theory  of  Computing,  New  Orleans,  LA  (ACM  Press,  1991)  465-475. 
[68]  N.  Littlestone  and  C.  Mesterharm,  An  apobayesian  relative  of  WINNOW, in:  M.C.  Mozer,  M.I.  Jordan 
and T. Petsche,  eds.,  Advances  in Neural  Information  Processing  Systems,  Vol. 9  (MIT  Press,  Cambridge, 
MA,  1997). 

[69]  N.  Littlestone  and  M.K.  Warmuth,  The  weighted  majority  algorithm,  Inform.  and  Comput.  108  ( 1994) 

212-261. 

[ 701  L.  Lovasz  and  M.  Simonovits,  On  the  randomized  complexity  of  volume  and  diameter,  in:  Proceedings 

IEEE  Symposium  on  Foundations  of  Computer  Science  (IEEE,  1992) 482-49 1. 

[ 7 1 ]  C.  Lund  and  M.  Yannakakis, On  the  hardness  of  approximating  minimization  problems,  in: Proceedings 

Annual  ACM  Symposium  on  the  Theory  of  Computing  (1993)  286-293). 

[72]  C.J.  Matheus  and  L.A.  Rendell,  Constructive  induction  on  decision  trees,  in:  Proceedings  IJCAI-89, 

Detroit,  MI  (Morgan  Kaufmann,  San  Mateo,  CA,  1989)  645-650. 

[73]  R.S.  Michalski,  Pattern  recognition  as  rule-guided  inductive  inference,  IEEE  Trans.  Pattern  Anal. 

Machine  Intell.  2  ( 1980)  349-361. 

[74]  M.  Minsky  and  S.  Papert,  Perceptrons:  An  Introduction  to  Computational  Geometry  (MIT  Press, 

Cambridge,  MA,  1969). 

[75]  T.M.  Mitchell,  Generalization  as  search,  Arf@ial 

reprinted  in:  J.W. 
Shavlik  and  T.G. Dietterich,  eds.,  Readings  in  Machine  Learning  (Morgan  Kaufmann,  San Mateo,  CA, 
1990). 

Intelligence  18  ( 1982)  203-226; 

[ 761  A.W.  Moore  and  M.S.  Lee,  Efficient  algorithms  for  minimizing  cross  validation  error,  in:  Proceedings 
Ilfh  International  Conference  on  Machine  Learning,  New  Brunswick,  NJ  (Morgan  Kaufmann,  San 
Mateo,  CA,  1994)  190-198. 

[ 771  S.W.  Norton,  Generating  better  decision 

trees, 

in:  Proceedings 

IJCAI-89,  Detroit,  MI  (Morgan 

Kaufmann,  San  Mateo,  CA,  1989)  800-805. 

[78]  M.J.  Pazzani  and  W.  Sarrett,  A  framework  for  the  average  case  analysis  of  conjunctive  learning 

algorithms,  Machine  Learning  9  (1992)  349-372. 

[ 791  G.  Pagallo and D. Haussler,  Boolean  feature discovery  in empirical  learning, Machine  Learning  5  ( 1990) 

71-99. 

[ 801  J.R.  Quinlan,  Learning  efficient  classification  procedures  and  their  application  to  chess  end  games,  in: 
R.S.  Michalski,  J.G.  Carbonell  and  T.M.  Mitchell,  eds.,  Machine  Learning:  An  Artificial  Intelligence 
Approach  (Morgan  Kaufmann,  San  Mateo,  CA,  1983). 

[ 8 I]  J.R.  Quinlan,  C4.5:  Programs  for  Machine  Learning  (Morgan  Kaufmann,  San  Mateo,  CA,  1993). 
[82]  S.  Rajamoney,  A  computational  approach  to  theory  revision,  in:  J.  Shrager  and  P  Langley,  eds., 
Computational  Models  of  Scientific  Discovery  and  Theory  Formation  (Morgan  Kaufmann,  San  Mateo, 
CA,  1990). 

[ 83 ]  R.L. Rivest  and R.E.  Schapire,  Inference  of finite automata using homing  sequences,  Inform.  and  Comput. 

103  (1993)  299-347. 

[ 841  D.E.  Rumelhart,  G. Hinton  and R.J.  Williams,  Learning  internal  representations  by error  propagation,  in: 
D.E. Rumelhart  and J.L. McClelland,  and the  PDP Research  Group, eds., Parallel Distributed Processing: 
Explorations  in  the  Microstructure  of  Cognition,  Vol.  1 (MIT  Press,  Cambridge,  MA,  1986). 

1851 C.  Sammut  and  R.B.  Banerji,  Learning  concepts  by  asking  questions,  in:  R.S. Michalski,  J.G.  Carbonell 
and  T.M.  Mitchell,  eds.,  Machine  Learning:  An  Artificial  Intelligence  Approach,  Vol.  2  (Morgan 
Kaufmann,  San  Mateo,  CA,  1986). 

[ 861 R.E.  Schapire,  The  strength  of  weak  learnability,  Machine  Learning  5  ( 1990)  197-227. 
[ 871 J.C.  Schlimmer,  Efficiently  inducing  determinations:  a complete  and efficient  search  algorithm  that  uses 
optimal  pruning,  in:  Proceedings  10th  International  Conference  on  Machine  Learning,  Amherst,  MA 
(Morgan  Kaufmann,  San  Mateo,  CA,  1993)  284-290. 

[ 881  PD.  Scott  and S. Markovitz,  Representation  generation  in an exploratory  learning  system,  in: D.H. Fisher, 
in  Unsupervised 

M.J.  Pazzani  and  P  Langley,  eds.,  Concept  Formation:  Knowledge  and  Experience 
Learning  (Morgan  Kaufmann,  San  Mateo,  CA,  1991). 

[ 89 ] H.S.  Seung,  M. Opper  and  H. Sompolinsky,  Query  by  committee,  Proceedings  5th Annual  Workshop on 

Computational  Learning  Theory,  Pittsburgh,  PA  (ACM  Press,  New  York, 1992)  287-294. 

A.L.  Blum.  P: Langley/Artificial  Intelligence  97  (1997)  245-271 

271 

1901  W.M.  Shen  and  H.A.  Simon,  Rule  creation  and  rule 

learning 

through  environmental 

exploration, 

in: 

Proceedings  IJCAI-89,  Detroit,  Ml  (Morgan  Kaufmann,  San  Mateo,  CA,  1989)  675-680. 

[911  A.  Sinclair 

and  M.  Jerrum,  Approximate 

counting, 

uniform  generation 

and  rapidly  mixing  Markov 

chains,  Inform.  and  Comput.  82  (1989)  93-133. 

1921  M.  Singh  and  G.M.  Provan,  A  comparison 

for  selective  and  non-selective 
in:  Proceedings  12th  Infernational  Conference  on  Machine  Learning,  lake  Tahoe, 

of  induction 

algorithms 

Bayesian  classifiers, 
CA  (Morgan  Kaufmann,  San  Mateo,  CA,  1995)  497-505. 

[93  ]  M.  Singh  and  G.M.  Provan,  Efficient 

in:  Proceedings 
13th  International  Conference  an  Machine  Learning,  Bari,  Italy  (Morgan  Kaufmann,  San  Mateo,  CA, 
1996). 

learning  of  selective  Bayesian  network  classifiers, 

1941  D.B.  Skalak,  Prototype  and  feature  selection  by  sampling  and  random  mutation  hill-climbing 

algorithms, 
in:  Proceedings  11th  International  Conference  on  Machine  Learning,  New  Brunswick,  NJ  (Morgan 
Kaufmann,  San  Mateo,  CA,  1994)  293-301. 

[95]  C.W.  Stanfill,  Memory-based 

reasoning 

applied 

to  English  pronunciation, 

in:  Proceedings  AAAI-87, 

Seattle,  WA  (AAAI  Press,  1987)  577-581. 

[ 961  K.M.  Ting,  Discretization 

of  continuous-valued 

attributes  and  instance-based 

learning,  Tech.  Rept.  No. 

49  1,  Basser  Department  of  Computer  Science,  University  of  Sydney 
and  D.  Kibler,  Instance-based 

[ 97  1 T.  Townsend-Weber 

( 1994). 

prediction  of  continuous  values, 

in:  Working Notes  of 

the  AAAI-94  Workshop on  Case-Based  Reasoning,  Seattle,  WA  (AAAI  Press,  1994)  30-35. 

[ 98  1 K.  Verbeurgt,  Learning  DNF  under 

the  uniform  distribution 

in  polynomial 

Annual  Workshop  on  Computational  Learning  Theory,  San  Francisco,  CA 
Mateo,  CA,  1990)  314-325. 
1991  S.A.  Vere,  Induction  of  concepts 

in  the  predicate  calculus, 

time, 
(Morgan  Kaufmann, 

in:  Proceedings  3rd 
San 

in:  Proceedings  IJCAI-75,  Tbilisi,  Georgia 

(Morgan  Kaufmann,  San  Mateo,  CA,  1975)  281-287. 

1 1001  V.  Vovk,  Aggregating 

strategies, 

in:  Proceedings  3rd  Annual  Workshop  on  Computational  Learning 

Theory,  San  Francisco,  CA  (Morgan  Kaufmann,  San  Mateo,  CA,  1990)  371-383. 

[ 1011  B.  Widrow  and  M.E.  Hoff,  Adaptive 

switching  circuits, 

in:  IRE  WESCON  Convention  Record  ( 1960) 

96-104. 

1 1021  PH.  Winston,  Learning 

structural  descriptions 

from  examples, 

in:  P.H.  Winston,  ed.,  The  Psychology 

of  Computer  Vision  (McGraw-Hill,  New  York,  1975). 

