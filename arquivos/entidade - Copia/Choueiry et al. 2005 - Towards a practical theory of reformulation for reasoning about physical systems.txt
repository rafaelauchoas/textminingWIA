Artiﬁcial Intelligence 162 (2005) 145–204

www.elsevier.com/locate/artint

Towards a practical theory of reformulation
for reasoning about physical systems

Berthe Y. Choueiry a,∗, Yumi Iwasaki b, Sheila McIlraith c

a Constraint Systems Laboratory, Department of Computer Science and Engineering,
University of Nebraska-Lincoln, Lincoln, NE 68588-0115, USA
b Woodinville, WA, USA
c Department of Computer Science, University of Toronto, Toronto, Ontario, Canada M55 3H5

Received 20 November 2001; accepted 14 November 2002

Available online 15 December 2004

The authors would like to dedicate this article to the memory of Robert S. Engelmore. He was a long-time friend
and senior colleague of the authors. He was also an important contributor to the initial discussion group at KSL
that eventually lead us to write this article. We dearly miss him and regret that he did not live to read this product
of the many discussions we had with him.

Abstract

Reformulation is ubiquitous in problem solving and is especially common in modeling physical
systems. In this paper we examine reformulation techniques in the context of reasoning about phys-
ical systems. This paper does not present a general theory of reformulation, but it studies a number
of known reformulation techniques to achieve a broad understanding of the space of available refor-
mulations. In doing so, we present a practical framework for specifying, classifying, and evaluating
various reformulation techniques applicable to this class of problems. Our framework provides the
terminology to specify the conditions under which a particular reformulation technique is applica-
ble, the cost associated with performing the reformulation, and the effects of the reformulation with
respect to the problem encoding.
 2004 Elsevier B.V. All rights reserved.

Keywords: Abstraction; Reformulation; Approximation; Reasoning about physical systems

* Corresponding author.

E-mail addresses: choueiry@cse.unl.edu (B.Y. Choueiry), Yumi_iwasaki@hotmail.com (Y. Iwasaki),

sheila@cs.toronto.edu (S. McIlraith).

0004-3702/$ – see front matter  2004 Elsevier B.V. All rights reserved.
doi:10.1016/j.artint.2004.01.004

146

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

1. Introduction

Reformulation plays an important role in various intellectual activities and is ubiqui-
tous in reasoning about physical systems. Reformulation improves the effectiveness of a
problem-solving process by recasting a problem into a new one that is tailored to a given
task. There are a number of reformulation techniques and the selection of reformulation
techniques must be carried out in the context of a problem-solving task.

In this paper we examine the role of reformulation in reasoning about physical systems.
To achieve a broad understanding of the space of available reformulations, we study and
discuss a number of known reformulation techniques. All the methods discussed in this
paper have been presented in the literature, and some are in common use in science and
engineering.

Our investigations yield two important contributions. First, they provide the ﬁrst prac-
tical framework for a comprehensive description of reformulation techniques. This frame-
work, based on our analysis of the ﬁeld, allows us to characterize reformulation techniques
including their applicability conditions and effects along with the assumptions and implicit
evaluation criteria underlying them. Such a framework is necessary for the future develop-
ment of an automatic mechanism to select a reformulation technique that is appropriate for
a task. Second, our study produces a survey of reformulation techniques in reasoning about
physical systems. Our framework was essential for undertaking this comparative analysis.
Indeed, without a uniform framework from which to characterize reformulation techniques
as diverse as we have considered, a meaningful comparison would have been impossible.
Informally, we deﬁne reformulation to be a transformation from one encoding of a
problem to another, given a particular problem-solving task. A problem-solving task is
accomplished by the application of a select sequence of reformulations to an initial problem
encoding to produce a ﬁnal encoding that directly addresses the task. We use the term
reformulation to subsume the notions of abstraction and approximation, while avoiding
the implication that such a transformation necessarily generalizes or simpliﬁes the domain
theory.

Given a reasoning problem, one may choose to reformulate for any of the following

reasons:

1. Engine-driven problem re-encoding: There may not be a method to solve the given
problem as is. In such a case, one may choose to reformulate the original problem
by approximating it by another problem (or a set of problems) for which a solution
method is known. For example, if the original problem requires one to solve a set of
nonlinear differential equations, engineers often substitute the original problem with a
set of linear differential equations that approximate the nonlinear equations.

2. Performance-driven problem re-encoding: The given problem may be too expensive to
solve by an available method, forcing one to reformulate into a similar problem that is
easier to solve. For example, if a given set of linear differential equation is too large to
solve with available computational resources, one may choose to aggregate to produce
a less detailed but smaller model.

3. Supporting cognitive insight: One may choose to reformulate in order to gain cognitive
insight into the problem or solution space. Mapping from Cartesian coordinates to

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

147

polar coordinates, mapping a time domain to a frequency domain, reformulating the
equations of an electric circuit in terms of the voltage and current into ones in terms of
power are all examples of reformulation that are sometimes performed to make certain
aspects of the system behavior easier to detect and/or analyze.

Note that a given reformulation can serve multiple purposes. A reformulation method
applied to reduce the cost of solution may also improve one’s understanding of the overall
behavior of a complex problem.

For the purposes of this paper, we have elected to focus on a speciﬁc class of prob-
lems, in an effort to develop a framework for characterizing reformulation methods with
sufﬁcient detail to be useful to practitioners and researchers alike. In particular, we exam-
ine reformulation techniques that can be exploited to reason about physical systems. The
long-term goal of our research is to develop an automatic task-driven capability that se-
lects and applies appropriate reformulation techniques in the course of problem solving.
An essential tool towards this ultimate goal is a practical framework for characterizing
and evaluating the merits and relevance of various reformulation techniques. This paper
proposes such a framework with respect to the restricted class of problems we described
above. The motivation for developing such a framework came from the observation that
much of the previous work on reformulation (including abstraction and approximation)
was not sufﬁcient for our purpose. It was either too speciﬁc to account for the large va-
riety of reformulation methods commonly used in reasoning about physical systems, or
too general for characterizing them in sufﬁcient details to allow informed selection among
them. The framework we present provides a signiﬁcant step towards our long-term goal
by deﬁning general criteria for understanding the properties and assessing the merits of
various reformulation procedures. More speciﬁcally, the framework provides the means to
identify the conditions under which a reformulation is applicable, the cost associated with
performing the reformulation, and the effects of the reformulation both with respect to the
problem encoding and with respect to the accuracy and cost of reasoning.

The paper is organized as follows. Section 2 discusses the motivations behind our en-
deavor and its scope. Section 3 describes the processing stages of reasoning about physical
systems to deﬁne the context for the types of reformulation we are interested in. The frame-
work for characterizing reformulation techniques is introduced in Section 4, where we
provide a detailed description of the framework itself and present a set of evaluators for
assessing the effects of reformulation. Section 5 uses the framework to actually analyze
a number of known reformulation techniques. Section 6 discusses related work. Finally,
Section 7 summarizes our contributions and outline directions for future research.

2. Motivation and scope

In this section, we ﬁrst state the motivations of this paper, then we justify our reasons for
using the term reformulation in an effort to clear up the ambiguity of previous terminology.

148

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

2.1. Motivation

Reformulation is ubiquitous in reasoning about complex systems. Analyzing the be-
havior of a complex electro-mechanical device in full detail, taking into account all its
electrical, mechanical, and thermal aspects is too costly and unnecessary for most purposes.
Engineers and scientists are accustomed to reformulating repeatedly the mathematical
model of a complex physical system in order to reduce the complexity of analysis, to
facilitate the explanation of the problem or a solution to others, or often, even to be able to
analyze the system at all. There are numerous known techniques for reformulation that are
used by engineers and scientists. AI literature also abounds with such techniques. A library
of reformulation techniques and an ability to select from it the most appropriate technique
for a given problem would greatly enhance the power of a program for reasoning about
physical systems.

To compile such a library or to enable informed selection from it, we need a systematic
way to compare and evaluate reformulation techniques, which the ﬁeld currently lacks.
This lack may be due to the fact that many commonly used reformulation techniques
are informally executed tools in engineers’ ‘bag of tricks’. A more signiﬁcant problem
in establishing a systematic classiﬁcation of reformulation techniques is their inherent di-
versity; they apply to different forms of models, at different stages of problem solving, and
to achieve different goals. We need, as a ﬁrst step, to articulate a vocabulary of attributes
to describe various key aspects of reformulation techniques. This vocabulary needs to be
informative enough to enable selection of appropriate techniques for different goals at dif-
ferent stages of problem solving.

The understanding and classiﬁcation of reformulations have been relatively well-
addressed in the literature. While some researchers have focused on speciﬁc domains such
as theorem proving [25], planning [13,34], or model-based reasoning [33,39], others have
tried to construct general theories in order to formalize the general properties of reformu-
lations [4,10,22]. We discuss these contributions in Section 6. While these previous works
mentioned above explored new grounds and established key properties of certain types of
reformulation techniques, we have found them to be too general and insufﬁcient to char-
acterize many of the techniques common in reasoning about physical systems. They are
typically concerned with only one aspect of the reformulation: its effects on the solution,
the models, the proof trees, the theorems, or the computational cost. They also do not apply
to models in the form of mathematical equations, which limits their usefulness in reasoning
about physical systems.

In surveying various abstraction and approximation techniques in kinematics theory
of rigid solid objects (KRSO), Davis also observes that abstraction and approximation
in KRSO do not ﬁt any one elegant metatheoretic structures such as those proposed by
Giunchiglia and Walsh, Nayak and Levy, or Weld. He concludes by stating that he does not
see any “overarching metalogical structure that sheds useful light on the relation between
these approximation techniques” and conjectures that “none will be found in many domains
of physical reasoning” [5]. While we agree with Davis’s conclusion that there is no clean
meta-theory of reformulation in reasoning about physical systems, we do believe that it
is possible to provide a framework that enables a systematic description of many aspects

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

149

of reformulation techniques that are critical in their selection. We also believe that such a
framework is a pre-requisite for compiling a library of reformulation techniques.

This paper is our attempt to formulate such a framework for systematic description of
reformulation techniques to enable their comparison and evaluation with respect to given
problems. Such a framework will not take the shape of an overarching metatheory of re-
formulation. Instead, the goal is to characterize reformulation techniques in such way that
one can select a technique that is appropriate for a given problem. We do not offer a formal
theory of reformulation with deﬁnitions and theorems. Instead, we discuss what aspects of
reformulation techniques and the context in which they are used must be made explicit if
we were to be able to compare them and make an intelligent choice among them in the
course of reasoning about physical systems. The goal is more modest than a construction
of a “grand-uniﬁed” theory of reformulation, but hopefully more realistic and, ultimately,
more useful. This paper proposes a structure for presenting a reformulation technique with
a view towards compilation of a library of a broad range of reformulation techniques and
enabling intelligent selection. Furthermore, we identify the dimensions of a problem that
are relevant for the selection of the reformulation and provide the terminology to describe
the effects of its application to the problem. In presenting this framework, we also hope
to shed new light on the confusion surrounding reformulation and to serve a pedagogical
purpose by explicating a number of reformulation techniques from a uniﬁed perspective.

2.2. Scope

This paper focuses on reformulation techniques in the context of reasoning about the
behavior of physical systems. We restrict ourselves to problems where the behavior of the
physical system in question is expressible as a set of lumped-parameter continuous models,
containing algebraic or differential equations, though reasoning need not be limited to
direct manipulation of equations. We require that the task be motivated by a speciﬁc query,
thus constraining the necessary computational machinery to solve the task. Finally, we
consider only reformulations of a speciﬁc problem instance as opposed to the construction
of gross properties of a population of instances.1

In Section 1, we have informally deﬁned reformulation as “transformation from one en-
coding of a problem to another”. Review of the literature on reformulation reveals that in
addition to transformation, two other general approaches to reformulation exist, namely se-
lection and formulating anew. When an existing encoding proves lacking in some respect,
instead of transforming the original, one could formulate a new encoding from scratch or
select another formulation from a set of existing alternatives. In general, since there must
be a way to use the knowledge of an encoding’s shortcomings to guide the process of cre-
ating or selecting an alternative, these three approaches do not represent totally orthogonal
dimensions in reformulation. Rather, they represent different points in the whole spectrum
of formulation techniques and are likely to share many technical issues.

1 The entire ﬁeld of statistics is devoted to the problem of producing and using higher-level descriptions of a
large population of instances. Though Amador and Weld discuss this kind of an aggregation method as a type of
reformulation [2], we will not consider such methods in this paper.

150

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

The apparent differences among transformation, selection, and new formulation tend to
be more a matter of focus of the work. Reformulation through selection presumes that there
exists a set of alternative formulations to begin with. The works that focus on selection
generally do not address the problem of generating the alternatives [1,36]. Techniques that
formulate anew focus on the problem of constructing a model in the ﬁrst place. While
transformation presumes that there exists at least one formulation to start with, the process
of altering an existing encoding can require much of the same type of knowledge needed for
constructing a new one. Both selection and transformation could be part of the process of a
new formulation: In some approaches, the construction of a new model involves generation
of possible candidates models followed by the selection among them [6]. In others, the
generation of an initial model is followed by the transformation if the initial model is
found to be optimizable with respect to some criteria [20,21].

While we recognize that selection and formulating anew are viable approaches to re-
formulation, we focus, in this paper, on reformulation as a transformation. In practice,
reformulation by transformation is an important, if not the most important, category of
reformulation techniques, as most of the abstraction and approximation techniques com-
monly used in engineering fall in this category. Reformulation via transformation is fre-
quently the only alternative in practice. In the real world, one often does not have the
luxury of multiple existing models or the necessary resources to formulate a new model
from scratch.

2.3. On the terminology

In the previous attempts to capture the nature of reformulation and its effects, the words
‘abstraction’ and ‘approximation’ have commonly been used to refer to various instances
of reformulation. Based on our observation that the words abstraction and approximation
are not appropriate to designate all reformulation procedures that are of interest, we choose
to use the term ‘reformulation’ as a general term that subsumes abstraction and approxi-
mation.

Abstraction and approximation.
It is rather difﬁcult to clearly distinguish between ab-
straction and approximation, and the notions of generalization and simpliﬁcation that they
imply. Both ‘abstraction’ and ‘approximation’ imply some reduction in the complexity
of the representation and of the reasoning process. In other words, they are supposed to
‘simplify’ the problem in some way. ‘Abstraction’ is often used to indicate some structural
modiﬁcations of a representation, whereas approximation refers to differences in numerical
values or other mathematical quantities [5]. Abstraction is thought to ‘simplify’ a descrip-
tion by dropping certain information, retaining only general characteristics, at the expense
of introducing some ambiguity. ‘Approximation’, in contrast, implies a replacement of a
detailed, precise and accurate model2 with one that is less accurate but is easier to compute
with. In both cases, the difﬁculty in producing a precise yet general deﬁnition of the term

2 In logic, model is used to refer to an interpretation of a theory. This is not the meaning we intend in this
document. By model we mean a collection of statements that are a (possibly non-mathematical) representation of
a given physical situation useful for automated analysis.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

151

lies in deﬁning the meaning of ‘simpler’. What is simpler almost always depends on what
aspect one is focusing on. For example, consider the case of a periodic square function. It
is sometimes used to approximate a sine function of the same periodicity, since the former
ranges over only two values instead of the continuum of values for the latter. However, the
square wave is in fact much richer and more complex in terms of harmonics than the sine
wave and, in this sense, the sine is an approximation of the square function.

Simpliﬁcation. Most work on abstraction and approximation techniques does not make
explicit the precise measure of simplicity used. Even in those cases where the notion of
simplicity is precisely deﬁned, the deﬁnitions are too narrow to account for the various
ways a reformulated problem differs from the original one. Rickel and Porter [28,29] deﬁne
simplicity in terms of the number of variables involved in the mathematical equations of a
model: the fewer variables a model has, the simpler it is. Nayak’s deﬁnition relies on two
things; subset relations among the causal relations implied by model fragments and the
approximation relations among model fragments [20]. Neither of these deﬁnitions allows
one, for example, to compare two mathematical models involving the same set of variables,
one being a nonlinear model and the other a linearized approximation of the ﬁrst.

Generalization. Another important change in a model that is brought about by abstrac-
tion or approximation, though it is often related to the notion of simplicity, is generality,
which is the range of phenomena captured by a model. Comparing the classical dynamics
and relativistic dynamics will help illustrate the subtlety of the notion of simplicity and its
relation to that of generality. The classical dynamics is a good approximation of the rela-
tivistic dynamics but only when the speeds of objects do not approach the speed of light
(i.e., when v/c (cid:2) 1). In other words, the classical dynamics is less general than the other.

These difﬁculties in deﬁning precisely what is abstraction or approximation have led us
to use instead the term ‘reformulation’. Rather than classifying examples of reformulation
as abstractions, generalizations, or approximations, we need to understand fully and artic-
ulate clearly what happens when a model is transformed by making explicit the effects of
reformulation on speciﬁc aspects such as the representation, the computational efﬁciency,
cognitive transparency and the result.

3. Reasoning about physical systems

Before discussing various reformulation procedures, we must set the stage by describing
the problem-solving context in which we wish to evaluate such procedures. As we stated
earlier, we concentrate on the types of physical systems that can be described with lumped-
parameter hybrid models containing algebraic and ordinary differential equations. In this
section, we describe the various processes that are executed in reasoning about physical
systems, and we illustrate these processes in terms of several examples drawn from the
literature. The purpose of this section is to explicate the context in which we wish to employ
and evaluate reformulation techniques. Since one of the theses of this article is that such
an evaluation cannot be made in a vacuum, it is imperative that we make the context as
explicit as possible.

152

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

3.1. Stages of modeling physical systems

Starting with a description of the task of interest, we perceive the entire endeavor of
reasoning about physical systems as a progression through the three processing stages
illustrated in Fig. 1, namely: the model, the equation, and the solution processing stages.

Task description. Reasoning about physical systems begins with a task description, which
describes the problem and establishes the problem solving goal. It consists of
the following four elements: the domain theory, the scenario, the query, and the
modeling assumptions.
The domain theory is a corpus of knowledge about the physical world. It con-
tains heterogeneous, possibly redundant, descriptions of the physical structure
and phenomena of interest, including logical statements, symbolic equations, and
numerical parameters. In the work on compositional modeling [6], the domain
theory is represented in the form of a library of model fragments.
In general, any proﬁtable computational effort spent on reasoning about a domain
theory must be motivated by a speciﬁc task, where a task can vary from prediction,
to explanation, to veriﬁcation, under a wide spectrum of hypothetical situations
and conditions. We deﬁne a task by a combination of a scenario, a query, a domain
theory, and a set of modeling assumptions.

Fig. 1. Reasoning about physical systems. Stages of reasoning and their corresponding processes.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

153

The scenario is a description of a particular problem instance (e.g., a set of system
components and their physical structure, and the initial conditions of the system).
The query is an explicit speciﬁcation of the user’s interests in terms of the vari-
ables, and their aspects of interest (e.g., the quantitative or qualitative values,
direction of change at speciﬁc time points, temporal evolution).
The modeling assumptions include assumptions that the problem solver makes in
order to broadly delimit the scope of the answer to the query (e.g., the temporal
and physical extent of its coverage, granularity). See [15] for a more detailed
discussion of modeling assumptions.

Model processing. Given a task description, the model building process assembles the rel-
evant aspects of the domain theory to produce a model, which is an instantiation of
a subset of the domain theory that is both consistent and sufﬁcient to address the
query. A model at this point often consists of knowledge of the physical structure
(components and their topology, for example) as well as knowledge of the rele-
vant physical phenomena (including the conditions under which they are active),
in contrast to the purely mathematical model of the following stage. Examples of
model building process include compositional modeling as in [6,15,20] and the
modeling algorithm of TRIPEL [28].
This process can be followed by a model reformulation process. Model reformu-
lation may involve structural consolidation [40], simpliﬁcation [21] or expansion
of a model through aggregation, replacement, deletion or addition of a description
of components or phenomena. Speciﬁc examples of reformulation carried out at
this stage are presented in Section 5.1.

Equation processing. Equation building produces an equation model either directly from
a task description or by extracting mathematical equations from a model describ-
ing the behavior of the system. For example, the Qualitative Physics Compiler [7]
converts a model expressed in QP Theory [8] into a set of qualitative differential
equations. Once equations are obtained, an equation reformulation process may
be carried out. An equation reformulation is often motivated by a desire to trans-
form the present equation model into a form that is easier to compute with or that
is amenable to a particular problem-solving engine.
We distinguish between non-equational models and equational ones because there
is a large class of mathematical techniques that operate solely on equations.
In fact, most well-known simpliﬁcation techniques operate on equations. They
include such techniques as dropping insigniﬁcant terms, linearizing non-linear
equations, aggregating nearly decomposable systems, and reformulating ordinary
differential equations as qualitative differential equations or qualitative differen-
tial equations as causal orderings. Some of these techniques are discussed in detail
in Section 5.2.

Solutions processing. Equations are given to a solver to produce one or more solutions.
Some examples of a solver are QSIM [14] and Matlab®. Once the equations are
solved, a solution reformulation process may subsequently be performed in or-
der to make the solution easier to comprehend or to facilitate further reasoning.
Examples of such reformulations include categorizing solutions, summarizing a
single solution, summarizing a set of solutions, and explanation generation [11].

154

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

As the problem becomes larger and more complex, there is an increasing need to
facilitate understanding of a solution. In Section 5.3, we discuss two such tech-
niques.
An interesting use of solution reformulation for the purpose of speeding up the
overall problem-solving process is demonstrated by Clancy and Kuipers [3].
Clancy and Kuipers interleave QSIM simulation with the aggregation of partial
solutions corresponding to chatter in a qualitative simulation. In so doing, they
signiﬁcantly improve the overall performance of QSIM on some problems.

We note that reasoning need not necessarily proceed through every stage deﬁned above.
It is common for a problem solver to omit a stage or to loop over stages. For instance, one
may go from a task description directly to the Equation Processing stage, or from the Model
Processing stage directly to the Solution Processing stage. QPE [9] is such an example
because it produces a solution directly from a model without explicitly generating a set of
equations. A problem solver may also loop through one or more of the stages. Ling and
Steinberg in [16] describe a three-step procedure for automated modeling that encompasses
model generation, simulation, and validation while looping over each of these steps (inner
loop) and over the whole process (outer loop). In the example of solution reformulation
mentioned above and discussed in Section 5.3.2, Clancy and Kuipers [3] loop over the
solution building and solution reformulation processes. Another example of such loops can
be found in the work on DME [17] for modeling device behavior. DME cycles through the
stages of Equation Processing and Solution Processing during simulation as the operating
conditions change and the model used for simulation must be updated.

Earlier, we have informally deﬁned reformulation to be a transformation from one en-
coding of a problem to another. Having described the stages of modeling physical systems
in which to study reformulation, we are now ready to deﬁne reformulation more narrowly
in this context. In the process of reasoning about physical systems as we have just outlined,
what we regard as reformulation is a transformation of an encoding at one stage to another
encoding at the same stage. Thus, such reformulation does not change the type of formal-
ism of the encoding but does change the content in such a way that the change is not a mere
consequence of making explicit what is implicit in the original.3 Fig. 1 reﬂects this narrow
deﬁnition of reformulation as each processing stage consists of a building step followed by
a reformulation step. We believe that restricting the context and narrowing our ﬁeld enables
us to lay out a more concrete and practical framework for informed comparison of refor-
mulation techniques which are relevant in reasoning about physical systems than would be
possible otherwise if we adopt a more general deﬁnition of reformulation. In the remainder
of this paper, we will refer to mechanisms that carry out a building step in any processing
stage of Fig. 1 as “solution engines” to distinguish them from reformulation engines. We

3 If we regard each encoding as a logical theory or as a model in the model theoretic sense, then the two
encodings would be two distinct and logically incompatible theories or models. It is open to discussion whether
such “models” of physical systems as those we studied in this paper should be interpreted as theories or models
in the model theoretic sense. However, such a distinction is not relevant to those of us actually trying to model a
physical system.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

155

use the term ‘engine’ broadly to include anything from an algorithm, to a special-purpose
simulation program, to a general-purpose solution package such as Mathematica®.

3.2. Characterizing reformulation

There are three general aspects in which we will characterize a reformulation technique.
First, we must specify the types of problems to which the technique is applicable. Second,
we must characterize the impact of the reformulation technique in terms of what aspects
of a problem it changes and how. Third, we must characterize the reformulation as a com-
putational process. The ﬁrst and second aspects are rather deﬁnitional as they describe the
applicability of the procedure and its raison d’être. The third aspect is more descriptive in
the sense that it represents a set of ways to evaluate the procedure as a computational tool.
We now brieﬂy introduce three examples of reformulation techniques that are discussed
in more detail in Section 5. The three examples are model simpliﬁcation by Nayak and
Joskowicz, aggregation of nearly decomposable systems by Simon and Ando, and behavior
abstraction for explanation by Mallory. We will then highlight the key features of these
techniques in order to motivate the framework presented in Section 4 for characterizing
reformulation.

• Nayak and Joskowicz propose a model reformulation technique that simpliﬁes a com-
positional model of a device [21]. Their reformulation technique simpliﬁes a model
by replacing the model fragments in a given model by alternative simpler fragments
whenever possible. The result is a model that has the same explanatory power as the
original but is provably most parsimonious.

• Simon and Ando describe a technique for aggregating a system of linear ordinary
differential equations to produce a smaller system [32]. The technique only applies to
a system that consists of weakly interacting components such that interactions within
each component are much stronger than those among components. Their technique
produces a smaller system of equations that describes the mid- to long-term behavior
of the original system.

• Mallory et al. propose to summarize the results of the qualitative simulation of a phys-
ical system in order to help users recognize ‘basic patterns of behavior’ [19]. Their
reformulation procedure summarizes the behavior of the system by generating a be-
havior graph that retains only those aspects of the behavior tree relevant to the query.
The result is a smaller, more abstract tree that retains the essential features that are
relevant to the query.

While all the three techniques aim to simplify a formulation, they differ from each other
with respect to important aspects, such as the contexts to which they are applicable and the
results they produce. Below, we identify the dimensions for characterizing them that are
critical for the practical use of these techniques.

3.2.1. What problem does the reformulation apply to?

One of the fundamental assumptions of our analysis is that reformulation must be mo-
tivated by a task, and furthermore, that the task itself must be driven by a speciﬁc query.

156

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

Each of the three example techniques is motivated by a question about a quantity (or a set
of quantities) associated with a system. Each however deals with a different formulation
of such a query since each technique applies at different stages of reasoning about phys-
ical systems of Fig. 1. Nayak and Joskowicz’s technique applies at the Model Processing
Stage to reformulate a conceptual model. Simon and Ando’s technique applies at the Equa-
tion Processing Stage to produce a different set of equations. Mallory’s technique applies
at the Solution Processing Stage and produces a more concise description of a behavior.
Thus, our framework requires that the deﬁnition of a problem include a speciﬁcation of its
formulation as well as the query.

3.2.2. What does the reformulation do?

The objective of reformulation is to change some aspect of the formulation of the orig-
inal problem. Thus, in order to characterize a reformulation technique, we must specify
precisely the features of the problem that are affected and the ways in which they are
changed.

While the three examples above all purport to affect the simplicity of a formulation,
the precise deﬁnition of simplicity is different as the problem formulation is different in
each case. In Nayak and Joskowicz’s example, the notion of simplicity is based on the
causal-approximation relation. A model A is simpler than a model B if and only if the set
of causal ordering relations implied by model A is a subset of those implied by model B.
In Simon and Ando’s example, simplicity is deﬁned in terms of the number of equations
and variables. In Mallory’s case, simplicity can be measured by the number of links and
nodes, and the labels of those nodes in a behavior tree. Thus, our framework requires the
deﬁnition of a set of evaluators to specify the features of a problem (i.e., formulation and
query) that are modiﬁed by the reformulation technique.

Furthermore, in order to fully specify the effects of reformulation on problems, we must
also specify how the affected features are to be compared. Comparators are speciﬁcations
of such means of comparison. Quite often, especially when the affected features are numer-
ical values or sets, comparators are straightforward numerical (<, =, >) or set (⊃, =, ⊂)
comparators. In the three examples above, the features affected by reformulation can be
compared in this manner. In Nayak and Joskowicz’s case, the comparator is a subset re-
lation over the sets of causal relations implied by models. In Simon and Ando’s example,
the numerical comparator, <, is sufﬁcient to compare the number of variables and equa-
tions. In Mallory’s example, subset relations over the links, nodes, and labels are used to
compare the features. In other words, in all three cases, the notion of simplicity that each
reformulation technique aims to achieve can be precisely deﬁned by specifying evaluators
to measure some features of the problem and comparators to compare these measures.

When a reformulation technique is deﬁned as a mathematical function between two
sets, the reformulation process can be characterized by the nature of the function such
as injective, surjective, one-to-one, etc., and two or more such processes compared for
strength or weakness, see [10].

3.2.3. Characteristics of the reformulation technique itself

Aside from describing a reformulation technique in terms of what it does, one needs to
be able to characterize it as a computational process using standard criteria. Computational

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

157

complexity is one such criterion that is almost always provided by the authors of such
techniques. In all our three examples, the procedures are tractable with respect to the class
of problems that satisfy the conditions speciﬁed by the procedures. In other cases, evalu-
ation of the reformulation technique in terms of the computational complexity is given in
slightly different forms, such as best/worst/average case with respect to a simulated/natural
population of a certain size.

Although the computational complexity is often provided when reformulation tech-
niques are proposed, there can be a number of alternative ways to characterize these
techniques. From the perspective of a person looking for a suitable tool for one’s problem
at hand, the availability of the code for applying the technique is often an issue in practical
settings. If the code for carrying out reformulation is available in several commercial pack-
ages, their price or their ease of use could be points of evaluation. As there can be as many
angles from which to evaluate reformulation techniques as there are people who wish to
use them to solve problems, this can only be a non-exhaustive set of evaluators.

In summary. For practical purposes a reformulation technique must be described along
three aspects:

1. The class of problems to which it applies.
2. What it does, in terms of what features of problems it targets and how it changes

them.

3. Its characteristics and cost as a computational tool.

Our framework provides the means to deﬁne a problem along with several types of
evaluators and comparators to characterize those aspects of the reformulation process. The
following section formally introduces the components of our framework.

4. Framework for characterizing reformulation

In this section we introduce a framework and a terminology for characterizing and eval-
uating reformulation techniques. Section 4.1 introduces the components of the framework
and reduces the selection of a sequence of reformulations to a planning task. Finally, Sec-
tion 4.2 introduces the attributes necessary to characterize a reformulation technique, so
that selection can be performed.

4.1. Components of the framework

Reformulation replaces an original problem by a new one, as shown in Fig. 2. We dis-
tinguish two primary components, the problem and the reformulation, and two composite
components, the process and the strategy, obtained from composing the former two.

4.1.1. Problem

We deﬁne a problem Pi as a three-tuple:
Pi = (cid:5)Queryi, Formi, Assmpti(cid:6).

158

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

Fig. 2. Reformulation.

• Queryi speciﬁes the question of the user, the one we are trying to answer by manip-

ulating the formulation. The query motivates the whole endeavor.

• Formi denotes the formulation, i.e., the conceptualization of the domain of, for in-
stance the studied physical artifact. This amounts to a list of objects, and their interre-
lationships (e.g., functions and relations). Depending on the particular stage of Fig. 1 to
which the reformulation method is applied, the particular formulation can be a concep-
tual model (Model Processing Stage), mathematical equations (Equations Processing
Stage), or even solutions (Solution Processing Stage).

• Finally, Assmpti designates the conditions under which the formulation is valid, e.g.,

the domain of applicability and the temporal granularity.

According to this terminology, the task description deﬁned in Section 3.1 and Fig. 1 is

a problem Pi where:

• Queryi is the query in the task description,
• Formi is the domain theory and scenario, and
• Assmpti is the set of modeling assumptions.

4.1.2. Reformulation technique

Our ultimate goal is to be able to automatically select a suitable reformulation technique
from among a collection of techniques. One step towards this goal is to articulate, for each
reformulation, its applicability conditions and algorithmic steps.

A reformulation technique is applied to an original problem P1 to produce the reformu-
lated problem, P2, as shown in Fig. 2. By examining a wide collection of reformulation
techniques, we have found that reformulation techniques typically modify the problem
encoding only: the query and assumptions often remain unchanged before and after refor-
mulation. Counterexamples do exist such as the ones discussed in Sections 5.2.1 and 5.2.3
where assumptions are reformulated. We describe the reformulation technique as a tuple:

R = (cid:5)Proc, Cond(cid:6),

where:

• Proc denotes an effectively computable procedure of which the input is P1 and the
output is P2. For example, the approximation of the behavior of a nearly-decomposable
system by aggregation, discussed in Section 5.2.1 is one instance of such a procedure.
• Cond denotes the applicability conditions. For example, the aggregation of nearly-
decomposable system of Section 5.2.1, requires that Form1 be a self-contained set

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

159

of nearly-decomposable linear ordinary differential equations and that the system be
stable.

It must be noted that Cond is a set of necessary conditions to be satisﬁed by P1 for
the reformulation to be technically applicable. However, whether or not R is an appropri-
ate reformulation to perform depends on the goal of reformulation, which is discussed in
Section 4.1.3. If, for example, the goal is to obtain more precise values for the variables
in the mid-term behavior of the system than can be produced from the aggregated sys-
tem, aggregation of the nearly-decomposable system would not be an appropriate choice
of the reformulation method, though the procedure is technically applicable. In practice,
sometimes it is not known whether Form1 satisﬁes Cond because some parameter values
are unknown or even the precise form of the equations is unknown. Even in such cases,
one sometimes applies the reformulation method. The validity of the results thus produced
will depend on the degree to which Cond is satisﬁed by Form1. In such cases, Cond
becomes the assumptions Assmpt2 underlying Form2. Such conditional reformulation
can still be quite useful in providing insight into the shape of the problem space. The car-
icatural reasoning discussed in Section 5.2.3 demonstrates the usefulness of conditional
reformulation.

In Fig. 3 the reformulation R is illustrated as a transition between nodes representing

two problems P1 and P2.

As mentioned in the introduction, the decision to perform a reformulation could be
motivated by the availability of a suitable solution engine and its performance for solv-
ing a problem.4 For example, if we have a slow but general-purpose solver for Ordinary
Differential Equations (ODEs) and a high-performance implementation of a numerical
simulation algorithm that works only with systems of linear equations, we might re-
formulate an original nonlinear system of equations, which would have to be solved
with the general-purpose solver, into a linear one that can be solved by the fast integra-
tor.

A solution engine is applied to a problem to produce a result as an answer to the query.
There could be multiple engines at one’s disposal to solve the original or reformulated
problems. Alternately, there could be none, when the problem is too difﬁcult. Further, we
may use the same means of solution before and after reformulation since the reformulation
of a problem, for instance by decomposition, can result in an important improvement of
performance for the same solution engine. Aggregation, discussed in Section 5.2.1, illus-
trates this situation.

Fig. 3. Reformulation process.

4 In this paper, we do not address reformulations that apply to the engine itself, as proposed in [10], because
such reformulations do not seem to arise in the class of problems we address.

160

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

4.1.3. Process and strategy

A problem, a reformulation technique and the problem resulting from applying the re-
formulation technique to the problem compose a reformulation process. Fig. 3 illustrated
such a process as a transition between two nodes.

A single reformulation process can be understood as one step towards providing an
answer to the query. A sequence of reformulations, commencing with the initial problem
and possibly including one or more engines, constitutes one strategy for addressing a task.
The execution of a strategy constitutes problem solving.

Thus, we deﬁne a strategy Si to be a sequence of reformulations, (cid:5)Ra, . . . , Rx (cid:6) that
is applied to an original problem P1. The path (cid:5)P1, Ra, P2, . . . , Rx , Pi (cid:6) in Fig. 4 is an
example of the execution of such a strategy. Any subsequence, Sk, of Si , starting at P1
and stopping at any intermediary problem Pk , between P1 and Pi , is also a strategy, and is
called a sub-strategy of Si .

We perceive reasoning about physical systems to proceed according to processes iden-
tiﬁed in Fig. 1. According to this ﬁgure, the content of the initial input, i.e., the task
description, is gradually modiﬁed by a combination of any number of processes culminat-
ing in an answer to the query. Given our deﬁnition of reformulation, any of these processes
is a reformulation. The stage of processing distinguishes whether the reformulation is ap-
plied to a collection of model fragments, an equational model, or to one or more solutions.
Reasoning about physical systems is thus a successive application of reformulation proce-
dures that transforms an initial problem encoding.

Problem solving as plan execution. Problem solving involves the successive application
of reformulation procedures to an initial problem encoding to produce a ﬁnal problem en-
coding. Clearly there could be multiple sequences of reformulations that could be applied
to address the problem-solving task, as illustrated in Fig. 5. Identifying such sequences of
reformulation procedures can be viewed as a planning problem in which the states are prob-
lem encodings, the transitions (or actions) are reformulations, and the plans are strategies.

Fig. 4. Strategy.

Fig. 5. Problem solving is plan execution.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

161

Hence, problem solving becomes an execution of the selected plan. In Fig. 5 we illustrate
a tree of four alternative strategies.

In practice, as for planning, resources may be limited, and one may want to associate
a utility or objective function to the problem-solving task. We expect the user to provide
the goal of the problem-solving task in terms of a goal test and of an objective function
that speciﬁes the importance of some desired features of the problem and the resources
available. In this context, selecting an optimal plan or strategy becomes a multi-criteria
optimization problem.

4.2. Evaluating and comparing components

Section 4.1 deﬁnes the primary components (i.e., problem and reformulation technique)
and the composite components (i.e., process and strategy) of our framework. To articulate
the goal driving the above-mentioned planning process, we identify the features of these
components relevant for selecting reformulation techniques and characterizing their ef-
fects. These features are divided into sets, relative to the components of our framework.
The sets of features are meant to be incrementally augmented and reﬁned as one explores,
deﬁnes, and proposes new reformulation techniques. There are two main categories of sets:

1. Evaluators, denoted Evals, evaluate some features of one component or a combina-

tion of components of the framework.

2. Comparators, denoted Compars, assess the change due to reformulation by compar-

ing the values of the evaluators.

The values returned by the evaluators and comparators are not necessarily quantitative;

they could be qualitative or logical.

4.2.1. Evaluators

In order to characterize a reformulation technique, we must ﬁrst deﬁne measures or
evaluators that capture relevant characteristics of the problem, the reformulation technique
and their inter-relationships.

Given a reformulation R = (cid:5)Proc, Cond(cid:6), and original problem Po, and the resulting
reformulated problem Pr , we distinguish four such sets of evaluators as illustrated in Fig. 6.
We distinguish four sets of such evaluators, Evalsprob, Evalsref, Evalsp+r, and
Evalsp+r+p corresponding to evaluators that assess, respectively, aspects of a problem,
a reformulation technique, and a combination of an original problem and a reformulation
technique and ﬁnally a reformulation process. We introduce also a ﬁfth set of evaluators

Fig. 6. Evaluators and the components to which they apply.

162

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

Evalsstrat to be computed from the initial four. Below we explain and provide examples
for each set of evaluators.

1. Evalsprob(Pi) is a set of measures to evaluate some characteristic of a problem Pi .
Pi could be the original problem Po or the reformulated one Pr . In the most general
case, the elements in this sets can be deﬁned with respect to any of the three elements
of the problem, i.e., the formulation, query and assumptions. An example of an eval-
uator that assesses the problem as a whole (i.e., formulation, query and assumptions)
is the complexity class of the problem (e.g., NP, PSPACE, and EXPSPACE). As we
mentioned in Section 4.1.2, we have found that in practice the query and assumptions
often remain unchanged before and after reformulation, and that most evaluators are
functions applied exclusively to the formulation of Po.
The evaluators in the set Evalsprob can provide an assessment of some quantitative
aspects of the problem (e.g., size) or of its logical properties (e.g., compactness, com-
pleteness, and decidability). They can also address qualitative aspects of the problem
(e.g., complexity class, or how ‘close’ the formulation is from answering the query).
For a system of equations, an example of a quantitative evaluator is the number of
equations or their degree, or the number of variables; an example of a qualitative eval-
uator is adherence of the equations to some canonical form. Other evaluators of the
formulation that appear in the literature include scope [38] (which is the range of
phenomena that it can describe), expressiveness, syntactic form, simplicity, general-
ity, relevance, absence of irrelevant information, and language restriction to familiar
terms [37].
It is important to deﬁne an evaluator in sufﬁcient detail. In the case of simplicity,
for example, we must deﬁne the speciﬁcs of how it is measured (e.g., the number of
variables/equations in a equation set, or the number of components in a model). Some
of the evaluators in Evalsprob are dedicated to assessing the quality of the answer to
the query as it is made explicit in Formi. The result is often a numerical, quantitative
value but it can also be a qualitative (e.g., an interval) or a logical one (e.g., truth
or negation of a sentence). It can also be a description of a behavior over time, or
sets of partials or global solutions. Examples of such evaluators are the soundness of
the answer and its precision. These are typically the evaluators to use in the test that
determines whether the goal test of the planning process of Section 4.1.3 is achieved.
2. Evalsref(R) is a set of measures to evaluate some characteristic of the reformulation

technique Rt . Examples include but are not limited to:
(a) the size of the code that implements the procedure,
(b) the programming language it is written in,
(c) the price of the commercial software,
(d) its hardware and software requirements and interfacing capabilities, and
(e) the type and value of human expertise required for exploiting it.

3. Evalsp+r(R, Po) includes functions that assess the behavior of the reformulation
technique relative to a given problem. A typical such evaluator is the computational
complexity of the procedure Proc when applied to the problem, and that of verifying
the conditions of the reformulation, Cond.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

163

4. Evalsp+r+p(R, Po, Pr ) is a set of measures that characterizes the reformulation
process, typically, in terms of the properties of a function (e.g., partial/total, injective,
surjective, bijective, or invertible).
For example, Struss requires reformulation procedures, which he calls representa-
tional transformations, to be surjective and not injective mappings [33]. Giunchiglia
and Walsh study procedures that are surjective total functions between two formal
systems [10]. Other examples include the following: homomorphism, isomorphism,
theorem increasing, decreasing, or constant [10], deducibility or negation preserv-
ing [10], upward/downward solution [34], upward or downward-failure [39], ordered
monotonicity [13], and safety [4].

5. In addition to these sets of evaluators, we introduce Evalsstrat(Si), a set of mea-
sures for assessing a problem-solving strategy Si . An element in this set is obtained
by considering some combination of the values along Si of an element in the above-
introduced evaluators (i.e., Evalsprob, Evalsref, Evalsp+r, and Evalsp+r+p).
For instance, the ﬁnancial cost of the strategy can be computed as the sum of the
prices of the individual software packages; its computational complexity as the maxi-
mum of their respective complexity; and, when the individual processes are functions,
the strategy can be characterized in term of the their composition. Elements of this set
is typically used in the objective function of the planning process discussed in Sec-
tion 4.1.3 to express preferences and manage resources.

4.2.2. Comparators

Comparators characterize the changes in the problem as the result of reformulation. In
order to assess the effect of one or more successive reformulations within or across alter-
native strategies, we deﬁne two sets of comparators, Comparsprob and Comparsstrat,
which compare features of problems and strategies respectively. In order to capture some
notion of change or evolution, the elements of Comparsprob and Comparsstrat gen-
erally reﬂect differences or ratios computed from the values returned by the evaluators
Evalsprob and Evalsstrat. Below we discuss these two sets and provide illustrating
examples.

1. Comparsprob(Pi , Pj ) denotes a set of effects that capture a change in some feature

of the problem as expressed in Evalsprob(Pi ) and Evalsprob(Pj ).
As for Evalsprob, some elements of Comparsprob(Pi , Pj ) assess the change that
one or more reformulations produce on some measure of the answer. Examples of such
comparators are a 10% loss in the accuracy of the answer, whether the answer in Pj

Fig. 7. Evaluating and comparing problems.

164

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

Fig. 8. Evaluating and comparing strategies.

is an overestimate (or an underestimate) of that in Pi , or whether or not the difference
between them can be bounded within a threshold.
When Pi and Pj
are two problems situated along the same strategy Sk,
Comparsprob(Pi , Pj ) captures the effect of applying a reformulation (or, a sequence
of reformulations) to Pi .
When the reformulations are deﬁned are functions, this outcome can sometimes be
predicted from considering the mathematical property of the reformulation itself (or,
the composition of the consecutive reformulations). For instance, when the reformula-
tion is an injective mapping, the size of Pj is bigger than or equal to that of Pi .
One possible effect of reformulation on the problem is to improve cognitive insight;
this is common at the Solution Reformulation Stage of Fig. 1. If the original formula-
tion is too complex for a user to understand, reformulation can produce a description
better suited to human understanding.

2. Comparsstrat(Si , Sj ) is a set that denotes the effects to two strategies Si and Sj .
Examples of elements of this set are increase in cost, loss of time, and consumption of
available resources.
When Si is a sub-strategy of Sj (or vice versa), Comparsstrat(Si , Sj ) indicates the
effects of extending a strategy by one or more reformulation steps. When Si and Sj are
two distinct strategies applied to the same problem, Comparsstrat(Si , Sj ) assesses
their relative merits.
Giunchiglia and Walsh in [10] introduce the operators “weaker than”, “stronger than”
and “equivalent” (≡) to express an order relation determine an order two alternative
mappings applied to the same problem.
A reformulation is said to be cost-wise beneﬁcial, when the cost of reformulating the
problem and then solving the reformulated problem does not exceed the cost of solving
the original problem. This can be expressed by an element in Comparsstrat.
Since one of the goals of reformulation is to improve overall problem-solving perfor-
mance, the reformulation procedure itself should not signiﬁcantly add to the computa-
tional cost. However, sometimes there is no solution engine applicable to the original
problem, and consequently any amount of effort to reformulate to make it solvable is
justiﬁable. A cost-intensive reformulation may also be justiﬁed when it is performed
off-line to improve runtime performance of a system.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

165

4.2.3. Summary

Table 1 summarizes the vocabulary and criteria for evaluating and comparing refor-
mulation introduced above. It also situates these terms with respect to the aspects of
reformulation that we discussed in Section 3.2.

4.2.4. Remarks

Observe that the evaluators for the problem, reformulation, and strategy are not nec-
essarily independent. For example, the simpliﬁcation of a set of equations often reduces
the size of the formulation (measured by Evalsprob) and the cost of applying the refor-
mulation procedure (measured by Evalsp+r and consequently by Evalsstrat), at the
expense of also reducing the precision of the result (measured again by Evalsprob).

In practice, the framework’s components seem often to be characterized and evaluated
with respect to a set of non-orthogonal, and sometimes redundant, features. Weld in [38]
suggests to assess a reformulation with respect to a set of independent features, which
he calls model dimensions. He identiﬁes scope, domain, resolution, and accuracy as four
such orthogonal dimensions. Whether or not a set of canonical features exists remains an
open problem. Our study, however, indicates that a rich and expressive, although possibly
redundant, coverage of the features is desirable in practice.

5. Illustrative examples

In this section, we discuss a number of reformulation developed in the literature of rea-
soning about physical systems. We review two or more techniques for each of the three
stages of reasoning shown in Fig. 1: the Model Processing, Equation Processing and So-
lution Processing stages. While all examples have been discussed in AI literature, they
originate from and are useful in a variety of science and engineering ﬁelds. For example,
Aggregation (Section 5.2.1) is drawn from econometrics literature. Linearization (Sec-
tion 5.2.2) is a common technique widely used in mathematics and engineering. Caricatural
reasoning (Section 5.2.3) is a formalization of the type of reasoning used in chemistry.

In discussing the reformulation techniques, we conform to the following pattern: First,
we brieﬂy summarize a technique in general terms. Then, using the vocabulary introduced
in the preceding section, we make explicit the components of the technique in the following
order:

1. The original and reformulated problems (Query, Form, and Assmpts).
2. The reformulation technique (Proc and Cond).
3. The evaluators and effects (Evals and Compars).

For each example, the ﬁrst part of the discussion explicates the types of problems that
the technique applies to. The second part details the reformulation procedure itself along
with its conditions. Finally, the third part describes the effects of the procedure in terms of
what aspects of the problems are changed and how. It also discusses the characteristics of
the reformulation procedure as a computational process. Additionally, whenever we ﬁnd it

Table 1
Summary of attributes and metrics

Notation

What does the reformulation do?

Evalsprob(Pi )

Evalsp+r+p(R, Po, Pr )

Evaluators

Examples

Notation

Comparators

Examples

1
6
6

Problem

Logical:
• Compactness, completeness, decidability
• Result: truth/negation of a sentence, soundness
Quantitative:
• Size, number of equations, degree of equations,
number of variables, number of components in a
model,

• Result: behavior over time, numerical or interval

value, precision

Qualitative:
• Complexity class, scope of model, expressiveness
of model, syntactic form, simplicity, generality,
relevance, restriction of language to familiar terms,
adherence of equations to some canonical form,
understandability

• Result: sets of partial or global solutions, result:
behavior over time, whether query is answered

Reformulation process

• Necessary, sufﬁcient approximation
Functions: partial/total, injective, surjective,
invertible, morphism, TI/TD/TC, upward/downward
solution or failure property, ordered monotonicity,
safety property, etc.

B
Y

.

.

Comparsprob(Pi, Pj )

• Set operators: ⊃, =, ⊂
• Numerical operators: <, =, >

C
h
o
u
e
i
r
y

e
t

a
l
.

/

A
r
t
i
ﬁ
c
i
a
l

I
n
t
e
l
l
i
g
e
n
c
e

1
6
2

(
2
0
0
5
)

1
4
5
–
2
0
4

(continued on next page)

Table 1 (Continued)

Notation

Examples

Characteristics of the reformulation itself

Evaluators

Notation

Comparators

Examples

Evalsref(R)

Reformulation technique

• Size of code
• Programming language
• Price of commercial software
• Interfacing capabilities (hardware/software)
• Human expertise required

Evalsp+r(R, Po)

• Computational complexity: Best, worst, average

case; empirical

Reformulation technique and problem

Evalsstrat(Si)

Combinations, to be deﬁned, of the values along Si
of an element of the above evaluators.
• Financial cost.
• Maximal/total computation cost in time/space.

Strategy

R: Reformulation technique.
Pi : A problem. Original problem: Po; reformulated problem Pr .
Si : A strategy, a sequence of problems are reformulations (cid:5)P1, Ra , P2, . . . , Rx , Pi (cid:6).

Comparsstrat(Si , Sj )
(Strategy)

• Weaker, stronger, equivalent

abstraction

• Cost intensive, Cost-efﬁcient

B
Y

.

.

C
h
o
u
e
i
r
y

e
t
a
l
.

/

A
r
t
i
ﬁ
c
i
a
l

I
n
t
e
l
l
i
g
e
n
c
e

1
6
2

(
2
0
0
5
)

1
4
5
–
2
0
4

1
6
7

168

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

is instructive to do so, we discuss the reformulation technique as part of a larger problem-
solving strategy and compare it with other possible strategies.

5.1. Reformulation at the model processing stage

The ﬁrst step in reasoning about physical systems is to build a model. A model is the
conceptual object we study and manipulate instead of studying the real physical device.
There can be as many possible models of a given subject as there are reasons for studying it.
While there is no one “correct” model, the usefulness of a model depends on the query one
tries to answer by constructing and studying the model. For a model to be useful, it must
contain enough information to answer the query with sufﬁcient precision and accuracy
while avoiding unnecessary detail.

The process of model construction sometimes amounts to model reformulation, where
one starts with some existing accepted model and modiﬁes it to suit the task at hand. It
is said that in most engineering applications a signiﬁcant part of the modeling effort is
spent on iterative modiﬁcation of the proposed model [26]. In AI, even works that focus
on formulating new models can involve an explicit reformulation step where the model
initially generated is modiﬁed to meet some optimality criteria.

In this section, we discuss two model-reformulation techniques. The ﬁrst one, Model
Simpliﬁcation by Nayak and Joskowicz, is an example of a technique developed as part
of a model-formulation method. In our discussion, we focus on the last step of the model
formulation process, in which the model initially generated is simpliﬁed to meet the au-
thor’s simplicity criterion. Since this reformulation technique is embedded in a modeling
approach, called compositional modeling, we must introduce brieﬂy the concept of com-
positional modeling in order to put our discussion in proper light.

Compositional modeling is an effective method for automatically formulating a behav-
ior model represented by a system of ordinary differential equations for a physical system
[6,15,21,29]. The basic idea in compositional modeling is to formulate a model of a given
situation by putting together pieces of descriptions, called model fragments, of physical
phenomena in the domain. Each model fragment describes one aspect of a component be-
havior or a physical process. A system formulates a model of a given situation by selecting
applicable model fragments and composing them. The main advantages of compositional
modeling are modularity and reusability of knowledge. It is, at least in principle, easier to
write and reuse model fragments than complete models. Since there can be multiple ways
to describe a given situation, each with varying degrees of details or with different model-
ing assumptions, it is possible to have several alternative model fragments describing the
same physical phenomenon. Nayak and Joskowicz’s reformulation technique involves re-
placing the model fragments in the existing model by simpler alternatives that describe the
same phenomenon.

The second example we discuss is Critical Abstraction by Williams [40]. As in Nayak
and Joskowicz’s work, the goal of the reformulation in Critical Abstraction is to produce
a model that is the simplest yet sufﬁcient to provide a causal explanation of the behavior
of interest. Critical Abstraction could very well be part of an overall model formulation
method, even though Williams did not present it as such.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

169

Fig. 9. Model simpliﬁcation.

5.1.1. Model simpliﬁcation

Nayak and Joskowicz propose a model reformulation technique that simpliﬁes a com-
positional model of a device, while maintaining its ability to provide a causal explanation
of the expected behavior of the device [21].

The primary objective of their work is to perform efﬁcient compositional modeling for
generating parsimonious causal explanations of the functioning of a device. They provide
tools for model-fragment library indexing and selection to support the construction of de-
vice models. The reformulation procedure is applied to the model thus built in order to
simplify it. The entire model formulation procedure is portrayed as a sequence of steps in
Fig. 9. While the compositional modeling contributions of this paper are particularly inter-
esting, it is the reformulation procedure following the initial model formulation that is of
relevance to us. Thus, we focus on this simpliﬁcation process in our discussion.

Given a device description, the expected behavior of a device, its structural and behav-
ioral constraints, and a library of model fragments organized and indexed in a particular
fashion, their model-building algorithm composes an initial coherent model of the device.
This initial model is coherent in that it explains the expected behavior, it is internally con-
sistent, and the set of equations implied by the model are not over-constrained. However,
it may not be as parsimonious as it could be; that is, it may be possible to further simplify
the model while maintaining the structural and behavioral constraints of the device, and
the ability of the model to explain the expected behavior. Their reformulation procedure
performs this simpliﬁcation by removing unnecessary model fragments from the model
and also by replacing model fragments by their approximations whenever it is possible to
do so without compromising the coherence of the model.

The organization of the library of model fragments makes explicit the causal approxi-
mation relations among alternative descriptions of a given phenomenon. Thus, alternative
simpler descriptions can be found without search, which enables an efﬁcient execution of
the approximation.

The metric for evaluating and comparing problems is parsimony of the formulation.
A parsimonious model contains only those model fragments necessary to explain the
expected behavior while satisfying the structural and behavioral constraints. The refor-
mulation algorithm guarantees that the resulting model is the most parsimonious in the
sense that no model fragment in the model can be removed or replaced by any of its causal
approximations without violating the structural and behavioral constraints.

The problems, P1 and P2

The original problem, P1, and the reformulated problem, P2, consist of the three-tuples,

(cid:5)Query1, Form1, Assmpt1(cid:6) and (cid:5)Query2, Form2, Assmpt2(cid:6).

Query1 = Query2: Generate a causal explanation of the speciﬁed expected behavior.

170

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

Form1 consists of:

1. A library of model fragments, where

• model fragments are grouped into assumption classes,
• each assumption class has one best model fragment of which the rest are causal

approximations, and

• the causal approximation relations are acyclic.

2. The expected behavior of the device in terms of an expected causal relations among a

pair of variables, represented as causes(v1, v2).

3. The structural and behavioral constraints.
4. A coherent causal model of the device comprised of instances of model fragments.

Form2: The same (1) through (3) as in Form1. As for (4), a simplest, coherent and causal

model of the device.

Assmpt1 = Assmpt2: The assumptions underlying the library of model fragments if

any.

The reformulation technique

Proc:

Input to the reformulation procedure is a compositional model that may contain
unnecessary model fragments and may not be as simple as possible. The reasons
for this are that the model formulation algorithm initially selects the most accurate
model fragment in each assumption class and also that it satisﬁes some constraints
by adding model fragments that may later prove unnecessary.
The reformulation procedure exploits two operators:
1. Replacement of a model fragment by one of its immediate causal approxima-

tions, as deﬁned in the model-fragment library; and

2. Removal of an unnecessary model fragment.
The ﬁrst operator is applied repeatedly while ensuring that the resultant model can
explain the expected behavior. This is achieved by an order of magnitude reasoner.
The second operator is then applied, again ensuring that the expected behavior can
be explained and that all the structural and behavioral coherence constraints are
satisﬁed. Note that the reformulation procedure generates one simplest adequate
model. More than one may exist but the procedure stops after ﬁnding the ﬁrst.

Cond: The model-fragment library must be organized in a manner that makes the causal
approximation relations among alternative descriptions of the same phenom-
ena explicit and alternative simpler model fragments readily available. This is
achieved as follows: Model fragments that describe the same physical phenom-
enon but are based on different assumptions are grouped into an assumption class.
Each assumption class is a collection of mutually contradictory descriptions of
the same phenomenon. For example, the behavior of a resistor can be described
as a constant resistance resistor or a temperature-dependent resistance. Each as-
sumption class is organized in a tree with model fragments as nodes and causal
approximation relations as links. Each assumption class has a single most accu-

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

171

rate description at the root, and all other descriptions are causal approximations
of it. Given two model fragments mi and mj , mi is a causal approximation of
mj if and only if mi is an approximation of mj and the causal ordering rela-
tions among variables of mj is a superset of those of mi . Approximation relations
among model fragments are given a priori in the library.

Evaluators and effects

Evalsprob, Comparsprob: Nayak and Joskowicz evaluate a formulation—more specif-
ically the model part of a formulation—in three ways, namely coherence, causal-
ity, and parsimony of a model. Coherence refers to the consistency and complete-
ness of a model with respect to the expected behavior and other given behavioral
and structural constraints. Causality refers to the ability of a model to explain the
expected causal relations among variables. Parsimony refers to the simplicity of a
model.
Coherence is in Evalsprob since it is a characteristics of a formulation—more
speciﬁcally, that of a model. Coherence of a model is deﬁned in terms of consis-
tency and completeness of a model. A model is coherent when it is both consistent
and complete. A model is consistent when it does not contain model fragments
with mutually contradictory assumptions. A model is complete when the set of
equations entailed by its model fragments are complete. A set of equations is
complete if it contains the same number of equations as variables and no subset
is overconstrained.
Causality is in Evalsprob as it is a characteristic of a formulation. More speciﬁ-
cally, it is a characteristic of the model as well as of the expected behavior, which
is expressed in terms of causal dependency relations between a pair of variables.
A model is a causal model if and only if the causal ordering generated from the
equations of the model entails the expected behavior.
The most important characteristics of the reformulation procedure is that it pre-
serves the coherence and causality properties of a model while making it most
parsimonious. Thus, the effect of the reformulation is expressed in terms of par-
simony.
Parsimony is in Compars(P1, P2) as it is deﬁned in terms of a binary relation,
simpler-than, between models instead of some measure of the degree of par-
simony of an individual model. The simpler-than relation in turn is deﬁned
in terms of a binary relation, approximation(m1, m2), between model frag-
ments. The approximation relations are given explicitly in the model-fragment
library. A model M1 is simpler-than M2 if for each model fragment m1 in M1,
either m1 is also in M2 or there is another model fragment m2 in M2 such that m1
is an approximation of m2. A model M1 is said to be parsimonious if it is coher-
ent and there is no other model strictly simpler than M1 that is also coherent. The
reformulation procedure is guaranteed to produce a parsimonious model. Though
there may be more than one, the procedure stops after producing one.
Evalsp+r: The authors evaluate the reformulation procedure in terms of its computa-
tional complexity, which is in Evalsp+r(R, P1). The complexity of the refor-

172

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

mulation procedure relative to the problem encoding is polynomial provided the
order of magnitude reasoning used to verify that the simpliﬁed model still explains
the expected behavior is polynomial. The tractability of the procedure is ensured
by the upward failure property regarding the need to explain the expected behav-
ior. The upward failure property in this case means that, if a model fails to explain
the expected behavior, any of its approximation will also fail, thus allowing one
to prune large parts of the search tree. The upward failure property is ensured by
the requirements that: (1) the approximation relations among model fragments in
the library are causal approximations, (2) the causal approximation relations are
acyclic, and (3) each assumption class has only one root, i.e., the most accurate
model fragment.

Discussion. The success of Nayak and Joskowicz’s elegant and efﬁcient model formula-
tion as well as simpliﬁcation procedures hinges entirely on the strong restrictions placed
on the organizations of the model-fragment library as well as on the types of constraints
it can handle. In other words, the model-fragment library (and to a large extent, the prob-
lem itself) must be carefully crafted for the procedure to run efﬁciently. This criticism also
applies also to other model composition techniques that rely on an elaborately structured
model-fragment library such as the one used in [15]. Because of the restrictions placed on
the problems and model-fragment library, the practicality and generality of the simpliﬁ-
cation procedure described here remains questionable. For example, the requirement that
all approximation relations must be causal approximations seems to leave out many other
useful types of approximations, such as linearization discussed in Section 5.2.2. Further,
the requirement that the builder of a model-fragment library must specify approximation
relations a priori is also restrictive. As we have illustrated in Section 2.3 with the example
of square and sine waves, given two different descriptions, the question of which is a useful
approximation of the other may very well depend on the question being asked.

As Pos points out [26], modeling is a naturally dynamic and iterative process, dur-
ing which problems themselves, including the assumptions, constraints, and formulations,
are likely to change. In light of this dynamic and iterative nature of the modeling en-
deavor, reformulation approaches that involve selection from pre-speciﬁed set of model
fragments using hard-wired approximation relations among them could prove too limit-
ing. Furthermore, the requirement for a large, carefully crafted model-fragment library to
be constructed makes the approaches that depend on such libraries less practical in most
cases. In the following section, we discuss a technique that does not require the use of a
model-fragment library, at least for the purpose of reformulation.

5.1.2. Critical abstractions

Williams proposes a reformulation technique, called Critical Abstraction, for simplify-
ing a model of a physical device to the furthest extent possible while preserving its ability
to answer a given query [40]. The information retained in the model should be necessary
and sufﬁcient to answer the query while supporting a causal explanation of the answer.
There are two inputs to the procedure: a model of the physical behavior, and a query on
a variable in the model. A model contains two types of information: physical components
and their behavior equations. The behavior equations describe the interactions among the

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

173

Fig. 10. Critical abstraction.

components as well as within the components. The output is a simpliﬁed version of the
model that is sufﬁcient to answer the query. See Fig. 10.

The reformulation is a three-step procedure that modiﬁes the model according to the
given query. First, superﬂuous interactions among mechanisms are eliminated. This is
achieved by ﬁrst building the causal ordering of the variables then eliminating the equa-
tions that are not causally upstream of the query variable. Second, the behavior descriptions
of each mechanism are simpliﬁed by combining equations to eliminate the variables and
interactions that are only internal to each mechanism. Third, individual variables and equa-
tions are modiﬁed to retain only those features that contribute to that determination of the
answer to the query.

As in the case of Nayak and Joskowicz’s reformulation technique, the metric for eval-
uating and comparing problems is parsimony of the formulation, though the deﬁnitions of
parsimony differ. While Williams’ concept of parsimony of a model is similar to that of
Nayak and Joskowicz’s in that it requires the set of variables and causal relations entailed
by a more parsimonious model to be a subset of those entailed by the original model, it
does not involve approximation relations. As a result, his reformulated model produces the
exact same answer to the query as the original model. Furthermore, Williams requires that
reformulation preserve the connection between equations and the mechanisms that produce
them. This requirement ensures that the resulting model contain enough information to ex-
plain the answer in terms, not only of the causal dependency relations among variables,
but also of the actual physical mechanisms that are responsible for the relations. Williams’
reformulation algorithm guarantees that the resulting model is the most parsimonious in
the sense that no model that is strictly more abstract can produce the same answer to the
query while maintaining the same causal and teleological explanatory power.

The problems, P1 and P2

The original problem, P1, and the reformulated problem, P2, consist of the three-tuples,

(cid:5)Query1, Form1, Assmpt1(cid:6) and (cid:5)Query2, Form2, Assmpt2(cid:6).

Query1 = Query2: Find, or verify, the value of a qualitative variable x.

Form1 and Form2 consist of the following:

1. A set of variables V .
2. A set of independent variables I V ⊂ V .
3. A set of equations I (called interactions) on the variables of V , such that

• each member of V appears in at least one equation,
• there is no simultaneity, and
• the equations are not redundant.

174

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

4. A set of mechanisms M (such as components, connections, and processes).
5. An onto function I M : I → M, which associates each interaction with the mechanism

that produces it.

In addition, Form2 is an abstraction of Form1, and Form2 is the most abstract possible
in the sense that no formulation that is strictly more abstract than Form2 can produce the
same answer to the query while maintaining the same causal explanatory power. Williams
deﬁnes abstraction as follows:

Deﬁnition 1 (Abstraction). Form2 is an abstraction of Form1 if V2 ⊂ V1, I V2 ⊂ I V1,
M2 ⊂ M1, and for every m in M1, its interactions in Form1 entail its interactions in
Form2. In other words, for each i in I2 and its corresponding mechanism m, i is satis-
ﬁed whenever m’s interactions in Form1 are satisﬁed.

Assmpt1 = Assmpt2: Whatever assumptions that underlie P1.

The reformulation technique, R

Williams places the following restrictions, Cond, on the set of equations:

• The equations describe static interactions.
• The equations are non-redundant.
• The equations do not contain simultaneity.

The reformulation procedure, Proc, consists of the following steps:

1. Determine the causal ordering among variables using the equations. The conditions
that there is no simultaneity and that the equations are non-redundant guarantee that a
causal ordering can be found efﬁciently and that it is unique. Remove all the variables,
interactions, and mechanisms upon which the query variable does not causally depend.
2. Simplify the remaining set of equations by eliminating the variables that are internal to
one mechanism and do not participate in interactions with other mechanisms. Again,
the lack of simultaneity in equations makes this step straightforward.

3. Since the query asks for the qualitative value of a variable, the last step turns the re-
maining equations into qualitative equations to the furthest extent possible without
losing the ability to answer the query. This step, which ultimately produces the ﬁnal
set of equations in the mixed qualitative and quantitative algebra, is achieved roughly
as follows. With each equation on the causal ordering starting from the query variable
and moving towards the independent variables on which the query variable causally
depends, the sign operator is pushed inwards as far as possible using the homomor-
phism of the sign operator with respect to multiplication, division and negation.5 This
process has the overall effect of pushing the sign operator from the query variable to

5 The sign operator [] is homomorphic with respect to multiplication, division and negation but not with addition
and subtraction. In other words, [a × b] = [a] × [b], [a/b] = [a]/[b], [−a] = −[a], but the same does not hold
in general for + and binary −.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

175

causally upstream variables and expressions as far as possible. At the same time, this
process identiﬁes the expressions whose qualitative values can only be determined by
evaluating them quantitatively in order to answer the query.

The requirement for static equations is essential for the claim that the procedure iden-
tiﬁes every landmark value for each variable necessary to answer the query and that those
are the only ones needed. Since the equations are static, simulation is not necessary for
such identiﬁcation. If the equations were dynamic, one would have to perform simula-
tion over time to identify relevant landmark values. The requirement that the equations be
non-redundant ensures that the causal ordering can be found and is unique. Otherwise, one
would have to select a non-redundant subset of the equations to determine the causal order-
ing. Finally, the requirement that the equations should not contain simultaneity ensures that
the procedure can always associate a unique mechanism with an equation, which Williams
regards as important for a causal explanation.

Evaluators and effects

Evalsprob, Comparsprob: Williams evaluates a formulation according to three as-
pects, namely its ability to answer the query, causality, and parsimony. The
evaluators seem similar to those of Nayak and Joskowicz’s on the surface but,
in fact, they are quite different in their actual deﬁnitions. The overriding con-
cern in Critical Abstraction is that reformulation should not change the answer
produced by the formulation. The goal of reformulation here is to simplify the
formulation to the furthest possible extent without altering any aspect of the an-
swer to the query. This is a signiﬁcant difference from reformulation in Nayak
and Joskowicz’s work, where reformulation produces a more approximate model,
which presumably will produce an answer that is different from the original. Like-
wise, the concept of parsimony in Nayak and Joskowicz’s work relies on that of
approximation while approximation plays no role in the concept of parsimony in
Critical Abstraction. As for causality, an important difference between the two is
that Williams includes the connection between the interactions and mechanisms
that produce the interactions in the overall concept of causality in a formulation
while Nayak and Joskowicz do not.
The ability to answer the query is in Evalsprob(P ), and the means of compar-
ison for this evaluator is that they must be exactly the same. The answer to the
query entailed by the two formulations must be exactly the same in all respects,
including the accuracy, precision, and ambiguity.6

Evalsp+r+p: The concept of causality Williams is concerned with includes the connec-
tion between interactions and mechanisms in addition to the causal dependency
relations among variables. This notion of causality is embodied in the last part of
the deﬁnition of abstraction presented above, which says that the abstraction must
preserve the connection between interactions and individual mechanisms that pro-

6 If the initial formulation is qualitative, the reformulation must not change the ambiguity in the answer.

176

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

duce them. We consider this evaluator to be in Evalsp+r+p(R, P1, P2) since it
is deﬁned as a relation between two formulations.
Parsimony here is also in Evalsp+r+p(R, P1, P2) as Williams’ concept of parsi-
mony is mainly that of abstraction as deﬁned above. It also includes a measure of
simplicity of the quantity space of variables. Informally, the less distinctions one
needs to make in the quantity space of a variable, the simpler the quantity space.7
In the mixed quantitative-qualitative algebra employed in Critical Abstraction,
this means that for each variable, the fewer quantitative expressions involving the
variable there are in the equations, the simpler its quantity space is. However,
Williams does not present a complete formal deﬁnition of parsimony.
Evalsp+r: The computational complexity of the reformulation is in Evalsp+r(R, P1).
Although Williams does not present an analysis of the computational complexity,
all the steps in the procedure appear linear with respect to the number of equations
and variables.

Discussion. The strength of Critical Abstraction is that it does not require a pre-
enumerated set of pieces of alternative descriptions, as do the model formulation, or
reformulation approaches that rely on model-fragment libraries. Based solely on the query,
Williams simpliﬁes the formulation by removing parts that are irrelevant to the query and
identifying the minimal set of quantitative distinctions that are necessary. Furthermore, this
reformulation is performed without compromising the causal explanatory power of the for-
mulation or its ability to answer the query. Williams accomplishes this without incurring a
large computational cost.

Also noteworthy is Williams’ attempt to abstract the quantity space of the variables
in accordance with the query. The last step in his reformulation procedure automatically
identiﬁes the necessary quantitative distinctions that must be made to answer the query.
This is in contrast to the more common practice in qualitative reasoning where landmark
values are, for most part, pre-enumerated.

In both of the approaches discussed in this section for reformulating models, the pro-
fessed goal of reformulation is a better explanation, where “better” means causally consis-
tent and simpler, even though the end product in both cases is another formulation and not
an explanation. A working assumption in both cases is that a simpler model will produce
a better explanation. It is also worth noting that their goal is not reduction of the com-
putational effort required to answer the query though one might expect such a reduction,
especially given both approaches produce simpler models in the sense of “more approxi-
mate” or “requiring less quantitative distinctions”. This suggests that what one perceives as
a satisfying causal explanation of a physical system involves more than just quantities and
equations; it must somehow establish connections between quantities and equations on one
hand and the components of the cognitive model of the physical system, may it be model
fragments (as in the Nayak and Joskowicz’s case) or mechanisms (as in Williams’). Exam-
ples of reformulation whose primary goal is reduction of computational cost are found in
the next section on reformulation at the Equation Processing Stage.

7 A quantitative variable can be considered to an inﬁnite number of distinctions.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

177

5.2. Reformulation at the equation processing stage

We now turn to examples of reformulation techniques at the Equation Processing Stage.
Reformulation of a system of equations is a common practice when the system is too large
or complex to be solved or analyzed directly. Thus, most of the techniques employed at this
stage, including the ﬁrst three discussed in this section, simplify the system through such
means as aggregating variables and equations (Section 5.2.1), substituting nonlinear equa-
tions by linear ones (Section 5.2.2), and decomposing the problem space (Section 5.2.3).
However, simpliﬁcation is not the goal of the last technique discussed in this section: In
Exaggeration (Section 5.2.4), a system of qualitative equations based on standard num-
bers is replaced by one based on nonstandard numbers, producing a more complex system
exhibiting more complex behavior.

5.2.1. Aggregation of nearly decomposable systems

Simon and Ando provide a formal basis for a reformulation technique that aggregates
variables in dynamic systems to reduce the size of the system [32]. The aggregation proce-
dure they describe takes a set of linear differential equations that is nearly decomposable
and produces a smaller aggregate. A nearly decomposable system is a system composed
of subsystems such that the interactions among subsystems are much weaker than the in-
teractions within each subsystem. If a given linear system is nearly decomposable, and if
the system is stable, then their procedure deﬁnes an aggregate variable for each subsystem
as a linear function of the original variables in the subsystem, and reformulates the entire
system in terms of the aggregate variables and equations. The result is a smaller system set
that adequately describes the mid-to-long term behavior of the original set. The primary
goal of aggregation is reduction of the cost of solving the equations since the size of the
system has a direct impact on the cost of ﬁnding the solution.

The basis for the aggregation procedure is the observation that the behavior of a nearly

decomposable system can be characterized in the following four stages [32]:

1. Short-run dynamics, where variables in each subsystem are moving towards their rel-

ative equilibrium independently of other subsystems.

2. Short-run equilibrium, where the most signiﬁcant root of each subsystem dominates

the behavior of the subsystem.

3. Long-run dynamics, where the variables in each subsystem move together towards

overall equilibrium.

4. Long-run equilibrium, where the most signiﬁcant root of the entire system dominates.

Aggregation produces a system that ignores the ﬁrst stage but models the remaining stages
with increasing accuracy as time goes on.

Simon provides a good example of aggregation of variables in the case of heat ﬂow
within a building [31]. Consider a building divided into a large number of rooms that are
in turn divided into a number of ofﬁces by partitions. The outside walls provide perfect
thermal insulation from the environment. The walls between rooms are good but imper-
fect insulators while the partitions within rooms are poor insulators. In this situation, the
temperature equilibrium among ofﬁces within one room will be reached very rapidly while

178

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

Fig. 11. Solving linear ordinary differential equations by aggregation.

equilibrium across rooms will be reached only slowly. Therefore, as long as we are not in-
terested in modeling rapid temperature ﬂuctuations within one room, a useful aggregation
is to have one temperature variable for each room and to assume that equilibrium within a
room is reached instantaneously.

Fig. 11 shows a strategy involving aggregation to solve linear differential equations. It
is depicted as a two-step strategy of aggregation (R1) followed by solution of the aggregate
system (R2). It also shows an alternative strategy of directly solving the original system as
the path (cid:5)P1, R3, P4(cid:6).

The problems, P1 and P2

The original problem, P1, and the reformulated problem, P2, consist of the three-tuples,

(cid:5)Query1, Form1, Assmpt1(cid:6) and (cid:5)Query2, Form2, Assmpt2(cid:6).

Query1 = Query2: What is the value of the variable x at time t, where x is a variable

in E.

Form1 consists of the following:

1. A set E of linear ordinary differential equations involving n variables and n equations.
2. A small value ε, to be used as the threshold value for classifying coefﬁcients in E into

those representing strong or weak interactions.

Form2 consists of the following:

1. A set E(cid:9) of linear ordinary differential equations involving m variables and m equa-

tions, where m < n.

2. n equations of the form xi = fi (yj ), one for each variable xi in E, where fi is a linear

function of yj , and yj is a variable in E(cid:9).

Assmpt1: None.

Assmpt2: The values of x for a relatively small t are not important (i.e., the short-run

dynamics is not of interest).

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

179

The reformulation techniques, R1 and R2

The aggregation technique R1 requires the following conditions to be satisﬁed by the

original system:

Cond1:

• The system is nearly decomposable with respect to the threshold value ε. This means
that given ε, the matrix consisting of the coefﬁcients in E can be put in a nearly block-
diagonal form, where the elements outside the diagonal sub-matrices are all less than
ε. This can be achieved by rearrangement of rows and columns if necessary. However,
no other manipulations, such as Gaussian elimination, are allowed.

• The behavior of the system represented by Form1 is stable.

The reformulation procedure, Proc1, consists of the following steps:

1. For each diagonal sub-matrix in Form1,

• solve the sub-matrix by itself,
• choose the most signiﬁcant root,
• deﬁne the aggregate variable as a linear function of the most signiﬁcant root. The
deﬁnition of the aggregate variable and the eigenvector associated with the most
signiﬁcant root determines the linear relation between the aggregate variable and
each of the variables in the subsystem.

2. Rewrite the original matrix by substituting each variable with its expression in terms

of the respective aggregate variable.

The solution step R2 follows R1. R1 has no conditions. Its procedure, Proc2, consists of
the following steps:

1. Solve E(cid:9).
2. Using the solutions for the variables yj ’s in E(cid:9) and the n equations of the form xi =

fi (yj ) in Form2, compute the values of xi ’s.

Iwasaki and Bhandari later generalized the original aggregation procedure to cases
where there are multiple signiﬁcant roots per system [12], but the intuition remains the
same.

Evaluators and effects

Evalsprob, Comparsprob: The immediate effect of aggregation is to reduce the size
of the system of differential equations that must be solved. Aggregation also has
an effect of making the solution less accurate. The size of a formulation is in
Evalsprob(P ). It is measured in terms of the number of variables in the system.
The size strictly decreases as a result of aggregation.
Another effect of aggregation is on the accuracy of a solution, which is also in
Evalsprob(P ). However, the comparator for this evaluator (Comparsprob(P1,

180

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

P2)) is more complicated than the comparator for the size. The magnitude of the
discrepancy between the two solutions in general decreases as t grows.
Evalsp+r: One can evaluate the reformulation procedure in terms of its computational
complexity. Solving the matrix analytically requires computing eigenvalues and
eigenvectors of the non-singular square matrix whose elements are the linear co-
efﬁcients in the equations. Given ε, the complexity of all the steps in Proc1 is
constant or linear, except for the step 1, whose complexity is O(m3
i ), where mi is
the size of the ith sub-matrix. Thus, the overall complexity of Proc1 is O(m3
max)
where mmax is the size of the largest sub-matrix.

Evalsstrat: Likewise, strategies can be evaluated with respect to their computa-
tional complexity. The complexity of the strategy represented by the path
(cid:5)P1, R1, P2, R2, P3(cid:6) is the greater of O(m3
max) and O(m3), where mmax is the
size of the largest sub-matrix and m is the size of the aggregate matrix. The com-
plexity of the strategy represented by the path (cid:5)P1, R3, P4(cid:6) is O(n3), where n is
obviously larger than m or mmax. The cost of computing the numerical solution
decreases8 in general.

Discussion. Aggregation reduces computational cost at the expense of some loss in ac-
curacy, though the discrepancy diminishes as t increases. That is why this reformulation
method is appropriate only when the short-run dynamics are not of interest. Of course,
if the short-run dynamics are of primary interest, one can always solve the subsystems
independently to obtain a reasonably accurate solution for a small t.

Finally, aggregation can have an effect on the cognitive transparency of a formulation.
Though in the work of Simon and Ando the issue of cognitive transparency of a model
did not arise, it is often an important consideration as we have seen in Section 5.1 on re-
formulation of models. Depending on the ways in which aggregate variables are deﬁned,
aggregation can make a formulation more compact and thus easier to understand or it can
make the physical meaning of a formulation more difﬁcult to comprehend. If the original
variables in a subsystem all have the same physical dimension, one could deﬁne the aggre-
gate variable so that it has an easily discernable physical signiﬁcance such as the sum or
the average. In this case, the resulting aggregate formulation will be easier to understand
as a description of a physical behavior. However, in other situations, for example if the
original variables have different dimensions, the aggregate formulation may be difﬁcult to
interpret physically even if it is useful computationally.

5.2.2. Stability analysis of nonlinear systems through linearization

Nonlinear systems can exhibit entirely new types of behavior compared to linear sys-
tems, but rarely are there tools available to analyze these behaviors. Fortunately, it is often
unnecessary to obtain detailed numerical or analytical solutions but is sufﬁcient to charac-
terize only some aspect of the system behavior. Existence of equilibrium points and their
stability are examples of aspects frequently studied about nonlinear systems [18].

8 The cost will be proportional to m × t, where m is the size of the system and t is the number of time steps at
which variables are evaluated.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

181

Fig. 12. Stability by linearization.

Linearization is a strategy commonly used to evaluate the stability of a nonlinear system
near equilibrium points. Linearization is justiﬁed because over a small time interval, the
performance of the system is approximately governed by the linear terms. Provided that
the linear terms do not vanish near the equilibrium point, these terms dominate and thus
determine stability around the point [18]. If the linear terms do vanish, a separate analysis
is required.

Once linearized, the stability of the system is determined by the location of the eigenval-
ues of the system matrix in the complex plane. Thus, the problem-solving strategy consists
of the two sequential steps shown in Fig. 12—R1, which transforms a nonlinear system
to a linear system, followed by R2, which determines stability. This strategy is motivated
by the lack of a general method to directly determine the stability of a nonlinear system.
For the purpose of stability analysis, there is actually an alternative to linearization. The
alternative involves a Liapunov function and is shown in Fig. 12 as a strategy represented
by the path (cid:5)P1, R3, P4(cid:6). R3 requires no computation with the condition that a Liapunov
function is known for the nonlinear system in P1, since the existence of such a function
immediately implies that the system is stable and the region over which the function is de-
ﬁned is the region of stability. However, ways to construct a Liapunov function are known
only for limited classes of nonlinear systems.

The problems P1, P2, and P3

Query1 = Query2 = Query3: Determine the stability of the system near an equilib-

rium point.

Form1 comprises:

1. A set of nonlinear, time-invariant9 differential equations ˙x(t) = f (x(t)) of n vari-

ables.

2. An equilibrium point for the nonlinear system.

Form2 is a set of time-invariant linear differential equations of n variables that approxi-
mate Form1 at an equilibrium point. Form2 has the same set of variables and the same

9 The dynamic behavior of a continuous system of n variables is described by a set of differential equations of
the following general form: ˙x(t) = f (x(t), t). The system is said to be time-invariant when the functions f do
not depend explicitly on time, i.e., ˙x(t) = f (x(t)).

182

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

number of equations as Form1. The system matrix of Form2 is the Jacobian of Form1
evaluated at the equilibrium point.

Form3 is Form2, the positions of its eigenvalues in the complex plane, and the result (i.e.,
one symbol of {stable, unstable, unknown}).

Assmpt1: None.

Assmpt2 = Assmpt3: Form2 is a valid approximation of Form1 near the equilibrium
point, i.e., linear terms in Form1 do not vanish in its vicinity.

The reformulation techniques R1 and R2

R1 transforms a nonlinear system to a linear system. It has no conditions, and the pro-

cedure is as follows:

Proc1: Compute and evaluate the Jacobian of Form1 at the equilibrium point. Construct

a linear system with the Jacobian as the system matrix.

R2 determines the stability of Form2. It has no conditions, and the procedure is as follows:

Proc2: Compute the eigenvalues, λi , of the system matrix of Form2. Then, the stability

is inferred as follows:
• If at least one eigenvalue is found to be in the right-hand side of the complex

plane (∃i, Re(λi) > 0), the system is unstable.

• If all eigenvalues are in the left-hand side of the complex plane (∀i, Re(λi ) <

0), the system is stable.

• If all eigenvalues are in the left-hand side of the complex plane, but at least one
has a zero real value (∃i, Re(λi ) = 0), then no conclusions can be drawn about
the stability, and one must analyze the higher-order terms of the function f .

Evaluators and effects

Evalsprob: As linearization is motivated by the lack of general method for deter-
mining stability of a nonlinear system, we can categorize it as an example
of engine-driven reformulation. More speciﬁcally, an evaluator of a problem
(Evalsprob(P ) for P1 and P2) is the existence of a general, computable pro-
cedure for determining stability.
Other aspects of the problem that are changed by linearization are accuracy and
generality of a formulation as a description of a physical system. Accuracy and
generality of a formulation with respect to the physical behavior that it describes
are elements of Evalsprob(P ) for P = P1 and P2. Presumably, P1 is more ac-
curate than P2, but the discrepancy between the two descriptions decreases as
the equilibrium point is approached. Accuracy is closely related to generality if
generality is deﬁned as the subspace in the n-dimensional space in which a for-
mulation is valid as a description of the system. P1 is a valid description in the

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

183

entire space in which the system is deﬁned, while P2 is only valid in the vicinity
of the equilibrium point.

Evalsp+r: The two reformulation-steps, R1 and R2, in the linearization strategy can be
evaluated with respect to the complexity of their procedure, Proc1 and Proc2,
relative to the problem encoding, Form1 and Form2, respectively. The worst-
case complexity of Proc1 is O(n3) where n is the number of variables in the
system. For R2, which requires the computation of the eigenvalues of the system
matrix, it is also O(n3) in general.

Evalsstrat: The two strategies represented in Fig. 12 as the path (cid:5)P1, R1, P2, R2, P3(cid:6),
involving linearization, and the path (cid:5)P1, R3, P4(cid:6), involving a Liapunov function,
can be evaluated in terms of their computational cost, which is in Evalsstrat.
The cost of the former is proportional to n3 as can be seen from the complexity of
its component procedures, while the cost of the latter is a constant. The two strate-
gies can also be characterized in terms of their generality, also in Evalsstrat:
Linearization is justiﬁed in cases where the linear terms do not vanish while Lia-
punov function can be used only in cases where one is known.

Discussion. Linearization is one of the most important approximation tools for modeling
the behavior of complex systems. Though basic equations of physics are nonlinear, lin-
earization is a practical necessity since so little of general nature is known about nonlinear
systems. Also, there are many situations where linear approximation is adequate for most
purposes. Linearization is particularly useful for stability analysis since one is interested
in the behavior of the system in a limited region where the linear terms tend to dominate.
Since it is motivated by lack of other methods to analyze nonlinear systems, we can classify
this reformulation as engine-driven.

Though linearization is performed only around an equilibrium point in the case dis-
cussed in this section, linear approximation can be used more generally to reason about
the global characteristics of nonlinear dynamic systems. In piecewise linear approxima-
tions of nonlinear ordinary differential equations, the entire space of behavior is divided
up into regions such that a different linear equation is used to approximate the behavior in
each region (e.g., [23,30]). The characteristics of the behavior of the original system can be
inferred by analyzing each linear region separately and by piecing the inferred behaviors
together. Sacks’ PLR system [30] analyzes nonlinear dynamic systems using piecewise
linear approximations. Given a nonlinear system, PLR semi-automatically breaks up the
space into regions and proposes a linear approximation for each region using information
about the original function (such as local extrema) as well as user input. Then, it analyzes
the behavior within each region and pieces them together to draw a phase diagram of the
entire space. In the process, PLR reﬁnes the initial linear approximations as necessary to
detect all the qualitatively signiﬁcant features of the behavior. Reasoning by PLR is ex-
pensive; the computational complexity of the inequality reasoning employed by PLR is
exponential in the length of its input. However, the expense is justiﬁable since nonlinear
dynamic systems can otherwise resist analysis altogether.

Breaking up a complicated behavior into smaller pieces each of which can be approxi-
mated by a simpler behavior is a common strategy employed by human experts analyzing

184

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

Fig. 13. Caricatural modeling.

a complex system. The reformulation technique discussed next also follows this strategy
in solving high-order algebraic equations.

5.2.3. Caricatural reasoning

In [41], Williams and Raiman study a reformulation technique used by analytical
chemists for analyzing a set of nonlinear, high-order algebraic equations such as those
describing the equilibrium behavior of a chemical reaction. High-order nonlinear equa-
tions10 are hard to solve in general, and, furthermore, the complexity of such systems of
equations makes it difﬁcult to gain clear insight into the space of possible solutions. They
introduce Caricatural reasoning as a technique for decomposing a problem space into a
set of regimes, each of which is deﬁned by a set of assumptions about the relative orders
of magnitude of certain terms. Such decomposition not only facilitates understanding of
the general shape of the solution space but also gives rise to a set of simpliﬁed equations
for each regime to approximate the solution in the regime. Since this reformulation oper-
ates solely on equations and variables, we place it among reformulation techniques for the
Equation Processing Stage.

Fig. 13 depicts the entire problem-solving strategy as a two-step process of decomposi-
tion (R1) followed by solution of a simpliﬁed set of equations (R2). Once the problem is
decomposed into regimes (as shown in P2), one can select a regime whose conditions are
satisﬁed by the set of givens and solve the simpliﬁed equations.

The problems P1, P2, and P3

Query1 = Query2 = Query3: Determine the values of the unknown variables.

Form1: A set E of nonlinear algebraic equations involving a set G of given parameters

and a set X of unknown variables.

Assmpt1: None.

Form2: A set of sets of simpliﬁed equations. Each set of equations is accompanied by a
set of validity conditions about the relative orders of magnitudes of the parameter
values.
Given values of the parameters in G.

10 When the degree of the equations is equal to or higher than ﬁve, there exist no general closed form solutions
(by Galois).

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

185

Assmpt2: None.

Form3: A set of simpliﬁed equations in a chosen regime.
Computed values of the unknowns in X.

Assmpt3: The given values of the parameters in G satisfy the validity conditions of the

chosen regime.

The reformulation techniques, R1 and R2

The entire problem-solving strategy consists of a sequence of two reformulation

processes, R1 and R2.

R1 replaces the original equation model by a patchwork of sets of simpliﬁed equations
and determines their validity conditions. R1 has no conditions. The procedure of R1 relies
heavily on a process called qualitative resolution to simplify and solve equations using
orders of magnitude information. R1 employs the machinery of Minima [40] for mixed
qualitative and quantitative algebraic reasoning and Estimates [27] for reasoning about
orders of magnitude information.

Proc1 consists of the following ﬁve steps:

1. For each equation in E, a set of caricatural assumptions is generated. An assumption
is generated by extracting from the equation the relative strength of the terms and by
exaggerating this feature. Each assumption is in the form of term1 (cid:2) term2, where
term1 and term2 are terms appearing on the opposite sides of the equation.

2. A set C of all consistent combinations of caricatural assumptions is generated. Each

such combination is called a dominance regime.

3. For each dominance regime c ∈ C, a set E(cid:9) of simpliﬁed equations is generated from
c and E. Each set of simpliﬁed equations holds under the assumptions deﬁning the
dominance regime.

4. For each dominance regime, a qualitative resolution mechanism is applied to the set of
simpliﬁed equations to generate equations each containing only one unknown variable.
5. The validity conditions for each dominance regime are computed. Validity conditions
are a set of relations involving only the parameters in G, and they are conditions that
can be tested once the values of G are provided to select an appropriate dominance
regime. Validity conditions are derived from c and E(cid:9) using repeated qualitative reso-
lution to eliminate all the appearances of unknown variables from c.

Once a problem is simpliﬁed by R1, it is solved by R2 for a speciﬁc case. Given a
speciﬁc set of values for the parameters in G, Proc2 selects an appropriate dominance
regime and solves the simpliﬁed equations for the unknown variables using any available
mechanism for solving algebraic equations. R2 has no particular conditions.

Evaluators and effects

Evalsprob, Comparsprob: The most immediate and obvious effect of caricatural rea-
soning is on the compactness of the formulation, which is in Evalsprob. Caricat-

186

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

ural reasoning makes a formulation less compact. While the original formulation
describes the entire behavior with one set of equations, reformulation results in a
long conjunction of conditionalized sets of equations.
A more important effect of caricatural reasoning is to simplify the equations. Sim-
plicity of equations, which is also in Evalsprob, is measured in this case by the
number of terms and the degrees of equations. Caricatural reasoning simpliﬁes
equations in these aspects since the addition of assumptions often results in the
removal of terms after the qualitative resolution of equations, though in some
cases an additional assumption results in no such simpliﬁcation.
Simpliﬁcation also has obvious implication on the solvability of a set of equa-
tions (also in Evalsprob). Simplicity in the above sense will generally make
equations easier to solve and even make hitherto unsolvable ones to have close-
form solutions. Note, however, that there is no guarantee that simpliﬁcation will
in fact make the equations solvable.
Another aspect of a formulation that Williams and Raiman emphasize is the cog-
nitive transparency, which is also in Evalsprob. Cognitive transparency of a
formulation refers to ease of deriving from the formulation insight into the behav-
ior of the whole system. Though this aspect is clearly subjective and difﬁcult to
deﬁne precisely, it is an important objective in many examples of reformulation.
In the case of caricatural reasoning, Williams and Raiman argue that decomposi-
tion of the problem space and simpliﬁcation of equations lead to more cognitive
transparency—an argument that seems to be borne out by the chemistry example
that they present.
Since caricatural reasoning is an approximation technique, the answer to the query
will be less accurate as a result. Accuracy of an answer is in Evalsprob. The
magnitude of the error will depend on the actual parameter values and the regime
selected to compute the answer. Williams and Raiman note that they are unable
to bind the error, and they recognize the importance of extending their method in
this direction.

Evalsp+r+p: An aspect of caricatural reasoning that

is repeatedly mentioned by
Williams and Raiman is that it preserves nonlinearity of equations, which is
in Evalsp+r+p. However, their own example shows that nonlinearity is not
necessarily preserved in all regimes. Stated more broadly, their objective is that
“approximation should preserve essential characteristics of the behavior”. This is
another qualitative evaluator that is difﬁcult to deﬁne precisely.

Evalsp+r: Caricatural reasoning could be evaluated in terms of its computational cost,
but the actual cost will depend on the types and size of the system of equations.
The authors do not provide complexity analysis of the steps in the reformulation
procedure. Even if the decomposition step (R1) is expensive, it may be cost-
effective if the cost can be amortized over a large number of actual problems
solved (R2).

Discussion. The most signiﬁcant contribution of caricatural reasoning is that it proposes
an automatic method to identify a meaningful decomposition of the problem space where
such decomposition is not immediately obvious. In contrast, in both linear approximation

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

187

Fig. 14. Comparative analysis by exaggeration.

of nonlinear systems (discussed in Section 5.2.2) and aggregation of nearly decomposable
systems (discussed in Section 5.2.1), a reasonable way to decompose is fairly obvious from
the original system. Furthermore, despite the fact that decomposition does not necessarily
guarantee that the resulting equations will be solvable, the decomposition itself is of much
value in this case since it provides a person with insight into the space of solutions and
helps her identify interesting families of solutions.

Caricatural reasoning achieves such decomposition through exaggeration of relative
magnitudes of terms in equations. Exaggeration is also the modus operandi of the reformu-
lation technique we turn to in the next section. However, exaggeration in the next section
has the effect of complicating a model, while it is used here for the purpose of simpliﬁca-
tion.

5.2.4. Exaggeration

Exaggeration is a technique proposed by Weld [36] for performing comparative analysis
[35]. Comparative analysis predicts how the behavior of a system will change as a result of
the perturbation, given a model of the system, its behavioral description and a perturbation
to the model. Weld proposes two techniques for comparative analysis, namely Qualita-
tive Differential Analysis and Exaggeration in [35]. Exaggeration is of particular interest
here since it involves a unique type of reformulation based on a change in the underlying
mathematics.

Exaggeration answers a comparative analysis question by taking perturbation to the
limit and comparing the resulting behavior with that of the original. It proceeds in three
steps, which Weld calls transformation, simulation, and scaling. First, the transformation
step reformulates a given qualitative model into a qualitative model in the space of hyper-
real numbers with some of the variables being given extreme values such as inﬁnite or
inﬁnitesimal according to the given perturbation. Second, the simulation step carries out
a qualitative simulation using HR–QSIM, which is QSIM, a widely used qualitative sim-
ulation program [14], extended with hyper-real numbers. Finally, the scaling step infers
the effects of the perturbation on the behavior by comparing the predicted behavior of the
transformed model to that of the original model obtained by QSIM.

We focus on the transformation step in our discussion. Unlike any other reformula-
tion techniques discussed in this paper, this reformulation involves very little syntactic
change since the essence of the transformation is a change in the model of numbers un-
derlying the formulation from the standard model to the nonstandard one. Exaggeration is
also unique in that the reformulation results in a more complex formulation than the orig-
inal one while most other reformulation techniques result in simpliﬁcation. Complication
of the formulation serves the present goal of comparative analysis by enabling comparison
of the behaviors of the two formulations.

188

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

The problems

Query1 = Query2: What is the consequence of the given perturbation on the behavior

of the system?

Form1 comprises:

1. A mathematical model in the form of qualitative differential equations (QDEs),
2. Initial state description, and
3. The perturbation.

Form2: QDEs in the hyper-real space, where some of the variable values are given ex-
treme values. The choice of such variables and their values depends on the given
perturbation.

Assmpt1 = Assmpt2: None.

The reformulation technique

Proc1: Reformulation of a standard QDE to a non-standard QDE requires little syntactic
change in the formulation. An important change in the formulation takes place in
the semantic level since, in Form1, all the variables and operators, which used
to be deﬁned over the reals, are now deﬁned over the hyperreals. The difﬁcult
part of the reformulation procedure is deciding how the perturbation is to be re-
ﬂected in a value of a variable in the nonstandard QDE. Weld does not provide a
deterministic procedure for making this decision in general. His implementation
works correctly for cases where the decision of which and how parameters should
be perturbed is straightforward, and it will not work for cases where the decision
requires more sophisticated reasoning. Weld describes the three parts of this as
follows:
1. Choose a parameter to exaggerate. This choice is obvious when the pertur-
bation is differential in nature (i.e., question of the type ‘what happens if the
value of x is increased (or decreased)?’) However, when the perturbation is of
a different nature, such as a change in the physical conﬁguration, there is no
established procedure for the selection. Weld’s implementation handles only
differential perturbations.

2. Choose the direction in which the parameter should be perturbed. There is
no well-deﬁned procedure for selecting the direction, but there are only two
possibilities, increase or decrease.

3. Select the ﬁnal value for the perturbed parameter. In some cases, this decision
is trivial, either inﬁnity or inﬁnitesimal depending on the direction of pertur-
bation. However, there are cases for which this does not work. Weld states ‘the
parameter should be transformed to the shortest distance that causes some pa-
rameter to reach an inﬁnite or inﬁnitesimal value’ without providing a general
procedure for doing so.

Cond1: None.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

189

Evaluators and effects

Evalsprob: An immediate effect of Exaggeration on the formulation is on its complexity,
which is in Evalsprob. Though there is little syntactic change in the qualitative
differential equations themselves, the shift to a nonstandard model increases the
number of landmark values in the quantity space of each qualitative variable. For
each landmark value l that is not +inf or −inf in the original quantity space of a
variable, the new quantity space include (HALO l−) and (HALO l+)11 in addition
to l. Therefore, the complexity of the formulation measured in terms of the size
of the quantity space of variables increases as a result of the reformulation.

Evalsp+r: This increase in complexity of the quantity space results in a more complex
predicted behavior. The state tree produced by QSIM from Form1 and HR–QSIM
from Form2 are in general not identical because the former is operating in the
space of standard numbers and the latter in the space of hyperreals, which results
in a tree with a higher branching factor. Consequently, care must be taken to de-
termine what differences in the predicted behavior are indicative of the effects of
the perturbation and what are due to the difference between QSIM and HR–QSIM.
The computational complexity of this scaling step is exponential in the depth of
the behavior tree but Weld states that it does not in general constitute a problem
since the constant factor is very low compared to that of simulation.

Evalsstrat: Exaggeration can be evaluated as a strategy for comparative analysis with
respect to its cost, its soundness, and the quality of explanations it provides
(Evalsstrat). The overall cost of the Exaggeration can be computed as the
combination of the costs of the three steps. Although the reformulation step itself
is tractable, both QSIM12 and HR–QSIM13 have a worst-case exponential com-
plexity. In his extensive analysis and comparison of Exaggeration and Qualitative
Differential Analysis [35], Weld points out that the inference made by the ex-
aggeration method is unsound. This is not to say that HR–QSIM is unsound but
that the inference made to answer the comparative analysis question by compar-
ing the two formulations is unsound since it assumes that the system responds
monotonically to perturbations, an assumption that does not necessarily hold. He
also shows that while DQ analysis has many computational advantages over exag-
geration, such as soundness and computational tractability, exaggeration produces
more intuitive and simpler explanations.

11 (HALO l−) and (HALO l+) are the notations that Weld used to represent the qualitative values corresponding
to the left and right half of the halo of l. They are the sets of all nonstandard numbers that are inﬁnitely close to l
but that are less than or greater than l.
12 Cost of the qualitative simulation by QSIM: In general, the time to produce the set of successor states for a
given state is linear with respect to the size of the model. However, since qualitative simulation can suffer from
intractable branching, the number of states produced for a given depth T of the tree can be exponential with
respect to T .
13 The cost of the qualitative simulation by HR–QSIM is similar to that of QSIM except that the branching factor
of the tree can be higher than that of QSIM. (However, Weld states that intractable branching occurred only in a
few pathological cases out of 50 problems tried.)

190

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

Discussion. One may question whether the transformation phase in Exaggeration should
in fact be regarded as reformulation since nothing on the surface of the formulation
changes. We claim that it is a bona ﬁde example of reformulation since a profound transfor-
mation takes place in the semantics of the formulation.14 Another example of reformulation
involving a change in the underlying model of numbers is switching between discrete and
continuous domains in interpreting equations. Approximating a discrete model by a con-
tinuous one (or vice versa) is a practice that is often useful but sometimes dangerous.
Switching between standard and nonstandard models of numbers appears less dangerous
by virtue of the transfer principle, which allows inferences made in the nonstandard uni-
verse to be applied to the standard universe,15 but as the difﬁculties in the scaling phase
of Exaggeration discussed by Weld suggest, care must be taken in interpreting the results
obtained using nonstandard models.

5.3. Reformulation at the solution processing stage

In this section, we review two reformulation techniques that operate with QSIM to make
its results more understandable. Given a set of qualitative constraints called qualitative dif-
ferential equations (QDE) governing the behavior of a system, QSIM predicts its possible
behaviors by iteratively generating a node that describes its qualitative state then using the
qualitative constraints to infer the set of next qualitative states that could follow the current
one [14]. The paths in the behavior tree correspond to possible behaviors of the dynamic
system, and thus represent solutions to the qualitative differential equations. The biggest
problem that limits the usefulness of a qualitative simulator such as QSIM is the complexity
of the results produced. While the ability to predict behavior from an imprecisely speciﬁed
model is quite valuable, the imprecision also leads to a high degree of ambiguity in the re-
sult. A typical QSIM output is a highly branching tree of states representing a large number
of possible courses of behavior. Complexity makes it difﬁcult for a person to extract useful
patterns of behavior from such output.

The goal of the ﬁrst example, discussed in Section 5.3.1, is to generate an explana-
tion. The goal of the second one, discussed in Section 5.3.2, is to eliminate insigniﬁcant
branching in qualitative simulation. The former ﬁts in a straightforward fashion into our
framework for reasoning about physical systems (Fig. 1): QSIM is used to solve the quali-
tative differential equations (Equation Processing Stage), and the reformulation procedure
is applied to the generated behaviors (Solution Processing Stage). The problem solving
process involving the second reformulation technique is more complex: the reformulation
of the solution is interleaved with equation solving, producing a more compact behavior

14 The surface similarity between the two formulations before and after transformation is a simple artifact of the
practice in nonstandard mathematics to use the same set of symbols for arithmetic operators as those in standard
mathematics.
15 A general form of transfer principle can be stated informally as follows (from “Standard and Nonstandard
Analysis” by R.F. Hoskins, Ellis Hoswood Limited, 1990): “Every properly formulated proposition with bounded
quantiﬁers about standard objects [. . .], will be true in the standard universe if and only if the corresponding propo-
sition (suitably re-interpreted if necessary) about their nonstandard counterparts [. . .] is true in the nonstandard
universe”.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

191

Fig. 15. Solution reformulation.

tree and enabling the simulation engine to carry out simulation further than otherwise pos-
sible.

5.3.1. Behavior abstraction for explanation

In [19], Mallory et al. make the results produced by QSIM easier to understand by
summarizing along user-speciﬁed dimensions. Their goal is to support individual users in
interpreting QSIM results. Fig. 15 illustrates their approach. Given the aspects of behavior
the user is interested in (such as a combination of variable values and their directions
of change), the reformulation procedure extracts a graph of abstract states from a QSIM
state tree. The procedure examines the states in the original tree, discards unnecessary
information from them, and aggregates adjacent nodes and arcs that are indistinguishable
from the perspective speciﬁed by the user. The result is an abstract graph that reveals only
information that the user wants to see. The result is also much smaller so that it can be
easily inspected visually to identify signiﬁcant patterns of behavior such as oscillation.

The problems, P1 and P2

Query1 = Query2: Predict the qualitative behavior of the given system over time.

Form1 comprises:

1. A state tree produced by QSIM. Each node of the tree represents a qualitative state of
the system and an arc a transition. Each path through the tree corresponds to a possible
behavior over time.

2. A method for labeling nodes. This method speciﬁes the dimensions of the behavior
along which aggregation is to take place. A typical method may specify the values and
directions of change of a subset of the variables, but it could potentially be any method
for labeling.

Form2: A graph of abstract states. Each node corresponds either to a node or to an aggre-
gate of nodes in Form1. Likewise, for the arcs. In addition, each node contains
only information speciﬁed by the labeling method.

Assmptn1 = Assmptn2: The same assumptions underlying QSIM.

The reformulation technique, R

Proc comprises the following steps:

1. Label each node in the tree by the given labeling method.

192

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

2. Generate a graph by aggregating nodes that are adjacent and have the same labels. Arcs
are also aggregated as necessitated by aggregation of the nodes. The exact conditions
for merging nodes and arcs are speciﬁed in [19], but roughly speaking, two nodes
are considered adjacent if “one is the successor of the other or if they share common
immediate predecessors or successors”.

Cond: None.

Evaluators and effects

Evalsprob, Comparsprob: The primary objective of Mallory’s technique is to improve
the understandability of the generated behavior by projecting it over the dimen-
sions speciﬁed by the user, which also reduces signiﬁcantly its size. With respect
to Size, which is be measured by the number of nodes and arcs in the graph, the
size of the abstracted behavior graph is guaranteed to be equal to or smaller than
that of the original one. Understandability is more difﬁcult to quantify, but
one aspect of it is the ratio of the amount of information that is pertinent to the
user’s speciﬁed interest to the total amount of information in the formulation. This
measure is also in Evalsprob(P ), and its value for P2 is always 1 as the authors
prove that the abstracted behavior graph is guaranteed to keep only those states
pertinent to the query, while the value for P1 is likely to be much smaller. Finally,
as an observation with respect to understandability, the authors report that the user
has to experiment with different speciﬁcations of the labeling method in order to
reach a satisfactory summary of the behavior of interest.

Evalsp+r+p: The authors evaluate the reformulation process with respect to Sound-
ness and Completeness. The authors deﬁne soundness to be that any path in
the abstract graph should correspond to at least one path in the original tree, and
completeness to be that any path in the original tree should correspond to some
path in the abstract graph. They prove that an abstract graph produced by their
procedure is always sound and complete with respect to the original tree.16
Evalsp+r: The reformulation procedure is evaluated with respect to its complexity,
which is polynomial in this case with respect to the size of the state tree, as-
suming that the complexity of the labeling method is no more than polynomial
with respect to the number of variables.

Discussion. Most solution reformulation techniques aim to facilitate the interpretation of
complex results produced by other programs through additional processing such as summa-
rizing, graphing, generating natural language explanations, and extracting speciﬁc aspects
of the results [11]. The work discussed in this section is a clear example of such techniques,
and though what is produced is another graph representing behaviors, it provides a good
basis for generating high-level explanations. In fact, the authors state that their ultimate
goal is to produce a natural language explanation of qualitative behavior.

The technique is based on a simple idea but the approach is quite general and allows a
wide range of perspectives for summarizing since one can basically provide any piece of

16 Thus, the procedure deﬁnes a surjective mapping between the original tree and the reformulated graph.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

193

code as a method for labeling the nodes. It places the burden of specifying the perspective
squarely on the user’s shoulder, which may be inevitable since it is impossible to anticipate
all the different perspectives from which users may wish to examine a given result.

5.3.2. Chatter elimination in qualitative simulation

In [3], Clancy and Kuipers address the problem of chatter in qualitative simulation. The
phenomenon of chatter in qualitative simulation is one of the important factors that un-
necessarily undermine the applicability qualitative simulation and complicate its results.
Chatter happens when the direction of change in a variable is constrained only by con-
tinuity. Since variables in such a situation are free to alternate among the three possible
directions of change, namely increasing (inc), steady (std), and decreasing (dec), this
may lead to intractable branching even though such branching reveals no signiﬁcant infor-
mation about the possible behaviors. The situation can be even worse since any number of
variables may be unconstrained. Branching due to chatter may cause the size of the behav-
ior tree to become inﬁnite, limiting the range of behaviors that can be explored by QSIM
[14]. Clancy and Kuipers propose two techniques, Chatter Box Abstraction and Dynamic
Chatter Abstraction, each of which can be used to eliminate all occurrences of chatter in
the behavior tree. We focus on Chatter Box Abstraction in our discussion since it can be
more clearly viewed as reformulation than Dynamic Chatter Abstraction.

To eliminate chatter during QSIM simulation, Chatter Box Abstraction is carried out
every time simulation enters a potentially chattering state. Simulation is ﬁrst suspended,
analysis is carried out to determine the chattering variables, and abstract states are inserted
into the behavior tree, before simulation is resumed. Fig. 16 illustrates the strategy of alter-
nating simulation and reformulation as a path leading from P0 to Pn. As a result of Chatter
Box Abstraction, regions of a behavior tree that can include a large number of chattering
states and, thus, potentially represent an inﬁnite number of distinct behaviors, are replaced
by abstract nodes, extending the range of models that can be simulated by QSIM.

The problems, P1 and P2

Query1 = Query2: Predict the behaviors of the physical system.

Form1 comprises:

Fig. 16. Chatter elimination.

194

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

1. A set of Qualitative Differential Equations (QDEs).
2. A partial behavior tree in which one of the states indicates the possibility of chattering.
(The condition for detecting this possibility is given below as the condition of the
reformulation procedure.)

Form2 comprises:

1. The same set of Qualitative Differential Equations (QDEs).
2. A partial behavior tree that is the same as above except for new abstract states that

replace the chattering states and their successors.

Assmptn1 = Assmptn2: The same assumptions underlying QSIM.

The reformulation technique, R

Before simulation starts, the constraints in the QDE are analyzed to identify the vari-
ables that can potentially chatter and those that can never chatter throughout simulation.
Variables that can chatter are further grouped into sets called chatter equivalency classes,
such that if any one variable in a class starts to chatter, all the remaining variables in the
class will also chatter.

Proc: The reformulation procedure consists of the following two steps. The ﬁrst step
determines which variables are actually chattering, and the second step creates
abstract states when chattering is detected.
1. Determination of variables that exhibit a chattering behavior is accomplished
by a recursive call to QSIM to explore a limited region of the state space. In
the recursive call, only the directions of change of the potentially chattering
variables are allowed to change. The result is analyzed for existence of chatter.
Chattering is detected if there are cycles in the graph of states that are only
distinguished by the directions of change in some variables.

2. Once chattering is detected, the states involved in the cycles are summarized
into one abstract state, where the direction of change of each chattering vari-
able is represented by a list of possible directions (inc, std, dec). The
successor states (i.e., the states that are successors of the states included in
the cycles but are not themselves in any cycle) are also abstracted in the same
manner. In other words, if any successor states are different from each other
only in the direction of change in the chattering variables, they are summarized
into one abstract state. Abstract states are inserted into the original behavior
graph and simulation resumes.

Cond: The reformulation procedure is triggered when a potentially chattering variable is

changing within a time-interval state.

Evaluators and effects

For the purpose of assessing the beneﬁts the reformulation, what should be evaluated in
the case of Chatter Box Abstraction is not the changes brought about by individual deploy-
ment of the reformulation procedure but the cumulative effects of repeated deployment

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

195

on the ﬁnal result as well as on the computational effectiveness. Thus, we must compare
Pn and Pm as well as the strategies represented by the paths leading to them from P0 in
Fig. 16.

Evalsprob, Comparsprob: The main effect of Chatter Box Abstraction is on the com-
pactness of the state tree. Compactness can be measured in terms of the number of
distinct behaviors (i.e., paths through a state tree). Clancy and Kuipers empirically
evaluate the effect of the technique with respect to this measure and demonstrate
1 to 3 orders of magnitude reduction in the number of behaviors predicted (Ta-
ble 1 in [3]). Such reduction is signiﬁcant in itself, but even more signiﬁcant is
the fact that it makes the result of simulation by QSIM much easier for a person
to comprehend. Cognitive transparency is also in Evalsprob, and even though
it is difﬁcult to quantify, it is an important aspect of a formulation that could de-
termine the usefulness the overall problem solving effort. In addition, Clancy and
Kuipers evaluate the abstracted tree Pn by its correctness with respect to Pm. They
guarantee that the set of trajectories in Pn is equal to the set in Pm with respect to
the non-chattering variables and that it is a super-set with respect to the chattering
variables.

Evalsp+r: The evaluator here is the computational complexity of Proc applied to
Form. Each time Proc is invoked, the number of states explored in step 1 of
the procedure is exponential in the worst case with respect to the number of chat-
tering variables.

Evalsstrat: By eliminating the behaviorally insigniﬁcant distinctions among states,
Chatter Box Abstraction cuts down on the branching factor of the behavior tree
and enables QSIM to explore a much larger portion of the state space than other-
wise possible for given computational resources. Interestingly, the whole compu-
tational process of simulation remains intractable with or without reformulation.
However, Chatter Box Abstraction improves computational effectiveness by en-
abling QSIM to avoid the futile effort of expanding inﬁnite sequences of chattering
states, thus using resources more efﬁciently. One way to measure computational
effectiveness is would be to measure the ratio of the amount of time wasted on
expanding chattering states versus the overall processing time.

Discussion. Unlike Chatter Box Abstraction, discussed above, Dynamic Chatter Abstrac-
tion [3] does not make recursive calls to QSIM to determine the chattering variables.
Instead, it relies on the current state and the constraints to determine if the current state
can lead to chattering. Dynamic Chatter Abstraction is more tightly integrated into the
qualitative simulation process, and though the algorithm of Dynamic Chatter Abstraction
is still exponential with respect to the number of chattering variables, the experimental
results reported by the authors show that the it can outperform Chatter Box Abstraction
by a factor of 4 to 10. Though it is devised to accomplish the same goal as Chatter Box
Abstraction, Dynamic Chatter Abstraction basically eliminates the need to reformulate by
spending more resources analyzing the current state and the constraints to avoid generating
chattering states.

196

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

Chatter Box Abstraction and Dynamic Chatter Abstraction along with the behavior ab-
straction technique discussed in Section 5.3.1 present an interesting spectrum of techniques
for reducing complexity. On one extreme, behavior abstraction tries to simplify after a
large complex result is produced. On the other extreme, Dynamic Chatter Abstraction tries
to avoid producing unnecessarily complex results in the ﬁrst place by doing more analy-
sis during simulation. Chatter Box Abstraction is somewhere in between as it interleaves
simulation and reformulation, trying to limit the area of behavior space that needs to be
reformulated at any one time.

5.4. Discussion

Reformulation techniques vary widely with respect to their goals, the types of formula-
tions they operate on, their actual effects on these formulations, their cost, etc. Having re-
viewed a variety of reformulation techniques, we now summarize our ﬁndings and present
general observations. Table 2 summarizes ﬁve key aspects of the reformulation techniques
examined in this paper. The ﬁrst column indicates the purpose of a technique, which is
the ultimate goal of the authors of the technique. The second column indicates the type of
formulation to which each technique is applicable. The effect column describes the princi-
ple change brought about by reformulation. The procedure column summarizes the major
steps for realizing the change. The last column lists the key information, besides a repre-
sentation of the system being modeled, that each technique relies on in order to carry out
the steps. In addition, the table indicates the reasoning stage at which each technique is
applied.

5.4.1. Purpose of reformulation

The purposes listed in Table 2 for each technique are the ones stated by its authors. In
general, the purpose of a reformulation falls in one of two categories; namely, facilitating
human understanding and facilitating problem solving. The former category encompasses
the techniques that seek better explanations and cognitive insight, and the latter includes
those aimed at reducing computational cost and making hitherto unsolvable problems into
solvable ones. The goals of these two categories are not mutually exclusive and some
reformulation techniques actually achieve the objectives of both categories. For example,
a technique that reduces the size of a formulation may decrease the cost of ﬁnding the
solution and make the formulation easier to understand at the same time. Exaggeration is
an oddball among all the techniques. Its purpose, which is to compare the behavior of a
reformulated model with that of the original one, does not fall in either category. We are
not aware of other examples of this type although they may exist.

We must point out that there is sometimes a leap of faith between the purpose of a
reformulation technique as stated by its authors and what it actually achieves. This is es-
pecially true in cases where improvement of human understanding is the claimed purpose
of reformulation. None of the works we surveyed actually demonstrates improved under-
standability of the resulting formulation. Those that aim to generate better explanations do
not actually generate explanations, but it is hoped that their reformulation improves the
quality of the explanations to be generated.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

197

Table 2
Summary of the reformulation technique

Technique

Purpose

Formulation
type

Effects

Procedure

Key information Stage

Model
Simpliﬁcation
(Section 6.1.1)

Critical
Abstraction
(Section 6.1.2)

Aggregation
(Section 6.2.1)
Linearization
(Section 6.2.2)

Better
explanation

Model
fragments

More
approximate
model

Replace by
approximate
model fragments

Better
explanation

Components,
Behaviors,
Interactions

Cost reduction Equations

Solvability

Equations

Less equations/
variables

Aggregate paths.
Eliminate parts

Less equations/
variables
Linear equations Focus.

Decompose.
Aggregate

Caricatural
Reasoning
(Section 6.2.3)
Exaggeration
(Section 6.2.4)

Solvability &
cognitive
insight
Comparative
analysis

Behavior
Abstraction
(Section 6.3.1)

Chatter
Elimination
(Section 6.3.2)

Better
explanation &
cognitive
insight
Cost reduction
& cognitive
insight

Equations

Lower degree for
equations

Equations

State graph

Nonstandard
model that allows
exaggeration of
values to extreme

Smaller behavior
graph

State graph

Smaller behavior
graph

Replace by
simpler model
Decompose.
Exaggerate

Replace the
underlying
semantics

Project.
Aggregate (after
simulation)

Aggregate
(during
simulation)

Model

Causal relation
of interest.
Assumption
classes.
Causal relation
of interest

Threshold
value ε
Equilibrium
point

Given
variables

Disturbed
variable and
variable of
interest

Labeling
method

Equation

Solution

5.4.2. Effects of reformulation

The effects of reformulation are the actual changes a particular technique brings about
in a formulation, distinct from the stated purpose discussed above. In the discussion of each
technique, we have identiﬁed what and how aspects of formulations are changed in terms
of a set of evaluators and comparators. The most common effect in all techniques, except
Exaggeration, is what one might generally call ‘simpliﬁcation’. However, as we argued in
Section 2.3, this labeling is insufﬁcient since the exact metrics of simplicity varies in every
case. We list below factors of simplicity used in different techniques. The precise deﬁnition
of simplicity sometimes involves multiple factors. For example, the deﬁnition of simplicity
in Model Simpliﬁcation relies both on the knowledge of approximation relations as well
as that of subset relations of causal orderings among variables.

• Simplicity based on the size of the formulation:

– Fewer variables, equations, and/or causal orderings.

Examples: Critical abstraction, Aggregation, Model simpliﬁcation.

– Fewer states and transitions.

Behavior abstraction, Chatter elimination.

198

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

• Simplicity based on the form of equations:

– Linear vs. nonlinear.

Example: Linearization.

– Degree of equations.

Example: Caricatural reasoning.

• Simplicity based on the concept of approximation:

– Example: Model simpliﬁcation.17

5.4.3. Reformulation procedure

While reformulation procedures are varied, some general types of manipulation emerge
from the examples, which we will call here generic methods. The generic methods we
have observed in our examples are replacement, decomposition, aggregation, and focus-
ing. A reformulation procedure often consists of the application of one or more generic
methods:

Replacement: Examples are Model simpliﬁcation, Linearization, Exaggeration. The re-
formulation consists in replacing a part or the entire formulation by another
formulation. The alternative used must have a well-deﬁned relationship to the
original but may not have been generated by the reformulation technique itself
(e.g., model simpliﬁcation).

Decomposition: Division of the entire problem space into smaller ones. Examples are

Aggregation and Caricatural reasoning.

Aggregation: Parts that are “adjacent” according to some criterion are merged. Examples
are Aggregation, Critical abstraction, Behavior abstraction, and Chatter elimina-
tion.

Focusing: Removing parts of the formulation from consideration to emphasize the remain-
ing parts. Examples are Critical abstraction, Behavior abstraction, and Lineariza-
tion. The labeling method of Behavior abstraction provides the projection mecha-
nism to ﬁlter out uninteresting information about each component. Linearization
focuses on the area around the equilibrium points, while ignoring everything else.

5.4.4. Key information

Every reformulation is guided by a speciﬁc query. All the techniques discussed, ex-
cept Chatter elimination, require some additional information, beyond the representation
of the system modeled, to address only the aspect of the formulation that is relevant to
the query. Three techniques (i.e., Model simpliﬁcation, Critical abstraction, and Exaggera-
tion) require the speciﬁcation of the variables of interest. Model simpliﬁcation and Critical
abstraction require also the speciﬁcation of causal relations among these variables. Lin-
earization requires the selection of an equilibrium point. The labeling method of Behavior
abstraction is a procedural way to specify one’s interest. Aggregation requires the speciﬁ-
cation of a threshold value (ε), which allows one to focus on mid- to long-term behavior.

17 The approximation relations among model fragments are stated a priori by the builder of the knowledge base.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

199

In Chatter elimination, it is implicit that the chattering behavior is uninteresting and needs
to be eliminated.

5.4.5. Processing stage

Since reformulation is simply one step in the larger endeavor of reasoning about phys-
ical systems, it is important to localize where a particular reformulation technique ﬁts in
the entire problem-solving process. Doing so enables one to discover other reformulation
opportunities for achieving the same purpose and to explore the trade-offs of reformulating
at different stages.

On the one hand, reformulating at an early stage has the advantage of having one model-
equations-solution sequence where the relations between the formulations at consecutive
steps are clear. Indeed, if one chooses to reformulate at a later stage, the causal relations be-
tween the original model and the end result of the physical system becomes more complex,
less direct, and can be harder to explain.

On the other hand, if the primary goal of applying a reformulation is to affect the per-
formance of a particular reasoning stage, it may be preferable to reformulate the encoding
closest to the targeted reasoning stage. Reformulating at an earlier stage may prevent us
from controlling the effects of reformulation we are seeking as precisely as we would wish.
For example, Model Simpliﬁcation (Section 5.1.1), which reformulates a model, results
in a consistent model-equations-solution sequence that will be easy to trace and explain.
However, if the primary goal were to produce equations that are easier to solve or solu-
tions that are less complex to interpret, the beneﬁts of Model Simpliﬁcation could be less
predictable.

6. Related work

Various general theories of reformulation, including abstraction and approximation,
have been proposed in the literature. Some of these theories provide an encompassing
high-level characterization. Others restrict their scope to some speciﬁc aspect (e.g., cost
or faithfulness of results). These theories proved to be essential to our understanding of
reformulation, but we found them to be of limited practical use in automating the selection
of reformulation techniques.

Giunchiglia and Walsh [10] introduced a general theory of abstraction applicable to
theorem proving in formal systems, and illustrated their theory in planning and common-
sense reasoning problems. They introduced a formal classiﬁcation of reformulation and its
properties, as well as highlighting their use for both provability and refutation. Importantly,
their theory also encompasses those techniques that may yield inconsistencies. Giunchiglia
and Walsh acknowledged that their terminology does not capture all desirable properties of
abstraction, such as the upward and downward-solution properties proposed by Tenenberg
[34] based on the structure of the proof tree in theorem proving. Plaisted [25], Cremonini
et al. [4], and Nayak and Levy [22] also explored abstraction theories that are restricted to
logical systems and to techniques that preserve consistency and correctness of proofs.

Each of these general theories of abstraction deals with only one, mainly logical, aspect
of reformulation and addresses only a few aspects of change that can be cleanly character-

200

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

ized formally. We ﬁnd that those logical theories are often not sufﬁcient, and sometimes not
useful, for characterizing the types of reformulation used by engineers or those proposed
in the literature about model-based reasoning. Another serious problem with those logical
theories is that they ignore the relevance of problem-solving goals of reformulation, which
we believe is crucial in understanding why and how aspects of a formulation are affected
by reformulation.

The goal-directed nature of reformulation is addressed by Weld and Addanki in [37,39].
Weld and Addanki [39] take a task-driven approach to reformulation and adopt Tenenberg’s
vocabulary [34] for describing the effects of the reformulation on the formulation. In his
paper on approximation reformulations [37], Weld discusses the idea of switching between
two models18 in order to ﬁnd the model whose predictions best match some observations.
In this work, models are organized as nodes in a graph, called a graph of models (GoM),
whose edges represent the approximation relations among models obtained by the elimina-
tion of one assumption. These works are more relevant to reasoning about physical systems
than the general, logical theories of abstraction, since many, but not all, reformulations used
by engineers can be seen as approximations. However, they only deal with only one type
of reformulation, namely approximation. In addition, the goal of approximation reformu-
lations is not to dynamically reformulate, but rather to exploit a ﬁxed set of models and
identify the one best that satisﬁes given criteria.

None of the theories discussed so far makes extensive analysis of complexity issues, nor
do they provide the terminology for quantitatively evaluating the effects of reformulation.
In contrast, the body of research on approximations in the computational complexity com-
munity [24], provides rigorous evaluation criteria with respect to cost. More speciﬁcally,
the reformulation procedure is constrained to logarithmic space mappings, and the goal
pursued is strictly the improvement of the computational cost, while providing a guaran-
teed bound on the quality of the result.19 While focusing on the computational aspects,
these approaches are not concerned with the important aspects of representation and ex-
pressiveness.

Our work comes closest in spirit to [5], where Davis studies approximation and ab-
straction and focuses on the practical application of reformulation techniques applied to
reasoning about solid object kinematics. Davis also stresses that the selection of the re-
formulation technique must be task-driven, in order to satisfy some well-deﬁned criteria.
Our work goes further in providing a practical framework for characterizing and compar-
ing many different aspects of reformulation techniques in the context of a speciﬁc type of
reasoning about physical systems.

7. Conclusions

In this paper we have provided a practical framework for characterizing, evaluating,
and selecting reformulation techniques in the context of reasoning about physical systems.

18 In his paper, a model is a set of equations.
19 In our terminology, this bound can be represented as a comparator the output of two strategies, the one that
computes the exact result and the one that approximates it.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

201

Our framework allows one to express the relevant quantitative and qualitative aspects of
a reformulation process, whereas previous theories addressed either logical properties or
computational aspects. Another important feature of our framework is that it requires one
to explicitly state the effects of reformulation in terms of what and how aspects of the
formulation are changed, which is essential in the selection and evaluation of reformulation
techniques with respect to a given goal.

Using our framework, we have presented an analysis of eight examples of reformulation
techniques applicable at various stages of reasoning. Though we have limited ourselves to
these eight examples in this paper, we are aware of the vastness of the space of refor-
mulation techniques in reasoning about physical systems. In mathematics, one transforms
geometry problems into algebra and vice versa, real analysis into non-standard analysis,
number theory into set theory, and so forth. In engineering, one transforms a time do-
main into a frequency domain, continuous problems into discrete problems, and so forth.
While all the examples in this paper have been discussed in the AI literature, their choice
does not reﬂect our belief that they are somehow the most common or the most important.
Rather, they were chosen because the set represents breadth in terms of the stages of ap-
plication, and also because descriptions of their goals, algorithms, along with examples of
their execution were readily available to us. This is central to this work since our goal is
to characterize reformulation techniques as a speciﬁc computational procedure with well-
deﬁned scope and effects. Furthermore these examples demonstrate the generality of our
framework since they originate from and are useful in a variety of science and engineering
ﬁelds. Both our framework and our narrow focus on the process of reasoning as set out in
Section 3 were essential in undertaking this study since a meaningful comparison of such
a diverse set of techniques would have been impossible without a uniform platform.

From our investigations, we have learned that characterizing a reformulation technique
in terms of what aspects of a formulation are affected and how these aspects are modi-
ﬁed reveals the hidden assumptions underlying the technique. It is essential to uncover and
clearly articulate these hidden assumptions since they determine which aspects of the orig-
inal formulation are targeted by reformulation, which is a prerequisite for automation. We
have learned that it is important to place the reformulation step in the context of the overall
process of reasoning about physical systems in order to precisely predict to what extent the
reformulation step contributes to realizing the goal of the problem-solving task. Indeed, we
have noticed that there is often a leap of faith between the stated goal of a reformulation
technique as presented in a technical paper and how well it achieves the goal. We have also
observed that, despite the diversity of the reformulation procedures that we have studied,
four distinct types of manipulation emerged as generic methods of reformulation.

Our analysis of the eight examples has also shed light on the question of how to select a
reformulation method in a speciﬁc context and for a speciﬁc goal. The selection and design
of a reformulation method requires the consideration of the following issues:

1. What is the goal of the reformulation?
2. What effect should the reformulation have on a formulation of the problem in order
to achieve the goal? In other words, which aspect of the original formulation makes it
difﬁcult to achieve the desired goal? Which components and what aspects need to be
changed and how?

202

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

3. At what stage of the problem-solving process is it most effective to reformulate in

order to achieve the goal?

4. What information, beyond the representation of the system being manipulated, is avail-

able to help focus the reformulation?

5. What cost are we willing to pay for reformulation?

Reformulation is ubiquitous and techniques for reformulation are varied. Given a broad
interpretation of reformulation as any change in the formulation of a problem, every
problem-solving step can be conceived as a reformulation. Nevertheless, in the practice
of reasoning about physical systems, it is useful to distinguish reformulation from other
problem solving steps that make explicit the implications of an existing formulation. Thus,
in the work presented here, we have made the distinction. We were able to do so by restrict-
ing the context of reformulation to a particular type of reasoning about physical systems
with three distinct stages. We believe that restricting the context and narrowing our ﬁeld
has enabled us to lay out a concrete and practical framework for informed comparison of
reformulation techniques that are relevant in reasoning about physical systems.

While our work is still one step away from a formal speciﬁcation of a language for
representing attributes of reformulation to enable automatic selection, we believe that the
analysis of techniques we present here identiﬁes and articulates the features essential to
such automatic selection. There are several directions in which this work can be extended:

• Develop a method for automatically selecting and composing reformulation tech-

niques, relative to a given goal.

• Identify a reasoning task for which a multitude of reformulation techniques exists.
Design and implement a system that consists of a formal representation of the task
and an encoding of these techniques together with an extended set of evaluators. Use
this system to validate our framework and to test its robustness under a variety of
conditions to be selected.

• Extend our framework to encompass engine reformulation. We have deliberately ig-
nored in our research all reformulations that apply to the engine20 itself (be it a
heuristic reasoner, or formal inference rules), because such reformulations do not seem
to arise in the class of problems we address. However, this may not hold for the most
general case, and one may have to examine this issue more closely and decide whether
more attributes are indeed required.

Acknowledgements

This work emerged from the discussions of a reading group on reformulation at the
Knowledge Systems Laboratory of Stanford University. The group also included William
Buchanan, Robert S. Engelmore, Richard Fikes, Tony Loeser, and Todd Neller. We would

20 In [10], Giunchiglia and Walsh consider the deductive machinery, denoted ∆, to be part of the formal system to
which the reformulation is applied. They distinguish between ∆-variant and ∆-invariant reformulation depending
on whether or not the same inference engine is used to solve both the original and reformulated problems.

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

203

like to thank Ernest Davis, Alon Y. Halevy, and the anonymous reviewers for their con-
structive comments. Berthe Y. Choueiry was supported by a fellowship for advanced
researchers from the Swiss National Science Foundation. Yumi Iwasaki was supported
in part by DARPA and the National Institute for Standards and Technology, Rapid Devel-
opment, Exploration and Optimization (RaDEO) program, under cooperative agreement
70NANBH0075. Sheila McIlraith was supported by the Natural Sciences and Engineering
Research Council of Canada (NSERC) and by Xerox Palo Alto Research Center.

References

[1] S. Addanki, R. Cremonini, J. Penberthy (Eds.), Reasoning about Assumptions in Graphs of Models. Read-

ings in Qualitative Reasoning about Physical Systems, Morgan Kaufmann, San Mateo, CA, 1990.

[2] F.G. Amador, D.S. Weld, Multi-level modeling of populations, in: 4th International Workshop on Qualitative

Physics, Lugano, Switzerland, 1990.

[3] D.J. Clancy, B. Kuipers, Static and dynamic abstraction solves the problem of chatter in qualitative simula-

tion, in: Proc. of AAAI-97, Providence, RI, 1997, pp. 118–125.

[4] R. Cremonini, K. Marriott, H. Søndergaard, A general theory of abstraction, in: Proc. of the 4th Australian

Joint Conference on Artiﬁcial Intelligence, Australia, 1990, pp. 121–134.

[5] E. Davis, Approximation and abstraction in solid object kinematics, TR706, 1995.
[6] B. Falkenhainer, K.D. Forbus, Compositional modeling: ﬁnding the right model for the job, Artiﬁcial Intel-

ligence 51 (1991) 95–143.

[7] A. Farquhar, A qualitative physics compiler, in: Proc. of AAAI-94, Seattle, WA, 1994, pp. 1168–1174.
[8] K.D. Forbus, Qualitative process theory, Artiﬁcial Intelligence 24 (1984) 85–168.
[9] K.D. Forbus, The qualitative process engine, in: D.S. Weld, J. de Kleer (Eds.), Readings in Qualitative

Reasoning about Physical Systems, Morgan Kaufmann, San Mateo, CA, 1990, pp. 220–235.
[10] F. Giunchiglia, T. Walsh, A theory of abstraction, Artiﬁcial Intelligence 57 (1992) 323–389.
[11] T.R. Gruber, P.O. Gautier, Machine-generated explanations of engineering models: a compositional model-

ing approach, in: Proc. of IJCAI-93, Chambéry, France, 1993, pp. 1502–1508.

[12] Y. Iwasaki, I. Bhandari, Formal basis for commonsense abstraction of dynamic systems, in: Proc. of the
National Conference on Artiﬁcial Intelligence (AAAI-88), Saint Paul, MN, Morgan Kaufmann, San Mateo,
CA, 1988.

[13] C.A. Knoblock, J.D. Tenenberg, Q. Yang, Characterizing abstraction hierarchies for planning, in: Proc. of

AAAI-91, Anaheim, CA, 1991, pp. 692–697.

[14] B. Kuipers, Qualitative simulation, Artiﬁcial Intelligence 29 (1986) 289–338.
[15] A.Y. Levy, Y. Iwasaki, R. Fikes, Automated model selection for simulation based on relevance reasoning,

Artiﬁcial Intelligence 96 (1997) 351–394.

[16] R. Ling, L. Steinberg, Automated modeling in computational heat transfer, in: Proc. of the AAAI Fall Sym-

posium on Scientiﬁc Modelling and AI, Cambridge, MA, 1992.

[17] C.M. Low, Y. Iwasaki, Model generation and simulation of device behavior with continuous and discrete

changes, Intelligent Systems Engineering 1 (2) (1992) 115–145.

[18] D.G. Luenberger, Introduction to Dynamic Systems: Theory, Models, and Applications, 1979.
[19] R.S. Mallory, B.W. Porter, B.J. Kuipers, Comprehending complex behavior graphs through abstractions,
in: Tenth International Workshop on Qualitative Physics, Fallen Leaf Lake, CA, 1996, pp. 137–146. AAAI
Technical Report WS-96-01.

[20] P.P. Nayak, Causal approximations, Artiﬁcial Intelligence 70 (1994) 277–334.
[21] P.P. Nayak, L. Joskowicz, Efﬁcient compositional modeling for generating causal explanations, Artiﬁcial

Intelligence 83 (1996) 193–227.

[22] P.P. Nayak, A.Y. Levy, A semantic theory of abstractions, in: Proc. of IJCAI-95, Montréal, Québec, 1995,

pp. 196–203.

[23] T. Nishida, K. Mizutani, S. Doshita, Automated analysis of qualitative behaviors of piecewise linear differ-

ential equations, New Gen. Comput. 11 (2) (1993) 159–177.

204

B.Y. Choueiry et al. / Artiﬁcial Intelligence 162 (2005) 145–204

[24] C.H. Papadimitriou, Computational complexity (1994) 299–328.
[25] D.A. Plaisted, Theorem proving with abstraction, Artiﬁcial Intelligence 16 (1981) 47–108.
[26] A. Pos, Automated Redesign of Engineering Models, The Dutch Graduate School for Information and

Knowledge Systems, 1997.

[27] O. Raiman, Order of magnitude reasoning, Artiﬁcial Intelligence 51 (1991) 11–38.
[28] J. Rickel, B. Porter, Automated modeling for answering prediction questions: selecting the time scale and

system boundary, in: Proc. of AAAI-94, Seattle, WA, 1994, pp. 1191–1198.

[29] J. Rickel, B. Porter, Automated modeling of complex systems to answer prediction questions, Artiﬁcial

Intelligence 93 (1997) 201–216.

[30] E.P. Sacks, Automatic qualitative analysis of dynamic systems using piecewise linear approximations, Arti-

ﬁcial Intelligence 41 (1990) 313–364.

[31] H.A. Simon, The architecture of complexity, nearly decomposable systems, in: The Sciences of the Artiﬁcial,

Cambridge, MA, 1969, pp. 99–101.

[32] H.A. Simon, A. Ando, Aggregation of variables in dynamic systems, Econometrica 29 (1961).
[33] P. Struss, On temporal abstraction in qualitative reasoning (a preliminary report), in: Proceedings of the
Seventh International Workshop on Qualitative Reasoning about Physical Systems, Orcas Island, WA, 1993,
pp. 219–227.

[34] J.D. Tenenberg, Inheritance in automated planning, in: First International Conference on Knowledge Repre-

sentation and Reasoning, Toronto, Ontario, 1989, pp. 475–485.

[35] D.S. Weld, Comparative analysis, Artiﬁcial Intelligence 36 (1988) 333–373.
[36] D.S. Weld, Approximation reformulations, in: Proc. of AAAI-90, Boston, MA, 1990, pp. 407–412.
[37] D.S. Weld, Exaggeration, Artiﬁcial Intelligence 43 (1990) 311–368.
[38] D.S. Weld, Reasoning about model accuracy, Artiﬁcial Intelligence 56 (1992) 255–300.
[39] D.S. Weld, S. Addanki, Task-driven model abstraction, in: 4th International Workshop on Qualitative

Physics, Lugano, Switzerland, 1990, pp. 16–30.

[40] B.C. Williams, Critical abstraction: generating simplest models for causal explanation, in: Proceedings of the
Fifth International Workshop on Qualitative Reasoning about Physical Systems, Austin, TX, 1991, pp. 77–
92.

[41] B.C. Williams, O. Raiman, Decompositional modeling through caricatural reasoning, in: Proc. of AAAI-94,

Seattle, WA, 1994, pp. 1199–1204.

