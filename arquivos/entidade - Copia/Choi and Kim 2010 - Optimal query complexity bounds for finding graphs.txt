Artiﬁcial Intelligence 174 (2010) 551–569

Contents lists available at ScienceDirect

Artiﬁcial Intelligence

www.elsevier.com/locate/artint

Optimal query complexity bounds for ﬁnding graphs
Sung-Soon Choi a,1, Jeong Han Kim a,b,∗,2

a Department of Mathematics, Yonsei University, Seoul, 120-749, Republic of Korea
b National Institute for Mathematical Sciences, Daejeon, 305-340, Republic of Korea

a r t i c l e

i n f o

a b s t r a c t

We consider the problem of ﬁnding an unknown graph by using queries with an additive
property. This problem was partially motivated by DNA shotgun sequencing and linkage
discovery problems of artiﬁcial intelligence.
Given a graph, an additive query asks the number of edges in a set of vertices while
a cross-additive query asks the number of edges crossing between two disjoint sets of
vertices. The queries ask the sum of weights for weighted graphs.
For a graph G with n vertices and at most m edges, we prove that there exists an algorithm
to ﬁnd the edges of G using O( m log n2
log(m+1) ) queries of both types for all m. The bound is best
possible up to a constant factor. For a weighted graph with a mild condition on weights, it
log m ) queries are enough provided m (cid:2) (log n)α for a suﬃciently large
is shown that O( m log n
constant α, which is best possible up to a constant factor if m (cid:3) n2−ε for any constant
ε > 0.
This settles, in particular, a conjecture of Grebinski [V. Grebinski, On the power of additive
combinatorial search model, in: Proceedings of the 4th Annual International Conference
on Computing and Combinatorics (COCOON 1998), Taipei, Taiwan, 1998, pp. 194–203] for
ﬁnding an unweighted graph using additive queries. We also consider the problem of
ﬁnding the Fourier coeﬃcients of a certain class of pseudo-Boolean functions as well as
a similar coin weighing problem.

m

© 2010 Elsevier B.V. All rights reserved.

Article history:
Received 6 August 2008
Received in revised form 12 February 2010
Accepted 13 February 2010
Available online 21 February 2010

Keywords:
Combinatorial search
Combinatorial group testing
Graph ﬁnding
Coin weighing
Fourier coeﬃcient
Pseudo-Boolean function
Littlewood–Offord theorem

1. Introduction

1.1. Graph ﬁnding problem

The problem of ﬁnding a graph is stated as follows. Suppose that a graph G has n vertices and at most m edges and that
the edges of G are unknown. We may consider two types of queries, additive queries and cross-additive queries. An additive
query asks the number of edges in a set of vertices while a cross-additive query asks the number of edges crossing between
two disjoint sets of vertices. The problem is to ﬁnd the edges of G by using as few queries as possible.

Additive queries have been motivated by a main process in shotgun sequencing [6,20]. Shotgun sequencing is a method
to determine the whole genome sequence in an organism’s DNA. In shotgun sequencing, it is required to order decoded

* Corresponding author at: National Institute for Mathematical Sciences, Daejeon, 305-340, Republic of Korea.

E-mail addresses: ss.choi@yonsei.ac.kr (S.-S. Choi), jehkim@nims.re.kr (J.H. Kim).

1 This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of

Education, Science and Technology (CRI, No. 2008-0054850).

2 This work was partially supported by Yonsei University Research Funds 2006-1-0078 and 2007-1-0025, and by the second stage of the Brain Korea 21

Project in 2007, and by the Korea Research Foundation Grant funded by the Korean Government (MOEHRD) (KRF-2006-312-C00455).

0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.artint.2010.02.003

552

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

fragments (called contigs) of the genome sequence. Given a set of contigs, a method called the multiplex PCR method [39]
tells how many pairs of the contigs are adjacent in the original sequence. Thus, the task of ordering contigs is reduced to
the problem of ﬁnding an unknown graph, which is a Hamiltonian cycle or path, by using additive queries. See e.g., [20].

Cross-additive queries have been motivated by the problem of ﬁnding the Fourier coeﬃcients for a certain class of
pseudo-Boolean functions. A pseudo-Boolean function is a real-valued function deﬁned on the set of binary sequences. It is
k-bounded if it can be expressed as a sum of subfunctions each of which depends on at most k input bits. For example,
given a 2-SAT formula, the number of clauses an assignment satisﬁes is a 2-bounded pseudo-Boolean function. In molecular
biology and biophysics, k-bounded functions have been used to study the evolution of a population of organisms in an
environment [25]. Speciﬁcally, k-bounded functions with small k have received attention in modeling living systems [26]
and real biological objects [17]. In evolutionary computation, k-bounded functions have been also used as a benchmark for
comparing heuristic algorithms [18,32,33,38]. Cross-additive queries are used to ﬁnd the Fourier coeﬃcients of 2-bounded
functions. More generally, cross-additive queries for k-bounded hypergraphs can be used to ﬁnd the Fourier coeﬃcients of
k-bounded functions, where k-bounded hypergraphs are hypergraphs whose hyperedges are of size at most k.

An algorithm to ﬁnd an unknown graph is called non-adaptive if each query in the algorithm is independent of the
answers for the previous queries. Otherwise, it is called adaptive. Non-adaptive algorithms are preferable to adaptive ones
particularly when the number of required queries is fairly large and parallel computation is available. There have been
a number of papers addressing the problem of ﬁnding a graph using additive queries. When the (unknown) graph is a
Hamiltonian cycle on n vertices, Grebinski and Kucherov [20] presented an adaptive algorithm using O(n) additive queries,
which is the best possible up to a constant factor. Later, Grebinski and Kucherov [21] provided an extensive work for several
types of graphs. In particular, for graphs with maximum degree bounded by d, they proved the existence of a non-adaptive
algorithm using O(dn) additive queries. When the graph is k-degenerate, the existence of a non-adaptive algorithm using
O(kn) additive queries was shown by Grebinski [19].

The fully general case that the graph has n vertices and at most m edges has been a matter of primary concern. It has
been conjectured by Grebinski [19] that there exists an algorithm to ﬁnd the unknown graph using O(m) additive queries
provided that m = Ω(n). The conjecture has not been settled for a decade. For general m, an adaptive algorithm using
O(m log n) additive queries by Angluin and Chen [4] is the best known to date. In fact, their algorithm uses less powerful
queries called membership queries, which ask the oracle only about the existence of an edge in a set of vertices. (There have
also been a number of papers addressing the problem of ﬁnding a graph using membership queries [2,3,5–7].) Recently,
Reyzin and Srivastava [34] presented a simpler adaptive algorithm using O(m log n) additive queries. In this paper, we prove
the conjecture of Grebinski in a stronger form, namely the existence of a non-adaptive algorithm using O( m log n2
queries. This bound is best possible and better than O(m) if log n2
m

(cid:3) log m, in particular, m > n2

log(m+1) ) additive

log n .

m

We shall focus on bounds for the number of required cross-additive queries. Note that cross-additive queries are less
strong than additive queries since a cross-additive query for the disjoint sets S, T of vertices can be answered by the three
additive queries for S ∪ T , S, and T . It can be easily shown that the converse is not true: For example, for a graph with
exactly one edge, Ω(log n) cross-additive queries are required to verify that it has only one edge while one additive query
is enough to verify the same. In the rest of this paper, we state results with respect to cross-additive queries. The same
statements hold for additive queries after simple modiﬁcations if necessary.

In this paper, we consider two versions of the graph ﬁnding problem. The ﬁrst one is

Problem 1 (Unweighted graphs).

Input:

Output:

an unweighted graph G for which the only information given is that
– G has the vertex set {1, . . . , n} and at most m edges
the edges of G

For the query complexity of the problem, we have the following.

Theorem 1.1. There is a non-adaptive algorithm that solves Problem 1 using O( m log n2

m

log(m+1) ) cross-additive queries.

The second is a generalized one for weighted graphs with a moderate condition on weights of edges.

Problem 2 (Weighted graphs).

Input:

Output:

a weighted graph G for which the only information given is that
– G has the vertex set {1, . . . , n} and at most m edges
– the weights of edges of G are between n
the edges of G

−a and nb in absolute value

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

553

For weighted graphs, a cross-additive query asks the sum of weights of the edges crossing between two disjoint sets of

vertices. We obtain bounds for the number of cross-additive queries required to solve the problem.

Theorem 1.2. For any ﬁxed constants a, b > 0, there are a constant α > 0 and a non-adaptive algorithm that solves Problem 2 using
O( m log n

log m ) cross-additive queries provided m (cid:2) (log n)α .

This extends the result of the conference version [13] that gives the bound when m is at least a constant power of n.3

Concerning the condition on weights, a remark is provided in Section 7.

Notice that we focused only on query complexity of algorithms. The presented algorithms are not computationally ef-
ﬁcient and we do not try to optimize them in time complexity. In terms of query complexity, the bounds in the above
theorems are optimal up to a constant factor for all or almost all m by the information-theoretic lower bounds. For un-
weighted graphs, the bound is optimal for all m. For weighted graphs, the bound is optimal if m (cid:3) n2−ε for any constant
ε > 0. We do not know yet that the bound is optimal for m in the other range, and we conjecture that it is so up to a
constant factor. Theorem 1.2 implies the existence of an algorithm that ﬁnds the weights of all edges with the same query
bound. This is because, once all edges are discovered, the weights of edges can be found by using at most m additional
queries (one for each edge). It is not diﬃcult to show that the algorithm is optimal by an information-theoretic lower
bound argument.

1.2. Related problems

Related to the graph ﬁnding problem, we consider the coin weighing problem and the problem of ﬁnding the Fourier

coeﬃcients of a 2-bounded function.

The problem of ﬁnding a graph can be regarded as an extension of the coin weighing problem. In the coin weighing
problem, we are given n coins among which there are some counterfeits. All the authentic coins have the same weight and
the weight is known. Authentic and counterfeit coins are indistinguishable from each other except that their weights are
different. Given a group of coins, the scale tells the sum of weights of the coins. The problem is to ﬁnd the counterfeits
using the scale as few times as possible. There are a few papers addressing the case where the weights of counterfeits are
identical [1,29,37]. When the weight of each counterfeit is a positive integer and the total weight of counterfeits is bounded
above by m, Lindström [28] constructed a non-adaptive algorithm to ﬁnd the counterfeits with O(m log n) weighings. Under
the same condition, Grebinski and Kucherov [21] proved the existence of a non-adaptive algorithm with an optimal number
log m ) weighings provided that m (cid:3) n1−ε for any
of weighings up to a constant factor. In particular, the algorithm uses O( m log n
constant ε > 0. In this paper, we show that the bound holds even for the general case that the weight of each counterfeit
is nearly arbitrary provided that the number of counterfeits is at most m. The formal deﬁnition of the problem concerned
is as follows.

Problem 3 (Coin weighing).

Input:

Output:

coins for which the only information given is that
– the total number of coins is n, and there are at most m counterfeits among them
– all the authentic coins have the same weight and the weight is known
– the weight difference between each counterfeit and an authentic coin is between n
the set of counterfeit coins

−a and nb

For this problem, we have

Theorem 1.3. For any ﬁxed constants a, b > 0, there is a non-adaptive algorithm that solves Problem 3 with O( m log n

log (m+1) ) weighings.

There have been many studies of the problem of ﬁnding the Fourier coeﬃcients of a k-bounded pseudo-Boolean function
f : {0, 1}n → R where k is a constant. Kargupta and Park [24] presented a deterministic adaptive algorithm that uses O(nk)
function evaluations. Later, Heckendorn and Wright [22] proposed a randomized adaptive algorithm. For the k-bounded
functions with O(n) non-zero Fourier coeﬃcients generated from a random model, they analyzed the algorithm to show
that, with negligible error probability, it ﬁnds the Fourier coeﬃcients in O(n2 log n) function evaluations on average. By
analyzing the algorithm of Heckendorn and Wright, Choi, Jung, and Moon [12] proved that, for a k-bounded function with
m non-zero Fourier coeﬃcients, O(γ (n, m)m log n) function evaluations are enough with negligible error probability, where
1
γ (n, m) is between n
2 and n depending on m. Recently, Choi, Jung, and Kim [11] provided a randomized adaptive algorithm
to ﬁnd the Fourier coeﬃcients with high probability in O(m log n) function evaluations.

3 While this paper was in the review process, we learned that for additive queries, Bshouty and Mazzawi [8] improved our result to get the bound for

arbitrary m.

554

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

Table 1
Summary of results.

n
m

range of m

condition
on weights

Coin weighing

# of coins
upper bound on
# of counterfeits

1 (cid:3) m (cid:3) n

# of vertices
upper bound on
# of edges
(cid:2)
n
2

1 (cid:3) m (cid:3)

(cid:3)

−a and nb
between n
(weight differences)

N/A

query complexity

optimality

O( m log n
log (m+1) )
for m (cid:3) n1−ε

O( m log n2
for all m

log(m+1) )

m

Unweighted graphs

Weighted graphs

Fourier coeﬃcients

# of vertices
upper bound on
# of edges

(log n)α (cid:3) m (cid:3)

(cid:3)

(cid:2)
n
2

−a and nb

between n
in absolute value
(edge weights)

O( m log n
log m )
for m (cid:3) n2−ε

# of input variables
upper bound on
# of non-zero
coeﬃcients
(log n)α (cid:3) m (cid:3)
(cid:2)
+ n + 1
n
2
between n
in absolute value
(non-zero coeﬃcients)

−a and nb

(cid:3)

O( m log n
log m )
for all m

For the problem of ﬁnding weights as well, all above bounds are optimal for all m.

In this paper, we consider the problem for 2-bounded functions with a mild condition on Fourier coeﬃcients as follows.

Problem 4 (Fourier coeﬃcients).

Input:

Output:

is a 2-bounded pseudo-Boolean function deﬁned on {0, 1}n

a function f for which the only information given is that
– f
– f has at most m non-zero Fourier coeﬃcients
– the non-zero Fourier coeﬃcients of f are between n
the Fourier coeﬃcients of f

−a and nb in absolute value

As we will see, the problem of ﬁnding the Fourier coeﬃcients of a 2-bounded function is reduced to a combination of
the coin weighing problem and the graph ﬁnding problem using cross-additive queries. We have the following corollary
from Theorems 1.2 and 1.3. An algorithm is called k-round if the sequence of queries (q1, . . . , q(cid:6)) used by the algorithm
can be divided into k parts (q1 , . . . , q(cid:6)1 ), (q(cid:6)1+1, . . . , q(cid:6)2 ), . . . , (q(cid:6)k−1+1, . . . , q(cid:6) ) so that each part is non-adaptive given the
previous parts. A non-adaptive algorithm is a one-round algorithm. The k-round algorithms with smaller k are preferable as
they can be executed using k sets of parallel computations.

Corollary 1.4. For any ﬁxed constants a, b > 0, there are a constant α > 0 and a 4-round algorithm that solves Problem 4 in O( m log n
log m )
function evaluations provided m (cid:2) (log n)α .

For the 2-bounded functions described in the corollary, the algorithm improves the bound of Choi, Jung, and Kim [11]
by a factor of log m and it is deterministic and fairly non-adaptive. There have been many papers addressing the problem of
ﬁnding the Fourier coeﬃcients of Boolean functions [9,23,31]. We, however, think that the problem is not closely related to
our problem as it is believed that there are not many non-trivial 2-bounded Boolean functions.

As for the theorems for the graph ﬁnding problem, we may show the optimality up to a constant factor for the above
two theorems by information-theoretic lower bound arguments. Theorem 1.3 is optimal if m (cid:3) n1−ε for any constant ε > 0.
However, for the problem of ﬁnding weights of coins, it implies the existence of an algorithm with optimal query complexity
for all m. By the same argument, Corollary 1.4 is optimal for all m in the speciﬁed range.

Table 1 summarizes our results.

1.3. Organization

To prove our results, we use an inequality describing the anti-concentration property of a sum of independent random
variables. It is a generalized Littlewood–Offord theorem [14,30] and easily follows from the previous generalizations in-
cluding ones in [15,16,27,35,36]. We describe the theorem in the next section. In Section 3, we prove Theorem 1.3 (coin
weighing) using probabilistic methods. The proof is relatively easy to follow and it would be a good illustration of how the
generalized Littlewood–Offord theorem is used. We prove Theorem 1.1 (unweighted graphs) in Section 4 and Theorem 1.2
(weighted graphs) in Section 5. In Section 6, Corollary 1.4 is proved by showing how the problem of ﬁnding the Fourier co-
eﬃcients of a 2-bounded pseudo-Boolean function is reduced to the graph ﬁnding problem and the coin weighing problem.
Concluding remarks will follow in the last section.

2. Anti-concentration inequality

Random variables of a certain type are highly concentrated near their means with high probability. There are a few
inequalities describing the concentration property. An example is the Chernoff bound [10], which shows that the sum of

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

555

i.i.d. random variables is highly concentrated near its mean. On the other hand, the Littlewood–Offord theorem [14,30]
describes the anti-concentration property of the sum of independent random variables. It gives an upper bound for the
probability that the sum of independent random variables having two different values is in an interval. There are several
generalizations of the theorem including those in [15,16,27,35,36]. We are going to use the following form, which can be
found in [27]. It is actually not hard to derive it using the original Littlewood–Offord theorem.

Theorem 2.1 (Generalized Littlewood–Offord theorem). Let a1, . . . , an, b1, . . . , bn and s > 0 be real numbers such that bi − ai (cid:2) s for
1 (cid:3) i (cid:3) n and let X1, . . . , Xn be independent random variables. Suppose that there is p with 0 < p < 1 such that Pr[ Xi (cid:3) ai] (cid:2) p and
Pr[ Xi (cid:2) bi] (cid:2) p for all 1 (cid:3) i (cid:3) n. Then, for any real number r,
(cid:6)

(cid:4)

Pr

r (cid:3)

n(cid:5)

i=1

Xi (cid:3) r + s

(cid:3) 2.6
√
pn

.

A version we will frequently use is

Corollary 2.2. Let ρ be a positive integer and s be a positive real number. Let X1, . . . , Xn be independent random variables at least
(cid:7)
t Xi ’s of which satisfy Pr[ Xi = 0] (cid:2) p and Pr[| Xi| (cid:2) s] (cid:2) p for some p with 0 < p < 1. Then, the probability that
n
i=1 Xi is in an
interval of length ρs is at most 3.7ρ√
pt

.

Proof. Without loss of generality, it may be assumed that Pr[ Xi = 0] (cid:2) p and Pr[| Xi| (cid:2) s] (cid:2) p for i = 1, . . . , t. This implies
that Pr[ Xi = 0] (cid:2) p
2 for i = 1, . . . , t. Since Xi ’s are independent, by Theo-
(cid:3) 3.7√
rem 2.1, the probability that
.
pt

(cid:7)
i=1 Xi conditioned on Xi ’s with i > t is in an interval of length s is at most 2.6
n
(cid:7)
i=1 Xi is in an interval of length s and the corollary follows by union bound. (cid:2)
n

2 and, either Pr[ Xi (cid:2) s] (cid:2) p

2 or Pr[ Xi (cid:3) −s] (cid:2) p

Thus, so is the probability that

√
2√
pt

3. Coin weighing problem

In this section, we prove Theorem 1.3. As the weight of an authentic coin is known in the coin weighing problem, we
may assume that the weight of an authentic coin is 0 and the weight of each counterfeit is non-zero (possibly negative).
An instance of the coin weighing problem may be regarded as an n-dimensional vector whose coordinates are given by the
weights of coins. For an instance x = (xi)n
i=1, let sp(x) denote the support set of x, that is, the set of counterfeit coins in x.
Similarly, a query can be regarded as a binary vector and a sequence of (cid:6) queries as an (cid:6) × n binary matrix.

The basic idea for the proof of Theorem 1.3 is as follows. A sequence of queries, or a binary matrix M, separates a pair
of instances x, y if a query in the sequence yields different answers for the two instances, i.e., Mx (cid:7)= M y. A sequence of
queries separates a set of instances if it separates every pair of instances in the set with distinct supports. In a non-adaptive
algorithm, its sequence of queries must separate the set of all instances. If there were only ﬁnitely many possible instances,
such a sequence might be enough as the given instance can be identiﬁed by checking all possible instances. Since there
are uncountably many instances, we consider a set of discretized instances: Let I be the set of all instances x = (xi) such
−(a+3). We prove the existence of a sequence S of queries separating I by more than m
that xi is a multiple of n
na+3 , i.e.,
na+3 for the binary matrix M induced by S and x, y ∈ I with distinct supports. We will write MS (x) for
(cid:8)Mx − M y(cid:8)∞ > m
Mx when we need to specify the sequence S.

Lemma 3.1. There exists a sequence S of (cid:6) queries with (cid:6) = O( m log n

log (m+1) ) such that

(cid:8)
(cid:8)
(cid:8)MS (x) − MS ( y)
(cid:8)

∞ >

m
na+3

for all x, y ∈ I with sp(x) (cid:7)= sp( y).

Once such a sequence S of queries is found, the following holds.

Lemma 3.2. Suppose that x is a given instance. Then,

(i) there exists y ∈ I such that sp(x) = sp( y) and

(cid:8)
(cid:8)
(cid:8)MS (x) − MS ( y)
(cid:8)

(cid:3) m

2na+3

;

∞

(ii) for all z ∈ I with (cid:8)MS (x) − MS (z)(cid:8)∞ (cid:3) m

2na+3 , sp(z) = sp(x).

556

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

Proof. Let yi be the closest multiple of n
and sp( y) = sp(x) as |xi| (cid:2) n

−a if xi (cid:7)= 0. The inequality in (i) follows from the fact that

−(a+3) to xi with ties broken by preferring the smaller one. Then, y belongs to I

(cid:8)
(cid:8)
(cid:8)MS (x) − MS ( y)
(cid:8)

(cid:3)

∞

n(cid:5)

i=1

|xi − yi| (cid:3) m
2na+3

.

For (ii), if (cid:8)MS (z) − MS (x)(cid:8)∞ (cid:3) m
na+3 , sp(z) must be sp( y), which is sp(x). (cid:2)
m

2na+3 , (cid:8)MS (z) − MS ( y)(cid:8)∞ (cid:3) m

na+3 by triangle inequality. Since S separates I by more than

Lemmas 3.1 and 3.2 give Theorem 1.3 by the following algorithm.

Algorithm
// x is an unknown instance.
compute MS (x);
1
for each z ∈ I
2
3
4
5

return sp( y);

if (cid:8)MS (x) − MS (z)(cid:8)∞ (cid:3) m
y ← z and break;

2na+3 ,

Now, we prove Lemma 3.1. Let D be the set of differences of instances in I with distinct supports, i.e., the set of vectors
x − y for all x, y ∈ I with sp(x) (cid:7)= sp( y). Since MS (·) is linear for a ﬁxed sequence S, Lemma 3.1 is equivalent to saying that
there exists a sequence S of (cid:6) queries such that (cid:8)MS (x)(cid:8)∞ > m
na+3 for all x ∈ D. A probabilistic method is used to prove
the following lemma, which immediately implies Lemma 3.1. In the following, a uniform random query means a uniform
random binary vector sampled from {0, 1}n.

Lemma 3.3. There exists a constant c > 0 such that

(cid:9)

Pr

There is x in D such that

(cid:8)
(cid:8)
(cid:8)MS (x)
(cid:8)

(cid:3) m
na+3

∞

(cid:10)

= o(1)

for a sequence S of (cid:6) = cm log n

log (m+1) independent uniform random queries.

Notice that for all x ∈ D, there is i such that |xi| (cid:2) n

−a since x is the difference of a pair of instances with distinct
−a, the
supports. For a uniform random query q, since ﬂipping the ith bit of q changes the answer χq(x) by at least n
−(cid:6). On the other
probability of |χq(x)| < 1
is at most 1/2. Thus, the probability of (cid:8)MS (x)(cid:8)∞ (cid:3) m
na+3 < 1
2na
2na
O(m log n). If m is bounded from above, then we may take large
hand, the size |D| of D is at most
−(cid:6) is not
enough c so that 2
enough to use the union bound as the size of D is larger than 2(cid:6) unless m is bounded from above. However, if there
are many, say t, i’s such that |xi| (cid:2) m
) by the generalized Littlewood–
Offord theorem presented in Section 2. Observing this, we have the following lemma. Hereafter, “P (m) holds for suﬃciently
large m” means that there is a constant m0 > 0 such that P (m) holds for all m (cid:2) m0.

−(cid:6)|D| = o(1). We may now assume that m is suﬃciently large. It turns out that the bound 2

na+3 , then the probability of |χq(x)| (cid:3) m
na+3

(cid:3)
(4na+b+3)i = e

is O( 1√
t

is at most 2

(cid:2)
n
i

2m
i=1

(cid:7)

Lemma 3.4. For suﬃciently large m, the following holds: Suppose that x ∈ D has at least
m
na+3 in absolute value and q is a uniform random query. Then,

(cid:9)

Pr

(cid:11)
(cid:11)
(cid:11) (cid:3) m
(cid:11)χq(x)
na+3

(cid:10)

(cid:3) m

−0.4.

m

log(m+1) coordinates larger than or equal to

Proof. Let Xi ’s be random variables such that Xi = xi if qi = 1 and Xi = 0 otherwise. Then, the lemma immediately follows
(cid:7)
log (m+1) i’s, | Xi| (cid:2)
by the generalized Littlewood–Offord theorem as χq(x) =
n
i=1 Xi , Xi ’s are independent, and, for at least
na+3 with probability 1
m

2 while it is 0 with probability 1

2 . (cid:2)

m

Let D1 be the set of all x ∈ D with less than

m

log (m+1) coordinates larger than or equal to m

na+3 in absolute value and

D2 = D \ D1. Lemma 3.4 implies

(cid:9)

Pr

There is x in D2 such that

(cid:10)

(cid:8)
(cid:8)
(cid:8)MS (x)
(cid:8)

(cid:3) m
na+3

∞

(cid:3) |D|m

−0.4(cid:6) = o(1)

(1)

for a large enough constant c in (cid:6) = cm log n
than m

na+3 in absolute value with 0. Let

log (m+1) . For x ∈ D1, let ˜x be the vector obtained by replacing the coordinates in x less
˜D1 be the set of all such ˜x. Notice that ˜x ∈ ˜D1 has at least one coordinate whose

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

557

−a. The same argument mentioned above gives Pr[(cid:8)MS (˜x)(cid:8)∞ < 1

−(cid:6) and the union bound

2na ] (cid:3) 2

absolute value is at least n
yields

(cid:9)

Pr

There is ˜x in ˜D1 such that

(cid:8)
(cid:8)
(cid:8)MS (˜x)
(cid:8)

∞ <

(cid:10)

1
2na

(cid:3) | ˜D1|2

−(cid:6) (cid:3) 2

−(cid:6)

(cid:12)

m
(cid:5)
log (m+1)

(cid:13)

(cid:13)

(cid:12)
n

(cid:2)

i=1

i

(cid:3)

4na+b+3

i = o(1)

for a large enough constant c in (cid:6). Since
(cid:8)
(cid:8)
(cid:8)
(cid:8)MS (˜x)

(cid:8)
(cid:8)
(cid:8)MS (x)
(cid:8)

−

(cid:2)

(cid:8)
(cid:8)
(cid:8)
(cid:8)MS (x − ˜x)

∞

∞

(cid:8)
(cid:8)
(cid:8)
(cid:8)MS (˜x)

∞

(cid:2)

∞

− 2m2
na+3

and m
na+3

na+3 < 1

+ 2m2
(cid:9)

2na , we have

Pr

There is x in D1 such that

(cid:10)

(cid:8)
(cid:8)
(cid:8)MS (x)
(cid:8)

(cid:3) m
na+3

∞

(cid:9)
(cid:3) Pr

There is ˜x in ˜D1 such that

(cid:8)
(cid:8)
(cid:8)MS (˜x)
(cid:8)

∞

(cid:3) m
na+3

+ 2m2
na+3

(cid:10)

= o(1).

(2)

Lemma 3.3 follows by (1) and (2).

4. Finding graphs

In this section, we prove Theorem 1.1. Unlike the coin weighing problem, the number of all possible instances is ﬁnite as
graphs are unweighted. Thus, a desired algorithm is immediately obtained by a sequence of cross-additive queries separating
every pair of all possible graphs. To be more precise, let G be the set of all graphs on the vertex set [n] = {1, . . . , n} having
at most m edges. For a graph G ∈ G and a sequence S of (cid:6) cross-additive queries, MS (G) is the (cid:6)-dimensional vector
consisting of the answers of the (cid:6) queries for G. The following implies Theorem 1.1.

Lemma 4.1. There exists a sequence S of (cid:6) cross-additive queries such that (cid:6) = O( m log n2

m

log(m+1) ) and

(cid:8)
(cid:8)
(cid:8)MS (G) − MS (H)
(cid:8)
∞ > 0

for all distinct G, H ∈ G.

We now prove Lemma 4.1. For two graphs G, H ∈ G, deﬁne the difference graph G − H to be the graph of which vertex
set is [n] and edge set is (E(G) \ E(H)) ∪ (E(H) \ E(G)). Edges in E(G) \ E(H) have weight 1 and edges in E(H) \ E(G) have
weight −1. Let D be the set of difference graphs G − H for distinct G, H ∈ G. As the answer of a query for a difference
graph G − H ∈ D is the answer for G minus the answer for H ,

MS (G − H) = MS (G) − MS (H)

for any sequence S of queries. Thus, Lemma 4.1 is equivalent to saying that there exists a sequence S of O( m log(n2/m)
log(m+1) )
queries such that (cid:8)MS (G)(cid:8)∞ > 0 for all G ∈ D. A random query (S, T ) means a cross-additive query for a pair of random
sets of vertices S, T such that each vertex independently belongs to S, T , and none of S and T , each with probability 1
3 so
that S ∩ T = ∅. We prove the following to obtain Lemma 4.1.

Lemma 4.2. There exists a constant c > 0 such that

(cid:14)
There is G in D such that
Pr

(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

(cid:15)

= 0

∞

= o(1)

for a sequence S of (cid:6) = cm log(n2/m)
log(m+1)

independent random queries.

Let μS,T (G) be the answer of a cross-additive query (S, T ) for a graph G. For G ∈ D, since G is the difference graph of

two distinct graphs, G has at least one edge. By this fact, we have the following.

Lemma 4.3. For G ∈ D and a random query (S, T ),
(cid:15)

(cid:14)
μS,T (G) = 0
Pr

(cid:3) 8
9

.

558

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

Proof. Take an edge e = {x, y} in G. We consider (S, T ) conditioned on ˜S = S ∩ ([n] \ {x, y}) and ˜T = T ∩ ([n] \ {x, y}). In
other words, all vertices except x and y are determined to be in S, T , or none of them. We show that μS,T (G) (cid:7)= 0 when
(S, T ) is one of ( ˜S, ˜T ), ( ˜S ∪ {x}, ˜T ), ( ˜S, ˜T ∪ { y}), and ( ˜S ∪ {x}, ˜T ∪ { y}), each of which occurs with probability 1/9.

The sum of weights of edges crossing between ˜S ∪ {x} and ˜T is that of edges crossing between ˜S and ˜T plus that of
edges crossing between {x} and ˜T . Similarly, the sum of weights of edges crossing between ˜S and ˜T ∪ { y} is that of edges
crossing between ˜S and ˜T plus that of edges crossing between ˜S and { y}. Thus, letting w G (e) be the weight of e,

μ ˜S∪{x}, ˜T ∪{ y}(G) = w G (e) + μ ˜S∪{x}, ˜T (G) + μ ˜S, ˜T ∪{ y}(G) − μ ˜S, ˜T (G).

If the last three terms are all 0, then μ ˜S∪{x}, ˜T ∪{ y}(G) = w G (e) (cid:7)= 0. (cid:2)

(cid:7)

(cid:3)
(cid:2)
(n
2)
2i = e
i

Thus, the probability of (cid:8)MS (G)(cid:8)∞ = 0 is at most ( 8

O(m log (n2/m)), we
may take large enough c in (cid:6) = cm log(n2/m)
so that (8/9)(cid:6)|D| = o(1) if m is bounded above. It may now be assumed that m
log(m+1)
is suﬃciently large. The bound (8/9)(cid:6) is not enough to use the union bound as the size of D is larger than (9/8)(cid:6) unless
m is bounded above. Notice that, for a random query (S, T ), μS,T (G) is a sum of weights of edges in G chosen randomly. If
the edges were chosen independently, one might hope to prove Lemma 4.2 by dividing D into two classes according to the
number of edges as in the coin weighing problem. However, there is a dependency among edges in general and the graphs
with many edges are further divided into subclasses to handle the dependency.

9 )(cid:6). As the size of D is at most

2m
i=1

Precisely, we divide D into four classes D1, D2, D3, and D4. The class D1 consists of graphs in D that have only few
(log (m+1))2 edges. The class D2 consists of graphs in D that have many vertices of high degree. To
edges, namely, less than
be precise, let δ = 0.1 and deﬁne U (G) for a graph G ∈ D to be the set of vertices of degree at least mδ in G. The class D2
is the set of graphs G in D with the size of U (G) being mδ or more. Deﬁne W (G) = [n] \ U (G). The class D3 is the set of
graphs G in D \ (D1 ∪ D2) such that there are at least
2(log (m+1))2 edges crossing between U (G) and W (G). The class D4 is
m
D \ (D1 ∪ D2 ∪ D3). It turns out that, for G ∈ D2 or D4 and a random query (S, T ), Pr[μS,T (G) = 0] is polynomially small
in m. On the other hand, it is not true for some graphs in D3 (e.g., the graphs G such that the size of U (G) is a constant
independent of n and there is no edge with both ends in W (G)). Instead of focusing on a single query, we consider queries
collectively for graphs in D3. Except for a negligible probability, there is a constant portion of the (cid:6) random queries for each
graph in D3 that give polynomially small bounds in m as shown below.

m

We consider four cases G ∈ Di , i = 1, . . . , 4, and show that

(cid:14)
Pr

There is G in Di such that

(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

(cid:15)

= 0

∞

= o(1)

for a large enough constant c in (cid:6).

Case 1. G ∈ D1.

As the number of graphs in D1 is at most

by Lemma 4.3.

Case 2. G ∈ D2.

Clearly, there are at most

(cid:7)

2m
i=1

(cid:3)
(cid:2)
(n
2)
2i = e
i

(cid:7)(cid:12)

m
(log(m+1))2

i=1

(cid:13)

(cid:3)
(cid:2)
(n
2)
2i = e
i

O( m log(n2/m)
log(m+1) )

, the desired inequality immediately follows

following lemma. The proof is based on two applications of the generalized Littlewood–Offord theorem.

O(m log (n2/m)) graphs in D2. Thus, the desired inequality is obtained by the

Lemma 4.4. For suﬃciently large m, the following holds: Suppose that G is a graph in D2 and (S, T ) is a random query. Then,

(cid:14)
μS,T (G) = 0
Pr

(cid:15)

(cid:3) m

−0.01δ.

Proof. As |U (G)| (cid:2) mδ , we may choose a subset Q of U (G) with |Q | = (cid:12)m0.1δ(cid:13). Denote by H the spanning subgraph of
G consisting of the edges in G except those with both ends in Q . (A spanning subgraph is a subgraph containing all the
vertices of the original graph.) The lemma follows by showing that

(cid:14)(cid:11)
(cid:11)
(cid:11)μS,T (H)
(cid:11) (cid:3) m0.2δ

Pr

(cid:15)

−0.01δ

(cid:3) m

for a random query (S, T ). This is because there are at most
implies |μS,T (H)| (cid:3) m0.2δ .

Given a pair of disjoint sets ( A, B) of vertices in ¯Q := [n] \ Q , we say a vertex v in Q is bad for ( A, B) if the sum of
weights of edges crossing between A and v is less than m0.2δ in absolute value, that is, |μ A,v (H)| < m0.2δ . (We simply write

(cid:3)
(cid:2)|Q |

2

(cid:3) m0.2δ edges with both ends in Q so that μS,T (G) = 0

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

559

μ A,v (H) for μ A,{v}(H).) A pair ( A, B) is called bad if there is a bad vertex in Q for ( A, B). It is good otherwise. Letting
( ˜S, ˜T ) = (S ∩ ¯Q , T ∩ ¯Q ), we have

(cid:14)(cid:11)
(cid:11)
(cid:11)μS,T (H)
(cid:11) (cid:3) m0.2δ

Pr

(cid:15)

=

(cid:15)
(cid:14)
( ˜S, ˜T )
Pr

(cid:14)(cid:11)
(cid:11)
(cid:11)μS,T (H)
(cid:11) (cid:3) m0.2δ

Pr

(cid:11)
(cid:15)
(cid:11)( ˜S, ˜T )

.

(cid:5)

( ˜S, ˜T )

The right hand side may be upper bounded by
(cid:15)

(cid:5)

(cid:14)
( ˜S, ˜T ) is bad
Pr

+

(cid:14)
Pr

(cid:15)
( ˜S, ˜T )

(cid:14)(cid:11)
(cid:11)
(cid:11)μS,T (H)
(cid:11) (cid:3) m0.2δ

(cid:11)
(cid:15)
(cid:11)( ˜S, ˜T )

.

Pr

( ˜S, ˜T ) good

Thus, it is enough to show that each of the above two terms is polynomially small in m, say m

For a vertex v ∈ Q , μ ˜S,v (H) is a sum of weights of edges {v, u} with u ∈ ¯Q chosen independently and with probability 1
3 .

As v is adjacent to at least mδ − |Q | (cid:2) mδ

2 vertices in ¯Q , the generalized Littlewood–Offord theorem gives

−0.04δ .

(cid:14)
Pr

(cid:15)
v is bad for ( ˜S, ˜T )

= Pr

(cid:14)(cid:11)
(cid:11)
(cid:11)μ ˜S,v (H)
(cid:11) < m0.2δ

(cid:15)

(cid:3) m

−0.2δ.

Thus,

(cid:14)
( ˜S, ˜T ) is bad
Pr

(cid:15)

(cid:3) |Q | · m

−0.2δ (cid:3) m

−0.1δ.

Now, we show that, given the event that ( ˜S, ˜T ) is good, the probability of |μS,T (H)| (cid:3) m0.2δ is polynomially small in m.
To this end, we consider the condition that ( ˜S, ˜T ) is ﬁxed to be a good pair. Since there is no edge of H with both ends
in Q , μS,T (H) is decomposed into

μS,T (H) = μS∩Q , ˜T (H) + μ ˜S,T ∩Q (H) + μ ˜S, ˜T (H).

For a vertex v in Q , let Y v be a random variable such that Y v = μv, ˜T (H) if v ∈ S, Y v = μ ˜S,v (H) if v ∈ T , and Y v = 0
otherwise. Then, μS∩Q , ˜T (H) + μ ˜S,T ∩Q (H) =

v∈Q Y v and

(cid:7)

μS,T (H) =

Y v + μ ˜S, ˜T (H).

(cid:5)

v∈Q

(cid:7)

Since there is no bad vertex of Q for ( ˜S, ˜T ), |Y v | (cid:2) m0.2δ if v ∈ T , which occurs with probability 1
Y v = 0 if v /∈ S ∪ T , which also occurs with probability 1
probability that |
v∈Q Y v + μ ˜S, ˜T (H)| (cid:3) m0.2δ is at most m
any good pair ( ˜S, ˜T ),
(cid:14)(cid:11)
(cid:11)
(cid:11)μS,T (H)
(cid:11) (cid:3) m0.2δ

3 . On the other hand,
3 . Since Y v ’s are independent and the size of Q is (cid:12)m0.1δ(cid:13), the
−0.04δ by the generalized Littlewood–Offord theorem. That is, for

(cid:11)
(cid:15)
(cid:11)( ˜S, ˜T )

−0.04δ.

(cid:3) m

Pr

(cid:2)

Case 3. G ∈ D3.

As mentioned above, a bound better than a constant may not be obtained for a random query for some graphs in D3.
Taking this into account, we use a conditioning argument for the random queries in a collective sense to prove the desired
inequality. We start with the following lemma.

Lemma 4.5. For suﬃciently large m, the following holds: Suppose that G is a graph in D3. Then, there is a pair of disjoint sets of vertices
( A, B) such that

(i) 0 < | A| (cid:3) 2m2δ and |B| = (cid:12)mδ(cid:13),
(ii) for each v ∈ B, there is u ∈ A that is adjacent to v, and
(iii) for all u ∈ B and v ∈ [n] \ A, u and v are not adjacent.

m

Proof. We prove the lemma by constructing the desired pair ( A, B). As G ∈ D3, the number of edges crossing between U (G)
2(log (m+1))2 )/mδ (cid:2)
2(log (m+1))2 . Since each vertex in W (G) is of degree less than mδ , there are at least (
and W (G) is at least
m8δ vertices in W (G) that are adjacent to at least one vertex in U (G). Thus, we may iteratively choose (cid:12)mδ(cid:13) such vertices
that are not adjacent to each other. This is possible as vertices in W (G) are of degree less than mδ and m8δ/(1 + mδ) (cid:2) mδ .
The set B consists of these vertices.

The set A consists of vertices in U (G) and vertices in W (G) \ B that are adjacent to a vertex in B. Since G /∈ D2 and
there are at most |B|mδ vertices in W (G) \ B described above, | A| (cid:3) |U (G)| + |B|mδ (cid:3) 2m2δ . Hence the properties (i)–(iii)
follow. (cid:2)

m

560

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

Let

For each graph G in D3, we choose a pair of disjoint sets of vertices ( A, B) satisfying the properties in Lemma 4.5
and denote by H(G) the bipartite subgraph of G on A ∪ B consisting of all edges in G between them. Recall that S is
the sequence of random queries (S, T ). Once all vertices in ¯B := V (G) \ B are determined to be in S, T or none of them
for the random queries (S, T ), MS (G[ ¯B]) is determined, where G[ ¯B] is the induced subgraph of G on ¯B. Also, MS (G) =
MS (H(G)) + MS (G[ ¯B]).

˜S be the sequence of restricted random queries ( ˜S, ˜T ) on ¯B. Then we may write M ˜S (G[ ¯B]) for MS (G[ ¯B]) as
MS (G[ ¯B]) depends only on ˜S. The quantity (cid:8)MS (H(G)) + M ˜S (G[ ¯B])(cid:8)∞ turns out to be zero with small enough proba-
˜S. For the set H of all possible bipartite graphs H , say on A ∪ B, satisfying (i) and (ii) in Lemma 4.5, we
bility for most of
divide the event {∃G ∈ D3 such that (cid:8)MS (G)(cid:8)∞ = 0} into events F H := {∃G ∈ D3 with H(G) = H such that (cid:8)MS (G)(cid:8)∞ = 0},
H ∈ H. Observe that
(cid:14)
∃G ∈ D3 such that
Pr

(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

Pr[F H ].

(cid:9) (cid:16)

= Pr

= 0

(cid:5)

F H

(cid:3)

(cid:10)

(cid:15)

∞

(cid:7)

H∈H

H∈H
˜S Pr[ ˜S] Pr[F H | ˜S], and will show that Pr[F H | ˜S] is small enough for most

To estimate Pr[F H ], we further consider Pr[F H ] =
of

˜S.
For a given graph H ∈ H on A ∪ B and the restricted random query ( ˜S, ˜T ) on ¯B, a vertex v ∈ B is called bad, if the sum
of weights of edges between v and ˜S ∩ A is 0, i.e., μv, ˜S∩ A(H) = 0. The restricted random query ( ˜S, ˜T ) is bad if more than
|B| vertices of B are bad for ( ˜S, ˜T ). The sequence ˜S = ( ˜S i, ˜T i)(cid:6)
i=1 is called bad if the number of bad queries ( ˜S i, ˜T i) is more
3
4
than 9(cid:6)
10 . The sequence is good if it is not bad. Observe that
Pr[F H ] (cid:3) Pr[ ˜S bad] + Pr[F H and ˜S good] = Pr[ ˜S bad] +

Pr[ ˜S] Pr[F H | ˜S].

(cid:5)

(3)

We ﬁrst show that Pr[ ˜S bad] is small enough.

˜S good

Lemma 4.6. For a given H ∈ H on A ∪ B, there is a constant β < 1 such that Pr[ ˜S bad] (cid:3) β(cid:6).

Proof. We ﬁrst show that a vertex v ∈ B is bad with probability at most 2/3. For each vertex v ∈ B, take u ∈ A that is
adjacent to v in H . Assuming ˜S i ∩ A \ {u} is determined, there are three possibilities for u, u ∈ ˜S i , u ∈ ˜T i or u /∈ ˜S i ∪ ˜T i . If
μv, ˜S i ∩ A\{u}(H) (cid:7)= 0, then v is not bad provided u /∈ ˜S i ∪ ˜T i , which occurs with probability 1/3. If μv, ˜S i ∩ A\{u}(H) = 0, then
u ∈ ˜S i implies that |μv, ˜S i ∩ A(H)| = 1. As u ∈ ˜S i with probability 1/3, v ∈ B is bad with probability at most 2/3.

Therefore, the expected number of bad vertices in B is at most 2
3

|B| and hence the probability that more than 3|B|/4
vertices in B is bad is at most 8/9 by Markov’s inequality. This implies that the expected number of bad queries ( ˜S i, ˜T i)
in ˜S is less than or equal to 8(cid:6)/9. Since ( ˜S i, ˜T i) are i.i.d., Chernoff’s large deviation inequality yields Pr[ ˜S bad] (cid:3) β(cid:6), for a
constant β < 1. (cid:2)

For an upper bound of the second part of (3), observe that
(cid:11)
(cid:11) ˜S

(cid:14)(cid:8)
(cid:8)MS (H) + M ˜S

Pr[F H | ˜S] (cid:3)

G[ ¯B]

= 0

(cid:3)(cid:8)
(cid:8)

(cid:5)

Pr

(cid:2)

∞

(cid:15)
.

(4)

G:H(G)=H

We show that

Lemma 4.7. For suﬃciently large m, the following holds: Suppose that ˜S is a good sequence of restricted random queries ( ˜S i, ˜T i) on ¯B.
Then,

(cid:14)(cid:8)
(cid:8)MS (H) + M ˜S

Pr

(cid:2)

(cid:3)(cid:8)
(cid:8)

G[ ¯B]

= 0

∞

(cid:15)

(cid:11)
(cid:11) ˜S

(cid:3) m

−δ(cid:6)/30.

Proof. Without loss of generality, it may be assumed that ( ˜S i, ˜T i) is good for 1 (cid:3) i (cid:3) (cid:6)/10. For each i in the range, we may
|B|
4 good vertices in B. (Of course, it actually means (cid:12)|B|/4(cid:13).) Under the condition that ( ˜S i, ˜T i) is given
choose a set B i of
and all the vertices in B \ B i have been determined to be in S i , T i , or none of them for the fully extended query (S i, T i),
the event that a vertex v ∈ B i belongs to T i (or none of S i and T i , resp.), which occurs with probability 1/3, changes the
sum of weights of edges crossing between S i and T i by at least 1 (or does not change the sum, resp.) independently of
the other vertices in B i . Thus, the generalized Littlewood–Offord theorem yields that the sum of weights of edges crossing
between S i and T i is a ﬁxed number with probability at most |B i|−0.4. Since |B i| = |B|
10 , are
independent,

(cid:2) m0.9δ and (S i, T i), 1 (cid:3) i (cid:3) (cid:6)

4

(cid:14)(cid:8)
(cid:8)MS (H) + M ˜S

Pr

(cid:2)

(cid:3)(cid:8)
(cid:8)

G[ ¯B]

= 0

∞

(cid:15)

(cid:11)
(cid:11) ˜S

(cid:3) m

−δ(cid:6)/30.

(cid:2)

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

561

Since there are at most

O(m log (n2/m)) graphs in D3, the above lemma and (4) imply that

Pr[F H | ˜S] (cid:3) exp

(cid:2)
m log

(cid:2)
n2/m

(cid:3)(cid:3)

+ O

(cid:2)

(cid:3) exp

−m log

(cid:3)(cid:3)

(cid:2)
n2/m

,

(cid:13)

(cid:3)
(cid:2)
(n
2)
2i = e
i

(cid:7)

2m
i=1
(cid:12)
− δ(cid:6) log m
30

for large enough c in (cid:6) = cm log(n2/m)
log(m+1)

. With this inequality and Lemma 4.6, (3) gives

Pr[F H ] (cid:3) β(cid:6)/2,

and we ﬁnally have
(cid:14)
∃G ∈ D3 such that
Pr

(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

= 0

∞

(cid:5)

(cid:15)

(cid:3)

Pr[F H ] (cid:3) |H|β(cid:6)/2.

H∈H

As the number of edges in a graph in H is at most 2m3δ and the number of possible edge weights (including 0) is at
most 3, we have

|H| (cid:3) 32m3δ

(cid:12)

(cid:18)

n(cid:17)
mδ

(cid:13) (cid:12)2m2δ (cid:13)(cid:5)

(cid:13)

(cid:12)
n

i

i=1

−(cid:6)/4

(cid:3) β

and the desired inequality follows.

Case 4. G ∈ D4.

For graphs G ∈ D4, we need two lemmas for the desired bound. The following is based on the fact that there are many

edges with both ends in W (G) while each vertex in W (G) has a bounded degree.

Lemma 4.8. For suﬃciently large m, the following holds: Suppose that G is a graph in D4. Then, G has an induced matching consisting
of (cid:12)mδ(cid:13) edges.

(cid:2)|U (G)|

Proof. As G /∈ D1 ∪ D2 ∪ D3, there are at least
m
2(log (m+1))2
edges crossing between U (G) and W (G) (as G /∈ D3). Since the size of U (G) is less than mδ (as G /∈ D2), there are at
4(log (m+1))2 edges with both ends in W (G) for
most
suﬃciently large m. Considering edges with both ends in W (G) only, iteratively take an edge e and delete e and all edges
that share a vertex with e. Since each vertex in W (G) has a degree less than mδ , we are able to take at least

(log (m+1))2 edges in G (as G /∈ D1) and there are less than

2 edges with both ends in U (G). Thus, there are at least

(cid:3) m2δ

m

m

(cid:3)

2

(cid:12)

m
4(log (m + 1))2

(cid:13)

(cid:19)(cid:2)

2mδ + 1

(cid:3)

(cid:2) mδ

edges. The ﬁrst (cid:12)mδ(cid:13) such edges constitute an induced matching as desired. (cid:2)

Since there are at most

(cid:7)

2m
i=1

(cid:3)
(cid:2)
(n
2)
2i = e
i

O(m log (n2/m)) different graphs in D4, the following lemma yields the desired

bound. To prove the lemma, the above lemma and the generalized Littlewood–Offord theorem are used.

Lemma 4.9. For suﬃciently large m, the following holds: Suppose that G is a graph in D4 and (S, T ) is a random query. Then,

(cid:14)
μS,T (G) = 0
Pr

(cid:15)

(cid:3) m

−0.4δ.

Proof. According to Lemma 4.8, G has an induced matching consisting of (cid:12)mδ(cid:13) edges. Let B be the set of the vertices
in the induced matching. We consider a random query (S, T ) conditioned on ˜S := S ∩ ¯B, ˜T := T ∩ ¯B, where ¯B := [n] \ B.
Let e1, . . . , e(cid:12)mδ(cid:13) be the (cid:12)mδ(cid:13) edges in the induced matching. For each edge ei = {ui, v i}, let Xi be the random variable
representing the contribution of ui and v i to the value μS,T (G) so that

μS,T (G) =

(cid:12)mδ (cid:13)(cid:5)

i=1

Xi + μ ˜S, ˜T (G).

This is possible since ei ’s are the only edges of G with both ends in B and they are pairwise disjoint. It is now possible to
(G) + w G (ei)
use the same argument used in Lemma 4.3. For each i, Xi may be 0, μui , ˜T (G), μ ˜S,v i
each with probability 1/9, corresponding to the events {ui, v i /∈ S ∪ T }, {ui ∈ S, v i /∈ S ∪ T }, {ui /∈ S ∪ T , v i ∈ T }, and {ui ∈ S,
v i ∈ T }, respectively. Since |w G (ei)| = 1, at least one of the last three values is 1/3 or more in absolute value. In particular,
Xi = 0, | Xi| (cid:2) 1/3 each with probability at least 1/9. By the generalized Littlewood–Offord theorem, the probability that
(cid:7)(cid:12)mδ (cid:13)

(G) or μui , ˜T (G) + μ ˜S,v i

−0.4δ and hence so is the probability that μS,T (G) = 0. (cid:2)

i=1 Xi + μ ˜S, ˜T (G) is a ﬁxed number is at most m

562

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

5. Finding weighted graphs

In this section, we prove Theorem 1.2. As in the coin weighing problem, we consider a set of discretized graphs to
extend the idea for unweighted graphs to weighted ones. Precisely, let G be the set of all possible graphs considered in
−a and nb in absolute value.
Theorem 1.2, i.e., the set of graphs on [n] having at most m edges whose weights are between n
−(a+6). A sequence S of queries separates a set of graphs if
Let I be the set of graphs in G with weights in multiples of n
it separates every pair of graphs in the set with different sets of edges. We prove the existence of a sequence of queries
separating I by more than m
na+6 . As in the previous section, MS (G) denotes the vector whose coordinates are given by the
answers of a sequence S of queries for a graph G.

Lemma 5.1. There exists a sequence S of (cid:6) cross-additive queries such that (cid:6) = O( m log n

log m ) and

(cid:8)
(cid:8)
(cid:8)MS (G) − MS (H)
(cid:8)

∞ >

m
na+6

for all G, H ∈ I with E(G) (cid:7)= E(H).

Given such a sequence S, the following holds. We omit the proof as it is essentially the same as that of Lemma 3.2.

Lemma 5.2. Suppose that G is a graph in G.

(i) There exists H ∈ I such that E(G) = E(H) and

(cid:8)
(cid:8)
(cid:8)MS (G) − MS (H)
(cid:8)

(cid:3) m

2na+6

.

∞

(ii) For all I ∈ I with (cid:8)MS (G) − MS (I)(cid:8)∞ (cid:3) m

2na+6 , E(I) = E(G).

Lemmas 5.1 and 5.2 imply Theorem 1.2 by the following algorithm: Given an unknown graph G in G, compute MS (G)
2na+6 . The set of edges in H is the one that we are looking for. In this

and ﬁnd H ∈ I such that (cid:8)MS (G) − MS (H)(cid:8)∞ (cid:3) m
algorithm, O( m log n

log m ) queries are required only in computing MS (G) and they are non-adaptive.

In the rest of this section, we prove Lemma 5.1. For two graphs G, H ∈ I, deﬁne the difference graph G − H to be the
weighted graph of which vertex set is [n] and edge set consists of those edges e with w G (e) − w H (e) (cid:7)= 0. Here, we use the
convention that w G (e) = 0 if e is not an edge of a weighted graph G. The weight of an edge e in G − H is w G (e) − w H (e).
Let D be the set of difference graphs G − H for G, H ∈ I with E(G) (cid:7)= E(H). Then, the following lemma implies Lemma 5.1.
As in the previous section, a random query (S, T ) means a cross-additive query for a pair of random sets of vertices S, T
such that each vertex independently belongs to S, T , and none of S and T , each with probability 1

3 so that S ∩ T = ∅.

Lemma 5.3 (Main lemma). There exists a constant c > 0 such that

Pr

There is G in D such that

(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

(cid:3) m
na+6

∞

(cid:10)

= o(1)

for a sequence S of (cid:6) = cm log n

log m independent random queries.

Notice that there may be edges of very small weight, e.g.,

1
na+6 , in absolute value. For that reason, it seems hard to prove
Lemma 5.3 by dividing the graphs in D regardless of weights of edges as in the proof for unweighted graphs. We divide D
into four classes D1, D2, D3, and D4 based on weights of edges. For the division of cases, we need to classify edges into
in absolute value. An edge is
three types according to their weights. An edge is called heavy if its weight is at least
called light if its weight is less than m
na+6 in absolute value. An edge that is neither heavy nor light is called middleweight. The
class D1 consists of graphs in D that have only few heavy edges, namely, less than m
log m heavy edges. The class D2 consists
of graphs in D that have many vertices incident with many heavy or middleweight edges. To be precise, let δ = 0.1 and
deﬁne U (G) for a weighted graph G to be the set of the vertices that are incident with at least mδ heavy or middleweight
edges. The class D2 is the set of graphs G in D with the size of U (G) being mδ or more. Deﬁne W (G) = [n] \ U (G). The
class D3 is the set of graphs G in D \ (D1 ∪ D2) such that there are at least
2 log m heavy edges crossing between U (G) and
W (G). The class D4 is D \ (D1 ∪ D2 ∪ D3). We consider four cases G ∈ Di , i = 1, . . . , 4, and show that

1
8mna

m

Pr

There is G in Di such that

(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

(cid:3) m
na+6

∞

(cid:10)

= o(1)

provided m (cid:2) (log n)α for a large constant α. The condition for the range of m is only used to derive the result for the
second case, G ∈ D2 (see Lemma 5.4).

(cid:9)

(cid:9)

Case 1. G ∈ D1.

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

563

For G ∈ D1, let ˜G be the graph obtained by removing the middleweight and light edges from G. Let
4na , S separates D1 by more than m

˜D1 denote the set
of such graphs ˜G. It turns out that if S separates ˜D1 by at least
na+6 . We ﬁrst derive
−a in absolute value as G is a
an inequality for
difference graph of two graphs in I with different sets of edges. Thus, every H ∈ ˜D1 has such an edge and Pr[|μS,T (H)| <
˜D1 is at most
4na ] for a random query (S, T ) is at most 8/9 by the same argument used in Lemma 4.3. As the size of
1
(cid:7)(cid:12) m
log m
i=1

˜D1. Notice that for G ∈ D1, there is an edge in G of weight at least n

(cid:3)
(cid:2)
(n
2)
(4na+b+6)i = e
i

O( m log n
log m )

1

(cid:13)

(cid:9)

Pr

There is H in ˜D1 such that

, the union bound yields
(cid:10)

(cid:8)
(cid:8)
(cid:8)MS (H)
(cid:8)

∞ <

1
4na

(cid:3) | ˜D1|

(cid:13)(cid:6)

(cid:12)

8

9

= o(1)

(5)

for a large enough constant c in (cid:6) = cm log n
log m .

Since G − ˜G consists of at most 2m − 1 middleweight or light edges,
(cid:8)
(cid:8)
(cid:8)
(cid:8)MS (G − ˜G)

(cid:8)
(cid:8)
(cid:8)
(cid:8)MS ( ˜G)

(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

(cid:8)
(cid:8)
(cid:8)
(cid:8)MS ( ˜G)

−

(cid:2)

(cid:2)

∞

∞

∞

− 2m − 1
8mna

.

∞

4na and (5) imply
(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

(cid:10)

(cid:3) m
na+6

∞

The fact that m
na+6
(cid:9)

+ 2m−1

8mna < 1

Pr

There is G in D1 such that

(cid:9)
(cid:3) Pr

There is ˜G in ˜D1 such that

Case 2. G ∈ D2.

(cid:8)
(cid:8)
(cid:8)MS ( ˜G)
(cid:8)

(cid:3) m
na+6

∞

+ 2m − 1
8mna

(cid:10)

= o(1).

The proof for D2 is analogous to that of the second case for unweighted graphs. Let us ﬁrst show that there is a set of

vertices incident with many edges of weights in some range while the weights of edges in the set are properly bounded.

Lemma 5.4. For some constants α > 0 and m0 > 0, the following holds provided m (cid:2) m0 and m (cid:2) (log n)α : Suppose that G is a graph
in D2. Then, for s = m

na+6 , there exist Q ⊆ [n] and i (cid:2) 0 such that

(i) the size of Q is (cid:12)m0.1δ(cid:13),
(ii) each edge with both ends in Q has a weight less than 2i+1s in absolute value, and
(iii) each vertex in Q is incident with at least m0.8δ edges with weights in [2i s, 2i+1s) in absolute value.

Proof. An edge in G is called type i if the absolute value of its weight is in [2i s, 2i+1s). For each vertex v of G, denote
di(v) to be the number of edges of type i containing v. Notice that a vertex v ∈ U (G) is contained in at least mδ edges
with weights s = m
na+6 or more in absolute value and all weights of edges are at most 2nb in absolute value. Let t be the
t−1
i=0 di(v) (cid:2) mδ and t = O(log n). Hence, we choose a large enough α > 0 so
minimum integer such that 2t s > 2nb. Then,
that there is an integer i with 0 (cid:3) i (cid:3) t − 1 such that di(v) (cid:2) m0.8δ if m (cid:2) (log n)α . We decompose U (G) into U 0, . . . , U t−1
so that v ∈ U i for the largest i with di(v) (cid:2) m0.8δ . As the size of U (G) is at least mδ , there is i such that the size of U i is at
least m0.95δ . Notice that all vertices in U i satisfy the property (iii).

(cid:7)

To construct the desired vertex set Q , we iteratively take a vertex u in U i and delete u and all vertices in U i that are
adjacent to u by an edge of type j for some j > i. Then, each time at most (t − i − 1)m0.8δ + 1 (cid:3) m0.85δ vertices in U i are to
be deleted and hence we are able to take (cid:12)m0.1δ(cid:13) vertices in U i , which is the property (i). The property (ii) follows by the
construction. (cid:2)

Clearly, there are at most
following lemma as (cid:6) = cm log n

(cid:7)

(cid:2)

(cid:3)
(n
2)
(4na+b+6)i = e
i

2m
i=1

O(m log n) graphs in D2. The desired bound for D2 is obtained by the

log m . The proof uses Lemma 5.4 and is essentially the same as that of Lemma 4.4.

Lemma 5.5. For some constants α > 0 and m0 > 0, the following holds provided m (cid:2) m0 and m (cid:2) (log n)α : Suppose that G is a graph
in D2 and (S, T ) is a random query. Then,
(cid:11)
(cid:11)
(cid:11) (cid:3) m
(cid:11)μS,T (G)
na+6

−0.01δ.

(cid:3) m

Pr

(cid:10)

(cid:9)

564

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

Proof. For G ∈ D2, we choose Q and i satisfying the properties in Lemma 5.4. Denote by H the spanning subgraph of G
consisting of the edges in G except those with both ends in Q . By (i) and (ii) of Lemma 5.4, the sum of weights of edges
with both ends in Q is at most
(cid:11)
(cid:11)
(cid:11)μS,T (H)
(cid:11) (cid:3)

(cid:3)
(cid:2)|Q |
2i+1s (cid:3) 2i sm0.2δ in absolute value. Thus, |μS,T (G)| (cid:3) m
na+6
(cid:11)
(cid:11)
(cid:11) (cid:3) s + 2i sm0.2δ (cid:3) 2i+1sm0.2δ
(cid:11)μS,T (G − H)

(cid:11)
(cid:11)
(cid:11) +
(cid:11)μS,T (G)

= s implies

2

as i (cid:2) 0. To obtain the inequality, it is enough to show
(cid:14)(cid:11)
(cid:11)
(cid:11)μS,T (H)
(cid:11) (cid:3) 2i+1sm0.2δ

−0.01δ.

(cid:3) m

Pr

(cid:15)

For a pair of disjoint sets ( A, B) of vertices in ¯Q := [n] \ Q , we say a vertex v ∈ Q is bad for ( A, B) if the sum of
weights of edges in H crossing between A and v is less than 2i+1sm0.2δ in absolute value, that is, |μ A,v (H)| < 2i+1sm0.2δ .
A pair ( A, B) is called bad if there is a bad vertex in Q for ( A, B). It is good otherwise. Letting ( ˜S, ˜T ) = (S ∩ ¯Q , T ∩ ¯Q ),
Pr[|μS,T (H)| (cid:3) 2i+1sm0.2δ] is upper bounded by
(cid:15)
(cid:14)
( ˜S, ˜T ) is bad
Pr

(cid:14)(cid:11)
(cid:11)
(cid:11)μS,T (H)
(cid:11) (cid:3) 2i+1sm0.2δ

(cid:15)
(cid:14)
( ˜S, ˜T )
Pr

(cid:11)
(cid:15)
(cid:11)( ˜S, ˜T )
.

(cid:5)

Pr

+

( ˜S, ˜T ) good

Thus, it is enough to show that each of the above two terms is polynomially small in m, say m

For a vertex v ∈ Q , μ ˜S,v (H) is a sum of weights of edges {v, u} with u ∈ ¯Q chosen independently and with probability 1
3 .
edges {v, u} with |w H (uv)| (cid:2) 2i s, the generalized Littlewood–Offord theorem gives

As there are at least m0.8δ − |Q | (cid:2) m0.8δ
2

−0.04δ .

(cid:14)
Pr

(cid:15)
v is bad for ( ˜S, ˜T )

= Pr

(cid:14)(cid:11)
(cid:11)
(cid:11)μ ˜S,v (H)
(cid:11) < 2i+1sm0.2δ

(cid:15)

(cid:3) m

−0.15δ.

Thus, the union bound yields

(cid:14)
(cid:15)
There is a bad vertex in Q for ( ˜S, ˜T )
Pr

−0.05δ.

(cid:3) |Q | · m

−0.15δ (cid:3) m
Now, we show that, given the condition that ( ˜S, ˜T ) is a ﬁxed pair without a bad vertex in Q , the probability of
|μS,T (H)| (cid:3) 2i+1sm0.2δ is polynomially small in m. Since there is no edge of H with both ends in Q , μS,T (H) is μ ˜S, ˜T (H)
plus a sum of contributions Y v of v over v ∈ Q such that Y v is μv, ˜T (H), μ ˜S,v (H), and 0 when v belongs to S, T , and none
of them, respectively. The contribution Y v changes at least |μ ˜S,v (H)| (cid:2) 2i+1sm0.2δ according to v ∈ T or v /∈ S ∪ T , each of
3 . As Y v ’s are independent for ﬁxed ( ˜S, ˜T ) and the size of Q is (cid:12)m0.1δ(cid:13), the probability of
which occurs with probability 1
−0.04δ by the generalized Littlewood–Offord theorem. That is, for any good pair ( ˜S, ˜T ),
|μS,T (H)| (cid:3) 2i+1sm0.2δ is at most m
(cid:14)(cid:11)
(cid:11)
(cid:11)
(cid:15)
(cid:11)μS,T (H)
(cid:11)( ˜S, ˜T )
(cid:11) (cid:3) 2i+1sm0.2δ

−0.04δ.

(cid:3) m

Pr

(cid:2)

Case 3. G ∈ D3.

The proof is analogous to that of the third case for unweighted graphs. We start with the following lemma similar to

Lemma 4.5.

Lemma 5.6. For suﬃciently large m, the following holds: Suppose that G is a graph in D3. Then, there is a pair of disjoint sets of vertices
( A, B) such that

(i) 0 < | A| (cid:3) 2m2δ and |B| = (cid:12)mδ(cid:13),
(ii) for each v ∈ B, there is u ∈ A that is adjacent to v by a heavy edge, and
(iii) for all u ∈ B and v ∈ [n] \ A, u and v are joined only by a light edge.

m

2 log m . Since each vertex in W (G) is incident to less than mδ heavy edges, there are ( m

Proof. We construct the desired pair ( A, B). As G ∈ D3, the number of heavy edges crossing between U (G) and W (G) is
2 log m )/mδ (cid:2) m8δ vertices in
at least
W (G) that are adjacent to at least one vertex in U (G) by a heavy edge in G. Thus, we may iteratively choose (cid:12)mδ(cid:13) such
vertices that are not adjacent to each other by heavy or middleweight edges, as vertices in W (G) are incident to at most
mδ heavy or middleweight edges and m8δ/(1 + mδ) (cid:2) mδ . The set B consists of these vertices.

The set A consists of vertices in U (G) and vertices in W (G) \ B that are adjacent to a vertex in B by a heavy or
middleweight edge. Since G /∈ D2 and there are at most |B|mδ vertices in W (G) \ B described above, | A| (cid:3) |U (G)| + |B|mδ (cid:3)
2m2δ . Hence the properties (i)–(iii) follow. (cid:2)

For each graph G in D3, we choose a pair of disjoint sets of vertices ( A, B) satisfying the properties in Lemma 5.6
and H(G) denotes the bipartite subgraph of G on A ∪ B consisting of all edges in G between them. Recall that S is the
sequence of random queries (S, T ). Once all vertices in ¯B := V (G) \ B are determined to be in S, T or none of them for

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

565

the random queries (S, T ), MS (G[ ¯B]) is determined, where G[ ¯B] is the induced subgraph of G on ¯B. It is expected that
(cid:8)MS (H(G)) + MS (G[ ¯B])(cid:8)∞ plays a major role in (cid:8)MS (G)(cid:8)∞. This is so since

(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

(cid:2)

∞

(cid:8)
(cid:8)MS

(cid:2)

(cid:3)
H(G)

+ MS

(cid:8)
(cid:8)MS

(cid:2)

(cid:3)
H(G)

(cid:2)

+ MS

(cid:2)

(cid:2)

(cid:3)(cid:8)
(cid:8)

G[ ¯B]

(cid:3)(cid:8)
(cid:8)

G[ ¯B]

∞

∞

− m
na+6
− 1

10na+2

(cid:12)

|B|2
4

(cid:13)

+ |B|n

.

(6)

Let

˜S be the sequence of restricted random queries ( ˜S, ˜T ) on ¯B. Then MS (G[ ¯B]) is M ˜S (G[ ¯B]) as MS (G[ ¯B]) depends
only on ˜S. The quantity (cid:8)MS (H(G)) + M ˜S (G[ ¯B])(cid:8)∞ turns out to be large, namely at least
na+6 , except for
a small enough probability for most of ˜S. For the set H of all possible weighted bipartite graphs H , say on A ∪ B, satisfying
(i) and (ii) in Lemma 5.6, we divide the event {∃G ∈ D3 such that (cid:8)MS (G)(cid:8)∞ (cid:3) m
} into events F H := {∃G ∈ D3 with
na+6
H(G) = H such that (cid:8)MS (G)(cid:8)∞ (cid:3) m
na+6
(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

}, H ∈ H. Then,

8mna > 1

∃G ∈ D3 such that

Pr[F H ].

(cid:9) (cid:16)

= Pr

10na+2

+ m

(cid:5)

F H

Pr

(cid:3)

(cid:10)

(cid:9)

(cid:10)

1

(cid:3) m
na+6

∞

(cid:7)

H∈H

H∈H

(cid:5)

To bound Pr[F H ], we further consider Pr[F H ] =
of

˜S.
For a given graph H ∈ H on A ∪ B and the restricted random query ( ˜S, ˜T ) on ¯B, a vertex v ∈ B is called bad, if the sum
16mna . The restricted random query ( ˜S, ˜T )
i=1 is called bad if the number of bad queries

of weights of edges crossing between v and ˜S ∩ A is negligible, i.e., |μv, ˜S∩ A(H)| < 1
is bad if more than 3
4
( ˜S i, ˜T i) is more than 9(cid:6)

|B| vertices of B are bad for it. The sequence ˜S = ( ˜S i, ˜T i)(cid:6)
10 . The sequence is good if it is not bad. We have

˜S Pr[ ˜S] Pr[F H | ˜S], and will show that Pr[F H | ˜S] is small enough for most

Pr[F H ] (cid:3) Pr[ ˜S bad] + Pr[F H and ˜S good] = Pr[ ˜S bad] +

Pr[ ˜S] Pr[F H | ˜S].

(7)

Let us ﬁrst show that Pr[ ˜S bad] is small enough.

˜S good

Lemma 5.7. For a given H ∈ H on A ∪ B, there is a constant β < 1 such that Pr[ ˜S bad] (cid:3) β(cid:6).

Proof. For a vertex v ∈ B,
(cid:9)
(cid:11)
(cid:11)
(cid:11) <
(cid:11)μv, ˜S∩ A(H)
Pr[v is bad] = Pr

(cid:10)

(cid:3) 2
3

1
16mna

8mna according to u ∈ ˜S or not.
as v is adjacent to a vertex u ∈ A by a heavy edge and so |μv, ˜S∩ A(H)| changes by at least
|B| and hence the probability that more than 3|B|/4 vertices
Thus, the expected number of bad vertices in B is at most 2
3
in B is bad is at most 8/9 by Markov’s inequality. This implies that the expected number of bad queries ( ˜S i, ˜T i) in ˜S is
less than or equal to 8(cid:6)/9. Since ( ˜S i, ˜T i) are i.i.d., Chernoff’s large deviation inequality yields Pr[ ˜S bad] (cid:3) β(cid:6) for a constant
β < 1. (cid:2)

1

To bound the second part of (7), notice that (6) and

Pr[F H | ˜S] (cid:3)

(cid:5)

(cid:9)

Pr

G:H(G)=H

(cid:8)
(cid:8)MS (H) + M ˜S

(cid:2)

(cid:3)(cid:8)
(cid:8)

G[ ¯B]

We show

1

8mna > 1
10na+2
(cid:11)
(cid:11)
(cid:11) ˜S

(cid:3) 1

na+6 imply

+ m
(cid:10)

.

(8)

∞

8mna

Lemma 5.8. For suﬃciently large m, the following holds: Suppose that ˜S is a good sequence of restricted random queries ( ˜S i, ˜T i) on ¯B.
Then,

(cid:9)

Pr

(cid:8)
(cid:8)MS (H) + M ˜S

(cid:2)

(cid:3)(cid:8)
(cid:8)

G[ ¯B]

∞

(cid:11)
(cid:11)
(cid:11) ˜S

(cid:3) 1

8mna

(cid:10)

(cid:3) m

−δ(cid:6)/30.

Proof. Suppose that ( ˜S i, ˜T i) is good and consider the fully extended random query (S i, T i). Then, there are at least (cid:12) |B|
m0.9δ good vertices in B and for a good vertex v ∈ B, the answer of the query (S i, T i) changes by at least
on whether v ∈ T or v /∈ S ∪ T , each of which occurs with probability 1

16mna depending
3 , independently of the other vertices in B. Thus, the

(cid:13) (cid:2)

4

1

566

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

answer of (S i, T i) is at most
theorem. Since at least (cid:6)/10 ( ˜S i, ˜T i)’s are good and (S i, T i)’s are independent, the lemma follows. (cid:2)

in absolute value with probability m

1
8mna

−0.4δ or less by the generalized Littlewood–Offord

Since there are e

O(m log n) graphs in D3, the above lemma and (8) imply that
(cid:13)

Pr[F H | ˜S] (cid:3) exp

+ O(m log n)

(cid:3) exp(−m log n),

(cid:12)
− δ(cid:6) log m
30

for large enough c in (cid:6) = cm log n
we ﬁnally have

log m . With this inequality and Lemma 5.7, (7) gives Pr[F H ] (cid:3) β(cid:6)/2 for suﬃciently large n and

(cid:9)

Pr

∃G ∈ D3 such that

(cid:8)
(cid:8)
(cid:8)MS (G)
(cid:8)

(cid:3) m
na+6

∞

(cid:10)

(cid:3)

(cid:5)

H∈H

Pr[F H ] (cid:3) |H|β(cid:6)/2.

Since, for the graphs in H, the number of edges is at most 2m3δ and the number of possible edge weights (including 0) is
at most 4nb+a+6, we have

|H| (cid:3)

(cid:2)

4nb+a+6

(cid:3)

2m3δ

(cid:12)

n
mδ

(cid:13) 2m2δ(cid:5)

(cid:13)

(cid:12)
n

i

i=1

−(cid:6)/4

(cid:3) β

and the desired inequality follows.

Case 4. G ∈ D4.

The proof is analogous to that of the fourth case for unweighted graphs. Let us ﬁrst show that for G ∈ D4, there is a

matching of a number of heavy edges if we do not consider the light edges in G.

Lemma 5.9. For suﬃciently large m, the following holds: Suppose that G is a graph in D4. Then, G has (cid:12)mδ(cid:13) pairwise disjoint heavy
edges (regarding edges as sets of two vertices) such that any other edge joining two vertices in the heavy edges is light.

Proof. Notice that, as G /∈ D1 ∪ D2 ∪ D3, there are at least
4 log m heavy edges with both ends in W (G) and each vertex in
W (G) is incident with less than mδ heavy or middleweight edges. Thus, we are able to iteratively choose a heavy edge e
and delete e and all the heavy and middleweight edges sharing a vertex with e at least

m

(cid:12)

m

4 log m

(cid:13)

(cid:2)

/

2mδ + 1

(cid:3)

(cid:2) mδ

times. The ﬁrst (cid:12)mδ(cid:13) edges chosen are the desired edges. (cid:2)

Since there are e

O(m log n) graphs in D4, the following lemma yields the desired bound.

Lemma 5.10. For suﬃciently large m, the following holds: Suppose that G is a graph in D4 and (S, T ) is a random query. Then,

(cid:9)

Pr

(cid:11)
(cid:11)
(cid:11) (cid:3) m
(cid:11)μS,T (G)
na+6

(cid:10)

(cid:3) m

−0.4δ.

Proof. According to Lemma 5.9, we may take (cid:12)mδ(cid:13) pairwise disjoint heavy edges e1, . . . , e(cid:12)mδ (cid:13) of G such that any other
edge joining two vertices contained in the heavy edges is light. Let B be the set of the vertices contained in the heavy
edges. Deﬁne H to be the spanning subgraph of G consisting of the edges in G except for the light edges with both ends
in B. Since the graph G − H has only the light edges with both ends in B and |B| = 2(cid:12)mδ(cid:13),

(cid:11)
(cid:11)
(cid:11) (cid:3)
(cid:11)μS,T (G − H)

(cid:13)

(cid:12)

2mδ
2

m
na+6

and so |μS,T (G)| (cid:3) m

(cid:11)
(cid:11)
(cid:11)μS,T (H)
(cid:11) (cid:3)

na+6 implies
(cid:11)
(cid:11)
(cid:11) +
(cid:11)μS,T (G)

(cid:11)
(cid:11)
(cid:11) (cid:3)
(cid:11)μS,T (G − H)

(cid:12)(cid:12)

2mδ
2

(cid:13)

(cid:13)

+ 1

m
na+6

<

1
8mna

.

To obtain the inequality, it is enough to show that

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

567

(cid:9)

Pr

(cid:11)
(cid:11)
(cid:11) <
(cid:11)μS,T (H)

(cid:10)

1
8mna

(cid:3) m

−0.4δ.

We consider a random query (S, T ) conditioned on ˜S := S ∩ ¯B, ˜T := T ∩ ¯B, where ¯B := [n] \ B. That is, all vertices in ¯B
have been determined to be in S, T , or none of them. For each heavy edge ei = {ui, v i} in B, let Xi be the random variable
representing the contribution of ui and v i to the value μS,T (H) so that

μS,T (H) =

(cid:12)mδ (cid:13)(cid:5)

i=1

Xi + μ ˜S, ˜T (H).

(H) or μui , ˜T (H) + μ ˜S,v i

This is possible since ei ’s are the only edges of H with both ends in B and they are pairwise disjoint. For each i, Xi may be 0,
(H) + w G (ei) each with probability 1/9, corresponding to the events {ui, v i /∈ S ∪ T },
μui , ˜T (H), μ ˜S,v i
{ui ∈ S, v i /∈ S ∪ T }, {ui /∈ S ∪ T , v i ∈ T } and {ui ∈ S, v i ∈ T }, respectively. Since ei is heavy, i.e., |w G (ei)| (cid:2) 1
8mna , at least one
of the last three values is
24mna each with probability at least
i=1 Xi + μ ˜S, ˜T (H) is in a ﬁxed interval of length

24mna or more in absolute value. In particular, Xi = 0, | Xi| (cid:2) 1

1/9. By the generalized Littlewood–Offord theorem, the probability that
4mna is at most m

−0.4δ and hence so is the probability that |μS,T (H)| < 1

8mna . (cid:2)

(cid:7)(cid:12)mδ (cid:13)

1

1

6. Finding Fourier coeﬃcients

The Walsh transform is a Fourier transform for the space of pseudo-Boolean functions in which a pseudo-Boolean func-
tion is represented as a linear combination of basis functions called Walsh functions [40]. For each subset S of [n], the Walsh
function corresponding to S, ψS : {0, 1}n → R, is deﬁned as

(cid:7)

ψS (x) = (−1)

i∈S xi

for x ∈ {0, 1}n. If we deﬁne an inner product of two pseudo-Boolean functions f and g as

(cid:18) f , g(cid:19) =

(cid:5)

x∈{0,1}n

f (x)g(x)
2n

,

the set of Walsh functions, {ψS | S ⊆ [n]}, becomes an orthonormal basis of the space of pseudo-Boolean functions. Hence,
a pseudo-Boolean function f can be represented as

f =

(cid:5)

S⊆[n]

(cid:20)f (S)ψS ,

where (cid:20)f (S) = (cid:18) f , ψS (cid:19) is called the Fourier coeﬃcient corresponding to S.

Now, we prove Corollary 1.4. Suppose that f

is a 2-bounded function satisfying the condition in Corollary 1.4. Then, the

Fourier coeﬃcients (cid:20)f (S) of f corresponding to S with |S| (cid:2) 3 are 0. Decompose f as follows:

f = (cid:20)f (∅) + f 1 + f 2,

where

(cid:5)

f 1 =

S⊆[n]: |S|=1

(cid:20)f (S)ψS

and

f 2 =

(cid:5)

(cid:20)f (S)ψS .

S⊆[n]: |S|=2

To ﬁnd the Fourier coeﬃcients of
coeﬃcient (cid:20)f (∅).

f , we separately ﬁnd the Fourier coeﬃcients of

f 2 and f 1 and then ﬁnd the Fourier

First, deﬁne G f as the weighted graph such that (i) V (G f ) = [n], (ii) for each pair e of vertices, e ∈ E(G f ) if and only if
(cid:20)f (e) (cid:7)= 0, and (iii) w G f (e) = (cid:20)f (e) for all e ∈ E(G f ). Since the weights of edges of G f are precisely the Fourier coeﬃcients of
f 2, consider the problem of ﬁnding the weighted graph G f . For a cross-additive query (S, T ), we have

μS,T (G f ) =

(cid:5)

e=uv: u∈S,v∈T

(cid:20)f (e) = f (0n) − f (1S ) − f (1T ) + f (1S∪T )

4

,

where 0n is the vector consisting of n zeros and 1 A for a subset A ⊆ [n] represents the vector consisting of 1 in the coor-
dinates in A and 0 in the rest. This means that any cross-additive query for G f can be answered by 4 function evaluations
of f . Since f has at most m non-zero Fourier coeﬃcients, G f has at most m edges. Hence, by Theorem 1.2, there exists a
log m ) function evaluations. For each edge in G f , its weight can be
non-adaptive algorithm to ﬁnd the edges of G f
found by one cross-additive query. Thus, all the weights of edges in G f can be found in O(m) function evaluations in a

in O( m log n

568

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

non-adaptive way. Overall, there exists a 2-round algorithm to ﬁnd the weights of edges in G f , consequently, the Fourier
coeﬃcients of f 2 in O( m log n

log m ) function evaluations.

Let x f be the n-dimensional vector whose ith element is the Fourier coeﬃcient (cid:20)f ({i}). That is, the elements of x f are

precisely the Fourier coeﬃcients of f 1. For q ∈ {0, 1}n, the following holds:

χq(x f ) = ( f (0n) − f 2(0n)) − ( f (q) − f 2(q))

2

.

If we regard x f as the vector representing the weights of n coins, this equation means that any coin weighing for x f can
be simulated by a constant number of function evaluations of f and f 2. Since f 2 is known by the algorithm for ﬁnding G f
and the value of f (0n) is evaluated by the algorithm, only one function evaluation of f , i.e.,
f (q), is required to know the
value of χq(x f ). The function f 1 has at most m non-zero Fourier coeﬃcients and so |sp(x f )| (cid:3) m. Hence, the non-adaptive
log m ) function evaluations. The value of each non-
algorithm in Theorem 1.3 identiﬁes the non-zero elements of x f
zero element of x f can be found by one coin weighing, i.e., one function evaluation of f . Thus, all the values of non-zero
elements of x f can be found in O(m) function evaluations in a non-adaptive way. Overall, there exists a 2-round algorithm
to ﬁnd x f , consequently, the Fourier coeﬃcients of f 1 in O( m log n

in O( m log n

log m ) function evaluations.

Once we obtain f 2 and f 1, we get the value of (cid:20)f (∅) by (cid:20)f (∅) = f (0n) − f 2(0n) − f 1(0n). This can be done without
additional function evaluations of f , since the value of f (0n) is evaluated by the algorithm for ﬁnding G f . Combining the
two algorithms for ﬁnding G f and x f along with this ﬁnal step, we obtain an algorithm to ﬁnd the Fourier coeﬃcients of
f using O( m log n
log m ) function evaluations. Since each of the two algorithms is a 2-round algorithm, the combined algorithm is
a 4-round algorithm, which proves Corollary 1.4.

7. Concluding remarks

In this paper, we proved the existence of optimal algorithms for the graph ﬁnding problem and two related problems.
Our results for weighted graphs are based on the condition that the weights of edges in absolute value (or corresponding
ones for the other problems) are bounded above by a polynomial in n and below by an inverse polynomial in n. The essential
part of the condition used in our proofs is that the ratio between the minimum and maximum is polynomially bounded
above in n. The polynomial ratio is best possible to derive the optimal bounds in our proofs and as yet we have no idea
how to obtain optimal bounds for more general conditions.

For ﬁnding weighted graphs, we obtained the results under the condition that m is at least a (large) constant power of
log n. Getting the results for smaller m is still open for cross-additive queries and so is for the problem of ﬁnding Fourier
coeﬃcients. (For additive queries, see [8] that appeared in the review process of this paper.) Also, although the proposed
algorithms are optimal, ﬁnding an explicit construction of optimal algorithms is still an open problem and, we think, an
important research direction. Another important issue in practical applications is the time complexity of the algorithms. In
this paper, we focused only on the query complexity and did not try to optimize the time complexity. It would be worth
trying to ﬁnd an optimal algorithm with a reasonable time complexity.

A natural extension of the graph ﬁnding problem is the hypergraph ﬁnding problem, especially when the hypergraph
is k-uniform for k = 3, 4, . . . . An algorithm for the problem would be useful to ﬁnd the Fourier coeﬃcients of certain
k-bounded pseudo-Boolean functions as described in Section 6.

References

[1] M. Aigner, Combinatorial Search, Wiley, New York, 1988.
[2] N. Alon, V. Asodi, Learning a hidden subgraph, SIAM Journal on Discrete Mathematics 18 (4) (2005) 697–712.
[3] N. Alon, R. Beigel, S. Kasif, S. Rudich, B. Sudakov, Learning a hidden matching, SIAM Journal on Computing 33 (2) (2004) 487–501.
[4] D. Angluin, J. Chen, Learning a hidden graph using O(log n) queries per edge, in: Proceedings of the 17th Annual Conference on Learning Theory (COLT

2004), Banff, Canada, 2004, pp. 210–223.

[5] D. Angluin, J. Chen, Learning a hidden hypergraph, Journal of Machine Learning Research 7 (2006) 2215–2236.
[6] R. Beigel, N. Alon, M.S. Apaydin, L. Fortnow, S. Kasif, An optimal procedure for gap closing in whole genome shotgun sequencing, in: Proceedings of

the Fifth Annual International Conference on Computational Molecular Biology (RECOMB 2001), Montreal, Canada, 2001, pp. 22–30.

[7] M. Bouvel, V. Grebinski, G. Kucherov, Combinatorial search on graphs motivated by bioinformatics applications: A brief survey, in: Proceedings of the

31st International Workshop on Graph-Theoretic Concepts in Computer Science (WG 2005), Metz, France, 2005, pp. 16–27.

[8] N.H. Bshouty, H. Mazzawi, Reconstructing weighted graphs with minimal query complexity, in: Proceedings of the 20th International Conference on

Algorithmic Learning Theory (ALT 2009), Porto, Portugal, 2009, pp. 97–109.

[9] N.H. Bshouty, C. Tamon, On the Fourier spectrum of monotone functions, Journal of the ACM 43 (4) (1996) 747–770.

[10] H. Chernoff, A measure of asymptotic eﬃciency for tests of a hypothesis based on the sum of observations, Annals of Mathematical Statistics 23 (1952)

493–509.

[11] S.S. Choi, K. Jung, J.H. Kim, Almost tight upper bound for ﬁnding Fourier coeﬃcients of k-bounded pseudo-Boolean functions, in: Proceedings of the

21st Annual Conference on Learning Theory (COLT 2008), Helsinki, Finland, 2008, pp. 123–134.

[12] S.S. Choi, K. Jung, B.R. Moon, Lower and upper bounds for linkage discovery, IEEE Trans. on Evolutionary Computation 13 (2) (2009) 201–216.
[13] S.S. Choi, J.H. Kim, Optimal query complexity bounds for ﬁnding graphs, in: Proceedings of the 40th ACM Symposium on Theory of Computing (STOC

2008), Victoria, Canada, 2008, pp. 749–758.

[14] P. Erd ˝os, On a lemma of Littlewood and Offord, Bulletin of the American Mathematical Society 51 (1945) 898–902.

S.-S. Choi, J.H. Kim / Artiﬁcial Intelligence 174 (2010) 551–569

569

[15] C.G. Esseen, On the Kolmogorov–Rogozin inequality for the concentration function, Z. Wahrscheinlichkeitstheorie Verw. Geb. 5 (1966) 210–216.
[16] C.G. Esseen, On the concentration function of a sum of independent random variables, Z. Wahrscheinlichkeitstheorie Verw. Geb. 9 (1968) 290–308.
[17] W. Fontana, P. Stadler, E. Bornberg-Bauer, T. Griesmacher, I. Hofacker, M. Tacker, P. Tarazona, E. Weinberger, P. Schuster, RNA folding and combinatory

landscapes, Physical Review E 47 (3) (1993) 2083–2099.

[18] D.E. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learning, Addison–Wesley, Reading, Massachusetts, 1989.
[19] V. Grebinski, On the power of additive combinatorial search model, in: Proceedings of the 4th Annual International Conference on Computing and

Combinatorics (COCOON 1998), Taipei, Taiwan, 1998, pp. 194–203.

[20] V. Grebinski, G. Kucherov, Reconstructing a Hamiltonian cycle by querying the graph: Application to DNA physical mapping, Discrete Applied Mathe-

matics 88 (1998) 147–165.

[21] V. Grebinski, G. Kucherov, Optimal reconstruction of graphs under the additive model, Algorithmica 28 (2000) 104–124.
[22] R.B. Heckendorn, A.H. Wright, Eﬃcient linkage discovery by limited probing, Evolutionary Computation 12 (4) (2004) 517–545.
[23] J. Jackson, An eﬃcient membership-query algorithm for learning DNF with respect to the uniform distribution, Journal of Computer and System

Sciences 55 (3) (1997) 42–65.

[24] H. Kargupta, B. Park, Gene expression and fast construction of distributed evolutionary representation, Evolutionary Computation 9 (1) (2001) 1–32.
[25] S.A. Kauffman, Adaptation on rugged ﬁtness landscapes, in: D. Stein (Ed.), Lectures in the Sciences of Complexity, Addison–Wesley, Redwood City,

1989, pp. 527–618.

[26] S.A. Kauffman, The Origins of Order: Self-Organization and Selection in Evolution, Oxford University Press, New York, 1993.
[27] L. Le Cam, On the distribution of sums of independent random variables, in: J. Neyman, L. Le Cam (Eds.), Bernoulli, Bayes, Laplace: Anniversary
Volume, Proceedings of an International Research Seminar, Statistical Laboratory, University of California, Berkeley, 1963, Springer-Verlag, New York,
1965, pp. 179–202.

[28] B. Lindström, On B2-sequences of vectors, Journal of Number Theory 4 (1972) 261–265.
[29] B. Lindström, Determining subsets by unramiﬁed experiments, in: J.N. Srivastava (Ed.), A Survey of Statistical Designs and Linear Models, North-Holland,

Amsterdam, 1975, pp. 407–418.

[30] J.E. Littlewood, A.C. Offord, On the number of real roots of a random algebraic equation. III, Mat. Sbornik 12 (1943) 277–285.
[31] Y. Mansour, Learning Boolean functions via the Fourier transform, in: V. Roychowdhury, K.Y. Siu, A. Orlitsky (Eds.), Theoretical Advances in Neural

Computation and Learning, Kluwer Academic, Dordrecht, 1994, pp. 391–424.

[32] H. Mühlenbein, T. Mahnig, FDA– A scalable evolutionary algorithm for the optimization of additively decomposed functions, Evolutionary Computa-

tion 7 (1) (1999) 45–68.

[33] M. Pelikan, D.E. Goldberg, E. Cantú-Paz, Linkage problem, distribution estimation, and Bayesian networks, Evolutionary Computation 8 (3) (2000)

311–340.

[34] L. Reyzin, N. Srivastava, Learning and verifying graphs using queries with a focus on edge counting, in: Proceedings of the 18th International Conference

on Algorithmic Learning Theory (ALT 2007), Sendai, Japan, 2007, pp. 285–297.

[35] B.A. Rogozin, An estimate for concentration functions, Theory of Probability and Its Applications 6 (1) (1961) 94–97.
[36] B.A. Rogozin, On the increase of dispersion of sums of independent random variables, Theory of Probability and Its Applications 6 (1) (1961) 97–99.
[37] H.S. Shapiro, S. Söderberg, A combinatory detection problem, American Mathematical Monthly 70 (1963) 1066–1070.
[38] M.J. Streeter, Upper bounds on the time and space complexity of optimizing additively separable functions, in: Proceedings of the Genetic and Evolu-

tionary Computation Conference (GECCO 2004), Seattle, USA, 2004, pp. 186–197.

[39] H. Tettelin, D. Radune, S. Kasif, H. Khouri, S.L. Salzberg, Optimized multiplex PCR: Eﬃciently closing a whole-genome shotgun sequencing project,

Genomics 62 (1999) 500–507.

[40] J.L. Walsh, A closed set of orthogonal functions, American Journal of Mathematics 55 (1923) 5–24.

