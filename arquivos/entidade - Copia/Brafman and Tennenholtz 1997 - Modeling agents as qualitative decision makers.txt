ELSEVIER 

Artificial  Intelligence 94 (1997) 217-268 

Artificial 
Intelligence 

Modeling  agents  as  qualitative  decision  makers 

Ronen  I.  Brafman  a,*, Moshe  Tennenholtz  b 
a Department  of  Computer  Science,  Universify of  British  Columbia,  Vancouvel;  B.C.,  Canada  V6T  124 
b Faculty  of  Industrial  Engineering  and  Management,  Technion  -  Israel  Institute of  Technology, 
Haifa  32000,  Israel 

Abstract 

this  requires  specifying 

We  investigate  the  semantic  foundations  of  a  method  for  modeling  agents  as  entities  with  a 
mental  state  which  was  suggested  by  McCarthy  and  by  Newell.  Our  goals  are  to  formalize  this 
modeling  approach  and  its  semantics,  to  understand  the  theoretical  and  practical  issues 
that  it 
the  model’s  parameters 
raises,  and  to  address  some  of  them.  In  particular, 
and  how  these  parameters  are  to  be  assigned  (i.e.,  their  grounding).  We propose  a basic  model  in 
which  the  agent  is  viewed  as a qualitative  decision  maker  with  beliefs,  preferences,  and  a decision 
strategy;  and  we  show  how  these  components  would  determine  the  agent’s  behavior.  We  ground 
this  model  in  the  agent’s  interaction  with  the  world,  namely,  in  its  actions.  This  is  done  by  viewing 
model  construction  as a  constraint  satisfaction  problem  in  which  we  search  for  a model  consistent 
with  the  agent’s  behavior  and  with  our  general  background  knowledge.  In  addition,  we  investigate 
the  conditions  under  which  a  mental  state  model  exists,  characterizing  a  class  of  “goal-seeking” 
agents  that  can  be  modeled  in  this  manner;  and  we  suggest  two  criteria  for  choosing  between 
consistent  models,  showing  conditions  under  which  they  lead  to  a  unique  choice  of  model.  @ 
1997  Elsevier  Science  B.V. 

Keywords:  Agent  modeling;  Mental  states;  Qualitative decision  making;  Belief  ascription;  Multi-agent 
systems;  Prediction 

1.  Introduction 

This  article 

investigates 
formal  notions  of  mental 
as 
agents  are  described 

the  semantic 
state 
if  they  are  qualitative 

to  represent 

foundations 

of  a  modeling  method 

and  reason  about  agents. 

that  uses 
In  this  method, 
state 

decision  makers  with  a  mental 

* Corresponding author. Email: brafman@cs.ubc.ca. 
’ Email: moshet@ie.technion.ac.il. 

0004-3702/97/$17.00  @  1997 Elsevier  Science  B.V. All  rights  reserved. 
PIISOOO4-3702(97)00024-6 

218 

R.I.  Brajinan,  M.  Tennenholtz/Art@ial 

Intelligence  94  (1997)  217-268 

consisting  of  mental  attributes 
such  models,  which  we  refer 
[43]  and  by  Newell 
this  modeling  process, 
some  of  them. 
The  ability 

such  as  beliefs,  knowledge, 
to  as  mental-level  models,  was  proposed  by  McCarthy 

and  preferences.  The  use  of 

[45],  and  our  goals  are  to  provide  a  formal  semantic  account  of 
it  raises,  and  to  address 

some  of  the  key  issues 

to  understand 

likely 

is  useful 

In  particular, 

to  model  agents 

in  many  settings. 

in  multi-agent 
the  success  of  one’s  action  or  plan  depends  on  the  actions  of  other  agents, 
systems, 
that  are 
and  a  good  model  of  these  agents  can  help  construct  more 
to  this 
more 
agents, 
task:  they  provide  an  abstract, 
and  they  are  built  from  intuitive  and  useful  attributes, 
such  as  beliefs,  goals  and  inten- 
tions.  The  abstract  nature  of  mental-level  models  has  a  number  of  important  practical 
implications. 

informed  plans 
properties 
way  of  representing 

to  succeed.  Mental-level  models  bring 

implementation-independent 

two  promising 

( 1)  A  single 
platforms 
guages  and  who  have  followed  different  design  paradigms. 

formalism 
and  written  by  designers  who  have  used  different  programming 

can  capture  different  agents  running  on  different  hardware 
lan- 

(2)  We  may  be  able 
the  internal 
these  abstract  models. 

to  construct  mental-level  models  without  privileged 

access 
details  do  not  appear 

to 
in 

state  of  the  agent  because 

implementation 

(3)  Fewer 

lower-level  details  may  mean  faster  computation. 
is  also  important 

and  analyzing 

agents,  much 

for  theoretical 

schemes.  The  second  property, 

[40]  allows  for  abstract  analysis  of  knowledge 

The  abstract  nature  of  mental-level  models 
It  provides  a  uniform  basis  for  comparing 
Computers  as  believers  paradigm 
resentation 
since  one  approach 
that  are  difficult 
intuitive  high-level  models. 
beliefs  and  goals  are  useful  when  we  want 
or  correct  erroneous  beliefs.  These  abilities  are  sought  after  in  cooperative  multi-agent 
systems, 
areas. 

analysis. 
like  Levesque’s 
rep- 
in  design  validation 
of  agents 
into 
designs, 
in  terms  of  their 
their  goals 

is  valuable 
low-level  descriptions 

such  as  procedural  programs  or  mechanical 

intuitiveness, 
is  to  transform 

systems,  and  in  user  interfaces, 

intuitive  descriptions  of  agents 

these  agents  achieve 

to  design  validation 

to  name  but  a  few 

in  intelligent 

In  addition, 

to  analyze, 

information 

to  help 

1.1.  Issues 

in  mental-level  modeling 

Despite 

their  promising 

properties,  mental-level  modeling  has  not  been  studied  ex- 
in  AI.  The  scarcity  of  citations  on  this  issue  in  a  recent  survey  of  work  on 

tensively 
mental  states  within  AI  by  Shoham  and  Cousins 
although  Newell’s  paper  on  the  Knowledge  Level  [45]  is  among 
papers 
up  on  these 
only  exception. 

[46],  Newell 
ideas  by  the  “logicist  community”.  He  mentions  Levesque’s 

[ 51,  in  his  perspective  paper 

[58]  attests 

laments 

to  this  fact.*  Similarly, 

the  most  referenced  AI 
the  lack  of  work  following 

[40]  as  the 

Given 

this  situation, 

it  is  worth  clarifying  what  we  view  as  the  four  central  questions 

in  mental-level  modeling.  They  are: 

* The  only  modeling 

related  works  there  deal  with  plan  recognition. 

R.I.  Bra&an,  M.  Tennenholtz/Art#cial 

Intelligence  94  (1997)  217-268 

219 

(  1)  Structure-what 
(2)  Grounding-how 

class  of  models  should  we  consider? 

can  we  base  the  model  construction  process  on  a  definite  and 

ob,jective  manifestation  of  the  agent? 

(3)  Existence-under  what  conditions  will  a  model  exist? 
(4)  Choice-how 

do  we  choose  between  different  models  that  are  consistent  with 

our  data? 

The  importance  of  the  first question  is obvious,  however,  the  others  deserve  a few  words 
of  explanation. 

Many  researchers  support  an  agent  design  approach  in  which  the  designer  specifies 
an initial  (database of  beliefs,  goals,  intentions,  etc.,  which  is then  explicitly  manipulated 
by  the  agent  (e.g.,  [ 6,49,52,57] 
and  much  of  the  work  on  belief  revision).  Grounding 
may  not  seem  a  crucial  issue  for  such  work  because  human  designers  are  likely  to 
find  memal  attitudes  natural  to  specify.  However,  grounding  is  crucial  for  modeling 
applications.  The  whole  point  here  is  that  we  cannot  directly  observe  the  mental  state 
of  another  agent.  Moreover,  there  are  good  reasons  why  general  background  knowledge 
alone  will  not  do.  First,  there  is  no  reason  we  should  know  the  mental  state  of  agents 
designed  by  other  designers,  a common  case  in multi-agent  systems.  Second,  one  cannot 
always  predict  into  the  distant  future  the  mental  state  of  agents  that  learn  and  adapt, 
even  if  she  designed  these  agents.  Finally,  and perhaps  most  importantly,  what  we mean 
by  the  mental  state  of  an agent  is  not  even  clear  when  this  agent  is not  designed  using  a 
knowledg,e-based  approach,  for  example,  when  it is a  C program  or  the  result  of  training 
a  neural  net.  This  last  point  is  crucial  if  we  take  seriously  Newell’s  idea  of  mental  state 
models  as  abstract  descriptions  of  agents.  Grounding  is  important  semantically  even 
from  the  design  perspective:  it  makes  concrete  the  abstract  Kripke  semantics  [35]  that 
is  often  used  in  the  literature,  and  it  allows  us  to  answer  a  central  question  in  the 
theoretical  analysis  of  agents  and  their  design:  Does  program  X implement  mental-level 
specification  Y? 

While  grounding  has  been  discussed  by  some  authors  (see  Section  8))  the  questions 
of  model  choice,  and  in  particular,  model  existence  have  not  received  much  attention. 
We  see  rnodel  existence  as  the  central  theoretical  question  in  this  area.  Answers  to  it 
will  allow  us  to  evaluate  any  proposal  for  mental-level  models  by  telling  us  under  what 
conditi0n.s  it  is  applicable  and  hence,  what  assumptions  or  biases  we  are  making  when 
we  model  agents  in  this  manner.  Techniques  for  choosing  among  possible  models  are 
crucial  for  practical  applications,  especially  prediction,  since  different  models  may  give 
rise  to  different  predictions. 

1.2.  An  overview  of  our  approach 

Having  described  the  main  questions  in  mental-level  modeling,  we  proceed  with  an 

overview  of  this  paper  and  its  view  of  mental-level  modeling. 

Model  structure 

We propose  a  structure  for  mental-level  models  (Sections  2  and  3)  that  is motivated 
by  work  in  decision  theory  [41]  and  previous  work  on  knowledge  ascription  [20,54] 
in  which  the  agent  is  described  as  a  qualitative  decision  maker.  This  model  contains 

220 

R.1. Brajkaan, M.  Tennenholtz/Arf#icial  intelligence  94  (1997)  2I7-268 

three  key  components: 
essential  components 
of  the  world, 

beliefs,  preferences, 

and  a  decision  criterion.  We  see  these  as  the 

of  any  mental-level 

structure,  accounting 

for  the  agent’s  perception 

its  goals,  and  its  method  of  choosing  actions  under  uncertainty. 

to  be  plausible.  For  example, 

The  beliefs  of  the  agent  determine  which  among 

states  of  the  world  it 
the  possible  worlds  of  interest  may  be  rainy 
tell 
the  agent  has  two  possible 
taking  or  leaving  an  umbrella,  whose  outcomes  are  described  by  the  following 

considers 
and  ~~~-~~~~y, and  the  agent  believes 
us  how  much 
actions, 
table: 

to  be  plausible.  The  agent’s  preferences 

it  likes  each  outcome.  For  example, 

the  possible 

suppose 

rainy 

rainy 

not-rainy 

take  umbrella 
leave  umbrella 

dry,  heavy 
wet,  light 

dry,  heavy,  look  stupid 
dry,  light 

The  agent’s  preferences 

tell  us  how  much 

these  values,  where 

it  values  each  of  these  outcomes.  We 
indicate  better 

larger  numbers 

will  use  real  numbers 
outcomes: 

to  describe 

take  umbrella 
leave  umbrella 

5 
-4 

-1 
10 

its  decision  criterion 

The  agent  chooses 

its  action  by  applying 

actions  at  the  plausible  worlds.  A  simple  example  of  a  decision  criterion 

different 
maximin,  in  which  the  action  with  the  best  worst-case  outcome 
if  the  agent  believes  both  worlds 
action  take  umbrella,  since  its  worst  case  outcome 
case  outcome  of  Zeuve  umbrella  ( -4).  However, 
be  plausible, 
is  now  much  better  than  that  of  take  umbrella  (-  1) . 

to  be  plausible 

to  the  outcome  of  the 
is 
is  chosen.  For  example, 
and  uses  muximin,  it  will  choose 
the 
is  -  1,  which  is  better  than  the  worst 
if  the  agent  believes  only  ~~t-r~~~y to 
( 10) 

it  will  choose  leave umbrella,  whose  (plausible)  worst  case  outcome 

evolves 

Our  description 

(Section  2);  then,  we  extend 

in  two  stages:  First,  we  model  simple  agents 

that  take  one- 
that  can 
shot  actions 
f Section  3).  In 
actions  and  make  observations 
take  a  number  of  consecutive 
the  dynamic  case,  we  must  also  stipulate  a  relationship  between  an  agent’s  mental  state 
at  different 

these  ideas  to  cover  dynamic  agents 

concerned  with  belief  change. 

times;  we  are  especially 

in  between 

In  Section  2.3,  we  show  how  a  model  for  an  agent  can  be  constructed  using  the  main 
in  a  context 
in  the  behavior 
observable.  The  main  tool  used  here  is  the  agency 

and  this  whole  context  should  be  grounded 

semantic  observation 
of  other  mental  attitudes, 
of  the  agent  which  is  (in  principle) 
~y~ut~e~i~, the  hypo~esis 

of  this  paper:  mental  attitudes 

their  meaning 

receive 

that 

( 1)  the  agent  can  be  described  via  beliefs,  preferences, 
in  the  mental-level  model  outlined  above  and 

and  a  decision  criterion, 

as 

R.I.  Brafman, M.  Tennenholtz/Arti$cial  Intelligence 94 (1997) 217-268 

221 

(2) 

state 

its  ascribed  mental 
manner. 
this  hypothesis,  we  can  view  the  problem  of  ascribing  a  mental  state  to  the  agent 
satisfaction  problem.  The  agent’s  ascribed  mental  state  must  be  such  that 

to  its  observed  behavior 

the  specified 

is  related 

in 

Under 
as  a  constraint 

( 1)  it  would  have  generated 
(2) 

it  is  consistent  with  the  background  knowledge. 

the  observed  behavior,  and 

the  above  example,  and  suppose  we  have  as  background 

consider 

For  instance, 
edge  the  agent’s  preferences 
is  muximin.  If  we  observe 
it  believes  not-rainy,  for  if  it  had  other  beliefs, 
We  put  special 

(as  specified 

the  agent  go  out  without  an  umbrella,  we  must  conclude 

knowl- 
in  the  table)  and  its  decision  criterion,  which 
that 
it  would  have  taken  a  different  action. 

focus  on  this  class  of  belief  ascription  problems. 

Once  a  model  of  an  agent  has  been  constructed, 

its  future 
In  Section  4,  we  pause  to  provide  some  insight  on  how  the  above  ideas  can  be 
the  behavior  of  the  modeled  agents  based  on  past  behavior.  We  exploit 
together 
the  modeled  agent’s  mental  state  and  its  next 

to  construct  mental-level  models  of  agents  based  on  their  behavior 

it  can  be  used  to  predict 

relationship 

between 

behavior. 
used  to  predict 
our  ability 
with  the  stipulated 
action. 

Model  existence  and  model  choice 

In  Section  5,  we  examine  model 

selection 

and  suggest  a  number  of  criteria 

for 

lead 

that  make 

fewer  assumptions 
among  models.  One  criterion  prefers  models 
choosing 
about  the  agent’s  beliefs,  while  the  other  criterion  prefers  models 
that  (in  some  precise 
sense)  have  a  better  explanatory  power.  Then,  we  provide  conditions  under  which  these 
criteria 
in  Section  6,  we 
for  whom  a  mental-level  model  can  be 
characterize 
two  rationality  postulates. 
constructed.  These  are  agents  whose  behavior  satisfies 
to  the  study  of  mental  states  in  AI.  First, 
approach.  Hence, 
this 
issues  it  involves.  Second, 

to  a  unique  model  choice  when  a  model  exists.  Next, 
a  class  of  “goal-seeking” 

Our  work  makes  a  number  of  contributions 
this  is  the  first  attempt 

we  believe 
paper  enhances  our  understanding 
we  make  a  number  of  semantic  contributions 

in  addressing 

these  issues. 

of  this  area  and  the  main 

this  modeling 

to  formalize 

agents 

( 1)  We  make 

the  notion  of  a  decision  criterion,  which  handles 

the  issue  of  choice 

an  explicit  part  of  the  model. 

under  uncertainty, 
(2)  We  provide  grounding 
(3)  We  are  the  first  to  emphasize 
(4)  We  suggest 

two  criteria 

for  mental-level  models  and  mental  attitudes. 

and  treat  the  question  of  model  existence. 
for  model  choice  and  give  conditions  under  which 

they 

lead  to  unique  model  choice. 

As  can  be  seen,  our  focus  in  this  paper  is  on  semantic 
and  processes  we  discuss  should  serve  to  deepen  our  understanding 
are  not  meant 
we  hope  that  the  insight  gained  can  form  the  basis  for  appropriate  algorithms 
structures.  3 

structures, 
of  these  issues.  They 
form,  although 
and  data 

to  be  practical  or  implemented 

issues.  The  concepts, 

in  their  extensive 

directly 

3 As  an  analogy,  naive  use  of  models  of  first-order 

logic  for  the  purpose  of  logical  deduction 

good  idea.  Instead,  one  would  either  use  a  theorem  prover  (which  manipulates 
model  checking  method  employing 

an  efficient  encoding  of  models. 

is  not  a  very 
symbols,  not  models)  or  some 

222 

R.I.  Brafman,  hf.  Tennenholtz/Artifkial  intelligence  94  (1997)  217-268 

Our  work  builds  on  much  previous  work;  In  Section  7  we  discuss 

its  relationship 

with  work  in  the  areas  of  economics 
relationship 
of  implementation 

issues  and  future  work  in  Section  9. 

of  this  work  with  existing  work  in  AI.  We  conclude  with  a  short  discussion 

and  game  theory,  and  in  Section  8  we  analyze 

the 

2,  Static  mental-level  models 

techniques 
(dynamic 

This  section  provides  a  formal  account  of  a proposed  structure  for  mental-level  models 
and  their  ascription  process.  Various  parts  of  the  model  we  propose  should  be  familiar 
theory,  and  in  particular,  qualitative 
to  readers  acquainted  with  decision 
theory,  game 
decision  making 
that  take  only 
in  Section  3).  These  agents  enter  one 
one-shot  actions 
set  of  states,  perform  an  action  and  restart.  We  start  in  Section  2.1  with  a 
of  a  possible 
the  central  concepts  used  in  our  model.  The  model  itself, 
motivating 
in  terms  of  its  beliefs,  preferences, 
in  which 
state  is  then  related 
and  a  decision 
and  grounded 
the  problem  of 
ascribing 

is  described 
in  Section  2.2.  This  mental 
in  Section  2.3  which  examines 

introducing 
the  mental  state  of  the  agent 

a  mental  state  to  an  agent,  and,  in  particular,  ascribing  beliefs. 

is  described 
criterion, 
in  the  agent’s  behavior 

(see,  e.g., 
agents  are  discussed 

[ 411).  We  examine 

static  agents 

example 

2. I.  Motivating 

example 

In  order  to  help  the  reader  relate  to  the  more  formal 
that  in~oduces 

start  with  an  example 

the  central  concepts  encounters 

later. 

treatment 

that  follows,  we  shall 

the  problem  of  designing 

Example  1.  Consider 
task  is  made  even  more  complex 
driving 
predict 
to  choose  actions 
problem 

that  promote 

to  illustrate 

requires 
their  future  behavior.  These  predictions, 

to  reason  about 

the  driver 

an  automated  driver.  This  complex 
if  our  driver  is  to  exercise  defensive  driving.  Defensive 
the  behavior  of  other  drivers 

to 
in  order 
its  safety.  4  We  shall  use  a  very  simple  version  of  this 

in  turn,  are  used  by  the  driver 

in  order 

some  of  our  ideas. 

The  modeled 

agent,  A,  is  another  vehicle  observed  by  our  agent  approaching 

an 
intersection  with  no  stop  sign.  Our  agent  is  approaching 
as  well,  and  it 
has  the  right  of  way.  It  must  decide  whether  or  not  it  should  stop,  and  in  order  to  do  so, 
it  will  try  to  determine  whether  or  not  agent  A  will  stop.  Its  first  step  is  to  construct  a 
model  of  agent  A’s  state.  It  constructs 
about  agent 
A’s  behavior 
about  drivers.  Our  agent 
models  agent  A  as  a  qualitative 
components 

this  model  by  using 
information 

together  with  general  background 

the  model  whose  fund~e~tal 

decision  maker  using 

this  inters~tion 

its  information 

are  shown 

in  Fig.  1. 

4 This  scenario  is  motivated  by  work  on  fighter pilot  simulators  which  are  used  in  air-combat  training.  Such 
simulators  must  perform  similar  reasoning  in the  context  of even  more  complex  group  activity. The  ability  to 
reason  about  the  mental  state  of  opponents,  i.e.,  their  beliefs,  goals,  and  intentions,  is  important  for  building 
good  simulators.  Groups  in  Australia  (AAII)  and California  (ISI)  have incorporated  such  technology  in their 
commercial  systems  to  a limited  extent  [ 611. 

R.I.  Brafmn,  M.  Tennenholtz/Art$cial 

Intelligence  94  (1997)  217-268 

223 

Model  Frame 

Applying  maximin 

Preferences  Assigned 

Filtered  using  Beliefs 

Fig.  1. Structure  of  mental-level  models. 

sl-atmther 
s2-anmother 
s3-no 
s4-no 
The  set  of  possible  actions 
slow  in  Fig.  1) . Each  action 
possible  worlds.  For  instance, 
Fig.  1  (top-left 

First, 
possible 
consider 

states  as  possible 

the  modeler 
supplies  a  context,  or  model  frame,  by  choosing  a  relevant  set  of 
states  of  the  world  and  a  set  of  actions  and  their  outcomes.  Our  agent  shall 
for  agent  A: 
the  following 
car  is  crossing  and  it  has  right  of  way; 
car  is  crossing  and  A  has  right  of  way; 
other  car  is  crossing  and  A  has  right  of  way; 
other  car  is  crossing  and  A  does  not  have  right  of  way. 

is:  {stop,  continue,  slow-down} 

(abbreviated 
leads  to  a  particular  outcome  when  executed 

as  stop,  go, 
in  one  of  the 
is  executed  at  state  sl,  a  collision  will  follow. 

if  continue 

table)  contains  a  more  complete  description  of  these  outcomes. 

collision 

the  desirability 

of  each  outcome 

is  a  pretty  bad  outcome; 

Next,  our  agent  must  assess 

of  view.  Naturally, 
is  also  not  a  good  option, 
specify  p:references  numerically, 
are  assigned 
able  to  re.asonably  assess  agent  A’s  value  function.  Fig.  1  (bottom-left 
one  reasonable 
Presumably, 

from  agent  A’s  point 
is  no  car 
to 
such  that  lower  values 
In  our  context,  we  would  expect  our  agent  to  be 
table)  provides 

choice  for  this  function. 
agent  A  has  some 

the  actual  state  of  the  world.  Let 

by  means  of  a  value  function, 

to  less  desirable  outcomes. 

It  is  often  convenient 

though  much  better 

stopping  when 

than  collision. 

information 

about 

there 

for  the  moment 

us  suppose 
When  agent  A  decides  which  action 
on  each  of  the  plausible  worlds-we 

that  its  information 

indicates 

to  take,  it  will  consider 
call  this  the  plausible  outcome  of  the  action-and 

that  sl  and  s2  are  plausible. 
the  effects  of  each  action 

224 

R.I. Brafinan, M. Tennenholiz/Art~ci~l  intelligence 94 (1997) 217-268 

it  will  compare 
preferred  plausible  outcome. 

these  effects 

in  order 

to  choose 

the  action 

that  gives  rise  to  the  most 

is  not  obvious.  This  is  where  agent  A’s  decision  criterion  comes 

the  bottom-right 

Examining 
is  the  case,  while  slow 

sl 
them 
criterion 
of  a  number  of  plausible 
is  cautious 
and  employs 
that  agent  A  will  stop. 
predict 

table 

is  better  when  s2  is  the  case;  hence, 

in  Fig.  1,  we  see  that  action  stop  is  better  when 
the  choice  between 
in.  A  decision 
in  the  context 
that  agent  A 
it  will 

the  values  of  alternative  actions 
that  our  agent  assumes 

In  that  case, 

earlier. 

states.  Let  us  suppose 
the  muximin  criterion  mentioned 

is  simply  a  method  for  comp~ing 

Unfortunately, 

our  agent  does  not  have  access 

to  the  internal 

to  ascribe  beliefs 

that  the  ascribed  model  must  satisfy 

it  cannot  observe  A’s  “beliefs”.  Therefore, 
it  must  be  able 
info~a~on 
criteria 
beliefs  of  agent  A  (in 
predict 
instance 
stipulated 
observations. 

of  a  constraint 
relationship 

the  actions 

satisfaction 

between 

about  drivers.  We  refer  to  this  as  the  ~e~ie~ff~cripti~~ 

is  descriptive  accuracy.  Hence, 

fact,  the  whole  ascribed  model)  must  be  such 
that  were  actually  observed.  Thus,  belief  ascription 

problem,  where 

the  constraints 

the  various  components 

state  of  agent  A,  so 
the  ascription  process, 
about  A  and  its  general 
problem.  The  main 
the  ascribed 
that  it  would 
is  a  special 
are  given  by  the 
of  the  model  and  our  agent’s 

in  order  to  complete 
its  information 

to  A  using 

In  our  particular 

scenario,  our  agent  observes 

that  agent  A  continues  driving  without 
slowing  down.  The  sets  of  plausible  worlds  that  agent  A  could  have  that  would  lead  it  to 
take  this  action  are:  {s3), 
that  our  agent  does  not  have  a  unique 
for  agent  A,  i.e.,  there  are  a  number  of  models  of  agent  A  that  are 
belief  ascription 
consistent  with  our  agent’s  information. 
In  Section  5  we  shall  consider  various  domain- 
independent 
that  can  be  used  to  choose  among  a  set  of  models  consistent  with 
our  information. 

(~41,  ($3,  ~4).  Notice 

heuristics 

the  int~rs~tion 

We  have  just  seen  an  example  of  belief  ascription.  Our  agent  has  concluded 
there  is  another  vehicle  approaching 

set  of  plausible  worlds  cannot  contain  states  $1 or  ~2. This  conclusion 

A  does  not  “believe” 
A”s  (ascribed) 
if  we  wish  to  improve  agent  A’s  design,  as  it  points  out  a  flaw  in  its  reasoning, 
useful 
sensing,  or  decision  making  abilities.  Otherwise,  a  model  of  an  agent’s  beliefs  may  not 
in  this  section.  However, 
be  very  useful 
it  will 
later,  where  our  agent  could  use  it  to 
become  useful 
that  A’S  beliefs 
predict  agent  A’s  future  behavior. 
persist  until  new  contradictory 
that  agent  A 
will  continue  at  its  current  speed,  ConsequentIy~  our  agent  must  stop  in  order  to  prevent 
an  accident. 

in  the  one-shot  static  model  we  describe 
in  the  dynamic 

that  agent 
since  agent 
is 

setting  discussed 
Intuitively, 

arrives,  our  agent  can  predict 

to  the  assumption 

information 

subject 

2.2.  Model  structure 

In  this  section,  we  describe  a  formal  model  of  static  agents.  At  this  stage,  we  shall  not 
concern  ourselves  with  how  one  constructs  a  model  for  a  particular  agent  but  rather  with 
should  be  used  to  describe  mental  states.  Many 
the  question  of  what  type  of  structure 
of  the  de~nitions 
to  readers  acqu~nted 
with  formal  work  on  mental  states  and  the  fund~e~tals 

used  here  are  standard  and  should  be  familiar 

of  decision 

theory. 

R.I.  Brafian,  M.  TennenhoWArtificial 

Intelligence  94  (1997)  217-268 

225 

[26]  and  of  Rosenschein 

We  start  with  a  low-level  description  of  an  agent,  motivated  by  the  work  of  Halpern 
is 
them  with  a  simplified  version  of 
a  thermostat,  which  we 

and  Moses 
defined.  To  clarify  our  definitions,  we  will  accompany 
McCarthy’s 
normally  dlo not  view  as  having  beliefs,  stresses  our  view  of  mental  states  as  modeling 
abstractions. 

[43].  The  choice  of  modeling 

the  mental-level  model 

[54],  on  top  of  which 

thermostat 

example 

In  [43],  McCarthy 

Example  2. 
devices.  Our  goal  is  to  formalize  his  informal  discussion  of  thermostats.  We  assume 
that  controls 
we  have  a  thermostat 
radiator.  The 
turn-on 
radiator.  It  chooses 
be  above  or  below  a  certain 

shows  how  we  often  ascribe  mental  states  to  simple 
that 
that  room’s 
to  this 
it  senses  the  temperature  of  the  room  to 

or  shut-ofS  the  hot  water  supply 

its  action  based  on  whether 

the  flow  of  hot  water  into 

threshold  value. 

in  a  room 

can  either 

thermostat 

states. 

Intuitively, 

set  of  possible 

the  environment 

states,  a  set  of  possible 

functions  within  an  environment, 

is  described  as  a  state  machine,  having  a  set  of  possible 
actions,  and  a  program,  which  we  call  its  protocol. 

Describing  agents.  An  agent 
(local) 
The  agent 
corresponding 
external 
system, 
of  generality,  we  will  assume 
the  effects  of  the  agent’s  actions  are  a  (deterministic) 
environment’s 
agent  and  the  environment 
transitions 
local  state  and  an  environment’s 
of  an  agent’s 
case  that  not  all  combinations 
possible,  and  those  global  states  that  are  possible  are  called  possible  worlds. 

also  modeled  as  a  state  machine  with  a 
all  things 
to  the  agent,  including  possibly  other  agents.  We  refer  to  the  state  of  the  whole 
as  a  global  state.  Without 
loss 
i.e.,  that  of  both  the  agent  and  the  environment 
does  not  perform  actions,  and  that 
the 
function  of  its  state  and 
state.  5  These  effects  are  described  by  the  transition  function.  Thus, 
the 
can  be  viewed  as  a  state  machine  with  two  components,  with 
It  may  be  the 
state  are 

at  each  state  corresponding 

to  the  agent’s  possible 

that  the  environment 

describes 

actions. 

Definition  3.  An  agent 

Set  Of 

lOCal 

StUteS, 

the  environment’s 
The  set  of  possible  worlds 
the  transition 
action 

to  a  new  global  state. 

Lid 

iS  its  Set  Of  aCtiOnS, 

is  a  three-tuple  A = (LA, AA,  PA),  where  LA  is  the  agent’s 
LE  iS 
states.  A  global  state  is  a  pair  (,!A, I&)  E  LA  x  LE. 
is  a  subset  S  of  the  set  of  global  states  LA  x  LE.  Finally, 
function,  T  :  (LA  x  LE )  x  Ad  +  (LA  x  LE )  maps  a  global  state  and  an 

set  of  possible 

and  PA 

iS  its  protocol. 

--+  Ad 

:  LA 

In  the  sequel  we  often  consider  partial  protocols 
to  each  state.  Partial  protocols  allow  us  to  represent 
behavior  has  been  observed  only 

in  a  subset  of  its  possible  states. 

in  which  an  action 
information 

is  not  assigned 
about  an  agent  whose 

Example  Z! (continued). 
to  the  case  when 

For  our  thermostat,  LA  =  { -,  +}.  The  symbol 

the  thermostat 

indicates 

a  temperature 

that  is  less  than 

-  corresponds 
the  desired 

5 A  framework 
can  be  mapped 
theory. 
in  game 

in  which  the  environment  does  act  and  in  which  the  outcomes  of  actions  are  non-deterministic 
into  our  framework  using  richer  state  descriptions 

and  larger  sets  of  states,  a  common  practice 

226 

RI.  bract, 

M.  T~~nen~~~k/Art~~iai 

Intelligence  94  (1997)  217-268 

temperatnre, 
room 
the  desired 
room 
The  environment’s 
relation  between 
the  possibility 
LA  x  LE.  We  chose  the  following 

of  measurement 

to  a  t~rn~rat~re 

greater  or  equal 

to 

and  the  symbol  +  corresponds 
temperature.  The 

thermostat’s 

shut-o#]. 
states,  LE,  are  (cold,ok,  hot}.  We  do  not  assume  any  necessary 
into  account 
is  exactly 

the  set  of  possible  worlds 

actions,  AA,  are  (turn-on, 

and  the  environment, 

error.  Therefore, 

the  states  of  the  thermostat 

taking 

transition 

function: 

1  (-,cold) 

(-f-,cold) 

(-,ok) 

(+,ok) 

(-,hot) 

(+,hot) 

turn-on 
shut-off 

(--,okl 
(-,culd) 

(+,ok) 
f+,coEd) 

(-,hot) 
(-to@ 

(+,hot) 
(+,ok) 

(-,faot) 
(-,ok) 

(+,hot) 
(+,ok) 

In  this  example, 

the  effects  of  an  action  on  the  environment 

state  of  the  thermostat. 
allows  us  to  make  certain 
influences  on  the  room’s 
actions.  Second,  we  ignore 
inconsequential-we 

In  addition, 

simplifications. 

do  not  depend  on  the 
the  fact  that  we  use  a  static,  “one-shot”  model 
First,  we  do  not  explicitly  model  external 
from  the  thermostat’s 

temperature  other  than  those  stemming 

the  effect  of  the  thermostat’s  actions  on  its  state  since 

it  is 

adopt  the  convention 

that  this  state  does  not  change. 

Finally, 

the  thermostat’s  protocol 

is  the  following: 

Given 

the  set  S  of  possible  worlds,  we  can  associate  with  each  local  state  1 of  the 
agent  a  subset  of  8,  PW( E) , consisting  of  all  worlds  in  which  the  local  state  of  the  agent 
is  1. 

Definition  4.  The  agent’s  set  of  worlds  possible  at  1, PW(  I>, is  defined  as  {w  E  S  ( 
the  agent’s 

local  state  in  w  is  I}. 

Thus,  PW(l)  are the  worlds  consistent  with  the  agent’s  state  of  information  when  its 
to 
laaows 
in  all  the  worlds 

local  state  is  a’. Halpern 
ascribe  knowledge 
some  fact  sp at  a  world  w  if  its  local  state  I  in  w  is  such  that  p  holds 
in  ~W(~). 

to  an  agent  at  a  world  w.  Roughly, 

they  say  that  the  agent 

[54]  use  this  definition 

[ 261  and  Rosenschein 

and  Moses 

Example  2  (continued).  While 
the  room 
knows  nothing  about 
from 
the  fact 
possible  world.  Intuitively,  we  are  allowing 
by  the  thermostat. 

the  thermostat,  by  definition,  knows 
temperature.  Formally, 

this  lack  of  knowledge 

that  we  made  all  elements  of  La  x  LE  possible, 

e.g., 

for  the  possibility 

of  a  measurement 

its  local  state,  it 
follows 
is  a 
error 

( -,  hot) 

Example  1  (continued).  Let  us  try  to  relate 
problem 
quite  complex.  Yet,  our  agent  cared  about  modeling  A  in  its  current  circumstances 

faced  by  our  driving  agent.  Agent  A,  which  our  agent 

to  the  modeling 
tried  to  model,  may  be 
only 

these  first  definitions 

R.I.  Brafkm,  M.  Tennenholtz/Art$cial  Intelligence 94 (1997) 217-268 

221 

(i.e.,  as  it 
current 
down}), 
are  Sl,sZ,SsrS& 
transition 

is  approaching 

local  state  (call 

the  intersection). 

Consequently, 
it  I),  A’s  current  possible  actions 

our  agent  cared  about  A’s 
slow- 
(i.e.,{srop, 

continue, 

and  A’s  current  action 

(i.e.,  confinue).  The  possible  states  of  the  environment 

and  so  thepossibleglobal 
implicitly 

is  described 

states  are  (I,s1),(1,~2),(1,~3),(l,~4). 

The 

in  Fig.  1. 6 

function 

If  truth  assignments 
if  a  world  S’  is  defined 
and  S’ are  identical,  we  obtain 

(for  some  given  language) 

are  attached 

to  be  accessible 

from  s  whenever 

to  each  world  in  S,  and 
local  states  in  s 

the  agent’s 

the  familiar  S5  Kripke  structure. 

is  the  set  of  worlds 

in  the  eyes  of  the  agent, 

to  each  local  state  1 a  nonempty 

(or  PW( 2) )  defines  what  is  theoretically 

Belief.  While  knowledge 
fines  what, 
consideration.  We  describe 
assigns 
modeled  as  a  function  of  the  agent’s 
description  of  the  agent  at  a  point 
role  beliefs  play  is  to  divide 
ble,  and  thus,  are  worthy  of  consideration, 
ignored. 

that  should  be  taken 
the  agent’s  beliefs  using  a  belief  assignment,  a  function 

possible,  belief  de- 
into 
that 
subset  of  the  set  of  possible  worlds.  Beliefs  are 
this  state  provides  a  complete 
its  beliefs.  The 
the  worlds  possible  at  a  local  state  to  those  that  are  plausi- 
and  can  be 
and  those  that  are  not  plausible, 

in  time,  so  it  should  also  determine 

local  state  because 

Definition  5.  A  belief  assignment  is  a  function,  B  :  LA  +  2’  \  8,  such  that  for  all 
1 E  Ld  we  have  that  B(l)  C_ PW(l).  We  refer  to  B(I)  as  the  worlds  plausible  at  1. 

Example  2  (continued).  One  possible  belief  assignment,  which  would  probably  make 
the  thermostat’s  designer  happy,  is  B ( -)  = { -,  cold}  and  B (+)  = { +, hot}.  From  now 
local  state  in  the  description  of  the  global  state  and  write, 
on  we  will  ignore 
e.g.,  B(S)  = {hot}. 

the  agent’s 

We  remark 

that  (after  adding 

interpretations 

to  each  world) 

this  approach  yields  a 

KD45  belief  operator  and  a  relationship  between  knowledge  and  belief  that  was  proposed 
by  Kraus  and  Lehmann 

in  [ 311. 

really  make  sense  as  part  of  a  fuller  description 

Preferences.  Beliefs 
mental  state,  which  has  additional 
order  over  possible  worlds,  which  may  be  viewed  as  an  embodiment 
desires.  There  are  various  assumptions 
agent’s  preferences. 
pre-order  on  the  set  of  possible  worlds  S.  However, 
richer  alg,ebraic  structure,  e.g.,  one  in  which  addition 
of  indifference). 
preferences. 

of  the  agent’s 
aspects.  One  of  these  aspects  is  the  agent’s  preference 
of  the  agent’s 
the  structure  of  the 
that  they  define  a  total 
in  some  cases,  we  may  need  a 
is  defined 

In  what  follows  we  will  use  value functions  to  represent 

(e.g.,  for  the  principle 
the  agent’s 

In  most  of  this  section,  we  will  only  assume 

that  can  be  made  about 

6 To precisely  conform  with  the  definitions,  we  would  have  had  to  include  the  set  of  outcomes  in  the  set  of 

possible  states  and  to  define  the  effects  of  actions  on  these  outcomes. 

228 

R.1. Brajinan,  M.  TennenholtdArtificial  Intelligence  94  (1997)  217-268 

Definition  6.  A  value  function 

is  a  function  u  : S  +  I& 

This  numeric 
this  representation, 
Value  functions 

representation 

is  most  convenient 

for  representing 

preferences.  Under 

the  state  st  is  at  least  as  preferred  as  state  s2  iflu 
are  usually  associated  with  the  work  of  von  Neumann 

2  u(  ~2). 
and  Morgen- 

functions 

stern  on  utility 
simple  pre-order  over  the  set  of  states,  and  we  do  not  need  to  incorporate 
additional 

[ 621.  However, 

their  utility 

functions 

express  more 

than  a 
all  of  their 

Because 

is  concerned  with  simple  agents 

that  take  one-shot  actions  and 
restart,  we  can  view  values  as  a  function  of  state.  In  Section  3,  we  will  need  to  look 
at  more  complex  value  functions 
than 
single  states. 

that  take  into  account  sequences  of  states  rather 

assumptions. 
this  section 

the  environment’s 

The  goal  of  our  thermostat 

is  for  the  room  temperature 

to  be 
state  is  ok  over  any  global 
state  is  either  cold  or  hot,  and  is  indifferent  between 
the  environment’s 

Example  2  (continued). 
ok.  Thus,  it  prefers  any  global  state  in  which  the  environment’s 
state  in  which 
cold  and  hot.  In  addition, 
state  is  identical,  e.g.,  ( -,  ok)  and  (+,  ok).  This  preference  order  can  be  represented  by 
state  (i.e., 
a  value  function  which  assigns  0  to  global  states  in  which  the  environment’s 
the  room  temperature) 
the 
environment’s 

is  hot  or  cold,  and  which  assigns  1  to  those  states  in  which 

it  is  indifferent 

state  is  ok. 

in  which 

between 

states 

(Remember 

in  the  actual  world.  However, 

the  result  of  following 
that  actions  have  deterministic 

Plausible  outcomes.  When  the  exact  state  of  the  world  is  known, 
some  protocol,  P,  is  also  precisely  known. 
effects).  Therefore,  we  can  evaluate  a  protocol  by  looking  at  the  value  of  the  state  it 
is  that  the 
the  more  common 
would  generate 
the  state  of  the  world  and  considers  a  number  of  states  to  be 
agent 
its  view  of  how  desirable  a  protocol  P  is  in  a  local 
plausible.  Then,  we  can  represent 
state  1 by  a  vector  whose  elements 
states  P  generates, 
i.e.,  the  worlds  generated  by  using  P  at  B(1).  We  refer  to  this  tuple  as  the  plausible 
outcome  of  P. 

are  the  values  of  the  plausible 

is  uncertain 

situation 

about 

Example  2  (continued). 
the  thermostat’s  possible  actions  at  each  of  the  environment’s 
stands  for  either 

The  following 

-  or  +): 

table  gives  the  value  of  the  outcome  of  each  of 
-k 

states  (where 

possible 

turn-on 
shut-off 

1 
0 

0 
1 

0 
1 

If  the  thermostat 

“knew” 

the  precise  state  of  the  world, 

choosing 
cold,  turn-on  would 
must  compare  vectors  of  plausible  outcomes 

an  action  based  on  the  value  of  its  outcome.  For  example, 
there  is  uncertainty, 

if  the  state 
the  thermostat 
instead  of  single  outcomes.  For  example, 

lead  to  the  best  outcome.  When 

it  would  have  no  trouble 
is 

R.I. Brafman. M. TennenhoWArrijicial Intelligence 94 (1997) 217-268 

229 

if  B(Z) 
plausible  outcome  of  the  action  shut-ofS  is  (0,l) 

=  {cold,  ok}, 

the  plausible  outcome  of  the  action 

. 

turn-on 

is  (l,O), 

and 

the 

Definition  7.  Given  a  transition 
fixed  enumeration 
is  a  tuple  whose  kth  element 
at  the  kth  state  of  B(1). 

function  T,  a  belief  assignment 

B,  and  an  arbitrary, 

of  the  elements  of  B(Z), 

the  plausible  outcome  of  a  protocol  P  in  1 

is  the  value  of  the  state  generated  by  applying  P  starting 

Note  that  because  we  are  considering 
outcomes  of  actions 

only  static  agents,  we  could  have  spoken  about 
instead  of  plausible  outcomes  of  protocols.  For  static 

plausible 
agents  both  are  identical. 

Decision  criteria.  While  values  are  easily  compared, 
compare  plausible  outcomes, 
choice  und’er  uncertainty 
risk.  This 
plausible  outcomes, 

it  is  not  a  priori  clear  how  to 
and  thus,  how  to  choose  among  protocols.  A  strategy 
for 
is  required,  which  depends  on,  e.g.,  the  agent’s  attitude  towards 
taking  a  set  of 

by  the  decision  criterion,  a  function 

the  set  of  most  preferred  among 

.strategy  is  represented 

returning 

them. 

encountered 

We  have  previously 
whose  wor:st  case  outcome 
which  selects  those  tuples  whose  average  outcome 
of  decision  criteria  consult 

[ 411.) 

is  maximal.  ’  Another  example 

the  maximin  criterion,  which  selects 

tuples 
is  the  principle  of  indifference 
is  maximal.  ’  (For  a  fuller  discussion 

those 

Example  8.  Consider 
matrix  was  used: 

the  example  given 

in  the  introduction 

in  which 

the  following 

We  have  seen 

that  when  both  worlds  are  plausible 

(5,  -  1)  and  ( -4,10).  When 
to  take  umbrella 
used,  the  plausible  outcome  of  leave  umbrella 

the  muximin  criterion 

is  preferred. 

the  two  plausible  outcomes 
is  used,  the  first  one,  corresponding 

are 

is  the  most  preferred.  However,  when  the  principle  of  indifference 

is 

Definition  9.  A  decision 

criterion 

is  a  function  p  :  UnEW 2””  +  UnEN 2R’  \  0  (i.e. 

from/to 
that  p(U)  2  U  (i.e.,  it  returns  a  non-empty 

sets  of  equal 

tuples  of  reals), 

length 

such  that  for  all  IA E  UnEW 2””  we  have 

subset  of  the  argument 

set). 

Notice 

p({w,u}) 

that  we  can  use  a  decision 
=  {w}  th en  we  can  say  that  w  is  more  preferred 

criterion 

to  compare 

than  u. 

tuples.  For 

instance, 

if 

7 We  apply 

this  criterion 

recursively, 

i.e.,  when 

two  tuples  have  the  same  worst  case  outcome,  we  compare 

the  next  to  worst,  and  so  on. 

8 With  an  infinite  set  of  tuples,  maximin and  the  principle  of  indifirence  may  not  have  a  set  of  most  preferred 

tuples  and  appropriate 

variants  of  these  criteria  must  be  defined. 

230 

R.I.  Brafkan,  M.  TennenholtdArtificial  Intelligence  94  (1997)  217-268 

Fig. 2.  The agency  ~yp~~~~~s. 

The  agency  hypothesis.  We  can  capture 
of  our  mental-level  model  using  the  following: 

the  relationship 

among  the  various  components 

Definition  10.  The  agency  ~y~~~~e~~~: The  agent’s  actual  protocol  has  a  plausible 
outcome 
set  of  plausible  outcomes  of  all  possible  protocols.  9 

to  the  agent’s  decision  criterion) 

that  is  most  preferred 

(according 

among 

the 

criterion 

The  agency  hypoth~is 

(see  Fig.  2).  It  states 

takes  the  view  of  a  rational  balance  among 
and  behavior 

the  agent’s  beliefs, 
that  the  agent  chooses 
to  its  decision  criterion.  Thus,  the 
is  dependent  upon  L1 (I)  and  u,  which  define  the  plausible  outcome 
these  different  plausible  outcomes.  By 
the  agent  as  a  qualitative  decision  maker,  the  agency  hypothesis  attributes  some 
that  it  would  take  an  action  whose  plausible 
rationality 
is  most  preferred  according 

values,  decision 
actions  whose  plausible  outcome 
choice  of  the  protocol 
of  each  protocol, 
viewing 
minimal 
outcome 

and  p,  which  chooses  among 

to  some  decision  criterion. 

to  the  agent,  assuming 

is  maximal  according 

We  can  now  formally  define  a  notion  of  a  mental-level  model. 

Definition  11.  A  mental-level  model 
(GA,AA,  B,  u,p),  where  B  is  a  belief  assignment, 
decision  criterion. 

for  an  agent  A  = {CA,AA,PA) 
u  is  a  value  function, 

is  a  tuple 
and  p  is  a 

Thus,  a  mental-level  model  provides  an  abstract 

tion  of  the  agent.  Instead  of  describing 
registers,  or  the  values  of  state  variables,  we  use  implementation-independent 
beliefs,  preferences, 

and  a  decision  criterion. 

implementa~on-inde~ndent 

descrip- 
its  local  state  in  terms  of  the  values  of  various 
notions: 

2.3.  Model  ascription 

We  have  taken 

the  first  step  towards 

a  general 
the  process  by  which  one  can  model  a  ~a~ic~lar  agent, 

structure.  Now,  we  come 

to  the  construction 

formalizing  mental-level  models  by  proposing 
task,  where  we  must  explain 
i.e.,  the  process  by  which 

g The  agent’s  possible  protocols,  are  implicitly  defined  by  the  set  of  actions  Ad  (cf.  Definition  3). 

R.I.  Braftnun,  M.  TennenholtdArtijcial 

Intelligence  94  (1997)  217-268 

231 

IOBSERVED  BEHAVIOR  1 

1 PREFERENCES  1 

[DERISION  CRITERION  1 

@EiiR-j 

Fig. 3. Model construction. 

that  we  can  realistically 

we  actually  ascribe  a  mental  state  to  some  particular  agent.  This  process  should  require 
information 
expect  the  modeling  agent  to  have.  This  information 
should  primarily  consist  of  the  modeled  agent’s  behavior,  which  is  a  concrete,  observable 
aspect  of  it.  This  behavior 
is  formally  captured  by  our  notion  of  a  protocol,  or  a  partial 
protocol  when  we  only  know  how  the  agent  acts  in  certain  situations.  Thus,  the  modeled 
agent’s  ascribed  mental  state  should  be  grounded 

in  its  behavior. 

The  agency  hypothesis 

on 
the  agent’s  mental  state  given  its  behavior.  That  is,  it  makes  only  certain  mental  states 
such 
consistent  with  a  given  behavior: 
behavior. 

supplies  a  basis  for  this  process  by  providing 

that  would  have  induced 

those  mental 

constraints 

states 

Definition  12.  A  mental-level  model 
protocol  P  if  for  all  1  E  ,cd 
preferred  according 

to  B, u,  and  p. 

for  A, (CA,AA, B, u,p) 

is  cunsisterzr  with  a 
it  is  the  case  that  the  plausible  outcome  of  P  is  most 

It  is  con,sistent  with  a  partial  protocol  P’  if  it  is  consistent  with  some  completion  P 

of  P’. 

This  key  definition 

tells  us  that  a  mental-level  model 

the  model 

is  such  that  an  agent  with  this  mental 

is  consistent  with  our  ob- 
state 
two  key  ideas  about  the 

the  observed  behavior.  This  definition  embodies 

of  the  agent  when 

servations 
would  display 
semantics  of  mental  states: 

( 1)  The  agent’s  ascribed  mental  state  is  grounded 

in  its  behavior 

(formally 

captured 

by  its  protocol), 

(2)  Separate  mental  attitudes, 

themselves, 
whole  mental  state  (see  Fig.  3). 

but  rather, 

they  receive 

their  meaning 

such  as  belief  and  preference, 

are  not  interpreted  by 
in  the  context  of  the  agent’s 

In  some  applications, 

such  as  design  validation 

around”  with  the  modeled  entity  and  obtain  sufficient  observations 
of  consistent  models.  However, 
multi-agent 
of  consistent  models 
that  reduce:  this  class  of  models.  The  problem  of  ascribing 
under  assumptions  Q  and  based  on  a  (possibly  partial)  protocol  P  can  be  stated  as: 

system,  we  may  not  be  able  to  make  enough  observations 

and  program  analysis,  we  can  “play 
the  class 
to  constrain 
such  as  agent  modeling  within  a 
to  reduce  the  class 

size.  Hence,  we  will  have  to  introduce  assumptions 

a  mental  state  to  an  agent 

in  other  applications, 

to  a  manageable 

232 

R.I.  Brafman,  M.  Tennenholtz/Artificial  Intelligence  94  (1997)  217-268 

Find  a  model  of  the  agent  consistent  with  P  that  satisjies 

the  assumptions  F. 

In  this  paper,  we  are  particularly 
modeling  problems, 
one  component 
components: 
presented 
belief  ascription  problem 

the  preferences 
so  far  can  be  applied 

that  of  belief  ascription. 

of  a  mental-level  model, 

interested 

in  a  special  class  of  constrained  mental-level 
In  belief  ascription  our  task  is  to  supply 
about  the  other 
the  general  approach 
to  other  ascription  problems,  e.g.,  goal  ascription.  The 

the  beliefs,  given  assumptions 

and  the  decision  criterion.  However, 

is  formally  defined  as  follows: 

Given  an  agent  A = (CA, Ad, ?A), a  value  function  U, and  the  decision  criterion 
p,  find  a  belief  assignment  B  such  that  (cd,  Ad,  B,  u,  p)  is  consistent  with  ‘pA. 

criterion 

to  it?  We  know 

the  thermostat’s  protocol  and  goals.  We  will  assume 
that  are  not  dominated  by  any  other 

Example  2  (continued).  Given  our  knowledge  of  the  thermostat,  what  beliefs  can  we 
that  its 
ascribe 
tuple.  lo 
decision 
Given 
2 
{cold}  and  at  least  one  of  ok  or  hot  are  in  B(  +). 
If  the  thermostat’s  beliefs  violate 
the  plausible  outcome  of  the  action  prescribed  by  its  protocol  would 
these  constraints, 
be  strictly 

than  the  plausible  outcome  of  the  other  action. 

on  the  thermostat’s 

simply  prefers 

this,  we  have 

beliefs:  B(  -) 

less  preferred 

the  following 

constraints 

tuples 

is  possible 

Our  treatment 

it  is  not  likely 

in  certain  contexts 

so  far  can  be  viewed  as  assuming  knowledge  of  the  agent’s 

While  access  to  such  information 
is  done  by  the  designer), 
not  make  use  of  such  information; 
the  modeled  agent’s  action.  In  general,  knowing 
possible  worlds  differ  depending 
sensors 
that  allow 
task  of  determining 
of  the  model 

local  state. 
(e.g.,  when  the  modeling 
in  many  other  contexts.  Yet,  notice  that  we  did 
the  ascription  process  described  here  relied  only  on 
the  local  state  matters  when  the  agent’s 
the  agent  has  reliable 
it  to  rule  out  certain  states  of  the  environment 
as  impossible.  This 
the  set  of  possible  worlds  is  a  difficult  one,  and  we  view  it  as  part 

on  the  local  state,  e.g.,  when 

framing  problem. 

The 
Example  13  (A  simple  game). 
problem  based  on  a  game  that  appears 

following 
in  [ 341: 

tree  describes 

a  one-person 

decision 

Initially 

the  agent  decides  whether 

obtained,  otherwise 
or  n,  with  a  payoff  of  x  >  1.  While  game 
how  games  should  be  played  when  the  environment 

the  environment 

to  choose  Y or  N.  If  Y is  chosen,  a  payoff  of  1  is 
chooses  either  y,  with  a  payoff  of  0  to  the  agent, 
are  mostly  concerned  with 
is  another  rational  agent,  we  ask  a 

theoreticians 

lo Tuple  u  dominates  w  if  every  component  of  the  u  is  at  least  as  good  as  the  corresponding 
and  some  component  of  u  is  strictly  better  than  the  corresponding 

component  of  w. 

component  of  w 

R.I.  Brafman,  M.  Tennenholtz/Artijicial  Intelligence  94  (1997)  217-268 

233 

is  interesting 

simple  question:  what  can  we  say  if  we  observed 
question 
decision  problem 
irrational 
that  the  environment  will  play  n. 

the  agent’s  first  move  to  be  N?  This 
it  is  easy  to  construct  a  two  person  game  based  on  this 
in  which  N  is  not  a  “rational”  move.  Such  behavior,  while  perhaps 
as  rational  given  certain  beliefs,  e.g., 

in  some  sense,  can  still  be  understood 

because 

The  following 

payoff  matrix  describes 

the  agent’s  decision  problem 

(the  different 

states  of  the  world  correspond 

to  the  environment’s 

behavior 

if  N  is  played): 

It  Y 

Y 
N 

1 
0 

n 

1 

X 

Having  chosen  N,  if  the  agent’s  decision  criterion 

is  maximin 

value  of  JC, the  agent  must  believe 
plausible 
be  chosen. 

is  inconsistent  with  the  agent’s  behavior, 

since  it  would 

that  the  environment  will  play  n.  Belief 

In  the  case  of  the  principle  of  indifference,  if  x  <  2,  N  is  chosen  only 

believes  n  to  be  the  only  plausible  world.  If  x  >  2  then  a  belief 
plausible  would  also  cause  N  to  be  preferred. 

then  regardless  of  the 
that  y  is 
that  Y  should 

imply 

if  the  agent 
that  both  worlds  are 

Another  decision  criterion 
state  s  is  the  difference  between 
of  act  in  s.  This  decision 
Here  is  the  “regret”  matrix  for  our  decision  problem: 

is  minima  regret. The  regret  of  performing 

action  act  in  a 
the  best  that  can  be  done  in  state  s  and  the  actual  payoff 
is  minimal. 

criterion  prefers  actions  whose  maximal 

regret 

Y 

0 
1 

n 

x-l 
0 

Y 
N 
3 

For  an  <agent following  minimax  regret,  if  x  <  2  the  agent  must  believe  n  to  follow 

N,  otherwise 

it  may  believe  either  IZ or  {n, y}. 

The  idea  of  ascribing 

to  an  agent 

those  belief  assignments 

that  make 

leads  to  an  interesting 

semantics 

for  belief,  stemming 

the 
it  satisfy 
from  its  ground- 

(ascribed) 

plausible  worlds  consist  of  those states  that  afSect 

agency  hyjpothesis 
ing  in  actions:  The  agent’s 
its  choice  of  action. 

To  better  understand 

this  subtle  point,  consider 

the  special  case  of  an  agent  whose 
to  an  experi- 
sets  of 

its  chosen  action 

in  which  we  examine 

actions  can  be  varied  arbitrarily.  We  subject 

set  of  possible 
ment 
to  be  considered 
for  a  global  state  s  E  PW(Z) 
possible  actions.  A  sufficient  condition 
(i.e.,  s  E  B(Z)  )  is  that  a  pair  a,  a’  of  actions  exists,  such  that 
plausible  by  this  agent 
and 
for  all  s’  E  PW(  1)  \  {  } s  we  have  that  r(s’,  a)  =  T(s’,a’) 
the  agent  would  choose  a  over  a’.  That  is,  a  and  a’  have  identical  effects  on  all  the 
s  to  be  plausible.  Other- 
worlds 
the  agent 
and 
wise, 

outcomes  of  a  and  a’  would  have  been 

in  a  local  state  I  given  different 

in  PW(Z) 
the  plausible 

but  T(s,a)  #  T(s,a’) 

the  agent  must  consider 

except  s.  Thus, 

this  agent 

identical, 

234 

R.I.  Brafmn,  M.  Tennenholtz/Artijicial  Intelligence  94  (I 997)  217-268 

would  have  not  shown  preference 
is  closely 
lief  [44]. 

related 

to  Savage’s  notion  of  null-states 

for  one  action  over  the  other.  This  view  of  beliefs 
[56]  and  Morris’  definition  of  be- 

3.  Dynamic  mental-level  models 

section,  we  described 

In  this  section  we  consider 

that  can  take  sequences  of  actions 

the  question  of  how  mental  states  at  different 

that  are  inadequate 
in  light  of  our  desire 

a  model  of  simple  static  agents  and  the  basis 
In  the  previous 
a  more  complex,  dynamic  model  of 
for  its  construction. 
interleaved  with  observations.  While  many  of 
agents 
the  same,  some  definitions  must  be  adjusted,  and  a  number  of 
the  basic 
ideas  remain 
times 
new  issues  arise,  most  significantly, 
those  aspects  of 
relate  to  each  other.  Using  a  running  example,  we  start  by  considering 
agents  and  show  how  they 
the  static  model 
the  agent’s 
can  be  revised 
are 
state  at  neighboring 
are  discussed. 
considered 
the  dynamics  of  an  agent’s  beliefs.  This  is  the 
More  central 
topic  of  Section  3.2  in  which  one  approach 
the 
problem  of  ascribing  a  mental  state  to  a  dynamic  agent,  which  is  not  an  easy  task.  This 
instances  of  the  problem  of 
task  can  be  simplified 
mental  state  ascription 
the  conditions  under  which 
this  is  possible. 

it  to  multiple 
if  we  are  able  to  reduce 
in  the  static  model,  and  we  examine 

time  points.  Some  aspects  of  this  problem  of  state  dynamics 

in  Section  3.1,  where  the  dynamics  of  the  agent’s  preferences 

is  the  problem  of  modeling 

In  Section  3.3,  we  examine 

for  modeling  dynamic 

the  relationship 

is  suggested. 

to  capture 

between 

3.1.  A  view  of  dynamic  agents 

Consider 

a  mobile 

robot  acting  within  a  grid  world  whose 

locations  on  its  way  from 
and  one  possible  path  are  depicted 
robot  either  stops  or  moves 
each  of  the  positions  marked  Z  (in  the  example 
that  is  accurate 
position  or  any  adjacent  position; 
readings. 

three 
the  initial  position  Z  to  the  goal  position  G.  This  domain 
the 
in  one  of  four  possible  directions.  The  robot  can  start  at 
sensor 
the  actual 
its  local  state  consists  of  all  its  past  position 

it  starts  at  Zo) ; it  has  a  position 

to  within  a  distance  of  one, 

in  Fig.  4.  We  shall  assume 

i.e.,  the  sensor  may 

that  at  each  position 

is  to  visit 

indicate 

task 

and 

We  could  use  a  static  model 

single  choice  among  possible  protocols 
state.  ‘t  However, 
the  task  of  prediction. 
Section  4)  requires  modeling 
to  predict 
explicit 

this  representation 
Intuitively, 

its  behavior 
the  relationship 

between 

to  capture 

this  setting  by  viewing 

the  agent  as  making  a 
(often  called  policies  or  strategies)  in  the  initial 
support 
is  too  coarse  and  does  not  adequately 
in 
this  model 
in  the  future.  This,  in  turn,  requires  us  to  use  a  model  that  makes 

(more 
time  point  and  using 

the  process  of  prediction 

an  agent  at  a  particular 

fully  discussed 

the  state  of  the  agent  at  different 

time  points. 

”  This  representation  of  the  agent’s  decision  problem  is  referred  to  as  a  strategic form  description  in  the 
game  theory  literature. 

R.1.  Brafmnn,  M.  Tennenholtz/Art@cial  Intelligence  94 (1997) 217-268 

235 

Fig.  4.  A  task  requiring  multiple  actions. 

In  order  to  model  the  behavior  described  in  Fig.  4  ad~uately,  a  number  of  obvious 
changes  must  be  made  to  our  model.  First,  we  are  no  longer  considering  single  states, 
but  sequences  of  states,  e.g.,  in  the  above  example,  we  care  about  the  robot’s  path. 
Following 
[ 201,  we  shall  use  the  term  run  to  refer  to  the  sequence  of  global  states  of 
the  agent/environment 

system  starting  with  its  initial  state. 

Runs  describe  the  state  of the  agent  and  the  environment  over  time.  A run  is feasible 
with  respect  to  an agent  Jr  if  it is a sequence  of  global  states  SO, ~1,. . .  such  that  for  all 
k  >  0,  there  exists  some  a  E  AA  for  which  S-( ~-1,  a> =  Sk. A run is protocol  consistent 
(comisten.t  for  short)  with  respect  to  an  agent  A  if  it  is  a  feasible  run  SO, ~1, . . .  such 
that  for  all  k  >  0,  7(.3k_i,a)  =  sk and  ~a is  consistent  with  J’s 
(partial)  protocol.12 
That  is,  this  run  could  be  generated  by  a particular  sequence  of  actions  of  the  agent  that 
is consistent  with  its  (possibly  partial)  protocol.  We denote  the  set  of  all  feasible  runs 
by  7’3, and  from  now  on,  by  a  run  we  always  mean  a feasible  run,  We  denote  the  set 
of  suffixes  of  feasible  runs  by  ‘7?+.  Intuitively,  a  modeling  agent  needs  only  consider 
consistent  runs  when  it  attempts  to  ascribe  the  modeled  agent’s  beliefs  about  the  past, 
having  observed  the  modeled  agent’s  past  behavior.  However,  the  modeling  agent  must 
consider  all  feasible  runs  as possible  when it is attempting  to  predict  the  modeled  agent’s 
future  behavior. 

Next,  we  reconsider  our  definitions  of  beliefs  and  preferences.  Now,  our  robot  cares 
about  runs  rather  than  single  states,  and  it  is natural  to  describe  it as  having  beliefs  over 
run  prefixes  and preferences  over  run  suffixes  rather  than  over  possible  worlds.  That  is, 
at each  state,  it has  beliefs  concerning  its  current  and past  states,  and  it  has  preferences 
over  its  future  states.  Thus,  when  the  robot  is  at position  2,  it  has  various  beliefs  about 
what  actual  path  it  followed  to  its  currertt  state,  and  it  has  various  preferences  over 

I2 That  is,  either  n  is  assigned  by  A’s  partial  protocol  to  its  local  state  in  Sk-_l, or  the  protocol  does  not 
specify  an  action  on  this  particular  local  state. 

236 

R.I.  Brafmn,  M.  Tennenholtz/Artificial  Intelligence  94  (1997)  217-268 

from  here  on.  It  may  believe 

that  its  path  consisted  of  the  path 
in  Fig.  4  up  to  position  2,  or  it  may  consider  a  number  of  similar  possible 
that  will 

how  it  should  proceed 
depicted 
paths.  Similarly, 
take  it  to  position  3  and  then  to  the  goal  over  all  other  run  suffixes.  l3 
these  ideas,  we  redefine 

it  would  prefer  run  suffixes 

to  the  task  description, 

In  order  to  formalize 

to  be  over  the  set  of 

the  value  function 

according 

run  suffixes. 

Definition  14.  A  value  function 

is  a  function  u  : %?,,f  +  R. 

run  prefixes. 

too.  Intuitively, 

run  prefixes  using 

the  model  conceptually,  we  can  represent 

that  the  protocol,  at  least  up  to  the  current  state,  is  known). 

instead  of  having  a  set  of 
to 
In  order 
their  initial  state 

of  beliefs  should  change, 
the  agent  should  have  a  set  of  plausible 

The  definition 
plausible  worlds, 
simplify 
(with  the  implicit  assumption 
effects,  and 
This  follows 
hence,  each 
run.  l4  For  example, 
in  Fig.  4,  the  first  5  motion  actions  combined  with  the  initial  state  (which  cannot  be 
in  the  figure)  determine  a  unique  current  global  state.  In  this  model, 
completely  depicted 
the  robot’s  actual  position,  as  well 
the  environment’s 
as  the  readings 
time.  The 
local  state  describes 

if  it  were  to  reach  a  certain  position  at  a  certain 
readings  obtained 

from  our  modeling 
initial  global  state  and  protocol  determine 

state  in  each  global  state  describes 

that  actions  have  deterministic 

the  sequence  of  position 

it  will  receive 

assumption 

a  unique 

so  far. 

Definition  15.  Let  I  &  LA  x  LE  be  the  set  of  initial  worlds,  and  define 
PWI  on  local  states  as  follows: 

the  function 

PW,(Z)  =  {so  E  I  1 SO, ~1,.  . .  is  a  consistent 

run  of  A, 

and  there  exists  some  integer  n  and  environment 
such  that  s,  =  (E, e)}. 

state  e 

PW,(Z) 

is  called 
A  belief  assignment 

BI(/)  C  PW,(O. 

the  set  of  possible 

initial  worlds  at  1. 

is  a  function  BI  : L  4  2I  \  8  such  that  for  all  I  E  L  we  have  that 

PW,  (1)  tells  us  from  which  initial  global  states  it  is  possible 
Let  us  consider  how  the  robot’s  beliefs  evolve.  Suppose 

to  reach  the  local  state  1. 
its  possible 

that  among 

the  position 

the  robot  finds  those  in  which 
initially 
initial  positions; 
readings  will  be  correct.  Suppose 

worlds 
plausible.  Hence, 
the  assumption 
the  four  possible 
that  future  sensor 
that  the  robot  starts  sensing  only 
after  its  first  motion  command,  which  is  “go  down”,  and  it  receives  a  reading  of  (2,2) 
is  (2,2). 
as  its  current  position.  Consequently, 

each  of  these  worlds  will  embody 

initial  worlds,  corresponding 

it  will  have  four  plausible 

that  its  current  position 

readings  are  accurate 

it  will  believe 

initial 
to  be  the  most 
to 

I3 In  certain  situations,  preferences  over  whole  runs  may  he  more  appropriate.  For  example,  depending  on  a 
number  of  modeling  choices, 
We  refer  the  reader 
t4 As  we  remarked 
of  actions 
what  position 

the  initial  state  of  the  environment.  This  means 
the  robot  will  obtain  at  each  point  in  time  at  each  possible  position. 

to  [ 81  for  a  more  complete  discussion  of  this  issue. 
in  Section  2,  non-determinism 

about  the  effect 
that  the  initial  state  specifies 

this  may  be  the  case  when  the  agent  has  no  information 

to  uncertainty 
reading 

is  handled  by  transforming 

about  its  past  positions. 

all  uncertainty 

about 

R.I. Brafmn,  M. TennenholkIArtQicial Intelligence 94 (I 997) 217-268 

231 

this  belief 

Clearly, 
sometimes  we  find  it  more  convenient 
plausible 

is  equivalent 

current  worlds,  rather  than  a  set  of  plausible 

initial  worlds. 

to  the  belief 

that  its  initial  position  was  ( 1,2). 

Indeed, 
the  agent’s  beliefs  using  a  set  of 

to  represent 

Definition  16.  PW,,, 
worlds: 

: L  4  2’  assigns 

to  each  local  state  1 its  set  of  possible  current 

PWdZ) 

= {sn  I  so,  Sl,. 

. .  is  a  consistent 
and  there  exists  some  environment 
such  that  S,  =  (I,  e)}. 

run  of  A,  SO E  I, 
state  e 

The  curretlt  belief  assignment 

is  the  function  B,,,  : L  +  2’  \  8  such  that 

&,A) 

=  {in  1 so, ~13.. .  is  a  consistent 

run  of  A, SO E B!(Z), 

and  for  some  integer  n  and  environment 
we  have  that  sn  =  (I,  e)}. 

state  e 

That 

is,  PW,,,(Z)  will  contain 

a  possible  world  s  if  the  agent’s 

local  state  in  s  is 
at  one  of  the  initial  worlds.  B,,,(Z)  will 
in 

run  that  commences 

in  a  consistent 

in  a  run  which  commences 

those  currently  possible  worlds  occurring 

1,  and  s  appears 
contain 
a  plausible 

initial  world. 

The  plausible 

outcome  of  a  protocol  P  at  a  local  state  I  is  defined  much  as  be- 
current  states,  each  protocol  would  define 
readings. 
earlier.  Hence,  with  each  proto- 
future 

fore.  For  our  robot,  given  a  set  of  plausible 
a  set  of  run  suffixes  which  correspond 
Each  such1  path  has  some  value,  as  we  explained 
col  we  can  associate 
paths. 

to  some  path  with  associated 

the  value  of  these  plausible 

a  tuple  of  values  signifying 

sensor 

Definition  17.  Given  an  arbitrary, 
of  a  protocol  P  in  1 is  a  tuple  whose  kth  element 
by  applying  P  starting  at  the  kth  state  of  B,,,(l). 

fixed  enumeration 

of  B,,,(Z), 

the  plausible  outcome 
is  the  value  of  the  run  suffix  generated 

like  probabilistic 

its  beliefs  will  change. 

In  the  current  example, 

to  one  following  new  observations. 

the  set  of  plausible  worlds,  which  originally 

contained 
In  general,  when 
it 

Notice  how,  in  our  example, 
four  members  has  been  reduced 
the  agent  acquires  new  information, 
was  pretty  much  obvious  how  the  robot’s  beliefs  should  change: 
was  consistent  with  the  agent’s  previous  beliefs,  and  it  could  be  incorporated 
process  much 
conditioning 
that  after  its  second  motion  command 
is  (3,1).  This 
reading 
reading  of  (3,2)  would  be  plausible, 
beliefs. 
this  situation, 
In 
beliefs.  For  example, 
readings, 
that  its  first  sensor  reading  was  inaccurate.  This  issue  of  belief  change 
Section  3.2.  But  first,  let  us  consider  preferences. 

its  previous  beliefs,  under  which  only  a 
yields  an  empty  set  of 
its 
take  precedence  over  future 
that  its  initial  position  was  ( 1,  1)  and 
is  dealt  with  in 

using  a 
or  logical  conjunction.  However,  suppose 
reading 

it  could  assume 
in  which  case  it  will  come 

that  current  readings 
to  believe 

there  are  various  ways 

and  simple  conditioning 

(another  “go  down”), 

is  inconsistent  with 

the  new  information 

the  robot’s  position 

the  robot  could 

in  which 

revise 

238 

R.I.  Brufman,  M.  TennenhoWArtificial 

Intelligence  94  (1997)  217-268 

about 

is  (3,6), 

its  position 

and  it  is  comparing 

of  pi.  More  generally, 

of  p  over  the  remainder 

is  consistent  with  its  belief 

It  has  two  possible  protocols,  one 

Unlike  beliefs,  we  envision  preferences 

the  state  of  the  world,  and  it  considers 

to  be  quite  stable.  For  example,  suppose 

that 
our  agent  believes 
two  possible  paths  p  and  p’ 
in  both  of  which  its  next  position  would  be  position  2.  If  it  prefers  p  to  p’  and  its  next 
that  it  has  now  reached  position  2,  it  should  prefer 
reading 
that  the  agent 
the  remainder 
is  uncert~n 
three  initial  states  plausible: 
st,  ~2.3. 
the  worlds, 
and  it  prefers 
respectively; 
the  same  action,  a,  to  the  agent’s  current 
agent’s  beliefs  do  not  change, 
most  natural 
protocol.  Similarly, 
to  the  second,  we  would  expect 
since 
following  property,  motivated  by  the  intuition  above,  is  satisfied  by  u  and  p. 

to  the  runs  ~1, r2,  r3  in  each  of 
to  the  runs  r{ , r-b, r-i  in  each  of  the  worlds, 
assign 
a,  the 
It  is 
that  the  agent  will  still  prefer  the  first  protocol  over  the  second 
the  first  protocol 
a, 
that  the 

that  it  had  similar  preferences 
these  protocols  do  not  differ  on  the  first  step.  ~ons~uently, 

that  both  protocols 
local  state  and  that  after  performing 
its  plausible 

if  we  learned  only  now  that  the  agent  preferred 

the  first  protocol.  Now,  suppose 

i.e.,  st,  ~2, sg  itre  still 

before  performing 

and  one  leading 

respectively; 

we  assume 

to  expect 

suppose 

leading 

states. 

initial 

to  a  value 

is  s,  followed  by 

Definition  18.  Let  s  . r,  where  s  E  &  x  L  and  r  E  7&f,  denote  a  run  suffix  whose 
first  state 
p  is  static  with 
r.  A  decision 
function  u  if  for  any  natural  number  k,  for  any  set  of  run  suf- 
respect 
. , , Sk ’  r-L  E  7&f,  we  have  that 
fixes  rl  , . . . , rk,  ri,  . . . , r;,  SI  . r-1,. . . , Sk . rk,  s1 - ri,. 
iff 
(u(sI 
(U(Tl),..-r 

is  at  least  as  preferred  as  (u(ri),...,u(ri)). 

as  (U(St . r{),...,U(sk. 

is  at  least  as  preferred 

the  run  suffix 

’  rl),...rU(Sk 

criterion 

U(Q)) 

’  rk)) 

ri)) 

That 

is,  if  we  compare 

that  have  an  identical 
tuple 
than  the  second 
if  the  first  tuple  of  values  of  the  same  run  suffixes,  but  with  the  first  state 

two  tuples  of  values  of  run  suffixes 
tuple  of  states  as  their  prefix,  then  the  first  tuple  is  more  desirable 
if  and  only 
truncated, 
Notice 

is  more  desirable 
that  given 
definition  of  plausible  outcome 

than  the  corresponding 
the  fact  that  p  is  static  with  respect 
to  use  Bi  instead  of  B,,,. 

to  II,  it  is  easy  to  modify 

second 

tuple. 

the 

We  have  explained  how  the  concepts  of  state,  beliefs,  and  preferences 
the  decision  process 

to  the  dynamic  model.  In  many  respects, 

we  move 
as  it  was  before.  At  each  state, 
uses  its  value  function 
the  most  desirable  one.  Consequently, 
constraint  we  must  satisfy 
with  the  choice  predicted  by  the  model. 

the  agent  compares 

change  when 
itself,  remains 
and 
to  choose 
is  similar,  and  the 
is  that  at  each  local  state,  the  agent’s  actual  choice  conforms 

the  model  ascription  process 

a  set  of  plausible  outcomes 

(which  we  assume 

to  be  fixed) 

and  decision  criterion 

the  decision  process 

in  static  and  dynamic 

The  main  difference  between 

the  notion  of  preference  over  runs,  we  must  define 

settings 
that  in  order  to  match 
the  plausible 
outcomes  based  on  sets  of  run  suffixes  rather  than  based  on  sets  of  states.  This  implies 
that  we  must  view  the  agent  as  choosing  between  protocols 
rather  than  single  actions, 
since  a  single  action  produces  a  single  next  state  while  a  protocol  produces  a  sequence  of 
states.  Indeed,  our  mobile  robot  must  choose  among  different  paths  rather  than  different 
next  positions;  next  positions  cannot  be  judged  “good”  or  “bad”  by  themselves  but  only 
1  to 
that  follow.  For  example, 
in  the  context  of  the  actions 

the  move  from  position 

is 

R.I. Brafman, M. Tennenholtz/Art$iciaI Intelligence 94 (1997) 217-268 

239 

position 
path.  It  is  not  good  if  it  is  followed  by  actions 

in  Fig.  4  is  good  if  it  is  followed  by  actions 

(3,3) 

that  lead  the  robot  back  to  ( 1,  1) . 

that  lead  to  the  displayed 

a  model 

two  practical 

its  preferences 

the  agent  repeat 

fashion  and  when 
justification 

This  view  of  the  decision  making  process  raises 

in  which  the  agent  makes  its  choice  once  and  for  all  is  equivalent 

issues:  The  first  issue 
is  why  should 
the  above  comparison  of  protocols  at  each  state;  it  can 
simply  make  the  choice  once  and  for  all.  Indeed,  we  will  show  that  under  certain  natural 
conditions, 
to 
this  is  true  only  when  the  agent’s  beliefs  change 
the  model  described  above.  However, 
in  a  particular 
have  certain  properties.  We  view  this 
these  constraints  on  the  model.  The  second 
result  as  an  indirect 
issue  has  to  do  with  the  feasibility  of  the  above  decision  process.  The  size  of  the  set  of 
in  the  number  of  steps  in  the  worst  case).  Hence, 
protocols 
for  the  decision  maker  or  for  its 
comparing 
a 
modeler. 
require 
small, 
approach 
assigning 
to  use 
of  assigning  values 
to  choices 
such  a  decomposed  model 
between 

to  run  suffixes.  In  Section  3.3,  we  show  when  it  is  possible 
the  decision  making  process 

l5  One  solution  would  be  to  compare  a  single  action  at  a  time.  Assuming 

simpler.  However, 
immediately 

is  not  a  feasible  alternative 

this  would 
plausible 

is  very  large  (exponential 

to  single  states,  rather 

fixed  set  of  actions, 

in  the  current  state. 

this  is  considerably 

than  the  more 

single  actions 

all  protocols 

for  adopting 

is  reduced 

in  which 

values 

3.2.  Belief  change 

As  the  agent  makes  new  observations, 

this  process  using  Fig.  5.  The  agent  is  initially 

initial  worlds  are  u, w, X, y.  It  has  two  actions 

its  local  state  changes  and  with  it  its  knowledge. 
in  local  state  I,  where 
it  can  take,  a  and  a’,  each 
is  u  or 
is  x  or  y  and  into  14 

to  one  of  two  new  local  states:  a  leads 
into  13 when 

We  can  illustrate 
the  possible 
leading 
n  and  into  12 otherwise.  a’  leads 
otherwise.  We  see  that  after  performing 
because 
case,  since 
function  of  its  action  and  the  actual  world.  For  example, 
initial  world 
local  state  is  12. 

either  action, 
increases 
the  agent’s  knowledge 
to  be  possible.  This  need  not  always  be  the 
the  agent  may  “forget”.  We  also  see  that  the  agent’s  new  local  state  is  a 
if  it  performs  a  and  the  actual 
is  y,  its  new 

is  X,  its  new  local  state  is  11. However, 

into  Ii  when 
the  initial  world 

if  the  actual  world 

the  initial  world 

initial  worlds 

it  considers 

fewer 

With 

the  change 

in  the  agent’s 

the  agent’s  preferences 

local  state  following  new  observations, 

its  ascribed 
mental  state  should  change  as  well.  Our  mental-level  model  has  two  components  which 
are  not  state-dependent, 
and  decision  criterion,  and  one  compo- 
In  the  static  model,  we  did  not  require  any 
nent  whiclh  is  state-dependent, 
relationship 
local  states,  and  the 
to  hold  between 
locally.  However,  now  local  states 
process  of  belief  ascription 
temporal  order.  We  wish  to  add  global 
are  more  closely 
constraints 
local  states.  That 
is,  we  would 

like  to  model  an  agent’s  belief  change. 

the  beliefs  of  the  agent  at  different 

that  reflect  these  relations  between 

could  have  been  done 

on  the  agent’s  beliefs 

to  one  another 

its  beliefs. 

through 

related 

I5 While 
be  implemented 
is  important 

this  point  makes 

intuitive  sense, 
in  a  very  efficient  manner 

the  process  of  choosing  among  protocols  can,  in  certain  contexts 
that  does  not  involve  an  explicit  comparison  of  all  alternatives. 

It 

to  remember 

that  we  are  only  committed 

to  the  semantics  of  this  process. 

240 

R.I.  Brawn,  M.  Tennenholtz/Art$icial 

Intelligence  94  (1997)  217-268 

‘A. 

r---- 
I  VW’ 
L____’ 

14 

Fig.  5.  The  change 

in  an  agent’s 

local  state  after  performing 

actions  a  and  a’,  respectively. 

There 

is  vast  literature  on  the  issue  of  how  an  agent  should  change 

[ 2,7,16,22,28,29] 
our  modeling  perspective  will  lead  us  to  ask  somewhat  different  questions. 

),  and  we  will  discuss 

its  relation 

(e.g., 
to  our  work  later  on.  However, 

its  beliefs 

In  what  follows,  we  assume  our  agent  does  not  forget.  Formally,  an  agent  has  pelfect 
recall  if  its  local  state  encodes  all  previous 
that  the  agent’s 
local  state  is  never  the  same  in  two  different  global  states  on  a  given  run.  Some  of  the 
following 
e.g.,  the 
results  also  apply  when  this  property  holds  under  weaker  conditions, 
agent  has  a  clock. 

local  states.  This 

implies 

Consider 
those  worlds 
whenever 

such  worlds  exist. 

the  following 

restriction  on  belief  change:  my  new  plausible  worlds  should  be 

that  were  previously  plausible  and  are  consistent  with  my  new  information 

l6  if  for  local  states  I, I’  such  that 
Definition  19.  A  belief  assignment  BJ  is  admissible, 
I’  follows 
that  BI  (I’)  = PWI  (1’)  n  BI  (1). 
If  PW,  (1’)  n  BI  (Z)  =  0,  1’ is  called  a  revision  state  and  the  agent’s  new  beliefs  BJ (I’) 
can  be  any  subset  of  PWI  (I’). 

I  on  some  run,  PWI  (Z’)  n BI  (I)  #  0  implies 

This  manner  of  revising  beliefs  given  consistent 

to  the  agent’s  current  beliefs.  Indeed, 

analogue  of  probabilistic 

qualitative 
information 
literature  on  belief  revision,  which  concentrates 
what  we  call  revision 
We  can  illustrate 

conditioning, 

states. 

information 
or  alternatively 
this  operation 

can  be  viewed  as  the 
the  new 
to  conjoining 
in  the 
is  quite  standard 
in 
the  agent’s  new  beliefs 

on  restricting 

then  BI(  El)  =  {x}.  However,  assume 

{x,  w}.  After  performing 
are  admissible 
arrives  at  12 after  performing 
at  Z2, even  if  its  beliefs  are  admissible 
agent’s  plausible  worlds 
beliefs 

to  reflect  this. 

the  process  of  belief  change  using  Fig.  5.  Assume 

that  BI(  Z)  = 
action  a  the  agent  finds  itself  in  state  Et. If  the  agent’s  beliefs 
that  BI(  1)  =  {x,  ZI} and  the  agent 
a.  Now  we  cannot  say  anything  about  the  agent’s  beliefs 
the 
its 

(except  of  course  B,(  Z2) &  {w,  y}).  Clearly, 
in  the  past  are  not  really  possible,  and  it  must  now  revise 

l6 This  is  unrelated 

to  the  game-theoretic 

notion  of  admissibility. 

R.I.  Brafman,  M.  TennenholtdArtificial  Intelligence  94  (1997)  217-268 

241 

If  we  were 

to  assume 
in  syntactic 

that  the  set  of  possible  worlds  consists  of  models  of  some 
the  new  data 
terms,  admissibility 

to  conjoining 

corresponds 

then, 

theory 
with  the  existing  beliefs,  whenever 

this  is  consistent. 

We  can  understand 
This  theorem  shows 
accordance 
static  ranking 
structure 
each  state  1’ the  set  BI(  I)  is  exactly 
respect 

to  the  admissibility 

to  this  ranking. 

this  restriction  better  through  the  following 
that  we  can  either  ascribe 

theorem. 
in 
locally 
requirement  or  we  can  ascribe  the  agent  a  more  complex 
in  each  state.  Specifically,  at 
that  are  minimal  with 

representation 
that  change 

that  uniquely  determines 

the  set  of  elements 

the  agent  beliefs 

in  PWI(Z) 

its  beliefs 

Definition  20.  A  well  founded 
ordered  set  0 
elements  minimal 
assigned 

to  elements  of  Q’. 

from  Q  to  a  well 
ranking  r  of  a  set  Q  is  a  mapping 
(which  we  will  take  to  be  the  integers).  Given  a  subset  Q’  of  Q,  the 
the  ranks 

in  Q’  are  those  that  are  assigned 

rank  among 

the  minimal 

A  ranking  of  Q  associates  each  member  of  Q  with  the  group  of  other  members  having 
to  them.  In  general 
the  same  rank  and  orders  these  groups  according 
one  speaks  of  a  total  pre-order  with  minimal  elements.  The  elements  of  lower  ranks  are 
considered  better,  more  preferred,  or  more  likely. 

to  the  rank  assigned 

Theorem  21.  Assuming  perfect  recall,  a  belief  assignment  B  is  admissible 
a  ranking 

,function  r  (i.e.,  a  total  pre-order) 

ifs  there  is 
initial  worlds  such  that 

on  the  possible 

BI(~)  =  {s  E  PW,(O 

I s  is  r-minimal 

in  PWl  (I)  }. 

similar 

Indeed, 

[ 22,361). 

to  obtain 

information 

the  agent’s 

in  a  revision 

to  ours  emerge 

on  the  agent’s  beliefs 

Patterns  of  belief  change 

at  a  special  case  of  belief 

the  AGM  postulates 
states.  However, 

using  partial  and  total  pre-orders  are  well  known. 

because  we  are  looking 
recall  and  we  do  not  need 

in  the  work  of  other  researchers 
relations  between  belief  revision  and  belief  update,  and  rep- 
It  was  shown  by  Grove 
[2]  can  be  repre- 
that  result, 
state  are  needed.  We  do  not 
revision: 
for  general  counterfac- 
queries  when 
a  revi- 
in  the  world,  only  a  lim- 
the 
on  the  modeler  and  allows  us  to 
task, 
repeatedly 
actual  worlds  but  with  the  same  local  state,  we  will  need  the  ad- 
to  obtain  a  fixed  ranking.  However,  we  note  that 

(e.g., 
resentations 
1251  that  any  revision  operator 
that  satisfies 
sented  using  a  ranking  of  the  set  of  possible 
constraints 
additional 
require 
such  constrains 
our  agent  has  perfect 
tual  reasoning.  We  learn  about 
it  receives 
sion  state.  However,  when  one  observes 
ited  number  of  such  revisions 
actual  state  of  the  world.  This  puts 
obtain 
starting  at  different 
ditional  properties  used 
the  difference  between  our  approach  and  the  AGM  approach 
tal.  They  ask  the  question: 
I  model 
next  subsection,  where  we  examine 
agents. 

is  more  fundamen- 
[2] 
I  change  my  beliefs?  We  ask:  how  should 
in  the 

the  belief  change  of  another  agent?  This  difference  becomes 

can  occur,  all  of  which  must  be  consistent  with 

this.  result.  When  we  observe  an  agent 

clearer 
for  modeling 

its  beliefs;  we  called 

an  agent  acting 

to  counterfactual 

that  contradicts 

less  constraints 

the  suitability 

this  situation 

of  admissible 

how  should 

to  account 

in  [25,29] 

performing 

the  same 

response 

beliefs 

242 

R.I.  Brafinan,  M.  TennenhoWArtijicial 

Intelligence  94  (1997)  217-268 

Finally, 

in  [lo]  we  investigate 

another  pattern  of  belief  change,  which  we  call  weak 
admissibility.  Weakly  admissible  beliefs  allow  the  new  plausible  worlds  to  contain  pos- 
sible  worlds 

that  were  not  plausible  before,  even  in  non-revision 

states. 

3.3.  Ascribing  admissible  beliefs 

Having  defined  mental-level  models 

for  dynamic  agents,  we  come  to  the  question  of 
their  construction.  The  general  process  of  ascribing  a  mental-level  model  and  the  special 
to  dynamic 
from  static 
case  of  ascribing 
agents.  Much 
that  are  consistent  with 
the  agent’s  behavior 

the  same  with  the  transition 
like  the  static  case,  we  must  search  for  models 

(i.e.,  its  protocol). 

remain 

beliefs 

Definition  22.  A  mental-level  model  for  a  dynamic  agent  (LA,  Ad,  B,,  u, p)  is  consis- 
tent  with  a  protocol  P  if  for  all  I  E  L  it  is  the  case  that  the  plausible  outcome  of  P  is 
most  preferred  according 
It  is  consistent  with  a 
to  BI,  u,  and  p,  and  B,  is  admissible. 
partial  protocol  P’  if  it  is  consistent  with  some  completion  P  of  P’. 

only  on  the  action 

is  that  in  the  dynamic  case  an  agent’s  beliefs 
in  other  states.  Thus,  we  cannot  decompose 

Two  properties  of  dynamic  agents  seem  to  make  ascribing  a  mental-level  model  more 
loss  of  locality, 
in  terms 
the  effect  of  actions.  The 

difficult.  Both  of  these  properties  have  to  do  with  an  apparent 
of  what  beliefs  are  acceptable  and  in  terms  of  how  we  evaluate 
first  problem 
by  its  beliefs 
second  problem 
dependent 
effect  of  actions 
Therefore,  we  cannot 
that  follow 
example,  buying  a  run-down 
it  later  and  sell  it  at  a  premium, 
place  of  residence.  As  we  will  see,  under 
decision 
criterion,  both  of  these  problems 
vector  concatenation. 

in  one  state  are  constrained 
the  ascription  of  beliefs.  The 
is  that  the  plausible  outcome  of  a  protocol  P  given  B(Z)  is  no  longer 
the 
to  I.  That 
locally  because  we  use  a  value  function  over  runs  rather 
than  states. 
the  actions 
from 
instead  of  single  actions.  For 
if  I  plan  to  renovate 
if  I  use  it  as  my 
the  following  weak  condition  on  the  agent’s 
can  be  handled.  Here,  we  use  o  to  denote 

it,  and  we  must  compare  complete  protocols 

apartment  may  lead  to  a  good  outcome 

say  how  good  a  single  action 

is,  we  no  longer  measure 

lead  to  a  bad  outcome 

that  P  assigns 

is  in  isolation 

or  it  may 

Definition  23.  Let  (w,  w’)  and  (u,  u’)  be  two  pairs  of  real  valued  vectors  such  that 
]wJ =  ]LJ and  Iw’I =  Iu’I.  A  d ecision  criterion 
if  u o w  is 
at  least  as  preferred  as  u’ o VV’ whenever  u  is  at  least  as  preferred  as  u’  and  w  is  at  least 
as  preferred  as  w’. 

the  sure-thing  principle 

satisfies 

the  sure-thing  principle 

Intuitively 
a  over  a’  when 
prefer  a  over  a’  when 
over  a’  when  the  plausible  worlds  are  WI, ~2, wg , w4, ~5, Wg. 17 

suppose  you  prefer  action 
(or  initial)  plausible  worlds  are  wt , ~2, wg,  and  you  also 
the  plausible  worlds  are  w4, ws,  Wrj. Then,  you  should  prefer  a 

[ 561  says  the  following: 

the  current 

I7 Our  definition  of  the  sure-thing  principle 
ences  in  the  basic  framework.  However, 

the  essential 

idea  is  the  same. 

is  not  the  same  as  Savage’s  original  definition  because  of  differ- 

R.I.  Brafman,  M.  Tennenholk/ArtQicial  Intelligence  94  (1997)  217-268 

243 

Throughout 
decision  criterion 
that  the  constraints 
make  the  ascription  process  easier. 

satisfies 

this  section,  we  restrict  ourselves 

the  sure-thing  principle.  Under 

to  mental-level  models 

the 
this  assumption,  we  can  show 

in  which 

imposed  on  beliefs  at  local  states  by  the  admissibility 

requirements 

;!4.  Let  P  be  an  agent’s  protocol. 

Theorem 
initial  state  and  at  subsequent 
an  admissible  belief  assignment  at  all  local  states. 

revision  states  based  on  this  protocol, 

If  this  agent  can  be  ascribed  beliefs  at  the 
it  can  be  ascribed 

That 

is,  if  we  are  able 

to  find  a  consistent 

local  states  the  belief  assignment 

assignment 
initial  state:  based  on  a  given  protocol,  we  are  guaranteed 
revision) 
admissibility 
assignment, 
beliefs  at  initial  states.  Hence,  admissibility, 
advantage. 

is  also  consistent.  Revision 
and  ascribing  beliefs 

of  belief 
that  in  the  following 

to  an  agent  at  its 
(non- 
the  criterion  of 
states  are  not  constrained  by  the  initial  belief 
to  the  task  of  ascribing 

that  is  obtained  by  following 

rather  than  being  a  handicap 

in  these  states  is  analogous 

is  actually  an 

from 

stemmed 

The  second  problem 

the  natural 
the  fact  that  in  the  dynamic 
definition  of  values 
is  over  runs.  If  we  could  provide  conditions  under  which  a  natural 
definition  of  values  over  states  is  possible,  we  would  not  have  this  problem.  This  will 
a  simpler,  decomposed  model  with  which  it  is  easier  to  work.  In 
allow  us  to  construct 
this  model,  we  will  need  to  consider 
than 
the  long-term 

effect  of  actions  only,  rather 

effect  of  protocols. 

the  immediate 

case, 

Definition  25.  A  local  value  function 
states  to  R.  Given  a  fixed  enumeration 
outcome  o:f an  action  a  at  I  is  the  (suitably  ordered) 
each  s  E  B,,,(Z). 

is  a  function 

ucur  from 
of  the  elements  of  B,,,(Z), 

tuple  containing 

the  set  S  of  global 
local 
u,,,(  a(  s)  )  for 

the  plausible 

rather 

With  these  ideas  we  can  proceed 

both  the  beliefs  and  the  values  are  localized.  That  is,  the  value  function 
states, 
consistent  with  a  protocol  P  (or  our  observations) 
1, we  use  the  state  based  value  function 
by  P  at  I  has  the  most  preferred 

to  define  a  decomposed  mental-level  model  in  which 
is  defined  over 
is 
is  much  easier:  For  each  local  state 
to  check  whether  the  action  P(  1) that  is  assigned 

than  run  suffixes.  Consequently, 

that  a  decomposed  model 

immediate  outcome. 

verifying 

Definition  26.  A  decomposed  mental-level  model 

is  a  tuple  (L,  actions,  Bcur, uCUrr p). 
A  decomposed  mental-level  model  (L,  actions,  Bcur, uCUT, p)  is  consistent  with  a  (pos- 

sibly  partial)  protocol  P  if 

(1) 

(2) 

for  any  local  state  1 on  which  P  is  defined, 
in  1 is  most  preferred  among  all  plausible 
if  E” follows  2 on  some  run,  then 

the  plausible 
local  outcomes; 

local  outcome  of  P(Z) 

&,,(I’) 

=  PW,,,(I’) 

n  {P(l)(s) 

t 3 E  &w(O), 

when  it  is  not  empty. 
last  condition 

says  that  B,,, 

(This 

is  admissible.) 

244 

R.I.  Brafmn,  M.  Tennenholtz/Art&ial 

Intelligence  94  (1997)  217-268 

Given  a  local  value  function  over  states,  in  order  to  ascribe  beliefs  to  the  agent,  we  no 
the  values  of  run 
the  immediate  plausible  outcome  of  single  actions, 
simple, 

longer  have  to  examine 
suffixes.  Instead,  we  simply  compare 
much  like  the  case  of  static  agents.  Hence,  decomposed  models  are  conceptually 
to  compute  with,  and  require  a  simpler  representation. 
are  simpler 

the  effects  of  complete  protocols  and  compare 

In  the  following  we  will  identify 

runs  of  bounded 

length  with  runs  containing 

a  suffix 

all  of  whose  states  are  identical.  We  can  show  the  following: 

Theorem  27.  Let  A  be  an  agent  whose  possible  runs  have  bounded 
we  can  ascribe  a  consistent  mental-level  model 
missible  and  p  satisfies 
decomposed  mental-level  model  (L,  Actions,  B,,,,  u,,,,  p). 

length,  for  which 
(L,  actions,  Bt , u, p),  where  Bt  is  ad- 
the  sure-thing  principle.  Then,  A can  be  ascn’bed  a  consistent 

A  corollary  of  these  results 

is  thus: 

that  this  agent  can 
Corollary  28.  Let  P  be  the  protocol  of  an  agent,  and  suppose 
revision  states  based  on 
be  ascribed  beliefs  at  the  initial  state  and  at  all  subsequent 
this  protocol  under  a  decision  criterion 
that  satisfies  the  sure-thing  principle.  Then,  this 
agent  can  be  ascribed  an  admissible  belief  assignment  at  all  local  states  and  a  local 
value  function  over  states  such  that  its  observed  action  has  a  most  preferred  plausible 
outcome  according 

to  the  local  value  function, 

a  standard  value  function 

requiring  a  process  analogous 

(over  runs)  with  an  equivalent 

replacing 
is  quite  difficult, 

to  a  run  suffix  based  on  whether  or  not  the  agent  wins 

In  practice, 
value  function 
Consider  an  agent  playing  chess  against  a  computer  program.  A  value  function  can  be 
in  this  run. 
assigned  naturally 
tell  the  agent  at  each  state  whether  a  particular  move  will 
A  local  value  function  would 
lead  it  to  a  position 
is 
is  often  unrealistic,  but  so 
really  an  ideal  heuristic.  Naturally,  computing 
is  struck  using 
is  the  task  of  comparing 
some  form  of  look-ahead 
i.e.,  a  less  than  perfect 
function. 
heuristic 

from  which  it  can  win,  tie,  or  lose.  Hence,  a  local  value  function 
such  a  function 
In  practice,  a  compromise 
function, 

to  dynamic  programming. 

all  possible  protocols. 

local  evaluation 

and  some 

local 

4.  Prediction 

One  central  application 

of  agent  modeling 

is  prediction. 

agent  context 
of  our  theory  can  be  combined 
mental-level  model.  The  context  we  would 
observed 
predict 
on  plan  recognition 
Section  9. 

an  agent 

taking  part  in  some  activity;  we  know 

its  next  actions.  The  approach  we  suggest  underlies 
to  air-combat 
and  some  of  its  applications 

In  this  section  we  pause 

to  obtain  an  approach 

like 

to  deal  with 

that  is  of  particular 

for  action  prediction 

in  the  multi- 
interest 
to  show  how  the  various  aspects 
using  a 
is  the  following:  We 
to 
some  of  the  work  done 
in 
simulation, 

its  goals;  and  we  wish 

discussed 

R.I.  Brafman,  M.  Tennenholtz/ArtiJicial  Intelligence  94  (I 997)  217-268 

245 

Mental  level 

mental  -level  model 

Background 

Observables 

(real world) 

predicted  behavior 

Fig.  6.  Three  step  prediction  process. 

a  mental-level  model  of  the  agent  based  on  actions  performed  until 

three  steps  (illustrated 

in  Fig.  6): 

To  predict  an  agent’s  next  action,  we  go  through 
( 1)  construct 
now; 
revise 
after  performing 

the  agent’s  ascribed  beliefs, 

the  last  action;  and 

(2) 

if  needed,  based  on  the  observations 

it  made 

(3)  pre.dict  the  action 

(or  protocol)  which  has  the  most  preferred  perceived  outcome 

based  on  these  beliefs. 

The  following 

example  serves  to  illustrate 

this  idea. 

Example  .29.  We  start  with  a  robot  located  at  an  initial  position  whose  task  is  to  find 
in  one  of  three  possible  positions:  A,  B,  or  C.  We  assume 
a  small  can  located 
that  the 
robot  knows 
these  to  be  the  only  possible  positions  of  the  can.  Hence,  we  have  three 
initial  states  of  the  environment.  The  robot  can  move  in  any  direction  and  can 
possible 
recognize 

the  can  from  a  distance  of  2  meters. 

(See  Fig.  7.) 

In  this  example,  a  run  would  describe 

the  trajectory  of  the  robot  through 

the  space, 
the  robot  has  observed 

the  position  of  the  can,  and,  at  each  point  along  the  run,  whether 
the  can. 

We  will  assume 

that  the  following  value  function 

preferences,  which  depend  on  the  length  x  of  the  robot’s 
not  it  terminates 

in  the  position  of  the  can, 

(over  runs)  describes 

the  robot’s 
trajectory,  and  on  whether  or 

Lld~ff.[o-x+20* 

1 

if  the  trajectory 

terminates  at  the  can, 

0  otherwise. 

{ 

criterion 

(although 

Having  observed 

initial  path,  as  shown 
knowledge  of  its  preferences 
the  following 

in  Fig.  8,  we  can  try  to  ascribe 
the  robot’s 
that 
it  beliefs  using  our  background 
to  the  principle  of 
it  uses  the  maximin 
indifference).  What  we  see  is  that  the  ascribed  plausible 
in 
which  the  can  is  in  {A,  B}  or  {A,  B,  C}.  That  is,  only  with  such  initial  plausible  worlds 
if  the  robot  believed  only 
would 
one  initial  possible  world  to  be  plausible 
in 
to  the  can’s  position 
that  state.  Similarly, 
toward 
them. 

the  robot’s  path  would  head  more 

initial  worlds  are  those 

if  {B,  C}  was  believed, 

trajectory.  For  example, 

it  would  head  directly 

and  the  assumption 

the  robot  choose 

the  observed 

also  applies 

246 

R.I.  Brafman, M.  Tennenholtz/Art$cial  Intelligence 94 (1997) 217-268 

I  I 

0  Initial  position 

10m. 

I 

Fig.  7.  Initial  set-up. 

Fig.  8.  Robot’s 

initial  path. 

Next,  suppose 

that  the  can 

is  in  B. At  its  current  position 

the  can  is  not  in  A,  and  its  local  state  has  changed 
robot’s  new  beliefs  are  now  revised 
are  admissible).  Given 
be  to  turn  to  its  left  (i.e.  toward  B). 

these  beliefs,  we  can  predict 

to  contain  B and  possibly  C  (assuming 

to  incorporate 

the  robot  can  see  that 
this  knowledge.  The 
its  beliefs 
that  the  robot’s  next  action  would 

its  beliefs 

(what  we  called 

in  Section  3.2  does  not  constrain 

One  weakness  of  this  approach 
contradicts 

observation 
definition  of  admissibility 
these  conditions.  Recalling 
a  ranking  over  the  set  of  possible  worlds  discussed 
if  we  have  observed 
be  overcome 
Then,  we  can  use  this  ranking 
Another  approach 
in  which  case  we  can  attempt 

is  that  we  have  little  to  say  when  the  modeled  agent’s 
that  our 
the  agent’s  new  beliefs  under 
and 
there,  we  can  see  this  problem  could 
its  ranking. 
the  agent’s  new  beliefs  even  in  a  revision  state. 
to  it, 
Indeed, 

is  possible  when  our  set  of  possible  worlds  has  more  structure 

the  relationship  between  an  admissible  belief  assignment 

the  ranking,  or  some  of  its  properties. 

in  the  past  and  have  learned 

states).  Recall 

this  agent 

to  deduce 

to  induce 

revision 

R.I.  Brajnan,  M.  Tennenholtz/ArtQicial  Intelligence  94  (1997)  217-268 

241 

the  set  of  possible  worlds  with  a  set  of  models  of  some  language, 
most  often  we  equate 
to  which  they  assign  similar 
in  which  case  different  worlds  are  related  by  the  sentences 
about  the  relationship  between 
truth  values.  We  can  then  introduce  various  assumptions 
are 
the  agent’s  beliefs,  as  stated 
in  the  literature,  and  a  number  of  such  methods  are 
made.  Various  relationships 
discussed 
consider  an  agent  that  knows  p  V  r  and  believes 
p Aq.  Suplpose  it  now  learn  14.  Thus,  none  of  the  worlds  that  were  previously  considered 
in  p  persists. 
plausible 

in  that  language,  before  and  after  new  observations 

are  still  plausible.  However,  we  could  still  assume 

in  [ 291.  As  an  illustration, 

that  its  belief 

appear 

5.  Choosing  among  belief  assignments 

this  is  similar 

there  is  more 

example,  often 

As  we  observed 

than  one  consistent 

to  assign.  One  common  approach  for  choosing  among  different  models 

in  the  thermostat 
Indeed,  we  find  this  to  be  the  case  even  in  simple  examples.  We  can 
to  restrict  further  the  kind  of  models 
is 
that  we  are  willing 
[ 171. 
learning 

belief  assignment. 
handle  thi:s problem  by  using  background  knowledge 
we  are  willing 
to  a  priori  restrict  or  rank  them.  In  the  first  case,  we  limit  the  models 
to  ascribe; 
In  the  latter  case  we  use  the  ranking  over  models 
model; 
use  the  additional 
models  of  some  propositional 
willing 
simple  formulas, 
use  some  measure  of  the  complexity  of  the  formula 

the  most  normal  consistent 
to  ascribe 
to  the  notion  of  preference  bias  [ 171. I8  In  particular,  we  could 
to 
the  class  of  models  we  are 

correspond 
to  relatively 
l9  Alternatively,  we  can 

the  belief  assignments 
of  primitive  propositions. 

to  the  restricted  hypothesis  space  bias in  machine 

the  set  of  possible  worlds  corresponds 

to  rank  the  different  models. 

language.  Then,  we  restrict 

structure  obtained  when 

such  as  conjunctions 

th.is  is  similar 

to’  ascribe 

in  which 

to  those 

In  this  section,  we  investigate 

two  domain-independent 

and  show 
they  lead  to  a  unique  choice  of  model.  We  start  with  some 
for  the 

for  the  static  model  and  continue  with  a  particular  heuristic 

choice  heuristics 

that  under  certain  conditions 
general  heuristics 
dynamic  model. 

5.1.  Choice  assumptions  for  static  agents 

A  common  modeling  bias  is  to  favor  models 

of  the 
data.  In  our  case,  we  would  like  the  ascribed  model  to  be  such  that,  at  each  state,  there  is 
a  unique  most  preferred  plausible  outcome 
outcomes. 

rather  than  a  set  of  most  preferred  plausible 

that  offer  adequate  explanation 

Definition  30.  A  mental-level  model 
states  I,  the  decision  criterion  p  returns  a  unique  plausible  outcome  when  applied 
set  of  plausible  outcomes  of  all  protocols 

(LA,  Ad,  B, u, p)  is  expkmatory  if  for  all  local 
to  the 

in  1. 

it  seems 

I8 In  practice, 
their  own. 
I9 This  was  pointed  out  to  us  by  Hector  Levesque. 

impose  biases 

that  people 

that  make  other  people’s  beliefs  or  preferences 

similar 

to 

248 

R.I.  Brfman,  M.  TennenholtdArtificial 

Intelligence  94  (1997)  217-268 

in  the  state  “-” 

Example  2  (continued).  Recall 
thermostat’s 
beliefs 
belief  assignments 
is  explanatory.  Given 
while  given  any  of  the  other 
choice  between 

satisfy 

turn-on  and  shut-off. 

to  only 

that  in  Section  2.3  we  were  able 

the 
to  constrain 
the  state  cold.  Four 
this  property.  However,  only  one  of  them,  B(  -)  =  {cold} 
turn-on, 
the  agent  must  choose 
to  the 

the  action 
is  indifferent 

three  belief  assignments, 

that  include 

the  agent 

those 

this  belief  assignment 

A  different  modeling  bias  is  toward  greater  generality.  Given  a  number  of  possible 
is  for  those  making 
equally  we& 
the  preference 
the  agent’s  beliefs.  That  is,  we  prefer  belief  assignments 

some  behavior 

that  explain 

models 
fewer  assumptions 
in  which  fewer  worlds  are  ruled  out. 

regarding 

Definition  31.  A  belief  assignment  B  is  more  general 
have  that  B’(Z)  G  B(l) 

and  B  #  B’. 

than  B’  if  for  all  1 E  LA  we 

Given  a  set  of  belief  assignments,  B,  B  E  t3  is  a  minimal  belief  assignment  with 

respect 

to  B  if  there  is  no  B’  E  I3  such  that  B’  is  more  general 

than  B. 

Thus,  minimal 

belief  assignments 
is,  they  exclude  as  implausible 
is  minimal 

explanatory 

That 
lief  assignment 
those  belief  assignments 
minimal 
tory. 

explanatory 

for  which 
belief  assignment 

to  the  agent 

ascribe 
the  weakest  set  of  beliefs. 
the  smallest  number  of  possible  worlds.  A  be- 
among 
is,  a 

if  it  is  a  minimal 
the  mental-level  model 

belief  assignment 
is  explanatory.  That 

rules  out  just  enough  worlds 

to  be  explana- 

Example  2  (continued).  Any  belief  assignment 
is  explanatory 
assignment 

for  the  state  +.  However, 

for  that  state:  {ok,  hot}. 

that  is  a  non-empty 
there  is  a  unique  minimal 

subset  of  {ok,  hot} 
belief 
explanatory 

To  summarize,  we  have  the  following 

(unique)  minimal  explanatory  belief  assignment 

for  the  thermostat: 

At  this  stage  we  believe  we  have  a  satisfactory 

formal  account  of  McCarthy’s 

mostat  example,  which  has  been  useful 
example 
examine  whether 

this  is  true  in  the  general  case. 

there  was  a  unique  minimal  explanatory  belief  assignment.  We  now  proceed 

in  demonstrating 

our  basic  concepts. 

ther- 
In  this 
to 

Example  32.  Consider 

the  following  decision  problem: 

I 

a1 

a2 

Sl 

2 
7 

s2 

2 
7 

$3 

11 
0 

s4 

2 
7 

$5 

2 
7 

R.I.  Brafmn,  M.  Tennenholtz/Art@cial  Intelligence  94  (1997)  217-268 

249 

the  agent  has  taken  action  at.  We  can  see  that  both  {st , ~2, sg}  and  {ss,  ~4, sg} 
reason 
there  is  no  unique  minimal  consistent  belief  assignment 

given  the  principle  of  insufficient 

belief  assignments 

it  would  have  to  contain  both  of  these  belief  assignments, 

this  belief  assignment, 

hence  it  would  have 
the  action  a2  is  more 

(explanatory) 

Suppose 
are  consistent 
as  a  decision  criterion.  However, 
because 
to  be  {st  , ~2, ~3, ~4, $5).  However  under 
preferred. 

We  just  saw  that  a  unique  minimal  belief  assignment  does  not  always  exist.  However, 
problems  with  certain  decision  criteria,  a  unique  minimal  belief 

and  a  unique  minimal 

explanatory  belief  assignment 

exist. 

for  belief  assignment 
assignment 

Definition  33.  Let  (u,  v’),  (x,  x’) , and  (w,  w’)  be  three  pairs  of  equal 
length  vectors 
is  closed  under  unions  if  u o  w o x  is  at  least  as  preferred 
of  reals.  A  decision  criterion 
as  u’ o  w’ o X’ whenever  u  o w  is  at  least  as  preferred  as  U’ o  w’  and  w o x  is  at  least  as 
preferred 

.as w’ o x’. 

When  we  substitute 

the  empty  vector  for  w  and  w’  in  this  definition,  we  obtain 
satisfies 

the 
this 
sure-thing  principle.  Thus,  a  decision  criterion 
principle.  An  intuitive 
that  p  prefers 
reading  of  this  property 
action  a  over  a’  when  the  plausible  worlds  are  x,  y,  z  and  it  also  prefers  a  over  a’  when 
it  would  prefer  a  over  a’ 
the  plausible  worlds  are  u, w, x.  If  p  is  closed  under  unions, 
also  when  the  plausible  worlds  are  U, w, x,  y,  z. 
of  insufficient 

that  is  closed  under  unions 
is  the  following: 

The  principle 

suppose 

reason 

unique  mmimal  belief  assignment 
closed  under  unions,  as  are  a  number  of  other  criteria  discussed 

in  Example  32.  However, 

is  not  closed  under  unions,  hence 
the  maximin 
in  [ 411. 

the  lack  of 
is 
criterion 

Theorem  34.  Given  a  belief  ascription  problem  with  a  decision  criterion 
under  unions, 
consistent  belief  assignment. 

that  is  closed 
exists  then  there  is  a  unique  minimal 

if  a  consistent  belief  assignment 

In  addition, 
and  consistent. 

the  above 

theorem  holds  when  we  replace  consistent  with  explanatory 

5.2.  Choosing  among  admissible  beliefs 

In  modeling  dynamic  agents,  we  prefer  to  use  admissible  belief  assignments 

in  which 

the  agent’s  beliefs  change  coherently  over  time.  However,  for  such  beliefs, 
bias  makes 
possible 
other  points. 

little  sense  without  some  additional  modifications.  As  we  now  show,  it  is 
to  have  less  general  beliefs  at  some  times  while  having  more  general  beliefs  at 

the  minimality 

Example  35.  Consider 
local  state  It :  a,  b,  c,  d.  After  the  first  action 
13 with  worlds  a,  b  possible 

the  following  case:  there  are  four  possible 

there  are  two  possible 

initial  worlds  in  the 
local  states  12 and 

in  12, and  worlds  c,  d  possible 

in  13. 

250 

R.I.  Brafmn,  M.  Tennenholtz/ArtQicial 

intelligence  94  (1997)  217-268 

11 

Assume 

that  in  11 we  can  consistently 

ascribe  beliefs 

ascribe  beliefs 

in  {a,  b},  and  in  Zs we  can  consistently 

in  {a,  b}  or  {a,  b, c},  in  12 we 
ascribe  belief 
admissible  belief  assignments:  Bi 
to  Ii,  {a,  b}  to  Z2 and  {c}  to  1s while  B2  assigns  {a,  b}  to  21, {a,  6)  to 
than  the  other.  Note  that  Bi  U  B2  is 

there  are  two  consistent 

is  more  general 

can  consistently 
in  {c}  or  {c,  d}.  Therefore, 
assigns  {a,  b,c} 
12 and  {c,  d}  to  Zs, none  of  which 
not  admissible. 

Hence, 

to  model 

at  earlier  points  of  time.  That  is,  we  prefer 

initial  belief  assignment,  making  as  few  initial  assumptions 

in  the  case  of  dynamic  agents  with  admissible  beliefs,  we  will  have  to  come 
is  to  prefer 
the  agent  as  setting 
about 
in  revi- 

up  with  a  weaker  notion  of  generality.  What  seems  to  us  most  appropriate 
generality 
out  with  a  minimal 
the  world.  This  also  implies 
sion  states  we  would  also  prefer  models 
Described 
missible, 
that  are  “thicker” 
preferences 
structure  are  often  described  as  “most  normal”,  and  structures 
bottom  are  often  preferred  because 

in  terms  of  the  static  ranking  associated  with  agents  whose  beliefs  are  ad- 
into  a  preference  of  rankings 
logics  such 
in  a  ranking 
that  are  “thicker”  at  the 
(see, 

in  the  context  of  non-monotonic 
logics,  worlds  minimal 

that  it  will  have  fewer  revision  states.  In  addition, 

they  make  fewer  assumptions  of  non-normality 

this  type  of  preference  over  models 

the  agent  makes  fewer  assumptions. 

are  well  known. 

In  non-monotonic 

at  the  bottom. 

in  which 

translates 

Indeed, 

e.g.,  L371). 

Definition  36.  An  admissible  belief  assignment  BI  is  more  general 
sented  as  ranking 
(where  By  is  the  set  of  states  in  the  mth  rank  of  BI). 

functions,  Br  and  Bi  are  identical  up  to  some  rank  m,  and  By  >  Bi”’ 

than  Bi  if,  repre- 

We  will  refer  to  this  definition  of  more  general 

in  the  context  of  admissible 

beliefs 

and  in  the  resulting  definition  of  minimal  belief  assignments. 

then  the  set  of  consistent  admissible  belief  assignments, 

if  non-empty, 

if  the  decision  criterion 

is  closed  under 
contains 

Theorem  37.  For  agents  with  perfect  recall, 
unions 
a  unique  minimal  belief  assignment. 

Again, 
consistent. 

the  above 

theorem  holds  when  we  replace  consistent  with  explanatory  and 

6.  Existence 

Given  any  approach 

to  modeling 

agents 

(or  other  entities 

the  most 

important 

questions 

is  under  which  conditions 

for  that  matter),  one  of 
is  adequate.  That 

this  model 

R.I.  Brawn,  M.  TennenhoWArttj5cial  Intelligence  94  (1997)  217-268 

251 

is,  what  k:ind  of  behaviors 
we  decide 
to  adopt 
agent’s  behavior.  We  are  finally 
tion.  2o 

the  model,  what 

implicit 
in  a  position 

can  we  model  using 

if 
this  approach.  Or  alternatively, 
about 
the 
are  we  making 
to  this  ques- 

to  provide  a  partial  answer 

assumptions 

similar 

and  a  value  function 

We  approach 

this 

task 

of  subject:ive  probability 
assignment 
we  will  describe 
behavior  under  which  a  mental-level  model  with  a  unique  minimal 
assignment 
ascribed. 

to  Savage’s  work  on  the  foundations 
in  a  manner 
[56].  Savage’s  approach  allows  us  to  ascribe  a  probability 
to  an  agent  based  on  its  choice  among  actions.  Similarly, 
the  agent’s 
belief 
can  be 

and  a  number  of  restrictions 

and  a  unique  minimal 

a  class  of  situations 

belief  assignment 

explanatory 

admissible 

admissible 

on 

ibis  observation 

First,  notice  that  we  can  trivially  ascribe  a  consistent  mental-level  model  to  any  agent. 
We  simply  make  all  runs  have  the  same  value  and  choose  any  decision  criterion  and  any 
beliefs. 
such  as, 
can  we  solve  a  belief  ascription  problem  or  when  can  we  ascribe 
under  what  conditions 
an  explanatory  model.  We  will  now  characterize 
a  class  of  agents  for  which  beliefs  can 
be  ascribed 

shows  that  we  should  ask  more  constrained  questions, 

signals 

Goal-seeking 

agents  are  agents  with  perfect  recall  whose  local  states  are  of  two  types: 
states.  These  agents  have  a  special  action,  called  HALT,  which 
the  end  of  a  run  and  must  eventually  be  performed  at  each  run.  The 
it  is  1  if  HALT 
agents  are  quite  natural  from 
state 

goal  states  and  non-goal 
intuitively 
value  of  a  run  is  determined  by  the  state  in  which  HALT  is  performed: 
in  a  goal  state  and  0  otherwise.  Goal-seeking 
is  performed 
the  AI  perspective, 
they  describe  agents 
of  the  world. 

that  act  to  bring  about  a  particular 

since 

the  agent  does  not  perform  actions  unless 

The  protocols  of  goal-seeking 

agents  satisfy 

notion  of  goal-seeking 

that  to  halt 

it  attain  a  good  state-its 

the  agent  must  halt  whenever 

body  a  minimal 
that 
reach  a  goal  state.  Thus, 
how  help 
tulate  says 
show  a  possible  world  under  which 
does  not  give  up  without 
would 
require 
matter  how  unlikely 
lates  refer 
later). 

to  the  set  PW,,,(  Z)  rather 

reason, 
it  to  stop  acting  only  when 

and 

the  prospect  of  reaching 

it  is  in  a  goal  state  or  when 

rational 

behavior.  The 

it  is  impossible 

effort  postulate 

two  weak  rationality  postulates 

that  em- 
says 
to 
they  can  some- 
efforts  are  rational.  The  rational  despair  pos- 
to 
the  agent 
(A  stronger  postulate 
that  goal,  no 
that  these  postu- 
possible  circularity 

it  can  never 
its  despair 

reach 
is  rational. 

the  goal.  Thus, 

it  is  impossible 

is.)  Notice 

(preventing 

to  B,,,(I) 

to  reach 

the  goal 

than 

the  agent  must  either  be  in  the  goal,  or  it  must  be  able 

Rational  Effort  Postulate.  The  protocol  either  assigns  HALT 
assigns  an  action 

that  weakly  dominates  HALT.  21 

to  a  local  state  I,  or  it 

Hence,  unless 

there  is  something  better  to  do,  HALT  is  assigned. 

results  of  this  nature  appear 

*’ Additional 
*’ Given  a  tuple  u  of  length  k,  let  u(i)  be  its  ith  element.  We  say  that  u’  weakly dominates  u  if  for  every 
1  <  i  <  k,  it  is  the  case  that  u’(i)  2  u(i)  and  for  some  1  <  i  <  k,  u’(i)  >  u(i). 

in  [ 12,131. 

252 

R.I. Brafman, M. Tennenholrz/Artifcial Intelligence 94 (1997) 217-268 

Rational  Despair  Postulate.  The  protocol 
there  is  no  protocol 
for  some  s  E  PW,,,(Z) 

in  a  non-goal 
that  achieves 

local  state  1 is  HALT  only 
the  goal. 

if 

Hence,  HALT 

is  assigned 

in  a  non-goal 

state  only 

if  it  is  possible 

that  the  goal 

is 

unachievable. 

Finally,  goal-seeking 

agents  use  weak  dominance 
strictly  prefer  w  over  u  iff  w  weakly  dominates  U. 

as  their  decision  criterion, 

i.e.,  they 

using  a  knowledge-based 

An  example  of  a  goal-seeking 

condition 

state,  some  motion  command 
and  some  termination 
not  programmed 
planner  or  an  ordinary  planner. 
requirements 
when 
connected 
condition 
with  most  reasonable  motion 
we  can  ascribe  beliefs 

on  the  robot’s 

(telling 

(telling 

In  that  case,  the  two  postulates 

paradigm  but  rather, 

agent  could  be  an  ordinary  mobile 
it  in  which  direction 
it  when  to  HALT).  Typically, 

robot  with  a  goal 
to  move  at  each  state), 
such  robots  are 
they  employ  a  motion 
into  two  weak 
condition.  For  instance,  a  robot  that  stops  only 
space  that  is  not 
the  two  postulates.  The 
as  its  decision  criterion  would  be  consistent 
theorem  shows, 

In  that  case,  as  the  following 

the  goal,  will  satisfy 

translate 

termination 

to  the  component 
that  it  employs  weak  dominance 

containing 

it  is  in  the  goal  or  if  it  is  in  a  component  of  its  configuration 

strategies. 
to  such  a  robot. 

If  A  is  a  goal-seeking 

it  can  be  ascribed  a  unique  mini- 
agent 
Theorem  38. 
mal  admissible  belief  assignment  and  a  unique  minimal  explanatory  admissible  belief 
assignment. 

then 

If  we  allow  a  decision  criterion 

that  is  stronger 

with  it),  we  may  have  to  drop  uniqueness 

than  weak  dominance 
from  the  statement  of  this  theorem. 

(but  consistent 

We  can  use  this  result  to  show  that  agents  with  perfect  recall  and  a  HALT  action  can 

the  actual  protocol 

be  ascribed  a  mental-level  model  with  a  fully  explanatory 
i.e.,  in  which 
to  show 
goal-seeking 
runs 
value  0. 

agents.  We  do  so  by  ascribing 

is  strictly  preferred  over  all  other  protocols.  We  need 
the  criteria  of 
in  which  all 
its  protocol  have  value  1,  while  all  other  runs  have 

to  these  agents  goals  so  that  they  satisfy 

to  the  agent  a  value  function 

admissible  belief  assignment, 

that  can  be  obtained  using 

that  we  can  attach 

rational 

choice  as  equivalent 

to  expected  utility  maximization 
distribution.  We  show  that  in  O/l  value  contexts  any  behavior 
can  be 
in  our  framework.  Let  us  define  a  B-type  agent  as  one  whose  beliefs 

Many  people  view 
under  some  probability 
consistent  with  expected  utility  maximization 
attributed  belief 
are  represented  by  a  probability 
expected  utility 
value  function, 
principle.  We  require  only  that  the  agent  perform  HALT  when  no  action  has  an  expected 
value  greater 

assignment,  whose  preferences  are  represented  by  a  O/ 1 

under  some  probability  distribution 

and  whose  decision  criterion 

is  based  on  the  maximum 

than  0. 

Corollary  39. 
as  a  plausible  outcome  maximizer 
decision  criterion  consistent  with  weak  dominance. 

If  an  agent  can  be  modeled  as  a  B-type  agent, 

it  can  also  be  modeled 
that  uses  some  admissible  belief  assignment  and  a 

R.I.  Brafinan, M.  Tennenholtz/Artijicial  Intelligence 94 (1997) 217-268 

253 

7.  Related1 work  in  economics  and  game  theory 

There 

is  much  work  within  AI  that  is  relevant 

to  the  framework  presented 

in  this 
the  most  closely 
theory,  and  decision- 
to  AI  research  has  been  pointed  out  by  many  researchers, 

in  the  next  section.  However, 

game 

lines  of  research  can  be  found  within  economics, 

paper,  and  this  work  will  be  discussed 
related 
theory,  areas  whose  relevance 
most  notably, 
directly 
on  qualitative  decision  making,  and  work  on  revealed  preference. 
briefly  describe 

[ 18,191). 
to  our  work:  work  on  subjective  probability 

these  fields  and  compare 

In  particular, 

Jon  Doyle 

relevant 

(e.g., 

three 

them  with  our  own  effort. 

and  choice 

topics  of  research  are 
theory,  work 
In  what  follows,  we 

7.1.  Subjective  probability  and  choice  theory 

analysis, 

and  game-theoreticians 

A  key  issue  for  economists 

is  how  to  model  agent  behavior.  An  agent  model  must  be  descriptively 

state  consisting 

to  mathematical 

should  be  amenable 

like  our  model.  However, 

i.e.,  it  must  correctly  predict  human  behavior 

systems 
curate, 
the  model 
practice.  One  of  the  most  popular  approaches  has  been 
entities  with  a  mental 
rion,  much 
this  paper,  beliefs 
in 
modeled  using  a  utility 
mization.  While 
is  mathematically 
analysis 
it  allows  us  to  represent 
tudes. 

appealing, 
tools  and  because  probability 

this  model  does  not  necessarily 

are  modeled 
function, 

partly  because 

in  economic 

to  model  economic 

so  that  it  can  be  used 

in  building  models  of  economic 
ac- 
contexts.  Moreover, 
in 
agents  as 
crite- 
than  use  the  qualitative  models  described 
are 
is  expected  utility  maxi- 
it 
advantage, 
offers  powerful 
importantly, 

is  well  developed.  More 

continuous  mathematics 

offer  a  computational 

and  a  decision 

distribution, 

using  a  probability 
and  the  decision  criterion 

finer  degrees  of  belief  and  preference 

of  beliefs,  preferences, 

as  well  as  risk  atti- 

preferences 

theory 

rather 

is  quite  elegant, 

it  is  by  no  means  clear 
But  while  the  quantitative  probabilistic  model 
that  it  is  an  adequate  model  of  human  behavior. 
In  particular,  most  people  do  not  feel 
they  perform  expected  utility  calculations  when  making  different  choices.  But  here  lies 
[ 321  is  founded: 
an  important  conceptual 
whether  or  not  the  agent  actually  makes 
no  consequence.  The  issue  is  whether  or  not  a  probabilistic  model  employed  externally 
lead  to  accurate  predictions 
has  the  re,quired  predictive  power.  That  is,  will  this  model 
of  observables.  This  idea  is  central 
to  our  modeling  approach  as  well,  and  it  is  the  basis 
of  Newell’s  concept  of  the  knowledge 

idea  upon  which  the  theory  of  modeling  choice 
its  decision  using  probabilistic 

reasoning 

level. 

is  of 

These  considerations 

lead  to  the  emphasis  placed  within  game 

theorems.  These 
the  model 
is,  in  principle,  observable, 

theorems 
is  applicable.  Because 

the  assumptions 
they  can  be  empirically 

are  on  the 
tested  and 

theory  on  existence, 
tell  the  modeler  under  what  assumptions 

or  representation 
on  the  agent’s  behavior 
agent’s  behavior  which 
(in)  validated. 

The  work  of  Savage 

[56]  provides  what  many  consider 

representation. 

Savage  provides  a  set  of  assumptions 

in  choice 

result 
approach 
utility  maximizer.  Again, 

to  action  choice  under  which 

this  does  not  mean 

to  be  the  most 

important 
about  the  agent’s 
it  can  be  modeled  as  if  it  were  an  expected 
that  the  agent  performs  expected  utility 

254 

R.I.  Brafinan,  M.  TennenholtzYArtiJicial 

Intelligence  94  (1997)  217-268 

calculations 
his/her  behavior.  One  of  Savage’s  famous  conditions 

in  his/her  head,  but  that  such  a  model  would  make  correct  predictions  of 

is  the  sure-thing  principle. 

adequacy 

their  descriptive 

by  psychologists 

to  be  quite  intuitive 

find  Savage’s  assumptions 

is  not  clear  and  considerable 

Many  people 
of  view.  However, 
been  expanded 
the  relevance  of  these.  studies 
artificial  agents 
representation 
on  the  manner 
of  beliefs 
details). 

from  a  normative  point 
effort  has 
(421).  However, 
adequacy  of  probabilistic  modeling  of 
is  not  clear.  In  order  to  obtain  better  descriptive  models,  various  weaker 
theorems  have  been  proposed.  These  theorems  make  weaker  assumptions 
representations 
[21] 

their  actions  and  use  weaker 
(see 

in  their  agents  models,  e.g.,  non-additive 

in  testing 
to  the  descriptive 

in  which  agents  choose 

their  validity 

probabilities 

(see  e.g., 

for  more 

their  application 

Savage’s  seminal 

result,  as  well  as  many  following  works,  make  two  strong  require- 
in  our  setting  quite  difficult:  One  must  supply  a  total 
to 
to  an  observer  of  the 

that  render 
ments 
pre-order  on  all  functions 
no  existing 
system.  Moreover, 
there  exists  a  partition  of  the  set  of  states  into  n  subsets,  all  of  which  are  equally 
Thus, 

from  initial  states  to  outcomes,  many  of  which  correspond 
will  not  be  available 

they  require  a  rich  state  description,  where  for  any  natural  number  12, 

the  number  of  states  must  be  infinite. 

real-world  action; 

this  info~ation 

likely. 

states 

of  mental 

to  compare 

it  can  provide 

It  is  still  early 

the  well  developed 

in  agent  models.  This 

theory  of  choice  with  our  approach, 
deals  with  discrete 
for  the 
foundations 
given 
consideration 
of  work  on  discrete  notions  of  belief  and  the  prevalence  of  qualitative 
tools  within  AI.  Moreover,  by  varying 

and  here  we  simply  note  a  number  of  differences.  Our  fo~alism 
semantic 
descriptions 
state.  Therefore, 
is  an  important 
use  of  these  mental 
the  abundance 
representation 
the  decision  criterion  we  may  be 
able  to  cover  different  classes  of  agents.  For  example,  one  type  of  decision  criteria  we  are 
resources. 
currently 
actual  agents.  Finally,  our  approach 
Such  criteria  may  be  better  suited 
does  require  substantial 
it 
background 
to  specify 
is  discrete  and  qualitative, 
for  model 
a  quantitative 
choice,  suggested 

than  that  needed 
In  addition,  we  hope  that  the  heuristics 

to  be  applicable.  However,  because 

for  modeling 
information 

in  Section  5,  can  lessen 

limited  computational 

probabilistic  model. 

it  should  require 

less  info~ation 

into  account 

this  burden. 

the  agent’s 

into  takes 

looking 

7.2.  ~~aZita?ive decision  theory 

theorists 

concerned  with 

While  Savage’s  work  is  of  great  importance 

to  decision 
decisions.  While 
there  is  much 
and  consequent  progress 
the  last  few  decades  have  seen  very  little  work  in  qualitative  decision 

it  is  of  no  lesser  value 
the  question:  how  should  one  make  one’s  own 
there  are  doubts  as  to  the  descriptive  adequacy  of  Savage’s  postulates, 
appeal.  22  Because  of  this  appeal 
statistics, 

in  the  disciplines  of  decision  analysis  and  Bayesian 
theory. 

about  their  vocative 

to  game-theorists, 

less  disa~~ment 

Work  in  qualitative 

decision 

question:  how  should  one  make  decisions  when  one  has  virtually  no  information 
the  likeliho~ 

theory  has  mostly  been  concerned  with  the  following 
about 
of  the  desirability 

states  of  the  world  but  a  good  ~sessment 

of  different 

?J Though 

there  is  no  consensus  on  this  matter.  See  the  articles 

in  [23]. 

R.I.  Brafrnan,  M.  TennenhoWArtQicial  Intelligence  94  (1997)  217-268 

255 

referred 
that  were  mentioned 

is  often 
of  different  outcomes.  This 
ignorance.  All  of  the  decision  criteria 
minmax  regret  and  the  principle  of  indifference,  have  been  studied 
However,  until  recently, 
for  these  qualitative  decision  models. 
the  maximin  and  minmax  regret  criteria.  Using 
should  be  viewed  as  existence 
criteria. 

to  as  decision  making  under  complete 
in  this  paper,  i.e.,  maximin, 
[41]. 
in  this  context 
to  Savage’s 
for 
the  language  of  this  paper,  these  results 
these  decision 

there  were  no  representation 

for  static  agents  employing 

In  [ 12,131,  we  present 

the  first  such  results 

theorems  analogous 

theorems 

Our  current  work  has  been  motivated  by  the  models  used  in  decision  making  under 
information.  Rather 
that  all  states  of  the  world  are  possible,  as  in  the  above  works  on  qualitative 
a  qualitative  notion  of  belief 

ignorance.  However  we  modified 
than  assume 
decision  making,  we  incorporated 
discriminate 

between  more  likely  and  less  likely  worlds. 

that  allows  the  agent  to 

to  capture  qualitative 

these  models 

7.3.  Revealed  preference 

Besides 

the  technical  differences, 

theory  and  our  work.  Both  approaches  desire  sound 

choice 
models  of  agents.  However,  choice 
utilities 
That  is,  th(ey  do  not  have  the  engineering  motivation  of  monitoring 
person 
order  to  explain  his/her  behavior  and  predict  his/her 

there  is  an  important,  but  subtle  difference  between 
for  abstract 
and 
theories. 
actual 
in 

the  behavior  of  generic  human  agents  in  general  economic 

a  model  of  this  particular  person 

the  goal  of  constructing 

theorists  wish  to  justify 

the  use  of  probabilities 

(or  agent)  with 

future  behavior. 

in  modeling 

a  particular, 

foundations 

theory 

is  to  predict 

can  be  of  considerable 

to  our  approach,  motivate 

Such  concerns,  which  are  central 

habits  of  an  agent,  we  may  be  able 

the  work  on  revealed 
in  economics.  The  goal  of  revealed  preference 
future 
of  single  agents  and  classes  of  agents.  For  example,  based  on  previous 
its  future  habits.  Such 
idea 
agents.  The  basic 
among  various 
is  that  of  choosing  a  good 
the  quantity  of  various  goods) 
income  y,  such  that  x  is  the  best  choice 
x  . p  =  y.  If  we  observe  a  choice  x  made  by  a  consumer  under 
that  this  consumer  prefers  x  over  all  other  x’  that  satisfy 

preference 
preferences 
consumption 
informaticln 
is  that  an  agent’s  choices 
options.  For  example, 
bundle  x  ( which  can  be  thought  of  as  a  vector  specifying 
given  a  certain  price  vector  p  and  available 
under 
given  y  and  p,  we  can  deduce 
the  given  constraint. 

reveal  his/her  preference 
in  economics 

in  various  settings 
the  consumer  problem 

to  predict 
to  many  economic 

the  constraint 

value 

(xi,  yi,pi) 

that  preferences 

the  assumptions 

a  set  of  ob- 
Under 
servations 
the 
consumer’s  general  preference  ordering  over  bundles.  For  example,  one  property,  called 
the  general 
that  preferences  are  acyclic.  We  refer 
the  reader 

revealed  preference  principle  stipulates 

together  with  basic  assumptions 

remain  stable  we  can  combine 

on  preferences 

to  “reveal” 

to  [ 331  for  more  details. 
The  aims  of  revealed  preference 
is  made 

overlap. 
an  agent  model  with  predictive  power. 
In  both  cases,  an  attempt 
is  that 
In  both  areas,  the  question  of  preference  persistence 
revealed  preference 
in  terms 
of  a  more  basic  agent  model.  Thus,  the  types  of  predictions  made  by  revealed  preference 

theory  and  of  our  work  have  considerable 
to  construct 

arises.  The  main  difference 
the  observed  preferences 

theory  does  not  attempt  to  explain 

256 

R.I.  Brafman,  M.  %menhol!z/Artificial 

intelligence  94  (1997)  217-268 

like  the  following:  First,  an  agent 

theory  are  somewhat 
from  its  left  side,  rather 
right,  rather 
agent  would  prefer  passing  an  object  from  the  left  side  rather  than  passing 
Such  deductions 

is  observed  passing  an  object 
than  its  right  side.  Next,  it  is  seen  passing  an  object  from  the 
the 
it  from  above. 

are  important,  but  their  scope  is  narrower. 

than  going  over  it.  Consequently, 

that,  when  possible, 

it  is  concluded 

8.  Related  work  in  AI 

The  understanding 

of  mental 

states  has  steadily  progressed 

various  researchers,  and  we  have  greatly  benefited 
to  discuss  some  of  the  more  relevant  work. 

from  many  existing 

Structure 

through 

the  effort  of 
ideas.  We  proceed 

. 

rather 

beliefs, 

than  agent  modeling, 

they  are  clearly  relevant 

their  aim  has  been  to  supply 

A  large  portion  of  the  AI  literature  on  formalizing  mental  states  deals  with  distinct 
and  their  dynamics.  However,  a  number 

suggested  more  complete  models  of  mental  state,  e.g.,  [ 6,49,52,55,57] 
intuitive  and  well  founded 

mental  attributes  such  as  belief  and  knowledge, 
of  researchers 
While 
tion  and  design, 
what  structure  our  model  should  have.  Rao  and  Georgeff 
uses  three  mental  components, 
abstract  agent  architecture 
bling 
notions  such  as  knowledge  and  goals.  Shoham 
ming 
lack  is  the  notion  of  a  decision  criterion,  which  embodies 
choice  under  uncertainty.  While 
the  need  for  deliberation 
caped  the  attention  of  AI  researchers 
common-sense 
porate  expected  payoff  calculations 
have  only  recently 

tools  for  agent  specifica- 
to  the  question  of 
that 
and  desires.  Pollack  et  al.  propose  an 
and  Kael- 
that  is  specified  using 
[ 571  presents  an  agent  oriented  program- 
these  structures 
to  action 
has  not  es- 
some  type  of 
[ 5 1 ]  incor- 
criteria 

the  agent’s  approach 
under  uncertainty 

language  based  on  the  notions  of  belief  and  commitment.  What 

shown  up  in  the  AI  literature  on  mental  states  [6,  lo]. 

based  on  similar  mental  states 

[55]  developed  an  interpreter 

goals,  and  Rao  and  Georgeff 

that  can  implement  behavior 

[ 521  define  an  interpreter 

for  decision  making), 

qualitative  decision 

[ 491.  Rosenschein 

[ 601  incorporates 

about  conflicting 

(e.g.,  Thomason 

deliberation 

intentions 

In  contrast,  we  do  not  include 

intentions 

resource  bounded  agents:  much 
intentions 

role  in  modeling 

important 
to  ignore  certain  possible  outcomes  of  its  actions, 
possible 
Nevertheless, 
of  any  agent  acting 
for  choice  under  uncertainty 

in  the  world:  perceptions 
(decision 

the  current  model  contains 

it  may  be  desirable 

actions.  Thus, 

criterion). 

in  our  model.  It  seems  that  intentions  play  an 
like  beliefs  allow  the  agent 
it  to  ignore  certain 
into  future  models. 
the  three  essential  aspects  of  the  mental  state 
and  a  method 

to  integrate 

intentions 

(beliefs), 

(values), 

allow 

goals 

Grounding 

A  number  of  important  works  ground  single  mental  attributes, 

they  show  us  how  to  model  a  particular 

agent,  and  when 

these  attributes  are  useful 

knowledge.  Methods  of  grounding 
when 
whether  an  agent  is  implementing 

a  particular  mental-level 

specification. 

in  our  modeling 

specifically,  belief  or 
context 
they  are  able  to  say 

R.I.  Brafman,  M.  knenholtz/Artijicial 

Intelligence  94  (1997)  217-268 

251 

Halpern  and  Moses 

[ 261  and  Rosenschein 

[ 541  ground 

the  notion  of  knowledge 

in 

between 

the  relationship 
We  discussed 
state  in  a  computer 
lead,  but  with  a  more  comprehensive 
agent’s  krmwledge  does  not  tell  us  about  this  agent’s  actual  behavior. 

the  local  state  of  a  machine  and  the  state  of  its  environment. 
their  work  in  Section  2.  This  research  was  the  first  to  ground  a  mental 
science  context,  and  we  were  motivated  by  the  desire  to  follow  their 
of  an  agent’s  state.  A  model  of  an 

description 

can  be  viewed  as  summarizing 

the: notion  of  belief  in  statistical 

Other  works  have  proposed  groundings 

in  the  end  of  Section  2.3.  Bacchus,  Grove,  Halpern,  and  Koller 

for  beliefs.  We  presented  our  view  of  the 
[4] 
semantics  of  beliefs 
information.  Their  work  answers  questions  such 
ground 
as:  what  should  we  believe  about  the  bird  Tweety  given  that  90%  of  birds  fly.  Statistical 
concrete  observation  of  the  world,  therefore, 
information 
they  pose  is  normative,  not 
it  is  grounded.  However,  one  should  notice  that  the  question 
act,  they 
descriptive.  While  statistics  can  help  us  form  beliefs  about  how  thermostats 
to  the  question  of  what  beliefs  we  should  ascribe 
do  not  provide  meaningful 
those  ideas  to  ascribe 
the  thermostat.  Even 
its  beliefs 
in 
accordance 

requires  knowing  what  statistical 
to  the  ideas  of  Bacchus  et  al. 

in  the  case  of  an  “intelligent” 

it  has,  and  that  it  is  acting 

agent,  using 

information 

answers 

Goldszmidt 

and  Pearl’s  e-semantics 

[ 1,241  views  beliefs  as  qualitative 

representations 

that  birds  fly  if  one  holds  that  Pr(FlylBird)  M  1.  This  ap- 
of  probabilities:  One  believes 
proach 
can  be  given  semantics 
close  to  ours,  since  subjective  probability 
is  semantically 
in  terms  of  the  agent’s  choice  of  actions.  However,  it  would  provide  a roundabout  manner 
beliefs  and  then,  we  discretize 
of  modeling 
agents: 
in  ascribing  probabilistic 
these  beliefs.  As  we  have  noted, 
beliefs 
requires 
a  sequence  of  probability 
assignment.  The 
semantics  of  this  sequence 

assignments 
is  not  clear. 

rather  than  a  single  probability 

process  used  by  e-semantics 

there  are  some  difficulties 

to  .agents,  and  moreover, 

them  probabilistic 

first,  we  ascribe 

the  discretization 

for  belief 

is  supplied  by  Levesque’s  work  on  making 
[ 38,401.  Levesque  provides  a functional  view  of  knowledge 
are  performed: 

types  on  which 

two  operations 

Another  concrete 

interpretation 

treating 

them  as  abstract  data 

“believers”  out  of  computers 
bases, 
TELL  and  ASK.  TELL  adds  new 
query  the  database.  Levesque  examines 
performed.  One  is  a  first-order 
well.  Levesque 
expressive 

language,  can  be  implemented 

information 

two  languages 

to  the  database, 

to 
and  ASK 
in  which  these  operations  can  be 
language,  and  the  other  contains  knowledge  operators  as 
relying  on  the  more 

is  used 

shows  that  the  stronger  TELL  and  ASK  operations, 

using  a  first-order 

language  only. 

functional 

similarities 

they  make 

of  these  structures 

between  Levesque’s 

There  are  interesting 

implementation 
responses 

view  of  knowledge 
bases  and  our  view  of  agents.  Levesque’s  abstract  definition  of  knowledge  bases  ignores 
in  term  of  their 
the  underlying 
to  our  functional 
behavior--the 
view  of  agents  emphasizing 
restricted  agents  having 
possible 
mechanical, 
although 
responses.  Our  work  examines 
tation  result, 

their  behavior:  Levesque’s  knowledge  bases  can  be  viewed  as 
to  their 
(computer, 
their  environment, 

than  query 
represen- 
though  somewhat  different,  provides  conditions  under  which  it  is  possible 

NO,  UNKNOWN-corresponding 
the  goals  of  most  situated  systems 

that  is  definitely  useful,  and  their  repertoire  of  actions 

this  more  general  class  of  agents.  Levesque’s 

them 
is  quite  similar 

involve  more  than  correctly 

to  queries.  However, 

three  actions-YES, 

to  queries.  This 

or  biological) 

and  defines 

representing 

is  different 

responses 

258 

R.I.  Brafman,  M.  Tennenholtz/ArtQicial  Intelligence  94  (1997)  217-268 

to  model  an  abstract  knowledge  base  using  first-order 
when  it  is  possible 

to  model  an  agent  using  beliefs,  preferences 

Levesque’s 

functional 

approach  enables  knowledge 
structures 

details 

that  are  irrelevant 

different  knowledge-representation 
plementation 
view  has  proven  extremely 
sentation 
we  hope  that  the  more  general  abstraction 
general  study  of  agents. 

[ 391.  Given 

fruitful 

representation 

logic.  Our  existence 

results  show 
and  a  decision  criterion. 
to  treat 
in  a  uniform  manner,  abstracting  away  im- 
to  their  query  answering  behavior.  This  abstract 
repre- 
in  the  more  limited  domain, 
given  here  will  serve  the  same  role  in  the 

issues  in  knowledge 

researchers 

central 

for  understanding 

the  success  of  Levesque’s  approach 

Plan  recognition 

one 

tries 

In  plan 

to  infer 

recognition 

the  prediction 

[3,14,27,30,48] 
with  them  or  observing 
task  we  discussed 

by  communicating 
resembles 
as  giving  a  semantic 
human  and  often 
a  general  semantic 
devices.  Grounding 
explicit  mental  state.  The  issue  of  existence 
circumscription 

the  plans  of  other  agents 
task  closely 
their  behavior.  This  modeling 
in  Section  4,  and  the  latter  can  be  viewed 
is 
to  provide 
theory  of  mental-level  models  as  abstract  description  of  agents  and 
have  an 
is  not  a  central 
is  not  dealt  with  either,  but  Kautz’s  use  of 

[30]  can  be  viewed  as  addressing 

its  acts  are  speech  acts.  Hence, 

issue,  since  human  agents 

this  field  does  not  attempt 

the  issue  of  model  choice. 

recognition.  Most  often, 

the  modeled  agent 

account  of  plan 

(presumably) 

to  our  work.  Pollack  explicitly 

they  together  with  its  goals  determine 

agent’s  beliefs  should  be  taken  into  account 

Given  these  general  differences,  among  the  work  on  plan  recognition  we  find  Pollack’s 
treats  plans  as  complex 
involving  beliefs  and  intentions  of  agents  with  goals.  One  of  her  points 
its  plans, 
of  our  use 
intentions  which  are 
than  ours,  and 
In  addition  Pollack  does  not 
considers  only  O/ 1  value 
that  achieve  the  goal  given  their  beliefs. 
in  our  case.)  Also,  Pollack  does  not  deal 

work  [ 481  to  be  the  most  relevant 
mental  attitudes 
is  that  the  planning 
since 
of  plausible  outcomes. 
missing 
she  presents  various 
deal  with  the  issue  of  choice  under  uncertainty.  She  implicitly 
functions 
(This 
with  the  issue  of  belief  change  explicitly. 

from  our  model.  However,  Pollack’s 
syntactic 

its  actions.  This  is  reminiscent 
incorporates 

and  her  agents  always  choose  actions 

In  addition,  Pollack’s  model 

is  more  syntactic 

the  maximin 

in  deducing 

inference. 

treatment 

for  plan 

to  using 

is  akin 

criteria 

rules 

Machine 

learning 

One  driving 
in  multi-agent 
predictions. 
in  a  manner 
instances 
brings  a  special  bias 
agency-hypothesis:  Machines 
with  a  purpose 
be  modeled 
but  we  believe 
model  existence 
better. 

accordingly. 

force  behind  our  research 
systems. 

In  machine 
For  example,  decision 

that  enables  predicting 
to  the  task  of  predicting 

[50]  provide 

a  way  of  modeling 

learning,  models  are  also  constructed 
trees 

is  the  desire  to  use  agent  models  for  prediction 
to  help  make 
observed 
instances.  Our  work 
in  the  form  of  the 
they  are  usually  designed 
they  should 
is  justified, 

the  classes  of  future 
agents’  behavior, 

assumptions; 
question  whether 

therefore, 
this  bias 

are  agents  of  their  designers; 

in  mind  and  with  some  underlying 

It  is  an  experimental 

that  it  is  quite  sensible.  One  of  the  reasons  we  consider 

to  be  important 

is  because  by  answering 

it  we  can  understand 

the  question  of 
this  bias 

R.I.  Brajinan,  M.  Tennenholtz/Artijicial  Intelligence  94  (1997)  217-268 

259 

9.  Discussion 

In  this  paper  we  formalized 

a  method 

informal, 

to  pursue 

advocated 

the  knowledge 

for  representing 

level  [45].  However, 

the  tone  of  both  papers 

In  fact,  in  [ 461,  Newell 

it.  Newell  also  advocated 

this  idea  in  [43]  where  he  motivated 

the  need  for  an  abstract  level  of  representation 

and  motivational. 
this  approach  within 

the  state  of  an  agent  using 
this  ap- 
this 
of  programs  and  machines, 
is,  in  gen- 
the  lack  of 
this  to 
treatment  of  these  ideas.  Our  approach 
for  a  mental-level 

mental  attributes.  McCarthy 
proach  and  suggested  a  number  of  ideas  for  formalizing 
idea,  stressing 
which  he  called 
eral,  intuitive, 
attempts 
be  the  first  work  within  AI  to  provide 
makes  a  number  of  semantic  contributions:  We  proposed  a  structure 
description  of  an  agent’s  state  which  makes  explicit 
der  uncerminty, 
is  grounded 
approach 
such  as  its  beliefs, 
preferences, 
the  properties  of  these  models,  showing  a  class  of  agents  that  can  be  modeled  using  this 
approach,  and  suggested 

to  choice  un- 
via  its  decision  criterion.  We  explained  how  this  high-level  description 
a  holistic 
state, 
in  the  context  of  other  mental  attributes,  such  as 
to  the  agent’s  behavior.  We  then  investigated 

to  the  issue  of  grounding, 
receives 

two  criteria  for  choosing  among  different  consistent  models. 

in  which  one  aspect  of  the  agent’s  mental 

the  “logicist  community”.  We  believe 

in  less  abstract  aspects  of  the  agent, 

and  the  whole  state  is  related 

its  actions.  We  advocated 

the  agent’s  approach 

its  meaning 

laments 

formal 

issues. 

attempts 

to  automate 

A  primary  concern 

It  should  be  noted 

treatment  of  the  semantic 

to  mental-level  modeling. 

the  design  of  agent  models 

these  and  other  implementation 

In  this  work,  we  were  not  explicitly  concerned  with  implementation 

issues.  Rather,  our 
that  previous 
of  formal  semantic  models  of  mental  states, 
foundations  of  mental- 

goal  was  to  clarify  basic  semantic  questions.  Despite  considerable  progress 
research  has  made  toward  an  understanding 
we  felt  that  clearer  and  more  complete 
level  models  was  needed.  However,  we  now  wish 
issues  pertaining 
important 
about 
discussed  earlier, 

representational 
there  have  been 
from  which  we  can  learn 
In  particular,  work  on  plan  recognition, 

to  briefly  discuss 
that 

is  of  considerable 
in  applications 
and  store  mental-level  models.  While 

interest. 
is  what  knowledge 

states  in  terms  of  possible  worlds, 

representing 
is  one  of  the  framing  decisions 

structures  should 
to 
this  would  not  be  a 
states.  23  The  choice  of  the  set  of 
that  the  modeler  must  make.  Usually, 
relevant  by 
to  some  set  of  propositions 
logics  for  representing  mental  states  as  natural 
logic  for  qualitative 

be  used  to  construct 
study 
suitable  way  of  actually 
possible  worlds 
these  worlds  will  be  truth  assignments 
the  modeler.  This  points 
tools  for  reasoning 
decision 
is  close  to  ours.  Similarly,  algorithms 
limited  set  of  decision  criteria;  and  its  semantics 
for  constructing  mental-level  models  will  be  needed.  We  have  designed  such  algorithms, 
which  appear 
and  are  only 
in  [ 91,  but  they  are  based  on  the  state  space  representation 
used  for  conceptual  understanding 

It  is  able  to  deal  with  beliefs,  preferences, 

it  is  natural  and  common 

the  semantics  of  mental 

In  particular,  Boutilier’s 

[6]  seems  promising. 

to  the  various 

these  models. 

of  this  issue. 

representation 

these  mental 

deemed 

theory 

and  a 

about 

probability 

23 Semantically, 
tations  of  probability  measures  employ,  e.g.,  Bayesian  nets  (471.  Similar  structures  developed 
notions  as  well,  e.g.,[  151,  may  be  of  use  here. 

are  also  defined  over  sets  of  possible  worlds,  yet  efficient  represen- 
for  discrete 

distributions 

260 

R.I.  Brafman,  M.  Tennenholtz/Art@cial  Intelligence  94  (1997)  217-268 

level, 

is  that 

in  this  paper 

of  the  knowledge 

the  concept  of  mental-level 

of  the  abstract  models  described 

[45] 
an  abstract  description 

implementation 
Indeed,  an  important  point  behind 

in  his  discussion 
of  a  system. 
that  the  modeling  process 

In  fact,  a  declarative 
may  not  be  desirable. 
models,  one  stressed  by  Newell 
If  we  push 
this  line  of  reasoning 
they  provide 
itself  need  not  employ  an  abstract 
another  step,  we  realize 
language,  but  can  simply  use  the  ideas  discussed  here  and  in  similar  work  as 
logical 
system  will  perform  mental-level  modeling  of 
abstract  specifications.  The  implemented 
other  agents  using  data-structures 
for  its  domain.  A 
and  algorithms 
case  in  point  are  some  of  the  recent  systems  used  in  the  air-combat  modeling  domain 
[ 53,591.  The  goal  of  workers  in  these  areas  is  to  provide  realistic  air-combat 
simulators. 
Combat  pilots  use  the  current  behavior  of  their  opponents 
them  with  goals  and 
intentions.  Then,  they  use  these  models 
Consequently, 
These  problems 
activity  and  ~~rdination. 

the  future  behavior  of  their  opponents. 
activity. 
involves  group 

one  would  expect  a  simulated  pilot 

by  the  fact  that  air-combat 

to  carry  out  such  modeling 

that  are  suitable 

are  complicated 

to  ascribe 

to  predict 

usually 

The  underlying 

realistic  predictions.  However,  mental-level  models  play 

semantic  models  used  by  these  authors  differ  from  ours  in  certain 
for 
respects,  placing  greater  emphasis  on  intention 
the  above  role  of 
generating 
system  is  guided  by  the  semantic 
abstract  specification 
and 
model, 
damain 
[59]  assigns  heuristic  values 
to  each  consistent  model  as  a  means  of  choosing  between  different  models  consistent 
with  the  modeler’s  current 

the  implementation 
specific  heuristics.  For  example,  Tambe’s  system 

itself  makes  extensive  use  of  domain  specific  info~ation 

ascription  which  seems  essential 

tools.  And  while  the  implemented 

information. 

Indeed,  often 

there  are  many  ways  to  model  a  given  behavior.  Therefore,  general 
the  agent  modeled  or  agents  of  its  type  is  important 

about 

information 

among  different  possible  models.  For  example,  belief  ascription 

constrained 
It  will  be  the  role  of  the  modeler  or  its  designer 

by  background 

is 
knowledge  of  the  agent’s  goals  and 
this 

to  supply 

Alternatively, 

the  modeler  may  gradu~ly 

learn  this  info~atio~. 

task  which  we  considered 

the  set  of  local  states  of  the  agent.  What’s  problematic 

approach  can  be  used 
local  state  changes  with  an  action 

that  of  identifying 
is  that  it  seems  to  require  access  to  the  internal 
that  the  following 
identify 
by  the  agent.  Of  course, 
agent  also  requires  access 
in  many  cases.  Otherwise,  we  simply  use  the  agent’s  actions  as  indications 
state  change.  As  one  would  expect, 
this  problem 
the  harder 
capabilities, 

as  part  of  the  specification  of  the  model  frame  was 
about  this  task 
state  of  the  agent.  However,  we  believe 
to  address 
to  a  certain  extent: 
taken  by  the  agent  or  new  observation  made 
that  are  made  by  the 
can  be  made 
of  local 
the  agent  and  its  sensory 
the  less  we  know  about 
is  and  the  harder  it  is  to  construct  a  model  frame. 

observations 
state,  but  reasonable  deductions 

in  principle, 
to  its  internal 

this  problem 

recognizing 

background 
for  discriminating 
a  modeling 
decision 
info~ation. 
Another 

problem 
criterion.24 

24 One  reason  for  stressing  belief  ascription,  which  requires  this  type  of  knowledge,  is  that  agents  can 
be  equipped  with  such  knowledge.  We  believe  that  an  agent’s  goals  and  its  approach  to  decision  making 
are  somewhat  stable  aspects  of  it,  determined  by  its  designer.  Therefore,  through  observations  over  time,  a 
reasonable  understanding  of  them  can  be attained.  However, dynamic  agents  that  learn  and  make observations 
will  have  much  less  stuble beliefs. 

R.I.  Braftnan, M.  TennenhoWArtificial 

Intelligence 94 (1997) 217-268 

261 

techniques 

Various  issues  remain 

is  required.  Refinements 

construction, 
.are  also  needed. 

In  particular, 
resources  available 
to  deal  with  bounded 

for  future  work.  Indeed,  much  more  research  on  efficient  repre- 
of  the  model 
and  reasoning 
sentation, 
a  realistic  model  must  somehow  deal  with  the 
structure 
to  real  agents.  The  notion  of  intention  may 
bounded  computational 
as  possibly  could  new  decision 
rationality, 
some  ability 
provide 
the  fact  that,  unlike  humans,  artificial  agents  are  likely 
criteria.  However,  considering 
it  is  unlikely 
criterion 
to  have  diverse  architectures, 
for  modeling 
will  be  adequate 
to  the  importance  of  more 
all  agents.  This,  again,  points 
that  will  give  us  a  better  sense  of  the  modeling  capabilities  of  different 
results 
existence 
proposed  models.  Finally, 
techniques 
as  observed  by  researchers  on  plan  recognition, 
for  choosing  among  candidate  models  are  of  great  practical 
and  they  should 
be  studied  farther.  We  hope  to  pursue  some  of  these  questions 

that  a  single  choice  of  decision 

in  our  future  work. 

importance 

Acknowledgment 

We  are  grateful 

to  Yoav  Shoham  for  important  discussions 

for  extensive  comments 
the  anonymous 

this  work,  to  Joe 
and  suggestions  on  previous  drafts  of  this  paper,  and  to 
referees,  Alvaro  de1  Val,  Bruce  Donald,  Nir  Friedman, 

Halpern 
Craig  Boutilier, 
and  Daphne  Koller 
this  work.  We  also  wish  to  thank  Robert  Aumann,  David  Kreps,  and  Dov  Monderer 
helping  us  put  this  work  in  perspective. 

on  earlier  versions  of 
for 

for  their  useful  comments 

and  suggestions 

regarding 

Parts  of  this  work  were  conducted  while 

the  authors  were  at  Stanford  University, 
:supported  by  grants  from  the  National  Science  Foundations,  Advanced  Research 
.4gency  and  the  Air  Force  Office  of  Scientific  Research. 

partially 
Projects 

Appendix  A.  Proofs 

Theorem  21.  Assuming 
a  ranking  function 

perfect  recall,  a  belief  assignment  B  is  admissible 

r  (i.e.,  a  total  pre-order) 

on  the  possible 

iff  there  is 
initial  worlds  such  that 

BI(Z)  =  (s  E  PW[(Z) 

I s  is  r-minimal 

in  PW,  (1) }. 

that  because  of  perfect  recall, 

assume  we  are  given  a  ranking,  we  must  show  that  it  induces 
initial 
the 
in  the  future,  as  long  as  it  is  still 

the  set  of  possible 

that  was  minimal 

that  any  world 

among 

beliefs.  First,  notice 

Proof.  For  one  direction, 
admissible 
worlds  can  only  decrease.  This  means 
possible  worlds  at  one  point,  will  always  be  minimal 
possible.  Let  Z and  1’ be  consecutive 
implies 
that  was  not  minimal 
B,(Z)  rl  PW,(Z’). 

local  states  on  some  run.  The  previous  observation 
i3, (1’)  2  BI  (1)  fl  PWr  (2’).  However,  when  B,  (I)  fl  PW,  (I’)  #  8  then  no  world 
in  PW[(  1’).  Hence,  BJ(  1’)  = 

in  PWf(  Z)  can  become  minimal 

For  the  other  direction, 

let  I  be  the  set  of  initial  possible  worlds  and  assume 

is  admissible,  we  construct  a  ranking 
initial 
recall).  Furthermore, 

perfect  recall  implies 

local  state  Ii,  12, the  sets  PW,  (11)  and  PWI  (Z2)  are  disjoint 

that  BI 
function  based  on  B,.  First,  notice  that  for  any  two 
(because  of  perfect 
that,  if  1;  is  a  local  state  that  can  be  reached 

262 

R.I.  Brafman,  hf.  Tennenholtz/Art@icial  Intelligence  94  (1997)  217-268 

the  ranking  over  each  set.  Therefore,  without 

in  PWl  (Zl )  and  1;  is  a  local  state  that  can  be  reached 

from  PW,  (11)  and  PW,(  12) and  then  unite  these  ranking 

from  a  state 
PW,(Z2),  then  PW,(  1:)  and  PWI(  1;)  are disjoint.  Therefore,  we  can  separately 
states  reachable 
that  preserves 
assume 
the  nth  step  of  the  algorithm  we  assign 
assign  BI(  2)  to  rank  1,  the  lowest 
local  states  at  time  n  in  all  runs  commencing 
states  in  BI (Zj)  ( 1 <  j  <  k)  that  have  not  been  assigned  a  rank  so  far. 

from  a  state  in 
rank  all 
in  any  manner 
loss  of  generality,  we  will 
initial  state  I  and  show  how  to  rank  PW1( I).  At 
set  to  the  nth  rank.  At  1 we 
(a  possibly  empty) 
rank.  Let  II,.  . . , Zk be  the  possible 
to  rank  n  all  those 

that  there  is  only  one  possible 

in  PW,(Z).  We  assign 

(most  normal) 

in  PW,  (I’).  Suppose 

in  a  run.  It  clearly  does  at  the  initial 

The  ranking  we  defined  defines  an  admissible  belief  assignment, 

and  we  now  have  to 
to  the  original  belief  assignment.  We  prove  this  by  induction  on 
show  that  it  is  identical 
the  time  at  which  a  local  state  appears 
local  state 
1. Let  Z’ be  some  local  state  that  appears  at  time  n  on  some  run  and  let  I”  be  one  of  its 
children 
(i.e.,  I”  is  a  local  state  at  time  IZ +  1  at  some  run  whose  local  state  at  time  IZ 
was  1’).  By  the  induction  hypothesis,  BI (I’)  is  indeed  equal  to  the  set  of  minimal 
ranked 
that  BI (Z”)  contains  a  state  that  is  not  in  B, (1’).  Because 
worlds 
BI  is  admissible,  B, (1’) fl PW,  (Z”) = 8.  Consequently,  BI (1’) f~ BI (1”)  = 8.  Moreover, 
for  any  i  that  may  have  preceded  Z”, it  is  the  case  that  BI (I^> n PW,  (Z”) = 8. Otherwise, 
some  state  in  PW,  (I”)  would  have  been  minimal  before, 
using  the  induction  hypothesis, 
that  none  of  the  states  in  PW,(Z”) 
and  hence  would  have  been 
would  have  been  assigned  a  rank  of  n  or  lower.  In  stage  n +  1 of  the  construction  process 
we  assigned  all  of  B, (1”)  the  rank  of  IZ +  1  while  the  rest  of  PW,  (Z”)  has  not  yet  been 
ranked  elements  of  PW,(Z”)  are  precisely 
assigned 
B,(Z”).  If  B[(Z”)  n  B(Z’)  #  8  then  we  know  that  B,(Z”)  = PW,(Z”)  n  B(Z’).  Since 
implies  PW! (1”)  C  PWI (I’),  we  get  that  B, (Z”)  are the  minimal  worlds 
perfect  recall 
in  PW,  (1”)  according 

in  B( I’).  This  means 

a  rank.  Therefore, 

to  the  ranking. 

the  minimally 

0 

Theorem  24.  Let  P  be  the protocol  of  an  agent.  Zf this  agent  can  be  ascribed  beliefs 
at  the  initial  state  and  at  subsequent  revision  states  based  on  this protocol,  it  can  be 
ascribed  an  admissible  belief  assignment  at  all  ZocaZ states. 

admissibility.  We  claim 

is  consistent  with  observed  behavior.  Suppose  not,  this  means 

the  agent’s  beliefs  at 
that  this  belief  assign- 
that  at  some  local 
is,  its  plausible 
than  that  of  the  observed  protocol).  However, 
the  fact  that  p  is  static 
to  to  U, and  that  the  beliefs  are  admissible,  we  will  show  that  if  a  protocol 
is  more  preferred  at  this  state,  it  would  have  been  more  preferred  at  the  previous 
this  process,  we  get  that  this  protocol  would  have  been  more 
the  fact  that  the  beliefs  assigned  at  the  initial  state  are 

Proof.  The  beliefs  ascribed  at  the  initial  states  uniquely  determine 
subsequent  non-revision 
states  assuming 
ment 
state  another  protocol  P’  would  be  chosen  given 
outcome  would  be  strictly  more  preferred 
using 
with  respect 
P’ 
state  as  well.  By  repeating 
preferred 
consistent  with  the  observed  protocol. 

the  sure-thing  principle, 

initially,  contradicting 

of  perfect  recall, 

the  assumptions 

these  beliefs. 

(That 

In  order  to  see  this,  let  I’  and  I”  be  the  children  of  a  local  state  Z (the  same  argument 
local  states  that  can  follow 
at  Z  the  action  assigned  by  the  actual  protocol,  which  by  our 

applies  when 
Z  when  we  perform 

there  are  more  children), 

that  is  the  possible 

R.I.  Brafinan,  M.  TennenhoWArtificial 

Intelligence  94  (1997)  217-268 

263 

This 

to  u  implies 

together  with 

that  the  agent’s  beliefs 

that  1”  is  not  a  revision 

the  same  from  1’ on,  since 

they  would  not  lead  to  I’.  Next,  suppose 

that  the  same  protocols  would  be  preferred 

state  then  we  can  conclude 
(based  on  admissibility). 

assumption, 
If  I”  is  a  revision 
its  B’)  are  identical 
static  with  respect 
states.  By  the  same  protocols  we  mean, 
in  comparing 
since 
the  protocol  P’  that  is  identical  with 
which  assigns  at  Z’, Z”, and  their  descendents 
respectively.  These 
state  is  reachable 
not  conflict.  We  know  that  P’  is  most  preferred 
that  p  is  static  with  respect 
those  protocols 
similar  argument 
B,  (1’)  rl  BI  (Z”)  =  8.  Therefore,  we  can  apply  the  sure-thing  principle 
the  set  of  protocols 
would  be  most  preferred  according 
to  P’  up  to  and  including  1. In  particular, 
most  preferred  at  Z (by  our  assumption).  Hence  P’  is  most  preferred  at  1.  0 

is  most  preferred  at  1. By  definition,  we  have  that  PW,  (1’)  fl  PWl(  2”)  = 0. 
in  1 and  I’  (i.e., 
the  fact  that  p  is 
in  both  local 
there  is  no  point 
protocols  on  1’ that  take  a  different  action  on  Z than  the  actual  protocol, 
state.  Consider 
1,  and 
the  protocols  most  preferred  at  1’ and  Z”, 
local  states,  since  no  local 
there  is 
recall).  Therefore, 
in  I’  given  B,  (1’).  Hence,  using  the  fact 
in  1 among 
in  Z were  exactly  BI(  1’).  A 
that  B,  (1)  =  BI(  I’)  U  B,  (Z”)  and  that 
to  show  that  P’ 
that  are  identical 
the  protocols 

to  different 
from  both  1’ and  I”  (because  of  perfect 

to  U,  we  get  that  P’  would  be  most  preferred 

the  actual  protocol  up  to  and 

this  class  of  protocols  contains 

to  P’  on  1, if  the  beliefs 

to  I”.  Now,  we  know 

that  are  identical 

to  BI  (1)  among 

two  protocols 

really  apply 

including 

applies 

Theorem  27.  Let  A be  an  agent  whose  possible 
length,  for  which 
we  can  ascribe  a  consistent  mental-level  model  (LA,  AA,  BJ,  u, p),  where  BI  is  admis- 
the  sure-thing  principle.  Then,  A can  be  ascribed  a  consistent 
sible  ana’  p  satisfies 
decomposed  mental-level  model  (13~~ AA,  B,,,,  uCur, p). 

runs  have  bounded 

i.e.,  p  satisfies 

and  is  static  with  respect 

the  proof  next.)  Hence  we  can  treat  its  protocol  as  a  backwards 

Proof.  Theorem  3  of  [ 111  says  that  when  an  agent  can  be  ascribed  admissible  beliefs 
the  sure- 
and  the  assumptions  we  have  made  in  this  paper  are  satisfied, 
thing  principle 
then  its 
to  u,  and  its  runs  have  bounded 
protocol  can  be  derived  from  its  mental-level  model  by  using  backwards 
(We  repeat 
protocol.  This  means 
action 
transform  BI  into  B,,,,  which  is  a  cosmetic  change.  Next,  we  assign 
the  backwards 
value,  the  value  of  the  run  suffix  obtained  by  running 
from  this  point  on.  Since  backwards 
choices,  which  are  now  embodied 

the  best  action  given 
in  the  value  of  each  state,  we  get  equivalent 

the  agent  can  be  viewed  as  choosing 
its  choices 

the  best 
actions.  We  first 
to  each  state  as  its 
induction  protocol 
its  future 
choices. 
that  an  agent  can  be 

Proof  of  the  relationship  with  backwards 

induction:  Suppose 

that  at  each  point, 
given 

length, 
induction 

for  the  following 

to  its  beliefs), 

(according 

induction 

induction 

chooses 

(BI)  _ 

to  which 

its  actual  protocol 

that  it  has  a  mental-level  model  with  admissible 
including 

ascribed  admissible  beliefs.  This  means 
beliefs  according 
the  initial  state.  We  now  show  that  P  is  a  backwards 
most  preferred  at  the  initial  state,  given  our  assumptions  on  p  and  u  and  the  assumption 
beliefs.  One  direction  of  this  claim  will  then  give  us  what  we  need  for 
of  admissible 
the  above 
the  statement  of  the  proof,  we  will  use  here 
a  stronger  notion  of  most  preferred  protocol,  where  whenever  P  and  P’  are  equally 
state  that  can  be 
is  more  preferred 
preferred 

is  most  preferred  at  all  states, 

in  local  state  I,  but  P 

induction  protocol 

in  some  revision 

for  A  iff  it  is 

to  simplify 

In  order 

theorem. 

264 

R.I.  5raf~~.  M.  Tenne~~oi~z/A~r~cia~ 

l~iel~i~e~~e  34  (1997)  217-268 

reached  by  both  protocols,  we  will  consider  P  to  be  more  preferred  at  I  as  well.  This 
will  apply  when  we  have  two  protocols 
states, 
but  diverge  on  revision 
is  identical,  and 
the  decision  criterion  will  not  distinguish 
them.  However,  once  we  get  to  the 
state,  one  will  be  preferred  over  the  other,  and  so  we  use  that  fact  already  at 
revision 
the  initial  state. 

states.  The  plausible  outcome  of  such  protocols 
between 

that  behave  identically  on  all  non-revision 

that  it  has  two  children 

theorem  P  is  also  most  preferred  at  its  children.  ~thout 

First  we  show  that  the  protocol  most  preferred  at  the  initial 

local  state  is  a  31  protocol, 
We  do  so  by  induction.  For  tree  of  depth  1 this  is  obvious.  Suppose  that  we  have  shown 
local  state,  be  of 
this  to  be  the  case  for  trees  of  depth  less  than  n  and  let  root,  the  initial 
height  n.  Suppose 
that  P  is  most  preferred  on  rout,  to  which  it  assigns 
the  actions  a.  By 
loss  of  generality, 
the  previous 
assume 
the  protocols 
that  the  BI  protocol  assigns  b  to 
most  preferred  on  1” and  I”  are  BI  protocols.  Suppose 
root.  By  its  definition, 
the  BI 
choices 
then 
for  the  children. 
is  better  than  doing  a  and  then  the  BI  protocol  on  a’s 
the  BI  protocol  on  B’s  children 
the  fact  that  P  is  most  preferred  at  root.  Therefore,  P  is 
children.  But  this  contradicts 
a  BI  protocol. 

selects 
If  it  chose  b  at  root,  then  this  means 

the  action  most  preferred  given 

I’  and  1”.  By  the  induction 

that  doing  b  and 

the  BI  protocol 

hy~thesis, 

We  must  now  show  that  BI  protocols  are  most  preferred  at  the  initial  state.  Suppose 
that  it  is  a  most  preferred  protocol  among  BI  protocols 
that  the  most  preferred  protocols  at  rapt  are  BI,  then  P 

that  P  is  BI  at  root.  This  means 
at  raat.  Since  we  have  shown 
is  at  least  as  preferred  as  them,  and  hence, 

is  most  preferred.  q 

Corollary  28.  L+et P  be the  obse~ed  protocol  of  an agent,  and  suppose  that  this  agent 
caa  be  ascn’bed  beliefs  at  the  initial  state  and  at  all  subsequent  revision  states  bused 
on  this  protocol.  Then,  it  can  be  ascribed  an  admissible  belief  assignment  at  all  local 
states  and  a  local  value function  over  states  such  that  its  observed  action  has  a  most 
preferred  plausible  outcome  according  to  the  local  value function. 

Proof.  This  is  an  immediate  conclusion 
us  to  ascribe  a  structure  with  admissible  beliefs,  while 
a  local  representation 

in  that  case.  Cl 

from  the  previous 

two  theorems.  The  first  allows 
to 

the  latter  allows  us  to  switch 

Theorem  34.  Given  a  be~i~ascription  problem  with  a  decision  cr~teria~ that  is closed 
alder  unions,  if  a  consistent  (~planato~) 
belief  ~signment  exists  then  there  is  a 
unique  minimal  (~~anatu~) 

consistent  belief  assignment. 

Proof.  The  belief  assignment 
actual  protocol  has  a  most  preferred  plausible 
criteria.  Since  the  decision  criterion 
B’  are consistent  belief  assignments 

has  to  satisfy 

local  criteria, 

outcome  with  respect 

i.e.,  for  any  local  state  the 
to  the  decision 
then  we  have  that  if  B  and 

is  closed  under  unions, 
then  so  is  B  U B’.  This  ensures  uniqueness. 

El 

Theorem  37.  For  agents  with  petfect  recall,  if  the  decision  criteria  is  closed  under 
unions  then  the  set  of  consistent  admissible  (explanatory)  belief  assignments,  if  non- 
empty,  contains  a  unique  minimal  (explanatory)  belief  assignment. 

R.I.  Brafmun,  M.  Tennenholtz/Artijicial  Intelligence  94  (1997)  217-268 

265 

that  the  agents  have  perfect 
as  rankings.  Assume 

Proof.  Given 
belief  assignments 
ments,  naae  of  which  is  more  general 
assignments  we  refer 
an  admissible 
Without 
the  ith  rank  of  B).  We  claim 
B”  = By  IJ  B;. 

loss  of  generality 

belief  assignment 

assume 

than  the  other.  (Throughout 

recall  we  know 

that  we  can  think  of  their 
that  B1  and  B2  are  two  different  belief  assign- 
this  proof,  the  belief 
.) .>  We  show  that  we  can  construct 
than  both.  Let  1 be  a  local  state. 
the  states  in 
belief  assignment  B  exists  such  that 

that  By  #  B$.  (We  use  B’  to  denote 

to  assign  subsets  of  PW,( 

that  is  more  general 

that  an  admissible 

the  decision  criterion 

of  admissibility 
a  state 

We  let  B  be  the  union  of  BI  and  B2  on  the  initial 

local  state.  Thus,  B”  =  By  U  Bi. 
is  closed  under  unions,  we  know  that  that’s  fine  for  these 
Because 
the  value  of  B  on  any  state  1  for 
states.  The  definition 
it 
to  be  consistent, 
is  easily  seen 
which  PWJ(  E)  contains 
is  either  B1 (I),  B2(1)  or  their  union,  all  of  which  are  consistent  with 
that  B(Z) 
implies 
to  B.  In  these  states  none  of  By 
the  protocol.  Next,  we  look  to  revision  states  according 
or  Bt  are: possible,  hence  we  can  again  assign 
the  union  of  BI  and  B2  on  these  states. 
This  process  continues  until  all  states  are  assigned,  and  as  was  seen,  at  each  point  B  is 
consistent  with  the  protocol. 

in  B ‘.  This 

implies 

since 

then 

0 

Theorem1  38.  If  A  is  a  goal-seeking  agent  then  it  can  be  ascribed  a  unique  mini- 
mal  admissible  belief  assignment  and  a  unique  minimal  explanatory  admissible  belief 
assignment. 

the  maximal 

by  any  other  protocol.  A  maximal 

there  is  a  world  s  E  PWf (I’)  such  that  the  goal  cannot  be  reached 

Proof.  We  construct  an  admissible  belief  assignment  BI.  Let  1’ be  the  initial 
local  state. 
If  P  performs  HALT  at  1’ then  either  it  is  a  goal  state  and  we  choose  BI (2’)  = PW,  (1’) 
or  otherwise 
from 
s  (using  Rational  Despair).  We  let  B,  (1’)  be  the  set  of  all  such  worlds.  If  P  does  not 
set,  S,  of  states  under  which  P  is  not  weakly 
perform  HALT  we  choose 
dominated 
the  decision  criterion 
is  closed  under  unions.  Such  a  set  is  not  empty  because  otherwise  P  must  be  HALT 
for  any 
(applying  Rational  Effort).  We  let  BI(  1’)  =  S  By  definition  of  admissibility, 
state  1 consistent  with  S  (i.e.,  S  II  PW,(  I)  #  0)  we  must  define  B,(  I> =  S n PW,  (Z), 
to  see  that  in  any  state  1 consistent  with  S,  P  is  still  not  weakly 
therefore,  we  need 
dominated 
that  for  some 
the  goal,  while  some  other  protocol,  P’,  does 
state  s  E  S fl  PWI  (I),  P  does  not  achieve 
is  the 
achieve 
the  goal.  But  this  means 
same  as  P  up  to  I,  and  the  same  as  P’  from  1.  P”  weakly  dominates  P 
in  I’.  This 
contradicts  our  choice  of  P  in  I’. 

that  there  is  some  protocol  P”  such  that  P” 

to  S  n PW,(  I).  Assume 

the  contrary.  This  means 

set  exists  because 

according 

In  states  I  not  consistent  with  S  (i.e.,  states  in  which  S n 2 =  0),  we  cannot  achieve 
that  at  these  local  states 
the  goal  in  any  state 
this  means 
that 
the  goal.  We  let  BI(  1) 

the  goal  using  P.  By  the  Rational  Effort  postulate 
the  protocol  must  be  HALT  (since  a  protocol 
does  not  weakly  dominate  HALT).  By  the  Rational  Despair  postulate 
there  is  some  world  s  E  PWl(Z) 
be  the  set  of  all  such  worlds. 

this  means 
that  cannot  achieve 

from  which  no  protocol  attains 

To  obtain 
an  explanatory 

the  result,  but  using  explanatory 
belief  assignment.  Uniqueness 

belief  assignment,  we  must  construct 
the  fact  that  the  decision 

from 

follows 

266 

R.I.  Brufman,  M.  Tennenholtz/Art@cial  Intelligence  94  (1997)  217-268 

is  closed  under  unions.  The  cons~uction 

criterion  used 
define  5’ to  be  the  set  of  states  on  which  the  protocol  achieves 
requirement 

the  uniqueness 

Finally,  notice 
criterion 

that  if  we  drop 
consistent  with  weak  dominance 

decision 
dominates  o  than  w  is  strictly  more  preferred 

than  0).  D 

(i.e.,  one 

is  as  above,  but  instead  we 

the  goal. 
then  we  can  allow  any 
if  w  weakly 
in  which 

39.  An  agent  that  can  be  modeled  as  a  B-type  agent  can  be  viewed  as  a 
Corollary 
plausible  outcome  maximizer  that  uses some admissible  belief assignment  and a decision 
criterion  consistent  with  weak  dominance. 

It  is  easy 
they  differ  from  goal-seeking 

to  check 

Proof. 
Thus, 
remark 

in  their  decision  criterion.  Using 
in  the  previous  proofs,  we  see  that  they  can  be  ascribed  admissible  beliefs. 

agents  only 

that  B-type  agents  satisfy  both  of  our  rationality 

postulate. 
the 
0 

References 

[ 1 ]  E.  Adams,  The  Logic  of  Conditionals  (Reidel,  Dordrecht,  Netherlands,  1975). 
[Z]  C.E.  Alchourron,  R G&denfors  and  D.  Makinson,  On  the logic  of  theory  change:  partial  meet  functions 

for  contraction  and  revision,  S. zygotic 

.?x@z 50  ( 1985) 510-530. 

[3]  J.E  Allen,  Recognizing  intentions  from  natural  language  utterances,  in:  M.  Brady  and  R.C.  Berwick, 

eds.,  Computational  Models  of Discourse  (MIT  Press,  Cambridge,  MA,  1983). 

[4]  E  Bacchus,  A.J.  Grove,  J.Y.  Halpern  and  D.  Roller,  Statistical  foundations  for  default  reasoning,  in: 

Proceedings  NCAI-95,  Montreal,  Que.  ( 199.5) 563-569. 

151 D.G.  Bobrow,  Artificial  intelligence  in  perspective:  a  retrospective  on  fifty  volumes  of  the  Arf~cial 

intelligence  Journal,  Artificial  ~ate~l~gen~e 59  f 1993)  5-20. 

[6 ]  C. Boutilier, Toward a logic for qualitative decision  theory, in: J. Doyle, E. Sandewall and P  Torasso, eds., 
Proceedings  4th  International  Conference  on  Principles  of  Knowledge  Representation  and  Reasnning, 
Bonn  (1994)  75-86. 

171 C. Boutilier  and  M. Goldszmidt,  Revising  by  conditional  beliefs,  in: Proceedings  AAAI-93,  Washington, 

DC  (1993)  648-654. 

[S] R.I.  Brafman,  Qualitative  models  of  information  and  decision  making:  Foundations  and  applications, 

Ph.D.  Thesis,  Stanford  University,  Stanford,  CA  ( 1996). 

[9]  R.I.  Brafman  and  M.  Tennenholtz,  Belief  ascription,  Working notes  (1994). 

[IO]  R.I.  Brafman  and  M.  Tennenholtz,  Belief  ascription  and  mental-level  modeling,  in:  J.  Doyle,  E. 
Sandewall  and  P. Torasso,  eds.,  Proceedings  4#h lnterna~onal  Conference  on  Principles  of  Knowledge 
Representation  and  Reasoning,  Bonn  ( 1994)  87-98. 

f 111 R.I. Brafman  and M. Tennenholtz,  Towards action prediction  using a mental-level  model,  in: Proceedings 

IJCAI-95,  Montreal,  Que.  (1995)  2010-2016. 

[ 121 R.I.  Brafman  and  M.  Tennenholtz,  On  the  foundations  of  qualitative  decision  theory,  in:  Proceedings 

AAAI-96,  Portland,  OR  (1996)  1291-1296. 

I 13 1 R-1. Brafman  and  M. Tennenholtz,  On the  axioma~~tion  of qualitative decision  criteria,  in: Proceedings 
AAAI  Spring  Symposium  on  Qualitative  Preferences  in  Deliberation  and  Practical  Reasoning  ( 1997) 
29-34. 

[ 141 E. Chart&k  and  RX  Goldman,  A Bayesian  model  of  plan recognition,  Artificial intelligence  64  ( 1993) 

53-80. 

[ 151 A.  Darwiche  and  J.  Pearl,  Symbolic  causal  networks,  in:  ~~oceed~n~s  AAAI-94,  Seattle,  WA  (1994) 

238-244. 

[ 161 A. de1 Val and  Y. Shoham,  Deriving  properties  of  belief  update  from theories  of  action,  in: Proceedings 

IJCAI-89,  Detroit,  MI  ( 1989)  584-589. 

R.I.  Brafman,  M.  TennenhoWArtificial 

Intelligence  94  (1997)  217-268 

267 

[17]  T.G.  Dietterich,  Machine 

learning, 

in:  Annual  Review  of  Compufer  Science,  Vol.  4  (Annual  Reviews 

Inc.,  Pi310 Alto,  CA,  1990)  255-306. 

[ 181  J.  Doyle,  Reasoned 
(1985)  87-90. 

assumptions 

and  Pareto  optimality, 

in:  Proceedings  IJCAI-85,  Los  Angeles,  CA 

[ 191  J.  Doyle,  Constructive  belief  and  rational 
[20]  R.  Fagin,  J.Y.  Halpem,  Y. Moses  and  M.Y.  Vardi,  Reasoning  about Knowledge  (MIT  Press,  Cambridge, 

representation,  Cornput. Intell.  5  ( 1989)  l-l  1. 

MA,  1995). 

[21]  PC.  Fishbum,  Nonlinear  Preference  and  Utility Theory  (Johns  Hopkins  University  Press,  Baltimore, 

MD,  1988). 

[ 221  N.  Friedman  and  J.Y.  Halpem.  A  knowledge-based 

in: 
Proceedings  5th  Conference  on  Theoretical  Aspects  of Reasoning  about  Knowledge,  San  Francisco,  CA 
(Morgan  Kaufmann,  Los  Altos,  CA,  1994). 

for  belief  change,  Part  I:  Foundations, 

framework 

[23]  P.  Gtidenfors 

and  N.E.  Sahlin,  eds.,  Decision,  Probability  and  Utility (Cambridge  University  Press, 

New  York,  1988). 

[24]  M.  Goldszmidt 

and  J.  Pearl,  Rank-based 

and  re.asoning  about  evidence  and  actions, 
Proceedings  3rd  International  Conference  (KR-92))  Cambridge,  MA  ( 1992)  661-672. 

to  belief  revision,  belief  update 
systems:  a  simple  approach 
in:  Principles  of  Knowledge  Representation  and  Reasoning: 

[25]  A.J.  Grove,  Two  modellings 
[26]  J.Y.  Halpem  and  Y.  Moses,  Knowledge 

for  theory  change,  J.  Philosophical  Logic  17  (1988)  157-170. 

and  common  knowledge 

in  a  distributed  environment, 

J.  ACM 

37  (1990)  549-587. 

R.L.  de  Man&as 

[27]  M.J.  Huber,  E.H.  Durfee  and  M.P.  Wellman,  The  automated  mapping  of  plans  for  plan  recognition, 

[28]  H.  Kaltsuno  and  A.  Mendelzon,  On  the  difference  between  updating  a  knowledge  base  and  revising 

in: 
and  D.  Poole,  eds.,  Uncertainty in AI,  Proceedings  10th  Conference  (1994)  344-352. 
it, 
in:  Principles  of  Knowledge  Representation  and  Reasoning:  Proceedings  2nd  International  Conference 
(KR-91),  Cambridge,  MA  (1991)  387-394. 

[29]  H.  Katsuno  and  A.O.  Mendelzon,  Propositional  knowledge  base  revision  and  minimal  change,  Art@cial 

Intelligence  52  (1991)  263-294. 

in:  Proceedings  AAAI-86,  Philadelphia,  PA  ( 1986). 

[ 301  H.A.  Kautz,  Generalized  plan  recognition, 
[31]  S.  Kraus  and  D.J.  Lehmann,  Knowledge,  belief  and  time,  Theoret.  Comput.  Sci.  58  (1988)  155-174. 
[32]  D.M.  Kreps,  Notes  on  the  Theory  of  Choice  (Westview  Press,  Boulder,  CO,  1988). 
[33]  D.M.  Kreps,  A  course  in Microeconomic  Theory  (Princeton  University  Press,  Princeton,  NJ,  1990). 
[ 341  D.M.  Kreps  and  R.  Wilson,  Sequential  equilibria,  Econometrica  50  ( 1982)  863-894. 
[35]  S.  Kri;pke,  Semantical 
[36]  P.  Larnarre  and  Y.  Shoham,  Knowledge, 

logic,  2  Math.  Logik  Grundlag.  Math.  9  ( 1963)  67-96. 
in:  Proceedings  4th 

certainty,  belief  and  conditionalization, 

considerations 

of  modal 

International  Conference  on  Principles  of  Knowledge  Representation  and  Reasoning,  Bonn  (1994). 
[37]  D.  Lelnmann  and  M.  Magidor,  What  does  a  conditional  knowledge  base  entail?,  Artificial Intelligence 

55  (1992) 

l-60. 

[ 381  H.J.  Levesque,  Foundations  of  a  functional  approach 

to  knowledge 

representation,  Art$cial  Intelligence 

23  (1984)  155-212. 
[ 391  H.J.  Levesque,  Knowledge 

(Annual  Reviews 

representation 

and  reasoning, 
Inc.,  Palo  Alto,  CA,  1986)  255-287. 

in:  Annual  Review of  Computer  Science,  Vol.  1 

[40]  H.J.  Levesque,  Making  believers  out  of  computers,  Artificial Intelligence  30  (1986)  81-108. 
[41]  R.D.  Lute  and  H.  Raiffa,  Games  and  Decisions  (John  Wiley  &  Sons,  New  York,  1957). 
[42]  M.  Machina,  Dynamic 
Economic  Literature 

27  (1989)  1622-1668. 

and  non-expected 

consistency 

utility  models  of  choice  under  uncertainty, 

J. 

[43]  J.  McCarthy,  Ascribing  mental  qualities 

to  machines, 

in:  M.  Ringle,  ed.,  Philosophical  Perspectives  in 

Artijkial  Intelligence  (Humanities  Press,  Atlantic  Highlands,  NJ,  1979). 

[44]  S.  Morris,  Revising  knowledge: 

in:  Proceedings  5th  Conference  on  Theoretical 
approach, 
Aspects  of Reasoning  about  Knowledge,  San  Francisco,  CA  (Morgan  Kaufmann,  Los  Altos,  CA,  1994). 

a  hierarchical 

[45]  A.  Newell,  The  knowledge 
[46]  A.  Newell,  Reflections  on  the  knowledge 
[47]  J.  Pearl,  Probabilistic  Reasoning  in  Intelligent  Systems:  Networks  of  Plausible  Inference 

level,  Art$cial  Intelligence  59  (1993)  31-38. 

level,  AI  Mag.  2  (2) 

(1981) 

l-20. 

(Morgan 

Kaufmann,  Palo  Alto,  CA,  1988). 

268 

R.I.  Brafman,  M.  Tennenholtz/Artijicial  Intelligence  94  (1997)  217-268 

[48]  M.E.  Pollack,  Plans  as  complex  mental  attitudes, 

in:  P.R.  Cohen, 

.I.  Morgan  and  M.E.  Pollack,  eds., 

Intentions  in  Communication  (MIT  Press,  Cambridge,  MA,  1990)  77-104. 

[49]  M.E.  Pollack,  D.J. 

Israel  and  M.  Bratman,  Towards  an  architecture 

for  resource-bounded 

agents, 

Technical  Report,  Technical  Note  425,  SRI  International,  Menlo  Park,  CA  ( 1987). 

[50]  J.R.  Quinlan, 
[Sl ]  AS.  Rao  and  M.P.  Georgeff,  Deliberation 

Induction  of  decision 

trees,  Machine  Learning  1  (1986)  81-106. 

and  its  role  in  the  formation  of  intentions, 

in:  Proceedings 

7th Annual  Conference  on  Uncertainty Arttficial Intelligence  (UAI-91),  Los  Angeles,  CA  ( 1991). 

[ 521  AS.  Rao  and  MI?  Geotgeff,  An  abstract  architecture 

for  rational  agents, 

Representation  and  Reasoning:  Proceedings  3rd  International  Conference 
(1992)  439-449. 

in:  Principles  of  Knowledge 
(KR-92).  Cambridge,  MA 

[ 531  A.S.  Rao  and  G.  Murray,  Multi-agent  mental  state  recognition  and  its  application 

to  air-combat  modeling, 

in:  Proceedings  13th  International  DAI  Workshop (DAI-13),  Seattle,  WA  (1994). 

[54]  S.J.  Rosenschein,  Formal 

theories  of  knowledge 

in  AI  and  robotics,  New  Generation  Compuf.  3  ( 1985) 

345-357. 
[ 551  S.J.  Rosenschein 

and  L.l?  Kaelbling,  A  situated  view  of  representation 

and  control,  Artificial Intelligence 

73  (1995)  149-174. 

[ 561  L.J.  Savage,  The  Foundations  of  Statistics (Dover,  New  York,  1972). 
[57]  Y. Shoham,  Agent-oriented 
[58]  Y.  Shoham 
Lakemeyer 
Berlin,  1994). 

and  S.B.  Cousins,  Logics  of  mental  attitudes 
and  B.  Nebel,  eds.,  Foundations  of  Knowledge  Representation  and  Reasoning  (Springer, 

in  AI:  a  very  preliminary 

survey, 

in:  G. 

programming,  Arttficial Intelligence  60  ( 1993)  51-92. 

[ 591  M.  Tambe,  Recursive  agent  and  agent-group 

tracking 

in  real-time,  dynamic  environments, 

in:  Proceedings 

International  Conference  on  Multi-Agent  Systems  (ICMAS-95))  San  Francisco,  CA  ( 1995). 

[60]  R.H.  Thomason,  Towards 

a  logical 

theory  of  practical 

reasoning, 

in:  AAAI  Spring  Symposium  on 

Reasoning  About  Mental  States:  Formal  Theories  and  Applications  ( 1993). 

[ 611  G.  Tidhar,  Personal  communication 
[ 621  J.  von  Neumann  and  0.  Morgenstem,  Theory  of  Games  and  Economic  Behavior  (Princeton  University 

(October  1996). 

Press,  Princeton,  NJ,  1944). 

