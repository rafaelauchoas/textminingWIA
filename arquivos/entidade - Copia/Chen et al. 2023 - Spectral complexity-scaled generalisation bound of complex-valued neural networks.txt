Artiﬁcial Intelligence 322 (2023) 103951

Contents lists available at ScienceDirect

Artiﬁcial  Intelligence

journal homepage: www.elsevier.com/locate/artint

Spectral  complexity-scaled  generalisation  bound  of 
complex-valued  neural  networks
Haowen Chen a,b,c,  Fengxiang He d,b,∗
a Department of Mathematics, ETH Zürich, 8092 Zürich, Switzerland
b JD Explore Academy, JD.com, Inc., Beijing, 100176, China
c Department of Mathematics, Faculty of Science, The University of Hong Kong, Hong Kong Special Administrative Region
d Artiﬁcial Intelligence and its Applications Institute, School of Informatics, University of Edinburgh, Edinburgh EH8 9AB, United Kingdom
e School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington NSW 2008, Australia

,  Shiye Lei e,  Dacheng Tao e,b

a  r  t  i  c  l  e 

i  n  f  o

a  b  s  t  r  a  c  t

Article history:
Received 12 November 2021
Received in revised form 22 May 2023
Accepted 27 May 2023
Available online 5 June 2023

Keywords:
Complex-valued neural networks
Generalisation
Spectral complexity

Complex-valued  neural  networks  (CVNNs)  have  been  widely  applied  in  various  ﬁelds, 
primarily  in  signal  processing  and  image  recognition.  Few  studies  have  focused  on  the 
generalisation of CVNNs, although it is vital to ensure the performance of CVNNs on unseen 
data.  This  study  is  the  ﬁrst  to  prove  a  generalisation  bound  for  complex-valued  neural 
networks.  The  bounds  increase  as  the  spectral  complexity  increases,  with  the  dominant 
factor being the product of the spectral norms of the weight matrices. Furthermore, this 
work  provides  a  generalisation  bound  for  CVNNs  trained  on  sequential  data,  which  is 
also  affected  by  the  spectral  complexity.  Theoretically,  these  bounds  are  derived  using 
the  Maurey  Sparsiﬁcation  Lemma  and  Dudley  entropy  integral.  We  conducted  empirical 
experiments  on  various  datasets  including  MNIST,  ashionMNIST,  CIFAR-10,  CIFAR-100, 
Tiny  ImageNet,  and  IMDB  by  training  complex-valued  convolutional  neural  networks. 
The Spearman rank-order correlation coeﬃcient and the corresponding p-values on these 
datasets provide strong proof of the statistically signiﬁcant correlation between the spectral 
complexity of a network and its generalisation ability, as measured by the spectral norm 
product of the weight matrices. The code is available at https://github .com /LeavesLei /cvnn _
generalization.
© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the 
CC BY-NC-ND license (http://creativecommons .org /licenses /by-nc -nd /4 .0/).

1.  Introduction

Complex-valued neural networks (CVNNs) have garnered signiﬁcant attention in various ﬁelds, such as signal processing 
[1,2], voice processing [3], and image reconstruction [4]. To reduce complex operations, it is natural to link CVNNs to two-
dimensional  real-valued  neural  networks  with  fewer  degrees  of  freedom  [5,6].  A  complex  number  consists  of  a  real  part 
and  imaginary  part,  which  can  alternatively  be  expressed  as  amplitude  and  phase.  When  performing  computations  using 
complex numbers, distinct arithmetic operations are applied separately to the real and imaginary parts.

Several recent studies endeavoured to investigate the different properties of CVNNs and built basic algorithms for their 
implementation.  For  example,  Nitta [7,8,9,10] proved  the  orthogonality  of  the  decision  boundary  of  complex-valued  neu-

* Corresponding author at: Artiﬁcial Intelligence and its Applications Institute, School of Informatics, University of Edinburgh, Edinburgh EH8 9AB, United 
Kingdom.

E-mail address: F.He@ed.ac.uk (F. He).

https://doi.org/10.1016/j.artint.2023.103951
0004-3702/© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://
creativecommons .org /licenses /by-nc -nd /4 .0/).

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

rones, addressed the redundancy problem of the parameters of CVNNs, extended the backpropagation algorithm to complex 
numbers,  and  Trabelsi  et al. [11] organised  the  essential  components  of  complex-valued  deep  neural  networks,  such  as 
complex convolutions, complex batch normalisation, and complex weight initialisation. Empirical studies were conducted to 
examine the experimental performance of CVNNs. Hirose and Yoshida [5] used different neural networks, including CVNNs, 
to process signals of different coherence and Nitta [7] found that for the same computational cost, CVNNs display a higher 
learning speed than real-valued neural networks.

Previous studies have shown satisfactory experimental performance for CVNNs. However, there is still a lack of theoretical 
analysis  of  their  generalisation  ability.  This  gap  in  understanding  has  motivated  us  to  derive  a  generalisation  bound  for 
CVNNs.

This  is  the  ﬁrst  study  to  provide  theoretical  evidence  for  the  generalisation  performance  of  CVNNs.  We  propose  novel 
upper  bounds  which  positively  correlate  with  the  spectral  complexity  of  CVNNs  trained  on  both  independent  identically 
distributed  (i.i.d.)  and  sequential  data.  The  spectral  complexity-scaled  upper  bounds  suggest  a  direct  correlation  between 
the generalisation ability of CVNNs and the spectral norm product of their complex-valued weight matrices.

From an empirical perspective, the experiments were conducted to investigate the inﬂuence of spectral complexity on 
the generalisation ability. Speciﬁcally, we trained CVNNs using stochastic gradient descent (SGD) on six standard datasets: 
CIFAR-10,  CIFAR-100,  MNIST,  FashionMNIST,  Tiny  ImageNet,  and  IMDB.  Excess  risks  were  collected  for  analysis.  When  the 
training error is almost zero across all datasets, the excess risk equals the test accuracy and is informative in expressing gen-
eralisation ability. In addition, because the change in the spectral norm product of the weight matrices primarily contributes 
to  the  change  in  spectral  complexity,  it  is  used  to  simulate  spectral  complexity.  Our  experimental  results  demonstrate  a 
strong correlation between the spectral-norm product and excess risk, which is consistent with our theoretical analysis. The 
code is available at https://github .com /LeavesLei /cvnn _generalization.

The  remainder  of  this  paper  is  organised  as  follows.  Section 2 presents  the  motivation  behind  the  research  and  pro-
vides a review of related work. Section 3 provides an introduction to the preliminaries of complex-valued neural networks. 
Section 4 presents the theoretical results, while Section 5 presents the experimental results. In Section 6, a comparison is 
made between CVNNs and real-valued neural networks to explore the novelties and advantages of the proposed bound. In 
Section 7, the practical applications of the proposed theorems in spectral normalisation algorithms are discussed in detail.

2.  Motivation and related works

Complex  values  are  widely  adopted  in  different  neural  networks  for  their  biological  [12],  computational  [7,13],  and 

representational advantages [14,15].

From  a  biological  perspective,  Reichert  and  Serre [12] proposed  that  the  complex-valued  neuronal  unit  is  a  more  ap-
propriate abstraction in modelling the activity of neurones in the brain than a real-valued unit. To better process cortical 
information, the modelling mechanism must consider both ﬁring rate and spike timing. In incorporating these two elements 
into deep neural networks, the amplitude of a complex-valued neuron represents the ﬁring rate and the phase represents 
the  spike  timing. When  two  inputs  of  an  excitatory  complex-valued  neuron  have  similar  or  dissimilar  phase  information, 
the magnitude of the net input may increase or decrease depending on whether the phases are similar, which correspond 
to synchronous and asynchronous situations, respectively. The incorporation of complex values into deep neural networks 
helps construct richer and more versatile representations.

Regarding the computational aspect, Danihelka et al. [13] combined long short-term memory (LSTM) with the concept of 
holographic reduced representations and used complex values to increase the eﬃciency of information retrieval. Experiments 
showed  that  this  method  achieves  a  faster  learning  speed  on  multiple  memorisation  tasks.  Nitta [7] extended  the  back-
propagation algorithm to complex values, preserving the basic idea of real-valued back-propagation, with updates conducted 
on  both  real  and  imaginary  parts.  Through  experiments,  it  was  demonstrated  that  under  the  same  time  complexity,  the 
learning  speed  of  complex  backpropagation  is  deﬁnitely  faster  than  the  real  speed  when  the  learning  rate  is  low,  that  is, 
less than 0.5.

Complex-valued neural networks also provide advantages over real-valued neural networks in terms of representational 
ability. Arjovsky et al. [14] proposed a unitary recurrent neural network (RNN) with unitary matrices as the weight matrix, 
to circumvent the well-studied gradient vanishing and gradient exploding issues. The unitary matrix is the generalised form 
of the orthogonal matrices in the complex ﬁeld, and the absolute value of its eigenvalue is 1. Compared to an orthogonal 
matrix, a complex-valued matrix has a richer representation, particularly in applications of the discrete Fourier Transform. 
Wisdom et al. [15] further proposed full-capacity unitary RNNs, thereby improving the performance over unitary evolution 
RNN (uRNN).

Given  these  advantages  and  applications  of  CVNNs,  an  increasing  number  of  researchers  have  been  investigating  the 
properties  of  complex-valued  neural  networks  to  provide  a  basic  framework  for  the  implementation  of  CVNNs.  Nitta [8]
demonstrated that the decision boundary of a two-layered complex-valued network is orthogonal, and for a three-layered 
network,  the  decision  boundary  is  nearly  orthogonal.  This  reﬂects  the  computational  power  and  versatility of  complex 
values. In their work, Trabelsi et al. [11] provided the building blocks for complex-valued deep neural networks, including 
complex batch normalisation and complex weight initialisation strategies. They also compared the performances of different 
activation functions on three datasets: CIFAR-10, CIFAR-100, and SVHN.

2

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

While  there  are  studies  presenting  empirical  results  on  the  generalisation  performance  of  complex-valued  neural  net-
works [7,5], there is still a lack of theoretical evidence to support these ﬁndings. Therefore, our study aims to present the 
ﬁrst upper bound for the generalisation error of CVNNs.

Various complexity measures have been proposed to derive an upper bound for the generalisation error of real-valued 
neural networks, such as VC-dimension and Rademacher complexity [16]. Bartlett et al. [17] proved a margin-based multi-
class  generalisation  bound  based  on  covering  number  and  Rademacher  complexity.  These  two  tools  are  also  used  in  our 
work. Compared with Bartlett et al. [17]’s work, our work deﬁnes the spectral complexity of CVNNs by deﬁning the spectral 
norm of a complex-valued matrix and providing generalisation bounds for complex-valued neural networks when processing 
regression tasks.

3.  Preliminaries

This  section  introduces  complex-valued  neural  networks  (CVNNs)  and  presents  the  notations  used  in  the  theoretical 

analysis.

3.1.  Model construction

Each  layer  of  CVNN  consists  of  several  complex-valued  neurones,  as  described  below.  The  input  signals,  weight  pa-
rameters,  threshold  values,  and  output  signals  are  all  complex  numbers  in  complex-valued  neurones.  Assuming  that  the 
complex-valued neurone n is linked with m neurones in the previous layer, the net input to this neurone n is described as 
follows:

T n
input

=

m(cid:2)

i=1

W in Xin + Hn.

Here,  T n
input denotes  the  complex-valued  network  input  of  neurone  n and  W in denotes  the  weight  connecting  the  n and 
i neurones  from  the  previous  layer.  Xin denotes  the  complex-valued  input  signal  from  neurone  i to  neurone  n and  Hn
denotes the threshold value of neurone n. If we denote  Re(T n
input, 
input, respectively, then the output of neurone n can be 

input) as the real and imaginary parts of  T n

input as the amplitude and phase of T n

input) and  Im(T n

(cid:3)
(cid:3)
(cid:3) and θ n

input

(cid:3)
(cid:3)
(cid:3)T n
respectively, and 
described as follows:
(cid:4)

T n
output

= fr

Re(T n

(cid:5)
input)

(cid:4)

(cid:5)
input)

Re(T n

+ f i

or

T n
output

i f p

= e

(cid:4)

(cid:5)

θ n
input

(cid:4)(cid:3)
(cid:3)
(cid:3)T n

input

(cid:3)
(cid:5)
(cid:3)
(cid:3)

.

fa

(1)

(2)

Equation (1) describes the output derived by applying the activation function separately to the real and imaginary parts, 
whereas  Equation  (2) describes  the  situation  when  the  activation  function  is  applied  to  the  amplitude  and  phase,  where 
fr is the activation function applied to the real part;  f i is the activation function applied to the imaginary part;  f p is the 
activation function applied to the phase; and  fa is the activation function applied to the amplitude.

3.2.  Complex-valued activation functions

Several forms of complex-valued activation functions corresponding to real-valued functions have been proposed.
Arjovsky et al. [14] has proposed a modReLU activation function, which preserves the phase information and applies the 

real-valued ReLU function to the amplitude. This function is described as follows:

modReLU(z) = ReLU(|z| + b)eiθz =

(cid:6)

(|z| + b) z
|z|
0

if |z| + b ≥ 0
otherwise

.

In this formula, |z| denotes the amplitude of the complex number z and b ∈ R denotes the threshold for the amplitude of 
z.

Nitta [10] applied  the  hyperbolic  tangent  function  to  the  real  and  imaginary  parts  of  the  input  complex  number  to 

propose the following activation function:

σ (z) = tanh(Re(z)) + i tanh(Im(z)),

√

where i =

−1, tanh(u) def= (exp(u) − exp(−u))/(exp(u) + exp(−u)), u ∈ R.

These two functions represent two main types of complex-valued activation functions with one applied to the real and 
imaginary parts, and the other applied to the amplitude and phase values. There are other variations of activation functions, 

3

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

such  as  zReLU  and  CReLU  [18].  These  different  activation  functions  have  different  properties  in  terms  of  satisfying  the 
Cauchy-Riemann  equations.  Therefore,  it  is  important  to  carefully  select  activation  functions  based  on  the  speciﬁc  task  at 
hand and the characteristics of the data.

3.3.  Basic notations and deﬁnitions

Suppose  S = {(z1, y1), (z2, y2), (z3, y3), ..., (zn, yn)|zi ∈ Z ⊂ Cd Z , yi ∈ Y ⊂ CdY } is the training sample set,  where  yi is 
the  corresponding  label  of  zi ,  d Z and  dY are  the  dimensions  of  the  z and  y separately.  We  deﬁne  D as  a  distribution 
following (zi, yi).

Assume that the network has L layers, and in the ith layer, an ρi -Lipschitz activation function σi : Cdi → Cdi (activation 
functions  such  as  the  CReLU  function  and  hyperbolic  tangent  function  can  be  used  here.  Their  Lipschitz  properties  are 
proven in Appendix A) and a weight matrix  Ai ∈ Cdi−1×di is applied to the input matrix passed from the previous layer. Let 
σi(0) = 0, A = ( A1, A2, ..., A L), and  FA be the function computed by CVNNs:

F A(z) := σL ( A LσL−1 ( A L−1 · · · σ1 ( A1z))) ,
(cid:7)

(cid:8)

with  output  FA(z) ∈ CdL
.  For  input  data 
{z1, z2, ..., zn},  a  matrix  Z ∈ Cn×d can  be  formed  by  collecting  each  zi in  the  ith  row.  Therefore,  the  output  of  this  neu-
ral network can be expressed as  FA(Z T ), the ith column of which is  FA(zi).

it  is  assumed  that  d0 = d Z = d, dL = dY ,  and  W = max{d0, d2, ..., dL}

To avoid ambiguity, it is necessary to clarify the deﬁnition of a complex-valued matrix norm. The norm of any complex 

matrix [ Ai, j] ∈ Cd×k is deﬁned as the norm of a corresponding real-valued matrix as follows:

(cid:10)

(cid:9)
(cid:9)

(cid:11)(cid:9)
(cid:9)

p

(cid:9)
(cid:9)

(cid:10)(cid:3)
(cid:3) Ai, j

(cid:3)
(cid:3)

(cid:11)(cid:9)
(cid:9)

(cid:5)=

p ,

Ai, j

where [ Ai, j] denotes the matrix whose i, jth entry is  Ai, j . In this paper, the L2 norm is calculated entrywise, which means 
that the L2 matrix norm is deﬁned to be the Frobenius norm, i.e.,

(cid:12)(cid:2)

(cid:2)

(cid:3)
(cid:3) Ai, j

(cid:3)
(cid:3)2.

(cid:8) A(cid:8)
2

(cid:5)=

i

j

Moreover, (cid:8)·(cid:8)σ denotes the spectral norm:

(cid:8) A(cid:8)σ := sup
=1

(cid:8)v(cid:8)

2

(cid:13)

(cid:8) A v(cid:8)
2

=

λmax( A∗ A),

∗

denotes the Hermitian transpose of A, and λmax denotes the largest absolute value of eigenvalues of A. Meanwhile, 

where A
(cid:8) A(cid:8)

p,q is deﬁned as:
(cid:4)(cid:9)
(cid:9) A:;1

(cid:8) A(cid:8)

: (cid:5)=

(cid:9)
(cid:9)
(cid:9)

p,q

(cid:9)
(cid:9)

p ,

(cid:9)
(cid:9) A:;2

(cid:9)
(cid:9)

p , ...,

(cid:9)
(cid:9) A:;m

(cid:9)
(cid:9)

(cid:5)(cid:9)
(cid:9)
(cid:9)
q

p

for  A ∈ Cd×m.

To investigate generalisation ability, it suﬃces to derive a high probability bound for the generalisation error:

E
(z, y)∼D

[l(F A(z), y)] − 1
n

n(cid:2)

i=1

l(F A(zi), yi),

where l(FA(z), y) : Z × Y → R denotes the loss function, which is usually set as

l(F A(z), y) = ||F A(z) − y||2.

Finally, the spectral complexity  RA of neural network  FA is deﬁned as follows [17]:
⎞

RA :=

ρi (cid:8) Ai(cid:8)σ

(cid:14)

L(cid:15)

i=1

(cid:16) ⎛
⎝

(cid:10)
i

(cid:9)
(cid:9)
(cid:9) A
(cid:9)2/3
2,1
(cid:8) Ai(cid:8)2/3
σ

L(cid:2)

i=1

3/2

⎠

.

This complexity measure plays a crucial role in the generalisation bound presented in the next section.

4

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

4.  Main theorems and proof sketch

4.1.  Generalisation bound

In this section, the main theorems of this study are presented.

Theorem 1 (Independent and identically distributed data). Let S = {(z1, y1), (z2, y2), (z3, y3), ..., (zn, yn)} be a sample dataset of 
size n with elements drawn independently and identically from distribution D. Given activation functions σi (σi is ρi -Lipschitz and 
σi(0) = 0) and weight matrices A = ( A1, A2, ..., A L), as stated in Section 3.3, then with probability of at least 1 − δ, the corresponding 
complex-valued neural network must satisfy the following:

E(z, y)∼D[l(F A(z), y)] − 1
n

n(cid:2)

i=1

l(F A(zi), yi)

√

+ 36 (cid:8)Z (cid:8)

2

≤ 8M
n

3
2

2 ln(2W ) ln(n)RA

n

(cid:12)

ln 2
δ
2n

,

+ 3M

where l(FA(z), y) = ||FA(z) − y||2 denotes the loss function, and l(FA(z), y) ≤ M for any (z, y).

It  is  observed  that  there  is  no  explicit  occurrence  of  any  combinatorial  parameters,  such  as  L (the  depth  of  neural 
networks). However, this upper bound depends on  L implicitly, as  RA is formed by each layer’s weight matrix norm and 
the Lipschitz constant of each layer’s activation function.

The full proof is provided in Appendix B, and a proof sketch is presented in Section 4.2.

Theorem 2 (Sequential data). Consider S = {(z1, y1), (z2, y2), (z3, y3), ..., (zn, yn)} as a sample dataset, where (zt)t≥1 is a sequence 
of random data adopted to ﬁlter (At)t≥1. Given the activation functions σi (σi is ρi -Lipschitz and σi(0) = 0), and weight matrices 
A = ( A1, A2, ..., A L), as stated in Section 3.3, then with a probability of at least 1 − δ, the corresponding complex-valued neural 
network must satisfy the following:

1

n

n(cid:2)

t=1

(E [l (zt, yt) | At−1] − l (zt, yt))

√

+ 24 (cid:8)Z (cid:8)

2

≤ 8M
n

2 ln(2W ) ln(n)RA

n

(cid:12)

+ M

ln 2
δ
2n

,

where l(z, y) = ||FA(z) − y||2 denotes the loss function, and l(z, y) ≤ M for any (z, y).

Theorem 2 illustrates the generalisation capability of complex-valued neural networks when dealing with sequential data. 
The proof sketch of this theorem is omitted from the main text because there exists some overlap with Theorem 1; however, 
the  full  proof  is  shown  in  Appendix D.  In  the  Appendix,  we  also  present  the  deﬁnitions  of  sequential  Rademacher  com-
plexity, sequential covering number, and sequential Dudley entropy integral, which were proposed in the work of Rakhlin 
et al. [19].

4.2.  Proof sketch

In this section, we provide the proof of Theorem 1, using the following lemmas:
The  proof  is  presented  in  three  steps: I) An  upper  bound  for  the  covering  number  is  obtained:  N ({Z A : A ∈
Cd×m, (cid:8) A(cid:8)
≤ a, (cid:8)}), as in Lemma 1. II) Starting with a single layer and applying the induction method, an upper bound 
for  the  covering  number  of  the  entire  network  is  derived.  This  result  is  illustrated  in  Lemma 2. III) The  upper  bound  of 
Rademacher  complexity  is  derived  via  Dudley  entropy  integral  and  the  above  covering  number  bound,  further  combining 
this bound with Theorem 3.

q,s

In  preparation  for  the  proof,  we  ﬁrst  state  Theorem 3,  which  is  a  crucial  tool  for  step III.  This  theorem  derives  the 
generalisation bound for regression in the case of L p loss function through Rademacher complexity. Let us recall the theorem 
presented by Mohri et al. [16].

Theorem 3 ([16]). Let l : Z × Y → R be an L p loss function bounded by M > 0; F be the hypothesis set; family G = {(x, y) (cid:12)→
l(FA(x), y) : FA ∈ F }, then for any δ, with probability at least 1 − δ, the following inequality holds:

(cid:12)

E
(x, y)∼D

[l(x, y)] ≤ 1
m

m(cid:2)

i=1

l (xi, yi) + 2 ˆRS (G) + 3M

log 2
δ
2m

5

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

where  ˆRS (G) denotes the empirical Rademacher complexity of family G.

Obviously,  to  bound  the  generalisation  error,  it  suﬃces  to  derive  an  upper  bound  for  the  Rademacher  complexity  of 
the  loss  function  family  G = {(x, y) (cid:12)→ l(FA(x), y) : FA ∈ F },  which  is  realised  through  the  ﬁrst  and  second  steps.  In  the 
following paragraphs, we illustrate the proof in detail, in three steps.

Step I

In this step, we obtain a matrix covering for the set of matrix products  Z A ( Z represents the data matrix passed 

to the present layer, and  A is instantiated as the weight matrix) under L2 norm. Lemma 1 is derived as follows:

Lemma 1. (p, q) and (r, s) are two conjugate exponents with p ≤ 2. Let a, b and (cid:8) be three positive real numbers, and let d and m be 
two positive integers. Impose a constraint on the norm of Z such that (cid:8)Z (cid:8)
p
(cid:24)

≤ b. Therefore, we have

(cid:23)

(cid:22)

(cid:5)

(cid:4)(cid:21)

ln N

Z A : A ∈ Cd×m, (cid:8) A(cid:8)q,s ≤ a

, (cid:8), (cid:8) · (cid:8)2

≤

a2b2m2/r
(cid:8)2

ln(4dm).

The  proof  of  Lemma 1 is  based  on  the  Maurey  sparsiﬁcation lemma.  This  lemma  inspired  us  to  cover  the  targeting 
set using a sparsifying convex hull of complex-valued matrices, which is constructed using the product of the rescaled data 
j . Moreover, to prove Theorem 1, constraints are imposed on || A||2,1
matrix Z [20] and some “standard matrices” such as ei eT
(i.e. q = 2,  s = 1), instead of || A||2, which helps avoid the occurrence of combinatorial numbers such as  L and  W outside 
the log term in the upper bound [17].

Step II After  obtaining  the  matrix  covering  upper  bound  in Step I,  the  idea  is  extended  to  the  proof  of  the  entire 
network  covering  number  upper  bound,  thereby  obtaining  Lemma 2.  The  proof  of  Lemma 2 relies  on  induction  and  on 
Lemma 1.

Lemma 2. (σ1, σ2, σ3, ..., σL) are ﬁxed activation functions with each σi being ρi − Lipschitz. The spectral norm bound of matrix Ai
is denoted by si and the matrix (2,1) norm is denoted by bi (i ∈ {1, 2, ..., L}). Given Z as the ﬁxed data matrix, where Z ∈ Cn×d, and 
each row denotes a group of data points, for any (cid:8), we have
⎞

⎛

ln N (F, (cid:8), (cid:8) · (cid:8)2) ≤

(cid:21)

(cid:7)

(cid:8)

(cid:7)

(cid:8)

(cid:8)Z (cid:8)2

4W 2

2 ln
(cid:8)2

L(cid:15)

⎝

j=1

⎠

j ρ2
s2

j

(cid:14)

(cid:25)

L(cid:2)

(cid:16)3

(cid:26)

2/3

bi
si

,

(cid:22)

i=1
(cid:9)
(cid:9)

(cid:9)
(cid:9) A

(cid:10)
i

where  F :=
complex-valued neural networks FA, and W denotes the maximum of {d0, d1, ..., dL}.

: A = ( A1, . . . , A L) , (cid:8) Ai(cid:8)σ ≤ si,

≤ bi

FA

Z T

2,1

is  the  family  of  outputs  generated  by  feasible  choices  of 

In  general,  we  separate  the  proof  of  Lemma 2 into  two  parts:  The  ﬁrst  part  determines  the  relationship  between  the 
entire network upper bound and the matrix covering bounds of the previous  L layers, which is addressed in Appendix B.2
Lemma 6. The second part combines Lemmas 1 and 6, which together provide Lemma 2 through the induction technique.

Step III

Since  we  are  only  deriving  a  bound  for  the  covering  number  of  the  whole  network  N (F , (cid:8), (cid:8)·(cid:8)

2) is  not 
suﬃcient.  We  still  have  to  derive  an  upper  bound  for  the  empirical  Rademacher  complexity  of  the  loss  function  family 
ˆRS (G). It is natural to connect these two concepts using the Dudley entropy integral. However, some preparation work is 
required to satisfy the condition for using the standard Dudley entropy integral.

The standard Dudley entropy integral only illustrates the relationship between N (G, (cid:8), (cid:8)·(cid:8)
2)

has upper bounds N (G, (cid:8), (cid:8)·(cid:8)

2) by N (F , (cid:8), (cid:8)·(cid:8)

2) and  ˆRS (G). Hence, Lemma 3

Lemma 3. Given family F := {FA(Z ) : A ∈ B1 × · · · × BL} and family G := {(z, y) (cid:12)→ l (FA(z), y) : FA ∈ F }, the covering number 
of these two families satisfy

N (F, (cid:8), (cid:8)·(cid:8)

2) ≥ N (G, (cid:8), (cid:8)·(cid:8)

2),

Let l (FA(z), y) = (cid:8)FA(z) − y(cid:8)

2.

(3)

Moreover, because the range of the loss function that we adopt does not lie in [0, 1], Lemma 4 calculates the covering 

number after rescaling.

Lemma 4. If a coeﬃcient, say α > 0, is multiplied by the targeting set G and distance constant (cid:8), then the covering number remains 
unchanged, that is,

N (G, (cid:8), (cid:8)·(cid:8)

2) = N (αG, α · (cid:8), (cid:8)·(cid:8)

2).

Here, αG represents a set obtained by scaling α to each element in G.

6

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Using Lemmas 3 and 4, the Rademacher complexity of G can be bounded through the Dudley entropy integral. We prove 
Theorem 1 by substituting  ˆRS (G) into Theorem 3 for the value of the upper bound obtained. A detailed proof is provided 
in Appendix B.3.

5.  Results of experiments

In  this  section,  we  present  the  experimental  results  of  training  complex-valued  neural  networks  using  SGD  on  six 

datasets: MNIST, FashionMNIST, CIFAR-10, CIFAR-100, Tiny ImageNet, and IMDB.

Before presenting the experimental results, a summary of the two upper bounds is derived in Theorems 1 and 2. Both 
for the independent identically distributed and sequential data cases, we showed that the derived upper bound scales with 
the spectral complexity of the complex-valued neural network:

RA :=

(cid:14)

L(cid:15)

i=1

ρi (cid:8) Ai(cid:8)σ

(cid:16) ⎛
⎝

(cid:10)
i

(cid:9)
(cid:9)
(cid:9) A
(cid:9)2/3
2,1
(cid:8) Ai(cid:8)2/3
σ

L(cid:2)

i=1

⎞

3/2

⎠

.

(cid:14)(cid:25)

(cid:28)

(cid:16)

L
i=1
(cid:14)(cid:25)

(cid:10)
i

(cid:26)3/2

(cid:9)
(cid:9)
(cid:9) A
(cid:9)2/3
2,1
(cid:8) Ai (cid:8)2/3
σ
(cid:9)
(cid:9)
(cid:9) A
(cid:9)2/3
2,1
(cid:8) Ai (cid:8)2/3
σ

L
i=1

(cid:10)
i

(cid:28)

are equivalent, the factor 

we assume that the change in 

change in  RA is 

(cid:4)(cid:27)

(cid:5)

L

i=1 ρi (cid:8) Ai(cid:8)σ

The formula for the spectral norm  RA consists of two parts: the Lipschitz constant of this neural network 

and  another  factor  related  to  the  sum  of  quotients  of  weight  matrix  norms 

.  Since  the  two  norms 

(cid:4)(cid:27)

(cid:5)

L

i=1 ρi (cid:8) Ai(cid:8)σ

(cid:14)(cid:25)

(cid:28)

L
i=1

(cid:10)
i

(cid:9)
(cid:9)
(cid:9) A
(cid:9)2/3
2,1
(cid:8) Ai (cid:8)2/3
σ

(cid:16)

(cid:26)3/2

remains in the interval [C1, C2] for some constants  C1 and  C2. Therefore, 

(cid:16)

(cid:26)3/2

is minor. Then, in the training process, the part which dominates the 

of activation functions (ρi ) remain unchanged. Therefore, we use the change in the spectral norm product (
simulate the changing trend in  RA.

for the Lipschitz constant of the neural network. This is because the Lipschitz constants 
(cid:8) Ai(cid:8)σ ) to 

L
i=1

(cid:27)

5.1.  Spectral norm of the weight matrix

First, the calculation of the spectral norm of the complex weight matrix in each convolutional layer is demonstrated.
Considering the complex-valued kernel W = X + Y i in each layer, where  X and Y are real-valued kernels, because each 
convolutional kernel corresponds to a matrix transformation [21], the real-valued weight matrices of kernels  X and  Y can 
be derived, which are denoted by C and  D. Hence, the complex-valued weight matrix  A of each layer can be expressed as 
C + Di. Then, by deﬁnition, the spectral norm of the complex-valued matrix  A is:

(cid:13)

(cid:8) A v(cid:8)2 =

λmax ( A∗ A)

(cid:8) A(cid:8)σ : = sup
(cid:8)v(cid:8)2=1
(cid:29)

=

λmax

(cid:7)

C T C + D T D + (C T D − C D T )i

(cid:8)

.

Here, because  A

∗

A is a Hermitian matrix, it has only real eigenvalues.

5.2.  Complex-valued convolutional neural networks

In our experiments, we trained complex-valued convolutional neural networks (CVCNN) and complex-valued multi-layer 
perceptrons  (CVMLP)  on  different  datasets.  The  IMDb  dataset  was  used  for  training  the  CVMLP.  The  proposed  complex-
valued neural network architectures are described in detail in Appendix E.2.

The CVCNN architecture consists of three types of layers: convolutional, maxpooling, and fully connected layers. The last 
layer is a CVMLP. The convolutional and maxpooling layers in CVCNN are analogous to the hidden layers in CVMLPs, as is 
the case in real-valued neural networks [22]. The operations on the convolutional and maxpooling layers can be expressed 
as matrix-vector multiplication, as shown in [23]. To analyze these results, we considered the convolutional and maxpooling 
layers separately.

5.2.1.  Matrix multiplication interpretation of convolution operations

First, we interpret the convolution process using matrix multiplication.
The convolutional layer usually receives as input a matrix of dimensions [h1 ∗ w 1 ∗ d1], which is a 3D matrix with length 
h1, width  w 1, and height d1. Further, the kernels are deﬁned as follows. Assuming that the number of output channels is 
d2, we have d2 kernels, which can be written as a matrix with dimensions [h2 ∗ w 2 ∗ d1]. Finally, we assume that the output 
matrix has dimensions [h3 ∗ w 3 ∗ d2]. Fig. 2 shows the input, kernel, and output matrices of the convolution process.

7

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

The relationship between the output and input matrix dimensions can be expressed as follows:

w 3 = (w 1 − w 2 + 2p + 1)/s
h3 = (h1 − h2 + 2p + 1)/s

where p denotes the padding number, and s indicates the stride number. Without loss of generality, we assume that  p = 0
and s = 1. The next step is to ﬂatten the input matrix into a vector and write these d2 3D kernel matrices into a 2D matrix.

Because the input matrix is of dimension [h1 ∗ w 1 ∗ d1], it can be ﬂattened into a vector x ∈ Rh1 w1d1 such that
(cid:5)

(cid:4)

x =

(1,1), x1
x1

(1,2), . . . , xd

(h,w), . . . , xd1

(1,1), . . . , xd1

(h1,w 1)

T

.

Here, xd

In  terms  of  constructing  the  kernel  matrix,  we  ﬁrst  consider  the  convolutional  matrix  K i

(h,w) denotes the (h, w, d)th entry of the input matrix (0 < h ≤ h1, 0 < w ≤ w 1, 0 < d ≤ d1).
(cid:4)
(h,1,d), . . . , ki
ki

(h,d) formed  by  the  vector 
h,w,d denotes the (h, w, d)th entry of the ith kernel (0 < h ≤ h2, 0 < w ≤ w 2, 
∈ Rw2 is 

ki
(h,d)
0 < d ≤ d1.  0 < i ≤ d2).  The  convolutional  matrix  K i
formed as follows:

(h,d) induced  by  the  vector  ki

(cid:4)
(h,1,d), . . . , ki
ki

∈ Rw2 , where ki

(h,w2,d)

(h,w2,d)

(h,d)

=

=

(cid:5)

(cid:5)

⎡

⎢
⎢
⎢
⎢
⎣

(h,2,d) ki
(h,1,d) ki
ki
(h,1,d) ki
ki
0
. . .
. . .

...

0

(h,3,d)
(h,2,d) ki
. . .
0

. . .

. . .

(h,3,d)

ki
(h,1,d)

ki
(h,w 2,d)
. . .
. . .

ki
(h,2,d)

0

ki
(h,w 2,d)
. . .

ki
(h,3,d)

and  K i

(h,d) is the Toeplitz matrix in R(w1−w2+1)×w1 .

The second step is to construct the convolutional matrix  K i
R(h1−h2+1)(w1−w2+1)×h1 w1 , which is constructed by considering  K i

. . .
. . .
. . .
. . . ki

⎤

⎥
⎥
⎥
⎥
⎦

,

0

0

...

(h,w 2,d)

d induced by matrix  K i

(h,d).  K i

d is a Toeplitz block matrix in 

(h,d) as its components. We can write  K i

d as follows:

⎡

⎢
⎢
⎢
⎢
⎣

K i

. . .

(2,d) K i
(1,d) K i
K i
(3,d)
(h2,d)
(2,d) K i
(1,d) K i
K i
. . .
0
. . .
. . .
. . .
. . .
(1,d) K i
K i
. . .
0

(3,d)

(2,d)

...

0

0

K i

(h2,d)
. . .
K i

(3,d)

. . .
. . .
. . .
. . . K i

⎤

⎥
⎥
⎥
⎥
⎦

0

0

...

(h2,d)

Therefore, the entire convolutional matrix  K ∈ Rd2(h1−h2+1)(w1−w2+1)×h1 w1d1 induced by the d2 kernels, can be viewed 

as a block matrix by taking  K i

d as its components. It is constructed as follows:

K =

⎡

⎢
⎢
⎢
⎢
⎣

K 1
1
K 2
1
...
K d2
1

K 1
2
K 2
2
. . .
K d2
2

⎤

⎥
⎥
⎥
⎥
⎦

. . . K 1
d1
. . . K 2
d1
...
. . .
. . . K d2
d1

Consequently, we regard  K as the weight matrix of the convolutional layer and apply its spectral norm to calculate the 

generalisation bound.

For  the  maxpooling  layer,  there  exists  a  weight  matrix  with  entries  of  either  0 or  1,  depending  on  which  entry  x is 
(1,1) (1 ≤ d ≤ d1)  is  reserved,  then  the  weight  matrix  M ∈

reserved  for  the  maxpooling  process.  For  instance,  if  each  xd
Rd1×h1 w1d1 of the maxpooling layer is

⎡

⎢
⎢
⎢
⎣

1 0 . . . 0 0 . . . 0 0 . . . 0
0 0 . . . 1 0 . . . 0 0 . . . 0
...
...
...
0 0 . . . 0 0 . . . 1 0 . . . 0

...

...

...

...

...

...

...

⎤

⎥
⎥
⎥
⎦ .

M is a sparse matrix and its (i, ih1 w 1) entry equals 1.

In conclusion, CVCNNs are a type of complex-valued neural network that still ﬁts the theoretical analysis presented in 

Section 4.

8

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Fig. 1. Plots of excess risk and spectral norm product (SN prod) as functions of the epoch. The right y-axis denotes the spectral norm product, and the left 
y-axis denotes the excess risk.

Fig. 2. The input matrix, kernel, and output matrix in a convolution process.

5.3.  Results

The architectures of the complex-valued neural networks of this study are described in Appendix E.2. The datasets used 
were MNIST, FashionMNIST, CIFAR-10, CIFAR-100, Tiny ImageNet, and IMDB. Descriptions of these datasets are presented in 
Appendix E.1. We trained CVCNNs using SGD on MNIST, FashionMNIST, CIFAR-10, CIFAR-100, and Tiny ImageNet to investi-
gate the generalisation bound derived in Theorem 1 and the CVNN trained on IMDB to investigate the generalisation bound 
derived in Theorem 2 for sequential training data. The results are presented in Fig. 1.

Fig. 1 displays excess risk and spectral norm product as a function of epoch. In addition, we performed Spearman rank-
order correlation tests on all excess risks and spectral norm products of MNIST, FashionMNIST, CIFAR-10, CIFAR-100, IMDB, 
and Tiny ImageNet. Spearman’s rank-order correlation coeﬃcients (SCCs) and p-values show that the correlation between 
the spectral norm product and generalisation ability is statistically signiﬁcant (p < 0.0051), as listed in Table 1. These results 
strongly support our theoretical ﬁndings.

1 The deﬁnition of “statistically signiﬁcant” has various versions, such as p < 0.05 and p < 0.01. This paper used a more rigorous approach (p < 0.005).

9

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Table 1
SCC and p values of the spectral norm product and excess risk.

CIFAR-10

SCC

p

CIFAR-100

SCC

p

MNIST

SCC

p

0.99

IMDB

3.703 × 10

−228

0.80

4.124 × 10

−23

0.99

9.044 × 10

−142

FashionMNIST

Tiny ImageNet

SCC

p

SCC

p

SCC

p

0.99

6.118 × 10

−194

0.99

3.703 × 10

−142

0.99

4.060 × 10

−125

6.  Comparison with real-valued neural networks

6.1.  Theoretical analysis

In  previous  studies,  Bartlett  et al. [17] proved  that  the  generalisation  bound  of  real-valued  neural  networks  (RVNNs) 
is positively correlated with spectral complexity. In his work, spectral complexity is deﬁned as the product of real-valued 
weight  matrix  spectral  norms,  as  deﬁned  in  the  following  formula,  where  Ai denotes  the  weight  matrices  of  real-valued 
neural networks, and Mi denotes the reference matrices.
(cid:16) ⎛
⎝

(cid:9)
(cid:9)2/3
2,1

(cid:9)
(cid:9) A

L(cid:2)

L(cid:15)

(cid:10)
i

(cid:10)
i

3/2

⎞

⎠

(cid:14)

RA :=

ρi (cid:8) Ai(cid:8)σ

i=1

i=1

− M
(cid:8) Ai(cid:8)2/3
σ

The  generalisation  bound  we  presented  for  CVNNs  is  similar  to  the  one  for  real-valued  neural  networks  (RVNNs)  pre-
sented  in  [17].  Theorem 4 illustrates  the  relationship  between  the  generalisation  bounds  of  RVNNs  and  their  spectral 
complexity.

(cid:7)
Theorem 4 ([17]). Let nonlinearities (σ1, . . . , σL) and reference matrices 
be given as above (that is, σi is ρi -Lipschitz and 
σi(0) = 0). Then for (x, y), (x1, y1), . . . , (xn, yn) drawn i.i.d. from any probability distribution over Rd × {1, . . . , k}, with probability 
at least 1 − δ over ((xi, yi))n

i=1, every margin γ > 0 and network FA : Rd → Rk with weight matrices A = ( A1, . . . , A L) satisfy

M1, . . ., M L

(cid:8)

$

%

(cid:14)

Pr

arg max
j

F A(x) j (cid:13)= y

≤ &Rγ (F A) + ’O

(cid:8)X(cid:8)2 RA
γ n

where &Rγ ( f ) ≤ n

−1

(cid:28)

i 1)

f (xi ) yi

≤γ +max j(cid:13)= yi

f (xi ) j

*

and (cid:8) X(cid:8)2 =

(

(cid:16)

ln(1/δ)
n

ln(W ) +

(cid:29)(cid:28)

(cid:8)xi(cid:8)2
2.

i

(4)

The main difference between the generalisation bounds of CVNNs and RVNNs lies in how the spectral complexity and 
spectral norm of the weight matrix are deﬁned. In our theorem, we redeﬁne the spectral norm of a complex-valued matrix, 
instead of reformulating the complex-valued matrix into a constrained real-valued matrix to calculate its spectral norm. By 
doing so, we preserve the unique structure of complex-valued matrices and avoid increasing computational costs. Reformu-
lating  a  complex-valued  matrix  into  a  real-valued  matrix  can  result  in  a  considerable  increase  in  the  matrix  dimensions, 
making the calculation of its spectral norm more complicated. Additionally, our contribution lies in proving that the gener-
alisation bound is positively correlated with the deﬁned spectral complexity, which inspired us to consider the effectiveness 
of spectral normalisation in CVNNs, as discussed in Section 7.

6.2.  Empirical analysis

Empirical comparisons of CVNNs and RVNNs have been conducted in various forms in the literature. For instance, Trabelsi 
et al. [11] controlled  the  number  of  parameters  of  both  RVNNs  and  CVNNs  with  a  trade-off  on  width  and  depth,  while 
Nitta [9] compared  the  learning  speed  of  CVNNs  and  RVNNs  under  the  same  architecture  or  with  a  controlled  number 
of parameters. However, it is challenging to impose a fair constraint on CVNNs and RVNNs during comparison due to the 
abundance of studies on real-valued neural networks compared to the limited work on investigating the best architecture 
for  complex-valued  neural  networks.  Although  this  study  presents  two  methods  and  roughly  compares  the  generalisation 
ability of CVNNs and RVNNs, the results cannot be used as conclusive evidence to indicate which type of neural network is 
better. A fair comparison of their performances remains an open problem.

In this study, we compared the performance of CVNNs with those of different RVNNs. The ﬁrst type of RVNN has the 
same number of parameters as CVNN, while the second type has the same architecture as CVNN. The results are presented 
in Fig. 3.

10

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Fig. 3. Change in excess risk with the number of epochs when complex-valued and real-valued neural networks are trained on different datasets. The solid 
line represents the performance of complex-valued neural networks. The dashed line represents the performance of real-valued neural networks with the 
same number of parameters. The dotted line represents the performance of real-valued neural networks with the same architecture.

As  shown  in  Figure  3,  it  is  diﬃcult  to  draw  conclusions  when  comparing  the  generalisation  performance  of  complex-
valued  and  real-valued  neural  networks.  Controlling  for  the  architecture,  the  generalisation  ability  of  real-valued  neural 
networks is superior to that of complex-valued neural networks. However, when controlling for the number of parameters, 
the performance of CVNNs is better when trained on CIFAR-10 and CIFAR-100.

7.  Applications

In this section, we demonstrate that the spectral regularisation algorithm is a practical application of our theory. Be-
cause  the  generalisation  bound  of  complex-valued  neural  networks  correlates  positively  with  the  product  of  the  spectral 
norms of the weight matrices, it is natural to investigate whether adding a regularisation term of the spectral norm to the 
loss function decreases excess risk in real-life training of complex-valued neural networks. We conducted experiments on 
MNIST, FashionMNIST, CIFAR-10, CIFAR-100, IMDB, and TingImageNet. The empirical results show that applying the spectral 
regularisation algorithm decreases excess risk signiﬁcantly, fully supporting our idea. The spectral regularisation algorithm 
is presented in Algorithm 1. The empirical results are presented in Fig. 4.

8.  Conclusions

In this study, we propose two generalisation bounds for complex-valued neural networks for i.i.d. and sequential data. 
Our  analysis  shows  that  the  bounds  scale  with  spectral  complexity,  which  includes  the  spectral  norm  product  of  weight 
matrices  as  a  factor. We  also  provide  empirical  evidence  to  support  our  theoretical  ﬁndings.  Our  work  contributes  to  the 
understanding of the generalisation ability of complex-valued neural networks and encourages further exploration of their 
unique properties.

Declaration of competing interest

The  authors  declare  that  they  have  no  known  competing  ﬁnancial  interests  or  personal  relationships  that  could  have 

appeared to inﬂuence the work reported in this paper.

11

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Fig. 4. Plots of excess risk as a function of epoch. The right y-axis denotes the spectral norm product, and the left y-axis denotes the excess risk. The solid 
line denotes CVNNs without spectral regularisation, whereas the dashed line denotes the results after adding spectral regularisation.

Algorithm 1: SGD with complex-valued spectral regularisation.

Input: Initialised network parameter θ 0, # training epoch N, spectral regularisation factor λ, learning rate α
Output: Optimised network parameter θ N
for t = 0 to N − 1 do

Sample a minibatch {(xi , yi )}k
for l = 1 to L do

i=1 from the training data.

Express the l-th layer weight matrix Al as Cl + Dl i where Cl and Dl are real-valued matrices
Compute the layer-wise complex-valued spectral norm, (cid:8) Al(cid:8)σ =
l Dl − Cl D T
C T

l Cl + D T
C T

l Dl +

λmax

(cid:29)

(cid:7)

(cid:7)

l

(cid:8)

(cid:8)

i

end
Compute Lt = 1
Update θ t+1 = θ t − α∇θ t

(cid:28)
k

k

Lt

i=1 (cid:11)( fθ t (xi ), yi ) + λ 

L
i=1

(cid:8) Ai (cid:8)σ

(cid:28)

end

Data availability

Data will be made available on request.

Acknowledgements

Mr Shiye Lei was supported in part by Australian Research Council Projects FL170100117 and IH180100002.

Appendix A.  Lipschitz properties of several activation function

In  this  section,  our  goal  is  to  prove  three  types  of  activation  functions  that  are  widely  used  in  complex-valued  neural 

networks being Lipschitz continuous.
The ﬁrst one is introduced in [10]:

σ1(z) = tanh(Re(z)) + itanh(Im(z)).

12

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

This  activation  function  applies  the  hyperbolic  tangent  function  on  both  the  real  part  and  the  imaginary  part.  Since  the 
derivative  of  the  hyperbolic  tangent  function  is  upper  bounded  by  1,  hence  we  can  see  that  σ1 is  1-Lipschitz  in  each 
coordinate, if we view the real part and imaginary part as different coordinates. Then we have

(cid:9)
(cid:9)σ1 (z1) − σ1
)(cid:10)

(cid:8)(cid:9)
(cid:9)

(cid:7)

(cid:15)
1

z

p

tanh (Re (z1)) − tanh

(cid:8)(cid:8)(cid:11)

(cid:7)

Re

(cid:7)

(cid:15)
1

z

p +

)(cid:7)

Re (z1) − Re
(cid:9)
(cid:9)
(cid:15)
i

(cid:9)
(cid:9)zi − z

p .

(cid:8)(cid:8)

(cid:7)

(cid:15)
1

z

(cid:7)

p +

Im (z1) − Im

=

≤

=

(cid:10)
tanh (Im (z1)) − tanh
(cid:7)

(cid:8)(cid:8)

* 1
p

p

(cid:15)
1

z

(cid:8)(cid:8)(cid:11)

* 1
p

p

(cid:7)

Im

(cid:7)

(cid:15)
1

z

The ﬁrst inequality holds because the hyperbolic tangent function is 1-Lipschitz.

The second type of activation function is the C ReLU function introduced in [11]:

σ2(z) = ReLU (Re(z)) + i ReLU (Im(z)).

This function also operates separately on both the real part and the imaginary part. The proof process is quite similar to the 
ﬁrst one since the ReLU function is also 1-Lipschitz.
(cid:8)(cid:9)
(cid:9)

(cid:7)

(cid:9)
(cid:9)σ1 (z1) − σ1
)(cid:10)

(cid:15)
1

z

p

tanh (Re (z1)) − tanh

(cid:8)(cid:8)(cid:11)

(cid:7)

Re

(cid:7)

(cid:15)
1

z

p +

)(cid:7)

Re (z1) − Re
(cid:9)
(cid:9)
(cid:15)
i

(cid:9)
(cid:9)zi − z

p .

(cid:8)(cid:8)

(cid:7)

(cid:15)
1

z

(cid:7)

p +

Im (z1) − Im

=

≤

=

(cid:10)
tanh (Im (z1)) − tanh
(cid:7)

(cid:8)(cid:8)

* 1
p

p

(cid:15)
1

z

(cid:8)(cid:8)(cid:11)

* 1
p

p

(cid:7)

Im

(cid:7)

(cid:15)
1

z

The third type of activation function is

σ3(z) = tanh(|z|) exp(iθ),

where  θ = arg(z).  If  we  write  z = x + yi and  use  vector  notation  to  represent  the  real  part  and  imaginary  part  of  the 
operation, we will get

⎡

⎢
⎣

,

=

+

Re(σ3(z))
Im(σ3(z))

tanh[(x2 + y2)

tanh[(x2 + y2)

1
2 ]

1
2 ]

x
(x2+ y2)
y
(x2+ y2)

1
2

1
2

⎤

⎥
⎦ .

Notice that

(cid:3)
(cid:3)
(cid:3)
(cid:3)tanh[(x2 + y2)
(cid:3)

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

≤ 1.

1
2 ]

1
(x2 + y2)

1
2

Hence, we have the following inequality

1

1

(cid:3)
(cid:3)
(cid:3)
(cid:3)tanh[(x2
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)tanh[(x2
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)tanh[(x2
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

1

≤

+

=

+ y2
1)

tanh[(x2
1
+ y2
1)

(x2
1

1
2

(x2
1

(x2
1

+ y2
1)

1
2 ]

+ y2
1)

1
2 ]

+ y2
1)

1
2 ]

x1
+ y2
1)

x1
+ y2
1)

x2
+ y2
1)

1
2

1
2

1
2

(x2
1
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

1
2 ]

|x1 − x2| +

− tanh[(x2
2

+ y2
2)

1
2 ]

− tanh[(x2
1

+ y2
1)

1
2 ]

x2
+ y2
2)

x2
+ y2
1)

1
2

1
2

(x2
2

(x2
1

− tanh[(x2
2

+ y2
2)

1
2 ]

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

1
2 ]

+ y2
1)

tanh[(x2
1
+ y2
1)

(x2
1

1
2

1
2

x2
+ y2
(x2
2)
2
− tanh[(x2
(x2
2

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

Noted that, assume  g(x) = tanh(x)

x

, then through calculation, we have 

13

+ y2
2)

1
2 ]

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

1
2

|x2|.

2
+ y2
2)
(cid:3)
(cid:3)
(cid:3) bounded by 1. Hence,  g(x) is 1-Lipschitz.
(cid:3)g
(cid:15)(x)

H. Chen, F. He, S. Lei et al.

Therefore, we have

Artiﬁcial Intelligence 322 (2023) 103951

1
2 ]

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

|x2|

+ y2
2)

2
+ y2
2)

1
2

1
2 ]

+ y2
1)

tanh[(x2
1
+ y2
1)
2 − (x2
2

(x2
1
+ y2
1)

1
2

1

≤

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)(x2
1
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

− tanh[(x2
(x2
2
(cid:3)
(cid:3)
(cid:3) |x2|
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

+ y2
2)

1
2

1
2

≤

≤|x2|

− y2
2
+ y2
2)

x2
1
+ y2
1)

(x2
1
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

+ y2
− x2
1
2
1
2 + (x2
2
(x1 + x2)
2 + (x2
+ y2
1)
2
( y1 + y2)
2 + (x2
+ y2
1)
2
≤α|x1 − x2| + α| y1 − y1|

+|x2|

(x2
1

(x2
1

1

1

+ y2
2)

+ y2
2)

|x2|
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

1
2

1
2

|x1 − x2|

| y1 − y2|

for some constant α such that |x2| ≤ α.

Then, we can bound the ﬁrst coordinate by

(cid:3)
(cid:3)
(cid:3)
(cid:3)tanh[(x2
(cid:3)

1

+ y2
1)

1
2 ]

x1
+ y2
1)

1
2

(x2
1

− tanh[(x2
2

+ y2
2)

1
2 ]

(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)

x2
+ y2
2)

1
2

(x2
2

≤(α + 1)|x1 − x2| + α| y1 − y1|.

Without loss of generality, the second coordinate is bounded by α|x1 − x2| + (α + 1)| y1 − y1|.

Finally, we have

(cid:8)σ3(z1) − σ3(z2)(cid:8)
(cid:7)

p

((α + 1)|x1 − x2| + α| y1 − y2|)p + (α|x1 − x2| + (α + 1)| y1 − y2|)p

(cid:8) 1
p

(cid:7)

M|x1 − x2|p + M| y1 − y2|p

(cid:8) 1
p = (2α + 1) (cid:8)z1 − z2(cid:8)

p ,

≤

≤

where M = (2α + 1)p , z1 = x1 + iy1 and z2 = x2 + iy2.

Hence, we have proved that σ3(z) = tanh(|z|)exp(iθ) is Lipschitz continuous.

Appendix B.  Proof of Theorem 1

B.1.  Proof of Lemma 1

Before proving Lemma 1, we ﬁrst introduce Maurey’s sparsiﬁcation lemma [24,17].

Lemma 5 (Maurey’s sparsiﬁcation lemma [24]). In a Hilbert space H equipped with norm || · ||, consider f ∈ H such that f =

αi (cid:13)= 0. Then, for any positive integer k, there always exist non negative 

n(cid:28)

i=1

αi gi

where gi ∈ H, αi are positive real numbers, and α =

integers k1, k2, ..., kn such that 

n(cid:28)

i=1

ki = k such that

n(cid:28)

i=1

(cid:9)
(cid:9)
(cid:9)
(cid:9)

(cid:9) f − α

k

(cid:9)
(cid:9)
2
(cid:9)
(cid:9)
(cid:9)

ki gi

n(cid:2)

i=1

≤ α
k

n(cid:2)

i=1

αi (cid:8)gi(cid:8)2 ≤ α2

k

(cid:8)gi(cid:8)2

max
i

i.e.

(cid:9)
(cid:9)
(cid:9)
(cid:9)
(cid:9)

n(cid:2)

i=1

αi
α

n(cid:2)

i=1

ki
k

gi

2

(cid:9)
(cid:9)
(cid:9)
(cid:9)
(cid:9)

≤

n(cid:2)

i=1

gi −

αi
k

(cid:8)gi(cid:8)2 ≤ α

k

(cid:8)gi(cid:8)2 .

max
i

14

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Proof. Deﬁne k i.i.d. random variable W 1, W 2, ..., W k such that  P (W 1 = α gi) = αi

α . Let W =

(cid:28)
k
i=1 W i
k

. Therefore,

E[W ] = E[W 1] = f .

Hence, we have

E[(cid:8) f − W (cid:8)2] = 1
k2

E[(cid:16)

( f − W i),

k(cid:2)

( f − W i)(cid:17)]

i=1

k(cid:2)

i=1
k(cid:2)

i=1

= 1

k2 E[

(cid:8) f − W i(cid:8)2]

E[(cid:8) f − W 1(cid:8)2]

(E[(cid:8)W 1(cid:8)2] − (cid:8) f (cid:8)2)

= 1
k
= 1
k
≤ 1
k
n(cid:2)

=

i=1
≤ α2
k

E[(cid:8)W 1(cid:8)2]

αi
kα

· α2 (cid:8)gi(cid:8)2

(cid:8)gi(cid:8)2 .

max
i

Since for a random variable, the minimal value it can take is at most the value of expectation, hence, there must exist a 

(cid:28)
k
i=i W i , and

sequence of k numbers (l1, l2, ..., lk) ∈ {1, 2, ..., n}k, such that W i = α gli , W =

(cid:8)W − f (cid:8)2 ≤ α2
k

(cid:8)gi(cid:8)2 .

max
i

To ﬁnish the proof, we assign integer ki mentioned in the lemma to be

ki =

k(cid:2)

j=1

=i. (cid:2)

1gl j

As Bartlett et al. [17] indicated, the Maurey sparsiﬁcation lemma only discussed the L1 norm case. Zhang [20] generalized 

this lemma to create bounds for non-L1 norm cases, which is also applicable in our proof of Lemma 1.

Proof of Lemma 1. Given the data matrix  Z ∈ Cn×d, re-scaling each column of the matrix  Z and get a matrix  Y ∈ Cn×d, 
where

(cid:9)
(cid:9)
(cid:9) .
(cid:9)Z:; j
Y :; j = Z:; j/
-
Set N = 4dm, k =
a2b2m2/r/(cid:8)2

.

{V 1, V 2, ..., V N } =

(cid:21)
σ Y eieT
j

(cid:21)

∪

σ Y cieT
j

, and α = am1/r(cid:8) X(cid:8)p . To construct an appropriate convex hull, we deﬁne

(cid:22)

: σ ∈ {−1, +1} , i ∈ {1, 2, ..., d} , j ∈ {i, 2, ..., m}
(cid:22)

: σ ∈ {−1, +1} , i ∈ {1, 2, ..., d} , j ∈ {i, 2, ..., m}

and

(cid:6)

(cid:6)

α
k

α
k

C =

=

N(cid:2)

i=1
k(cid:2)

ki V i : ki ≥ 0,

N(cid:2)

i=1

ki = k

/

/

V lm

: (l1, . . . , lm) ∈ [N]k

,

m=1

where ki

(cid:5)=

(cid:28)
k
m=1 1lm=i .

Here,  ei deﬁnes  the  d-dimensional  standard  vector,  e j deﬁnes  the  m-dimensional  standard  vector,  and  ci deﬁnes  the 

d-dimensional vector in which only the ith entry equals 

−1, and other entries equal 0.

√

15

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Because of the way V i deﬁned and  p ≤ 2, we have

max
i

(cid:8)V i(cid:8)

2

≤ max
i

{(cid:8)Y ei(cid:8)

2 , (cid:8)Y ci(cid:8)

2

} = max

i

{(cid:8)Y ei(cid:8)

2

} = max

i

(cid:8) Xei(cid:8)
(cid:8) Xei(cid:8)

2

p

≤ 1.

The ﬁrst equality is due to the deﬁnition of complex-valued vector norms, and the second equality holds because of the 

Next,  it  suﬃces  to  prove  that  C is  a  cover  of 

monotonicity of matrix norm in terms of p.
(cid:9)
(cid:9)
(cid:9)Z A − α
k

by (cid:8) for some (k1, ..., kN ).

N
i=1 ki V i

(cid:9)
(cid:9)
(cid:9)

(cid:28)

2

Deﬁne M ∈ Rd×m where the element of each row j equals 

(cid:9)
(cid:9) Z:; j

(cid:9)
(cid:9)

p

, hence we have

0

Z A : A ∈ Cd×m, (cid:8) A(cid:8)q,s ≤ a

1

.  To  prove  this,  we  desire  to  bound 

Z A = Y (M (cid:19) A),

where (cid:19) represents the Hadamard product.
(cid:25)(cid:9)
(cid:4)(cid:9)
(cid:9)
(cid:9) Z:,1
(cid:9)

(cid:8)M(cid:8)p,r =

(cid:9)
(cid:9) Z:,d

(cid:9)
(cid:9)
(cid:9)
(cid:9)

(cid:9)
(cid:9)

(cid:9)
(cid:9)

p , . . . ,

p

(cid:5)(cid:9)
(cid:9)
(cid:9)

, . . . ,

p

(cid:9)
(cid:9)
(cid:9)

(cid:4)(cid:9)
(cid:9) Z:,1

(cid:9)
(cid:9)

= m1/r

(cid:9)
(cid:9) Z:,d

(cid:9)
(cid:9)

p

(cid:5)(cid:9)
(cid:9)
(cid:9)

p

p , . . . ,

⎞

1/p

⎠

Z

p
i, j

⎛

⎝

d(cid:2)

n(cid:2)

= m1/r

i=1

j=1
= m1/r(cid:8)Z (cid:8)p.

(cid:9)
(cid:9)

(cid:9)
(cid:9)
(cid:9)

(cid:4)(cid:9)
(cid:9) Z:,1
⎛

p , . . . ,

(cid:9)
(cid:9) Z:,d
⎞

1/p

(cid:5)(cid:9)
(cid:9)
(cid:9)

(cid:9)
(cid:9)

p

(cid:26)(cid:9)
(cid:9)
(cid:9)
(cid:9)
r

p

= m1/r

⎝

(cid:9)
(cid:9) Z:, j

(cid:9)
(cid:9)p
p

d(cid:2)

j=1

⎠

Hence, if we denote  S = M (cid:19) A, we have

(cid:8)S(cid:8)1 ≤ (cid:16)M, | A|(cid:17) ≤ (cid:8)M(cid:8)p,r(cid:8) A(cid:8)q,s ≤ m1/r(cid:8)Z (cid:8)pa = α.

We can see that  Z A indeed lies in a convex hull related with {V 1, V 2, ..., V N }:

Z A = Y M

d(cid:2)

m(cid:2)

(cid:4)

(cid:7)

(cid:8)

Re

Mi, j

eie

(cid:10)
j

+ Im

(cid:7)

(cid:8)

Mi, j

cieT
j

(cid:5)

= Y

i=1

j=1

d(cid:2)

m(cid:2)

= (cid:8)M(cid:8)1

(cid:14)

(cid:7)

Re
Mi j
(cid:8)M(cid:8)1

(cid:8)

(cid:4)

(cid:5)

+

Y eie

(cid:10)
j

(cid:8)

(cid:4)

(cid:7)

Im
Mi j
(cid:8)M(cid:8)1

Y cie

(cid:10)
j

(cid:16)

(cid:5)

i=1

j=1
∈ α · conv ({V 1, . . . , V N }) ,

where conv ({V 1, . . . , V N }) denotes the convex hull formed by {V 1, V 2, ..., V N }.

Finally, by Lemma 5, there exist non-negative integers (k1, k2, ..., kN ) such that

(cid:9)
(cid:9)
(cid:9)
(cid:9)

(cid:9) Z A − α

k

N(cid:2)

i=1

ki V i

(cid:9)
(cid:9)
(cid:9)
(cid:9)
(cid:9)

2

2

=

(cid:9)
(cid:9)
(cid:9)
(cid:9)

(cid:9)Y M − α

k

N(cid:2)

i=1

ki V i

(cid:9)
(cid:9)
(cid:9)
(cid:9)
(cid:9)

2

2

(cid:8)V i(cid:8)
2

≤ α2
max
k
i
a2m2/r(cid:8)Z (cid:8)2
p

≤

k

≤ (cid:8)2.

Hence, C is a covering of the desire set. Since the cardinality of set C equals Nk , we have the target inequality:

(cid:4)(cid:21)

ln N

Z A : A ∈ Cd×m, (cid:8) A(cid:8)q,s ≤ a

(cid:22)

(cid:23)

(cid:5)

, (cid:8), (cid:8) · (cid:8)2

≤

(cid:24)

a2b2m2/r
(cid:8)2

ln(4dm). (cid:2)

16

H. Chen, F. He, S. Lei et al.

B.2.  Proof of Lemma 2

Artiﬁcial Intelligence 322 (2023) 103951

(cid:9)
(cid:9)

(cid:9)
(cid:9) Ai

&Z i − &Ai

As  stated  in  the  third  section,  this  lemma  shall  be  proved  by  mathematical  induction.  The  basic  idea  is  as  follows. 
Denotes  Z i to be the data set passing from the i-1th layer to the ith layer ( Z 0 = Z T ). According to Lemma 1, assume that 
ﬁxed speciﬁc layer i, there exists a sequence of covering matrices (&A0, &A1, ..., &Ai−1) for i-1 previous layers, and a covering 
matrix  &Ai such  that 
≤ (cid:8) for  some  (cid:8) > 0.  As  a  consequence,  the  input  data  for  the  i+1th  layer  shall  be 
Z i+1 = σi+1( Ai Z i), and &Z i+1 = σi+1(&Ai
&Z i).
(cid:9)
(cid:9)
(cid:9)
(cid:9) Ai Z i − &Ai
(cid:9)
(cid:9)
&Z i
2
2
(cid:7)(cid:9)
(cid:9)
(cid:9) Ai Z i − Ai
(cid:9)
&Z i
2
(cid:9)
(cid:9)
(cid:9)
(cid:9) Z i − &Z i
2

≤ ρi
≤ ρi
≤ ρi (cid:8) Ai(cid:8)σ

(cid:9)
(cid:9) Z i+1 − &Z i+1

(cid:9)
(cid:9) Ai
+ ρi(cid:8)i.

&Z i − &Ai

&Z i

&Z i

(cid:9)
(cid:9)

+

(cid:8)

2

2

Since the ﬁrst term of the right-hand side part depends on the inductive hypothesis, hence intuitively, we can see that the 
covering  number  upper  bound  depends  on  the  product  of  spectral  norms  of  all  covering  matrices.  The  detailed  proof  is 
illustrated as follows.

spaces are equipped with (cid:8)·(cid:8)
and the ﬁrst layer’s input  Z ∈ V 1 have the constraint: (cid:8) Z (cid:8)

We  ﬁrst  deﬁne  two  sequences  of  vector  space  {V 1, V 2, ..., V L},  and  {W 2, W 3, ..., W L+1}.  The  ﬁrst  sequence  of  vector 
W . For each layer’s input matrix,  Z i ∈ V i , 

V , and the second sequences are equipped with (cid:8)·(cid:8)

≤ B.
Moreover,  under  our  assumptions,  Ai can  be  viewed  as  a  linear  operator:  V i → W i+1,  and  the  norm  of  each  linear 

V

operation is deﬁned as:

(cid:8) Ai(cid:8)

V →W

= sup
| Z |V ≤1

(cid:8) Ai Z (cid:8)

W

= ci.

σi can be treated as a mapping from W i → V i , and the ρi -lipschitz property means

(cid:9)
(cid:9)σi(z) − σi(z

(cid:15)

(cid:9)
(cid:9)
)

≤ ρi

V

(cid:9)
(cid:9)z − z

(cid:9)
(cid:9)

(cid:15)

W .

With these preparations, we claim the following lemma which is based on a similar lemma raised by Bartlett et al. [17].

Lemma 6 ([17]). Assume that a sequence of positive numbers ((cid:8)1, . . . , (cid:8)L), along with Lipschitz non-linear mappings (σ1, . . . , σL)
(where σi is ρi - Lipschitz), and linear operator norm bounds (c1, . . . , cL) as described above are given. Suppose the sequence of 
matrices A = ( A1, . . . , A L) lies within B1 ×· · ·×BL where Bi are classes satisfying the property that each Ai ∈ Bi has (cid:8) Ai(cid:8)
≤ ci . 
l= j+1 ρlcl, the complex-valued neural network images F :=
Let data  Z be given with (cid:8)Z (cid:8)
{FA(Z ) : A ∈ B1 × · · · × BL, (cid:8)Z (cid:8)

≤ B} has the following covering number bound

≤ B. Then, deﬁne τ :=

j≤L (cid:8) jρ j

V →W

(cid:28)

(cid:27)

V

L

V

N (F, τ , (cid:8)·(cid:8)

V ) ≤

L(cid:15)

i=1

(cid:7)

sup
A1,..., Ai−1
∈B
∀ j<i. A j
j

(cid:8)

(cid:4)(cid:21)

N

(cid:7)

Ai F

A1,..., Ai−1

(cid:8)(Z ) : Ai ∈ Bi

, (cid:8)i, (cid:8) · (cid:8)W

.

(cid:22)

(cid:5)

Proof. The lemma is proved by Mathematical induction.

A sequence of covering set {F1, F2, ..., FL} is constructed where Fi covers W i .
Base case: When i=1, we have F1 to be constructed according to Lemma 1, and

|F1| ≤ N ({ A1 Z : A1 ∈ B1} , (cid:8)1, (cid:8) · (cid:8)W ) =: N1.

Inductive Hypothesis: Assume that for i=n, we can ﬁnd a (cid:8)n -covering Fn for set 

0

AnFA1,...,An−1 (Z ) : An ∈ Bn

1

such that:

|Fn| ≤

n(cid:15)

l=1

Nl.

Induction Step: For every element  F ∈ Fn, construct an (cid:8)n+1-cover Gn+1(F ) of

{ An+1σn(F ) : An+1 ∈ Bn+1} .

Since these covers are proper, meaning  F = An+1 F ( A1,..., An)(Z ) for some matrices ( A1, . . . , An) ∈ B1 × · · · × Bn, it follows 

that

(cid:7)0

N

An+1 F A1,..., An (Z ) : An+1 ∈ Bn+1

1

(cid:8)

, (cid:8)n+1, (cid:8) · (cid:8)W

|Gn+1(F )| ≤ sup
A1,..., An
∈B

(cid:7)

(cid:8)

∀ j≤i. A j
=: Nn+1.

j

17

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Lastly, we can form the cover
2

Fn+1 :=

Gn+1(F ),

F ∈Fn

whose cardinality satisﬁes

|Fn+1| ≤ |Fn| · Nn+1 ≤

n+1(cid:15)

l=1

Nl.

Deﬁne H := {σL(F ) : F ∈ FL}. It’s trivial to see that the cardinality of H is the same as FL . Then, it suﬃces to show that 

H is indeed a covering of F . If we ﬁx any ( A1, . . . , A L) satisfying the constraints, then recursively, we denote

F 1 = A1 Z ∈ W 2, G i = σi (F i) ∈ V i+1

F i+1 = Ai+1G i ∈ W i+2.

In other words, we need to prove that there exist &G L ∈ H such that 

(cid:9)
(cid:9)G L − &G L

(cid:9)
(cid:9)

V

≤ τ .

(cid:9)
(cid:9) Ai

&G i−1 − &F i

(cid:9)
(cid:9)

W

≤ (cid:8)i,  and set &G i := σi

(cid:8)

(cid:7)
&F i

.

Base case:  Set &G 0 = Z .
Inductive hypothesis:  Choose &F i ∈ Fi with
Induction Step:
(cid:9)
(cid:9)G i+1 − &G i+1

(cid:9)
(cid:9)

V

≤ ρi+1
≤ ρi+1
≤ ρi+1 (cid:8) Ai+1(cid:8)
⎛

(cid:9)
(cid:9)
(cid:9)F i+1 − &F i+1
(cid:9)
W
(cid:9)
(cid:9)
(cid:9)F i+1 − Ai+1
(cid:9)
&G i
(cid:9)
(cid:9)G i − &G i
i(cid:15)

V →W

(cid:2)

W

≤ ρi+1ci+1

⎝

(cid:8) jρ j

ρlcl

l= j+1

j≤i
i+1(cid:15)

(cid:2)

=

(cid:8) jρ j

ρlcl

&G i − &F i+1 |

(cid:9)
(cid:9)

W

+ ρi+1
(cid:9)
(cid:9)

(cid:9)
(cid:9) Ai+1
+ ρi+1(cid:8)i+1
V
⎞

⎠ + ρi+1(cid:8)i+1

j≤i+1

l= j+1

= γ .

Hence, we get proved. (cid:2)

To prove Lemma 2, the key idea is to apply the result of Lemma 1 and Lemma 6.

Proof of Lemma 2. To begin with, we assume the same setting as above. However, to prove Lemma 2, (cid:8)·(cid:8)
and the operator norm is set to the spectral norm, i.e. (cid:8) Ai(cid:8)
deﬁned as

2, 
= (cid:8) Ai(cid:8)σ . Also, the sequence of number {(cid:8)1, (cid:8)2, ..., (cid:8)L} are 

= (cid:8)·(cid:8)

= (cid:8)·(cid:8)

V →W

W

V

(cid:8)i :=

(cid:27)

αi(cid:8)
j>i ρ j s j

ρi

where αi := 1
¯α

(cid:26)

2/3

(cid:25)

bi
si

,

¯α :=

(cid:25)

L(cid:2)

j=1

(cid:26)

2/3

.

b j
s j

By this setting, we ﬁnd that the γ deﬁned in Lemma 6 satisﬁes

(cid:2)

L(cid:15)

(cid:8) jρ j

τ ≤

ρlsl =

j≤L

l= j+1

(cid:2)

j≤L

α j(cid:8) = (cid:8).

Then

(cid:7)
F|S , (cid:8), (cid:8) · (cid:8)2

(cid:8)

ln N

≤

≤

L(cid:2)

i=1

L(cid:2)

i=1

(cid:4)(cid:21)

ln N

(cid:7)

Ai F

A1,..., Ai−1

(cid:5)

(cid:4)

(cid:8)

(cid:10)

Z

(cid:22)

: Ai ∈ Bi

, (cid:8)i, (cid:8) · (cid:8)2

(cid:7)

sup
A1,..., Ai−1
∈B
∀ j<i. A j
j

(cid:8)

(cid:7)

sup
A1,..., Ai−1
∈B
∀ j<i, A j
j

(cid:8)

(cid:25)3

ln N

(cid:7)

F

A1,..., Ai−1

(cid:5)(cid:10)

(cid:4)

(cid:8)

(cid:10)

Z

(cid:10) :

( Ai)

(cid:9)
(cid:9)
(cid:9) A

(cid:9)
(cid:9)
(cid:9)

(cid:10)
i

2,1

18

(cid:5)

4

(cid:26)

≤ bi

, (cid:8)i, (cid:8) · (cid:8)2

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

≤

L(cid:2)

i=1

(cid:7)

sup
A1,..., Ai−1
∈B
∀ j<i. A j
j

(cid:8)

(cid:9)
(cid:9)
(cid:9)F

(cid:7)

b2
i

A1,..., Ai−1
(cid:8)2
i

(cid:8)(cid:10)

(cid:7)

(cid:8)

(cid:10)

Z

(cid:9)
(cid:9)
2
(cid:9)
2

(cid:4)

(cid:5)

ln

4W 2

.

The ﬁrst equality holds because we use  L2 norms here. Hence the covering number for a matrix and its transpose are 
(cid:9)
(cid:9)
(cid:9)F

the same. To further simplify the formula, we can upper bound 

(cid:8)(cid:10)

(cid:9)
(cid:9)
(cid:9)

by

(cid:10)

Z

(cid:7)

2

(cid:7)

(cid:8)

A1,..., Ai−1

2

(cid:9)
(cid:9)
(cid:9)
(cid:9)F

(cid:7)

A1,..., Ai−1

(cid:5)(cid:10)

(cid:4)

(cid:8)

(cid:10)

Z

(cid:9)
(cid:9)
(cid:9)F

(cid:7)

=

(cid:9)
(cid:9)
(cid:9)
(cid:9)
2

(cid:4)

(cid:8)

(cid:10)

Z

(cid:5)(cid:9)
(cid:9)
(cid:9)
2

(cid:4)

A1,..., Ai−1
(cid:4)

(cid:5)

(cid:10)

Z

− σi−1(0)(cid:8)2

Ai−1 F

= (cid:8)σi−1
(cid:9)
(cid:9)
(cid:9) Ai−1 F

≤ ρi−1

(cid:7)

A1,..., Ai−2

(cid:8)

(cid:4)

(cid:7)

(cid:8)

A1,..., Ai−2
(cid:9)
(cid:9)
(cid:9)F

(cid:7)

A1,..., Ai−2

(cid:5)

(cid:10)

X

(cid:4)

(cid:8)

(cid:9)
(cid:9)
− 0
(cid:9)
2
(cid:5)(cid:9)
(cid:9)
(cid:9)

(cid:10)

Z

.

2

≤ ρi−1 (cid:8) Ai−1(cid:8)

σ

Inductively, we have

(cid:9)
(cid:9)
(cid:9)
(cid:9)F

(cid:7)

max
j

A1,..., Ai−1

(cid:5)(cid:10)

(cid:4)

(cid:8)

(cid:10)

Z

e j

(cid:9)
(cid:9)
(cid:9)
(cid:9)

2

≤ (cid:8)Z (cid:8)2

(cid:9)
(cid:9)

(cid:9)
(cid:9) A j

ρ j

σ .

i−1(cid:15)

j=1

Finally, we obtain

(cid:7)

ln N

F|S , (cid:8), (cid:8) · (cid:8)2

L(cid:2)

(cid:8)

≤

(cid:27)

b2
i

(cid:8)Z (cid:8)2
2

j<i ρ2
j
(cid:8)2
i

(cid:9)
(cid:9) A j

(cid:9)
(cid:9)2
σ

(cid:4)

(cid:5)

ln

4W 2

(cid:8)

(cid:7)

sup
A1,..., Ai−1
∈B
∀ j<i. A j
j
(cid:27)

b2
i B2

i=1

L(cid:2)

i=1

(cid:7)

(cid:7)

B2 ln

B2 ln

≤

=

=

(cid:4)

(cid:5)

ln

4W 2

j s2

j

j<i ρ2
(cid:8)2
i
(cid:8) (cid:27)

4W 2

L
j=1 ρ2

j s2

j

(cid:8)2
(cid:8) (cid:27)

4W 2

(cid:8)2

L
j=1 ρ2

j s2

j

L(cid:2)

i=1
(cid:4)

b2
i
i s2
α2

i

(cid:5)

¯α3

. (cid:2)

B.3.  Proof of Theorem 1

As  stated  in  the  third  section,  the  main  theorem  we  used  to  prove  Theorem 1 is  the  Dudley  Entropy  Integral.  The 

standard Dudley Entropy Integral introduces a method to obtain Rademacher complexity bound via covering number [16].

Theorem 5 ([16]). Let F be a real-valued function class taking values in [0, 1], and assume that 0 ∈ F . Then

(cid:8)

(cid:7)

R

F|S

≤ inf
α>0

⎛

⎜
⎝

4α√
n

+ 12
n

√
n6

(cid:29)

α

(cid:7)

log N

F|S , ε, (cid:8) · (cid:8)2

⎞

(cid:8)
dε.

⎟
⎠

Proof. [17] Let  N ∈ N be  arbitrary  and  let εi =
N

(cid:7)
F|S , εi, (cid:8) · (cid:8)2

, so that

(cid:8)

√

n2

−(i−1) for  each  i ∈ [N].  For  each  i,  let  V i denote  the  cover  achieving 

∀ f ∈ F ∃v ∈ V i

(cid:16)

1/2

( f (xt) − vt)2

≤ εi,

(cid:14)

n(cid:2)

t=1

and |V i| = N

(cid:7)
F|S , εi, (cid:8) · (cid:8)2

(cid:8)

. For a ﬁxed  f ∈ F , let v i[ f ] denote the nearest element in V i . Then

19

H. Chen, F. He, S. Lei et al.
(cid:14)

n(cid:2)

(cid:16)

Artiﬁcial Intelligence 322 (2023) 103951

E

sup
f ∈F
(cid:14)

εi f (xt)

t=1
$

n(cid:2)

= E

≤ E

+ E

(cid:14)

(cid:14)

sup
f ∈F

sup
f ∈F

sup
f ∈F

$

t=1
n(cid:2)

$

t=1
n(cid:2)

t=1

(cid:4)

(cid:8)t

f (xt) − v N
t

(cid:5)
[ f ]

+

%(cid:16)

(cid:5)
[ f ]

(cid:4)

(cid:8)t

f (xt) − v N
t
%(cid:16)

(cid:8)t v 1
t

[ f ]

.

N−1(cid:2)

n(cid:2)

(cid:4)

(cid:8)t

(cid:14)

i=1

t=1
N−1(cid:2)

+

E

sup
f ∈F

i=1

v i
t

[ f ] − v i+1

t

$

n(cid:2)

t=1

(cid:4)

(cid:8)t

(cid:5)
[ f ]

−

n(cid:2)

t=1

v i
t

[ f ] − v i+1

t

%(cid:16)

(cid:8)t v 1
t

[ f ]

%(cid:16)

(cid:5)
[ f ]

For the third term, observe that it suﬃces to take V 1 = {0}, which implies

⎡

$

n(cid:2)

(cid:2)

f ∈F

t=1

⎣

E sup
(cid:8)

%

⎤

(cid:8)t v 1
t

[ f ]

= 0

⎦ .

The ﬁrst term may be handled using Cauchy-Schwarz as follows:

⎡

$

n(cid:2)

(cid:2)

(cid:4)

(cid:8)t

f (xt) − v N
t

(cid:5)
[ f ]

%⎤
⎦

f ∈F

((cid:8)t)2

t=1
8
9
9
:

sup
f ∈F

n(cid:2)

(cid:7)

t=1

f (xt) − v N
t

(cid:8)
[ f ]

2

⎣

E sup
(cid:8)

8
9
9
:

≤

E

n(cid:2)

√

t=1
nεN .

≤

Last to take care of are the terms of the form

$

n(cid:2)

(cid:4)

(cid:8)t

t=1

E sup
(cid:8)

v i
t

[ f ] − v i+1

t

(cid:5)
[ f ]

%

.

0

1

For each i, let W i =
⎡

$

n(cid:2)

(cid:2)

⎣

f ∈F

t=1

E sup
(cid:8)

v i[ f ] − v i+1[ f ] | f ∈ F
%⎤
⎦ ≤ E sup
w∈W i

[ f ] − v i+1

(cid:5)
[ f ]

v i
t

(cid:4)

t

(cid:8)t

%

(cid:8)t wt

,

$

n(cid:2)

t=1

. Then |W i| ≤ |V i| |V i+1| ≤ |V i+1|2,

and furthermore

8
9
9
:

sup
w∈W i

n(cid:2)

t=1

w 2
t

= sup
f ∈F

(cid:9)
(cid:9)
(cid:9)
(cid:9)
(cid:9)v i[ f ] − v i+1[ f ]
(cid:9)

2

(cid:9)
(cid:9)
(cid:9)
(cid:9)
≤ sup
(cid:9)v i[ f ] − ( f (x1) , . . . , f (xn))
(cid:9)
f ∈F
(cid:9)
(cid:9)
(cid:9)
(cid:9)
(cid:9)( f (x1) , . . . , f (xn)) − v i+1[ f ]
(cid:9)

2

2

+ sup
f ∈F
≤ εi + εi+1
= 3εi+1.

With this observation, the standard Massart ﬁnite class lemma [16] implies

$

n(cid:2)

%

sup
w∈W i

E
(cid:8)
8
9
9
:

≤

2 sup
w∈W i

(cid:8)t wt

t=1

n(cid:2)

t=1

(wt)2 log |W i| ≤ 3

(cid:13)

2 log |W i|εi+1 ≤ 6

(cid:13)

log |V i+1|εi+1.

20

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Collecting all terms, establishes

E(cid:8) sup
f ∈F

n(cid:2)

t=1

(cid:8)t f (xt) ≤ εN

N−1(cid:2)

√

n + 6

(cid:29)

εi+1

log N

(cid:7)

F|S , εi+1, (cid:8) · (cid:8)2

(cid:8)

i=1
N(cid:2)

√

≤ εN

n + 12

(cid:29)

(εi − εi+1)

log N

(cid:7)

F|S , εi, (cid:8) · (cid:8)2

(cid:8)

i=1
√
n6

(cid:29)

√

n + 12

≤ εN

εN+1

(cid:7)

log N

F|S , ε, (cid:8) · (cid:8)2

(cid:8)
dε.

Finally, select any α > 0 and take N be the largest integer with εN+1 > α. Then εN = 4εN+2 < 4α, and so

√
n6

(cid:29)

√

n + 12

εN

(cid:7)

log N

F|S , ε, (cid:8) · (cid:8)2

(cid:8)
dε

√

≤4α

n + 12

εN+1
√
n6

(cid:29)

log N

α

(cid:7)

F|S , ε, (cid:8) · (cid:8)2

(cid:8)
dε. (cid:2)

However,  it’s  worth  noticing  that  N (F , (cid:8), (cid:8)·(cid:8)

2) can  not  be  directly  used  in  Theorem 3 to  obtain  the  upper  bound  of 

ˆRS (G). Hence we raise Lemma 3 and Lemma 4 to make it applicable. We shall ﬁrst prove these two lemmas.

Proof of Lemma 3. Consider  H ⊂ F is  a  cover  of  family  F which  satisﬁes  that  the  cardinality  of  H equals  the  covering 
number of F . Then for any FA ∈ F , we have a corresponding h ∈ H such that

(cid:8)FA(Z ) − h(Z )(cid:8)
2

≤ (cid:8).

Then consider (cid:8)FA(Z ) − Y (cid:8)

∈ G, we have

2

|(cid:8)FA(Z ) − Y (cid:8)
2

− (cid:8)h(Z ) − Y (cid:8)

2

| ≤ |(cid:8)FA(Z ) − Y − h(Z ) + Y (cid:8)
2
= |(cid:8)FA(Z ) − h(Z )(cid:8)
≤ (cid:8).

|

2

|

Therefore, it’s trivial that  ¯H = {(cid:8)h(Z ) − Y (cid:8)
Hence, the covering number of G is less than the covering number of F . (cid:2)

: h ∈ H} is a cover of G, and the cardinality of H equals that of  ¯H.

2

Proof of Lemma 4. Consider  H ⊂ G is  a  cover  of  family  G which  satisﬁes  that  the  cardinality  of  H equals  the  covering 
number of G. For any  g ∈ G, there exist h ∈ H such that

(cid:8)α g − αh(cid:8)
2

= α (cid:8)g − h(cid:8)
2
Therefore, αH is a cover of αG.

≤ α(cid:8).

Vice versa, if αH is a cover of αG, then H is a cover of G.
Hence, Lemma 4 is get proved. (cid:2)

After all preparations have been done, the proof of Theorem 1 is given as follows:
(cid:21)

Proof of Theorem 1. Consider family F =

FA (z) : A = ( A1, . . . , A L) , (cid:8) Ai(cid:8)σ ≤ si,

G = {(z, y) (cid:12)→ l (F A(z), y) : F A ∈ F} .

As a consequence of Lemma 3,

(cid:7)

N

F|S , (cid:8), (cid:8) · (cid:8)2

(cid:8)

≥ N

(cid:7)

G|S , (cid:8), (cid:8) · (cid:8)2

(cid:8)

(cid:9)
(cid:9) A

(cid:22)

(cid:9)
(cid:9)

≤ bi

2,1

, and family

(cid:10)
i

when the loss function is set to be l(FA(z), y) = (cid:8)FA(z) − y(cid:8)
2. Since in the standard Dudley Entropy Integral, it requires 
the  value  of  loss  function  to  be  always  located  in  the  interval  [0, 1],  and  we  make  the  assumption  that l(FA(z), y) ≤ M
always holds for the given data set, hence, we can rescale the loss function by  1
M .

21

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Deﬁne  ¯G =

0

(z, y) (cid:12)→ 1

N (G, M(cid:8), (cid:8) · (cid:8)2) = N

M l (FA(z), y) : FA ∈ F
¯G, (cid:8), (cid:8) · (cid:8)2

(cid:7)

(cid:8)

.

1

, then Lemma 4 indicates

Therefore

(cid:7)

N

ln N

≤

¯G|S , (cid:8), (cid:8) · (cid:8)2
(cid:7)
¯G|S , (cid:8), (cid:8) · (cid:8)2
(cid:8)
(cid:7)

(cid:8)Z (cid:8)2

4W 2

2 ln
M2(cid:8)2

(cid:8)

(cid:7)

≤ N
(cid:8)

F|S , M(cid:8), (cid:8) · (cid:8)2

(cid:7)

(cid:8)

(cid:8)

,

≤ ln N
⎛

L(cid:15)

⎝

j=1

⎠

j ρ2
s2

j

F|S , M(cid:8), (cid:8) · (cid:8)2
⎞
(cid:26)

(cid:14)

(cid:25)

L(cid:2)

2/3

(cid:16)3

.

bi
si
(cid:5) (cid:25)

i=1

L
j=1 s2

j ρ2

j

If we denote  R = (cid:8)Z (cid:8)2

2 ln
As stated in Theorem 3,

(cid:8) (cid:4)(cid:27)

(cid:7)
4W 2

ˆRS ( ¯G) = E
σ

[sup
¯g∈ ¯G

= E
σ

[sup
g∈G

1

n

1

n

n(cid:2)

i=1
n(cid:2)

i=1

σi

¯g(zi)]

σi

1

M

g(zi)]

(cid:28)

(cid:26)
3

2/3

(cid:4)

(cid:5)

L
i=1

bi
si

, then we have ln N

(cid:7)
¯G|S , (cid:8), (cid:8) · (cid:8)2

(cid:8)

≤ R

M2(cid:8)2 .

(cid:7)

ln N

¯G|S , ε, (cid:8) · (cid:8)2

⎞

(cid:8)
dε

⎟
⎠

= 1
M

ˆRS (G)
⎛

⎜
⎝

⎛

⎜
⎝

(cid:14)

≤ inf
α>0

≤ inf
α>0

= inf
α>0

4α√
n

4α√
n

4α√
n

+ 12
n

√
n6

(cid:29)

α
√
n6

(

⎞

⎟
⎠

+ 12
n

α
√

+ 12

R

Mn

ln

dε

R
M2(cid:8)2
(cid:16)

√
n
α

.

To make the upper bound neater, we make a simple choice at α = 1

n , hence,

ˆRS (G) ≤ 4M
n3/2

√

+ 18 (cid:8) Z (cid:8)

2

2 ln(2W ) ln nRA

n

.

Plugging this upper bound into Theorem 2, the desired result can be obtained. (cid:2)

Appendix C.  PAC learnability of complex-valued neural networks

In this section, we desire to present the proof which shows that complex-valued neural networks are PAC-learnable.
We denote  f S to be the empirical error minimizer, i.e.,  f S = arg
f ∈F

l ( f (zi), yi). Similarly,  f

min 1
n

is the expected error 

n(cid:28)

i=1

,

l ( f (zi), yi)

.  R( f ) and &R( f ) respectively represents the expected error and the empirical 

minimizer:  f = arg
f ∈F

error.

+
min E 

1
n

n(cid:28)

i=1

The concept of PAC-learnable is deﬁned as follows.

Deﬁnition 1 (PAC-learnable). Let F be a hypothesis set. A is a PAC-learnable algorithm if there exists a polynomial function 
poly  (·, ·, ·, ·) such  that  for  any  (cid:8) > 0 and  δ > 0,  for  all  distributions  D over  Z ,  the  following  holds  for  any  sample  size 
m ≥ poly(1/(cid:8), 1/δ, n, size (c)):

P
S∼Dm

[R ( f S ) − R( f ) ≤ (cid:8)] ≥ 1 − δ.

Here  f and  f S are deﬁned above.

22

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Corollary 1. Deﬁne the loss function to be l(FA(z), y) = ||FA(z) − y||2, and is upper-bounded by a constant M. For a complex-valued 
neural network: FA(z) := σL ( A LσL−1 ( A L−1 · · · σ1 ( A1z))), where activation functions σi are ρi -Lipschitz, it is PAC-learnable.

Proof. It suﬃces to prove that  R( f S ) − R( f ) ≤ (cid:8) via the generalization upper bound under high probability.

Since

R( f S ) − R( f ) = R( f S ) − &R( f S ) + &R( f S ) − R( f )
≤ R( f S ) − &R( f S ) + &R( f ) − R( f ),

the last inequality holds because  f S is the empirical error minimizer, therefore, we have

|R( f S ) − R( f )| ≤
≤

(cid:3)
(cid:3)
(cid:3)R( f S ) − &R( f S ) + &R( f ) − R( f )
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3) +
(cid:3)R( f S ) − &R( f S )
(cid:3)&R( f ) − R( f )
(cid:3)
(cid:3)
(cid:3) .
(cid:3)&R( f ) − R( f )

≤ 2 sup
f ∈F

Hence

P (|R( f S ) − R( f )| ≤ (cid:8)) ≥ P

(cid:14)

(cid:3)
(cid:3)
(cid:3) ≤ (cid:8)
(cid:3)&R( f ) − R( f )
2

sup
f ∈F

(cid:16)

.

As in Theorem 1, we have for any  f ∈ F

⎛

⎝

P

(cid:3)
(cid:3)
(cid:3) ≤ 8M
(cid:3)R( f ) − &R( f )
n

3
2

≥1 − δ.

√

+ 36|| Z ||2

2ln(2W )ln(n)RA

n

(cid:12)

⎞

⎠

ln 2
δ
2n

+ 3M

Notice that, these two statements are equivalent:
(cid:3)
(cid:3)
(cid:3) ≤ (cid:8)
(cid:3)&R( f ) − R( f )
2
(cid:3)
(cid:3)
(cid:3) ≤ (cid:8)
(cid:3)R( f ) − &R( f )
2

⇔∀ f ∈ F,

sup
f ∈F

.

Hence, we can claim that if

√

+ 36|| Z ||2

(cid:8)
2

≥ 8M
n

3
2

2ln(2W )ln(n)RA

n

(cid:12)

ln 2
δ
2n

,

+ 3M

then

(cid:14)

P

sup
f ∈F

(cid:3)
(cid:3)
(cid:3) ≤ (cid:8)
(cid:3)&R( f ) − R( f )
2

(cid:16)

≥ 1 − δ

i.e.

P (|R( f S ) − R( f )| ≤ (cid:8)) ≥ 1 − δ.
Hence, we can get the conclusion that if

⎛

n ≥ 8
(cid:8)3

⎝8M + 36 (cid:8)Z (cid:8)

2

(cid:13)

2 ln (2W )RA + 3M

(cid:12)

⎞
3
⎠

,

ln 2
δ

2

then

P (|R( f S ) − R( f )| ≤ (cid:8)) ≥ 1 − δ.

Therefore, PAC-learnability of complex-valued neural networks get proved. (cid:2)

Appendix D.  Generalization of sequential data

In this section, we aim at proving Theorem 2. Theorem 2 shows an extension of generalization to sequential data case. 

Therefore, sequential analogues of complexities [19] are presented in this section to complete the proof.

23

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

D.1.  Sequential Rademacher complexity

In the case of classical complexity measure, we use the expectation of the supremum of Rademacher process to deﬁne 
the Rademacher complexity. In the sequential Rademacher case, the intuition is quite similar. Rakhlin et al. [19] illustrated 
a  binary  tree  process  to  be  the  analogue  of  Rademacher  process,  which  coincides  with  Rademacher  process  under  i.i.d.
assumption, but behaves differently in general. The notion of a tree is deﬁned as following:

“A Z -valued tree z of depth n is a rooted complete binary tree with nodes labelled by elements of Z . We identify the 
tree  z with  the  sequence  (z1, . . . , zn) of  labelling functions  zi : {±1}i−1 (cid:12)→ Z which  provide  the  labels  for  each  node. 
Here,  z1 ∈ Z is the label for the root of the tree, while  zi for  i > 1 is the label of the node obtained by following the 
path  of  length  i − 1 from  the  root,  with  +1 indicating  ‘right’  and  −1 indicating  ‘left’.  A  path  of  length  n is  given  by 
the sequence (cid:8) = ((cid:8)1, . . . , (cid:8)n) ∈ {±1}n. For brevity, we shall often write zt((cid:8)), but it is understood that zt only depends 
only on the preﬁx ((cid:8)1, . . . , (cid:8)t−1) of (cid:8). Given a tree  z and a function  f : Z (cid:12)→ R, we deﬁne the composition  f ◦ z as a 
real-valued tree given by the labelling functions ( f ◦ z1, . . . , f ◦ zn).” [19]

Therefore, the deﬁnition of sequential Rademacher complexity is stated in Deﬁnition 2.

Deﬁnition 2 ([19]). For a Z -valued tree z with depth n, then the sequential Rademacher complexity of a function class

Gsq := {(zt, yt ) (cid:12)→ l (FA(z), y) : FA ∈ F } is deﬁned as follows:

$

1

n

sup
f ∈F

n(cid:2)

t=1

%

(cid:8)tl ( f (zt((cid:8))) , yt)

,

Rsq

n (Gsq, z) = E

and

Rsq

n (Gsq) = sup
z

Rsq

n (Gsq, z).

Here (cid:8)t is the Rademacher variables taking value from {+1, −1} with equal probability.

D.2.  Sequential Rademacher complexity generalization bound

When investigating the relation between generalization error and Rademacher complexity, we have the following theo-

rem.

Theorem 6. Given function class F , sample S = {(z1, y1), (z2, y2), ..., (zn, yn)} where (zi, yi ) are i.i.d. data points, we have

%

$

%

$

1

n

n(cid:2)

i=1

E

sup
f ∈F

f (zi, yi) − E [ f ]

≤ E

f (zi, yi) − E [ f ]

≤ 2R(F)

1

n

sup
f ∈F

n(cid:2)

i=1

where R(F ) = E[&RS (F )].

For sequential Rademacher complexity, Rakhlin et al. [19] proved a similar theorem.

Theorem 7. Given function class F , sample S = {(z1, y1), (z2, y2), ..., (zn, yn)} where (z1, y1) are sequential data points, then the 
following inequality holds:

$

E

1

n

sup
f ∈F

n(cid:2)

t=1

(E [ f (zt, yt) | At−1] − f (zt, yt))

≤ 2Rsq

n (F)

%

where Rsq

n (F ) denotes the sequential Rademacher complexity.

If the function class F is bounded, i.e. for any  f ∈F , (cid:8) f (cid:8)∞ ≤M, then the generalization error  1
n
is sharply concentrated around its expectation, which leads to Corollary 2.

(cid:8)

− f (zt, yt)

(cid:28)
n
t=1

(cid:7)
E [ f (zt, yt) | At−1]

Corollary 2. Assume that for the target function class, any  f ∈ F , we have (cid:8) f (cid:8)∞ ≤ M. Given sample  S = {(z1, y1), (z2, y2), ...,
(zn, yn)} where (z1, y1) are sequential data points, then under probability at least 1 − δ the following inequality holds:

(cid:12)

1

n

n(cid:2)

t=1

(E [ f (zt, yt) | At−1] − f (zt, yt)) ≤ 2Rsq

n (F) + M

log 2
δ

2n

.

24

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Proof. This corollary is a consequence of McDiarmid’s Inequality and Theorem 6. By McDiarmid’s Inequality, since (cid:8) f (cid:8)∞ ≤
M, we have

P (|(cid:5)(F) − E[(cid:5)(F)]| ≥ t) ≤ 2 exp
(cid:7)
E [ f (zt, yt) | At−1] − f (zt, yt)

(cid:28)
n
t=1

where (cid:5)(F ) = 1
n
plexity upper bound. (cid:2)

(cid:4)
−2nt2/M2

(cid:5)

(cid:8)

. Then by Theorem 6, we can get the sequential Rademacher com-

As a consequence of Corollary 1, it’s necessary to bound the sequential Rademacher complexity if we want to prove the 

generalization upper bound. This leads to the introduction of sequential Dudley Entropy Integral.

D.3.  Sequential Dudley entropy integral

Before stating the sequential Dudley Entropy Integral, we ﬁrst present the deﬁnition of sequential covering number [19]

Deﬁnition 3 (Sequential covering number). A set C is a sequential α-cover (with respect to (cid:11)p -norm) of F ⊆ RZ
of depth n if

on a tree z

∀ f ∈ F, ∀(cid:8) ∈ {±1}n, ∃c ∈ C

s.t.

|ct((cid:8)) − f (zt((cid:8)))|p

≤ α.

(cid:14)

1

n

n(cid:2)

t=1

(cid:16)

1/p

The sequential covering number of a function class F on a given tree z is deﬁned as

N sq

p (α, F, z) = min
p (α, F , n) := supz

and N sq

0

1
|C| : C is an α-cover w.r.t. (cid:11)p-norm of F on z
p (α, F , z).

N sq

,

Rakhlin et al. [19] provides the sequential version Dudley Entropy Integral as follows:

Theorem 8 (Sequential Dudley Entropy Integral). For p ≥ 2, the sequential Rademacher complexity of a function class F ⊆ [−1, 1]Z
on a Z -valued tree of depth n satisﬁes
16

(cid:29)

⎧
⎨

Rsq

n (F) ≤ inf
α

⎩4α + 12√

n

log N sq

2 (δ, F, n)dδ

α

⎫
⎬
⎭ .

Notice that for the classical α-cover of F with regard to l2 norm, denote it by  V , we have for any given data matrix  Z , 

and for any FA ∈ F , there exist v ∈ V such that

(cid:8)FA(Z ) − v(Z )(cid:8)
2

≤ α.

Since given a set of sequential data, (cid:8)FA(Z ) − v(Z )(cid:8)

=

2

(cid:12)

n(cid:28)

t=1

(FA(zt ) − v(zt ))2.

Hence, if

(cid:8)FA(Z ) − v(Z )(cid:8)
2

≤ α,

then we have
8
9
9
: 1
n

n(cid:2)

t=1

(FA(zt) − v(zt))2 ≤ α√

n

.

Hence, as a consequence of Lemma 2,

ln N sq
2 (

α√
n

, Gsq, n) ≤ ln N (G, α, (cid:8)·(cid:8)

2) ≤

(cid:8)Z (cid:8)

2 ln 4W 2
α2

sq
A,

R

where  Gsq denotes  the  loss  function  family  of  the  sequential  data  set,  R
under the case of sequential data set, and G denotes the loss function family of the i.i.d. data set.

sq
A denotes  the  spectral  complexity  of  the  CVNNs 

Hence, we have

(cid:7)

ln N sq
2

α, Gsq, n

(cid:8)

≤

(cid:8)Z (cid:8)2 ln 4W 2
nα2

sq
A.

R

25

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

Supplementary Table E.2
Detailed model architectures for different datasets.

MNIST/FashionMNIST/CIFAR-10/CIFAR-100
5 × 5, 10
maxpool, 2 × 2
5 × 5, 20
maxpool, 2 × 2
fc-500

Tiny ImageNet
5 × 5, 10
maxpool, 2 × 2
(5 × 5, 20) × 2
maxpool, 2 × 2
fc-500

IMDB

fc-500

fc-200

abs
fc-10/100, softmax

abs
fc-200, softmax

abs
fc-2, softmax

D.4.  Proof of Theorem 2

As a consequence of the previous Sections D.1-D.3, we have

(cid:7)

ln N sq
2

(cid:8), Gsq, n

(cid:8)

≤

(cid:8)Z (cid:8)2 ln 4W 2
n(cid:8)2

sq
A

R

for any (cid:8) ∈ R+

.

Therefore, by Lemma 4 and the sequential Dudley Entropy Integral, we can derive the following bound for the sequential 

Rademacher complexity:

n (Gsq) ≤ 4M
Rsq
n

+ 12

√

ln n

RA

Mn

where M denotes the upper bound for the loss function.

After plugging the above inequality into Corollary 2, we can get the desired bound stated in Theorem 2.

Appendix E.  Additional experiments details

The section provides all the additional details of our experiments. The code is available at https://github .com /LeavesLei /

cvnn _generalization.

E.1.  Datasets

Our experiments are conducted on six datasets: MNIST [25], FashionMNIST [26], CIFAR-10, CIFAR-100, [27], IMDB [28], 

and Tiny ImageNet [29]. The details of these datasets are shown as follows.

• MNIST consists of 60, 000 training images and 10, 000 test images from 10 different classes. It can be downloaded from 

http://yann .lecun .com /exdb /mnist/.

• FashionMNIST consists of  60, 000 training images and  10, 000 test images from  10 different classes. It can be down-

loaded from https://github .com /zalandoresearch /fashion -mnist.

• CIFAR-10 consists of 50, 000 training images and 10, 000 test images from 10 different classes, and CIFAR-100 has the 
same data as CIFAR-10 while images in CIFAR-100 belong to 100 classes. CIFAR-10 and CIFAR-100 can be downloaded 
from https://www.cs .toronto .edu /~kriz /cifar.html.

• IMDB is  a  movie  reviews  sentiment  classiﬁcation  dataset,  in  which  each  of  training  and  test  sets  consists  of  25, 000

movie reviews from 2 different classes. It can be downloaded from http://ai .stanford .edu /~amaas /data /sentiment/.

• Tiny ImageNet consists of 100, 000 training images and 10, 000 test images from 200 different classes. It can be down-

loaded from http://cs231n .stanford .edu /tiny-imagenet -200 .zip.

For the image datasets, i.e., MNIST, FashionMNIST, CIFAR-10, CIFAR-100, and Tiny ImageNet, we normalize each pixel of 
the  images  from  the  datasets  to  the  range  of  [0, 1] before  feeding  them  into  the  neural  network.  For  the  IMDB  dataset, 
we  perform  data  pre-processing  following  https://github .com /manavgakhar /imdbsentiment /blob /master /IMdB _sentiment _
analysis _project .ipynb.

E.2.  Model architectures

We employ the Python package complexPyTorch [30] to implement our CVNNs, which include complex-value CNNs and 
complex-value MLPs. The detailed architectures of CVNNs are presented in Supplementary Table E.2, and all the parameters 
in these network architectures are complex values except for the last layer.

In Supplementary Table E.2, “5 × 5, 10” denotes that the convolutional layer has 5 × 5 kernel size and 10 output channels. 
The strides for all convolutional layers are set to 1. “fc-500” denotes the fully-connected layer with the output features of 

26

H. Chen, F. He, S. Lei et al.

Artiﬁcial Intelligence 322 (2023) 103951

500.  All  convolutional  layers  and  fully-connected  layers  are  followed  the  ReLU  layer  except  for  the  last  layer.  “abs”  is  the 
absolute layer that computes the absolute value of each element in input and can convert complex values to real values.

E.3.  Implementation details

This section provides all the additional implementation details for our experiments.
Model training. We employ SGD to optimize all the models with momentum = 0.9.
Training strategy for MNIST and FashionMNIST. Every model is trained by SGD for 100 epochs, in which the batch size 

is set as 1024, and the learning rate is ﬁxed to 0.01.

Training strategy for CIFAR-10 and CIFAR-100. Models are trained by SGD for 100 epochs, in which the batch size is set 

as 128, and the learning rate is ﬁxed to 0.01.

Training strategy for IMDB. Models are trained by SGD for 100 epochs, in which the batch size is set as 512. The learning 

rate is initialized as 0.01 and decayed by 0.2 every 40 epoch.

Training strategy for Tiny ImageNet. Models are trained by SGD for 100 epochs, in which the batch size is set as 128. 

The learning rate is initialized as 0.01 and decayed by 0.2 every 40 epoch.

References

[1] S.L.  Goh,  D.P.  Mandic,  Nonlinear  adaptive  prediction  of  complex-valued  signals  by  complex-valued  prnn,  IEEE  Trans.  Signal  Process.  53  (2005) 

1827–1836.

[2] A.  Hirose,  R.  Eckmiller,  Behavior  control  of  coherent-type  neural  networks  by  carrier-frequency  modulation,  IEEE  Trans.  Neural  Netw.  7  (1996) 

[3] H. Sawada, R. Mukai, S. Araki, S. Makino, Polar coordinate based nonlinear function for frequency-domain blind source separation, IEICE Trans. Fundam. 

[4] E.K. Cole, J.Y. Cheng, J.M. Pauly, S.S. Vasanawala, Analysis of deep complex-valued convolutional neural networks for mri reconstruction, arXiv preprint 

[5] A. Hirose, S. Yoshida, Generalization characteristics of complex-valued feedforward neural networks in relation to signal coherence, IEEE Trans. Neural 

1032–1034.

Electron. Commun. Comput. Sci. 86 (2003) 590–596.

arXiv:2004 .01738, 2020.

Netw. Learn. Syst. 23 (2012) 541–551.

[6] A. Hirose, Complex-Valued Neural Networks, vol. 400, Springer Science & Business Media, 2012.
[7] T. Nitta, An extension of the back-propagation algorithm to complex numbers, Neural Netw. 10 (1997) 1391–1415.
[8] T. Nitta, Orthogonality of decision boundaries in complex-valued neural networks, Neural Comput. 16 (2004) 73–97.
[9] T. Nitta, On the inherent property of the decision boundary in complex-valued neural networks, Neurocomputing 50 (2003) 291–303.
[10] T. Nitta, Redundancy of the parameters of the complex-valued neural network, Neurocomputing 49 (2002) 423–428.
[11] C. Trabelsi, O. Bilaniuk, Y. Zhang, D. Serdyuk, S. Subramanian, J.F. Santos, S. Mehri, N. Rostamzadeh, Y. Bengio, C.J. Pal, Deep complex networks, arXiv 

[12] D.P. Reichert, T. Serre, Neuronal synchrony in complex-valued deep networks, arXiv preprint arXiv:1312 .6115, 2013.
[13] I. Danihelka, G. Wayne, B. Uria, N. Kalchbrenner, A. Graves, Associative long short-term memory, in: International Conference on Machine Learning, 

preprint arXiv:1705 .09792, 2017.

PMLR, 2016, pp. 1986–1994.

[14] M.  Arjovsky,  A.  Shah,  Y.  Bengio,  Unitary  evolution  recurrent  neural  networks,  in:  International  Conference  on  Machine  Learning,  PMLR,  2016, 

pp. 1120–1128.

[15] S.  Wisdom,  T.  Powers,  J.  Hershey,  J.  Le  Roux,  L.  Atlas,  Full-capacity  unitary  recurrent  neural  networks,  Adv.  Neural  Inf.  Process.  Syst.  29  (2016) 

4880–4888.

[16] M. Mohri, A. Rostamizadeh, A. Talwalkar, Foundations of Machine Learning, MIT Press, 2018.
[17] P. Bartlett, D.J. Foster, M. Telgarsky, Spectrally-normalized margin bounds for neural networks, arXiv preprint arXiv:1706 .08498, 2017.
[18] N. Guberman, On complex valued convolutional neural networks, arXiv preprint arXiv:1602 .09046, 2016.
[19] A. Rakhlin, K. Sridharan, A. Tewari, Sequential complexities and uniform martingale laws of large numbers, Probab. Theory Relat. Fields 161 (2015) 

111–153.

[20] T. Zhang, Statistical analysis of some multi-category large margin classiﬁcation methods, J. Mach. Learn. Res. 5 (2004) 1225–1251.
[21] P.-C. Guo, A Frobenius norm regularization method for convolutional kernels to avoid unstable gradient problem, arXiv preprint arXiv:1907.11235, 

2019.

2017.

[22] A. Botalb, M. Moinuddin, U. Al-Saggaf, S.S. Ali, Contrasting convolutional neural network (cnn) with multi-layer perceptron (mlp) for big data analysis, 

in: 2018 International Conference on Intelligent and Advanced System (ICIAS), IEEE, 2018, pp. 1–5.

[23] Y.H. Geum, A.K. Rathie, H. Kim, Matrix expression of convolution and its generalized continuous form, Symmetry 12 (2020) 1791.
[24] G. Pisier, Remarques sur un résultat non publié de B. Maurey, in: Séminaire Analyse Fonctionnelle, 1981, pp. 1–12.
[25] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proc. IEEE 86 (1998) 2278–2324.
[26] H. Xiao, K. Rasul, R. Vollgraf, Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, arXiv preprint arXiv:1708 .07747, 

[27] A. Krizhevsky, G. Hinton, Learning multiple layers of features from tiny images, Technical Report, Citeseer, 2009.
[28] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, C. Potts, Learning word vectors for sentiment analysis, in: Proceedings of the 49th Annual Meeting of 
the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, Portland, Oregon, USA, 2011, 
pp. 142–150, http://www.aclweb .org /anthology /P11 -1015.

[29] Y. Le, X. Yang, Tiny imagenet visual recognition challenge, CS 231N 7 (2015) 7.
[30] M.W. Matthès, Y. Bromberg, J. de Rosny, S.M. Popoff, Learning and avoiding disorder in multimode ﬁbers, Phys. Rev. X 11 (2021) 021060, https://

doi .org /10 .1103 /PhysRevX .11.021060.

27

