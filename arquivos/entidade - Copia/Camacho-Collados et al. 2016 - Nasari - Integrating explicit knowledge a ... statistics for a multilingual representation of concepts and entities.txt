Artiﬁcial Intelligence 240 (2016) 36–64

Contents lists available at ScienceDirect

Artiﬁcial  Intelligence

www.elsevier.com/locate/artint

Nasari:  Integrating  explicit  knowledge  and  corpus  statistics 
for  a  multilingual  representation  of  concepts  and  entities
José Camacho-Collados a,∗
a Department of Computer Science, Sapienza University of Rome, Italy
b Language Technology Lab, Department of Theoretical and Applied Linguistics, University of Cambridge, United Kingdom

,  Mohammad Taher Pilehvar b,1,  Roberto Navigli a

a  r  t  i  c  l  e 

i  n  f  o

a  b  s  t  r  a  c  t

Article history:
Received 23 December 2015
Received in revised form 14 July 2016
Accepted 25 July 2016
Available online 16 August 2016

Keywords:
Semantic representation
Lexical semantics
Word Sense Disambiguation
Semantic similarity
Sense clustering
Domain labeling

Owing to the need for a deep understanding of linguistic items, semantic representation 
is  considered  to  be  one  of  the  fundamental  components  of  several  applications  in 
Natural Language Processing and Artiﬁcial Intelligence. As a result, semantic representation 
has  been  one  of  the  prominent  research  areas  in  lexical  semantics  over  the  past 
decades. However, due mainly to the lack of large sense-annotated corpora, most existing 
representation  techniques  are  limited  to  the  lexical  level  and  thus  cannot  be  effectively 
applied  to  individual  word  senses.  In  this  paper  we  put  forward  a  novel  multilingual 
vector  representation,  called Nasari,  which  not  only  enables  accurate  representation 
of  word  senses  in  different  languages,  but  it  also  provides  two  main  advantages  over 
existing  approaches:  (1) high  coverage,  including  both  concepts  and  named  entities, 
(2) comparability across languages and linguistic levels (i.e., words, senses and concepts), 
thanks to the representation of linguistic items in a single uniﬁed semantic space and in 
a  joint  embedded  space,  respectively.  Moreover,  our  representations  are  ﬂexible,  can  be 
applied to multiple applications and are freely available at http :/ /lcl .uniroma1.it /nasari/. As 
evaluation  benchmark,  we  opted  for  four  different  tasks,  namely,  word  similarity,  sense 
clustering, domain labeling, and Word Sense Disambiguation, for each of which we report 
state-of-the-art performance on several standard datasets across different languages.

© 2016 Elsevier B.V. All rights reserved.

1.  Introduction

Semantic  representation,  i.e.,  modeling  the  semantics  of  a  linguistic  item2 in  a  mathematical  or  machine  interpretable 
form, is a fundamental problem in Natural Language Processing (NLP) and Artiﬁcial Intelligence (AI). Because they represent 
the lowest linguistic level, word senses play a vital role in natural language understanding. Effective representations of word 
senses can be directly useful to Word Sense Disambiguation [93], semantic similarity [13,129,106], coarsening sense inven-
tories [92,124], alignment of lexical resources [101,98,108], lexical substitution [75], and semantic priming [100]. Moreover, 
sense-level  representation  can  be  directly  extended  to  applications  requiring  word  representations,  with  the  added  bene-
ﬁt  that  it  provides  extra  semantic  information.  Turney  and  Pantel  [129] provide  a  review  of  some  of  the  applications  of 

* Corresponding author.

E-mail address: collados@di.uniroma1.it (J. Camacho-Collados).

1 Work mainly done at the Sapienza University of Rome.
2 Throughout this article by a linguistic item we mean any kind of linguistic unit that can bear a meaning, i.e., a word sense, a word, a phrase, a sentence 
or a larger piece of text.

http://dx.doi.org/10.1016/j.artint.2016.07.005
0004-3702/© 2016 Elsevier B.V. All rights reserved.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

37

word representation, including: automatic thesaurus generation [21,22], word similarity [25,128,113] and clustering [103], 
query expansion [140], information extraction [61], semantic role labeling [29,104], spelling correction [53], and Word Sense 
Disambiguation [93].

The Vector Space Model (VSM) is a prominent approach for semantic representation. The model represents a linguistic 
item as a vector (or a point) in an n-dimensional semantic space, i.e., a mathematical space wherein each of the n dimen-
sions (hence, axes of the space) denotes a single linguistic entity, such as a word. The popularity of the VSM representation 
is due to two main reasons. Firstly, it is straightforward to view vectors as sets of features and directly apply various ma-
chine learning techniques on them. Secondly, the model enjoys support from the ﬁeld of Cognitive Science wherein several 
studies have empirically or theoretically suggested that various aspects of human cognition accord with VSMs [36,64].

However, most VSM-based techniques, whether in their conventional co-occurrence based form [119,129,63], or in their 
newer predictive branch [20,81,8], usually base their computation on the distributional statistics derived from text corpora. 
Hence,  in  order  to  be  able  to  represent  individual  meanings  of  words  (i.e.,  word  senses),  these  techniques  require  large 
amounts  of  disambiguated  text  prior  to  modeling.  Additionally,  Word  Sense  Induction  techniques  [103,11,58,27] require 
sense-annotated data, if their induced sense clusters are to be mapped to an existing sense inventory. However, providing 
sense-annotated  data  on  a  large  scale  is  a  time-consuming  process  which  has  to  be  carried  out  separately  for  each  word 
sense and repeated for each new language of interest, i.e., the so-called knowledge acquisition bottleneck. Importantly, the 
largest manual effort for providing a wide-coverage sense-annotated dataset dates back to 1993, in the case of the SemCor 
corpus  [85].  In  fact,  although  cheap  and  fast  annotations  could  be  obtained  by  means  of  Amazon  Mechanical  Turk  [123,
55], games with a purpose [133,131,56], or voluntary collaborative editing such as in Wikipedia [77], producing annotated 
resources  manually  is  still  an  onerous  task.  On  the  other  hand,  the  performance  of  Word  Sense  Disambiguation  (WSD) 
techniques is still far from ideal [93], which in its turn prevents a reliable automatic sense-annotation of large text corpora 
that can be used for modeling individual word senses. This hinders the functionality of this group of vector space models 
in tasks such as WSD that require the representation of individual word senses.

There have been several efforts to adapt and apply distributional approaches to the representation of word senses [103,
12,114,47,68]. However, most of these techniques cannot provide representations that are already linked to a standard sense 
inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data 
[48]. Recently, there have been attempts to address this issue and to obtain vectors for individual word senses by exploiting 
the  WordNet  semantic  network  [74,106,108,116] and  its  glosses  [19].  These  approaches,  however,  are  either  restricted  to 
the representation of concepts deﬁned in WordNet and to the English language only, or are designed for speciﬁc tasks.

In our recent work [16], we proposed a method that exploits the structural knowledge derived from semantic networks, 
together with distributional statistics from text corpora, to produce effective representations of individual word senses or 
concepts. Our approach provides two main advantages in comparison to previous VSM techniques. Firstly, it is multilingual, 
as  it  can  be  directly  applied  for  the  representation  of  concepts  in  dozens  of  languages.  Secondly,  each  vector  represents 
a  concept,  irrespective  of  its  language,  in  a  uniﬁed  semantic  space  having  concepts  as  its  dimensions,  permitting  direct 
comparison of different representations across languages and hence enabling cross-lingual applications.

In this article, we improve our approach, referred to as Nasari (Novel Approach to a Semantically-Aware Representation 
of  Items)  henceforth,  and  extend  their  application  to  a  wider  range  of  tasks  in  lexical  semantics.  Speciﬁcally,  the  novel 
contributions are as follow:

1. We propose a new formulation for fast computation of lexical speciﬁcity (Section 3.1.1).
2. We propose a new ﬂexible way to get continuous embedded vector representations, with the added beneﬁt of obtaining 

a semantic space shared by BabelNet synsets, words and texts (Section 3.3).

3. We put forward a technique for improved computation of weights in the uniﬁed vectors and show how it can improve 

the accuracy and eﬃciency of the representations (Section 3.4).

4. We  compute  and  assign  weights  to  individual  edges  in  our  semantic  network  (Section 4.1)  and  show  by  means  of 

different experiments the advantage we gain when using this new weighted graph (Section 10).

5. We release the lexical and uniﬁed vector representations for ﬁve different languages (English, French, German, Italian 

and Spanish) and the embedded vector representations for the English language at http :/ /lcl .uniroma1.it /nasari/.

In addition to these contributions, we also devised robust frameworks that enable direct application of our representa-
tions to four different tasks: Semantic Similarity (Section 6), Sense Clustering (Section 7), Domain Labeling (Section 8) and 
Word Sense Disambiguation (Section 9). For each of the tasks, we carried out a comprehensive set of evaluations on several 
datasets in order to verify the reliability and ﬂexibility of Nasari different datasets and tasks. We provide a summary of the 
experiments in Section 5.

The rest of this article is structured as follows. We ﬁrst provide an introduction of some of the most widely used knowl-
edge  resources  in  lexical  semantics,  in  Section 2.  After  which,  in  Section 3 we  describe  our  methodology  to  convert  text 
into  lexical,  embedded  and  uniﬁed  vectors.  The  process  to  obtain  vector  representations  for  synset  vectors  by  leveraging 
the knowledge resources described in Section 2, and the methodology to obtain vectors from text described in Section 3, is 
presented in Section 4. We present a summary of the experiments and the performance of Nasari across tasks in Section 5. 
Then, we describe some applications of the vectors with their respective frameworks and experiments in Sections 6 (Seman-
tic Similarity), 7 (Sense Clustering), 8 (Domain Labeling), and 9 (Word Sense Disambiguation). We analyze the performance 

38

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

of  different  components  of  our  model  in  Section 10.  Finally,  we  discuss  the  related  work  in  Section 11 and  provide  the 
concluding remarks in Section 12.

2.  Knowledge resources

Knowledge resources can be divided into two general categories: expert made and collaboratively constructed. Each type 
has its own advantages and limits. Manually-annotated resources feature highly-accurate encoding of concepts and semantic 
relationships between them but, with a few exceptions, are usually limited in their lexical coverage, and are typically focused 
on a speciﬁc language only. A good example is WORDNET [83], a semantic network whose basic units are synsets. A synset 
represents a concept which may be expressed through nouns, verbs, adjectives or adverbs and is composed of the different 
lexicalizations (i.e., synonyms that are used to express it). For example, the synset of the middle of the day concept comprises 
six lexicalizations: noon, twelve noon, high noon, midday, noonday, noontide. Synsets may also be seen as nodes in a semantic 
network. These nodes are connected to each other by means of lexical or semantic relations (hypernymy, meronymy, etc.). 
These relations are seen as the edges in the WordNet semantic network. Despite being one of the largest and most complete 
manually-made lexical resources, WordNet still lacks coverage of lemmas and senses from domain speciﬁc lexicons (e.g., law 
or medicine), named entities, creative slang usages, or those for technology that came into existence only recently.

On  the  other  hand,  collaboratively-constructed  resources,  such  as WIKIPEDIA,  provide  features  such  as  multilinguality, 
wide  coverage  and  up-to-dateness.  As  of  September  2015,  Wikipedia  provides  more  than  100K  articles  in  over  ﬁfty  lan-
guages.  This  coverage  is  steadily  increasing.  For  instance,  the  English  Wikipedia  alone  receives  750  new  articles  per  day. 
Each of these articles provides, for its corresponding concept, a great deal of information in the form of textual informa-
tion,  tables,  infoboxes,  and  various  relations  (such  as  redirections,  disambiguations,  and  categories).  These  features  have 
persuaded many researchers over the past few years to exploit the huge amounts of semi-structured knowledge available in 
such collaborative resources for different NLP applications [46,125].

The types of knowledge available in the expert-based and collaboratively-constructed resources make them complemen-
tary. This has motivated researchers to combine various lexical resources across the two categories [101,108]. A prominent 
example  is BABELNET [98],  which  provides  a  mapping  of  WordNet  to  a  number  of  collaboratively-constructed  resources, 
including  Wikipedia.  The  structure  of  BabelNet3 is  similar  to  that  of  WordNet.  Synsets  are  the  main  linguistic  units  and 
are  connected  to  other  semantically  related  synsets,  whose  lexicalizations  are  multilingual  in  this  case.  For  instance,  the 
synset  corresponding  to  United States is  represented  with  a  set  of  multilingual  lexicalizations  including  United_StatesEN, 
United_States_of_AmericaEN, AmericaEN, U.S.EN, and U.S.A.EN in  English,  Estados_UnidosES, Estados_Unidos_de_AméricaES, EEUUES, 
E.E.U.U.ES, and EE. UU.ES in Spanish, and Stati_Uniti_d’AmericaIT, Stati_UnitiIT, AmericaIT, and U.S.A.IT in Italian. The relations be-
tween  synsets  are  the  ones  coming  from  WordNet  (hypernyms,  hyponyms,  etc.),  plus  new  relations  coming  from  other 
resources such as Wikipedia hyperlinks and WikiData4 relations (e.g. Madrid capital of Spain). BabelNet is the largest mul-
tilingual semantic network available, containing 13,789,332 synsets (6,418,418 concepts and 7,370,914 named entities) and 
354,538,633 relations for 271 languages.5 For the English language, BabelNet contains 4,403,148 synsets with at least one 
Wikipedia page associated and 117,653 synsets with one WordNet synset associated, from which 99,705 synsets are com-
posed of both a Wikipedia page and a WordNet synset.

The gist of our approach lies in its combination of different types of knowledge from complementary resources. Specif-
ically,  our  representation  approach  utilizes  the  following  sources  of  knowledge:  lexico-semantic  relations  in  WordNet, 
BabelNet’s mapping of WordNet synsets and Wikipedia articles, texts within Wikipedia articles and the inter-article links of 
Wikipedia. In our experiments we used WordNet 3.0 which covers more than 117K unique nouns in about 80K synsets, the 
Wikipedia dump of December 2014, and BabelNet 3.0, which covers 271 languages and contains over 13 million synsets.

3.  Representing texts as vectors

One  of  the  contributions  of  this  article  is  the  framework  we  are  proposing  for  transforming  texts  into  three  different 
kinds  of  vector:  lexical,  embedded  and  uniﬁed.  Our  lexical  vectors  follow  the  conventional  approach  for  representing  a 
linguistic  item  in  a  semantic  space  with  words  as  its  dimensions  [103] (multiword  expressions  are  also  considered).  The 
weights  in  these  vectors  are  usually  computed  on  the  basis  of  raw  term  frequencies  (tf )  or  normalized  frequencies,  such 
as  tf-idf [52].  Instead,  we  use  lexical  speciﬁcity  for  the  computation  of  the  weights  in  our  lexical  vectors.  Having  a  solid 
statistical basis, lexical speciﬁcity provides several advantages over the previously mentioned measures [17] (see Section 10
for  a  comparison  of  lexical  speciﬁcity  and  tf-idf ).  In  what  follows  in  this  section  we  ﬁrst  explain  lexical  speciﬁcity  and 
propose an eﬃcient way for its fast computation (Section 3.1). We then provide more details of our three types of vector, 
i.e., lexical (Section 3.2), embedded (Section 3.3) and uniﬁed (Section 3.4).

3 http :/ /babelnet .org/.
4 https :/ /www.wikidata .org.
5 The statistics are taken from the BabelNet 3.0 release, which is the version used in our experiments. More statistics can be found at http :/ /babelnet .
org /stats.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

39

3.1.  Lexical speciﬁcity

Lexical speciﬁcity [62] is a statistical measure based on the hypergeometric distribution.6 The measure has been widely 
used in different NLP applications including term extraction [28], textual data analysis [66] and domain-based term disam-
biguation  [14,10],  but  it  has  rarely  been  used  to  measure  weights  in  a  vector  space  model.  Lexical  speciﬁcity  essentially 
computes  the  set  of  most  representative  words  for  a  given  text  based  on  the  hypergeometric  distribution.  In  our  setting, 
we are interested in representing a given text, hereafter referred to as the sub-corpus SC, through a vector comprising the 
weighted set of its most relevant words or concepts. In order to compute lexical speciﬁcity, we need a reference corpus RC
which should be a superset of SC. Lexical speciﬁcity computes the weights for each word by contrasting the frequencies of 
that word across SC and RC.

Following  the  notation  of  [16],  let  T and  t be  the  respective  total  number  of  content  words  in  RC and  SC,  while  F
and  f denote the frequency of a given word  w in RC and SC, respectively. Our goal is to compute a weight quantifying 
the association strength of  w with our text SC. We compute the probability of a word  w having a frequency equal to or 
in our sub-corpus SC using a hypergeometric distribution which takes as its parameters the frequency of  w
higher than  f
in the reference corpus RC, i.e., F, and the sizes of RC and SC, i.e., T and t, respectively. A word w with a high probability 
is one with a high occurrence chance across arbitrary subsets of RC of size t. Hence, the representative words of a given 
sub-corpus will be those with low probabilities since these speciﬁc words are the most suitable ones for distinguishing the 
sub-corpus  from  the  reference  corpus.  As  a  result,  the  computed  probability  is  inversely  proportional  to  the  relevance  of 
the  word  w to  SC.  In  order  to  make  the  relation  directly  proportional,  thus  making  the  weights  more  interpretable,  we 
apply the − log10 operation to the computed probabilities as has been customary in the literature [28,42]. This logarithmic 
operation  also  speeds  up  the  calculations  (more  details  in  the  following  section).  Moreover,  using  log10,  instead  of  for 
instance the natural logarithm, has the added beneﬁt of leading to an easy calculation of the prior probability. For example, 
−5 = 0.00005. 
if  an  item  has  a  lexical  speciﬁcity  of  5.0,  it  means  that  the  probability  of  observing  that  item  in  SC is  10
Therefore, the lexical speciﬁcity of  w in SC is given by the following expression:

spec(T , t, F , f ) = − log10 P (X ≥ f )

(1)
where  X represents a random variable following a hypergeometric distribution with parameters  F , t and T and  P ( X ≥ f ) is 
deﬁned as follows:

P (X ≥ f ) =

F(cid:2)

i= f

P (X = i)

(2)

where  P ( X = i) represents  the  probability  of  a  given  word  to  appear  exactly  i times  in  the  subcorpus  SC according  to 
the hypergeometric distribution of parameters  F , t and  T . We propose an eﬃcient implementation of Equation (2) in the 
following section.

3.1.1.  Eﬃcient implementation of lexical speciﬁcity

According to Equation (2), the computation of the hypergeometric distribution involves summing (F − f ) + 1 addends, 

each of which is calculated as follows7:
(cid:3)
F
i

(cid:4)(cid:3)

(cid:4)

P (X = i) =

(cid:3)

T −F
t−i
(cid:4) =
T
t

F !(T − F )!t!(T − t)!
T !i!(F − i)!(t − i)!(T − F − t + i)!

(3)

Given  that  the  summation  range  of  Equation  (2) is  generally  directly  proportional  to  the  size  of  the  corpus,  the  com-
putation of lexical speciﬁcity can be quite expensive on large corpora wherein the value of  F tends to be very high. Lafon 
[62] proposed a method to reduce the computation cost of Equation (2). According to this method, one can ﬁrst calculate 
P ( X = i) only for the smallest i (i.e.,  f ) and then calculate the rest of probabilities, i.e.,  P ( X = f + 1), ...,  P ( X = F ), using 
the following property of the hypergeometric distribution:
P (X = i + 1) = P (X = i)(F − i)(t − i)
(i + 1)(T − F − t + i + 1)

(4)

Lafon  [62] also  suggested  using  the  well-known  Stirling  formula  for  the  computation  of  the  factorial  components  in 

Equation (3). According to the Stirling formula, the logarithm of a factorial can be approximated as follows:

log n! = n log n − n + 1
2

log (2πn)

(5)

6 “The hypergeometric distribution is a discrete probability distribution that describes the probability of k successes in n draws, without replacement, 
from a ﬁnite population of size N that contains exactly K successes, wherein each draw is either a success or a failure. In statistics, the hypergeometric 
test uses the hypergeometric distribution to calculate the statistical signiﬁcance of having drawn a speciﬁc k successes (out of n total draws) from the 
aforementioned population” (https :/ /en .wikipedia .org /wiki /Hypergeometric _distribution).
7 In the cases where i > t, which may occur if F > t, the probability P ( X = i) is equal to 0.

40

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

Fig. 1. Hypergeometric distribution for the word mathematics in an arbitrary sub-corpus (SC) of size 100,000 in Wikipedia.

Thanks  to  the  application  of  the  Stirling  formula  we  can  transform  Equation (3) into  a  summation.  Despite  these  im-
provements in the calculation of lexical speciﬁcity, there remain issues when the above computation is to be applied to a 
large  reference  corpus.  One  of  the  main  problems  is  the  multiplication  of  potentially  very  small  quantities.  Speciﬁcally,  a 
64-bit binary ﬂoating-point number, which is the one typically used in current computers, has an approximate range from 
+308.  During  the  computation  of  lexical  speciﬁcity  on  large  corpora,  the  lower  bound  can  be  reached 
10
several  times.  Our  solution  to  solve  this  problem  (which  even  optimizes  the  calculations)  is  obtained  via  the  next  two 
equations. Firstly, we rewrite Equation (4) by extracting the common factor  P ( X = f ):

−308 through  10

P (X ≥ f ) =

F(cid:2)

i= f

P (X = i) = P (X = f )

F(cid:2)

i= f

ai

(6)

where a f = 1 and ai = ai−1

(F −i)(t−i)

(i+1)(T −F −t+i+1) , ∀i = f + 1, ..., F .

Now we only need to apply the logarithm to both sides of the equation in order to transform the previous multiplication 
into  an  addition  and  thus  avoid  small  values.  In  this  way  we  also  avoid  unnecessary  exponentials  in  the  calculations  of 
P ( X = f ):

− log10 P (X ≥ f ) = − log10 P (X = f ) − log10

(cid:5) F(cid:2)

(cid:6)

ai

i= f

(7)

Therefore, according to Equation (1) and by applying a change of logarithm base, we can compute lexical speciﬁcity given 

the four parameters T , t,  F , and  f as follows:

spec(T , t, F , f ) = −k loge P (X = f ) − log10

(cid:5) F(cid:2)

(cid:6)

ai

i= f

(8)

where k is the natural logarithm of 10 (i.e., loge 10).

(cid:7)

For computational feasibility, the 

F
i= f ai sum is usually not computed until F . Instead, a stopping criterion is introduced 
into  the  loop.  Since  the  probability  mass  in  the  tail  of  the  hypergeometric  distribution  is  in  most  cases  mathematically 
insigniﬁcant with respect the ﬁnal cumulative probability distribution, the stopping criterion is usually satisﬁed well before 
reaching to the ﬁnal  F value, which considerably reduces the computation time.

As an example we show in Fig. 1 the estimated probability distribution for the word mathematics in an arbitrary sub-
corpus SC of 100,000 content words from Wikipedia. If the word mathematics occurs more than twenty times in SC, the 
word  is  considered  to  be  very  speciﬁc  to  the  given  subcorpus,  since,  as  we  can  see  from  Fig. 1,  most  of  the  probability 
mass  in  the  hypergeometric  distribution  is  concentrated  in  the  left  part  of  the  distribution  range.  The  distribution  range 
extends until 70,029, which is the number of occurrences of the word mathematics in the whole Wikipedia. However, the 
−20 and  rapidly  gets  much  smaller.  This  illustrates  the  point  made  above, 
probability  P ( X = 45) is  already  as  small  as  10
in which the right tail of the probability mass is generally insigniﬁcant to values close to the expected value, and adding a 
stopping condition might make the calculations much faster, while not having any noticeable effect to the ﬁnal speciﬁcity 
score.

The next three sections provide more details on our three types of vector and on how we leverage lexical speciﬁcity for 

their construction.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

41

3.2.  Lexical vector representation

So  far  we  have  explained  how  lexical  speciﬁcity  can  be  used  to  determine  the  relevance  of  words  for  a  given  text. 
In  this  section  we  explain  how  we  leverage  lexical  speciﬁcity  in  order  to  construct  a  lexical  vector  for  a  given  text  (i.e., 
SC). Throughout the article the texts considered come from Wikipedia, thus we use the whole Wikipedia as our reference 
corpus (RC). Our lexical vectors have individual words as their dimensions, therefore, in our lexical semantic space, a text 
is  represented  on  the  basis  of  its  association  with  a  set  of  lexical  items,  i.e.,  words.  By  contrasting  the  term  frequencies 
across SC and RC, we compute the lexical speciﬁcity of each term for the given subcorpus.

Speciﬁcally,  in  order  to  compute  our  lexical  vector  (cid:5)vlex(SC),  we  simply  iterate  over  all  the  content  words  in  our  sub-
corpus  SC (only  words  with  a  total  frequency  greater  than  or  equal  to  ﬁve  in  the  whole  Wikipedia  are  considered)  and 
compute lexical speciﬁcity for each of them. We then prune the resulting vectors by keeping only those words that are rele-
vant to the target text with a conﬁdence of 99% or more according to the hypergeometric distribution ( P ( X ≥ f ) ≤ 0.01), as 
also performed in earlier works [10,17]. Words with weights below the aforementioned threshold are considered as zero di-
mensions. The vector truncation step helps reduce noise. Additionally, the truncation helps in speeding up the computation 
of the vectors, as they will be sparse and therefore computationally easier to work with.

In  our  setting  we  also  consider  multiword  expressions  when  they  appear  as  lexicalizations  of  piped  links.8 Note  that 
we  apply  lexical  speciﬁcity  to  content  words  (nouns,  verbs  and  adjectives)  after  tokenization  and  lemmatization,  but  for 
notational simplicity we will keep using the term “word” to refer to them.

3.3.  Embedded vector representation

In  recent  years,  semantic  representation  has  experienced  a  resurgence  of  interest  in  the  use  of  neural  network-based 
learning, a trend usually referred to as word embeddings. In addition to their fast processing of massive amounts of text, 
word embeddings have proved to be reliable techniques for modeling the semantics of words on the basis of their contexts. 
However, the application of these word-based techniques to the representation of word senses is not trivial and is bound 
to the availability of large amounts of sense-annotated data. There have been efforts aimed at learning sense-speciﬁc em-
beddings without needing to resort to sense-annotated data, often through clustering the contexts in which a word appears 
[138,47,99]. However, the resulting representations are usually not aligned to existing sense inventories.

We  put  forward  an  approach  that  allows  us  to  plug  in  an  arbitrary  word  embedding  representation  with  that  of  our 
lexical vector representations, providing three main advantages: (1) beneﬁting from the word-based knowledge derived as 
a result of learning from massive corpora for our sense-level representation; (2) reducing the dimensionality of our lexical 
space to a ﬁxed-size continuous space; and (3) providing a shared semantic space between words and synsets (more details 
in Section 4), hence enabling a direct comparison of words and synsets.

Our approach exploits the compositionality of word embeddings. According to this property, a compositional phrase rep-
resentation can be obtained by combining, usually averaging, its constituents’ representations [82]. For instance, the vector 
representation obtained by averaging the vectors of the words Vietnam and capital is very close to the vector representation 
of the word Hanoi in the semantic space of word embeddings. Our approach builds on this property and plugs a trained 
word embedding-based representation into our lexical vectors.

Speciﬁcally,  given  an  input  text  T and  a  space  of  word  embeddings  E,  we  ﬁrst  calculate  the  lexical  vector  of  T (i.e., 

(cid:5)vlex(T )) as explained in Section 3.2 and then map our lexical vector to the semantic space  E as follows:

(cid:7)

E(T ) =

w∈(cid:5)vlex(T )
(cid:7)

(cid:3)

(cid:4)
rank(w,(cid:5)vlex(T )) E(w)

1

w∈(cid:5)vlex(T )

1
rank(w,(cid:5)vlex(T ))

(9)

where  E(w) is the embedding-based representation of the word  w in  E, and rank(w, (cid:5)vlex(T )) is the rank of the dimension 
corresponding to the word w in the lexical vector (cid:5)vlex(T ), thus giving more importance to the higher weighted dimensions. 
In Section 10 we compare this harmonic average giving more importance to higher weighted words over a simple average. 
One of the main advantages of this representation combination technique is its ﬂexibility, since any word embedding space 
can be given as input. As we show in our experiments in Sections 6.1 and 7.1, this combination enables us to beneﬁt from 
word-speciﬁc knowledge and improve it by integrating it into our sense-speciﬁc representations.

3.4.  Uniﬁed vector representation

We also propose a third representation, which we call uniﬁed, that, in contrast to the lexical vector representation which 
has potentially ambiguous words as individual dimensions, has BabelNet synsets as its individual dimensions. Algorithm 1

8 A piped link is a hyperlink which is found within the Wikipedia article that redirects the user to another Wikipedia page. For example, the piped link 
[[dockside_crane|Crane_(machine)]] is a hyperlink that appears as dockside_crane in the text, but links to the Wikipedia page titled Crane_(machine). The 
Wikipedia article is therefore represented with a suitable lexicalization that preserves the grammatical and syntactic structure, the contextual coherency 
and the ﬂow of the sentence.

42

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

for each hypernym h of l in BabelNet

if ∃ l1, l2 ∈ SC:  l1, l2 hyponyms of h and l1 (cid:12)= l2 then

Algorithm 1 Uniﬁed vector construction.
Input: A reference corpus RC and a sub-corpus SC
Output: the uniﬁed vector (cid:5)us where (cid:5)us(h) is the dimension corresponding to the synset h
1: T ← size(RC)
2: t ← size(SC)
3: H ← ∅
4: for each lemma l ∈ SC
5:
H ← H ∪ {h}
6:
7: (cid:5)u ← empty vector
8: for each h ∈ H
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: return (cid:5)u

F ← 0
f ← 0
hyper pass ← F alse
for each lexicalization lex of h
F ← F + freq(lex, RC)
f ← f + freq(lex, SC)
spech ← speciﬁcity(T , t, freq(lex, RC), freq(lex, SC))
if spech ≥ specthres then
hyper pass ← T rue

F ← F + freq(lex, RC)
f ← f + freq(lex, SC)
(cid:5)u(h) ← speciﬁcity(T , t, F , f )

for each lexicalization lex of hypo

for each hyponym hypo of h

if hyper pass then

shows  the  construction  process  of  a  uniﬁed  vector  given  the  sub-corpus  SC.  The  algorithm  ﬁrst  clusters  together  those 
words  in  SC that  have  a  sense  sharing  the  same  hypernym  (h in  the  algorithm)  according  to  the  WordNet  taxonomy 
integrated in BabelNet (lines 4–6).

On  all  hyponym  clusters  we  impose  the  restriction  that  they  should  have  at  least  one  lexicalization  of  the  hypernym 
above the standard lexical speciﬁcity threshold 2 (lines 16–18). The reason why we include this in the uniﬁed representation 
is to reduce some noise detected by applying the old uniﬁed algorithm [16]. Finally, if the cluster passes the threshold, the 
speciﬁcity  is  computed  for  the  set  of  all  the  hyponyms  of h,  even  those  which  do  not  occur  in  the  sub-corpus  SC (lines 
20–24). As in Section 3.1,  F and  f denote the frequencies in the reference corpus RC (Wikipedia) and the sub-corpus SC, 
respectively. In this case, the frequencies correspond to the aggregation of frequencies of h and all its hyponyms.

Our clustering of sibling words into a single cluster represented by their common hypernym transforms a lexical space 
into a uniﬁed semantic space. This space has multilingual synsets as dimensions, enabling their direct comparability across 
languages. We evaluated this feature of the uniﬁed vectors on the task of cross-lingual word similarity in Section 6.1.3. The 
clustering  may  also  be  viewed  as  an  implicit  disambiguation  of  potentially  ambiguous  words,  as  they  are  disambiguated 
into their intended sense represented by their hypernym, resulting in a more accurate semantic representation.

3.5.  Vector comparison

As our vector comparison method for the lexical and uniﬁed vectors we use the square-rooted Absolute Weighted Over-
lap [17,16], which is based on the Weighted Overlap measure [106]. For notational brevity, we will refer to the square-rooted 
Absolute Weighted Overlap as Weighted Overlap (WO). WO compares two vectors on the basis of their overlapping dimen-
sions, which are harmonically weighted by their absolute rankings. For this measure the vectors are viewed as semantic sets
or  ranked lists [135],  as  the  weights  are  only  used  to  sort  the  elements  within  the  vector  and  their  actual  values  are  not 
used in the calculation. Formally, Weighted Overlap between two vectors  (cid:5)v 1 and  (cid:5)v 2 is deﬁned as follows:

(cid:8)
(cid:9)
(cid:9)
(cid:10)

W O ( (cid:5)v 1, (cid:5)v 2) =

(cid:7)

(cid:3)

d∈O

rank(d, (cid:5)v 1) + rank(d, (cid:5)v 2)
i=1(2i)−1

(cid:7)|O |

(cid:4)−1

(10)

where  O is the set of overlapping dimensions (i.e., concepts or words) between the two vectors and rank(d, (cid:5)v i) is the rank 
of dimension d in the vector  (cid:5)v i . Absolute WO differs from the original WO, which takes into account the relative ranks of 
the dimensions with respect to the overlapping dimensions, instead of considering all the dimensions of the vector. Owing 
to the use of absolute ranks this measure gives lower scores in comparison to the original WO. This is the reason behind the 
use of the square-root operator, which smooths the distribution of values over the [0,1] scale. This metric has been shown 
to suit speciﬁcity-based vectors more than the conventional cosine distance [17].

In contrast, we use cosine for comparing our embedded vector representations. The dimensions of the embedded repre-
sentations are not interpretable and the dimension values do not represent weights, thus rank-based WO is not applicable 
on this setting. Cosine is the usual measure used in the literature to measure similarity in an embedding space [81,19,68].

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

43

Fig. 2. Our procedure for getting contextual information of the sample BabelNet synset represented by its main sense reptile1
n .

4.  From a synset to its vector representations

In Section 3 we proposed three vector representations of an arbitrary text or subcorpus SC belonging to a larger collec-
tion. We now see how we leverage these representations to obtain a semantic vector representation for concepts and named 
entities.  As  knowledge  base  we  use  BabelNet,9 a  multilingual  encyclopedic  dictionary  which  merges  WordNet  with  other 
lexical and encyclopedic resources such as Wikipedia and Wiktionary, thanks to its use of an automatic mapping algorithm 
[97,98]. We chose BabelNet due to its large coverage of named entities and concepts in hundreds of languages. Moreover, 
concepts and named entities are organized into a full-ﬂedged taxonomy which integrates the WordNet taxonomy, which is 
the one used in our experiments, and, from its latest versions, the Wikipedia Bitaxonomy [34], WikiData, and is-a relations 
coming  from  open  information  extraction  techniques  [26].  Our  approach  makes  use  of  the  full  power  of  BabelNet,  as  it 
exploits the complementary information of the distributional statistics in Wikipedia articles that are tied to the taxonomi-
cal relations in BabelNet. In our experiments, we used version 3.0 of BabelNet (released in December 2014) which covers 
around 6.5M concepts and more than 7M named entities in 271 different languages. The rest of this section is divided into 
two parts. We ﬁrst show how we collect contextual information for a given synset (Section 4.1) and then explain how this 
contextual information is processed in order to obtain our vector representations (Section 4.2).

4.1.  Getting contextual information for a given synset

The goal of the ﬁrst step is to create a subcorpus SCs for a given BabelNet synset  s. Let Ws be the set containing the 
Wikipedia page corresponding to the concept s (wps henceforth) and all the related Wikipedia pages that have an outgoing 
link to that page. Note that at this stage Ws might be empty if there is no Wikipedia page corresponding to the BabelNet 
synset s. We further enrich Ws by adding the corresponding Wikipedia pages of the hypernyms and hyponyms of s in the 
taxonomy of BabelNet. Fig. 2 illustrates our procedure for obtaining contextual information. Let SC s be the set of content 
words occurring in the Wikipedia pages of Ws after tokenization and lemmatization. The frequency of each content word 
w of SC s is calculated as follows:
n(cid:2)

f (w) =

λi f i(w)

i=1

(11)

where n is the number of Wikipedia pages in Ws,  f i(w) is the frequency of  w in the Wikipedia page  pi ∈ Ws (i = 1, ..., n), 
and  λi is  the  weight  assigned  to  the  page  pi to  denote  its  importance.  In  the  following  subsection  we  explain  how  we 
calculate the weight λi for a given page  pi .

4.1.1.  Weighting semantic relations

In this section we explain how we weight the BabelNet semantic relations (i.e., λi in Equation (11)) between the target 
synset  s and  the  i-th  page  in  Ws.  In  previous  versions  of Nasari [17,16] we  were  making  an  assumption  that  all  the 
Wikipedia pages in Ws were equally important (i.e., λi = 1, ∀i ≤ n). In this article we set more meaningful weights for these 
pages on the basis of the source and type of semantic connection to the target synset s.

A Wikipedia page in Ws may come from three different sources (see Section 4.1): (1) the Wikipedia page corresponding 
to  s (wps), (2) the related Wikipedia pages that have an outgoing link to the page  wps, and (3) the Wikipedia pages that 
are connected to  s through taxonomic relations in BabelNet. We compute and assign a weight in the  [0, 1] range for the 
pages of each type as follows:

9 See Section 2 for more information about BabelNet.

44

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

Table 1
Top-weighted dimensions from the lexical vectors of the ﬁnancial and geographical senses of bank.

Bank (ﬁnancial institution)

Bank (geography)

English

bank
banking
deposit
credit
money
loan
commercial_bank
central_bank

French

banque
bancaire
crédit
ﬁnancier
postal
client
dépôt
billet

Spanish

banco
bancario
banca
ﬁnanciero
préstamo
entidad
déposito
crédito

English

river
stream
bank
riparian
creek
ﬂow
water
watershed

French

eau
castor
berge
canal
barrage
zone
perchlorate
humide

Spanish

banco
limnología
ecología
barrera
estuarios
isla
interés
laguna

Table 2
Top-weighted dimensions from the uniﬁed vectors of the ﬁnancial and geographical senses of bank. We represent each synset by one of its word senses. 
Word senses marked with the same symbol across languages correspond to the same BabelNet synset.

Bank (ﬁnancial institution)

Bank (geography)

English

‡bank2
n
reserve2
n
(cid:4)ﬁnancial_institution1
n
(cid:13)deposit8
n
banking2
n
†ﬁnance1
n

French

‡banque1
n
•fonds2
n
(cid:13)dépôt9
n
◦emprunt2
n
paiement1
n
argent2
n

Spanish

English

French

Spanish

‡banco1
n
(cid:4)Institución_ﬁnanciera1
n
(cid:13)depósito15
n
†Finanzas1
n
•dinero2
n
◦préstamo2
n

(cid:4)stream1
n
river1
n
‡body_of_water1
n
ﬂow1
n
course2
n
bank1
n

eau1
n
eau15
n
excrément1
n
castor1
n
‡étendue_d’eau1
n
fourrure1
n

inclinación9
n
lago1
n
‡cuerpo_de_agua1
n
(cid:4)arroyo1
n
tierra11
n
costa1
n

1. The Wikipedia page corresponding to the BabelNet synset s (i.e.,  wps) is assigned the highest possible weight of 1.
2. The weights for the related Wikipedia pages that have an outgoing link to wps are computed as follows. We ﬁrst com-
pute the lexical vectors of these Wikipedia pages, as well as for wps . We then apply Weighted Overlap (see Section 3.5) 
to calculate the similarity between the lexical vectors of each of these pages and that of  wps . These similarity scores 
denote the weight of each related Wikipedia page. In order to reduce the high number of ingoing links in some cases, 
and to improve the quality of these links, we prune the ingoing links to include only the top 100 links on the basis of 
their similarity scores and those whose similarity score is higher than 0.25.

3. Given there is a possibility that a particular synset does not have a Wikipedia page associated with it, the Wikipedia 
pages coming from taxonomic relations cannot be calculated as in the previous case. In this case, the Wikipedia pages 
coming from taxonomic relations are given a ﬁxed score of 0.85, which was calculated as follows. We picked a set of 
100 random taxonomic relations and calculated the average similarity score among the 100 pairs by using our previous
Nasari system.

4.2.  Transforming the contextual information into vector representations

Once we have gathered a corpus SC s for a given BabelNet synset s and computed the associated frequencies  f (w) for 
each word w in SC s, we proceed to calculate the lexical, embedded and uniﬁed vectors of s as explained in Sections 3.2, 3.3
and 3.4, respectively. In our experiments, we used the whole Wikipedia corpus as our reference corpus RC (Wikipedia dump 
of December 2014).10 We computed Nasari lexical and uniﬁed vectors for English, German, French, Italian, and Spanish. The 
number of synset vectors for each of these languages is, respectively, 4.42M, 1.51M, 1.48M, 1.10M and 1.07M. On average, for 
the English language, the contextual information of a synset is composed of a subcorpus SC s of 1561 words in total coming 
from 17 Wikipedia pages. For the embedded vectors, we took as word embeddings the pre-trained word and phrase vectors 
from Word2Vec.11 These vectors were trained on a 100-billion English corpus from Google News and have 300 dimensions.

Lexical and uniﬁed synset vectors example  We show in Tables 1 and 2, respectively, the top-weighted dimensions of the lexical 
and uniﬁed vector representations for the ﬁnancial and geographical senses of the noun bank in three different languages, 
i.e., English, French and Spanish. As can be seen, the two senses of bank are clearly identiﬁed and distinguished from each 
other according to the top dimensions of their vectors, irrespective of their language and type. Additionally, note that the 
uniﬁed vectors are comparable across languages. We mark in Table 2, across different languages, those word senses12 that 
correspond to the same BabelNet synset. It can be seen from the Table that the uniﬁed vectors in different languages share 
many of their top elements.

10 Each language uses the Wikipedia corpus in its respective language as reference corpus.
11 The pre-trained Word2Vec word embeddings were downloaded at https :/ /code .google .com /p /word2vec/.
12 We use the sense notation of [93]: word

n is the nth sense of the word with part of speech p.

p

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

45

Table 3
Closest embedded vectors from the BabelNet synsets corresponding to the ﬁnancial and geographical senses of bank, and from the word bank.

Bank (ﬁnancial institution)

Bank (geography)

bank

Closest senses

Cosine

Closest senses

Cosine

Closest senses

Cosine

Deposit account
Universal bank
British banking
German banking
Commercial bank
Banking in Israel
Financial institution
Community bank

0.99
0.99
0.98
0.98
0.98
0.98
0.98
0.97

Stream bed
Current (stream)
River engineering
Braided river
Fluvial terrace
Bar (river morphology)
River
Perennial stream

0.98
0.97
0.97
0.97
0.97
0.97
0.97
0.96

Bank (ﬁnancial institution)
Universal bank
British banking
German banking
Branch (banking)
McFadden Act
Four Northern Banks
State bank

0.86
0.86
0.86
0.85
0.85
0.85
0.84
0.84

Word and synset embeddings example  The  dimensions  are  not  interpretable  in  the  embedded  vectors.  Therefore,  a  better 
way to distinguish different senses would be to show their closest elements in the space (using cosine as vector similarity 
measure).  Table 3 shows  the  eight  closest  senses  to  the  word  bank,  as  well  as  those  closest  to  two  speciﬁc  senses  of 
this word, i.e., the ﬁnancial and geographical senses (recall that in our embedded vector representation words and synsets 
share  the  same  space).  In  this  case,  both  senses  of  bank are  again  clearly  distinguished  by  their  closest  BabelNet  synsets 
in  the  space.  Looking  at  the  closest  senses  to  the  word  bank we  can  see  that  most  of  these  are  rather  somehow  to  the 
ﬁnancial meaning of bank, with lower cosine values, though. This shows that the predominant sense of the word bank in 
the Google News corpus (on which the word embeddings are trained) is clearly its ﬁnancial sense. We note that using our 
embedded vector representation one can easily compute the predominance of the senses of a word by directly comparing 
the representation of that word with those of its individual senses. Our shared space also provides a suitable framework for 
studying the ambiguity of words.

5.  Summary of the experiments

In  order  to  assess  the  reliability  and  ﬂexibility  of  our  technique  across  different  datasets  and  tasks,  we  carried  out  a 
comprehensive  set  of  evaluations.  Speciﬁcally,  we  considered  four  different  tasks:  Semantic  Similarity  (Section 6),  Sense 
Clustering  (Section 7),  Domain  Labeling  (Section 8)  and  Word  Sense  Disambiguation  (Section 9).  A  brief  overview  of  the 
evaluation benchmarks and the results across the four tasks follows:

1. Semantic similarity. Nasari proved to be highly reliable in the task of semantic similarity measurement, as it provides 

state-of-the-art performance on several datasets across different evaluation benchmarks:
• Mono-lingual word similarity on four standard word similarity datasets, namely, MC-30 [84], WS-Sim [33], SimLex-999 
[43] and RG-65 [117]. In addition to these English word similarity datasets, we also assessed the multilinguality of 
our approach on the RG-65 dataset in three other languages.

• Cross-lingual word similarity on six different cross-lingual datasets on the basis of RG-65 [15].
In addition to the above three word similarity benchmarks, we also assessed the capability of our approach to provide 
comparable semantic representations for different types of linguistic items. Speciﬁcally, we opted for the SemEval-2014 
task on Cross-Level Semantic Similarity [57]. Despite not being tuned for this task, our approach achieved near state-of-
the-art performance on the word to sense similarity measurement dataset.

2. Sense clustering. We constructed a highly competitive unsupervised system on the basis of the Nasari representations, 
outperforming state-of-the-art supervised systems on two manually-annotated Wikipedia sense clustering datasets [23].
3. Domain labeling. We used our system for annotating synsets of a large lexical semantic resource (BabelNet), and bench-
marked  our  system  against  three  automatic  baselines  on  two  gold  standard  datasets:  a  dataset  of  domain-labeled 
WordNet  synsets  coming  from  WordNet  3.0,  and  a  new  manually-constructed  dataset  of  domain-labeled  BabelNet 
synsets. Nasari outperformed all automatic baselines, demonstrating that our approach is not only reliable, but is also 
ﬂexible across different tasks.

4. Word Sense Disambiguation. We  proposed  a  simple  framework  for  a  knowledge-rich  unsupervised  disambiguation 
system.  Our  system  obtained  state-of-the-art  results  on  multilingual  All-Words  Word  Sense  Disambiguation  using 
Wikipedia as sense inventory, evaluated on the SemEval-2013 dataset [95], and on English All-Words Word Sense Dis-
ambiguation using WordNet as sense inventory, evaluated on the SemEval-2007 [111] and SemEval-2013 [95] datasets. 
Additionally, we performed an experiment to measure the reliability of our semantic representations for named entities, 
obtaining the best results among all unsupervised systems and near state-of-the-art performance on the SemEval-2015 
WSD dataset [88].

6.  Semantic similarity

Semantic  similarity  is  the  most  popular  benchmark  for  the  evaluation  of  different  semantic  representation  techniques. 
The task here is to measure the semantic closeness of two linguistic items. The similarity of two items can be directly com-
puted  by  comparing  their  corresponding  vector  representations.  As  we  mentioned  in  Section 3.5,  we  opted  for  Weighted 

46

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

Overlap  as  our  vector  comparison  method  for  lexical  and  uniﬁed  representations,  and  cosine  for  the  embedded  repre-
sentations. Note that by using our approach we obtain representations for individual BabelNet synsets. Moreover, because 
BabelNet merges different resources, our representations can be used to calculate the semantic similarity between any two 
semantic units within and across different resources, for instance between two Wikipedia pages, two WordNet synsets, or a 
Wikipedia page and a WordNet synset.

6.1.  Evaluation

We benchmark our semantic similarity procedure on the word similarity task. Word similarity is a speciﬁc task from se-
mantic similarity in which we measure how semantically close two words are. In order to be able to compute the similarity 
between words we ﬁrst need to map the two words to their corresponding synsets. However, this mapping is a straight-
forward process, thanks to the multilingual sense inventory of BabelNet. As frequently done in this task, we measure the 
similarity between two words  w and  w

as the similarity between their closest senses [115,13,106,17]:

(cid:15)

sim(w, w

(cid:15)

) =

max
(cid:5)v1∈Lw , (cid:5)v2∈L

(cid:15)

w

VC( (cid:5)v 1, (cid:5)v 2)

(12)

where Lw represents the set of synsets which contain  w as one of its lexicalizations. As vector comparison VC we use WO 
(see Section 3.5) to compare lexical and uniﬁed representations, and cosine for the embedded representations.

Note  that,  thanks  to  our  uniﬁed  representation,  w and  w

may  belong  to  different  languages.  Throughout  this  section 
on  the  tasks  based  on  semantic  similarity, NASARIlexical and NASARIuniﬁed represent  the  systems  based  on  the  lexical  and 
uniﬁed vectors, respectively. We refer to the combination of both lexical and uniﬁed vectors as NASARI. This combination 
is based on the average similarity scores given by lexical and uniﬁed vectors for each sense pair. We also report results of 
our NASARIembed vector representations which use the pre-trained Word2Vec vectors as input. We performed experiments 
on  monolingual  word  similarity  for  English  and  other  languages  (presented  in  Sections 6.1.1 and  6.1.2,  respectively)  and 
cross-lingual similarity (presented in Section 6.1.3). Additionally, we evaluate our embedded representations in a cross-level 
semantic similarity task in Section 6.1.4.

(cid:15)

6.1.1.  Monolingual word similarity: English
Datasets  The  majority  of  benchmarks  for  word  similarity  are  available  only  for  the  English  language.  We  compare  our 
approach  with  other  state-of-the-art  word  similarity  systems  on  standard  English  word  similarity  datasets.  We  chose  the 
standard  MC-30  [84],  WordSim-353  [33],  and  SimLex-999  [43] as  evaluation  benchmarks. MC-30 consists  of  a  subset  of 
RG-65 [117] which was re-annotated following new similarity guidelines. WordSim-353 consists of 353 word pairs, includ-
ing  both  concepts  and  named  entities.  In  the  original  WordSim-353  similarity  conﬂated  relatedness  in  the  same  dataset. 
In  order  to  avoid  this  conﬂation,  [1] cleverly  divided  the  dataset  into  two  subsets:  the  ﬁrst  one  concerned  relatedness 
while the second subset focused on similarity, the latter being the one used in our experiments. We will refer to this sim-
ilarity  subset  of  203  word  pairs  as WS-Sim henceforth.  Finally,  we  took  the  noun  pairs  from  the SimLex-999 dataset  as 
our last evaluation benchmark. The complete SimLex-999 dataset is composed of 999 word pairs, 666 of which are noun 
pairs.

Comparison systems  We  selected  state-of-the-art  approaches  which  are  available  online  as  comparison  systems.  These 
systems  can  be  split  into  two  categories:  knowledge-based  and  corpus-based.  As  knowledge-based,  we  selected  two  ap-
proaches  based  on  the  WordNet  semantic  graph:  [106, ADW]13 and  [69, Lin].14 Another  knowledge-based  approach  is 
[35, ESA],15 which represents a word in a semantic space of Wikipedia articles. We also compared our systems with four 
corpus-based  approaches.16 Firstly,  we  took  the  pre-trained  word embeddings of Word2Vec [81],17 the  same  used  for  our
Nasariembed system  (see  Section 4.2).  Then,  we  took  the  best  predictive  and  count-based  models  for  semantic  similarity 
released  by [8].18 The  best  predictive  model  is  based  on  Word2Vec  (Best-Word2Vec henceforth),  while  the  best  count-
based  models  (PMI-SVD)  are  traditional  co-occurrence  vectors  based  on  Pointwise  Mutual  Information  (PMI)  combined 
with  a  Singular  Value  Decomposition  (SVD)  dimensionality  reduction.  Finally,  we  benchmarked  our  system  against  two 
embedding-based sense representation approaches. The ﬁrst approach, Chen henceforth [19], leverages word embeddings, 
WordNet glosses and a WSD system for creating sense embeddings.19 The second one, called SensEmbed [48], uses Babel-

13 ADW implementation available at https :/ /github .com /pilehvar /ADW.
14 Results for Lin were obtained from the WS4J implementation available at https :/ /code .google .com /p /ws4j/.
15 ESA implementation available at DKProSimilarity package [7].
16 All the corpus-based approaches mentioned in the paper use cosine as comparison measure.
17 The pre-trained models are available at https :/ /code .google .com /p /word2vec/. They were trained on a Google News corpus of about 100 billion words.
18 Both models were trained on a 2.8 billion-token corpus including the English Wikipedia. They are available at clic.cimec.unitn.it/composes/semantic-
vectors.html.
19 The sense representations were downloaded from http :/ /pan .baidu .com /s /1eQcPK8i.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

47

Table 4
Pearson (r) and Spearman (ρ) correlations of different similarity measures with human judgements on RG-65, MC-30, WS-Sim and SimLex-999 (noun 
instances) datasets. We show the best performance obtained by [8] out of 48 conﬁgurations across different datasets including WS-Sim and RG-65 (high-
lighted by ‡). We show the SensEmbed conﬁguration tuned on the SimLex-999 dataset (highlighted by †). The inter-annotator agreement of the whole 
WordSim-353 (highlighted with (cid:13)) was reported to be 0.61, no inter-annotator agreement has been reported for the WS-Sim subset.

MC-30

WS-Sim

SimLex-999 (nouns)

Average

Nasari
Nasarilexical
Nasariuniﬁed
Nasariembed
ESA
Lin
ADW
Chen
Word2Vec

Best-Word2Vec
Best-PMI-SVD
SensEmbed

IAA

r

0.89
0.88
0.88
0.91
0.59
0.76
0.79
0.82
0.80

0.83‡
0.76‡
0.89

–

ρ

0.78
0.81
0.78
0.83
0.65
0.72
0.83
0.82
0.80

0.83‡
0.71‡
0.88

–

r

0.74
0.74
0.72
0.68
0.45
0.66
0.63
0.63
0.76

0.76‡
0.68‡
0.65

–

ρ

0.72
0.73
0.70
0.68
0.53
0.62
0.67
0.64
0.77

0.78‡
0.66‡
0.75

(cid:13)
0.61

r

0.50
0.51
0.49
0.48
0.16
0.58
0.44
0.48
0.46

0.48
0.40
0.46†

–

ρ

0.49
0.49
0.48
0.46
0.23
0.58
0.45
0.44
0.45

0.49
0.40
0.47†

0.61

r

0.71
0.71
0.70
0.69
0.40
0.67
0.62
0.64
0.67

0.69
0.61
0.67

ρ

0.67
0.68
0.65
0.66
0.47
0.64
0.65
0.63
0.67

0.70
0.59
0.70

Net as the main knowledge source and also relies on pre-disambiguated text by using a WSD system. We report the results 
of these last two methods when using the same closest senses strategy used by our systems.

Results  Table 4 shows Pearson and Spearman correlation performance of our systems and all comparison systems on the 
three  considered  datasets.20 Both  lexical  and  uniﬁed  vectors,  especially  the  lexical  ones,  prove  to  be  quite  robust  across 
datasets. The combination of both lexical and uniﬁed vectors does not show any noticeable improvement over the lexical 
vectors single-handed. Our system gets the highest average Pearson correlation among all systems, outperforming even the 
embedding-based approaches which use one dataset (SensEmbed) or two datasets (Best-Word2Vec) in order to tune their 
hyperparameters.21 In  terms  of  Spearman  correlation,  our  system  based  on  the  lexical  vectors  also  achieves  the  highest 
average performance among the systems which do not use any of the datasets for tuning with a single point advantage over 
Word2Vec. Nasariembed also  proves  to  be  quite  competitive,  outperforming  all  Word2Vec  approaches  in  terms  of  Pearson 
correlation and obtaining the best overall result on MC-30.

Lin, which does not perform particularly well on MC-30 and WS-Sim, surprisingly obtains the best overall performance 
on the SimLex-999 dataset, which is largest considered dataset, consisting of 666 noun pairs. Our system gets the second 
best overall performance on this dataset. A closer look at the output of the similarity scores given by our system compared 
to  the  gold  standard  shows  noticeable  errors  when  measuring  the  similarity  between  antonym  pairs,  which  are  heavily 
represented in this dataset. These antonym pairs were given consistently low values across the dataset, irrespective of the 
target words, whereas we argue that the similarity scores ought to vary according to the particular semantics of the antonym 
pairs. For instance, the pair day-night gets a score of 1.9 in the 0–10 scale, while our system gets a much higher 8.0 score.22
A similar  phenomenon  is  found  on  the  sunset–sunrise pair.  Nevertheless,  in  both  cases  the  words  in  the  pair  belong  to 
coordinate synsets in WordNet. In fact, recent works [121,105,91] have shown how signiﬁcant performance improvements 
can  be  obtained  on  this  dataset  by  simply  tweaking  usual  word  embedding  approaches  to  handle  antonymy.  This  differs 
from the scores given in the WordSim-353 dataset, in which antonym pairs were considered as similar [43]. It is outside 
the scope of this work to change this feature of our system in order to resolve its judgment differences with respect to the 
human annotation of antonym pairs in the SimLex-999 dataset.

6.1.2.  Multilingual word similarity
Datasets  We took the RG-65 dataset as evaluation benchmark. The language of this dataset was originally English [117]. 
It  was  later  translated  into  French  [54],  German  [39] and  Spanish  [15].  We  used  the  four  versions  of  the  dataset  for  our 
experiments.

Comparison systems  We  benchmark  our  system  against  other  multilingual  word  similarity  approaches. Wiki-wup [110]
and LSA-Wiki [38] are  systems  which  use  Wikipedia  as  their  main  knowledge  resource.  We  also  provide  results  for 
co-occurrence-based methods such as PMI and SOC-PMI [54] and for the newer word embeddings [31]. For word embed-

20 Inter-annotator agreement (IAA) is also reported for those datasets for which this information is available. IAA is reported in terms of average pairwise 
Spearman correlation.
21 [67] showed that with a ﬁne tuning, Word2Vec can achieve a 0.79 Spearman correlation performance on WS-Sim, higher than the 0.77 Spearman 
correlation reported by [8] on that dataset.
22 All scores have been converted to the 0–10 scale for this example.

48

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

Table 5
Pearson (r) and Spearman (ρ) correlation performance of different systems on the English, French, German and Spanish RG-65 datasets. The inter-annotator 
of the English RG-65 (highlighted with (cid:13)) was calculated for a subset of ﬁfteen annotators.

English

Nasari
Nasarilexical
Nasariuniﬁed
Nasariembed
SOC-PMI
PMI
LSA-Wiki
Wiki-wup
Word2Vec
Retroﬁtting

Nasaripoly-embed
Polyglot-embed

r

0.81
0.80
0.80
0.82
0.61
0.41
0.65
0.59
–
–

0.74
0.51

ρ

0.78
0.78
0.76
0.80
–
–
0.69
–
0.73
0.77

0.77
0.55

French

Nasari
Nasarilexical
Nasariuniﬁed
–
SOC-PMI
PMI
LSA-Wiki
–
Word2Vec
Retroﬁtting

Nasaripoly-embed
Polyglot-embed

r

0.82
0.80
0.82
–
0.19
0.34
0.57
–
–
–

0.60
0.38

ρ

0.73
0.70
0.76
–
–
–
0.52
–
0.47
0.61

0.69
0.35

German

Nasari
Nasarilexical
Nasariuniﬁed
–
SOC-PMI
PMI
–
Wiki-wup
Word2Vec
Retroﬁtting

Nasaripoly-embed
Polyglot-embed

r

0.69
0.69
0.71
–
0.27
0.40
–
0.65
–
–

0.46
0.18

ρ

0.65
0.67
0.68
–
–
–
–
–
0.53
0.60

0.52
0.15

Spanish

Nasari
Nasarilexical
Nasariuniﬁed
Nasariembed
–
–
–
–
Best-Word2Vec
–

Nasaripoly-embed
Polyglot-embed

r

0.85
0.85
0.82
0.79
–
–
–
–
0.80
–

0.68
0.51

ρ

0.79
0.79
0.77
0.77
–
–
–
–
0.80
–

0.74
0.56

IAA

(cid:13)
0.85

–

IAA

-

-

IAA

0.81

–

IAA

0.83

–

dings we report results for the Word2Vec model23 and for an approach retroﬁtting these Word2Vec vectors into WordNet 
(Retroﬁtting) [31]. For the Spanish language no result was reported in [31] for Word2Vec, so we trained Word2Vec with 
the same hyperparameters of Best-Word2Vec [8] on the Spanish Billion Words Corpus24 [18]. We used these Spanish word 
embeddings as input for our Nasariembed system in this language. Additionally, we report results for pre-trained embeddings 
in all four languages [5, Polyglot-embed].25 These vectors have sixty-four dimensions and were trained on the Wikipedia 
corpus. We also compare this system with our embedded representations of synsets by using the polyglot word embeddings 
as input continuous representations (see Section 3.3). We will refer to this latter method as Nasaripoly-embed.

Results  Table 5 shows  Pearson  and  Spearman  correlation  performance  of  our  systems  and  all  comparison  systems  on 
the  RG-65  word  similarity  datasets  for  English,  French,  German  and  Spanish.26 Our  system  outperforms  all  multilingual 
comparison  systems  in  English,  French  and  German  in  terms  of  both  Pearson  and  Spearman  correlation.  For  the  Span-
ish language our system surprisingly slightly outperforms the human inter-annotator agreement (which was calculated in 
terms of average pairwise Pearson correlation), hence demonstrating the competitiveness of our approach in this language 
too.

The Polyglot-embed multilingual representations do not show a particular potential for the task. The reason behind these 
results  may  be  due,  apart  from  the  inherent  ambiguity  of  words,  to  their  low  dimensionality  (64)  and  small  vocabulary 
(100K words). However, our embedded representation using these word embeddings (Nasaripoly-embed) hugely improves the 
original  vectors  (obtaining  an  average  twenty-three  Pearson  and  twenty-eight  Spearman  correlation  points  improvement).
Nasaripoly-embed, despite achieving lower results than our three representations, achieves competitive results with respect to 
other comparison systems, with the added beneﬁt of being applicable to many languages (pre-trained polyglot embeddings 
are available for more than a hundred languages).

6.1.3.  Cross-lingual word similarity
Datasets  We have chosen the RG-65 cross-lingual datasets released by [15] for English, French, German and Spanish. These 
datasets27 were  automatically  constructed  by  taking  the  manually-curated  multilingual  RG-65  datasets  from  the  previous 
Section as input. In total, we evaluated on six datasets consisting of all the possible language pair combinations for the four 
languages.

Comparison systems  As cross-lingual comparison systems, we have included the best results provided by the CL-MSR-2.0
system [60]. This system applies PMI on an English–French parallel corpus obtained from WordNet. Additionally, we provide 
results for some of the best performing systems in English word similarity by using English as a pivot language.28 Baseline 
pivot systems include the WordNet-based system ADW [106], the pre-trained Word2Vec word embeddings [81] and the top 
performing Word2Vec model in similarity obtained by [8] (Best-Word2Vec), and the best count-based model obtained by 
[8] (PMI-SVD). See Section 6.1.1 for more details on these comparison systems. We also report results for our system using 

23 For English, the pre-trained models of Word2Vec trained on a Google News corpus of 100 billion words were considered for the evaluation. For French 
and German, a corpus of a 1 billion tokens from Wikipedia was used for training.
24 Downloaded from http :/ /crscardellino .me /SBWCE/.
25 The pre-trained polyglot word representations were downloaded from https :/ /sites .google .com /site /rmyeid /projects /polyglot.
26 Inter-annotator agreement (IAA) is also reported for the languages for which this information is available. IAA is reported in terms of average pairwise 
Pearson correlation.
27 The cross-lingual datasets are available at http :/ /lcl .uniroma1.it /similarity-datasets/.
28 Non-English words are translated by using Google Translate.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

49

Table 6
Pearson (r) and Spearman (ρ) correlation performances of different similarity measures on the six cross-lingual RG-65 datasets. Notation: English (EN), 
French (FR), German (DE), Spanish (ES).

Measure

EN-FR

EN-DE

EN-ES

FR-DE

FR-ES

DE-ES

Average

Nasariuniﬁed
CL-MSR-2.0

Nasaripi vot
ADW pi vot
Word2Vecpi vot
Best-Word2Vecpi vot
Best-PMI-SVDpi vot

r

0.84
0.30

0.79
0.80
0.77
0.75
0.76

ρ

0.79
–

0.69
0.82
0.82
0.84
0.76

r

0.79
–

0.78
0.73
0.70
0.69
0.72

ρ

0.79
–

0.76
0.82
0.73
0.76
0.74

r

0.84
–

0.80
0.78
0.76
0.75
0.77

ρ

0.82
–

0.74
0.84
0.80
0.82
0.77

r

0.75
–

0.79
0.72
0.65
0.77
0.65

ρ

0.70
–

0.70
0.77
0.70
0.73
0.69

r

0.86
–

0.80
0.81
0.75
0.74
0.76

ρ

0.78
–

0.67
0.81
0.76
0.79
0.74

r

0.81
–

0.72
0.68
0.64
0.64
0.62

ρ

0.80
–

0.68
0.72
0.63
0.64
0.61

r

0.82
–

0.78
0.75
0.71
0.72
0.71

ρ

0.78
–

0.71
0.80
0.74
0.76
0.72

the combination of lexical and uniﬁed English Nasari vectors. We refer to all these systems using English as pivot language 
as pivot.

Results  Table 6 shows cross-lingual word similarity results according to Pearson and Spearman correlation performance. In 
this section we only report results for our uniﬁed vector representations, as their dimensions are BabelNet synsets, which are 
multilingual and therefore may be used for direct cross-lingual comparison. Our uniﬁed vector representations outperform 
all  comparison  systems  (both  types)  in  terms  of  Pearson  correlation  performance  except  for  the  French-German  pair,  in 
which our pivot system obtains the best result.  It is interesting to note that our English monolingual similarity proves to 
be the most robust across language pairs among all pivot systems according to Pearson correlation measure, demonstrating 
the reliability of our system also on a purely monolingual scheme. Pivot systems prove to be competitive, outperforming the 
only cross-lingual baseline which does not use a pivot language. In fact, despite obtaining relatively modest Pearson results, 
ADW obtains the best results according to the Spearman correlation measure (our uniﬁed vector representations obtain the 
second best result overall). In terms of the harmonic mean of Pearson and Spearman, used as oﬃcial measure in a previous 
semantic similarity SemEval task [57] and in previous works [41], our system outperforms ADW (second overall system) by 
three points (0.80 to 0.77), demonstrating the effectiveness of our direct cross-lingual word comparison with respect to the 
use of English as a pivot language.

6.1.4.  Cross-level semantic similarity

Finally,  we  evaluated  our  embedded  representations  on  the  word  to  sense  semantic  similarity  task.  Recall  from  Sec-
tion 4.2 that  our  embedded  vector  representations  share  the  same  space  with  word  embeddings.  Therefore,  in  order  to 
calculate the similarity between a word and a sense, we only have to compute the cosine similarity between their respec-
tive vector representations. In this experiment, we take the BabelNet sense representation of a word sense if it is modeled 
by Nasari. Otherwise, in order to increase the coverage of our system, we simply take the word embedding of the lemma 
of the word sense as its representation.

Dataset  As  our  benchmark  we  opted  for  the  Word to Sense (word2sense)  similarity  subtask  of  the SemEval-2014 Cross-
Level Semantic Similarity (CLSS)  task  [57].  The  subtask  provides  500  word–sense  pairs  for  its  test  dataset.  Each  pair  is 
associated  with  a  score  denoting  the  semantic  overlap  between  the  two  items.  From  the  dataset  we  took  the  subset  in 
which the senses were nominal29 (277 pairs). This dataset includes many words that are not usually integrated in a knowl-
edge source, such as slang words. Our embedded representation model is particularly suitable for this task as it provides a 
single semantic space for words and BabelNet senses, hence expanding the coverage far beyond the vocabulary of BabelNet.

Comparison systems  Thirty-eight  systems  participated  in  the  word2sense subtask.  We  compare  the  performance  of  our 
embedded  representations  with  the  three  best  performing  participating  systems  in  this  subtask. Meerkat Maﬁa [59] is  a 
system  that  relies  on  Latent  Semantic  Analysis  (LSA)  and  uses  external  dictionaries  to  handle  OOV  words. SemantiKLUE
[112] combines a set of different unsupervised and supervised techniques to measure semantic similarity. The third system, 
the most similar to our system, is SimCompass [80], which relies on deep learning word embeddings and uses WordNet as 
its only knowledge source.

Results  Table 7 shows  Pearson  and  Spearman  correlation  performance  of  the Nasari system  with  embedded  represen-
tations  together  with  the  three  comparison  systems.  Meerkat  Maﬁa  obtains  the  best  overall  performance  on  this  dataset. 
Our system is the second best system, outperforming the remaining 37 participating systems of the SemEval task. Interest-
ingly, Nasariembed provides a considerable improvement over SimCompass (0.09 and 0.07 in terms of Pearson and Spearman 
correlations, respectively), which is also based on word embeddings and uses WordNet as lexical resource.

29 Note that our embedded representations can be used to measure the similarity between words with any Part Of Speech tag.

50

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

Table 7
Pearson and Spearman correlation performance of 
different  systems  on  the  word2sense test  set  of 
SemEval-2014 task on Cross-Level Semantic Simi-
larity.

Nasariembed
Meerkat Maﬁa
SemantiKLUE
SimCompass

r

0.40
0.44
0.39
0.31

ρ

0.40
0.44
0.39
0.33

7.  Sense clustering

Our second application focuses on sense clustering. Some sense inventories suffer from a high granularity of their sense 
inventory. This high granularity could possibly affect the performance of applications based on their sense inventories [102]
and, hence, clustering their senses could be beneﬁcial.

Given our setup, we could seamlessly perform sense clustering in BabelNet, WordNet or Wikipedia. We follow the same 
procedure as semantic similarity for sense clustering. Following [23], we view sense clustering as a binary classiﬁcation task 
in which given a pair of senses the task is to decide if they have to be merged or not. In the usual setting of clustering, 
where senses which are semantically related are clustered together, we rely on our similarity scale and simply cluster a pair 
of items (synsets, senses or pages) together provided that their similarity exceeds the middle point in our similarity scale, 
i.e., 0.5 in the scale of [0, 1], and with a minimum overlap between vectors of ﬁve dimensions. In speciﬁc sense clustering 
settings, this middle-point threshold may be changed to another value, or determined using a tuning dataset.

7.1.  Evaluation: Wikipedia sense clustering

Given the high granularity of the Wikipedia sense inventory, clustering related senses may improve systems which take 
Wikipedia as their knowledge source [46]. Wikipedia-based Word Sense Disambiguation [77,24] is an example of an appli-
cation which may beneﬁt from this sense inventory clustering.

7.1.1.  Datasets

Wikipedia can be considered as a sense inventory wherein the different meanings of a word are denoted by the articles 
listed  in  its  disambiguation  page  [78].  Starting  from  these  Wikipedia  disambiguation  pages  and  with  the  help  of  human 
annotation,  [23] created  two  Wikipedia  sense  clustering  datasets.  In  these  datasets,  clustering  is  viewed  as  a  binary  clas-
siﬁcation  task  in  which  all  possible  pairings  of  senses  of  a  word  are  annotated  whether  they  should  be  clustered  or  not. 
The  ﬁrst  dataset,  which  we  will  refer  to  as 500-pair dataset,  contains  500  pairs,  357  of  which  are  set  to  belong  to  the 
same cluster or clustered, and the remaining 143 to not clustered. The second dataset, referred to as the SemEval dataset, 
is based on a set of highly ambiguous words taken from SemEval evaluations [77] and consists of 925 pairs, 162 of which 
are positively labeled (clustered). Parameter_(computer_programming)-Parameter and Fatigue(medical)-Fatigue(safety) are two 
sample pairs of Wikipedia pages that should be merged.

As explained above, our system is based on Semantic Similarity (see Section 6) for the sense clustering tasks. Two senses 
(in this case two Wikipedia pages) are set to be clustered if their similarity is greater than or equal to the middle point of 
our similarity scale (i.e., 0.5).

7.1.2.  Results

Our  experiments  are  carried  out  on  the  500-pair  and  SemEval  datasets.  We  set  two  naive  baselines:  one  considering 
all  the  pairs  as  positive  or  clustered  (Baselinecluster),  and  another  one  doing  the  opposite,  i.e.,  not  clustering  any  of  the 
test  pairs  (Baselineno-cluster).  We  also  compare  our  system  to  two  systems  proposed  by  [23].  Both  systems  exploit  the 
structure  and  content  of  the  Wikipedia  pages  by  using  a  multi-feature  Support  Vector  Machine  classiﬁer  trained  on  an 
automatically-labeled dataset. This ﬁrst system is totally monolingual (it only makes use of English Wikipedia pages), while 
the second system also exploits Wikipedia multilinguality.30 We will refer to the ﬁrst system as SVM-monolingual and to 
the second system as SVM-multilingual.

Table 8 shows  the  results  obtained  for  the  Wikipedia  sense  clustering  task  in  the  500-pair  and  SemEval  datasets.  The 
results are shown in terms of accuracy (number of correctly labeled pairs divided by total number of instance pairs) and 
F-Measure (harmonic mean of precision and recall). As we can see from the Table, our system in its unsupervised setting 
achieves  a  very  high  accuracy,  outperforming  both  systems  of  [23] on  the  SemEval  dataset  and  SVM-monolingual  on  the 
500-pair dataset. Only the supervised system of [23] using information of Wikipedia pages in different languages outper-
forms our main combined Nasari system in terms of accuracy (no F-Measure results were reported) by a narrow margin. Our 

30 For this second system we report their results for the system conﬁguration which exploits Wikipedia pages in four different languages (English, German, 
Spanish, and Italian).

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

51

Table 8
Accuracy (Acc.) and F-Measure (F1) percentages of different systems on the two 
manually-annotated English Wikipedia sense clustering datasets.

Measure

System type

500-pair

SemEval

Nasari
Nasarilexical
Nasariuniﬁed
Nasariembed
SVM-monolingual
SVM-multilingual
Baselineno-cluster
Baselinecluster

unsupervised
unsupervised
unsupervised
unsupervised
supervised
supervised
–
–

Acc.

83.8
81.6
82.6
81.2
77.4
84.4
71.4
28.6

F1

70.5
65.4
69.5
65.9
–
–
0.0
44.5

Acc.

87.4
85.7
87.2
86.3
83.5
85.5
82.5
17.5

F1

63.1
57.4
63.1
45.5
–
–
0.0
29.8

system, in any of the three variants, comfortably outperforms the naive baselines in terms of both accuracy and F-Measure. 
When  comparing  our  three  systems,  the  combination  of  both  lexical  and  uniﬁed  vectors  outperforms  both  single-handed 
components. However, both lexical- and uniﬁed- based systems (and embedding-based) also prove to be highly competitive 
single-handed, outperforming all baselines on the SemEval dataset, including the multilingual approach of [23].

8.  Domain labeling

Taking a BabelNet synset (or a Wikipedia page, or a WordNet synset) as input, the task in domain labeling consists of 
automatically tagging this synset or page with one of the domains in a given set. The domain labeling task has proven to 
be  useful  when  integrated  into  a  given  lexical  resource  [127,70] and  has  several  direct  applications,  such  as  Word  Sense 
Disambiguation [71,3,30] and Text Categorization [94]. WordNet version 3.0 has domains for some of its synsets. However, 
the  number  of  domains  used  is  quite  large  (357  domains)  and  they  are  not  uniformly  balanced.  For  instance,  there  is  a 
domain named Ethiopia containing a single synset, but no other domains referring to different countries are to be found. 
There are other domains with single synsets, such as Molecular Biology or Cytology, whereas some domains are annotated 
with  a  relatively  high  number  of  instances,  such  as  Law with  534  annotated  instances.  Moreover,  the  coverage  of  these 
domains is rather poor: only 4098 synsets have been annotated with at least one domain.

In this section we present a Nasari-based approach to automatically tag a much larger lexical resource (BabelNet) using 
a different set of domains and achieving signiﬁcantly higher coverage. Our creation of domain labels for BabelNet synsets 
relies on our lexical vectors.31 The ﬁrst step consists of creating a lexical vector for each domain. To this end, we follow the 
procedure which was explained in Section 3.2 and learn a lexical vector for each given domain. We do this by using sets of 
seed Wikipedia pages which characterize a given domain. As context for learning the vectors of a given domain we use the 
concatenations of all the texts corresponding to the Wikipedia pages of the seeds.

Then, in order to ﬁnd the domain of a synset we computed Weighted Overlap between the corresponding English Nasari
lexical  vector  and  the  lexical  vector  of  each  domain.  For  a  given  BabelNet  synset  s,  we  pick  the  domain  with  maximal 
similarity:

ˆd(s) = arg max

d∈D

W O (

(cid:5)Nasarilex(s), (cid:5)vlex(d))

(13)

(cid:5)Nasarilex(s) is the Nasari lexical vector of synset s and  (cid:5)vlex(d) is the lexical vector of the domain d. Similarly to the 
where 
sense clustering task, we tagged synsets with a domain provided that the minimum overlap between their respective lexical 
vectors  exceeded  ﬁve  dimensions.  For  notational  brevity,  we  will  refer  to  the  domain  of  synset  s whose  score  is  highest 
across all domains as its top domain.

Wikipedia domains and seeds  To  select  our  set  of  domains  we  used  Wikipedia  featured  articles,32 in  which  a  set  of  33 
domains  (e.g.  Animals,  Meteorology or  Music)  is  provided.  For  each  domain,  Wikipedians  have  selected  several  Wikipedia 
pages  which  best  represent  that  domain.  We  will  refer  to  these  Wikipedia  pages  already  tagged  with  a  domain  as  seeds. 
Each domain has a different number of available pre-tagged Wikipedia pages, ranging from only 9 (Mathematics domain) to 
189 (Media domain), totalling 4230 Wikipedia pages overall. From the set of domains we decided to remove the Companies
domain, while retaining the more general Business, economics, and ﬁnance domain, as we thought Companies might conﬂate 
with other domains. For instance, Nestlé or Toyota may be tagged with the Companies domain, but also with Food and drink
and Transport and travel, respectively. We also modiﬁed some domain names in order to make them more general by taking 
into account their given seeds. For example, the domain name Law was changed to Law and crime. The ﬁnal set of labels 
includes 32 different domains. Table 9 shows all these domains in alphabetical order.

31 We used lexical vectors because they were shown to perform better for English in Sections 6 and 7.
32 https://en.wikipedia.org/wiki/Wikipedia:Featured_articles.

52

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

Table 9
Our set of thirty-two domains.

Animals
Art, architecture, and archaeology
Biology
Business, economics, and ﬁnance
Chemistry and mineralogy
Computing
Culture and society
Education

Engineering and technology
Food and drink
Games and video games
Geography and places
Geology and geophysics
Health and medicine
Heraldry, honors, and vexillology
History

Language and linguistics
Law and Crime
Literature and theatre
Mathematics
Media
Meteorology
Music
Numismatics and currencies

Philosophy and psychology
Physics and astronomy
Politics and government
Religion, mysticism and mythology
Royalty and nobility
Sport and recreation
Transport and travel
Warfare and defense

By  applying  our  pipeline  on  the  Wikipedia  seeds  over  3.9 M  BabelNet  synsets  (from  a  total  of  4.4M  English Nasari
lexical  vectors)  were  tagged  with  at  least  one  domain.  Over  90%  of  the  500K  synsets  that  were  not  annotated  with  a 
domain label were isolated Wikipedia pages (i.e., pages that are not linked by any other Wikipedia page) composed of only 
a few sentences.

8.1.  Experiments

In this section we report our experiments on the domain labeling task. First, in Section 8.1.1 we explain the construction 
of our gold standard domain-labeled datasets. Then, we describe our baseline systems in Section 8.1.2 and compare them 
against our system on the newly created gold standard datasets in Section 8.1.3.

8.1.1.  Gold standard dataset construction

In order to evaluate the performance of our domain labeling approach we constructed two gold standard domain labeled 

datasets.

WordNet domain-labeled dataset  For the construction of this dataset, we took the WordNet 3.0 synsets which were man-
ually tagged with domains. The domain set of WordNet differs from our set of domains (see Table 9 for our ﬁnal domain 
set).  Therefore,  we  performed  a  manual  mapping  from  the  WordNet  domains  to  our  domain  set  in  order  to  make  them 
comparable.  Domains  in  WordNet  were  mapped  to  one  of  our  domains  provided  that  the  surface  form  of  the  WordNet 
domain matched the surface form of one of our domain labels. For instance, a WordNet synset whose domain was either 
Business, Economics or Finance was to be mapped to the domain Business, economics, and ﬁnance. There are WordNet synsets 
tagged with more than one domain in WordNet, but we considered only those with a single domain in WordNet for the gold 
standard construction. As a result, we obtained a gold standard dataset of 1540 WordNet synsets tagged with our domain 
set.33

BabelNet domain-labeled dataset  In  order  to  have  a  more  realistic  distribution  of  BabelNet  synsets  comprising  not  only 
synsets which belong to the WordNet sense inventory, we created a second gold-standard dataset based on BabelNet. For 
this, we randomly sampled 200 BabelNet synsets with at least one English lexicalization from the set of all 6.5M possible 
BabelNet synsets. Of these, 65% were integrated in Wikipedia and only 1.5% belonged to WordNet (the remaining synsets 
were  mostly  integrated  in  WikiData  only).  Two  annotators  manually  labeled  these  200  synsets.  They  were  instructed  to 
mark each synset with a single domain only. Any disagreements were adjudicated in a ﬁnal phase by the two annotators. 
The inter-annotator agreement was computed to be 86%, which may be viewed as an upper-bound for the performance of 
automatic systems.

8.1.2.  Comparison systems

As benchmark for our system, we developed three different baselines: two baselines based on Wikipedia and a third one 
propagating domains using a lexical resource taxonomy. Similarly to our approach, the ﬁrst two baselines construct a lexical 
vector for each domain. As seeds, the Wikipedia-based baselines used the same set of Wikipedia pages as our system. Lexical 
vectors were also computed for each Wikipedia page and the similarity between a Wikipedia page and each domain was 
calculated. Finally, the Wikipedia page with the top domain similarity score was selected. Vectors were constructed following 
a classic vector space model scheme in which the resulting vectors are individual content words and the similarity between 
vectors is calculated by using the standard cosine similarity measure. The only difference between the two baselines lays 
in  the  calculation  of  weights  for  each  dimension. Wikipedia-TF calculates  weights  on  the  basis  of  term  frequencies  (TF), 
whereas Wikipedia-TFidf combines term frequency with the conventional inverse document frequency weighting scheme 
[52, tf-idf ]. For these two Wikipedia-based systems, we relied on the mapping provided in BabelNet 3.0 between WordNet 
synsets and Wikipedia pages.

The third baseline system, Taxo-Prop henceforth, uses a taxonomy-based domain propagation. The system takes seeds 
from  the  respective  domain-labeled  gold  standard  datasets  (Section 8.1.1).  Algorithm 2 shows  the  process  for  obtaining  a 

33 There is no overlap between these 1540 WordNet synsets and the Wikipedia seeds taken by our system.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

53

Algorithm 2 Taxonomy-based Domain Propagation (Taxo-Prop).
Input: a non-tagged synset s, a set of domain-tagged synsets D, and a function T ax(s) which associates a synset s in the reference sense inventory with 

the set of its hyponyms and hypernyms in the taxonomy

S prev ← S
for each Synset s

(cid:15) ∈ S prev
for each Neighbour synset n ∈ T ax(s

Output: a domain tag for the input synset s
1: Set S ← {s}
2: Frequency domain dictionary F ← ∅
3: tie ← T rue
4: S prev ← ∅
5: while tie and |S prev | < |S|
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:

ˆd(s) = arg maxd∈F F [d]
if (|F | = 1) or (maxd∈F F [d] > max

Domain dn ← D(n)
if dn (cid:12)∈ F then
F [dn] ← 1

if n /∈ S then
S ← S ∪ {n}
if n ∈ D then

F [dn] ← F [dn] + 1

if |F | > 0 then

else

(cid:15))

tie ← F alse

20:
21: if ti e then
22:
23: else
24:

return ˆd(s)

return null

d∈F \{ˆd(s)} F [d]) then

domain  label  for  a  non-tagged  synset  s.  The  system  is  based  on  a  taxonomy  and  works  iteratively.  First,  it  goes  over  all 
the neighbors of  s in the taxonomy and checks whether they are tagged with a domain (lines 7–16 in the Algorithm). In 
the  case  where  a  particular  domain  ˆd is  encountered  more  often  than  any  other  domain  among  the  neighbors’  domains, 
s is tagged with  ˆd (line 24 in the Algorithm). Otherwise, we repeat the process and move up and down in the taxonomy, 
thereby  checking  for  the  domain  tags  of  the  neighbors of  the  neighbors.  We  repeat  this  until  a  domain  appearing  more 
frequently than any other domain is found (lines 5–20 in the Algorithm). In order to test the algorithm on the datasets we 
used both WordNet Taxo-Prop (WN) and BabelNet Taxo-Prop (BN) taxonomies and carried out 10-fold cross validation on 
the test dataset.34

Finally, we also compared with WN-Domains-3.2 [70,9], which is the latest released version of WordNet Domains.35 The 
system is in essence very similar to the Taxo-Prop system described above, in the sense that it takes seeds for each domain 
(manually  selected  for  synsets  that  are  located  high  in  the  taxonomy)  as  input,  and  spreads  them  through  the  WordNet 
taxonomy. This system involves an undetermined amount of manual intervention in the selection of seeds (“a small number 
of high level synsets are manually annotated with their pertinent Subject Code Fields36”), and manual curation (“the main problems 
are detected and the manual annotations are corrected”) [70]. WN-Domains-3.2 was released for WordNet 2.0. For testing it on 
the WordNet-based dataset we used the mapping between versions 2.0 and 3.0 of WordNet.37

8.1.3.  Results

Results are shown in Table 10 in terms of standard precision, recall and F-Measure. When comparing the two Wikipedia-
based  systems,  tf-idf proves  to  be  more  reliable  than  using  term  frequency  only,  but  its  performance  is  still  signiﬁcantly 
below our Nasari-based system. It is interesting to note that our system is robust across datasets, while Wikipedia-based 
approaches  experience  a  drastically  reduced  performance  on  the  BabelNet  dataset.  This  is  due  to  the  fact  that  Wikipedia 
pages associated with WordNet synsets are, in general, richer and longer than an average Wikipedia page (in the BabelNet 
dataset, synsets were extracted randomly). In contrast, Taxo-Prop achieves more competitive results, obtaining a lower pre-
cision than our system, but the highest overall recall on the WordNet dataset. However, this may lead to wrong conclusions. 
Given  that  the  coverage  of  our  system  is  actually  considerably  larger  than  all  the  synsets  covered  in  WordNet,  the  recall 
of our approach is in fact larger than any system relying on WordNet as its only knowledge resource (there are only 117K 
synsets  in  the  whole  of  WordNet).  Additionally,  the  WordNet-based  approach  has  the  advantage  of  annotating  in  exactly 
the same number of domains as occur in the gold standard dataset. Our system used 4230 Wikipedia pages of 32 different 
domains  as  seeds,  in  contrast  to  the  1386  domain-labeled  WordNet  synsets  of  27  different  domains  comprising  the  gold 

34 In order to make the results more reliable and less sensitive to the dataset order, we repeated this experiment ten times. The gold standard dataset 
was shuﬄed each time and the ﬁnal score was obtained by averaging the results of the ten different runs.
35 WordNet Domains are available at http :/ /wndomains .fbk.eu/.
36 Subject Code Fields corresponds to domain labels in our notation.
37 https :/ /wordnet .princeton .edu /wordnet /download /current-version/.

54

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

Table 10
Precision, recall and F-measure percentages of different systems on the gold standard WordNet and 
BabelNet domain-labeled datasets.

WordNet dataset

BabelNet dataset

Precision

Recall

F-Measure

Precision

Recall

F-Measure

Nasarilexical
Wikipedia-TF
Wikipedia-TFidf

Taxo-Prop (WN)
Taxo-Prop (BN)

77.9
25.4
45.9

71.3
73.5

WN-Domains-3.2

93.6

70.1
16.4
29.7

70.7
73.5

64.4

73.8
19.9
36.1

71.0
73.5

76.3

62.3
3.4
8.8

–
48.3

–

40.5
2.5
6.5

–
37.2

–

49.1
2.9
7.5

–
42.0

–

standard dataset. As a measure to show how much supervision each system was using in each case, we calculated its seed 
density, which is the percentage of synsets used on average for each domain as seeds. Formally, it is calculated as the ratio 
of the average number of seeds per domain to the total number of synsets in the given resource. In fact, the seed density is 
signiﬁcantly higher in the WordNet-based system (0.044% vs. 0.001% of our system).

WN-Domains-3.2  outperforms  our  system  in  terms  of  F-Measure  by  2.5  absolute  percentage  points  in  the  WordNet 
dataset.  Interestingly,  despite  using  as  benchmark  a  subset  of  WordNet,  our  system  obtains  a  higher  recall  than  WN-
Domains-3.2.  Additionally,  as  remarked  above,  our  system  annotates  a  signiﬁcantly  higher  number  of  instances,  including 
many more named entities and specialized concepts which are not covered by WordNet (over 3.9M domain-labeled synsets 
annotated by our system as opposed to the 74K synsets annotated by WN-Domains-3.2). In terms of precision, WN-Domains-
3.2,  which  involves  an  undetermined  amount  of  manual  curation,  outperforms  our  default  system.  However,  by  simply 
adding  a  conﬁdence  threshold,  our  system  can  considerably  increase  its  precision.  For  instance,  by  only  tagging  synsets 
whose top domain score is higher than the middle point of our similarity scale (i.e., 0.5), we obtain comparable results in 
terms of precision percentage (92.5%) to the WN-Domains-3.2 system, while still obtaining a considerably higher coverage.

Note that in both our system and WN-Domains-3.2, the range of domains considered in the original systems was larger 
than  the  number  of  domains  found  in  the  gold  standard,  which  increases  the  error  margin.  For  instance,  in  the  original 
setting of our system we considered 32 domains (see Table 9), of which only 27 were present in the gold standard dataset. 
By  analyzing  the  errors  given  by  our  system,  we  realized  that  there  are  synsets  that  might  be  tagged  with  more  than 
one domain. If we take the top three domain tags into account, the precision of our system increases to 91.8% and 83.1%, 
with recall being 82.7% and 54% in the WordNet and the BabelNet datasets, respectively. For example, our system tags the 
WordNet synset corresponding to the concept angular_velocity1
n with Mathematics as top domain by a narrow margin, but 
in  this  case  it  would  also  be  tagged  with  the  Physics and astronomy domain as  second  domain,  which  would  be  the  right 
answer according to the gold dataset. As a second source of error, we realized that it is arguable whether many of the false 
positives given by our system are, in fact, entirely wrong. Indeed, in many cases the judgement made by our system could 
be considered as justiﬁable, and equally correct to the tagging found in the gold dataset. For instance, the synset represented 
by the data processing sense of operation is tagged with the Mathematics domain, while the gold domain is Computing. In 
this case, it is clear that the synset could be tagged with either of the two domains. Another example is the WordNet synset 
aesthetics1
n, deﬁned in WordNet as The branch of philosophy dealing with beauty and taste (emphasizing the evaluative criteria that 
are applied to art), which is tagged with the Philosophy and psychology domain by our system instead of the Art, architecture, 
and archeology domain label found in the gold dataset.

9.  Word sense disambiguation

Word  Sense  Disambiguation  (WSD)  is  a  core  task  in  natural  language  understanding.  Given  a  target  word  in  context, 
the  task  consists  of  associating  it  with  an  entry  in  a  given  sense  repository  [93].  WSD  may  eventually  be  applied  to  any 
Natural Language Processing task, enabling an understanding of the sentences by the machine which is not usually achieved 
by  mainstream  statistical  approaches,  and  could  beneﬁt  applications  such  as  Machine  Translation  [134] and  Information 
Retrieval [120], to name but a few.

In  Section 9.1 we  present  the  different  resources  which  may  be  used  as  knowledge  repositories  for  WSD.  A  uniﬁed 

framework for WSD based on Nasari is presented in Section 9.2. Experiments are presented in Section 9.3.

9.1.  Sense inventories

One of the main knowledge sense repositories used in this task was the manually constructed WordNet [95,111], which 
usually leads to a ﬁne-grained type of disambiguation given the nature of the senses in WordNet. Another resource more 
recently used for this task is Wikipedia [78,24,95], due to its wide coverage of named entities and multilinguality. A newer 
resource used as a knowledge repository that is gaining popularity thanks to its multilinguality and large coverage is Babel-
Net [95,88,136], which is our main resource. Given the nature of our vectors, and in contrast to other WSD systems, we can 

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

55

seamlessly disambiguate in any of these resources (BabelNet integrates, among other resources, WordNet and Wikipedia). In 
the following section, we propose a uniﬁed framework for disambiguating words in context irrespective of the resource.

9.2.  Framework for word sense disambiguation

In [16] we presented a WSD framework in which we used the lexical vectors and then calculated the overlap between 
the target word vector and its context, harmonically weighting the ranks of the overlapping words in the target word vector. 
This method considers each word in context to be equally important (same weight) in the disambiguation process. In this 
section we present a more suitable approach which keeps to the spirit of previous lexical semantics applications and gives 
each word its weight in context.

Given a set of target words in a text T , we build a lexical vector for the context, as explained in Section 3.2. Then, for 
each target word  w in the text T , we retrieve the set of all the possible BabelNet synsets which have this target word as 
one of its lexicalizations, a set we refer to as Lw . Finally, we simply compute Weighted Overlap (see Section 3.5) between 
(cid:5)vlex(T ) (the lexical vector of the text T ) and the Nasari vectors corresponding to the BabelNet synsets that contain senses 
of  w.  In  our  setting,  the  top  BabelNet  synset  in  terms  of  WO  score  (ˆs)  is  selected  as  the  best  sense  of  the  given  target 
word:

ˆs = arg max

s∈Lw

W O ((cid:5)vlex(T ),

(cid:5)
Nasarilex(s))

9.3.  Experiments

(14)

We perform Word Sense Disambiguation experiments using two sense inventories: Wikipedia and WordNet. Recall from 
Section 9.1 that,  since  our  main  knowledge  sense  inventory  is  BabelNet,  we  can  seamlessly  disambiguate  instances  using 
either of these two knowledge sources. The setting of the system is the same in both cases, with only one difference: we 
use only BabelNet synsets38 which are mapped to Wikipedia page or WordNet synset when disambiguating with either of 
these resources, respectively.

As  has  often  been  done  in  the  literature  [132,142,89],  we  use  a  back-off  strategy  to  the  Most  Frequent  Sense  (MFS) 
baseline in the cases when our system does not provide a conﬁdent answer. Hence, in our WSD framework, we only tagged 
those  instances  whose  top  similarity  score  (see  Section 9.2 for  more  details  on  our  WSD  system)  is  higher  than  a  given 
threshold θ . In order to compute θ , we use the English Wikipedia trial dataset provided within the SemEval-2013 WSD task 
[95]. The top performing value of θ was 0.20, value that is used across all WSD experiments.39

Section 9.3.1 presents  multilingual WSD  experiments  using Wikipedia  as  main  sense  inventory  (a  task  that  is  strongly 
related  to  the  Wikiﬁcation task  [78]),  Section 9.3.2 presents  experiments  for  the  Named  Entity  Disambiguation  task  using 
BabelNet as sense inventory, and ﬁnally Section 9.3.3 presents the WSD results for English using WordNet as sense inventory.

9.3.1.  Multilingual word sense disambiguation using Wikipedia

We used the SemEval-2013 all-words WSD dataset [95] as benchmark for our multilingual evaluations.40,41 This dataset 
includes  texts  for  ﬁve  different  languages  (English,  French,  German,  Italian  and  Spanish)  with  an  average  of  1303  disam-
biguated instances per language, including multiword expressions and named entities.

Comparison systems  As  comparison  system  we  include Babelfy [89],42 a  state-of-the-art  graph-based  system  for  multi-
lingual  joint  WSD  and  Entity  Linking.  Babelfy  relies  on  random  walks  in  the  BabelNet  semantic  network  combined  with 
various  graph-based  heuristics.  We  also  report  results  for  the  best  run  on  every  language  of  the  top  SemEval-2013  sys-
tem [40, UMCC-DLSI]. As baseline, although diﬃcult to beat in some WSD tasks [93], we include the Most Frequent Sense 
(MFS43) heuristic. Finally, we report results from MUFFIN [16], our previous WSD system based on the Nasari vectors that, 
in contrast, used a WSD framework in which words in context were considered equally important.

38 In order to avoid disambiguating with synsets which are rarely used in practise and are isolated in the BabelNet graph, throughout all the experiments 
we only considered those BabelNet synsets with at least thirty edges in the BabelNet graph.
39 We considered values of θ from 0 to 1 with a step size of 0.05.
40 In our experiments we used the Wikipedia dump of December 2014, as opposed to the one used in the original SemEval-2013 dataset. A few Wikipedia 
page titles had been updated since the creation of the dataset, so we had to update these titles in the gold standard too.
41 We release this updated gold standard dataset in our website. Note that the Wikipedia page titles are the unique identiﬁers for a Wikipedia page, 
hence a change in a Wikipedia page title automatically modiﬁes this unique identiﬁer. For instance, the English Wikipedia page titled Seven-day week in the 
SemEval 2013 dataset has been updated in Wikipedia and is currently titled simply Week.
42 http :/ /babelfy.org/.
43 MFS was provided as baseline by the task organizers. However, the MFS score for French was ﬁxed with respect to [16], which showed a lower MFS 
F-Measure score. The scorer provided by the organizers was case-sensitive whereas a few Wikipedia page titles in the gold standard ﬁle did not match the 
casing of those in the baseline ﬁle, which were all lowercased. This led to misalignments between the gold standard and the baseline ﬁle.

56

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

Table 11
F-Measure percentage performance on the SemEval-2013 Multilingual WSD datasets using 
Wikipedia as sense inventory.

System
Nasarilexical
Muﬃn

Babelfy
UMCC-DLSI
MFS

English

French

Italian

German

Spanish

Average

86.3
84.5
87.4
54.8
80.2

76.2
71.4
71.6
60.5
74.9

83.7
81.9
84.3
58.3
82.2

83.2
83.1
81.6
61.0
83.0

82.9
85.1
83.8
58.1
82.1

82.5
81.2
81.7
58.5
79.3

Table 12
F-Measure  percentage  performance  on  the  English  Named  En-
tity Disambiguation dataset from the Multilingual All-Words Sense 
Disambiguation and Entity Linking SemEval-2015 task using Ba-
belNet as sense inventory.

System
Nasarilexical
DFKI
SUDOKU
el92
MFS

Type

unsupervised
supervised
unsupervised
systems mix
–

F-Measure

87.1
88.9
87.0
86.1
85.7

Results  Table 11 shows  F-Measure  percentage  results  for  our  system  and  all  comparison  systems  on  the  SemEval-2013 
dataset. As we can see from the table, although our system only achieves state-of-the-art results for French and German, it 
does achieve the best average performance among all languages, demonstrating its robustness across languages and outper-
forming the current state-of-the-art results of Babelfy. Our system outperforms our previous WSD approach Muﬃn by over 
a point on average, highlighting our improvements on this particular WSD task for which we proposed a new framework 
(see Section 9.2).

9.3.2.  English Named Entity Disambiguation using BabelNet

In order to evaluate the quality of our named entity representations, we performed experiments on the Named Entity 
Disambiguation task. Given that Nasari provides semantic representations for both concepts and named entities, this task 
was  analogous  to  Word  Sense  Disambiguation  (see  Section 9.2)  with  the  only  difference  being  that  in  this  task  we  only 
considered entity synsets as candidates. To this end, we used the English named entity dataset from the SemEval-2015 Task 
on All-Words Sense Disambiguation and Entity Linking [88]. This dataset consists of 85 named entities to disambiguate.

Comparison systems  We  benchmarked  our  disambiguation  system  against  the  top  three  best  performing  systems  in  the 
task, which were also the only ones outperforming the MFS baseline: DFKI [137], SUDOKU [72], and el92 [118]. DFKI is a 
multi-objective system based on both global unsupervised and local supervised objectives. SUDOKU uses the Personalized 
PageRank algorithm upon disambiguating monosemous instances within the text. Finally, el92 is based on a weighted voting 
of various disambiguation systems: Wikipedia Miner [87], TagME [32], DBpedia Spotlight [76], and Babelfy [89].

Results  Table 12 shows F-Measure percentage results on the Named Entity portion of the SemEval-2015 WSD dataset.44
Our system obtains the second overall position of all the 17 systems that participated in the SemEval-2015 Named Entity 
Disambiguation task. The combination of global unsupervised and local supervised objectives of DFKI obtains the best overall 
results.  As  we  show  in  Section 9.3.3 and  discuss  in  Section 9.4,  our  system,  based  solely  on  global  semantic  features, 
generally improves when including local supervised features.

9.3.3.  English Word Sense Disambiguation using WordNet

For the task of English WSD using WordNet as main sense inventory, we used two recent SemEval WSD datasets: ﬁne-
grained  all-words SemEval-2007 [111] and  all-words SemEval-2013 [95].  We  performed  experiments  on  the  162  noun 
instances of the SemEval-2007 dataset. SemEval-2013’s dataset contains 1644 instances.

Comparison systems  We include the state-of-the-art IMS system [143] as a supervised system. As unsupervised systems, 
we report the performance of two graph-based approaches that are based on random walks over their respective semantic 
networks: BabelNet [89, Babelfy] and WordNet [4, UKB]. Another approach that uses BabelNet as reference knowledge base 
is Multi-Objective [136] which  views  WSD  as  a  multi-objective  optimization  problem.  We  also  report  the  results  of  the 

44 We found an inaccuracy in an instance of the gold standard dataset. The unambiguous instance KAlgebra is disambiguated with the KAlgebra concept in 
the Catalan language, which belongs to a separate synset of the general KAlgebra concept in all languages. This instance is repeated nine times within the 
dataset. By ﬁxing this issue, our system achieves F-Measure results of over 90%.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

57

Table 13
F-Measure percentage performance on the SemEval-2013 and SemEval-2007 (noun 
instances) English all-words WSD datatets using WordNet as sense inventory (ﬁne-
grained).

System
Nasarilexical
Nasarilexical + IMS
Muﬃn

Babelfy
UKB
UMCC-DLSI
Multi-Objective
IMS
MFS

SemEval-2013

SemEval-2007

66.7
67.0
66.0
65.9
61.3
64.7
72.8
65.3
63.2

66.7
68.5
66.0
62.7
56.0
–
66.0
67.3
65.8

best conﬁguration of the top-performing system in the SemEval-2013 dataset, namely UMCC-DLSI [40]. As in Section 9.3.1, 
we also include our earlier WSD system MUFFIN for comparison. Finally, we include a system called NASARI+IMS, which is 
based on our WSD framework with the only difference being that in this system we back-off to IMS instead of MFS.45

Results  Table 13 shows  the  F-Measure  percentage  performance  of  all  systems  on  the  SemEval-2007  and  SemEval-2013 
WSD datasets. Similarly to the WSD results using Wikipedia as main sense inventory (Section 9.3.1), our system Nasari out-
performs our previous Muﬃn system. Nasari in its default setting backing-off to MFS is only surpassed by Multi-Objective 
in SemEval-2013 and IMS in SemEval-2017, outperforming the remaining systems in both datasets.

Our  system  backing-off  to  IMS  (Nasari+IMS)  improves  our  default Nasari system  in  both  datasets,  obtaining  the  best 
performance among all systems on the SemEval-2007 dataset. We remark that Nasari is an unsupervised system based on 
global contexts, while IMS is a supervised system based on local contexts. This combination of local and global contexts has 
already shown to be beneﬁcial for WSD tasks [45,107,136].

9.4.  Discussion: global and local contexts

Our  method  functions  by  analyzing  the  whole  context  of  a  given  target  word  which  has  to  be  disambiguated.  Hence, 
it  can  fail  to  capture  the  correct  intended  meaning  of  the  word  in  the  case  where  the  local  context  plays  a  key  role  in 
determining  a  speciﬁc  meaning  of  the  target  word,  especially  in  a  ﬁne-grained  disambiguation  setting.  For  instance,  the 
following example from the SemEval-2013 Word Sense Disambiguation test set shows a case where our system committed 
a mistake for the ﬁne-grained sense deﬁnition of the target word behaviour. However, we can see that the mistake can be 
solved by analyzing the local context which is the typical setting in supervised systems:

(1) The expulsion presumably forged by two players of Real Madrid (Xabi Alonso and Sergio Ramos) in the game played on 
the 23rd of November against Ajax in European Champions League has caused rivers of ink to be written about if such 
behaviour is or is not unsportmanlike and if, both players should be sanctioned by UEFA.

Our  system  was  not  able  to  make  a  conﬁdent  selection  among  the  sense  behaviour3

n (The aggregate of the responses or 
reactions or movements made by an organism in any situation) and behaviour4
n (Manner of acting or controlling yourself ), picking 
the latter by a narrow margin. In this case, we can leverage a system that exploits local contexts, such as IMS, for a more 
accurate disambiguation.

On the other hand, there are cases in which it is not possible to fully distinguish the intended meaning by just looking 
at  the  local  context  of  the  target  word.  The  following  sentence  from  the  SemEval  2013  dataset  is  an  example  for  such  a 
case:

(2) This way, and since Real Madrid will ﬁnish as leader of its group, both players will fulﬁll the prescribed sanction during 

the next game of league.

In this case, IMS picks sanction1

n (Formal and explicit approval), which is also the most frequent sense for the noun sanction. 
In fact the system is misled by just considering the local context and ignoring a more general picture that would be given by 
the global context. In this case, Nasari correctly captures the semantics within the text and chooses sanction2
n (A mechanism 
of social control for enforcing a society’s standards).

In  both  cases  the  combination  of Nasari and  IMS  provides  the  correct  answer.  In  general  the  combination  of  both 
methods  shows  a  consistent  improvement  over  the  single  system  components.  In  fact,  the  results  of  the  combination 
of  a  knowledge-based  global-context  disambiguation  system  (i.e., Nasari)  with  a  state-of-the-art  supervised  local-context 

45 The MFS baseline was obtained from the SemCor sense-annotated corpus [85].

58

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

Table 14
Ablation test results for different system settings. We report Pearson (r) and Spearman (ρ) correlations on RG-65, MC-30, WS-Sim and SimLex-999 (noun 
subset) word similarity datasets (columns 2–7). For Word Sense Disambiguation, F-Measure percentage performance is shown on the SemEval-2007 and 
SemEval-2013 datasets that use WordNet as their sense inventory (columns 8–9).

Word similarity

Word Sense Disambiguation

MC-30

WS-Sim

SL-999 (nouns)

SemEval-2007

SemEval-2013

Nasarilexical
Nasari-TFidf
Nasari-TFidf-3000d
Nasari-unif.weight

Nasariembed
Nasari-av.embed

r

0.88
0.84
0.85
0.86

0.91
0.81

ρ

0.81
0.77
0.79
0.79

0.83
0.75

r

0.74
0.71
0.72
0.73

0.68
0.58

ρ

0.73
0.71
0.72
0.72

0.68
0.63

r

0.51
0.46
0.48
0.49

0.48
0.40

ρ

0.49
0.46
0.47
0.48

0.46
0.41

F-Measure

F-Measure

66.7
66.0
66.0
66.0

–
–

66.7
66.1
65.9
66.4

–
–

approach  (i.e.,  IMS)  proves  to  be  quite  robust  across  datasets,  outperforming  many  strong  baselines  as  we  can  see  from 
Table 13.

10.  Analysis

In order to gain a better insight into the role some of the key components of our system’s pipeline play in the overall 
performance, we carried out an ablation test. In particular, we were interested in evaluating the impact and importance of 
the following three components:

1. Lexical speciﬁcity. To check how lexical speciﬁcity (see Section 3.1) fares against the standard tf-idf measure [52], we 
generated Nasari lexical vectors in which weights were calculated using the conventional tf-idf. Given a word  w, we 
calculate T F idf (w) as follows:

T F idf (w) = f (w) log

|D|
|{p ∈ D : w ∈ p}|

(15)

where  f (w) is the frequency of  w in the subcorpus SC s representing the contextual information of the synset  s (see 
Section 4.1) and  D is the set of all pages in Wikipedia. We computed two sets of tf-idf -based lexical vectors. The ﬁrst 
version,  called Nasari-TFidf,  keeps  all  the  dimensions  in  the  vector.  For  the  second  version, Nasari-TFidf-3000d,  we 
follow [37] and prune the vector to its top 3000 non-zero dimensions. This pruning is similar to the one performed au-
tomatically by lexical speciﬁcity, which reduces the number of non-zero dimensions while retaining the interpretability 
of the vector dimensions.

2. Weighted semantic relations. To  assess  the  advantage  we  gain  from  introducing  weights  to  semantic  relations  (see 
Section 4.1.1), we computed a version of our lexical vectors in which the semantic relations were uniformly weighted 
(i.e., λi = 1, ∀i ∈ {1, . . . , n} in Equation (11)), as was the case in our earlier work [16]. We will refer to this version as
Nasari-unif.weight.

3. Combination strategy of embeddings. Finally,  we  carried  out  an  analysis  to  compare  the  harmonic  combination  of 
word  embeddings  (see  Section 3.3)  against  uniform  combination  (i.e.,  averaging).  For  this  purpose,  we  computed  the 
embedding vector for a given synset as the centroid of all the embeddings of the words present in its corresponding 
lexical vector. We will refer to this variant as Nasari-av.embed in our tables.

We evaluated the ﬁrst two components in an intrinsic task (word similarity) as well as a downstream application (Word 
Sense Disambiguation). For the third component, we compared our default Nasariembed and the embedding representations 
obtained  through  uniform  weighting  in  the  word  similarity  task.  We  performed  the  evaluations  on  the  same  datasets  as 
those used in Section 6.1.1 for word similarity and in Section 9.3.3 for Word Sense Disambiguation with WordNet as sense 
inventory.  The  whole  pipeline  for  both  tasks  was  left  unchanged  for  all  variants,  except  for  the  components  mentioned 
above.

Table 14 shows the results of the ablation test on different word similarity and Word Sense Disambiguation datasets. Our 
default Nasarilexical system consistently outperforms all baselines in all datasets of both tasks, demonstrating the reliability 
of the proposed lexical speciﬁcity and the preweighting of the semantic relations. This result is especially meaningful taking 
into account that our default system is the one with the fewest non-zero dimensions on average among the four evaluated 
approaches. In fact, the average number of non-zero dimensions of our Nasarilexical vectors was 162, which is lower than the 
280 non-zero dimensions of Nasari-unif.weight, 1033 of Nasari-TFidf-3000d,46 and 1561 of Nasari-TFidf. This low average 
number of non-zero dimensions enables a fast processing of the vectors, i.e., they are computationally faster to work with.

46 In Nasari-TFidf-3000d the maximum number of non-zero dimensions is set to 3000, but in many cases the vector has actually a lower number of 
non-zero dimensions.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

59

As  far  as  the Nasariembed vectors  are  concerned,  our  default  system  consistently  obtained  signiﬁcantly  better  results 
when compared to the baseline (Nasari-av.embed). In general, Nasari-av.embed produces consistently high similarity values, 
even for non-similar pairs. This is due to the fact that words that are not very relevant to the input synset (i.e., relatively 
low lexical speciﬁcity values) are given the same weight as words that are clearly more relevant (i.e., high lexical speciﬁcity 
values). This, in turn, is why a weighted average of the word embeddings in the lexical vector leads to more accurate results 
than a simple average.

11.  Related work

In addition to the semantic representation of word senses, which is the main topic of this article, we brieﬂy review the 
recent literature on the two most popular applications on which we evaluated our representations: semantic similarity and 
Word Sense Disambiguation.

11.1.  Representation of word senses

Most research studies in semantic representation have so far concentrated on the representation of words, as can be seen 
from  the  numerous  available  word  similarity  datasets  and  benchmarks,  while  relatively  few  studies  have  focused  on  the 
representation of word senses or concepts. This is partly due to the so-called knowledge acquisition bottleneck that arises 
because  the  application  of  distributional  word  modeling  techniques  (which  are  the  prominent  representation  approach) 
at the sense level would require the availability of high-coverage sense-annotated data. However, word representations are 
known to suffer from some issues which dampen their suitability for tasks that require accurate representations of meaning. 
The most important drawback with word representations lies in their inability to model polysemy and homonymy, as they 
conﬂate different meanings that a word can have into a single representation [130,114]. For instance, a word representation 
for the word bank does not distinguish between the ﬁnancial institution and the river bank meanings of the word (the noun 
bank has ten senses according to WordNet 3.0). The approach of [31] which leverages semantic lexicons to improve word 
representations also suffers from the same drawback.

Because  they  represent  the  lowest  linguistic  level,  word  senses  and  concepts  play  a  crucial  role  in  natural  language 
understanding. Since at this level individual meanings of a word are identiﬁed and separately modeled, the resulting rep-
resentations are ideal for accurate semantic representation. In addition, the ﬁne-grained representation of word senses can 
be  directly  extended  to  higher  linguistic  levels  [13],  such  as  words,  which  makes  them  quite  interesting.  These  features 
have  recently  attracted  the  attention  of  different  research  studies.  Most  of  these  techniques  view  sense  representation  as 
a speciﬁc type of word representation and try to adapt the existing distributional word modeling techniques to the sense 
level,  usually  through  clustering  the  contexts  in  which  a  word  appears  [138,47,99].  The  fundamental  assumption  here  is 
that the intended meaning of a word mainly depends on its context and hence one can obtain sense-speciﬁc contexts for 
a given word sense by clustering the contexts in which the word appears in a given text corpus. Various clustering-based 
techniques usually differ in their clustering procedure and how this is combined with the representation technique. How-
ever, these models are often limited to representing only those senses that are covered in the underlying corpus. Moreover, 
the sense representations obtained using these methods are usually not linked to any sense inventory, and therefore such 
linking has to be carried out, either manually, or with the help of sense-annotated data if the representations are to be used 
for direct applications such as Word Sense Disambiguation.

Most  sense  modeling  techniques  have  based  their  representation  on  the  knowledge  derived  from  resources  such  as 
WordNet. Earlier techniques exploit the information provided in WordNet, such as the synonymous words in a synset, for 
the representation of word senses [79,2]. More recent approaches usually adapt distributional models to the sense level on 
the  basis  of  lexico-semantic  knowledge  derived  from  lexical  resources  such  as Wikipedia  [35,77], WordNet  [19,50,116] or 
other language-speciﬁc semantic networks [51]. WordNet is also viewed as a semantic network where its individual synsets 
are represented on the basis of graph-based algorithms [106]. Word Sense Disambiguation of large amounts of textual data 
has  also  been  explored  as  a  means  of  obtaining  high-coverage  annotated  data  for  learning  sense  representations  based 
on neural networks, a representation referred to as sense embeddings [48]. [19], which uses WordNet as main knowledge 
source, also relies on WSD for obtaining their sense representations. However, these two approaches are hampered by their 
inherently imperfect WSD systems.

Additionally, these techniques are often limited to the reduced coverage of WordNet and to the English language only. In 
contrast, our method provides a multilingual representation of word senses on the basis of the complementary knowledge of 
two different resources, enabling a signiﬁcantly higher coverage of speciﬁc domains and named entities. Our representations 
are not only multilingual, but can also be compared across languages through our uniﬁed representations.

11.2.  Semantic similarity

Semantic  similarity  between  word  senses  is  usually  computed  on  the  basis  of  the  structural  properties  of  lexical 
databases  such  as  WordNet  [6,13],  or  thesauri  such  as  Roget’s  [90,49].  These  measures  often  represent  a  lexical  resource 
as a semantic network and then exploit the networks for the computation of semantic similarity between a pair of word 
senses. The conventional WordNet-based similarity techniques take as their source of information either only the structural 

60

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

properties of the WordNet semantic network, such as graph distance and the lowest common super-ordinate of two word 
senses [44,65,139], or combine the structural information with statistics obtained from text corpora [115,69]. Collaboratively-
constructed  resources  such  as  Wikipedia  and  Wiktionary  have  also  been  used  as  underlying  lexical  resources  in  different 
semantic similarity techniques [41,126,86]. More recent sense similarity methods ﬁrst perform random walks on the seman-
tic networks [106,141,109] in order to model individual word senses and then use these representations for the computation 
of sense similarity. All these techniques, however, are limited to the knowledge provided by their underlying semantic re-
source. In contrast, our approach combines expert-based and encyclopedic knowledge from two different types of resource, 
providing  three  advantages:  (1)  more  effective  measurement  of  similarity  based  on  rich  semantic  representations,  (2)  the 
possibility of measuring cross-resource semantic similarity, i.e., between Wikipedia pages and WordNet synsets, and (3) the 
possibility of comparing the semantics of word senses across different languages.

11.3.  Word Sense Disambiguation

Word Sense Disambiguation is a task that can beneﬁt signiﬁcantly from the representation of word senses, mainly due to 
its sense-level application. Based on the type of resources they use, WSD techniques can be put into two main categories: 
knowledge-based and supervised [93]. Supervised systems receive sense-annotated data as their source of information, i.e., 
a  set  of  contexts  in  which  a  speciﬁc  sense  of  a  word  appears.  These  systems  analyze  the  provided  data  and  capture  the 
context in which a speciﬁc word sense is more likely to appear. It Makes Sense [143, IMS] is an example of a supervised 
system  which,  despite  using  a  small  set  of  conventional  features  and  a  simple  linear  classiﬁer,  has  been  among  the  best 
performers  on  different  WSD  benchmarks.  However,  the  performance  of  supervised  systems  very  much  depends  on  the 
availability of sense-annotated data for the target word sense [107]. Hence, the applicability of these systems is limited to 
those words and languages for which such data is available, practically restricting them to a small subset of word senses 
and  mainly  for  the  English  language  only.  Knowledge-based  approaches,  on  the  other  hand,  do  not  suffer  from  the  lack 
of  sense-annotated  data  and  therefore  provide  a  relatively  higher  coverage.  These  systems  usually  exploit  the  structural 
or  lexical-semantic  information  in  lexical  resources  for  disambiguation  [122,96,4].  However,  similarly  to  their  supervised 
counterparts, knowledge-based techniques are mostly limited to the English language only. Recent years have seen a growing 
interest  in  multilingual WSD  [95].  Multilinguality  is  usually  offered  by  methods  that  exploit  the  structural  information  of 
large-scale multilingual lexical resources such as Wikipedia [40,73,46]. Babelfy [89] is such a WSD system which performs 
random walks on the BabelNet multilingual semantic network [98] and makes use of densest subgraph heuristics. However, 
the approach is limited to the WSD and Entity Linking tasks. In contrast, our approach is global, as it can be used in different 
NLP tasks, including WSD and Entity Linking.

12.  Conclusions

In this article we presented Nasari, a novel technique for the representation of concepts and named entities in arbitrary 
languages. Our approach combines the structural knowledge from semantic networks with the statistical information derived 
from text corpora for effective representation of millions of BabelNet synsets, including WordNet nominal synsets and all 
Wikipedia pages. We evaluated our representations in a wide range of NLP tasks and applications: semantic similarity, sense 
clustering, Word Sense Disambiguation, and domain labeling. We reported state-of-the-art performance on several datasets 
across these tasks and in different languages.

Three type of sense representation were put forward: two explicit vector representations (uniﬁed and lexical), in which 
vector dimensions are interpretable, and a latent embedding-based representation. Each representation has its own advan-
tages and limitations. In general, a combination of lexical and uniﬁed vectors led to the most reliable results in the semantic 
similarity and sense clustering experiments (Sections 6 and 7). Among the three representations, the lexical representation 
(i.e., Nasarilexical) obtained the best performance in monolingual settings. However, although the lexical vectors are sparse 
and hence computationally faster to process, their dimensionality is high and equal to the size of the vocabulary. In con-
trast, our embedded representation (i.e., Nasariembed) has a ﬁxed low number of latent dimensions. Additionally, embedded 
synset vectors share the same space with the word embeddings used as input. As regards our uniﬁed representation (i.e.,
Nasariuniﬁed), not only does it provide an effective way for representing word senses in different languages, but, thanks to 
its uniﬁed semantic space, it also enables a direct comparison of different representations across languages. In addition to 
being multilingual, Nasari improves over the existing techniques by providing a high coverage for millions of concepts and 
named entities deﬁned in the BabelNet sense inventory.

Release  We  are  releasing  the  complete  set  of  representations  obtained  using  our  technique  for  ﬁve  different  languages 
(English, Spanish, French, German and Italian) at  http :/ /lcl .uniroma1.it /nasari, and we plan to generate representations for 
more languages in the near future. We also provide a Python script for the computation of lexical speciﬁcity. Finally, do-
main labels are included in the BabelNet 3.5 release version47 and the gold standard domain-labeled datasets used for our 
experiments (Section 8.1.1) are provided in the website.

47 BabelNet domain labels are based on Nasari and have been extended by using a set of taxonomy-based heuristics. BabelNet 3.5 includes 1.65M synsets 
annotated with at least one domain label.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

61

Future work  As future work we plan to pursue three main directions. Firstly, we aim to compute a global representation 
for each concept by exploiting the statistical information obtained from multiple languages. Secondly, we plan to develop a 
framework for a more meaningful combination of our representations in a supervised system for improved joint WSD and 
Entity Linking. Thirdly, we plan to integrate our multilingual semantic representations into different end-user applications, 
such as Machine Translation.

Acknowledgements

The authors gratefully acknowledge the support of the ERC Starting Grant MultiJEDI No. 259234.

References

[1] E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pa ¸sca, A. Soroa, A study on similarity and relatedness using distributional and WordNet-based ap-

[2] E.  Agirre,  O.L.  de  Lacalle,  Publicly  available  topic  signatures  for  all  WordNet  nominal  senses,  in:  Proceedings  of  LREC,  Lisbon,  Portugal,  2004, 

proaches, in: Proceedings of NAACL, 2009, pp. 19–27.

pp. 1123–1126.

[3] E. Agirre, O.L. de Lacalle, A. Soroa, Knowledge-based WSD on speciﬁc domains: performing better than generic supervised WSD, in: Proceedings of 

the 21st International Joint Conference on Artiﬁcial Intelligence, IJCAI, Pasadena, California, 2009, pp. 1501–1506.
[4] E. Agirre, A. Soroa, Personalizing PageRank for Word Sense Disambiguation, in: Proceedings of EACL, 2009, pp. 33–41.
[5] R. Al-Rfou, B. Perozzi, S. Skiena, Polyglot: distributed word representations for multilingual nlp, in: Proceedings of the Seventeenth Conference on 

Computational Natural Language Learning, Soﬁa, Bulgaria, 2013, pp. 183–192.

[6] S. Banerjee, T. Pedersen, An adapted Lesk algorithm for Word Sense Disambiguation using WordNet, in: Proceedings of the Third International Con-

ference on Computational Linguistics and Intelligent Text Processing, CICLing’02, Mexico City, Mexico, 2002, pp. 136–145.

[7] D. Bär, T. Zesch, I. Gurevych, DKPro similarity: an open source framework for text similarity, in: Proceedings of the 51st Annual Meeting of the 

Association for Computational Linguistics: System Demonstrations, Soﬁa, Bulgaria, August 2013, pp. 121–126.

[8] M. Baroni, G. Dinu, G. Kruszewski, Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors, in: 

Proceedings of ACL, 2014, pp. 238–247.

[9] L. Bentivogli, P. Forner, B. Magnini, E. Pianta, Revising the wordnet domains hierarchy: semantics, coverage and balancing, in: Proceedings of the 

Workshop on Multilingual Linguistic Resources, Association for Computational Linguistics, 2004, pp. 101–108.

[10] M.-B. Billami, J. Camacho-Collados, E. Jacquey, L. Kister, Annotation sémantique et validation terminologique en texte intégral en SHS, in: Proceedings 

of TALN, 2014, pp. 363–376.

[11] S. Bordag, Word sense induction: triplet-based clustering and automatic evaluation, in: Proceedings of the 11th Conference on European Chapter of 

the Association for Computational Linguistics, EACL, Trento, Italy, 2006, pp. 137–144.

[12] S. Brody, M. Lapata, Bayesian Word Sense Induction, in: Proceedings of EACL, 2009, pp. 103–111.
[13] A. Budanitsky, G. Hirst, Evaluating WordNet-based measures of lexical semantic relatedness, Comput. Linguist. 32 (1) (2006) 13–47.
[14] J. Camacho-Collados, M. Billami, E. Jacquey, L. Kister, Approche statistique pour le ﬁltrage terminologique des occurrences de candidats termes en 

texte intégral, in: Proceedings of JADT, 2014, pp. 121–133.

[15] J. Camacho-Collados, M.T. Pilehvar, R. Navigli, A framework for the construction of monolingual and cross-lingual word similarity datasets, in: Pro-
ceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language 
Processing – Short Papers, Beijing, China, 2015, pp. 1–7.

[16] J. Camacho-Collados, M.T. Pilehvar, R. Navigli, A uniﬁed multilingual semantic representation of concepts, in: Proceedings of the 53rd Annual Meeting 
of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, Beijing, China, 2015, 
pp. 741–751.

[17] J. Camacho-Collados, M.T. Pilehvar, R. Navigli, NASARI: a novel approach to a semantically-aware representation of items, in: Proceedings of NAACL, 

[18] C. Cardellino, Spanish billion words corpus and embeddings, http://crscardellino.me/SBWCE/, March 2016.
[19] X.  Chen,  Z.  Liu,  M.  Sun,  A  uniﬁed  model  for  word  sense  representation  and  disambiguation,  in:  Proceedings  of  EMNLP,  Doha,  Qatar,  2014, 

[20] R. Collobert, J. Weston, A uniﬁed architecture for natural language processing: deep neural networks with multitask learning, in: Proceedings of ICML, 

[21] C.J. Crouch, A cluster-based approach to thesaurus construction, in: Proceedings of the 11th Annual International ACM SIGIR Conference on Research 

and Development in Information Retrieval, SIGIR ’88, 1988, pp. 309–320.

[22] J.R. Curran, M. Moens, Improvements in automatic thesaurus extraction, in: Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition 

2015, pp. 567–577.

pp. 1025–1035.

2008, pp. 160–167.

– vol. 9, ULA ’02, 2002, pp. 59–66.

ing, Hissar, Bulgaria, 2013, pp. 164–171.

[23] B. Dandala, C. Hokamp, R. Mihalcea, R.C. Bunescu, Sense clustering using Wikipedia, in: Proceedings of Recent Advances in Natural Language Process-

[24] B. Dandala, R. Mihalcea, R. Bunescu, Word sense disambiguation using Wikipedia, in: The People’s Web Meets NLP, Springer, 2013, pp. 241–262.
[25] S.C. Deerwester, S.T. Dumais, T.K. Landauer, G.W. Furnas, R.A. Harshman, Indexing by latent semantic analysis, J. Am. Soc. Inf. Sci. 41 (6) (1990) 

[26] C. Delli Bovi, L. Telesca, R. Navigli, Large-scale information extraction from textual deﬁnitions through deep syntactic and semantic analysis, Trans. 

Assoc. Comput. Linguist. 3 (2015) 529–543.

[27] A. Di Marco, R. Navigli, Clustering and diversifying web search results with graph-based word sense induction, Comput. Linguist. 39 (3) (2013) 

[28] P. Drouin, Term extraction using non-technical corpora as a point of leverage, Terminology 9 (1) (2003) 99–115.
[29] K. Erk, A simple, similarity-based model for selectional preferences, in: Proceedings of the 45th Annual Meeting of the Association for Computational 

Linguistics, Prague, Czech Republic, 2007, pp. 216–223.

[30] S. Faralli, R. Navigli, A new minimally-supervised framework for domain word sense disambiguation, in: Proceedings of the 2012 Joint Conference on 

Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jeju, Korea, 2012, pp. 1411–1422.

[31] M.  Faruqui,  J.  Dodge,  S.K.  Jauhar,  C.  Dyer,  E.  Hovy,  N.A.  Smith,  Retroﬁtting  word  vectors  to  semantic  lexicons,  in:  Proceedings  of  NAACL,  2015, 

391–407.

709–754.

pp. 1606–1615.

62

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

[32] P. Ferragina, U. Scaiella, Tagme: on-the-ﬂy annotation of short text fragments (by Wikipedia entities), in: Proceedings of the 19th ACM International 

Conference on Information and Knowledge Management, ACM, 2010, pp. 1625–1628.

[33] L. Finkelstein, G. Evgenly, M. Yossi, R. Ehud, S. Zach, W. Gadi, R. Eytan, Placing search in context: the concept revisited, ACM Trans. Inf. Syst. 20 (1) 

(2002) 116–131.

pp. 1606–1611.

[34] T. Flati, D. Vannella, T. Pasini, R. Navigli, Two is bigger (and better) than one: the Wikipedia bitaxonomy project, in: Proceedings of the 52nd Annual 

Meeting of the Association for Computational Linguistics, Baltimore, USA, 2014, pp. 945–955.

[35] E.  Gabrilovich,  S.  Markovitch,  Computing  semantic  relatedness  using  Wikipedia-based  explicit  semantic  analysis,  in:  Proceedings  of  IJCAI,  2007, 

[36] P. Gärdenfors, Conceptual Spaces: The Geometry of Thought, The MIT Press, 2004.
[37] Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, S. Lazebnik, Improving image-sentence embeddings using large weakly annotated photo collections, in: 

[38] R. Granada, C. Trojahn, R. Vieira, Comparing semantic relatedness between word pairs in Portuguese using Wikipedia, in: Computational Processing 

Computer Vision—ECCV 2014, Springer, 2014, pp. 529–545.

of the Portuguese Language, 2014, pp. 170–175.

[39] I. Gurevych, Using the structure of a conceptual network in computing semantic relatedness, in: Proceedings of IJCNLP, 2005, pp. 767–778.
[40] Y. Gutiérrez, Y. Castañeda, A. González, R. Estrada, D.D. Piug, I.J. Abreu, R. Pérez, A. Fernández Orquín, A. Montoyo, R. Muñoz, F. Camara, UMCC_DLSI: 
reinforcing a ranking algorithm with sense frequencies and multidimensional semantic resources to solve multilingual word sense disambiguation, 
in: Proceedings of SemEval 2013, 2013, pp. 241–249.

[41] S. Hassan, R. Mihalcea, Semantic relatedness using salient semantic analysis, in: Proceedings of AAAI, 2011, pp. 884–889.
[42] S. Heiden, J.-P. Magué, B. Pincemin, et al., Txm: une plateforme logicielle open-source pour la textométrie—conception et développement, in: Statistical 
Analysis of Textual Data—Proceedings of 10th International Conference Journées d’Analyse Statistique des Données Textuelles, vol. 2, Rome, Italy, 2010, 
pp. 1021–1032.

[43] F. Hill, R. Reichart, A. Korhonen, Simlex-999: evaluating semantic models with (genuine) similarity estimation, arXiv:1408.3456, 2014.
[44] G. Hirst, D. St-Onge, Lexical chains as representations of context for the detection and correction of malapropisms, in: C. Fellbaum (Ed.), WordNet: An 

Electronic Lexical Database, MIT Press, 1998, pp. 305–332.

[45] J. Hoffart, M.A. Yosef, I. Bordino, H. Fürstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, G. Weikum, Robust disambiguation of named entities in text, 
in: Proceedings of the Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2011, pp. 782–792.
[46] E.H. Hovy, R. Navigli, S.P. Ponzetto, Collaboratively built semi-structured content and artiﬁcial intelligence: the story so far, Artif. Intell. 194 (2013) 

[47] E.H. Huang, R. Socher, C.D. Manning, A.Y. Ng, Improving word representations via global context and multiple word prototypes, in: Proceedings of 

2–27.

ACL, Jeju Island, South Korea, 2012, pp. 873–882.

[48] I. Iacobacci, M.T. Pilehvar, R. Navigli, Sensembed: learning sense embeddings for word and relational similarity, in: Proceedings of the 53rd Annual 
Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (volume 1: Long 
Papers), Association for Computational Linguistics, Beijing, China, 2015, pp. 95–105.

[49] M. Jarmasz, S. Szpakowicz, Roget’s thesaurus and semantic similarity, in: Proceedings of RANLP, 2003, pp. 212–219.
[50] S.K. Jauhar, C. Dyer, E. Hovy, Ontologically grounded multi-sense representation learning for semantic vector space models, in: Proceedings of NAACL, 

[51] R. Johansson, L.N. Pina, Embedding a semantic network in a word space, in: Proceedings of NAACL, 2015, pp. 1428–1433.
[52] K.S. Jones, A statistical interpretation of term speciﬁcity and its application in retrieval, J. Doc. 28 (1972) 11–21.
[53] M.P. Jones, J.H. Martin, Contextual spelling correction using latent semantic analysis, in: Proceedings of the Fifth Conference on Applied Natural 

[54] C. Joubarne, D. Inkpen, Comparison of semantic similarity for different languages using the Google n-gram corpus and second-order co-occurrence 

Language Processing, ANLC ’97, 1997, pp. 166–173.

measures, in: Advances in Artiﬁcial Intelligence, 2011, pp. 216–221.

[55] D. Jurgens, Embracing ambiguity: a comparison of annotation methodologies for crowdsourcing word sense labels, in: HLT-NAACL, 2013, pp. 556–562.
[56] D. Jurgens, R. Navigli, It’s all fun and games until someone annotates: video games with a purpose for linguistic annotation, Trans. Assoc. Comput. 

Linguist. 2 (2014) 449–464.

[57] D. Jurgens, M.T. Pilehvar, R. Navigli, Semeval-2014 task 3: cross-level semantic similarity, in: SemEval 2014, 2014, pp. 17–26.
[58] D. Jurgens, K. Stevens, Measuring the impact of sense similarity on Word Sense Induction, in: Proceedings of the First Workshop on Unsupervised 

Learning in NLP, EMNLP ’11, Edinburgh, Scotland, 2011, pp. 113–123.

[59] A. Kashyap, L. Han, R. Yus, J. Sleeman, T. Satyapanich, S. Gandhi, T. Finin, Meerkat maﬁa: multilingual and cross-level semantic textual similarity 
systems, in: Proceedings of the 8th International Workshop on Semantic Evaluation, Association for Computational Linguistics, 2014, pp. 416–423.
[60] A. Kennedy, G. Hirst, Measuring semantic relatedness across languages, in: Proceedings of xLiTe: Cross-Lingual Technologies Workshop at the Neural 

Information Processing Systems Conference, 2012.

[61] A.H.F. Laender, B.A. Ribeiro-Neto, A.S. da Silva, J.S. Teixeira, A brief survey of web data extraction tools, SIGMOD Rec. 31 (2) (2002) 84–93.
[62] P. Lafon, Sur la variabilité de la fréquence des formes dans un corpus, Mots 1 (1980) 127–165.
[63] T. Landauer, S. Dooley, Latent semantic analysis: theory, method and application, in: Proceedings of CSCL, 2002, pp. 742–743.
[64] T.K. Landauer, S.T. Dumais, A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction, and representation of knowl-

edge, Psychol. Rev. 104 (2) (1997) 211–240.

Lexical Database, MIT Press, 1998, pp. 265–283.

[65] C. Leacock, M. Chodorow, Combining local context and WordNet similarity for word sense identiﬁcation, in: C. Fellbaum (Ed.), WordNet: An Electronic 

[66] L. Lebart, A. Salem, L. Berry, Exploring Textual Data, Kluwer Academic Publishers, 1998.
[67] O. Levy, Y. Goldberg, I. Dagan, Improving distributional similarity with lessons learned from word embeddings, Trans. Assoc. Comput. Linguist. 3 

2015, pp. 683–693.

[68] J. Li, D. Jurafsky, Do multi-sense embeddings improve natural language understanding?, in: Proceedings of the 2015 Conference on Empirical Methods 

in Natural Language Processing, EMNLP, Lisbon, Portugal, 2015, pp. 1722–1732.

[69] D. Lin, An information-theoretic deﬁnition of similarity, in: Proceedings of the Fifteenth International Conference on Machine Learning, San Francisco, 

(2015) 211–225.

CA, 1998, pp. 296–304.

[70] B. Magnini, G. Cavaglia, Integrating subject ﬁeld codes into WordNet, in: LREC, 2000, pp. 1413–1418.
[71] B. Magnini, C. Strapparava, G. Pezzulo, A. Gliozzo, The role of domain information in word sense disambiguation, Nat. Lang. Eng. 8 (04) (2002) 

359–373.

[72] S.L. Manion, Sudoku: treating word sense disambiguation & entity linking as a deterministic problem—via an unsupervised & iterative approach, in: 

9th International Workshop on Semantic Evaluation, SemEval 2015, 2015, p. 365.

[73] S.L.  Manion,  R.  Sainudiin,  Daebak!:  peripheral  diversity  for  multilingual  word  sense  disambiguation,  in:  Proceedings  of  SemEval  2013,  2013, 

pp. 250–254.

[74] M. Matuschek, I. Gurevych, Dijkstra-WSA: a graph-based approach to word sense alignment, Trans. Assoc. Comput. Linguist. 1 (2013) 151–164.

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

63

[75] D. McCarthy, R. Navigli, The English lexical substitution task, Lang. Resour. Eval. 43 (2) (2009) 139–159.
[76] P.N. Mendes, M. Jakob, A. García-Silva, C. Bizer, Dbpedia spotlight: shedding light on the web of documents, in: Proceedings of the 7th International 

Conference on Semantic Systems, ACM, 2011, pp. 1–8.

[77] R. Mihalcea, Using Wikipedia for automatic Word Sense Disambiguation, in: Proceedings of NAACL-HLT-07, Rochester, NY, 2007, pp. 196–203.
[78] R. Mihalcea, A. Csomai, Wikify! Linking documents to encyclopedic knowledge, in: Proceedings of the Sixteenth ACM Conference on Information and 

Knowledge Management, Lisbon, Portugal, 2007, pp. 233–242.

[79] R.  Mihalcea,  D.  Moldovan,  An  automatic  method  for  generating  sense  tagged  corpora,  in:  Proceedings  AAAI  ’99,  Orlando,  Florida,  USA,  1999, 

[80] R. Mihalcea, J. Wiebe, Simcompass: using deep learning word embeddings to assess cross-level similarity, in: SemEval 2014, 2014, p. 560.
[81] T.  Mikolov,  K.  Chen,  G.  Corrado,  J.  Dean,  Eﬃcient  estimation  of  word  representations  in  vector  space,  CoRR,  arXiv:1301.3781,  http://

pp. 461–466.

arxiv.org/abs/1301.3781, 2013.

[82] T. Mikolov, I. Sutskever, K. Chen, G.S. Corrado, J. Dean, Distributed representations of words and phrases and their compositionality, in: Advances in 

Neural Information Processing Systems, 2013, pp. 3111–3119.

[83] G.A. Miller, R. Beckwith, C.D. Fellbaum, D. Gross, K. Miller, WordNet: an online lexical database, Int. J. Lexicogr. 3 (4) (1990) 235–244.
[84] G.A. Miller, W.G. Charles, Contextual correlates of semantic similarity, Lang. Cogn. Processes 6 (1) (1991) 1–28.
[85] G.A. Miller, C. Leacock, R. Tengi, R. Bunker, A semantic concordance, in: Proceedings of the 3rd DARPA Workshop on Human Language Technology, 

Plainsboro, N.J., 1993, pp. 303–308.

[86] D. Milne, I.H. Witten, An effective, low-cost measure of semantic relatedness obtained from Wikipedia links, in: Proceedings of the Workshop on 

Wikipedia and Artiﬁcial Intelligence: An Evolving Synergy at AAAI-08, Chicago, IL, 2008, pp. 25–30.

[87] D. Milne, I.H. Witten, Learning to link with Wikipedia, in: Proc. of CIKM-08, 2008, pp. 509–518.
[88] A. Moro, R. Navigli, Semeval-2015 task 13: multilingual all-words sense disambiguation and entity linking, in: Proceedings of SemEval-2015, 2015, 

pp. 288–297.

[89] A. Moro, A. Raganato, R. Navigli, Entity linking meets word sense disambiguation: a uniﬁed approach, Trans. Assoc. Comput. Linguist. 2 (2014) 

231–244.

[90] J. Morris, G. Hirst, Lexical cohesion computed by thesaural relations as an indicator of the structure of text, Comput. Linguist. 17 (1) (1991) 21–43.
[91] N. Mrkši ´c, D.Ó. Séaghdha, B. Thomson, M. Gaši ´c, L. Rojas-Barahona, P.-H. Su, D. Vandyke, T.-H. Wen, S. Young, Counter-ﬁtting word vectors to linguistic 

constraints, arXiv preprint, arXiv:1603.00892, 2016.

[92] R. Navigli, Meaningful clustering of senses helps boost word sense disambiguation performance, in: Proceedings of the 21st International Conference 

on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, 2006, pp. 105–112.

[93] R. Navigli, Word Sense Disambiguation: a survey, ACM Comput. Surv. 41 (2) (2009) 1–69.
[94] R. Navigli, S. Faralli, A. Soroa, O. de Lacalle, E. Agirre, Two birds with one stone: learning semantic models for text categorization and Word Sense 
Disambiguation, in: Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM, Glasgow, UK, 2011, pp. 2317–2320.
[95] R.  Navigli,  D.  Jurgens,  D.  Vannella,  SemEval-2013  task  12:  multilingual  word  sense  disambiguation,  in:  Proceedings  of  SemEval  2013,  2013, 

pp. 222–231.

[96] R. Navigli, M. Lapata, Graph connectivity measures for unsupervised Word Sense Disambiguation, in: Proceedings of IJCAI, 2007, pp. 1683–1688.
[97] R. Navigli, S.P. Ponzetto, BabelNet: building a very large multilingual semantic network, in: Proceedings of the 48th Annual Meeting of the Association 

for Computational Linguistics, ACL, Uppsala, Sweden, 2010, pp. 216–225.

[98] R. Navigli, S.P. Ponzetto, BabelNet: the automatic construction, evaluation and application of a wide-coverage multilingual semantic network, Artif. 

[99] A. Neelakantan, J. Shankar, A. Passos, A. McCallum, Eﬃcient non-parametric estimation of multiple embeddings per word in vector space, in: Pro-

Intell. 193 (2012) 217–250.

ceedings of EMNLP, Doha, Qatar, 2014, pp. 1059–1069.

(2007) 137–163.

[100] J.H. Neely, D.E. Keefe, K.L. Ross, Semantic priming in the lexical decision task: roles of prospective prime-generated expectancies and retrospective 

semantic matching, J. Exper. Psychol., Learn., Mem., Cogn. 15 (1989) 1003–1019.

[101] E. Niemann, I. Gurevych, The people’s web meets linguistic knowledge: automatic sense alignment of Wikipedia and WordNet, in: Proceedings of the 

Ninth International Conference on Computational Semantics, 2011, pp. 205–214.

[102] M. Palmer, H. Dang, C. Fellbaum, Making ﬁne-grained and coarse-grained sense distinctions, both manually and automatically, Nat. Lang. Eng. 13 (2) 

[103] P. Pantel, D. Lin, Discovering word senses from text, in: Proceedings of KDD, 2002, pp. 613–619.
[104] M. Pennacchiotti, D. De Cao, R. Basili, D. Croce, M. Roth, Automatic induction of FrameNet lexical units, in: Proceedings of the Conference on Empirical 

Methods in Natural Language Processing, EMNLP ’08, 2008, pp. 457–465.

[105] N.T.  Pham,  A.  Lazaridou,  M.  Baroni,  A  multitask  objective  to  inject  lexical  contrast  into  distributional  semantics,  in:  Proceedings  of  ACL,  2015, 

pp. 21–26.

[106] M.T. Pilehvar, D. Jurgens, R. Navigli, Align, disambiguate and walk: a uniﬁed approach for measuring semantic similarity, in: Proceedings of ACL, 2013, 

pp. 1341–1351.

40 (4) (2014) 837–881.

pp. 87–92.

2014, 2014, pp. 532–540.

[107] M.T. Pilehvar, R. Navigli, A large-scale pseudoword-based evaluation framework for state-of-the-art word sense disambiguation, Comput. Linguist. 

[108] M.T. Pilehvar, R. Navigli, A robust approach to aligning heterogeneous lexical resources, in: Proceedings of ACL, 2014, pp. 468–478.
[109] M.T. Pilehvar, R. Navigli, From senses to texts: an all-in-one graph-based approach for measuring semantic similarity, Artif. Intell. 228 (2015) 95–128.
[110] S.P. Ponzetto, M. Strube, Knowledge derived from Wikipedia for computing semantic relatedness, J. Artif. Intell. Res. 30 (2007) 181–212.
[111] S. Pradhan, E. Loper, D. Dligach, M. Palmer, SemEval-2007 task-17: English lexical sample, SRL and all words, in: Proceedings of SemEval, 2007, 

[112] T. Proisl, S. Evert, P. Greiner, B. Kabashi, Semantiklue: robust semantic similarity at multiple levels using maximum weight matching, in: SemEval 

[113] K. Radinsky, E. Agichtein, E. Gabrilovich, S. Markovitch, A word at a time: computing word relatedness using temporal semantic analysis, in: Proceed-

ings of the 20th International Conference on World Wide Web, WWW ’11, 2011, pp. 337–346.

[114] J. Reisinger, R.J. Mooney, Multi-prototype vector-space models of word meaning, in: Proceedings of ACL, 2010, pp. 109–117.
[115] P. Resnik, Using information content to evaluate semantic similarity in a taxonomy, in: Proceedings of IJCAI, 1995, pp. 448–453.
[116] S. Rothe, H. Schütze, Autoextend: extending word embeddings to embeddings for synsets and lexemes, in: Proceedings of the 53rd Annual Meeting 
of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (volume 1: Long Papers), 
Association for Computational Linguistics, Beijing, China, July 2015, pp. 1793–1803.

[117] H. Rubenstein, J.B. Goodenough, Contextual correlates of synonymy, Commun. ACM 8 (10) (1965) 627–633.
[118] P. Ruiz, T. Poibeau, El92: entity linking combining open source annotators via weighted voting, in: 9th International Workshop on Semantic Evaluation, 

SemEval 2015, 2015, pp. 355–359.

[119] G. Salton, A. Wong, C.S. Yang, A vector space model for automatic indexing, Commun. ACM 18 (11) (1975) 613–620.

64

J. Camacho-Collados et al. / Artiﬁcial Intelligence 240 (2016) 36–64

[120] H. Schütze, J. Pedersen, Information retrieval based on word senses, in: Proceedings of SDAIR’95, Las Vegas, Nevada, 1995, pp. 161–175.
[121] R. Schwartz, R. Reichart, A. Rappoport, Symmetric pattern based word embeddings for improved word similarity prediction, in: CoNLL 2015, 2015, 

[122] R. Sinha, R. Mihalcea, Unsupervised graph-based Word Sense Disambiguation using measures of word semantic similarity, in: Proceedings of ICSC, 

[123] R. Snow, B. O’Connor, D. Jurafsky, A. Ng, Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks, in: Proc. of 

pp. 258–267.

2007, pp. 363–369.

EMNLP-08, 2008, pp. 254–263.

[124] R. Snow, S. Prakash, D. Jurafsky, A.Y. Ng, Learning to merge word senses, in: Proceedings of the 2007 Joint Conference on Empirical Methods in 

Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL, Prague, Czech Republic, 2007, pp. 1005–1014.

[125] A. Søgaard, Ž. Agi ´c, H.M. Alonso, B. Plank, B. Bohnet, A. Johannsen, Inverted indexing for cross-lingual NLP, in: The 53rd Annual Meeting of the Asso-
ciation for Computational Linguistics and the 7th International Joint Conference of the Asian Federation of Natural Language Processing, ACL-IJCNLP 
2015, 2015, pp. 1713–1722.

[126] M. Strube, S.P. Ponzetto, WikiRelate! Computing semantic relatedness using Wikipedia, in: Proceedings of the 21st National Conference on Artiﬁcial 

Intelligence – vol. 2, AAAI’06, Boston, Massachusetts, 2006, pp. 1419–1424.

[127] D. Tuﬁ ¸s, R. Ion, L. Bozianu, A. Ceau ¸su, D.  ¸Stef˘anescu, Romanian wordnet: current state, new applications and prospects, in: Proceedings of 4th Global 

WordNet Conference, GWC, 2008, pp. 441–452.

[128] P.D. Turney, M.L. Littman, J. Bigham, V. Shnayder, Combining independent modules to solve multiple-choice synonym and analogy problems, in: 

Proceedings of Recent Advances in Natural Language Processing, Borovets, Bulgaria, 2003, pp. 482–489.

[129] P.D. Turney, P. Pantel, From frequency to meaning: vector space models of semantics, J. Artif. Intell. Res. 37 (2010) 141–188.
[130] A. Tversky, I. Gati, Similarity, separability, and the triangle inequality, Psychol. Rev. 89 (2) (1982) 123–154.
[131] D. Vannella, D. Jurgens, D. Scarﬁni, D. Toscani, R. Navigli, Validating and extending semantic knowledge bases using video games with a purpose, in: 

Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Baltimore, USA, 2014, pp. 1294–1304.

[132] F. Vasilescu, P. Langlais, G. Lapalme, Evaluating variants of the lesk approach for disambiguating words, in: LREC, 2004.
[133] J.N. Venhuizen, V. Basile, K. Evang, J. Bos, Gamiﬁcation for word sense labeling, in: Proceedings of the 10th International Conference on Computational 

semantics (IWCS 2013) – Short Papers, 2013, pp. 397–403.

[134] D. Vickrey, L. Biewald, M. Teyssier, D. Koller, Word sense disambiguation for machine translation, in: Proceedings of Conference on Empirical Methods 

in Natural Language Processing, Vancouver, Canada, 2005, pp. 771–778.

[135] W. Webber, A. Moffat, J. Zobel, A similarity measure for indeﬁnite rankings, ACM Trans. Inf. Syst. 28 (4) (2010) 1–38.
[136] D. Weissenborn, L. Hennig, F. Xu, H. Uszkoreit, Multi-objective optimization for the joint disambiguation of nouns and named entities, in: Proceed-
ings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language 
Processing (volume 1: Long Papers), Beijing, China, 2015, pp. 596–605.

[137] D. Weissenborn, F. Xu, H. Uszkoreit, Dfki: multi-objective optimization for the joint disambiguation of entities and nouns & deep verb sense disam-

biguation, in: 9th International Workshop on Semantic Evaluation, SemEval 2015, 2015, pp. 335–339.

[138] J. Weston, A. Bordes, O. Yakhnenko, N. Usunier, Connecting language and knowledge bases with embedding models for relation extraction, in: Pro-

[139] Z. Wu, M. Palmer, Verbs semantics and lexical selection, in: Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics, 

ceedings of EMNLP, Seattle, Washington, USA, 2013, pp. 1366–1371.

ACL ’94, Las Cruces, New Mexico, 1994, pp. 133–138.

[140] J. Xu, W.B. Croft, Query expansion using local and global document analysis, in: Proceedings of the 19th Annual International ACM SIGIR Conference 

on Research and Development in Information Retrieval, SIGIR ’96, 1996, pp. 4–11.

[141] E. Yeh, D. Ramage, C.D. Manning, E. Agirre, A. Soroa, WikiWalk: random walks on Wikipedia for semantic relatedness, in: Proceedings of the Workshop 

on Graph-Based Methods for Natural Language Processing, 2009, pp. 41–49.

[142] Z. Zhong, H.T. Ng, It makes sense: a wide-coverage Word Sense Disambiguation system for free text, in: Proceedings of the 48th Annual Meeting of 

the Association for Computational Linguistics, ACL, Uppsala, Sweden, 2010, pp. 78–83.

[143] Z. Zhong, H.T. Ng, It makes sense: a wide-coverage word sense disambiguation system for free text, in: Proceedings of the ACL System Demonstrations, 

2010, pp. 78–83.

