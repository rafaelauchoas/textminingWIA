ELSEVIER 

Artificial  Intelligence  103  (1998)  49-76 

Artificial 
Intelligence 

Vehicles  capable  of dynamic  vision: 
a new  breed  of technical  beings? 

Ernst  D.  Dickmanns 

’ 

Universiftit der  Bundeswehs  Munich,  D-85577  Neubiberg,  German_y 

Abstract 

A  survey 

in  the  field,  encompassing 

is  given  on  two  decades  of  developments 
computing  power  by  four  orders  of  magnitude.  The  ‘4-D  approach’ 
methods  from  systems  dynamics  and  control  engineering  with  methods  from  AI  has  allowed  to create 
in  the  technical  realm:  autonomous  road  vehicle  guidance  in 
vehicles  with  unprecedented  capabilities 
public  traffic  on  freeways  at  speeds  beyond  130 km/h,  on-board-autonomous 
landing  approaches  of 
aircraft,  and  landmark  navigation  for  AGV’s,  for  road  vehicles 
and  for  helicopters 
0  1998  Elsevier  Science  B.V. All  rights reserved. 

turn-offs  onto  cross-roads, 
in  the  latter  case). 

in low-level  flight  (real-time,  hardware-in-the-loop 

integrating  expectation-based 

an  increase 

simulations 

including 

in 

Keywords;  Machine vision;  Autonomous  vehicles;  Mobile robots; Dynamic scene  understanding;  Image 
processing 

1.  Introduction 

Road  vehicle  guidance  based  on  video-signal 

in  Japan  [39],  in  Europe  [26],  and  in  the  USA  [21].  While 

processing  has  been  picked  up  indepen- 
in  Japan  analog  signal 
dently 
processing  has  been  used  and  (quasi-steady)  AI-methods  predominated 
in  the  US,  recursive 
estimation  methods  well  known  from  systems  engineering  have  been  extended  to  image  se- 
the  resulting  method  has  been  dubbed 
quence  processing  at  the  author’s  institute 
‘4-D  approach’, 
then,  dis- 
in  the  problem  domain.  The  numerical 
regarding 
of  recursive  estimation  which  directly 
efficiency  and  compactness 
finally,  led  to  its  wide- 
allowed  control  applications 
in  the  vision  community.  Artificial  neural  nets  (ANN)  also  found  wide 
spread  acceptance 

(UBM); 
in  contrast  to  the  2-D,  2.5-D,  and  3-D  methods  under  discussion 

variable 
in  state  representation 
for  generating  behavioral  capabilities, 

time  as  the  fourth  independent 

’ Email: emst.dickmanns@unibw-muenchen.de. 

00043702/98/$ 
PII: SOOO4-3702(98)0007 

-see 

I -X 

front  matter  0  1998  Elsevier  Science  B.V. All rights  reserved 

acceptance 
1000  pixel  =  1 Kpel),  usually,  was  much  less  than  with  recursive  estimation 
image,  even  at  a  higher  image  rate). 

in  the  USA  13 l]  and  around  the  globe  even  though  image  resolution  used  (about 
(80  Kpel  per 

road  vehicles 

to  run  autonomously 

Both  methods  allowed 

control 
the  4-D  approach  allowed 

along  highways  and  other 
types  of  roads  up  to  rather  high  speeds,  initially  on  empty  roads  only  [7,30],  but  finally 
in  normal  freeway  traffic  also  [ 11,3 1,371; however,  while  ANN’s  stayed  confined 
to  either 
[ 171 at  a  time  (the  other  mode  had  to  be  controlled 
lateral  [25,3  l]  or  longitudinal 
the  spatio- 
to  detect,  track  and  determine 
by  a  human  driver), 
to  about 
on  a  3-D  surface) 
temporal 
a  dozen  other  objects 
the  own 
vehicle 
VITA  II  of  Daimler-Benz  and  VaMP  of  UBM  [ 1 1,401, may  well  be  considered  as  the first 
two  road  vehicles  of  a  new  species  capable  of  understanding 
and  of  reacting  properly 

to  the  actual  needs  on  their  own  (completely  autonomous). 

relative 
in  front  of  and  behind 

in  the  European  project  Prometheus: 

[ 121.  The  two  final  demonstrator 

in  a  range  of  up  to  100  meters 

(part  of)  their  environment 

and  velocity  components 

state  (position 

vehicles 

Dynamic 

remote  sensing 
requires 

for  intelligent  motion  control 
in  an  environment  with  rapidly 
changing  elements 
the  use  of  valid  spatio-temporal  models  for  efficient  handling 
of  the  large  data  streams  involved.  Other  objects  have  to  be  recognized  with  their  relative 
motion  components, 
this 
for  collision  avoidance; 
has  to  be  achieved  while  the  own  vehicle  body  carrying 
the  cameras  moves  in  an  intended 
way  and  is,  simultaneously, 

the  near  ones  even  with  high  precision 

subject  to  perturbations  hardly  predictable. 
to  vision 

in  addition 

sensing 

For  this  complex 

scenario, 
rate  feedback 

is  of  great  help; 
inertial 
to  a  viewing  direction  control  device  allows  to  stabilize 
negative  angular 
and 
in  the  image  sequence.  Measured  accelerations 
the  appearance  of  stationary  objects 
velocities  will,  via  signal 
and  rotational 
positions  affecting  the  perspective  mapping  process.  These  predictions  are  good  in  the  short 
inertial  sensors  are 
run,  but  may  drift  slowly  in  the  long  run,  especially  when  inexpensive 
used.  These  drifts.  however,  can  easily  be  compensated 
of  static 
scene  elements. 

yield  predictions 

for  translational 

interpretation 

integration, 

by  visual 

2.  Simultaneous 

representations 

on  differential  and  multiple 

integral  scales 

systems, 
In  order  to 
and  integral 

Combined  use  of  inertial  and  visual  sensing 

is  well  known 

from  biological 

introduced: 

simultaneous 

to  eyes  in  vertebrates. 

e.g.,  the  vestibular  apparatus  and  its  interconnections 
make  optimal  use  of  inertial  and  visual  signals, 
on  different  scales  both  in  space  and  in  time  are  being  exploited;  Table  1 
representations 
the  point  ‘here  and 
the  upper  left  corner  represents 
shows  the  four  categories 
of  a  sensor  or  an  actuator  with  the  real 
now’  in  space  and  time  where  all  interactions 
world  take  place.  Inertial  sensors  yield  information 
(arrow  1 from 
field  (1,l)  to  field  (3,3)  in  the  table)  and  turn  rates  of  this  point.  Within  a rigid  structure  of 
an  object  the  turn  rates  are  the  same  all  over  the  body;  therefore, 
rate  signals  (arrow  2 from  field  (1,3)  to  (3,3))  are  drawn  on  the  spatial  object  level  (row  3). 
The  local  surface  of  a  structure  may  be  described  by  the  change  of  its  tangent  direction 

the  inertially  measured 

on  local  accelerations 

differential 

along  some  arc  length; 
geometrical  characterization 

this  is  called  curvature  and  is  an  element  of  local  shape.  It  is  a 
form;  row  2  in  Table  1 

of  this  part  of  the  object  in  differential 

E.D.  Dickmanns 

/Art&id 

Intelligence 

103  (lYY8)  49-76 

Table  1 
Differential  and  integral  representations 

on  different  scales  for  dynamic  perception 

measuremen 

represents 
or  curved  ones)  in  the  image  under  certain  aspect  conditions. 

these  local  spatial  differentials  which  may  cause  specific  edge  features  (straight 

(represented 

the  feature  distribution 

to  be  local  spatial  integrals 

Single  objects  may  be  considered 

in  connection  with  the  aspect  conditions 

also  these  arrangements  of  objects  of  relevance 

for  behavior  decision  and  reactive  control.  For  this  reason, 

in  row  3  of 
on 
Table  I),  the  shapes  of  which  are  determined  by  their  spatial  curvature  distributions 
and  the  photometric  properties  of 
the  surface; 
in  the  image.  Since,  in  general,  several 
the  surface  they  determine 
objects  may  be  viewed  simultaneously, 
in 
a  task  context,  called  ‘geometrical  elements  of  a  situation’,  are  perceived  and  taken  into 
the  visual  data  input 
account 
process, 
labeled  by  the  index  3  at  the  corresponding 
field  (3,3),  has  three  components: 
features  not  yet  associated  with  an 
object, 
with  a  strong  predictive 
component 
other  objects.  Looked  at  this  way,  vision  simultaneously 
both  on  differential 
maneuvering, 

for  the  environment  which  preshapes  the  maneuver  space  for  the  self  and  all  the 

(row  2)  and  integral  scales  (rows:  3  for  a  single  objects,  4  for  local 

tracking  component 
and  (3~)  the  perception 

(3b)  the  object-oriented 
improvement, 

the  so-called  detection  component; 

and  5  for  mission  performance). 

arrows  into  the  central 

provides  geometrical 

(3a)  for  measured 

for  efficiency 

interpretation 

information 

element 

52 

E.D.  Dickmanns 

/Artificial 

Intelligence  103  (1998)  49-76 

Temporal  change 

is  represented 

in  column  2  which  yields 

the  corresponding 

time 

derivatives 
to  the  elements 
associated  with  numerical  differentiation 
Awcos(wt)), 
this  operation 
from  odometry; 
especially, 
image  points.  Even  on  the  feature 
effect,  as  used  in  recursive  estimation, 

in  the  column 

to  the  left.  Because  of  noise  amplification 

of  high  frequency 

signals 

is  usable  only  for  smooth  signals, 
it  is  avoided  deliberately 

to  do  optical 

like  for  computing 

(d/dt  (A  sin(wt))  = 
speed 
at 

flow  computation 

level,  the  operation  of  integration  with  a  smoothing 

is  preferred. 
In  the  matrix  field  (3,2)  of  Table  1 the  key  knowledge  elements  and  the  corresponding 
tools  for  sampled  data  processing  are  indicated:  due  to  mass  and  limited  energy  availability, 
in  the  real  world  are  constrained;  good  models  for  unperturbed  motion  of 
motion  processes 
in  the  natural  and  engineering 
to  specific  classes  are  available 
sciences 
objects  belonging 
rate  of  change  of  the  state  variables  on 
which  represent 
the  dependence  of  the  temporal 
‘dynamical  models’.  For 
both  the  state-  and  the  control  variables,  These  are  the  so-called 
constant  control  inputs  over  the  integration  period,  these  models  can  be  integrated 
to  yield 
difference  equations  which  link  the  states  of  objects 
in  column  3  of  Table  1 to  those  in 
the  gap  of  column  2;  in  control  engineering,  methods  and 
column  1,  thereby  bridging 
to  handle  all  problems  arising.  Once  the  states 
libraries  with  computer  codes  are  available 
at  one  point  in  time  are  known, 
time  derivatives  are  delivered  by  these 
models. 

the  corresponding 

Recursive  estimation 

techniques 

developed 

since 

the  60s  exploit 

this  knowledge  by 

making  state  predictions  over  one  cycle  disregarding  perturbations; 
models  are  applied  yielding  predicted  measurements. 
communicated 
efficiency 
from 
measured  features  then  yields  the  prediction  errors  used  for  state  update. 

to  improve 
(arrow  4  from  field  (3,3)  to  (1,3)  in  Table  1  on  the  object 

to  (2,3)  on  the  feature  extraction 

to  the  image  processing 

in  order 

stage 

(3,3) 

then,  the  measurement 

In  the  4-D  approach, 

these  are 

image  evaluation 
level,  and  arrow  5 

level).  A  comparison  with 

the  actually 

inputs; 

In  order  to  better  understand  what  is  going  to  happen  on  a  larger  scale,  these  predictions 
likely 
in  road  vehicle  guidance, 
to  have  a  longer 
term  state 
I  and  by  arrow  6;  Section  6 

may  be  repeated  several  to  many  times  in  a very  fast  in  advance  simulation  assuming 
control 
a  finite  sequence  of  ‘feed-forward’ 
transition  effect.  These  are  represented 
below  will  deal  with  these  problems. 

for  stereotypical  maneuvers 
control 

is  known 
in  field  (4,4)  of  Table 

like  lane  changes 
inputs 

of  perturbation 

is  used.  With 

effects,  direct  state  feedback  well  known 

For  the  compensation 
engineering 

control 
for  state  transition  of  the  closed  loop  system  can  be  specified 
characteristics 
and  row  4  in  Table  1). This  is knowledge  also  linking  differential 
representations 
ones;  low  frequency  and  high  frequency  components  may  be  handled  separately 
or  in  the  frequency  domain  (Laplace-transform) 
left  open  and  indicated  by  the  empty  row  and  column 

from 
and  damping 
(field  (3,4) 
to integral 
in  the  time 
as  usual  in  aero-space  engineering.  This  is 

theory,  eigenvalues 

in  Table  1. 

systems 

linear 

behavioral 

The  various  feed-forward  and  feedback  control  laws  which  may  be  used  in  superimposed 
If  a  sufficiently 
situations 
for  achieving 
is  given. 
in  Sections  6-8. 

modes  constitute 
rich  set  of  these  modes 
when 
to  activate 
mission  goals, 
This  is  represented  by  field  (n,  n)  (lower  right)  and  will  be  discussed 

vehicle. 
is  able  to  recognize 

capabilities  with  which  parameters 

of  the  autonomous 

and  if  the  system 

of  entire  missions 

these  behavioral 

for  autonomous 

the  capability 

performance 

is  available, 

capabilities 

E.D.  Dickmanns  /Artijicial 

Intdligence  103  (1998)  49-76 

53 

levels,  an  elegant  symbiosis  of  control  engineering 

requires  proper  sequencing  of  behavioral  capabilities 

symbolic 

representations 

in 
on  the  higher,  more  abstract 
can  thus  be 

and  AI-methods 

Essentially,  mission  performance 
the  task  context;  with  corresponding 
system 
realized. 

3.  Task  domains 

Though 

the  approach 

is  very  general  and  has  been  adapted  to  other  task  domains  also, 

only  road  and  air  vehicle  guidance  will  be  discussed  here. 

3.1.  Road  vehicles 

environments 

for  autonomous 

The  most  well  structured 

vehicles  are  freeways  with 
for  construction  parameters 
limited  access  (high  speed  vehicles  only)  and  strict  regulations 
level 
like  lane  widths,  maximum 
crossings.  For  this  reason,  even  though  speed  driven  may  be  high,  usually,  freeway  driving 
has  been  selected  as  the  first-task  domain  for  autonomous  vehicle  guidance  by  our  group 
in  1985. 

and  slopes,  on-  and  off-ramps,  no  same 

curvatures 

Six  perceptual 

and  behavioral 

capabilities 

recognition 

performance  on  freeways: 
(2)  obstacle 
stopping; 
changing  performance; 
information 
navigation 

On  well  kept  freeways 

(1)  lane  recognition 
reaction, 
and  proper 

and  mission 
for  navigation 
are  sufficient 
and  lane  following  with  adequate  speed; 
into  convoy  driving  or 
transition 
e.g., 
for  lane  change,  and  lane 
lanes,  their  availability 

traffic  signs;  (5)  reading  and  interpreting 
(4)  reading  and  obeying 
including  proper  lane  selection,  and  (6)  handling  entries  and  exits. 
to  check  surface  structure  or  to  watch 
to  unexpected 

from  the  side.  Nonetheless, 

it  is  usually  not  necessary 

safe  reaction 

(3)  recognition  of  neighboring 

for  humans  or  animals  entering 
events  must  be  required  for  a  mature  autonomous 

system. 

On  normal  state  roads  the  variability  of  road  parameters  and  of  traffic  participants 

much  larger;  especially,  same  level  crossings  and  oncoming 
between  objects, 
be  limited 
animals  are  normal 
and  surface  state  may  well  be  poor  on  lower  order  roads,  e.g.,  potholes,  especially 
transition  zone  to  the  shoulders. 

is 
traffic  increase  relative  speed 
speed  may 
as  well  as  many  kinds  of 
lane  width  may  be  less  in  the  average, 
in  the 

increasing  hazard  potential  even  though 

lower  level.  Bicyclists 

traffic  participants. 

and  pedestrians 

In  addition, 

to  a  much 

traveling 

thereby 

In  urban  traffic,  things  may  be  even  worse  with  respect  to  crowdedness 

subjects.  These  latter  mentioned 
autonomous  driving  because  of  scene  complexity  and  computing  performance 

to  be  not  yet  amenable 
required. 

are  considered 

environments 

roadways  with 

However,  driving  on  minor 

concrete  sealing,  has  been  attacked  for  research  purposes 
performed 
known 
done  including  obstacle  avoidance.  However, 
situations, 

traffic,  even  without  macadam  or 
in  the  past,  and  may  soon  be 
available  now.  If  it  is 
is  going  to  support  the  vehicle,  even  cross  country  driving  can  be 
in  these 

to  human  capabilities 
systems  can  compete. 

there  is  still  a long  way  to  go  until  autonomous 

computing  power  becoming 

safely  with  the  increasing 

that  the  ground 

if  compared 

little 

and  crossing  of 
to 

54 

E.D.  Dickmanns  /Artijcid 

Intelligence  103 (1998)  49-76 

3.2.  Air  vehicles 

As  compared 

to  ground  vehicles  with  3,  full  6  degrees  of  freedom  are  available 

for 

shaping,  here.  In  addition,  due  to  air  turbulence 

trajectory 
is 
environment  may  be  much  harder 
considered  mandatory 
like 
lanes  on  roads  are  not  available  once  the  aircraft  is  airborne  at  higher  altitudes.  Microwave 
electronic  guidelines  have  been  established 

in  addition,  visual  navigation  guidelines 

than  on  roads.  For  this  reason, 

in  this  task  domain; 

the  perturbation 

and  winds, 

instead. 

sensing 

inertial 

aircraft 

Vision  allows 

the  most  typical 

task  is  the  landing  approach 

the  pilot  or  an  autonomous 

to  certain 
to  navigate 
for  fixed 
to  a  prepared 
site  usually  marked  by  the  large  letter  H  for  a 
of  the  capabilities  of 
landing  aids  like  ILS  or  MLS  (or  D-GPS 
in  the  future),  machine  vision  also  allows  to  detect  obstacles  on  the  runway  and 

landmarks; 
wing  aircraft,  or  to  the  small 
helicopter.  These  tasks  have  been  selected  for  first  demonstrations 
seeing  aircraft.  Contrary 
possibly 
to  react  in  a  proper  manner. 

to  other  electronic 

relative 

runway 

landing 

For  flights  close  to  the  Earth  surface, 

terrain 

lines; 

and  power 

thus,  obstacle  avoidance 

as  buildings 
natural  extension  of  this  technique 
for  helicopters.  For  the  latter,  the  capability  of  recognizing 
ground  and  of  hovering 
will  improve  rescue  capabilities 

and  delivery  performance. 

in  a  fixed  position 

for  unmanned 

relative 

in  nap-of-the-Earth 

formations  may  be  recognized 

as  well 
is  a 
air  vehicles,  both  with  fixed  wings  and 
structures  or  objects  on  the 

flights 

to  these  objects  despite  perturbations, 

Motion  control 

for  fixed  wing  aircraft  and  for  helicopters 

from  each 
other;  by  the  use  of  proper  dynamical  models  and  control 
laws  it  has  been  shown  that  the 
4-D  approach  allows  to  turn  each  craft  into  an  autonomous  agent  capable  of  fully  automatic 
mission  performance.  This  will  be  discussed 

is  quite  different 

in  Section  8. 

4.  The  sensory  systems 

The  extremely  high  data  rates  of  image  sequences  are  both  an  advantage 

(with  respect  to 

in  acquiring  new  information  on  both  environment 

and  on  other  objects/subjects) 

versatility 
and  a disadvantage 
the  information 
rely  on  conventional 
variables  with  minimal 

time  delay. 

(with  respect  to  computing  power  needed  and  delay  time  incurred  until 
has  been  extracted  from  the  data).  For  this  reason  it  makes  sense  to  also 
sensors  in  addition,  since  they  deliver  information  on  specific  output 

4.1.  Conventional  sensor3 

speedometers 

as  well  as  sensors 

For  ground  vehicles,  odometers, 

like  actuators  and  pointing  devices  are  commonplace. 

and 
angles  of  subparts 
For  aircraft, 
pressure  measurement  devices  yield  information  on  speed  and  altitude  flown;  here,  inertial 
angular 
gyros  are 
sensors 
image  sequence 
standard.  Evaluating 
this  information 
processing  considerably.  Based  on  the  experience  gained 
the 
and  angular  rate  sensors  have  been  adopted 
inertial  sensors  like  accelerometers 
inexpensive 

rate-  and  vertical  as  well  as  directional 
in  conjunction  with  vision  alleviates 

in  air  vehicle  applications, 

like  accelerometers, 

for  positions 

E.D.  Dickmanns 

/Artificial 

fntrlligrncr 

103 (1998)  49-76 

Fig.  I. Binocular  camera  arrangement  of  VaMP  in front  of  the  rearward  looking  mirror  in the  center  of  the  upper 
part  of  the  front  wind  shield. 

for  road  vehicles 
vision.  Part  of  this  has  already  been  discussed 

too,  because  of  the  beneficial 

and  complementary 

effects  relative 
in  Section  2  and  will  be  detailed  below. 

to 

4.2.  Vision  semors 

for  practical  purposes. 

Because  of  the  large  viewing 

ranges  required,  a  single  camera  as  vision  sensor 
In  the  past,  bifocal  camera  arrangements 

is  by 
(see 
no  means  sufficient 
Fig.  1) with  a  wide  angle  (about  4.5”)  and  a  telecamera 
fix 
relative  to  each  other  on  a two-axis  platform  for  viewing  direction  control  have  been  used 
[ 12,291;  in  future  systems, 
field 
of  view  (>  100’)  from  two  divergently  mounted  wide  angle  cameras  and  a  3-chip  color 
CCD-camera  will  be  used  [ 131. For  high-speed  driving  on  German  Autobahnen, 
even  a 
fourth  camera  with  a relatively  strong  tele-lens  will  be  added  allowing 
at 
several  hundred  meters  distance. 

trinocular  camera  arrangements  with  a  wide  simultaneous 

(about  15”  aperture)  mounted 

lane  recognition 

All  these  data  are  evaluated  25  times  per  second,  the  standard  European  video  rate. 

4.3.  Global  positioning  system  (GPS-)  sensor 

For  landmark  navigation 

in  connection  with  maps  a  GPS-receiver  has  been  integrated 
into  one  system  in  order  to  have  sufficiently  good  initial  conditions 
for  landmark  detection. 
Even  though  only  the  least  accurate  C/A  code  is  being  used,  in  connection  with  inertial 
time  of 
sensing 
operation 

[ 181; GPS  signals  are  available  only  once  every  second. 

can  be  achieved  after  some 

good  accuracies 

interpretation 

and  map 

5.  Spatio-temporal 

perception: 

the  4-D  approach 

Since  the  late  7Os, observer  techniques  as developed 

used  at  UBM  in  the  field  of  motion  control  by  computer  vision 
H.J.  Wuensche  did  a thorough  comparison  between  observer-  and  Kalman  filter  realizations 

in  systems  dynamics 

[23]  have  been 
[26,27].  In  the  early  8Os, 

56 

E.D.  Dickmanns 

/Arti&ial 

Intelligence 

103  (1998)  49-76 

Fig.  2.  Multiple  feedback 
control  of  image  acquisition  and  -processing 
control  (lower  right  comer). 

loops  on  different  space  scales  for  efficient  scene  interpretation 
(lower  left  comer),  3-D  ‘imagination’-space 

and  behavior  control: 
in  upper  half;  motion 

applied 

to  vision 

in  recursive  estimation 
pendulum  on  an  electro-cart  by  computer  vision 
extended  Kalman 
root  formulation) 
as  standard  methods  to  all  dynamic  vision  problems  at  UBM. 

filter  (EKF)  with  numerical 
and  sequential  updates  after  each  new  measurement 

an  inverted 
[43].  Since  then,  refined  versions  of  the 
square 
(UDUT-factorization, 
have  been  applied 

task  of  balancing 

for  the  original 

stabilization 

Based  on  experience  gained  from  ‘satellite  docking’  [44],  road  vehicle  guidance,  and  on- 
board  autonomous  aircraft  landing  approaches  by  machine  vision,  it  was  realized  in  the  mid 
for  several  aspects  of 
80s  that  the  joint  use  of  dynamical  models  and  temporal  predictions 
the  overall  problem  in  parallel  was  the  key  to  achieving  a quantum  jump  in  the  performance 
level  of  autonomous 
for  the 
physical  objects  observed  and  control  computation  based  on  these  estimated  states  it  was 
thus  gained  to  the  image  feature  extraction  and  to  the  feature 
the  feedback  of  knowledge 
aggregation 
in  efficiency  of  image  sequence  evaluation 
of  one  to  two  orders  of  magnitude. 

systems  based  on  machine  vision.  Beside  state  estimation 

(See  Fig.  2 for  a  graphical  overview.) 

level  which  allowed  for  an  increase 

Following 
determining: 

state  prediction, 

the  shape  and  the  measurement  models  were  exploited 

for 

l  viewing  direction  control  by  pointing 
l  locations 

in  the  image  where  information 
state  estimation  could  be  found  (feature  selection); 

the  two-axis  platform  carrying 

the  cameras; 

for  most  easy,  nonambiguous 

and  accurate 

E.D.  Dickmanns  /Art&ial 

Intelligence  103  (1998)  49-76 

51 

. 

the  orientation  of  edge  features  which  allowed  to  reduce  the  number  of  search  masks 
and  directions 
the  length  of  the  search  path  as  function  of  the  actual  measurement  uncertainty; 

for  robust  yet  efficient  and  precise  edge  localization; 

for  efficient 

feature  aggregation 

guided  by  the  idea  of  the  ‘Gestalt’  of 

. 
.  strategies 

. 

objects,  and 
the  Jacobian  matrices  of  first-order  derivatives 
state  components 
interpretation 
constraints, 
This  integral  use  of: 

to 
for 
of  the  motion  process  in  a  least  squares  error  sense,  given  the  motion 

in  the  dynamical  models  which  contain 

of  feature  positions 

information 

relative 

rich 

the  features  measured,  and  the  statistical  properties  known. 

(1)  dynamical  models 

for  motion  of  and  around 

the  center  of  gravity 

taking  actual 

control  outputs  and  time  delays  into  account; 

(2)  spatial  (3-D)  shape  models  for  specifying  visually  measurable 
(3)  the  perspective  mapping  models,  and 
(4)  prediction  error  feedback 

for  estimation  of  the  object  state  in  3-D  space  and  time 
and  in  closed  loop  form  was  termed  the  ‘4-D  approach’.  It is  far  more 
in 

simultaneously 
than  a recursive  estimation  algorithm  based  on  some  arbitrary  model  assumption 
some  arbitrary  subspace  or  in  the  image  plane. 

features; 

It  is  estimated 
papers  referring 
temporal  models  based  on  physical  processes. 

from  a  scan  of  recent  publications 
in  the  field  that  even  today  most  of  the 
to  ‘Kalman  filters’  do  not  take  advantage  of  this  integrated  use  of  spatio- 

Initially, 

in  our  applications 

just  the  ego-vehicle  has  been  assumed 

to  be  moving  on  a 

control 

relative 

allow  a  wide  simultaneous 

(skew)  stereo  interpretation, 

and  a  small  area  with  high  image  resolution 

smooth  surface  or  trajectory,  with  the  cameras  fixed  to  the  vehicle  body.  In  the  meantime, 
to  rather  general  scenarios  are  available  with  several  cameras  spatially  arranged 
solutions 
on  a  platform  which  may  be  pointed  by  voluntary 
to  the  vehicle  body. 
field  of  view,  a  central  area  for 
These  camera  arrangements 
trinocular 
for 
‘tele’-vision.  The  vehicle  may  move  in  full  6  degrees  of  freedom;  while  moving,  several 
other  objects  may  move  independently 
in  front  of  a  stationary  background.  One  of  these 
objects  may  be  ‘fixated’  (tracked)  by  the  pointing  device  using  inertial  and  visual  feedback 
signals 
image.  A  newly 
appearing  object  in  the  wide  field  of  view  may  trigger  a fast  viewing  direction  change  such 
that  this  object  can  be  analysed 
this  corresponds 
to  ‘saccadic’  vision  as  known  from  vertebrates  and  allows  very  much  reduced  data  rates 
trades  the  need  for  time-sliced  attention  control 
for  a complex  sense  of  vision.  It  essentially 
against  a data  rate  reduction  of  l-2  orders  of 
and  sampled-data  based  scene  reconstruction 
magnitude  as  compared 

in  more  detail  by  one  of  the  tele-cameras; 

the  object  (almost)  centered 

in  the  entire  simultaneous 

in  the  high  resolution 

to  full  resolution 

field  of  view. 

for  keeping 

The  4-D  approach  lends  itself  for  this  type  of  vision  since  both  object-orientation 

temporal 
(‘dynamical’)  models  are  available 
design  for  dynamic  vision  has  been  termed  EMS-vision 
focal  and  Saccadic); 
miniature  TV-cameras  on  a two-axis  pointing  platform  named  ‘Multi-focal  active/reactive 
Vehicle  Eye’  MarVEye 

and  the 
in  the  system  already.  This  complex  system 
(from  Expectation-based,  Multi- 
set  of  four 

implemented  with  an  experimental 

it  is  actually  being 

[ 131. 

58 

E. D.  Dickmanns 

/  Art@ial 

Intelligence 

103  ( 1998) 49-76 

In  the  rest  of  the  paper,  major  developmental 

decade  and  results  achieved  will  be  reviewed.  As  an  introduction, 
summarize 

the  basic  assumptions  underlying 

the  4-D  approach. 

steps  in  the  4-D  approach  over  the  last 
in  the  next  section  we 

5.1.  Basic  assumptions  underlying 

the  4-D  approach 

It  is  the  explicit  goal  of  this  approach  to  take,  as  much  as  possible,  advantage  of  physical 
in 

in  the  real  world.  Models  developed 

over  the  last  centuries, 

in  simulation 

technology 

(decision  and  control)  over  the  last  decades  form  the  base  for 

and  mathematical  models  of  processes  happening 
the  natural  sciences  and  in  engineering 
and  in  systems  engineering 
computer-internal 
(1)  The 

(mesoscopic)  world  observed 

representations 

of  real-world  processes: 

independent 
describing 

variables; 
these  processes. 
(2)  All  interactions  with  the  real  world  happen 

nonrelativistic 

happens 

in  3-D  space  and 

(Newtonian)  models 

time  as  the 
for 

are  sufficient 

‘here  and  now’,  at  the  location  of  the 
body  carrying  special  input/output  devices;  especially 
the  locations  of  the  sensors 
(for  signal  or  data  input)  and  of  the  actuators  (for  control  output)  as  well  as  those 
body  regions  with  strongest 
the  wheels 
of  ground  vehicles)  are  of  highest  importance. 

interaction  with  the  world  (as,  for  example, 

(3)  Eficient 

interpretation  of  sensor  signals  requires  background 

the  processes 
characteristics. 
components  not  graspable  at  one  point  in  time.  Similarly, 

observed 
Invariants 

and  controled, 

about 
temporal 
for  process  understanding  may  be  abstract  model 

its  spatial  and 

knowledge 

is  both 

that 

(4)  efJicient  computation  of’  (favorable  or  optimal)  control  outputs  can  only  be  done 
theory  provides 

(or  partial)  process  models 

into  account;  control 

taking  complete 
the  methods  for  fast  and  stable  reactions. 

(5)  Wise  behavioral  decisions 

or  feedback 

feed-forward 

require  knowledge 

outcome 
about 
and 
control  modes 
of  special 
these  results  are  obtained  from  integration  of  the  dynamical  models. 
environments; 
This  may  have  been  done  beforehand  and  stored  appropriately,  or  may  be  done  on 
the  spot  if  analytical 
solutions  are  available  or  numerical  ones  can  be  derived  in  a 
small  fraction  of  real-time  as  becomes  possible  now  with  the  increasing  processing 
from 
power  available.  Behaviors  are  realized  by  triggering 
point  4  above. 

the  longer-term 
in  certain 

the  modes  available 

situations 

(6)  Situations  are made  up  of  arrangements  of  objects,  other  active  subjects,  and  of  the 

own  goals  pursued; 

therefore, 

(7)  it  is  essential 

to  recognize  single  objects  and  subjects, 
their  intentions 

in  order  to  be  able  to  make  meaningful 

their  relative  state,  and  for 

the  future  development 

of  a  situation 

(which 

is  needed 

for 

the  latter  also,  if  possible, 
predictions 
about 
successful  behavioral  decisions). 

(8)  As  the  term  re-cognition 

tells,  in  the  usual  case  it  is  assumed 

that  objects  seen  are 

(at  least)  generically  known  already,  only  their  appearance  here  (in  the  geometrical 
range  of  operation  of  the  senses)  and  now  is  new;  this  allows  a  fast  jump 
object  hypothesis  when  first  visual 
knowledge, 
background 
Exploiting 

to  an 
impressions 
arrive  through  sets  of  features. 
the  model  based  perception  process  has  to 

E.D.  Dickmanns  /Artificial 

Intvlligencr  103  (1998)  49-76 

59 

be  initiated.  Free  parameters 
efficiently  by  attention  control  and  the  use  of  special  algorithms  and  behaviors. 

in  the  generic  object  models  may  be  determined 

(9)  In  order  to  be  able  to  do  step  8  efficiently,  knowledge  about 
in  the  context  of  ‘tusk  domains’ 
In  addition,  knowledge  about  discriminating 
(indexing 

be  provided 
represented. 
correct  hypothesis  generation 

in  which  likely  co-occurrences 
is  essential 
features 
into  the  object  data  base). 

‘the  world’  has  to 
are 
for 

(10)  Most  efficient  object 

time 

compute 

powerful 

(class)  description 

by  invariants 
constraints 
are  sufficiently 

is  usually  done 
in  3-D 
or  stereotypical  motion 
the 
in  an  image  (in  a 

to  compute 

(for  motion 

to  numerically 

(for  shape)  and 

this  allows  a  very  flexible  general 

space 
sequences);  modern  microprocessors 
visual  appearance  of  an  object  under  given  aspect  conditions 
in  parallel) 
single  one,  or  even  in  several  ones  with  different  mapping  parameters 
at  runtime.  They  are  even  powerful  enough 
the  Jacobian 
matrices  (for  sensor/object  pairs)  of  features  evaluated  with  respect  to  object  state 
for  recursive 
or  parameter  values; 
state  and  parameter  estimation.  The  inversion  of  perspective  projection 
is  thus 
reduced  to  a least  squares  model  fit once  the  recursive  process  has  been  started.  The 
of  the  overall  process  are 
underlying 
sufficiently  good  representations 
rates  like  video  frequency 
interpretation 

scene,  newly  appearing  objects 
search  processes 
will  occur  in  restricted  ureas  of  the  image  such  that  bottom-up 
may  be  confined 
to  these  areas.  Passing  cars,  for  example,  always  enter  the  field  of 
view  from  the  side  just  above  the  ground;  a  small  class  of  features  allows  to  detect 
them  reliably. 

of  the  nonlinear 
(25  or  30  Hz)  this  is  usually 

assumption  here  is  that  local  linearizations 

real  process;  for  high  evaluation 

process  of  a  dynamic 

framework 

the  case. 

(1 1)  In  a  running 

(12)  Subjects, 

i.e.,  objects  with  the  capability  of  self  induced  generation 

actuation,  are  characterized  by  typical 
motion  behavior 
(similar 

to  shape  in  the  spatial  domain). 

(sometimes 
in  certain  situations.  This  may  also  be  used  for  recognizing 

stereotypical, 

of  control 
i.e.,  predictive) 
them 

(13)  The  same  object/subject  may  be  represented 

at  different  scales  with 
various  degrees  of  detuil;  this  allows  flexible  and  efficient  use  in  changing  contexts 
(e.g.,  as  a  function  of  distance  or  degree  of  attention). 

internally 

5.2.  Survey  on  the  structure  of  the  4-D  upproach 

Fig.  3  shows  the  main  three  activities  running 

in  parallel 

in  an  advanced  version  of  the 

4-D  approach: 

(1)  Detection  of  objects  from  typical  collections  of  features  not  yet  assigned 

to  some 
object  already  tracked  (center  left,  upward  arrow  1); when  these  feature  collections 
are  stable  over  several  frames,  an  object  hypothesis  has  to  be  formed  and  the  new 
object  is  added  to  the  list  of  those  regularly 

tracked  (arrow  2 to  the  right). 

(2)  Trucking  of  objects  and  state  estimation 

in  Fig.  3;  first,  with  the  control  output  chosen,  a  single  step  prediction 
in  3-D  space  and 
components, 
both  translational 

path  concentrating 
and  rotational  degrees  of  freedom,  and  (b)  the  ‘what’-signal 

in  the  loop  to  the  lower  right 
is  done 
real  world’.  This  step  consists  of  two 
in 
path 

on  progress  of  motion 

(a)  the  ‘where’-signal 

the  ‘imagined 

is  shown 

time, 

60 

E.D.  Dickmanns  /Art$cial 

Intelligence  103  (199X)  49-76 

I baeli@+ouad- 

4-D  atwroach  to dynamic  machine  vision: 
,..:..:i y...::,; :,//:‘.;  ;  .t;~~;,~!y~..~~, 

~,j~<;.$;$~.:j-~ 

:,I:, j 

..a ;;).&;;..:j:, 

Fig.  3.  Survey  on  the  4-D  approach 
(central  arrow  upwards), 
top),  the  latter  two  being  driven  by  prediction  error  feedback. 

tracking  and  state  estimation 

to  dynamic  machine  vision  with  three  major  areas  of  activity:  object  detection 
loop  in  lower  right),  and  learning  (loop  in  center 

(recursive 

dealing  with  object  shape.  (In  order  not  to  overburden 
are  not  shown.) 

the  figure  these  components 

(3)  Learning 

concentrating 

is  done  with 

from  observation 

the  same  data  as  for  tracking 

(center 
this  is  not  a  single  step  loop  but  rather  a  low  frequency 
on  ‘constant’  parameters,  or  it  even  is  an  off- 

top  in  Fig.  3);  however, 
estimation  component 
line  component  with  batch  processing  of  stored  data.  This  is  an  actual  construction 
site  in  code  development 
towards 
becoming  more  autonomous 
in  new  task  domains  as experience  of  the  system  grows. 
Both  dynamical  models  (for  the  ‘where’-part)  and  shape  models  (for  the  ‘what’-part 
shall  be  learnable). 

at  present  which  will  open  up  the  architecture 

Another  component  under  development  not  detailed 
behavior  decision; 

this  will  be  discussed 

in  Section  6. 

in  Fig.  3  is  situation  assessment  and 

5.3.  Generic  4-D  object  classes 

The  efficiency  of  the  4-D  approach 

to  dynamic  vision 

is  achieved  by  associating 

background  knowledge  about  classes  of  objects  and  their  behavioral  capabilities  with  the 
data  input.  This  knowledge 
typical 
adapted 
translational 

for  object  classes 
to  the  special  case  at  hand.  Motion  descriptions 

in  the  models  have  to  be 
(the 
both  of  which 

in  space)  and  for  rotational  movements, 

is  fixed  while  specific  parameters 

for  the  center  of  gravity 

form,  that  is,  structural 

is  available 

information 

in  generic 

trajectory 

object 

E.D.  Dickmanns  /Artificial 

Intelligence  103  (I 998)  49-76 

61 

together  form  the  so-called 
the  ‘what’-problem.  Typically,  summing  and  averaging  of  feature  positions 
solve  the  where-problem  while  differencing 
what-problem. 

are  separated  from  shape  descriptions,  called 
to 
the 

feature  positions  contributes 

‘where’-problem, 

to  solving 

is  needed 

5.3.1.  Motion  description 

Possibilities 

for  object  trajectories  are  so  abundant 

reasonable  effort.  However,  good  models  are  usually  available  describing 
over  time  as  a  function  of  the  actual  state, 
These  so-called 
(i  =  f(x,  u,  u’, t))  with  x  as  the  n-component 
vector  and  IJ’ as  perturbation 

‘dynamical  models’,  usually,  are  sets  of  nonlinear  differential 

that  they  cannot  be  represented  with 
their  evolution 
inputs. 
equations 
control 

state  vector,  u  as  r-component 

and  the  perturbation 

the  control- 

input. 

Through 

linearization 

around  a nominal 

trajectory  xN(t), 

are  obtained  which  can  be  integrated  analytically 
matrix  description 

for  small  cycle  times  T 

to  yield  the  (approximate) 

locally  linearized  descriptions 
local  transition 

x[(k  +  l)T]  =  Ax[kT]  +  Bu[kT]  +  u[kT]. 

(1) 

The  elements  of  the  matrices  A  and  B  are  obtained 
af/FfIul~  by  standard  methods  from  systems  theory. 

from  F(t)  =  af/aXlN 

and  G(t)  = 

Usually, 

the  states  cannot  be  measured  directly  but  through  the  output  variables  _Y given 

by 

YWTI  =  h(X[kTl,  p,  kT)  +  w[kT], 

(2) 

where  h  may  be  a  nonlinear  mapping 
represents  measurement  noise. 

(see  below),  p  are  mapping  parameters 

and  w 

On  the  basis  of  Eq.  (1)  a distinction  between 

‘objects’  proper  and  ‘subjects’  can  be  made: 
if  there  is  no  dependence  on  controls  u  in  the  model,  or  if  this  u(r)  is  input  by  another  agent 
we  speak  of  an  ‘object’,  controlled  by  a subject  in  the  latter  case.  If  u[kT]  may  be  activated 
outputs  or  by  results 
by  some  internal  activity  within  the  object,  be  it  by  pre-programmed 
obtained 

from  processing  of  measurement  data,  we  speak  of  a  ‘subject’. 

5.3.2.  Shape  and feature  description 

in  the  image  may  vary  considerably 

see  [4].  Since  objects  may  be  seen  at  different 

With  respect  to  shape,  objects  and  subjects  are  treated  in  the  same  fashion.  Only  rigid 
objects  and  objects  consisting  of  several  rigid  parts  linked  by  joints  have  been  treated;  for 
elastic  and  plastic  modeling, 
the 
appearance 
in  size.  At  large  ranges  the  3-D  shape  of 
the  object,  usually,  is  of  no  importance 
most  of  the  information 
aspect  conditions; 
is  necessary 
perceiving 

seen  contains 
to  the  observer,  and  the  cross-section 
for  tracking.  However,  this  cross-section  depends  on  the  angular 
and  aspect-dependent  modeling  of  shape 
for  efficient  dynamic  vision.  This  will  be  discussed  briefly  for  the  task  of 

road  vehicles  as  they  appear  in  normal  road  traffic. 

therefore,  both  coarse-to-fine 

ranges 

Coarse-to-jne 

shape  models 

distance,  any  road  vehicle  may  be  adequately  described  by  its  encasing 
is  convenient 
values  of  these  parameters  are  of  no  importance  at larger  distances; 

the  front  at  a  large 
this 
rectangle; 
since  this  shape  just  has  two  parameters,  width  b  and  height  h.  Absolute 
the  proper  scale  may  be 

in  2-D:  seen  from  behind  or  from 

62 

ED.  Dickmunns  /Artificial 

Intelligence 

IO.? (1998)  49-76 

4 

‘4 

1--------t 
I 

I 

L-l 

Fig. 4. Coarse to fine shape model of a car in rear view: (a) encasing rectangle  (U-shape);  (b) polygonal  silhouette; 
(c) silhouette with internal structure. 

inferred  from  other  known  object  seen,  like  road  or  lane  width  at  that  distance.  Trucks 
buses)  and  cars  can  easily  be  distinguished.  Our  experience 
and  thus  the  height  of  the  object  may  be  omitted  without  loss  of  functionality 
this  spatially  curved  region  of  the  car  body  together  with  varying  environmental 
may  make  reliable 
U-shape  of  unit  height  (corresponding 
to  be  sufficient  until 
the  image.  Depending 
distances. 

l-2  dozen  pixels  can  be  found  on  a  line  crossing 
on  the  focal 

conditions 
thus,  a  simple 
seems 
in 
to  different  absolute 

(or 
tells  that  even  the  upper  limit 
in 

tracking  of  the  upper  body  boundary  very  difficult); 

to  about  1  m  turned  out  to  be  practical) 

length  used,  this  corresponds 

(reflections 

the  object 

Fig.  4(a)  shows  this  shape  model.  If the  object  in  the  image  is  large  enough  so  that  details 
as 
in  the  latter 
features  like  the  license  plate,  the  tires  or  the  signal  light  groups  (usually 

may  be  distinguished 
shown  in  Fig.  4(b)  or  even  with  internal  details  (Fig.  4(c))  may  be  chosen; 
case,  area-based 
in  yellow  or  reddish  color)  may  allow  more  robust  recognition 

reliably  by  feature  extraction,  a  polygonal 

shape  approximation 

and  tracking. 

the  width  of  the  car  in  the  image  will  start  increasing 

If  the  view  is  from  an  oblique  direction, 
comes  into  play.  Even  with  viewing  conditions 
vehicle  observed, 
the  larger  length  of  the  body  and  due  to  the  sine-effect 
to  determine 
measurements; 
proven  to  be  much  more  robust  and  reliable 

therefore,  switching 

the  lateral  aspect  angle,  body  width  and-length 

in  mapping.  Usually,  it  is  impossible 
from  visual 
has 

simultaneously 

to  the  body  diagonal  as  a  shape  representation 

in  real-world  scenes  1321. 

the  depth  dimension 

(length  of  the  vehicle) 
slightly  off  the  axis  of  symmetry  of  the 
rapidly  because  of 

Just  for  tracking  and  relative  state  estimation, 

taking  one  of  the  vertical  edges  of  the 
lower  body  and  the  lower  bound  of  the  object  body  has  proven 
in  most 
cases  [38];  this,  of  course,  is  domain  specific  knowledge  which  has  to  be  introduced  when 
specifying 

the  features  for  measurement 

to  be  sufficient 

has  to  be 
In  general,  modeling  of  well  measurable 
tells  that  area  based  features  should  play 
dependent  on  the  aspect  conditions.  Experience 
an  important 
this  has  been  realized  by  observing 
the  average  grey  value  on  the  vehicle-side  of  edge  features  detected;  with  more  computing 
power  available,  color  profiles  in  certain  cross-sections 

role  in  robust  object  tracking. 

yield  improved  performance. 

recognition 

for  object 

Initially, 

in  the  shape  model. 
features 

Full  3-D  models  with  direrent  degrees  qf  detail:  similar 

to  the  2-D  rear  silhouette, 

different  models  may  also  be  used  for  3-D  shape.  The  one  corresponding 
the  encasing  box  with  perpendicular 
in  the  image,  and  their  separation 
line  may  be  measured  precisely,  good  estimates  of  the 
overall  body  dimensions  may  be  obtained  from  small  image  sizes  already.  Since  space  does 
not  allow  more  details  here,  the  interested 

if  these  surfaces  can  be  easily  distinguished 

reader  is  referred  to  132,331. 

to  Fig.  4(a)  is 

surfaces; 

E.D.  Dickmanns 

/Artificial 

Intelligence 

103  (1998)  49-76 

63 

Fig.  5.  Intelligent  control  of  image  feature  extraction  parameters 
with  a window 
and  state  estimation 

(for  edges,  marked 
label  Eij)  and  ‘Triangle’  (labeled  T, large  rectangles  with  broken  lines  for  efficient  object  tracking 

in  the  algorithms  CRONOS 

in  the  4-D  approach). 

5.4.  Image  feature  extraction 

Due  to  space  restrictions, 

this  topic  will  not  be  detailed  here;  the  interested 

reader  is 

referred  to  [8,15].  Fig.  5  shows  a  survey  on  the  method  used. 

Two  types  of  feature  extraction  algorithms  are  used:  oriented  edge  features  extracted  by 

ternary  mask  correlations 
and  area-based 
image  plane  (a  new  one). 

segmentations 

in  horizontal  or  vertical  search  paths  (a  rather  old  component), 
of  ‘stripes’  of  certain  widths,  arbitrarily  oriented 

in  the 

The  intelligent 

control  of  the  parameters  of  these  algorithms 

In  the  4-D  approach, 
representations 

for  efficient 
from  the  spatio- 
tracking. 
temporal 
and  application  of  perspective  mapping.  From  Fig.  5  it  may  be 
seen  that  a small  percentage  of image  data  properly  analysed  allows  to  track  objects  reliably 
frequently 
and  precisely  when  used  in  a  tight  bottom-up 
(25  Hz);  this  has  to  be  seen  in  the  context  of  Fig.  2. 

these  parameters  are  set  by  predictions 

loop  traversed 

and  top-down 

is  essential 

5.5.  State  estimation 

The  basic  approach  has  been  described  many 

and  has 
remained 
the  same  for  visual  relative  state  estimation  over  years  by  now.  However,  in  order 
to  be  able  to  better  deal  with  the  general  case  of  scene  recognition  under  (more  strongly) 
perturbed  ego-motion, 

an  inertially  based  component  has  been  added  [41,42]. 

(see  [2,6,9,10,38,44]) 

times 

This  type  of  state  estimation 

is  not  new  at  all  if  compared 

for  missiles;  however,  here  only  very  inexpensive 
are  being  used.  This  is  acceptable  only  because 
by  a visual  state  estimation 

loop  running 

in  parallel, 

to  inertial  navigation, 

e.g., 
accelerometers 
and  angular  rate  sensors 
the  resulting  drift  problems  are  handled 
the  combined  use 

thereby  resembling 

64 

E.D.  Dickmanns 

/A@cial 

Intelligence 

103  (1998)  49-76 

of  (relatively  poor)  inertial  signals  from  the  vestibular  apparatus  and  of  visual  signals 
vertebrate  perception.  Some  of  these  inertial  signals  may  also  be  used  for  stabilizing 
viewing  direction  with  respect  to  the  stationary  environment 
of  angular  rates  to  the  pointing  device  carrying 
very  high  rates  in  our  systems  (500  Hz,  see  [35]). 

in 
the 
by  direct  negative  feedback 
the  cameras.  This  feedback  actually  runs  at 

5.5.1. 

The  advantage  of  this  new  component 

Inertially  based  ego-state  estimation 

(IbSE) 
is  three-fold: 
along,  and  rotational  speed  components 

these  components 

(2)  the  quantities  measured  correspond 

can  be  integrated  numerically 

(1)  because  of  the  direct  encoding  of 
around  body  fixed  axes,  time  delays 
to  yield  predictions  of 
actually 
they  are  more 
from  a  theoretical  model  disregarding  perturbations  which  are 
the  inertial 
to 
leading 

are  available, 
thereby 

to  the  forces  and  moments 

the  effects  of  perturbations; 

in  perturbation  models, 

therefore, 

(3)  if  good  models  for  the  eigen-behavior 

including 

accelerations 
are  negligeable; 
positions; 
exerted  on  the  vehicle 
valuable 
unknown, 
measurements 
deeper  understanding 

in  general; 

than  predictions 

allow  to  estimate  parameters 
of  environmental 

effects. 

5.5.2.  Dynamic  vision 

With  respect 

to  ego-state 

recognition, 

vision  now  has  reduced  but  still  essential 

func- 

longterm 

interpretation 

It  has  to  stabilize 

tionality. 
and  it  has  to  yield  information 
to  the  road  and  road  curvature 
to  other  vehicles  or  obstacles, 
frequency  viewing  direction  component 
for  feature  extraction  and  leads  to  higher  efficiency  of  the  overall  system. 

relative 
inertially.  With  respect 
since  the  high- 
is  known  now;  this  reduces  search  range  required 

in  vehicle  guidance,  not  measurable 
the  vision 

task  also  is  slightly  alleviated 

to  the  stationary  environment, 

like  position  and  orientation 

on  the  environment, 

relative 

since 

these  items 

These  effects  can  only  be  achieved  using 

spatio-temporal  models  and  perspective 
mapping, 
in  the  image  plane. 
With  different  measurement  models  for  all  the  cameras  used,  a  single  object  model  and 
loop  may  be  fed  with  image  data  from  all  cameras  relevant.  Jacobian 
its  recursive 
matrices  now  exist  for  each  object/sensor  pair. 

link  inertial  measurements 

to  features 

iteration 

The  nonlinear  measurement  Eq.  (2)  is  linearized  around  the  predicted  nominal  state  XN 
(without 

and  the  nominal  parameter  set  PN  yielding 

the  noise  term) 

YWI  =  YN@fl  +SYrktl=  h(XN[kT],  PN,  kT)  +  c,sx + c,sp, 

(3) 

and  the  parameters 

and  c,,  =  ah/aplN 

are  the  Jacobian  matrices  with  respect  to  the 
where  c,  =  ah/ax]N 
state  components 
involved.  Since  the  first  terms  to  the  right  hand  side 
of  the  equality  sign  are  equal  by  definition,  Eq.  (3)  may  be  used  to  determine  6x  and  Sp  in 
a least  squares  sense  from  6y  as  the  prediction  error  messured 
given);  this  is 
the  core  of  recursive  estimation. 

(observability 

5.6.  Situation  assessment 

For  each  object  an  estimation 

loop  is  set  up  yielding  best  estimates 

to  the  ego-vehicle 
the  velocity 

including 

all  spatial  velocity  components.  For  stationary 

is  the  negative  of  ego-speed,  of  course.  Since 

this  is  known 

for  the  relative  state 
landmarks, 
from 

rehably 

E.D.  Dickmums 

/Art$cial 

Intelligence  103  (1998)  49-76 

65 

conventional  measurements, 
monocular  vision  exploiting  motion  stereo  [ 19,28,38]. 

the  distance 

to  the  landmark  can  be  determined 

even  with 

environment 

on  the  surrounding 

With  all  this  information 

and  on  the  most  essential 
in  a task  context 
objects  in  it  available,  an  interpretation  process  can  evaluate 
and  come  up  with  a  conclusion  whether  to  proceed  with  the  behavioral  mode  running  or 
exploiting  dynamical  models 
to  switch  to  a  different  mode.  Fast  in-advance 
for  the  near-term 
and  alternative 
and 
evolution  of  the  situation.  By  comparing 
stored  results.  these  decisions  are  made. 

simulations 
inputs  yield  possible  alternatives 
the  options  or  by  resorting 

to  pre-computed 

stereotypical 

the  situation 

control 

6.  Generation  of behavioral  capabilities 

Dynamic  vision  is  geared  to  closed-loop  behavior 

in  a task  context;  the  types  of behavior 
of  relevance,  of  course,  depend  on  the  special 
task  domain.  The  general  aspect  is  that 
behaviors  are  generated  by  control  output.  There  are  two  basically  different  types  of  control 
generation: 

(1)  triggering 

stored 
forward  control,  by  events  actually  observed,  and 

of  (generically) 

the  activation 

time  histories, 

so-called 

feed- 

(2)  gearing  actual  control 

to  the  difference  between  desired  and  actual  state  of  relevant 

systems,  so-called  feedback  control. 

In  both  cases,  actual  control  parameters  may  depend  on  the  situation  given.  A  very  general 
method  is  to  combine 
the  two  given  above  (as  a  third  case  in  the  list),  which  is  especially 
easy  in  the  4-D  approach  where  dynamical  models  are  already  available 
motion  understanding. 

for  the  part  of 

The  general  feed-forward  control  law  in  generic  form  is 

U(r)  =  s&M, 

TM), 

with  0  <  r  =  t  -  trrig  <  (ZM), 

(4) 

where  PM  may  contain  averaged  state  components 

(like  speed). 

A  typical 

feed-forward 

control  element 

is  the  steer  control  output  for  lane  change: 
is  set  in  five  phases  during 

in 
the 
rp  each,  consist  of  a 

for  example, 

the  steer  rate  h-dot 

formulation, 
time  rM;  the  first  and  the  final  control  phase  of  duration 

a  generic 
maneuver 
constant  steer  rate,  say  R.  In  the  second  and  fourth  phase  of  same  duration, 
the  amplitude 
is  of  opposite  sign  to  the  first  and  last  one.  In  the  third  phase  the  steer  rate  is  zero;  it  may 
be  missing  at  all  (duration  zero).  The  parameters  R,  TM, tp  have  to  be  selected  such  that 
at  (TV  +  Am) 
the  same  as 
before;  these  parameters,  of  course,  depend  on  the  speed  driven. 

the  lateral  offset  is  just  one  lane  width  with  vehicle  heading 

Given  this  idealized  control  law,  the  corresponding 

state  component 

for  0  c  r  =  t -  tTfig -c (TM +  AQ)  can  be  computed  according 
the  additional 
transition 
disturbances  during  the  maneuver, 
superimposed 

the  difference  Ax(t)  =  xc(r) 
to  force  the  real  trajectory 

is  not  completed  at  the  time  when  the  control 

state  feedback  controller 

time  histories  XC(~) 
to  a good  dynamical  model; 
the 

input  ends.  In  order  to  counteract 

time  period  ArD  at the  end  is  added  because  in  real  dynamical  maneuvers 

-  x(t)  may  be  used  in  a 
towards  the  ideal  one. 

(5) 

The  general  state  feedback  control  law  is 

u(r)  =  -KTAx(s), 

66 

E.D.  Dickmwms  /Artijicial 

lntelligmcr  103 (19%‘)  49-76 

(optimal 

with  K  being  the  r  x  n  gain  matrix.  The  gain  coefficients  may  be  set  by  pole  placement 
in  control 
or  by  a  Riccati  design 
linear  quadratic 
engineering 
along 
the  time  axis:  while  pole  placement  specifies  the  eigenvalues  of  the  closed  loop  system,  the 
Riccati  design  minimizes  weighted 

include  knowledge  about  behavioral  characteristics 

integrals  of  state  errors  and  control  inputs. 
use  of  dynamical  models  for  both  perception  and  control  and  for  the 

controller)  well  known 

[20].  Both  methods 

The  simultaneous 

evaluation  process  leading 

to  behavior  decision  makes  this  approach  so  efficient. 

Combining 

feed-forward  direct  control  and  actual  error  feedback, 

the  system  will  realize 
the  commanded  behavior  as  close  as  possible  and  deal  with  perturbations  without  the  need 
for  replanning  on  the  higher  levels. 

All,  that  is  needed  for  mission  performance  of  any  specific  system  then  is  a  sufficiently 
rich  set  of  feed-forward  and  feedback  behavioral  capabilities.  These  have  to  be  activated  in 
the  effect 
the  right  sequence  such  that  the  goals  are  achieved 
of  each  behavioral  capability  has  to  be  represented  on  the  upper  decision 
level  by  global 
descriptions  of  their  effects: 

in  the  end.  For  this  purpose, 

it  is  sufficient 

(1)  For  feed-forward  behaviors  with  corrective 

to  just  represent 

(case  3  given 
time 
above) 
needed;  note  that  this  is  a quasi-static  description  as  used  in  AI-methods.  This  level 
does  not  have  to  worry  about  real-time  dynamics,  being  taken  care  off  by  the  lower 
levels.  It  just  has  to  know  in  which  situations 
these  behavioral  capabilities  may  be 
activated  with  which  parameter  set. 

feedback  superimposed 
initial  and  final  conditions 

including 

(2)  For  feedback  behaviors 

it  is  sufficient 

to  know  when  this  mode  may  be  used;  these 
fast  reactions  may  run  over  unlimited  periods  of  time  if  not  interrupted 
in  road  vehicle  guidance; 

reflex-like 
by  some  special  event.  A  typical  example  is  lane  following 
the  integral  of  speed  then  is  the  distance 
the  road.  These  values  are  given  in  information 
tables,  and  can  be  used  for  checking  mission  progress  on  the  upper  level. 

irrespective  of  the  curvatures  of 
like  maps  or 

systems  for  planning, 

traveled, 

Performing  more  complex  missions  on  this  basis  has  just  begun.  Fig.  6  shows  the  dual 
in  the  lower  part  and  by 
representation 
AI-methods 
road  vehicle 
guidance 

by  control  engineering 
(state  charts  of  an  automaton) 

in  the  upper  part  for  longitudinal 

(procedural)  methods 

(from  [24]). 

The  newly  available  computing  power  will  lead  to  quick  progress  on  this  mission 

level, 

now  that  the  general  concept  has  been  defined. 

7.  System  integration 

7.1.  Overall  architecture  qf cognitive  system 

Fig.  7 gives  a survey  on  the  new  object-oriented 

system  architecture  under  development. 
like  for  inertial 
On  the  lowest  (signal-) 
stabilization  of  gaze  direction  with  negative  angular  rate  feedback.  In  general,  no  temporal 
models  are  used  on  this  level  except  data  smoothing 
the  data  are 
processed  as  they  arrive  from  the  sensors. 

loops  may  be  closed  directly 

and  drift  elimination; 

level  some  feedback 

E.D.  Dickrnunns 

/Artificial 

Intelligence 

103  (1998)  49-76 

67 

task:  driving  in  a  lane 

automaton 

for 

longitudinal  control 

rule-based  level:  decision 

longitudinal  control 

/ 

4D-level:  execution 

\ 

Fig.  6.  The  task  drive  in  a  lane  as  an  example  for  coupling  of  the  4-D  level  to  the  rule-based 

level 

In  image  processing, 

several 

feature  extraction 

level  (thin  arrows  downward 

are  sent  back  for  interpretation 

features  are  grouped  and  preprocessed 

algorithms 

are  run  controlled 
in  lower  left);  features  according 

from 
to  the 
of  the  prediction  error  (adjacent  big  arrows 
(left 

for  hypothesis  generation 

the  higher  system 
‘Gestalt’-idea 
upward).  Unassigned 
on  next  higher  level). 
On  this  second 

specialist  processes  for  recognizing 
of  these  for  ground  vehicle  applications: 

level, 

full  use  is  being  made  of  spatio-temporal  models; 

there  are 
and  tracking  objects  of  certain  classes.  Shown  are  five 

(1)  detection  and  tracking  of  other  vehicles  and  obstacles  (ODT); 
(2)  detection  and  tracking  of  landmarks 
(3)  tracking  of  the  road  with  lanes  and  crossroads 
(4)  three-dimensional 

interpretation 
height-profile  of  the  track  to  be  driven  (3DS),  and 

including 

(LDT); 

of  the  surface  in  front  of  the  vehicle,  at  least  the 

relative  ego-state  (RDT); 

(5)  full  inertial  state  of  the  own  body  based  on  accelerometer- 

and  angular  rate  data, 

stabilized  by  specific  visual  inputs. 

The  resulting  actual  state  and  parameter  vectors  of  all  of  these  objects  are  communicated 
a dynamic  data  base  (wide  arrow  upwards)  which  forms  the  link  for  information 
between 

the  control  engineering- 

intelligence  modules. 

and  the  artificial 

exchange 

to 

E.D.  Dickmanns 

/Artijcial 

Intelligence 

10.3 (1998)  49-76 

Fig.  7.  OveraIl  architecture  of  cognitive  system 

for  recursive 

the  effects  of  maneuvers 

The  same  dynamical  models  used 

for 
the  actual 
on 
rear  part  of  this  level,  central  slice).  In  addition,  dynamical  models  are  used  to 
simulations  of  likely  trajectories  of 
to  be  handled  (most  backward 

loop  for  forcing 
‘temporal  predictions’ 

feedback 
the  ideal  one;  block 

predicting 
trajectory  under  perturbations 
central 
support  the  decision  making 
other  objects/subjects 
slice  of  the  block). 

levels  by  fast  in-advance 

of  crucial  importance 

are  also  exploited 

for  the  situation 

(superimposed 

estimation 

towards 

To  the  right  end  on  this  level  are  the  control  computation  blocks  which  also  make  use 
time  histories  and  for 

proper  feed-forward 

control 

of  dynamical  models  for  determining 
fixing  closed  loop  eigenmotion 

characteristics. 

The  third  vertical 

and  data  from  background 

(bottom-up) 
(activated 
interact  for  behavior-  and  mission  monitoring.  Map  information 
4-D  background  knowledge 
the  actual  task  list  for  mission  performance. 

level  contains  a  data  substrate  on  which  both  actual  dynamical  data 
in  a  top-down 
fashion) 
(rear  right)  and  generic 
(shown  at  the  left  end)  are  represented  on  this  level  as  well  as 

knowledge 

Halfway  upwards 
is  shown  containing 
activities: 
decisions  are  performed  here,  in  a  rather  rudimentary 

towards 
rule-based  processing 

the  highest 

level  an  intermediate 

sublevel 

(lightly-textured) 

situation  assessment 
form  and  procedurally 

and  behavior 
fixed  in  the 

E.D.  Dickmanns 

/Artijicial 

Intelligence  103  (1998)  49-76 

69 

right  in  the  figure).  This  is  being  expanded 

code  up  to  now  (foreground 
flexibility  by  introducing 
measures; 
decision  and  monitoring  process  is  being  created  (dubbed 
for  lack  of  a  more  precise  and  confined  expression)  which  can  activate  behaviors 
modify  control 

towards  more 
a  value  system  with  explicitly 
stated  goals  and  performance 
to  the  general  scheme  propagated  by  [I  1. In  addition,  a central 
‘self’  in  central  top  of  the  figure 
and 

this  corresponds 

laws. 

Both  control  output  and  state  time  histories 

resulting 

are  logged  for  off-line  analysis 
level  may  look 
they  may  apply  system 
identification 
for  improving  background  knowledge 

(left-most  disc  on  data  level);  evaluation  processes  on  the  intermediate 
at  statistical  performance 
algorithms,  or  they  may  analyse 
like  eliminating 
In  the  long 

errors  from  maps. 
run,  hopefully, 

intelligence  will  emerge 

the  data  obtained 

parameters 

achieved, 

real 

from 

for  learning,  based  on  the  framework  developed  by  the  natural 
engineering 
approaches.  The  goal 
powerful  methods  and  computer  codes  developed 
engineering, 
will  be  organized 
applications.  Knowledge 
access  from  all  task  domains. 

to  task  domains 
of  general  applicability 

this  starting  point 
sciences  and  modern 
as  possible  of  the 
in  the  fields  of  systems  dynamics,  control 
This  experience 
in  actual 
for  easy 

in  order  to  limit  search  spaces 
should  be  stored  separately 

and  computer  graphics/animation. 

is  to  take  as  much  advantage 

techniques 

simulation 

according 

The  ‘self’  shall  have  the  capabilities 

to  communicate  models,  software  packages  on 
the  rule  based  processing 
laws  and  mission 
plans  etc.  with  other  subjects  (human  or  else).  Over  time,  it  will  accumulate  an  individual 
line  of  experience  making 
from  the  same  basic 
platform.  The  future  has  to  show  where  the  limits  for  this  line  of  development  will  be. 

it  unique  among  many  others  starting 

level  and  data  structures 

like  maps,  control 

7.2.  Multiple 

loops  in  dynamic  scene  understanding 

The  principles  discussed  above  have  lead  to  parallel  realizations  of  multiple 

interpretation  process  both  in  space  and  in  time;  Fig.  2 has  displayed 
the  upper  half  of  the  figure,  the  essential  scales  for  feedback 
local  situation 
for  achieving  mission  goals  are  being  done  (see  Table  1 also). 

level,  and  the  global  mission  performance 

loops  in  the 
the  spatial  aspects.  In 
loops  are  the  object  level,  the 
level  on  which  behavior  decisions 

in  the  scene.  The  multiple 

These  decisions  may  be  based  on  both  local  and  extended  predictions  of  the  actual 
situation  and  on  knowledge  about  behavioral  capabilities  of  the  own  vehicle  and  of  other 
subjects 
in  the  time  domain  are 
scale  for  inertial  viewing  direction 
displayed 
control 
to  several  hours  for  ground  and  flight  vehicles  on  the  mission  scale  encompassing 
sequences  of  maneuvers  and  feedback  behavioral  modes. 

in  Fig.  8;  they  range  from  the  millisecond 

loops  used  in  our  system 

The  outermost 

two  loops  labeled  ‘quasi-static’  are  closed,  up  to  now,  mainly  by  human 
on  the 
systems 
techniques  as  well  as 

operators  and  software  developers.  They  are  being 
system 
dynamics,  control  engineering, 
methods  from  AI  has  become  feasible. 

it  is  felt  that  a  unified  approach  encompassing 

tackled  now  for  automation 

structure  developed; 

computer  simulation 

and  animation 

70 

E.D.  Dickmanns  /Artijicial 

Intelligence  103 (19%‘) 4-W 

I4 
I 

extended 

presence 

H 
I 

Fig.  8.  Multiple 
representational 

feedback 

loops  on  different 

time  scales 

in  (visual)  cognition 

systems  and  corresponding 

levels. 

8.  Experimental 

results 

8. I.  Road  vehicles 

The  autonomous 

for  observing 

the  vehicle.  With  its  46  transputers 

road  vehicle  VaMP  (see  Fig.  9)  and  its  twin  VITA  II  of  Daimler- 
in  normal  freeway  traffic  in  France,  Germany 

Benz  have  shown  remarkable  performance 
and  Denmark  since  1994.  VaMP  has  two  pairs  of  bifocal  camera  sets  of  focal  lengths  7.5 
and  24  mm;  one  looks  to  the  front,  the  other  one  to  the  rear.  With  320  by  240  pixels 
road  and  traffic  up  to  about  100  m  in  front  of 
per  image  this  is  sufficient 
it  has  been  able  in 
and  behind 
lane  width,  number  of  lanes,  type  of  lane  markings, 
1994  to  recognize 
its 
road  curvature, 
to  the  lane  and  to  the  driveway,  and  the  relative  state 
own  position  and  attitude  relative 
of  up  to  ten  other  vehicles 
five  in  each  hemisphere. 
Prometheus  near  Paris,  VaMP  has 
At  the  final  demonstration 
of  free  lane  driving  and  convoy  driving  at  speeds  up  to 
demonstrated 
for  passing  and  even  the 
130 km/h  in  normally  dense  three-lane 
decision  whether 
[22]. 
The  human  safety  pilot  just  had  to  check  the  validity  of  the  decision  and  to  give  a go-ahead 
input. 

lane  changes  were  safely  possible  have  been  done  autonomously 

including 
of  the  EUREKA-project 

their  velocity  components, 

traffic  [ 111; lane  changing 

for  image  processing 

its  capabilities 

In  the  meantime, 

transputers  had  been  replaced  by  PowerPCs  MPC  601  with  an  order 
of  magnitude  more  computing  power.  A  long  range  trip  over  about  1600  km  to  a  project 
in  which  about  95%  of  the 
in  1995  has  been  performed 
meeting 

in  Odense,  Denmark 

71 

Fig.  9.  The  autonomous  vehicle  VaMP  of  UBM 

distance  could  be  traveled  fully  automatically, 
freedom.  Maximum 

speed  on  a free  stretch  in  the  northern  German  plain  was  180  km/h. 

in  both  longitudinal  and  lateral  degrees  of 

construction 

Since  only  black-and-white 

video  signals  have  been  evaluated  with  edge 

they  could  not  be  picked  up  early  enough  due  to  lack  of  simultaneous 

feature 
sites  with  yellow  markings  on  top  of  the  white  ones 
into  the  own  lane  very  near  by  posed 

extraction  algorithms, 
could  not  be  handled;  also,  passing  vehicles  cutting 
problems  because 
took  too  long  to  converge  to  a stable 
field  of  view,  and  because  monocular  range  estimation 
interpretation.  For  these  reasons, 
is  now  being  improved  with  a  wide  field  of 
view  from  two  divergently  oriented  wide  angle  cameras  with  a  central  region  of  overlap 
(3-chip)  color  camera  also  covers 
for  stereo  interpretation; 
the  central  part  of  the  stereo  field-of-view.  This  allows  for  trinocular  stereo  and  area-based 
object  recognition. 
Dual-PentiumPro 

processors  now  provide  the  processing  power  for  tens  of  thousands  of 

a  high  resolution 

additionally, 

the  system 

mask  evaluations  with  CRONOS  per  video  cycle  and  processor. 

since  1985  which  has  demonstrated 

VaMoRs,  the  5-ton  van  in  operation 

road  driving,  has  seen  the  sequence  of  microprocessors 
and  PowerPCs  back  to  general  purpose 

quite  a  few 
from  Intel 
‘first’  in  autonomous 
Intel  Pentium  and 
8086.  80x86,  via  transputers 
its 
PentiumPro. 
capability  of  driving  on  state  and  on  minor  unsealed  roads  at  speeds  up  to  50  km/h  (1992); 
it  is  able  to  recognize  hilly  terrain  and  to  estimate  vertical  road  curvature 
to  the 
horizontal  one  [lo]. 

to  early  high-speed  driving  on  freeways  [7]  it  has  demonstrated 

In  addition 

in  addition 

Recognizing 

cross-roads  of  unknown  width  and  angular  orientation  has  been  demon- 
strated  as  well  as  turning  off  onto  these  roads.  even  with  tight  curves  requiring  an  initial 
[ 14,281.  These  capabilities  will  also  be 
maneuver 
considerably 
field  of 
view  and  area  based  color  image  processing. 

improved  by  the  new  camera  arrangement  with  a  wide  simultaneous 

to  the  opposite  direction  of  the  curve 

Performing 

entire  missions 

based  on  digital  maps  has  been 

started 

[19]  and 

alleviated  now  by  a  GPS-receiver 
introduced 
of  about  10 000  km  in  fully  autonomous  driving  on  many  types  of  roadways. 

in  combination  with  inertial  state  estimation 
[28,42].  The  vehicles  VaMoRs  and  VaMP  together  have  accumulated 

is 
recently 
a record 

72 

E.D.  Dickmums 

/Art@icial  Intellipzce 

103  (1998)  49-76 

8.2.  Air  vehicles 

control 

in  hardware-in-the-loop 

After  the  feasibility  of  autonomous 
machine  vision  had  been  demonstrated 
approaches 
sensing  and  both  wind  and  gust  disturbances 
of  the  safety  regulations, 
aircraft,  a twin  turbo-prop  of  about  5-ton  weight,  near  the  ground; 
flying  but  the  vision  system  determined  all  12  state  components 
distances  below  900  m  from  runway 

in  all  six  degrees  of  freedom  by  dynamic 
landing 
inertial 
led  to  first  flight  tests  in  1991  [34].  Because 
the 
the  human  pilot  did  the 
relative  to  the  runway  for 

for  the  case  of  straight-in,  unperturbed 
[16],  a  second  effort  including 

vision  system  was  not  allowed 

the  autonomous 

simulations 

to  control 

threshold. 

The  next  step  was  to  introduce  bifocal  vision  with  a  mild  and  a  stronger 

tele  lens  in 
connection  with  the  new  transputer 
system  in  the  early  90s;  1993,  in  another  set  of  flight 
it  was  proved  that  visual 
experiments  with  the  same  aircraft  of  the  University  of  Brunswick 
range  could  be  doubled,  essentially,  but  more  computing  power  would  be  needed  for  robust 
tracking  and  initialization.  The  PowerPC  satisfied  these  requirements; 
to 
detect  large  obstacles  on  the  runway  sufficiently  early  for  safe  reactions 
guidance  and  control 

task  performed  up  to  now  in  hardware-in- 
landmark 
flight  near  the  ground 
the-loop 
navigation.  The  capability  of  performing 
a  small  scale  mission  starting  at  one  end  of  the 
airport  of  Brunswick,  hying  along  a  selected  sequence  of  waypoints  on  the  airport  and  in 
to  the  airport  from  the  other  side  and  slowing  down  for 
the  vicinity 
landing  at  a helicopter 

The  most  demanding 
real-time 

‘H’  at  the  other  end  has  been  demonstrated 

[41,42]  (see  Fig.  10). 

(road  forks),  returning 

it  is  now  possible 

is  helicopter 

simulations 

including 

[ 181. 

this  demanding 

In  connection  with 

in  the  context  of  mission  performance. 

task,  a  complete 
separate  inertial  and  visual  state  estimation  components, 

developed  containing 
of  GPS  signals  and  data  fusion 
provisions  have  been  made  to  integrate  coarse-scale 
imaging 
radar  system  under  development 
radar 
solution 
streams  by  an  intelligent 
system  will  unload 
difficult  task  in  a  situation  where  he  is  stressed  to  the  limit  already. 

software  package  has  been 
integration 
In  addition, 
image  data  from  a  synthetic  aperture 
elsewhere.  The  combined  use  of  all-weather 
an  optimal 
these  data 
the  pilot  from  a  very 

images 
systems.  The  capability  of  interpreting 

for  future  helicopter  guidance 

images  and  high-resolution 

on-board  computer 

optical  or  infrared 

is  considered 

9.  ‘Technical  beings’ 

There  is  an  ongoing  discussion 

best  architecture 
for  realizing 
neural  nets  have  been  proposed  as  roads  leading 

to  these  type  of  creatures. 

as  to  what  technical  beings  may  be  like  and  what  the 
architecture  and 

these  agents  might  be  [3,36],  subsumption 

Looking  at  the  results  achieved  with  the  4-D  approach 
that  quite  a  few  problems 

seem  unreasonable 
scenarios  with  the  other  approaches  may  be  avoided  taking 
long  term  results  in  the  natural  sciences  and  engineering. 

to  expect 

to  dynamic  vision, 
to  be  encountered 

it  does  not 
in  complex 
this  route  which  builds  upon 

It  has  the  advantage  of  having  a clear  notion  of  space  and  time,  of  objects,  subjects  and 
to  handle  multi- 

processes,  and  of  the  spatio-temporal 
ple  independent  objects  and  subjects  with  own  intentions,  goals  and  control  capabilities. 

structure  necessary 

representational 

Fig.  10.  Landmark 
simulations 
taxiways, 

for  a  small  mission  near  the  airport  of  Brunswick: 

frame  and Heli  H  during  final  approach. 

navigation 

for  helicopters 

has  been  demonstrated 

real-time 
(a)  Tracking  of  ‘Crossing  2’;  (b)  Tracking  of 

in  hardware-in-the-loop, 

In  [5]  a  corresponding 

framework  has  been  given  which  allows 

representational 
systems  with  minimal 

additional 

to  the  task  domain  may  be  entered 

effort  on  the  methodical 
through  corresponding 

real-world  problems.  A  corresponding 

architecture 

in  the  near  future  will  be  sufficient 

specific 

handle  even  complex 
knowledge 
structures.  Computing  power  available 
complex 
guidance 

real-time, 
is  discussed 

in  [24]. 

to 
side; 
data 
to  solve  rather 
for  road  vehicle 

As  compared 

to  the  other  approaches  pursued, 

and  simulation 

state  of  the  art  in  engineering 
these  autonomous 
achieve 
sometimes  heard  that  these  systems  will  be  ‘closed’  as  opposed 
is  not  intelligible 

these  goals,  or  how  to  learn 

from  this  point  of  view. 

systems  and  providing 

technology; 

to  achieve 

the  pledge 

is  to  take  advantage  of  the 
for 
them  with  background  knowledge  of  how  to 

introducing  goal  functions 

them  will  be  essential.  The  argument 
ones 

to  neural-net-based 

10.  Conclusions 

The  4-D  approach 

to  dynamic  machine  vision  developed 

along 

by  cybernetics 

and  conventional 

engineering 

long 

time  ago  does  seem 

the  lines 

layed  out 
to  satisfy  all 

74 

E.D.  Dickmanns 

/Artijicial 

Intelligence 

103  (1998)  49-76 

and  control  processes 

it  shares  with  ‘Artificial 

the  expectations 
Complex  perception 
conditions 
and  mission-control 
graphics  and  -simulation  have  been  complemented 
computer  vision. 

Intelligence’-and 

‘Neural  Net’-approaches. 

like  ground  vehicle  guidance  under  diverse 
as  well  as  maneuver- 
tools  of  computer 

for  dealing  with  the  inverse  problem  of 

and  in  rather  complex  scenes  have  been  demonstrated 

in  full  six  degrees  of  freedom.  The  representational 

Computing  power  is  arriving  now  for  handling 

real-word  problems 

of  robustness  encountered  up  to  now  due  to  black-and-white 
understanding 
and  texture,  both  very  demanding  with  respect  to  processing  power. 

can  now  be  complemented 

by  area-based 

as-well-as  edge-based 
including 

in  real-time.  Lack 
image 
color 

representations 

Taking  advantage  of  well  suited  methods 

the 
best  of  every  field  in  a unified  overall  approach  will  be  the  most  promising  way  to  go.  The 
good  old  stuff  should  not  be  discarded 

in  competing  approaches  and  combining 

too  early. 

References 

Ll]  J.S.  Albus,  A.M.  Meystel,  A  reference  model  architecture 

for  design  and  implementation 

of  intelligent 

control  in  large  and  complex  systems,  Intemat.  J.  of  Intelligent  Control  and  Systems  1 (1)  (1996)  15-30. 

121 R.  Behringer,  Visuelle  Erkennung  und  Interpretation 

des  Fahrspurverlaufes 

durch  Rechnersehen 

fur  ein 

autonomes  StraRenfahrzeug,  Ph.D.  Thesis,  UniBwM,  LRT,  1996. 

[3]  R.A.  Brooks,  A.M.  Flynn,  Robot  beings, 

in:  IEEEiRSJ 

International  Workshop  on  Intelligent  Robots  and 

Systems,  Tsukuba,  Japan,  September  1989,  pp.  2-10. 

[4]  D.  DeCarlo,  D.  Metaxas,  The  integration  of  optical  flow  and  deformable  models  with  applications 

to  human 
in:  IEEE  Computer  Society  Conference  on  Computer  Vision  and  Pattern 

face  shape  and  motion  estimation, 
Recognition,  San  Francisco,  CA,  June  1996,  pp.  23 l-238. 
fur  visuelle  Wahrnehmung 

(51  D.  Dickmanns,  Rahmensystem 
INF,  1997. 

Thesis.  UniBwM, 

verartderlicher  Szenen  durch  Computer,  Ph.D. 

[6]  E.D.  Dickmanns,  4-D-dynamic 

scene  analysis  with  integral  spatio-temporal  models. 

in:  Proc.  4th  Internat. 

Symposium  on  Robotics  Research,  Santa  Cruz,  CA.  1987. 

[7]  E.D.  Dickmanns,  A.  Zapp,  Autonomous  high  speed  road  vehicle  guidance  by  computer  vision,  in:  Proc.  10th 

IFAC  World  Congress  Munich,  Preprint,  Vol.  4,  1987,  pp.  232-237. 

[ 81  E.D.  Dickmanns,  V. Graefe.  Dynamic  monocular  machine  vision,  Machine  Vision  and  Applications  1 (1988) 

223-240. 
E.D.  Dickmanns,  V.  Graefe,  Applications 
Applications 

I  (1988)  241-26  I. 

of  dynamic  monocular  machine  vision,  Machine  Vision  and 

[9]  E.D.  Dickmanns,  Machine  perception  exploiting  high-level  spatio-temporal  models,  AGARD  Lecture  Series 

185  ‘Machine  Perception’,  Hampton,  VA/Munich,  Madrid,  September/October 

1992. 

[IO] E.D.  Dickmanns,  B.  Mysliwetz,  Recursive  3-D  road  and  relative  ego-state 

recognition, 

IEEE  Transactions 

on  Pattern  Anal.  Machine  Intell.  (Special  Issue  on  ‘Interpretation  of  3-D  Scenes’)  14 (2)  (1992)  199-213. 

[I  I]  E.D.  Dickmanns,  R.  Behringer,  D.  Dickmanns,  T.  Hildebrandt,  M.  Maurer,  F.  Thomanek, 

in:  Proc.  Intelligent  Vehicles  Symposium 

J.  Schiehlen, 
‘94,  Paris,  October  1994, 

The  seeing  passenger  car  ‘VaMoRs-P’, 
pp.  68-73. 
[ 121  E.D.  Dickmanns, 

Performance 

improvements 

for  autonomous 

road  vehicles, 

Internal.  Conference 

on 

Intelligent  Autonomous  Systems  (IAS-4),  Karlsruhe,  1995. 
[ 131 ED.  Dickmanns,  Road  vehicle  eyes  for  high  precision  navigation, 

in:  Linkwitz  et  al.  (Eds.),  High  Precision 

Navigation,  Dtimmler,  Bonn,  1995,  pp.  329-336. 
[ 141 E.D.  Dickmanns,  N.  Mtiller,  Scene  recognition 

and  navigation  capabilities 

for  lane  changes  and  turns  in 

vision-based  vehicle  guidance.  Control  Engineering  Practice,  2nd  IFAC  Conf.  on  Intelligent  Autonomous 
Vehicles  ‘95,  Helsinki.  1995. 

E.D.  Dickmanns /Arti$cial  Intelligence  103 (1998)  49-76 

IS 

[ 151  E.D.  Dickmanns,  S. F&t,  A.  Schubert,  D.  Dickmanns, 
scenes,  Technical  Report  UniBwM/LRT/WE-13/FB/97-1, 

[16]  G.  Eberl,  Automat&her 
[ 171  H.  Fritz,  Model-based  neural  distance  control  for  autonomous 

1997. 
Landeanflug  durch  Rechnersehen,  Ph.D.  Thesis.  UniBwM,  LRT,  1987. 

Intelligently  controlled 

feature  extraction 

in  dynamic 

road  vehicles,  in:  Proc.  Intelligent  Vehicles  ‘96 

Symposium,  Tokyo,  1996,  pp.  29-34. 

[ 181 S.  Ftirst,  S.  Werner,  D.  Dickmanns,  E.D.  Dickmanns,  Landmark 

landing 
in:  Proc.  AeroSense-97,  Conference  3088,  Orlando,  FL.  April 

and  autonomous 

navigation 

approach  with  obstacle  detection  for  aircraft, 
20-25.  1997. 

[ 191 C.  Hock.  Wissensbasierte  Fahrzeugfiihrung  mit  Landmarken 

fur  autonome  Roboter,  Ph.D.  Thesis,  UniBwM. 

LRT.  1994. 

[ZO] T.  Kailath,  Linear  Systems,  Prentice-Hall,  Englewood  Cliffs,  NJ.  1980. 
I2 I]  P.J.  Klass,  DARPA  envisions  new  generation  of  machme 

intelligence,  Aviation  Week  &  Space  Technology, 

April  198.5, 47-54. 
1221 C.  Kuiawski,  Deciding 

the  behaviour  of  an  autonomous  mobile  road  vehicle, 

in:  2nd  IFAC  Conference  on 

[23] 

~241 

1251 

P-61 

~271 

1281 

I291 

1301 

1311 

[321 

[331 

[341 

[351 
I361 

(371 

1381 

I391 

[401 

r411 

of 

(Ed.), 

in:  Proc.  IEEE 

vehicles. 

interpretation 

for  real-time 

Ph.D.  Thesis. 

in:  T.S.  Huang 

‘92,  Denver,  1992. 

in  neural  network, 

in:  D.S.  Touretaky 

for  autonomous  vehicles, 

in  Neural  Information  Processing  Systems 

(Ed.),  Proc.  1st Conference  on  Intelligent  Autonomous 

und  Navigieren  mit  einem  sehenden  StralJenfahrzeug, 

I,  Morgan  Kaufmann,  San  Mateo,  CA,  1989. 
for  mobile  robot  guidance,  Ph.D.  Thesis,  CMU  [CMU-CS-92. 

the  state  of  a linear  system,  IEEE  Trans.  on  Mil.  Electronics  8 (1964)  29G-293. 
in:  AeroSense-97. 

Intelligent  Autonomous  Vehicles,  Helsinki,  June  1995. 
D.G.  Luenberger,  Observing 
M.  Maurer,  E.D.  Dickmanns,  An  advanced  control  architecture 
Conference  3087,  Orlando,  FL,  April  20-25,  1997. 
K.  Mecklenburg,  T.  Hrycej,  U.  Franke,  H.  Fritz,  Neural  control  of  autonomous 
Vehicular  Technology  Conference 
H.G.  Meissner,  Steuerung  dynamischer  Systeme  aufgrund  bildhafter  Informationen,  Ph.D.  Thesis,  UniBwM. 
LRT,  1982. 
H.G.  Meissner,  E.D.  Dickmanns,  Control  of  an  unstable  plant  by  computer  vision. 
Image  Sequence  Processing  and  Dynamic  Scene  Analysis,  Springer,  Berlin,  1983,  pp.  532-548. 
N.  Mtiller,  Autonomes  Manovrieren 
UniBwM,  LRT.  1996. 
B.  Mysliwetz,  E.D.  Dickmanns,  A  vision  system  with  active  gaze  control 
well  structured  dynamic  scenes,  in:  L.O.  Hertzberger 
Systems  (IAS-I),  Amsterdam,  December  1986.  pp.  477483. 
land  vehicle 
D.A.  Pomerleau.  ALVINN:  an  autonomous 
Advances 
D.A.  Pomerleau,  Neural  network  perception 
I 151, Carnegie  Mellon  University,  Pittsburgh,  PA,  February  1992. 
M.  Schmid,  3-D-Erkennung 
UniBwM,  LRT,  1994. 
1. Schick,  E.D.  Dickmanns,  Simultaneous  estimation  of  3-D  shape  and  motion  of  objects  by  computer  vision. 
in:  Proc.  IEEE  Workshop  on  Visual  Motion,  Princeton,  NJ,  1991. 
ER.  Schell,  Bordautonomer 
lung,  Ph.D.  Thesis,  UniBwM,  LRT,  1992. 
J.  Schiehlen,  Kameraplattformen 
1~. Steels,  The  biology  and  technology  of  intelligent  autonomous 
lvano.  Italy,  March  1-12,  1993. 
F. Thomanek,  D.  Dickmanns,  Obstacle  detection, 
guidance, 
19Y2, pp.  1399-1406. 
F. Thomanek.  Visuelle  Erkennung  und  Zustandsschatzung 
Fahrreugftihrung, 
S.  Tsugawa,  T. Yatabe,  T. Hirose.  S.  Matsumoto.  An  automobile  with  artificial 
79.  Tokyo,  1979,  pp.  893-895. 
B.  Ulmer.  VITA  II-Active 
Paris,  October  1994. 
S.  Werner,  S.  Ftirst,  D.  Dickmanns,  E.D.  Dickmanns,  A  vision-based  multi-sensor  machine  perception 
system  for  autonomous 
in:  Proc.  Enhanced  and  Synthetic  Vision,  AeroSense-96. 
Orlando.  FL.  April  1996. 

automatischer  Landeanflug  aufgrund  bildhafter  und  inertialer  MeBdatenauswer- 

fur  aktiv  sehende  Fahrzeuge,  Ph.D.  Thesis,  UniBwM,  LRT,  1995. 

road  vehicle 
(IROS)  Vol.  II,  Raleigh. 

in  real  traffic,  in:  Proc.  Intelligent  Vehicles  Symposium 

Intemat.  Conf.  on  Intelligent  Robots  and  Systems 

in  Echtzeit  aus  monokularen  Bildfolgen, 

agents,  NATO  Advanced  Study  Institute. 

Ph.D.  Thesis,  UniBwM,  LRT,  1996. 

von  mehreren  Straaenfahrzeugen 

tracking  and  state  estimation 

in:  Proc.  IEEE/RSJ 

collision  avoidance 

landing  approach. 

in:  Proc.  IJCAI- 

von  Fahrzeugen 

for  autonomous 

zur  autonomen 

Ph.D.  Thesis. 

intelligence, 

aircraft 

(Ed.), 

‘04. 

76 

E.D.  Dickmanns  /Artificial 

Intellipmce  103  (1998)  49-76 

(421 S.  Werner,  Maschinelle  Wahmehmung 

fur  den  bordautonomen 

automat&hen 

Hubschauberflug, 

Ph.D. 

Thesis,  UniBwM,  LRT,  1997. 

1431 H.-J.  Wuensche,  Verbesserte  Regehmg 

Sichtinformation 
Report  HSBwiLRTlWE 

unter  Bertlcksichtigung 

13aAB/83-2,  1983. 

eines  dynamischen 
der  Einfliisse  verschiedener  Zustandsschatzer 

Systems  durch  Auswettung 

redundanter 
und  Abtastzeiten, 

[44]  H.-J.  Wuensche,  Detection  and  control  of  mobile  robot  motion  by  real-time  computer  vision,  in:  N.  Marquino 

(Ed.),  Advances 

in  Intelligent  Robotics  Systems,  Proc.  SPIE,  Vol.  727,  1986,  pp. lOC109. 

