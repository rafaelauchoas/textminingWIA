Artiﬁcial Intelligence 174 (2010) 726–748

Contents lists available at ScienceDirect

Artiﬁcial Intelligence

www.elsevier.com/locate/artint

Analysis of a probabilistic model of redundancy in unsupervised
information extraction
Doug Downey a,∗

, Oren Etzioni b, Stephen Soderland b

a Northwestern University, 2133 Sheridan Road, Evanston, IL 60208, United States
b University of Washington, Box 352350, Seattle, WA 98195, United States

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 7 July 2009
Received in revised form 14 April 2010
Accepted 26 April 2010
Available online 29 April 2010

Keywords:
Information extraction
Unsupervised
World Wide Web

Unsupervised Information Extraction (UIE) is the task of extracting knowledge from
text without the use of hand-labeled training examples. Because UIE systems do not
require human intervention, they can recursively discover new relations, attributes, and
instances in a scalable manner. When applied to massive corpora such as the Web, UIE
systems present an approach to a primary challenge in artiﬁcial intelligence: the automatic
accumulation of massive bodies of knowledge.
A fundamental problem for a UIE system is assessing the probability that its extracted
information is correct. In massive corpora such as the Web, the same extraction is found
repeatedly in different documents. How does this redundancy impact the probability of
correctness?
We present a combinatorial “balls-and-urns” model, called Urns, that computes the impact
of sample size, redundancy, and corroboration from multiple distinct extraction rules
on the probability that an extraction is correct. We describe methods for estimating
Urns’s parameters in practice and demonstrate experimentally that for UIE the model’s
log likelihoods are 15 times better, on average, than those obtained by methods used in
previous work. We illustrate the generality of the redundancy model by detailing multiple
applications beyond UIE in which Urns has been effective. We also provide a theoretical
foundation for Urns’s performance, including a theorem showing that PAC Learnability in
Urns is guaranteed without hand-labeled data, under certain assumptions.

© 2010 Elsevier B.V. All rights reserved.

1. Introduction

Automatically extracting knowledge from text is the task of Information Extraction (IE). When applied to the Web, IE
promises to radically improve Web search engines, allowing them to answer complicated questions by synthesizing infor-
mation across multiple Web pages. Further, extraction from the Web presents a new approach to a fundamental challenge
in artiﬁcial intelligence: the automatic accumulation of massive bodies of knowledge.

IE on the Web is particularly challenging due to the variety of different concepts expressed. The strategy employed for
previous, small-corpus IE is to hand-label examples for each target concept, and uses the examples to train an extractor [19,
38,7,9,29,27]. On the Web, hand-labeling examples of each concept are intractable—the number of concepts of interest
is simply far too large. IE without hand-labeled examples is referred to as Unsupervised Information Extraction (UIE). UIE

* Corresponding author.

E-mail addresses: ddowney@eecs.northwestern.edu (D. Downey), etzioni@cs.washington.edu (O. Etzioni), soderlan@cs.washington.edu (S. Soderland).
URLs: http://www.cs.northwestern.edu/~ddowney/ (D. Downey), http://www.cs.washington.edu/homes/etzioni/ (O. Etzioni),

http://www.cs.washington.edu/homes/soderlan/ (S. Soderland).

0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.artint.2010.04.024

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

727

systems such as KnowItAll [16–18] and TextRunner [3,4] have demonstrated that at Web scale, automatically-generated
textual patterns can perform UIE for millions of diverse facts. As a simple example, an occurrence of the phrase “C such as
x” suggests that the string x is a member of the class C, as in the phrase “ﬁlms such as Star Wars” [22].1

However, all extraction techniques make errors, and a key problem for an IE system is determining the probability that
extracted information is correct. Speciﬁcally, given a corpus, and a set of extractions XC for a class C , we wish to estimate
P (x ∈ C|corpus) for each x ∈ XC . In UIE, where hand-labeled examples are unavailable, the task is particularly challenging.
How can we automatically assign probabilities of correctness to extractions for arbitrary target concepts, without hand-
labeled examples?

This paper presents a solution to the above question that applies across a broad spectrum of UIE systems and techniques.
It relies on the KnowItAll hypothesis, which states that extractions that occur more frequently in distinct sentences in a corpus
are more likely to be correct.

KnowItAll hypothesis: Extractions drawn more frequently from distinct
sentences in a corpus are more likely to be correct.

The KnowItAll hypothesis holds on the Web. Intuitively, we would expect the KnowItAll hypothesis to hold because although
extraction errors occur (e.g., KnowItAll erroneously extracts California as a City name from the phrase “states con-
taining large cities, such as California”), errors occurring in distinct sentences tend to be different.2 Thus, typically a given
erroneous extraction is repeated only a limited number of times. Further, while the Web does contain some misinformation
(for example, the statement “Elvis killed JFK” appears almost 200 times on the Web according to a major search engine),
this tends to be the exception (the correct statement “Oswald killed JFK” occurs over 3000 times).

At Web-scale, the KnowItAll hypothesis can identify many correct extractions due to redundancy: individual facts are
often repeated many times, and in many different ways. For example, consider the TextRunner Web information extraction
system, which extracts relational statements between pairs of entities (e.g., from the phrase “Edison invented the light bulb,”
TextRunner extracts the relational statement Invented(Edison, light bulb)). In an experiment with a set of about
500 million Web pages, ignoring the extractions occurring only once (which tend to be errors), TextRunner extracted 829
million total statements, of which only 218 million were unique (on average, 3.8 repetitions per statement). Well-known
facts can be repeated many times. According to a major search engine, the Web contains over 10,000 statements that
Thomas Edison invented the light bulb, and this fact is expressed in dozens of different ways (“Edison invented the light
bulb,” “The light bulb, invented by Thomas Edison,” ”Thomas Edison, after ten thousand trials, invented a workable light
bulb,” etc.).

Although the KnowItAll hypothesis is simply stated, leveraging it to assess extractions is non-trivial. For example, the
10,000th most frequently extracted Film is dramatically more likely to be correct than the 10,000th most frequently ex-
tracted US President, due to the relative sizes of the target sets. In UIE, this distinction must be identiﬁed without any
hand-labeled data. This paper shows that a probabilistic model of the KnowItAll hypothesis, coupled with the redundancy
of the Web, can power UIE for arbitrary target concepts. The primary contributions are discussed below.

1.1. The urns model of redundancy in text

The KnowItAll hypothesis states that the probability that an extraction is correct increases with its repetition. But by

how much? How can we precisely quantify our conﬁdence in an extraction given the available textual evidence?

We present an answer to these questions in the form of the Urns model—an instance of the classic “balls-and-urns”
model from combinatorics. In Urns, extractions are represented as draws from an urn, where each ball in the urn is labeled
with either a correct extraction, or an error—and different labels can be repeated on different numbers of balls. Given
the frequency distribution in the urn for labels in the target set and error set, we can compute the probability that an
observed label is a target element based on how many times it is drawn. A key insight of Urns is that when the frequency
distributions have predictable structure (for example, in textual corpora the distributions tend to the Zipﬁan), they can be
estimated without hand-labeled data.

We prove that when the frequency of each label in the urn is drawn from a mixture of two Zipﬁan distributions (one
for the target class and another for errors), the parameters of Urns can be learned without hand-labeled data. When the
data exhibits a certain separability criterion, PAC learnability is guaranteed. We also demonstrate that Urns is effective in
practice. In experiments with UIE on the Web, the probabilities produced by the model are shown to be 15 times better, on
average, when compared with techniques from previous work [14].

1 Here, the term class may also refer to relations between multiple strings, e.g. the ordered pair (Chicago, Illinois) is a member of the Locate-

dIn class.

2 Two sentences are distinct when they are not comprised of exactly the same word sequence. We stipulate that sentences be distinct to avoid placing

undue credence in content that is simply duplicated across many different pages, a common occurrence on the Web.

728

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

1.2. Paper outline

The paper proceeds as follows. We describe the Urns model in Section 2, experimentally demonstrate its effectiveness
in UIE, and detail applications beyond UIE in which the model has been employed. The theoretical results characterizing the
Urns model are presented in Section 3. We discuss future work in Section 4, and conclude.

2. The URNS model

In this section, we describe the Urns model for assigning probabilities of correctness to extractions. We begin by formally
introducing the model, then describe our implementation and a set of experiments establishing the model’s effectiveness
for UIE.

The Urns model takes the form of a classic “balls-and-urns” model from combinatorics. We ﬁrst consider the single urn

case, for simplicity, and then generalize to the full multiple Urns Model used in our experiments.

We think of IE abstractly as a generative process that maps text to extractions. Extractions repeat because distinct
sentences may yield the same extraction. For example, the sentence containing “Scenic towns such as Yakima. . .” and the
sentence containing “Washington towns such as Yakima. . .” both lead us to believe that Yakima is a correct extraction of
the relation City(x).

Each potential extraction is modeled as a labeled ball in an urn. A label represents either an instance of the target
relation, or an error. The information extraction process is modeled as repeated draws from the urn, with replacement.
Thus, in the above example, two balls are drawn from the urn, each with the label “Yakima”. The labels are instances of the
relation City(x). Each label may appear on a different number of balls in the urn. Finally, there may be balls in the urn
with error labels such as “California”, representing cases where the extraction process generated a label that is not a member
of the target relation.

Formally, the parameters that characterize an urn are:

• C – the set of unique target labels; |C| is the number of unique target labels in the urn.
• E – the set of unique error labels; |E| is the number of unique error labels in the urn.
• num(b) – the function giving the number of balls labeled by b where b ∈ C ∪ E. num(B) is the multi-set giving the

number of balls for each label b ∈ B.

Of course, extraction systems do not have access to these parameters directly. The goal of an extraction system is to
discern which of the labels it extracts are in fact elements of C , based on the number of repetitions of each label. Thus, the
central question we are investigating is: given that a particular label x was extracted k times in a set of n draws from the urn, what
is the probability that x ∈ C ?

In deriving this probability formally below, we assume the system has access to multi-sets num(C) and num(E) giving
the number of times the labels in C and E appear on balls in the urn. In our experiments, we provide methods that estimate
these multi-sets in the unsupervised and supervised settings.

We derive the probability that an element extracted k of n times is of the target class as follows. First, we have that:

P (x appears k times in n draws|x ∈ C) =

(cid:2)

r∈num(C)

(cid:4)
k

(cid:3)

(cid:4)(cid:3)

(cid:3)
n

k

r

s

1 − r
s

(cid:4)

n−k

(cid:5)

P

num(x) = r|x ∈ C

(cid:6)

where s is the total number of balls in the urn, and the sum is taken over possible repetition rates r.

Then we can express the desired quantity using Bayes Rule:

P (x ∈ C|x appears k times in n draws) = P (x appears k times in n draws|x ∈ C)P (x ∈ C)

P (x appears k times in n draws)

.

(1)

Note that these expressions include prior information about the label x—for example, P (x ∈ C) is the prior probability
that the string x is a target label, and P (num(x) = r|x ∈ C) represents the probability that a target label x is repeated
on r balls in the urn. In general, integrating this prior information could be valuable for extraction systems; however, in
the analysis and experiments that follow, we make the simplifying assumption of uniform priors, yielding the following
simpliﬁed form:

Proposition 1.

P (x ∈ C|x appears k times in n draws) =

(cid:7)

(cid:7)

r∈num(C)( r
r(cid:5)∈num(C∪E)( r(cid:5)

s )k(1 − r
s )k(1 − r(cid:5)

s )n−k
s )n−k

.

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

729

Fig. 1. Schematic illustration of the number of distinct labels in the C and E sets with repetition rate r. The “confusion region” is shaded.

2.0.1. The uniform special case

For illustration, consider the simple case in which all labels from C are repeated on the same number of balls. That is,
num(ci) = R C for all ci ∈ C , and assume also that num(ei) = R E for all ei ∈ E. While these assumptions are unrealistic (in
fact, we use a Zipf distribution for num(b) in our experiments), they are a reasonable approximation for the majority of
labels, which lie on the ﬂat tail of the Zipf curve.

Deﬁne p to be the precision of the extraction process; that is, the probability that a given draw comes from the target

class. In the uniform case, we have:
|C|R C
|E|R E + |C|R C

p =

.

The probability that a particular element of C appears in a given draw is then pC = p/|C|, and similarly p E = (1 − p)/|E|.
s )n−k as
We use a Poisson model to approximate the binomial from Proposition 1. That is, we approximate ( r
(cid:5)
−λ/(
n
k

s . Using this approximation, with algebra we have:

(cid:6)
k!), where λ is rn

s )k(1 − r

λke

P USC(x ∈ C|x appears k times in n draws) ≈

1
)ken(pC −p E )

.

1 + |E|

|C| ( p E
pC

(2)

In general, we expect the extraction process to be noisy but informative, such that pC > p E . Notice that when this is
true, Eq. (2) shows that the odds that x ∈ C increase exponentially with the number of times k that x is extracted, but also
decrease exponentially with the sample size n.

A few numerical examples illustrate the behavior of this equation. The examples assume that the precision p is 0.9. Let
|C| = |E| = 2000. This means that R C = 9 × R E —target balls are nine times as common in the urn as error balls. Now, for
k = 3 and n = 10,000 we have P (x ∈ C) = 93.0%. Thus, we see that a small number of repetitions can yield high conﬁdence
in an extracted label. However, when the sample size increases so that n = 20,000, and the other parameters are unchanged,
then P (x ∈ C) drops to 19.6%. On the other hand, if C balls repeat much more frequently than E balls, say R C = 90 × R E
(with |E| set to 20,000, so that p remains unchanged), then P (x ∈ C) rises to 99.9%.

The above examples enable us to illustrate the advantages of Urns over the noisy-or model used in previous IE work
[25,1]. The noisy-or model for IE assumes that each extraction is an independent assertion, correct a fraction p of the time,
that the extracted label is correct. The noisy-or model assigns the following probability to extracted labels:

P noisy-or(x ∈ C|x appears k times) = 1 − (1 − p)k.

Therefore, the noisy-or model will assign the same probability—99.9%—in all three of the above examples. Yet, as explained
above, 99.9% is only correct in the case for which n = 10,000 and R C = 90 × R E . As the other two examples show, for
different sample sizes or repetition rates, the noisy-or model can be highly inaccurate. This is not surprising given that the
noisy-or model ignores the sample size and the repetition rates. Section 2.2 quantiﬁes the improvements over the noisy-or
obtained by Urns in practice.

2.0.2. Applicability of the Urns model

Under what conditions does our redundancy model provide accurate probability estimates? We address this question
formally in Section 3, but informally two primary criteria must hold. First, labels from the target set C must be repeated
on more balls in the urn than labels from the E set, as in Fig. 1. The shaded region in Fig. 1 represents the “confusion
region”—if we classify labels based solely on extraction count, half of the labels in this region will be classiﬁed incorrectly,
even with the ideal classiﬁer and inﬁnite data, because for these examples there simply isn’t enough information to decide
whether they belong to C or E. Thus, our model is effective when the confusion region is relatively small. Secondly, even
for a small confusion region, the sample size n must be large enough to approximate the two distributions shown in Fig. 1;
otherwise the probabilities output by the model will be inaccurate.

730

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

2.0.3. Multiple urns

We now generalize our model to encompass multiple urns. When we have multiple extraction mechanisms for the same
target class, we could simply sum the extraction counts for each example and apply the single-urn model as described in
the previous section. However, this approach forfeits differences between the extraction mechanisms that may be informative
for classiﬁcation. For example, an IE system might employ several patterns for extracting city names, e.g. “cities including
x” and “x and other towns.” It is often the case that different patterns have different modes of failure, so labels extracted
by multiple patterns are generally more likely to be correct than those appearing for a single pattern. Previous work in
co-training has shown that leveraging distinct uncorrelated “views” of the data is often valuable [5]. We model this situation
by introducing multiple urns, where each urn represents a different pattern.3

Instead of n total extractions, in the multi-urn case we have a sample size nm for each urn m ∈ M, with the label for
example x appearing km times. Let A(x, (k1, . . . , km), (n1, . . . , nm)) denote this event. Further, let Am(x, k, n) be the event
that label x appears k times in n draws from urn m, and assuming that the draws from each urn are independent, we have:

Proposition 2.

(cid:5)

P

x ∈ C

(cid:5)

(cid:8)
(cid:8) A

x, (k1, . . . , km), (n1, . . . , nm)

(cid:6)(cid:6)

(cid:7)

(cid:9)

=

(cid:7)

ci ∈C

x∈C∪E

m∈M P ( Am(ci, km, nm))
(cid:9)
m∈M P ( Am(x, km, nm))

.

With multiple urns, the distributions of labels among balls in the urns are represented by multi-sets numm(C) and
numm(E). Expressing the correlation between numm(x) and numm(cid:5) (x) is an important modeling decision. Multiple urns are
especially beneﬁcial when the repetition rates for elements of C are more strongly correlated across different urns than
they are for elements of E—that is, when numm(x) and numm(cid:5) (x) are proportionally more similar for x ∈ C than for x ∈ E.
Fortunately, this turns out to be the case in practice in IE. We describe our method for modeling multi-urn correlation in
Section 2.1.1.

2.1. Implementation of Urns

This section describes how we implement Urns for both UIE and supervised IE, and identiﬁes the assumptions made in

each case.

In order to compute probabilities for extracted labels, we need a method for estimating num(C) and num(E). For the
purpose of estimating these sets from labeled or unlabeled data, we assume that num(C) and num(E) are Zipf distributed,
−zC . We can then char-
meaning that if ci is the ith most frequently repeated label in C , then num(ci) is proportional to i
acterize the num(C) and num(E) sets with ﬁve parameters: the set sizes |C| and |E|, the shape parameters zC and zE , and
the extraction precision p.

2.1.1. Multiple urns

To model multiple urns, we consider different precisions pm for each urn, but make the simplifying assumption that
the size and shape parameters are the same for all urns. As mentioned above, we expect repetition rate correlation across
urns to be higher for elements of the C set than for the E set. We model this correlation as follows: ﬁrst, elements of
the C set are assumed to come from the same location on the Zipf curve for all urns, that is, their relative frequencies are
perfectly correlated. Some elements of the E set are similar, and have the same relative frequency across urns—we refer
to these as global errors. However, the rest of the E set is made up of local errors, meaning that they appear for only
one kind of mechanism (for example, “Eastman Kodak” is extracted as an instance of Film only in phrases involving the
word “ﬁlm”, and not in those involving the word “movie.”). Formally, local errors are labels that are present in some urns
and not in others. Each type of local error makes up some fraction of the E set, and these fractions are the parameters
of our correlation model. Assuming this simple correlation model and identical size and shape parameters across urns is
too restrictive in general—differences between mechanisms are often more complex. However, our assumptions allow us to
compute probabilities eﬃciently (as described below), and don’t appear to hurt performance signiﬁcantly in practice (i.e.
when compared with an “ideal” model as in Section 2.2.1).

With this correlation model, if a label x is an element of C or a global error, it will be present in all urns. In terms of

Proposition 2, the probability that a label x appears km times in nm draws from m is:

(cid:5)

(cid:6)
Am(x, km, nm)

P

=

(cid:5)

(cid:6)
km
fm(x)

(cid:5)

(cid:6)
nm−km
1 − fm(x)

(3)

(cid:3)

(cid:4)

nm
km

where fm(x) is the frequency of label x. That is,

−zC

fm(ci) = pm Q C i
fm(ei) = (1 − pm)Q E i

for ci ∈ C,
−zE

for ei ∈ E.

3 We may lump several patterns into a single urn if they tend to behave similarly.

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

731

In these expressions, i is the frequency rank of the label, assumed to be the same across all urns, and Q C and Q E are
normalizing constants such that

(cid:2)

(cid:2)

−zC =

Q C i

−zE = 1.

Q E i

ci ∈C

ei ∈E

For a local error x which is not present in urn m, P ( Am(x, km, nm)) is 1 if km = 0 and 0 otherwise. Substituting these

expressions for P ( Am(x, km, nm)) into Proposition 2 gives the ﬁnal form of our Urns model.

2.1.2. Eﬃcient computation

A feature of our implementation is that it allows for eﬃcient computation of probabilities. In general, computing the sum
in Proposition 2 over the potentially large C and E sets would require signiﬁcant computation for each label. However, given
a ﬁxed number of urns, with num(C) and num(E) Zipf distributed, an integral approximation to the sum in Proposition 2
(using a Poisson in place of the binomial in Eq. (3)) can be solved in closed form in terms of incomplete Gamma functions.
The details of this approximation and its solution for the single-urn case are given in Section 3.4 The closed form expression
can be evaluated quickly, and thus probabilities for labels can be obtained eﬃciently. This solution leverages our assumptions
that size and shape parameters are identical across urns, and that relative frequencies are perfectly correlated. Finding
eﬃcient techniques for computing probabilities under less stringent assumptions is an item of future work.

2.1.3. Supervised parameter estimation

In the event that a large sample of hand-labeled training examples is available for each target class of interest, we can
directly estimate each of the parameters of Urns. In our experiments, we use Differential Evolution to identify parameter
settings that approximately maximize the conditional log likelihood of the training data [40].5 Differential Evolution is
a population-based stochastic optimization technique, appropriate for optimizing the non-convex likelihood function for
Urns. Once the parameters are set, the model yields a probability for each extracted label, given the number of times km it
appears in each urn and the number of draws nm from each urn.

2.1.4. Unsupervised parameter estimation

Estimating model parameters in an unsupervised setting requires making a number of assumptions tailored to the spe-
ciﬁc task. Below, we detail the assumptions employed in Urns for UIE. It is important to note that while these assumptions
are speciﬁc to UIE, they are not speciﬁc to a particular target class. As argued in [17], UIE systems cannot rely on per-
class information—in the form of either assumptions or hand-labeled training examples—if they are to scale to extracting
information on arbitrary classes that are not speciﬁed in advance.

Implementing Urns for UIE requires a solution to the challenging problem of estimating num(C) and num(E) using only
untagged data. Let U be the multi-set consisting of the number of times each unique label was extracted in a given corpus.
|U | is the number of unique labels encountered, and the sample size n =

(cid:7)

r∈U r.

In order to learn num(C) and num(E) without hand-labeled data, we make the following assumptions:

• Because the number of different possible errors is nearly unbounded, we assume that the error set is very large.6
• We assume that both num(C) and num(E) are Zipf distributed where the zE parameter is set to 1.
• In our experience with KnowItAll, we found that while different extraction rules have differing precision, each rule’s
precision is stable across different classes [17]. For example, the precision of the extractor “cities such as x” and “insects
such as y” are similar. Urns takes this precision as an input. To demonstrate that Urns is not overly sensitive to this
parameter, we chose a ﬁxed value (0.9) and used it as the precision pm for all urns in our experiments.7 Section 2.2.5
provides evidence that the observed p value tends to be relatively stable across different target classes.

We then use Expectation Maximization (EM) over U in order to arrive at appropriate values for |C| and zC (these two

quantities uniquely determine num(C) given our assumptions). Our EM algorithm proceeds as follows:

1. Initialize |C| and zC to starting values.
2. Repeat until convergence:

(a) E-step Assign probabilities to each element of U using Proposition (1).
(b) M-step Set |C| and zC from U using the probabilities assigned in the E-step (details below).

4 For the multi-urn solution, which is obtained through a symbolic integration package and therefore complicated, we refer the reader to the Java

implementation of the solution which is available for download—see [12], Appendix A.
5 Speciﬁcally, we use the Differential Evolution routine built into Mathematica 5.0.
6 In our experiments, we set |E| = 106. A sensitivity analysis showed that changing |E| by an order of magnitude, in either direction, resulted in only

small changes to our results.

7 A sensitivity analysis showed that choosing a substantially higher (0.95) or lower (0.80) value for pm still resulted in Urns outperforming the noisy-or

model by at least a factor of 8 and PMI by at least a factor of 10 in the experiments described in Section 2.2.1.

732

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

We obtain |C| and zC in the M-step by ﬁrst estimating the rank-frequency distribution for labels from C in the untagged
data U . From U and the probabilities found in the E-step, we can obtain E C [k], the expected number of labels from C that
were extracted k times for k (cid:2) 1 (the k = 0 case is detailed below). We then round these fractional expected counts into a
discrete rank-frequency distribution with a number of elements equal to the expected total number of labels from C in the
k E C [k]. We obtain zC by ﬁtting a Zipf curve to this rank-frequency distribution by linear regression on a
untagged data,
log–log scale.8

(cid:7)

Lastly, we set |C| =

k E C [k]+ unseen, where we estimate the number of unseen labels of the C set (i.e. those with k = 0)
using Good–Turing estimation [20]. Good–Turing estimation provides an estimate of the probability mass of the unseen labels
(speciﬁcally, the estimate is equal to the expected fraction of the draws from C that extracted labels seen only once). To
convert this probability into a number of unseen labels, we simply assume that each unseen label has probability equal to
that of the least frequent seen label. A potentially more accurate method would choose unseen such that the actual number
of unique labels observed is equal to that expected by the model (where the latter is measured e.g. by sampling). Such
methods are an item of future work.

(cid:7)

This unsupervised learning strategy proved effective for target classes of different sizes; for example, Urns learned pa-
rameters such that the number of elements of the Country relation with non-negligible extraction probability was about
two orders of magnitude smaller than that of the Film and City classes, which approximately agrees with the actual
relative sizes of these sets.

2.2. Urns: Experimental results

How accurate is Urns at assigning probabilities of correctness to extracted labels? In this section, we answer this ques-

tion by comparing the accuracy of Urns’s probabilities against other methods from previous work.

This section begins by describing our experimental results for IE under two settings: unsupervised and supervised. We
ﬁrst describe two unsupervised methods from previous work: the noisy-or model and PMI. We then compare Urns with
these methods experimentally, and lastly compare Urns with several baseline methods in a supervised setting.

We evaluated our algorithms on extraction sets for the classes City(x), Film(x), Country(x), and Mayo-
rOf(x,y), taken from experiments with the KnowItAll system performed in [17]. The sample size n was 64,605 for
City, 135,213 for Film, 51,390 for Country and 46,858 for MayorOf. The extraction patterns were partitioned into urns
based on the name they employed for their target relation (e.g. “country” or “nation”) and whether they were left-handed
(e.g. “countries including x”) or right-handed (e.g. “x and other countries”). We chose this partition because it results in
extraction mechanisms that make relatively uncorrelated errors, as assumed in the multiple-urns model. For example, the
phrase “Toronto, Canada and other cities” will mislead a right-handed pattern into extracting “Canada” as a City candi-
date, whereas a left-handed pattern is far less prone to this error. Each combination of relation name and handedness was
treated as a separate urn, resulting in four urns for each of City(x), Film(x), and Country(x), and two urns for
MayorOf(x, y).9,10

For each relation, we tagged a random sample of 1000 extracted labels, using external knowledge bases (the Tipster
Gazetteer for cities and the Internet Movie Database for ﬁlms) and manually tagging those instances not found in a knowl-
edge base. For Country and MayorOf, we manually veriﬁed correctness for all extracted labels, using the Web. Countries
were marked correct provided they were a correct name (including abbreviations) of a current country, and mayors were
marked correct if the person was a mayor of the city at some point in time. In the UIE experiments, we evaluate our
algorithms on all 1000 examples, and in the supervised IE experiments we perform 10-fold cross validation.

2.2.1. UIE experiments

We compare Urns against two other methods for unsupervised information extraction. First, in the noisy-or model used
m∈M (1 − pm)km , where pm

in previous work, an extracted label appearing km times in each urn is assigned probability 1 −
is the extraction precision for urn m. We describe the second method below.

(cid:9)

Our previous work on KnowItAll used Pointwise Mutual Information (PMI) to obtain probability estimates for extracted
labels [17]. Speciﬁcally, the PMI between an extracted label and a set of automatically generated discriminator phrases (e.g.,
“movies such as x”) is computed from Web search engine hit counts. These PMI scores are used as features in a Naive Bayes
Classiﬁer (NBC) to produce a probability estimate for the label. The NBC is trained using a set of automatically bootstrapped
seed instances. The positive seed instances are taken to be those having the highest PMI with the discriminator phrases

8 To help ensure that our probability estimates are increasing with k, if zC falls below 1, we adjust zE to be less than zC .
9 Draws from Urns are intended to represent independent evidence. Because the same sentence can be duplicated across multiple different Web
documents, in these experiments we consider only each unique sentence containing an extraction to be a draw from Urns. In experiments with other
possibilities, including counting the number of unique documents producing each label, or simply counting every extraction of each label, we found that
for UIE, performance differences between the various approaches were small compared to the differences between Urns and other methods.
10 In the unsupervised setting, we assumed that the fraction of errors in the urns that are local is 0.1, and that errors appearing for only left- or only
right-handed patterns were equally prevalent to those appearing for only one label. The only exception was the City class, where because the target class
is the union of the two class names (“city” and “town”) rather than the intersection (as with “ﬁlm” and “movie”), we assumed that no local errors appeared
for only one name. Altering these settings (or indeed, simply using a single urn—see Section 2.2.4) had negligible impact on the results in Fig. 2.

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

733

Fig. 2. Deviation of average log likelihood from the ideal for four relations (lower is better). On average, Urns outperforms noisy-or by a factor of 15, and
PMI by a factor of 20.

Table 1
Improved eﬃciency due to Urns. The top row reports the number of search engine queries
made by KnowItAll using PMI divided by the number of queries for KnowItAll using Urns.
The bottom row shows that PMI’s queries increase with k—the average number of distinct
labels for each relation. Thus, speedup tends to vary inversely with the average number of
times each label is drawn.

Speedup
Average k

City
17.3×
3.7

Film
9.5×
4.0

MayorOf
1.9×
20.7

Country
3.1×
23.3

after the bootstrapping process; the negative seeds are taken from the positive seeds of other relations, as in other work
(e.g., [25]).

Although PMI was shown in [17] to rank extracted labels fairly well, it has two signiﬁcant shortcomings. First, obtaining
the hit counts needed to compute the PMI scores is expensive, as it requires a large number of queries to a public Web
search engine (or, alternatively, the expensive construction of a local Web-scale inverted index). Second, the seeds produced
by the bootstrapping process are often noisy and not representative of the overall distribution of extractions [39]. This
combined with the probability polarization introduced by the NBC tends to give inaccurate probability estimates.

2.2.2. Discussion of UIE results

The results of our unsupervised experiments are shown in Fig. 2. We plot deviation from the ideal log likelihood—deﬁned
as the maximum achievable log likelihood given our feature set. Speciﬁcally, for each class C deﬁne an ideal model P ideal(x)
equal to the fraction of test set labels with the same extraction counts as x that are correct. We deﬁne the ideal log
likelihood as:

ideal log likelihood =

log P ideal(x) +

.

(4)

(cid:2)

x∈C

(cid:2)

log

x∈E

(cid:5)

(cid:6)
1 − P ideal(x)

Our experimental results demonstrate that Urns overcomes the weaknesses of PMI. First, Urns’s probabilities are far
more accurate than PMI’s, achieving a log likelihood that is a factor of 20 closer to the ideal, on average (Fig. 2). Second,
Urns is substantially more eﬃcient as shown in Table 1.

This eﬃciency gain requires some explanation. These experiments were performed using the KnowItAll system, which
relies on queries to Web search engines to identify Web pages containing potential extractions. The number of queries
KnowItAll can issue daily is limited, and querying over the Web is, by far, KnowItAll’s most expensive operation. Thus,
number of search engine queries is our eﬃciency metric. Let d be the number of discriminator phrases used by the PMI
explained above. The PMI method requires O (d) search engine queries to compute the PMI of each extracted label from
search engine hit counts. In contrast, Urns computes probabilities directly from the set of extractions—requiring no additional
queries, which cuts KnowItAll’s queries by factors ranging from 1.9 to 17.

As explained in Section 2.0.1, the noisy-or model ignores target set size and sample size, which leads it to assign prob-
abilities that are far too high for the Country and MayorOf relations, where the average number of times each label is
extracted is high (see bottom row of Table 1). This is further illustrated for the Country relation in Fig. 3. The noisy-or
model assigns appropriate probabilities for low sample sizes, because in this case most extracted labels are in fact correct,
as predicted by the noisy-or model. However, as sample size increases, the fraction of correct labels decreases—and the
noisy-or estimate worsens. On the other hand, Urns avoids this problem by accounting for the interaction between target
set size and sample size, adjusting its probability estimates as sample size increases. Given suﬃcient sample size, Urns per-
forms close to the ideal log likelihood, improving slightly with more samples as the estimates obtained by the EM process

734

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

Fig. 3. Deviation of average log likelihood from the ideal as sample size varies for the Country relation (lower is better). Urns performs close to the ideal
given suﬃcient sample size, whereas noisy-or becomes less accurate as sample size increases.

become more accurate. Overall, Urns assigns far more accurate probabilities than the noisy-or model, and its log likelihood
is a factor of 15 closer to the ideal, on average. The very large differences between Urns and both the noisy-or model and
PMI suggest that, even if the performance of Urns degrades in other domains, it is quite likely to still outperform both PMI
and the noisy-or model.

Our computation of log-likelihood contains a numerical detail that could potentially inﬂuence our results. To avoid the
possibility of a likelihood of zero, we restrict the probabilities generated by Urns and the other methods to lie within the
range (0.00001, 0.99999). Widening this range tended to improve Urns’s performance relative to the other methods, as this
increases the penalty for erroneously assigning extreme probabilities—a problem more prevalent for PMI and noisy-or than
for Urns. If we narrow the range by two digits of precision, to (0.001, 0.999), Urns still outperforms PMI by a factor of
15, and noisy-or by a factor of 13. Thus, we are comfortable that the differences observed are not an artifact of this design
decision.

Lastly, although we focus our evaluation on the quality of each method’s probability estimates in terms of likelihood,
the advantage of Urns is also reﬂected in other metrics such as classiﬁcation accuracy. When we convert each method’s
probability estimate into a classiﬁcation (positive for a label iff the probability estimate is greater than 0.5), we ﬁnd that
Urns has an average accuracy of approximately 81%, compared with PMI at 63% and noisy-or at 47%. Thus, Urns decreases
classiﬁcation error over the previous methods by a factor of 1.9× to 2.8×. Urns ranks the majority of extracted labels in
a manner similar to the noisy-or model (which ranks by overall frequency). Thus, Urns offers comparable performance to
noisy-or in terms of e.g. area under the precision/recall curve [6]. However, the correlations captured by multiple urns can
improve the ranking of suﬃciently frequent labels, as detailed in Section 2.2.4.

2.2.3. Supervised IE experiments

We compare Urns with three supervised methods. All methods utilize the same feature set as Urns, namely the extrac-

tion counts km.

• noisy-or – Has one parameter per urn, making a set of M parameters (h1, . . . , hM ), and assigns probability equal to

(cid:10)

1 −

(1 − hm)km .

m∈M

• logistic regression – Has M + 1 parameters (a, b1, b2, . . . , bM ), and assigns probability equal to

1
(cid:7)

m∈M kmbm

.

1 + ea+

• SVM – Consists of an SVM classiﬁer with a Gaussian kernel. To transform the output of the classiﬁer into a probability,
we use the probability estimation built-in to LIBSVM [8], which is based on logistic regression of the SVM decision
values.

Parameters maximizing the conditional likelihood of the training data were found for the noisy-or and logistic regression
models using Differential Evolution.11 For those models and Urns, we performed 20 iterations of Differential Evolution

11 For logistic regression, different convex optimization methods are applicable; however, in our experiments the Differential Evolution routine appeared
to converge to an optimum, and we do not believe the choice of optimization method impacted the logistic regression results.

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

735

Table 2
Supervised IE experiments. Deviation from the ideal log likelihood for each method and each relation (lower is better). The overall performance differences
are small, with Urns 19% closer to the ideal than noisy-or, on average, and 10% closer than logistic regression. The overall performance of SVM is close to
that of Urns.

noisy-or
logistic regression
SVM
Urns

City

0.0439
0.0466
0.0444
0.0418

Film

0.1256
0.0893
0.0865
0.0764

Mayor

0.0857
0.0655
0.0659
0.0721

Country

Average

0.0795
0.1020
0.0769
0.0823

0.0837
0.0759
0.0684
0.0681

Table 3
Label-precision of the K highest-ranked extracted labels for varying values
of K between 10 and 200. Across the ﬁve K values shown, Urns reduces
error over the single-urn model by an average of 29%.

Number of highest-ranked extracted labels

Single-urn

10
20
50
100
200

1
0.9875
0.925
0.8375
0.7075

Urns

1
1
0.955
0.845
0.71

using 400 distinct search points. In the SVM case, we performed grid search to ﬁnd the kernel parameters giving the best
likelihood performance for each training set—this grid search was required to get acceptable performance from the SVM on
our task.

The results of our supervised learning experiments are shown in Table 2. Urns, because it is more expressive, is able to
outperform the noisy-or and logistic regression models. In terms of deviation from the ideal log likelihood, we ﬁnd that on
average Urns outperforms the noisy-or model by 19%, logistic regression by 10%, but SVM by only 0.4%.

2.2.4. Beneﬁt from multiple urns

The previous results use the full multi-urn model. How much of Urns’s large performance advantage in UIE is due to

multiple urns?

In terms of likelihood, as measured in Fig. 2, we found that the impact of multiple urns is negligible. This is primarily
because the majority of extracted labels occur only a handful of times, and in these cases the multiple-urn model lacks
enough data to estimate the correlation of counts across urns.

Multiple urns can offer some performance beneﬁt, however, for more commonly extracted labels. We evaluated the
effect of multiple urns for UIE across the four relations shown in Fig. 2, computing the average label-precision at K , equal
to the fraction of the K highest-probability labels which are correct. The results under the single-urn and full Urns model
are shown in Table 3 for varying K . The full Urns model always performs at least as well as the single-urn model, and
sometimes provides much higher precision. In fact, using multiple urns reduces the error by 29% on average for the ﬁve K
values shown in the table.

2.2.5. Is p a “universal constant”?

Our UIE experiments employed an extraction precision parameter p of 0.9. While Urns still massively outperforms
previous methods even if this value is adjusted to 0.8 or 0.95, the accuracy of Urns’s probabilities does degrade as p is
altered away from 0.9.

In this section, we attempt to measure how consistent the observed p value is across varying classes. This experiment
differs somewhat from those presented above. In order to test across a wide variety of classes, we moved beyond the
KnowItAll experiments from [17] and used the TextRunner system to provide instances of classes [3]. To choose classes
to investigate, we randomly selected 12 nouns from WordNet for which there were at least 100 extractions (not necessarily
unique) in TextRunner. We excluded nouns which were overly general such that nearly any extraction would be correct
(e.g., the class Example) and nouns which are rarely or never used to name concrete instances (e.g., the class Purchases).
The results in this section were compiled by querying TextRunner for 100 sentences containing extractions for each class.12
While TextRunner provides greater coverage than KnowItAll, precision in general is lower. One of the inaccuracies of the
TextRunner system is that it often fails to delimit the boundaries of extractions properly (e.g., it extracts the phrase “alkanes
or cycloalkanes” as an instance of the Solvents class). We found that we could improve the precision of TextRunner by
over 20% on average by post-processing all extractions, breaking on conjunctions or punctuation (i.e. the previous example
becomes simply “alkanes”). Our results employ this heuristic.

The results of the experiment are shown in Table 4. For each class, “p Observed” gives the fraction of the 100 extractions
tagged correct (by manual inspection). The average p value observed across classes of 0.84 is lower than the value of 0.9

12 The list of excluded nouns and the labeled extractions for each selected class are available for download; see [12], Appendix A.

736

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

Table 4
Average p values for various classes, measured from 100 hand-tagged examples per class. Three
of the 12 classes have p values in bold, indicating a statistically signiﬁcantly difference from the
mean of 0.84 (signiﬁcance level of 0.01, Fisher Exact Test). However, if we adjust the estimate of p
per class according to how frequently it occurs in the “such as” pattern (using the factor hclass ; see
text), none of the resulting p + hclass values are signiﬁcantly different from the mean.

Class

solvents
devices
thinkers
relaxants
mushrooms
mechanisms
resorts
flies
tones
wounds
machines
cultures

p Observed

Hits(class such as)
Hits(class)

p Observed + hclass

0.98
0.93
0.93
0.92
0.86
0.85
0.85
0.84
0.77
0.77
0.69
0.67

0.201
0.022
0.013
0.010
0.001
0.017
0.002
0.0004
0.001
0.002
0.002
0.002

0.85
0.87
0.89
0.89
0.90
0.80
0.88
0.93
0.83
0.80
0.71
0.70

we use in our previous experiments; this reﬂects the relatively lower precision of TextRunner as well as the increased
diﬃculty of extracting common nouns (versus the proper noun extractions used previously). The results show that while
there is substantial regularity in observed p values, the values are not perfectly consistent. In fact, three classes (with “p
Observed” values in bold) differ signiﬁcantly from the average observed p value (at signiﬁcance level of 0.01, Fisher Exact
Test).

Given that we observe variability in p values across classes, an important question is whether the correct p value for a
given class, pclass, can be predicted. We observed empirically that the precision of extractions for a class increases with how
relatively frequently the class name is used in extraction patterns. As an example, the phrase “cultures such as x” appears
infrequently relative to the word “cultures,” as shown in Table 4 in terms of Web hit counts obtained from a search engine.
In turn, the class Cultures exhibits a relatively low p value. Intuitively, this result makes sense—class names which are
more “natural” for naming instances should both appear more frequently in extraction patterns, and provide more precise
extractions.

We can exploit the above intuition by adjusting the estimate of extraction precision for each class by a factor hclass. For

illustration, based on the values in Table 4, we devised the following adjustment factor:

hclass = 0.08

(cid:3)
−2.36 − log10

(cid:4)

Hits(class such as)
Hits(class)

.

(5)

The adjustment factor can give us a more accurate estimate of the precision for a given class pclass = p − hclass.

Obviously, the expression hclass is heuristic and could be further reﬁned using additional experiments. Nonetheless, ad-
justing by the factor does allow us to obtain better precision estimates across classes. The quantity “p Observed + hclass”
has only 57% of the variance of the original “p Observed” (and the same mean, by construction). Further, none of the ob-
served differences of “p Observed + hclass” are statistically signiﬁcantly different from the original mean, using the same
signiﬁcance test employed previously.

Lastly, we should mention that even without any adjustment factor, the variance in p value across classes is not substan-
tially greater than that employed in our sensitivity analysis in Section 2.2. Thus, we expect the performance advantages of
Urns over the noisy-or and PMI models to extend to these other classes as well.

2.3. Urns: Other applications

Urns is a general model. For any classiﬁcation task, if one of the features represents a count of observations following a
mixture of Zipf distributions as assumed by Urns, the model can be employed. In this section, we highlight three examples
of how the Urns model has been applied to tasks other than that of assigning probabilities of correctness to extractions.

2.3.1. Estimating UIE precision and recall

An attractive feature of Urns is that it enables us to estimate its expected recall and precision as a function of sample
size. If the distributions in Fig. 1 cross at the dotted line shown then, given a suﬃciently large sample size n, expected recall
will be the fraction of the area under the C curve lying to the right of the dotted line.

For a given sample size n, deﬁne τn to be the least number of appearances k at which an extracted label is more likely
to be from the C set than the E set (given the distributions in Fig. 1, τn can be computed using Proposition 1). Then we
have:

E[TruePositives] = |C| −

(cid:2)

τn−1(cid:2)

(cid:4)(cid:3)

(cid:3)
n

(cid:4)
k

(cid:3)

(cid:4)

n−k

r

s

1 − r
s

r∈num(C)

k=0

k

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

737

Table 5
Estimating precision and recall in UIE. Listed is the Urns model estimate for precision and recall, along with the actual measured quantities, for four classes.
The major differences between the classes—that the MayorOf and Country classes have roughly two orders of magnitude lower recall than the City
and Film classes—is qualitatively reﬂected by the model.

City
Country
Film
MayorOf

n

64605
51390
135213
46858

E[Recall]

Actual recall

E[Precision]

Actual precision

12900
37
25900
58

14300
176
23400
158

0.78
0.63
0.79
0.62

0.84
0.77
0.68
0.79

where we deﬁne “true positives” to be the number of extracted labels ci ∈ C for which the model assigns probability
P (ci ∈ C) > 0.5.

The expected number of false positives is similarly:
(cid:4)
k

(cid:4)(cid:3)

τn−1(cid:2)

(cid:2)

(cid:3)
n

r

E[FalsePositives] = |E| −

r∈num(E)

k=0

k

s

(cid:3)

1 − r
s

(cid:4)

n−k

.

The expected precision of the system can then be approximated as:

E[Precision] ≈

E[TruePositives]
E[FalsePositives] + E[TruePositives]

.

To illustrate the potential beneﬁt of the above calculations and evaluate their accuracy, we computed expected recall and
precision for the particular num(C) and num(E) learned (in the unsupervised setting) in our experiments in Section 2.2.
The results appear in Table 5. The recall estimates are within 11% of the actual recall (that is, the estimated number of
correct examples in our set of extracted labels, based on the hand-tagged test set) for the City and Film classes. Further,
the estimates reﬂect the important qualitative difference between the large City and Film classes as compared with the
smaller MayorOf and Country classes.

Were we to increase the sample size n for the Film class and the Country class each to 1,000,000, the model predicts
that we would increase our Film recall by 81%, versus only 4% for Country. Thus, the above equations allow an informa-
tion extraction system to dynamically choose how to allocate resources to match given precision and recall goals, even in
the absence of hand-labeled data.

2.3.2. Estimating the functionality of relations

Knowledge of which relations in a knowledge base are functional is valuable for a variety of different tasks. Previous work
has shown that knowledge of functional relations can be used to automatically detect contradictions in text [11,34], and to
automatically identify extractor errors in IE [1]. For example, if we know that the Headquartered relation is functional
and we see one document asserting that Intel is headquartered in Santa Clara, and another asserting it is headquartered in
Phoenix, we can determine that either the documents contradict each other, or we have made an error in extraction. In this
section, we illustrate how Urns can be used to automatically compute the probability that a phrase denotes a functional
relation.

The discussion in this section is based on a set of extracted tuples. An extracted tuple takes the form R(x, y) where
(roughly) x is the subject of a sentence, y is the object, and R is a phrase denoting the relationship between them. If the
relation denoted by R is functional, then typically the object y is a function of the subject x. Thus, our discussion focuses
on this possibility, though the analysis is easily extended to the symmetric case.

The main evidence that a relation R(x, y) is functional comes from the distribution of y values for a given x value. If R
denotes a function and x is unambiguous, then we expect the extractions to be predominantly a single y value, with a few
outliers due to noise.

Example A in Fig. 4 has strong evidence for a functional relation. 66 out of 70 extractions for was_born_in (Mozart,
PLACE) have the same y value. An ambiguous x argument, however, can make a functional relation appear non-functional.
Example B refers to multiple real-world individuals named “John Adams” and has a distribution of y values that appears
less functional than example C, which has a non-functional relation.

Logically, a relation R is functional in a variable x if it maps it to a unique variable y: ∀x, y1, y2 R(x, y1) ∧ R(x, y2) ⇒
y1 = y2. Thus, given a large random sample of ground instances of R, we could detect with high conﬁdence whether R is
functional. In text, the situation is far more complex due to ambiguity, polysemy, synonymy, and other linguistic phenomena.
To decide whether R is functional in x for all x, we ﬁrst consider how to detect whether R is locally functional for
a particular value of x. We later combine the local functionality probabilities to estimate the global functionality of a
relation.13 Local functionality for a given x can be modeled in terms of the global functionality of R and the ambiguity of x.

13 We compute global functionality as the average local scores, weighted by the probability that x is unambiguous.

738

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

Fig. 4. Functional relations such as example A have a different distribution of y values than non-functional relations such as C. Ambiguous x argument as
in B, however, can make a functional relation appear non-functional.

We later outline an EM-style algorithm that alternately estimates the probability that R is functional and the probability
that x is ambiguous.

Let θ f

R be the probability that R(x, ·) is locally functional for a random x, and let Θ f be the vector of these parameters
x represents the probability that x is locally unambiguous for random R, and Θ u the vector

across all relations R. Likewise, θ u
for all x.

We wish to determine the maximum a posteriori (MAP) functionality and ambiguity parameters given the observed data D,

that is arg maxΘ f ,Θ u P (Θ f , Θ u|D). By Bayes Rule:
(cid:6)
(cid:5)

(cid:6)

(cid:5)

(cid:6)

(cid:5)

P

Θ f , Θ u|D

∝ P

D|Θ f , Θ u

P

Θ f , Θ u

.

(6)

We outline a generative model for the data, P (D|Θ f , Θ u). Let R

∗
x indicate the event that the relation R is locally func-
tional for the argument x, and that x is locally unambiguous for R. Also, let D indicate the set of observed tuples, and deﬁne
D R(x,·) as the multi-set containing the frequencies for extractions of the form R(x, ·).

Let us assume that the event R

x , and further assume that given these two parameters, local
ambiguity and local functionality are conditionally independent. We obtain the following expression for the probability of
R

∗
x depends only on θ f

R and θ u

∗
x given the parameters:
= θ f

|Θ f , Θ u

R

P

(cid:6)

(cid:5)

∗
x

R θ u
x .

We assume each set of data D R(x,·) is generated independently of all other data and parameters, given R

the above we have:
(cid:5)
D|Θ f , Θ u

P

(cid:6)

=

(cid:10)
(cid:5)

(cid:5)

P

D R(x,·)|R

(cid:6)

∗
x

θ f
R θ u

x

+ P

(cid:5)

D R(x,·)|¬R

∗
x

(cid:6)(cid:5)

R,x

1 − θ f

R θ u

x

(cid:6)(cid:6)

.

∗
x . From this and

(7)

or not R

These independence assumptions allow us to express P (D|Θ f , Θ u) in terms of distributions over D R(x,·) given whether

∗
x holds. We use a single-urn model to estimate these probabilities based on binomial distributions.

(cid:7)

Let k = max D R(x,·), and let n =

D R(x,·); we will approximate the distribution over D R(x,·) in terms of k and n. In the
single-urn model, if R(x, ·) is locally functional and unambiguous, k has a binomial distribution with parameters n and p,
where p is the precision of the extraction process. If R(x, ·) is not locally functional and unambiguous, then we expect k
to typically take on smaller values. Empirically, we ﬁnd that the underlying frequency of the most frequent element in the
¬R

∗
x case tends to follow a Beta distribution.
Under the model, the probability of the evidence given R

∗
x is:

(cid:5)

P

D R(x,·)|R

(cid:6)

∗
x

(cid:5)
k, n|R

(cid:6)

∗
x

=

≈ P

pk(1 − p)n−k.

(cid:4)

(cid:3)
n

k

And the probability of the evidence given ¬R

∗
x is:

(cid:5)

P

D R(x,·)|¬R

(cid:6)

∗
x

(cid:5)
k, n|¬R

(cid:6)

∗
x

=

≈ P

(cid:4) 1(cid:11)

(cid:3)
n

k

0

(cid:5)k+α f −1(1 − p

(cid:5))n+β f −1−k

p

B(α f , β f )

(cid:5) =

dp

(cid:6)

(cid:5)
Γ (n − k + β f )Γ (α f + k)
n
k
B(α f , β f )Γ (α f + β f + n)

,

where n is the sum over D R(x,·), Γ is the Gamma function and B is the Beta function. α f and β f are the parameters of the
Beta distribution for the ¬R

∗
x case (in practice, these are estimated empirically).

Substituting Eq. (9) into Eq. (7) and applying an appropriate prior gives the probability of parameters Θ f and Θ u given
the observed data D. However, Eq. (7) contains a large product of sums—with two independent vectors of coeﬃcients, Θ f
and Θ u —making it diﬃcult to optimize analytically.

If we knew which arguments were ambiguous, we would ignore them in computing the functionality of a relation. Like-
wise, if we knew which relations were non-functional, we would ignore them in computing the ambiguity of an argument.
Instead, we initialize the Θ f and Θ u arrays randomly, and then execute an EM-style algorithm to arrive at a high-probability
setting of the parameters.

Note that if Θ u is ﬁxed, we can compute the expected fraction of locally unambiguous arguments x for which R is locally
functional, using D R(x(cid:5),·) and Eq. (9). Likewise, for ﬁxed Θ f , for any given x we can compute the expected fraction of locally
functional relations R that are locally unambiguous for x.

(8)

(9)

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

739

Speciﬁcally, we repeat until convergence:

1. Set θ f
R
2. Set θ u
x

= 1
sR
= 1
sx

(cid:7)

(cid:7)

x P (R
R P (R

∗
x
∗
x

|D R(x,·))θ u
|D R(x,·))θ f

x for all R.
R for all x.

(cid:7)

In both steps above, the sums are taken over only those x or R for which D R(x,·) is non-empty. Also, the normalizer
sR =

x and likewise sx =

(cid:7)

x θ u

R θ f
R .

By iteratively setting the parameters to the expectations in steps 1 and 2, we arrive at a good setting of the parameters.
The above algorithm is experimentally investigated in [34], showing that the technique effectively identiﬁes functional

relations, and can power effective contradiction detection.

2.3.3. Synonym resolution

The last application of Urns we will discuss is that of resolving which strings refer to the same objects or relations.
In text, the same object is often referred to by multiple distinct names—“U.S.” and “United States” each refer to the same
country, for example. Likewise, relationships between objects are often expressed as multiple distinct paraphrases (e.g., “x
is the capital of y” and “x, capital of y”).

The Resolver system performs Synonym Resolution—taking as input a set of extracted tuples (as discussed above, e.g., Is-
CapitalOf(D.C., United States)) and returning a set of clusters, where each cluster contains coreferential object
strings or relationship strings [42].

Here we provide a high-level description of how Resolver employs an Urns-like model, deferring to [42] for the details.
Consider the task of determining whether two strings s1 and s2 refer to the same object, based on a set of tuples each
including either s1 or s2 as an argument. Resolver speciﬁes a urn-based generative process for the observed tuples; namely,
the set of potential tuples for si are modeled as labels on balls in a urn, and the actual observed tuples involving si are
modeled as draws from the urn. Resolver assumes that if s1 and s2 refer to the same object, then the urn contents for s1
are maximally similar to those for s2; otherwise, the two urns can differ to a greater or lesser degree. With this assumption,
Resolver computes the probability that s1 and s2 co-refer based on how frequently they participate in similar tuples. This
method is shown to be effective for resolving synonymous strings in practice.

2.4. Related work

In contrast to the bulk of previous IE work, our focus is on unsupervised IE (UIE) where Urns substantially outperforms

previous methods (Fig. 2).

In addition to the noisy-or models we compare against in our experiments, the IE literature contains a variety of heuris-
tics using repetition as an indication of the veracity of extracted information. For example, Riloff and Jones [33] rank
extractions by the number of distinct patterns generating them, plus a factor for the reliability of the patterns. Our work is
intended to formalize these heuristic techniques, and unlike the noisy-or models, we explicitly model the distribution of the
target and error sets (our num(C) and num(E)), which is shown to be important for good performance in Section 2.2.1. The
accuracy of the probability estimates produced by the heuristic and noisy-or methods is rarely evaluated explicitly in the IE
literature, although most systems make implicit use of such estimates. For example, bootstrap-learning systems start with
a set of seed instances of a given relation, which are used to identify extraction patterns for the relation; these patterns
are in turn used to extract further instances (e.g. [33,25,1,30]). As this process iterates, random extraction errors result in
overly general extraction patterns, leading the system to extract further erroneous instances. The more accurate estimates
of extraction probabilities produced by Urns would help prevent this “concept drift.”

Skounakis and Craven [37] develop a probabilistic model for combining evidence from multiple extractions in a super-
vised setting. Their problem formulation differs from ours, as they classify each occurrence of an extraction, and then use a
binomial model along with the false positive and true positive rates of the classiﬁer to obtain the probability that at least
one occurrence is a true positive. Similar to the above approaches, they do not explicitly account for sample size n, nor do
they model the distribution of target and error extractions.

Culotta and McCallum [10] provide a model for assessing the conﬁdence of extracted information using conditional ran-
dom ﬁelds (CRFs). Their work focuses on assigning accurate conﬁdence values to individual occurrences of an extracted ﬁeld
based on textual features. This is complementary to our focus on combining conﬁdence estimates from multiple occurrences
of the same extracted label. In fact, each possible feature vector processed by the CRF in [10] can be thought of as a virtual
urn m in our Urns. The conﬁdence output of Culotta and McCallum’s model could then be used to provide the precision pm
for the urn.

Our UIE task is related to previous work in automatically devising logical statements from text [24,36] and unsupervised
semantic role labeling [41,21,32]. UIE is distinct in that the target output is a knowledge base of factual relations, rather
than an interpretation of text in terms of logic or labeled semantic roles. Because our UIE approach operates over a large
corpus, we do not attempt to identify all semantic assertions in the text corpus. Instead, we focus on only factual assertions
that can be identiﬁed automatically at relatively high precision (using e.g. extraction patterns), and present methods for
combining this evidence at Web-scale.

740

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

Our work is similar in spirit to BLOG, a language for specifying probability distributions over sets with unknown objects
[28]. As in our work, BLOG models can express observations as draws from an unknown set of balls in an urn. Whereas
BLOG is intended to be a general modeling framework for probabilistic ﬁrst-order logic with varying sets of objects, our
work is directed at modeling redundancy in IE. We also provide supervised and unsupervised learning methods for our
model that are effective for data sets containing many thousands of examples, along with experiments demonstrating their
eﬃcacy in practice.

One of the problems our EM-based algorithm for learning Urns parameters must solve is estimating the parameter |C|,
the size of the target set. This problem has commonalities with the classic “capture–recapture” problem from ecology, in
which the goal is to estimate the size of an animal population by capturing and marking a sample of the population, then
re-sampling at a later time [31]. There are a number of signiﬁcant differences between the capture–recapture problem and
estimating Urns parameters, however. First, Urns attempts to learn the parameter |C| from observations which are co-
mingled with samples from a confounding error distribution. Second, Urns must also characterize how the frequencies of
the target set vary (in terms of the Zipﬁan shape parameter zC ). In order to overcome these additional parameter estimation
diﬃculties, Urns exploits problem structures often found in textual domains, such as the fact that extraction frequencies
tend to be Zipf distributed.

3. URNS: Theoretical results

The Urns model was shown in the previous section to be effective in practice for UIE and other applications. In this
section, we analyze the Urns model theoretically. To better understand the behavior of Urns, we would like the be able to
characterize how class probability increases with extraction count. Further, we would like a guarantee on Urns’s accuracy
given suﬃcient unlabeled data. How does accuracy increase with sample size? Can the parameters of the model be learned
from unlabeled data in general?

Speciﬁcally, we investigate the following questions in the context of a single-urn model:

1. In the model, at what rate does the probability that an extracted label is of the target class increase with the number

of extractions k?

2. What are suﬃcient conditions for accurate classiﬁcation, given the parameters of the model? What sample size n is

suﬃcient to achieve a given level of classiﬁcation accuracy?

3. Can the parameters of the model be learned from unlabeled data?
4. Can the Urns model provide accurate classiﬁcations for extractions, i.e. is PAC-learnability guaranteed?

We begin by considering the ﬁrst two questions in the uniform special case previously introduced in Section 2.0.1.
The uniform case, while not fully realistic, does provides qualitatively interpretable results useful for illustration. We then
address all four questions in the more realistic Zipﬁan model used in our experiments.

In the below, for notational convenience we will utilize in place of the multi-set num(C) a multi-set F C containing, for
f = 1.
each element of C , the relative fraction of balls labeled with that element. We deﬁne F E similarly, such that
Then the following expression (adapted from Eq. (1)) speciﬁes the probability that x is an element of C given the observed
values of k and n:

f ∈F C ∪F E

(cid:7)

P (x ∈ C|k, n) =

(cid:7)

(cid:7)

f k(1 − f )n−k

f ∈F C

f ∈F C ∪F E

f k(1 − f )n−k

.

We will also refer to the classiﬁer output by Urns, which is a function from extracted labels to a binary value, indicating

that Urns’s probability is greater than 0.5 (positive) or less than 0.5 (negative).

3.1. Theoretical results: Known parameters

This section presents our theoretical results when the parameters of Urns are known. In the following, we examine Urns

under two sets of assumptions, the Uniform Special Case (USC) and the Zipﬁan Case (ZC), deﬁned below.

Theorems 3 and 5 address question (1) above in each model, describing how class probability increases with the number
of times k a label is extracted. Speciﬁcally, we provide expressions for the increase in the odds ratio odds(k, n) = P (x ∈
C|k, n)/(1 − P (x ∈ C|k, n)) in terms of k. Theorems 4 and 7 address question (2). Let cknown indicate the classiﬁer output by
Urns when the parameters are known; we provide upper bounds on the expected error E[error(cknown)] in terms of the
sample size n and the model parameters.

3.2. Analyzing the uniform special case

The Uniform Special Case (USC) of the Urns model, ﬁrst introduced in Section 2.0.1, is characterized by the following

assumptions:

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

741

USC1 Each target label has the same probability pC of being selected in a single draw, and each error label has a corre-

sponding probability p E .

USC2 Each label from C is repeated on more balls in the urn than is each label from E (that is, pC > p E ).
USC3 Frequency observations k are Poisson distributed (as in Eq. (2)).

3.2.1. Theoretical results in the USC

The following theorem states how the odds ratio odds(k, n) increases with k in the USC.

Theorem 3. In the USC
(cid:3)

odds(k1, n)
odds(k2, n)

=

(cid:4)k1−k2

.

pC
p E

Proof. Follows from the posterior probability in the USC (from Eq. (2)):

P (x ∈ C|k, n) =

1
)ken(pC −p E )

.

(cid:2)

1 + |E|

|C| ( p E
pC

(10)

(11)

Along with assumption USC2, Theorem 3 illustrates that in the USC the odds that an element is a member of the target
class increase exponentially with repetition. The increase is hastened when the target and error classes are less confusable
(i.e. as pC increases relative to p E ).

How accurately we can classify extracted labels, given the parameters of the model and the sample size? Let cknown
indicate the Urns classiﬁer when the parameters are known. The following theorem provides an upper bound on the error
of cknown in the USC in terms of the sample size n, and the separability pC − p E between the C and E sets.

Theorem 4. In the USC, the expected error E[error(cknown)] < (cid:9) when the sample size n satisﬁes:

n (cid:2) 12pC ln 1/(cid:9)
(pC − p E )2

.

(12)

Proof. Deﬁne a model m with a threshold τ = pC +p E
such that Pm(x ∈ C|k, n) (cid:2) 0.5 whenever k (cid:2) nτ , and Pm(x ∈ C|k, n) <
0.5 otherwise. Since we can calculate the optimal threshold when the parameters are known, E[error(cknown)] is no worse
than the expected error made by model m (which utilizes a potentially sub-optimal threshold). We express the expected
error of model m over the full set C ∪ E by summing the expected contribution of each label (equal to the probability that
the label appears a number of times resulting in misclassiﬁcation).
(cid:7)
(cid:7)

(cid:7)

(cid:7)

2

(cid:12)

(cid:13)
error(cknown)

E

=

x∈E

k(cid:2)nτ P (k|x ∈ E, n) +

x∈C

k<nτ P (k|x ∈ C, n)

|C ∪ E|

.

(13)

Employing Chernoff bounds, we can bound the probability that a given label deviates from its expected frequency enough
to be misclassiﬁed. The Chernoff bounds we employ state that for a random variable X =
i Xi equal to the sum of
independent Bernoulli random variables Xi , the probability that X exceeds its expectation μ by more than a factor (1 + δ),
for any δ > 0, is bounded as:

(cid:7)

(cid:5)

(cid:6)
X > (1 + δ)μ

P

< e

−μδ2/3.

Likewise, the probability that X is suﬃciently less than its expectation is bounded as:

(cid:5)

(cid:6)
X < (1 − δ)μ

P

−μδ2/2

< e

(14)

(15)

for any δ > 0.

Let d = pC − p E . Then we have:
(cid:2)

(cid:2)

P (k|x ∈ E, n) =

P (k|x ∈ E, n)

k(cid:2)nτ

k(cid:2)n(p E +d/2)

(cid:5)
k (cid:2) n(p E + d/2)|x ∈ E
(cid:5)
k > n(pC + d/2)|x ∈ E
−nd2/(12pC )

(cid:6)

(cid:6)

= P
(cid:3) P

< e

where the last inequality uses the Chernoff bound in Eq. (14) with μ = npC and δ = d/(2pC ). Similarly, using the bound in
Eq. (15), we have:

742

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

(cid:2)

k<nτ

P (k|x ∈ C, n) =

(cid:2)

P (k|x ∈ C, n)

k<n(pC −d/2)

(cid:5)
k < n(pC − d/2)|x ∈ C
−nd2/(8pC )

(cid:6)

= P

< e

< e

−nd2/(12pC ).

Algebra gives the ﬁnal result. (cid:2)

Theorem 4 yields the following corollary, which states that under the assumptions of the USC, even a weakly indicative
extractor (one for which pC − p E is just slightly greater than zero) can provide an arbitrarily accurate classiﬁer, given
suﬃciently large n. This statement is akin to similar results in boosting algorithms in machine learning [35].

Corollary 1. In the USC, for any (cid:9) > 0, any extractor for which pC − p E > 0 can be used to achieve accuracy of 1 − (cid:9) given suﬃcient
sample size n.

3.3. Analyzing the Zipﬁan single-urn case

The USC is a reasonable approximation for labels on the ﬂat tail of the Zipf curve, but it is clearly an oversimpliﬁcation for
all labels. The following theorems are analogous to those presented for the USC above, but employ the more realistic Zipﬁan
single-urn assumptions. In particular, we assume that the target and error sets are governed by known Zipf distributions,
described below, with sizes |C| and |E| and shape parameters zC and zE . Further, we assume draws are generated from
a mixture of these Zipf distributions, governed by a known mixing parameter p giving the probability that a single draw
comes from C :
(cid:2)

p =

f .

f ∈F C

(16)

As in our experiments, we will ﬁnd it more mathematically convenient to work with a continuous representation of the
commonly discrete Zipﬁan distribution. Integrating over the continuous representation will allow us to arrive at closed-form
expressions for class probability in terms of gamma functions (Theorem 5). In the discrete Zipﬁan case, it is assumed that
the ith most frequent element of C has frequency αC /izC , for αC a normalization constant. In our continuous representation,
the frequency of each element of C is itself a random variable drawn by choosing a uniform x from the range [1, |C| + 1]
and then mapping x to the curve f C (x) = αC /xzC to obtain a frequency. The normalization constant αC is:

αC =

p
(cid:14) |C|+1

1

.

1
xzC dx

(17)

The normalization constant is chosen such that if we draw |C| frequencies for the labels of the C set, the expected sum of
the frequencies is p, as desired. The frequency of each element of E is deﬁned analogously. We will refer to the functions
f C and f E as frequency curves.

As in the USC, for a label in the ZC with underlying frequency f we assume the observed count k is Poisson distributed
with expected value nf . Thus, the likelihood of observing an example of the set S (used to denote either of the C or E sets)
a total of k times in n draws is:

P Z C (k|x ∈ S, n) = 1
|S|

|S|+1(cid:11)

1

(nαS x
enαS x

−zS )k
−zS k!

dx.

(18)

The solution of this equation in terms of incomplete gamma functions is given below in Theorem 5, Eq. (19).

We state the assumptions in the ZC as follows:

ZC1 The distributions of labels from C and E are each Zipﬁan as deﬁned above, with mixing parameter p. That is, the

likelihood of the data is governed by Eq. (18).

ZC2 Conﬁdence increases with repetition; that is, P (x ∈ C|k) increases monotonically with k.
ZC3 The error label frequency curve has positive probability mass below the minimum target label frequency; that is

αE /izE < αC /(|C| + 1)zE for some known i < |E| + 1.

ZC4 Analogously, the target label frequency curve has positive probability mass above the maximum error label frequency;

that is αC /izC > αE for some known i > 1.

ZC5 Both the target and error set have non-zero probability mass in the urn; that is, p, 1 − p > M for some known lower

bound M > 0.

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

743

Assumptions ZC3 and ZC4 encode an assumption that given a suﬃcient number of distinct labels in the urn, with high
probability the most frequent labels will be target labels and the least frequent will be error labels. These assumptions will
allow us to establish PAC learnability from unlabeled data alone.

To lend justiﬁcation to the above assumptions, we note that we would expect them to hold at least approximately in
Unsupervised Information Extraction applications. The Zipﬁan nature of extractions and monotonicity (ZC1 and ZC2) are well
known to hold approximately in practice. Further, assumption ZC3 is certainly empirically true when one considers that, as
a simple example, for any target set element there exist multiple less-frequent misspellings in the error set. Assumption
ZC4 tends to be at least approximately true in practice: the most frequently extracted labels tend to be instances of the
target class. Assumption ZC5 is nearly trivially true in practice, we would always expect the target and error sets to have
probability mass above some non-zero minimum value.

3.3.1. Theoretical results in the ZC

We start by explicitly expressing how the odds that an element is a member of the target class increases with the

number of repetitions:

Theorem 5. In the ZC, the odds ratio

odds(k + 1, n)
odds(k, n)

=

(k − 1/zC )g(k, zC , np, |C| + 1, αC ) + h(k − 1/zC ,
(k − 1/zE )g(k, zE , n(1 − p), |E| + 1, αE ) + h(k − 1/zE ,

np
)
(|C|+1)zC αC
n(1−p)
(|E|+1)zE αE

)

where

(cid:5)
k

(cid:5)

h

, n

(cid:6)

(cid:5)

= n

(cid:5)

(cid:5)k

(cid:5)

en

and

with

P (k|x ∈ C, n) = np
αC

1/zC

g(k, zC , np, |C| + 1, αC )

(cid:5)
k

(cid:5)

g

(cid:5)

(cid:5)

(cid:5)

, α(cid:5)

, s

, n

, z

(cid:6)

(cid:3)

= Γ

(cid:5) − 1/z

(cid:5)

,

k

(cid:5)

n
s(cid:5)z(cid:5)α(cid:5)

(cid:4)

(cid:3)

− Γ

(cid:5) − 1/z

(cid:5)

,

k

(cid:4)

(cid:5)

n
α(cid:5)

assuming that neither zC nor zE are exactly equal to 1.

(19)

Proof. Given that |C|, |E|, k (cid:2) 1, and zC , zE (cid:11)= 1 the above result is obtained by symbolic integration in Mathematica and
algebra.14 (cid:2)

Theorem 5 does not utilize any assumptions other than the Zipﬁan mixture (ZC1). Eq. (19) is the closed-form likelihood
expression used to perform eﬃcient inference in our experiments. Of course, the odds ratio given above is complex. An
illustration of how class probability varies with k is shown in Fig. 5. In order to provide qualitative insights, the odds ratio
should be simpliﬁed into a more interpretable bound; this is an item of future work.

We also wish to bound the classiﬁcation error of Urns for the ZC. The following theorem provides a bound relative to
the error of the optimal classiﬁer, which utilizes both the Urns parameters and the precise frequencies of each label (rather
than simply the observed counts). As such, the optimal classiﬁer exhibits the best classiﬁcation performance that can be
achieved using the extraction count alone.

Deﬁnition 6. The optimal classiﬁer is one which classiﬁes each label optimally given knowledge of both the urn parameters,
as well as the precise frequency in the urn of each label.

Deﬁne τ such that the classiﬁcation threshold of the optimal classiﬁer for a given n is equal to nτ . From assumption ZC2,
we know that a single such τ exists. Then the following theorem illustrates that as the sample size increases, the expected
error falls off nearly linearly toward that of the optimal classiﬁer.

Theorem 7. In the ZC, given any δ > 0, the expected error of urns is bounded as:

(cid:12)

(cid:13)
error(cknown)

E

(cid:3) β + K C (δ) + K E (δ)
(|C| + |E|)n1−δ

where K C (δ) and K E (δ) are constants (with respect to n) deﬁned below, and β is the expected error of the optimal classiﬁer.

14 For reference, the speciﬁc Mathematica commands involved in the proof are available online, see http://www.cs.northwestern.edu/~ddowney/data/
urnsIntegration.html.

744

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

Fig. 5. Probabilities assigned in the Urns model, in the Uniform Special Case (USC), and Zipﬁan Case (ZC) as the Zipﬁan shape parameters vary. For very ﬂat
Zipf curves (zC = 0.45, zE = 0.4), ZC is similar to USC, but ZC differs from USC as the shape parameters increase and diverge from each other. A zE value of
1.1 implies some errors have high extraction frequency, meaning that as k increases, class probability in the ZC converges to one more slowly than in the
USC. In the above, |E| = 20,000, |C| = 500, p = 0.9, and n = 10,000.

The constants K C and K E are deﬁned as follows (for S denoting C or E):

(cid:15)

K S (δ) = max

3/δ, 1 +

(cid:16)

1
(αS x−zS − xτ

S )2

dx

xτ
−1(cid:11)
S

1

(20)

where xτ

S is deﬁned to be the unique value such that f S (xτ

S ) = τ , and αS is the normalization constant (see Eq. (17)).

Proof. Following the proof of Theorem 4, we aggregate the probabilities that the elements are misclassiﬁed.

We present the analysis for expected error on elements of the C set; the E set is analogous. When the parameters are
known, Urns makes errors the optimal classiﬁer does not if and only if the true frequency of a target label x is greater than
the threshold τ , but the observed count is less than nτ . We bound the probability that an element with true frequency
−zC > τ appears fewer than nτ times in n draws using Chebyshev’s inequality. Chebyshev’s inequality bounds the
of αC x
probability that a random variable Y with expectation μ and variance σ 2 appears suﬃciently far from its expectation:

(cid:5)

P

|Y − μ| > rσ

(cid:6)

(cid:3) 1
r2

.

For a Poisson random variable with expected value p

pression also bounds the probability that the deviation exceeds r
in misclassiﬁcation (nαC x
expected error on the C set:

−zC − nxτ

n, so the above ex-
n equal to the smallest deviation resulting
C ), and integrating over the frequency curve f C , we have the following bound for the

n. Setting r

n for 0 < p

(cid:5) < 1, σ is bounded above by

√

√

(cid:5)

√

(cid:12)

(cid:13)
errorC (cknown)

E

xτ
C(cid:11)

(cid:3)

(cid:3) βC +

min

1,

1

1
n(αC x−zC − xτ

C )2

(cid:4)

dx

(21)

where βC is the fraction of the expected error of the optimal classiﬁer due to elements of C (namely, the probability mass
of elements of C with frequency less than τ ).

Deﬁne:

γn = 1
n

+

xτ
−1/n(cid:11)
C

1

1
n(αC x−zC − xτ

C )2

xτ
C(cid:11)

(cid:3)

dx (cid:2)

min

1,

1

1
n(αC x−zC − xτ

C )2

(cid:4)

dx.

We claim γn (cid:3) K C (δ)/n1−δ , given which the theorem follows. The proof of the claim proceeds by induction. First, note
that the n = 1 case, that γ1 (cid:3) K C (δ), holds by construction of K C (δ)—the second term in the max function in Eq. (20) is
equal to γ1. Then assuming γn (cid:3) K C (δ)/n1−δ , consider the n + 1 case:

γn+1 = nγn
n + 1

+

xτ
C

−1/(n+1)

(cid:11)

xτ
C

−1/n

1
n(αC x−zC − xτ

C )2

dx (cid:3) K C (δ)nδ
n + 1

+ 1
n2

= K C (δ)

(n + 1)1−δ

(cid:3)

nδ
(n + 1)δ

+ (n + 1)1−δ
K C (δ)n2

(cid:4)

.

It remains to show that:

(cid:3)

nδ
(n + 1)δ

+ (n + 1)1−δ
K C (δ)n2

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

(cid:4)

(cid:3) 1.

745

(22)

With algebra, this is equivalent to the statement that K C (δ)n2((n + 1)δ − nδ) (cid:2) n + 1. From the generalized binomial

theorem, (n + 1)δ is at least as large as nδ + δnδ−1 − δ(1 − δ)nδ−2/2. With algebra, we have:

(cid:5)

K C (δ)n2

(n + 1)δ − nδ

(cid:6)

(cid:2) K C (δ)δn1+δ

(cid:2) 3n1+δ

2

2

(cid:2) n + 1

as desired, using the fact that K C (δ) (cid:2) 3/δ. (cid:2)

3.4. Theoretical results with unknown parameters

In unsupervised classiﬁcation, in general we are not given the Urns parameters in advance, and must learn these from
unlabeled data. In this section, we provide theorems bounding the error in unsupervised classiﬁcation even when the
parameter values are unknown. The following theorem shows that with high probability the parameter values of Urns can
be estimated accurately from unlabeled data alone, as the total number of distinct labels in the urn u = |C| + |E| increases,
with n ﬁxed.

Theorem 8. In the ZC, for any δ, (cid:9) > 0, given suﬃciently large u = |C| + |E| for ﬁxed n, we can obtain an estimate of the parameters
of f C and f E such that with probability 1 − δ each estimate lies within (cid:9) of the true parameter value.

Proof. The frequency curves f C and f E can be converted into functions gC (λ) and g E (λ) giving the probability density of a
particular frequency λ for labels in the C (resp. E) set. These functions are themselves power law distributions. For example,
in the error set case:

(cid:17)

g E (λ) =

L E
λ(1+zE )/zE
0

for aE (cid:3) x (cid:3) b E ,
for x < aE or x > b E ,

(23)

for a suitable constant L E where zE indicates the exponent from the original frequency curve. The distribution of error
labels in the model is completely characterized by four parameters: L E and zE , the minimal frequency aE , and the maximal
frequency b E .

The probability that a particular label appears k times in n extractions can then be written as follows:

P (k|n) =

n(cid:11)

(cid:5)

0

gC (λ) + g E (λ)

(cid:6) e

−λλk
k!

dλ.

(24)

Let g(x) = gC (x)+ g E (x). When written in the form of Eq. (24), the distribution over k becomes an instance of a compound
Poisson process, for which the existence of effective estimators of g(x) is well-known. In particular, Theorem 1 from [26]
states that for any x < n we can obtain a sequence of estimates ˆgu(x) of g(x) such that E[ ˆgu(x) − g(x)]2 = o(1) as u → ∞.
Thus, for any given δ(cid:5), (cid:9)(cid:5) > 0, we have with probability 1 − δ(cid:5)
for u suﬃciently large. It remains to
convert this estimator of g(x) into estimators of each of the Urns parameters. In the re-written model (Eq. (23)) we will
employ, there are eight total parameters characterizing the mixture components gC and g E . We present the construction
for the four parameters of g E , the gC case is analogous.

that | ˆgu(x) − g(x)| < (cid:9)(cid:5)

Consider two estimates ˆgu(x0) and ˆgu(rx0) where x0, rx0 < αC /(|C| + 1). That is, x0 and rx0 are suﬃciently small that
gC (x0) and gC (rx0) are zero by assumption ZC3. By algebra, in this region (1 + zE )/zE = (ln g(x0) − ln g(rx0))/(ln r), so zE is
a continuous and bounded function of g(x0) and g(rx0) on the domain of interest. This implies we can estimate zE within
(cid:9) with probability 1 − δ given our estimator ˆgu , for u suitably large. Likewise, L E is a continuous and bounded function of
g(x) and zE , so we can estimate L E effectively.

(cid:5)/2. Thus, the minimal xi such that ˆgu(x) > M

It remains to obtain an estimator for the limits of support aE and b E . We begin with the minimal limit aE . We construct
from 0 to n a uniform lattice of estimates { ˆgu(xi)} each (cid:9) apart. By assumption ZC5, g E (x) > M
for x ∈ [aE , aE + (cid:9)) for a
(cid:5)
given that (cid:9) is suﬃciently small. By taking u suitably large, we can ensure with probability 1 − δ that
known constant M
(cid:5)/2 and that the x j (cid:2) aE falling in the interval [aE , aE + (cid:9)) has estimate ˆgu(x j) >
∀xi < aE , | ˆgu(xi) − g(xi)| = | ˆgu(xi)| < M
(cid:5)/2 is with probability 1 − δ an estimate within (cid:9) of aE . Estimating the
M
maximal limit of support b E is similar. The same procedure is employed, except that because gC (b E ) is non-zero, we
instead identify successive estimates ˆgu(xk) and ˆgu(xk+1) that differ by a suﬃciently large margin, where xk is greater than
our estimate for aE . By taking u suﬃciently large and (cid:9) suﬃciently small, with probability 1 − δ the value xk is within (cid:9)
of b E . (cid:2)

(cid:5)

746

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

3.4.1. PAC learnability under Urns

In this section, we show that a suﬃciently informative extractor that follows the Urns model can be used to PAC learn
from only unlabeled data. Here, we assume we have additional features for each label beyond just the extraction counts (for
example, other features could include the co-occurrence counts of each label with textual contexts other than the extractors,
as in [15]).

Our result is expressed in terms of a given, ﬁxed concept class of binary classiﬁers mapping the input features to {0, 1},

denoted as C—as is typical in the PAC-learning setting, we assume our target function (having zero error) is in C.

Our result requires that a “separability” criterion holds on the concept class C. This criterion states that no two distinct

concepts in C agree on too large a fraction of the instance space:

Deﬁnition 9. A concept class C(cid:5)
that c(x) = c

(cid:5)(x) is less than 1 − (cid:9).

is (cid:9)-separable if for any distinct concepts c, c

(cid:5) ∈ C(cid:5)

, the fraction of examples x ∈ X such

We also require an extractor that is suﬃciently informative. We state this criteria in terms of the minimal expected
classiﬁcation error that can be achieved using the extraction counts, in the limit of u and n large. This is equivalent to the
area of the “confusion region” in Fig. 1, which we deﬁne formally as:

Deﬁnition 10. The area of the confusion region of an extractor is:

(cid:18) τ(cid:11)

min
τ

0

∞(cid:11)

(cid:19)

gC (λ) dλ +

g E (λ) dλ

.

τ

(25)

Given this deﬁnition, we can state the following result, which shows that Urns is able to PAC learn from unlabeled data

alone.

Proposition 11. If C is (cid:9)-separable, given an extractor that follows the ZC with confusion region of area less than 1 − (cid:9)/2, C is
PAC-learnable from unlabeled data alone.

Proof. By Theorem 8, with high probability we can obtain the parameters of Urns within an error of (cid:9)(cid:5)
, for any (cid:9)(cid:5) > 0.
Because the optimal classiﬁcation threshold τ is a continuous and bounded function of the Urns parameters (see Eq. (25)),
Urns can achieve accuracy arbitrarily close to the confusion region size. Thus, the error of Urns is less than 1 − (cid:9)/2, given n
and u suﬃciently large, meaning it assigns classiﬁcations different from those of the target classiﬁer on fewer than 1 − (cid:9)/2
of the examples. By the separability criterion, the target concept is the only hypothesis that differs from the output of Urns
on so few examples. Thus, an algorithm that returns the concept c ∈ C most similar to the output of Urns will always return
the target concept. (cid:2)

3.5. Related work

Joachims provides theoretical results in supervised textual classiﬁcation that use the Zipﬁan structure of text to arrive
at error bounds for Support Vector Machine classiﬁers on textual data [23]. The strong performance of SVMs in our super-
vised experiments corroborate Joachims’s claim that these classiﬁers are effective on textual data. However, in contrast to
Joachims’s work, our theoretical results (and experiments) are focused on the unsupervised case. We show that when the
Zipﬁan structure holds, unsupervised learning is possible under certain assumptions.

Our result showing that PAC Learnability is guaranteed in the Urns model (Proposition 11) extends a previous result
showing that a single “monotonic feature” is suﬃcient to PAC-learn under certain assumptions (a monotonic feature is
one, like the extraction counts we consider, whose value increases monotonically with class probability) [13]. The primary
advantage of our result is that it does not require that the extraction counts be conditionally independent of the other
features given the class, a strong assumption which is shown to be problematic in practice in [12]. Our result avoids this
assumption by exploiting problem structure inherent in extraction, as expressed by the Urns model.

4. Future work

The techniques described in this paper leave open many potential areas of future work. One important direction is
developing a probabilistic model for multiple extractors that is more ﬂexible than multiple urns. The correlation model
used for multiple urns is limited and can only handle a small, pre-deﬁned set of distinct mechanisms. Language modeling
techniques for UIE from recent work leverage all contextual information when assessing extractions, rather than relying on
a select set of extraction patterns [15,2]. However, currently these techniques only rank extracted labels, and do not output
probabilities or classiﬁcations. A model that produces probabilities of correctness without labeled data, like Urns, yet also
leverages all available contextual information is an important target of future work.

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

747

When utilizing Urns for UIE in practice, the EM-based algorithm we employ to learn Urns parameters from unlabeled
data could be improved in a number of ways. The algorithm often requires a sample size of hundreds or thousands of
unlabeled observations of each class in order to be effective (as illustrated in Fig. 3). For classes where data is less plentiful,
such as many of the relations extracted by the TextRunner system, the parameter learning algorithm is less effective. We
expect that Urns could be modiﬁed to learn accurate parameters for much smaller data sets, through the use of priors or
more robust likelihood-maximization techniques.

Urns also requires that a reasonable estimate of the precision of the extraction process be known. We demonstrated
that this requirement is not prohibitive when extracting instances of classes drawn from WordNet, using generic extraction
patterns; the extraction frequency can be assumed or adjusted from unlabeled text in such a way that the probabilities pro-
duced by Urns still offer large improvements over previous techniques. However, for “Open IE” systems such as TextRunner
which discover target relations from text, the situation is more complex [3]. In TextRunner, extraction precision can vary
greatly across the discovered relations; thus, the probabilities output by Urns in this case are less accurate. Automatically
estimating extraction precision across relations in Open IE systems is an area of future work.

5. Conclusions

This paper described methods for identifying correct extractions in UIE, without the use of hand-labeled training data.
The Urns model estimates the probability that an extraction is correct, based on sample size, redundancy, and corroboration
from multiple distinct extraction rules. We described supervised and unsupervised methods for estimating the parameters
of the model from data, and reported on experiments showing that Urns massively outperforms previous methods in the
unsupervised case, and is slightly better than baseline methods in the supervised case. We also detailed several other
applications in which the general Urns model of redundancy has been effective. Our theoretical results show how the
accuracy of Urns improves with sample size, that the parameters of Urns can be estimated without hand-labeled data, and
that Urns guarantees PAC-learnability from unlabeled data alone, given certain conditions.

Acknowledgements

This research was supported in part by NSF grants IIS-0535284 and IIS-0312988, DARPA contract NBCHD030010, ONR
grants N00014-02-1-0324 and N00014-08-1-0431, and gifts from Google, and carried out at the University of Washington’s
Turing Center. The ﬁrst author was supported by a Microsoft Research Graduate Fellowship sponsored by Microsoft Live
Labs. Google generously allowed us to issue a large number of queries to their XML API to facilitate our experiments. We
thank Pedro Domingos, Anna Karlin, Marina Meila, and Dan Weld for helpful discussions, and Jeff Bigham for comments on
previous drafts. Also, thanks to Alex Yates for suggesting we consider this problem.

References

[1] E. Agichtein, L. Gravano, Snowball: Extracting relations from large plain-text collections, in: Proc. of the Fifth ACM International Conference on Digital

Libraries, 2000.

[2] A. Ahuja, D. Downey, Improved extraction assessment through better language models, in: Human Language Technologies: Annual Conference of the

North American Chapter of the Association for Computational Linguistics (NAACL HLT), 2010.

[3] M. Banko, M. Cafarella, S. Soderland, M. Broadhead, O. Etzioni, Open information extraction from the Web, in: Proc. of IJCAI, 2007.
[4] M. Banko, O. Etzioni, The tradeoffs between traditional and open relation extraction, in: Proceedings of ACL, 2008.
[5] A. Blum, T. Mitchell, Combining labeled and unlabeled data with co-training, in: COLT: Proceedings of the Workshop on Computational Learning Theory,

Morgan Kaufmann Publishers, 1998, pp. 92–100.

[6] M. Cafarella, D. Downey, S. Soderland, O. Etzioni, Knowitnow: Fast, scalable information extraction from the Web, in: Proc. of EMNLP, 2005.
[7] M. Califf, R. Mooney, Relational learning of pattern-match rules for information extraction, in: Working Notes of AAAI Spring Symposium on Applying

Machine Learning to Discourse Processing, AAAI Press, Menlo Park, CA, 1998, pp. 6–11.

[8] C. Chang, C. Lin, LIBSVM: a library for support vector machines, 2001.
[9] F. Ciravegna, Adaptive information extraction from text by rule induction and generalisation, in: Proc. of the 17th International Joint Conference on

Artiﬁcial Intelligence (IJCAI 2001), Seattle, Washington, 2001, pp. 1251–1256.

[10] A. Culotta, A. McCallum, Conﬁdence estimation for information extraction, in: HLT-NAACL, 2004.
[11] M.-C. de Marneffe, A. Rafferty, C.D. Manning, Finding contradictions in text, in: ACL 2008, 2008.
[12] D. Downey, Redundancy in Web-scale information extraction: probabilistic model and experimental results, PhD thesis, University of Washington, 2008.
[13] D. Downey, O. Etzioni, Look ma, no hands: Analyzing the monotonic feature abstraction for text classiﬁcation, in: Advances in Neural Information

Processing Systems (NIPS) 21, 2008, January 2009.

[14] D. Downey, O. Etzioni, S. Soderland, A probabilistic model of redundancy in information extraction, in: Proc. of IJCAI, 2005.
[15] D. Downey, S. Schoenmackers, O. Etzioni, Sparse information extraction: Unsupervised language models to the rescue, in: Proc. of ACL, 2007.
[16] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, T. Shaked, S. Soderland, D. Weld, A. Yates, Web-scale information extraction in KnowItAll, in:

WWW, New York City, New York, 2004, pp. 100–110.

[17] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, T. Shaked, S. Soderland, D. Weld, A. Yates, Unsupervised named-entity extraction from the Web:

An experimental study, Artiﬁcial Intelligence 165 (1) (2005) 91–134.

[18] O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, A. Yates, Methods for domain-independent information extraction
from the Web: An experimental comparison, in: Proc. of the 19th National Conference on Artiﬁcial Intelligence (AAAI-04), San Jose, California, 2004,
pp. 391–398.

[19] D. Freitag, A. McCallum, Information extraction with HMMs and shrinkage, in: Proceedings of the AAAI-99 Workshop on Machine Learning for Infor-

mation Extraction, Orlando, Florida, 1999.

[20] W.A. Gale, G. Sampson, Good–Turing frequency estimation without tears, J. Quantitative Linguistics 2 (3) (1995) 217–237.

748

D. Downey et al. / Artiﬁcial Intelligence 174 (2010) 726–748

[21] T. Grenager, C.D. Manning, Unsupervised discovery of a statistical verb lexicon, in: Conference on Empirical Methods in Natural Language Processing.
[22] M. Hearst, Automatic acquisition of hyponyms from large text corpora, in: Proc. of the 14th International Conference on Computational Linguistics,

Nantes, France, 1992, pp. 539–545.

[23] T. Joachims, Learning to Classify Text Using Support Vector Machines: Methods, Theory and Algorithms, Kluwer Academic Publishers, Norwell, MA,

USA, 2002.

[24] M. Liakata, S. Pulman, From trees to predicate-argument structures, in: Proceedings of the 19th International Conference on Computational Linguistics,

Association for Computational Linguistics, Morristown, NJ, USA, 2002, pp. 1–7.

[25] W. Lin, R. Yangarber, R. Grishman, Bootstrapped learning of semantic classes from positive and negative examples, in: Proc. of ICML-2003 Workshop

on The Continuum from Labeled to Unlabeled Data, Washington, DC, 2003, pp. 103–111.

[26] W.-L. Loh, Estimating the mixing density of a mixture of power series distributions, in: S.S. Gupta, J.O. Berger (Eds.), Statist. Decision Theory and

Related Topics V, Springer, New York, 1993, pp. 87–98.

[27] A. McCallum, Eﬃciently inducing features of conditional random fields, in: Proceedings of the Nineteenth Conference on Uncertainty in Artiﬁcial

Intelligence, Acapulco, Mexico, 2003, pp. 403–410.

[28] B. Milch, B. Marthi, S. Russell, D. Sontag, D.L. Ong, A. Kolobov, Blog: Probabilistic models with unknown objects, in: L.D. Raedt, T. Dietterich, L. Getoor,
S.H. Muggleton (Eds.), Probabilistic, Logical and Relational Learning – Towards a Synthesis, in: Dagstuhl Seminar Proceedings, vol. 05051, Internationales
Begegnungs- und Forschungszentrum für Informatik (IBFI), Schloss Dagstuhl, Germany, 2006, http://drops.dagstuhl.de/opus/volltexte/2006/416 [date of
citation: 2006-01-01].

[29] K. Nigam, J. Lafferty, A. McCallum, Using maximum entropy for text classiﬁcation, in: Proc. of IJCAI-99 Workshop on Machine Learning for Information

Filtering, Stockholm, Sweden, 1999, pp. 61–67.

[30] M. Pasca, D. Lin, J. Bigham, A. Lifchits, A. Jain, Organizing and searching the world wide web of facts – step one: The one-million fact extraction

challenge, in: AAAI 2006, AAAI Press, 2006.

[31] K.H. Pollock, J.D. Nichols, C. Brownie, J.E. Hines, Statistical Inference for Capture–Recapture Experiments, Wildlife Society Monogr., vol. 107, 1990.
[32] S.D. Richardson, W.B. Dolan, L. Vanderwende, Mindnet: acquiring and structuring semantic information from text, in: Proceedings of the 17th Interna-

tional Conference on Computational Linguistics, Association for Computational Linguistics, Morristown, NJ, USA, 1998, pp. 1098–1102.

[33] E. Riloff, R. Jones, Learning dictionaries for information extraction by multi-level bootstrapping, in: AAAI/IAAI, 1999.
[34] A. Ritter, S. Soderland, D. Downey, O. Etzioni, It’s a contradiction – no, it’s not: A case study using functional relations, in: EMNLP, 2008.
[35] R.E. Schapire, The strength of weak learnability, Mach. Learn. 5 (2) (1990) 197–227.
[36] L. Schubert, Can we derive general world knowledge from texts?, in: Proceedings of the Second International Conference on Human Language Tech-

nology Research, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2002, pp. 94–97.

[37] M. Skounakis, M. Craven, Evidence combination in biomedical natural-language processing, in: BIOKDD, 2003.
[38] S. Soderland, Learning information extraction rules for semi-structured and free text, Mach. Learn. 34 (1–3) (1999) 233–272.
[39] S. Soderland, O. Etzioni, T. Shaked, D. Weld, The use of Web-based statistics to validate information extraction, in: AAAI-04 Workshop on Adaptive Text

Extraction and Mining, 2004, pp. 21–26.

[40] R. Storn, K. Price, Differential evolution – a simple and eﬃcient heuristic for global optimization over continuous spaces, J. Global Optim. 11 (4) (1997)

341–359.

[41] R.S. Swier, S. Stevenson, Unsupervised semantic role labelling, in: Proceedings on EMNLP, 2004, pp. 95–102.
[42] A. Yates, O. Etzioni, Unsupervised resolution of objects and relations on the Web, in: Proc. of HLT, 2007.

