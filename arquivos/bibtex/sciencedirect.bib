@article{CHU20121,
title = {A complete solution to the Maximum Density Still Life Problem},
journal = {Artificial Intelligence},
volume = {184-185},
pages = {1-16},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000124},
author = {Geoffrey Chu and Peter J. Stuckey},
keywords = {Combinatorial optimization, Search, Constraint programming, Dynamic programming},
abstract = {The Maximum Density Still Life Problem (CSPLib prob032) is to find the maximum number of live cells that can fit in an nÃ—n region of an infinite board, so that the board is stable under the rules of ConwayÊ¼s Game of Life. It is considered a very difficult problem and has a raw search space of O(2n2). Previous state of the art methods could only solve up to n=20. We give a powerful reformulation of the problem into one of minimizing â€œwastageâ€ instead of maximizing the number of live cells. This reformulation allows us to compute very strong upper bounds on the number of live cells, which dramatically reduces the search space. It also gives us significant insights into the nature of the problem. By combining these insights with several powerful techniques: remodeling, lazy clause generation, bounded dynamic programming, relaxations, and custom search, we are able to solve the Maximum Density Still Life Problem for all n. This is possible because the Maximum Density Still Life Problem is in fact well behaved mathematically for sufficiently large n (around n>200) and if such very large instances can be solved, then there exist ways to construct provably optimal solutions for all n from a finite set of base solutions. Thus we show that the Maximum Density Still Life Problem has a closed form solution and does not require exponential time to solve.}
}
@article{CHEN2023103951,
title = {Spectral complexity-scaled generalisation bound of complex-valued neural networks},
journal = {Artificial Intelligence},
volume = {322},
pages = {103951},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103951},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000978},
author = {Haowen Chen and Fengxiang He and Shiye Lei and Dacheng Tao},
keywords = {Complex-valued neural networks, Generalisation, Spectral complexity},
abstract = {Complex-valued neural networks (CVNNs) have been widely applied in various fields, primarily in signal processing and image recognition. Few studies have focused on the generalisation of CVNNs, although it is vital to ensure the performance of CVNNs on unseen data. This study is the first to prove a generalisation bound for complex-valued neural networks. The bounds increase as the spectral complexity increases, with the dominant factor being the product of the spectral norms of the weight matrices. Furthermore, this work provides a generalisation bound for CVNNs trained on sequential data, which is also affected by the spectral complexity. Theoretically, these bounds are derived using the Maurey Sparsification Lemma and Dudley entropy integral. We conducted empirical experiments on various datasets including MNIST, ashionMNIST, CIFAR-10, CIFAR-100, Tiny ImageNet, and IMDB by training complex-valued convolutional neural networks. The Spearman rank-order correlation coefficient and the corresponding p-values on these datasets provide strong proof of the statistically significant correlation between the spectral complexity of a network and its generalisation ability, as measured by the spectral norm product of the weight matrices. The code is available at https://github.com/LeavesLei/cvnn_generalization.}
}
@article{OHSAWA1997131,
title = {Networked bubble propagation: a polynomial-time hypothetical reasoning method for computing near-optimal solutions},
journal = {Artificial Intelligence},
volume = {91},
number = {1},
pages = {131-154},
year = {1997},
note = {Artificial Intelligence Research in Japan},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00061-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000616},
author = {Yukio Ohsawa and Mitsuru Ishizuka},
keywords = {Hypothetical reasoning, Knowledge network, Approximate solution method, Polynomial-time inference},
abstract = {Hypothetical reasoning (abduction) is an important knowledge processing framework because of its theoretical basis and its usefulness for solving practical problems including diagnosis, design, etc. In many cases, the most probable hypotheses set for diagnosis or the least expensive one for design is desirable. Cost-based abduction, where a numerical weight is assigned to each hypothesis and an optimal solution hypotheses set with minimal sum of element hypotheses' weights is searched, deals with such problems. However, slow inference speed is its crucial problem: cost-based abduction is NP-complete. In order to achieve a tractable inference of cost-based abduction, we aim at obtaining a nearly, rather than exactly, optimal solution. For this approach, an approximate solution method exploited in mathematical programming is quite beneficial. On the other hand, from the standpoint of knowledge processing, it is also important to realize inference on a network which reflects knowledge structure. Knowledge structure is a fruitful information for an efficient inference. In this paper, we propose an inference method which works on a knowledge network, based on a mechanism similar to the pivot and complement method, an efficient approximate 0â€“1 integer programming method to find a near-optimal solution within a polynomial time of O(N4), where N is the number of variables or hypotheses. We reformalize this method by a new type of network on which inference is executed by propagating bubbles. This method achieves an inference time of O(N2) by executing each bubble propagation within a small sub-network, i.e., by taking advantage of the knowledge structure.}
}
@article{MCCARTHY20071174,
title = {From here to human-level AI},
journal = {Artificial Intelligence},
volume = {171},
number = {18},
pages = {1174-1182},
year = {2007},
note = {Special Review Issue},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001476},
author = {John McCarthy},
keywords = {Human-level AI, Elaboration tolerance},
abstract = {Human-level AI will be achieved, but new ideas are almost certainly needed, so a date cannot be reliably predictedâ€”maybe five years, maybe five hundred years. I'd be inclined to bet on this 21st century. It is not surprising that human-level AI has proved difficult and progress has been slowâ€”though there has been important progress. The slowness and the demand to exploit what has been discovered has led many to mistakenly redefine AI, sometimes in ways that preclude human-level AIâ€”by relegating to humans parts of the task that human-level computer programs would have to do. In the terminology of this paper, it amounts to settling for a bounded informatic situation instead of the more general common sense informatic situation. Overcoming the â€œbrittlenessâ€ of present AI systems and reaching human-level AI requires programs that deal with the common sense informatic situationâ€”in which the phenomena to be taken into account in achieving a goal are not fixed in advance. We discuss reaching human-level AI, emphasizing logical AI and especially emphasizing representation problems of information and of reasoning. Ideas for reasoning in the common sense informatic situation include nonmonotonic reasoning, approximate concepts, formalized contexts and introspection.}
}
@article{NAITABDALLAH201325,
title = {Constraint propagation as information maximization},
journal = {Artificial Intelligence},
volume = {197},
pages = {25-38},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000210},
author = {A. {Nait Abdallah} and M.H. {van Emden}},
keywords = {Constraint-satisfaction problems, Information partial order, Intervals, Propagation},
abstract = {This paper draws on diverse areas of computer science to develop a unified view of computation:â€¢Optimization in operations research, where a numerical objective function is maximized under constraints, is generalized from the numerical total order to a non-numerical partial order that can be interpreted in terms of information.â€¢Relations are generalized so that there are relations of which the constituent tuples have numerical indexes, whereas in other relations these indexes are variables. The distinction is essential in our definition of constraint-satisfaction problems.â€¢Constraint-satisfaction problems are formulated in terms of semantics of conjunctions of atomic formulas of predicate logic.â€¢Approximation structures, which are available for several important domains, are applied to solutions of constraint-satisfaction problems. As application we treat constraint-satisfaction problems over reals. These cover a large part of numerical analysis, most significantly nonlinear equations and inequalities. The chaotic algorithm analyzed in the paper combines the efficiency of floating-point computation with the correctness guarantees of arising from our logico-mathematical model of constraint-satisfaction problems.}
}
@article{EITER199957,
title = {Computing intersections of Horn theories for reasoning with models},
journal = {Artificial Intelligence},
volume = {110},
number = {1},
pages = {57-101},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00021-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000211},
author = {Thomas Eiter and Toshihide Ibaraki and Kazuhisa Makino},
keywords = {Automated reasoning, Model-based reasoning, Characteristic models, Algorithms and complexity, Knowledge representation},
abstract = {Model-based reasoning has been proposed as an alternative form of representing and accessing logical knowledge bases. In this approach, a knowledge base is represented by a set of characteristic models. In this paper, we consider computational issues when combining logical knowledge bases, which are represented by their characteristic models; in particular, we study taking their logical intersection. We present low-order polynomial time algorithms or prove intractability for the major computation problems in the context of knowledge bases which are Horn theories. In particular, we show that a model of the intersection Î£ of Horn theories Î£1,â€¦,Î£l, represented by their characteristic models, can be found in linear time, and that some characteristic model of Î£ can be found in polynomial time. Moreover, we present an algorithm which enumerates all the models of Î£ with polynomial delay. The analogous problem for the characteristic models is proved to be intractable, even if the possible exponential size of the output is taken into account. Furthermore, we show that approximate computation of the set of characteristic models is difficult as well. Nonetheless, we show that deduction from Î£ is possible for a large class of queries in polynomial time, while abduction turns out to be intractable. We also consider a generalization of Horn theories, and prove negative results for the basic questions, indicating that an extension of the positive results beyond Horn theories is not immediate.}
}
@article{KORF2001199,
title = {Time complexity of iterative-deepening-Aâˆ—},
journal = {Artificial Intelligence},
volume = {129},
number = {1},
pages = {199-218},
year = {2001},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(01)00094-7},
url = {https://www.sciencedirect.com/science/article/pii/S0004370201000947},
author = {Richard E. Korf and Michael Reid and Stefan Edelkamp},
keywords = {Problem solving, Heuristic search, Iterative-deepening-A, Time complexity, Branching factor, Heuristic branching factor, Sliding-tile puzzles, Eight Puzzle, Fifteen Puzzle, Rubik's Cube},
abstract = {We analyze the time complexity of iterative-deepening-Aâˆ— (IDAâˆ—). We first show how to calculate the exact number of nodes at a given depth of a regular search tree, and the asymptotic brute-force branching factor. We then use this result to analyze IDAâˆ— with a consistent, admissible heuristic function. Previous analyses relied on an abstract analytic model, and characterized the heuristic function in terms of its accuracy, but do not apply to concrete problems. In contrast, our analysis allows us to accurately predict the performance of IDAâˆ— on actual problems such as the sliding-tile puzzles and Rubik's Cube. The heuristic function is characterized by the distribution of heuristic values over the problem space. Contrary to conventional wisdom, our analysis shows that the asymptotic heuristic branching factor is the same as the brute-force branching factor. Thus, the effect of a heuristic function is to reduce the effective depth of search by a constant, relative to a brute-force search, rather than reducing the effective branching factor.}
}
@article{RAMONI2001209,
title = {Robust Bayes classifiers},
journal = {Artificial Intelligence},
volume = {125},
number = {1},
pages = {209-226},
year = {2001},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(00)00085-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370200000850},
author = {Marco Ramoni and Paola Sebastiani},
keywords = {Bayes classifier, Missing data, Probability intervals},
abstract = {Naive Bayes classifiers provide an efficient and scalable approach to supervised classification problems. When some entries in the training set are missing, methods exist to learn these classifiers under some assumptions about the pattern of missing data. Unfortunately, reliable information about the pattern of missing data may be not readily available and recent experimental results show that the enforcement of an incorrect assumption about the pattern of missing data produces a dramatic decrease in accuracy of the classifier. This paper introduces a Robust Bayes Classifier (rbc) able to handle incomplete databases with no assumption about the pattern of missing data. In order to avoid assumptions, the rbc bounds all the possible probability estimates within intervals using a specialized estimation method. These intervals are then used to classify new cases by computing intervals on the posterior probability distributions over the classes given a new case and by ranking the intervals according to some criteria. We provide two scoring methods to rank intervals and a decision theoretic approach to trade off the risk of an erroneous classification and the choice of not classifying unequivocally a case. This decision theoretic approach can also be used to assess the opportunity of adopting assumptions about the pattern of missing data. The proposed approach is evaluated on twenty publicly available databases.}
}
@article{BLOCH2003141,
title = {Representation and fusion of heterogeneous fuzzy information in the 3D space for model-based structural recognitionâ€”Application to 3D brain imaging},
journal = {Artificial Intelligence},
volume = {148},
number = {1},
pages = {141-175},
year = {2003},
note = {Fuzzy Set and Possibility Theory-Based Methods in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(03)00018-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370203000183},
author = {Isabelle Bloch and Thierry GÃ©raud and Henri MaÃ®tre},
keywords = {Heterogeneous knowledge representation, Fuzzy mathematical morphology, Fuzzy classification, Fuzzy spatial relationships, Fuzzy fusion, Fuzzy pattern recognition, Model-based structural recognition, Brain imaging},
abstract = {We present a novel approach to model-based pattern recognition where structural information and spatial relationships have a most important role. It is illustrated in the domain of 3D brain structure recognition using an anatomical atlas. Our approach performs segmentation and recognition of the scene simultaneously. The solution of the recognition task is progressive, processing successively different objects, and using different pieces of knowledge about the object and about relationships between objects. Therefore, the core of the approach is the knowledge representation part, and constitutes the main contribution of this paper. We make use of a spatial representation of each piece of information, as a spatial fuzzy set representing a constraint to be satisfied by the searched object, thanks in particular to fuzzy mathematical morphology operations. Fusion of these constraints allows us to select, segment and recognize the desired object.}
}
@article{LAWRY20041,
title = {A framework for linguistic modelling},
journal = {Artificial Intelligence},
volume = {155},
number = {1},
pages = {1-39},
year = {2004},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2003.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370203001899},
author = {Jonathan Lawry},
keywords = {Random sets, Linguistic constraints, Fuzzy labels, Label semantics, Bayesian inference},
abstract = {A new framework for linguistic reasoning is proposed based on a random set model of the degree of appropriateness of a label. Labels are assumed to be chosen from a finite predefined set of labels and the set of appropriate labels for a value is defined as a random set-valued function from a population of individuals into the set of subsets of labels. Appropriateness degrees are then evaluated relative to the distribution on this random set where the appropriateness degree of a label corresponds to the probability that it is contained in the set of appropriate labels. This interpretation is referred to as label semantics. A natural calculus for appropriateness degrees is described which is weakly functional while taking into account the logical structure of expressions. Given this framework it is shown that a bayesian approach can be adopted in order to infer probability distributions on the underlying variable given constraints both in the form of linguistic expressions and mass assignments. In addition, two conditional measures are introduced for evaluating the appropriateness of a linguistic expression given other linguistic information.}
}
@article{GOLDSMITH2004139,
title = {Theory revision with queries: Horn, read-once, andÂ parity formulas},
journal = {Artificial Intelligence},
volume = {156},
number = {2},
pages = {139-176},
year = {2004},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020400030X},
author = {Judy Goldsmith and Robert H. Sloan and BalÃ¡zs SzÃ¶rÃ©nyi and GyÃ¶rgy TurÃ¡n},
keywords = {Theory revision, Knowledge revision, Horn formulas, Query learning, Computational learning theory, Boolean function learning},
abstract = {A theory, in this context, is a Boolean formula; it is used to classify instances, or truth assignments. Theories can model real-world phenomena, and can do so more or less correctly. The theory revision, or concept revision, problem is to correct a given, roughly correct concept. This problem is considered here in the model of learning with equivalence and membership queries. AÂ revision algorithm is considered efficient if the number of queries it makes is polynomial in the revision distance between the initial theory and the target theory, and polylogarithmic in the number of variables and the size of the initial theory. The revision distance is the minimal number of syntactic revision operations, such as the deletion or addition of literals, needed to obtain the target theory from the initial theory. Efficient revision algorithms are given for Horn formulas and read-once formulas, where revision operators are restricted to deletions of variables or clauses, and for parity formulas, where revision operators include both deletions and additions of variables. We also show that the query complexity of the read-once revision algorithm is near-optimal.}
}
@article{IRFAN201479,
title = {On influence, stable behavior, and the most influential individuals in networks: A game-theoretic approach},
journal = {Artificial Intelligence},
volume = {215},
pages = {79-119},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000812},
author = {Mohammad T. Irfan and Luis E. Ortiz},
keywords = {Computational game theory, Social network analysis, Influence in social networks, Nash equilibrium, Computational complexity},
abstract = {We introduce a new approach to the study of influence in strategic settings where the action of an individual depends on that of others in a network-structured way. We propose network influence games as a game-theoretic model of the behavior of a large but finite networked population. In particular, we study an instance we call linear-influence games that allows both positive and negative influence factors, permitting reversals in behavioral choices. We embrace pure-strategy Nash equilibrium, an important solution concept in non-cooperative game theory, to formally define the stable outcomes of a network influence game and to predict potential outcomes without explicitly considering intricate dynamics. We address an important problem in network influence, the identification of the most influential individuals, and approach it algorithmically using pure-strategy Nash-equilibria computation. Computationally, we provide (a) complexity characterizations of various problems on linear-influence games; (b) efficient algorithms for several special cases and heuristics for hard cases; and (c) approximation algorithms, with provable guarantees, for the problem of identifying the most influential individuals. Experimentally, we evaluate our approach using both synthetic network influence games and real-world settings of general interest, each corresponding to a separate branch of the U.S. Government. Mathematically, we connect linear-influence games to important models in game theory: potential and polymatrix games.}
}
@article{YIP1996309,
title = {Model simplification by asymptotic order of magnitude reasoning},
journal = {Artificial Intelligence},
volume = {80},
number = {2},
pages = {309-348},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00068-9},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000689},
author = {Kenneth Man-kam Yip},
abstract = {One of the hardest problems in reasoning about a physical system is finding an approximate model that is mathematically tractable and yet captures the essence of the problem. This paper describes an implemented program AOM which automates a powerful simplification method. AOM is based on two domain-independent ideas: self-consistent approximations and asymptotic order of magnitude reasoning. The basic operation of AOM consists of five steps: (1) assign order of magnitude estimates to terms in the equations, (2) find maximal terms of each equation, i.e., terms that are not dominated by any other terms in the same equation, (3) consider all possible n-term dominant balance assumptions, (4) propagate the effects of the balance assumptions, and (5) remove partial models based on inconsistent balance assumptions. AOM also exploits constraints among equations and submodels. We demonstrate its power by showing how the program simplifies difficult fluid models described by coupled nonlinear partial differential equations with several parameters. We believe the derivation given by AOM is more systematic and easily understandable than those given in published papers.}
}
@article{MOSES1996229,
title = {Off-line reasoning for on-line efficiency: knowledge bases},
journal = {Artificial Intelligence},
volume = {83},
number = {2},
pages = {229-239},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00015-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000151},
author = {Yoram Moses and Moshe Tennenholtz},
abstract = {The complexity of reasoning is a fundamental issue in AI. In many cases, the fact that an intelligent system needs to perform reasoning on-line contributes to the difficulty of this reasoning. This paper considers the case in which an intelligent system computes whether a query is entailed by the system's knowledge base. It investigates how an initial phase of off-line preprocessing and design can improve the on-line complexity considerably. The notion of an efficient basis for a query language is presented, and it is shown that off-line preprocessing can be very effective for query languages that have an efficient basis. The usefulness of this notion is illustrated by showing that a fairly expressive language has an efficient basis. A dual notion of an efficient disjunctive basis for a knowledge base is introduced, and it is shown that off-line preprocessing is worthwhile for knowledge bases that have an efficient disjunctive basis.}
}
@article{LUCAS2005233,
title = {Bayesian network modelling through qualitative patterns},
journal = {Artificial Intelligence},
volume = {163},
number = {2},
pages = {233-263},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204001766},
author = {Peter J.F. Lucas},
keywords = {Bayesian networks, Knowledge representation, Qualitative reasoning},
abstract = {In designing a Bayesian network for an actual problem, developers need to bridge the gap between the mathematical abstractions offered by the Bayesian-network formalism and the features of the problem to be modelled. Qualitative probabilistic networks (QPNs) have been put forward as qualitative analogues to Bayesian networks, and allow modelling interactions in terms of qualitative signs. They thus have the advantage that developers can abstract from the numerical detail, and therefore the gap may not be as wide as for their quantitative counterparts. A notion that has been suggested in the literature to facilitate Bayesian-network development is causal independence. It allows exploiting compact representations of probabilistic interactions among variables in a network. In the paper, we deploy both causal independence and QPNs in developing and analysing a collection of qualitative, causal interaction patterns, called QC patterns. These are endowed with a fixed qualitative semantics, and are intended to offer developers a high-level starting point when developing Bayesian networks.}
}
@article{KRAUS199779,
title = {Negotiation and cooperation in multi-agent environments},
journal = {Artificial Intelligence},
volume = {94},
number = {1},
pages = {79-97},
year = {1997},
note = {Economic Principles of Multi-Agent Systems},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00025-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000258},
author = {Sarit Kraus},
keywords = {Distributed Artificial Intelligence, Multi-agent systems, Cooperation, Negotiation},
abstract = {Automated intelligent agents inhabiting a shared environment must coordinate their activities. Cooperationâ€”not merely coordinationâ€”may improve the performance of the individual agents or the overall behavior of the system they form. Research in Distributed Artificial Intelligence (DAI) addresses the problem of designing automated intelligent systems which interact effectively. DAI is not the only field to take on the challenge of understanding cooperation and coordination. There are a variety of other multi-entity environments in which the entities coordinate their activity and cooperate. Among them are groups of people, animals, particles, and computers. We argue that in order to address the challenge of building coordinated and collaborated intelligent agents, it is beneficial to combine AI techniques with methods and techniques from a range of multi-entity fields, such as game theory, operations research, physics and philosophy. To support this claim, we describe some of our projects, where we have successfully taken an interdisciplinary approach. We demonstrate the benefits in applying multi-entity methodologies and show the adaptations, modifications and extensions necessary for solving the DAI problems}
}
@article{WANG1995161,
title = {A typed resolution principle for deduction with conditional typing theory},
journal = {Artificial Intelligence},
volume = {75},
number = {2},
pages = {161-194},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00027-X},
url = {https://www.sciencedirect.com/science/article/pii/000437029400027X},
author = {Tie-Cheng Wang},
abstract = {The formal reasoning involved in solving a real world problem usually consists of two parts: type reasoning associated with the type structure of the problem domain, and general reasoning associated with the kernel formulation. This paper introduces a typed resolution principle for a typed predicate calculus. This principle permits a separation of type reasoning from general reasoning even when the background typing theory shares the same language constructs with the kernel formulation. Such a typing theory is required for an accurate formulation of the type structure of a computer program which contains partial functions and predicate subtypes, and also is useful for efficiently proving certain theorems from mathematics and logic by typed (sorted) formulation and deduction. The paper presents a typed deduction procedure which employs type reasoning as a form of constraints to general reasoning for speeding up the proof discovery. The paper also discusses further refinements of the procedure by incorporating existing refinements of untyped resolution.}
}
@article{DENECKER2007332,
title = {Inductive situation calculus},
journal = {Artificial Intelligence},
volume = {171},
number = {5},
pages = {332-360},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000409},
author = {Marc Denecker and Eugenia Ternovska},
keywords = {Knowledge representation, Inductive definitions, Situation calculus},
abstract = {Temporal reasoning has always been a major test case for knowledge representation formalisms. In this paper, we develop an inductive variant of the situation calculus in ID-logic, classical logic extended with inductive definitions. This logic has been proposed recently and is an extension of classical logic. It allows for a uniform representation of various forms of definitions, including monotone inductive definitions and non-monotone forms of inductive definitions such as iterated induction and induction over well-founded posets. We show that the role of such complex forms of definitions is not limited to mathematics but extends to commonsense knowledge representation. In the ID-logic axiomatization of the situation calculus, fluents and causality predicates are defined by simultaneous induction on the well-founded poset of situations. The inductive approach allows us to solve the ramification problem for the situation calculus in a uniform and modular way. Our solution is among the most general solutions for the ramification problem in the situation calculus. Using previously developed modularity techniques, we show that the basic variant of the inductive situation calculus without ramification rules is equivalent to Reiter-style situation calculus.}
}
@article{BOGAERTS201551,
title = {Grounded fixpoints and their applications in knowledge representation},
journal = {Artificial Intelligence},
volume = {224},
pages = {51-71},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000521},
author = {Bart Bogaerts and Joost Vennekens and Marc Denecker},
keywords = {Approximation fixpoint theory, Lattice operator, Stable semantics, Well-founded semantics, Groundedness, Logic programming, Autoepistemic logic, Abstract argumentation, Abstract dialectical frameworks},
abstract = {In various domains of logic, researchers have made use of a similar intuition: that facts (or models) can be derived from the ground up. They typically phrase this intuition by saying, e.g., that the facts should be grounded, or that they should not be unfounded, or that they should be supported by cycle-free arguments, et cetera. In this paper, we formalise this intuition in the context of algebraical fixpoint theory. We define when a lattice element xâˆˆL is grounded for lattice operator O:Lâ†’L. On the algebraical level, we investigate the relationship between grounded fixpoints and the various classes of fixpoints of approximation fixpoint theory, including supported, minimal, Kripkeâ€“Kleene, stable and well-founded fixpoints. On the logical level, we investigate groundedness in the context of logic programming, autoepistemic logic, default logic and argumentation frameworks. We explain what grounded points and fixpoints mean in these logics and show that this concept indeed formalises intuitions that existed in these fields. We investigate which existing semantics are grounded. We study the novel semantics for these logics that is induced by grounded fixpoints, which has some very appealing properties, not in the least its mathematical simplicity and generality. Our results unveil a remarkable uniformity in intuitions and mathematics in these fields.}
}
@article{WAEGEMAN20111223,
title = {On the ERA ranking representability of pairwise bipartite ranking functions},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1223-1250},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001955},
author = {Willem Waegeman and Bernard {De Baets}},
keywords = {Pairwise bipartite ranking, Reciprocal preference relation, Cycle transitivity, Receiver operating characteristics (ROC) analysis, Graph theory, Multi-class classification, Decision theory, Machine learning},
abstract = {In domains like decision theory and social choice theory it is known for a long time that stochastic transitivity properties yield necessary and sufficient conditions for the ranking or utility representability of reciprocal preference relations. In this article we extend these results for reciprocal preference relations originating from the pairwise comparison of random vectors in a machine learning context. More specifically, the expected ranking accuracy (ERA) is such a reciprocal relation that occurs in multi-class classification problems, when ranking or utility functions are fitted to the data in a pairwise manner. We establish necessary and sufficient conditions for which these pairwise bipartite ranking functions can be simplified to a single ranking function such that the pairwise expected ranking accuracies of both models coincide. Similarly as for more common reciprocal preference relations, cycle transitivity plays a crucial role in this new setting. We first consider the finite sample case, for which expected ranking accuracy can be estimated by means of the area under the ROC curve (AUC), and subsequently, we further generalize these results to the underlying distributions. It turns out that the ranking representability of pairwisely compared random vectors can be expressed elegantly in a distribution-independent way by means of a specific type of cycle transitivity, defined by a conjunctor that is closely related to the algebraic product.}
}
@article{SIMON199595,
title = {Artificial intelligence: an empirical science},
journal = {Artificial Intelligence},
volume = {77},
number = {1},
pages = {95-127},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00039-H},
url = {https://www.sciencedirect.com/science/article/pii/000437029500039H},
author = {Herbert A. Simon},
abstract = {My initial tasks in this paper are, first, to delimit the boundaries of artificial intelligence, then, to justify calling it a science: is AI science, or is it engineering, or some combination of these? After arguing that it is (at least) a science, I will consider how it is best pursued: in particular, the respective roles for experiment and theory in developing AI. I will rely more on history than on speculation, for our actual experience in advancing the field has much to tell us about how we can continue and accelerate that advance. Many of my examples will be drawn from work with which I have been associated, for I can speak with greater confidence about what motivated that work and its methods (and about its defects) than I can about the work of others. My goal, however, is not to give you a trip through history, but to make definite proposals for our future priorities, using history, where relevant, as evidence for my views.}
}
@article{LAFORTE1998265,
title = {Why GÃ¶del's theorem cannot refute computationalism},
journal = {Artificial Intelligence},
volume = {104},
number = {1},
pages = {265-286},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00052-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000526},
author = {Geoffrey LaForte and Patrick J. Hayes and Kenneth M. Ford},
keywords = {GÃ¶del, Computationalism, Truth},
abstract = {GÃ¶del's theorem is consistent with the computationalist hypothesis. Roger Penrose, however, claims to prove that GÃ¶del's theorem implies that human thought cannot be mechanized. We review his arguments and show how they are flawed. Penrose's arguments depend crucially on ambiguities between precise and imprecise senses of key terms. We show that these ambiguities cause the GÃ¶del/Turing diagonalization argument to lead from apparently intuitive claims about human abilities to paradoxical or highly idiosyncratic conclusions, and conclude that any similar argument will also fail in the same ways.}
}
@article{LARSON201617,
title = {Automated conjecturing I: Fajtlowicz's Dalmatian heuristic revisited},
journal = {Artificial Intelligence},
volume = {231},
pages = {17-38},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001575},
author = {C.E. Larson and N. {Van Cleemput}},
keywords = {Automated conjecturing, Automated conjecture-making, Mathematical discovery, Automated scientific discovery, Dalmatian heuristic},
abstract = {We discuss a new implementation of, and new experiments with, Fajtlowicz's Dalmatian conjecture-making heuristic. Our program makes conjectures about relations of real number invariants of mathematical objects. Conjectures in matrix theory, number theory, and graph theory are reported, together with an experiment in using conjectures to automate game play. The program can be used in a way that, by design, advances mathematical research. These experiments suggest that automated conjecture-making can be a useful ability in the design of machines that can perform a variety of tasks that require intelligence.}
}
@article{HORIYAMA2002189,
title = {Ordered binary decision diagrams as knowledge-bases},
journal = {Artificial Intelligence},
volume = {136},
number = {2},
pages = {189-213},
year = {2002},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(02)00119-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370202001194},
author = {Takashi Horiyama and Toshihide Ibaraki},
keywords = {Knowledge representation, Automated reasoning, Ordered binary decision diagrams (OBDDs), Recognition problems, Unate functions, Horn functions},
abstract = {We consider the use of ordered binary decision diagrams (OBDDs) as a means of realizing knowledge-bases, and show that, from the view point of space requirement, the OBDD-based representation is more efficient and suitable in some cases, compared with the traditional CNF-based and/or model-based representations. We then present polynomial time algorithms for the two problems of testing whether a given OBDD represents a unate Boolean function, and of testing whether it represents a Horn function.}
}
@article{PEASE2017181,
title = {Lakatos-style collaborative mathematics through dialectical, structured and abstract argumentation},
journal = {Artificial Intelligence},
volume = {246},
pages = {181-219},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300267},
author = {Alison Pease and John Lawrence and Katarzyna Budzynska and Joseph Corneli and Chris Reed},
keywords = {Automated theorem proving, Automated reasoning, Abstract argumentation, Argumentation, Collaborative intelligence, Dialogue games, Lakatos, Mathematical argument, Structured argumentation, Social creativity, Philosophy of mathematical practice},
abstract = {The simulation of mathematical reasoning has been a driving force throughout the history of Artificial Intelligence research. However, despite significant successes in computer mathematics, computers are not widely used by mathematicians apart from their quotidian applications. An oft-cited reason for this is that current computational systems cannot do mathematics in the way that humans do. We draw on two areas in which Automated Theorem Proving (ATP) is currently unlike human mathematics: firstly in a focus on soundness, rather than understandability of proof, and secondly in social aspects. Employing techniques and tools from argumentation to build a framework for mixed-initiative collaboration, we develop three complementary arcs. In the first arc â€“ our theoretical model â€“ we interpret the informal logic of mathematical discovery proposed by Lakatos, a philosopher of mathematics, through the lens of dialogue game theory and in particular as a dialogue game ranging over structures of argumentation. In our second arc â€“ our abstraction level â€“ we develop structured arguments, from which we induce abstract argumentation systems and compute the argumentation semantics to provide labelings of the acceptability status of each argument. The output from this stage corresponds to a final, or currently accepted proof artefact, which can be viewed alongside its historical development. Finally, in the third arc â€“ our computational model â€“ we show how each of these formal steps is available in implementation. In an appendix, we demonstrate our approach with a formal, implemented example of real-world mathematical collaboration. We conclude the paper with reflections on our mixed-initiative collaborative approach.}
}
@article{QIU2007239,
title = {A note on Trillas' CHC models},
journal = {Artificial Intelligence},
volume = {171},
number = {4},
pages = {239-254},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206001421},
author = {Daowen Qiu},
keywords = {Orthocomplemented lattices, Orthomodular lattices, Residuated lattices, Conjectures, Consequences, Quantum logic},
abstract = {Trillas et al. [E. Trillas, S. Cubillo, E. CastiÃ±eira, On conjectures in orthocomplemented lattices, Artificial Intelligence 117 (2000) 255â€“275] recently proposed a mathematical model for conjectures, hypotheses and consequences (abbr. CHCs), and with this model we can execute certain mathematical reasoning and reformulate some important theorems in classical logic. We demonstrate that the orthomodular condition is not necessary for holding Watanabe's structure theorem of hypotheses, and indeed, in some orthocomplemented but not orthomodular lattices, this theorem is still valid. We use the CHC operators to describe the theorem of deduction, the theorem of contradiction and the Lindenbaum theorem of classical logic, and clarify their existence in the CHC models; a number of examples is presented. And we re-define the CHC operators in residuated lattices, and particularly reveal the essential differences between the CHC operators in orthocomplemented lattices and residuated lattices.}
}
@article{BASILICO201278,
title = {Patrolling security games: Definition and algorithms for solving large instances with single patroller and single intruder},
journal = {Artificial Intelligence},
volume = {184-185},
pages = {78-123},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000240},
author = {Nicola Basilico and Nicola Gatti and Francesco Amigoni},
keywords = {Security games, Adversarial patrolling, Algorithmic game theory},
abstract = {Security games are gaining significant interest in artificial intelligence. They are characterized by two players (a defender and an attacker) and by a set of targets the defender tries to protect from the attackerÊ¼s intrusions by committing to a strategy. To reach their goals, players use resources such as patrollers and intruders. Security games are Stackelberg games where the appropriate solution concept is the leaderâ€“follower equilibrium. Current algorithms for solving these games are applicable when the underlying game is in normal form (i.e., each player has a single decision node). In this paper, we define and study security games with an extensive-form infinite-horizon underlying game, where decision nodes are potentially infinite. We introduce a novel scenario where the attacker can undertake actions during the execution of the defenderÊ¼s strategy. We call this new game class patrolling security games (PSGs), since its most prominent application is patrolling environments against intruders. We show that PSGs cannot be reduced to security games studied so far and we highlight their generality in tackling adversarial patrolling on arbitrary graphs. We then design algorithms to solve large instances with single patroller and single intruder.}
}
@article{TRILLAS2000255,
title = {On conjectures in orthocomplemented lattices},
journal = {Artificial Intelligence},
volume = {117},
number = {2},
pages = {255-275},
year = {2000},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00108-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299001083},
author = {Enric Trillas and Susana Cubillo and Elena CastiÃ±eira},
keywords = {Orthocomplemented, Orthomodular and Boolean lattices, Conjectures, Consequences, Hypotheses and their structure},
abstract = {A mathematical model for conjectures in orthocomplemented lattices is presented. After defining when a conjecture is a consequence or a hypothesis, some operators of conjectures, consequences and hypotheses are introduced and some properties they show are studied. This is the case, for example, of being monotonic or non-monotonic operators. As orthocomplemented lattices contain orthomodular lattices and Boolean algebras, they offer a sufficiently broad framework to obtain some general results that can be restricted to such particular, but important, lattices. This is, for example, the case of the structure's theorem for hypotheses. Some results are illustrated by examples of mathematical or linguistic character, and an appendix on orthocomplemented lattices is included.}
}
@article{ASUNCION20121,
title = {Ordered completion for first-order logic programs on finite structures},
journal = {Artificial Intelligence},
volume = {177-179},
pages = {1-24},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211001263},
author = {Vernon Asuncion and Fangzhen Lin and Yan Zhang and Yi Zhou},
keywords = {Answer set programming, Ordered completion, Knowledge representation, Nonmonotonic reasoning},
abstract = {In this paper, we propose a translation from normal first-order logic programs under the stable model semantics to first-order sentences on finite structures. The translation is done through, what we call, ordered completion which is a modification of ClarkÊ¼s completion with some auxiliary predicates added to keep track of the derivation order. We show that, on finite structures, classical models of the ordered completion of a normal logic program correspond exactly to the stable models of the program. We also extend this result to normal programs with constraints and choice rules. From a theoretical viewpoint, this work clarifies the relationships between normal logic programming under the stable model semantics and classical first-order logic. It follows that, on finite structures, every normal program can be defined by a first-order sentence if new predicates are allowed. This is a tight result as not every normal logic program can be defined by a first-order sentence if no extra predicates are allowed or when infinite structures are considered. Furthermore, we show that the result cannot be extended to disjunctive logic programs, assuming that NPâ‰ coNP. From a practical viewpoint, this work leads to a new type of ASP solver by grounding on a programÊ¼s ordered completion instead of the program itself. We report on a first implementation of such a solver based on several optimization techniques. Our experimental results show that our solver compares favorably to other major ASP solvers on the Hamiltonian Circuit program, especially on large domains.}
}
@article{MUNOZVELASCO20191,
title = {On coarser interval temporal logics},
journal = {Artificial Intelligence},
volume = {266},
pages = {1-26},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218305964},
author = {Emilio MuÃ±oz-Velasco and Mercedes PelegrÃ­n and Pietro Sala and Guido Sciavicco and Ionel Eduard Stan},
keywords = {Modal and temporal logic, (Un)Decidability, Complexity},
abstract = {The primary characteristic of interval temporal logic is that intervals, rather than points, are taken as the primitive ontological entities. Given their generally bad computational behavior of interval temporal logics, several techniques exist to produce decidable and computationally affordable temporal logics based on intervals. In this paper we take inspiration from Golumbic and Shamir's coarser interval algebras, which generalize the classical Allen's Interval Algebra, in order to define two previously unknown variants of Halpern and Shoham's logic (HS) based on coarser relations. We prove that, perhaps surprisingly, the satisfiability problem for the coarsest of the two variants, namely HS3, not only is decidable, but PSpace-complete in the finite/discrete case, and PSpace-hard in any other case; besides proving its complexity bounds, we implement a tableau-based satisfiability checker for it and test it against a systematically generated benchmark. Our results are strengthened by showing that not all coarser-than-Allen's relations are a guarantee of decidability, as we prove that the second variant, namely HS7, remains undecidable in all interesting cases.}
}
@article{WOLTER20101498,
title = {Qualitative reasoning with directional relations},
journal = {Artificial Intelligence},
volume = {174},
number = {18},
pages = {1498-1507},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001530},
author = {D. Wolter and J.H. Lee},
keywords = {Qualitative spatial reasoning},
abstract = {Qualitative spatial reasoning (QSR) pursues a symbolic approach to reasoning about a spatial domain. Qualitative calculi are defined to capture domain properties in relation operations, granting a relation algebraic approach to reasoning. QSR has two primary goals: providing a symbolic model for human common-sense level of reasoning and providing efficient means for reasoning. In this paper, we dismantle the hope for efficient reasoning about directional information in infinite spatial domains by showing that it is inherently hard to decide consistency of a set of constraints that represents positions in the plane by specifying directions from reference objects. We assume that these reference objects are not fixed but only constrained through directional relations themselves. Known QSR reasoning methods fail to handle this information.}
}
@article{AO2023103954,
title = {Entropy Estimation via Uniformization},
journal = {Artificial Intelligence},
pages = {103954},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103954},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223001005},
author = {Ziqiao Ao and Jinglai Li},
keywords = {entropy estimation,  nearest neighbor estimator, normalizing flow, uniformization},
abstract = {Entropy estimation is of practical importance in information theory and statistical science. Many existing entropy estimators suffer from fast growing estimation bias with respect to dimensionality, rendering them unsuitable for high-dimensional problems. In this work we propose a transform-based method for high-dimensional entropy estimation, which consists of the following two main ingredients. Firstly, we provide a modified k-nearest neighbors (k-NN) entropy estimator that can reduce estimation bias for samples closely resembling a uniform distribution. Second we design a normalizing flow based mapping that pushes samples toward the uniform distribution, and the relation between the entropy of the original samples and the transformed ones is also derived. As a result the entropy of a given set of samples is estimated by first transforming them toward the uniform distribution and then applying the proposed estimator to the transformed samples. The performance of the proposed method is compared against several existing entropy estimators, with both mathematical examples and real-world applications.}
}
@article{DONNELLY2004145,
title = {A formal theory for reasoning about parthood, connection, and location},
journal = {Artificial Intelligence},
volume = {160},
number = {1},
pages = {145-172},
year = {2004},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204001134},
author = {Maureen Donnelly},
keywords = {Spatial reasoning, Mereotopology, Formal ontology, Physical objects, Holes},
abstract = {In fields such as medicine, geography, and mechanics, spatial reasoning involves reasoning about entities that may coincide without overlapping. Some examples are: cavities and invading particles, passageways and valves, geographic regions and tropical storms. The purpose of this paper is to develop a formal theory of spatial relations for domains that include coincident entities. The core of the theory is a clear distinction between mereotopological relations, such as parthood and connection, and relative location relations, such as coincidence. To guide the development of the formal theory, I construct mathematical models in which nontrivial relative location relations are defined.}
}
@article{KORF20029,
title = {Disjoint pattern database heuristics},
journal = {Artificial Intelligence},
volume = {134},
number = {1},
pages = {9-22},
year = {2002},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(01)00092-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370201000923},
author = {Richard E. Korf and Ariel Felner},
keywords = {Problem solving, Single-agent search, Heuristic search, Heuristic evaluation functions, Pattern databases, Sliding-tile puzzles, Fifteen Puzzle, Twenty-Four Puzzle, Rubik's Cube},
abstract = {We describe a new technique for designing more accurate admissible heuristic evaluation functions, based on pattern databases [J. Culberson, J. Schaeffer, Comput. Intelligence 14 (3) (1998) 318â€“334]. While many heuristics, such as Manhattan distance, compute the cost of solving individual subgoals independently, pattern databases consider the cost of solving multiple subgoals simultaneously. Existing work on pattern databases allows combining values from different pattern databases by taking their maximum. If the subgoals can be divided into disjoint subsets so that each operator only affects subgoals in one subset, then we can add the pattern-database values for each subset, resulting in a more accurate admissible heuristic function. We used this technique to improve performance on the Fifteen Puzzle by a factor of over 2000, and to find optimal solutions to 50 random instances of the Twenty-Four Puzzle.}
}
@article{DELEDESMA1997281,
title = {A computational approach to George Boole's discovery of mathematical logic},
journal = {Artificial Intelligence},
volume = {91},
number = {2},
pages = {281-307},
year = {1997},
note = {Scientific Discovery},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00017-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000179},
author = {Luis {de Ledesma} and Aurora PÃ©rez and Daniel Borrajo and Luis M. Laita},
abstract = {This paper reports a computational model of Boole's discovery of Logic as a part of Mathematics. George Boole (1815â€“1864) found that the symbols of Logic behaved as algebraic symbols, and he then rebuilt the whole contemporary theory of Logic by the use of methods such as the solution of algebraic equations. Study of the different historical factors that influenced this achievement has served as background for our two main contributions: a computational representation of Boole's Logic before it was mathematized; and a production system, BOOLE2, that rediscovers Logic as a science that behaves exactly as a branch of Mathematics, and that thus validates to some extent the historical explanation. The system's discovery methods are found to be general enough to handle three other cases: two versions of a Geometry due to a contemporary of Boole, and a small subset of the Differential Calculus.}
}
@article{DENOEUX2010479,
title = {Representing uncertainty on set-valued variables using belief functions},
journal = {Artificial Intelligence},
volume = {174},
number = {7},
pages = {479-499},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000159},
author = {Thierry DenÅ“ux and Zoulficar Younes and Fahed Abdallah},
keywords = {Dempsterâ€“Shafer theory, Evidence theory, Conjunctive knowledge, Lattice, Uncertain reasoning, Multi-label classification},
abstract = {A formalism is proposed for representing uncertain information on set-valued variables using the formalism of belief functions. A set-valued variable X on a domain Î© is a variable taking zero, one or several values in Î©. While defining mass functions on the frame 22Î© is usually not feasible because of the double-exponential complexity involved, we propose an approach based on a definition of a restricted family of subsets of 2Î© that is closed under intersection and has a lattice structure. Using recent results about belief functions on lattices, we show that most notions from Dempsterâ€“Shafer theory can be transposed to that particular lattice, making it possible to express rich knowledge about X with only limited additional complexity as compared to the single-valued case. An application to multi-label classification (in which each learning instance can belong to several classes simultaneously) is demonstrated.}
}
@article{GORDON2007875,
title = {The Carneades model of argument and burden of proof},
journal = {Artificial Intelligence},
volume = {171},
number = {10},
pages = {875-896},
year = {2007},
note = {Argumentation in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000677},
author = {Thomas F. Gordon and Henry Prakken and Douglas Walton},
keywords = {Argument structure, Argument graphs, Argument evaluation, Argumentation schemes, Burden of proof, Proof standards, Legal argument},
abstract = {We present a formal, mathematical model of argument structure and evaluation, taking seriously the procedural and dialogical aspects of argumentation. The model applies proof standards to determine the acceptability of statements on an issue-by-issue basis. The model uses different types of premises (ordinary premises, assumptions and exceptions) and information about the dialectical status of statements (stated, questioned, accepted or rejected) to allow the burden of proof to be allocated to the proponent or the respondent, as appropriate, for each premise separately. Our approach allows the burden of proof for a premise to be assigned to a different party than the one who has the burden of proving the conclusion of the argument, and also to change the burden of proof or applicable proof standard as the dialogue progresses from stage to stage. Useful for modeling legal dialogues, the burden of production and burden of persuasion can be handled separately, with a different responsible party and applicable proof standard for each. Carneades enables critical questions of argumentation schemes to be modeled as additional premises, using premise types to capture the varying effect on the burden of proof of different kinds of questions.}
}
@article{KOMAROVA20041,
title = {Optimizing the mutual intelligibility of linguistic agents in a shared world},
journal = {Artificial Intelligence},
volume = {154},
number = {1},
pages = {1-42},
year = {2004},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2003.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370203001693},
author = {Natalia Komarova and Partha Niyogi},
keywords = {Linguistic agents, Optimal communication, Language learning, Language evolution, Game theory, Multi-agent systems},
abstract = {We consider the problem of linguistic agents that communicate with each other about a shared world. We develop a formal notion of a language as a set of probabilistic associations between form (lexical or syntactic) and meaning (semantic) that has general applicability. Using this notion, we define a natural measure of the mutual intelligibility, F(L,Lâ€²), between two agents, one using the language L and the other using Lâ€². We then proceed to investigate three important questions within this framework: (1)Â Given a language L, what language Lâ€² maximizes mutual intelligibility with L? We find surprisingly that Lâ€² need not be the same as L and we present algorithms for approximating Lâ€² arbitrarily well. (2)Â How can one learn to optimally communicate with a user of language L when L is unknown at the outset and the learner is allowed a finite number of linguistic interactions with the user of L? We describe possible algorithms and calculate explicit bounds on the number of interactions needed. (3)Â Consider a population of linguistic agents that learn from each other and evolve over time. Will the community converge to a shared language and what is the nature of such a language? We characterize the evolutionarily stable states of a population of linguistic agents in a game-theoretic setting. Our analysis has significance for a number of areas in natural and artificial communication where one studies the design, learning, and evolution of linguistic communication systems.}
}
@article{BENCHCAPON2007619,
title = {Argumentation in artificial intelligence},
journal = {Artificial Intelligence},
volume = {171},
number = {10},
pages = {619-641},
year = {2007},
note = {Argumentation in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000793},
author = {T.J.M. Bench-Capon and Paul E. Dunne},
keywords = {Argumentation models, Dialogue processes, Argument diagrams and schemes, Agent-based negotiation, Practical reasoning},
abstract = {Over the last ten years, argumentation has come to be increasingly central as a core study within Artificial Intelligence (AI). The articles forming this volume reflect a variety of important trends, developments, and applications covering a range of current topics relating to the theory and applications of argumentation. Our aims in this introduction are, firstly, to place these contributions in the context of the historical foundations of argumentation in AI and, subsequently, to discuss a number of themes that have emerged in recent years resulting in a significant broadening of the areas in which argumentation based methods are used. We begin by presenting a brief overview of the issues of interest within the classical study of argumentation: in particular, its relationshipâ€”in terms of both similarities and important differencesâ€”to traditional concepts of logical reasoning and mathematical proof. We continue by outlining how a number of foundational contributions provided the basis for the formulation of argumentation models and their promotion in AI related settings and then consider a number of new themes that have emerged in recent years, many of which provide the principal topics of the research presented in this volume.}
}
@article{GUTIERREZBASULTO2023103808,
title = {Answering regular path queries mediated by unrestricted SQ ontologies},
journal = {Artificial Intelligence},
volume = {314},
pages = {103808},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103808},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001485},
author = {VÃ­ctor GutiÃ©rrez-Basulto and YazmÃ­n IbÃ¡Ã±ez-GarcÃ­a and Jean Christoph Jung and Filip Murlak},
keywords = {Ontology-mediated query answering, Description logic, Number restrictions, Data complexity},
abstract = {A prime application of description logics is ontology-mediated query answering, with the query language often reaching far beyond instance queries. Here, we investigate this task for positive existential two-way regular path queries and ontologies formulated in the expressive description logic SQu, where SQu denotes the extension of the basic description logic ALC with transitive roles (S) and qualified number restrictions (Q) which can be unrestrictedly applied to both non-transitive and transitive roles (â‹…u). Notably, the latter is usually forbidden in expressive description logics. As the main contribution, we show decidability of ontology-mediated query answering in that setting and establish tight complexity bounds, namely 2ExpTime-completeness in combined complexity and coNP-completeness in data complexity. Since the lower bounds are inherited from the fragment ALC, we concentrate on providing upper bounds. As main technical tools we establish a tree-like countermodel property and a characterization of when a query is not satisfied in a tree-like interpretation. Together, these results allow us to use an automata-based approach to query answering.}
}
@article{ZHOU2009240,
title = {A comparative runtime analysis of heuristic algorithms for satisfiability problems},
journal = {Artificial Intelligence},
volume = {173},
number = {2},
pages = {240-257},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001458},
author = {Yuren Zhou and Jun He and Qing Nie},
keywords = {Boolean satisfiability, Heuristic algorithms, Random walk, () EA, Hybrid algorithm, Expected first hitting time, Runtime analysis},
abstract = {The satisfiability problem is a basic core NP-complete problem. In recent years, a lot of heuristic algorithms have been developed to solve this problem, and many experiments have evaluated and compared the performance of different heuristic algorithms. However, rigorous theoretical analysis and comparison are rare. This paper analyzes and compares the expected runtime of three basic heuristic algorithms: RandomWalk, (1+1) EA, and hybrid algorithm. The runtime analysis of these heuristic algorithms on two 2-SAT instances shows that the expected runtime of these heuristic algorithms can be exponential time or polynomial time. Furthermore, these heuristic algorithms have their own advantages and disadvantages in solving different SAT instances. It also demonstrates that the expected runtime upper bound of RandomWalk on arbitrary k-SAT (kâ©¾3) is O((kâˆ’1)n), and presents a k-SAT instance that has Î˜((kâˆ’1)n) expected runtime bound.}
}
@article{WALLEY19961,
title = {Measures of uncertainty in expert systems},
journal = {Artificial Intelligence},
volume = {83},
number = {1},
pages = {1-58},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00009-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000097},
author = {Peter Walley},
keywords = {Inference, Decision, Prevision, Bayesian theory, Dempster-Shafer theory, Belief functions, Possibility theory, Lower probability, Upper probability, Imprecise probabilities, Conditional probability, Independence},
abstract = {This paper compares four measures that have been advocated as models for uncertainty in expert systems. The measures are additive probabilities (used in the Bayesian theory), coherent lower (or upper) previsions, belief functions (used in the Dempster-Shafer theory) and possibility measures (fuzzy logic). Special emphasis is given to the theory of coherent lower previsions, in which upper and lower probabilities, expectations and conditional probabilities are constructed from initial assessments through a technique of natural extension. Mathematically, all the measures can be regarded as types of coherent lower or upper previsions, and this perspective gives some insight into the properties of belief functions and possibility measures. The measures are evaluated according to six criteria: clarity of interpretation; ability to model partial information and imprecise assessments, especially judgements expressed in natural language; rules for combining and updating uncertainty, and their justification; consistency of models and inferences; feasibility of assessment; and feasibility of computations. Each of the four measures seems to be useful in special kinds of problems, but only lower and upper previsions appear to be sufficiently general to model the most common types of uncertainty.}
}
@article{GUNS20176,
title = {MiningZinc: A declarative framework for constraint-based mining},
journal = {Artificial Intelligence},
volume = {244},
pages = {6-29},
year = {2017},
note = {Combining Constraint Solving with Mining and Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001435},
author = {Tias Guns and Anton Dries and Siegfried Nijssen and Guido Tack and Luc {De Raedt}},
keywords = {Constraint-based mining, Itemset mining, Constraint programming, Declarative modeling, Pattern mining},
abstract = {We introduce MiningZinc, a declarative framework for constraint-based data mining. MiningZinc consists of two key components: a language component and an execution mechanism. First, the MiningZinc language allows for high-level and natural modeling of mining problems, so that MiningZinc models are similar to the mathematical definitions used in the literature. It is inspired by the Zinc family of languages and systems and supports user-defined constraints and functions. Secondly, the MiningZinc execution mechanism specifies how to compute solutions for the models. It is solver independent and supports both standard constraint solvers and specialized data mining systems. The high-level problem specification is first translated into a normalized constraint language (FlatZinc). Rewrite rules are then used to add redundant constraints or solve subproblems using specialized data mining algorithms or generic constraint programming solvers. Given a model, different execution strategies are automatically extracted that correspond to different sequences of algorithms to run. Optimized data mining algorithms, specialized processing routines and generic solvers can all be automatically combined. Thus, the MiningZinc language allows one to model constraint-based itemset mining problems in a solver independent way, and its execution mechanism can automatically chain different algorithms and solvers. This leads to a unique combination of declarative modeling with high-performance solving.}
}
@article{YOSHIDA199563,
title = {CLIP: concept learning from inference patterns},
journal = {Artificial Intelligence},
volume = {75},
number = {1},
pages = {63-92},
year = {1995},
note = {AI Research in Japan},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00066-A},
url = {https://www.sciencedirect.com/science/article/pii/000437029400066A},
author = {Ken'ichi Yoshida and Hiroshi Motoda},
abstract = {A new concept-learning method called CLIP (concept learning from inference patterns) is proposed that learns new concepts from inference patterns, not from positive/negative examples that most conventional concept learning methods use. The learned concepts enable an efficient inference on a more abstract level. We use a colored digraph to represent inference patterns. The graph representation is expressive enough and enables the quantitative analysis of the inference pattern frequency. The learning process consists of the following two steps: (1) Convert the original inference patterns to a colored digraph, and (2) Extract a set of typical patterns which appears frequently in the digraph. The basic idea is that the smaller the digraph becomes, the smaller the amount of data to be handled becomes and, accordingly, the more efficient the inference process that uses these data. Also, we can reduce the size of the graph by replacing each frequently appearing graph pattern with a single node, and each reduced node represents a new concept. Experimentally, CLIP automatically generates multilevel representations from a given physical/single-level representation of a carry-chain circuit. These representations involve abstract descriptions of the circuit, such as mathematical and logical descriptions.}
}
@article{KONTCHAKOV20101093,
title = {Logic-based ontology comparison and module extraction, with an application to DL-Lite},
journal = {Artificial Intelligence},
volume = {174},
number = {15},
pages = {1093-1141},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000962},
author = {Roman Kontchakov and Frank Wolter and Michael Zakharyaschev},
keywords = {Description logic, Ontology, Module extraction, Entailment, Computational complexity, Uniform interpolation, Forgetting},
abstract = {We develop a formal framework for comparing different versions of ontologies, and apply it to ontologies formulated in terms of DL-Lite, a family of â€˜lightweightâ€™ description logics designed for data-intensive applications. The main feature of our approach is that we take into account the vocabulary (=signature) with respect to which one wants to compare ontologies. Five variants of difference and inseparability relations between ontologies are introduced and their respective applications for ontology development and maintenance discussed. These variants are obtained by generalising the notion of conservative extension from mathematical logic and by distinguishing between differences that can be observed among concept inclusions, answers to queries over ABoxes, by taking into account additional context ontologies, and by considering a model-theoretic, language-independent notion of difference. We compare these variants, study their meta-properties, determine the computational complexity of the corresponding reasoning tasks, and present decision algorithms. Moreover, we show that checking inseparability can be automated by means of encoding into QBF satisfiability and using off-the-shelf general purpose QBF solvers. Inseparability relations between ontologies are then used to develop a formal framework for (minimal) module extraction. We demonstrate that different types of minimal modules induced by these inseparability relations can be automatically extracted from real-world medium-size DL-Lite ontologies by composing the known tractable syntactic locality-based module extraction algorithm with our non-tractable extraction algorithms and using the multi-engine QBF solver aqme. Finally, we explore the relationship between uniform interpolation (or forgetting) and inseparability.}
}
@article{LEONG1998209,
title = {Multiple perspective dynamic decision making},
journal = {Artificial Intelligence},
volume = {105},
number = {1},
pages = {209-261},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00082-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000824},
author = {Tze Yun Leong},
keywords = {Decision making, Knowledge representation, Multiple perspective reasoning, Probabilistic reasoning, Temporal reasoning, Semi-Markov decision processes},
abstract = {Decision making often involves deliberations in different perspectives. Distinct perspectives or views support knowledge acquisition and representation suitable for different types or stages of inference in the same discourse. This work presents a general paradigm for multiple perspective decision making over time and under uncertainty. Based on a unifying task definition and a common vocabulary for the relevant decision problems, this new paradigm balances the trade-off between model transparency and solution efficiency in current decision frameworks. The new paradigm motivates the design of DynaMoL (Dynamic decision Modeling Language), a general language for modeling and solving dynamic decision problems. The DynaMoL framework differentiates inferential and representational support for the modeling task from the solution or computation task. The dynamic decision grammar defines an extensible decision ontology and supports complex problem specification with multiple interfaces. The graphical presentation convention governs parameter visualization in multiple perspectives. The mathematical representation as semi-Markov decision process facilitates formal model analysis and admits multiple solution methods. A set of general translation techniques is devised to manage the different perspectives and representations of the decision parameters and constraints. DynaMoL has been evaluated on a prototype implementation, via some comprehensive case studies in medicine. The results demonstrate practical promise of the framework.}
}
@article{ZHANG20101307,
title = {A logic-based axiomatic model of bargaining},
journal = {Artificial Intelligence},
volume = {174},
number = {16},
pages = {1307-1322},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001414},
author = {Dongmo Zhang},
keywords = {Bargaining solution, Axiomatic model of bargaining, Logical model of negotiation, Ordinal bargaining, Game theory},
abstract = {This paper introduces an axiomatic model for bargaining analysis. We describe a bargaining situation in propositional logic and represent bargainers' preferences in total pre-orders. Based on the concept of minimal simultaneous concessions, we propose a solution to n-person bargaining problems and prove that the solution is uniquely characterized by five logical axioms: Consistency, Comprehensiveness, Collective rationality, Disagreement, and Contraction independence. This framework provides a naive solution to multi-person, multi-issue bargaining problems in discrete domains. Although the solution is purely qualitative, it can also be applied to continuous bargaining problems through a procedure of discretization, in which case the solution coincides with the Kalaiâ€“Smorodinsky solution.}
}
@article{MULLER2021103546,
title = {Kandinsky Patterns},
journal = {Artificial Intelligence},
volume = {300},
pages = {103546},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103546},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000977},
author = {Heimo MÃ¼ller and Andreas Holzinger},
keywords = {Explainable AI, Explainability, Synthetic test data, Ground truth},
abstract = {Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple, self-contained hence controllable synthetic test data sets for the development, validation and training of visual tasks and explainability in artificial intelligence (AI). Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable by human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an â€œinfallible authorityâ€ defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for testing explainability, interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns. We provide a Github repository and invite the international AI research community to a challenge to experiment with our Kandinsky Patterns. The goal is to help expand and advance the field of AI, and in particular to contribute to the increasingly important field of explainable AI.}
}
@article{HULLERMEIER2003335,
title = {Possibilistic instance-based learning},
journal = {Artificial Intelligence},
volume = {148},
number = {1},
pages = {335-383},
year = {2003},
note = {Fuzzy Set and Possibility Theory-Based Methods in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(03)00019-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370203000195},
author = {Eyke HÃ¼llermeier},
keywords = {Possibility theory, Fuzzy set theory, Machine learning, Instance-based learning, Nearest neighbor classification, Probability},
abstract = {A method of instance-based learning is introduced which makes use of possibility theory and fuzzy sets. Particularly, a possibilistic version of the similarity-guided extrapolation principle underlying the instance-based learning paradigm is proposed. This version is compared to the commonly used probabilistic approach from a methodological point of view. Moreover, aspects of knowledge representation such as the modeling of uncertainty are discussed. Taking the possibilistic extrapolation principle as a point of departure, an instance-based learning procedure is outlined which includes the handling of incomplete information, methods for reducing storage requirements and the adaptation of the influence of stored cases according to their typicality. First theoretical and experimental results showing the efficiency of possibilistic instance-based learning are presented as well.}
}
@article{BI20081731,
title = {The combination of multiple classifiers using an evidential reasoning approach},
journal = {Artificial Intelligence},
volume = {172},
number = {15},
pages = {1731-1751},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000799},
author = {Yaxin Bi and Jiwen Guan and David Bell},
keywords = {Ensemble methods, Dempster's rule of combination, Evidential reasoning, Evidential structures, Combination functions},
abstract = {In many domains when we have several competing classifiers available we want to synthesize them or some of them to get a more accurate classifier by a combination function. In this paper we propose a â€˜class-indifferentâ€™ method for combining classifier decisions represented by evidential structures called triplet and quartet, using Dempster's rule of combination. This method is unique in that it distinguishes important elements from the trivial ones in representing classifier decisions, makes use of more information than others in calculating the support for class labels and provides a practical way to apply the theoretically appealing Dempsterâ€“Shafer theory of evidence to the problem of ensemble learning. We present a formalism for modelling classifier decisions as triplet mass functions and we establish a range of formulae for combining these mass functions in order to arrive at a consensus decision. In addition we carry out a comparative study with the alternatives of simplet and dichotomous structure and also compare two combination methods, Dempster's rule and majority voting, over the UCI benchmark data, to demonstrate the advantage our approach offers.}
}
@article{KERSTING2017188,
title = {Relational linear programming},
journal = {Artificial Intelligence},
volume = {244},
pages = {188-216},
year = {2017},
note = {Combining Constraint Solving with Mining and Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001010},
author = {Kristian Kersting and Martin Mladenov and Pavel Tokmakov},
keywords = {Machine learning, Optimization, Relational logic, Statistical relational learning, Linear programming, Symmetry, (Fractional) automorphism, Color-refinement, Lifted probabilistic inference, Lifted linear programming, Equitable partitions, Orbit partitions},
abstract = {We propose relational linear programming, a simple framework for combining linear programs (LPs) and logic programs. A relational linear program (RLP) is a declarative LP template defining the objective and the constraints through the logical concepts of objects, relations, and quantified variables. This allows one to express the LP objective and constraints relationally for a varying number of individuals and relations among them without enumerating them. Together with a logical knowledge base, effectively a logic program consisting of logical facts and rules, it induces a ground LP. This ground LP is solved using lifted linear programming. That is, symmetries within the ground LP are employed to reduce its dimensionality, if possible, and the reduced program is solved using any off-the-shelf LP solver. In contrast to mainstream LP template languages such as AMPL, which features a mixture of declarative and imperative programming styles, RLP's relational nature allows a more intuitive representation of optimization problems, in particular over relational domains. We illustrate this empirically by experiments on approximate inference in Markov logic networks using LP relaxations, on solving Markov decision processes, and on collective inference using LP support vector machines.}
}
@article{GOTTLOB2000243,
title = {A comparison of structural CSP decomposition methods},
journal = {Artificial Intelligence},
volume = {124},
number = {2},
pages = {243-282},
year = {2000},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(00)00078-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370200000783},
author = {Georg Gottlob and Nicola Leone and Francesco Scarcello},
keywords = {Constraint satisfaction, Decomposition methods, Hypergraphs, Tractable cases, Degree of cyclicity, Treewidth, Hypertree width, Tree-clustering, Cycle cutsets, Biconnected components},
abstract = {We compare tractable classes of constraint satisfaction problems (CSPs). We first give a uniform presentation of the major structural CSP decomposition methods. We then introduce a new class of tractable CSPs based on the concept of hypertree decomposition recently developed in Database Theory, and analyze the cost of solving CSPs having bounded hypertree-width. We provide a framework for comparing parametric decomposition-based methods according to tractability criteria and compare the most relevant methods. We show that the method of hypertree decomposition dominates the others in the case of general CSPs (i.e., CSPs of unbounded arity). We also make comparisons for the restricted case of binary CSPs. Finally, we consider the application of decomposition methods to the dual graph of a hypergraph. In fact, this technique is often used to exploit binary decomposition methods for nonbinary CSPs. However, even in this case, the hypertree-decomposition method turns out to be the most general method.}
}
@article{GRECO20111877,
title = {On the complexity of core, kernel, and bargaining set},
journal = {Artificial Intelligence},
volume = {175},
number = {12},
pages = {1877-1910},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000749},
author = {Gianluigi Greco and Enrico Malizia and Luigi Palopoli and Francesco Scarcello},
keywords = {Coalitional games, Compact representations, Computational complexity, Solution concepts, Bounded treewidth},
abstract = {Coalitional games model scenarios where players can collaborate by forming coalitions in order to obtain higher worths than by acting in isolation. A fundamental issue of coalitional games is to single out the most desirable outcomes in terms of worth distributions, usually called solution concepts. Since decisions taken by realistic players cannot involve unbounded resources, recent computer science literature advocated the importance of assessing the complexity of computing with solution concepts. In this context, the paper provides a complete picture of the complexity issues arising with three prominent solution concepts for coalitional games with transferable utility, namely, the core, the kernel, and the bargaining set, whenever the game worth-function is represented in some reasonably compact form. The starting points of the investigation are the settings of graph games and of marginal contribution nets, where the worth of any coalition can be computed in polynomial time in the size of the game encoding and for which various open questions were stated in the literature. The paper answers these questions and, in addition, provides new insights on succinctly specified games, by characterizing the computational complexity of the core, the kernel, and the bargaining set in relevant generalizations and specializations of the two settings. Concerning the generalizations, the paper shows that dealing with arbitrary polynomial-time computable worth functionsâ€”no matter of the specific game encoding being consideredâ€”does not provide any additional source of complexity compared to graph games and marginal contribution nets. Instead, only for the core, a slight increase in complexity is exhibited for classes of games whose worth functions encode NP-hard optimization problems, as in the case of certain combinatorial games. As for specializations, the paper illustrates various tractability results on classes of bounded treewidth graph games and marginal contribution networks.}
}
@article{MELIS199965,
title = {Knowledge-based proof planning},
journal = {Artificial Intelligence},
volume = {115},
number = {1},
pages = {65-105},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00076-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000764},
author = {Erica Melis and JÃ¶rg Siekmann},
keywords = {Theorem proving, Planning, Automated proof planning, Meta-level reasoning, Integrating constraint solvers},
abstract = {Knowledge-based proof planning is a new paradigm in automated theorem proving (ATP) which swings the motivational pendulum back to its AI origins in that it employs and further develops many AI principles and techniques such as hierarchical planning, knowledge representation in frames and control-rules, constraint solving, tactical and meta-level reasoning. It differs from traditional search-based techniques in ATP not least with respect to its level of abstraction: the proof of a theorem is planned at an abstract level and an outline of the proof is found. This outline, i.e., the abstract proof plan, can be recursively expanded and it will thus construct a proof within a logical calculus. The plan operators represent mathematical techniques familiar to a working mathematician. While the knowledge of a domain is specific to the mathematical field, the representational techniques and reasoning procedures are general-purpose. The general-purpose planner makes use of this mathematical domain knowledge and of the guidance provided by declaratively represented control-rules which correspond to mathematical intuition about how to prove a theorem in a particular situation. These rules provide a basis for meta-level reasoning and goal-directed behaviour. We demonstrate our approach for the mathematical domain of limit theorems, which was proposed as a challenge to automated theorem proving by the late Woody Bledsoe. Using the proof planner of the Î©mega system we were able to solve all the well known challenge theorems including those that cannot be solved by any of the existing traditional systems.}
}
@article{LIN2011264,
title = {From answer set logic programming to circumscription via logic of GK},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {264-277},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000378},
author = {Fangzhen Lin and Yi Zhou},
keywords = {Logic programming, Answer set programming, Logic of GK, Circumscription, Nonmonotonic reasoning, Knowledge representation and reasoning},
abstract = {We first embed Pearce's equilibrium logic and Ferraris's propositional general logic programs in Lin and Shoham's logic of GK, a nonmonotonic modal logic that has been shown to include as special cases both Reiter's default logic in the propositional case and Moore's autoepistemic logic. From this embedding, we obtain a mapping from Ferraris's propositional general logic programs to circumscription, and show that this mapping can be used to check the strong equivalence between two propositional logic programs in classical logic. We also show that Ferraris's propositional general logic programs can be extended to the first-order case, and our mapping from Ferraris's propositional general logic programs to circumscription can be extended to the first-order case as well to provide a semantics for these first-order general logic programs.}
}
@article{GELFOND20023,
title = {Logic programming and knowledge representationâ€”The A-Prolog perspective},
journal = {Artificial Intelligence},
volume = {138},
number = {1},
pages = {3-38},
year = {2002},
note = {Knowledge Representation and Logic Programming},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(02)00207-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370202002072},
author = {Michael Gelfond and Nicola Leone},
keywords = {Logic programming, Nonmonotonic reasoning, Default reasoning, Answer set programming},
abstract = {In this paper we give a short introduction to logic programming approach to knowledge representation and reasoning. The intention is to help the reader to develop a â€˜feelâ€™ for the field's history and some of its recent developments. The discussion is mainly limited to logic programs under the answer set semantics. For understanding of approaches to logic programming built on well-founded semantics, general theories of argumentation, abductive reasoning, etc., the reader is referred to other publications.}
}
@article{BOUZY200139,
title = {Computer Go: An AI oriented survey},
journal = {Artificial Intelligence},
volume = {132},
number = {1},
pages = {39-103},
year = {2001},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(01)00127-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370201001278},
author = {Bruno Bouzy and Tristan Cazenave},
keywords = {Computer Go survey, Artificial intelligence methods, Evaluation function, Heuristic search, Combinatorial game theory, Automatic knowledge acquisition, Cognitive science, Mathematical morphology, Monte Carlo methods},
abstract = {Since the beginning of AI, mind games have been studied as relevant application fields. Nowadays, some programs are better than human players in most classical games. Their results highlight the efficiency of AI methods that are now quite standard. Such methods are very useful to Go programs, but they do not enable a strong Go program to be built. The problems related to Computer Go require new AI problem solving methods. Given the great number of problems and the diversity of possible solutions, Computer Go is an attractive research domain for AI. Prospective methods of programming the game of Go will probably be of interest in other domains as well. The goal of this paper is to present Computer Go by showing the links between existing studies on Computer Go and different AI related domains: evaluation function, heuristic search, machine learning, automatic knowledge generation, mathematical morphology and cognitive science. In addition, this paper describes both the practical aspects of Go programming, such as program optimization, and various theoretical aspects such as combinatorial game theory, mathematical morphology, and Monte Carlo methods.}
}
@article{CAMACHOCOLLADOS201636,
title = {Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities},
journal = {Artificial Intelligence},
volume = {240},
pages = {36-64},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300820},
author = {JosÃ© Camacho-Collados and Mohammad Taher Pilehvar and Roberto Navigli},
keywords = {Semantic representation, Lexical semantics, Word Sense Disambiguation, Semantic similarity, Sense clustering, Domain labeling},
abstract = {Owing to the need for a deep understanding of linguistic items, semantic representation is considered to be one of the fundamental components of several applications in Natural Language Processing and Artificial Intelligence. As a result, semantic representation has been one of the prominent research areas in lexical semantics over the past decades. However, due mainly to the lack of large sense-annotated corpora, most existing representation techniques are limited to the lexical level and thus cannot be effectively applied to individual word senses. In this paper we put forward a novel multilingual vector representation, called Nasari, which not only enables accurate representation of word senses in different languages, but it also provides two main advantages over existing approaches: (1) high coverage, including both concepts and named entities, (2) comparability across languages and linguistic levels (i.e., words, senses and concepts), thanks to the representation of linguistic items in a single unified semantic space and in a joint embedded space, respectively. Moreover, our representations are flexible, can be applied to multiple applications and are freely available at http://lcl.uniroma1.it/nasari/. As evaluation benchmark, we opted for four different tasks, namely, word similarity, sense clustering, domain labeling, and Word Sense Disambiguation, for each of which we report state-of-the-art performance on several standard datasets across different languages.}
}
@article{HORVATH200131,
title = {Learning logic programs with structured background knowledgeâ˜†â˜†An extended abstract of this paper appeared in: L.Â De Raedt (Ed.), Proceedings of the Fifth International Workshop on Inductive Logic Programming, Tokyo, Japan, 1995, pp. 53â€“76, Scientific Report of the Department of Computer Science, Katholieke Universiteit Leuven, and also in the post-conference volume: L.Â De Raedt (Ed.), Advances in Inductive Logic Programming, IOS Press, Amsterdam/Ohmsha, Tokyo, 1996, pp. 172â€“191.},
journal = {Artificial Intelligence},
volume = {128},
number = {1},
pages = {31-97},
year = {2001},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(01)00062-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370201000625},
author = {TamÃ¡s HorvÃ¡th and GyÃ¶rgy TurÃ¡n},
keywords = {Computational learning theory, PAC learning, Concept learning, Inductive logic programming, VC-dimension},
abstract = {The efficient learnability of restricted classes of logic programs is studied in the PAC framework of computational learning theory. We develop the product homomorphism method, which gives polynomial PAC learning algorithms for a nonrecursive Horn clause with function-free ground background knowledge, if the background knowledge satisfies some structural properties. The method is based on a characterization of the concept that corresponds to the relative least general generalization of a set of positive examples with respect to the background knowledge. The characterization is formulated in terms of products and homomorphisms. In the applications this characterization is turned into an explicit combinatorial description, which is then translated into the language of nonrecursive Horn clauses. We show that a nonrecursive Horn clause is polynomially PAC-learnable if there is a single binary background predicate and the ground atoms in the background knowledge form a forest. If the ground atoms in the background knowledge form a disjoint union of cycles then the situation is different, as the shortest consistent hypothesis may have exponential size. In this case polynomial PAC-learnability holds if a different representation language is used. We also consider the complexity of hypothesis finding for multiple clauses in some restricted cases.}
}
@article{BRADLEY2001139,
title = {Reasoning about nonlinear system identification},
journal = {Artificial Intelligence},
volume = {133},
number = {1},
pages = {139-188},
year = {2001},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(01)00143-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370201001436},
author = {Elizabeth Bradley and Matthew Easley and Reinhard Stolle},
keywords = {Automated model building, System identification, Qualitative reasoning, Qualitative physics, Knowledge representation framework, Reasoning framework, Input-output modeling},
abstract = {System identification is the process of deducing a mathematical model of the internal dynamics of a system from observations of its outputs. The computer program Pret automates this process by building a layer of artificial intelligence (AI) techniques around a set of traditional formal engineering methods. Pret takes a generate-and-test approach, using a small, powerful meta-domain theory that tailors the space of candidate models to the problem at hand. It then tests these models against the known behavior of the target system using a large set of more-general mathematical rules. The complex interplay of heterogeneous reasoning modes that is involved in this process is orchestrated by a special first-order logic system that uses static abstraction levels, dynamic declarative meta control, and a simple form of truth maintenance in order to test models quickly and cheaply. Unlike other modeling toolsâ€”most of which use libraries to model small, well-posed problems in limited domains and rely on their users to supply detailed descriptions of the target systemâ€”Pret works with nonlinear systems in multiple domains and interacts directly with the real world via sensors and actuators. This approach has met with success in a variety of simulated and real applications, ranging from textbook systems to real-world engineering problems.}
}
@article{FAN2011914,
title = {On the phase transitions of random k-constraint satisfaction problems},
journal = {Artificial Intelligence},
volume = {175},
number = {3},
pages = {914-927},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001906},
author = {Yun Fan and Jing Shen},
keywords = {Constraint satisfaction problem, Phase transition},
abstract = {Constraint satisfaction has received increasing attention over the years. Intense research has focused on solving all kinds of constraint satisfaction problems (CSPs). In this paper, first we propose a random CSP model, named k-CSP, that guarantees the existence of phase transitions under certain circumstances. The exact location of the phase transition is quantified and experimental results are provided to illustrate the performance of the proposed model. Second, we revise the model k-CSP to a random linear CSP by incorporating certain linear structure to constraint relations. We also prove the existence of the phase transition and exhibit its exact location for this random linear CSP model.}
}
@article{BALDI201478,
title = {The dropout learning algorithm},
journal = {Artificial Intelligence},
volume = {210},
pages = {78-122},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000216},
author = {Pierre Baldi and Peter Sadowski},
keywords = {Machine learning, Neural networks, Ensemble, Regularization, Stochastic neurons, Stochastic gradient descent, Backpropagation, Geometric mean, Variance minimization, Sparse representations},
abstract = {Dropout is a recently introduced algorithm for training neural networks by randomly dropping units during training to prevent their co-adaptation. A mathematical analysis of some of the static and dynamic properties of dropout is provided using Bernoulli gating variables, general enough to accommodate dropout on units or connections, and with variable rates. The framework allows a complete analysis of the ensemble averaging properties of dropout in linear networks, which is useful to understand the non-linear case. The ensemble averaging properties of dropout in non-linear logistic networks result from three fundamental equations: (1) the approximation of the expectations of logistic functions by normalized geometric means, for which bounds and estimates are derived; (2) the algebraic equality between normalized geometric means of logistic functions with the logistic of the means, which mathematically characterizes logistic functions; and (3) the linearity of the means with respect to sums, as well as products of independent variables. The results are also extended to other classes of transfer functions, including rectified linear functions. Approximation errors tend to cancel each other and do not accumulate. Dropout can also be connected to stochastic neurons and used to predict firing rates, and to backpropagation by viewing the backward propagation as ensemble averaging in a dropout linear network. Moreover, the convergence properties of dropout can be understood in terms of stochastic gradient descent. Finally, for the regularization properties of dropout, the expectation of the dropout gradient is the gradient of the corresponding approximation ensemble, regularized by an adaptive weight decay term with a propensity for self-consistent variance minimization and sparse representations.}
}
@article{FABER2011278,
title = {Semantics and complexity of recursive aggregates in answer set programming},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {278-298},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437021000038X},
author = {Wolfgang Faber and Gerald Pfeifer and Nicola Leone},
keywords = {Nonmonotonic reasoning, Answer set programming, Aggregates, Computational complexity},
abstract = {The addition of aggregates has been one of the most relevant enhancements to the language of answer set programming (ASP). They strengthen the modelling power of ASP in terms of natural and concise problem representations. Previous semantic definitions typically agree in the case of non-recursive aggregates, but the picture is less clear for aggregates involved in recursion. Some proposals explicitly avoid recursive aggregates, most others differ, and many of them do not satisfy desirable criteria, such as minimality or coincidence with answer sets in the aggregate-free case. In this paper we define a semantics for programs with arbitrary aggregates (including monotone, antimonotone, and nonmonotone aggregates) in the full ASP language allowing also for disjunction in the head (disjunctive logic programming â€” DLP). This semantics is a genuine generalization of the answer set semantics for DLP, it is defined by a natural variant of the Gelfondâ€“Lifschitz transformation, and treats aggregate and non-aggregate literals in a uniform way. This novel transformation is interesting per se also in the aggregate-free case, since it is simpler than the original transformation and does not need to differentiate between positive and negative literals. We prove that our semantics guarantees the minimality (and therefore the incomparability) of answer sets, and we demonstrate that it coincides with the standard answer set semantics on aggregate-free programs. Moreover, we carry out an in-depth study of the computational complexity of the language. The analysis pays particular attention to the impact of syntactical restrictions on programs in the form of limited use of aggregates, disjunction, and negation. While the addition of aggregates does not affect the complexity of the full DLP language, it turns out that their presence does increase the complexity of normal (i.e., non-disjunctive) ASP programs up to the second level of the polynomial hierarchy. However, we show that there are large classes of aggregates the addition of which does not cause any complexity gap even for normal programs, including the fragment allowing for arbitrary monotone, arbitrary antimonotone, and stratified (i.e., non-recursive) nonmonotone aggregates. The analysis provides some useful indications on the possibility to implement aggregates in existing reasoning engines.}
}
@article{IBARAKI19991,
title = {Functional dependencies in Horn theories},
journal = {Artificial Intelligence},
volume = {108},
number = {1},
pages = {1-30},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00114-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298001143},
author = {Toshihide Ibaraki and Alexander Kogan and Kazuhisa Makino},
keywords = {Knowledge representation, Horn theory, Functional dependency, Condensation, Computational complexity, Conjunctive normal form, Acyclic directed graph},
abstract = {This paper studies functional dependencies in Horn theories, both when the theory is represented by its clausal form and when it is defined as the Horn envelope of a set of models. We provide polynomial algorithms for the recognition of whether a given functional dependency holds in a given Horn theory, as well as polynomial algorithms for the generation of some representative sets of functional dependencies. We show that some problems of inferring functional dependencies (e.g., constructing an irredundant FD-cover) are computationally difficult. We also study the structure of functional dependencies that hold in a Horn theory, showing that every such functional dependency is in fact a single positive term Boolean function, and prove that for any Horn theory the set of its minimal functional dependencies is quasi-acyclic. Finally, we consider the problem of condensing a Horn theory, prove that any Horn theory has a unique condensation, and develop an efficient polynomial algorithm for condensing Horn theories.}
}
@article{MELIS2008656,
title = {Proof planning with multiple strategies},
journal = {Artificial Intelligence},
volume = {172},
number = {6},
pages = {656-684},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001968},
author = {Erica Melis and Andreas Meier and JÃ¶rg Siekmann},
keywords = {Theorem proving, Proof planning, Blackboard architecture, Planning, Meta-reasoning},
abstract = {Proof planning is a technique for theorem proving which replaces the ultra-efficient but blind search of classical theorem proving systems by an informed knowledge-based planning process that employs mathematical knowledge at a human-oriented level of abstraction. Standard proof planning uses methods as operators and control rules to find an abstract proof plan which can be expanded (using tactics) down to the level of the underlying logic calculus. In this paper, we propose more flexible refinements and a modification of the proof planner with an additional strategic level of control above the previous proof planning control. This strategic control guides the cooperation of the problem solving strategies by meta-reasoning. We present a general framework for proof planning with multiple strategies and describe its implementation in the Multi system. The benefits are illustrated by several large case studies, which significantly push the limits of what can be achieved by a machine today.}
}
@article{OHLBACH19991,
title = {Modal logics, description logics and arithmetic reasoning},
journal = {Artificial Intelligence},
volume = {109},
number = {1},
pages = {1-31},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00011-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000119},
author = {Hans JÃ¼rgen Ohlbach and Jana Koehler},
keywords = {Modal logic, Description logic, Combination of inference systems, Mathematical programming},
abstract = {We introduce mathematical programming and atomic decomposition as the basic modal (T-Box) inference techniques for a large class of modal and description logics. The class of description logics suitable for the proposed methods is strong on the arithmetical side. In particular there may be complex arithmetical conditions on sets of accessible worlds (role fillers). The atomic decomposition technique can deal with set constructors for modal parameters (role terms) and parameter (role) hierarchies specified in full propositional logic. Besides the standard modal operators, a number of other constructors can be added in a relatively straightforward way. Examples are graded modalities (qualified number restrictions) and also generalized quantifiers like â€œmostâ€, â€œn%â€, â€œmoreâ€ and â€œmanyâ€.}
}
@article{EPPE2018105,
title = {A computational framework for conceptual blending},
journal = {Artificial Intelligence},
volume = {256},
pages = {105-129},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S000437021730142X},
author = {Manfred Eppe and Ewen Maclean and Roberto Confalonieri and Oliver Kutz and Marco Schorlemmer and Enric Plaza and Kai-Uwe KÃ¼hnberger},
keywords = {Computational creativity, Conceptual blending, Cognitive science, Answer set programming},
abstract = {We present a computational framework for conceptual blending, a concept invention method that is advocated in cognitive science as a fundamental and uniquely human engine for creative thinking. Our framework treats a crucial part of the blending process, namely the generalisation of input concepts, as a search problem that is solved by means of modern answer set programming methods to find commonalities among input concepts. We also address the problem of pruning the space of possible blends by introducing metrics that capture most of the so-called optimality principles, described in the cognitive science literature as guidelines to produce meaningful and serendipitous blends. As a proof of concept, we demonstrate how our system invents novel concepts and theories in domains where creativity is crucial, namely mathematics and music.}
}
@article{KOHLAS199871,
title = {Model-based diagnostics and probabilistic assumption-based reasoning},
journal = {Artificial Intelligence},
volume = {104},
number = {1},
pages = {71-106},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00060-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000605},
author = {J. Kohlas and B. Anrig and R. Haenni and P.A. Monney},
keywords = {Model-based diagnostics, Assumption-based reasoning, ATMS, Probability, Propositional logic, Support, Algorithm of Abraham},
abstract = {The mathematical foundations of model-based diagnostics or diagnosis from first principles have been laid by Reiter (1987). In this paper we extend Reiter's ideas of model-based diagnostics by introducing probabilities into Reiter's framework. This is done in a mathematically sound and precise way which allows one to compute the posterior probability that a certain component is not working correctly given some observations of the system. A straightforward computation of these probabilities is not efficient and in this paper we propose a new method to solve this problem. Our method is logic-based and borrows ideas from assumption-based reasoning and ATMS. We show how it is possible to determine arguments in favor of the hypothesis that a certain group of components is not working correctly. These arguments represent the symbolic or qualitative aspect of the diagnosis process. Then they are used to derive a quantitative or numerical aspect represented by the posterior probabilities. Using two new theorems about the relation between Reiter's notion of conflict and our notion of argument, we prove that our so-called degree of support is nothing but the posterior probability that we are looking for. Furthermore, a model where each component may have more than two different operating modes is discussed and a new algorithm to compute posterior probabilities in this case is presented.}
}
@article{WYSOCKI2023103839,
title = {Assessing the communication gap between AI models and healthcare professionals: Explainability, utility and trust in AI-driven clinical decision-making},
journal = {Artificial Intelligence},
volume = {316},
pages = {103839},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103839},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001795},
author = {Oskar Wysocki and Jessica Katharine Davies and Markel Vigo and Anne Caroline Armstrong and DÃ³nal Landers and Rebecca Lee and AndrÃ© Freitas},
keywords = {Explainable model, Explainable AI, ML in healthcare, User study, Clinical decision support, Automation bias, Confirmation bias, Explanation's impact},
abstract = {This paper contributes with a pragmatic evaluation framework for explainable Machine Learning (ML) models for clinical decision support. The study revealed a more nuanced role for ML explanation models, when these are pragmatically embedded in the clinical context. Despite the general positive attitude of healthcare professionals (HCPs) towards explanations as a safety and trust mechanism, for a significant set of participants there were negative effects associated with confirmation bias, accentuating model over-reliance and increased effort to interact with the model. Also, contradicting one of its main intended functions, standard explanatory models showed limited ability to support a critical understanding of the limitations of the model. However, we found new significant positive effects which repositions the role of explanations within a clinical context: these include reduction of automation bias, addressing ambiguous clinical cases (cases where HCPs were not certain about their decision) and support of less experienced HCPs in the acquisition of new domain knowledge.}
}
@article{THWAITES2013291,
title = {Causal identifiability via Chain Event Graphs},
journal = {Artificial Intelligence},
volume = {195},
pages = {291-315},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212001099},
author = {Peter Thwaites},
keywords = {Back Door theorem, Bayesian Network, Causal identifiability, Causal manipulation, Chain Event Graph, Conditional independence, Front Door theorem},
abstract = {We present the Chain Event Graph (CEG) as a complementary graphical model to the Causal Bayesian Network for the representation and analysis of causally manipulated asymmetric problems. Our focus is on causal identifiability â€” finding conditions for when the effects of a manipulation can be estimated from a subset of events observable in the unmanipulated system. CEG analogues of PearlÊ¼s Back Door and Front Door theorems are presented, applicable to the class of singular manipulations, which includes both PearlÊ¼s basic Do intervention and the class of functional manipulations possible on Bayesian Networks. These theorems are shown to be more flexible than their Bayesian Network counterparts, both in the types of manipulation to which they can be applied, and in the nature of the conditioning sets which can be used.}
}
@article{FAN20121,
title = {A general model and thresholds for random constraint satisfaction problems},
journal = {Artificial Intelligence},
volume = {193},
pages = {1-17},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212001038},
author = {Yun Fan and Jing Shen and Ke Xu},
keywords = {Constraint satisfaction problem, Phase transition},
abstract = {In this paper, we study the relation among the parameters in their most general setting that define a large class of random CSP models d-k-CSP where d is the domain size and k is the length of the constraint scopes. The model d-k-CSP unifies several related models such as the model RB and the model k-CSP. We prove that the model d-k-CSP exhibits exact phase transitions if klnd increases no slower than the logarithm of the number of variables. A series of experimental studies with interesting observations are carried out to illustrate the solubility phase transition and the hardness of instances around phase transitions.}
}
@article{CHITTARO2004147,
title = {Hierarchical model-based diagnosis based on structural abstraction},
journal = {Artificial Intelligence},
volume = {155},
number = {1},
pages = {147-182},
year = {2004},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2003.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370203001887},
author = {Luca Chittaro and Roberto Ranon},
keywords = {Model-based diagnosis, Abstraction, Hierarchical reasoning},
abstract = {Abstraction has been advocated as one of the main remedies for the computational complexity of model-based diagnosis. However, after the seminal work published in the early nineties, little research has been devoted to this topic. In this paper, we consider one of the types of abstraction commonly used in diagnosis, i.e., structural abstraction, investigating it both from a theoretical and practical point of view. First, we provide a new formalization for structural abstraction that generalizes and extends previous ones. Then, we present two new different techniques for model-based diagnosis that automatically derive easier-to-diagnose versions of a (hierarchical) diagnosis problem on the basis of the available observations. The two proposed techniques are formulated as extensions of the well-known Mozetic's algorithm [I.Â Mozetic, Hierarchical diagnosis, in: W.H.L.Â Console, J. deÂ Kleer (Eds.), Readings in Model-Based Diagnosis, Morgan Kaufmann, San Mateo, CA, 1992, pp. 354â€“372], and experimentally contrasted with it to evaluate the obtained efficiency gains.}
}
@article{KOCH2003177,
title = {Enhancing disjunctive logic programming systems by SAT checkers},
journal = {Artificial Intelligence},
volume = {151},
number = {1},
pages = {177-212},
year = {2003},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(03)00078-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437020300078X},
author = {Christoph Koch and Nicola Leone and Gerald Pfeifer},
keywords = {Disjunctive logic programming, Nonmonotonic reasoning, Head-cycle-free programs, Answer set programs, Stable model checking},
abstract = {Disjunctive logic programming (DLP) with stable model semantics is a powerful nonmonotonic formalism for knowledge representation and reasoning. Reasoning with DLP is harder than with normal (âˆ¨-free) logic programs, because stable model checkingâ€”deciding whether a given model is a stable model of a propositional DLP programâ€”is co-NP-complete, while it is polynomial for normal logic programs. This paper proposes a new transformation Î“M(P), which reduces stable model checking to UNSATâ€”i.e., to deciding whether a given CNF formula is unsatisfiable. The stability of a model M of a program P thus can be verified by calling a Satisfiability Checker on the CNF formula Î“M(P). The transformation is parsimonious (i.e., no new symbol is added), and efficiently computable, as it runs in logarithmic space (and therefore in polynomial time). Moreover, the size of the generated CNF formula never exceeds the size of the input (and is usually much smaller). We complement this transformation with modular evaluation results, which allow for efficient handling of large real-world reasoning problems. The proposed approach to stable model checking has been implemented in DLVâ€”a state-of-the-art implementation of DLP. A number of experiments and benchmarks have been run using SATZ as Satisfiability checker. The results of the experiments are very positive and confirm the usefulness of our techniques.}
}
@article{CHOI2008179,
title = {Phase transition in a random NK landscape model},
journal = {Artificial Intelligence},
volume = {172},
number = {2},
pages = {179-203},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001099},
author = {Sung-Soon Choi and Kyomin Jung and Jeong Han Kim},
keywords = {NK landscape, Fitness function, Solubility, Phase transition, Satisfiability problem},
abstract = {An analysis for the phase transition in a random NK landscape model, NK(n,k,z), is given. This model is motivated from population genetics and the solubility problem for the model is equivalent to a random (k+1)-SAT problem. Gao and Culberson [Y. Gao, J. Culberson, An analysis of phase transition in NK landscapes, Journal of Artificial Intelligence Research 17 (2002) 309â€“332] showed that a random instance generated by NK(n,2,z) with z>z0=27âˆ’754 is asymptotically insoluble. Based on empirical results, they conjectured that the phase transition occurs around the value z=z0. We prove that an instance generated by NK(n,2,z) with z<z0 is soluble with positive probability by providing a polynomial time algorithm. Using branching process arguments, we prove again that an instance generated by NK(n,2,z) with z>z0 is asymptotically insoluble. The results show the phase transition around z=z0 for NK(n,2,z). In the course of the analysis, we introduce a generalized random 2-SAT formula, which is of self interest, and show its phase transition phenomenon.}
}
@article{BOUTSINAS20011,
title = {Artificial nonmonotonic neural networks},
journal = {Artificial Intelligence},
volume = {132},
number = {1},
pages = {1-38},
year = {2001},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(01)00126-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370201001266},
author = {B. Boutsinas and M.N. Vrahatis},
keywords = {Nonmonotonic reasoning, Neural networks, Hybrid systems, Inheritance networks, Unconstrained optimization, DNA sequence analysis},
abstract = {In this paper, we introduce Artificial Nonmonotonic Neural Networks (ANNNs), a kind of hybrid learning systems that are capable of nonmonotonic reasoning. Nonmonotonic reasoning plays an important role in the development of artificial intelligent systems that try to mimic common sense reasoning, as exhibited by humans. On the other hand, a hybrid learning system provides an explanation capability to trained Neural Networks through acquiring symbolic knowledge of a domain, refining it using a set of classified examples along with Connectionist learning techniques and, finally, extracting comprehensible symbolic information. Artificial Nonmonotonic Neural Networks acquire knowledge represented by a multiple inheritance scheme with exceptions, such as nonmonotonic inheritance networks, and then can extract the refined knowledge in the same scheme. The key idea is to use a special cell operation during training in order to preserve the symbolic meaning of the initial inheritance scheme. Methods for knowledge initialization, knowledge refinement and knowledge extraction are introduced. We, also, prove that these methods address perfectly the constraints imposed by nonmonotonicity. Finally, performance of ANNNs is compared to other well-known hybrid systems, through extensive empirical tests.}
}
@article{DOBBE2021103555,
title = {Hard choices in artificial intelligence},
journal = {Artificial Intelligence},
volume = {300},
pages = {103555},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103555},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221001065},
author = {Roel Dobbe and Thomas {Krendl Gilbert} and Yonatan Mintz},
keywords = {AI ethics, AI safety, AI governance, AI regulation, Philosophy of artificial intelligence, Sociotechnical systems},
abstract = {As AI systems are integrated into high stakes social domains, researchers now examine how to design and operate them in a safe and ethical manner. However, the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested. In this paper, we examine the vagueness in debates about the safety and ethical behavior of AI systems. We show how this vagueness cannot be resolved through mathematical formalism alone, instead requiring deliberation about the politics of development as well as the context of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness in terms of distinct design challenges at key stages in AI system development. The resulting framework of Hard Choices in Artificial Intelligence (HCAI) empowers developers by 1) identifying points of overlap between design decisions and major sociotechnical challenges; 2) motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed. As such, HCAI contributes to a timely debate about the status of AI development in democratic societies, arguing that deliberation should be the goal of AI Safety, not just the procedure by which it is ensured.}
}
@article{MORATZ20112099,
title = {A condensed semantics for qualitative spatial reasoning about oriented straight line segments},
journal = {Artificial Intelligence},
volume = {175},
number = {16},
pages = {2099-2127},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000890},
author = {Reinhard Moratz and Dominik LÃ¼cke and Till Mossakowski},
keywords = {Qualitative spatial reasoning, Relation algebra, Affine geometry},
abstract = {More than 15 years ago, a set of qualitative spatial relations between oriented straight line segments (dipoles) was suggested by Schlieder. However, it turned out to be difficult to establish a sound constraint calculus based on these relations. In this paper, we present the results of a new investigation into dipole constraint calculi which uses algebraic methods to derive sound results on the composition of relations of dipole calculi. This new method, which we call condensed semantics, is based on an abstract symbolic model of a specific fragment of our domain. It is based on the fact that qualitative dipole relations are invariant under orientation preserving affine transformations. The dipole calculi allow for a straightforward representation of prototypical reasoning tasks for spatial agents. As an example, we show how to generate survey knowledge from local observations in a street network. The example illustrates the fast constraint-based reasoning capabilities of dipole calculi. We integrate our results into two reasoning tools which are publicly available.}
}
@article{FLECK19961,
title = {The topology of boundaries},
journal = {Artificial Intelligence},
volume = {80},
number = {1},
pages = {1-27},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00051-4},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000514},
author = {Margaret M. Fleck},
abstract = {High-level representations used in reasoning distinguish a special set of boundary locations, at which function values can change abruptly and across which adjacent regions may not be connected. Standard models of space and time, based on segmenting Rn, do not allow these possibilities because they have the wrong topological structure at boundaries. This mismatch has made it difficult to develop formal mathematical models for high-level reasoning algorithms. This paper shows how to modify an Rn model so as to have an appropriate topological structure. It then illustrates how the new models support standard reasoning algorithms, provide simple models for previously difficult situations, and suggest interesting new analyses based on change or non-change in scene topology.}
}
@article{LEV2023103843,
title = {PeerNomination: A novel peer selection algorithm to handle strategic and noisy assessments},
journal = {Artificial Intelligence},
volume = {316},
pages = {103843},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103843},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001837},
author = {Omer Lev and Nicholas Mattei and Paolo Turrini and Stanislav Zhydkov},
keywords = {Peer selection, Strategyproofness, Optimality, Noisy opinions, Reweighting},
abstract = {In peer selection a group of agents must choose a subset of themselves, as winners for, e.g., peer-reviewed grants or prizes. We take a Condorcet view of this aggregation problem, assuming that there is an objective ground-truth ordering over the agents. We study agents that have a noisy perception of this ground truth and give assessments that, even when truthful, can be inaccurate. Our goal is to select the best set of agents according to the underlying ground truth by looking at the potentially unreliable assessments of the peers. Besides being potentially unreliable, we also allow agents to be self-interested, attempting to influence the outcome of the decision in their favour. Hence, we are focused on tackling the problem of impartial (or strategyproof) peer selection â€“ how do we prevent agents from manipulating their reviews while still selecting the most deserving individuals, all in the presence of noisy evaluations? We propose a novel impartial peer selection algorithm, PeerNomination, that aims to fulfil the above desiderata. We provide a comprehensive theoretical analysis of the recall of PeerNomination and prove various properties, including impartiality and monotonicity. We also provide empirical results based on computer simulations to show its effectiveness compared to the state-of-the-art impartial peer selection algorithms. We then investigate the robustness of PeerNomination to various levels of noise in the reviews. In order to maintain good performance under such conditions, we extend PeerNomination by using weights for reviewers which, informally, capture some notion of reliability of the reviewer. We show, theoretically, that the new algorithm preserves strategyproofness and, empirically, that the weights help identify the noisy reviewers and hence to increase selection performance.1}
}
@article{REYNOLDS1996303,
title = {Stochastic modelling of Genetic Algorithms},
journal = {Artificial Intelligence},
volume = {82},
number = {1},
pages = {303-330},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00091-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000913},
author = {David Reynolds and Jagannathan Gomatam},
abstract = {This paper presents stochastic models for two classes of Genetic Algorithms. We present important distinctions throughout between classes of Genetic Algorithms which sample with and without replacement, in terms of their search dynamics. For both classes of algorithm, we derive sufficient conditions for convergence, and analyse special cases of Genetic Algorithm optimisation. We also derive a long-run measure of crossover bias for optimisation via Genetic Algorithms, which has practical implications with respect to the choice of crossover operators. For a class of Genetic Algorithms, we provide theoretical underpinning of a class of empirically derived results, by proving that the algorithms degenerate to randomised, cost-independent search as mutation probabilities increase. For an alternative class of Genetic Algorithms, we show that degeneration accompanies excessive crossover rates. In formulating the models, important definitions are introduced which capture in simple form the probabilistic properties of the genetic operators, which provides models which are independent of solution encoding schemes.}
}
@article{FIONDA2016143,
title = {Building knowledge maps of Web graphs},
journal = {Artificial Intelligence},
volume = {239},
pages = {143-167},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300807},
author = {Valeria Fionda and Claudio Gutierrez and Giuseppe PirrÃ²},
keywords = {Maps of the web, Navigation, Web of data, Linked data, Semantic web},
abstract = {We research the problem of building knowledge maps of graph-like information. There exist well-consolidated cartographic principles and techniques for mapping physical landscapes. However, we live in the digital era and similarly to the Earth, the Web is simply too large and its interrelations too complex for anyone to grasp much of it through direct observation. Thus, the problem of applying cartographic principles also to digital landscapes is intriguing. We introduce a mathematical formalism that captures the general notion of map of a graph and enables its development and manipulation in a semi-automated way. We present an implementation of our formalism on the Web of Linked Data graph and discuss algorithms that efficiently generate and combine (via an algebra) regions and maps. We present the MaGe tool, implementing the map framework, and discuss examples of knowledge maps.}
}
@article{ARZIGONCZAROWSKI1998187,
title = {From environments to representationsâ€”a mathematical theory of artificial perceptions},
journal = {Artificial Intelligence},
volume = {102},
number = {2},
pages = {187-247},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00061-7},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000617},
author = {Z. Arzi-Gonczarowski and D. Lehmann},
keywords = {Mathematical foundations of AI, Artificial perception and cognition, Categorical formalizations in AI},
abstract = {Perception is the recognition of elements and events in the environment, usually through integration of sensory impressions. It is considered here as a broad, high-level, object centered, phenomenon which happens at and above the level of holistic recognition of objects and events, where semantics begin to play a role. We propose and develop a mathematical theory of artificial perceptions. A basic mathematical category is defined. Its objects are perceptions, consisting of world elements, connotations, and a three-valued true, false, undefined predicative correspondence between them. Morphisms describe paths between perceptions. This structure serves as premises for a mathematical theory. The theory provides rigorous tools of scrutiny that deal with fundamental issues of AI such as the diversity and embodiment of artificial perceptions. It extends and systematizes certain intuitive pre-theoretical conceptions about perception, about improving and/or completing an agent's perceptual grasp, about transition between various perceptions, etc. Mathematical tools and methods are used to formalize reasonable ways to go about producing a meaningful cognitive image of the environment from every perception.}
}
@article{BELARDINELLI2009982,
title = {Quantified epistemic logics for reasoning about knowledge in multi-agent systems},
journal = {Artificial Intelligence},
volume = {173},
number = {9},
pages = {982-1013},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000307},
author = {F. Belardinelli and A. Lomuscio},
keywords = {Knowledge representation and Logic, Mathematical Logic, Multi-agent systems},
abstract = {We introduce quantified interpreted systems, a semantics to reason about knowledge in multi-agent systems in a first-order setting. Quantified interpreted systems may be used to interpret a variety of first-order modal epistemic languages with global and local terms, quantifiers, and individual and distributed knowledge operators for the agents in the system. We define first-order modal axiomatisations for different settings, and show that they are sound and complete with respect to the corresponding semantical classes. The expressibility potential of the formalism is explored by analysing two MAS scenarios: an infinite version of the muddy children problem, a typical epistemic puzzle, and a version of the battlefield game. Furthermore, we apply the theoretical results here presented to the analysis of message passing systems [R. Fagin, J. Halpern, Y. Moses, M. Vardi, Reasoning about Knowledge, MIT Press, 1995; L. Lamport, Time, clocks, and the ordering of events in a distributed system, Communication of the ACM 21 (7) (1978) 558â€“565], and compare the results obtained to their propositional counterparts. By doing so we find that key known meta-theorems of the propositional case can be expressed as validities on the corresponding class of quantified interpreted systems.}
}
@article{BAPTISTA2022103667,
title = {Relation between prognostics predictor evaluation metrics and local interpretability SHAP values},
journal = {Artificial Intelligence},
volume = {306},
pages = {103667},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103667},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000078},
author = {Marcia L. Baptista and Kai Goebel and Elsa M.P. Henriques},
keywords = {Local interpretability, Model-agnostic interpretability, SHAP values, Monotonicity, Trendability, Prognosability},
abstract = {Maintenance decisions in domains such as aeronautics are becoming increasingly dependent on being able to predict the failure of components and systems. When data-driven techniques are used for this prognostic task, they often face headwinds due to their perceived lack of interpretability. To address this issue, this paper examines how features used in a data-driven prognostic approach correlate with established metrics of monotonicity, trendability, and prognosability. In particular, we use the SHAP model (SHapley Additive exPlanations) from the field of eXplainable Artificial Intelligence (XAI) to analyze the outcome of three increasingly complex algorithms: Linear Regression, Multi-Layer Perceptron, and Echo State Network. Our goal is to test the hypothesis that the prognostics metrics correlate with the SHAP model's explanations, i.e., the SHAP values. We use baseline data from a standard data set that contains several hundred run-to-failure trajectories for jet engines. The results indicate that SHAP values track very closely with these metrics with differences observed between the models that support the assertion that model complexity is a significant factor to consider when explainability is a consideration in prognostics.}
}
@article{SEGRE200271,
title = {Nagging: A scalable fault-tolerant paradigm for distributed search},
journal = {Artificial Intelligence},
volume = {140},
number = {1},
pages = {71-106},
year = {2002},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(02)00228-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437020200228X},
author = {Alberto Maria Segre and Sean Forman and Giovanni Resta and Andrew Wildenberg},
keywords = {Parallel/distributed search algorithms, Search pruning, Game tree search, Branch and bound, Boolean satisfiability},
abstract = {This paper describes nagging, a technique for parallelizing search in a heterogeneous distributed computing environment. Nagging exploits the speedup anomaly often observed when parallelizing problems by playing multiple reformulations of the problem or portions of the problem against each other. Nagging is both fault tolerant and robust to long message latencies. In this paper, we show how nagging can be used to parallelize several different algorithms drawn from the artificial intelligence literature, and describe how nagging can be combined with partitioning, the more traditional search parallelization strategy. We present a theoretical analysis of the advantage of nagging with respect to partitioning, and give empirical results obtained on a cluster of 64 processors that demonstrate nagging's effectiveness and scalability as applied to Aâˆ— search, Î±Î² minimax game tree search, and the Davisâ€“Putnam algorithm.}
}
@article{SHEHORY19991,
title = {Emergent cooperative goal-satisfaction in large-scale automated-agent systems},
journal = {Artificial Intelligence},
volume = {110},
number = {1},
pages = {1-55},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00018-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000181},
author = {Onn Shehory and Sarit Kraus and Osher Yadgar},
keywords = {Multi-agent systems, Task allocation},
abstract = {Cooperation among autonomous agents has been discussed in the DAI community for several years. Papers about cooperation (Conte et al., 1991; Rosenschein, 1986), negotiation (Kraus and Wilkenfeld, 1991), distributed planning (Conry et al., 1988), and coalition formation (Ketchpel, 1994; Sandholm and Lesser, 1997), have provided a variety of approaches and several algorithms and solutions to situations wherein cooperation is possible. However, the case of cooperation in large-scale multi-agent systems (MAS) has not been thoroughly examined. Therefore, in this paper we present a framework for cooperative goal-satisfaction in large-scale environments focusing on a low-complexity physics-oriented approach. The multi-agent systems with which we deal are modeled by a physics-oriented model. According to the model, MAS inherit physical properties, and therefore the evolution of the computational systems is similar to the evolution of physical systems. To enable implementation of the model, we provide a detailed algorithm to be used by a single agent within the system. The model and the algorithm are appropriate for large-scale, dynamic, Distributed Problem Solver systems, in which agents try to increase the benefits of the whole system. The complexity is very low, and in some specific cases it is proved to be optimal. The analysis and assessment of the algorithm are performed via the well-known behavior and properties of the modeling physical system.}
}
@article{BALDI20181,
title = {Learning in the machine: Random backpropagation and the deep learning channel},
journal = {Artificial Intelligence},
volume = {260},
pages = {1-35},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218300985},
author = {Pierre Baldi and Peter Sadowski and Zhiqin Lu},
keywords = {Deep learning, Neural networks, Backpropagation, Local learning},
abstract = {Random backpropagation (RBP) is a variant of the backpropagation algorithm for training neural networks, where the transpose of the forward matrices are replaced by fixed random matrices in the calculation of the weight updates. It is remarkable both because of its effectiveness, in spite of using random matrices to communicate error information, and because it completely removes the taxing requirement of maintaining symmetric weights in a physical neural system. To better understand random backpropagation, we first connect it to the notions of local learning and learning channels. Through this connection, we derive several alternatives to RBP, including skipped RBP (SRBP), adaptive RBP (ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their computational complexity. We then study their behavior through simulations using the MNIST and CIFAR-10 benchmark datasets. These simulations show that most of these variants work robustly, almost as well as backpropagation, and that multiplication by the derivatives of the activation functions is important. As a follow-up, we study also the low-end of the number of bits required to communicate error information over the learning channel. We then provide partial intuitive explanations for some of the remarkable properties of RBP and its variations. Finally, we prove several mathematical results for RBP and its variants including: (1) the convergence to optimal fixed points for linear chains of arbitrary length; (2) convergence to fixed points for linear autoencoders with decorrelated data; (3) long-term existence of solutions for linear systems with a single hidden layer, and their convergence in special cases; and (4) convergence to fixed points of non-linear chains, when the derivative of the activation functions is included.}
}
@article{HAHMANN20091424,
title = {Stonian p-ortholattices: A new approach to the mereotopology RT0},
journal = {Artificial Intelligence},
volume = {173},
number = {15},
pages = {1424-1440},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000794},
author = {Torsten Hahmann and Michael Winter and Michael Gruninger},
keywords = {Qualitative spatial reasoning (QSR), Mereotopology, Region-based space, Stonian p-ortholattice, Non-distributive pseudocomplemented lattice},
abstract = {This paper gives an algebraic representation of the subtheories RTâˆ’, RTECâˆ’, and RT of Asher and Vieu's first-order ontology of mereotopology RT0. It corrects and extends previous work on the representation of these mereotopologies. We develop the theory of p-ortholattices â€“ lattices that are both orthocomplemented and pseudocomplemented â€“ and show that together with the Stone identity (xâ‹…y)*=x*+y* or equivalent definitions the natural class of Stonian p-ortholattices can be defined. The main contribution of the paper consists of a representation theorem for RTâˆ’ as Stonian p-ortholattices. Moreover, it is shown that the class of models of RTECâˆ’ is isomorphic to the non-distributive Stonian p-ortholattices and a characterization of RT is given by a set of four algebras of which one need to be a subalgebra of the present lattice model. As corollary we obtain that Axiom (A11) â€“ existence of two externally connected regions â€“ is in fact a theorem of the remaining axioms of RT.}
}
@article{LISSOVOI2023103804,
title = {When move acceptance selection hyper-heuristics outperform Metropolis and elitist evolutionary algorithms and when not},
journal = {Artificial Intelligence},
volume = {314},
pages = {103804},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103804},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001448},
author = {Andrei Lissovoi and Pietro S. Oliveto and John Alasdair Warwicker},
keywords = {Hyper-heuristics, Runtime analysis, Non-elitism, Metropolis, Move acceptance operators, Theory},
abstract = {Selection hyper-heuristics (HHs) are automated algorithm selection methodologies that choose between different heuristics during the optimisation process. Recently, selection HHs choosing between a collection of elitist randomised local search heuristics with different neighbourhood sizes have been shown to optimise standard unimodal benchmark functions from evolutionary computation in the optimal expected runtime achievable with the available low-level heuristics. In this paper, we extend our understanding of the performance of HHs to the domain of multimodal optimisation by considering a Move Acceptance HH (MAHH) from the literature that can switch between elitist and non-elitist heuristics during the run. In essence, MAHH is a non-elitist search heuristic that differs from other search heuristics in the source of non-elitism. We first identify the range of parameters that allow MAHH to hillclimb efficiently and prove that it can optimise the standard hillclimbing benchmark function OneMax in the best expected asymptotic time achievable by unbiased mutation-based randomised search heuristics. Afterwards, we use standard multimodal benchmark functions to highlight function characteristics where MAHH outperforms elitist evolutionary algorithms and the well-known Metropolis non-elitist algorithm by quickly escaping local optima, and ones where it does not. Since MAHH is essentially a non-elitist random local search heuristic, the paper is of independent interest to researchers in the fields of artificial intelligence and randomised search heuristics.}
}
@article{RAPHAEL2002217,
title = {A hybrid graphical model for rhythmic parsing},
journal = {Artificial Intelligence},
volume = {137},
number = {1},
pages = {217-238},
year = {2002},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(02)00192-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370202001923},
author = {Christopher Raphael},
keywords = {Conditional Gaussian distribution, Rhythmic parsing, MAP estimate, Dynamic programming, Music recognition, Hybrid graphical model},
abstract = {A method is presented for the rhythmic parsing problem: Given a sequence of observed musical note onset times, we simultaneously estimate the corresponding notated rhythm and tempo process. A graphical model is developed that represents the evolution of tempo and rhythm and relates these hidden quantities to an observable performance. The rhythm variables are discrete and the tempo and observation variables are continuous. We show how to compute the globally most likely configuration of the tempo and rhythm variables given an observation of note onset times. Experiments are presented on both MIDI data and a data set derived from an audio signal. A generalization to computing MAP estimates for arbitrary conditional Gaussian distributions is outlined.}
}
@article{PINI20111272,
title = {Incompleteness and incomparability in preference aggregation: Complexity results},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1272-1289},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001980},
author = {M.S. Pini and F. Rossi and K.B. Venable and T. Walsh},
keywords = {Preference in multi-agent systems, Preference aggregation, Computational complexity, Uncertainty, Elicitation},
abstract = {We consider how to combine the preferences of multiple agents despite the presence of incompleteness and incomparability in their preference relations over possible candidates. The set of preference relations of an agent may be incomplete because, for example, there is an ongoing preference elicitation process. There may also be incomparability as this is useful, for example, in multi-criteria scenarios. We focus here on the problem of computing the possible and necessary winners, that is, those candidates which can be, or always are, the most preferred for the agents. Possible and necessary winners are useful in many scenarios including preference elicitation. First, we show that testing possible and necessary winners is in general a computationally intractable problem for STV with unweighted votes and the cup rule with weighted votes, as is providing a good approximation of such sets. Then, we identify general properties of the preference aggregation function, such as independence to irrelevant alternatives, which are sufficient for such sets to be computed in polynomial time. Finally, we show how possible and necessary winners can be used to focus preference elicitation. We show that it is computationally intractable for the cup rule with weighted votes in the worst-case to decide when to terminate elicitation. However, we identify a property of the preference aggregation function that allows us to decide when to terminate elicitation in polynomial time, by focusing on possible and necessary winners.}
}
@article{LIU2010442,
title = {A note on minimal d-separation trees for structural learning},
journal = {Artificial Intelligence},
volume = {174},
number = {5},
pages = {442-448},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000135},
author = {Binghui Liu and Jianhua Guo and Bing-Yi Jing},
keywords = {Bayesian network, Clique tree, Minimal d-separation tree, Minimal triangulation, Separation tree, Structural learning},
abstract = {Structural learning of a Bayesian network is often decomposed into problems related to its subgraphs, although many approaches without decomposition were proposed. In 2006, Xie, Geng and Zhao proposed using a d-separation tree to improve the power of conditional independence tests and the efficiency of structural learning. In our research note, we study a minimal d-separation tree under a partial ordering, by which the maximal efficiency can be obtained. Our results demonstrate that a minimal d-separation tree of a directed acyclic graph (DAG) can be constructed by searching for the clique tree of a minimal triangulation of the moral graph for the DAG.}
}
@article{ZHOU20131,
title = {Belief functions on distributive lattices},
journal = {Artificial Intelligence},
volume = {201},
pages = {1-31},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S000437021300043X},
author = {Chunlai Zhou},
keywords = {Dempsterâ€“Shafer theory, MÃ¶bius transforms, Distributive lattices, de Morgan lattices, First degree entailments},
abstract = {The Dempsterâ€“Shafer theory of belief functions is an important approach to deal with uncertainty in AI. In the theory, belief functions are defined on Boolean algebras of events. In many applications of belief functions in real world problems, however, the objects that we manipulate is no more a Boolean algebra but a distributive lattice. In this paper, we employ BirkhoffÊ¼s representation theorem for finite distributive lattices to extend the Dempsterâ€“Shafer theory to the setting of distributive lattices, which has a mathematical theory as attractive as in that of Boolean algebras. Moreover, we use this more general theory to provide a framework for reasoning about belief functions in a deductive approach on non-classical formalisms which assume a setting of distributive lattices. As an illustration of this approach, we investigate the theory of belief functions for a simple epistemic logic the first-degree-entailment fragment of relevance logic R by providing an axiomatization for reasoning about belief functions for this logic and by showing that the complexity of the satisfiability problem of a belief formula with respect to the class of the corresponding Dempsterâ€“Shafer structures is NP-complete.}
}
@article{CHOI2010551,
title = {Optimal query complexity bounds for finding graphs},
journal = {Artificial Intelligence},
volume = {174},
number = {9},
pages = {551-569},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000251},
author = {Sung-Soon Choi and Jeong Han Kim},
keywords = {Combinatorial search, Combinatorial group testing, Graph finding, Coin weighing, Fourier coefficient, Pseudo-Boolean function, Littlewoodâ€“Offord theorem},
abstract = {We consider the problem of finding an unknown graph by using queries with an additive property. This problem was partially motivated by DNA shotgun sequencing and linkage discovery problems of artificial intelligence. Given a graph, an additive query asks the number of edges in a set of vertices while a cross-additive query asks the number of edges crossing between two disjoint sets of vertices. The queries ask the sum of weights for weighted graphs. For a graph G with n vertices and at most m edges, we prove that there exists an algorithm to find the edges of G using O(mlogn2mlog(m+1)) queries of both types for all m. The bound is best possible up to a constant factor. For a weighted graph with a mild condition on weights, it is shown that O(mlognlogm) queries are enough provided mâ©¾(logn)Î± for a sufficiently large constant Î±, which is best possible up to a constant factor if mâ©½n2âˆ’Îµ for any constant Îµ>0. This settles, in particular, a conjecture of Grebinski [V. Grebinski, On the power of additive combinatorial search model, in: Proceedings of the 4th Annual International Conference on Computing and Combinatorics (COCOON 1998), Taipei, Taiwan, 1998, pp. 194â€“203] for finding an unweighted graph using additive queries. We also consider the problem of finding the Fourier coefficients of a certain class of pseudo-Boolean functions as well as a similar coin weighing problem.}
}
@article{FREUND1999103,
title = {Statics and dynamics of induced systems},
journal = {Artificial Intelligence},
volume = {110},
number = {1},
pages = {103-134},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00020-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029900020X},
author = {Michael Freund},
keywords = {Nonmonotonic reasoning, Preferential relations, Rationality, Stratification, Conditional revision, Rationalization, Full meet base revision},
abstract = {A collection of formulae, regarded as a set of prerequisite-free normal defaults, generates a nonmonotonic inference relation through its Reiter skeptical extension. The structure of the initial set totally determines the behavior of the associated inference relation, and the aim of this paper is to investigate in two directions the link that exists between a set of defaults and its induced inference relation. First, we determine the structural conditions corresponding to the important property of rationality. For this purpose, we introduce the notion of stratification for a set of defaults, and prove that stratified sets are exactly those that induce a rational inference relation. This result is shown to have interesting consequences in belief revision theory, as it can be used to define a nontrivial full meet revision operator for belief bases. Then, we adopt a dynamic point of view and study the effects, on the induced inference relation, of a change in the set of defaults. In this perspective, the set of defaults, considered as a knowledge base, together with its induced inference relation is treated as an expert system. We show how to modify the original set of defaults in order to obtain as output a rational relation. We propose a revision procedure that enables the user to incorporate a new data in the knowledge base, and we finally show what changes can be performed on the original set of defaults in order to take into account a particular conditional that has to retracted from or added to the primitive induced inference relation.}
}
@article{LUSTREK2006620,
title = {Is real-valued minimax pathological?},
journal = {Artificial Intelligence},
volume = {170},
number = {6},
pages = {620-642},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000117},
author = {Mitja LuÅ¡trek and MatjaÅ¾ Gams and Ivan Bratko},
keywords = {Game playing, Minimax, Pathology, Game tree, Real value, Chess},
abstract = {Deeper searches in game-playing programs relying on the minimax principle generally produce better results. Theoretical analyses, however, suggest that in many cases minimaxing amplifies the noise introduced by the heuristic function used to evaluate the leaves of the game tree, leading to what is known as pathological behavior, where deeper searches produce worse results. In most minimax models analyzed in previous research, positions' true values and sometimes also heuristic values were only losses and wins. In contrast to this, a model is proposed in this paper that uses real numbers for both true and heuristic values. This model did not behave pathologically in the experiments performed. The mechanism that causes deeper searches to produce better evaluations is explained. A comparison with chess is made, indicating that the model realistically reflects position evaluations in chess-playing programs. Conditions under which the pathology might appear in a real-value model are also examined. The essential difference between our real-value model and the common two-value model, which causes the pathology in the two-value model, is identified. Most previous research reports that the pathology tends to disappear when there are dependences between the values of sibling nodes in a game tree. In this paper, another explanation is presented which indicates that in the two-value models the error of the heuristic evaluation was not modeled realistically.}
}
@article{GUAN199877,
title = {Rough computational methods for information systems},
journal = {Artificial Intelligence},
volume = {105},
number = {1},
pages = {77-103},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00090-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000903},
author = {J.W. Guan and D.A. Bell},
keywords = {Intelligent information systems, Database and knowledge base systems},
abstract = {Rough set theory is a relatively new mathematical tool for use in computer applications in circumstances which are characterized by vagueness and uncertainty. The technique called rough analysis can be applied very fruitfully in artificial intelligence and cognitive sciences. Although this methodology has been shown to be successful in dealing with the vagueness of many real-life applications, there are still several theoretical problems to be solved, and we also need to consider practical issues if we want to apply the theory. It is the latter set of issues we address here, in the context of handling and analysing large data sets during the knowledge representation process. Some of the associated problems (for example, the general problem of finding all â€œkeysâ€) have been shown to be NP-hard. Thus, it is important to seek efficient computational methods for the theory. In rough set theory, a table called an information system or a database relation is used as a special kind of formal language to represent knowledge syntactically. Semantically, knowledge is defined as classifications of information systems. The use of rough analysis does not involve the details of rough set theory directly, but it uses the same basic classification techniques. We discuss computational methods for the rough analysis of databases.}
}
@article{KONEV2015103,
title = {Computer-aided proof of ErdÅ‘s discrepancy properties},
journal = {Artificial Intelligence},
volume = {224},
pages = {103-118},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000429},
author = {Boris Konev and Alexei Lisitsa},
keywords = {ErdÅ‘s discrepancy problem, Computer-aided proof, Propositional satisfiability},
abstract = {In 1930s Paul ErdÅ‘s conjectured that for any positive integer C in any infinite Â±1 sequence (xn) there exists a subsequence xd,x2d,x3d,â€¦,xkd, for some positive integers k and d, such that |âˆ‘i=1kxiâ‹…d|>C. The conjecture has been referred to as one of the major open problems in combinatorial number theory and discrepancy theory. For the particular case of C=1 a human proof of the conjecture exists; for C=2 a bespoke computer program had generated sequences of length 1124 of discrepancy 2, but the status of the conjecture remained open even for such a small bound. We show that by encoding the problem into Boolean satisfiability and applying the state of the art SAT solvers, one can obtain a discrepancy 2 sequence of length 1160 and a proof of the ErdÅ‘s discrepancy conjecture for C=2, claiming that no discrepancy 2 sequence of length 1161, or more, exists. In the similar way, we obtain a precise bound of 127â€‰645 on the maximal lengths of both multiplicative and completely multiplicative sequences of discrepancy 3. We also demonstrate that unrestricted discrepancy 3 sequences can be longer than 130â€‰000.}
}
@article{VERWER2017368,
title = {Auction optimization using regression trees and linear models as integer programs},
journal = {Artificial Intelligence},
volume = {244},
pages = {368-395},
year = {2017},
note = {Combining Constraint Solving with Mining and Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000788},
author = {Sicco Verwer and Yingqian Zhang and Qing Chuan Ye},
keywords = {Auction design, Machine learning, Optimization, Integer linear programming, Regression},
abstract = {In a sequential auction with multiple bidding agents, the problem of determining the ordering of the items to sell in order to maximize the expected revenue is highly challenging. The challenge is largely due to the fact that the autonomy and private information of the agents heavily influence the outcome of the auction. The main contribution of this paper is two-fold. First, we demonstrate how to apply machine learning techniques to solve the optimal ordering problem in sequential auctions. We learn regression models from historical auctions, which are subsequently used to predict the expected value of orderings for new auctions. Given the learned models, we propose two types of optimization methods: a black-box best-first search approach, and a novel white-box approach that maps learned regression models to integer linear programs (ILP), which can then be solved by any ILP-solver. Although the studied auction design problem is hard, our proposed optimization methods obtain good orderings with high revenues. Our second main contribution is the insight that the internal structure of regression models can be efficiently evaluated inside an ILP solver for optimization purposes. To this end, we provide efficient encodings of regression trees and linear regression models as ILP constraints. This new way of using learned models for optimization is promising. As the experimental results show, it significantly outperforms the black-box best-first search in nearly all settings.}
}
@article{EITER2003157,
title = {A logic programming approach to knowledge-state planning, II: The DLVK system},
journal = {Artificial Intelligence},
volume = {144},
number = {1},
pages = {157-211},
year = {2003},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(02)00367-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370202003673},
author = {Thomas Eiter and Wolfgang Faber and Nicola Leone and Gerald Pfeifer and Axel Polleres},
keywords = {Deductive planning system, Disjunctive logic programming, Answer sets, Knowledge-states, Incomplete information, Conformant planning, Secure planning},
abstract = {In PartÂ I of this series of papers, we have proposed a new logic-based planning language, called K. This language facilitates the description of transitions between states of knowledge and it is well suited for planning under incomplete knowledge. Nonetheless, K also supports the representation of transitions between states of the world (i.e., states of complete knowledge) as a special case, proving to be very flexible. In the present PartÂ II, we describe the DLVK planning system, which implements K on top of the disjunctive logic programming system DLV. This novel planning system allows for solving hard planning problems, including secure planning under incomplete initial states (often called conformant planning in the literature), which cannot be solved at all by other logic-based planning systems such as traditional satisfiability planners. We present a detailed comparison of the DLVK system to several state-of-the-art conformant planning systems, both at the level of system features and on benchmark problems. Our results indicate that, thanks to the power of knowledge-state problem encoding, the DLVK system is competitive even with special purpose conformant planning systems, and it often supplies a more natural and simple representation of the planning problems.}
}
@article{GEBSER201252,
title = {Conflict-driven answer set solving: From theory to practice},
journal = {Artificial Intelligence},
volume = {187-188},
pages = {52-89},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000409},
author = {Martin Gebser and Benjamin Kaufmann and Torsten Schaub},
keywords = {Answer set programming, Logic programming, Nonmonotonic reasoning},
abstract = {We introduce an approach to computing answer sets of logic programs, based on concepts successfully applied in Satisfiability (SAT) checking. The idea is to view inferences in Answer Set Programming (ASP) as unit propagation on nogoods. This provides us with a uniform constraint-based framework capturing diverse inferences encountered in ASP solving. Moreover, our approach allows us to apply advanced solving techniques from the area of SAT. As a result, we present the first full-fledged algorithmic framework for native conflict-driven ASP solving. Our approach is implemented in the ASP solver clasp that has demonstrated its competitiveness and versatility by winning first places at various solver contests.}
}
@article{HERNANDEZORALLO20101508,
title = {Measuring universal intelligence: Towards an anytime intelligence test},
journal = {Artificial Intelligence},
volume = {174},
number = {18},
pages = {1508-1539},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001554},
author = {JosÃ© HernÃ¡ndez-Orallo and David L. Dowe},
keywords = {Measurement of intelligence, Artificial intelligence, Psychometrics, Algorithmic information theory, Kolmogorov complexity, Algorithmic probability, Turing test, Universal intelligence, Computerized adaptive testing, Compression, Inductive inference, Prediction, Minimum Message Length (MML), Reinforcement learning},
abstract = {In this paper, we develop the idea of a universal anytime intelligence test. The meaning of the terms â€œuniversalâ€ and â€œanytimeâ€ is manifold here: the test should be able to measure the intelligence of any biological or artificial system that exists at this time or in the future. It should also be able to evaluate both inept and brilliant systems (any intelligence level) as well as very slow to very fast systems (any time scale). Also, the test may be interrupted at any time, producing an approximation to the intelligence score, in such a way that the more time is left for the test, the better the assessment will be. In order to do this, our test proposal is based on previous works on the measurement of machine intelligence based on Kolmogorov complexity and universal distributions, which were developed in the late 1990s (C-tests and compression-enhanced Turing tests). It is also based on the more recent idea of measuring intelligence through dynamic/interactive tests held against a universal distribution of environments. We discuss some of these tests and highlight their limitations since we want to construct a test that is both general and practical. Consequently, we introduce many new ideas that develop early â€œcompression testsâ€ and the more recent definition of â€œuniversal intelligenceâ€ in order to design new â€œuniversal intelligence testsâ€, where a feasible implementation has been a design requirement. One of these tests is the â€œanytime intelligence testâ€, which adapts to the examinee's level of intelligence in order to obtain an intelligence score within a limited time.}
}
@article{FAGES2014116,
title = {Filtering AtMostNValue with difference constraints: Application to the shift minimisation personnel task scheduling problem},
journal = {Artificial Intelligence},
volume = {212},
pages = {116-133},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000423},
author = {Jean-Guillaume Fages and Tanguy LapÃ¨gue},
keywords = {Constraint-Programming, Global constraints, , Shift Minimisation Personnel Task Scheduling Problem},
abstract = {The problem of minimising the number of distinct values among a set of variables subject to difference constraints occurs in many real-life contexts. This is the case of the Shift Minimisation Personnel Task Scheduling Problem, introduced by Krishnamoorthy et al., which is used as a case study all along this paper. Constraint-Programming enables to formulate this problem easily, through several AllDifferent constraints and a single AtMostNValue constraint. However, the independence of these constraints results in a poor lower bounding, hence a difficulty to prove optimality. This paper introduces a formalism to describe a family of propagators for AtMostNValue. In particular, we provide simple but significant improvement of the state-of-the-art AtMostNValue propagator of BessiÃ¨re et al., to filter the conjunction of an AtMostNValue constraint and disequalities. In addition, we provide an original search strategy which relies on constraint reification. Extensive experiments show that our contribution significantly improves a straightforward model, so that it competes with the best known approaches from Operational Research.}
}
@article{BRANDT2009221,
title = {Ranking games},
journal = {Artificial Intelligence},
volume = {173},
number = {2},
pages = {221-239},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001446},
author = {Felix Brandt and Felix Fischer and Paul Harrenstein and Yoav Shoham},
keywords = {Multi-agent systems, Game theory, Strict competitiveness, -player games, Solution concepts, Computational complexity},
abstract = {The outcomes of many strategic situations such as parlor games or competitive economic scenarios are rankings of the participants, with higher ranks generally at least as desirable as lower ranks. Here we define ranking games as a class of n-player normal-form games with a payoff structure reflecting the players' von Neumannâ€“Morgenstern preferences over their individual ranks. We investigate the computational complexity of a variety of common game-theoretic solution concepts in ranking games and deliver hardness results for iterated weak dominance and mixed Nash equilibrium when there are more than two players, and for pure Nash equilibrium when the number of players is unbounded but the game is described succinctly. This dashes hope that multi-player ranking games can be solved efficiently, despite their profound structural restrictions. Based on these findings, we provide matching upper and lower bounds for three comparative ratios, each of which relates two different solution concepts: the price of cautiousness, the mediation value, and the enforcement value.}
}
@article{ZUCKERMAN2009392,
title = {Algorithms for the coalitional manipulation problem},
journal = {Artificial Intelligence},
volume = {173},
number = {2},
pages = {392-412},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001963},
author = {Michael Zuckerman and Ariel D. Procaccia and Jeffrey S. Rosenschein},
keywords = {Computational social choice, Voting, Manipulation, Computational complexity},
abstract = {We investigate the problem of coalitional manipulation in elections, which is known to be hard in a variety of voting rules. We put forward efficient algorithms for the problem in Borda, Maximin and Plurality with Runoff, and analyze their windows of error. Specifically, given an instance on which an algorithm fails, we bound the additional power the manipulators need in order to succeed. We finally discuss the implications of our results with respect to the popular approach of employing computational hardness to preclude manipulation.}
}
@article{ZHOU201758,
title = {A progression semantics for first-order logic programs},
journal = {Artificial Intelligence},
volume = {250},
pages = {58-79},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S000437021730070X},
author = {Yi Zhou and Yan Zhang},
keywords = {Logic programming, Stable model, Progression, First-order},
abstract = {In this paper, we propose a progression semantics for first-order normal logic programs, and show that it is equivalent to the well-known stable model (answer set) semantics. The progressional definition sheds new insights into Answer Set Programming (ASP), for instance, its relationships to Datalog, First-Order Logic (FOL) and Satisfiability Modulo Theories (SMT). As an example, we extend the notion of boundedness in Datalog for ASP, and show that it coincides with the notions of recursion-freeness and loop-freeness under program equivalence. In addition, we prove that boundedness precisely captures first-order definability for normal logic programs on arbitrary structures. Finally, we show that the progressional definition suggests an alternative translation from ASP to SMT, which yields a new way of implementing first-order ASP.}
}
@article{REBOLLEDO2006667,
title = {Rough intervalsâ€”enhancing intervals for qualitative modeling of technical systems},
journal = {Artificial Intelligence},
volume = {170},
number = {8},
pages = {667-685},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000208},
author = {M. Rebolledo},
keywords = {Rough set, Qualitative modeling, Qualitative reasoning, Interval-based knowledge representation, Vagueness and uncertainty management},
abstract = {The success of model-based industrial applications generally depends on how exactly models reproduce the behavior of the real system that they represent. However, the complexity of industrial systems makes the construction of accurate models difficult. An alternative is the qualitative description of process states, for example by means of the discretization of continuous variable spaces in intervals. In order to reach the required precision in the modeling of complex dynamic systems, interval-based representations usually produce qualitative models, which are sometimes too large for practical use. The approach introduced in this paper incorporates vague and uncertain information based on principles of the Rough Set Theory as a way of enhancing the information contents in interval-based qualitative models. The resulting models are more compact and precise than ordinary qualitative models.}
}
@article{TRUSZCZYNSKI20101285,
title = {Reducts of propositional theories, satisfiability relations, and generalizations of semantics of logic programs},
journal = {Artificial Intelligence},
volume = {174},
number = {16},
pages = {1285-1306},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001426},
author = {Miroslaw TruszczyÅ„ski},
keywords = {Logic programming, Stable models, Supported models, Reducts, Logic HT},
abstract = {Over the years, the stable-model semantics has gained a position of the correct (two-valued) interpretation of default negation in programs. However, for programs with aggregates (constraints), the stable-model semantics, in its broadly accepted generalization stemming from the work by Pearce, Ferraris and Lifschitz, has a competitor: the semantics proposed by Faber, Leone and Pfeifer, which seems to be essentially different. Our goal is to explain the relationship between the two semantics. Pearce, Ferraris and Lifschitz's extension of the stable-model semantics is best viewed in the setting of arbitrary propositional theories. We propose here an extension of the Faberâ€“Leoneâ€“Pfeifer semantics, or FLP semantics, for short, to the full propositional language, which reveals both common threads and differences between the FLP and stable-model semantics. We use our characterizations of FLP-stable models to derive corresponding results on strong equivalence and on normal forms of theories under the FLP semantics. We apply a similar approach to define supported models for arbitrary propositional theories, and to study their properties.}
}
@article{GONZALES20111153,
title = {Decision making with multiple objectives using GAI networks},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1153-1179},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002092},
author = {C. Gonzales and P. Perny and J.Ph. Dubus},
keywords = {Graphical models, GAI decomposable utility, Preference representation, Multiobjective optimization, Multiagent decision making, Compromise search, Fairness},
abstract = {This paper deals with preference representation on combinatorial domains and preference-based recommendation in the context of multicriteria or multiagent decision making. The alternatives of the decision problem are seen as elements of a product set of attributes and preferences over solutions are represented by generalized additive decomposable (GAI) utility functions modeling individual preferences or criteria. Thanks to decomposability, utility vectors attached to solutions can be compiled into a graphical structure closely related to junction trees, the so-called GAI network. Using this structure, we present preference-based search algorithms for multicriteria or multiagent decision making. Although such models are often non-decomposable over attributes, we actually show that GAI networks are still useful to determine the most preferred alternatives provided preferences are compatible with Pareto dominance. We first present two algorithms for the determination of Pareto-optimal elements. Then the second of these algorithms is adapted so as to directly focus on the preferred solutions. We also provide results of numerical tests showing the practical efficiency of our procedures in various contexts such as compromise search and fair optimization in multicriteria or multiagent problems.}
}
@article{GRANT2005163,
title = {A logic-based model of intention formation and action for multi-agent subcontracting},
journal = {Artificial Intelligence},
volume = {163},
number = {2},
pages = {163-201},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204001924},
author = {John Grant and Sarit Kraus and Donald Perlis},
keywords = {Intentions, Subcontracting, Cooperative agents, Syntactic logic, Minimal model semantics},
abstract = {We present a formalism for representing the formation of intentions by agents engaged in cooperative activity. We use a syntactic approach presenting a formal logical calculus that can be regarded as a meta-logic that describes the reasoning and activities of the agents. Our central focus is on the evolving intentions of agents over time, and the conditions under which an agent can adopt and maintain an intention. In particular, the reasoning time and the time taken to subcontract are modeled explicitly in the logic. We axiomatize the concept of agent interactions in the meta-language, show that the meta-theory is consistent and describe the unique intended model of the meta-theory. In this context we deal both with subcontracting between agents and the presence of multiple recipes, that is, multiple ways of accomplishing tasks. We show that under various initial conditions and known facts about agent beliefs and abilities, the meta-theory representation yields good results.}
}
@article{MCDERMOTT20071183,
title = {Level-headed},
journal = {Artificial Intelligence},
volume = {171},
number = {18},
pages = {1183-1186},
year = {2007},
note = {Special Review Issue},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001488},
author = {Drew McDermott},
keywords = {Speculation, Methodology, Natural language},
abstract = {I don't believe that human-level intelligence is a well defined goal. As the cognitive-science community learns more about thinking and computation, the mileposts will keep changing in ways that we can't predict, as will the esteem we assign to past accomplishments. It would be fun to have a computer that could solve brain teasers as well as the average scientist, but focusing on such things, besides being parochial, overlooks the crucial role language plays in everything humans do, a role we understand hardly at all on a computational level. I am optimistic that we will eventually figure language out, but not without new ideas. Plus, when we can talk to machines, will we understand each other?}
}
@article{GRAFF20101254,
title = {Practical performance models of algorithms in evolutionary program induction and other domains},
journal = {Artificial Intelligence},
volume = {174},
number = {15},
pages = {1254-1276},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S000437021000127X},
author = {Mario Graff and Riccardo Poli},
keywords = {Evolution algorithms, Program induction, Performance prediction, Algorithm taxonomies, Algorithm selection problem},
abstract = {Evolutionary computation techniques have seen a considerable popularity as problem solving and optimisation tools in recent years. Theoreticians have developed a variety of both exact and approximate models for evolutionary program induction algorithms. However, these models are often criticised for being only applicable to simplistic problems or algorithms with unrealistic parameters. In this paper, we start rectifying this situation in relation to what matters the most to practitioners and users of program induction systems: performance. That is, we introduce a simple and practical model for the performance of program-induction algorithms. To test our approach, we consider two important classes of problems â€” symbolic regression and Boolean function induction â€” and we model different versions of genetic programming, gene expression programming and stochastic iterated hill climbing in program space. We illustrate the generality of our technique by also accurately modelling the performance of a training algorithm for artificial neural networks and two heuristics for the off-line bin packing problem. We show that our models, besides performing accurate predictions, can help in the analysis and comparison of different algorithms and/or algorithms with different parameters setting. We illustrate this via the automatic construction of a taxonomy for the stochastic program-induction algorithms considered in this study. The taxonomy reveals important features of these algorithms from the performance point of view, which are not detected by ordinary experimentation.}
}
@article{SRIVASTAVA20121,
title = {Applicability conditions for plans with loops: Computability results and algorithms},
journal = {Artificial Intelligence},
volume = {191-192},
pages = {1-19},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000914},
author = {Siddharth Srivastava and Neil Immerman and Shlomo Zilberstein},
keywords = {Automated planning, Plans with loops, Plan verification, Reachability in abacus programs, Generalized planning},
abstract = {The utility of including loops in plans has been long recognized by the planning community. Loops in a plan help increase both its applicability and the compactness of its representation. However, progress in finding such plans has been limited largely due to lack of methods for reasoning about the correctness and safety properties of loops of actions. We present novel algorithms for determining the applicability and progress made by a general class of loops of actions. These methods can be used for directing the search for plans with loops towards greater applicability while guaranteeing termination, as well as in post-processing of computed plans to precisely characterize their applicability. Experimental results demonstrate the efficiency of these algorithms. We also discuss the factors which can make the problem of determining applicability conditions for plans with loops incomputable.}
}
@article{VATCHEVA2006472,
title = {Experiment selection for the discrimination of semi-quantitative models of dynamical systems},
journal = {Artificial Intelligence},
volume = {170},
number = {4},
pages = {472-506},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205002092},
author = {Ivayla Vatcheva and Hidde {de Jong} and Olivier Bernard and Nicolaas J.I. Mars},
keywords = {Qualitative and semi-quantitative modeling and simulation, Model discrimination, Information theory, Population biology, Computer-supported modeling},
abstract = {Modeling an experimental system often results in a number of alternative models that are all justified by the available experimental data. To discriminate among these models, additional experiments are needed. Existing methods for the selection of discriminatory experiments in statistics and in artificial intelligence are often based on an entropy criterion, the so-called information increment. A limitation of these methods is that they are not well-adapted to discriminating models of dynamical systems under conditions of limited measurability. Moreover, there are no generic procedures for computing the information increment of an experiment when the models are qualitative or semi-quantitative. This has motivated the development of a method for the selection of experiments to discriminate among semi-quantitative models of dynamical systems. The method has been implemented on top of existing implementations of the qualitative and semi-quantitative simulation techniques QSIM, Q2, and Q3. The applicability of the method to real-world problems is illustrated by means of an example in population biology: the discrimination of four competing models of the growth of phytoplankton in a bioreactor. The models have traditionally been considered equivalent for all practical purposes. Using our model discrimination approach and experimental data we show, however, that two of them are superior for describing phytoplankton growth under a wide range of experimental conditions.}
}
@article{MANCINI2007985,
title = {Exploiting functional dependencies in declarative problem specifications},
journal = {Artificial Intelligence},
volume = {171},
number = {16},
pages = {985-1010},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000926},
author = {Toni Mancini and Marco Cadoli},
keywords = {Modeling, Reformulation, Second-order logic, Constraint satisfaction problems},
abstract = {In this paper we tackle the issue of the automatic recognition of functional dependencies among guessed predicates in constraint problem specifications. Functional dependencies arise frequently in pure declarative specifications, because of the intermediate results that need to be computed in order to express some of the constraints, or due to precise modeling choices, e.g., to provide multiple viewpoints of the search space in order to increase constraint propagation. In either way, the recognition of dependencies greatly helps solvers, allowing them to avoid spending search on unfruitful branches, while maintaining the highest degree of declarativeness. By modeling constraint problem specifications as second-order formulae, we provide a characterization of functional dependencies in terms of semantic properties of first-order ones, and prove undecidability of the problem of their recognition. Despite such negative result, we advocate the (in many cases effective) possibility of using automated tools to mechanize this task. Additionally, we show how suitable search procedures can be automatically synthesized in order to exploit recognized dependencies. We present opl examples of various problems, taken from bio-informatics, planning and resource allocation, and show how in many cases opl greatly benefits from the addition of such search procedures. Moreover, we also give evidence that writing sophisticated ad-hoc search procedures that handle dependencies exploiting the peculiarities of the particular problem is a very difficult and error-prone task which in many cases does not seem to pay-off.}
}
@article{ZHANG2009437,
title = {Computing the fault tolerance of multi-agent deployment},
journal = {Artificial Intelligence},
volume = {173},
number = {3},
pages = {437-465},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001951},
author = {Yingqian Zhang and Efrat Manisterski and Sarit Kraus and V.S. Subrahmanian and David Peleg},
keywords = {Multi-agent deployment, Fault tolerance, Algorithms, Replication},
abstract = {A deployment of a multi-agent system on a network refers to the placement of one or more copies of each agent on network hosts, in such a manner that the memory constraints of each node are satisfied. Finding the deployment that is most likely to tolerate faults (i.e. have at least one copy of each agent functioning and in communication with other agents) is a challenge. In this paper, we address the problem of finding the probability of survival of a deployment (i.e. the probability that a deployment will tolerate faults), under the assumption that node failures are independent. We show that the problem of computing the survival probability of a deployment is at least NP-hard. Moreover, it is hard to approximate. We produce two algorithms to accurately compute the probability of survival of a deploymentâ€”these algorithms are expectedly exponential. We also produce five heuristic algorithms to estimate survival probabilitiesâ€”these algorithms work in acceptable time frames. We report on a detailed set of experiments to determine the conditions under which some of these algorithms perform better than the others.}
}
@article{BIBEL1998183,
title = {Let's plan it deductively!},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {183-208},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00064-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000642},
author = {W. Bibel},
keywords = {Reasoning about actions and causality, Planning, Automated deduction, Transition logic, Linear connection method, Frame problem, Ramification, Qualification},
abstract = {The paper describes a transition logic, TL, and a deductive formalism for it. It shows how various important aspects (such as ramification, qualification, specificity, simultaneity, indeterminism etc.) involved in planning (or in reasoning about action and causality for that matter) can be modelled in TL in a rather natural way. (The deductive formalism for) TL extends the linear connection method proposed earlier by the author by embedding the latter into classical logic, so that classical and resource-sensitive reasoning coexist within TL. The attraction of a logical and deductive approach to planning is emphasized and the state of automated deduction briefly described.}
}
@article{DEGIACOMO2000109,
title = {ConGolog, a concurrent programming language based on the situation calculus},
journal = {Artificial Intelligence},
volume = {121},
number = {1},
pages = {109-169},
year = {2000},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(00)00031-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437020000031X},
author = {Giuseppe {De Giacomo} and Yves LespÃ©rance and Hector J. Levesque},
keywords = {Cognitive robotics, Reasoning about actions, Situation calculus, Semantics of programs, Concurrency},
abstract = {As an alternative to planning, an approach to high-level agent control based on concurrent program execution is considered. A formal definition in the situation calculus of such a programming language is presented and illustrated with some examples. The language includes facilities for prioritizing the execution of concurrent processes, interrupting the execution when certain conditions become true, and dealing with exogenous actions. The language differs from other procedural formalisms for concurrency in that the initial state can be incompletely specified and the primitive actions can be user-defined by axioms in the situation calculus. Some mathematical properties of the language are proven, for instance, that the proposed semantics is equivalent to that given earlier for the portion of the language without concurrency.}
}
@article{SANDHOLM2007382,
title = {Perspectives on multiagent learning},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {382-391},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000525},
author = {Tuomas Sandholm},
keywords = {Multiagent learning, Learning in games, Reinforcement learning, Game theory},
abstract = {I lay out a slight refinement of Shoham et al.'s taxonomy of agendas that I consider sensible for multiagent learning (MAL) research. It is not intended to be rigid: senseless work can be done within these agendas and additional sensible agendas may arise. Within each agenda, I identify issues and suggest directions. In the computational agenda, direct algorithms are often more efficient, but MAL plays a role especially when the rules of the game are unknown or direct algorithms are not known for the class of games. In the descriptive agenda, more emphasis should be placed on establishing what classes of learning rules actually model learning by multiple humans or animals. Also, the agenda is, in a way, circular. This has a positive side too: it can be used to verify the learning models. In the prescriptive agendas, the desiderata need to be made clear and should guide the design of MAL algorithms. The algorithms need not mimic humans' or animals' learning. I discuss some worthy desiderata; some from the literature do not seem well motivated. The learning problem is interesting both in cooperative and noncooperative settings, but the concerns are quite different. For many, if not most, noncooperative settings, future work should increasingly consider the learning itself strategically. Lower bounds cut across the agendas. They can be derived on the computational complexity and on the number of interactions needed.}
}
@article{VANBENTHEM2011428,
title = {McCarthy variations in a modal key},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {428-439},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000500},
author = {Johan {van Benthem}},
keywords = {Circumscription, Fixed-point logic, Structural rules, Belief change, Situation Calculus, Temporal logic, Regression method, Dynamic epistemic logic},
abstract = {We take a fresh look at some major strands in John McCarthy's work from a logician's perspective. First, we re-analyze circumscription in dynamic logics of belief change under hard and soft information. Next, we re-analyze the regression method in the Situation Calculus in terms of update axioms for dynamicâ€“epistemic temporal logics. Finally, we draw some general methodological comparisons between â€˜Logical AIâ€™ and practices in modal logic, pointing at some creative tensions.}
}
@article{TANG2010749,
title = {Designing competitions between teams of individuals},
journal = {Artificial Intelligence},
volume = {174},
number = {11},
pages = {749-766},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S000437021000069X},
author = {Pingzhong Tang and Yoav Shoham and Fangzhen Lin},
keywords = {Team competition, Mechanism design, Truthfulness, Moral hazard, Dominant strategy implementation},
abstract = {We consider a setting with two teams, each with a number of players. There is an ordering of all players that determines outcome of matches between any two players from the opposing teams. Neither the teams nor the competition designer know this ordering, but each team knows the derived ordering of strengths among its own players. Each team announces an ordering of its players, and the competition designer schedules matches according to the announced orderings. This setting in general allows for two types of manipulations by a team: Misreporting the strength ordering (lack of truthfulness), and deliberately losing a match (moral hazard). We prove necessary and sufficient conditions for a set of competition rules to have the properties that truthful reporting are dominant strategies and maximum effort in matches are Nash equilibrium strategies, and certain fairness conditions are met. Extensions of the original setting are discussed.}
}
@article{GIANG2005137,
title = {Decision making on the sole basis of statistical likelihood},
journal = {Artificial Intelligence},
volume = {165},
number = {2},
pages = {137-163},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000500},
author = {Phan H. Giang and Prakash P. Shenoy},
keywords = {Decision theory, Likelihood, Statistical inference, Ambiguity attitude},
abstract = {This paper presents a new axiomatic decision theory for choice under uncertainty. Unlike Bayesian decision theory where uncertainty is represented by a probability function, in our theory, uncertainty is given in the form of a likelihood function extracted from statistical evidence. The likelihood principle in statistics stipulates that likelihood functions encode all relevant information obtainable from experimental data. In particular, we do not assume any knowledge of prior probabilities. Consequently, a Bayesian conversion of likelihoods to posterior probabilities is not possible in our setting. We make an assumption that defines the likelihood of a set of hypotheses as the maximum likelihood over the elements of the set. We justify an axiomatic system similar to that used by von Neumann and Morgenstern for choice under risk. Our main result is a representation theorem using the new concept of binary utility. We also discuss how ambiguity attitudes are handled. Applied to the statistical inference problem, our theory suggests a novel solution. The results in this paper could be useful for probabilistic model selection.}
}
@article{YOUNG2007429,
title = {The possible and the impossible in multi-agent learning},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {429-433},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000367},
author = {H. Peyton Young},
keywords = {Equilibrium, Learning, Dynamics},
abstract = {This paper surveys recent work on learning in games and delineates the boundary between forms of learning that lead to Nash equilibrium, and forms that lead to weaker notions of equilibrium or to none at all.}
}
@article{SCHOCKAERT20111815,
title = {Solving conflicts in information merging by a flexible interpretation of atomic propositions},
journal = {Artificial Intelligence},
volume = {175},
number = {11},
pages = {1815-1855},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000531},
author = {Steven Schockaert and Henri Prade},
keywords = {Information fusion, Similarity-based reasoning, Prioritized knowledge bases, Possibilistic logic, Penalty logic},
abstract = {Although many techniques for merging conflicting propositional knowledge bases have already been proposed, most existing work is based on the idea that inconsistency results from the presence of incorrect pieces of information, which should be identified and removed. In contrast, we take the view in this paper that conflicts are often caused by statements that are inaccurate rather than completely false, suggesting to restore consistency by interpreting certain statements in a flexible way, rather than ignoring them completely. In accordance with this view, we propose a novel approach to merging which exploits extra-logical background information about the semantic relatedness of atomic propositions. Several merging operators are presented, which are based on different formalizations of this background knowledge, ranging from purely qualitative approaches, related to possibilistic logic, to quantitative approaches with a probabilistic flavor. Both syntactic and semantic characterizations are provided for each merging operator, and the computational complexity is analyzed.}
}
@article{VILHELM200067,
title = {Think!: A unified numericalâ€“symbolic knowledge representation scheme and reasoning system},
journal = {Artificial Intelligence},
volume = {116},
number = {1},
pages = {67-85},
year = {2000},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00095-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000958},
author = {Christian Vilhelm and Pierre Ravaux and Daniel Calvelo and Alexandre Jaborska and Marie-Christine Chambrin and Michel Boniface},
keywords = {Knowledge representation, Symbolic reasoning, Numeric reasoning, Truth propagation},
abstract = {More and more applications of artificial intelligence technologies are made in biomedical software and equipment. These applications are multiple: intelligent alarms, intelligent monitoring, diagnosis support,â€¦. Several different knowledge representation schemes are already in use: decision trees, first-order logic expert systems, calculations (mathematical modeling), trained neural network simulations,â€¦. All these techniques have their own preferred field of application, and they do not overlap. Building a complete diagnosis support tool would require the use of several of these techniques. The problem is therefore communication between these very different systems and the complexity of the composite result. This paper describes the Think! formalism: a unified symbolic-connectionist representation scheme which tries to subsume some of the precited formalisms. Being able to integrate these knowledge representation schemes in a single model enables us to use existing knowledge bases and existing knowledge extraction techniques to make them communicate and work together.}
}
@article{AMIR200549,
title = {Partition-based logical reasoning for first-order and propositional theories},
journal = {Artificial Intelligence},
volume = {162},
number = {1},
pages = {49-88},
year = {2005},
note = {Reformulation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204001778},
author = {Eyal Amir and Sheila McIlraith},
keywords = {Reasoning with structure, Theorem proving, First-order logic, SAT, Tree decomposition, Graphical models, Parallel computation, Distributed computation},
abstract = {In this paper we show how tree decomposition can be applied to reasoning with first-order and propositional logic theories. Our motivation is two-fold. First, we are concerned with how to reason effectively with multiple knowledge bases that have overlap in content. Second, we are concerned with improving the efficiency of reasoning over a set of logical axioms by partitioning the set with respect to some detectable structure, and reasoning over individual partitions either locally or in a distributed fashion. To this end, we provide algorithms for partitioning and reasoning with related logical axioms in propositional and first-order logic. Many of the reasoning algorithms we present are based on the idea of passing messages between partitions. We present algorithms for both forward (data-driven) and backward (query-driven) message passing. Different partitions may have different associated reasoning procedures. We characterize a class of reasoning procedures that ensures completeness and soundness of our message-passing algorithms. We further provide a specialized algorithm for propositional satisfiability checking with partitions. Craig's interpolation theorem serves as a key to proving soundness and completeness of all of these algorithms. An analysis of these algorithms emphasizes parameters of the partitionings that influence the efficiency of computation. We provide a greedy algorithm that automatically decomposes a set of logical axioms into partitions, following this analysis.}
}
@article{PROCACCIA20091133,
title = {The learnability of voting rules},
journal = {Artificial Intelligence},
volume = {173},
number = {12},
pages = {1133-1149},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000460},
author = {Ariel D. Procaccia and Aviv Zohar and Yoni Peleg and Jeffrey S. Rosenschein},
keywords = {Computational social choice, Computational learning theory, Multiagent systems},
abstract = {Scoring rules and voting trees are two broad and concisely-representable classes of voting rules; scoring rules award points to alternatives according to their position in the preferences of the voters, while voting trees are iterative procedures that select an alternative based on pairwise comparisons. In this paper, we investigate the PAC-learnability of these classes of rules. We demonstrate that the class of scoring rules, as functions from preferences into alternatives, is efficiently learnable in the PAC model. With respect to voting trees, while in general a learning algorithm would require an exponential number of samples, we show that if the number of leaves is polynomial in the size of the set of alternatives, then a polynomial training set suffices. We apply these results in an emerging theory: automated design of voting rules by learning.}
}
@article{KRAUS19981,
title = {Reaching agreements through argumentation: a logical model and implementation},
journal = {Artificial Intelligence},
volume = {104},
number = {1},
pages = {1-69},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00078-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000782},
author = {Sarit Kraus and Katia Sycara and Amir Evenchik},
keywords = {Automated negotiation, Argumentation, BDI model},
abstract = {In a multi-agent environment, where self-motivated agents try to pursue their own goals, cooperation cannot be taken for granted. Cooperation must be planned for and achieved through communication and negotiation. We present a logical model of the mental states of the agents based on a representation of their beliefs, desires, intentions, and goals. We present argumentation as an iterative process emerging from exchanges among agents to persuade each other and bring about a change in intentions. We look at argumentation as a mechanism for achieving cooperation and agreements. Using categories identified from human multi-agent negotiation, we demonstrate how the logic can be used to specify argument formulation and evaluation. We also illustrate how the developed logic can be used to describe different types of agents. Furthermore, we present a general Automated Negotiation Agent which we implemented, based on the logical model. Using this system, a user can analyze and explore different methods to negotiate and argue in a noncooperative environment where no centralized mechanism for coordination exists. The development of negotiating agents in the framework of the Automated Negotiation Agent is illustrated with an example where the agents plan, act, and resolve conflicts via negotiation in a Blocks World environment.}
}
@article{SLANEY2001119,
title = {Blocks World revisited},
journal = {Artificial Intelligence},
volume = {125},
number = {1},
pages = {119-153},
year = {2001},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(00)00079-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370200000795},
author = {John Slaney and Sylvie ThiÃ©baux},
keywords = {Blocks World, Planning benchmarks, Random/hard problems, Approximation algorithms},
abstract = {Contemporary AI shows a healthy trend away from artificial problems towards real-world applications. Less healthy, however, is the fashionable disparagement of â€œtoyâ€ domains: when properly approached, these domains can at the very least support meaningful systematic experiments, and allow features relevant to many kinds of reasoning to be abstracted and studied. A major reason why they have fallen into disrepute is that superficial understanding of them has resulted in poor experimental methodology and consequent failure to extract useful information. This paper presents a sustained investigation of one such toy: the (in)famous Blocks World planning problem, and provides the level of understanding required for its effective use as a benchmark. Our results include methods for generating random problems for systematic experimentation, the best domain-specific planning algorithms against which AI planners can be compared, and observations establishing the average plan quality of near-optimal methods. We also study the distribution of hard/easy instances, and identify the structure that AI planners must be able to exploit in order to approach Blocks World successfully.}
}
@article{NAU20101323,
title = {When is it better not to look ahead?},
journal = {Artificial Intelligence},
volume = {174},
number = {16},
pages = {1323-1338},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001402},
author = {Dana S. Nau and Mitja LuÅ¡trek and Austin Parker and Ivan Bratko and MatjaÅ¾ Gams},
keywords = {Lookahead pathology, Minimax, Game-tree search},
abstract = {In situations where one needs to make a sequence of decisions, it is often believed that looking ahead will help produce better decisions. However, it was shown 30 years ago that there are â€œpathologicalâ€ situations in which looking ahead is counterproductive. Two long-standing open questions are (a) what combinations of factors have the biggest influence on whether lookahead pathology occurs, and (b) whether it occurs in real-world decision-making. This paper includes simulation results for several synthetic game-tree models, and experimental results for three well-known board games: two chess endgames, kalah (with some modifications to facilitate experimentation), and the 8-puzzle. The simulations show the interplay between lookahead pathology and several factors that affect it; and the experiments confirm the trends predicted by the simulation models. The experiments also show that lookahead pathology is more common than has been thought: all three games contain situations where it occurs.}
}
@article{VANHENTENRYCK1998209,
title = {A gentle introduction to Numerica},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {209-235},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00053-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000538},
author = {Pascal {Van Hentenryck}},
abstract = {Numerica is a modeling language for stating and solving global optimization problems. It makes it possible to express these problems in a notation close to the way these problems are stated in textbooks or scientific papers. In addition, the constraint-solving algorithm of Numerica, which combines techniques from numerical analysis and artificial intelligence, provides many guarantees about correctness, convergence, and completeness. This paper is a gentle introduction to Numerica. It highlights some of the main difficulties of global optimization and illustrates the functionality of Numerica by contrasting it to traditional methods. It also presents the essence of the constraint-solving algorithm of Numerica in a novel, high-level, way.}
}
@article{VANKRIEKEN2022103602,
title = {Analyzing Differentiable Fuzzy Logic Operators},
journal = {Artificial Intelligence},
volume = {302},
pages = {103602},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103602},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221001533},
author = {Emile {van Krieken} and Erman Acar and Frank {van Harmelen}},
keywords = {Fuzzy logic, Neural-symbolic AI, Learning with constraints},
abstract = {The AI community is increasingly putting its attention towards combining symbolic and neural approaches, as it is often argued that the strengths and weaknesses of these approaches are complementary. One recent trend in the literature is weakly supervised learning techniques that employ operators from fuzzy logics. In particular, these use prior background knowledge described in such logics to help the training of a neural network from unlabeled and noisy data. By interpreting logical symbols using neural networks, this background knowledge can be added to regular loss functions, hence making reasoning a part of learning. We study, both formally and empirically, how a large collection of logical operators from the fuzzy logic literature behave in a differentiable learning setting. We find that many of these operators, including some of the most well-known, are highly unsuitable in this setting. A further finding concerns the treatment of implication in these fuzzy logics, and shows a strong imbalance between gradients driven by the antecedent and the consequent of the implication. Furthermore, we introduce a new family of fuzzy implications (called sigmoidal implications) to tackle this phenomenon. Finally, we empirically show that it is possible to use Differentiable Fuzzy Logics for semi-supervised learning, and compare how different operators behave in practice. We find that, to achieve the largest performance improvement over a supervised baseline, we have to resort to non-standard combinations of logical operators which perform well in learning, but no longer satisfy the usual logical laws.}
}
@article{LAWRY20091539,
title = {Uncertainty modelling for vague concepts: A prototype theory approach},
journal = {Artificial Intelligence},
volume = {173},
number = {18},
pages = {1539-1558},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000903},
author = {Jonathan Lawry and Yongchuan Tang},
keywords = {Epistemic vagueness, Prototype theory, Label semantics, Random sets},
abstract = {An epistemic model of the uncertainty associated with vague concepts is introduced. Label semantics theory is proposed as a framework for quantifying an agent's uncertainty concerning what labels are appropriate to describe a given example. An interpretation of label semantics is then proposed which incorporates prototype theory by introducing uncertain thresholds on the distance between elements and prototypes for description labels. This interpretation naturally generates a functional calculus for appropriateness measures. A more general model with distinct threshold variables for different labels is discussed and we show how different kinds of semantic dependence can be captured in this model.}
}
@article{POLI2006953,
title = {Backward-chaining evolutionary algorithms},
journal = {Artificial Intelligence},
volume = {170},
number = {11},
pages = {953-982},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000543},
author = {Riccardo Poli and William B. Langdon},
keywords = {Evolutionary computation, Genetic algorithm, Genetic programming, Efficient search, Backward chaining, Tournament selection},
abstract = {Starting from some simple observations on a popular selection method in Evolutionary Algorithms (EAs)â€”tournament selectionâ€”we highlight a previously-unknown source of inefficiency. This leads us to rethink the order in which operations are performed within EAs, and to suggest an algorithmâ€”the EA with efficient macro-selectionâ€”that avoids the inefficiencies associated with tournament selection. This algorithm has the same expected behaviour as the standard EA but yields considerable savings in terms of fitness evaluations. Since fitness evaluation typically dominates the resources needed to solve any non-trivial problem, these savings translate into a reduction in computer time. Noting the connection between the algorithm and rule-based systems, we then further modify the order of operations in the EA, effectively turning the evolutionary search into an inference process operating in backward-chaining mode. The resulting backward-chaining EA creates and evaluates individuals recursively, backward from the last generation to the first, using depth-first search and backtracking. It is even more powerful than the EA with efficient macro-selection in that it shares all its benefits, but it also provably finds fitter solutions sooner, i.e., it is a faster algorithm. These algorithms can be applied to any form of population based search, any representation, fitness function, crossover and mutation, provided they use tournament selection. We analyse their behaviour and benefits both theoretically, using Markov chain theory and space/time complexity analysis, and empirically, by performing a variety of experiments with standard and back-ward chaining versions of genetic algorithms and genetic programming.}
}
@article{YING2002253,
title = {Lattice-theoretic models of conjectures, hypotheses and consequences},
journal = {Artificial Intelligence},
volume = {139},
number = {2},
pages = {253-267},
year = {2002},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(02)00225-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370202002254},
author = {Mingsheng Ying and Huaiqing Wang},
keywords = {Uncertain reasoning, Conjecture, Hypothesis, Consequence, Orthocomplemented lattice, Residuated lattice},
abstract = {Trillas, Cubillo and CastiÃ±eira [Artificial Intelligence 117 (2000) 255â€“275] defined several interesting operators in orthocomplemented lattices. These operators give a quite general algebraic model for conjectures, consequences and hypotheses. We present some properties of conjectures, consequences and hypotheses in orthocomplemented lattices, which complement or improve the results by Trillas, Cubillo and CastiÃ±eira. Furthermore, we introduce the graded versions of these notions in the setting of residuated lattices and derive some of their properties. These graded notions provide certain mathematical tools for modelling conjectures, consequences and hypotheses in the environment where uncertain and vague information is involved.}
}
@article{WASHIO1997103,
title = {A new approach to quantitative and credible diagnosis for multiple faults of components and sensors},
journal = {Artificial Intelligence},
volume = {91},
number = {1},
pages = {103-130},
year = {1997},
note = {Artificial Intelligence Research in Japan},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00060-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000604},
author = {T. Washio and M. Sakuma and M. Kitamura},
keywords = {Model-based diagnosis, Multiple faults, Minimal over-constrained subset, Causal ordering, Assumptive structural equation, First principle, Function, Process system},
abstract = {Many practical applications of system diagnosis require the credible identification of multiple faults of nonlinear components and sensors in quantitative measures. However, the state of the art of diagnosis technique is considered to be still insufficient to meet these severe requirements. The approach of diagnosis using the traditional linear system identification theory can diagnose the disturbed parameters of a system in detail and evaluate the quantitative amplitude of the disturbance. However, it hardly provides the diagnosis of the multiple faults and the diagnosis of the components having high nonlinearity. On the other hand, some recent model-based diagnosis approaches can diagnose the multiple faults even for highly nonlinear components, though they do not provide the detailed diagnosis of elements indivisibly involved in components and the quantitative amplitudes of the faults. The method proposed in this paper provides an efficient remedy to achieve all of the practical requirements, i.e., the credible, detailed and quantitative diagnosis of multiple faults of nonlinear components and sensors. Our study newly proposes the frameworks of optimal constraints and causal ordering of physical systems. Also, a systematic and strict theory to synthesize these frameworks together with the model-based diagnosis is provided to characterize an optimal consistency checking method in diagnosis and to evaluate quantitative amplitudes of faulty disturbances. First, the detection of faulty behaviors of an objective component is performed based on the quantitative consistency checking between observations and the optimal constraints, called as â€œminimal overconstraintsâ€, consisting of first principles in the components. Second, once if some inconsistencies are detected, a mathematical operation of model-based diagnosis derives the candidates of faulty elements and functions even under multiple fault conditions. Third, the anomalous quantities directly disturbed by the faulty elements are identified systematically based on causal ordering. Furthermore, the quantitative deviations of these quantities are evaluated by using the minimal over-constraints. The performance of the proposed method is demonstrated through an example to diagnose an electric water heater. The ability of this diagnosis has been confirmed for the multiple faults in nonlinear and dynamic systems.}
}
@article{POLICICCHIO201261,
title = {GAMoN: Discovering M-of-N{Â¬,âˆ¨} hypotheses for text classification by a lattice-based Genetic Algorithm},
journal = {Artificial Intelligence},
volume = {191-192},
pages = {61-95},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000896},
author = {Veronica L. Policicchio and Adriana Pietramala and Pasquale Rullo},
abstract = {While there has been a long history of rule-based text classifiers, to the best of our knowledge no M-of-N-based approach for text categorization has so far been proposed. In this paper we argue that M-of-N hypotheses are particularly suitable to model the text classification task because of the so-called â€œfamily resemblanceâ€ metaphor: â€œthe members (i.e., documents) of a family (i.e., category) share some small number of features, yet there is no common feature among all of them. Nevertheless, they resemble each otherâ€. Starting from this conjecture, we provide a sound extension of the M-of-N approach with negation and disjunction, called M-of-N{Â¬,âˆ¨}, which enables to best fit the true structure of the data. Based on a thorough theoretical study, we show that the M-of-N{Â¬,âˆ¨} hypothesis space has two partial orders that form complete lattices. GAMoN is the task-specific Genetic Algorithm (GA) which, by exploiting the lattice-based structure of the hypothesis space, efficiently induces accurate M-of-N{Â¬,âˆ¨} hypotheses. Benchmarking was performed over 13 real-world text data sets, by using four rule induction algorithms: two GAs, namely, BioHEL and OlexGA, and two non-evolutionary algorithms, namely, C4.5 and Ripper. Further, we included in our study linear SVM, as it is reported to be among the best methods for text categorization. Experimental results demonstrate that GAMoN delivers state-of-the-art classification performance, providing a good balance between accuracy and model complexity. Further, they show that GAMoN can scale up to large and realistic real-world domains better than both C4.5 and Ripper.}
}
@article{ZAFFALON20131,
title = {Probability and time},
journal = {Artificial Intelligence},
volume = {198},
pages = {1-51},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000246},
author = {Marco Zaffalon and Enrique Miranda},
keywords = {Temporal reasoning, Imprecise probabilities, Conditioning, Lower previsions, Sets of desirable gambles, Coherence, Conglomerability},
abstract = {Probabilistic reasoning is often attributed a temporal meaning, in which conditioning is regarded as a normative rule to compute future beliefs out of current beliefs and observations. However, the well-established â€˜updating interpretationâ€™ of conditioning is not concerned with beliefs that evolve in time, and in particular with future beliefs. On the other hand, a temporal justification of conditioning was proposed already by De Moivre and Bayes, by requiring that current and future beliefs be consistent. We reconsider the latter approach while dealing with a generalised version of the problem, using a behavioural theory of imprecise probability in the form of coherent lower previsions as well as of coherent sets of desirable gambles, and letting the possibility space be finite or infinite. We obtain that using conditioning is normative, in the imprecise case, only if one establishes future behavioural commitments at the same time of current beliefs. In this case it is also normative that present beliefs be conglomerable, which is a result that touches on a long-term controversy at the foundations of probability. In the remaining case, where one commits to some future behaviour after establishing present beliefs, we characterise the several possibilities to define consistent future assessments; this shows in particular that temporal consistency does not preclude changes of mind. And yet, our analysis does not support that rationality requires consistency in general, even though pursuing consistency makes sense and is useful, at least as a way to guide and evaluate the assessment process. These considerations narrow down in the special case of precise probability, because this formalism cannot distinguish the two different situations illustrated above: it turns out that the only consistent rule is conditioning and moreover that it is not rational to be willing to stick to precise probability while using a rule different from conditioning to compute future beliefs; rationality requires in addition the disintegrability of the present-time probability.}
}
@article{SEN2002179,
title = {Believing others: Pros and cons},
journal = {Artificial Intelligence},
volume = {142},
number = {2},
pages = {179-203},
year = {2002},
note = {International Conference on MultiAgent Systems 2000},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(02)00289-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370202002898},
author = {Sandip Sen},
keywords = {Reciprocity, Agents, Cooperation, Adaptation, Trust, Relationships},
abstract = {In open environments there is no central control over agent behaviors. On the contrary, agents in such systems can be assumed to be primarily driven by self interests. Under the assumption that agents remain in the system for significant time periods, or that the agent composition changes only slowly, we have previously presented a prescriptive strategy for promoting and sustaining cooperation among self-interested agents. The adaptive, probabilistic policy we have prescribed promotes reciprocative cooperation that improves both individual and group performance in the long run. In the short run, however, selfish agents could still exploit reciprocative agents. In this paper, we evaluate the hypothesis that the exploitative tendencies of selfish agents can be effectively curbed if reciprocative agents share their â€œopinionsâ€ of other agents. Since the true nature of agents is not known a priori and is learned from experience, believing others can also pose its own hazards. We provide a learned trust-based evaluation function that is shown to resist both individual and concerted deception on the part of selfish agents in a package delivery domain.}
}
@article{VIRGOLIN2023103840,
title = {On the robustness of sparse counterfactual explanations to adverse perturbations},
journal = {Artificial Intelligence},
volume = {316},
pages = {103840},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103840},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001801},
author = {Marco Virgolin and Saverio Fracaros},
keywords = {Counterfactual explanation, Explainable machine learning, Explainable artificial intelligence, Robustness, Uncertainty},
abstract = {Counterfactual explanations (CEs) are a powerful means for understanding how decisions made by algorithms can be changed. Researchers have proposed a number of desiderata that CEs should meet to be practically useful, such as requiring minimal effort to enact, or complying with causal models. In this paper, we consider the interplay between the desiderata of robustness (i.e., that enacting CEs remains feasible and cost-effective even if adverse events take place) and sparsity (i.e., that CEs require only a subset of the features to be changed). In particular, we study the effect of addressing robustness separately for the features that are recommended to be changed and those that are not. We provide definitions of robustness for sparse CEs that are workable in that they can be incorporated as penalty terms in the loss functions that are used for discovering CEs. To carry out our experiments, we create and release code where five data sets (commonly used in the field of fair and explainable machine learning) have been enriched with feature-specific annotations that can be used to sample meaningful perturbations. Our experiments show that CEs are often not robust and, if adverse perturbations take place (even if not worst-case), the intervention they prescribe may require a much larger cost than anticipated, or even become impossible. However, accounting for robustness in the search process, which can be done rather easily, allows discovering robust CEs systematically. Robust CEs make additional intervention to contrast perturbations much less costly than non-robust CEs. We also find that robustness is easier to achieve for the features to change, posing an important point of consideration for the choice of what counterfactual explanation is best for the user. Our code is available at: https://github.com/marcovirgolin/robust-counterfactuals.}
}
@article{DIEZ19961,
title = {Local conditioning in Bayesian networks},
journal = {Artificial Intelligence},
volume = {87},
number = {1},
pages = {1-20},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00118-2},
url = {https://www.sciencedirect.com/science/article/pii/0004370295001182},
author = {F.J. DÃ­ez},
abstract = {Local conditioning (LC) is an exact algorithm for computing probability in Bayesian networks, developed as an extension of Kim and Pearl's algorithm for singly-connected networks. A list of variables associated to each node guarantees that only the nodes inside a loop are conditioned on the variable which breaks it. The main advantage of this algorithm is that it computes the probability directly on the original network instead of building a cluster tree, and this can save time when debugging a model and when the sparsity of evidence allows a pruning of the network. The algorithm is also advantageous when some families in the network interact through AND/OR gates. A parallel implementation of the algorithm with a processor for each node is possible even in the case of multiply-connected networks.}
}
@article{GAL20122270,
title = {Plan recognition in exploratory domains},
journal = {Artificial Intelligence},
volume = {176},
number = {1},
pages = {2270-2290},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211001093},
author = {YaÊ¼akov Gal and Swapna Reddy and Stuart M. Shieber and Andee Rubin and Barbara J. Grosz},
keywords = {Plan recognition, User modeling},
abstract = {This paper describes a challenging plan recognition problem that arises in environments in which agents engage widely in exploratory behavior, and presents new algorithms for effective plan recognition in such settings. In exploratory domains, agentsÊ¼ actions map onto logs of behavior that include switching between activities, extraneous actions, and mistakes. Flexible pedagogical software, such as the application considered in this paper for statistics education, is a paradigmatic example of such domains, but many other settings exhibit similar characteristics. The paper establishes the task of plan recognition in exploratory domains to be NP-hard and compares several approaches for recognizing plans in these domains, including new heuristic methods that vary the extent to which they employ backtracking, as well as a reduction to constraint-satisfaction problems. The algorithms were empirically evaluated on peopleÊ¼s interaction with flexible, open-ended statistics education software used in schools. Data was collected from adults using the software in a lab setting as well as middle school students using the software in the classroom. The constraint satisfaction approaches were complete, but were an order of magnitude slower than the heuristic approaches. In addition, the heuristic approaches were able to perform within 4% of the constraint satisfaction approaches on student data from the classroom, which reflects the intended user population of the software. These results demonstrate that the heuristic approaches offer a good balance between performance and computation time when recognizing peopleÊ¼s activities in the pedagogical domain of interest.}
}
@article{BODEN1998347,
title = {Creativity and artificial intelligence},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {347-356},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00055-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000551},
author = {Margaret A. Boden},
abstract = {Creativity is a fundamental feature of human intelligence, and a challenge for AI. AI techniques can be used to create new ideas in three ways: by producing novel combinations of familiar ideas; by exploring the potential of conceptual spaces; and by making transformations that enable the generation of previously impossible ideas. AI will have less difficulty in modelling the generation of new ideas than in automating their evaluation.}
}
@article{SHOHAM1997139,
title = {On the emergence of social conventions: modeling, analysis, and simulations},
journal = {Artificial Intelligence},
volume = {94},
number = {1},
pages = {139-166},
year = {1997},
note = {Economic Principles of Multi-Agent Systems},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00028-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000283},
author = {Yoav Shoham and Moshe Tennenholtz},
keywords = {Conventions, Emergent behavior, Coordination, Convergence rate},
abstract = {We define the notion of social conventions in a standard game-theoretic framework, and identify various criteria of consistency of such conventions with the principle of individual rationality. We then investigate the emergence of such conventions in a stochastic setting; we do so within a stylized framework currently popular in economic circles, namely that of stochastic games. This framework comes in several forms; in our setting agents interact with each other through a random process, and accumulate information about the system. As they do so, they continually reevaluate their current choice of strategy in light of the accumulated information. We introduce a simple and natural strategy-selection rule, called highest cumulative reward (HCR). We show a class of games in which HCR guarantees eventual convergence to a rationally acceptable social convention. Most importantly, we investigate the efficiency with which such social conventions are achieved. We give an analytic lower bound on this rate, and then present results about how HCR works out in practice. Specifically, we pick one of the most basic games, namely a basic coordination game (as defined by Lewis), and through extensive computer simulations determine not only the effect of applying HCR, but also the subtle effects of various system parameters, such as the amount of memory and the frequency of update performed by all agents.}
}
@article{MENGSHOEL2010984,
title = {Understanding the scalability of Bayesian network inference using clique tree growth curves},
journal = {Artificial Intelligence},
volume = {174},
number = {12},
pages = {984-1006},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000846},
author = {Ole J. Mengshoel},
keywords = {Probabilistic reasoning, Bayesian networks, Clique tree clustering, Clique tree growth, -ratio, Continuous approximation, Gompertz growth curves, Controlled experiments, Regression},
abstract = {One of the main approaches to performing computation in Bayesian networks (BNs) is clique tree clustering and propagation. The clique tree approach consists of propagation in a clique tree compiled from a BN, and while it was introduced in the 1980s, there is still a lack of understanding of how clique tree computation time depends on variations in BN size and structure. In this article, we improve this understanding by developing an approach to characterizing clique tree growth as a function of parameters that can be computed in polynomial time from BNs, specifically: (i) the ratio of the number of a BN's non-root nodes to the number of root nodes, and (ii) the expected number of moral edges in their moral graphs. Analytically, we partition the set of cliques in a clique tree into different sets, and introduce a growth curve for the total size of each set. For the special case of bipartite BNs, there are two sets and two growth curves, a mixed clique growth curve and a root clique growth curve. In experiments, where random bipartite BNs generated using the BPART algorithm are studied, we systematically increase the out-degree of the root nodes in bipartite Bayesian networks, by increasing the number of leaf nodes. Surprisingly, root clique growth is well-approximated by Gompertz growth curves, an S-shaped family of curves that has previously been used to describe growth processes in biology, medicine, and neuroscience. We believe that this research improves the understanding of the scaling behavior of clique tree clustering for a certain class of Bayesian networks; presents an aid for trade-off studies of clique tree clustering using growth curves; and ultimately provides a foundation for benchmarking and developing improved BN inference and machine learning algorithms.}
}
@article{ZUCKERMAN20121,
title = {Manipulating the quota in weighted voting games},
journal = {Artificial Intelligence},
volume = {180-181},
pages = {1-19},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000021},
author = {Michael Zuckerman and Piotr Faliszewski and Yoram Bachrach and Edith Elkind},
keywords = {Weighted voting games, Manipulation, Complexity},
abstract = {Weighted voting games provide a simple model of decision-making in human societies and multi-agent systems. Such games are described by a set of players, a list of playersÊ¼ weights, and a quota; a coalition of the players is said to be winning if the total weight of its members meets or exceeds the quota. The power of a player in a weighted voting game is traditionally identified with her Shapleyâ€“Shubik index or her Banzhaf index, two classic power measures that reflect the playerÊ¼s marginal contribution under different coalition formation scenarios. In this paper, we investigate by how much one can change a playerÊ¼s power, as measured by these indices, by modifying the quota. We give tight bounds on the changes in the individual playerÊ¼s power that can result from a change in quota. We then describe an efficient algorithm for determining whether there is a value of the quota that makes a given player a dummy, i.e., reduces her power (as measured by both indices) to 0. We also study how the choice of quota can affect the relative power of the players. Finally, we investigate scenarios where oneÊ¼s choice in setting the quota is constrained. We show that optimally choosing between two values of the quota is complete for the complexity class PP, which is believed to be significantly more powerful than NP. On the other hand, we empirically demonstrate that even small changes in quota can have a significant effect on a playerÊ¼s power.}
}
@article{SUZUKI2023103825,
title = {Strategyproof Allocation Mechanisms with Endowments and M-convex Distributional Constraints},
journal = {Artificial Intelligence},
volume = {315},
pages = {103825},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103825},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001655},
author = {Takamasa Suzuki and Akihisa Tamura and Kentaro Yahiro and Makoto Yokoo and Yuzhe Zhang},
keywords = {Controlled school choice, M-convex set, Strategyproofness, Top trading cycles, Deferred acceptance, Distributional constraints},
abstract = {We consider an allocation problem of multiple types of objects to agents, where each type of object has multiple copies (e.g., multiple seats in a school), each agent is endowed with an object, and some distributional constraints are imposed on the allocation (e.g., minimum/maximum quotas). We develop two mechanisms that are strategyproof, feasible (they always satisfy distributional constraints), and individually rational, assuming the distributional constraints are represented by an M-convex set. One mechanism, based on Top Trading Cycles, is Pareto efficient; the other, which belongs to the mechanism class specified by Kojima et al. [1], satisfies a relaxed fairness requirement. The class of distributional constraints we consider contains many situations raised from realistic matching problems, including individual minimum/maximum quotas, regional maximum quotas, type-specific quotas, and distance constraints. Finally, we experimentally evaluate the performance of these mechanisms by a computer simulation.}
}
@article{ASHLAGI20091441,
title = {Two-terminal routing games with unknown active players},
journal = {Artificial Intelligence},
volume = {173},
number = {15},
pages = {1441-1455},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000800},
author = {Itai Ashlagi and Dov Monderer and Moshe Tennenholtz},
keywords = {Routing games, Ignorance, Splittable, unsplittable, Safety-level equilibrium},
abstract = {We analyze 2-terminal routing games with linear cost functions and with unknown number of active players. We deal with both splittable and unsplittable models. We prove the existence and uniqueness of a symmetric safety-level equilibrium in such games and show that in many cases every player benefits from the common ignorance about the number of players. Furthermore, we prove new theorems on existence and uniqueness of equilibrium in 2-terminal convex routing games with complete information.}
}
@article{SHANAHAN1995249,
title = {A circumscriptive calculus of events},
journal = {Artificial Intelligence},
volume = {77},
number = {2},
pages = {249-284},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00036-Z},
url = {https://www.sciencedirect.com/science/article/pii/000437029400036Z},
author = {Murray Shanahan},
abstract = {A calculus of events is presented in which domain constraints, concurrent events, and events with nondeterministic effects can be represented. The paper offers a nonmonotonic solution to the frame problem for this formalism that combines two of the techniques developed for the situation calculus, namely causal and state-based minimisation. A theorem is presented which guarantees that temporal projection will not interfere with minimisation in this solution, even in domains with ramifications, concurrency, and nondeterminism. Finally, the paper shows how the formalism can be extended to cope with continuous change, whilst preserving the conditions for the theorem to apply.}
}
@article{MCCLUSKEY19971,
title = {Engineering and compiling planning domain models to promote validity and efficiency},
journal = {Artificial Intelligence},
volume = {95},
number = {1},
pages = {1-65},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00034-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000349},
author = {T.L. McCluskey and J.M. Porteous},
keywords = {Planning, Knowledge compilation, Domain modelling},
abstract = {This paper postulates a rigorous method for the construction of classical planning domain models. We describe, with the help of a non-trivial example, a tool-supported method for encoding such models. The method results in an â€œobject-centredâ€ specification of the domain that lifts the representation from the level of the literal to the level of the object. Thus, for example, operators are defined in terms of how they change the state of objects, and planning states are defined as amalgams of the objects' states. The method features two classes of tools: for initial capture and validation of the domain model; and for operationalising the domain model (a process we call compilation) for later planning. Here we focus on compilation tools used to generate macros and goal orders to be utilised at plan generation time. We describe them in depth, and evaluate empirically their combined benefits in plan generation speed-up. The method's main benefit is in helping the modeller to produce a tight, valid and operational domain model. It also has the potential benefits of (i) forcing a change of emphasis in classical planning research to encompass knowledge-based aspects of target planning domains in a systematic manner, (ii) helping to bridge the gap between the research area of theoretical but unrealistic planning on the one hand, and â€œscruffyâ€ but real-world planning on the other, (iii) a commitment to a knowledge representation form which allows powerful techniques for planning domain model validation and planning algorithm speed-up can be bound up into a tool-supported environment.}
}
@article{MCGREGGOR20141,
title = {Fractals and Ravens},
journal = {Artificial Intelligence},
volume = {215},
pages = {1-23},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000587},
author = {Keith McGreggor and Maithilee Kunda and Ashok Goel},
keywords = {Analogy, Visual reasoning, Fractal representations, Computational psychometrics},
abstract = {We report a novel approach to visual analogical reasoning, one afforded expressly by fractal representations. We first describe the nature of visual analogies and fractal representations. Next, we exhibit the Fractal Ravens algorithm through a detailed example, describe its performance on all major variants of the Raven's Progressive Matrices tests, and discuss the implications and next steps. In addition, we illustrate the importance of considering the confidence of the answers, and show how ambiguity may be used as a guide for the automatic adjustment of the problem representation. To our knowledge, this is the first published account of a computational model's attempt at the entire Raven's test suite.}
}
@article{GOROGIANNIS20111479,
title = {Instantiating abstract argumentation with classical logic arguments: Postulates and properties},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1479-1497},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002201},
author = {Nikos Gorogiannis and Anthony Hunter},
keywords = {Computational models of argument, Abstract argumentation, Logical argumentation},
abstract = {In this paper we investigate the use of classical logic as a basis for instantiating abstract argumentation frameworks. In the first part, we propose desirable properties of attack relations in the form of postulates and classify several well-known attack relations from the literature with regards to the satisfaction of these postulates. Furthermore, we provide additional postulates that help us prove characterisation results for these attack relations. In the second part of the paper, we present postulates regarding the logical content of extensions of argument graphs that may be constructed with classical logic. We then conduct a comprehensive study of the status of these postulates in the context of the various combinations of attack relations and extension semantics.}
}
@article{DECOOMAN20081400,
title = {Imprecise probability trees: Bridging two theories of imprecise probability},
journal = {Artificial Intelligence},
volume = {172},
number = {11},
pages = {1400-1427},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S000437020800026X},
author = {Gert {de Cooman} and Filip Hermans},
keywords = {Game-theoretic probability, Imprecise probabilities, Coherence, Conglomerability, Event tree, Probability tree, Imprecise probability tree, Lower prevision, Immediate prediction, Prequential Principle, Law of large numbers, Hoeffding's inequality, Markov chain, Random process},
abstract = {We give an overview of two approaches to probability theory where lower and upper probabilities, rather than probabilities, are used: Walley's behavioural theory of imprecise probabilities, and Shafer and Vovk's game-theoretic account of probability. We show that the two theories are more closely related than would be suspected at first sight, and we establish a correspondence between them that (i) has an interesting interpretation, and (ii) allows us to freely import results from one theory into the other. Our approach leads to an account of probability trees and random processes in the framework of Walley's theory. We indicate how our results can be used to reduce the computational complexity of dealing with imprecision in probability trees, and we prove an interesting and quite general version of the weak law of large numbers.}
}
@article{VALIANT2000231,
title = {Robust logics},
journal = {Artificial Intelligence},
volume = {117},
number = {2},
pages = {231-253},
year = {2000},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(00)00002-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370200000023},
author = {Leslie G. Valiant},
keywords = {Learning, Reasoning, Deduction, Soundness, Robustness, Binding problem, Learning rules, Learning relations, PAC learning, PAC semantics},
abstract = {Suppose that we wish to learn from examples and counter-examples a criterion for recognizing whether an assembly of wooden blocks constitutes an arch. Suppose also that we have preprogrammed recognizers for various relationships, e.g., on-top-of(x,y), above(x,y), etc. and believe that some possibly complex expression in terms of these base relationships should suffice to approximate the desired notion of an arch. How can we formulate such a relational learning problem so as to exploit the benefits that are demonstrably available in propositional learning, such as attribute-efficient learning by linear separators, and error-resilient learning? We believe that learning in a general setting that allows for multiple objects and relations in this way is a fundamental key to resolving the following dilemma that arises in the design of intelligent systems: Mathematical logic is an attractive language of description because it has clear semantics and sound proof procedures. However, as a basis for large programmed systems it leads to brittleness because, in practice, consistent usage of the various predicate names throughout a system cannot be guaranteed, except in application areas such as mathematics where the viability of the axiomatic method has been demonstrated independently. In this paper we develop the following approach to circumventing this dilemma. We suggest that brittleness can be overcome by using a new kind of logic in which each statement is learnable. By allowing the system to learn rules empirically from the environment, relative to any particular programs it may have for recognizing some base predicates, we enable the system to acquire a set of statements approximately consistent with each other and with the world, without the need for a globally knowledgeable and consistent programmer. We illustrate this approach by describing a simple logic that has a sound and efficient proof procedure for reasoning about instances, and that is rendered robust by having the rules learnable. The complexity and accuracy of both learning and deduction are provably polynomial bounded.}
}
@article{KLENK20091615,
title = {Analogical model formulation for transfer learning in AP Physics},
journal = {Artificial Intelligence},
volume = {173},
number = {18},
pages = {1615-1638},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209001052},
author = {Matthew Klenk and Ken Forbus},
keywords = {Transfer learning, Analogical reasoning, Model formulation, Case-based reasoning},
abstract = {Transfer learning is the ability to apply previously learned knowledge to new problems or domains. In qualitative reasoning, model formulation is the process of moving from the unruly, broad set of concepts used in everyday life to a concise, formal vocabulary of abstractions, assumptions, causal relationships, and models that support problem-solving. Approaching transfer learning from a model formulation perspective, we found that analogy with examples can be used to learn how to solve AP Physics style problems. We call this process analogical model formulation and implement it in the Companion cognitive architecture. A Companion begins with some basic mathematical skills, a broad common sense ontology, and some qualitative mechanics, but no equations. The Companion uses worked solutions, explanations of example problems at the level of detail appearing in textbooks, to learn what equations are relevant, how to use them, and the assumptions necessary to solve physics problems. We present an experiment, conducted by the Educational Testing Service, demonstrating that analogical model formulation enables a Companion to learn to solve AP Physics style problems. Across six different variations of relationships between base and target problems, or transfer levels, a Companion exhibited a 63% improvement in initial performance. While already a significant result, we describe an in-depth analysis of this experiment to pinpoint the causes of failures. Interestingly, the sources of failures were primarily due to errors in the externally generated problem and worked solution representations as well as some domain-specific problem-solving strategies, not analogical model formulation. To verify this, we describe a second experiment which was performed after fixing these problems. In this second experiment, a Companion achieved a 95.8% improvement in initial performance due to transfer, which is nearly perfect. We know of no other problem-solving experiments which demonstrate performance of analogical learning over systematic variations of relationships between problems at this scale.}
}
@article{HUANG199877,
title = {Object identification: a Bayesian analysis with application to traffic surveillance},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {77-93},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00067-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000678},
author = {Timothy Huang and Stuart Russell},
keywords = {Object identification, Matching, Data association, Bayesian inference, Traffic surveillance},
abstract = {Object identificationâ€”the task of deciding that two observed objects are in fact one and the same objectâ€”is a fundamental requirement for any situated agent that reasons about individuals. Object identity, as represented by the equality operator between two terms in predicate calculus, is essentially a first-order concept. Raw sensory observations, on the other hand, are essentially propositionalâ€”especially when formulated as evidence in standard probability theory. This paper describes patterns of reasoning that allow identity sentences to be grounded in sensory observations, thereby bridging the gap. We begin by defining a physical event space over which probabilities are defined. We then introduce an identity criterion, which selects those events that correspond to identity between observed objects. From this, we are able to compute the probability that any two objects are the same, given a stream of observations of many objects. We show that the appearance probability, which defines how an object can be expected to appear at subsequent observations given its current appearance, is a natural model for this type of reasoning. We apply the theory to the task of recognizing cars observed by cameras at widely separated sites in a freeway network, with new heuristics to handle the inevitable complexity of matching large numbers of objects and with online learning of appearance probability models. Despite extremely noisy observations, we are able to achieve high levels of performance.}
}
@article{LIN1998273,
title = {Applications of the situation calculus to formalizing control and strategic information: the Prolog cut operator},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {273-294},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00054-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029800054X},
author = {Fangzhen Lin},
abstract = {We argue that the situation calculus is a natural formalism for representing and reasoning about control and strategic information. As a case study, in this paper we provide a situation calculus semantics for the Prolog cut operator, the central search control operator in Prolog. We show that our semantics is well-behaved when the programs are properly stratified, and that according to this semantics, the conventional implementation of the negation-as-failure operator using cut is provably correct with respect to the stable model semantics.}
}
@article{THWAITES2010889,
title = {Causal analysis with Chain Event Graphs},
journal = {Artificial Intelligence},
volume = {174},
number = {12},
pages = {889-909},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000810},
author = {Peter Thwaites and Jim Q. Smith and Eva Riccomagno},
keywords = {Back Door Theorem, Bayesian Network, Causal manipulation, Chain Event Graph, Conditional independence, Event tree, Graphical model},
abstract = {As the Chain Event Graph (CEG) has a topology which represents sets of conditional independence statements, it becomes especially useful when problems lie naturally in a discrete asymmetric non-product space domain, or when much context-specific information is present. In this paper we show that it can also be a powerful representational tool for a wide variety of causal hypotheses in such domains. Furthermore, we demonstrate that, as with Causal Bayesian Networks (CBNs), the identifiability of the effects of causal manipulations when observations of the system are incomplete can be verified simply by reference to the topology of the CEG. We close the paper with a proof of a Back Door Theorem for CEGs, analogous to Pearl's Back Door Theorem for CBNs.}
}
@article{OZTURK20111194,
title = {Representing preferences using intervals},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1194-1222},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S000437021000202X},
author = {Meltem Ã–ztÃ¼rk and Marc Pirlot and Alexis TsoukiÃ s},
keywords = {Preference modeling, Interval representation, Intransitivity, Thresholds},
abstract = {In this paper we present a general framework for the comparison of intervals when preference relations have to established. The use of intervals in order to take into account imprecision and vagueness in handling preferences is well known in the literature, but a general theory on how such models behave is lacking. In the paper we generalize the concept of interval (allowing the presence of more than two points). We then introduce the structure of the framework based on the concept of relative position and component set. We provide an exhaustive study of 2-point and 3-point intervals comparison and show the way to generalize such results to n-point intervals.}
}
@article{EREV2007423,
title = {Multi-agent learning and the descriptive value of simple models},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {423-428},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000057},
author = {Ido Erev and Alvin E. Roth},
keywords = {Reinforcement learning, Fictitious play, Equivalent Number of Observations (ENO), Reciprocation},
abstract = {Behavioral research suggests that human learning in some multi-agent systems can be predicted with surprisingly simple â€œforesight-freeâ€ models. The current note discusses the implications of this research, and its relationship to the observation that social interactions tend to complicate learning.}
}
@article{GREEN20081094,
title = {Domain permutation reduction for constraint satisfaction problems},
journal = {Artificial Intelligence},
volume = {172},
number = {8},
pages = {1094-1118},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207002093},
author = {Martin J. Green and David A. Cohen},
keywords = {Complexity, Constraint satisfaction problem, CSP, NP-completeness, Renamable Horn, Stable marriage, Tractability},
abstract = {This paper is concerned with the Constraint Satisfaction Problem (CSP). It is well-known that the general CSP is NP-hard. However, there has been significant success in discovering subproblems which are tractable (polynomial time solvable). One of the most effective ways to obtain a tractable subproblem has been to force all of the constraint relations to lie in some tractable language. In this paper we define a new way of identifying tractable subproblems of the CSP. Let P be an arbitrary CSP instance and Î“ be any tractable language. Suppose there exists, for each variable of P, a permutation of the domain such the resultant permuted constraint relations of P all lie in Î“. The domain permuted instance is then an instance of a tractable class and can be solved by the polynomial time algorithm for Î“. Solutions to P can be obtained by inverting the domain permutations. The question, for a given class of instances and language Î“, whether such a set of domain permutations can be found efficiently is the key to this method's tractability. One of the important contributions made in this paper is the notion of a â€œlifted constraint instanceâ€ which is a powerful tool to study this question. â€¢We consider the open problem of discovering domain permutations which make instances max-closed. We show that, for bounded arity instances over a Boolean domain this problem is tractable, while for domain size three it is intractable even for binary instances.â€¢We give a simple proof verifying the tractability of discovering domain permutations which make instances row convex. We refute a published result by giving a simple proof of the intractability of discovering domain permutations which make instances, even with domain size four, connected row convex.â€¢We demonstrate that triangulated and stable marriage instances are reducible, via domain permutations, to max-closed instances. This provides a simple explanation for arc consistency deciding these instances.â€¢We verify with a simple direct proof the tractability of identification of renamable Horn instances, and the intractability of finding the largest renamable Horn subset of clauses of an instance of SAT.â€¢We describe natural tractable classes which properly extend the maximal relational classes arising from tractable constraint languages. We believe that domain permutation reductions have a significant chance of being useful in practical applications.}
}
@article{CHOUEIRY2005145,
title = {Towards a practical theory of reformulation for reasoning about physical systems},
journal = {Artificial Intelligence},
volume = {162},
number = {1},
pages = {145-204},
year = {2005},
note = {Reformulation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S000437020400195X},
author = {Berthe Y. Choueiry and Yumi Iwasaki and Sheila McIlraith},
keywords = {Abstraction, Reformulation, Approximation, Reasoning about physical systems},
abstract = {Reformulation is ubiquitous in problem solving and is especially common in modeling physical systems. In this paper we examine reformulation techniques in the context of reasoning about physical systems. This paper does not present a general theory of reformulation, but it studies a number of known reformulation techniques to achieve a broad understanding of the space of available reformulations. In doing so, we present a practical framework for specifying, classifying, and evaluating various reformulation techniques applicable to this class of problems. Our framework provides the terminology to specify the conditions under which a particular reformulation technique is applicable, the cost associated with performing the reformulation, and the effects of the reformulation with respect to the problem encoding.}
}
@article{CARAGIANNIS201231,
title = {On the approximability of Dodgson and Young elections},
journal = {Artificial Intelligence},
volume = {187-188},
pages = {31-51},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000434},
author = {Ioannis Caragiannis and Jason A. Covey and Michal Feldman and Christopher M. Homan and Christos Kaklamanis and Nikos Karanikolas and Ariel D. Procaccia and Jeffrey S. Rosenschein},
keywords = {Computational social choice, Approximation algorithms},
abstract = {The voting rules proposed by Dodgson and Young are both designed to find an alternative closest to being a Condorcet winner, according to two different notions of proximity; the score of a given alternative is known to be hard to compute under either rule. In this paper, we put forward two algorithms for approximating the Dodgson score: a combinatorial, greedy algorithm and an LP-based algorithm, both of which yield an approximation ratio of Hmâˆ’1, where m is the number of alternatives and Hmâˆ’1 is the (mâˆ’1)st harmonic number. We also prove that our algorithms are optimal within a factor of 2, unless problems in NP have quasi-polynomial-time algorithms. Despite the intuitive appeal of the greedy algorithm, we argue that the LP-based algorithm has an advantage from a social choice point of view. Further, we demonstrate that computing any reasonable approximation of the ranking produced by DodgsonÊ¼s rule is NP-hard. This result provides a complexity-theoretic explanation of sharp discrepancies that have been observed in the social choice theory literature when comparing Dodgson elections with simpler voting rules. Finally, we show that the problem of calculating the Young score is NP-hard to approximate by any factor. This leads to an inapproximability result for the Young ranking.}
}
@article{BELLE2018189,
title = {Reasoning about discrete and continuous noisy sensors and effectors in dynamical systems},
journal = {Artificial Intelligence},
volume = {262},
pages = {189-221},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S000437021830314X},
author = {Vaishak Belle and Hector J. Levesque},
keywords = {Knowledge representation, Reasoning about action, Reasoning about knowledge, Reasoning about uncertainty, Probabilistic logical models, Cognitive robotics},
abstract = {Among the many approaches for reasoning about degrees of belief in the presence of noisy sensing and acting, the logical account proposed by Bacchus, Halpern, and Levesque is perhaps the most expressive. While their formalism is quite general, it is restricted to fluents whose values are drawn from discrete finite domains, as opposed to the continuous domains seen in many robotic applications. In this work, we show how this limitation in that approach can be lifted. By dealing seamlessly with both discrete distributions and continuous densities within a rich theory of action, we provide a very general logical specification of how belief should change after acting and sensing in complex noisy domains.}
}
@article{GUNTER1997357,
title = {The common order-theoretic structure of version spaces and ATMSs},
journal = {Artificial Intelligence},
volume = {95},
number = {2},
pages = {357-407},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00033-7},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000337},
author = {Carl A. Gunter and Teow-Hin Ngair and Devika Subramanian},
keywords = {Version spaces, ATMS, Concept learning, Truth maintenance, Label update algorithms, Anti-chains, Partial orders, Admissibility},
abstract = {We demonstrate how order-theoretic abstractions can be useful in identifying, formalizing, and exploiting relationships between seemingly dissimilar AI algorithms that perform computations on partially-ordered sets. In particular, we show how the order-theoretic concept of an anti-chain can be used to provide an efficient representation for such sets when they satisfy certain special properties. We use anti-chains to identify and analyze the basic operations and representation optimizations in the version space learning algorithm and the assumption-based truth maintenance system (ATMS). Our analysis allows us to (1) extend the known theory of admissibility of concept spaces for incremental version space merging, and (2) develop new, simpler label-update algorithms for ATMSs with DNF assumption formulas.}
}
@article{SCHOCKAERT2009258,
title = {Spatial reasoning in a fuzzy region connection calculus},
journal = {Artificial Intelligence},
volume = {173},
number = {2},
pages = {258-298},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S000437020800146X},
author = {Steven Schockaert and Martine {De Cock} and Etienne E. Kerre},
keywords = {Spatial reasoning, Region connection calculus, Fuzzy set theory},
abstract = {Although the region connection calculus (RCC) offers an appealing framework for modelling topological relations, its application in real-world scenarios is hampered when spatial phenomena are affected by vagueness. To cope with this, we present a generalization of the RCC based on fuzzy set theory, and discuss how reasoning tasks such as satisfiability and entailment checking can be cast into linear programming problems. We furthermore reveal that reasoning in our fuzzy RCC is NP-complete, thus preserving the computational complexity of reasoning in the RCC, and we identify an important tractable subfragment. Moreover, we show how reasoning tasks in our fuzzy RCC can also be reduced to reasoning tasks in the original RCC. While this link with the RCC could be exploited in practical reasoning algorithms, we mainly focus on the theoretical consequences. In particular, using this link we establish a close relationship with the Eggâ€“Yolk calculus, and we demonstrate that satisfiable knowledge bases can be realized by fuzzy regions in any dimension.}
}
@article{BASYE1995139,
title = {Learning dynamics: system identification for perceptually challenged agents},
journal = {Artificial Intelligence},
volume = {72},
number = {1},
pages = {139-171},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00023-T},
url = {https://www.sciencedirect.com/science/article/pii/000437029400023T},
author = {Kenneth Basye and Thomas Dean and Leslie Pack Kaelbling},
abstract = {From the perspective of an agent, the input/output behavior of the environment in which it is embedded can be described as a dynamical system. Inputs correspond to the actions executable by the agent in making transitions between states of the environment. Outputs correspond to the perceptual information available to the agent in particular states of the environment. We view dynamical system identification as inference of deterministic finite-state automata from sequences of input/output pairs. The agent can influence the sequence of input/output pairs it is presented by pursuing a strategy for exploring the environment. We identify two sorts of perceptual errors: errors in perceiving the output of a state and errors in perceiving the inputs actually carried out in making a transition from one state to another. We present efficient, high-probability learning algorithms for a number of system identification problems involving such errors. We also present the results of empirical investigations applying these algorithms to learning spatial representations.}
}
@article{TENNENHOLTZ19981,
title = {On stable social laws and qualitative equilibria},
journal = {Artificial Intelligence},
volume = {102},
number = {1},
pages = {1-20},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00033-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000332},
author = {Moshe Tennenholtz},
keywords = {Social laws, Qualitative decision making},
abstract = {This paper introduces and investigates the notion of qualitative equilibria, or stable social laws, in the context of qualitative decision making. Previous work in qualitative decision theory has used the maximin decision criterion for modelling qualitative decision making. When several decision-makers share a common environment, a corresponding notion of equilibrium can be defined. This notion can be associated with the concept of a stable social law. This paper initiates a basic study of stable social laws; in particular, it discusses the stability benefits one obtains from using social laws rather than simple conventions, the existence of stable social laws under various assumptions, the computation of stable social laws, and the representation of stable social laws in a graph-theoretic framework.}
}
@article{KRAUS1996297,
title = {An overview of incentive contracting},
journal = {Artificial Intelligence},
volume = {83},
number = {2},
pages = {297-346},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00059-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000593},
author = {Sarit Kraus},
abstract = {Agents may contract some of their tasks to other agents even when they do not share a common goal. An agent may try to contract some of the tasks that it cannot perform by itself, or that may be performed more efficiently by other agents. One self-motivated agent may convince another self-motivated agent to help it with its task, by promises of rewards, even if the agents are not assumed to be benevolent. We propose techniques that provide efficient ways for agents to make incentive contracts in varied situations: when agents have full information about the environment and each other, or when agents do not know the exact state of the world. We consider situations of repeated encounters, cases of asymmetric information, situations where the agents lack information about each other, and cases where an agent subcontracts a task to a group of agents. Situations in which there is competition among possible contractor agents or possible manager agents are also considered. In all situations we assume that the contractor can choose a level of effort when carrying out the task and we would like the contractor to carry out the task efficiently without the need of close observation by the manager.}
}
@article{QIAN2010597,
title = {Positive approximation: An accelerator for attribute reduction in rough set theory},
journal = {Artificial Intelligence},
volume = {174},
number = {9},
pages = {597-618},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000548},
author = {Yuhua Qian and Jiye Liang and Witold Pedrycz and Chuangyin Dang},
keywords = {Rough set theory, Attribute reduction, Decision table, Positive approximation, Granular computing},
abstract = {Feature selection is a challenging problem in areas such as pattern recognition, machine learning and data mining. Considering a consistency measure introduced in rough set theory, the problem of feature selection, also called attribute reduction, aims to retain the discriminatory power of original features. Many heuristic attribute reduction algorithms have been proposed however, quite often, these methods are computationally time-consuming. To overcome this shortcoming, we introduce a theoretic framework based on rough set theory, called positive approximation, which can be used to accelerate a heuristic process of attribute reduction. Based on the proposed accelerator, a general attribute reduction algorithm is designed. Through the use of the accelerator, several representative heuristic attribute reduction algorithms in rough set theory have been enhanced. Note that each of the modified algorithms can choose the same attribute reduct as its original version, and hence possesses the same classification accuracy. Experiments show that these modified algorithms outperform their original counterparts. It is worth noting that the performance of the modified algorithms becomes more visible when dealing with larger data sets.}
}
@article{DASH20081800,
title = {A note on the correctness of the causal ordering algorithm},
journal = {Artificial Intelligence},
volume = {172},
number = {15},
pages = {1800-1808},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000829},
author = {Denver Dash and Marek J. Druzdzel},
keywords = {Causality, Structural equation models},
abstract = {In this paper we examine in detail the algorithm of Simon [H.A. Simon, Causal ordering and identifiability, in: W.C. Hood, T.C. Koopmans (Eds.), Studies in Econometric Method. Cowles Commission for Research in Economics, Monograph No. 14, John Wiley & Sons, Inc., New York, 1953, pp. 49â€“74, Chapter III], called the causal ordering algorithm (COA), used for constructing the â€œcausal orderingâ€ of a system given a complete specification of the system in terms of a set of â€œstructuralâ€ equations that govern the variables in the system. This algorithm constructs a graphical characterization of the model in a form that we call a partial causal graph. Simon argued in [H.A. Simon, Causal ordering and identifiability, in: W.C. Hood, T.C. Koopmans (Eds.), Studies in Econometric Method. Cowles Commission for Research in Economics, Monograph No. 14, John Wiley & Sons, Inc., New York, 1953, pp. 49â€“74, Chapter III] and subsequent papers that a graph so generated explicates causal structure among variables in the model. We formalize this claim further by proving that any causal model based on a one-to-one correspondence between equations and variables must be consistent with the COA.}
}
@article{COHEN2006983,
title = {The complexity of soft constraint satisfaction},
journal = {Artificial Intelligence},
volume = {170},
number = {11},
pages = {983-1016},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020600052X},
author = {David A. Cohen and Martin C. Cooper and Peter G. Jeavons and Andrei A. Krokhin},
keywords = {Soft constraints, Valued constraint satisfaction, Combinatorial optimisation, Submodular functions, Tractability, Multimorphism},
abstract = {Over the past few years there has been considerable progress in methods to systematically analyse the complexity of constraint satisfaction problems with specified constraint types. One very powerful theoretical development in this area links the complexity of a set of constraints to a corresponding set of algebraic operations, known as polymorphisms. In this paper we extend the analysis of complexity to the more general framework of combinatorial optimisation problems expressed using various forms of soft constraints. We launch a systematic investigation of the complexity of these problems by extending the notion of a polymorphism to a more general algebraic operation, which we call a multimorphism. We show that many tractable sets of soft constraints, both established and novel, can be characterised by the presence of particular multimorphisms. We also show that a simple set of NP-hard constraints has very restricted multimorphisms. Finally, we use the notion of multimorphism to give a complete classification of complexity for the Boolean case which extends several earlier classification results for particular special cases.}
}
@article{JANZING20121,
title = {Information-geometric approach to inferring causal directions},
journal = {Artificial Intelligence},
volume = {182-183},
pages = {1-31},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000045},
author = {Dominik Janzing and Joris Mooij and Kun Zhang and Jan Lemeire and Jakob Zscheischler and Povilas DaniuÅ¡is and Bastian Steudel and Bernhard SchÃ¶lkopf},
keywords = {Deterministic causal relations, Pythagorean triple, Causeâ€“effect pairs},
abstract = {While conventional approaches to causal inference are mainly based on conditional (in)dependences, recent methods also account for the shape of (conditional) distributions. The idea is that the causal hypothesis â€œX causes Yâ€ imposes that the marginal distribution PX and the conditional distribution PY|X represent independent mechanisms of nature. Recently it has been postulated that the shortest description of the joint distribution PX,Y should therefore be given by separate descriptions of PX and PY|X. Since description length in the sense of Kolmogorov complexity is uncomputable, practical implementations rely on other notions of independence. Here we define independence via orthogonality in information space. This way, we can explicitly describe the kind of dependence that occurs between PY and PX|Y making the causal hypothesis â€œY causes Xâ€ implausible. Remarkably, this asymmetry between cause and effect becomes particularly simple if X and Y are deterministically related. We present an inference method that works in this case. We also discuss some theoretical results for the non-deterministic case although it is not clear how to employ them for a more general inference method.}
}
@article{NISHIDA19973,
title = {Grammatical description of behaviors of ordinary differential equations in two-dimensional phase space},
journal = {Artificial Intelligence},
volume = {91},
number = {1},
pages = {3-32},
year = {1997},
note = {Artificial Intelligence Research in Japan},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00055-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000550},
author = {Toyoaki Nishida},
keywords = {Qualitative reasoning, Intelligent scientific computation, Dynamical systems theory, Flow grammar, Flow mapping},
abstract = {The major task of qualitative analysis of systems of ordinary differential equations is to recognize the global pattern of solution curves in the phase space. In this paper, I present a flow grammar, a grammatical specification of all possible patterns of solution curves one may see in the phase space. I describe a flow pattern, a semi-symbolic representation of the patterns of solution patterns in the phase space and show how an important class of flow patterns can be specified by the flow grammar. I then show that the flow grammar presented in this paper can generate any flow pattern resulting from any structurally stable flow on a plane. I also describe several properties of the flow grammar related to the enumeration of patterns. In particular, I estimate the upper limit of the number of applications of rewriting rules needed to derive a given flow pattern. Finally, I describe how the flow grammar is used in qualitative analysis to plan, monitor and interpret the result of numerical computation.}
}
@article{CADOLI2006779,
title = {Automated reformulation of specifications by safe delay of constraints},
journal = {Artificial Intelligence},
volume = {170},
number = {8},
pages = {779-801},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000257},
author = {Marco Cadoli and Toni Mancini},
keywords = {Modelling, Reformulation, Second-order logic, Propositional satisfiability, Constraint satisfaction problems},
abstract = {In this paper we propose a form of reasoning on specifications of combinatorial problems, with the goal of reformulating them so that they are more efficiently solvable. The reformulation technique highlights constraints that can be safely â€œdelayedâ€, and solved afterwards. Our main contribution is the characterization (with soundness proof) of safe-delay constraints with respect to a criterion on the specification, thus obtaining a mechanism for the automated reformulation of specifications applicable to a great variety of problems, e.g., graph coloring, bin-packing, and job-shop scheduling. This is an advancement with respect to the forms of reasoning done by state-of-the-art-systems, which typically just detect linearity of specifications. Another contribution is an experimentation on the effectiveness of the proposed technique using six different solvers, which reveals promising time savings.}
}
@article{ARKOUDAS20091367,
title = {Vivid: A framework for heterogeneous problem solving},
journal = {Artificial Intelligence},
volume = {173},
number = {15},
pages = {1367-1405},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000666},
author = {Konstantine Arkoudas and Selmer Bringsjord},
keywords = {Vivid, Heterogeneous reasoning, Problem solving, Diagrams, DPLs, Assumption bases, Named system states, Worlds, 3-valued logic},
abstract = {We introduce Vivid, a domain-independent framework for mechanized heterogeneous reasoning that combines diagrammatic and symbolic representation and inference. The framework is presented in the form of a family of denotational proof languages (DPLs). We present novel formal structures, called named system states, that are specifically designed for modeling potentially underdetermined diagrams. These structures allow us to deal with incomplete information, a pervasive feature of heterogeneous problem solving. We introduce a notion of attribute interpretations that enables us to interpret first-order relational signatures into named system states, and develop a formal semantic framework based on 3-valued logic. We extend the assumption-base semantics of DPLs to accommodate diagrammatic reasoning by introducing general inference mechanisms for the valid extraction of information from diagrams, and for the incorporation of sentential information into diagrams. A rigorous big-step operational semantics is given, on the basis of which we prove that the framework is sound. We present examples of particular instances of Vivid in order to solve a series of problems, and discuss related work.}
}
@article{LEWIS2016204,
title = {Hierarchical conceptual spaces for concept combination},
journal = {Artificial Intelligence},
volume = {237},
pages = {204-227},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300492},
author = {Martha Lewis and Jonathan Lawry},
keywords = {Conceptual spaces, Concept composition, Random sets},
abstract = {We introduce a hierarchical framework for conjunctive concept combination based on conceptual spaces and random set theory. The model has the flexibility to account for composition of concepts at various levels of complexity. We show that the conjunctive model includes linear combination as a special case, and that the more general model can account for non-compositional behaviours such as overextension, non-commutativity, preservation of necessity and impossibility of attributes and to some extent, attribute loss or emergence. We investigate two further aspects of human concept use, the conjunction fallacy and the â€˜guppy effectâ€™.}
}
@article{BALDASSARRE2009857,
title = {Strengths and synergies of evolved and designed controllers: A study within collective robotics},
journal = {Artificial Intelligence},
volume = {173},
number = {7},
pages = {857-875},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000022},
author = {Gianluca Baldassarre and Stefano Nolfi},
keywords = {Neural networks, Genetic algorithms, Self-organisation, Motor schema-based controllers, Potential fields, Modularity, Multi-variable statistical regression},
abstract = {This paper analyses the strengths and weaknesses of self-organising approaches, such as evolutionary robotics, and direct design approaches, such as behaviour-based controllers, for the production of autonomous robots' controllers, and shows how the two approaches can be usefully combined. In particular, the paper proposes a method for encoding evolved neural-network based behaviours into motor schema-based controllers and then shows how these controllers can be modified and combined to produce robots capable of solving new tasks. The method has been validated in the context of a collective robotics scenario in which a group of physically assembled simulated autonomous robots are requested to produce different forms of coordinated behaviours (e.g., coordinated motion, walled-arena exiting, and light pursuing).}
}
@article{HULLERMEIER20081897,
title = {Label ranking by learning pairwise preferences},
journal = {Artificial Intelligence},
volume = {172},
number = {16},
pages = {1897-1916},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020800101X},
author = {Eyke HÃ¼llermeier and Johannes FÃ¼rnkranz and Weiwei Cheng and Klaus Brinker},
keywords = {Preference learning, Ranking, Pairwise classification, Constraint classification},
abstract = {Preference learning is an emerging topic that appears in different guises in the recent literature. This work focuses on a particular learning scenario called label ranking, where the problem is to learn a mapping from instances to rankings over a finite number of labels. Our approach for learning such a mapping, called ranking by pairwise comparison (RPC), first induces a binary preference relation from suitable training data using a natural extension of pairwise classification. A ranking is then derived from the preference relation thus obtained by means of a ranking procedure, whereby different ranking methods can be used for minimizing different loss functions. In particular, we show that a simple (weighted) voting strategy minimizes risk with respect to the well-known Spearman rank correlation. We compare RPC to existing label ranking methods, which are based on scoring individual labels instead of comparing pairs of labels. Both empirically and theoretically, it is shown that RPC is superior in terms of computational efficiency, and at least competitive in terms of accuracy.}
}
@article{MONDERER2009180,
title = {Strong mediated equilibrium},
journal = {Artificial Intelligence},
volume = {173},
number = {1},
pages = {180-195},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001422},
author = {Dov Monderer and Moshe Tennenholtz},
keywords = {Multi-agent systems, Game theory, Mediator, Mediated equilibrium, Strong equilibrium},
abstract = {Stability against potential deviations by sets of agents is a most desired property in the design and analysis of multi-agent systems. However, unfortunately, this property is typically not satisfied. In game-theoretic terms, a strong equilibrium, which is a strategy profile immune to deviations by coalition, rarely exists. This paper suggests the use of mediators in order to enrich the set of situations where we can obtain stability against deviations by coalitions. A mediator is defined to be a reliable entity, which can ask the agents for the right to play on their behalf, and is guaranteed to behave in a pre-specified way based on messages received from the agents. However, a mediator cannot enforce behavior; that is, agents can play in the game directly, without the mediator's help. A mediator generates a new game for the players, the mediated game. We prove some general results about mediators, and mainly concentrate on the notion of strong mediated equilibrium, which is just a strong equilibrium at the mediated game. We show that desired behaviors, which are stable against deviations by coalitions, can be obtained using mediators in several classes of settings.}
}
@article{DAVIS20081540,
title = {Pouring liquids: A study in commonsense physical reasoning},
journal = {Artificial Intelligence},
volume = {172},
number = {12},
pages = {1540-1578},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000611},
author = {Ernest Davis},
keywords = {Liquids, Qualitative physical reasoning, Naive physics, Qualitative spatial reasoning},
abstract = {This paper presents a theory that supports commonsense, qualitative reasoning about the flow of liquid around slowly moving solid objects; specifically, inferring that liquid can be poured from one container to another, given only qualitative information about the shapes and motions of the containers. It shows how the theory and the problem specification can be expressed in a first-order language; and demonstrates that this inference and other similar inferences can be justified as deductive conclusions from the theory and the problem specification.}
}
@article{UCKELMAN20101222,
title = {Compactly representing utility functions using weighted goals and the max aggregator},
journal = {Artificial Intelligence},
volume = {174},
number = {15},
pages = {1222-1246},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000998},
author = {Joel Uckelman and Ulle Endriss},
keywords = {Preference representation, Preference aggregation},
abstract = {Weighted propositional formulas can be used to model preferences over combinatorial domains: each formula represents a goal we would like to see satisfied, the weight of a formula represents the importance of the goal in question, and to assess the desirability of a given alternative we aggregate the weights of the goals satisfied by that alternative. One of several options is to aggregate by using the maximum of the weights of the satisfied goals. This approach gives rise to a family of preference representation languages, one for each of a range of possible restrictions we can impose on either formulas or weights. We analyze the properties of these languages and establish results regarding their expressivity, and absolute and relative succinctness. We also study the computational complexity of the problem of finding the best and the worst alternative for a given set of weighted goals, and of finding an alternative that is optimal for a group of agents, for a range of different notions of collective optimality proposed in social choice theory and welfare economics.}
}
@article{ZINKEVICH2007440,
title = {A hierarchy of prescriptive goals for multiagent learning},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {440-447},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000513},
author = {Martin Zinkevich and Amy Greenwald and Michael L. Littman},
abstract = {A great deal of theoretical effort in multiagent learning involves either embracing or avoiding the inherent symmetry between the problem and the solution. Regret minimization is an approach to the prescriptive, non-cooperative goal that explicitly breaks this symmetry, but, since it makes no assumptions about the adversary, it achieves only limited guarantees. In this paper, we consider a hierarchy of goals that begins with the basics of regret minimization and moves towards the utility guarantees achievable by agents that could also guarantee converging to a game-theoretic equilibrium.}
}
@article{YING2006581,
title = {Linguistic quantifiers modeled by Sugeno integrals},
journal = {Artificial Intelligence},
volume = {170},
number = {6},
pages = {581-606},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S000437020600021X},
author = {Mingsheng Ying},
keywords = {High level knowledge representation and reasoning, Natural language understanding, Computing with words, Fuzzy logic, Quantifier, Fuzzy measure, Sugeno's integral},
abstract = {Since quantifiers have the ability of summarizing the properties of a class of objects without enumerating them, linguistic quantification is a very important topic in the field of high level knowledge representation and reasoning. This paper introduces a new framework for modeling quantifiers in natural languages in which each linguistic quantifier is represented by a family of fuzzy measures, and the truth value of a quantified proposition is evaluated by using Sugeno's integral. This framework allows us to have some elegant logical properties of linguistic quantifiers. We compare carefully our new model of quantification and other approaches to linguistic quantifiers. A set of criteria for linguistic quantification was proposed in the previous literature. The relationship between these criteria and the results obtained in the present paper is clarified. Some simple applications of the Sugeno's integral semantics of quantifiers are presented.}
}
@article{FREUND1998209,
title = {Preferential reasoning in the perspective of Poole default logic},
journal = {Artificial Intelligence},
volume = {98},
number = {1},
pages = {209-235},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00053-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000532},
author = {Michael Freund},
keywords = {Nonmonotonic reasoning, Preferential relations, Rationality, Poole systems, Default logic},
abstract = {The sceptical inference relation associated with a Poole system without constraints is known to have a simple semantic representation by means of a smooth order directly defined on the set of interpretations associated with the underlying language. Conversely, we prove in this paper that, on a finite prepositional language, any preferential inference relation defined by such a model is induced by a Poole system without constraints. In the particular case of rational relations, the associated set of defaults may be chosen to be minimal; it then consists of a set of formulae, totally ordered through classical implication, with cardinality equal to the height of the given relation. This result can be applied to knowledge representation theory and corresponds, in revision theory, to Grove's family of spheres. In the framework of conditional knowledge bases and default extensions, it implies that any rational inference relation may be considered as the rational closure of a minimal knowledge base. An immediate consequence of this is the possibility of replacing any conditional knowledge base by a minimal one that provides the same amount of information.}
}
@article{DILIGENTI2017143,
title = {Semantic-based regularization for learning and inference},
journal = {Artificial Intelligence},
volume = {244},
pages = {143-165},
year = {2017},
note = {Combining Constraint Solving with Mining and Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001344},
author = {Michelangelo Diligenti and Marco Gori and Claudio SaccÃ },
keywords = {Learning with constraints, Kernel machines, FOL},
abstract = {This paper proposes a unified approach to learning from constraints, which integrates the ability of classical machine learning techniques to learn from continuous feature-based representations with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning. Learning tasks are modeled in the general framework of multi-objective optimization, where a set of constraints must be satisfied in addition to the traditional smoothness regularization term. The constraints translate First Order Logic formulas, which can express learning-from-example supervisions and general prior knowledge about the environment by using fuzzy logic. By enforcing the constraints also on the test set, this paper presents a natural extension of the framework to perform collective classification. Interestingly, the theory holds for both the case of data represented by feature vectors and the case of data simply expressed by pattern identifiers, thus extending classic kernel machines and graph regularization, respectively. This paper also proposes a probabilistic interpretation of the proposed learning scheme, and highlights intriguing connections with probabilistic approaches like Markov Logic Networks. Experimental results on classic benchmarks provide clear evidence of the remarkable improvements that are obtained with respect to related approaches.}
}
@article{ZAFARI201659,
title = {POPPONENT: Highly accurate, individually and socially efficient opponent preference model in bilateral multi issue negotiations},
journal = {Artificial Intelligence},
volume = {237},
pages = {59-91},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300364},
author = {Farhad Zafari and Faria Nassiri-Mofakham},
keywords = {Bilateral multi issue negotiation, Opponent modeling, Bidding strategy, Acceptance strategy, Perceptron, Multi bipartite gradient descent},
abstract = {In automated bilateral multi issue negotiations, two intelligent automated agents negotiate on behalf of their owners regarding many issues in order to reach an agreement. Modeling the opponent can excessively boost the performance of the agents and increase the quality of the negotiation outcome. State of the art models accomplish this by considering some assumptions about the opponent which restricts the applicability of the models in real scenarios. In this study, a less restricted technique where perceptron units (POPPONENT) are applied in modeling the preferences of the opponent is proposed. This model adopts the Multi Bipartite version of the Standard Gradient Descent search algorithm (MBGD) to find the best hypothesis, which is the best preference profile. In order to evaluate the accuracy and performance of this proposed opponent model, it is compared with the state of the art models available in the Genius repository. This results in the devised setting which approves the higher accuracy of POPPONENT compared to the most accurate state of the art model. Evaluating the model in the real world negotiation scenarios in the Genius framework also confirms its high accuracy in relation to the state of the art models in estimating the utility of offers. The findings here indicate that this proposed model is individually and socially efficient. This proposed MBGD method could also be adopted in other practical areas of Artificial Intelligence.}
}
@article{LIN20161,
title = {A formalization of programs in first-order logic with a discrete linear order},
journal = {Artificial Intelligence},
volume = {235},
pages = {1-25},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S000437021630011X},
author = {Fangzhen Lin},
keywords = {Program semantics, Reasoning about programs, First-order logic},
abstract = {We consider the problem of representing and reasoning about computer programs, and propose a translation from a core procedural iterative programming language to first-order logic with quantification over the domain of natural numbers that includes the usual successor function and the â€œless thanâ€ linear order, essentially a first-order logic with a discrete linear order. Unlike Hoare's logic, our approach does not rely on loop invariants. Unlike the typical temporal logic specification of a program, our translation does not require a transition system model of the program, and is compositional on the structures of the program. Some non-trivial examples are given to show the effectiveness of our translation for proving properties of programs.}
}
@article{KANG201232,
title = {Exploiting symmetries for single- and multi-agent Partially Observable Stochastic Domains},
journal = {Artificial Intelligence},
volume = {182-183},
pages = {32-57},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000057},
author = {Byung Kon Kang and Kee-Eung Kim},
keywords = {POMDP, POSG, Symmetry, Graph automorphism},
abstract = {While Partially Observable Markov Decision Processes (POMDPs) and their multi-agent extension Partially Observable Stochastic Games (POSGs) provide a natural and systematic approach to modeling sequential decision making problems under uncertainty, the computational complexity with which the solutions are computed is known to be prohibitively expensive. In this paper, we show how such high computational resource requirements can be alleviated through the use of symmetries present in the problem. The problem of finding the symmetries can be cast as a graph automorphism (GA) problem on a graphical representation of the problem. We demonstrate how such symmetries can be exploited in order to speed up the solution computation and provide computational complexity results.}
}
@article{HOSSAIN1997303,
title = {An extension of QSIM with qualitative curvature},
journal = {Artificial Intelligence},
volume = {96},
number = {2},
pages = {303-350},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00051-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000519},
author = {Abul Hossain and Kumar S. Ray},
keywords = {Qualitative simulation, Qualitative magnitude, Qualitative curvature, Reasonable function, Qualitative constraint equations},
abstract = {The aim of this work is to extend the existing method of QSIM with qualitative curvature. We consider a new definition of the reasonable function by introducing the concept of the point of inflection, We generate the new tables for P-transitions and I-transitions and ultimately justify the need of the new definition of the reasonable function. We demonstrate that the new definition of the reasonable function produces qualitatively accurate curvature profile of the response which is absent in the existing QSIM.}
}
@article{MANNOR2007417,
title = {Multi-agent learning for engineers},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {417-422},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000070},
author = {Shie Mannor and Jeff S. Shamma},
keywords = {Multi-agent systems, Cooperative control, Distributed control, Learning in games, Nash equilibrium},
abstract = {As suggested by the title of Shoham, Powers, and Grenager's position paper [Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Intelligence 171 (7) (2007) 365â€“377, this issue], the ultimate lens through which the multi-agent learning framework should be assessed is â€œwhat is the question?â€. In this paper, we address this question by presenting challenges motivated by engineering applications and discussing the potential appeal of multi-agent learning to meet these challenges. Moreover, we highlight various differences in the underlying assumptions and issues of concern that generally distinguish engineering applications from models that are typically considered in the economic game theory literature.}
}
@article{BASRI1995327,
title = {Localization and homing using combinations of model views},
journal = {Artificial Intelligence},
volume = {78},
number = {1},
pages = {327-354},
year = {1995},
note = {Special Volume on Computer Vision},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00021-6},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000216},
author = {Ronen Basri and Ehud Rivlin},
abstract = {Navigation involves recognizing the environment, identifying the current position within the environment, and reaching particular positions. We present a method for localization (the act of recognizing the environment), positioning (the act of computing the exact coordinates of a robot in the environment), and homing (the act of returning to a previously visited position) from visual input. The method is based on representing the scene as a set of 2D views and predicting the appearances of novel views by linear combinations of the model views. The method accurately approximates the appearance of scenes under weak-perspective projection. Analysis of this projection as well as experimental results demonstrate that in many cases this approximation is sufficient to accurately describe the scene. When weak-perspective approximation is invalid, either a larger number of models can be acquired or an iterative solution to account for the perspective distortions can be employed. The method has several advantages over other approaches. It uses relatively rich representations; the representations are 2D rather than 3D; and localization can be done from only a single 2D view without calibration. The same principal method is applied for both the localization and positioning problems, and a simple â€œqualitativeâ€ algorithm for homing is derived from this method.}
}
@article{WU2008945,
title = {Reachability analysis of uncertain systems using bounded-parameter Markov decision processes},
journal = {Artificial Intelligence},
volume = {172},
number = {8},
pages = {945-954},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020700210X},
author = {Di Wu and Xenofon Koutsoukos},
keywords = {Reachability analysis, Uncertain systems, Markov decision processes},
abstract = {Verification of reachability properties for probabilistic systems is usually based on variants of Markov processes. Current methods assume an exact model of the dynamic behavior and are not suitable for realistic systems that operate in the presence of uncertainty and variability. This research note extends existing methods for Bounded-parameter Markov Decision Processes (BMDPs) to solve the reachability problem. BMDPs are a generalization of MDPs that allows modeling uncertainty. Our results show that interval value iteration converges in the case of an undiscounted reward criterion that is required to formulate the problems of maximizing the probability of reaching a set of desirable states or minimizing the probability of reaching an unsafe set. Analysis of the computational complexity is also presented.}
}
@article{BABKA201319,
title = {Complexity issues related to propagation completeness},
journal = {Artificial Intelligence},
volume = {203},
pages = {19-34},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000726},
author = {Martin Babka and TomÃ¡Å¡ Balyo and OndÅ™ej ÄŒepek and Å tefan GurskÃ½ and Petr KuÄera and VÃ¡clav VlÄek},
keywords = {Boolean functions, Satisfiability, Knowledge compilation, Empowering implicates, Unit propagation, Propagation completeness},
abstract = {Knowledge compilation is a process of adding more information to a knowledge base in order to make it easier to deduce facts from the compiled base than from the original one. One type of knowledge compilation occurs when the knowledge in question is represented by a Boolean formula in conjunctive normal form (CNF). The goal of knowledge compilation in this case is to add clauses to the input CNF until a logically equivalent propagation complete CNF is obtained. A CNF is called propagation complete if after any partial substitution of truth values all logically entailed literals can be inferred from the resulting CNF formula by unit propagation. The key to this type of knowledge compilation is the ability to generate so-called empowering clauses. A clause is empowering for a CNF if it is an implicate and for some partial substitution of truth values it enlarges the set of entailed literals inferable by unit propagation. In this paper we study several complexity issues related to empowering implicates, propagation completeness, and its relation to resolution proofs. We show several results: (a) given a CNF and a clause it is co-NP complete to decide whether the clause is an empowering implicate of the CNF, (b) given a CNF it is NP-complete to decide whether there exists an empowering implicate for it and thus it is co-NP complete to decide whether a CNF is propagation complete, and (c) there exist CNFs to which an exponential number of clauses must be added to make them propagation complete.}
}
@article{YIP1991179,
title = {Understanding complex dynamics by visual and symbolic reasoning},
journal = {Artificial Intelligence},
volume = {51},
number = {1},
pages = {179-221},
year = {1991},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(91)90111-V},
url = {https://www.sciencedirect.com/science/article/pii/000437029190111V},
author = {Kenneth Man-Kam Yip},
abstract = {Professional scientists and engineers routinely use nonverbal reasoning processes and graphical representations to organize their thoughts and as part of the process of solving otherwise verbally presented problems. This paper presents a computational theory and an implemented system that capture some aspects of this style of reasoning. The system, consisting of a suite of computer programs collectively known as KAM, uses numerical methods as a means to shift back and forth between symbolic and geometric methods of reasoning. The KAM program has three novel features: (1) it articulates the idea that â€œvisual mechanisms are useful for problem solvingâ€ into a workable computational theory, (2) it applies the approach to a domain of great technical difficulty, the field of complex nonlinear chaotic dynamics, and (3) it demonstrates the power of the approach by solving problems of real interest to working scientists and engineers.}
}
@article{CASTILLO1997395,
title = {Tail uncertainty analysis in complex systems},
journal = {Artificial Intelligence},
volume = {96},
number = {2},
pages = {395-419},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00052-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000520},
author = {Enrique Castillo and Cristina Solares and Patricia GÃ³mez},
keywords = {Bounded variables, Fast probability integration method, Likelihood weighing, Monotonic transformation, Tail simulation, Uncertainty analysis},
abstract = {The paper presents an efficient computational method for estimating the tails of a target variable Z which is related to other set of bounded variables X = (X1,â€¦, Xn) by an increasing (decreasing) relation Z = h(X1,â€¦, Xn). To this aim, variables Xi, i = 1,â€¦, n are sequentially simulated in such a manner that Z = h(x1,â€¦, xi âˆ’ 1, Xi,â€¦, Xn) is guaranteed to be in the tail of Z. The method is shown to be very useful to perform an uncertainty analysis of Bayesian networks, when very large confidence intervals for the marginal/conditional probabilities are required, as in reliability or risk analysis. The method is shown to behave best when all scores coincide and is illustrated with several examples, including two examples of application to real cases. A comparison with the fast probability integration method, the best known method to date for solving this problem, shows that it gives better approximations.}
}
@article{PRATTHARTMANN20051,
title = {Temporal prepositions and their logic},
journal = {Artificial Intelligence},
volume = {166},
number = {1},
pages = {1-36},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000640},
author = {Ian Pratt-Hartmann},
keywords = {Natural language, Temporal prepositions, Interval temporal logic, Computational complexity},
abstract = {A fragment of English featuring temporal prepositions and the order-denoting adjectives first and last is defined by means of a context-free grammar. The phrase-structures which this grammar assigns to the sentences it recognizes are viewed as formulas of an interval temporal logic, whose satisfaction-conditions faithfully represent the meanings of the corresponding English sentences. It is shown that the satisfiability problem for this logic is NEXPTIME-complete. The computational complexity of determining logical relationships between English sentences featuring the temporal constructions in question is thus established.}
}
@article{CLIMER2006714,
title = {Cut-and-solve: An iterative search strategy for combinatorial optimization problems},
journal = {Artificial Intelligence},
volume = {170},
number = {8},
pages = {714-738},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000233},
author = {Sharlee Climer and Weixiong Zhang},
keywords = {Search strategies, Branch-and-bound, Branch-and-cut, Anytime algorithms, Linear programming, Traveling Salesman Problem},
abstract = {Branch-and-bound and branch-and-cut use search trees to identify optimal solutions to combinatorial optimization problems. In this paper, we introduce an iterative search strategy which we refer to as cut-and-solve and prove optimality and termination for this method. This search is different from traditional tree search as there is no branching. At each node in the search path, a relaxed problem and a sparse problem are solved and a constraint is added to the relaxed problem. The sparse problems provide incumbent solutions. When the constraining of the relaxed problem becomes tight enough, its solution value becomes no better than the incumbent solution value. At this point, the incumbent solution is declared to be optimal. This strategy is easily adapted to be an anytime algorithm as an incumbent solution is found at the root node and continuously updated during the search. Cut-and-solve enjoys two favorable properties. Since there is no branching, there are no â€œwrongâ€ subtrees in which the search may get lost. Furthermore, its memory requirement is negligible. For these reasons, it has potential for problems that are difficult to solve using depth-first or best-first search tree methods. In this paper, we demonstrate the cut-and-solve strategy by implementing a generic version of it for the Asymmetric Traveling Salesman Problem (ATSP). Our unoptimized implementation outperformed state-of-the-art solvers for five out of seven real-world problem classes of the ATSP. For four of these classes, cut-and-solve was able to solve larger (sometimes substantially larger) problems. Our code is available at our websites.}
}
@article{VREESWIJK1997225,
title = {Abstract argumentation systems},
journal = {Artificial Intelligence},
volume = {90},
number = {1},
pages = {225-279},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00041-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000410},
author = {Gerard A.W. Vreeswijk},
keywords = {Nonmonotonic logic, Defeasible reasoning, Argumentation},
abstract = {In this paper, we develop a theory of abstract argumentation systems. An abstract argumentation system is a collection of â€œdefeasible proofsâ€, called arguments, that is partially ordered by a relation expressing the difference in conclusive force. The prefix â€œabstractâ€ indicates that the theory is concerned neither with a specification of the underlying language, nor with the development of a subtheory that explains the partial order. An unstructured language, without logical connectives such as negation, makes arguments not (pairwise) inconsistent, but (groupwise) incompatible. Incompatibility and difference in conclusive force cause defeat among arguments. The aim of the theory is to find out which arguments eventually emerge undefeated. These arguments are considered to be in force. Several results are established. The main result is that arguments that are in force are precisely those that are in the limit of a so-called complete argumentation sequence.}
}
@article{WALTHER200017,
title = {Proving theorems by reuse},
journal = {Artificial Intelligence},
volume = {116},
number = {1},
pages = {17-66},
year = {2000},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00096-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029900096X},
author = {Christoph Walther and Thomas Kolbe},
keywords = {Deduction and theorem proving, Machine learning, Problem solving and search, Knowledge representation, Analogy, Abstraction, Reuse},
abstract = {We investigate the improvement of theorem proving by reusing previously computed proofs. We have developed and implemented the Plagiator system which proves theorems by mathematical induction with the aid of a human advisor: If a base or step formula is submitted to the system, it tries to reuse a proof of a previously verified formula. If successful, labour is saved, because the number of required user interactions is decreased. Otherwise the human advisor is called for providing a hand crafted proof for such a formula, which subsequentlyâ€”after some (automated) preparation stepsâ€”is stored in the system's memory, to be in stock for future reasoning problems. Besides the potential savings of resources, the performance of the overall system is improved, because necessary lemmata might be speculated as the result of an attempt to reuse a proof. The success of the approach is based on our techniques for preparing given proofs as well as by our methods for retrieval and adaptation of reuse candidates which are promising for future proof reuses. We prove the soundness of our approach and illustrate its performance with several examples.}
}
@article{FRANK199887,
title = {Search in games with incomplete information: a case study using Bridge card play},
journal = {Artificial Intelligence},
volume = {100},
number = {1},
pages = {87-123},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00082-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000829},
author = {Ian Frank and David Basin},
keywords = {Game tree search, Incomplete information, Game theory, Computer Bridge},
abstract = {We examine search algorithms in games with incomplete information, formalising a best defence model of such games based on the assumptions typically made when incomplete information problems are analysed in expert texts. We show that equilibrium point strategies for optimal play exist for this model, and define an algorithm capable of computing such strategies. Using this algorithm as a reference we then analyse search architectures that have been proposed for the incomplete information game of Bridge. These architectures select strategies by analysing some statistically significant collection of complete information sub-games. Our model allows us to clearly state the limitations of such architectures in producing expert analysis, and to precisely formalise and distinguish the problems that lead to sub-optimality. We illustrate these problems with simple game trees and with actual play situations from Bridge itself.}
}
@article{KLEITER1996143,
title = {Propagating imprecise probabilities in Bayesian networks},
journal = {Artificial Intelligence},
volume = {88},
number = {1},
pages = {143-161},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00021-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000215},
author = {Gernot D. Kleiter},
abstract = {Often experts are incapable of providing â€œexactâ€ probabilities; likewise, samples on which the probabilities in networks are based must often be small and preliminary. In such cases the probabilities in the networks are imprecise. The imprecision can be handled by second order probability distributions. It is convenient to use beta or Dirichlet distributions to express the uncertainty about probabilities. The problem of how to propagate point probabilities in a Bayesian network now is transformed into the problem of how to propagate Dirichlet distributions in Bayesian networks. It is shown that the propagation of Dirichlet distributions in Bayesian networks with incomplete data results in a system of probability mixtures of beta-binomial and Dirichlet distributions. Approximate first order probabilities and their second order probability density functions are obtained by stochastic simulation. A number of properties of the propagation of imprecise probabilities are discussed by the use of examples. An important property is that the imprecision of inferences increases rapidly as new premises are added to an argument. The imprecision can be used as a pruning criterion in a network to keep the number of variables involved in an inferential argument small. Thus, imprecision may be used as an Ockam's razor in Bayesian networks.}
}
@article{ANDERSON20081045,
title = {Active logic semantics for a single agent in a static world},
journal = {Artificial Intelligence},
volume = {172},
number = {8},
pages = {1045-1063},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001993},
author = {Michael L. Anderson and Walid Gomaa and John Grant and Don Perlis},
keywords = {Logic, Active logic, Nonmonotonic logic, Paraconsistent logic, Semantics, Soundness, Brittleness, Autonomous agents, Time},
abstract = {For some time we have been developing, and have had significant practical success with, a time-sensitive, contradiction-tolerant logical reasoning engine called the active logic machine (ALMA). The current paper details a semantics for a general version of the underlying logical formalism, active logic. Central to active logic are special rules controlling the inheritance of beliefs in general (and of beliefs about the current time in particular), very tight controls on what can be derived from direct contradictions (P&Â¬P), and mechanisms allowing an agent to represent and reason about its own beliefs and past reasoning. Furthermore, inspired by the notion that until an agent notices that a set of beliefs is contradictory, that set seems consistent (and the agent therefore reasons with it as if it were consistent), we introduce an â€œapperception functionâ€ that represents an agent's limited awareness of its own beliefs, and serves to modify inconsistent belief sets so as to yield consistent sets. Using these ideas, we introduce a new definition of logical consequence in the context of active logic, as well as a new definition of soundness such that, when reasoning with consistent premises, all classically sound rules remain sound in our new sense. However, not everything that is classically sound remains sound in our sense, for by classical definitions, all rules with contradictory premises are vacuously sound, whereas in active logic not everything follows from a contradiction.}
}
@article{COOPER2010570,
title = {Generalizing constraint satisfaction on trees: Hybrid tractability and variable elimination},
journal = {Artificial Intelligence},
volume = {174},
number = {9},
pages = {570-584},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000342},
author = {Martin C. Cooper and Peter G. Jeavons and AndrÃ¡s Z. Salamon},
keywords = {Constraint satisfaction, Tractability, Computational complexity, Arc consistency, Variable ordering, Variable elimination},
abstract = {The Constraint Satisfaction Problem (CSP) is a central generic problem in artificial intelligence. Considerable progress has been made in identifying properties which ensure tractability in such problems, such as the property of being tree-structured. In this paper we introduce the broken-triangle property, which allows us to define a novel tractable class for this problem which significantly generalizes the class of problems with tree structure. We show that the broken-triangle property is conservative (i.e., it is preserved under domain reduction and hence under arc consistency operations) and that there is a polynomial-time algorithm to determine an ordering of the variables for which the broken-triangle property holds (or to determine that no such ordering exists). We also present a non-conservative extension of the broken-triangle property which is also sufficient to ensure tractability and can also be detected in polynomial time. We show that both the broken-triangle property and its extension can be used to eliminate variables, and that both of these properties provide the basis for preprocessing procedures that yield unique closures orthogonal to value elimination by enforcement of consistency. Finally, we also discuss the possibility of using the broken-triangle property in variable-ordering heuristics.}
}
@article{NIEPERT201329,
title = {On the conditional independence implication problem: A lattice-theoretic approach},
journal = {Artificial Intelligence},
volume = {202},
pages = {29-51},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S000437021300060X},
author = {Mathias Niepert and Marc Gyssens and Bassem Sayrafi and Dirk {Van Gucht}},
keywords = {Conditional independence, Probability and lattice theory},
abstract = {Conditional independence is a crucial notion in the development of probabilistic systems which are successfully employed in areas such as computer vision, computational biology, and natural language processing. We introduce a lattice-theoretic framework that permits the study of the conditional independence (CI) implication problem relative to the class of discrete probability measures. Semi-lattices are associated with CI statements and a finite, sound and complete inference system relative to semi-lattice inclusions is presented. This system is shown to be (1) sound and complete for inferring general from saturated CI statements and (2) complete for inferring general from general CI statements. We also show that the general probabilistic CI implication problem can be reduced to that for elementary CI statements. The completeness of the inference system together with its lattice-theoretic characterization yields a criterion we can use to falsify instances of the probabilistic CI implication problem as well as several heuristics that approximate this falsification criterion in polynomial time. We also propose a validation criterion based on representing constraints and sets of constraints as sparse 0â€“1 vectors which encode their semi-lattices. The validation algorithm works by finding solutions to a linear programming problem involving these vectors and matrices. We provide experimental results for this algorithm and show that it is more efficient than related approaches.}
}
@article{SEN2004183,
title = {Average-case analysis of best-first search in two representative directed acyclic graphs},
journal = {Artificial Intelligence},
volume = {155},
number = {1},
pages = {183-206},
year = {2004},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S000437020400013X},
author = {Anup K. Sen and Amitava Bagchi and Weixiong Zhang},
keywords = {Graph search, Average-case complexity, A, Job sequencing, Traveling salesman},
abstract = {Many problems that arise in the real world have search spaces that are graphs rather than trees. Understanding the properties of search algorithms and analyzing their performance have been major objectives of research in AI. But most published work on the analysis of search algorithms has been focused on tree search, and comparatively little has been reported on graph search. One of the major obstacles in analyzing average-case complexity of graph search is that no single graph can serve as a suitable representative of graph search problems. In this paper we propose one possible approach to analyzing graph search. We take two problem domains for which the search graphs are directed acyclic graphs of similar structure, and determine the average case performance of the best-first search algorithm Aâˆ— on these graphs. The first domain relates to one-machine job sequencing problems in which a set of jobs must be sequenced on a machine in such a way that a penalty function is minimized. The second domain concerns the Traveling Salesman Problem. Our mathematical formulation extends a technique that has been used previously for analyzing tree search. We demonstrate the existence of a gap in computational cost between two classes of problem instances. One class has exponential complexity and the other has polynomial complexity. For the job sequencing domain we provide supporting experimental evidence showing that problems exhibit a huge difference in computational cost under different conditions. For the Traveling Salesman Problem, our theoretical results reflect on the long-standing debate on the expected complexity of branch-and-bound algorithms for solving the problem, indicating that the complexity can be polynomial or exponential, depending on the accuracy of the heuristic function used.}
}
@article{BRINGMANN201322,
title = {Speeding up many-objective optimization by Monte Carlo approximations},
journal = {Artificial Intelligence},
volume = {204},
pages = {22-29},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000738},
author = {Karl Bringmann and Tobias Friedrich and Christian Igel and Thomas VoÃŸ},
keywords = {Evolutionary algorithm, Multi-objective optimization, Pareto-front approximation, Hypervolume indicator},
abstract = {Many state-of-the-art evolutionary vector optimization algorithms compute the contributing hypervolume for ranking candidate solutions. However, with an increasing number of objectives, calculating the volumes becomes intractable. Therefore, although hypervolume-based algorithms are often the method of choice for bi-criteria optimization, they are regarded as not suitable for many-objective optimization. Recently, Monte Carlo methods have been derived and analyzed for approximating the contributing hypervolume. Turning theory into practice, we employ these results in the ranking procedure of the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) as an example of a state-of-the-art method for vector optimization. It is empirically shown that the approximation does not impair the quality of the obtained solutions given a budget of objective function evaluations, while considerably reducing the computation time in the case of multiple objectives. These results are obtained on common benchmark functions as well as on two design optimization tasks. Thus, employing Monte Carlo approximations makes hypervolume-based algorithms applicable to many-objective optimization.}
}
@article{DARWICHE199745,
title = {A logical notion of conditional independence: properties and applications},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {45-82},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00042-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000428},
author = {Adnan Darwiche},
keywords = {Independence, Structure-based reasoning, Graphoids, Causal networks, Pruning knowledge bases, Relevance, Logic, Probability},
abstract = {We propose a notion of conditional independence with respect to prepositional logic and study some of its key properties. We present several equivalent formulations of the proposed notion, each oriented towards a specific application of logical reasoning such as abduction and diagnosis. We suggest a framework for utilizing logical independence computationally by structuring a propositional logic database around a directed acyclic graph. This structuring explicates many of the independences satisfied by the underlying database. Based on these structural independences, we develop an algorithm for a class of structured databases that is not necessarily Horn. The algorithm is linear in the size of a database structure and can be used for deciding entailment, computing abductions and diagnoses. The presented results are motivated by similar results in the literature on probabilistic and constraint-based reasoning.}
}
@article{HEMASPAANDRA2007255,
title = {Anyone but him: The complexity of precluding an alternative},
journal = {Artificial Intelligence},
volume = {171},
number = {5},
pages = {255-285},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000379},
author = {Edith Hemaspaandra and Lane A. Hemaspaandra and JÃ¶rg Rothe},
keywords = {Approval voting, Computational complexity, Computational resistance, Condorcet voting, Destructive control, Election systems, Plurality voting, Preference aggregation, Multiagent systems, Vote suppression},
abstract = {Preference aggregation in a multiagent setting is a central issue in both human and computer contexts. In this paper, we study in terms of complexity the vulnerability of preference aggregation to destructive control. In particular, we study the ability of an election's chair to, through such mechanisms as voter/candidate addition/suppression/partition, ensure that a particular candidate (equivalently, alternative) does not win. And we study the extent to which election systems can make it impossible, or computationally costly (NP-complete), for the chair to execute such control. Among the systems we studyâ€”plurality, Condorcet, and approval votingâ€”we find cases where systems immune or computationally resistant to a chair choosing the winner nonetheless are vulnerable to the chair blocking a victory. Beyond that, we see that among our studied systems no one system offers the best protection against destructive control. Rather, the choice of a preference aggregation system will depend closely on which types of control one wishes to be protected against. We also find concrete cases where the complexity of or susceptibility to control varies dramatically based on the choice among natural tie-handling rules.}
}
@article{XU2007514,
title = {Random constraint satisfaction: Easy generation of hard (satisfiable) instances},
journal = {Artificial Intelligence},
volume = {171},
number = {8},
pages = {514-534},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000653},
author = {Ke Xu and FrÃ©dÃ©ric Boussemart and Fred Hemery and Christophe Lecoutre},
keywords = {Phase transition, Constraint network, Hard random instances},
abstract = {In this paper, we show that the models of random CSP instances proposed by Xu and Li [K. Xu, W. Li, Exact phase transitions in random constraint satisfaction problems, Journal of Artificial Intelligence Research 12 (2000) 93â€“103; K. Xu, W. Li, Many hard examples in exact phase transitions with application to generating hard satisfiable instances, Technical report, CoRR Report cs.CC/0302001, Revised version in Theoretical Computer Science 355 (2006) 291â€“302] are of theoretical and practical interest. Indeed, these models, called RB and RD, present several nice features. First, it is quite easy to generate random instances of any arity since no particular structure has to be integrated, or property enforced, in such instances. Then, the existence of an asymptotic phase transition can be guaranteed while applying a limited restriction on domain size and on constraint tightness. In that case, a threshold point can be precisely located and all instances have the guarantee to be hard at the threshold, i.e., to have an exponential tree-resolution complexity. Next, a formal analysis shows that it is possible to generate forced satisfiable instances whose hardness is similar to unforced satisfiable ones. This analysis is supported by some representative results taken from an intensive experimentation that we have carried out, using complete and incomplete search methods.}
}
@article{DUNTSCH1998109,
title = {Uncertainty measures of rough set prediction},
journal = {Artificial Intelligence},
volume = {106},
number = {1},
pages = {109-137},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00091-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000915},
author = {Ivo DÃ¼ntsch and GÃ¼nther Gediga},
keywords = {Rough set model, Minimum description length principle, Attribute prediction},
abstract = {The main statistics used in rough set data analysis, the approximation quality, is of limited value when there is a choice of competing models for predicting a decision variable. In keeping within the rough set philosophy of non-invasive data analysis, we present three model selection criteria, using information theoretic entropy in the spirit of the minimum description length principle. Our main procedure is based on the principle of indifference combined with the maximum entropy principle, thus keeping external model assumptions to a minimum. The applicability of the proposed method is demonstrated by a comparison of its error rates with results of C4.5, using 14 published data sets.}
}
@article{BASILICO2017220,
title = {Adversarial patrolling with spatially uncertain alarm signals},
journal = {Artificial Intelligence},
volume = {246},
pages = {220-257},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300279},
author = {Nicola Basilico and Giuseppe {De Nittis} and Nicola Gatti},
keywords = {Security games, Adversarial patrolling, Algorithmic game theory},
abstract = {When securing complex infrastructures or large environments, constant surveillance of every area is not affordable. To cope with this issue, a common countermeasure is the usage of cheap but wide-ranged sensors, able to detect suspicious events that occur in large areas, supporting patrollers to improve the effectiveness of their strategies. However, such sensors are commonly affected by uncertainty. In the present paper, we focus on spatially uncertain alarm signals. That is, the alarm system is able to detect an attack but it is uncertain on the exact position where the attack is taking place. This is common when the area to be secured is wide, such as in border patrolling and fair site surveillance. We propose, to the best of our knowledge, the first Patrolling Security Game where a Defender is supported by a spatially uncertain alarm system, which non-deterministically generates signals once a target is under attack. We show that finding the optimal strategy is FNP-hard even in tree graphs and APX-hard in arbitrary graphs. We provide two (exponential time) exact algorithms and two (polynomial time) approximation algorithms. Finally, we show that, without false positives and missed detections, the best patrolling strategy reduces to stay in a place, wait for a signal, and respond to it at best. This strategy is optimal even with non-negligible missed detection rates, which, unfortunately, affect every commercial alarm system. We evaluate our methods in simulation, assessing both quantitative and qualitative aspects.}
}
@article{SHOHAM2007365,
title = {If multi-agent learning is the answer, what is the question?},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {365-377},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000495},
author = {Yoav Shoham and Rob Powers and Trond Grenager},
abstract = {The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area.11This article has a long history and owes many debts. A first version was presented at the NIPS workshop, Multi-Agent Learning: Theory and Practice, in 2002. A later version was presented at the AAAI Fall Symposium in 2004 [Y. Shoham, R. Powers, T. Grenager, On the agenda(s) of research on multi-agent learning, in: AAAI 2004 Symposium on Artificial Multi-Agent Learning (FS-04-02), AAAI Press, 2004]. Over time it has gradually evolved into the current form, as a result of our own work in the area as well as the feedback of many colleagues. We thank them all collectively, with special thanks to members of the multi-agent group at Stanford in the past three years. Rakesh Vohra and Michael Wellman provided detailed comments on the latest draft which resulted in substantive improvements, although we alone are responsible for the views put forward. This work was supported by NSF ITR grant IIS-0205633 and DARPA grant HR0011-05-1.}
}
@article{KELLY2010865,
title = {Property persistence in the situation calculus},
journal = {Artificial Intelligence},
volume = {174},
number = {12},
pages = {865-888},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S000437021000072X},
author = {Ryan F. Kelly and Adrian R. Pearce},
keywords = {Situation calculus, Automated reasoning, Property persistence},
abstract = {We develop a new automated reasoning technique for the situation calculus that can handle a class of queries containing universal quantification over situation terms. Although such queries arise naturally in many important reasoning tasks, they are difficult to automate in the situation calculus due to the presence of a second-order induction axiom. We show how to reduce queries about property persistence, a common type of universally-quantified query, to an equivalent form that does not quantify over situations and so is amenable to existing reasoning techniques. Our algorithm replaces induction with a meta-level fixpoint calculation; crucially, this calculation uses only first-order reasoning with a limited set of axioms. The result is a powerful new tool for verifying sophisticated domain properties in the situation calculus.}
}
@article{POOLE19977,
title = {The independent choice logic for modelling multiple agents under uncertainty},
journal = {Artificial Intelligence},
volume = {94},
number = {1},
pages = {7-56},
year = {1997},
note = {Economic Principles of Multi-Agent Systems},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00027-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000271},
author = {David Poole},
abstract = {Inspired by game theory representations, Bayesian networks, influence diagrams, structured Markov decision process models, logic programming, and work in dynamical systems, the independent choice logic (ICL) is a semantic framework that allows for independent choices (made by various agents, including nature) and a logic program that gives the consequence of choices. This representation can be used as a specification for agents that act in a world, make observations of that world and have memory, as well as a modelling tool for dynamic environments with uncertainty. The rules specify the consequences of an action, what can be sensed and the utility of outcomes. This paper presents a possible-worlds semantics for ICL, and shows how to embed influence diagrams, structured Markov decision processes, and both the strategic (normal) form and extensive (game-tree) form of games within the ICL. It is argued that the ICL provides a natural and concise representation for multi-agent decision-making under uncertainty that allows for the representation of structured probability tables, the dynamic construction of networks (through the use of logical variables) and a way to handle uncertainty and decisions in a logical representation.}
}
@article{ZOHAR20081917,
title = {Mechanisms for information elicitation},
journal = {Artificial Intelligence},
volume = {172},
number = {16},
pages = {1917-1939},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001033},
author = {Aviv Zohar and Jeffrey S. Rosenschein},
keywords = {Information elicitation, Mechanism design, Information trade},
abstract = {We study the computational aspects of information elicitation mechanisms in which a principal attempts to elicit the private information of other agents using a carefully selected payment scheme based on proper scoring rules. Scoring rules, like many other mechanisms set in a probabilistic environment, assume that all participating agents share some common belief about the underlying probability of events. In real-life situations however, the underlying distributions are not known precisely, and small differences in beliefs of agents about these distributions may alter their behavior under the prescribed mechanism. We examine two related models for the problem. The first model assumes that agents have a similar notion of the probabilities of events, and we show that this approach leads to efficient design algorithms that produce mechanisms which are robust to small changes in the beliefs of agents. In the second model we provide the designer with a more precise and discrete set of alternative beliefs that the seller of information may hold. We show that construction of an optimal mechanism in that case is a computationally hard problem, which is even hard to approximate up to any constant. For this model, we provide two very different exponential-time algorithms for the design problem that have different asymptotic running times. Each algorithm has a different set of cases for which it is most suitable. Finally, we examine elicitation mechanisms that elicit the confidence rating of the seller regarding its information.}
}
@article{MIYAZAKI1997155,
title = {k-Certainty Exploration Method: an action selector to identify the environment in reinforcement learning},
journal = {Artificial Intelligence},
volume = {91},
number = {1},
pages = {155-171},
year = {1997},
note = {Artificial Intelligence Research in Japan},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00062-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000628},
author = {Kazuteru Miyazaki and Masayuki Yamamura and Shigenobu Kobayashi},
keywords = {Reinforcement learning, Q-learning, Markov decision processes, Policy Iteration Algorithm, -Certainty Exploration Method},
abstract = {Reinforcement learning aims to adapt an agent to an unknown environment according to rewards. There are two issues to handle delayed reward and uncertainty. Q-learning is a representative reinforcement learning method. It is used in many works since it can learn an optimum policy. However, Q-learning needs numerous trials to converge to an optimum policy. If the target environments can be described in Markov decision processes, we can identify them from statistics of sensor-action pairs. When we build the correct environment model, we can derive an optimum policy with the Policy Iteration Algorithm. Therefore, we can construct an optimum policy through identifying environments efficiently. We separate the learning process into two phases: identifying an environment and determining an optimum policy. We propose the k-Certainty Exploration Method for identifying an environment. After that, an optimum policy is determined by the Policy Iteration Algorithm. We call a rule k-certainty if and only if it has been selected k times or more. The k-Certainty Exploration Method excepts any loop of rules that already achieve k-certainty. We show its effectiveness by comparing it with Q-learning in two experiments. One is Sutton's maze-like environment, the other is an original environment where an optimum policy varies according to a parameter.}
}
@article{GIUNCHIGLIA1996197,
title = {A metatheory of a mechanized object theory},
journal = {Artificial Intelligence},
volume = {80},
number = {2},
pages = {197-241},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00002-X},
url = {https://www.sciencedirect.com/science/article/pii/000437029500002X},
author = {Fausto Giunchiglia and Paolo Traverso},
abstract = {In this paper we propose a metatheory, MT, which represents the computation which implements its object theory, OT, and, in particular, the computation which implements deduction in OT. To emphasize this fact we say that MT is a metatheory of a mechanized object theory. MT has some â€œunusualâ€ properties, e.g. it explicitly represents failure in the application of inference rules, and the fact that large amounts of the code implementing OT are partial, i.e. they work only for a limited class of inputs. These properties allow us to use MT to express and prove tactics, i.e. expressions which specify how to compose possibly failing applications of inference rules, to interpret them procedurally to assert theorems in OT, to compile them into the system implementation code, and, finally, to generate MT automatically from the system code. The definition of MT is part of a larger project which aims at the implementation of self-reflective systems, i.e. systems which are able to introspect their own code, to reason about it and, possibly, to extend or modify it.}
}
@article{GELLY20111856,
title = {Monte-Carlo tree search and rapid action value estimation in computer Go},
journal = {Artificial Intelligence},
volume = {175},
number = {11},
pages = {1856-1875},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S000437021100052X},
author = {Sylvain Gelly and David Silver},
keywords = {Computer Go, Monte-Carlo, Search, Reinforcement learning},
abstract = {A new paradigm for search, based on Monte-Carlo simulation, has revolutionised the performance of computer Go programs. In this article we describe two extensions to the Monte-Carlo tree search algorithm, which significantly improve the effectiveness of the basic algorithm. When we applied these two extensions to the Go program MoGo, it became the first program to achieve dan (master) level in 9Ã—9 Go. In this article we survey the Monte-Carlo revolution in computer Go, outline the key ideas that led to the success of MoGo and subsequent Go programs, and provide for the first time a comprehensive description, in theory and in practice, of this extended framework for Monte-Carlo tree search.}
}
@article{THIELSCHER2011120,
title = {A unifying action calculus},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {120-141},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000469},
author = {Michael Thielscher},
keywords = {Knowledge representation, Reasoning about actions, Situation Calculus},
abstract = {McCarthy's Situation Calculus is arguably the oldest special-purpose knowledge representation formalism, designed to axiomatize knowledge of actions and their effects. Four decades of research in this area have led to a variety of alternative formalisms: While some approaches can be considered instances or extensions of the classical Situation Calculus, like Reiter's successor state axioms or the Fluent Calculus, there are also special planning languages like ADL and approaches based on a linear (rather than branching) time structure like the Event Calculus. The co-existence of many different calculi has two main disadvantages: The formal relations among them is a largely open issue, and a lot of today's research concerns the transfer of specific results from one approach to another. In this paper, we present a unifying action calculus, which encompasses (well-defined classes of) all of the aforementioned formalisms. Our calculus not only facilitates comparisons and translations between specific approaches, it also allows to solve interesting problems for various calculi at once. We exemplify this by providing a general, calculus-independent solution to a problem of practical relevance, which is intimately related to McCarthy's quest for elaboration tolerant formalisms: the modularity of domain axiomatizations.}
}
@article{NEEDHAM2005103,
title = {Protocols from perceptual observations},
journal = {Artificial Intelligence},
volume = {167},
number = {1},
pages = {103-136},
year = {2005},
note = {Connecting Language to the World},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000986},
author = {Chris J. Needham and Paulo E. Santos and Derek R. Magee and Vincent Devin and David C. Hogg and Anthony G. Cohn},
keywords = {Cognitive vision, Autonomous learning, Unsupervised clustering, Symbol grounding, Inductive logic programming, Spatio-temporal reasoning},
abstract = {This paper presents a cognitive vision system capable of autonomously learning protocols from perceptual observations of dynamic scenes. The work is motivated by the aim of creating a synthetic agent that can observe a scene containing interactions between unknown objects and agents, and learn models of these sufficient to act in accordance with the implicit protocols present in the scene. Discrete concepts (utterances and object properties), and temporal protocols involving these concepts, are learned in an unsupervised manner from continuous sensor input alone. Crucial to this learning process are methods for spatio-temporal attention applied to the audio and visual sensor data. These identify subsets of the sensor data relating to discrete concepts. Clustering within continuous feature spaces is used to learn object property and utterance models from processed sensor data, forming a symbolic description. The progol Inductive Logic Programming system is subsequently used to learn symbolic models of the temporal protocols presented in the presence of noise and over-representation in the symbolic data input to it. The models learned are used to drive a synthetic agent that can interact with the world in a semi-natural way. The system has been evaluated in the domain of table-top game playing and has been shown to be successful at learning protocol behaviours in such real-world audio-visual environments.}
}
@article{DEBONA2015140,
title = {Measuring inconsistency in probabilistic logic: rationality postulates and Dutch book interpretation},
journal = {Artificial Intelligence},
volume = {227},
pages = {140-164},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000922},
author = {Glauber {De Bona} and Marcelo Finger},
keywords = {Probabilistic reasoning, Probabilistic logic, Inconsistency measures},
abstract = {Inconsistency measures have been proposed as a way to manage inconsistent knowledge bases in the AI community. To deal with inconsistencies in the context of conditional probabilistic logics, rationality postulates and computational efficiency have driven the formulation of inconsistency measures. Independently, investigations in formal epistemol-ogy have used the betting concept of Dutch book to measure an agent's degree of incoherence. In this paper, we show the impossibility of joint satisfiability of the proposed postulates, proposing to replace them by more suitable ones. Thus we reconcile the rationality postulates for inconsistency measures in probabilistic bases and show that several inconsistency measures suggested in the literature and computable with linear programs satisfy the reconciled postulates. Additionally, we give an interpretation for these feasible measures based on the formal epistemology concept of Dutch book, bridging the views of two so far separate communities in AI and Philosophy. In particular, we show that incoherence degrees in formal epistemology may lead to novel approaches to inconsistency measures in the AI view.}
}
@article{CADOLI200589,
title = {Compiling problem specifications into SAT},
journal = {Artificial Intelligence},
volume = {162},
number = {1},
pages = {89-120},
year = {2005},
note = {Reformulation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204001936},
author = {Marco Cadoli and Andrea Schaerf},
keywords = {Automatic generation of problem reformulation, Executable specifications, SAT problem, NP-complete problems},
abstract = {We present a compiler that translates a problem specification into a propositional satisfiability test (SAT). Problems are specified in a logic-based language, called np-spec, which allows the definition of complex problems in a highly declarative way, and whose expressive power is such as to capture all problems which belong to the complexity class NP. The target SAT instance is solved using any of the various state-of-the-art solvers available from the community. The system obtained is an executable specification language for all NP problems which shows interesting computational properties. The performance of the system has been tested on a few classical problems, namely graph coloring, Hamiltonian cycle, job-shop scheduling, and on a real-world scheduling application, namely the tournament scheduling problem.}
}
@article{CARO201434,
title = {A branch and prune algorithm for the computation of generalized aspects of parallel robots},
journal = {Artificial Intelligence},
volume = {211},
pages = {34-50},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000125},
author = {S. Caro and D. Chablat and A. Goldsztejn and D. Ishii and C. Jermann},
keywords = {Numerical constraints, Parallel robots, Singularities, Generalized aspects},
abstract = {Parallel robots enjoy enhanced mechanical characteristics that have to be contrasted with a more complicated design. In particular, they often have parallel singularities at some poses, and the robots may become uncontrollable, and could even be damaged, in such configurations. The computation of the connected components in the set of nonsingular reachable configurations, called generalized aspects, is therefore a key issue in their design. This paper introduces a new method, based on numerical constraint programming, to compute a certified enclosure of the generalized aspects. Though this method does not allow counting their number rigorously, it constructs inner approximations of the nonsingular workspace that allow commanding parallel robots safely. It also provides a lower-bound on the exact number of generalized aspects. It is moreover the first general method able to handle any parallel robot in theory, though its computational complexity currently restricts its usage to robots with three degrees of freedom. Finally, the constraint programming paradigm it relies on makes it possible to consider various additional constraints (e.g., collision avoidance), making it suitable for practical considerations.}
}
@article{SCHWARTZ1997103,
title = {Dynamic reasoning with qualified syllogisms},
journal = {Artificial Intelligence},
volume = {93},
number = {1},
pages = {103-167},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00020-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000209},
author = {Daniel G. Schwartz},
keywords = {Default reasoning, Dynamic reasoning systems, Fuzzy likelihood, Fuzzy probabilities, Fuzzy quantifiers, Multiple inheritance, Nonmonotonic reasoning, Qualified syllogisms, The frame problem, Unless},
abstract = {A qualified syllogism is a classical Aristotelean syllogism that has been â€œqualifiedâ€ through the use of fuzzy quantifiers, likelihood modifiers, and usuality modifiers, e.g., â€œMost birds can fly; Tweety is a bird; therefore, it is likely that Tweety can fly.â€ This paper introduces a formal logic Q of such syllogisms and shows how this may be employed in a system of nonmonotonic reasoning. In process are defined the notions of path logic and dynamic reasoning system (DRS). The former is an adaptation of the conventional formal system which explicitly portrays reasoning as an activity that takes place in time. The latter consists of a path logic together with a multiple-inheritance hierarchy. The hierarchy duplicates some of the information recorded in the path logic, but additionally provides an extralogical specificity relation. The system uses typed predicates to formally distinguish between properties and kinds of things. The effectiveness of the approach is demonstrated through analysis of several â€œpuzzlesâ€ that have appeared previously in the literature, e.g., Tweety the Bird, Clyde the Elephant, and the Nixon Diamond. It is also outlined how the DRS framework accommodates other reasoning techniquesâ€”in particular, predicate circumscription, a â€œlocalizedâ€ version of default logic, a variant of nonmonotonic logic, and reason maintenance. Furthermore it is seen that the same framework accomodates a new formulation of the notion of unless. A concluding section discusses the relevance of these systems to the well-known frame problem.}
}
@article{JOSHI1998117,
title = {Role of constrained computational systems in natural language processing},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {117-132},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00065-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000654},
author = {Aravind K. Joshi},
keywords = {Abstract character of adjoining, Adjoining, Almost parse, Centering, Center of an utterance, Complexity of inference, Control of inference, Constrained formal systems, Finite state transducers, Lexicalized grammars, Lexicalized tree-adjoining grammars, Local computations on complex structures, Local statistical computations, Locality of structures, Locally monadic structure, Monadic predicate, Substitution, Supertags, Supertagging, Universal and stipulative constraints},
abstract = {The use of constrained formal/computational systems just adequate for modeling various aspects of languageâ€”syntax, semantics, pragmatics and discourse, among others, has proved to be an effective research strategy leading to deep understanding of these aspects, with implications to both machine processing and human processing. This approach enables one to distinguish between the universal and stipulative constraints. This is in contrast to an approach where we start with the most powerful formal/computational system and then model the phenomena by making all constraints stipulative in a sense. The use of constrained systems for modeling leads to some novel ways of describing locality of structures and brings out the relationship between the complexity of description of primitives and local computations over them. These ideas serve to unify theoretical, computational and statistical aspects of natural languages processing in AI. It is expected that this approach will be productive in other domains of AI.}
}
@article{BOROS1999219,
title = {Logical analysis of binary data with missing bits},
journal = {Artificial Intelligence},
volume = {107},
number = {2},
pages = {219-263},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00110-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298001106},
author = {Endre Boros and Toshihide Ibaraki and Kazuhisa Makino},
keywords = {Knowledge discovery, Data mining, Logical analysis of data, Boolean functions, Partially defined Boolean functions, Missing bits, NP-hardness},
abstract = {We model a given pair of sets of positive and negative examples, each of which may contain missing components, as a partially defined Boolean function with missing bits (pBmb) (T~,F~), where T~-âŠ‚{0,1*}n and F~-âŠ‚{0,1*}n, and â€œ*â€ stands for a missing bit. Then we consider the problem of establishing a Boolean function (an extension) f : 0, 1n â†’ 0, 1 belonging to a given function class C, such that f is true (respectively, false) for every vector in T~ (respectively, in F~. This is a fundamental problem, encountered in many areas such as learning theory, pattern recognition, example-based knowledge bases, logical analysis of data, knowledge discovery and data mining. In this paper, depending upon how to deal with missing bits, we formulate three types of extensions called robust, consistent and most robust extensions, for various classes of Boolean functions such as general, positive, Horn, threshold, decomposable and k-DNF. The complexity of the associated problems are then clarified; some of them are solvable in polynomial time while the others are NP-hard.}
}
@article{SUKHIJA2023103922,
title = {GoSafeOpt: Scalable safe exploration for global optimization of dynamical systems},
journal = {Artificial Intelligence},
volume = {320},
pages = {103922},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103922},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000681},
author = {Bhavya Sukhija and Matteo Turchetta and David Lindner and Andreas Krause and Sebastian Trimpe and Dominik Baumann},
keywords = {Model-free learning, Bayesian optimization, Safe learning},
abstract = {Learning optimal control policies directly on physical systems is challenging. Even a single failure can lead to costly hardware damage. Most existing model-free learning methods that guarantee safety, i.e., no failures, during exploration are limited to local optima. This work proposes GoSafeOpt as the first provably safe and optimal algorithm that can safely discover globally optimal policies for systems with high-dimensional state space. We demonstrate the superiority of GoSafeOpt over competing model-free safe learning methods in simulation and hardware experiments on a robot arm.}
}
@article{CHEN2017105,
title = {Latent tree models for hierarchical topic detection},
journal = {Artificial Intelligence},
volume = {250},
pages = {105-124},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300735},
author = {Peixian Chen and Nevin L. Zhang and Tengfei Liu and Leonard K.M. Poon and Zhourong Chen and Farhan Khawar},
keywords = {Probabilistic graphical models, Text analysis, Hierarchical latent tree analysis, Hierarchical topic detection},
abstract = {We present a novel method for hierarchical topic detection where topics are obtained by clustering documents in multiple ways. Specifically, we model document collections using a class of graphical models called hierarchical latent tree models (HLTMs). The variables at the bottom level of an HLTM are observed binary variables that represent the presence/absence of words in a document. The variables at other levels are binary latent variables that represent word co-occurrence patterns or co-occurrences of such patterns. Each latent variable gives a soft partition of the documents, and document clusters in the partitions are interpreted as topics. Latent variables at high levels of the hierarchy capture long-range word co-occurrence patterns and hence give thematically more general topics, while those at low levels of the hierarchy capture short-range word co-occurrence patterns and give thematically more specific topics. In comparison with LDA-based methods, a key advantage of the new method is that it represents co-occurrence patterns explicitly using model structures. Extensive empirical results show that the new method significantly outperforms the LDA-based methods in term of model quality and meaningfulness of topics and topic hierarchies.}
}
@article{GELSEY19951,
title = {Automated reasoning about machines},
journal = {Artificial Intelligence},
volume = {74},
number = {1},
pages = {1-53},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00003-J},
url = {https://www.sciencedirect.com/science/article/pii/000437029400003J},
author = {Andrew Gelsey},
abstract = {Numerical simulation is often used in predicting machine behavior, a basic capability for many tasks such as design and fault diagnosis. However, using simulators requires considerable human effort both to create behavioral models and to analyze and understand simulation results. I describe algorithms which automate the kinematic and dynamical analysis needed to create behavioral models and which automate the intelligent control of computational simulations needed to understand a machine's behavior over both short and long time scales. The input is a description of a machine's geometry and material properties, and the output is a behavioral model for the machine and a concise qualitative/quantitative prediction of the machine's long-term behavior. My algorithms have been implemented in a working program which can predict a machine's behavior over both short and long time periods. At present this work is limited to mechanical devices, particularly clockwork mechanisms.}
}
@article{YU201220,
title = {On the approximation ability of evolutionary optimization with application to minimum set cover},
journal = {Artificial Intelligence},
volume = {180-181},
pages = {20-33},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000033},
author = {Yang Yu and Xin Yao and Zhi-Hua Zhou},
keywords = {Evolutionary algorithms, Approximation algorithm, Approximation ratio, -Set cover, Time complexity analysis},
abstract = {Evolutionary algorithms (EAs) are heuristic algorithms inspired by natural evolution. They are often used to obtain satisficing solutions in practice. In this paper, we investigate a largely underexplored issue: the approximation performance of EAs in terms of how close the solution obtained is to an optimal solution. We study an EA framework named simple EA with isolated population (SEIP) that can be implemented as a single- or multi-objective EA. We analyze the approximation performance of SEIP using the partial ratio, which characterizes the approximation ratio that can be guaranteed. Specifically, we analyze SEIP using a set cover problem that is NP-hard. We find that in a simple configuration, SEIP efficiently achieves an Hn-approximation ratio, the asymptotic lower bound, for the unbounded set cover problem. We also find that SEIP efficiently achieves an (Hkâˆ’kâˆ’18k9)-approximation ratio, the currently best-achievable result, for the k-set cover problem. Moreover, for an instance class of the k-set cover problem, we disclose how SEIP, using either one-bit or bit-wise mutation, can overcome the difficulty that limits the greedy algorithm.}
}
@article{DELIGKAS2022103784,
title = {Two's company, three's a crowd: Consensus-halving for a constant number of agents},
journal = {Artificial Intelligence},
volume = {313},
pages = {103784},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103784},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001242},
author = {Argyrios Deligkas and Aris Filos-Ratsikas and Alexandros Hollender},
keywords = {Consensus-halving, Fair division, Computational complexity, Query complexity, Robertson-Webb},
abstract = {We consider the Îµ-Consensus-Halving problem, in which a set of heterogeneous agents aim at dividing a continuous resource into two (not necessarily contiguous) portions that all of them simultaneously consider to be of approximately the same value (up to Îµ). This problem was recently shown to be PPA-complete, for n agents and n cuts, even for very simple valuation functions. In a quest to understand the root of the complexity of the problem, we consider the setting where there is only a constant number of agents, and we consider both the computational complexity and the query complexity of the problem. For agents with monotone valuation functions, we show a dichotomy: for two agents the problem is polynomial-time solvable, whereas for three or more agents it becomes PPA-complete. Similarly, we show that for two monotone agents the problem can be solved with polynomially-many queries, whereas for three or more agents, we provide exponential query complexity lower bounds. These results are enabled via an interesting connection to a monotone Borsuk-Ulam problem, which may be of independent interest. For agents with general valuations, we show that the problem is PPA-complete and admits exponential query complexity lower bounds, even for two agents.}
}
@article{DENOEUX2008234,
title = {Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence},
journal = {Artificial Intelligence},
volume = {172},
number = {2},
pages = {234-264},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001063},
author = {Thierry DenÅ“ux},
keywords = {Evidence theory, Dempsterâ€“Shafer theory, Transferable belief model, Distinct evidence, Idempotence, Information fusion},
abstract = {Dempster's rule plays a central role in the theory of belief functions. However, it assumes the combined bodies of evidence to be distinct, an assumption which is not always verified in practice. In this paper, a new operator, the cautious rule of combination, is introduced. This operator is commutative, associative and idempotent. This latter property makes it suitable to combine belief functions induced by reliable, but possibly overlapping bodies of evidence. A dual operator, the bold disjunctive rule, is also introduced. This operator is also commutative, associative and idempotent, and can be used to combine belief functions issues from possibly overlapping and unreliable sources. Finally, the cautious and bold rules are shown to be particular members of infinite families of conjunctive and disjunctive combination rules based on triangular norms and conorms.}
}
@article{GOTTLOB2023103936,
title = {Polynomial combined first-order rewritings for linear and guarded existential rules},
journal = {Artificial Intelligence},
volume = {321},
pages = {103936},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103936},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000826},
author = {Georg Gottlob and Marco Manna and Andreas Pieris},
keywords = {Ontologies, Existential rules, Tuple-generating dependencies, Guardedness, Conjunctive queries, Query answering, Query rewriting, Combined approach},
abstract = {We consider the problem of ontological query answering, that is, the problem of answering a database query (typically a conjunctive query) in the presence of an ontology. This means that during the query answering process we also need to take into account the knowledge that can be inferred from the given database and ontology. Building, however, ontology-aware database systems from scratch, with sophisticated optimization techniques, is a highly non-trivial task that requires a great engineering effort. Therefore, exploiting conventional database systems is an important route towards efficient ontological query answering. Nevertheless, standard database systems are unaware of ontologies. An approach to ontological query answering that enables the use of standard database systems is the so-called polynomial combined query rewriting, originally introduced in the context of description logics: the conjunctive query q and the ontology Î£ are rewritten in polynomial time into a first-order query qÎ£ (in a database-independent way), while the database D and the ontology Î£ are rewritten in polynomial time into a new database DÎ£ (in a query-independent way), such that the answer to q in the presence of Î£ over D coincides with the answer to qÎ£ over DÎ£. The latter can then be computed by exploiting a conventional database system. In this work, we focus on linear and guarded existential rules, which form robust rule-based languages for modeling ontologies, and investigate the limits of polynomial combined query rewriting. In particular, we show that this type of rewriting can be successfully applied to (i) linear existential rules when the rewritten query can use the full power of first-order queries, (ii) linear existential rules when the arity of the underlying schema is fixed and the rewritten query is positive existential, namely it uses only existential quantification, conjunction, and disjunction, and (iii) guarded existential rules when the underlying schema is fixed and the rewritten query is positive existential. We can show that the above results reach the limits (under standard complexity-theoretic assumptions such as Image 1) of polynomial combined query rewriting in the case of linear and guarded existential rules.}
}
@article{GATTI20081119,
title = {Alternating-offers bargaining with one-sided uncertain deadlines: an efficient algorithm},
journal = {Artificial Intelligence},
volume = {172},
number = {8},
pages = {1119-1157},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001981},
author = {Nicola Gatti and Francesco {Di Giunta} and Stefano Marino},
keywords = {Automated negotiations, Game theory, Multiagent systems},
abstract = {In the arena of automated negotiations we focus on the principal negotiation protocol in bilateral settings, i.e. the alternating-offers protocol. In the scientific community it is common the idea that bargaining in the alternating-offers protocol will play a crucial role in the automation of electronic transactions. Notwithstanding its prominence, literature does not present a satisfactory solution to the alternating-offers protocol in real-world settings, e.g. in presence of uncertainty. In this paper we game theoretically analyze this negotiation problem with one-sided uncertain deadlines and we provide an efficient solving algorithm. Specifically, we analyze the situation where the values of the parameters of the buyer are uncertain to the seller, whereas the parameters of the seller are common knowledge (the analysis of the reverse situation is analogous). In this particular situation the results present in literature are not satisfactory, since they do not assure the existence of an equilibrium for every value of the parameters. From our game theoretical analysis we find two choice rules that apply an action and a probability distribution over the actions, respectively, to every time point and we find the conditions on the parameters such that each choice rule can be singularly employed to produce an equilibrium. These conditions are mutually exclusive. We show that it is always possible to produce an equilibrium where the actions, at any single time point, are those prescribed either by the first choice rule or by the second one. We exploit this result for developing a solving algorithm. The proposed algorithm works backward by computing the equilibrium from the last possible deadline of the bargaining to the initial time point and by applying at each time point the actions prescribed by the choice rule whose conditions are satisfied. The computational complexity of the proposed algorithm is asymptotically independent of the number of types of the player whose deadline is uncertain. With linear utility functions, it is O(mâ‹…TÂ¯) where m is the number of the issues and TÂ¯ is the length of the bargaining.}
}
@article{GOERTZEL20071161,
title = {Human-level artificial general intelligence and the possibility of a technological singularity: A reaction to Ray Kurzweil's The Singularity Is Near, and McDermott's critique of Kurzweil},
journal = {Artificial Intelligence},
volume = {171},
number = {18},
pages = {1161-1173},
year = {2007},
note = {Special Review Issue},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001464},
author = {Ben Goertzel},
keywords = {Strong AI, AGI, Self-modifying software, Singularity virtual worlds, Language acquisition},
abstract = {An analysis of Ray Kurzweil's recent book The Singularity Is Near is given, along with Drew McDermott's recent critique. The conclusion is that Kurzweil does an excellent job of fleshing out one particular plausible scenario regarding the future of AI, in which human-level AI first arrives via human-brain emulation. McDermott's arguments against the notion of Singularity via iteratively self-improving AI, as described by Kurzweil, are considered and found wanting. However, it is pointed out that the scenario focused on by Kurzweil is not the only plausible one; and an alternative is discussed, in which human-level AI arrives first via non-human-like AI's operating virtual worlds.}
}
@article{RAKSHIT2015165,
title = {Differential evolution for noisy multiobjective optimization},
journal = {Artificial Intelligence},
volume = {227},
pages = {165-189},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000909},
author = {Pratyusha Rakshit and Amit Konar},
keywords = {Noise, Differential evolution for multiobjective optimization, Sampling, Interquartile range, Skewness, Dominance probability},
abstract = {We propose an extension of multiobjective optimization realized with the differential evolution algorithm to handle the effect of noise in objective functions. The proposed extension offers three merits with respect to its traditional counterpart. First, an adaptive selection of the sample size for the periodic fitness evaluation of a trial solution based on the fitness variance in its local neighborhood is proposed. This avoids the computational complexity associated with the unnecessary reevaluation of quality solutions without disregarding the necessary evaluations for relatively poor solutions to ensure accuracy in fitness estimates. The second strategy is concerned with determining the expected value of the noisy fitness samples on the basis of their distribution, instead of their conventional averaging, as the fitness measure of the trial solutions. Finally, a new crowding-distance-induced probabilistic selection criterion is devised to promote quality solutions from the same rank candidate pool to the next generation, ensuring the population quality and diversity in the objective spaces. Computer simulations performed on a noisy version of a well-known set of 23 benchmark functions reveal that the proposed algorithm outperforms its competitors with respect to inverted generational distance, spacing, error ratio, and hypervolume ratio metrics.}
}
@article{ZAHAVI2008514,
title = {Duality in permutation state spaces and the dual search algorithm},
journal = {Artificial Intelligence},
volume = {172},
number = {4},
pages = {514-540},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001865},
author = {Uzi Zahavi and Ariel Felner and Robert C. Holte and Jonathan Schaeffer},
keywords = {Heuristics, Search, Admissibility, Duality},
abstract = {Geometrical symmetries are commonly exploited to improve the efficiency of search algorithms. A new type of symmetry in permutation state spaces, duality, is introduced. Each state has a dual state. Both states share important attributes such as their distance to the goal. Given a state S, it is shown that an admissible heuristic of the dual state of S is an admissible heuristic for S. This provides opportunities for additional heuristic evaluations. An exact definition of the class of problems where duality exists is provided. A new search algorithm, dual search, is presented which switches between the original state and the dual state when it seems likely that the switch will improve the chance of reaching the goal faster. The decision of when to switch is very important and several policies for doing this are investigated. Experimental results show significant improvements for a number of applications, for using the dual state's heuristic evaluation and/or dual search.}
}
@article{GRANT20081064,
title = {Analysing inconsistent first-order knowledgebases},
journal = {Artificial Intelligence},
volume = {172},
number = {8},
pages = {1064-1093},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S000437020700197X},
author = {John Grant and Anthony Hunter},
keywords = {Measuring inconsistency, Paraconsistent logics, Inconsistency tolerance, Analysing inconsistency, Conflict resolution},
abstract = {It is well-known that knowledgebases may contain inconsistencies. We provide a framework of measures, based on a first-order four-valued logic, to quantify the inconsistency of a knowledgebase. This allows for the comparison of the inconsistency of diverse knowledgebases that have been represented as sets of first-order logic formulae. We motivate the approach by considering some examples of knowledgebases for representing and reasoning with ontological knowledge and with temporal knowledge. Analysing ontological knowledge (including the statements about which concepts are subconcepts of other concepts, and which concepts are disjoint) can be problematical when there is a lack of knowledge about the instances that may populate the concepts, and analysing temporal knowledge (such as temporal integrity constraints) can be problematical when considering infinite linear time lines isomorphic to the natural numbers or the real numbers or more complex structures such as branching time lines. We address these difficulties by providing algebraic measures of inconsistency in first-order knowledgebases.}
}
@article{AGRE19951,
title = {Computational research on interaction and agency},
journal = {Artificial Intelligence},
volume = {72},
number = {1},
pages = {1-52},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00054-5},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000545},
author = {Philip E. Agre},
abstract = {Recent research in artificial intelligence has developed computational theories of agents' involvements in their environments. Although inspired by a great diversity of formalisms and architectures, these research projects are unified by a common concern: using principled characterizations of agents' interactions with their environments to guide analysis of living agents and design of artificial ones. This article offers a conceptual framework for such theories, surveys several other fields of research that hold the potential for dialogue with these new computational projects, and summarizes the principal contributions of the articles in this special double volume. It also briefly describes a case study in these ideasâ€”a computer program called Toast that acts as a short-order breakfast cook. Because its designers have discovered useful structures in the world it inhabits, Toast can employ an extremely simple mechanism to decide what to do next.}
}
@article{BORGWARDT201523,
title = {The limits of decidability in fuzzy description logics with general concept inclusions},
journal = {Artificial Intelligence},
volume = {218},
pages = {23-55},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214001167},
author = {Stefan Borgwardt and Felix Distel and Rafael PeÃ±aloza},
keywords = {Fuzzy description logics, Triangular norms, Ontology consistency, Decidability},
abstract = {Fuzzy description logics (DLs) can be used to represent and reason with vague knowledge. This family of logical formalisms is very diverse, each member being characterized by a specific choice of constructors, axioms, and triangular norms, which are used to specify the semantics. Unfortunately, it has recently been shown that the consistency problem in many fuzzy DLs with general concept inclusion axioms is undecidable. In this paper, we present a proof framework that allows us to extend these results to cover large classes of fuzzy DLs. On the other hand, we also provide matching decidability results for most of the remaining logics. As a result, we obtain a near-universal classification of fuzzy DLs according to the decidability of their consistency problem.}
}
@article{KHARDON1997169,
title = {Defaults and relevance in model-based reasoning},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {169-193},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00044-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000441},
author = {Roni Khardon and Dan Roth},
keywords = {Knowledge representation, Common-sense reasoning, Learning to reason, Reasoning with models, Context, Default reasoning},
abstract = {Reasoning with model-based representations is an intuitive paradigm, which has been shown to be theoretically sound and to possess some computational advantages over reasoning with formula-based representations of knowledge. This paper studies these representations and further substantiates the claim regarding their advantages. In particular, model-based representations are shown to efficiently support reasoning in the presence of varying context information, handle efficiently fragments of Reiter's default logic and provide a useful way to integrate learning with reasoning. Furthermore, these results are closely related to the notion of relevance. The use of relevance information is best exemplified by the filtering process involved in the algorithm developed for reasoning within context. The relation of defaults to relevance is viewed through the notion of context, where the agent has to find plausible context information by using default rules. This view yields efficient algorithms for default reasoning. Finally, it is argued that these results support an incremental view of reasoning in a natural way, and the notion of relevance to the environment, captured by the Learning to Reason framework, is discussed.}
}
@article{HICKEY1996157,
title = {Noise modelling and evaluating learning from examples},
journal = {Artificial Intelligence},
volume = {82},
number = {1},
pages = {157-179},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00094-8},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000948},
author = {Ray J. Hickey},
abstract = {The means of evaluating, using artificial data, algorithms, such as ID3, which learn concepts from examples is enhanced and referred to as the method of artificial universes. The central notions are that of a class model and its associated representations in which a class attribute is treated as a dependent variable with description attributes functioning as the independent variables. The nature of noise in the model is discussed and modelled using information-theoretic ideas especially that of majorisation. The notion of an irrelevant attribute is also considered. The ideas are illustrated through the construction of a small universe which is then altered to increase noise. Learning curves for ID3 used on data generated from these universes are estimated from trials. These show that increasing noise has a detrimental effect on learning.}
}
@article{MURRAYRUST20111697,
title = {Towards a model of musical interaction and communication},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1697-1721},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000038},
author = {Dave Murray-Rust and Alan Smaill},
keywords = {Multi-agent systems, Musical agents, Communicative acts, Human computer interaction},
abstract = {In this paper we argue there is a need for a formalised description of musical interaction, which allows reasoning about the musical decisions of human and computational players. To this end, we define a simple model of musical transmission which is amenable to distribution among several musical agents. On top of this, we construct a model of musical perception, based on analysis functions from the musical surface to values on lattices. These values are then used to construct a musical context, allowing for a music-oriented version of concepts such as common ground. This context allows for the interpretation of individual musical output as a stream of discrete actions, with the possibility of constructing sets of performative actions, analogous to those used in Speech Act Theory. This allows musical agent systems to construct output in terms of a communicative dialogue, and should enable more responsive, intelligent participation from these virtual musicians. Finally, we discuss a prototype system which implements these concepts in order to perform piano duets with human performers, and discuss how this theory can be seen as a better defined extension of previous theories.}
}
@article{THIMM20131,
title = {Inconsistency measures for probabilistic logics},
journal = {Artificial Intelligence},
volume = {197},
pages = {1-24},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000131},
author = {Matthias Thimm},
keywords = {Inconsistency measures, Inconsistency management, Probabilistic reasoning, Probabilistic conditional logic},
abstract = {Inconsistencies in knowledge bases are of major concern in knowledge representation and reasoning. In formalisms that employ model-based reasoning mechanisms inconsistencies render a knowledge base useless due to the non-existence of a model. In order to restore consistency an analysis and understanding of inconsistencies are mandatory. Recently, the field of inconsistency measurement has gained some attention for knowledge representation formalisms based on classical logic. An inconsistency measure is a tool that helps the knowledge engineer in obtaining insights into inconsistencies by assessing their severity. In this paper, we investigate inconsistency measurement in probabilistic conditional logic, a logic that incorporates uncertainty and focuses on the role of conditionals, i.e. ifâ€“then rules. We do so by extending inconsistency measures for classical logic to the probabilistic setting. Further, we propose novel inconsistency measures that are specifically tailored for the probabilistic case. These novel measures use distance measures to assess the distance of a knowledge base to a consistent one and therefore takes the crucial role of probabilities into account. We analyze the properties of the discussed measures and compare them using a series of rationality postulates.}
}
@article{BARAL1998107,
title = {Formalizing narratives using nested circumscription},
journal = {Artificial Intelligence},
volume = {104},
number = {1},
pages = {107-164},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00070-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000708},
author = {Chitta Baral and Alfredo Gabaldon and Alessandro Provetti},
keywords = {Narratives, Nested abnormality theories, Circumscription, Reasoning about actions, Value minimization},
abstract = {Representing and reasoning about narratives together with the ability to do hypothetical reasoning is important for agents in a dynamic world. These agents need to record their observations and action executions as a narrative and at the same time, to achieve their goals against a changing environment, they need to make plans (or re-plan) from the current situation. The early action formalisms did one or the other. For example, while the original situation calculus was meant for hypothetical reasoning and planning, the event calculus was more appropriate for narratives. Recently, there have been some attempts at developing formalisms that do both. Independently, there has also been a lot of recent research in reasoning about actions using circumscription. Of particular interest to us is the research on using high-level languages and their logical representation using nested abnormality theories (NATs)â€”a form of circumscription with blocks that make knowledge representation modular. Starting from theories in the high-level language l, which is extended to allow concurrent actions, we define a translation to NATs that preserves both narrative and hypothetical reasoning. We initially use the high level language l, and then extend it to allow concurrent actions. In the process, we study several knowledge representation issues such as filtering, and restricted monotonicity with respect to NATs. Finally, we compare our formalization with other approaches, and discuss how our use of NATs makes it easier to incorporate other features of action theories, such as constraints, to our formalization.}
}
@article{CANDUCCI2022103579,
title = {Probabilistic modelling of general noisy multi-manifold data sets},
journal = {Artificial Intelligence},
volume = {302},
pages = {103579},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103579},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221001302},
author = {M. Canducci and P. TiÃ±o and M. Mastropietro},
keywords = {Latent variable models, Dimensionality estimation, Multi-manifold Learning, Riemannian manifolds, Generative topographic mapping, Density estimation, Probabilistic modelling},
abstract = {The intrinsic nature of noisy and complex data sets is often concealed in low-dimensional structures embedded in a higher dimensional space. Number of methodologies have been developed to extract and represent such structures in the form of manifolds (i.e. geometric structures that locally resemble continuously deformable intervals of Rj1). Usually a-priori knowledge of the manifold's intrinsic dimensionality is required. Additionally, their performance can often be hampered by the presence of a significant high-dimensional noise aligned along the low-dimensional core manifold. In real-world applications, the data can contain several low-dimensional structures of different dimensionalities. We propose a framework for dimensionality estimation and reconstruction of multiple noisy manifolds embedded in a noisy environment. To the best of our knowledge, this work represents the first attempt at detection and modelling of a set of coexisting general noisy manifolds by uniting two aspects of multi-manifold learning: the recovery and approximation of core noiseless manifolds and the construction of their probabilistic models. The easy-to-understand hyper-parameters can be manipulated to obtain an emerging picture of the multi-manifold structure of the data. We demonstrate the workings of the framework on two synthetic data sets, presenting challenging features for state-of-the-art techniques in Multi-Manifold learning. The first data set consists of multiple sampled noisy manifolds of different intrinsic dimensionalities, such as MÃ¶bius strip, toroid and spiral arm. The second one is a topologically complex set of three interlocked toroids. Given the absence of such unified methodologies in the literature, the comparison with existing techniques is organized along the two separate aspects of our approach mentioned above, namely manifold approximation and probabilistic modelling. The framework is then applied to a complex data set containing simulated gas volume particles from a particle simulation of a dwarf galaxy interacting with its host galaxy cluster. Detailed analysis of the recovered 1D and 2D manifolds can help us to understand the nature of Star Formation in such complex systems.}
}
@article{SEGRE1996301,
title = {Exploratory analysis of speedup learning data using expectation maximization},
journal = {Artificial Intelligence},
volume = {85},
number = {1},
pages = {301-319},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00115-8},
url = {https://www.sciencedirect.com/science/article/pii/0004370295001158},
author = {Alberto Maria Segre and Geoffrey J. Gordon and Charles P. Elkan},
abstract = {Experimental evaluations of speedup learning methods have in the past used non-parametric hypothesis testing to determine whether or not learning is beneficial. We show here how to obtain deeper insight into the comparative performance of learning methods through a complementary parametric approach to data analysis. In this approach experimental data is used to estimate values for the parameters of a statistical model of the performance of a problem solver. To model problem solvers that use speedup learning methods, we propose a two-component linear model that captures how learned knowledge may accelerate the solution of some problems while leaving the solution of others relatively unchanged. We show how to apply expectation maximization (EM), a statistical technique, to fit this kind of multi-component model. EM allows us to fit the model in the presence of censored data, a methodological difficulty common to experiments involving speedup learning.}
}
@article{GUROV2022103728,
title = {Knowledge-based strategies for multi-agent teams playing against Nature},
journal = {Artificial Intelligence},
volume = {309},
pages = {103728},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103728},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000686},
author = {Dilian Gurov and Valentin Goranko and Edvin Lundberg},
keywords = {Multi-agent games, Imperfect information, Higher-order knowledge, Knowledge-based strategies, Strategy synthesis, Dec-POMDP},
abstract = {We study teams of agents that play against Nature towards achieving a common objective. The agents are assumed to have imperfect information due to partial observability, and have no communication during the play of the game. We propose a natural notion of higher-order knowledge of agents. Based on this notion, we define a class of knowledge-based strategies, and consider the problem of synthesis of strategies of this class. We introduce a multi-agent extension, MKBSC, of the well-known knowledge-based subset construction applied to such games. Its iterative applications turn out to compute higher-order knowledge of the agents. We show how the MKBSC can be used for the design of knowledge-based strategy profiles, and investigate the transfer of existence of such strategies between the original game and in the iterated applications of the MKBSC, under some natural assumptions. We also relate and compare the â€œintensionalâ€ view on knowledge-based strategies based on explicit knowledge representation and update, with the â€œextensionalâ€ view on finite memory strategies based on finite transducers and show that, in a certain sense, these are equivalent.}
}
@article{LASKEY2008140,
title = {MEBN: A language for first-order Bayesian knowledge bases},
journal = {Artificial Intelligence},
volume = {172},
number = {2},
pages = {140-178},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001312},
author = {Kathryn Blackmond Laskey},
keywords = {Bayesian network, Graphical probability models, Knowledge representation, Multi-entity Bayesian network, Probabilistic logic, Uncertainty in artificial intelligence},
abstract = {Although classical first-order logic is the de facto standard logical foundation for artificial intelligence, the lack of a built-in, semantically grounded capability for reasoning under uncertainty renders it inadequate for many important classes of problems. Probability is the best-understood and most widely applied formalism for computational scientific reasoning under uncertainty. Increasingly expressive languages are emerging for which the fundamental logical basis is probability. This paper presents Multi-Entity Bayesian Networks (MEBN), a first-order language for specifying probabilistic knowledge bases as parameterized fragments of Bayesian networks. MEBN fragments (MFrags) can be instantiated and combined to form arbitrarily complex graphical probability models. An MFrag represents probabilistic relationships among a conceptually meaningful group of uncertain hypotheses. Thus, MEBN facilitates representation of knowledge at a natural level of granularity. The semantics of MEBN assigns a probability distribution over interpretations of an associated classical first-order theory on a finite or countably infinite domain. Bayesian inference provides both a proof theory for combining prior knowledge with observations, and a learning theory for refining a representation as evidence accrues. A proof is given that MEBN can represent a probability distribution on interpretations of any finitely axiomatizable first-order theory.}
}
@article{YANG2013440,
title = {Improving resource allocation strategies against human adversaries in security games: An extended study},
journal = {Artificial Intelligence},
volume = {195},
pages = {440-469},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S000437021200152X},
author = {Rong Yang and Christopher Kiekintveld and Fernando OrdÃ³Ã±ez and Milind Tambe and Richard John},
keywords = {Bounded rationality, Stackelberg games, Decision-making},
abstract = {Stackelberg games have garnered significant attention in recent years given their deployment for real world security. Most of these systems, such as ARMOR, IRIS and GUARDS have adopted the standard game-theoretical assumption that adversaries are perfectly rational, which is standard in the game theory literature. This assumption may not hold in real-world security problems due to the bounded rationality of human adversaries, which could potentially reduce the effectiveness of these systems. In this paper, we focus on relaxing the unrealistic assumption of perfectly rational adversary in Stackelberg security games. In particular, we present new mathematical models of human adversariesÊ¼ behavior, based on using two fundamental theory/method in human decision making: Prospect Theory (PT) and stochastic discrete choice model. We also provide methods for tuning the parameters of these new models. Additionally, we propose a modification of the standard quantal response based model inspired by rank-dependent expected utility theory. We then develop efficient algorithms to compute the best response of the security forces when playing against the different models of adversaries. In order to evaluate the effectiveness of the new models, we conduct comprehensive experiments with human subjects using a web-based game, comparing them with models previously proposed in the literature to address the perfect rationality assumption on part of the adversary. Our experimental results show that the subjectsÊ¼ responses follow the assumptions of our new models more closely than the previous perfect rationality assumption. We also show that the defender strategy produced by our new stochastic discrete choice model outperform the previous leading contender for relaxing the assumption of perfect rationality. Furthermore, in a separate set of experiments, we show the benefits of our modified stochastic model (QRRU) over the standard model (QR).11This paper significantly extends our previous conference paper (Yang et al., 2011) [1] by providing (i) new methods for setting parameters of the Prospect Theory model; (ii) an additional variant of Quantal Response model and a new algorithm to compute defender strategies against the new model; (iii) a more comprehensive set of experiments which includes multiple new algorithms and updated settings for the algorithms; (iv) new analysis of the robustness of different defender strategies and the predictive accuracy of different models; (v) additional discussion of related work.}
}
@article{KONTCHAKOV201443,
title = {Spatial reasoning with RCC8 and connectedness constraints in Euclidean spaces},
journal = {Artificial Intelligence},
volume = {217},
pages = {43-75},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214001039},
author = {Roman Kontchakov and Ian Pratt-Hartmann and Michael Zakharyaschev},
keywords = {Qualitative spatial reasoning, Spatial logic, Euclidean space, Connectedness, Satisfiability, Complexity},
abstract = {The language RCC8 is a widely-studied formalism for describing topological arrangements of spatial regions. The variables of this language range over the collection of non-empty, regular closed sets of n-dimensional Euclidean space, here denoted RC+(Rn), and its non-logical primitives allow us to specify how the interiors, exteriors and boundaries of these sets intersect. The key question is the satisfiability problem: given a finite set of atomic RCC8-constraints in m variables, determine whether there exists an m-tuple of elements of RC+(Rn) satisfying them. These problems are known to coincide for all nâ‰¥1, so that RCC8-satisfiability is independent of dimension. This common satisfiability problem is NLogSpace-complete. Unfortunately, RCC8 lacks the means to say that a spatial region comprises a â€˜single pieceâ€™, and the present article investigates what happens when this facility is added. We consider two extensions of RCC8: RCC8c, in which we can state that a region is connected, and RCC8câˆ˜, in which we can instead state that a region has a connected interior. The satisfiability problems for both these languages are easily seen to depend on the dimension n, for nâ‰¤3. Furthermore, in the case of RCC8câˆ˜, we show that there exist finite sets of constraints that are satisfiable over RC+(R2), but only by â€˜wildâ€™ regions having no possible physical meaning. This prompts us to consider interpretations over the more restrictive domain of non-empty, regular closed, polyhedral sets, RCP+(Rn). We show that (a) the satisfiability problems for RCC8c (equivalently, RCC8câˆ˜) over RC+(R) and RCP+(R) are distinct and both NP-complete; (b) the satisfiability problems for RCC8c over RC+(R2) and RCP+(R2) are identical and NP-complete; (c) the satisfiability problems for RCC8câˆ˜ over RC+(R2) and RCP+(R2) are distinct, and the latter is NP-complete. Decidability of the satisfiability problem for RCC8câˆ˜ over RC+(R2) is open. For nâ‰¥3, RCC8c and RCC8câˆ˜ are not interestingly different from RCC8. We finish by answering the following question: given that a set of RCC8c- or RCC8câˆ˜-constraints is satisfiable over RC+(Rn) or RCP+(Rn), how complex is the simplest satisfying assignment? In particular, we exhibit, for both languages, a sequence of constraints Î¦n, satisfiable over RCP+(R2), such that the size of Î¦n grows polynomially in n, while the smallest configuration of polygons satisfying Î¦n cuts the plane into a number of pieces that grows exponentially. We further show that, over RC+(R2), RCC8c again requires exponentially large satisfying diagrams, while RCC8câˆ˜ can force regions in satisfying configurations to have infinitely many components.}
}
@article{BERLEANT1997215,
title = {Qualitative and quantitative simulation: bridging the gap},
journal = {Artificial Intelligence},
volume = {95},
number = {2},
pages = {215-255},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00050-7},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000507},
author = {Daniel Berleant and Benjamin J. Kuipers},
keywords = {Qualitative, Simulation, Semi-quantitative, Intervals, QSIM, Q3},
abstract = {Shortcomings of qualitative simulation and of quantitative simulation motivate combining them to do simulations exhibiting strengths of both. The resulting class of techniques is called semiquantitative simulation. One approach to semi-quantitative simulation is to use numeric intervals to represent incomplete quantitative information. In this research we demonstrate semi-quantitative simulation using intervals in an implemented semi-quantitative simulator called Q3. Q3 progressively refines a qualitative simulation, providing increasingly specific quantitative predictions which can converge to a numerical simulation in the limit while retaining important correctness guarantees from qualitative and interval simulation techniques. Q3's simulations are based on a technique we call step size refinement. While a pure qualitative simulation has a very coarse step size, representing the state of a system trajectory at relatively few qualitatively distinct states, Q3 interpolates newly explicit states between distinct qualitative states, thereby representing more states which instantiate new constraints, leading to improved quantitative inferences. Q3's techniques have been used for prediction, measurement interpretation, diagnosis, and even analysis of the probabilities of qualitative behaviors. Because Q3 shares important expressive and inferential properties of both qualitative and quantitative simulation, Q3 helps to bridge the gap between qualitative and quantitative simulation.}
}
@article{SCHOCKAERT20081158,
title = {Temporal reasoning about fuzzy intervals},
journal = {Artificial Intelligence},
volume = {172},
number = {8},
pages = {1158-1193},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000039},
author = {Steven Schockaert and Martine {De Cock}},
keywords = {Temporal reasoning, Interval algebra, Fuzzy set theory},
abstract = {Traditional approaches to temporal reasoning assume that time periods and time spans of events can be accurately represented as intervals. Real-world time periods and events, on the other hand, are often characterized by vague temporal boundaries, requiring appropriate generalizations of existing formalisms. This paper presents a framework for reasoning about qualitative and metric temporal relations between vague time periods. In particular, we show how several interesting problems, like consistency and entailment checking, can be reduced to reasoning tasks in existing temporal reasoning frameworks. We furthermore demonstrate that all reasoning tasks of interest are NP-complete, which reveals that adding vagueness to temporal reasoning does not increase its computational complexity. To support efficient reasoning, a large tractable subfragment is identified, among others, generalizing the well-known ORD Horn subfragment of the Interval Algebra (extended with metric constraints).}
}
@article{YU20081809,
title = {A new approach to estimating the expected first hitting time of evolutionary algorithms},
journal = {Artificial Intelligence},
volume = {172},
number = {15},
pages = {1809-1832},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000830},
author = {Yang Yu and Zhi-Hua Zhou},
keywords = {Evolutionary algorithms, Expected first hitting time, Convergence rate, Computational complexity},
abstract = {Evolutionary algorithms (EA) have been shown to be very effective in solving practical problems, yet many important theoretical issues of them are not clear. The expected first hitting time is one of the most important theoretical issues of evolutionary algorithms, since it implies the average computational time complexity. In this paper, we establish a bridge between the expected first hitting time and another important theoretical issue, i.e., convergence rate. Through this bridge, we propose a new general approach to estimating the expected first hitting time. Using this approach, we analyze EAs with different configurations, including three mutation operators, with/without population, a recombination operator and a time variant mutation operator, on a hard problem. The results show that the proposed approach is helpful for analyzing a broad range of evolutionary algorithms. Moreover, we give an explanation of what makes a problem hard to EAs, and based on the recognition, we prove the hardness of a general problem.}
}
@article{KOLLER1997167,
title = {Representations and solutions for game-theoretic problems},
journal = {Artificial Intelligence},
volume = {94},
number = {1},
pages = {167-215},
year = {1997},
note = {Economic Principles of Multi-Agent Systems},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00023-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000234},
author = {Daphne Koller and Avi Pfeffer},
keywords = {Game theory, Algorithms, Imperfect information, Multi-agent systems, Game playing, Logic programming, Poker},
abstract = {A system with multiple interacting agents (whether artificial or human) is often best analyzed using game-theoretic tools. Unfortunately, while the formal foundations are well-established, standard computational techniques for game-theoretic reasoning are inadequate for dealing with realistic games. This paper describes the Gala system, an implemented system that allows the specification and efficient solution of large imperfect information games. The system contains the first implementation of a recent algorithm, due to Koller, Megiddo and von Stengel. Experimental results from the system demonstrate that the algorithm is exponentially faster than the standard algorithm in practice, not just in theory. It therefore allows the solution of games that are orders of magnitude larger than were previously possible. The system also provides a new declarative language for compactly and naturally representing games by their rules. As a whole, the Gala system provides the capability for automated game-theoretic analysis of complex real-world situations.}
}
@article{GALLES19979,
title = {Axioms of causal relevance},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {9-43},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00047-7},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000477},
author = {David Galles and Judea Pearl},
keywords = {Causality, Graphoids, Causal models, Counterfactuals, Actions},
abstract = {This paper develops axioms and formal semantics for statements of the form â€œX is causally irrelevant to Y in context Zâ€, which we interpret to mean â€œChanging X will not affect Y once Z is held constantâ€. The axiomization of causal irrelevance is contrasted with the axiomization of informational irrelevance, as in â€œFinding X will not alter our belief in Y, once we know Zâ€. Two versions of causal irrelevance are analyzed: probabilistic and deterministic. We show that, unless stability is assumed, the probabilistic definition yields a very loose structure that is governed by just two trivial axioms. Under the stability assumption, probabilistic causal irrelevance is isomorphic to path interception in cyclic graphs. Under the deterministic definition, causal irrelevance complies with all of the axioms of path interception in cyclic graphs except transitivity. We compare our formalism to that of Lewis (1973) and offer a graphical method of proving theorems about causal relevance.}
}
@article{SUTTON1999181,
title = {Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
journal = {Artificial Intelligence},
volume = {112},
number = {1},
pages = {181-211},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00052-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000521},
author = {Richard S. Sutton and Doina Precup and Satinder Singh},
keywords = {Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes},
abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include optionsâ€”closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.}
}
@article{LOMBARDI2017343,
title = {Empirical decision model learning},
journal = {Artificial Intelligence},
volume = {244},
pages = {343-367},
year = {2017},
note = {Combining Constraint Solving with Mining and Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216000126},
author = {Michele Lombardi and Michela Milano and Andrea Bartolini},
keywords = {Combinatorial optimization, Machine learning, Complex systems, Local search, Constraint programming, Mixed integer non-linear programming, SAT modulo theories, Artificial neural networks, Decision trees},
abstract = {One of the biggest challenges in the design of real-world decision support systems is coming up with a good combinatorial optimization model. Often enough, accurate predictive models (e.g. simulators) can be devised, but they are too complex or too slow to be employed in combinatorial optimization. In this paper, we propose a methodology called Empirical Model Learning (EML) that relies on Machine Learning for obtaining components of a prescriptive model, using data either extracted from a predictive model or harvested from a real system. In a way, EML can be considered as a technique to merge predictive and prescriptive analytics. All models introduce some form of approximation. Citing G.E.P. Box [1] â€œEssentially, all models are wrong, but some of them are usefulâ€. In EML, models are useful if they provide adequate accuracy, and if they can be effectively exploited by solvers for finding high-quality solutions. We show how to ground EML on a case study of thermal-aware workload dispatching. We use two learning methods, namely Artificial Neural Networks and Decision Trees and we show how to encapsulate the learned model in a number of optimization techniques, namely Local Search, Constraint Programming, Mixed Integer Non-Linear Programming and SAT Modulo Theories. We demonstrate the effectiveness of the EML approach by comparing our results with those obtained using expert-designed models.}
}
@article{BENELIYAHUZOHARY1997421,
title = {Reasoning with minimal models: efficient algorithms and applications},
journal = {Artificial Intelligence},
volume = {96},
number = {2},
pages = {421-449},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00060-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029700060X},
author = {Rachel Ben-Eliyahu-Zohary and Luigi Palopoli},
keywords = {Minimal models, Disjunctive databases, Disjunctive default logic, Disjunctive logic programs, Stable model semantics, Linear time algorithms},
abstract = {Reasoning with minimal models is at the heart of many knowledge-representation systems. Yet it turns out that this task is formidable, even when very simple theories are considered. In this paper, we introduce the elimination algorithm, which performs, in linear time, minimal model finding and minimal model checking for a significant subclass of positive CNF theories which we call positive head-cycle-free (HCF) theories. We also prove that the task of minimal entailment is easier for positive HCF theories than it is for the class of all positive CNF theories. Finally, we show how variations of the elimination algorithm can be applied to allow queries posed on disjunctive deductive databases and disjunctive default theories to be answered in an efficient way.}
}
@article{BRAFMAN1998317,
title = {On the knowledge requirements of tasks},
journal = {Artificial Intelligence},
volume = {98},
number = {1},
pages = {317-349},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00061-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000611},
author = {Ronen I. Brafman and Joseph Y. Halpern and Yoav Shoham},
keywords = {Knowledge, Sensor design, Configuration space, Manipulation tasks, (Skeletal) knowledge-based programs, Knowledge complexity, Knowledge capability},
abstract = {In order to successfully perform a task, a situated system requires some information about its domain. If we can understand what information the system requires, we may be able to equip it with more suitable sensors or make better use of the information available to it. These considerations have motivated roboticists to examine the issue of sensor design, and in particular, the minimal information required to perform a task. We show here that reasoning in terms of what the robot knows and needs to know to perform a task is a useful approach for analyzing these issues. We extend the formal framework for reasoning about knowledge, already used in AI and distributed computing, by developing a set of basic concepts and tools for modeling and analyzing the knowledge requirements of tasks. We investigate properties of the resulting framework, and show how it can be applied to robotics tasks.}
}
@article{LANG2008991,
title = {On propositional definability},
journal = {Artificial Intelligence},
volume = {172},
number = {8},
pages = {991-1017},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000027},
author = {JÃ©rÃ´me Lang and Pierre Marquis},
keywords = {Knowledge representation, Propositional logic, Computational complexity, Definability, Hypothesis discriminability, Reasoning about action and change},
abstract = {In standard propositional logic, logical definability is the ability to derive the truth value of some propositional symbols given a propositional formula and the truth values of some propositional symbols. Although appearing more or less informally in various AI settings, a computation-oriented investigation of the notion is still lacking, and this paper aims at filling the gap. After recalling the two definitions of definability, which are equivalent in standard propositional logic (while based on different intuitions), and defining a number of related notions, we give several characterization results, and many complexity results for definability. We also show close connections with hypothesis discriminability and with reasoning about action and change.}
}
@article{DECOOMAN20111911,
title = {Independent natural extension},
journal = {Artificial Intelligence},
volume = {175},
number = {12},
pages = {1911-1950},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000737},
author = {Gert {de Cooman} and Enrique Miranda and Marco Zaffalon},
keywords = {Epistemic irrelevance, Epistemic independence, Independent natural extension, Strong product, Factorisation, Coherent lower previsions},
abstract = {There is no unique extension of the standard notion of probabilistic independence to the case where probabilities are indeterminate or imprecisely specified. Epistemic independence is an extension that formalises the intuitive idea of mutual irrelevance between different sources of information. This gives epistemic independence very wide scope as well as appeal: this interpretation of independence is often taken as natural also in precise-probabilistic contexts. Nevertheless, epistemic independence has received little attention so far. This paper develops the foundations of this notion for variables assuming values in finite spaces. We define (epistemically) independent products of marginals (or possibly conditionals) and show that there always is a unique least-committal such independent product, which we call the independent natural extension. We supply an explicit formula for it, and study some of its properties, such as associativity, marginalisation and external additivity, which are basic tools to work with the independent natural extension. Additionally, we consider a number of ways in which the standard factorisation formula for independence can be generalised to an imprecise-probabilistic context. We show, under some mild conditions, that when the focus is on least-committal models, using the independent natural extension is equivalent to imposing a so-called strong factorisation property. This is an important outcome for applications as it gives a simple tool to make sure that inferences are consistent with epistemic independence judgements. We discuss the potential of our results for applications in Artificial Intelligence by recalling recent work by some of us, where the independent natural extension was applied to graphical models. It has allowed, for the first time, the development of an exact linear-time algorithm for the imprecise probability updating of credal trees.}
}
@article{LIU2006909,
title = {Analyzing the degree of conflict among belief functions},
journal = {Artificial Intelligence},
volume = {170},
number = {11},
pages = {909-924},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000658},
author = {Weiru Liu},
keywords = {Dempsterâ€“Shafer theory, Dempster's combination rule, Conflicting beliefs, Betting commitments},
abstract = {The study of alternative combination rules in DS theory when evidence is in conflict has emerged again recently as an interesting topic, especially in data/information fusion applications. These studies have mainly focused on investigating which alternative would be appropriate for which conflicting situation, under the assumption that a conflict is identified. The issue of detection (or identification) of conflict among evidence has been ignored. In this paper, we formally define when two basic belief assignments are in conflict. This definition deploys quantitative measures of both the mass of the combined belief assigned to the emptyset before normalization and the distance between betting commitments of beliefs. We argue that only when both measures are high, it is safe to say the evidence is in conflict. This definition can be served as a prerequisite for selecting appropriate combination rules.}
}
@article{PRADHAN1996363,
title = {The sensitivity of belief networks to imprecise probabilities: an experimental investigation},
journal = {Artificial Intelligence},
volume = {85},
number = {1},
pages = {363-397},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(96)00002-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370296000021},
author = {Malcolm Pradhan and Max Henrion and Gregory Provan and Brendan {Del Favero} and Kurt Huang},
keywords = {Probabilistic reasoning, Bayesian networks},
abstract = {Bayesian belief networks are being increasingly used as a knowledge representation for reasoning under uncertainty. Some researchers have questioned the practicality of obtaining the numerical probabilities with sufficient precision to create belief networks for large-scale applications. In this work, we investigate how precise the probabilities need to be by measuring how imprecision in the probabilities affects diagnostic performance. We conducted a series of experiments on a set of real-world belief networks for medical diagnosis in liver and bile disease. We examined the effects on diagnostic performance of (1) varying the mappings from qualitative frequency weights into numerical probabilities, (2) adding random noise to the numerical probabilities, (3) simplifying from quaternary domains for diseases and findingsâ€”absent, mild, moderate, and severeâ€”to binary domainsâ€”absent and present, and (4) using test cases that contain diseases outside the network. We found that even extreme differences in the probability mappings and large amounts of noise lead to only modest reductions in diagnostic performance. We found no significant effect of the simplification from quaternary to binary representation. We also found that outside diseases degraded performance modestly. Overall, these findings indicate that even highly imprecise input probabilities may not impair diagnostic performance significantly, and that simple binary representations may often be adequate. These findings of robustness suggest that belief networks are a practical representation without requiring undue precision.}
}
@article{HOEY2016134,
title = {Affect control processes: Intelligent affective interaction using a partially observable Markov decision process},
journal = {Artificial Intelligence},
volume = {230},
pages = {134-172},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S000437021500140X},
author = {Jesse Hoey and Tobias SchrÃ¶der and Areej Alhothali},
keywords = {Affect, Emotion, Sociology, Affect control theory, Markov decision process, Intelligent tutoring system, Assistive technology, Humanâ€“computer interaction},
abstract = {This paper describes a novel method for building affectively intelligent human-interactive agents. The method is based on a key sociological insight that has been developed and extensively verified over the last twenty years, but has yet to make an impact in artificial intelligence. The insight is that resource bounded humans will, by default, act to maintain affective consistency. Humans have culturally shared fundamental affective sentiments about identities, behaviours, and objects, and they act so that the transient affective sentiments created during interactions confirm the fundamental sentiments. Humans seek and create situations that confirm or are consistent with, and avoid and suppress situations that disconfirm or are inconsistent with, their culturally shared affective sentiments. This â€œaffect control principleâ€ has been shown to be a powerful predictor of human behaviour. In this paper, we present a probabilistic and decision-theoretic generalisation of this principle, and we demonstrate how it can be leveraged to build affectively intelligent artificial agents. The new model, called BayesAct, can maintain multiple hypotheses about sentiments simultaneously as a probability distribution, and can make use of an explicit utility function to make value-directed action choices. This allows the model to generate affectively intelligent interactions with people by learning about their identity, predicting their behaviours using the affect control principle, and taking actions that are simultaneously goal-directed and affect-sensitive. We demonstrate this generalisation with a set of simulations. We then show how our model can be used as an emotional â€œplug-inâ€ for artificially intelligent systems that interact with humans in two different settings: an exam practice assistant (tutor) and an assistive device for persons with a cognitive disability.}
}
@article{CONITZER2006607,
title = {Complexity of constructing solutions in the core based on synergies among coalitions},
journal = {Artificial Intelligence},
volume = {170},
number = {6},
pages = {607-619},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000099},
author = {Vincent Conitzer and Tuomas Sandholm},
abstract = {Coalition formation is a key problem in automated negotiation among self-interested agents, and other multiagent applications. A coalition of agents can sometimes accomplish things that the individual agents cannot, or can accomplish them more efficiently. Motivating the agents to abide by a solution requires careful analysis: only some of the solutions are stable in the sense that no group of agents is motivated to break off and form a new coalition. This constraint has been studied extensively in cooperative game theory: the set of solutions that satisfy it is known as the core. The computational questions around the core have received less attention. When it comes to coalition formation among software agents (that represent real-world parties), these questions become increasingly explicit. In this paper we define a concise, natural, general representation for games in characteristic form that relies on superadditivity. In our representation, individual agents' values are given as well as values for those coalitions that introduce synergies. We show that this representation allows for efficient checking of whether a given outcome is in the core. We then show that determining whether the core is nonempty is NP-complete both with and without transferable utility. We demonstrate that what makes the problem hard in both cases is determining the collaborative possibilities (the set of outcomes possible for the grand coalition); we do so by showing that if these are given, the problem becomes solvable in time polynomial in the size of the representation in both cases. However, we then demonstrate that for a hybrid version of the problem, where utility transfer is possible only within the grand coalition, the problem remains NP-complete even when the collaborative possibilities are given. Finally, we show that for convex characteristic functions, a solution in the core can be computed efficiently (in O(nl2) time, where n is the number of agents and l is the number of synergies), even when the collaborative possibilities are not given in advance.}
}
@article{SAFFIOTTI1995481,
title = {A multivalued logic approach to integrating planning and control},
journal = {Artificial Intelligence},
volume = {76},
number = {1},
pages = {481-526},
year = {1995},
note = {Planning and Scheduling},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00088-I},
url = {https://www.sciencedirect.com/science/article/pii/000437029400088I},
author = {Alessandro Saffiotti and Kurt Konolige and Enrique H. Ruspini},
abstract = {elligent agents embedded in a dynamic, uncertain environment should incorporate capabilities for both planned and reactive behavior. Many current solutions to this dual need focus on one aspect, and treat the other one as secondary. We propose an approach for integrating planning and control based on behavior schemas, which link physical movements to abstract action descriptions. Behavior schemas describe behaviors of an agent, expressed as trajectories of control actions in an environment, and goals can be defined as predicates on these trajectories. Goals and behaviors can be combined to produce conjoint goals and complex controls. The ability of multivalued logics to represent graded preferences allows us to formulate tradeoffs in the combination. Two composition theorems relate complex controls to complex goals, and provide the key to using standard knowledge-based deliberation techniques to generate complex controllers. We report experiments in planning and execution on a mobile robot platform, Flakey.}
}
@article{GRECO2010382,
title = {On the power of structural decompositions of graph-based representations of constraint problems},
journal = {Artificial Intelligence},
volume = {174},
number = {5},
pages = {382-409},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209001507},
author = {Gianluigi Greco and Francesco Scarcello},
keywords = {Constraint satisfaction, Decomposition methods, Hypergraphs, Dual graphs, Incidence graphs, Primal graphs, Treewidth},
abstract = {The Constraint Satisfaction Problem (CSP) is a central issue of research in Artificial Intelligence. Due to its intractability, many efforts have been made in order to identify tractable classes of CSP instances, and in fact deep and useful results have already been achieved. In particular, this paper focuses on structural decomposition methods, which are essentially meant to look for near-acyclicity properties of the graphs or hypergraphs that encode the structure of the constraints interactions. In general, constraint scopes comprise an arbitrary number of variables, and thus this structure may be naturally encoded via hypergraphs. However, in many practical applications, decomposition methods are applied over suitable graph representations of the (possibly non-binary) CSP instances at hand. Despite the great interest in such binary approaches, a formal analysis of their power, in terms of their ability of identifying islands of tractability, was missing in the literature. The aim of this paper is precisely to fill this gap, by studying the relationships among binary structural methods, and by providing a clear picture of the tractable fragments of CSP that can be specified with respect to each of these decomposition approaches, when they are applied to binary representations of non-binary CSP instances. In particular, various long-standing questions about primal, dual and incidence graph encodings are answered. The picture is then completed by comparing methods on binary encodings with methods specifically conceived for non-binary constraints.}
}
@article{ALVIANO2012156,
title = {Magic Sets for disjunctive Datalog programs},
journal = {Artificial Intelligence},
volume = {187-188},
pages = {156-192},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000562},
author = {Mario Alviano and Wolfgang Faber and Gianluigi Greco and Nicola Leone},
keywords = {Logic programming, Stable models, Magic Sets, Answer set programming, Data integration},
abstract = {In this paper, a new technique for the optimization of (partially) bound queries over disjunctive Datalog programs with stratified negation is presented. The technique exploits the propagation of query bindings and extends the Magic Set optimization technique (originally defined for non-disjunctive programs). An important feature of disjunctive Datalog programs is non-monotonicity, which calls for non-deterministic implementations, such as backtracking search. A distinguishing characteristic of the new method is that the optimization can be exploited also during the non-deterministic phase. In particular, after some assumptions have been made during the computation, parts of the program may become irrelevant to a query under these assumptions. This allows for dynamic pruning of the search space. In contrast, the effect of the previously defined Magic Set methods for disjunctive Datalog is limited to the deterministic portion of the process. In this way, the potential performance gain by using the proposed method can be exponential, as could be observed empirically. The correctness of the method is established and proved in a formal way thanks to a strong relationship between Magic Sets and unfounded sets that has not been studied in the literature before. This knowledge allows for extending the method and the correctness proof also to programs with stratified negation in a natural way. The proposed method has been implemented in the DLV system and various experiments on synthetic as well as on real-world data have been conducted. The experimental results on synthetic data confirm the utility of Magic Sets for disjunctive Datalog, and they highlight the computational gain that may be obtained by the new method with respect to the previously proposed Magic Set method for disjunctive Datalog programs. Further experiments on data taken from a real-life application show the benefits of the Magic Set method within an application scenario that has received considerable attention in recent years, the problem of answering user queries over possibly inconsistent databases originating from integration of autonomous sources of information.}
}
@article{KLIEGR2021103458,
title = {A review of possible effects of cognitive biases on interpretation of rule-based machine learning models},
journal = {Artificial Intelligence},
volume = {295},
pages = {103458},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103458},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000096},
author = {TomÃ¡Å¡ Kliegr and Å tÄ›pÃ¡n BahnÃ­k and Johannes FÃ¼rnkranz},
keywords = {Cognitive bias, Cognitive illusion, Interpretability, Machine learning, Rule induction},
abstract = {While the interpretability of machine learning models is often equated with their mere syntactic comprehensibility, we think that interpretability goes beyond that, and that human interpretability should also be investigated from the point of view of cognitive science. The goal of this paper is to discuss to what extent cognitive biases may affect human understanding of interpretable machine learning models, in particular of logical rules discovered from data. Twenty cognitive biases are covered, as are possible debiasing techniques that can be adopted by designers of machine learning algorithms and software. Our review transfers results obtained in cognitive psychology to the domain of machine learning, aiming to bridge the current gap between these two areas. It needs to be followed by empirical studies specifically focused on the machine learning domain.}
}
@article{FIONDA20131,
title = {The complexity of mixed multi-unit combinatorial auctions: Tractability under structural and qualitative restrictions},
journal = {Artificial Intelligence},
volume = {196},
pages = {1-25},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437021200166X},
author = {Valeria Fionda and Gianluigi Greco},
keywords = {Mixed multi-unit combinatorial auctions, Computational complexity, Structural decomposition methods},
abstract = {Mixed multi-unit combinatorial auctions (MMUCAs) are extensions of classical combinatorial auctions (CAs) where bidders trade transformations of goods rather than just sets of goods. Solving MMUCAs, i.e., determining the sequences of bids to be accepted by the auctioneer, is computationally intractable in general. However, differently from classical combinatorial auctions, little was known about whether polynomial-time solvable classes of MMUCAs can be singled out on the basis of their characteristics. The paper fills this gap, by studying the computational complexity of MMUCA instances under structural and qualitative restrictions, which characterize interactions among bidders and types of bids involved in the various transformations, respectively.}
}
@article{LAWRY201220,
title = {On truth-gaps, bipolar belief and the assertability of vague propositions},
journal = {Artificial Intelligence},
volume = {191-192},
pages = {20-41},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000902},
author = {Jonathan Lawry and Yongchuan Tang},
keywords = {Vagueness, Truth-gaps, Valuation pairs, Semantic uncertainty, Integrated uncertainty, Bipolar belief measures},
abstract = {This paper proposes an integrated approach to indeterminacy and epistemic uncertainty in order to model an intelligent agentÊ¼s decision making about the assertability of vague statements. Initially, valuation pairs are introduced as a model of truth-gaps for propositional logic sentences. These take the form of lower and upper truth-valuations representing absolutely true and not absolutely false respectively. In particular, we consider valuation pairs based on supervaluationist principles and also on KleeneÊ¼s three-valued logic. The relationship between Kleene valuation pairs and supervaluation pairs is then explored in some detail with particular reference to a natural ordering on semantic precision. In the second part of the paper we extend this approach by proposing bipolar belief pairs as an integrated model combining epistemic uncertainty and indeterminacy. These comprise of lower and upper belief measures on propositional sentences, defined by a probability distribution on a finite set of possible valuation pairs. The properties of these measures are investigated together with their relationship to different types of uncertainty measure. Finally, we apply bipolar belief measures in a preliminary decision theoretic study so as to begin to understand how the use of vague expressions can help to mitigate the risk associated with making forecasts or promises. This then has potential applications to natural language generation systems.}
}
@article{LESPERANCE199569,
title = {Indexical knowledge and robot actionâ€”a logical account},
journal = {Artificial Intelligence},
volume = {73},
number = {1},
pages = {69-115},
year = {1995},
note = {Computational Research on Interaction and Agency, Part 2},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00010-X},
url = {https://www.sciencedirect.com/science/article/pii/000437029400010X},
author = {Yves LespÃ©rance and Hector J. Levesque},
abstract = {The knowledge required for action is generally indexical rather than objective. For example, a robot that knows the relative position of an object is generally able to go and pick it up; he need not know its absolute position. Agents may have very incomplete knowledge of their situation in terms of what objective facts hold and still be able to achieve their goals. This paper presents a formal theory of knowledge and action, embodied in a modal logic, that handles the distinction between indexical and objective knowledge and allows a proper specification of the knowledge prerequisites and effects of action. Several kinds of robotics situations involving indexical knowledge are formalized within the framework; these examples show how actions can be specified so as to avoid making excessive requirements upon the knowledge of agents.}
}
@article{YIN201832,
title = {Optimal defense against election control by deleting voter groups},
journal = {Artificial Intelligence},
volume = {259},
pages = {32-51},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S000437021830050X},
author = {Yue Yin and Yevgeniy Vorobeychik and Bo An and Noam Hazon},
keywords = {Election control, Protecting elections, Security games},
abstract = {Election control encompasses attempts from an external agent to alter the structure of an election in order to change its outcome. This problem is both a fundamental theoretical problem in social choice, and a major practical concern for democratic institutions. Consequently, this issue has received considerable attention, particularly as it pertains to different voting rules. In contrast, the problem of how election control can be prevented or deterred has been largely ignored. We introduce the problem of optimal defense against election control, including destructive and constructive control, where manipulation is allowed at the granularity of groups of voters (e.g., voting locations) through a denial-of-service attack, and the defender allocates limited protection resources to prevent control. We consider plurality voting, and show that it is computationally hard to prevent both types of control, though destructive control itself can be performed in polynomial time. For defense against destructive control, we present a double-oracle framework for computing an optimal prevention strategy. We show that both defender and attacker best response subproblems are NP-complete, and develop exact mixed-integer linear programming approaches for solving these, as well as fast heuristic methods. We then extend this general approach to develop effective algorithmic solutions for defense against constructive control. Finally, we generalize the model and algorithmic approaches to consider uncertainty about voter preferences. Experiments conducted on both synthetic and real data demonstrate that the proposed computational framework can scale to realistic problem instances.1}
}
@article{FATIMA20081673,
title = {A linear approximation method for the Shapley value},
journal = {Artificial Intelligence},
volume = {172},
number = {14},
pages = {1673-1699},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000696},
author = {Shaheen S. Fatima and Michael Wooldridge and Nicholas R. Jennings},
keywords = {Coalitional game theory, Shapley value, Approximation method},
abstract = {The Shapley value is a key solution concept for coalitional games in general and voting games in particular. Its main advantage is that it provides a unique and fair solution, but its main drawback is the complexity of computing it (e.g., for voting games this complexity is #p-complete). However, given the importance of the Shapley value and voting games, a number of approximation methods have been developed to overcome this complexity. Among these, Owen's multi-linear extension method is the most time efficient, being linear in the number of players. Now, in addition to speed, the other key criterion for an approximation algorithm is its approximation error. On this dimension, the multi-linear extension method is less impressive. Against this background, this paper presents a new approximation algorithm, based on randomization, for computing the Shapley value of voting games. This method has time complexity linear in the number of players, but has an approximation error that is, on average, lower than Owen's. In addition to this comparative study, we empirically evaluate the error for our method and show how the different parameters of the voting game affect it. Specifically, we show the following effects. First, as the number of players in a voting game increases, the average percentage error decreases. Second, as the quota increases, the average percentage error decreases. Third, the error is different for players with different weights; players with weight closer to the mean weight have a lower error than those with weight further away. We then extend our approximation to the more general k-majority voting games and show that, for n players, the method has time complexity O(k2n) and the upper bound on its approximation error is O(k2/n).}
}
@article{CHABERT20091079,
title = {Contractor programming},
journal = {Artificial Intelligence},
volume = {173},
number = {11},
pages = {1079-1100},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000381},
author = {Gilles Chabert and Luc Jaulin},
keywords = {Constraint processing, Interval methods, Solver design, Programming languages},
abstract = {This paper describes a solver programming method, called contractor programming, that copes with two issues related to constraint processing over the reals. First, continuous constraints involve an inevitable step of solver design. Existing softwares provide an insufficient answer by restricting users to choose among a list of fixed strategies. Our first contribution is to give more freedom in solver design by introducing programming concepts where only configuration parameters were previously available. Programming consists in applying operators (intersection, composition, etc.) on algorithms called contractors that are somehow similar to propagators. Second, many problems with real variables cannot be cast as the search for vectors simultaneously satisfying the set of constraints, but a large variety of different outputs may be demanded from a set of constraints (e.g., a paving with boxes inside and outside of the solution set). These outputs can actually be viewed as the result of different contractors working concurrently on the same search space, with a bisection procedure intervening in case of deadlock. Such algorithms (which are not strictly speaking solvers) will be made easy to build thanks to a new branch & prune system, called paver. Thus, this paper gives a way to deal harmoniously with a larger set of problems while giving a fine control on the solving mechanisms. The contractor formalism and the paver system are the two contributions. The approach is motivated and justified through different cases of study. An implementation of this framework named Quimper is also presented.}
}
@article{GIUNCHIGLIA1997409,
title = {Representing action: indeterminacy and ramifications},
journal = {Artificial Intelligence},
volume = {95},
number = {2},
pages = {409-438},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00037-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000374},
author = {Enrico Giunchiglia and G.Neelakantan Kartha and Vladimir Lifschitz},
keywords = {Action languages, Circumscription, Nested abnormality theories, Nondeterministic actions, Ramification problem},
abstract = {We define and study a high-level language for describing actions, more expressive than the action language A introduced by Gelfond and Lifschitz. The new language, AR, allows us to describe actions with indirect effects (ramifications), nondeterministic actions, and actions that may be impossible to execute. It has symbols for nonpropositional fluents and for the fluents that are exempt from the commonsense law of inertia. Temporal projection problems specified using the language AR can be represented as nested abnormality theories based on the situation calculus.}
}
@article{YAMAN20111290,
title = {Democratic approximation of lexicographic preference models},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1290-1307},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002018},
author = {Fusun Yaman and Thomas J. Walsh and Michael L. Littman and Marie desJardins},
keywords = {Lexicographic models, Preference learning, Bayesian methods},
abstract = {Lexicographic preference models (LPMs) are an intuitive representation that corresponds to many real-world preferences exhibited by human decision makers. Previous algorithms for learning LPMs produce a â€œbest guessâ€ LPM that is consistent with the observations. Our approach is more democratic: we do not commit to a single LPM. Instead, we approximate the target using the votes of a collection of consistent LPMs. We present two variations of this methodâ€”variable voting and model votingâ€”and empirically show that these democratic algorithms outperform the existing methods. Versions of these democratic algorithms are presented in both the case where the preferred values of attributes are known and the case where they are unknown. We also introduce an intuitive yet powerful form of background knowledge to prune some of the possible LPMs. We demonstrate how this background knowledge can be incorporated into variable and model voting and show that doing so improves performance significantly, especially when the number of observations is small.}
}
@article{BENELIYAHU1996113,
title = {Default reasoning using classical logic},
journal = {Artificial Intelligence},
volume = {84},
number = {1},
pages = {113-150},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00095-X},
url = {https://www.sciencedirect.com/science/article/pii/000437029500095X},
author = {Rachel Ben-Eliyahu and Rina Dechter},
abstract = {In this paper we show how propositional default theories can be characterized by classical propositional theories: for each finite default theory, we show a classical propositional theory such that there is a one-to-one correspondence between models for the latter and extensions of the former. This means that computing extensions and answering queries about coherence, set-membership and set-entailment are reducible to propositional satisfiability. The general transformation is exponential but tractable for a subset which we call 2-DTâ€”a superset of network default theories and disjunction-free default theories. Consequently, coherence and set-membership for the class 2-DT is NP-complete and set-entailment is co-NP-complete. This work paves the way for the application of decades of research on efficient algorithms for the satisfiability problem to default reasoning. For example, since prepositional satisfiability can be regarded as a constraint satisfaction problem (CSP), this work enables us to use CSP techniques for default reasoning. To illustrate this point we use the taxonomy of tractable CSPs to identify new tractable subsets for Reiter's default logic. Our procedures allow also for computing stable models of extended logic programs.}
}
@article{SAVELLI2006643,
title = {Existential assertions and quantum levels on the tree of the situation calculus},
journal = {Artificial Intelligence},
volume = {170},
number = {6},
pages = {643-652},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000063},
author = {Francesco Savelli},
keywords = {Knowledge representation, Reasoning about actions and change, Situation calculus},
abstract = {In a seminal paper, Reiter introduced a variant of the situation calculus along with a set of its properties. To the best of our knowledge, one of these properties has remained unproved and ignored despite its relevance to the planning problem and the expressivity of the theories of actions. We state this property in a more general form and provide its proof. Intuitively, whenever a theory of actions entails that there exists a situation satisfying a first order formula (e.g., a goal), at least one such situation must be found within a predetermined distance from the initial situation. This distance is finite and the same in all the models of the theory, since it depends only on the theory and the formula at hand.}
}
@article{GUTIERREZ2017123,
title = {From model checking to equilibrium checking: Reactive modules for rational verification},
journal = {Artificial Intelligence},
volume = {248},
pages = {123-157},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300413},
author = {Julian Gutierrez and Paul Harrenstein and Michael Wooldridge},
keywords = {Complexity of equilibria, Reactive modules, Temporal logic},
abstract = {Model checking is the best-known and most successful approach to formally verifying that systems satisfy specifications, expressed as temporal logic formulae. In this article, we develop the theory of equilibrium checking, a related but distinct problem. Equilibrium checking is relevant for multi-agent systems in which system components (agents) are assumed to be acting rationally in pursuit of delegated goals, and is concerned with understanding what temporal properties hold of such systems under the assumption that agents select strategies in equilibrium. The formal framework we use to study this problem assumes agents are modelled using Reactive Modules, a system modelling language that is used in a range of practical model checking systems. Each agent (or player) in a Reactive Modules game is specified as a nondeterministic guarded command program, and each player's goal is specified with a temporal logic formula that the player desires to see satisfied. A strategy for a player in a Reactive Modules game defines how that player selects enabled guarded commands for execution over successive rounds of the game. For this general setting, we investigate games in which players have goals specified in Linear Temporal Logic (in which case it is assumed that players choose deterministic strategies) and in Computation Tree Logic (in which case players select nondeterministic strategies). For each of these cases, after formally defining the game setting, we characterise the complexity of a range of problems relating to Nash equilibria (e.g., the computation or the verification of existence of a Nash equilibrium or checking whether a given temporal formula is satisfied on some Nash equilibrium). We then go on to show how the model we present can be used to encode, for example, games in which the choices available to players are specified using STRIPS planning operators.}
}
@article{TANG20112010,
title = {Discovering theorems in game theory: Two-person games with unique pure Nash equilibrium payoffs},
journal = {Artificial Intelligence},
volume = {175},
number = {14},
pages = {2010-2020},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000762},
author = {Pingzhong Tang and Fangzhen Lin},
keywords = {Computer-aided theorem discovery, Game theory, Pure Nash equilibrium, Uniqueness of pure Nash equilibrium payoff, Strict games, Strictly competitive games},
abstract = {In this paper we provide a logical framework for two-person finite games in strategic form, and use it to design a computer program for discovering some classes of games that have unique pure Nash equilibrium payoffs. The classes of games that we consider are those that can be expressed by a conjunction of two binary clauses, and our program re-discovered Kats and ThisseÊ¼s class of weakly unilaterally competitive two-person games, and came up with several other classes of games that have unique pure Nash equilibrium payoffs. It also came up with new classes of strict games that have unique pure Nash equilibria, where a game is strict if for both player different profiles have different payoffs.}
}
@article{PINEDA2007197,
title = {Conservation principles and action schemes in the synthesis of geometric concepts},
journal = {Artificial Intelligence},
volume = {171},
number = {4},
pages = {197-238},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206001433},
author = {Luis A. Pineda},
keywords = {Diagrammatic reasoning, Diagrammatic theorem-proving, Knowledge representation, Geometric description, Geometric abstraction, Conservation principles, Action schemes, Structured learning, Synthetic concepts},
abstract = {In this paper a theory for the synthesis of geometric concepts is presented. The theory is focused on a constructive process that synthesizes a function in the geometric domain representing a geometric concept. Geometric theorems are instances of this kind of concepts. The theory involves four main conceptual components: conservation principles, action schemes, descriptions of geometric abstractions and reinterpretations of diagrams emerging during the generative process. A notion of diagrammatic derivation in which the external representation and its interpretation are synthesized in tandem is also introduced in this paper. The theory is exemplified with a diagrammatic proof of the Theorem of Pythagoras. The theory also illustrates how the arithmetic interpretation of this theorem is produced in tandem with its diagrammatic derivation under an appropriate representational mapping. A second case study in which an arithmetic theorem is synthesized from an underlying geometric concept is also included. An interactive prototype program in which the inference load is shared between the system and the human user is also presented. The paper is concluded with a reflection on the expressive power of diagrams, their effectiveness in representation and inference, and the relation between synthetic and analytic knowledge in the realization of theorems and their proofs.}
}
@article{ZABKAR20111604,
title = {Learning qualitative models from numerical data},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1604-1619},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000361},
author = {Jure Å½abkar and Martin MoÅ¾ina and Ivan Bratko and Janez DemÅ¡ar},
keywords = {Qualitative modelling, Regression, Partial derivatives, Monotone models},
abstract = {Qualitative models describe relations between the observed quantities in qualitative terms. In predictive modelling, a qualitative model tells whether the output increases or decreases with the input. We describe PadÃ©, a new method for qualitative learning which estimates partial derivatives of the target function from training data and uses them to induce qualitative models of the target function. We formulated three methods for computation of derivatives, all based on using linear regression on local neighbourhoods. The methods were empirically tested on artificial and real-world data. We also provide a case study which shows how the developed methods can be used in practice.}
}
@article{GERBER2008351,
title = {Representation of occurrences for road vehicle traffic},
journal = {Artificial Intelligence},
volume = {172},
number = {4},
pages = {351-391},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001142},
author = {R. Gerber and H.-H. Nagel},
keywords = {Computer vision, Knowledge representation, Temporal reasoning, Reasoning about actions and change, Fuzzy metric-temporal logic},
abstract = {Our 3D-model-based Computer Vision subsystem extracts vehicle trajectories from monocular digitized videos recording road vehicles in inner-city traffic. Steps are documented which import these quantitative geometrical results into a conceptual representation based on a Fuzzy Metric-Temporal Horn Logic (FMTHL, see [K.H. SchÃ¤fer, Unscharfe zeitlogische Modellierung von Situationen und Handlungen in Bildfolgenauswertung und Robotik, Dissertation, 1996]). The facts created by this import step can be understood as verb phrases which describe elementary actions of vehicles in image sequences of road traffic scenes. The current contribution suggests a complete conceptual representation of elementary vehicle actions and reports results obtained by an implementation of this approach from real-world traffic videos.}
}
@article{SELMAN199617,
title = {Generating hard satisfiability problems},
journal = {Artificial Intelligence},
volume = {81},
number = {1},
pages = {17-29},
year = {1996},
note = {Frontiers in Problem Solving: Phase Transitions and Complexity},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00045-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000453},
author = {Bart Selman and David G. Mitchell and Hector J. Levesque},
keywords = {Satisfiability, Random problems, Phase transitions, 4.3, Benchmarks, Empirical study},
abstract = {We report results from large-scale experiments in satisfiability testing. As has been observed by others, testing the satisfiability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satisfiability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability testing procedures.}
}
@article{BOUVERET2009343,
title = {Computing leximin-optimal solutions in constraint networks},
journal = {Artificial Intelligence},
volume = {173},
number = {2},
pages = {343-364},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001495},
author = {Sylvain Bouveret and Michel LemaÃ®tre},
keywords = {Leximin, Fairness, Multiobjective optimization, Constraint programming},
abstract = {In many real-world multiobjective optimization problems one needs to find solutions or alternatives that provide a fair compromise between different conflicting objective functionsâ€”which could be criteria in a multicriteria context, or agent utilities in a multiagent contextâ€”while being efficient (i.e. informally, ensuring the greatest possible overall agents' satisfaction). This is typically the case in problems implying human agents, where fairness and efficiency requirements must be met. Preference handling, resource allocation problems are another examples of the need for balanced compromises between several conflicting objectives. A way to characterize good solutions in such problems is to use the leximin preorder to compare the vectors of objective values, and to select the solutions which maximize this preorder. In this article, we describe five algorithms for finding leximin-optimal solutions using constraint programming. Three of these algorithms are original. Other ones are adapted, in constraint programming settings, from existing works. The algorithms are compared experimentally on three benchmark problems.}
}
@article{AKGUN2022103751,
title = {Conjure: Automatic Generation of Constraint Models from Problem Specifications},
journal = {Artificial Intelligence},
volume = {310},
pages = {103751},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103751},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000911},
author = {Ã–zgÃ¼r AkgÃ¼n and Alan M. Frisch and Ian P. Gent and Christopher Jefferson and Ian Miguel and Peter Nightingale},
keywords = {Constraint modelling, Constraint programming, Combinatorial optimization, Constraint satisfaction problem},
abstract = {When solving a combinatorial problem, the formulation or model of the problem is critical to the efficiency of the solver. Automating the modelling process has long been of interest because of the expertise and time required to produce an effective model of a given problem. We describe a method to automatically produce constraint models from a problem specification written in the abstract constraint specification language Essence. Our approach is to incrementally refine the specification into a concrete model by applying a chosen refinement rule at each step. Any non-trivial specification may be refined in multiple ways, creating a space of models to choose from. The handling of symmetries is a particularly important aspect of automated modelling. Many combinatorial optimisation problems contain symmetry, which can lead to redundant search. If a partial assignment is shown to be invalid, we are wasting time if we ever consider a symmetric equivalent of it. A particularly important class of symmetries are those introduced by the constraint modelling process: modelling symmetries. We show how modelling symmetries may be broken automatically as they enter a model during refinement, obviating the need for an expensive symmetry detection step following model formulation. Our approach is implemented in a system called Conjure. We compare the models produced by Conjure to constraint models from the literature that are known to be effective. Our empirical results confirm that Conjure can reproduce successfully the kernels of the constraint models of 42 benchmark problems found in the literature.}
}
@article{KRAUS1995297,
title = {Multiagent negotiation under time constraints},
journal = {Artificial Intelligence},
volume = {75},
number = {2},
pages = {297-345},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00021-R},
url = {https://www.sciencedirect.com/science/article/pii/000437029400021R},
author = {Sarit Kraus and Jonathan Wilkenfeld and Gilad Zlotkin},
abstract = {Research in distributed artificial intelligence (DAI) is concerned with how automated agents can be designed to interact effectively. Negotiation is proposed as a means for agents to communicate and compromise to reach mutually beneficial agreements. The paper examines the problems of resource allocation and task distribution among autonomous agents which can benefit from sharing a common resource or distributing a set of common tasks. We propose a strategic model of negotiation that takes the passage of time during the negotiation process itself into account. A distributed negotiation mechanism is introduced that is simple, efficient, stable, and flexible in various situations. The model considers situations characterized by complete as well as incomplete information, and ones in which some agents lose over time while others gain over time. Using this negotiation mechanism autonomous agents have simple and stable negotiation strategies that result in efficient agreements without delays even when there are dynamic changes in the environment.}
}
@article{FROESE2009466,
title = {Enactive artificial intelligence: Investigating the systemic organization of life and mind},
journal = {Artificial Intelligence},
volume = {173},
number = {3},
pages = {466-500},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208002105},
author = {Tom Froese and Tom Ziemke},
keywords = {Embodied, Situated, Enactive, Cognitive science, Agency, Autonomy, Intentionality, Design principles, Natural cognition, Modeling},
abstract = {The embodied and situated approach to artificial intelligence (AI) has matured and become a viable alternative to traditional computationalist approaches with respect to the practical goal of building artificial agents, which can behave in a robust and flexible manner under changing real-world conditions. Nevertheless, some concerns have recently been raised with regard to the sufficiency of current embodied AI for advancing our scientific understanding of intentional agency. While from an engineering or computer science perspective this limitation might not be relevant, it is of course highly relevant for AI researchers striving to build accurate models of natural cognition. We argue that the biological foundations of enactive cognitive science can provide the conceptual tools that are needed to diagnose more clearly the shortcomings of current embodied AI. In particular, taking an enactive perspective points to the need for AI to take seriously the organismic roots of autonomous agency and sense-making. We identify two necessary systemic requirements, namely constitutive autonomy and adaptivity, which lead us to introduce two design principles of enactive AI. It is argued that the development of such enactive AI poses a significant challenge to current methodologies. However, it also provides a promising way of eventually overcoming the current limitations of embodied AI, especially in terms of providing fuller models of natural embodied cognition. Finally, some practical implications and examples of the two design principles of enactive AI are also discussed.}
}
@article{FUDENBERG2007378,
title = {An economist's perspective on multi-agent learning},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {378-381},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000501},
author = {Drew Fudenberg and David K. Levine},
keywords = {Learning in games, Multi-agent learning, Machine learning},
abstract = {We comment on the Shoham, Powers, and Grenager survey of multi-agent learning and game theory, emphasizing that some of their categories are important for economics and others are not. We also try to correct some minor imprecisions in their discussion of the economics literature on learning in games.}
}
@article{SANDEWALL20101431,
title = {Defeasible inheritance with doubt index and its axiomatic characterization},
journal = {Artificial Intelligence},
volume = {174},
number = {18},
pages = {1431-1459},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S000437021000144X},
author = {Erik Sandewall},
keywords = {Defeasible inheritance, Multiple inheritance with exceptions, Nonmonotonic logic, Underlying semantics},
abstract = {This article introduces and uses a representation of defeasible inheritance networks where links in the network are viewed as propositions, and where defeasible links are tagged with a quantitative indication of the proportion of exceptions, called the doubt index. This doubt index is used for restricting the length of the chains of inference. The representation also introduces the use of defeater literals that disable the chaining of subsumption links. The use of defeater literals replaces the use of negative defeasible inheritance links, expressing â€œmost A are not Bâ€. The new representation improves the expressivity significantly. Inference in inheritance networks is defined by a combination of axioms that constrain the contents of network extensions, a heuristic restriction that also has that effect, and a nonmonotonic operation of minimizing the set of defeater literals while retaining consistency. We introduce an underlying semantics that defines the meaning of literals in a network, and prove that the axioms are sound with respect to this semantics. We also discuss the conditions for obtaining completeness. Traditional concepts, assumptions and issues in research on nonmonotonic or defeasible inheritance are reviewed in the perspective of this approach.}
}
@article{PIRRI2011378,
title = {The well-designed logical robot: Learning and experience from observations to the Situation Calculus},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {378-415},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000524},
author = {Fiora Pirri},
keywords = {Visual perception, Action space, Action recognition, Parametric probability model, Learning knowledge, Inference from visual perception to knowledge representation, Theory of action, Learning theory of action from visual perception},
abstract = {The well-designed logical robot paradigmatically represents, in the words of McCarthy, the abilities that a robot-child should have to reveal the structure of reality within a â€œlanguage of thoughtâ€. In this paper we partially support McCarthy's hypothesis by showing that early perception can trigger an inference process leading to the â€œlanguage of thoughtâ€. We show this by defining a systematic transformation of structures of different formal languages sharing the same signature kernel for actions and states. Starting from early vision, visual features are encoded by descriptors mapping the space of features into the space of actions. The densities estimated in this space form the observation layer of a hidden states model labelling the identified actions as observations and the states as action preconditions and effects. The learned parameters are used to specify the probability space of a first-order probability model. Finally we show how to transform the probability model into a model of the Situation Calculus in which the learning phase has been reified into axioms for preconditions and effects of actions and, of course, these axioms are expressed in the language of thought. This shows, albeit partially, that there is an underlying structure of perception that can be brought into a logical language.}
}
@article{KNOX2023103829,
title = {Reward (Mis)design for autonomous driving},
journal = {Artificial Intelligence},
volume = {316},
pages = {103829},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103829},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001692},
author = {W. Bradley Knox and Alessandro Allievi and Holger Banzhaf and Felix Schmitt and Peter Stone},
keywords = {Reinforcement learning, Reward design, Utility, Cost, Safety, Risk, Autonomous driving},
abstract = {This article considers the problem of diagnosing certain common errors in reward design. Its insights are also applicable to the design of cost functions and performance metrics more generally. To diagnose common errors, we develop 8 simple sanity checks for identifying flaws in reward functions. We survey research that is published in top-tier venues and focuses on reinforcement learning (RL) for autonomous driving (AD). Specifically, we closely examine the reported reward function in each publication and present these reward functions in a complete and standardized format in the appendix. Wherever we have sufficient information, we apply the 8 sanity checks to each surveyed reward function, revealing near-universal flaws in reward design for AD that might also exist pervasively across reward design for other tasks. Lastly, we explore promising directions that may aid the design of reward functions for AD in subsequent research, following a process of inquiry that can be adapted to other domains.}
}
@article{MONDERER2007448,
title = {Learning equilibrium as a generalization of learning to optimize},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {448-452},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000069},
author = {Dov Monderer and Moshe Tennenholtz},
keywords = {Learning, Machine learning, Learning equilibrium},
abstract = {We argue that learning equilibrium is an appropriate generalization to multi-agent systems of the concept of learning to optimize in single-agent setting. We further define and discuss the concept of weak learning equilibrium.}
}
@article{JONSSON20181,
title = {Constants and finite unary relations in qualitative constraint reasoning},
journal = {Artificial Intelligence},
volume = {257},
pages = {1-23},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217301698},
author = {Peter Jonsson},
keywords = {Constraint satisfaction, Qualitative reasoning, Computational complexity},
abstract = {Extending qualitative CSPs with the ability of restricting selected variables to finite sets of possible values has been proposed as an interesting research direction with important applications, cf. â€œQualitative constraint satisfaction problems: an extended framework with landmarksâ€ by Li, Liu, and Wang (2013) [48]. Previously presented complexity results for this kind of extended formalisms have typically focused on concrete examples and not on general principles. We propose three general methods. The first two methods are based on analysing the given CSP from a model-theoretical perspective, while the third method is based on directly analysing the growth of the representation of solutions. We exemplify the methods on temporal and spatial formalisms including Allen's algebra and RCC-5.}
}
@article{CABALAR2011346,
title = {Formalising the Fisherman's Folly puzzle},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {346-377},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000408},
author = {Pedro Cabalar and Paulo E. Santos},
keywords = {Common sense reasoning, Qualitative spatial reasoning, Reasoning about actions and change},
abstract = {This paper investigates the challenging problem of encoding the common sense knowledge involved in the manipulation of spatial objects from a reasoning about actions and change perspective. In particular, we propose a formal solution to a puzzle composed of non-trivial objects (such as holes and strings) assuming a version of the Situation Calculus written over first-order Equilibrium Logic, whose models generalise the stable model semantics.}
}
@article{MEIR2012123,
title = {Algorithms for strategyproof classification},
journal = {Artificial Intelligence},
volume = {186},
pages = {123-156},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S000437021200029X},
author = {Reshef Meir and Ariel D. Procaccia and Jeffrey S. Rosenschein},
keywords = {Mechanism design, Classification, Game theory, Approximation},
abstract = {The strategyproof classification problem deals with a setting where a decision maker must classify a set of input points with binary labels, while minimizing the expected error. The labels of the input points are reported by self-interested agents, who might lie in order to obtain a classifier that more closely matches their own labels, thereby creating a bias in the data; this motivates the design of truthful mechanisms that discourage false reports. In this paper we give strategyproof mechanisms for the classification problem in two restricted settings: (i) there are only two classifiers, and (ii) all agents are interested in a shared set of input points. We show that these plausible assumptions lead to strong positive results. In particular, we demonstrate that variations of a random dictator mechanism, that are truthful, can guarantee approximately optimal outcomes with respect to any family of classifiers. Moreover, these results are tight in the sense that they match the best possible approximation ratio that can be guaranteed by any truthful mechanism. We further show how our mechanisms can be used for learning classifiers from sampled data, and provide PAC-style generalization bounds on their expected error. Interestingly, our results can be applied to problems in the context of various fields beyond classification, including facility location and judgment aggregation.}
}
@article{BODIRSKY2007185,
title = {Determining the consistency of partial tree descriptions},
journal = {Artificial Intelligence},
volume = {171},
number = {2},
pages = {185-196},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206001445},
author = {Manuel Bodirsky and Martin Kutz},
keywords = {Tree descriptions, Constraint satisfaction problems, Graph algorithms, Left-linear point algebra},
abstract = {We present an efficient algorithm that decides the consistency of partial descriptions of ordered trees. The constraint language of these descriptions was introduced by Cornell in computational linguistics; the constraints specify for pairs of nodes sets of admissible relative positions in an ordered tree. Cornell asked for an algorithm to find a tree structure satisfying these constraints. This computational problem generalizes the common-supertree problem studied in phylogenetic analysis, and also generalizes the network consistency problem of the so-called left-linear point algebra. We present the first polynomial time algorithm for Cornell's problem, which runs in time O(mn), where m is the number of constraints and n the number of variables in the constraint.}
}
@article{HARRISON201722,
title = {Infinitary equilibrium logic and strongly equivalent logic programs},
journal = {Artificial Intelligence},
volume = {246},
pages = {22-33},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300164},
author = {Amelia Harrison and Vladimir Lifschitz and David Pearce and AgustÃ­n Valverde},
keywords = {Answer set programming, Strong equivalence, Logic of here-and-there},
abstract = {Strong equivalence is an important concept in the theory of answer set programming. Informally speaking, two sets of rules are strongly equivalent if they have the same meaning in any context. Equilibrium logic was used to prove that sets of rules expressed as propositional formulas are strongly equivalent if and only if they are equivalent in the logic of here-and-there. We extend this line of work to formulas with infinitely long conjunctions and disjunctions, show that the infinitary logic of here-and-there characterizes strong equivalence of infinitary formulas, and give an axiomatization of that logic. This is useful because of the relationship between infinitary formulas and logic programs with local variables.}
}
@article{RAO1995461,
title = {An active vision architecture based on iconic representations},
journal = {Artificial Intelligence},
volume = {78},
number = {1},
pages = {461-505},
year = {1995},
note = {Special Volume on Computer Vision},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00026-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000267},
author = {Rajesh P.N. Rao and Dana H. Ballard},
abstract = {Active vision systems have the capability of continuously interacting with the environment. The rapidly changing environment of such systems means that it is attractive to replace static representations with visual routines that compute information on demand. Such routines place a premium on image data structures that are easily computed and used. The purpose of this paper is to propose a general active vision architecture based on efficiently computable iconic representations. This architecture employs two primary visual routines, one for identifying the visual image near the fovea (object identification), and another for locating a stored prototype on the retina (object location). This design allows complex visual behaviors to be obtained by composing these two routines with different parameters. The iconic representations are comprised of high-dimensional feature vectors obtained from the responses of an ensemble of Gaussian derivative spatial filters at a number of orientations and scales. These representations are stored in two separate memories. One memory is indexed by image coordinates while the other is indexed by object coordinates. Object location matches a localized set of model features with image features at all possible retinal locations. Object identification matches a foveal set of image features with all possible model features. We present experimental results for a near real-time implementation of these routines on a pipeline image processor and suggest relatively simple strategies for tackling the problems of occlusions and scale variations. We also discuss two additional visual routines, one for top-down foveal targeting using log-polar sensors and another for looming detection, which are facilitated by the proposed architecture.}
}
@article{JEANTET20111366,
title = {Computing rank dependent utility in graphical models for sequential decision problems},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1366-1389},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.019},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002080},
author = {Gildas Jeantet and Olivier Spanjaard},
keywords = {Algorithmic decision theory, Rank dependent utility, Decision trees, Influence diagrams, Planning under uncertainty},
abstract = {This paper is devoted to automated sequential decision in AI. More precisely, we focus here on the Rank Dependent Utility (RDU) model. This model is able to encompass rational decision behaviors that the Expected Utility model cannot accommodate. However, the non-linearity of RDU makes it difficult to compute an RDU-optimal strategy in sequential decision problems. This has considerably slowed the use of RDU in operational contexts. In this paper, we are interested in providing new algorithmic solutions to compute an RDU-optimal strategy in graphical models. Specifically, we present algorithms for solving decision tree models and influence diagram models of sequential decision problems. For decision tree models, we propose a mixed integer programming formulation that is valid for a subclass of RDU models (corresponding to risk seeking behaviors). This formulation reduces to a linear program when mixed strategies are considered. In the general case (i.e., when there is no particular assumption on the parameters of RDU), we propose a branch and bound procedure to compute an RDU-optimal strategy among the pure ones. After highlighting the difficulties induced by the use of RDU in influence diagram models, we show how this latter procedure can be extended to optimize RDU in an influence diagram. Finally, we provide empirical evaluations of all the presented algorithms.}
}
@article{SPRACKLEN2023103915,
title = {Automated streamliner portfolios for constraint satisfaction problems},
journal = {Artificial Intelligence},
volume = {319},
pages = {103915},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103915},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000619},
author = {Patrick Spracklen and Nguyen Dang and Ã–zgÃ¼r AkgÃ¼n and Ian Miguel},
keywords = {Constraint programming, Constraint modelling, Constraint satisfaction problem, Algorithm selection},
abstract = {Constraint Programming (CP) is a powerful technique for solving large-scale combinatorial problems. Solving a problem proceeds in two distinct phases: modelling and solving. Effective modelling has a huge impact on the performance of the solving process. Even with the advance of modern automated modelling tools, search spaces involved can be so vast that problems can still be difficult to solve. To further constrain the model, a more aggressive step that can be taken is the addition of streamliner constraints, which are not guaranteed to be sound but are designed to focus effort on a highly restricted but promising portion of the search space. Previously, producing effective streamlined models was a manual, difficult and time-consuming task. This paper presents a completely automated process to the generation, search and selection of streamliner portfolios to produce a substantial reduction in search effort across a diverse range of problems. The results demonstrate a marked improvement in performance for both Chuffed, a CP solver with clause learning, and lingeling, a modern SAT solver.}
}
@article{DUFOURD199873,
title = {Geometric construction by assembling solved subfigures},
journal = {Artificial Intelligence},
volume = {99},
number = {1},
pages = {73-119},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00070-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000702},
author = {Jean-FranÃ§ois Dufourd and Pascal Mathis and Pascal Schreck},
keywords = {Geometric formal construction, System of geometric constraints, Computer-aided design, Local solving, Assembling of figures, Multi-agent system, Blackboard},
abstract = {Among the expected contributions of Artificial Intelligence to Computer-Aided Design is the possibility of constructing a geometric object, the description of which is given by a system of topological and dimensional constraints. This paper presents the theoretical foundations of an original approach to formal geometric construction of rigid bodies in the Euclidian plane, based on invariance under displacements and relaxation of positional constraints. This general idea allows to explain in greater detail several methods proposed in the literature. One of the advantages of this approach is its ability to efficiently generalize and join together different methods for local solving. The paper also describes the main features of a powerful and extensible operational prototype based on these ideas, which can be viewed as a simple multi-agent system with a blackboard. Finally, some significant examples solved by this prototype are presented.}
}
@article{LIN2008823,
title = {Negotiating with bounded rational agents in environments with incomplete information using an automated agent},
journal = {Artificial Intelligence},
volume = {172},
number = {6},
pages = {823-851},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001518},
author = {Raz Lin and Sarit Kraus and Jonathan Wilkenfeld and James Barry},
keywords = {Bilateral negotiation, Bounded rationality, Incomplete information, Automated agent},
abstract = {Many tasks in day-to-day life involve interactions among several people. Many of these interactions involve negotiating over a desired outcome. Negotiation in and of itself is not an easy task, and it becomes more complex under conditions of incomplete information. For example, the parties do not know in advance the exact tradeoff of their counterparts between different outcomes. Furthermore information regarding the preferences of counterparts might only be elicited during the negotiation process itself. In this paper we propose a model for an automated negotiation agent capable of negotiating with bounded rational agents under conditions of incomplete information. We test this agent against people in two distinct domains, in order to verify that its model is generic, and thus can be adapted to any domain as long as the negotiators' preferences can be expressed in additive utilities. Our results indicate that the automated agent reaches more agreements and plays more effectively than its human counterparts. Moreover, in most of the cases, the automated agent achieves significantly better agreements, in terms of individual utility, than the human counterparts playing the same role.}
}
@article{STAMATATOS200537,
title = {Automatic identification of music performers with learning ensembles},
journal = {Artificial Intelligence},
volume = {165},
number = {1},
pages = {37-56},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000196},
author = {Efstathios Stamatatos and Gerhard Widmer},
keywords = {Machine learning, Classification, Ensemble learning, Music},
abstract = {This article addresses the problem of identifying the most likely music performer, given a set of performances of the same piece by a number of skilled candidate pianists. We propose a set of very simple features for representing stylistic characteristics of a music performer, introducing â€˜norm-basedâ€™ features that relate to a kind of â€˜averageâ€™ performance. A database of piano performances of 22 pianists playing two pieces by FrÃ©dÃ©ric Chopin is used in the presented experiments. Due to the limitations of the training set size and the characteristics of the input features we propose an ensemble of simple classifiers derived by both subsampling the training set and subsampling the input features. Experiments show that the proposed features are able to quantify the differences between music performers. The proposed ensemble can efficiently cope with multi-class music performer recognition under inter-piece conditions, a difficult musical task, displaying a level of accuracy unlikely to be matched by human listeners (under similar conditions).}
}
@article{JEFFERSON20101407,
title = {Implementing logical connectives in constraint programming},
journal = {Artificial Intelligence},
volume = {174},
number = {16},
pages = {1407-1429},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000974},
author = {Christopher Jefferson and Neil C.A. Moore and Peter Nightingale and Karen E. Petrie},
keywords = {Constraint programming, Constraint satisfaction problems, Propagation algorithms, Logical connectives},
abstract = {Combining constraints using logical connectives such as disjunction is ubiquitous in constraint programming, because it adds considerable expressive power to a constraint language. We explore the solver architecture needed to propagate such combinations of constraints efficiently. In particular we describe two new features named satisfying sets and constraint trees. We also make use of movable triggers (Gent et al., 2006) [1], and with these three complementary features we are able to make considerable efficiency gains. A key reason for the success of Boolean Satisfiability (SAT) solvers is their ability to propagate Or constraints efficiently, making use of movable triggers. We successfully generalise this approach to an Or of an arbitrary set of constraints, maintaining the crucial property that at most two constraints are active at any time, and no computation at all is done on the others. We also give an And propagator within our framework, which may be embedded within the Or. Using this approach, we demonstrate speedups of over 10,000 times in some cases, compared to traditional constraint programming approaches. We also prove that the Or algorithm enforces generalised arc consistency (GAC) when all its child constraints have a GAC propagator, and no variables are shared between children. By extending the Or propagator, we present a propagator for AtLeastK, which expresses that at least k of its child constraints are satisfied in any solution. Some logical expressions (e.g. exclusive-or) cannot be compactly expressed using And, Or and AtLeastK. Therefore we investigate reification of constraints. We present a fast generic algorithm for reification using satisfying sets and movable triggers.}
}
@article{HALPERN2011220,
title = {Dealing with logical omniscience: Expressiveness and pragmatics},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {220-235},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000457},
author = {Joseph Y. Halpern and Riccardo Pucella},
keywords = {Logic, Knowledge, Logical omniscience, Awareness, Impossible worlds, Probability},
abstract = {We examine four approaches for dealing with the logical omniscience problem and their potential applicability: the syntactic approach, awareness, algorithmic knowledge, and impossible possible worlds. Although in some settings these approaches are equi-expressive and can capture all epistemic states, in other settings of interest (especially with probability in the picture), we show that they are not equi-expressive. We then consider the pragmatics of dealing with logical omniscienceâ€”how to choose an approach and construct an appropriate model.}
}
@article{BEER1995173,
title = {A dynamical systems perspective on agent-environment interaction},
journal = {Artificial Intelligence},
volume = {72},
number = {1},
pages = {173-215},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00005-L},
url = {https://www.sciencedirect.com/science/article/pii/000437029400005L},
author = {Randall D. Beer},
abstract = {Using the language of dynamical systems theory, a general theoretical framework for the synthesis and analysis of autonomous agents is sketched. In this framework, an agent and its environment are modeled as two coupled dynamical systems whose mutual interaction is in general jointly responsible for the agent's behavior. In addition, the adaptive fit between an agent and its environment is characterized in terms of the satisfaction of a given constraint on the trajectories of the coupled agent-environment system. The utility of this framework is demonstrated by using it to first synthesize and then analyze a walking behavior for a legged agent.}
}
@article{DELGADO20111498,
title = {Efficient solutions to factored MDPs with imprecise transition probabilities},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1498-1527},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000026},
author = {Karina Valdivia Delgado and Scott Sanner and Leliane Nunes {de Barros}},
keywords = {Probabilistic planning, Markov Decision Process, Robust planning},
abstract = {When modeling real-world decision-theoretic planning problems in the Markov Decision Process (MDP) framework, it is often impossible to obtain a completely accurate estimate of transition probabilities. For example, natural uncertainty arises in the transition specification due to elicitation of MDP transition models from an expert or estimation from data, or non-stationary transition distributions arising from insufficient state knowledge. In the interest of obtaining the most robust policy under transition uncertainty, the Markov Decision Process with Imprecise Transition Probabilities (MDP-IPs) has been introduced to model such scenarios. Unfortunately, while various solution algorithms exist for MDP-IPs, they often require external calls to optimization routines and thus can be extremely time-consuming in practice. To address this deficiency, we introduce the factored MDP-IP and propose efficient dynamic programming methods to exploit its structure. Noting that the key computational bottleneck in the solution of factored MDP-IPs is the need to repeatedly solve nonlinear constrained optimization problems, we show how to target approximation techniques to drastically reduce the computational overhead of the nonlinear solver while producing bounded, approximately optimal solutions. Our results show up to two orders of magnitude speedup in comparison to traditional â€œflatâ€ dynamic programming approaches and up to an order of magnitude speedup over the extension of factored MDP approximate value iteration techniques to MDP-IPs while producing the lowest error of any approximation algorithm evaluated.}
}
@article{HIRSCH1996267,
title = {Relation algebras of intervals},
journal = {Artificial Intelligence},
volume = {83},
number = {2},
pages = {267-295},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00042-9},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000429},
author = {Robin Hirsch},
abstract = {Given a representation of a relation algebra we construct relation algebras of pairs and of intervals. If the representation happens to be complete, homogeneous and fully universal then the pair and interval algebras can be constructed direct from the relation algebra. If, further, the original relation algebra is Ï‰-categorical we show that the interval algebra is too. The complexity of relation algebras is studied and it is shown that every pair algebra with infinite representations is intractable. Applications include constructing an interval algebra that combines metric and interval expressivity.}
}
@article{BOGAERTS201843,
title = {Fixpoint semantics for active integrity constraints},
journal = {Artificial Intelligence},
volume = {255},
pages = {43-70},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217301364},
author = {Bart Bogaerts and LuÃ­s Cruz-Filipe},
keywords = {Active integrity constraints, Approximation fixpoint theory},
abstract = {Active integrity constraints (AICs) constitute a formalism to associate with a database not just the constraints it should adhere to, but also how to fix the database in case one or more of these constraints are violated. The intuitions regarding which repairs are â€œgoodâ€ given such a description are closely related to intuitions that live in various areas of non-monotonic reasoning, such as logic programming and autoepistemic logic. In this paper, we apply approximation fixpoint theory, an abstract, algebraic framework designed to unify semantics of non-monotonic logics, to the field of AICs. This results in a new family of semantics for AICs. We study properties of our new semantics and relationships to existing semantics. In particular, we argue that two of the newly defined semantics stand out. Grounded repairs have a simple definition that is purely based on semantic principles that semantics for AICs should adhere to. And, as we show, they coincide with the intended interpretation of AICs on many examples. The second semantics of interest is the AFT-well-founded semantics: it is a computationally cheap semantics that provides upper and lower bounds for many other classes of repairs.}
}
@article{GUPTA199545,
title = {3-D motion estimation from motion field},
journal = {Artificial Intelligence},
volume = {78},
number = {1},
pages = {45-86},
year = {1995},
note = {Special Volume on Computer Vision},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00031-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000313},
author = {Naresh C. Gupta and Laveen N. Kanal},
abstract = {Several experiments suggest that the first stage of motion perception is the measurement of visual motion. The result of this stage is called the motion field, which assigns a velocity vector to each point in the image plane. The second stage involves interpreting the motion field in terms of objects and motion in the three-dimensional world. Recovering 3-D motion of the object from the motion field has been difficult owing to the nonlinear system of equations involved, and the sensitivity of the system to noise. The need for the stability of the system is essential as only the optical flow field can be recovered from a sequence of images, which is at best a crude approximation to the motion field. We define two sets of â€œbasicâ€ parameters, which can be recovered from the motion field by solving a linear system of equations. The relationship between the basic parameters and the motion parameter being one-to-one and linear, we obtain a closed form solution for the 3-D motion parameter by solving a system of linear equations only. We prove the correctness, completeness and robustness of the approach and in that sense the problem of recovering the motion parameter from the motion field may be said to be â€œsolvedâ€. We present the results of extensive experimentation with real and simulated image sequences.}
}
@article{NORDH20081245,
title = {What makes propositional abduction tractable},
journal = {Artificial Intelligence},
volume = {172},
number = {10},
pages = {1245-1284},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000222},
author = {Gustav Nordh and Bruno Zanuttini},
keywords = {Abduction, Propositional logic, Computational complexity},
abstract = {Abduction is a fundamental form of nonmonotonic reasoning that aims at finding explanations for observed manifestations. This process underlies many applications, from car configuration to medical diagnosis. We study here the computational complexity of deciding whether an explanation exists in the case when the application domain is described by a propositional knowledge base. Building on previous results, we classify the complexity for local restrictions on the knowledge base and under various restrictions on hypotheses and manifestations. In comparison to the many previous studies on the complexity of abduction we are able to give a much more detailed picture for the complexity of the basic problem of deciding the existence of an explanation. It turns out that depending on the restrictions, the problem in this framework is always polynomial-time solvable, NP-complete, coNP-complete, or Î£2P-complete. Based on these results, we give an a posteriori justification of what makes propositional abduction hard even for some classes of knowledge bases which allow for efficient satisfiability testing and deduction. This justification is very simple and intuitive, but it reveals that no nontrivial class of abduction problems is tractable. Indeed, tractability essentially requires that the language for knowledge bases is unable to express both causal links and conflicts between hypotheses. This generalizes a similar observation by Bylander et al. for set-covering abduction.}
}
@article{SHULTS199791,
title = {Proving properties of continuous systems: qualitative simulation and temporal logic},
journal = {Artificial Intelligence},
volume = {92},
number = {1},
pages = {91-129},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00050-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000501},
author = {Benjamin Shults and Benjamin J. Kuipers},
keywords = {Temporal logic, Qualitative simulation, Model checking, Differential equations},
abstract = {We demonstrate an automated method for proving temporal logic statements about solutions to ordinary differential equations (ODEs), even in the face of an incomplete specification of the ODE. The method combines an implemented, on-the-fly, model checking algorithm for statements in the temporal logic CTLâˆ— with the output of the qualitative simulation algorithm QSIM. Based on the QSIM Guaranteed Coverage Theorem, we prove that for certain CTLâˆ— statements, Î¦, if Î¦ is true for the temporal structure produced by QSIM, then a corresponding temporal statement, Î¦, holds for the solution of any ODE consistent with the qualitative differential equation (QDE) that QSIM used to generate the temporal structure.}
}
@article{LUO2007161,
title = {A spectrum of compromise aggregation operators for multi-attribute decision making},
journal = {Artificial Intelligence},
volume = {171},
number = {2},
pages = {161-184},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206001391},
author = {Xudong Luo and Nicholas R. Jennings},
keywords = {Aggregation operator, Uninorm, Nullnorm, Risk, Multi-attribute decision making, Multi-attribute auction},
abstract = {In many decision making problems, a number of independent attributes or criteria are often used to individually rate an alternative from an agent's local perspective and then these individual ratings are combined to produce an overall assessment. Now, in cases where these individual ratings are not in complete agreement, the overall rating should be somewhere in between the extremes that have been suggested. However, there are many possibilities for the aggregated value. Given this, this paper systematically explores the space of possible compromise operators for such multi-attribute decision making problems. Specifically, we axiomatically identify the complete spectrum of such operators in terms of the properties they should satisfy, and show the main ones that are widely usedâ€”namely averaging operators, uninorms and nullnormsâ€”represent only three of the nine types we identify. For each type, we then go onto analyse their properties and discuss how specific instances can actually be developed. Finally, to illustrate the richness of our framework, we show how a wide range of operators are needed to model the various attitudes that a user may have for aggregation in a given scenario (bidding in multi-attribute auctions).}
}
@article{FRENCH201356,
title = {On the succinctness of some modal logics},
journal = {Artificial Intelligence},
volume = {197},
pages = {56-85},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000222},
author = {Tim French and Wiebe {van der Hoek} and Petar Iliev and Barteld Kooi},
keywords = {Knowledge representation, Modal logic, Succinctness, Epistemic logic, Finite model theory of modal logic, Description logics, Boolean modal logic},
abstract = {One way of comparing knowledge representation formalisms that has attracted attention recently is in terms of representational succinctness, i.e., we can ask whether one of the formalisms allows for a more â€˜economicalâ€™ encoding of information than the other. Proving that one logic is more succinct than another becomes harder when the underlying semantics is stronger. We propose to use Formula Size Games (as put forward by Adler and Immerman (2003) [1], but we present them as games for one player, called Spoiler), games that are played on two sets of models, and that directly link the length of a play in which Spoiler wins the game with the size of a formula, i.e., a formula that is true in the first set of models but false in all models of the second set. Using formula size games, we prove the following succinctness results for m-dimensional modal logic, where one has a set I={i1,â€¦,im} of indices for m modalities: (1) on general Kripke models (and also on binary trees), a definition [âˆ€Î“]Ï†=â‹€iâˆˆÎ“[i]Ï† (with Î“âŠ†I) makes the resulting logic exponentially more succinct for m>1; (2) several modal logics use such abbreviations [âˆ€Î“]Ï†, e.g., in description logics the construct corresponds to adding role disjunctions, and an epistemic interpretation of it is â€˜everybody in Î“ knowsâ€™. Indeed, we show that on epistemic models (i.e., S5-models), the logic with [âˆ€Î“]Ï† becomes more succinct for m>3; (3) the results for the logic with â€˜everybody knowsâ€™ also hold for a logic with â€˜somebody knowsâ€™, and (4) on epistemic models, Public Announcement Logic is exponentially more succinct than epistemic logic, if m>3. The latter settles an open problem raised by Lutz (2006) [18].}
}
@article{LI201567,
title = {Integrating representation learning and skill learning in a human-like intelligent agent},
journal = {Artificial Intelligence},
volume = {219},
pages = {67-91},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214001349},
author = {Nan Li and Noboru Matsuda and William W. Cohen and Kenneth R. Koedinger},
keywords = {Agent learning, Representation learning, Student modeling},
abstract = {Building an intelligent agent that simulates human learning of math and science could potentially benefit both cognitive science, by contributing to the understanding of human learning, and artificial intelligence, by advancing the goal of creating human-level intelligence. However, constructing such a learning agent currently requires manual encoding of prior domain knowledge; in addition to being a poor model of human acquisition of prior knowledge, manual knowledge-encoding is both time-consuming and error-prone. Previous research has shown that one of the key factors that differentiates experts and novices is their different representations of knowledge. Experts view the world in terms of deep functional features, while novices view it in terms of shallow perceptual features. Moreover, since the performance of learning algorithms is sensitive to representation, the deep features are also important in achieving effective machine learning. In this paper, we present an efficient algorithm that acquires representation knowledge in the form of â€œdeep featuresâ€, and demonstrate its effectiveness in the domain of algebra as well as synthetic domains. We integrate this algorithm into a machine-learning agent, SimStudent, which learns procedural knowledge by observing a tutor solve sample problems, and by getting feedback while actively solving problems on its own. We show that learning â€œdeep featuresâ€ reduces the requirements for knowledge engineering. Moreover, we propose an approach that automatically discovers student models using the extended SimStudent. By fitting the discovered model to real student learning curve data, we show that it is a better student model than human-generated models, and demonstrate how the discovered model may be used to improve a tutoring system's instructional strategy.}
}
@article{SCHACHTE2010585,
title = {Information loss in knowledge compilation: A comparison of Boolean envelopes},
journal = {Artificial Intelligence},
volume = {174},
number = {9},
pages = {585-596},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000354},
author = {Peter Schachte and Harald SÃ¸ndergaard and Leigh Whiting and Kevin Henshall},
keywords = {Boolean approximation, Co-clones, Knowledge bases, Tractable inference},
abstract = {Since Selman and Kautz's seminal work on the use of Horn approximation to speed up the querying of knowledge bases, there has been great interest in Boolean approximation for AI applications. There are several Boolean classes with desirable computational properties similar to those of the Horn class. The class of affine Boolean functions, for example, has been proposed as an interesting alternative to Horn for knowledge compilation. To investigate the trade-offs between precision and efficiency in knowledge compilation, we compare, analytically and empirically, four well-known Boolean classes, and their combinations, for ability to preserve information. We note that traditional evaluation which explores unit-clause consequences of random hard 3-CNF formulas does not tell the full story, and we complement that evaluation with experiments based on a variety of assumptions about queries and the underlying knowledge base.}
}
@article{ROSENSCHEIN1995149,
title = {A situated view of representation and control},
journal = {Artificial Intelligence},
volume = {73},
number = {1},
pages = {149-173},
year = {1995},
note = {Computational Research on Interaction and Agency, Part 2},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00056-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000567},
author = {Stanley J. Rosenschein and Leslie Pack Kaelbling},
abstract = {Intelligent agents are systems that have a complex, ongoing interaction with an environment that is dynamic and imperfectly predictable. Agents are typically difficult to program because the correctness of a program depends on the details of how the agent is situated in its environment. In this paper, we present a methodology for the design of situated agents that is based on situated-automata theory. This approach allows designers to describe the informational content of an agent's computational states in a semantically rigorous way without requiring a commitment to conventional run-time symbolic processing. We start by outlining this situated view of representation, then show how it contributes to design methodologies for building systems that track perceptual conditions and take purposeful actions in their environments.}
}
@article{GOGATE2011694,
title = {SampleSearch: Importance sampling in presence of determinism},
journal = {Artificial Intelligence},
volume = {175},
number = {2},
pages = {694-729},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001797},
author = {Vibhav Gogate and Rina Dechter},
keywords = {Probabilistic inference, Approximate inference, Importance sampling, Markov chain Monte Carlo, Bayesian networks, Markov networks, Satisfiability, Model counting, Constraint satisfaction},
abstract = {The paper focuses on developing effective importance sampling algorithms for mixed probabilistic and deterministic graphical models. The use of importance sampling in such graphical models is problematic because it generates many useless zero weight samples which are rejected yielding an inefficient sampling process. To address this rejection problem, we propose the SampleSearch scheme that augments sampling with systematic constraint-based backtracking search. We characterize the bias introduced by the combination of search with sampling, and derive a weighting scheme which yields an unbiased estimate of the desired statistics (e.g., probability of evidence). When computing the weights exactly is too complex, we propose an approximation which has a weaker guarantee of asymptotic unbiasedness. We present results of an extensive empirical evaluation demonstrating that SampleSearch outperforms other schemes in presence of significant amount of determinism.}
}
@article{DAO201770,
title = {Constrained clustering by constraint programming},
journal = {Artificial Intelligence},
volume = {244},
pages = {70-94},
year = {2017},
note = {Combining Constraint Solving with Mining and Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000806},
author = {Thi-Bich-Hanh Dao and Khanh-Chuong Duong and Christel Vrain},
keywords = {Constrained clustering, Bi-criterion clustering, Constraint programming, Modeling, Global optimization constraint, Filtering algorithm},
abstract = {Constrained Clustering allows to make the clustering task more accurate by integrating user constraints, which can be instance-level or cluster-level constraints. Few works consider the integration of different kinds of constraints, they are usually based on declarative frameworks and they are often exact methods, which either enumerate all the solutions satisfying the user constraints, or find a global optimum when an optimization criterion is specified. In a previous work, we have proposed a model for Constrained Clustering based on a Constraint Programming framework. It is declarative, allowing a user to integrate user constraints and to choose an optimization criterion among several ones. In this article we present a new and substantially improved model for Constrained Clustering, still based on a Constraint Programming framework. It differs from our earlier model in the way partitions are represented by means of variables and constraints. It is also more flexible since the number of clusters does not need to be set beforehand; only a lower and an upper bound on the number of clusters have to be provided. In order to make the model-based approach more efficient, we propose new global optimization constraints with dedicated filtering algorithms. We show that such a framework can easily be embedded in a more general process and we illustrate this on the problem of finding the optimal Pareto front of a bi-criterion constrained clustering task. We compare our approach with existing exact approaches, based either on a branch-and-bound approach or on graph coloring on twelve datasets. Experiments show that the model outperforms exact approaches in most cases.}
}
@article{CHEN2011890,
title = {Loop-separable programs and their first-order definability},
journal = {Artificial Intelligence},
volume = {175},
number = {3},
pages = {890-913},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002183},
author = {Yin Chen and Fangzhen Lin and Yan Zhang and Yi Zhou},
keywords = {Answer set programming, First-order definability, Knowledge representation, Nonmonotonic reasoning},
abstract = {An answer set program with variables is first-order definable on finite structures if the set of its finite answer sets can be captured by a first-order sentence. Characterizing classes of programs that are first-order definable on finite structures is theoretically challenging and of practical relevance to answer set programming. In this paper, we identify a non-trivial class of answer set programs called loop-separable programs and show that they are first-order definable on finite structures.}
}
@article{VANALLEN2008483,
title = {Quantifying the uncertainty of a belief net response: Bayesian error-bars for belief net inference},
journal = {Artificial Intelligence},
volume = {172},
number = {4},
pages = {483-513},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001294},
author = {Tim {Van Allen} and Ajit Singh and Russell Greiner and Peter Hooper},
keywords = {Bayesian belief network, Variance, Bucket elimination, Credible interval, Error bars},
abstract = {A Bayesian belief network models a joint distribution over variables using a DAG to represent variable dependencies and network parameters to represent the conditional probability of each variable given an assignment to its immediate parents. Existing algorithms assume each network parameter is fixed. From a Bayesian perspective, however, these network parameters can be random variables that reflect uncertainty in parameter estimates, arising because the parameters are learned from data, or because they are elicited from uncertain experts. Belief networks are commonly used to compute responses to queriesâ€”i.e., return a number for P(H=h|E=e). Parameter uncertainty induces uncertainty in query responses, which are thus themselves random variables. This paper investigates this query response distribution, and shows how to accurately model this distribution for any query and any network structure. In particular, we prove that the query response is asymptotically Gaussian and provide its mean value and asymptotic variance. Moreover, we present an algorithm for computing these quantities that has the same worst-case complexity as inference in general, and also describe straight-line code when the query includes all n variables. We provide empirical evidence that (1) our approximation of the variance is very accurate, and (2) a Beta distribution with these moments provides a very accurate model of the observed query response distribution. We also show how to use this to produce accurate error bars around these responsesâ€”i.e., to determine that the response to P(H=h|E=e) is xÂ±y with confidence 1âˆ’Î´.}
}
@article{GENT199659,
title = {The satisfiability constraint gap},
journal = {Artificial Intelligence},
volume = {81},
number = {1},
pages = {59-80},
year = {1996},
note = {Frontiers in Problem Solving: Phase Transitions and Complexity},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00047-X},
url = {https://www.sciencedirect.com/science/article/pii/000437029500047X},
author = {Ian P. Gent and Toby Walsh},
keywords = {Search phase transitions, Satisfiability, Constraint propagation, Hard problems},
abstract = {We describe an experimental investigation of the satisfiability phase transition for several different classes of randomly generated problems. We show that the â€œconventionalâ€ picture of easy-hard-easy problem difficulty is inadequate. In particular, there is a region of very variable problem difficulty where problems are typically underconstrained and satisfiable. Within this region, problems can be orders of magnitude harder than problems in the middle of the satisfiability phase transition. These extraordinarily hard problems appear to be associated with a â€œconstraint gapâ€. That is, a region where search is a maximum as the amount of constraint propagation is a minimum. We show that the position and shape of this constraint gap change little with problem size. Unlike hard problems in the middle of the satisfiability phase transition, hard problems in the variable region are not critically constrained between satisfiability and unsatisfiability. Indeed, hard problems in the variable region often contain a small and unique minimal unsatisfiable subset or reduce at an early stage in search to a hard unsatisfiable subproblem with a small and unique minimal unsatisfiable subset. The difficulty in solving such problems is thus in identifying the minimal unsatisfiable subset from the many irrelevant clauses. The existence of a constraint gap greatly hinders our ability to find such minimal unsatisfiable subsets. However, it remains open whether these problems remain hard for more intelligent backtracking procedures. We conjecture that these results will generalize both to other SAT problem classes, and to the phase transitions of other NP-hard problems.}
}
@article{LUO201726,
title = {CCEHC: An efficient local search algorithm for weighted partial maximum satisfiability},
journal = {Artificial Intelligence},
volume = {243},
pages = {26-44},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216301382},
author = {Chuan Luo and Shaowei Cai and Kaile Su and Wenxuan Huang},
keywords = {Local search, Weighted partial maximum satisfiability, Emphasis on hard clauses},
abstract = {Weighted maximum satisfiability and (unweighted) partial maximum satisfiability (PMS) are two significant generalizations of maximum satisfiability (MAX-SAT), and weighted partial maximum satisfiability (WPMS) is the combination of the two, with more important applications in practice. Recently, great breakthroughs have been made on stochastic local search (SLS) for weighted MAX-SAT and PMS, resulting in several state-of-the-art SLS algorithms CCLS, Dist and DistUP. However, compared to the great progress of SLS on weighted MAX-SAT and PMS, the performance of SLS on WPMS lags far behind. In this paper, we present a new SLS algorithm named CCEHC for WPMS. CCEHC employs an extended framework of CCLS with a heuristic emphasizing hard clauses, called EHC. With strong accents on hard clauses, EHC has three components: a variable selection mechanism focusing on configuration checking based only on hard clauses, a weighting scheme for hard clauses, and a biased random walk component. Extensive experiments demonstrate that CCEHC significantly outperforms its state-of-the-art SLS competitors. Further experimental results on comparing CCEHC with a state-of-the-art complete solver show the effectiveness of CCEHC on a number of application WPMS instances, and indicate that CCEHC might be beneficial in practice. Also, empirical analyses confirm the effectiveness of each component underlying the EHC heuristic.}
}
@article{THIMM2017267,
title = {The first international competition on computational models of argumentation: Results and analysis},
journal = {Artificial Intelligence},
volume = {252},
pages = {267-294},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217301030},
author = {Matthias Thimm and Serena Villata},
keywords = {Formal argumentation, Algorithms},
abstract = {We report on the First International Competition on Computational Models of Argumentation (ICCMA'15) which took place in the first half of 2015 and focused on reasoning tasks in abstract argumentation frameworks. Performance of submitted solvers was evaluated on four computational problems wrt. four different semantics relating to the verification of the acceptance status of arguments, and computing jointly acceptable sets of arguments. In this paper, we describe the technical setup of the competition, and give an overview on the submitted solvers. Moreover, we report on the results and discuss our findings.}
}
@article{GREINER1996177,
title = {PALO: a probabilistic hill-climbing algorithm},
journal = {Artificial Intelligence},
volume = {84},
number = {1},
pages = {177-208},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00040-2},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000402},
author = {Russell Greiner},
keywords = {Computational learning theory, Hill-climbing, Speed-up learning, Utility problem, Knowledge compilation, Theory revision, Prioritized default theories},
abstract = {Many learning systems search through a space of possible performance elements, seeking an element whose expected utility, over the distribution of problems, is high. As the task of finding the globally optimal element is often intractable, many practical learning systems instead hill-climb to a local optimum. Unfortunately, even this is problematic as the learner typically does not know the underlying distribution of problems, which it needs to determine an element's expected utility. This paper addresses the task of approximating this hill-climbing search when the utility function can only be estimated by sampling. We present a general algorithm, palo, that returns an element that is, with provably high probability, essentially a local optimum. We then demonstrate the generality of this algorithm by presenting three distinct applications that respectively find an element whose efficiency, accuracy or completeness is nearly optimal. These results suggest approaches to solving the utility problem from explanation-based learning, the multiple extension problem from nonmonotonic reasoning and the tractability/completeness tradeoff problem from knowledge representation.}
}
@article{DREYFUS1996171,
title = {Response to my critics},
journal = {Artificial Intelligence},
volume = {80},
number = {1},
pages = {171-191},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00088-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000887},
author = {Hubert L. Dreyfus}
}
@article{LARROSA2008204,
title = {A logical approach to efficient Max-SAT solving},
journal = {Artificial Intelligence},
volume = {172},
number = {2},
pages = {204-233},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S000437020700104X},
author = {Javier Larrosa and Federico Heras and Simon {de Givry}},
keywords = {Max-SAT, Search, Inference},
abstract = {Weighted Max-SAT is the optimization version of SAT and many important problems can be naturally encoded as such. Solving weighted Max-SAT is an important problem from both a theoretical and a practical point of view. In recent years, there has been considerable interest in finding efficient solving techniques. Most of this work focuses on the computation of good quality lower bounds to be used within a branch and bound DPLL-like algorithm. Most often, these lower bounds are described in a procedural way. Because of that, it is difficult to realize the logic that is behind. In this paper we introduce an original framework for Max-SAT that stresses the parallelism with classical SAT. Then, we extend the two basic SAT solving techniques: search and inference. We show that many algorithmic tricks used in state-of-the-art Max-SAT solvers are easily expressible in logical terms in a unified manner, using our framework. We also introduce an original search algorithm that performs a restricted amount of weighted resolution at each visited node. We empirically compare our algorithm with a variety of solving alternatives on several benchmarks. Our experiments, which constitute to the best of our knowledge the most comprehensive Max-SAT evaluation ever reported, demonstrate the practical usability of our approach.}
}
@article{MAUSAM2010619,
title = {Panlingual lexical translation via probabilistic inference},
journal = {Artificial Intelligence},
volume = {174},
number = {9},
pages = {619-637},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000561},
author = { Mausam and Stephen Soderland and Oren Etzioni and Daniel S. Weld and Kobi Reiter and Michael Skinner and Marcus Sammer and Jeff Bilmes},
keywords = {Lexical translation, Multilinguality},
abstract = {This paper introduces a novel approach to the task of lexical translation between languages for which no translation dictionaries are available. We build a massive translation graph, automatically constructed from over 630 machine-readable dictionaries and Wiktionaries. In this graph each node denotes a word in some language and each edge (vi,vj) denotes a word sense shared by vi and vj. Our current graph contains over 10,000,000 nodes and expresses more than 60,000,000 pairwise translations. The composition of multiple translation dictionaries leads to a transitive inference problem: if word A translates to word B which in turn translates to word C, what is the probability that C is a translation of A? The paper describes a series of probabilistic inference algorithms that solve this problem at varying precision and recall levels. All algorithms enable us to quantify our confidence in a translation derived from the graph, and thus trade precision for recall. We compile the results of our best inference algorithm to yield PanDictionary, a novel multilingual dictionary. PanDictionary contains more than four times as many translations as in the largest Wiktionary at precision 0.90 and over 200,000,000 pairwise translations in over 200,000 language pairs at precision 0.8.}
}
@article{KUTER2009669,
title = {Task decomposition on abstract states, for planning under nondeterminism},
journal = {Artificial Intelligence},
volume = {173},
number = {5},
pages = {669-695},
year = {2009},
note = {Advances in Automated Plan Generation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001987},
author = {Ugur Kuter and Dana Nau and Marco Pistore and Paolo Traverso},
keywords = {Planning in nondeterministic domains, Hierarchical task-network (HTN) planning, Binary decision diagrams},
abstract = {Although several approaches have been developed for planning in nondeterministic domains, solving large planning problems is still quite difficult. In this work, we present a new planning algorithm, called Yoyo, for solving planning problems in fully observable nondeterministic domains. Yoyo combines an HTN-based mechanism for constraining its search and a Binary Decision Diagram (BDD) representation for reasoning about sets of states and state transitions. We provide correctness theorems for Yoyo, and an experimental comparison of it with MBP and ND-SHOP2, the two previously-best algorithms for planning in nondeterministic domains. In our experiments, Yoyo could easily deal with problem sizes that neither MBP nor ND-SHOP2 could scale up to, and could solve problems about 100 to 1000 times faster than MBP and ND-SHOP2.}
}
@article{NIGHTINGALE2011586,
title = {The extended global cardinality constraint: An empirical survey},
journal = {Artificial Intelligence},
volume = {175},
number = {2},
pages = {586-614},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S000437021000175X},
author = {Peter Nightingale},
keywords = {Global cardinality constraint, Constraint programming, Global constraints, Propagation algorithms},
abstract = {The Extended Global Cardinality Constraint (EGCC) is a vital component of constraint solving systems, since it is very widely used to model diverse problems. The literature contains many different versions of this constraint, which trade strength of inference against computational cost. In this paper, I focus on the highest strength of inference usually considered, enforcing generalized arc consistency (GAC) on the target variables. This work is an extensive empirical survey of algorithms and optimizations, considering both GAC on the target variables, and tightening the bounds of the cardinality variables. I evaluate a number of key techniques from the literature, and report important implementation details of those techniques, which have often not been described in published papers. Two new optimizations are proposed for EGCC. One of the novel optimizations (dynamic partitioning, generalized from AllDifferent) was found to speed up search by 5.6 times in the best case and 1.56 times on average, while exploring the same search tree. The empirical work represents by far the most extensive set of experiments on variants of algorithms for EGCC. Overall, the best combination of optimizations gives a mean speedup of 4.11 times compared to the same implementation without the optimizations.}
}
@article{NOCK200725,
title = {A Real generalization of discrete AdaBoost},
journal = {Artificial Intelligence},
volume = {171},
number = {1},
pages = {25-41},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206001111},
author = {Richard Nock and Frank Nielsen},
keywords = {AdaBoost, Boosting, Ensemble learning},
abstract = {Scaling discrete AdaBoost to handle real-valued weak hypotheses has often been done under the auspices of convex optimization, but little is generally known from the original boosting model standpoint. We introduce a novel generalization of discrete AdaBoost which departs from this mainstream of algorithms. From the theoretical standpoint, it formally displays the original boosting property, as it brings fast improvements of the accuracy of a weak learner up to arbitrary high levels; furthermore, it brings interesting computational and numerical improvements that make it significantly easier to handle â€œas isâ€. Conceptually speaking, it provides a new and appealing scaling to R of some well known facts about discrete (ada)boosting. Perhaps the most popular is an iterative weight modification mechanism, according to which examples have their weights decreased iff they receive the right class by the current discrete weak hypothesis. In our generalization, this property does not hold anymore, as examples that receive the right class can still be reweighted higher with real-valued weak hypotheses. From the experimental standpoint, our generalization displays the ability to produce low error formulas with particular cumulative margin distribution graphs, and it provides a nice handling of those noisy domains that represent Achilles' heel for common Adaptive Boosting algorithms.}
}
@article{VANBEVERN201619,
title = {H-index manipulation by merging articles: Models, theory, and experiments},
journal = {Artificial Intelligence},
volume = {240},
pages = {19-35},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300844},
author = {RenÃ© {van Bevern} and Christian Komusiewicz and Rolf Niedermeier and Manuel Sorge and Toby Walsh},
keywords = {Citation index, Hirsch index, Parameterized complexity, Exact algorithms, AI's 10 to watch},
abstract = {An author's profile on Google Scholar consists of indexed articles and associated data, such as the number of citations and the H-index. The author is allowed to merge articles; this may affect the H-index. We analyze the (parameterized) computational complexity of maximizing the H-index using article merges. Herein, to model realistic manipulation scenarios, we define a compatibility graph whose edges correspond to plausible merges. Moreover, we consider several different measures for computing the citation count of a merged article. For the measure used by Google Scholar, we give an algorithm that maximizes the H-index in linear time if the compatibility graph has constant-size connected components. In contrast, if we allow to merge arbitrary articles (that is, for compatibility graphs that are cliques), then already increasing the H-index by one is NP-hard. Experiments on Google Scholar profiles of AI researchers show that the H-index can be manipulated substantially only if one merges articles with highly dissimilar titles.}
}
@article{XIE2006422,
title = {Decomposition of structural learning about directed acyclic graphs},
journal = {Artificial Intelligence},
volume = {170},
number = {4},
pages = {422-439},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000026},
author = {Xianchao Xie and Zhi Geng and Qiang Zhao},
keywords = {Bayesian network, Conditional independence, Decomposition, Directed acyclic graph, Junction tree, Structural learning, Undirected graph},
abstract = {In this paper, we propose that structural learning of a directed acyclic graph can be decomposed into problems related to its decomposed subgraphs. The decomposition of structural learning requires conditional independencies, but it does not require that separators are complete undirected subgraphs. Domain or prior knowledge of conditional independencies can be utilized to facilitate the decomposition of structural learning. By decomposition, search for d-separators in a large network is localized to small subnetworks. Thus both the efficiency of structural learning and the power of conditional independence tests can be improved.}
}
@article{SOLNON2010850,
title = {AllDifferent-based filtering for subgraph isomorphism},
journal = {Artificial Intelligence},
volume = {174},
number = {12},
pages = {850-864},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000718},
author = {Christine Solnon},
keywords = {Subgraph isomorphism, Constraint programming, All different constraint},
abstract = {The subgraph isomorphism problem involves deciding if there exists a copy of a pattern graph in a target graph. This problem may be solved by a complete tree search combined with filtering techniques that aim at pruning branches that do not contain solutions. We introduce a new filtering algorithm based on local all different constraints. We show that this filtering is stronger than other existing filterings â€” i.e., it prunes more branches â€” and that it is also more efficient â€” i.e., it allows one to solve more instances quicker.}
}
@article{LIN1997131,
title = {How to progress a database},
journal = {Artificial Intelligence},
volume = {92},
number = {1},
pages = {131-167},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00044-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000446},
author = {Fangzhen Lin and Ray Reiter},
keywords = {Situation calculus, Theories of actions, Regression, Progression, STRIPS, Strongest postconditions},
abstract = {One way to think about a STRIPS operator is as a mapping from databases to databases, in the following sense: suppose we want to know what the world would be like if an action, represented by the STRIPS operator Î±, were done in some world, represented by the STRIPS database D0. To find out, simply perform the operator Î± on D0 (by applying Î±'s elementary add and delete revision operators to D0). We describe this process as progressing the databaseD0 in response to the action Î±. In this paper, we consider the general problem of progressing an initial database in response to a given sequence of actions. We appeal to the situation calculus and an axiomatization of actions which addresses the frame problem (Reiter (1991)). This setting is considerably more general than STRIPS. Our results concerning progression are mixed. The (surprising) bad news is that, in general, to characterize a progressed database we must appeal to second-order logic. The good news is that there are many useful special cases for which we can compute the progressed database in first-order logic; not only that, we can do so efficiently. Finally, we relate these results about progression to STRIPS-like systems by providing a semantics for such systems in terms of a purely declarative situation calculus axiomatization for actions and their effects. On our view, STRIPS operators provide a mechanism for computing the progression of an initial situation calculus database under the effects of an action. We illustrate this idea by describing two different STRIPS mechanisms, and proving their correctness with respect to their situation calculus specifications.}
}
@article{LIBERATORE2008265,
title = {Redundancy in logic II: 2CNF and Horn propositional formulae},
journal = {Artificial Intelligence},
volume = {172},
number = {2},
pages = {265-299},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001105},
author = {Paolo Liberatore},
keywords = {Propositional logic, Computational complexity, Redundancy},
abstract = {We report results about the redundancy of formulae in 2CNF form. In particular, we give a slight improvement over the trivial redundancy algorithm and give some complexity results about some problems related to finding Irredundant Equivalent Subsets (i.e.s.) of 2CNF formulae. The problems of checking whether a 2CNF formula has a unique i.e.s. and checking whether a clause in is all its i.e.s.'s are polynomial. Checking whether a 2CNF formula has an i.e.s. of a given size and checking whether a clause is in some i.e.s.'s of a 2CNF formula are polynomial or NP-complete depending on whether the formula is cyclic. Some results about Horn formulae are also reported.}
}
@article{QADIR2011673,
title = {From Bidirectional Associative Memory to a noise-tolerant, robust Protein Processor Associative Memory},
journal = {Artificial Intelligence},
volume = {175},
number = {2},
pages = {673-693},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001785},
author = {Omer Qadir and Jerry Liu and Gianluca Tempesti and Jon Timmis and Andy Tyrrell},
keywords = {Self-organising, Self-regulating, Associative Memory, Protein processing, Hetero-associative, BAM, PRLAB, SOIAM, SABRE, Mobile robotics},
abstract = {Protein Processor Associative Memory (PPAM) is a novel architecture for learning associations incrementally and online and performing fast, reliable, scalable hetero-associative recall. This paper presents a comparison of the PPAM with the Bidirectional Associative Memory (BAM), both with Kosko's original training algorithm and also with the more popular Pseudo-Relaxation Learning Algorithm for BAM (PRLAB). It also compares the PPAM with a more recent associative memory architecture called SOIAM. Results of training for object-avoidance are presented from simulations using player/stage and are verified by actual implementations on the E-Puck mobile robot. Finally, we show how the PPAM is capable of achieving an increase in performance without using the typical weighted-sum arithmetic operations or indeed any arithmetic operations.}
}
@article{TINKHAM19981,
title = {Schema induction for logic program synthesis},
journal = {Artificial Intelligence},
volume = {98},
number = {1},
pages = {1-47},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00055-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000556},
author = {Nancy Lynn Tinkham},
keywords = {Inductive logic programming, Inductive inference, Automatic programming, Learning},
abstract = {Prolog program synthesis can be made more efficient by using schemata which capture similarities in previously-seen programs. Such schemata narrow the search involved in the synthesis of a new program. We define a generalization operator for forming schemata from programs and a downward refinement operator for constructing programs from schemata. These operators define schema-hierarchy graphs which can be used to aid in the synthesis of new programs. Algorithms are presented for efficiently obtaining least generalizations of schemata, for adding new schemata to a schema-hierarchy graph, and for using schemata to construct new programs.}
}
@article{LORINI2011814,
title = {A logic for reasoning about counterfactual emotions},
journal = {Artificial Intelligence},
volume = {175},
number = {3},
pages = {814-847},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002110},
author = {Emiliano Lorini and FranÃ§ois Schwarzentruber},
keywords = {Modal logic, Emotions, },
abstract = {The aim of this work is to propose a logical framework for the specification of cognitive emotions that are based on counterfactual reasoning about agents' choices. The prototypical counterfactual emotion is regret. In order to meet this objective, we exploit the well-known STIT logic (Belnap et al. (2001) [9], Horty (2001) [30], Horty and Belnap (1995) [31]). STIT logic has been proposed in the domain of formal philosophy in the nineties and, more recently, it has been imported into the field of theoretical computer science where its formal relationships with other logics for multi-agent systems such as ATL and Coalition Logic (CL) have been studied. STIT is a very suitable formalism to reason about choices and capabilities of agents and groups of agents. Unfortunately, the version of STIT with agents and groups has been recently proved to be undecidable and not finitely axiomatizable. In this work we study a decidable and finitely axiomatizable fragment of STIT with agents and groups which is sufficiently expressive for our purpose of formalizing counterfactual emotions. We call dfSTIT our STIT fragment. After having extended dfSTIT with knowledge modalities, in the second part of article, we exploit it in order to formalize four types of counterfactual emotions: regret, rejoicing, disappointment, and elation. At the end of the article we present an application of our formalization of counterfactual emotions to a concrete example.}
}
@article{SADEH19961,
title = {Variable and value ordering heuristics for the job shop scheduling constraint satisfaction problem},
journal = {Artificial Intelligence},
volume = {86},
number = {1},
pages = {1-41},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00098-4},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000984},
author = {Norman Sadeh and Mark S. Fox},
abstract = {Practical constraint satisfaction problems (CSPs) such as design of integrated circuits or scheduling generally entail large search spaces with hundreds or even thousands of variables, each with hundreds or thousands of possible values. Often, only a very tiny fraction of all these possible assignments participates in a satisfactory solution. This article discusses techniques that aim at reducing the effective size of the search space to be explored in order to find a satisfactory solution by judiciously selecting the order in which variables are instantiated and the sequence in which possible values are tried for each variable. In the CSP literature, these techniques are commonly referred to as variable and value ordering heuristics. Our investigation is conducted in the job shop scheduling domain. We show that, in contrast with problems studied earlier in the CSP literature, generic variable and value heuristics do not perform well in this domain. This is attributed to the difficulty of these heuristics to properly account for the tightness of constraints and/or the connectivity of the constraint graphs induced by job shop scheduling CSPs. A new probabilistic framework is introduced that better captures these key aspects of the job shop scheduling search space. Empirical results show that variable and value ordering heuristics derived within this probabilistic framework often yield significant improvements in search efficiency and significant reductions in the search time required to obtain a satisfactory solution. The research reported in this article was the first one, along with the work of Keng and Yun (1989), to use the CSP problem solving paradigm to solve job shop scheduling problems. The suite of benchmark problems it introduced has been used since then by a number of other researchers to evaluate alternative techniques for the job shop scheduling CSP. The article briefly reviews some of these more recent efforts and shows that our variable and value ordering heuristics remain quite competitive}
}
@article{RAHWAN201295,
title = {Anytime coalition structure generation in multi-agent systems with positive or negative externalities},
journal = {Artificial Intelligence},
volume = {186},
pages = {95-122},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000288},
author = {Talal Rahwan and Tomasz Michalak and Michael Wooldridge and Nicholas R. Jennings},
keywords = {Mechanism design, Classification, Game theory, Approximation},
abstract = {Much of the literature on multi-agent coalition formation has focused on Characteristic Function Games, where the effectiveness of a coalition is not affected by how the other agents are arranged in the system. In contrast, very little attention has been given to the more general class of Partition Function Games, where the emphasis is on how the formation of one coalition could influence the performance of other co-existing coalitions in the system. However, these inter-coalitional dependencies, called externalities from coalition formation, play a crucial role in many real-world multi-agent applications where agents have either conflicting or overlapping goals. Against this background, this paper is the first computational study of coalitional games with externalities in the multi-agent system context. We focus on the Coalition Structure Generation (CSG) problem which involves finding an exhaustive and disjoint division of the agents into coalitions such that the performance of the entire system is optimized. While this problem is already very challenging in the absence of externalities, due to the exponential size of the search space, taking externalities into consideration makes it even more challenging as the size of the input, given n agents, grows from O(2n) to O(nn). Our main contribution is the development of the first CSG algorithm for coalitional games with either positive or negative externalities. Specifically, we prove that it is possible to compute upper and lower bounds on the values of any set of disjoint coalitions. Building upon this, we prove that in order to establish a worst-case guarantee on solution quality it is necessary to search a certain set of coalition structures (which we define). We also show how to progressively improve this guarantee with further search. Since there are no previous CSG algorithms for games with externalities, we benchmark our algorithm against other state-of-the-art approaches in games where no externalities are present. Surprisingly, we find that, as far as worst-case guarantees are concerned, our algorithm outperforms the others by orders of magnitude. For instance, to reach a bound of 3 given 24 agents, the number of coalition structures that need to be searched by our algorithm is only 0.0007% of that needed by Sandholm et al. (1999) [1], and 0.5% of that needed by Dang and Jennings (2004) [2]. This is despite the fact that the other algorithms take advantage of the special properties of games with no externalities, while ours does not.}
}
@article{TONELLI2013203,
title = {Wikipedia-based WSD for multilingual frame annotation},
journal = {Artificial Intelligence},
volume = {194},
pages = {203-221},
year = {2013},
note = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000720},
author = {Sara Tonelli and Claudio Giuliano and Kateryna Tymoshenko},
keywords = {Frame annotation, Multilingual FrameNets, Word sense disambiguation, FrameNetâ€“Wikipedia mapping},
abstract = {Many applications in the context of natural language processing have been proven to achieve a significant performance when exploiting semantic information extracted from high-quality annotated resources. However, the practical use of such resources is often biased by their limited coverage. Furthermore, they are generally available only for English and few other languages. We propose a novel methodology that, starting from the mapping between FrameNet lexical units and Wikipedia pages, automatically leverages from Wikipedia new lexical units and example sentences. The goal is to build a reference data set for the semi-automatic development of new FrameNets. In addition, this methodology can be adapted to perform frame identification in any language available in Wikipedia. Our approach relies on a state-of-the-art word sense disambiguation system that is first trained on English Wikipedia to assign a page to the lexical units in a frame. Then, this mapping is further exploited to perform frame identification in English or in any other language available in Wikipedia. Our approach shows a high potential in multilingual settings, because it can be applied to languages for which other lexical resources such as WordNet or thesauri are not available.}
}
@article{REID1995289,
title = {Recognition of object classes from range data},
journal = {Artificial Intelligence},
volume = {78},
number = {1},
pages = {289-326},
year = {1995},
note = {Special Volume on Computer Vision},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00062-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000623},
author = {I.D. Reid and J.M. Brady},
keywords = {Object recognition, Parametric objects, Range data, Stereo},
abstract = {We develop techniques for recognizing instances of 3D object classes (which may consist of multiple and/or repeated sub-parts with internal degrees of freedom, linked by parameterized transformations), from sets of 3D feature observations. Recognition of a class instance is structured as a search of an interpretation tree in which geometric constraints on pairs of sensed features not only prune the tree, but are used to determine upper and lower bounds on the model parameter values of the instance. A real-valued constraint propagation network unifies the representations of the model parameters, model constraints and feature constraints, and provides a simple and effective mechanism for accessing and updating parameter values. Recognition of objects with multiple internal degrees of freedom, including non-uniform scaling and stretching, articulations, and sub-part repetitions, is demonstrated and analysed for two different types of real range data: 3D edge fragments from a stereo vision system, and position/surface normal data derived from planar patches extracted from a range image.}
}
@article{ANTONELLI19971,
title = {Defeasible inheritance on cyclic networks},
journal = {Artificial Intelligence},
volume = {92},
number = {1},
pages = {1-23},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00053-7},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000537},
author = {Gian Aldo Antonelli},
keywords = {Cyclic networks, Defeasible inheritance, Theories of truth},
abstract = {In this paper, we are going to present a new notion of â€œextensionâ€ for defeasible inheritance networks that allows us to deal with cyclic nets. Horty has shown that cyclic nets need not have extensions in the sense of Touretzky. This paper presents a generalization of that notion of extension that can be applied to cyclic nets. The present proposal is inspired by a somewhat unexpected analogy between cyclic nets and â€œsemantically closedâ€ languages, i.e., languages containing their own truth predicate. Accordingly, this approach to defeasible inheritance networks with cycles shows similarities to the solution of semantic paradoxes put forth by Kripke.}
}
@article{BRAFMAN20111180,
title = {Relational preference rules for control},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1180-1193},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001992},
author = {Ronen I. Brafman},
keywords = {Preference models, Preference rules, Relational models, Rule-based systems, Command and control automation},
abstract = {Value functions are defined over a fixed set of outcomes. In work on preference handling in AI, these outcomes are usually a set of assignments over a fixed set of state variables. If the set of variables changes, a new value function must be elicited. Given that in most applications the state variables are properties (attributes) of objects in the world, this implies that the introduction of new objects requires re-elicitation of preferences. However, often, the user has in mind preferential information that is much more generic, and which is relevant to a given type of domain regardless of the precise number of objects of each kind and their properties. Such information requires the introduction of relational models. Following in the footsteps of work on probabilistic relational models (PRMs), we suggest in this work a rule-based, relational language of preferences. This language extends regular rule-based languages and leads to a much more flexible approach for specifying control rules for autonomous systems. It also extends standard generalized-additive value functions to handle a dynamic universe of objects. Given any specific set of objects this specification induces a generalized-additive value function over assignments to the controllable attributes associated with these objects. We then describe a prototype of a decision support system for command and control centers we developed to illustrate and study the use of these rules.}
}
@article{LOMUSCIO20071011,
title = {Bounded model checking for knowledge and real time},
journal = {Artificial Intelligence},
volume = {171},
number = {16},
pages = {1011-1038},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001038},
author = {Alessio Lomuscio and Wojciech Penczek and BoÅ¼ena WoÅºna},
keywords = {Temporal epistemic logics, Model checking, Interpreted systems, Real time systems},
abstract = {We present TECTLK, a logic to specify knowledge and real time in multi-agent systems. We show that the TECTLK model checking problem is decidable, and we present an algorithm for bounded model checking based on a discretisation method. We exemplify the use of the technique by means of the â€œRailroad Crossing Systemâ€, a popular example in the multi-agent systems literature.}
}
@article{DAVIS2011299,
title = {How does a box work? A study in the qualitative dynamics of solid objects},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {299-345},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000421},
author = {Ernest Davis},
keywords = {Qualitative physics, Naive physics, Commonsense reasoning, Solid objects, Kinematics, Dynamics, Planning},
abstract = {This paper is an in-depth study of qualitative physical reasoning about one particular scenario: using a box to carry a collection of objects from one place to another. Specifically we consider the plan, plan1 â€œLoad objects uCargo into box oBox one by one; carry oBox from location l1 to location l2â€. We present qualitative constraints on the shape, starting position, and material properties of uCargo and oBox and on the characteristics of the motion that suffice to make it virtually certain that plan1 can be successfully executed. We develop a theory, consisting mostly of first-order statements together with two default rules, that supports an inference of the form â€œIf conditions XYZ hold, and the agent attempts to carry out plan1 then presumably he will succeedâ€. Our theory is elaboration tolerant in the sense that carrying out the analogous inference for carrying objects in boxes with lids, in boxes with small holes, or on trays can reuse much of the same knowledge. The theory integrates reasoning about continuous time, Euclidean space, commonsense dynamics of solid objects, and semantics of partially specified plans.}
}
@article{GREINER199621,
title = {Probably approximately optimal satisficing strategies},
journal = {Artificial Intelligence},
volume = {82},
number = {1},
pages = {21-44},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00010-0},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000100},
author = {Russell Greiner and Pekka Orponen},
abstract = {A satisficing search problem consists of a set of probabilistic experiments to be performed in some order, seeking a satisfying configuration of successes and failures. The expected cost of the search depends both on the success probabilities of the individual experiments, and on the search strategy, which specifies the order in which the experiments are to be performed. A strategy that minimizes the expected cost is optimal. Earlier work has provided â€œoptimizing functionsâ€ that compute optimal strategies for certain classes of search problems from the success probabilities of the individual experiments. We extend those results by providing a general model of such strategies, and an algorithm pao that identifies an approximately optimal strategy when the probability values are not known. The algorithm first estimates the relevant probabilities from a number of trials of each undetermined experiment, and then uses these estimates, and the proper optimizing function, to identify a strategy whose cost is, with high probability, close to optimal. We also show that if the search problem can be formulated as an and-or tree, then the pao algorithm can also â€œlearn while doingâ€, i.e. gather the necessary statistics while performing the search.}
}
@article{XING200547,
title = {MaxSolver: An efficient exact algorithm for (weighted) maximum satisfiability},
journal = {Artificial Intelligence},
volume = {164},
number = {1},
pages = {47-80},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000160},
author = {Zhao Xing and Weixiong Zhang},
keywords = {Weighted maximum satisfiability, DPLL, Unit propagation, Linear programming, Nonlinear programming, Variable ordering},
abstract = {Maximum Boolean satisfiability (max-SAT) is the optimization counterpart of Boolean satisfiability (SAT), in which a variable assignment is sought to satisfy the maximum number of clauses in a Boolean formula. A branch and bound algorithm based on the Davisâ€“Putnamâ€“Logemannâ€“Loveland procedure (DPLL) is one of the most competitive exact algorithms for solving max-SAT. In this paper, we propose and investigate a number of strategies for max-SAT. The first strategy is a set of unit propagation or unit resolution rules for max-SAT. We summarize three existing unit propagation rules and propose a new one based on a nonlinear programming formulation of max-SAT. The second strategy is an effective lower bound based on linear programming (LP). We show that the LP lower bound can be made effective as the number of clauses increases. The third strategy consists of a binary-clause first rule and a dynamic-weighting variable ordering rule, which are motivated by a thorough analysis of two existing well-known variable orderings. Based on the analysis of these strategies, we develop an exact solver for both max-SAT and weighted max-SAT. Our experimental results on random problem instances and many instances from the max-SAT libraries show that our new solver outperforms most of the existing exact max-SAT solvers, with orders of magnitude of improvement in many cases.}
}
@article{CARAGIANNIS20111655,
title = {Voting almost maximizes social welfare despite limited communication},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1655-1671},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000506},
author = {Ioannis Caragiannis and Ariel D. Procaccia},
keywords = {Computational social choice},
abstract = {In cooperative multiagent systems an alternative that maximizes the social welfareâ€”the sum of utilitiesâ€”can only be selected if each agent reports its full utility function. This may be infeasible in environments where communication is restricted. Employing a voting rule to choose an alternative greatly reduces the communication burden, but leads to a possible gap between the social welfare of the optimal alternative and the social welfare of the one that is ultimately elected. Procaccia and Rosenschein (2006) [13] have introduced the concept of distortion to quantify this gap. In this paper, we present the notion of embeddings into voting rules: functions that receive an agentÊ¼s utility function and return the agentÊ¼s vote. We establish that very low distortion can be obtained using randomized embeddings, especially when the number of agents is large compared to the number of alternatives. We investigate our ideas in the context of three prominent voting rules with low communication costs: Plurality, Approval, and Veto. Our results arguably provide a compelling reason for employing voting in cooperative multiagent systems.}
}
@article{NIELSEN2007754,
title = {An application of formal argumentation: Fusing Bayesian networks in multi-agent systems},
journal = {Artificial Intelligence},
volume = {171},
number = {10},
pages = {754-775},
year = {2007},
note = {Argumentation in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000732},
author = {SÃ¸ren Holbech Nielsen and Simon Parsons},
keywords = {Argument in agent systems, Argumentation frameworks, Application, Bayesian networks},
abstract = {We consider a multi-agent system where each agent is equipped with a Bayesian network, and present an open framework for the agents to agree on a possible consensus network. The framework builds on formal argumentation, and unlike previous solutions on graphical consensus belief, it is sufficiently general to allow for a wide range of possible agreements to be identified.}
}
@article{GRECO201719,
title = {Constrained coalition formation on valuation structures: Formal framework, applications, and islands of tractability},
journal = {Artificial Intelligence},
volume = {249},
pages = {19-46},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300516},
author = {Gianluigi Greco and Antonella Guzzo},
keywords = {Coalitional games, Solution concepts, Computational complexity, Treewidth, Marginal contribution networks},
abstract = {Coalition structure generation is the problem of partitioning the agents of a given environment into disjoint and exhaustive coalitions so that the whole available worth is maximized. While this problem has been classically studied in settings where all coalitions are allowed to form, it has been recently reconsidered in the literature moving from the observation that environments often forbid the formation of certain coalitions. By following this latter perspective, a model for coalition structure generation is proposed where constraints of two different kinds can be expressed simultaneously. Indeed, the model is based on the concept of valuation structure, which consists of a set of pivotal agents that are pairwise incompatible, plus an interaction graph prescribing that a coalition C can form only if the subgraph induced over the nodes/agents in C is connected. It is shown that valuation structures can be used to model a number of relevant problems arising in real-world application domains. Then, the complexity of coalition structure generation over valuation structures is studied, by assuming that the functions associating each coalition with its worth are given as input according to some compact encodingâ€”rather than explicitly listing all exponentially-many associations. In particular, islands of tractability are identified based on the topological properties of the underlying interaction graphs and on suitable algebraic properties of the given worth functions. Finally, stability issues over valuation structures are studied too, by considering the core as the prototypical solution concept.}
}
@article{HELMERT2009503,
title = {Concise finite-domain representations for PDDL planning tasks},
journal = {Artificial Intelligence},
volume = {173},
number = {5},
pages = {503-535},
year = {2009},
note = {Advances in Automated Plan Generation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001926},
author = {Malte Helmert},
keywords = {Automated planning, Problem reformulation, PDDL, SAS},
abstract = {We introduce an efficient method for translating planning tasks specified in the standard PDDL formalism into a concise grounded representation that uses finite-domain state variables instead of the straight-forward propositional encoding. Translation is performed in four stages. Firstly, we transform the input task into an equivalent normal form expressed in a restricted fragment of PDDL. Secondly, we synthesize invariants of the planning task that identify groups of mutually exclusive propositions which can be represented by a single finite-domain variable. Thirdly, we perform an efficient relaxed reachability analysis using logic programming techniques to obtain a grounded representation of the input. Finally, we combine the results of the third and fourth stage to generate the final grounded finite-domain representation. The presented approach has originally been implemented as part of the Fast Downward planning system for the 4th International Planning Competition (IPC4). Since then, it has been used in a number of other contexts with considerable success, and the use of concise finite-domain representations has become a common feature of state-of-the-art planners.}
}
@article{BODEN1995161,
title = {Modelling creativity: reply to reviewers},
journal = {Artificial Intelligence},
volume = {79},
number = {1},
pages = {161-182},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00074-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000747},
author = {Margaret A. Boden},
abstract = {The reviewers of The Creative Mind (henceforth TCM) have raised a host of interesting points. Most fall into seven groups: the definition of creativity; the distinction between H-creativity and P-creativity; the role of the social context; the role of evaluation; the four Lovelace questions; specific computational mechanisms used for modelling creativity; AI-models as aids to creativity; and the treatment of music in TCM. I shall group my replies accordingly.}
}
@article{AIGUIER2018160,
title = {Belief revision, minimal change and relaxation: A general framework based on satisfaction systems, and applications to description logics},
journal = {Artificial Intelligence},
volume = {256},
pages = {160-180},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217301686},
author = {Marc Aiguier and Jamal Atif and Isabelle Bloch and CÃ©line Hudelot},
keywords = {Abstract belief revision, Relaxation, AGM theory, Satisfaction systems, Description logics},
abstract = {Belief revision of knowledge bases represented by a set of sentences in a given logic has been extensively studied but for specific logics, mainly propositional, and also recently Horn and description logics. Here, we propose to generalize this operation from a model-theoretic point of view, by defining revision in the abstract model theory of satisfaction systems. In this framework, we generalize to any satisfaction system the characterization of the AGM postulates given by Katsuno and Mendelzon for propositional logic in terms of minimal change among interpretations. In this generalization, the constraint on syntax independence is partially relaxed. Moreover, we study how to define revision, satisfying these weakened AGM postulates, from relaxation notions that have been first introduced in description logics to define dissimilarity measures between concepts, and the consequence of which is to relax the set of models of the old belief until it becomes consistent with the new pieces of knowledge. We show how the proposed general framework can be instantiated in different logics such as propositional, first-order, description and Horn logics. In particular for description logics, we introduce several concrete relaxation operators tailored for the description logic ALC and its fragments EL and ELU, discuss their properties and provide some illustrative examples.}
}
@article{BARTO199581,
title = {Learning to act using real-time dynamic programming},
journal = {Artificial Intelligence},
volume = {72},
number = {1},
pages = {81-138},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00011-O},
url = {https://www.sciencedirect.com/science/article/pii/000437029400011O},
author = {Andrew G. Barto and Steven J. Bradtke and Satinder P. Singh},
abstract = {Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence. Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known. We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience. RTDP generalizes Korf's Learning-Real-Time-A* algorithm to problems involving uncertainty. We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems. We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins' Q-Learning algorithm. A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and relevant concepts and algorithms from control theory.}
}
@article{HAZON201326,
title = {Physical search problems with probabilistic knowledge},
journal = {Artificial Intelligence},
volume = {196},
pages = {26-52},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000027},
author = {Noam Hazon and Yonatan Aumann and Sarit Kraus and David Sarne},
keywords = {Graph search, Economic search},
abstract = {This paper considers the problem of an agent or a team of agents searching for a resource or tangible good in a physical environment, where the resource or good may possibly be obtained at one of several locations. The cost of acquiring the resource or good at a given location is uncertain (a priori), and the agents can observe the true cost only when physically arriving at this location. Sample applications include agents in exploration and patrol missions (e.g., an agent seeking to find the best location to deploy sensing equipment along its path). The uniqueness of these settings is in that the cost of observing a new location is determined by distance from the current one, impacting the consideration for the optimal search order. Although this model captures many real world scenarios, it has not been investigated so far. We analyze three variants of the problem, differing in their objective: minimizing the total expected cost, maximizing the success probability given an initial budget, and minimizing the budget necessary to obtain a given success probability. For each variant, we first introduce and analyze the problem with a single agent, either providing a polynomial solution to the problem or proving it is NP-complete. We also introduce a fully polynomial time approximation scheme algorithm for the minimum budget variant. In the multi-agent case, we analyze two models for managing resources, shared and private budget models. We present polynomial algorithms that work for any fixed number of agents, in the shared or private budget model. For non-communicating agents in the private budget model, we present a polynomial algorithm that is suitable for any number of agents. We also analyze the difference between homogeneous and heterogeneous agents, both with respect to their allotted resources and with respect to their capabilities. Finally, we define our problem in an environment with self-interested agents. We show how to find a Nash equilibrium in polynomial time, and prove that the bound on the performance of our algorithms, with respect to the social welfare, is tight.}
}
@article{LIU2010295,
title = {Logic programs with abstract constraint atoms: The role of computations},
journal = {Artificial Intelligence},
volume = {174},
number = {3},
pages = {295-315},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209001465},
author = {Lengning Liu and Enrico Pontelli and Tran Cao Son and Miroslaw TruszczyÅ„ski},
keywords = {Logic programs with abstract constraint atoms, Answer sets, Computations},
abstract = {We provide a new perspective on the semantics of logic programs with arbitrary abstract constraints. To this end, we introduce several notions of computation. We use the results of computations to specify answer sets of programs with constraints. We present the rationale behind the classes of computations we consider, and discuss the relationships among them. We also discuss the relationships among the corresponding concepts of answer sets. One of those concepts has several compelling characterizations and properties, and we propose it as the correct generalization of the answer-set semantics to the case of programs with arbitrary constraints. We show that several other notions of an answer set proposed in the literature for programs with constraints can be obtained within our framework as the results of appropriately selected classes of computations.}
}
@article{FELNER20111570,
title = {Inconsistent heuristics in theory and practice},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1570-1603},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000221},
author = {Ariel Felner and Uzi Zahavi and Robert Holte and Jonathan Schaeffer and Nathan Sturtevant and Zhifu Zhang},
keywords = {Heuristic search, Admissible heuristics, Inconsistent heuristics, A, IDA},
abstract = {In the field of heuristic search it is usually assumed that admissible heuristics are consistent, implying that consistency is a desirable attribute. The term â€œinconsistent heuristicâ€ has, at times, been portrayed negatively, as something to be avoided. Part of this is historical: early research discovered that inconsistency can lead to poor performance for AâŽ (nodes might be re-expanded many times). However, the issue has never been fully investigated, and was not re-considered after the invention of IDAâŽ. This paper shows that many of the preconceived notions about inconsistent heuristics are outdated. The worst-case exponential time of inconsistent heuristics is shown to only occur on contrived graphs with edge weights that are exponential in the size of the graph. Furthermore, the paper shows that rather than being something to be avoided, inconsistent heuristics often add a diversity of heuristic values into a search which can lead to a reduction in the number of node expansions. Inconsistent heuristics are easy to create, contrary to the common perception in the AI literature. To demonstrate this, a number of methods for achieving effective inconsistent heuristics are presented. Pathmax is a way of propagating inconsistent heuristic values in the search from parent to children. This technique is generalized into bidirectional pathmax (BPMX) which propagates values from a parent to a child node, and vice versa. BPMX can be integrated into IDAâŽ and AâŽ. When inconsistent heuristics are used with BPMX, experimental results show a large reduction in the search effort required by IDAâŽ. Positive results are also presented for AâŽ searches.}
}
@article{FAN201466,
title = {Logical characterizations of regular equivalence in weighted social networks},
journal = {Artificial Intelligence},
volume = {214},
pages = {66-88},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S000437021400068X},
author = {Tuan-Fang Fan and Churn-Jung Liau},
keywords = {Weighted social network, Many-valued modal logic, Regular equivalence, Bisimulation, Reasoning under uncertainty or imprecision},
abstract = {Social network analysis is a methodology used extensively in social science. Classical social networks can only represent the qualitative relationships between actors, but weighted social networks can describe the degrees of connection between actors. In a classical social network, regular equivalence is used to capture the similarity between actors based on their links to other actors. Specifically, two actors are deemed regularly equivalent if they are equally related to equivalent others. The definition of regular equivalence has been extended to weighted social networks in two ways. The first definition, called regular similarity, considers regular equivalence as an equivalence relation that commutes with the underlying graph edges; while the second definition, called generalized regular equivalence, is based on the notion of role assignment or coloring. A role assignment (resp. coloring) is a mapping from the set of actors to a set of roles (resp. colors). The mapping is regular if actors assigned to the same role have the same roles in their neighborhoods. Recently, it was shown that social positions based on regular equivalence can be syntactically expressed as well-formed formulas in a kind of modal logic. Thus, actors occupying the same social position based on regular equivalence will satisfy the same set of modal formulas. In this paper, we present analogous results for regular similarity and generalized regular equivalence based on many-valued modal logics.}
}
@article{ZHANG2023103918,
title = {The first AI4TSP competition: Learning to solve stochastic routing problems},
journal = {Artificial Intelligence},
volume = {319},
pages = {103918},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103918},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000644},
author = {Yingqian Zhang and Laurens Bliek and Paulo {da Costa} and Reza {Refaei Afshar} and Robbert Reijnen and Tom Catshoek and DaniÃ«l Vos and Sicco Verwer and Fynn Schmitt-Ulms and AndrÃ© Hottung and Tapan Shah and Meinolf Sellmann and Kevin Tierney and Carl Perreault-Lafleur and Caroline Leboeuf and Federico Bobbio and Justine Pepin and Warley Almeida Silva and Ricardo Gama and Hugo L. Fernandes and Martin Zaefferer and Manuel LÃ³pez-IbÃ¡Ã±ez and Ekhine Irurozki},
keywords = {AI for TSP competition, Travelling salesman problem, Routing problem, Stochastic combinatorial optimization, Surrogate-based optimization, Deep reinforcement learning},
abstract = {This paper reports on the first international competition on AI for the traveling salesman problem (TSP) at the International Joint Conference on Artificial Intelligence 2021 (IJCAI-21). The TSP is one of the classical combinatorial optimization problems, with many variants inspired by real-world applications. This first competition asked the participants to develop algorithms to solve an orienteering problem with stochastic weights and time windows (OPSWTW). It focused on two learning approaches: surrogate-based optimization and deep reinforcement learning. In this paper, we describe the problem, the competition setup, and the winning methods, and give an overview of the results. The winning methods described in this work have advanced the state-of-the-art in using AI for stochastic routing problems. Overall, by organizing this competition we have introduced routing problems as an interesting problem setting for AI researchers. The simulator of the problem has been made open-source and can be used by other researchers as a benchmark for new learning-based methods. The instances and code for the competition are available at https://github.com/paulorocosta/ai-for-tsp-competition.}
}
@article{CLEMENTINI1997317,
title = {Qualitative representation of positional information},
journal = {Artificial Intelligence},
volume = {95},
number = {2},
pages = {317-356},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00046-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000465},
author = {Eliseo Clementini and Paolino Di Felice and Daniel HernÃ¡ndez},
keywords = {Spatial reasoning, Qualitative representation, Distance, Orientation, Position, Frame of reference},
abstract = {A framework for the qualitative representation of positional information in a two-dimensional space is presented. Qualitative representations use discrete quantity spaces, where a particular distinction is introduced only if it is relevant to the context being modeled. This allows us to build a flexible framework that accommodates various levels of granularity and scales of reasoning. Knowledge about position in large-scale space is commonly represented by a combination of orientation and distance relations, which we express in a particular frame of reference between a primary object and a reference object. While the representation of orientation comes out to be more straightforward, the model for distances requires that qualitative distance symbols be mapped to geometric intervals in order to be compared; this is done by defining structure relations that are able to handle, among others, order of magnitude relations; the frame of reference with its three components (distance system, scale, and type) captures the inherent context dependency of qualitative distances. The principal aim of the qualitative representation is to perform spatial reasoning: as a basic inference technique, algorithms for the composition of positional relations are developed with respect to same and different frames of reference. The model presented in this paper has potential applications in areas as diverse as Geographical Information Systems (GIS), Computer Aided Design (CAD), and Document Recognition.}
}
@article{STEIN20112021,
title = {Algorithms and mechanisms for procuring services with uncertain durations using redundancy},
journal = {Artificial Intelligence},
volume = {175},
number = {14},
pages = {2021-2060},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000853},
author = {S. Stein and E.H. Gerding and A.C. Rogers and K. Larson and N.R. Jennings},
keywords = {Mechanism design, Multi-agent systems, Service-oriented computing, Uncertainty, Redundancy},
abstract = {In emerging service-oriented systems, such as computational clouds or grids, software agents are able to automatically procure distributed services to complete computational tasks. However, service execution times are often highly uncertain and service providers may have incentives to lie strategically about this uncertainty to win more customers. In this paper, we argue that techniques from the field of artificial intelligence are instrumental to addressing these challenges. To this end, we first propose a new decision-theoretic algorithm that allows a single service consumer agent to procure services for a computational task with a strict deadline. Crucially, this algorithm uses redundancy in a principled manner to mitigate uncertain execution times and maximise the consumerÊ¼s expected utility. We present both an optimal variant that uses a novel branch-and-bound formulation, and a fast heuristic that achieves near-optimal performance. Using simulations, we demonstrate that our algorithms outperform approaches that do not employ redundancy by up to 130% in some settings. Next, as the algorithms require private information about the providersÊ¼ capabilities, we show how techniques from mechanism design can be used to incentivise truthfulness. As no existing work in this area deals with uncertain execution times and redundant invocations, we extend the state of the art by proposing a number of payment schemes for these settings. In a detailed analysis, we prove that our mechanisms fulfil a range of desirable economic properties, including incentive compatibility, and we discuss suboptimal variants that scale to realistic settings with hundreds of providers. We show experimentally that our mechanisms extract a high surplus and that even our suboptimal variants typically achieve a high efficiency (95% or more in a wide range of settings).}
}
@article{CHANG2007434,
title = {No regrets about no-regret},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {434-439},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000045},
author = {Yu-Han Chang},
keywords = {Multi-agent learning, Regret-minimization, Game theory},
abstract = {No-regret is described as one framework that game theorists and computer scientists have converged upon for designing and evaluating multi-agent learning algorithms. However, Shoham, Powers, and Grenager also point out that the framework has serious deficiencies, such as behaving sub-optimally against certain reactive opponents. But all is not lost. With some simple modifications, regret-minimizing algorithms can perform in many of the ways we wish multi-agent learning algorithms to perform, providing safety and adaptability against reactive opponents. We argue that the research community should have no regrets about no-regret methods.}
}
@article{LI201551,
title = {On redundant topological constraints},
journal = {Artificial Intelligence},
volume = {225},
pages = {51-76},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000569},
author = {Sanjiang Li and Zhiguo Long and Weiming Liu and Matt Duckham and Alan Both},
keywords = {Qualitative spatial reasoning, Region connection calculus, Redundancy, Prime subnetwork, Distributive subalgebra},
abstract = {Redundancy checking is an important task in the research of knowledge representation and reasoning. In this paper, we consider redundant qualitative constraints. For a set Î“ of qualitative constraints, we say a constraint (xRy) in Î“ is redundant if it is entailed by the rest of Î“. A prime subnetwork of Î“ is a subset of Î“ which contains no redundant constraints and has the same solution set as Î“. It is natural to ask how to compute such a prime subnetwork, and when it is unique. We show that this problem is in general intractable, but becomes tractable if Î“ is over a tractable subalgebra S of a qualitative calculus. Furthermore, if S is a subalgebra of the Region Connection Calculus RCC8 in which weak composition distributes over nonempty intersections, then Î“ has a unique prime subnetwork, which can be obtained in cubic time by removing all redundant constraints simultaneously from Î“. As a by-product, we show that any path-consistent network over such a distributive subalgebra is minimal and globally consistent in a qualitative sense. A thorough empirical analysis of the prime subnetwork upon real geographical data sets demonstrates the approach is able to identify significantly more redundant constraints than previously proposed algorithms, especially in constraint networks with larger proportions of partial overlap relations.}
}
@article{HERNANDEZORALLO201674,
title = {Computer models solving intelligence test problems: Progress and implications},
journal = {Artificial Intelligence},
volume = {230},
pages = {74-107},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001538},
author = {JosÃ© HernÃ¡ndez-Orallo and Fernando MartÃ­nez-Plumed and Ute Schmid and Michael Siebers and David L. Dowe},
keywords = {Intelligence tests, Cognitive models, Artificial intelligence, Intelligence evaluation},
abstract = {While some computational models of intelligence test problems were proposed throughout the second half of the XXth century, in the first years of the XXIst century we have seen an increasing number of computer systems being able to score well on particular intelligence test tasks. However, despite this increasing trend there has been no general account of all these works in terms of how they relate to each other and what their real achievements are. Also, there is poor understanding about what intelligence tests measure in machines, whether they are useful to evaluate AI systems, whether they are really challenging problems, and whether they are useful to understand (human) intelligence. In this paper, we provide some insight on these issues, in the form of nine specific questions, by giving a comprehensive account of about thirty computer models, from the 1960s to nowadays, and their relationships, focussing on the range of intelligence test tasks they address, the purpose of the models, how general or specialised these models are, the AI techniques they use in each case, their comparison with human performance, and their evaluation of item difficulty. As a conclusion, these tests and the computer models attempting them show that AI is still lacking general techniques to deal with a variety of problems at the same time. Nonetheless, a renewed attention on these problems and a more careful understanding of what intelligence tests offer for AI may help build new bridges between psychometrics, cognitive science, and AI; and may motivate new kinds of problem repositories.}
}
@article{DAVIS201746,
title = {Commonsense reasoning about containers using radically incomplete information},
journal = {Artificial Intelligence},
volume = {248},
pages = {46-84},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300383},
author = {Ernest Davis and Gary Marcus and Noah Frazier-Logue},
keywords = {Commonsense reasoning, Physical reasoning, Spatial reasoning, Containers},
abstract = {In physical reasoning, humans are often able to carry out useful reasoning based on radically incomplete information. One physical domain that is ubiquitous both in everyday interactions and in many kinds of scientific applications, where reasoning from incomplete information is very common, is the interaction of containers and their contents. We have developed a preliminary knowledge base for qualitative reasoning about containers, expressed in a sorted first-order language of time, geometry, objects, histories, and actions. We have demonstrated that the knowledge suffices to justify a number of commonsense physical inferences, based on very incomplete knowledge.}
}
@article{TUYLS2007406,
title = {What evolutionary game theory tells us about multiagent learning},
journal = {Artificial Intelligence},
volume = {171},
number = {7},
pages = {406-416},
year = {2007},
note = {Foundations of Multi-Agent Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000082},
author = {Karl Tuyls and Simon Parsons},
keywords = {Evolutionary game theory, Replicator dynamics, Multiagent learning},
abstract = {This paper discusses If multi-agent learning is the answer, what is the question? [Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Intelligence 171 (7) (2007) 365â€“377, this issue] from the perspective of evolutionary game theory. We briefly discuss the concepts of evolutionary game theory, and examine the main conclusions from [Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Intelligence 171 (7) (2007) 365â€“377, this issue] with respect to some of our previous work. Overall we find much to agree with, concluding, however, that the central concerns of multiagent learning are rather narrow compared with the broad variety of work identified in [Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Inteligence 171 (7) (2007) 365â€“377, this issue].}
}
@article{SERINA20101369,
title = {Kernel functions for case-based planning},
journal = {Artificial Intelligence},
volume = {174},
number = {16},
pages = {1369-1406},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001293},
author = {Ivan Serina},
keywords = {Case-based planning, Domain-independent planning, Case-based reasoning, Heuristic search for planning, Kernel functions},
abstract = {Case-based planning can take advantage of former problem-solving experiences by storing in a plan library previously generated plans that can be reused to solve similar planning problems in the future. Although comparative worst-case complexity analyses of plan generation and reuse techniques reveal that it is not possible to achieve provable efficiency gain of reuse over generation, we show that the case-based planning approach can be an effective alternative to plan generation when similar reuse candidates can be chosen. In this paper we describe an innovative case-based planning system, called OAKplan, which can efficiently retrieve planning cases from plan libraries containing more than ten thousand cases, choose heuristically a suitable candidate and adapt it to provide a good quality solution plan which is similar to the one retrieved from the case library. Given a planning problem we encode it as a compact graph structure, that we call Planning Encoding Graph, which gives us a detailed description of the topology of the planning problem. By using this graph representation, we examine an approximate retrieval procedure based on kernel functions that effectively match planning instances, achieving extremely good performance in standard benchmark domains. The experimental results point out the effect of the case base size and the importance of accurate matching functions for global system performance. Overall, we show that OAKplan is competitive with state-of-the-art plan generation systems in terms of number of problems solved, CPU time, plan difference values and plan quality when cases similar to the current planning problem are available in the plan library.}
}
@article{SERNA2022103682,
title = {Sensitive loss: Improving accuracy and fairness of face representations with discrimination-aware deep learning},
journal = {Artificial Intelligence},
volume = {305},
pages = {103682},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103682},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000224},
author = {Ignacio Serna and Aythami Morales and Julian Fierrez and Nick Obradovich},
keywords = {Machine behavior, Bias, Fairness, Discrimination, Machine learning, Learning representations, Face, Biometrics},
abstract = {We propose a discrimination-aware learning method to improve both the accuracy and fairness of biased face recognition algorithms. The most popular face recognition benchmarks assume a distribution of subjects without paying much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. We also propose a notational framework for algorithmic discrimination with application to face biometrics. The experiments include three popular face recognition models and three public databases composed of 64,000 identities from different demographic groups characterized by sex and ethnicity. We experimentally show that learning processes based on the most used face databases have led to popular pre-trained deep face models that present evidence of strong algorithmic discrimination. Finally, we propose a discrimination-aware learning method, Sensitive Loss, based on the popular triplet loss function and a sensitive triplet generator. Our approach works as an add-on to pre-trained networks and is used to improve their performance in terms of average accuracy and fairness. The method shows results comparable to state-of-the-art de-biasing networks and represents a step forward to prevent discriminatory automatic systems.}
}
@article{HORTON199725,
title = {Clause trees: a tool for understanding and implementing resolution in automated reasoning},
journal = {Artificial Intelligence},
volume = {92},
number = {1},
pages = {25-89},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00046-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029600046X},
author = {J.D. Horton and Bruce Spencer},
keywords = {Automated theorem proving, Redundancy, Minimality, Proof procedures},
abstract = {A new methodology/data structure, the clause tree, is developed for automated reasoning based on resolution in first order logic. A clause tree T on a set S of clauses is a 4-tuple ã€ˆN, E, L, Mã€‰, where N a set of nodes, divided into clause nodes and atom nodes, E is a set of edges, each of which joins a clause node to an atom node, L is a labeling of N âˆª E which assigns to each clause node a clause of S, to each atom node an instance of an atom of some clause of S, and to each edge either + or âˆ’. The edge joining a clause node to an atom node is labeled by the sign of the corresponding literal in the clause. A resolution is represented by unifying two atom nodes of different clause trees which represent complementary literals. The merge of two identical literals is represented by placing the path joining the two corresponding atom nodes into the set M of chosen merge paths. The tail of the merge path becomes a closed leaf, while the head remains an open leaf which can be resolved on. The clause cl(T) that T represents is the set of literals corresponding to the labels of the open leaves modified by the signs of the incident edges. The fundamental purpose of a clause tree T is to show that cl(T) can be derived from S using resolution. Loveland's model elimination ME, the selected literal procedure SL, and Shostak's graph construction procedure GC are explained in a unified manner using clause trees. The condition required for choosing a merge path whose head is not a leaf is given. This allows a clause tree to be built in one way (the build ordering) but justified as a proof in another (the proof ordering). The ordered clause set restriction and the foothold score restriction are explained using the operation on clause trees of merge path reversal. A new procedure called ALPOC, which combines ideas from ME, GC and Spencer's ordered clause set restriction (OC), to form a new procedure tighter than any of the top down procedures above, is developed and shown to be sound and complete. Another operation on clause trees called surgery is defined, and used to define a minimal clause tree. Any non-minimal clause tree can be reduced to a minimal clause tree using surgery, thereby showing that non-minimal clause trees are redundant. A sound procedure MinALPOC that produces only minimal clause trees is given. Mergeless clause trees are shown to be equivalent to each of input resolution, unit resolution and relative Horn sets, thereby giving short proofs of some known results. Many other new proof procedures using clause trees are discussed briefly, leaving many open questions.}
}
@article{SELMAN1996273,
title = {Critical behavior in the computational cost of satisfiability testing},
journal = {Artificial Intelligence},
volume = {81},
number = {1},
pages = {273-295},
year = {1996},
note = {Frontiers in Problem Solving: Phase Transitions and Complexity},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00056-9},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000569},
author = {Bart Selman and Scott Kirkpatrick},
keywords = {Dynamical critical phenomena, Phase transition, Finite-size scaling, Satisfiability, -satisfiability, Computational cost scaling, Complexity},
abstract = {In previous work, we employed finite-size scaling, a method from statistical mechanics, to explore the crossover from the SAT regime of K-SAT, where almost all randomly generated expressions are satisfiable, to the UNSAT regime, where almost all are not. In this work, we extend the experiments to cover critical behavior in the computational cost. We find that the median computational cost takes on a universal form across the transition regime. Finite-size scaling accounts for its dependence on N (the number of variables) and on M (the number of clauses in the k-CNF expression). We also inquire into the sources of the complexity by studying distributions of computational cost. In the SAT phase we observe an unusually wide range of costs. The median cost increases linearly with N, while the mean is significantly increased over the median by a small fraction of cases in which exponentially large costs are incurred. We show that the large spread in cost of finding assignments is mainly due to the variability of running time of the Davis-Putnam (DP) procedure, used to determine the satisfiability of our expressions. In particular, if we consider a single satisfiable expression and run DP many times, each time randomly relabelling the variables in the expression, the resulting distribution of costs nearly reproduces the distribution of costs encountered by running DP search once on each of many such randomly generated satisfiable expressions. There are intriguing similarities and differences between these effects and kinetic phenomena studied in statistical physics, in glasses and in spin glasses.}
}
@article{PIERCE1997169,
title = {Map learning with uninterpreted sensors and effectors},
journal = {Artificial Intelligence},
volume = {92},
number = {1},
pages = {169-227},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00051-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000513},
author = {David Pierce and Benjamin J. Kuipers},
keywords = {Spatial semantic hierarchy, Map learning, Cognitive maps, Feature learning, Abstract interfaces, Action models, Changes of representation},
abstract = {This paper presents a set of methods by which a learning agent can learn a sequence of increasingly abstract and powerful interfaces to control a robot whose sensorimotor apparatus and environment are initially unknown. The result of the learning is a rich hierarchical model of the robot's world (its sensorimotor apparatus and environment). The learning methods rely on generic properties of the robot's world such as almost-everywhere smooth effects of motor control signals on sensory features. At the lowest level of the hierarchy, the learning agent analyzes the effects of its motor control signals in order to define a new set of control signals, one for each of the robot's degrees of freedom. It uses a generate-and-test approach to define sensory features that capture important aspects of the environment. It uses linear regression to learn models that characterize context-dependent effects of the control signals on the learned features. It uses these models to define high-level control laws for finding and following paths defined using constraints on the learned features. The agent abstracts these control laws, which interact with the continuous environment, to a finite set of actions that implement discrete state transitions. At this point, the agent has abstracted the robot's continuous world to a finite-state world and can use existing methods to learn its structure. The learning agent's methods are evaluated on several simulated robots with different sensorimotor systems and environments.}
}
@article{DAMOTTASALLESBARRETO2008454,
title = {Restricted gradient-descent algorithm for value-function approximation in reinforcement learning},
journal = {Artificial Intelligence},
volume = {172},
number = {4},
pages = {454-482},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001257},
author = {AndrÃ© {da Motta Salles Barreto} and Charles W. Anderson},
keywords = {Reinforcement learning, Neuro-dynamic programming, Value-function approximation, Radial-basis-function networks},
abstract = {This work presents the restricted gradient-descent (RGD) algorithm, a training method for local radial-basis function networks specifically developed to be used in the context of reinforcement learning. The RGD algorithm can be seen as a way to extract relevant features from the state space to feed a linear model computing an approximation of the value function. Its basic idea is to restrict the way the standard gradient-descent algorithm changes the hidden units of the approximator, which results in conservative modifications that make the learning process less prone to divergence. The algorithm is also able to configure the topology of the network, an important characteristic in the context of reinforcement learning, where the changing policy may result in different requirements on the approximator structure. Computational experiments are presented showing that the RGD algorithm consistently generates better value-function approximations than the standard gradient-descent method, and that the latter is more susceptible to divergence. In the pole-balancing and Acrobot tasks, RGD combined with SARSA presents competitive results with other methods found in the literature, including evolutionary and recent reinforcement-learning algorithms.}
}
@article{ZHANG1995241,
title = {Performance of linear-space search algorithms},
journal = {Artificial Intelligence},
volume = {79},
number = {2},
pages = {241-292},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00047-6},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000476},
author = {Weixiong Zhang and Richard E. Korf},
abstract = {Search algorithms that use space linear in the search depth are widely employed in practice to solve difficult problems optimally, such as planning and scheduling. In this paper, we study the average-case performance of linear-space search algorithms, including depth-first branch-and-bound (DFBnB), iterative-deepening (ID), and recursive best-first search (RBFS). To facilitate our analyses, we use a random tree T(b,d) that has mean branching factor b, depth d, and node costs that are the sum of the costs of the edges from the root to the nodes. We prove that the expected number of nodes expanded by DFBnB on a random tree is no more than bd times the expected number of nodes expanded by best-first search (BFS) on the same tree, which usually requires space that is exponential in depth d. We also show that DFBnB is asymptotically optimal when BFS runs in exponential time, and ID and RBFS are asymptotically optimal when the edge costs of T(b,d) are integers. If bp0 is the expected number of children of a node whose costs are the same as that of their parent, then the expected number of nodes expanded by these three linear-space algorithms is exponential when bpo < 1, at most O(d4) when bp0 = 1, and at most quadratic when bp0 > 1. In addition, we study the heuristic branching factor of T(b,d) and the effective branching factor of BFS, DFBnB, ID, and RBFS on T(b,d). Furthermore, we use our analytic results to explain a surprising anomaly in the performance of these algorithms, and to predict the existence of a complexity transition in the Asymmetric Traveling Salesman Problem.}
}
@article{YING20051,
title = {Knowledge transformation and fusion in diagnostic systems},
journal = {Artificial Intelligence},
volume = {163},
number = {1},
pages = {1-45},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204001572},
author = {Mingsheng Ying},
keywords = {Diagnostic system, Diagnostic specification, Notion of diagnosis, Knowledge transformation, Knowledge fusion, Specification morphism, Operations of specification},
abstract = {Diagnostic systems depend on knowledge bases specifying the causal, structural or functional interactions among components of the diagnosed objects. A diagnostic specification in a diagnostic system is a semantic interpretation of a knowledge base. We introduce the notion of diagnostic specification morphism and some operations of diagnostic specifications that can be used to model knowledge transformation and fusion, respectively. The relation between diagnostic methods in the source system and the target system of a specification morphism is examined. Also, representations of diagnostic methods in a composed system modelled by operations of specifications are given in terms of the corresponding diagnostic methods in its component systems.}
}
@article{VANS1998135,
title = {A belief network approach to optimization and parameter estimation: application to resource and environmental management},
journal = {Artificial Intelligence},
volume = {101},
number = {1},
pages = {135-163},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00010-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000101},
author = {Olli Vans},
keywords = {Bayesian methods, Belief networks, Environmental policies, Hybrid models, Parameter estimation, Probabilistic models, Optimization, Resource management, Water quality},
abstract = {An approach to use Bayesian belief networks in optimization is presented, with an illustration on resource and environmental management. A belief network is constructed to work parallel to a deterministic model, and it is used to update conditional probabilities associated with different components of that model. The divergence between prior and posterior probability distributions at the model components is used as an indication on the inconsistency between model structure, parameter values, and other information used. An iteration scheme was developed to force prior and posterior distributions to become equal. This removes inconsistencies between different sources of information. The scheme can be used in different optimization tasks including parameter estimation and optimization between various policy options. Also multiobjective optimization is possible. The approach is illustrated with an example on cost-effective management of river water quality.}
}
@article{GABALDON201125,
title = {Non-Markovian control in the Situation Calculus},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {25-48},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000482},
author = {Alfredo Gabaldon},
keywords = {Reasoning about actions, Situation Calculus},
abstract = {In reasoning about actions, it is commonly assumed that the dynamics of domains satisfies the Markov Property: the executability conditions and the effects of all actions are fully determined by the present state of the system. This is true in particular in Reiter's Basic Action Theories in the Situation Calculus. In this paper, we generalize Basic Action Theories by removing the Markov property restriction, making it possible to directly axiomatize actions whose effects and executability conditions may depend on past and even alternative, hypothetical situations. We then generalize Reiter's regression operator, which is the main computational mechanism used for reasoning with Basic Action Theories, so that it can be used with non-Markovian theories.}
}
@article{ASUNCION201572,
title = {Ordered completion for logic programs with aggregates},
journal = {Artificial Intelligence},
volume = {224},
pages = {72-102},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000533},
author = {Vernon Asuncion and Yin Chen and Yan Zhang and Yi Zhou},
keywords = {Knowledge representation and reasoning, Answer Set Programming, Aggregates, First-order logic, Logic programming},
abstract = {We consider the problem of translating first-order answer set programs with aggregates into first-order sentences with the same type of aggregates. In particular, we show that, on finite structures, normal logic programs with convex aggregates, which cover both monotone and antimonotone aggregates as well as the aggregates appearing in most benchmark programs, can always be captured in first-order logic with the same type of aggregates by introducing auxiliary predicates. More precisely, we prove that every finite stable model of a normal program with convex aggregates is corresponding to a classical model of its enhanced ordered completion. This translation then suggests an alternative way for computing the stable models of such kind of programs. We report some experimental results, which demonstrate that our solver GROCv2 is comparable to the state-of-the-art answer set solvers. We further show that convex aggregates form a maximal class for this purpose. That is, we can always construct a normal logic program under any given non-convex aggregate context and prove that it can never be translated into first-order sentences with the same type of aggregates unless NP=coNP.}
}
@article{MOLINO2015157,
title = {Playing with knowledge: A virtual player for â€œWho Wants to Be a Millionaire?â€ that leverages question answering techniques},
journal = {Artificial Intelligence},
volume = {222},
pages = {157-181},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000259},
author = {Piero Molino and Pasquale Lops and Giovanni Semeraro and Marco {de Gemmis} and Pierpaolo Basile},
keywords = {Language game, Question answering, Natural language processing, Artificial intelligence, Decision making},
abstract = {This paper describes the techniques used to build a virtual player for the popular TV game â€œWho Wants to Be a Millionaire?â€. The player must answer a series of multiple-choice questions posed in natural language by selecting the correct answer among four different choices. The architecture of the virtual player consists of 1) a Question Answering (QA) module, which leverages Wikipedia and DBpedia datasources to retrieve the most relevant passages of text useful to identify the correct answer to a question, 2) an Answer Scoring (AS) module, which assigns a score to each candidate answer according to different criteria based on the passages of text retrieved by the Question Answering module, and 3) a Decision Making (DM) module, which chooses the strategy for playing the game according to specific rules as well as to the scores assigned to the candidate answers. We have evaluated both the accuracy of the virtual player to correctly answer to questions of the game, and its ability to play real games in order to earn money. The experiments have been carried out on questions coming from the official Italian and English boardgames. The average accuracy of the virtual player for Italian is 79.64%, which is significantly better than the performance of human players, which is equal to 51.33%. The average accuracy of the virtual player for English is 76.41%. The comparison with human players is not carried out for English since, playing successfully the game heavily depends on the players' knowledge about popular culture, and in this experiment we have only involved a sample of Italian players. As regards the ability to play real games, which involves the definition of a proper strategy for the usage of lifelines in order to decide whether to answer to a question even in a condition of uncertainty or to retire from the game by taking the earned money, the virtual player earns â‚¬ 114,531 on average for Italian, and â‚¬ 88,878 for English, which exceeds the average amount earned by the human players to a greater extent (â‚¬ 5926 for Italian).}
}
@article{HUNTER20101007,
title = {On the measure of conflicts: Shapley Inconsistency Values},
journal = {Artificial Intelligence},
volume = {174},
number = {14},
pages = {1007-1026},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S000437021000086X},
author = {Anthony Hunter and SÃ©bastien Konieczny},
keywords = {Inconsistency management, Inconsistency tolerance, Inconsistency measures, Conflict resolution, Paraconsistency, Shapley values},
abstract = {There are relatively few proposals for inconsistency measures for propositional belief bases. However inconsistency measures are potentially as important as information measures for artificial intelligence, and more generally for computer science. In particular, they can be useful to define various operators for belief revision, belief merging, and negotiation. The measures that have been proposed so far can be split into two classes. The first class of measures takes into account the number of formulae required to produce an inconsistency: the more formulae required to produce an inconsistency, the less inconsistent the base. The second class takes into account the proportion of the language that is affected by the inconsistency: the more propositional variables affected, the more inconsistent the base. Both approaches are sensible, but there is no proposal for combining them. We address this need in this paper: our proposal takes into account both the number of variables affected by the inconsistency and the distribution of the inconsistency among the formulae of the base. Our idea is to use existing inconsistency measures in order to define a game in coalitional form, and then to use the Shapley value to obtain an inconsistency measure that indicates the responsibility/contribution of each formula to the overall inconsistency in the base. This allows us to provide a more reliable image of the belief base and of the inconsistency in it.}
}
@article{SHEHORY1998165,
title = {Methods for task allocation via agent coalition formation},
journal = {Artificial Intelligence},
volume = {101},
number = {1},
pages = {165-200},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00045-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000459},
author = {Onn Shehory and Sarit Kraus},
keywords = {Multi-agent cooperation, Coalition formation, Task allocation},
abstract = {Task execution in multi-agent environments may require cooperation among agents. Given a set of agents and a set of tasks which they have to satisfy, we consider situations where each task should be attached to a group of agents that will perform the task. Task allocation to groups of agents is necessary when tasks cannot be performed by a single agent. However it may also be beneficial when groups perform more efficiently with respect to the single agents' performance. In this paper we present several solutions to the problem of task allocation among autonomous agents, and suggest that the agents form coalitions in order to perform tasks or improve the efficiency of their performance. We present efficient distributed algorithms with low ratio bounds and with low computational complexities. These properties are proven theoretically and supported by simulations and an implementation in an agent system. Our methods are based on both the algorithmic aspects of combinatorics and approximation algorithms for NP-hard problems. We first present an approach to agent coalition formation where each agent must be a member of only one coalition. Next, we present the domain of overlapping coalitions. We proceed with a discussion of the domain where tasks may have a precedence order. Finally, we discuss the case of implementation in an open, dynamic agent system. For each case we provide an algorithm that will lead agents to the formation of coalitions, where each coalition is assigned a task. Our algorithms are any-time algorithms, they are simple, efficient and easy to implement.}
}
@article{BOUTILIER2015190,
title = {Optimal social choice functions: A utilitarian view},
journal = {Artificial Intelligence},
volume = {227},
pages = {190-213},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000892},
author = {Craig Boutilier and Ioannis Caragiannis and Simi Haber and Tyler Lu and Ariel D. Procaccia and Or Sheffet},
keywords = {Computational social choice},
abstract = {We adopt a utilitarian perspective on social choice, assuming that agents have (possibly latent) utility functions over some space of alternatives. For many reasons one might consider mechanisms, or social choice functions, that only have access to the ordinal rankings of alternatives by the individual agents rather than their utility functions. In this context, one possible objective for a social choice function is the maximization of (expected) social welfare relative to the information contained in these rankings. We study such optimal social choice functions under three different models, and underscore the important role played by scoring functions. In our worst-case model, no assumptions are made about the underlying distribution and we analyze the worst-case distortionâ€”or degree to which the selected alternative does not maximize social welfareâ€”of optimal (randomized) social choice functions. In our average-case model, we derive optimal functions under neutral (or impartial culture) probabilistic models. Finally, a very general learning-theoretic model allows for the computation of optimal social choice functions (i.e., ones that maximize expected social welfare) under arbitrary, sampleable distributions. In the latter case, we provide both algorithms and sample complexity results for the class of scoring functions, and further validate the approach empirically.}
}
@article{RIGAS2018248,
title = {Algorithms for electric vehicle scheduling in large-scale mobility-on-demand schemes},
journal = {Artificial Intelligence},
volume = {262},
pages = {248-278},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218303199},
author = {Emmanouil S. Rigas and Sarvapali D. Ramchurn and Nick Bassiliades},
keywords = {Mixed integer programming, Heuristic search, Local search, Max-flow, Electric vehicles, Shared vehicles, Mobility on demand},
abstract = {We study a setting where Electric Vehicles (EVs) can be hired to drive from pick-up to drop-off points in a Mobility-on-Demand (MoD) scheme. The goal of the system is, either to maximize the number of customers that are serviced, or the total EV utilization. To do so, we characterise the optimisation problem as a max-flow problem in order to determine the set of feasible trips given the available EVs at each location. We then model and solve the EV-to-trip scheduling problem offline and optimally using Mixed Integer Programming (MIP) techniques and show that the solution scales up to medium sized problems. Given this, we develop two non-optimal algorithms, namely an incremental-MIP algorithm for medium to large problems and a greedy heuristic algorithm for very large problems. Moreover, we develop a tabu search-based local search technique to further improve upon and compare against the solution of the non-optimal algorithms. We study the performance of these algorithms in settings where either battery swap or battery charge at each station is used to cope with the EVs' limited driving range. Moreover, in settings where EVs need to be scheduled online, we propose a novel algorithm that accounts for the uncertainty in future trip requests. All algorithms are empirically evaluated using real-world data of locations of shared vehicle pick-up and drop-off stations. In our experiments, we observe that when all EVs carry the same battery which is large enough for the longest trips, the greedy algorithm with battery swap with the max-flow solution as a pre-processing step, provides the optimal solution. At the same time, the greedy algorithm with battery charge is close to the optimal (97% on average) and is further improved when local search is used. When some EVs do not have a large enough battery to execute some of the longest trips, the incremental-MIP generates solutions slightly better than the greedy, while the optimal algorithm is the best but scales up to medium sized problems only. Moreover, the online algorithm is shown to be on average at least 90% of the optimal. Finally, the greedy algorithm scales to 10-times more tasks than the incremental-MIP and 1000-times more than the static MIP in reasonable time.}
}
@article{ASH1996317,
title = {Using action-based hierarchies for real-time diagnosis},
journal = {Artificial Intelligence},
volume = {88},
number = {1},
pages = {317-347},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00024-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000240},
author = {David Ash and Barbara Hayes-Roth},
keywords = {Reactive planning, Decision trees, Diagnosis, Real-time planning, Heuristics},
abstract = {An intelligent agent diagnoses perceived problems so that it can respond to them appropriately. Basically, the agent performs a series of tests whose results discriminate among competing hypotheses. Given a specific diagnosis, the agent performs the associated action. Using the traditional information-theoretic heuristic to order diagnostic tests in a decision tree, the agent can maximize the information obtained from each successive test and thereby minimize the average time (number of tests) required to complete a diagnosis and perform the appropriate action. However, in real-time domains, even the optimal sequence of tests cannot always be performed in the time available. Nonetheless, the agent must respond. For agents operating in real-time domains, we propose an alternative action-based approach in which: (a) each node in the diagnosis tree is augmented to include an ordered set of actions, each of which has positive utility for all of its children in the tree; and (b) the tree is structured to maximize the expected utility of the action available at each node. Upon perceiving a problem, the agent works its way through the tree, performing tests that discriminate among successively smaller subsets of potential faults. When a deadline occurs, the agent performs the best available action associated with the most specific node it has reached so far. Although the action-based approach does not minimize the time required to complete a specific diagnosis, it provides positive utility responses, with step-wise improvements in expected utility, throughout the diagnosis process. We present theoretical and empirical results contrasting the advantages and disadvantages of the information-theoretic and action-based approaches.}
}
@article{ZHANG2006739,
title = {Solving logic program conflict through strong and weak forgettings},
journal = {Artificial Intelligence},
volume = {170},
number = {8},
pages = {739-778},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000221},
author = {Yan Zhang and Norman Y. Foo},
keywords = {Conflict solving, Knowledge representation, Answer set semantics, Logic program update, Computational complexity},
abstract = {We consider how to forget a set of atoms in a logic program. Intuitively, when a set of atoms is forgotten from a logic program, all atoms in the set should be eliminated from this program in some way, and other atoms related to them in the program might also be affected. We define notions of strong and weak forgettings in logic programs to capture such intuition, reveal their close connections to the notion of forgetting in classical propositional theories, and provide a precise semantic characterization for them. Based on these notions, we then develop a general framework for conflict solving in logic programs. We investigate various semantic properties and features in relation to strong and weak forgettings and conflict solving in the proposed framework. We argue that many important conflict solving problems can be represented within this framework. In particular, we show that all major logic program update approaches can be transformed into our framework, under which each approach becomes a specific conflict solving case with certain constraints. We also study essential computational properties of strong and weak forgettings and conflict solving in the framework.}
}
@article{EITER20081644,
title = {Semantic forgetting in answer set programming},
journal = {Artificial Intelligence},
volume = {172},
number = {14},
pages = {1644-1672},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000684},
author = {Thomas Eiter and Kewen Wang},
keywords = {Answer set programming, Nonmonotonic logic programs, Knowledge representation, Forgetting, Computational complexity},
abstract = {The notion of forgetting, also known as variable elimination, has been investigated extensively in the context of classical logic, but less so in (nonmonotonic) logic programming and nonmonotonic reasoning. The few approaches that exist are based on syntactic modifications of a program at hand. In this paper, we establish a declarative theory of forgetting for disjunctive logic programs under answer set semantics that is fully based on semantic grounds. The suitability of this theory is justified by a number of desirable properties. In particular, one of our results shows that our notion of forgetting can be entirely captured by classical forgetting. We present several algorithms for computing a representation of the result of forgetting, and provide a characterization of the computational complexity of reasoning from a logic program under forgetting. As applications of our approach, we present a fairly general framework for resolving conflicts in inconsistent knowledge bases that are represented by disjunctive logic programs, and we show how the semantics of inheritance logic programs and update logic programs from the literature can be characterized through forgetting. The basic idea of the conflict resolution framework is to weaken the preferences of each agent by forgetting certain knowledge that causes inconsistency. In particular, we show how to use the notion of forgetting to provide an elegant solution for preference elicitation in disjunctive logic programming.}
}
@article{SAY199675,
title = {Qualitative system identification: deriving structure from behavior},
journal = {Artificial Intelligence},
volume = {83},
number = {1},
pages = {75-141},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00016-X},
url = {https://www.sciencedirect.com/science/article/pii/000437029500016X},
author = {A.C.Cem Say and Selahattin Kuru},
abstract = {Qualitative reasoning programs (which perform simulation, comparative analysis, data interpretation, etc.) either take the model of the physical system to be considered as input, or compose it using a library of model fragments and input information about how to combine them. System identification is the task of creating models of systems, using data about their behaviors. We present the qualitative system identification algorithm QSI, which takes as input a set of qualitative behaviors of a physical system, and produces as output a constraint model of the system. QSI's output is guaranteed to produce its input when simulated. Furthermore, the QSI-made models usually contain meaningful â€œdeepâ€ parameters of the system which do not appear in the input behaviors. Various aspects of QSI and its applicability to diagnosis, as well as the model fragment formulation problem, are discussed.}
}
@article{MENGSHOEL2008955,
title = {Understanding the role of noise in stochastic local search: Analysis and experiments},
journal = {Artificial Intelligence},
volume = {172},
number = {8},
pages = {955-990},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000040},
author = {Ole J. Mengshoel},
keywords = {Stochastic local search, Noise, Markov chain models, Expected hitting times, Rational functions, Noise response curves, Probabilistic reasoning, Bayesian networks, Most probable explanation, Systematic experiments, Polynomial approximation, Convexity},
abstract = {Stochastic local search (SLS) algorithms have recently been proven to be among the best approaches to solving computationally hard problems. SLS algorithms typically have a number of parameters, optimized empirically, that characterize and determine their performance. In this article, we focus on the noise parameter. The theoretical foundation of SLS, including an understanding of how to the optimal noise varies with problem difficulty, is lagging compared to the strong empirical results obtained using these algorithms. A purely empirical approach to understanding and optimizing SLS noise, as problem instances vary, can be very computationally intensive. To complement existing experimental results, we formulate and analyze several Markov chain models of SLS in this article. In particular, we compute expected hitting times and show that they are rational functions for individual problem instances as well as their mixtures. Expected hitting time curves are analytical counterparts to noise response curves reported in the experimental literature. Hitting time analysis using polynomials and convex functions is also discussed. In addition, we present examples and experimental results illustrating the impact of varying noise probability on SLS run time. In experiments, where most probable explanations in Bayesian networks are computed, we use synthetic problem instances as well as problem instances from applications. We believe that our results provide an improved theoretical understanding of the role of noise in stochastic local search, thereby providing a foundation for further progress in this area.}
}
@article{DEWEERD201367,
title = {How much does it help to know what she knows you know? An agent-based simulation study},
journal = {Artificial Intelligence},
volume = {199-200},
pages = {67-92},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000441},
author = {Harmen {de Weerd} and Rineke Verbrugge and Bart Verheij},
keywords = {Agent-based models, Evolution of theory of mind},
abstract = {In everyday life, people make use of theory of mind by explicitly attributing unobservable mental content such as beliefs, desires, and intentions to others. Humans are known to be able to use this ability recursively. That is, they engage in higher-order theory of mind, and consider what others believe about their own beliefs. In this paper, we use agent-based computational models to investigate the evolution of higher-order theory of mind. We consider higher-order theory of mind across four different competitive games, including repeated single-shot and repeated extensive form games, and determine the advantage of higher-order theory of mind agents over their lower-order theory of mind opponents. Across these four games, we find a common pattern in which first-order and second-order theory of mind agents clearly outperform opponents that are more limited in their ability to make use of theory of mind, while the advantage for deeper recursion to third-order theory of mind is limited in comparison.}
}
@article{COHEN19951,
title = {Pac-learning non-recursive prolog clauses},
journal = {Artificial Intelligence},
volume = {79},
number = {1},
pages = {1-38},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00034-4},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000344},
author = {William W. Cohen},
keywords = {Machine learning, inductive logic programming, pac-learning},
abstract = {Recently there has been an increasing amount of research on learning concepts expressed in subsets of Prolog; the term inductive logic programming (ILP) has been used to describe this growing body of research. This paper seeks to expand the theoretical foundations of ILP by investigating the pac-learnability of logic programs. We focus on programs consisting of a single function-free non-recursive clause, and focus on generalizations of a language known to be pac-learnable: namely, the language of determinate function-free clauses of constant depth. We demonstrate that a number of syntactic generalizations of this language are hard to learn, but that the language can be generalized to clauses of constant locality while still allowing pac-learnability. More specifically, we first show that determinate clauses of log depth are not pac-learnable, regardless of the language used to represent hypotheses. We then investigate the effect of allowing indeterminacy in a clause, and show that clauses with k indeterminate variables are as hard to learn as DNF. We next show that a more restricted language of clauses with bounded indeterminacy is learnable using k-CNF to represent hypotheses, and that restricting the â€œlocalityâ€ of a clause to a constant allows pac-learnability even if an arbitrary amount of indeterminacy is allowed. This last result is also shown to be a strict generalization of the previous result for determinate function-free clauses of constant depth. Finally, we present some extensions of these results to logic programs with multiple clauses.}
}
@article{BYLANDER1998335,
title = {Worst-case analysis of the perception and exponentiated update algorithms},
journal = {Artificial Intelligence},
volume = {106},
number = {2},
pages = {335-352},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00098-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000988},
author = {Tom Bylander},
keywords = {Learning algorithms, Absolute loss bounds, Mistake bounds, Randomized classification algorithms},
abstract = {The absolute loss is the absolute difference between the desired and predicted outcome. This paper demonstrates worst-case upper bounds on the absolute loss for the Perception learning algorithm and the Exponentiated Update learning algorithm, which is related to the Weighted Majority algorithm. The bounds characterize the behavior of the algorithms over any sequence of trials, where each trial consists of an example and a desired outcome interval (any value in the interval is an acceptable outcome). The worst-case absolute loss of both algorithms is bounded by: the absolute loss of the best linear function in a comparison class, plus a constant dependent on the initial weight vector, plus a per-trial loss. The per-trial loss can be eliminated if the learning algorithm is allowed a tolerance from the desired outcome. For concept learning, the worst-case bounds lead to mistake bounds that are comparable to past results.}
}
@article{DELIN2006409,
title = {Discovering the linear writing order of a two-dimensional ancient hieroglyphic script},
journal = {Artificial Intelligence},
volume = {170},
number = {4},
pages = {409-421},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205002146},
author = {Shou {de Lin} and Kevin Knight},
keywords = {Ancient script, Decipher, Luwian, Hieroglyphic, Unsupervised learning, Estimation-maximization, Linear order, Discovery, Writing system, Natural language process},
abstract = {This paper demonstrates how machine learning methods can be applied to deal with a real-world decipherment problem where very little background knowledge is available. The goal is to discover the linear order of a two-dimensional ancient script, Hieroglyphic Luwian. This paper records a complete decipherment process including encoding, modeling, parameter learning, optimization, and evaluation. The experiment shows that the proposed approach is general enough to recover the linear order of various manually generated two-dimensional scripts without needing to know in advance what language they represent and how the two-dimensional scripts were generated. Since the proposed method does not require domain specific knowledge, it can be applied not only to language problems but also order discovery tasks in other domains such as biology and chemistry.}
}
@article{AAS2021103502,
title = {Explaining individual predictions when features are dependent: More accurate approximations to Shapley values},
journal = {Artificial Intelligence},
volume = {298},
pages = {103502},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103502},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000539},
author = {Kjersti Aas and Martin Jullum and Anders LÃ¸land},
keywords = {Feature attribution, Shapley values, Kernel SHAP, Dependence},
abstract = {Explaining complex or seemingly simple machine learning models is an important practical problem. We want to explain individual predictions from such models by learning simple, interpretable explanations. Shapley value is a game theoretic concept that can be used for this purpose. The Shapley value framework has a series of desirable theoretical properties, and can in principle handle any predictive model. Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions. Like several other existing methods, this approach assumes that the features are independent. Since Shapley values currently suffer from inclusion of unrealistic data instances when features are correlated, the explanations may be very misleading. This is the case even if a simple linear model is used for predictions. In this paper, we extend the Kernel SHAP method to handle dependent features. We provide several examples of linear and non-linear models with various degrees of feature dependence, where our method gives more accurate approximations to the true Shapley values.}
}
@article{RAM199725,
title = {Continuous case-based reasoning},
journal = {Artificial Intelligence},
volume = {90},
number = {1},
pages = {25-77},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00037-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000379},
author = {A. Ram and J.C. SantamarÃ­a},
keywords = {Case-based reasoning, Machine learning, Reinforcement learning, Robot navigation, Reactive control, Motor schema-based navigation},
abstract = {Case-based reasoning systems have traditionally been used to perform high-level reasoning in problem domains that can be adequately described using discrete, symbolic representations. However, many real-world problem domains, such as autonomous robotic navigation, are better characterized using continuous representations. Such problem domains also require continuous performance, such as on-line sensorimotor interaction with the environment, and continuous adaptation and learning during the performance task. This article introduces a new method for continuous case-based reasoning, and discusses its application to the dynamic selection, modification, and acquisition of robot behaviors in an autonomous navigation system, SINS (self-improving navigation system). The computer program and the underlying method are systematically evaluated through statistical analysis of results from several empirical studies. The article concludes with a general discussion of case-based reasoning issues addressed by this research.}
}
@article{MELO20111757,
title = {Decentralized MDPs with sparse interactions},
journal = {Artificial Intelligence},
volume = {175},
number = {11},
pages = {1757-1789},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000634},
author = {Francisco S. Melo and Manuela Veloso},
keywords = {Multiagent coordination, Sparse interaction, Decentralized Markov decision processes},
abstract = {Creating coordinated multiagent policies in environments with uncertainty is a challenging problem, which can be greatly simplified if the coordination needs are known to be limited to specific parts of the state space. In this work, we explore how such local interactions can simplify coordination in multiagent systems. We focus on problems in which the interaction between the agents is sparse and contribute a new decision-theoretic model for decentralized sparse-interaction multiagent systems, Dec-SIMDPs, that explicitly distinguishes the situations in which the agents in the team must coordinate from those in which they can act independently. We relate our new model to other existing models such as MMDPs and Dec-MDPs. We then propose a solution method that takes advantage of the particular structure of Dec-SIMDPs and provide theoretical error bounds on the quality of the obtained solution. Finally, we show a reinforcement learning algorithm in which independent agents learn both individual policies and when and how to coordinate. We illustrate the application of the algorithms throughout the paper in several multiagent navigation scenarios.}
}
@article{ZHANG1996223,
title = {A study of complexity transitions on the asymmetric traveling salesman problem},
journal = {Artificial Intelligence},
volume = {81},
number = {1},
pages = {223-239},
year = {1996},
note = {Frontiers in Problem Solving: Phase Transitions and Complexity},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00054-2},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000542},
author = {Weixiong Zhang and Richard E. Korf},
keywords = {Traveling salesman problem, Phase transitions, Problem solving, Combinatorial optimization, Complexity, Search, Branch, bound},
abstract = {The traveling salesman problem (TSP) is one of the best-known combinatorial optimization problems. Branch-and-bound (BnB) is the best method for finding an optimal solution of the TSP. Previous research has shown that there exists a transition in the average computational complexity of BnB on random trees. We show experimentally that when the intercity distances of the asymmetric TSP are drawn uniformly from 0,1,2,â€¦, r, the complexity of BnB experiences an easy-hard transition as r increases. We also observe easy-hard-easy complexity transitions when asymmetric intercity distances are chosen from a log-normal distribution. This transition pattern is similar to one previously observed on the symmetric TSP. We then explain these different transition patterns by showing that the control parameter that determines the complexity is the number of distinct intercity distances.}
}
@article{PRADA200980,
title = {Teaming up humans with autonomous synthetic characters},
journal = {Artificial Intelligence},
volume = {173},
number = {1},
pages = {80-103},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001161},
author = {Rui Prada and Ana Paiva},
keywords = {Group dynamics, Teamwork, Social intelligence, Autonomous synthetic characters, Believability, Computerâ€“human interaction},
abstract = {Autonomous synthetic characters have the potential to promote the social engagement of users in virtual environments, enhancing their interaction experience. In computer games, for example, poor interaction with game characters can drastically detract from the gaming experience, making the design of autonomous synthetic characters an important issue. In particular, in Role Playing Games (RPGs), for example, users and autonomous characters often perform in a group. Usually, the role of such characters is very limited since they lack the social skills to perform coherently in group scenarios. The goal of the work presented here is to endow autonomous synthetic characters with social skills that allow them to perform in groups with human members. However, to successfully achieve this, it is not enough to assure that the characters behave in a coherent manner from an individual perspective or that they are able to perform the group task optimally. It is also necessary that the autonomous characters exhibit behaviours that are coherent with the group's composition, context and structure. For this reason, we have developed a model to support group dynamics of autonomous synthetic characters (SGD model) inspired by theories developed in human social psychological sciences. This model defines the knowledge that each individual should build about the others and the group, and how this knowledge drives their interactions. The model was used in a collaborative computer game that was tested with users. The results showed that the model had a positive effect on the users' social engagement, namely, on their trust and identification with the group.}
}
@article{SEN199643,
title = {Graph search methods for non-order-preserving evaluation functions: applications to job sequencing problems},
journal = {Artificial Intelligence},
volume = {86},
number = {1},
pages = {43-73},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00094-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000941},
author = {Anup K. Sen and Amitava Bagchi},
abstract = {Graph search with Aâˆ— is frequently faster than tree search. But Aâˆ— graph search operates correctly only when the evaluation function is order-preserving. In the non-orderpreserving case, no paths can be discarded and the entire explicit graph must be stored in memory. Such situations arise in one-machine minimum penalty job sequencing problems when setup times are sequence dependent. GREC, the unlimited memory version of a memory-constrained search algorithm of the authors called MREC, has a clear advantage over Aâˆ—in that it is able to find optimal solutions to such problems. At the same time, it is as efficient as Aâˆ— in solving graph search problems with order-preserving evaluation functions. Experimental results indicate that in the non-order-preserving case, GREC is faster than both best-first and depth-first tree search, and can solve problem instances of larger size than best-first tree search.}
}
@article{BONET20081579,
title = {Heuristics for planning with penalties and rewards formulated in logic and computed through circuits},
journal = {Artificial Intelligence},
volume = {172},
number = {12},
pages = {1579-1604},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000350},
author = {Blai Bonet and HÃ©ctor Geffner},
keywords = {Planning, Planning heuristics, Planning with rewards, Knowledge compilation},
abstract = {The automatic derivation of heuristic functions for guiding the search for plans is a fundamental technique in planning. The type of heuristics that have been considered so far, however, deal only with simple planning models where costs are associated with actions but not with states. In this work we address this limitation by formulating a more expressive planning model and a corresponding heuristic where preferences in the form of penalties and rewards are associated with fluents as well. The heuristic, that is a generalization of the well-known delete-relaxation heuristic, is admissible, informative, but intractable. Exploiting a correspondence between heuristics and preferred models, and a property of formulas compiled in d-DNNF, we show however that if a suitable relaxation of the domain, expressed as the strong completion of a logic program with no time indices or horizon is compiled into d-DNNF, the heuristic can be computed for any search state in time that is linear in the size of the compiled representation. This representation defines an evaluation network or circuit that maps states into heuristic values in linear-time. While this circuit may have exponential size in the worst case, as for OBDDs, this is not necessarily so. We report empirical results, discuss the application of the framework in settings where there are no goals but just preferences, and illustrate the versatility of the account by developing a new heuristic that overcomes limitations of delete-based relaxations through the use of valid but implicit plan constraints. In particular, for the Traveling Salesman Problem, the new heuristic captures the exact cost while the delete-relaxation heuristic, which is also exponential in the worst case, captures only the Minimum Spanning Tree lower bound.}
}
@article{GEREVINI2009619,
title = {Deterministic planning in the fifth international planning competition: PDDL3 and experimental evaluation of the planners},
journal = {Artificial Intelligence},
volume = {173},
number = {5},
pages = {619-668},
year = {2009},
note = {Advances in Automated Plan Generation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001847},
author = {Alfonso E. Gerevini and Patrik Haslum and Derek Long and Alessandro Saetti and Yannis Dimopoulos},
keywords = {Automated planning, Planning systems, PDDL, Planning languages, Knowledge representation in planning, Preferences in planning, Plan constraints, International planning competition, Benchmarks for planning, Experimental evaluation of planning systems},
abstract = {The international planning competition (IPC) is an important driver for planning research. The general goals of the IPC include pushing the state of the art in planning technology by posing new scientific challenges, encouraging direct comparison of planning systems and techniques, developing and improving a common planning domain definition language, and designing new planning domains and problems for the research community. This paper focuses on the deterministic part of the fifth international planning competition (IPC5), presenting the language and benchmark domains that we developed for the competition, as well as a detailed experimental evaluation of the deterministic planners that entered IPC5, which helps to understand the state of the art in the field. We present an extension of pddl, called pddl3, allowing the user to express strong and soft constraints about the structure of the desired plans, as well as strong and soft problem goals. We discuss the expressive power of the new language focusing on the restricted version that was used in IPC5, for which we give some basic results about its compilability into pddl2. Moreover, we study the relative performance of the IPC5 planners in terms of solved problems, CPU time, and plan quality; we analyse their behaviour with respect to the winners of the previous competition; and we evaluate them in terms of their capability of dealing with soft goals and constraints, and of finding good quality plans in general. Overall, the results indicate significant progress in the field, but they also reveal that some important issues remain open and require further research, such as dealing with strong constraints and computing high quality plans in metric-time domains and domains involving soft goals or constraints.}
}
@article{SUN1995241,
title = {Robust reasoning: integrating rule-based and similarity-based reasoning},
journal = {Artificial Intelligence},
volume = {75},
number = {2},
pages = {241-295},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00028-Y},
url = {https://www.sciencedirect.com/science/article/pii/000437029400028Y},
author = {Ron Sun},
abstract = {The paper attempts to account for common patterns in commonsense reasoning through integrating rule-based reasoning and similarity-based reasoning as embodied in connectionist models. Reasoning examples are analyzed and a diverse range of patterns is identified. A principled synthesis based on simple rules and similarities is performed, which unifies these patterns that were before difficult to be accounted for without specialized mechanisms individually. A two-level connectionist architecture with dual representations is proposed as a computational mechanism for carrying out the theory. It is shown in detail how the common patterns can be generated by this mechanism. Finally, it is argued that the brittleness problem of rule-based models can be remedied in a principled way, with the theory proposed here. This work demonstrates that combining rules and similarities can result in more robust reasoning models, and many seemingly disparate patterns of commonsense reasoning are actually different manifestations of the same underlying process and can be generated using the integrated architecture, which captures the underlying process to a large extent.}
}
@article{VANDEEMTER20081219,
title = {Fully generated scripted dialogue for embodied agents},
journal = {Artificial Intelligence},
volume = {172},
number = {10},
pages = {1219-1244},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000234},
author = {Kees {van Deemter} and Brigitte Krenn and Paul Piwek and Martin Klesen and Marc SchrÃ¶der and Stefan Baumann},
keywords = {Embodied conversational agents, Fully generated scripted dialogue, Multimodal interfaces, Emotion modelling, Affective reasoning, Natural language generation, Speech synthesis, Body language},
abstract = {This paper presents the NECA approach to the generation of dialogues between Embodied Conversational Agents (ECAs). This approach consist of the automated construction of an abstract script for an entire dialogue (cast in terms of dialogue acts), which is incrementally enhanced by a series of modules and finally â€œperformedâ€ by means of text, speech and body language, by a cast of ECAs. The approach makes it possible to automatically produce a large variety of highly expressive dialogues, some of whose essential properties are under the control of a user. The paper discusses the advantages and disadvantages of NECA's approach to Fully Generated Scripted Dialogue (FGSD), and explains the main techniques used in the two demonstrators that were built. The paper can be read as a survey of issues and techniques in the construction of ECAs, focusing on the generation of behaviour (i.e., focusing on information presentation) rather than on interpretation.}
}
@article{HORSWILL19951,
title = {Analysis of adaptation and environment},
journal = {Artificial Intelligence},
volume = {73},
number = {1},
pages = {1-30},
year = {1995},
note = {Computational Research on Interaction and Agency, Part 2},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00057-8},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000578},
author = {Ian Horswill},
abstract = {Designers often improve the performance of artificial agents by specializing them. We can make a rough, but useful distinction between specialization to a task and specialization to an environment. Specialization to an environment can be difficult to understand: it may be unclear on what properties of the environment the agent depends, or in what manner it depends on each individual property. In this paper, I discuss a method for analyzing specialization into a series of conditional optimizations: formal transformations which, given some constraint on the environment, map mechanisms to more efficient mechanisms with equivalent behavior. I apply the technique to the analysis of the vision and control systems of a working robot system in day to day use in our laboratory. The method is not intended as a general theory for automated synthesis of arbitrary specialized agents. Nonetheless, it can be used to perform post-hoc analysis of agents so as to make explicit the environment properties required by the agent and the computational value of each property. This post-hoc analysis helps explain performance in normal environments and predict performance in novel environments. In addition, the transformations brought out in the analysis of one system can be reused in the synthesis of future systems.}
}
@article{MUELLER20061017,
title = {Event calculus and temporal action logics compared},
journal = {Artificial Intelligence},
volume = {170},
number = {11},
pages = {1017-1029},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000567},
author = {Erik T. Mueller},
keywords = {Commonsense reasoning, Reasoning about action and change, Event calculus, Temporal action logics (TAL)},
abstract = {We compare the event calculus and temporal action logics (TAL), two formalisms for reasoning about action and change. We prove that, if the formalisms are restricted to integer time, inertial fluents, and relational fluents, and if TAL action type specifications are restricted to definite reassignment of a single fluent, then the formalisms are not equivalent. We argue that equivalence cannot be restored by using more general TAL action type specifications. We prove however that, if the formalisms are further restricted to single-step actions, then they are logically equivalent.}
}
@article{LI201545,
title = {Bi-goal evolution for many-objective optimization problems},
journal = {Artificial Intelligence},
volume = {228},
pages = {45-65},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000995},
author = {Miqing Li and Shengxiang Yang and Xiaohui Liu},
keywords = {Evolutionary multi-objective optimization, Many-objective optimization, Proximity, Diversity, Bi-goal evolution},
abstract = {This paper presents a meta-objective optimization approach, called Bi-Goal Evolution (BiGE), to deal with multi-objective optimization problems with many objectives. In multi-objective optimization, it is generally observed that 1) the conflict between the proximity and diversity requirements is aggravated with the increase of the number of objectives and 2) the Pareto dominance loses its effectiveness for a high-dimensional space but works well on a low-dimensional space. Inspired by these two observations, BiGE converts a given multi-objective optimization problem into a bi-goal (objective) optimization problem regarding proximity and diversity, and then handles it using the Pareto dominance relation in this bi-goal domain. Implemented with estimation methods of individuals' performance and the classic Pareto nondominated sorting procedure, BiGE divides individuals into different nondominated layers and attempts to put well-converged and well-distributed individuals into the first few layers. From a series of extensive experiments on four groups of well-defined continuous and combinatorial optimization problems with 5, 10 and 15 objectives, BiGE has been found to be very competitive against five state-of-the-art algorithms in balancing proximity and diversity. The proposed approach is the first step towards a new way of addressing many-objective problems as well as indicating several important issues for future development of this type of algorithms.}
}
@article{BONDARENKO199763,
title = {An abstract, argumentation-theoretic approach to default reasoning},
journal = {Artificial Intelligence},
volume = {93},
number = {1},
pages = {63-101},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00015-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000155},
author = {A. Bondarenko and P.M. Dung and R.A. Kowalski and F. Toni},
keywords = {Default reasoning, Nonmonotonic reasoning, Abduction, Argumentation, Nonmonotonic logics, Default logic, Autoepistemic logic, Nonmonotonic modal logics, Circumscription, Logic programming, Theorist},
abstract = {We present an abstract framework for default reasoning, which includes Theorist, default logic, logic programming, autoepistemic logic, non-monotonic modal logics, and certain instances of circumscription as special cases. The framework can be understood as a generalisation of Theorist. The generalisation allows any theory formulated in a monotonic logic to be extended by a defeasible set of assumptions. An assumption can be defeated (or â€œattackedâ€) if its â€œcontraryâ€ can be proved, possibly with the aid of other conflicting assumptions. We show that, given such a framework, the standard semantics of most logics for default reasoning can be understood as sanctioning a set of assumptions, as an extension of a given theory, if and only if the set of assumptions is conflict-free (in the sense that it does not attack itself) and it attacks every assumption not in the set. We propose a more liberal, argumentation-theoretic semantics, based upon the notion of admissible extension in logic programming. We regard a set of assumptions, in general, as admissible if and only if it is conflict-free and defends itself (by attacking) every set of assumptions which attacks it. We identify conditions for the existence of extensions and for the equivalence of different semantics.}
}
@article{RENOOIJ20081470,
title = {Enhanced qualitative probabilistic networks for resolving trade-offs},
journal = {Artificial Intelligence},
volume = {172},
number = {12},
pages = {1470-1494},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000519},
author = {Silja Renooij and Linda C. {van der Gaag}},
keywords = {Probabilistic reasoning, Qualitative reasoning, Trade-off resolution},
abstract = {Qualitative probabilistic networks were designed to overcome, to at least some extent, the quantification problem known to probabilistic networks. Qualitative networks abstract from the numerical probabilities of their quantitative counterparts by using signs to summarise the probabilistic influences between their variables. One of the major drawbacks of these qualitative abstractions, however, is the coarse level of representation detail that does not provide for indicating strengths of influences. As a result, the trade-offs modelled in a network remain unresolved upon inference. We present an enhanced formalism of qualitative probabilistic networks to provide for a finer level of representation detail. An enhanced qualitative probabilistic network differs from a basic qualitative network in that it distinguishes between strong and weak influences. Now, if a strong influence is combined, upon inference, with a conflicting weak influence, the sign of the net influence may be readily determined. Enhanced qualitative networks are purely qualitative in nature, as basic qualitative networks are, yet allow for resolving some trade-offs upon inference.}
}
@article{ONTANON2012129,
title = {A defeasible reasoning model of inductive concept learning from examples and communication},
journal = {Artificial Intelligence},
volume = {193},
pages = {129-148},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212001063},
author = {Santiago OntaÃ±Ã³n and Pilar Dellunde and LluÃ­s Godo and Enric Plaza},
keywords = {Induction, Logic, Argumentation, Machine learning, Concept learning},
abstract = {This paper introduces a logical model of inductive generalization, and specifically of the machine learning task of inductive concept learning (ICL). We argue that some inductive processes, like ICL, can be seen as a form of defeasible reasoning. We define a consequence relation characterizing which hypotheses can be induced from given sets of examples, and study its properties, showing they correspond to a rather well-behaved non-monotonic logic. We will also show that with the addition of a preference relation on inductive theories we can characterize the inductive bias of ICL algorithms. The second part of the paper shows how this logical characterization of inductive generalization can be integrated with another form of non-monotonic reasoning (argumentation), to define a model of multiagent ICL. This integration allows two or more agents to learn, in a consistent way, both from induction and from arguments used in the communication between them. We show that the inductive theories achieved by multiagent induction plus argumentation are sound, i.e. they are precisely the same as the inductive theories built by a single agent with all data.}
}
@article{CAMINADA2007286,
title = {On the evaluation of argumentation formalisms},
journal = {Artificial Intelligence},
volume = {171},
number = {5},
pages = {286-310},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000410},
author = {Martin Caminada and Leila Amgoud},
keywords = {Formal argumentation, Nonmonotonic logic, Commonsense reasoning},
abstract = {Argumentation theory has become an important topic in the field of AI. The basic idea is to construct arguments in favor and against a statement, to select the â€œacceptableâ€ ones and, finally, to determine whether the original statement can be accepted or not. Several argumentation systems have been proposed in the literature. Some of them, the so-called rule-based systems, use a particular logical language with strict and defeasible rules. While these systems are useful in different domains (e.g. legal reasoning), they unfortunately lead to very unintuitive results, as is discussed in this paper. In order to avoid such anomalies, in this paper we are interested in defining principles, called rationality postulates, that can be used to judge the quality of a rule-based argumentation system. In particular, we define two important rationality postulates that should be satisfied: the consistency and the closure of the results returned by that system. We then provide a relatively easy way in which these rationality postulates can be warranted for a particular rule-based argumentation system developed within a European project on argumentation.}
}
@article{IWASAKI201549,
title = {Finding core for coalition structure utilizing dual solution},
journal = {Artificial Intelligence},
volume = {222},
pages = {49-66},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000119},
author = {Atsushi Iwasaki and Suguru Ueda and Naoyuki Hashimoto and Makoto Yokoo},
keywords = {Game theory, Cooperative games, Core, Coalition structure},
abstract = {When forming the grand coalition is not possible or optimal, agents need to create a coalition structure. The idea of the core can be extended to such a case. In this paper, we propose an innovative exact algorithm called CoreD to check core-non-emptiness for coalition structures. A more straightforward exact algorithm based on existing techniques, which we call CoreP, first obtains the value of optimal coalition structure by solving an integer programming problem. Then, it checks whether that value can be divided without making a blocking (dissatisfied) coalition. In contrast, CoreD first finds a minimal value of the optimal coalition structure so that there exists no blocking coalition. Next, it checks whether the optimal value equals the minimal value We empirically show that when the core is empty, CoreD is by far superior to CoreP. Also, to find a second-best payoff vector when the core is empty, we propose a new solution concept called the weak Îµ-core+, which can utilize the approximate value of the optimal coalition structure. Based on the idea of CoreD, we further develop an algorithm for checking the non-emptiness of the weak Îµ-core+.}
}
@article{KOLOBOV201219,
title = {Discovering hidden structure in factored MDPs},
journal = {Artificial Intelligence},
volume = {189},
pages = {19-47},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000598},
author = {Andrey Kolobov and  Mausam and Daniel S. Weld},
keywords = {Markov Decision Process, MDP, Planning under uncertainty, Generalization, Abstraction, Basis function, Nogood, Heuristic, Dead end},
abstract = {Markov Decision Processes (MDPs) describe a wide variety of planning scenarios ranging from military operations planning to controlling a Mars rover. However, todayÊ¼s solution techniques scale poorly, limiting MDPsÊ¼ practical applicability. In this work, we propose algorithms that automatically discover and exploit the hidden structure of factored MDPs. Doing so helps solve MDPs faster and with less memory than state-of-the-art techniques. Our algorithms discover two complementary state abstractions â€” basis functions and nogoods. A basis function is a conjunction of literals; if the conjunction holds true in a state, this guarantees the existence of at least one trajectory to the goal. Conversely, a nogood is a conjunction whose presence implies the non-existence of any such trajectory, meaning the state is a dead end. We compute basis functions by regressing goal descriptions through a determinized version of the MDP. Nogoods are constructed with a novel machine learning algorithm that uses basis functions as training data. Our state abstractions can be leveraged in several ways. We describe three diverse approaches â€” GOTH, a heuristic function for use in heuristic search algorithms such as RTDP; ReTrASE, an MDP solver that performs modified Bellman backups on basis functions instead of states; and SixthSense, a method to quickly detect dead-end states. In essence, our work integrates ideas from deterministic planning and basis function-based approximation, leading to methods that outperform existing approaches by a wide margin.}
}
@article{VANDITMARSCH2012133,
title = {Local properties in modal logic},
journal = {Artificial Intelligence},
volume = {187-188},
pages = {133-155},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S000437021200046X},
author = {Hans {van Ditmarsch} and Wiebe {van der Hoek} and Barteld Kooi},
keywords = {Knowledge representation, Modal logic, Correspondence, Canonicity, Local properties, Epistemic logic},
abstract = {In modal logic, when adding a syntactic property to an axiomatisation, this property will semantically become true in all models, in all situations, under all circumstances. For instance, adding a property like Kapâ†’Kbp (agent b knows at least what agent a knows) to an axiomatisation of some epistemic logic has as an effect that such a property becomes globally true, i.e., it will hold in all states, at all time points (in a temporal setting), after every action (in a dynamic setting) and after any communication (in an update setting), and every agent will know that it holds, it will even be common knowledge. We propose a way to express that a property like the above only needs to hold locally: it may hold in the actual state, but not in all states, and not all agents may know that it holds. We achieve this by adding relational atoms to the language that represent (implicitly) quantification over all formulas, as in âˆ€p(Kapâ†’Kbp). We show how this can be done for a rich class of modal logics and a variety of syntactic properties. We then study the epistemic logic enriched with the syntactic property â€˜knowing at least as much asâ€™ in more detail. We show that the enriched language is not preserved under bisimulations. We also demonstrate that adding public announcements to this enriched epistemic logic makes it more expressive, which is for instance not true for the â€˜standardâ€™ epistemic logic S5.}
}
@article{DELGRANDE20122223,
title = {Parallel belief revision: Revising by sets of formulas},
journal = {Artificial Intelligence},
volume = {176},
number = {1},
pages = {2223-2245},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211001111},
author = {James Delgrande and Yi Jin},
keywords = {Knowledge representation and reasoning, Belief change, Iterated belief revision, Epistemic states},
abstract = {The area of belief revision studies how a rational agent may incorporate new information about a domain into its belief corpus. An agent is characterised by a belief state K, and receives a new item of information Î± which is to be included among its set of beliefs. Revision then is a function from a belief state and a formula to a new belief state. We propose here a more general framework for belief revision, in which revision is a function from a belief state and a finite set of formulas to a new belief state. In particular, we distinguish revision by the set {Î±,Î²} from the set {Î±âˆ§Î²}. This seemingly innocuous change has significant ramifications with respect to iterated belief revision. A problem in approaches to iterated belief revision is that, after first revising by a formula and then by a formula that is inconsistent with the first formula, all information in the original formula is lost. This problem is avoided here in that, in revising by a set of formulas S, the resulting belief state contains not just the information that members of S are believed to be true, but also the counterfactual supposition that if some members of S were later believed to be false, then the remaining members would nonetheless still be believed to be true. Thus if some members of S were in fact later believed to be false, then the other elements of S would still be believed to be true. Hence, we provide a more nuanced approach to belief revision. The general approach, which we call parallel belief revision, is independent of extant approaches to iterated revision. We present first a basic approach to parallel belief revision. Following this we combine the basic approach with an approach due to Jin and Thielscher for iterated revision. Postulates and semantic conditions characterising these approaches are given, and representation results provided. We conclude with a discussion of the possible ramifications of this approach in belief revision in general.}
}
@article{PEREIRA2013240,
title = {Using Wikipedia to learn semantic feature representations of concrete concepts in neuroimaging experiments},
journal = {Artificial Intelligence},
volume = {194},
pages = {240-252},
year = {2013},
note = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000756},
author = {Francisco Pereira and Matthew Botvinick and Greg Detre},
keywords = {Wikipedia, Matrix factorization, fMRI, Semantic features},
abstract = {In this paper we show that a corpus of a few thousand Wikipedia articles about concrete or visualizable concepts can be used to produce a low-dimensional semantic feature representation of those concepts. The purpose of such a representation is to serve as a model of the mental context of a subject during functional magnetic resonance imaging (fMRI) experiments. A recent study by Mitchell et al. (2008) [19] showed that it was possible to predict fMRI data acquired while subjects thought about a concrete concept, given a representation of those concepts in terms of semantic features obtained with human supervision. We use topic models on our corpus to learn semantic features from text in an unsupervised manner, and show that these features can outperform those in Mitchell et al. (2008) [19] in demanding 12-way and 60-way classification tasks. We also show that these features can be used to uncover similarity relations in brain activation for different concepts which parallel those relations in behavioral data from human subjects.}
}
@article{GAO20091343,
title = {Data reductions, fixed parameter tractability, and random weighted d-CNF satisfiability},
journal = {Artificial Intelligence},
volume = {173},
number = {14},
pages = {1343-1366},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000691},
author = {Yong Gao},
keywords = {Weighted CNF satisfiability, Fixed parameter tractability, Data reduction, Random instances, Phase transitions, Probabilistic analysis, Resolution complexity},
abstract = {Data reduction is a key technique in the study of fixed parameter algorithms. In the AI literature, pruning techniques based on simple and efficient-to-implement reduction rules also play a crucial role in the success of many industrial-strength solvers. Understanding the effectiveness and the applicability of data reduction as a technique for designing heuristics for intractable problems has been one of the main motivations in studying the phase transition of randomly-generated instances of NP-complete problems. In this paper, we take the initiative to study the power of data reductions in the context of random instances of a generic intractable parameterized problem, the weighted d-CNF satisfiability problem. We propose a non-trivial random model for the problem and study the probabilistic behavior of the random instances from the model. We design an algorithm based on data reduction and other algorithmic techniques and prove that the algorithm solves the random instances with high probability and in fixed-parameter polynomial time O(dknm) where n is the number of variables, m is the number of clauses, and k is the fixed parameter. We establish the exact threshold of the phase transition of the solution probability and show that in some region of the problem space, unsatisfiable random instances of the problem have parametric resolution proof of fixed-parameter polynomial size. Also discussed is a more general random model and the generalization of the results to the model.}
}
@article{PARODI199847,
title = {Empirically-derived estimates of the complexity of labeling line drawings of polyhedral scenes},
journal = {Artificial Intelligence},
volume = {105},
number = {1},
pages = {47-75},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00077-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000770},
author = {P. Parodi and R. Lancewicki and A. Vijh and J.K. Tsotsos},
keywords = {Line drawings, Computational complexity, Random polyhedral scenes},
abstract = {Several results have been obtained in the past about the complexity of understanding line drawings of polyhedral scenes. Kirousis and Papadimitriou (1988) have shown that the problem of labeling line drawings of trihedral scenes is NP-complete. The human brain, however, seems to grasp at a glance the 3D structure associated with a line drawing. A possible explanation of this discrepancy, offered by Kirousis and Papadimitriou themselves, is that the worst-case complexity does not reflect the real difficulty of labeling line drawings, which might be far less in the average or in â€œtypicalâ€ cases. However, no statistical analysis has ever been carried out to test this conjecture. The core of this paper is an algorithm for the generation of random instances of polyhedral scenes. Random instances of line drawings are then obtained as perspective projections of these scenes, and can be used as an input to standard labeling algorithms so as to derive experimental estimates of the complexity of these algorithms. The results indicate that the median-case complexity is linear in the number of junctions. This substantiates the conjecture that â€œtypicalâ€ instances of line drawings are easy to label, and may help explain the ease by which the brain is able to solve the problem.}
}
@article{BYLANDER1996241,
title = {A probabilistic analysis of prepositional STRIPS planning},
journal = {Artificial Intelligence},
volume = {81},
number = {1},
pages = {241-271},
year = {1996},
note = {Frontiers in Problem Solving: Phase Transitions and Complexity},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00055-0},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000550},
author = {Tom Bylander},
keywords = {Planning, STRIPS, Average-case analysis, PSPACE-complete},
abstract = {I present a probabilistic analysis of prepositional STRIPS planning. The analysis considers two assumptions. One is that each possible precondition (likewise postcondition) of an operator is selected independently of other pre- and postconditions. The other is that each operator has a fixed number of preconditions (likewise postconditions). Under both assumptions, I derive bounds for when it is highly likely that a planning instance can be efficiently solved, either by finding a plan or proving that no plan exists. Roughly, if planning instances under either assumption have n propositions (ground atoms) and g goals, and the number of operators is less than an O(n In g) bound, then a simple, efficient algorithm can prove that no plan exists for most instances. If the number of operators is greater than an Î©(n In g) bound, then a simple, efficient algorithm can find a plan for most instances. The two bounds differ by a factor that is exponential in the number of pre- and postconditions. A similar result holds for plan modification, i.e., solving a planning instance that is close to another planning instance with a known plan. Thus it appears that prepositional STRIPS planning, a PSPACE-complete problem, exhibits a easy-hard-easy pattern as the number of available operators increases with a narrow range of hard problems. An empirical study demonstrates this pattern for particular parameter values. Because prepositional STRIPS planning is PSPACE-complete, this extends previous phase transition analyses, which have focused on NP-complete problems. Also, the analysis shows that surprisingly simple algorithms can solve a large subset of the planning problem.}
}
@article{FERRARIS2011236,
title = {Stable models and circumscription},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {236-263},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000470},
author = {Paolo Ferraris and Joohyung Lee and Vladimir Lifschitz},
keywords = {Answer set programming, Circumscription, Nonmonotonic reasoning, Program completion, Stable models},
abstract = {The concept of a stable model provided a declarative semantics for Prolog programs with negation as failure and became a starting point for the development of answer set programming. In this paper we propose a new definition of that concept, which covers many constructs used in answer set programming and, unlike the original definition, refers neither to grounding nor to fixpoints. It is based on a syntactic transformation similar to parallel circumscription.}
}
@article{BASTIAANSE201696,
title = {Making the right exceptions},
journal = {Artificial Intelligence},
volume = {238},
pages = {96-118},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300637},
author = {Harald Bastiaanse and Frank Veltman},
keywords = {Circumscription, Defaults, Nonmonotonic logic, Inheritance networks},
abstract = {This paper is about the logical properties of sentences of the form S's are normally P, and starts from the idea that any logical theory for such sentences should meet the following simple requirement: If the only available information about some object x is that x has property S, it must be valid to infer by default that x has all the properties P that objects with property S normally have. We investigate how this requirement can be met by theories developed within the framework of circumscription, and specify a constraint â€“ the exemption principle â€“ that must be satisfied to do so. This principle determines in cases of conflicting default rules which objects are exempted from which rules, and, as such, is the main source for the capricious logical behavior of the sentences we are interested in. To facilitate comparison (and implementation) we supply an algorithm for inheritance networks and prove that arguments that can be expressed in both frameworks are valid on the circumscriptive account if and only if the inheritance algorithm has a positive outcome.}
}
@article{AMGOUD2009413,
title = {Using arguments for making and explaining decisions},
journal = {Artificial Intelligence},
volume = {173},
number = {3},
pages = {413-436},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S000437020800194X},
author = {Leila Amgoud and Henri Prade},
keywords = {Decision making, Argumentation},
abstract = {Arguments play two different roles in day life decisions, as well as in the discussion of more crucial issues. Namely, they help to select one or several alternatives, or to explain and justify an already adopted choice. This paper proposes the first general and abstract argument-based framework for decision making. This framework follows two main steps. At the first step, arguments for beliefs and arguments for options are built and evaluated using classical acceptability semantics. At the second step, pairs of options are compared using decision principles. Decision principles are based on the accepted arguments supporting the options. Three classes of decision principles are distinguished: unipolar, bipolar or non-polar principles depending on whether i) only arguments pros or only arguments cons, or ii) both types, or iii) an aggregation of them into a meta-argument are used. The abstract model is then instantiated by expressing formally the mental states (beliefs and preferences) of a decision maker. In the proposed framework, information is given in the form of a stratified set of beliefs. The bipolar nature of preferences is emphasized by making an explicit distinction between prioritized goals to be pursued, and prioritized rejections that are stumbling blocks to be avoided. A typology that identifies four types of argument is proposed. Indeed, each decision is supported by arguments emphasizing its positive consequences in terms of goals certainly satisfied and rejections certainly avoided. A decision can also be attacked by arguments emphasizing its negative consequences in terms of certainly missed goals, or rejections certainly led to by that decision. Finally, this paper articulates the optimistic and pessimistic decision criteria defined in qualitative decision making under uncertainty, in terms of an argumentation process. Similarly, different decision principles identified in multiple criteria decision making are restated in our argumentation-based framework.}
}
@article{ALBRECHT201663,
title = {Belief and truth in hypothesised behaviours},
journal = {Artificial Intelligence},
volume = {235},
pages = {63-94},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300236},
author = {Stefano V. Albrecht and Jacob W. Crandall and Subramanian Ramamoorthy},
keywords = {Autonomous agents, Multiagent systems, Game theory, Type-based method},
abstract = {There is a long history in game theory on the topic of Bayesian or â€œrationalâ€ learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.}
}
@article{ATKINSON20181,
title = {Taking account of the actions of others in value-based reasoning},
journal = {Artificial Intelligence},
volume = {254},
pages = {1-20},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217301078},
author = {Katie Atkinson and Trevor Bench-Capon},
keywords = {Value-based reasoning, Practical reasoning, Expected utility, Argumentation schemes, Ultimatum Game, Prisoner's Dilemma},
abstract = {Practical reasoning, reasoning about what actions should be chosen, is highly dependent both on the individual values of the agent concerned and on what others choose to do. Hitherto, computational models of value-based argumentation for practical reasoning have required assumptions to be made about the beliefs and preferences of other agents. Here we present a new method for taking the actions of others into account that does not require these assumptions: the only beliefs and preferences considered are those of the agent engaged in the reasoning. Our new formalism draws on utility-based approaches and expresses the reasoning in the form of arguments and objections, to enable full integration with value-based practical reasoning. We illustrate our approach by showing how value-based reasoning is modelled in two scenarios used in experimental economics, the Ultimatum Game and the Prisoner's Dilemma, and we present an evaluation of our approach in terms of these experiments. The evaluation demonstrates that our model is able to reproduce computationally the results of ethnographic experiments, serving as an encouraging validation exercise.}
}
@article{ROVEDA2022103771,
title = {Q-Learning-based model predictive variable impedance control for physical human-robot collaboration},
journal = {Artificial Intelligence},
volume = {312},
pages = {103771},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103771},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001114},
author = {Loris Roveda and Andrea Testa and Asad Ali Shahid and Francesco Braghin and Dario Piga},
keywords = {Physical human-robot collaboration, Industry 4.0, Machine learning, Model-based reinforcement learning control, Neural networks, Q-Learning, Stability, Variable impedance control},
abstract = {Physical human-robot collaboration is increasingly required in many contexts (such as industrial and rehabilitation applications). The robot needs to interact with the human to perform the target task while relieving the user from the workload. To do that, the robot should be able to recognize the human's intentions and guarantee safe and adaptive behavior along the intended motion directions. The robot-control strategies with such attributes are particularly demanded in the industrial field, where the operator guides the robot manually to manipulate heavy parts (e.g., while teaching a specific task). With this aim, this work proposes a Q-Learning-based Model Predictive Variable Impedance Control (Q-LMPVIC) to assist the operators in a physical human-robot collaboration (pHRC) tasks. A Cartesian impedance control loop is designed to implement a decoupled compliant robot dynamics. The impedance control parameters (i.e., setpoint and damping parameters) are then optimized online in order to maximize the performance of the pHRC. For this purpose, an ensemble of neural networks is designed to learn the modeling of the human-robot interaction dynamics while capturing the associated uncertainties. The derived modeling is then exploited by the model predictive controller (MPC), enhanced with the stability guarantees by means of Lyapunov constraints. The MPC is solved by making use of a Q-Learning method that, in its online implementation, uses an actor-critic algorithm to approximate the exact solution. Indeed, the Q-learning method provides an accurate and highly efficient solution (in terms of computational time and resources). The proposed approach has been validated through experimental tests, in which a Franka EMIKA panda robot has been used as a test platform. Each user was asked to interact with the robot along the controlled vertical z Cartesian direction. The proposed controller has been compared with a model-based reinforcement learning variable impedance controller (MBRLC) previously developed by some of the authors in order to evaluate the performance. As highlighted in the achieved results, the proposed controller is able to improve the pHRC performance. Additionally, two industrial tasks (a collaborative assembly and a collaborative deposition task) have been demonstrated to prove the applicability of the proposed solution in real industrial scenarios.}
}
@article{DREYFUS20071137,
title = {Why Heideggerian AI failed and how fixing it would require making it more Heideggerian},
journal = {Artificial Intelligence},
volume = {171},
number = {18},
pages = {1137-1160},
year = {2007},
note = {Special Review Issue},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001452},
author = {Hubert L. Dreyfus}
}
@article{DAVIES201420,
title = {Complexity of and algorithms for the manipulation of Borda, Nanson's and Baldwin's voting rules},
journal = {Artificial Intelligence},
volume = {217},
pages = {20-42},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000885},
author = {Jessica Davies and George Katsirelos and Nina Narodytska and Toby Walsh and Lirong Xia},
keywords = {Social choice, Voting methods, Manipulation, Borda voting, Nanson's voting rule, Baldwin's voting rule},
abstract = {We investigate manipulation of the Borda voting rule, as well as two elimination style voting rules, Nanson's and Baldwin's voting rules, which are based on Borda voting. We argue that these rules have a number of desirable computational properties. For unweighted Borda voting, we prove that it is NP-hard for a coalition of two manipulators to compute a manipulation. This resolves a long-standing open problem in the computational complexity of manipulating common voting rules. We prove that manipulation of Baldwin's and Nanson's rules is computationally more difficult than manipulation of Borda, as it is NP-hard for a single manipulator to compute a manipulation. In addition, for Baldwin's and Nanson's rules with weighted votes, we prove that it is NP-hard for a coalition of manipulators to compute a manipulation with a small number of candidates. Because of these NP-hardness results, we compute manipulations using heuristic algorithms that attempt to minimise the number of manipulators. We propose several new heuristic methods. Experiments show that these methods significantly outperform the previously best known heuristic method for the Borda rule. Our results suggest that, whilst computing a manipulation of the Borda rule is NP-hard, computational complexity may provide only a weak barrier against manipulation in practice. In contrast to the Borda rule, our experiments with Baldwin's and Nanson's rules demonstrate that both of them are often more difficult to manipulate in practice. These results suggest that elimination style voting rules deserve further study.}
}
@article{PAPAKONSTANTINOU2011648,
title = {Mechanism design for the truthful elicitation of costly probabilistic estimates in distributed information systems},
journal = {Artificial Intelligence},
volume = {175},
number = {2},
pages = {648-672},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001773},
author = {Athanasios Papakonstantinou and Alex Rogers and Enrico H. Gerding and Nicholas R. Jennings},
keywords = {Multiagent systems, Scoring rules, Auction theory, Mechanism design},
abstract = {This paper reports on the design of a novel two-stage mechanism, based on strictly proper scoring rules, that allows a centre to acquire a costly forecast of a future event (such as a meteorological phenomenon) or a probabilistic estimate of a specific parameter (such as the quality of an expected service), with a specified minimum precision, from one or more agents. In the first stage, the centre elicits the agents' true costs and identifies the agent that can provide an estimate of the specified precision at the lowest cost. Then, in the second stage, the centre uses an appropriately scaled strictly proper scoring rule to incentivise this agent to generate the estimate with the required precision, and to truthfully report it. In particular, this is the first mechanism that can be applied to settings in which the centre has no knowledge about the actual costs involved in the generation an agents' estimates and also has no external means of evaluating the quality and accuracy of the estimates it receives. En route to this mechanism, we first consider a setting in which any single agent can provide an estimate of the required precision, and the centre can evaluate this estimate by comparing it with the outcome which is observed at a later stage. This mechanism is then extended, so that it can be applied in a setting where the agents' different capabilities are reflected in the maximum precision of the estimates that they can provide, potentially requiring the centre to select multiple agents and combine their individual results in order to obtain an estimate of the required precision. For all three mechanisms (the original and the two extensions), we prove their economic properties (i.e. incentive compatibility and individual rationality) and then perform a number of numerical simulations. For the single agent mechanism we compare the quadratic, spherical and logarithmic scoring rules with a parametric family of scoring rules. We show that although the logarithmic scoring rule minimises both the mean and variance of the centre's total payments, using this rule means that an agent may face an unbounded penalty if it provides an estimate of extremely poor quality. We show that this is not the case for the parametric family, and thus, we suggest that the parametric scoring rule is the best candidate in our setting. Furthermore, we show that the â€˜multiple agentâ€™ extension describes a family of possible approaches to select agents in the first stage of our mechanism, and we show empirically and prove analytically that there is one approach that dominates all others. Finally, we compare our mechanism to the peer prediction mechanism introduced by Miller et al. (2007) [29] and show that the centre's total expected payment is the same in both mechanisms (and is equal to total expected payment in the case that the estimates can be compared to the actual outcome), while the variance in these payments is significantly reduced within our mechanism.}
}
@article{COSTEMARQUIS2007730,
title = {On the merging of Dung's argumentation systems},
journal = {Artificial Intelligence},
volume = {171},
number = {10},
pages = {730-753},
year = {2007},
note = {Argumentation in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000719},
author = {Sylvie Coste-Marquis and Caroline Devred and SÃ©bastien Konieczny and Marie-Christine Lagasquie-Schiex and Pierre Marquis},
keywords = {Argumentation frameworks, Argument in agent system},
abstract = {In this paper, the problem of deriving sensible information from a collection of argumentation systems coming from different agents is addressed. The underlying argumentation theory is Dung's one: each argumentation system gives both a set of arguments and the way they interact (i.e., attack or non-attack) according to the corresponding agent. The inadequacy of the simple, yet appealing, method which consists in voting on the agents' selected extensions calls for a new approach. To this purpose, a general framework for merging argumentation systems from Dung's theory of argumentation is presented. The objective is achieved through a three-step process: first, each argumentation system is expanded into a partial system over the set of all arguments considered by the group of agents (reflecting that some agents may easily ignore arguments pointed out by other agents, as well as how such arguments interact with her own ones); then, merging is used on the expanded systems as a way to solve the possible conflicts between them, and a set of argumentation systems which are as close as possible to the whole profile is generated; finally, voting is used on the selected extensions of the resulting systems so as to characterize the acceptable arguments at the group level.}
}
@article{KIKUTI20111346,
title = {Sequential decision making with partially ordered preferences},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1346-1365},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002067},
author = {Daniel Kikuti and Fabio Gagliardi Cozman and Ricardo Shirota Filho},
keywords = {Sequential decision making under uncertainty, Partially ordered preferences, Sets of probability measures, Criteria of choice, Consequentialist and resolute norms, Linear and multilinear programming},
abstract = {This paper presents new insights and novel algorithms for strategy selection in sequential decision making with partially ordered preferences; that is, where some strategies may be incomparable with respect to expected utility. We assume that incomparability amongst strategies is caused by indeterminacy/imprecision in probability values. We investigate six criteria for consequentialist strategy selection: Î“-Maximin, Î“-Maximax, Î“-Maximix, Interval Dominance, Maximality and E-admissibility. We focus on the popular decision tree and influence diagram representations. Algorithms resort to linear/multilinear programming; we describe implementation and experiments.}
}
@article{NGUYEN20121,
title = {Generating diverse plans to handle unknown and partially known user preferences},
journal = {Artificial Intelligence},
volume = {190},
pages = {1-31},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000707},
author = {Tuan Anh Nguyen and Minh Do and Alfonso Emilio Gerevini and Ivan Serina and Biplav Srivastava and Subbarao Kambhampati},
keywords = {Planning, Partial preferences, Diverse plans, Heuristics, Search},
abstract = {Current work in planning with preferences assumes that user preferences are completely specified, and aims to search for a single solution plan to satisfy these. In many real world planning scenarios, however, the user may provide no knowledge or at best partial knowledge of her preferences with respect to a desired plan. In such situations, rather than presenting a single plan as the solution, the planner must instead provide a set of plans containing one or more plans that are similar to the one that the user really prefers. In this paper, we first propose the usage of different measures to capture the quality of such plan sets. These are domain-independent distance measures based on plan elements (such as actions, states, or causal links) if no knowledge of the user preferences is given, or the Integrated Convex Preference (ICP) measure in case incomplete knowledge of such preferences is provided. We then investigate various heuristic approaches to generate sets of plans in accordance with these measures, and present empirical results that demonstrate the promise of our methods.}
}
@article{LIBERATORE20081317,
title = {Redundancy in logic III: Non-monotonic reasoning},
journal = {Artificial Intelligence},
volume = {172},
number = {11},
pages = {1317-1359},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000258},
author = {Paolo Liberatore},
keywords = {Logical redundancy, Non-monotonic reasoning, Computational complexity, Circumscription, Default logic},
abstract = {Results about the redundancy of certain versions of circumscription and default logic are presented. In particular, propositional circumscription where all variables are minimized and skeptical default logics are considered. This restricted version of circumscription is shown to have the unitary redundancy property: a CNF formula is redundant (it is equivalent to one of its proper subsets) if and only if it contains a redundant clause (it is equivalent to itself minus one clause); default logic does not have this property in general. We also give the complexity of checking redundancy in the considered formalisms.}
}
@article{LIN1998201,
title = {What robots can do: robot programs and effective achievability},
journal = {Artificial Intelligence},
volume = {101},
number = {1},
pages = {201-226},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00041-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000411},
author = {Fangzhen Lin and Hector J. Levesque},
keywords = {Robotics, Cognitive robotics, Robot programs, Achievability, Effective achievability, Abilities, Theories of actions, Situation calculus},
abstract = {In this paper, we propose a definition of goal achievability: given a basic action theory describing an initial state of the world and some primitive actions available to a robot, including some actions which return binary sensing information, what goals can be achieved by the robot? The main technical result of the paper is a proof that a simple robot programming language is universal, in that any effectively achievable goal can be achieved by getting the robot to execute one of the robot programs. The significance of this result is at least twofold. First, it is in many ways similar to the equivalence theorem between Turing machines and recursive functions, but applied to robots whose actions are specified by an action theory. Secondly, it provides formal justifications for using the simple robot programming language as a foundation for our work on robotics.}
}
@article{AZIZ20181,
title = {Fixing balanced knockout and double elimination tournaments},
journal = {Artificial Intelligence},
volume = {262},
pages = {1-14},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218302583},
author = {Haris Aziz and Serge Gaspers and Simon Mackenzie and Nicholas Mattei and Paul Stursberg and Toby Walsh},
keywords = {Knockout tournaments, Seedings of a tournament, Computational complexity},
abstract = {Balanced knockout tournaments are one of the most common formats for sports competitions, and are also used in elections and decision-making. We consider the computational problem of finding the optimal draw for a particular player in such a tournament. The problem has generated considerable research within AI in recent years. We prove that checking whether there exists a draw in which a player wins is NP-complete, thereby settling an outstanding open problem. Our main result has a number of interesting implications on related counting and approximation problems. We present a memoization-based algorithm for the problem that is faster than previous approaches. Moreover, we highlight two natural cases that can be solved in polynomial time. All of our results also hold for the more general problem of counting the number of draws in which a given player is the winner. Finally, we show that our main NP-completeness result extends to a variant of balanced knockout tournaments called double-elimination tournaments.}
}
@article{BOGAERTS2021103550,
title = {A framework for step-wise explaining how to solve constraint satisfaction problems},
journal = {Artificial Intelligence},
volume = {300},
pages = {103550},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103550},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221001016},
author = {Bart Bogaerts and Emilio Gamba and Tias Guns},
keywords = {Artificial intelligence, Constraint solving, Explanation},
abstract = {We explore the problem of step-wise explaining how to solve constraint satisfaction problems, with a use case on logic grid puzzles. More specifically, we study the problem of explaining the inference steps that one can take during propagation, in a way that is easy to interpret for a person. Thereby, we aim to give the constraint solver explainable agency, which can help in building trust in the solver by being able to understand and even learn from the explanations. The main challenge is that of finding a sequence of simple explanations, where each explanation should aim to be as cognitively easy as possible for a human to verify and understand. This contrasts with the arbitrary combination of facts and constraints that the solver may use when propagating. We propose the use of a cost function to quantify how simple an individual explanation of an inference step is, and identify the explanation-production problem of finding the best sequence of explanations of a CSP. Our approach is agnostic of the underlying constraint propagation mechanisms, and can provide explanations even for inference steps resulting from combinations of constraints. In case multiple constraints are involved, we also develop a mechanism that allows to break the most difficult steps up and thus gives the user the ability to zoom in on specific parts of the explanation. Our proposed algorithm iteratively constructs the explanation sequence by using an optimistic estimate of the cost function to guide the search for the best explanation at each step. Our experiments on logic grid puzzles show the feasibility of the approach in terms of the quality of the individual explanations and the resulting explanation sequences obtained.}
}
@article{HAMID20091221,
title = {A novel sequence representation for unsupervised analysis of human activities},
journal = {Artificial Intelligence},
volume = {173},
number = {14},
pages = {1221-1244},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000629},
author = {Raffay Hamid and Siddhartha Maddi and Amos Johnson and Aaron Bobick and Irfan Essa and Charles Isbell},
keywords = {Temporal reasoning, Scene analysis, Computer vision},
abstract = {Formalizing computational models for everyday human activities remains an open challenge. Many previous approaches towards this end assume prior knowledge about the structure of activities, using which explicitly defined models are learned in a completely supervised manner. For a majority of everyday environments however, the structure of the in situ activities is generally not known a priori. In this paper we investigate knowledge representations and manipulation techniques that facilitate learning of human activities in a minimally supervised manner. The key contribution of this work is the idea that global structural information of human activities can be encoded using a subset of their local event subsequences, and that this encoding is sufficient for activity-class discovery and classification. In particular, we investigate modeling activity sequences in terms of their constituent subsequences that we call event n-grams. Exploiting this representation, we propose a computational framework to automatically discover the various activity-classes taking place in an environment. We model these activity-classes as maximally similar activity-cliques in a completely connected graph of activities, and describe how to discover them efficiently. Moreover, we propose methods for finding characterizations of these discovered classes from a holistic as well as a by-parts perspective. Using such characterizations, we present a method to classify a new activity to one of the discovered activity-classes, and to automatically detect whether it is anomalous with respect to the general characteristics of its membership class. Our results show the efficacy of our approach in a variety of everyday environments.}
}
@article{HILD20081195,
title = {The measurement of ranks and the laws of iterated contraction},
journal = {Artificial Intelligence},
volume = {172},
number = {10},
pages = {1195-1218},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000362},
author = {Matthias Hild and Wolfgang Spohn},
keywords = {Ranking theory, Belief revision theory, Difference measurement, Contraction, Iterated contraction},
abstract = {Ranking theory delivers an account of iterated contraction; each ranking function induces a specific iterated contraction behavior. The paper shows how to reconstruct a ranking function from its iterated contraction behavior uniquely up to multiplicative constant and thus how to measure ranks on a ratio scale. Thereby, it also shows how to completely axiomatize that behavior. The complete set of laws of iterated contraction it specifies amend the laws hitherto discussed in the literature.}
}
@article{MIRANDA2009104,
title = {Coherence graphs},
journal = {Artificial Intelligence},
volume = {173},
number = {1},
pages = {104-144},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001173},
author = {Enrique Miranda and Marco Zaffalon},
keywords = {Walley's strong and weak coherence, Coherent lower previsions, Graphical models, Probabilistic logic, Satisfiability},
abstract = {We study the consistency of a number of probability distributions, which are allowed to be imprecise. To make the treatment as general as possible, we represent those probabilistic assessments as a collection of conditional lower previsions. The problem then becomes proving Walley's (strong) coherence of the assessments. In order to maintain generality in the analysis, we assume to be given nearly no information about the numbers that make up the lower previsions in the collection. Under this condition, we investigate the extent to which the above global task can be decomposed into simpler and more local ones. This is done by introducing a graphical representation of the conditional lower previsions that we call the coherence graph: we show that the coherence graph allows one to isolate some subsets of the collection whose coherence is sufficient for the coherence of all the assessments; and we provide a polynomial-time algorithm that finds the subsets efficiently. We show some of the implications of our results by focusing on three models and problems: Bayesian and credal networks, of which we prove coherence; the compatibility problem, for which we provide an optimal graphical decomposition; probabilistic satisfiability, of which we show that some intractable instances can instead be solved efficiently by exploiting coherence graphs.}
}
@article{BANDYAPADHYAY2023103948,
title = {How to find a good explanation for clustering?},
journal = {Artificial Intelligence},
volume = {322},
pages = {103948},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103948},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000942},
author = {Sayan Bandyapadhyay and Fedor V. Fomin and Petr A. Golovach and William Lochet and Nidhi Purohit and Kirill Simonov},
keywords = {Explainable clustering, Clustering with outliers, Multivariate analysis},
abstract = {k-means and k-median clustering are powerful unsupervised machine learning techniques. However, due to complicated dependencies on all the features, it is challenging to interpret the resulting cluster assignments. Moshkovitz, Dasgupta, Rashtchian, and Frost proposed an elegant model of explainable k-means and k-median clustering in ICML 2020. In this model, a decision tree with k leaves provides a straightforward characterization of the data set into clusters. We study two natural algorithmic questions about explainable clustering. (1) For a given clustering, how to find the â€œbest explanationâ€ by using a decision tree with k leaves? (2) For a given set of points, how to find a decision tree with k leaves minimizing the k-means/median objective of the resulting explainable clustering? To address the first question, we introduce a new model of explainable clustering. Our model, inspired by the notion of outliers in robust statistics, is the following. We are seeking a small number of points (outliers) whose removal makes the existing clustering well-explainable. For addressing the second question, we initiate the study of the model of Moshkovitz et al. from the perspective of multivariate complexity. Our rigorous algorithmic analysis sheds some light on the influence of parameters like the input size, dimension of the data, the number of outliers, the number of clusters, and the approximation ratio, on the computational complexity of explainable clustering.}
}
@article{MITCHELL1996111,
title = {Some pitfalls for experimenters with random SAT},
journal = {Artificial Intelligence},
volume = {81},
number = {1},
pages = {111-125},
year = {1996},
note = {Frontiers in Problem Solving: Phase Transitions and Complexity},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00049-6},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000496},
author = {David G. Mitchell and Hector J. Levesque},
keywords = {Satisfiability, Random problems, Phase transitions, Experimental design},
abstract = {We consider the use of random CNF formulas in evaluating the performance of SAT testing algorithms, and in particular the role that the phase transition phenomenon plays in this use. Examples from the literature illustrate the importance of understanding the properties of formula distributions prior to designing an experiment. We expect this to be of increasing importance in the field.}
}
@article{BOGAERTS2018167,
title = {Safe inductions and their applications in knowledge representation},
journal = {Artificial Intelligence},
volume = {259},
pages = {167-185},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S000437021830122X},
author = {Bart Bogaerts and Joost Vennekens and Marc Denecker},
keywords = {Approximation fixpoint theory, Lattice operator, Inductive definitions, Induction process, Construction, Well-founded semantics, Groundedness, Logic programming, Autoepistemic logic, Abstract argumentation},
abstract = {In many knowledge representation formalisms, a constructive semantics is defined based on sequential applications of rules or of a semantic operator. These constructions often share the property that rule applications must be delayed until it is safe to do so: until it is known that the condition that triggers the rule will continue to hold. This intuition occurs for instance in the well-founded semantics of logic programs and in autoepistemic logic. In this paper, we formally define the safety criterion algebraically. We study properties of so-called safe inductions and apply our theory to logic programming and autoepistemic logic. For the latter, we show that safe inductions manage to capture the intended meaning of a class of theories on which all classical constructive semantics fail.}
}
@article{JELASITY19981,
title = {GAS, a concept on modeling species in genetic algorithms},
journal = {Artificial Intelligence},
volume = {99},
number = {1},
pages = {1-19},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00071-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000714},
author = {MÃ¡rk Jelasity and JÃ³zsef Dombi},
keywords = {Multimodal optimalization, Niching, Niche radius problem, Genetic algorithms},
abstract = {This paper introduces a niching technique called GAS (S stands for species) which dynamically creates a subpopulation structure (taxonomic chart) using a radius function instead of a single radius, and a â€œcoolingâ€ method similar to simulated annealing. GAS offers a solution to the niche radius problem with the help of these techniques. A method based on the speed of species is presented for determining the radius function. Speed functions are given for both real and binary domains. We also discuss the sphere packing problem on binary domains using some tools of coding theory to make it possible to evaluate the output of the system. Finally two problems are examined empirically. The first is a difficult test function with unevenly spread local optima. The second is an NP-complete combinatorial optimization task, where a comparison is presented to the traditional genetic algorithm.}
}
@article{DUNG1995321,
title = {On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games},
journal = {Artificial Intelligence},
volume = {77},
number = {2},
pages = {321-357},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00041-X},
url = {https://www.sciencedirect.com/science/article/pii/000437029400041X},
author = {Phan Minh Dung},
keywords = {Argumentation, Nonmonotonic reasoning, Logic programming, -person games, The stable marriage problem},
abstract = {The purpose of this paper is to study the fundamental mechanism, humans use in argumentation, and to explore ways to implement this mechanism on computers. We do so by first developing a theory for argumentation whose central notion is the acceptability of arguments. Then we argue for the â€œcorrectnessâ€ or â€œappropriatenessâ€ of our theory with two strong arguments. The first one shows that most of the major approaches to nonmonotonic reasoning in AI and logic programming are special forms of our theory of argumentation. The second argument illustrates how our theory can be used to investigate the logical structure of many practical problems. This argument is based on a result showing that our theory captures naturally the solutions of the theory of n-person games and of the well-known stable marriage problem. By showing that argumentation can be viewed as a special form of logic programming with negation as failure, we introduce a general logic-programming-based method for generating meta-interpreters for argumentation systems, a method very much similar to the compiler-compiler idea in conventional programming.}
}
@article{BURGHARDT20051,
title = {E-generalization using grammars},
journal = {Artificial Intelligence},
volume = {165},
number = {1},
pages = {1-35},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000202},
author = {Jochen Burghardt},
keywords = {Equational theory, Generalization, Inductive logic programming},
abstract = {We extend the notion of anti-unification to cover equational theories and present a method based on regular tree grammars to compute a finite representation of E-generalization sets. We present a framework to combine Inductive Logic Programming and E-generalization that includes an extension of Plotkin's lgg theorem to the equational case. We demonstrate the potential power of E-generalization by three example applications: computation of suggestions for auxiliary lemmas in equational inductive proofs, computation of construction laws for given term sequences, and learning of screen editor command sequences.}
}
@article{PHAM20081752,
title = {Modelling and solving temporal reasoning as propositional satisfiability},
journal = {Artificial Intelligence},
volume = {172},
number = {15},
pages = {1752-1782},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000805},
author = {Duc Nghia Pham and John Thornton and Abdul Sattar},
keywords = {Temporal reasoning, Interval Algebra, Satisfiability, Satisfiability modulo theories, DPLL, Search},
abstract = {Representing and reasoning about time dependent information is a key research issue in many areas of computer science and artificial intelligence. One of the best known and widely used formalisms for representing interval-based qualitative temporal information is Allen's interval algebra (IA). The fundamental reasoning task in IA is to find a scenario that is consistent with the given information. This problem is in general NP-complete. In this paper, we investigate how an interval-based representation, or IA network, can be encoded into a propositional formula of Boolean variables and/or predicates in decidable theories. Our task is to discover whether satisfying such a formula can be more efficient than finding a consistent scenario for the original problem. There are two basic approaches to modelling an IA network: one represents the relations between intervals as variables and the other represents the end-points of each interval as variables. By combining these two approaches with three different Boolean satisfiability (SAT) encoding schemes, we produced six encoding schemes for converting IA to SAT. In addition, we also showed how IA networks can be formulated into satisfiability modulo theories (SMT) formulae based on the quantifier-free integer difference logic (QF-IDL). These encodings were empirically studied using randomly generated IA problems of sizes ranging from 20 to 100 nodes. A general conclusion we draw from these experimental results is that encoding IA into SAT produces better results than existing approaches. More specifically, we show that the new point-based 1-D support SAT encoding of IA produces consistently better results than the other alternatives considered. In comparison with the six different SAT encodings, the SMT encoding came fourth after the point-based and interval-based 1-D support schemes and the point-based direct scheme. Further, we observe that the phase transition region maps directly from the IA encoding to each SAT or SMT encoding, but, surprisingly, the location of the hard region varies according to the encoding scheme. Our results also show a fixed performance ranking order over the various encoding schemes.}
}
@article{BONET2007606,
title = {Resolution for Max-SAT},
journal = {Artificial Intelligence},
volume = {171},
number = {8},
pages = {606-618},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000422},
author = {MarÃ­a Luisa Bonet and Jordi Levy and Felip ManyÃ },
keywords = {Satisfiability, Resolution, Completeness, Saturation, Max-SAT, Weighted Max-SAT},
abstract = {Max-SAT is the problem of finding an assignment minimizing the number of unsatisfied clauses in a CNF formula. We propose a resolution-like calculus for Max-SAT and prove its soundness and completeness. We also prove the completeness of some refinements of this calculus. From the completeness proof we derive an exact algorithm for Max-SAT and a time upper bound. We also define a weighted Max-SAT resolution-like rule, and show how to adapt the soundness and completeness proofs of the Max-SAT rule to the weighted Max-SAT rule. Finally, we give several particular Max-SAT problems that require an exponential number of steps of our Max-SAT rule to obtain the minimal number of unsatisfied clauses of the combinatorial principle. These results are based on the corresponding resolution lower bounds for those particular problems.}
}
@article{STERN201939,
title = {Probably bounded suboptimal heuristic search},
journal = {Artificial Intelligence},
volume = {267},
pages = {39-57},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218306015},
author = {Roni Stern and Gal Dreiman and Richard Valenzano},
keywords = {Artificial intelligence, Heuristic search},
abstract = {Finding an optimal solution to a search problem is often desirable, but can be too difficult in many cases. A common approach in such cases is to try to find a solution whose suboptimality is bounded, where a parameter Ïµ defines how far from optimal a solution can be while still being acceptable. A scarcely studied alternative is to try to find a solution that is probably optimal, where a parameter Î´ defines the confidence required in the solution's optimality. This paper explores this option and introduces the concept of a probably bounded-suboptimal search (pBS search) algorithm. Such a search algorithm accepts two parameters, Ïµ and Î´, and outputs a solution that with probability at least 1âˆ’Î´ costs at most 1+Ïµ times the optimal solution. A general algorithmic framework for pBS search algorithms is proposed. Several instances of this framework are described and analyzed theoretically and experimentally on a range of search domains. Results show that pBS search algorithms are often faster than a state-of-the-art bounded-suboptimal search algorithm. This shows in practice that finding solutions that satisfy a given suboptimality bound with high probability can be done faster than finding solutions that satisfy the same suboptimality bound with certainty.}
}
@article{KERNISBERNER1998169,
title = {Characterizing the principle of minimum cross-entropy within a conditional-logical framework},
journal = {Artificial Intelligence},
volume = {98},
number = {1},
pages = {169-208},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00068-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000684},
author = {Gabriele Kern-Isberner},
keywords = {Probabilistic reasoning, Minimum cross-entropy, Conditionals, Knowledge representation, Nonmonotonic reasoning, Expert systems},
abstract = {The principle of minimum cross-entropy (ME-principle) is often used as an elegant and powerful tool to build up complete probability distributions when only partial knowledge is available. The inputs it may be applied to are a prior distribution P and some new information R, and it yields as a result the one distribution Pâˆ— that satisfies R and is closest to P in an information-theoretic sense. More generally, it provides a â€œbestâ€ solution to the problem â€œHow to adjust P to R?â€ In this paper, we show how probabilistic conditionals allow a new and constructive approach to this important principle. Though popular and widely used for knowledge representation, conditionals quantified by probabilities are not easily dealt with. We develop four principles that describe their handling in a reasonable and consistent way, taking into consideration the conditional-logical as well as the numerical and probabilistic aspects. Finally, the ME-principle turns out to be the only method for adjusting a prior distribution to new conditional information that obeys all these principles. Thus a characterization of the ME-principle within a conditional-logical framework is achieved, and its implicit logical mechanisms are revealed clearly.}
}
@article{BUERMANN2022103769,
title = {Multi-robot adversarial patrolling strategies via lattice paths},
journal = {Artificial Intelligence},
volume = {311},
pages = {103769},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103769},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001096},
author = {Jan Buermann and Jie Zhang},
keywords = {Multi-robot systems, Robots in adversarial settings},
abstract = {In full-knowledge multi-robot adversarial patrolling, a group of robots has to detect an adversary who knows the robots' strategy. The adversary can easily take advantage of any deterministic patrolling strategy, which necessitates the employment of a randomised strategy. While the Markov decision process has been the dominant methodology in computing the penetration detection probabilities on polylines, we apply enumerative combinatorics to characterise the penetration detection probabilities for four penetration configurations. It allows us to provide the closed formulae of these probabilities and facilitates characterising optimal random defence strategies. Comparing to iteratively updating the Markov transition matrices, we empirically show that our method reduces the runtime by up to several hours. This allows us extensive simulations on the two dominant robot movement types for patrolling a perimeter showing that a movement with direction is up to 0.4 more likely to detect an adversary. Therefore, our approach greatly benefits the theoretical and empirical analysis of optimal patrolling strategies with extendability to more complicated attacks and other structured environments.}
}
@article{WHITEHEAD1995271,
title = {Reinforcement learning of non-Markov decision processes},
journal = {Artificial Intelligence},
volume = {73},
number = {1},
pages = {271-306},
year = {1995},
note = {Computational Research on Interaction and Agency, Part 2},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00012-P},
url = {https://www.sciencedirect.com/science/article/pii/000437029400012P},
author = {Steven D. Whitehead and Long-Ji Lin},
abstract = {Techniques based on reinforcement learning (RL) have been used to build systems that learn to perform nontrivial sequential decision tasks. To date, most of this work has focused on learning tasks that can be described as Markov decision processes. While this formalism is useful for modeling a wide range of control problems, there are important tasks that are inherently non-Markov. We refer to these as hidden state tasks since they arise when information relevant to identifying the state of the environment is hidden (or missing) from the agent's immediate sensation. Two important types of control problems that resist Markov modeling are those in which (1) the system has a high degree of control over the information collected by its sensors (e.g., as in active vision), or (2) the system has a limited set of sensors that do not always provide adequate information about the current state of the environment. Existing RL algorithms perform unreliably on hidden state tasks. This article examines two general approaches to extending reinforcement learning to hidden state tasks. The Consistent Representation (CR) Method unifies recent approaches such as the Lion algorithm, the G-algorithm, and CS-QL. The method is useful for learning tasks that require the agent to control its sensory inputs. However, it assumes that, by appropriate control of perception, the external states can be identified at each point in time from the immediate sensory inputs. A second, more general set of algorithms in which the agent maintains internal state over time is also considered. These stored-state algorithms, though quite different in detail, share the common feature that each derives its internal representation by combining immediate sensory inputs with internal state which is maintained over time. The relative merits of these methods are considered and conditions for their useful application are discussed.}
}
@article{ARIELI199897,
title = {The value of the four values},
journal = {Artificial Intelligence},
volume = {102},
number = {1},
pages = {97-141},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00032-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000320},
author = {Ofer Arieli and Arnon Avron},
keywords = {Bilattices, Paraconsistency, Multiple-valued systems, Preferential logics, Reasoning},
abstract = {In his well-known paper â€œHow computer should thinkâ€ Belnap (1977) argues that four-valued semantics is a very suitable setting for computerized reasoning. In this paper we vindicate this thesis by showing that the logical role that the four-valued structure has among Ginsberg's bilattices is similar to the role that the two-valued algebra has among Boolean algebras. Specifically, we provide several theorems that show that the most useful bilattice-valued logics can actually be characterized as four-valued inference relations. In addition, we compare the use of three-valued logics with the use of four-valued logics, and show that at least for the task of handling inconsistent or uncertain information, the comparison is in favor of the latter.}
}
@article{GROSZ1996269,
title = {Collaborative plans for complex group action},
journal = {Artificial Intelligence},
volume = {86},
number = {2},
pages = {269-357},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00103-4},
url = {https://www.sciencedirect.com/science/article/pii/0004370295001034},
author = {Barbara J. Grosz and Sarit Kraus},
abstract = {The original formulation of SharedPlans by B. Grosz and C. Sidner (1990) was developed to provide a model of collaborative planning in which it was not necessary for one agent to have intentions-to toward an act of a different agent. Unlike other contemporaneous approaches (J.R. Searle, 1990), this formulation provided for two agents to coordinate their activities without introducing any notion of irreducible joint intentions. However, it only treated activities that directly decomposed into single-agent actions, did not address the need for agents to commit to their joint activity, and did not adequately deal with agents having only partial knowledge of the way in which to perform an action. This paper provides a revised and expanded version of SharedPlans that addresses these shortcomings. It also reformulates Pollack's (1990) definition of individual plans to handle cases in which a single agent has only partial knowledge; this reformulation meshes with the definition of SharedPlans. The new definitions also allow for contracting out certain actions. The formalization that results has the features required by Bratman's (1992) account of shared cooperative activity and is more general than alternative accounts (H. Levesque et al., 1990; E. Sonenberg et al., 1992).}
}
@article{LOMBARDI2010500,
title = {Allocation and scheduling of Conditional Task Graphs},
journal = {Artificial Intelligence},
volume = {174},
number = {7},
pages = {500-529},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000263},
author = {Michele Lombardi and Michela Milano},
keywords = {Constraint Programming, Probabilistic reasoning, Scenarios, Conditional Task Graphs, Conditional constraints, Optimization},
abstract = {We propose an original, complete and efficient approach to the allocation and scheduling of Conditional Task Graphs (CTGs). In CTGs, nodes represent activities, some of them are branches and are labeled with a condition, arcs rooted in branch nodes are labeled with condition outcomes and a corresponding probability. A task is executed at run time if the condition outcomes that label the arcs in the path to the task hold at schedule execution time; this can be captured off-line by adopting a stochastic model. Tasks need for their execution either unary or cumulative resources and some tasks can be executed on alternative resources. The solution to the problem is a single assignment of a resource and of a start time to each task so that the allocation and schedule is feasible in each scenario and the expected value of a given objective function is optimized. For this problem we need to extend traditional constraint-based scheduling techniques in two directions: (i) compute the probability of sets of scenarios in polynomial time, in order to get the expected value of the objective function; (ii) define conditional constraints that ensure feasibility in all scenarios. We show the application of this framework on problems with objective functions depending either on the allocation of resources to tasks or on the scheduling part. Also, we present the conditional extension to the timetable global constraint. Experimental results show the effectiveness of the approach on a set of benchmarks taken from the field of embedded system design. Comparing our solver with a scenario based solver proposed in the literature, we show the advantages of our approach both in terms of execution time and solution quality.}
}
@article{SANNER2009748,
title = {Practical solution techniques for first-order MDPs},
journal = {Artificial Intelligence},
volume = {173},
number = {5},
pages = {748-788},
year = {2009},
note = {Advances in Automated Plan Generation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001884},
author = {Scott Sanner and Craig Boutilier},
keywords = {MDPs, First-order logic, Planning},
abstract = {Many traditional solution approaches to relationally specified decision-theoretic planning problems (e.g., those stated in the probabilistic planning domain description language, or PPDDL) ground the specification with respect to a specific instantiation of domain objects and apply a solution approach directly to the resulting ground Markov decision process (MDP). Unfortunately, the space and time complexity of these grounded solution approaches are polynomial in the number of domain objects and exponential in the predicate arity and the number of nested quantifiers in the relational problem specification. An alternative to grounding a relational planning problem is to tackle the problem directly at the relational level. In this article, we propose one such approach that translates an expressive subset of the PPDDL representation to a first-order MDP (FOMDP) specification and then derives a domain-independent policy without grounding at any intermediate step. However, such generality does not come without its own set of challengesâ€”the purpose of this article is to explore practical solution techniques for solving FOMDPs. To demonstrate the applicability of our techniques, we present proof-of-concept results of our first-order approximate linear programming (FOALP) planner on problems from the probabilistic track of the ICAPS 2004 and 2006 International Planning Competitions.}
}
@article{GRANDI201345,
title = {Lifting integrity constraints in binary aggregation},
journal = {Artificial Intelligence},
volume = {199-200},
pages = {45-66},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000416},
author = {Umberto Grandi and Ulle Endriss},
keywords = {Collective decision making, Computational social choice, Multi-issue domains, Combinatorial vote, Judgment aggregation},
abstract = {We consider problems in which several individuals each need to make a yes/no choice regarding a number of issues and these choices then need to be aggregated into a collective choice. Depending on the application at hand, different combinations of yes/no may be considered rational. We describe rationality assumptions as integrity constraints using a simple propositional language and we explore the question of whether or not a given aggregation procedure will lift a given integrity constraint from the individual to the collective level, i.e., whether the collective choice will be rational whenever all individual choices are.}
}
@article{XUE201565,
title = {Optimizing ontology alignments through a Memetic Algorithm using both MatchFmeasure and Unanimous Improvement Ratio},
journal = {Artificial Intelligence},
volume = {223},
pages = {65-81},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000399},
author = {Xingsi Xue and Yuping Wang},
keywords = {Ontology alignment, Memetic Algorithm, MatchFmeasure, Unanimous Improvement Ratio},
abstract = {There are three main drawbacks of current evolutionary approaches for determining the weights of ontology matching system. The first drawback is that it is difficult to simultaneously deal with several pairs of ontologies, i.e. finding a universal weight configuration that can be used for different ontology pairs without adjustment. The second one is that a reference alignment between two ontologies to be aligned should be given in advance which could be very expensive to obtain especially when the scale of ontologies is considerably large. The last one arises from f-measure, a generally used evaluation metric of the alignment's quality, which may cause the bias improvement of the solution. To overcome these three defects, in this paper, we propose to use both MatchFmeasure, a rough evaluation metric on no reference alignment to approximate f-measure, and Unanimous Improvement Ratio (UIR), a measure that complements MatchFmeasure, in the process of optimizing the ontology alignments by Memetic Algorithm (MA). The experimental results have shown that the MA using both MatchFmeasure and UIR is effective to simultaneously align multiple pairs of ontologies and avoid the bias improvement caused by MatchFeasure. Moreover, the comparison with state-of-the-art ontology matching systems further indicates the effectiveness of the proposed method.}
}
@article{CIANCARINI2010670,
title = {Monte Carlo tree search in Kriegspiel},
journal = {Artificial Intelligence},
volume = {174},
number = {11},
pages = {670-684},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000536},
author = {Paolo Ciancarini and Gian Piero Favini},
keywords = {Games, Chess, Kriegspiel, Incomplete information, Monte Carlo tree search},
abstract = {Partial information games are excellent examples of decision making under uncertainty. In particular, some games have such an immense state space and high degree of uncertainty that traditional algorithms and methods struggle to play them effectively. Monte Carlo tree search (MCTS) has brought significant improvements to the level of computer programs in games such as Go, and it has been used to play partial information games as well. However, there are certain games with particularly large trees and reduced information in which a naive MCTS approach is insufficient: in particular, this is the case of games with long matches, dynamic information, and complex victory conditions. In this paper we explore the application of MCTS to a wargame-like board game, Kriegspiel. We describe and study three MCTS-based methods, starting from a very simple implementation and moving to more refined versions for playing the game with little specific knowledge. We compare these MCTS-based programs to the strongest known minimax-based Kriegspiel program, obtaining significantly better experimental results with less domain-specific knowledge.}
}
@article{ZABKAR201654,
title = {Extracting qualitative relations from categorical data},
journal = {Artificial Intelligence},
volume = {239},
pages = {54-69},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300704},
author = {Jure Å½abkar and Ivan Bratko and Janez DemÅ¡ar},
keywords = {Qualitative modeling, Machine learning, Ceteris paribus},
abstract = {Qualitative modeling is traditionally concerned with the abstraction of numerical data. In numerical domains, partial derivatives describe the relation between the independent and dependent variable; qualitatively, they tell us the trend of the dependent variable. In this paper, we address the problem of extracting qualitative relations in categorical domains. We generalize the notion of partial derivative by defining the probabilistic discrete qualitative partial derivative (PDQ PD). PDQ PD is a qualitative relation between the target class c and the discrete attribute; the derivative corresponds to ordering the attribute's values, ai, by P(c|ai) in a local neighborhood of the reference point, respecting the ceteris paribus principle. We present an algorithm for computation of PDQ PD from labeled attribute-based training data. Machine learning algorithms can then be used to induce models that explain the influence of the attribute's values on the target class in different subspaces of the attribute space.}
}
@article{JONSSON1998143,
title = {A unifying approach to temporal constraint reasoning},
journal = {Artificial Intelligence},
volume = {102},
number = {1},
pages = {143-155},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00031-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000319},
author = {Peter Jonsson and Christer BÃ¤ckstrÃ¶m},
keywords = {Temporal constraint reasoning, Disjunctive linear relations, Complexity, Algorithms},
abstract = {We present a formalism, Disjunctive Linear Relations (DLRs), for reasoning about temporal constraints. DLRs subsume most of the formalisms for temporal constraint reasoning proposed in the literature and is therefore computationally expensive. We also present a restricted type of DLRs, Horn DLRs, which have a polynomial-time satisfiability problem. We prove that most approaches to tractable temporal constraint reasoning can be encoded as Horn DLRs, including the ORD-Horn algebra by Nebel and BÃ¼rckert and the simple temporal constraints by Dechter et al. Thus, DLRs is a suitable unifying formalism for reasoning about temporal constraints.}
}
@article{MIYASHITA1995377,
title = {CABINS: a framework of knowledge acquisition and iterative revision for schedule improvement and reactive repair},
journal = {Artificial Intelligence},
volume = {76},
number = {1},
pages = {377-426},
year = {1995},
note = {Planning and Scheduling},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00089-J},
url = {https://www.sciencedirect.com/science/article/pii/000437029400089J},
author = {Kazuo Miyashita and Katia Sycara},
abstract = {Practical scheduling problems generally require allocation of resources in the presence of a large, diverse and typically conflicting set of constraints and optimization criteria. The ill-structuredness of both the solution space and the desired objectives make scheduling problems difficult to formalize. This paper describes a case-based learning method for acquiring context-dependent user optimization preferences and tradeoffs and using them to incrementally improve schedule quality in predictive scheduling and reactive schedule management in response to unexpected execution events. The approach, implemented in the CABINS system, uses acquired user preferences to dynamically modify search control to guide schedule improvement. During iterative repair, cases are exploited for: (1) repair action selection, (2) evaluation of intermediate repair results and (3) recovery from revision failures. The method allows the system to dynamically switch between repair heuristic actions, each of which operates with respect to a particular local view of the problem and offers selective repair advantages. Application of a repair action tunes the search procedure to the characteristics of the local repair problem. This is achieved by dynamic modification of the search control bias. There is no a priori characterization of the amount of modification that may be required by repair actions. However, initial experimental results show that the approach is able to (a) capture and effectively utilize user scheduling preferences that were not present in the scheduling model, (b) produce schedules with high quality, without unduly sacrificing efficiency in predictive schedule generation and reactive response to unpredictable execution events along a variety of criteria that have been recognized as important in real operating environments.}
}
@article{GINSBERG199589,
title = {Approximate planning},
journal = {Artificial Intelligence},
volume = {76},
number = {1},
pages = {89-123},
year = {1995},
note = {Planning and Scheduling},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00077-E},
url = {https://www.sciencedirect.com/science/article/pii/000437029400077E},
author = {Matthew L. Ginsberg},
abstract = {This paper makes two linked contributions. First, we argue that planning systems, instead of being correct (every plan returned achieves the goal) and complete (all such plans are returned), should be approximately correct and complete, in that most plans returned achieve the goal and that most such plans are returned. The first contribution we make is to formalize this notion. Our second aim is to demonstrate the practical importance of these ideas. We argue that the cached plans used by case-based planners are best thought of as approximate as opposed to exact, and also show that we can use our approach to plan for subgoals g1 and g2 separately and to combine the plans generated to produce a plan for the conjoined goal g1Î›g2. The computational benefits of working with subgoals separately have long been recognized, but attempts to do so using correct and complete planners have failed.}
}
@article{AMENDOLA2016219,
title = {Semi-equilibrium models for paracoherent answer set programs},
journal = {Artificial Intelligence},
volume = {234},
pages = {219-271},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300029},
author = {Giovanni Amendola and Thomas Eiter and Michael Fink and Nicola Leone and JoÃ£o Moura},
keywords = {Answer set programming, Equilibrium logic, Paracoherent reasoning, Splitting sequences, Inconsistency management},
abstract = {The answer set semantics may assign a logic program to model, due to logical contradiction or unstable negation, which is caused by cyclic dependency of an atom on its negation. While logical contradictions can be handled with traditional techniques from paraconsistent reasoning, instability requires other methods. We consider resorting to a paracoherent semantics, in which 3-valued interpretations are used where a third truth value besides true and false expresses that an atom is believed true. This is at the basis of the semi-stable model semantics, which was defined using a program transformation. In this paper, we give a model-theoretic characterization of semi-stable models, which makes the semantics more accessible. Motivated by some anomalies of semi-stable model semantics with respect to basic epistemic properties, we propose an amendment that satisfies these properties. The latter has both a transformational and a model-theoretic characterization that reveals it as a relaxation of equilibrium logic, the logical reconstruction of answer set semantics, and is thus called the semi-equilibrium model semantics. We consider refinements of this semantics to respect modularity in the rules, based on splitting sets, the major tool for modularity in modeling and evaluating answer set programs. In that, we single out classes of canonical models that are amenable for customary bottom-up evaluation of answer set programs, with an option to switch to a paracoherent mode when lack of an answer set is detected. A complexity analysis of major reasoning tasks shows that semi-equilibrium models are harder than answer sets (i.e., equilibrium models), due to a global minimization step for keeping the gap between true and believed true atoms as small as possible. Our results contribute to the logical foundations of paracoherent answer set programming, which gains increasing importance in inconsistency management, and at the same time provide a basis for algorithm development and integration into answer set solvers.}
}
@article{FREUND2008570,
title = {On the notion of concept I},
journal = {Artificial Intelligence},
volume = {172},
number = {4},
pages = {570-590},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001324},
author = {Michael Freund},
keywords = {Categorization, Concept, Extension, Intension, Typicality, Membership, Modular orders, Fuzzy sets, Formal concepts analysis},
abstract = {It is well known that classical set theory is not expressive enough to adequately model categorization and prototype theory. Recent work on compositionality and concept determination showed that the quantitative solution initially offered by classical fuzzy logic also led to important drawbacks. Several qualitative approaches were thereafter tempted, that aimed at modeling membership through ordinal scales or lattice fuzzy sets. Most of the solutions obtained by these theoretical constructions however are of difficult use in categorization theory. We propose a simple qualitative model in which membership relative to a given concept f is represented by a function that takes its value in a finite abstract set Af equipped with a total order. This function is recursively built through a stratification of the set of concepts at hand based on a notion of complexity. Similarly, the typicality associated with a concept f will be described using an ordering that takes into account the characteristic features of f. Once the basic notions of membership and typicality are set, the study of compound concepts is possible and leads to interesting results. In particular, we investigate the internal structure of concepts, and obtain the characterization of all smooth subconcepts of a given concept.}
}
@article{KAMINSKI1995285,
title = {A comparative study of open default theories},
journal = {Artificial Intelligence},
volume = {77},
number = {2},
pages = {285-319},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00035-Y},
url = {https://www.sciencedirect.com/science/article/pii/000437029400035Y},
author = {Michael Kaminski},
abstract = {The paper examines the definitions of open default theories known from the literature. First it is shown that none of them is satisfactory either for formal or for intuitive reasons. Next a new approach is considered. It is free from the obvious deficiencies of the known definitions, but possesses their positive properties.}
}
@article{MENGSHOEL20061137,
title = {Controlled generation of hard and easy Bayesian networks: Impact on maximal clique size in tree clustering},
journal = {Artificial Intelligence},
volume = {170},
number = {16},
pages = {1137-1174},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000816},
author = {Ole J. Mengshoel and David C. Wilkins and Dan Roth},
keywords = {Probabilistic reasoning, Bayesian networks, Tree clustering inference, Maximal clique size, -ratio, Random generation, Controlled experiments},
abstract = {This article presents and analyzes algorithms that systematically generate random Bayesian networks of varying difficulty levels, with respect to inference using tree clustering. The results are relevant to research on efficient Bayesian network inference, such as computing a most probable explanation or belief updating, since they allow controlled experimentation to determine the impact of improvements to inference algorithms. The results are also relevant to research on machine learning of Bayesian networks, since they support controlled generation of a large number of data sets at a given difficulty level. Our generation algorithms, called BPART and MPART, support controlled but random construction of bipartite and multipartite Bayesian networks. The Bayesian network parameters that we vary are the total number of nodes, degree of connectivity, the ratio of the number of non-root nodes to the number of root nodes, regularity of the underlying graph, and characteristics of the conditional probability tables. The main dependent parameter is the size of the maximal clique as generated by tree clustering. This article presents extensive empirical analysis using the Hugin tree clustering approach as well as theoretical analysis related to the random generation of Bayesian networks using BPART and MPART.}
}
@article{KURTONINA1999303,
title = {Expressiveness of concept expressions in first-order description logics},
journal = {Artificial Intelligence},
volume = {107},
number = {2},
pages = {303-333},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00109-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029800109X},
author = {Natasha Kurtonina and Maarten {de Rijke}},
keywords = {Knowledge representation, Description Logic, Expressive power, Model theory, Semantic characterizations},
abstract = {We introduce a method for characterizing the expressive power of concept expressions in first-order description logics. The method is essentially model-theoretic in nature in that it gives preservation results uniquely identifying a wide range of description logics as fragments of first-order logic. The languages studied in the paper all belong to the well-known â„±â„’Â¯ and %plane1D;49C;â„’ hierarchies.}
}
@article{KOHLAS20081360,
title = {Semiring induced valuation algebras: Exact and approximate local computation algorithms},
journal = {Artificial Intelligence},
volume = {172},
number = {11},
pages = {1360-1399},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000428},
author = {J. Kohlas and N. Wilson},
keywords = {Semirings, Local computation, Join tree decompositions, Soft constraints, Uncertainty, Valuation networks, Valuation algebras},
abstract = {Local computation in join trees or acyclic hypertrees has been shown to be linked to a particular algebraic structure, called valuation algebra. There are many models of this algebraic structure ranging from probability theory to numerical analysis, relational databases and various classical and non-classical logics. It turns out that many interesting models of valuation algebras may be derived from semiring valued mappings. In this paper we study how valuation algebras are induced by semirings and how the structure of the valuation algebra is related to the algebraic structure of the semiring. In particular, c-semirings with idempotent multiplication induce idempotent valuation algebras and therefore permit particularly efficient architectures for local computation. Also important are semirings whose multiplicative semigroup is embedded in a union of groups. They induce valuation algebras with a partially defined division. For these valuation algebras, the well-known architectures for Bayesian networks apply. We also extend the general computational framework to allow derivation of bounds and approximations, for when exact computation is not feasible.}
}
@article{GOTTLOB2010105,
title = {Bounded treewidth as a key to tractability of knowledge representation and reasoning},
journal = {Artificial Intelligence},
volume = {174},
number = {1},
pages = {105-132},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209001192},
author = {Georg Gottlob and Reinhard Pichler and Fang Wei},
keywords = {Fixed-parameter tractability, Treewidth, Monadic datalog, Abduction, Closed world reasoning, Disjunctive logic programming},
abstract = {Several forms of reasoning in AI â€“ like abduction, closed world reasoning, circumscription, and disjunctive logic programming â€“ are well known to be intractable. In fact, many of the relevant problems are on the second or third level of the polynomial hierarchy. In this paper, we show how the notion of treewidth can be fruitfully applied to this area. In particular, we show that all these problems become tractable (actually, even solvable in linear time), if the treewidth of the involved formulae or programs is bounded by some constant. Clearly, these theoretical tractability results as such do not immediately yield feasible algorithms. However, we have recently established a new method based on monadic datalog which allowed us to design an efficient algorithm for a related problem in the database area. In this work, we exploit the monadic datalog approach to construct new algorithms for logic-based abduction.}
}
@article{BARNARD200513,
title = {Word sense disambiguation with pictures},
journal = {Artificial Intelligence},
volume = {167},
number = {1},
pages = {13-30},
year = {2005},
note = {Connecting Language to the World},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205001116},
author = {Kobus Barnard and Matthew Johnson},
keywords = {Word sense disambiguation, Image auto-annotation, Region labeling, Statistical models},
abstract = {We introduce using images for word sense disambiguation, either alone, or in conjunction with traditional text based methods. The approach is based on a recently developed method for automatically annotating images by using a statistical model for the joint probability for image regions and words. The model itself is learned from a data base of images with associated text. To use the model for word sense disambiguation, we constrain the predicted words to be possible senses for the word under consideration. When word prediction is constrained to a narrow set of choices (such as possible senses), it can be quite reliable. We report on experiments using the resulting sense probabilities as is, as well as augmenting a state of the art text based word sense disambiguation algorithm. In order to evaluate our approach, we developed a new corpus, ImCor, which consists of a substantive portion of the Corel image data set associated with disambiguated text drawn from the SemCor corpus. Our experiments using this corpus suggest that visual information can be very useful in disambiguating word senses. It also illustrates that associated non-textual information such as image data can help ground language meaning.}
}
@article{COOPER2010449,
title = {Soft arc consistency revisited},
journal = {Artificial Intelligence},
volume = {174},
number = {7},
pages = {449-478},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000147},
author = {M.C. Cooper and S. {de Givry} and M. Sanchez and T. Schiex and M. Zytnicki and T. Werner},
keywords = {Valued constraint satisfaction problem, Weighted constraint satisfaction problem, Soft constraints, Constraint optimization, Local consistency, Soft arc consistency, Graphical model, Submodularity},
abstract = {The Valued Constraint Satisfaction Problem (VCSP) is a generic optimization problem defined by a network of local cost functions defined over discrete variables. It has applications in Artificial Intelligence, Operations Research, Bioinformatics and has been used to tackle optimization problems in other graphical models (including discrete Markov Random Fields and Bayesian Networks). The incremental lower bounds produced by local consistency filtering are used for pruning inside Branch and Bound search. In this paper, we extend the notion of arc consistency by allowing fractional weights and by allowing several arc consistency operations to be applied simultaneously. Over the rationals and allowing simultaneous operations, we show that an optimal arc consistency closure can theoretically be determined in polynomial time by reduction to linear programming. This defines Optimal Soft Arc Consistency (OSAC). To reach a more practical algorithm, we show that the existence of a sequence of arc consistency operations which increases the lower bound can be detected by establishing arc consistency in a classical Constraint Satisfaction Problem (CSP) derived from the original cost function network. This leads to a new soft arc consistency method, called, Virtual Arc Consistency which produces improved lower bounds compared with previous techniques and which can solve submodular cost functions. These algorithms have been implemented and evaluated on a variety of problems, including two difficult frequency assignment problems which are solved to optimality for the first time. Our implementation is available in the open source toulbar2 platform.}
}
@article{BACCHUS199675,
title = {From statistical knowledge bases to degrees of belief},
journal = {Artificial Intelligence},
volume = {87},
number = {1},
pages = {75-143},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00003-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000033},
author = {Fahiem Bacchus and Adam J. Grove and Joseph Y. Halpern and Daphne Koller},
abstract = {An intelligent agent will often be uncertain about various properties of its environment, and when acting in that environment it will frequently need to quantify its uncertainty. For example, if the agent wishes to employ the expected-utility paradigm of decision theory to guide its actions, it will need to assign degrees of belief (subjective probabilities) to various assertions. Of course, these degrees of belief should not be arbitrary, but rather should be based on the information available to the agent. This paper describes one approach for inducing degrees of belief from very rich knowledge bases, that can include information about particular individuals, statistical correlations, physical laws, and default rules. We call our approach the random-worlds method. The method is based on the principle of indifference: it treats all of the worlds the agent considers possible as being equally likely. It is able to integrate qualitative default reasoning with quantitative probabilistic reasoning by providing a language in which both types of information can be easily expressed. Our results show that a number of desiderata that arise in direct inference (reasoning from statistical information to conclusions about individuals) and default reasoning follow directly from the semantics of random worlds. For example, random worlds captures important patterns of reasoning such as specificity, inheritance, indifference to irrelevant information, and default assumptions of independence. Furthermore, the expressive power of the language used and the intuitive semantics of random worlds allow the method to deal with problems that are beyond the scope of many other nondeductive reasoning systems.}
}
@article{TENORTH2017151,
title = {Representations for robot knowledge in the KnowRob framework},
journal = {Artificial Intelligence},
volume = {247},
pages = {151-169},
year = {2017},
note = {Special Issue on AI and Robotics},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000843},
author = {Moritz Tenorth and Michael Beetz},
keywords = {Knowledge representation, Autonomous robots, Knowledge-enabled robotics},
abstract = {In order to robustly perform tasks based on abstract instructions, robots need sophisticated knowledge processing methods. These methods have to supply the difference between the (often shallow and symbolic) information in the instructions and the (detailed, grounded and often real-valued) information needed for execution. For filling these information gaps, a robot first has to identify them in the instructions, reason about suitable information sources, and combine pieces of information from different sources and of different structure into a coherent knowledge base. To this end we propose the KnowRob knowledge processing system for robots. In this article, we discuss why the requirements of a robot knowledge processing system differ from what is commonly investigated in AI research, and propose to re-consider a KR system as a semantically annotated view on information and algorithms that are often already available as part of the robot's control system. We then introduce representational structures and a common vocabulary for representing knowledge about robot actions, events, objects, environments, and the robot's hardware as well as inference procedures that operate on this common representation. The KnowRob system has been released as open-source software and is being used on several robots performing complex object manipulation tasks. We evaluate it through prototypical queries that demonstrate the expressive power and its impact on the robot's performance.}
}
@article{CHALKIADAKIS201676,
title = {Characteristic function games with restricted agent interactions: Core-stability and coalition structures},
journal = {Artificial Intelligence},
volume = {232},
pages = {76-113},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001812},
author = {Georgios Chalkiadakis and Gianluigi Greco and Evangelos Markakis},
keywords = {Coalitional games, Solution concepts, Computational complexity, Treewidth, Marginal contribution networks},
abstract = {In many real-world settings, the structure of the environment constrains the formation of coalitions among agents. These settings can be represented by characteristic function games, also known as coalitional games, equipped with interaction graphs. An interaction graph determines the set of all feasible coalitions, in that a coalition C can form only if the subgraph induced over the nodes/agents in C is connected. Our work analyzes stability issues arising in such environments, by focusing on the core as a solution concept, and by considering the coalition structure viewpoint, that is, without assuming that the grand-coalition necessarily forms. The complexity of the coalition structure core is studied over a variety of interaction graph structures of interest, including complete graphs, lines, cycles, trees, and nearly-acyclic graphs (formally, having bounded treewidth). The related stability concepts of the least core and the cost of stability are also studied. Results are derived for the setting of compact coalitional games, i.e., for games that are implicitly described via a compact encoding, and where simple calculations on this encoding are to be performed in order to compute the payoff associated with any coalition. Moreover, specific results are provided for compact games defined via marginal contribution networks, an expressive encoding mechanism that received considerable attention in the last few years.}
}
@article{BRAFMAN2008325,
title = {Graphically structured value-function compilation},
journal = {Artificial Intelligence},
volume = {172},
number = {2},
pages = {325-349},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001130},
author = {Ronen I. Brafman and Carmel Domshlak},
keywords = {Reasoning about preferences, Qualitative decision theory, Multi-attribute decision making},
abstract = {Classical work on eliciting and representing preferences over multi-attribute alternatives has attempted to recognize conditions under which value functions take on particularly simple and compact form, making their elicitation much easier. In this paper we consider preferences over discrete domains, and show that for a certain class of simple and intuitive qualitative preference statements, one can always generate compact value functions consistent with these statements. These value functions maintain the independence structure implicit in the original statements. For discrete domains, these representation theorems are much more general than previous results. However, we also show that it is not always possible to maintain this compact structure if we add explicit ordering constraints among the available outcomes.}
}
@article{BERNARDINI2021103486,
title = {A unifying look at sequence submodularity},
journal = {Artificial Intelligence},
volume = {297},
pages = {103486},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103486},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000370},
author = {Sara Bernardini and Fabio Fagnani and Chiara Piacentini},
keywords = {Submodularity, Sequence submodularity, Greedy algorithms, Suboptimal algorithms, Detection problems, Search-and-tracking, Environmental monitoring, Scheduling, Recommender systems},
abstract = {Several real-world problems in engineering and applied science require the selection of sequences that maximize a given reward function. Optimizing over sequences as opposed to sets requires exploring an exponentially larger search space and can become prohibitive in most cases of practical interest. However, if the objective function is submodular (intuitively, it exhibits a diminishing return property), the optimization problem becomes more manageable. Recently, there has been increasing interest in sequence submodularity in connection with applications such as recommender systems and online ad allocation. However, mostly ad hoc models and solutions have emerged within these applicative contexts. In consequence, the field appears fragmented and lacks coherence. In this paper, we offer a unified view of sequence submodularity and provide a generalized greedy algorithm that enjoys strong theoretical guarantees. We show how our approach naturally captures several application domains, and our algorithm encompasses existing methods, improving over them.}
}
@article{AGOTNES200945,
title = {Reasoning about coalitional games},
journal = {Artificial Intelligence},
volume = {173},
number = {1},
pages = {45-79},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001021},
author = {Thomas Ã…gotnes and Wiebe {van der Hoek} and Michael Wooldridge},
keywords = {Multi-agent systems, Knowledge representation, Coalitional games, Modal logic},
abstract = {We develop, investigate, and compare two logic-based knowledge representation formalisms for reasoning about coalitional games. The main constructs of Coalitional Game Logic (cgl) are expressions for representing the ability of coalitions, which may be combined with expressions for representing the preferences that agents have over outcomes. Modal Coalitional Game Logic (mcgl) is a normal modal logic, in which the main construct is a modality for expressing the preferences of groups of agents. For both frameworks, we give complete axiomatisations, and show how they can be used to characterise solution concepts for coalitional games. We show that, while cgl is more expressive than mcgl, the former can only be used to reason about coalitional games with finitely many outcomes, while mcgl can be used to reason also about games with infinitely many outcomes, and is in addition more succinct. We characterise the computational complexity of satisfiability for cgl, and give a tableaux-based decision procedure.}
}
@article{BOCHMAN2004105,
title = {A causal approach to nonmonotonic reasoning},
journal = {Artificial Intelligence},
volume = {160},
number = {1},
pages = {105-143},
year = {2004},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204001213},
author = {Alexander Bochman},
keywords = {Nonmonotonic reasoning, Causality, Abduction, Reasoning about action and change},
abstract = {We introduce logical formalisms of production and causal inference relations based on input/output logics of Makinson and Van der Torre [J. Philos. Logic 29 (2000) 383â€“408]. These inference relations will be assigned, however, both standard semantics (giving interpretation to their rules), and natural nonmonotonic semantics based on the principle of explanation closure. The resulting nonmonotonic formalisms will be shown to provide a logical representation of abductive reasoning, and a complete characterization of causal nonmonotonic reasoning from McCain and Turner [Proc. AAAI-97, Providence, RI, 1997, pp. 460â€“465]. The results of the study suggest production and causal inference as general nonmonotonic formalisms providing an alternative representation for a significant part of nonmonotonic reasoning.}
}
@article{MICHAEL2010639,
title = {Partial observability and learnability},
journal = {Artificial Intelligence},
volume = {174},
number = {11},
pages = {639-669},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000366},
author = {Loizos Michael},
keywords = {Partial observability, Appearance, Reality, Masking process, Sensing, Missing information, Autodidactic learning, Probably approximately correct, Information recovery, Reduction},
abstract = {When sensing its environment, an agent often receives information that only partially describes the current state of affairs. The agent then attempts to predict what it has not sensed, by using other pieces of information available through its sensors. Machine learning techniques can naturally aid this task, by providing the agent with the rules to be used for making these predictions. For this to happen, however, learning algorithms need to be developed that can deal with missing information in the learning examples in a principled manner, and without the need for external supervision. We investigate this problem herein. We show how the Probably Approximately Correct semantics can be extended to deal with missing information during both the learning and the evaluation phase. Learning examples are drawn from some underlying probability distribution, but parts of them are hidden before being passed to the learner. The goal is to learn rules that can accurately recover information hidden in these learning examples. We show that for this to be done, one should first dispense the requirement that rules should always make definite predictions; â€œdon't knowâ€ is sometimes necessitated. On the other hand, such abstentions should not be done freely, but only when sufficient information is not present for definite predictions to be made. Under this premise, we show that to accurately recover missing information, it suffices to learn rules that are highly consistent, i.e., rules that simply do not contradict the agent's sensory inputs. It is established that high consistency implies a somewhat discounted accuracy, and that this discount is, in some defined sense, unavoidable, and depends on how adversarially information is hidden in the learning examples. Within our proposed learning model we prove that any PAC learnable class of monotone or read-once formulas is also learnable from incomplete learning examples. By contrast, we prove that parities and monotone-term 1-decision lists, which are properly PAC learnable, are not properly learnable under the new learning model. In the process of establishing our positive and negative results, we re-derive some basic PAC learnability machinery, such as Occam's Razor, and reductions between learning tasks. We finally consider a special case of learning from partial learning examples, where some prior bias exists on the manner in which information is hidden, and show how this provides a unified view of many previous learning models that deal with missing information. We suggest that the proposed learning model goes beyond a simple extension of supervised learning to the case of incomplete learning examples. The principled and general treatment of missing information during learning, we argue, allows an agent to employ learning entirely autonomously, without relying on the presence of an external teacher, as is the case in supervised learning. We call our learning model autodidactic to emphasize the explicit disassociation of this model from any form of external supervision.}
}
@article{AKMAL2020103384,
title = {Quantifying controllability in temporal networks with uncertainty},
journal = {Artificial Intelligence},
volume = {289},
pages = {103384},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2020.103384},
url = {https://www.sciencedirect.com/science/article/pii/S000437022030134X},
author = {Shyan Akmal and Savana Ammons and Hemeng Li and Michael Gao and Lindsay Popowski and James C. Boerkoel},
keywords = {Scheduling, Temporal planning, Controllability, Probabilistic simple temporal networks, Simple temporal networks with uncertainty},
abstract = {Controllability for Simple Temporal Networks with Uncertainty (STNUs) has thus far been limited to three levels: strong, dynamic, and weak. Because of this, there is currently no systematic way for an agent to assess just how far from being controllable an uncontrollable STNU is. We provide new insights inspired by a geometric interpretation of STNUs to introduce the degrees of strong and dynamic controllability â€” continuous metrics that measure how far a network is from being controllable. We utilize these metrics to approximate the probabilities that an STNU can be dispatched successfully offline and online respectively. We introduce new methods for predicting the degrees of strong and dynamic controllability for uncontrollable networks. We further generalize these metrics by defining likelihood of controllability, a controllability measure that applies to Probabilistic Simple Temporal Networks (PSTNs). Finally, we empirically demonstrate that these metrics are good predictors of actual dispatch success rate for STNUs and PSTNs.}
}
@article{LIERLER20141,
title = {Relating constraint answer set programming languages and algorithms},
journal = {Artificial Intelligence},
volume = {207},
pages = {1-22},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213001094},
author = {Yuliya Lierler},
keywords = {(Constraint) answer set programming, Constraint satisfaction processing, Satisfiability modulo theories},
abstract = {Recently a logic programming language AC was proposed by Mellarkod et al. [1] to integrate answer set programming and constraint logic programming. Soon after that, a clingcon language integrating answer set programming and finite domain constraints, as well as an ezcsp language integrating answer set programming and constraint logic programming were introduced. The development of these languages and systems constitutes the appearance of a new AI subarea called constraint answer set programming. All these languages have something in common. In particular, they aim at developing new efficient inference algorithms that combine traditional answer set programming procedures and other methods in constraint programming. Yet, the exact relation between the constraint answer set programming languages and the underlying systems is not well understood. In this paper we address this issue by formally stating the precise relation between several constraint answer set programming languages â€“ AC, clingcon, ezcsp â€“ as well as the underlying systems.}
}
@article{MORGENSTERN1998237,
title = {Inheritance comes of age: applying nonmonotonic techniques to problems in industry},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {237-271},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00073-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000733},
author = {Leora Morgenstern},
keywords = {Artificial intelligence, Inheritance, Inheritance networks, Semantic networks, Expert systems, Nonmonotonic reasoning, First-order logic, Rule-based systems, Insurance, Medical insurance, Benefits inquiry, Applications, Commercial applications, Medical AI, Nonmonotonic logic, Nonmonotonic techniques},
abstract = {Nonmonotonic reasoning is virtually absent from industry and has been so since its inception; the result is that the field is becoming increasingly marginalized within AI. We argue that this is largely because researchers in the area focus exclusively on commonsense problems which are irrelevant to industry and because few efficient algorithms or tools have been developed. A sensible strategy is thus to focus on industry problems and to develop solutions within tractable subtheories of nonmonotonic logic. We examine an example of nonmonotonic reasoning in industryâ€”inheritance of business rules in the medical insurance domainâ€”and show how the paradigm of inheritance with exceptions can be extended to a broader and more powerful kind of nonmonotonic reasoning. This is done by introducing formula-augmented semantic networks (FANs), semantic networks which attach wellformed formulae to nodes. The problem of inheriting well-formed formulae within this structure is explored, and an algorithm is given and discussed. Finally we discuss the underlying lessons that can be generalized to other industry problems.}
}
@article{SHIRAZI2011193,
title = {First-order logical filtering},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {193-219},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000512},
author = {Afsaneh Shirazi and Eyal Amir},
keywords = {Filtering, First-order logic, Belief update, Situation calculus},
abstract = {Logical filtering is the process of updating a belief state (set of possible world states) after a sequence of executed actions and perceived observations. In general, it is intractable in dynamic domains that include many objects and relationships. Still, potential applications for such domains (e.g., semantic web, autonomous agents, and partial-knowledge games) encourage research beyond intractability results. In this paper we present polynomial-time algorithms for filtering belief states that are encoded as First-Order Logic (FOL) formulas. Our algorithms are exact in many cases of interest. They accept belief states in FOL without functions, permitting arbitrary arity for predicates, infinite universes of elements, and equality. They enable natural representation with explicit references to unidentified objects and partially known relationships, still maintaining tractable computation. Previous results focus on more general cases that are intractable or permit only imprecise filtering. Our algorithms guarantee that belief-state representation remains compact for STRIPS actions (among others) with unbounded-size domains. This guarantees tractable exact filtering indefinitely for those domains. The rest of our results apply to expressive modeling languages, such as partial databases and belief revision in FOL.}
}
@article{ITOH2007453,
title = {Partially observable Markov decision processes with imprecise parameters},
journal = {Artificial Intelligence},
volume = {171},
number = {8},
pages = {453-490},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000471},
author = {Hideaki Itoh and Kiyohiko Nakamura},
keywords = {POMDP, Second-order beliefs, Parameter set, Probability interval},
abstract = {This study extends the framework of partially observable Markov decision processes (POMDPs) to allow their parameters, i.e., the probability values in the state transition functions and the observation functions, to be imprecisely specified. It is shown that this extension can reduce the computational costs associated with the solution of these problems. First, the new framework, POMDPs with imprecise parameters (POMDPIPs), is formulated. We consider (1) the interval case, in which each parameter is imprecisely specified by an interval that indicates possible values of the parameter, and (2) the point-set case, in which each probability distribution is imprecisely specified by a set of possible distributions. Second, a new optimality criterion for POMDPIPs is introduced. As in POMDPs, the criterion is to regard a policy, i.e., an action-selection rule, as optimal if it maximizes the expected total reward. The expected total reward, however, cannot be calculated precisely in POMDPIPs, because of the parameter imprecision. Instead, we estimate the total reward by adopting arbitrary second-order beliefs, i.e., beliefs in the imprecisely specified state transition functions and observation functions. Although there are many possible choices for these second-order beliefs, we regard a policy as optimal as long as there is at least one of such choices with which the policy maximizes the total reward. Thus there can be multiple optimal policies for a POMDPIP. We regard these policies as equally optimal, and aim at obtaining one of them. By appropriately choosing which second-order beliefs to use in estimating the total reward, computational costs incurred in obtaining such an optimal policy can be reduced significantly. We provide an exact solution algorithm for POMDPIPs that does this efficiently. Third, the performance of such an optimal policy, as well as the computational complexity of the algorithm, are analyzed theoretically. Last, empirical studies show that our algorithm quickly obtains satisfactory policies to many POMDPIPs.}
}
@article{GENT20081973,
title = {Generalised arc consistency for the AllDifferent constraint: An empirical survey},
journal = {Artificial Intelligence},
volume = {172},
number = {18},
pages = {1973-2000},
year = {2008},
note = {Special Review Issue},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001410},
author = {Ian P. Gent and Ian Miguel and Peter Nightingale},
keywords = {Constraint programming, Constraint satisfaction problems, Global constraints, AllDifferent},
abstract = {The AllDifferent constraint is a crucial component of any constraint toolkit, language or solver, since it is very widely used in a variety of constraint models. The literature contains many different versions of this constraint, which trade strength of inference against computational cost. In this paper, we focus on the highest strength of inference, enforcing a property known as generalised arc consistency (GAC). This work is an analytical survey of optimizations of the main algorithm for GAC for the AllDifferent constraint. We evaluate empirically a number of key techniques from the literature. We also report important implementation details of those techniques, which have often not been described in published papers. We pay particular attention to improving incrementality by exploiting the strongly-connected components discovered during the standard propagation process, since this has not been detailed before. Our empirical work represents by far the most extensive set of experiments on variants of GAC algorithms for AllDifferent. Overall, the best combination of optimizations gives a mean speedup of 168 times over the same implementation without the optimizations.}
}
@article{RICKEL1997201,
title = {Automated modeling of complex systems to answer prediction questions},
journal = {Artificial Intelligence},
volume = {93},
number = {1},
pages = {201-260},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00052-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000525},
author = {Jeff Rickel and Brace Porter},
keywords = {Automated modeling, Reasoning about physical systems, Large knowledge bases},
abstract = {A question about the behavior of a complex, physical system can be answered by simulating the systemâ€”the challenge is building a model of the system that is appropriate for answering the question. If the model omits relevant aspects of the system, the predicted behavior may be wrong. If, on the other hand, the model includes many aspects that are irrelevant to the question, it may be difficult to simulate and explain. The leading approach to automated modeling, â€œcompositional modelingâ€, constructs a simplest adequate model for a question from building blocks (â€œmodel fragmentsâ€) that are designed by knowledge engineers. This paper presents a new compositional modeling algorithm that constructs models from simpler building blocksâ€”the individual influences among system variablesâ€”and addresses important modeling issues that previous programs left to the knowledge engineer. In the most rigorous test of a modeling algorithm to date, we implemented our algorithm, applied it to a large knowledge base for plant physiology, and asked a domain expert to evaluate the models it produced.}
}
@article{CHEN199963,
title = {Reasoning about nondeterministic and concurrent actions: A process algebra approach},
journal = {Artificial Intelligence},
volume = {107},
number = {1},
pages = {63-98},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00104-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298001040},
author = {Xiao Jun Chen and Giuseppe {De Giacomo}},
keywords = {Process algebra, Modal mu-calculus, Reasoning about actions, Concurrency, Model checking, Logical implication},
abstract = {We present a framework for reasoning about processes (complex actions) that are constituted by several concurrent activities performed by various interacting agents. The framework is based on two distinct formalisms: a representation formalism, which is a CCS-like process algebra associated with an explicit global store, and a reasoning formalism, which is an extension of modal mucalculus, a powerful logic of programs that subsumes dynamic logics such as PDL and Î”PDL, and branching temporal logics such as CTL and CTLâˆ—. The reasoning service of interest in this setting is model checking in contrast to logical implication. This framework, although directly applicable only when complete information on the system behavior is available, has several interesting features for reasoning about actions in Artificial Intelligence. Indeed, it inherits formal and practical tools from the area of Concurrency in Computer Science, to deal with complex actions, treating suitably aspects like nonterminating executions, parallelism, communications, and interruptions.}
}
@article{LUDDECKE201944,
title = {Distributional semantics of objects in visual scenes in comparison to text},
journal = {Artificial Intelligence},
volume = {274},
pages = {44-65},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370219300177},
author = {Timo LÃ¼ddecke and Alejandro Agostini and Michael Fauth and Minija Tamosiunaite and Florentin WÃ¶rgÃ¶tter},
keywords = {Object semantics, Vision and language, Semantics, Distributional hypothesis, Computer vision},
abstract = {The distributional hypothesis states that the meaning of a concept is defined through the contexts it occurs in. In practice, often word co-occurrence and proximity are analyzed in text corpora for a given word to obtain a real-valued semantic word vector, which is taken to (at least partially) encode the meaning of this word. Here we transfer this idea from text to images, where pre-assigned labels of other objects or activations of convolutional neural networks serve as context. We propose a simple algorithm that extracts and processes object contexts from an image database and yields semantic vectors for objects. We show empirically that these representations exhibit on par performance with state-of-the-art distributional models over a set of conventional objects. For this we employ well-known word benchmarks in addition to a newly proposed object-centric benchmark.}
}
@article{NORMAN201051,
title = {A logic of delegation},
journal = {Artificial Intelligence},
volume = {174},
number = {1},
pages = {51-71},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209001088},
author = {Timothy J. Norman and Chris Reed},
keywords = {Delegation, Groups, Imperatives, Responsibility, Agent communication},
abstract = {Delegation is a foundational concept for understanding and engineering systems that interact and execute tasks autonomously. By extending recent work on tensed action logic, it becomes possible to pin down a specific interpretation of responsibility with a well specified semantics and a convenient and intuitive logic for expression. Once descriptions of direct agent responsibility can be formed, there is a foundation upon which to characterise the dynamics of how responsibility can be acquired, transferred and discharged and, in particular, how delegation can be effected. The resulting logic, designed specifically to cater for responsibility and delegation, can then be employed to offer an axiological and semantic exploration of the related concepts of forbearance, imperatives and group communication.}
}
@article{MCGEACHIE20111122,
title = {The local geometry of multiattribute tradeoff preferences},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1122-1152},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002031},
author = {Michael McGeachie and Jon Doyle},
keywords = {Decision theory, Preference representation, Multiattribute tradeoffs,  reasoning},
abstract = {Existing representations for multiattribute ceteris paribus preference statements have provided useful treatments and clear semantics for qualitative comparisons, but have not provided similarly clear representations or semantics for comparisons involving quantitative tradeoffs. We use directional derivatives and other concepts from elementary differential geometry to interpret conditional multiattribute ceteris paribus preference comparisons that state bounds on quantitative tradeoff ratios. This semantics extends the familiar economic notion of marginal rate of substitution to multiple continuous or discrete attributes. The same geometric concepts also provide means for interpreting statements about the relative importance of different attributes.}
}
@article{SANDEWALL2011416,
title = {From systems to logic in the early development of nonmonotonic reasoning},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {416-427},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000494},
author = {Erik Sandewall},
keywords = {Nonmonotonic reasoning, Frame problem, Truth maintenance, Defeasible inheritance history of AI},
abstract = {This note describes how the notion of nonmonotonic reasoning emerged in Artificial Intelligence from the mid-1960's to 1980. It gives particular attention to the interplay between three kinds of activities: design of high-level programming systems for AI, design of truth-maintenance systems, and the development of nonmonotonic logics. This was not merely a development from logic to implementation; in several cases there was a development from a system design to a corresponding logic. The article concludes with some reflections on the roles and relationships between logicist theory and system design in AI, and in particular in Knowledge Representation.}
}
@article{LU201411,
title = {Concept drift detection via competence models},
journal = {Artificial Intelligence},
volume = {209},
pages = {11-28},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000034},
author = {Ning Lu and Guangquan Zhang and Jie Lu},
keywords = {Concept drift, Competence model, Case-base maintenance, Incremental supervised learning, Classification},
abstract = {Detecting changes of concepts, such as a change of customer preference for telecom services, is very important in terms of prediction and decision applications in dynamic environments. In particular, for case-based reasoning systems, it is important to know when and how concept drift can effectively assist decision makers to perform smarter maintenance operations at an appropriate time. This paper presents a novel method for detecting concept drift in a case-based reasoning system. Rather than measuring the actual case distribution, we introduce a new competence model that detects differences through changes in competence. Our competence-based concept detection method requires no prior knowledge of case distribution and provides statistical guarantees on the reliability of the changes detected, as well as meaningful descriptions and quantification of these changes. This research concludes that changes in data distribution do reflect upon competence. Eight sets of experiments under three categories demonstrate that our method effectively detects concept drift and highlights drifting competence areas accurately. These results directly contribute to the research that tackles concept drift in case-based reasoning, and to competence model studies.}
}
@article{LI20041,
title = {Generalized Region Connection Calculus},
journal = {Artificial Intelligence},
volume = {160},
number = {1},
pages = {1-34},
year = {2004},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204001195},
author = {Sanjiang Li and Mingsheng Ying},
keywords = {(Generalized) Region Connection Calculus, Qualitative spatial reasoning, (Generalized) Boolean connection algebra, Mereology, Mereotopology, Continuous space, Discrete space},
abstract = {The Region Connection Calculus (RCC) is one of the most widely referenced system of high-level (qualitative) spatial reasoning. RCC assumes a continuous representation of space. This contrasts sharply with the fact that spatial information obtained from physical recording devices is nowadays invariably digital in form and therefore implicitly uses a discrete representation of space. Recently, Galton developed a theory of discrete space that parallels RCC, but question still lies in that can we have a theory of qualitative spatial reasoning admitting models of discrete spaces as well as continuous spaces? In this paper we aim at establishing a formal theory which accommodates both discrete and continuous spatial information, and a generalization of Region Connection Calculus is introduced. GRCC, the new theory, takes two primitives: the mereological notion of part and the topological notion of connection. RCC and Galton's theory for discrete space are both extensions of GRCC. The relation between continuous models and discrete ones is also clarified by introducing some operations on models of GRCC. In particular, we propose a general approach for constructing countable RCC models as direct limits of collections of finite models. Compared with standard RCC models given rise from regular connected spaces, these countable models have the nice property that each region can be constructed in finite steps from basic regions. Two interesting countable RCC models are also given: one is a minimal RCC model, the other is a countable sub-model of the continuous space R2.}
}
@article{LABREUCHE20111410,
title = {A general framework for explaining the results of a multi-attribute preference model},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1410-1448},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001979},
author = {Christophe Labreuche},
keywords = {Preferences, Decision theory, Argumentation, Weight},
abstract = {The automatic generation of an explanation of the prescription made by a multi-attribute decision model is crucial in many applications, such as recommender systems. This task is complex since the quantitative models are not designed to be easily explainable. The major limitation of the previous research is that there is no formal justification of the arguments that are selected in the explanation. The goal of this paper is to define a general framework to justify which arguments shall be selected, in the case where the decision model is based on weights assigned to the attributes. Due to the complexity of explaining a preference model based on utility theory, several explanation reasonings are necessary to cover all cases â€“ ranging from situations where the prescription is trivial to situations where the prescription is much more tight. The set of selected arguments is, in this framework, a non-dominated element of a combinatorial structure in the sense of an order relation. Our general approach is instantiated precisely on three models: the probabilistic expected utility model, the qualitative pessimistic minmax model and the concordance rule, which are all constructed from a weight vector.}
}
@article{BRAFMAN1997217,
title = {Modeling agents as qualitative decision makers},
journal = {Artificial Intelligence},
volume = {94},
number = {1},
pages = {217-268},
year = {1997},
note = {Economic Principles of Multi-Agent Systems},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00024-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000246},
author = {Ronen I. Brafman and Moshe Tennenholtz},
keywords = {Agent modeling, Mental states, Qualitative decision making, Belief ascription, Multi-agent systems, Prediction},
abstract = {We investigate the semantic foundations of a method for modeling agents as entities with a mental state which was suggested by McCarthy and by Newell. Our goals are to formalize this modeling approach and its semantics, to understand the theoretical and practical issues that it raises, and to address some of them. In particular, this requires specifying the model's parameters and how these parameters are to be assigned (i.e., their grounding). We propose a basic model in which the agent is viewed as a qualitative decision maker with beliefs, preferences, and a decision strategy; and we show how these components would determine the agent's behavior. We ground this model in the agent's interaction with the world, namely, in its actions. This is done by viewing model construction as a constraint satisfaction problem in which we search for a model consistent with the agent's behavior and with our general background knowledge. In addition, we investigate the conditions under which a mental state model exists, characterizing a class of â€œgoal-seekingâ€ agents that can be modeled in this manner; and we suggest two criteria for choosing between consistent models, showing conditions under which they lead to a unique choice of model.}
}
@article{HUBER2006462,
title = {Ranking functions and rankings on languages},
journal = {Artificial Intelligence},
volume = {170},
number = {4},
pages = {462-471},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205001736},
author = {Franz Huber},
keywords = {Extension theorem for rankings on languages, Probabilities, Ranking functions, Rankings on languages, Spohn},
abstract = {The Spohnian paradigm of ranking functions is in many respects like an order-of-magnitude reverse of subjective probability theory. Unlike probabilities, however, ranking functions are only indirectlyâ€”via a pointwise ranking function on the underlying set of possibilities Wâ€”defined on a field of propositions A over W. This research note shows under which conditions ranking functions on a field of propositions A over W and rankings on a language L are induced by pointwise ranking functions on W and the set of models for L, ModL, respectively.}
}
@article{LIU20112170,
title = {Foundations of instance level updates in expressive description logics},
journal = {Artificial Intelligence},
volume = {175},
number = {18},
pages = {2170-2197},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000993},
author = {Hongkai Liu and Carsten Lutz and Maja MiliÄiÄ‡ and Frank Wolter},
keywords = {Description logics, ABoxes, Updates},
abstract = {In description logic (DL), ABoxes are used for describing the state of affairs in an application domain. We consider the problem of updating ABoxes when the state changes, assuming that update information is described at an atomic level, i.e., in terms of possibly negated ABox assertions that involve only atomic concepts and roles. We analyze such basic ABox updates in several standard DLs, in particular addressing questions of expressibility and succinctness: can updated ABoxes always be expressed in the DL in which the original ABox was formulated and, if so, what is the size of the updated ABox? It turns out that DLs have to include nominals and the â€˜@â€™ constructor of hybrid logic for updated ABoxes to be expressible, and that this still holds when updated ABoxes are approximated. Moreover, the size of updated ABoxes is exponential in the role depth of the original ABox and the size of the update. We also show that this situation improves when updated ABoxes are allowed to contain additional auxiliary symbols. Then, DLs only need to include nominals for updated ABoxes to exist, and the size of updated ABoxes is polynomial in the size of both the original ABox and the update.}
}
@article{JABBARIARFAEE20112075,
title = {Learning heuristic functions for large state spaces},
journal = {Artificial Intelligence},
volume = {175},
number = {16},
pages = {2075-2098},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000877},
author = {Shahab {Jabbari Arfaee} and Sandra Zilles and Robert C. Holte},
keywords = {Heuristic search, Planning, Learning heuristics},
abstract = {We investigate the use of machine learning to create effective heuristics for search algorithms such as IDAâŽ or heuristic-search planners such as FF. Our method aims to generate a sequence of heuristics from a given weak heuristic h0 and a set of unsolved training instances using a bootstrapping procedure. The training instances that can be solved using h0 provide training examples for a learning algorithm that produces a heuristic h1 that is expected to be stronger than h0. If h0 is so weak that it cannot solve any of the given instances we use random walks backward from the goal state to create a sequence of successively more difficult training instances starting with ones that are guaranteed to be solvable by h0. The bootstrap process is then repeated using hi in lieu of hiâˆ’1 until a sufficiently strong heuristic is produced. We test this method on the 24-sliding-tile puzzle, the 35-pancake puzzle, RubikÊ¼s Cube, and the 20-blocks world. In every case our method produces a heuristic that allows IDAâŽ to solve randomly generated problem instances quickly with solutions close to optimal. The total time for the bootstrap process to create strong heuristics for these large state spaces is on the order of days. To make the process effective when only a single problem instance needs to be solved, we present a variation in which the bootstrap learning of new heuristics is interleaved with problem-solving using the initial heuristic and whatever heuristics have been learned so far. This substantially reduces the total time needed to solve a single instance, while the solutions obtained are still close to optimal.}
}
@article{BELARDINELLI20183,
title = {Second-order propositional modal logic: Expressiveness and completeness results},
journal = {Artificial Intelligence},
volume = {263},
pages = {3-45},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218303886},
author = {Francesco Belardinelli and Wiebe {van der Hoek} and Louwe B. Kuijer},
keywords = {Modal logic, Knowledge representation, Second-order propositional modal logic, Epistemic logic, Local properties},
abstract = {In this paper we advance the state-of-the-art on the application of second-order propositional modal logic (SOPML) in the representation of individual and group knowledge, as well as temporal and spatial reasoning. The main theoretical contributions of the paper can be summarised as follows. Firstly, we introduce the language of (multi-modal) SOPML and interpret it on a variety of different classes of Kripke frames according to the features of the accessibility relations and of the algebraic structure of the quantification domain of propositions. We provide axiomatisations for some of these classes, and show that SOPML is unaxiomatisable on the remaining classes. Secondly, we introduce novel notions of (bi)simulations and prove that they indeed preserve the interpretation of formulas in (the universal fragment of) SOPML. Then, we apply this formal machinery to study the expressiveness of Second-order Propositional Epistemic Logic (SOPEL) in representing higher-order knowledge, i.e., the knowledge agents have about other agents' knowledge, as well as graph-theoretic notions (e.g., 3-colorability, Hamiltonian paths, etc.). The final outcome is a rich formalism to represent and reason about relevant concepts in artificial intelligence, while still having a model checking problem that is no more computationally expensive than that of the less expressive quantified boolean logic.}
}
@article{MALO201386,
title = {Automated query learning with Wikipedia and genetic programming},
journal = {Artificial Intelligence},
volume = {194},
pages = {86-110},
year = {2013},
note = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000768},
author = {Pekka Malo and Pyry Siitari and Ankur Sinha},
keywords = {Wikipedia, Genetic programming, Concept recognition, Information filtering, Automatic indexing, Query definition},
abstract = {Most of the existing information retrieval systems are based on bag-of-words model and are not equipped with common world knowledge. Work has been done towards improving the efficiency of such systems by using intelligent algorithms to generate search queries, however, not much research has been done in the direction of incorporating human-and-society level knowledge in the queries. This paper is one of the first attempts where such information is incorporated into the search queries using Wikipedia semantics. The paper presents Wikipedia-based Evolutionary Semantics (Wiki-ES) framework for generating concept based queries using a set of relevance statements provided by the user. The query learning is handled by a co-evolving genetic programming procedure. To evaluate the proposed framework, the system is compared to a bag-of-words based genetic programming framework as well as to a number of alternative document filtering techniques. The results obtained using Reuters newswire documents are encouraging. In particular, the injection of Wikipedia semantics into a GP-algorithm leads to improvement in average recall and precision, when compared to a similar system without human knowledge. A further comparison against other document filtering frameworks suggests that the proposed GP-method also performs well when compared with systems that do not rely on query-expression learning.}
}
@article{PINKAS1995203,
title = {Reasoning, nonmonotonicity and learning in connectionist networks that capture propositional knowledge},
journal = {Artificial Intelligence},
volume = {77},
number = {2},
pages = {203-247},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00032-V},
url = {https://www.sciencedirect.com/science/article/pii/000437029400032V},
author = {Gadi Pinkas},
abstract = {The paper presents a connectionist framework that is capable of representing and learning propositional knowledge. An extended version of propositional calculus is developed and is demonstrated to be useful for nonmonotonic reasoning, dealing with conflicting beliefs and for coping with inconsistency generated by unreliable knowledge sources. Formulas of the extended calculus are proved to be equivalent in a very strong sense to symmetric networks (like Hopfield networks and Boltzmann machines), and efficient algorithms are given for translating back and forth between the two forms of knowledge representation. A fast learning procedure is presented that allows symmetric networks to learn representations of unknown logic formulas by looking at examples. A connectionist inference engine is then sketched whose knowledge is either compiled from a symbolic representation or learned inductively from training examples. Experiments with large scale randomly generated formulas suggest that the parallel local search that is executed by the networks is extremely fast on average. Finally, it is shown that the extended logic can be used as a high-level specification language for connectionist networks, into which several recent symbolic systems may be mapped. The paper demonstrates how a rigorous bridge can be constructed that ties together the (sometimes opposing) connectionist and symbolic approaches.}
}
@article{TANG20091041,
title = {Computer-aided proofs of Arrow's and other impossibility theorems},
journal = {Artificial Intelligence},
volume = {173},
number = {11},
pages = {1041-1053},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000320},
author = {Pingzhong Tang and Fangzhen Lin},
keywords = {Social choice theory, Arrow's theorem, Mullerâ€“Satterthwaite theorem, Sen's theorem, Knowledge representation, Computer-aided theorem proving},
abstract = {Arrow's impossibility theorem is one of the landmark results in social choice theory. Over the years since the theorem was proved in 1950, quite a few alternative proofs have been put forward. In this paper, we propose yet another alternative proof of the theorem. The basic idea is to use induction to reduce the theorem to the base case with 3 alternatives and 2 agents and then use computers to verify the base case. This turns out to be an effective approach for proving other impossibility theorems such as Mullerâ€“Satterthwaite and Sen's theorems as well. Motivated by the insights of the proof, we discover a new theorem with the help of computer programs. We believe this new proof opens an exciting prospect of using computers to discover similar impossibility or even possibility results.}
}
@article{THRUN199821,
title = {Learning metric-topological maps for indoor mobile robot navigation},
journal = {Artificial Intelligence},
volume = {99},
number = {1},
pages = {21-71},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00078-7},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000787},
author = {Sebastian Thrun},
keywords = {Autonomous robots, Exploration, Mobile robots, Neural networks, Occupancy grids, Path planning, Planning, Robot mapping, Topological maps},
abstract = {Autonomous robots must be able to learn and maintain models of their environments. Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their complexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and consistent topological maps are often difficult to learn and maintain in large-scale environments, particularly if momentary sensor data is highly ambiguous. This paper describes an approach that integrates both paradigms: grid-based and topological. Grid-based maps are learned using artificial neural networks and naive Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms, the approach presented here gains advantages from both worlds: accuracy/consistency and efficiency. The paper gives results for autonomous exploration, mapping and operation of a mobile robot in populated multi-room environments.}
}
@article{BOUTILIER1996143,
title = {Abduction to plausible causes: an event-based model of belief update},
journal = {Artificial Intelligence},
volume = {83},
number = {1},
pages = {143-166},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00097-2},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000972},
author = {Craig Boutilier},
abstract = {The Katsuno and Mendelzon (KM) theory of belief update has been proposed as a reasonable model for revising beliefs about a changing world. However, the semantics of update relies on information which is not readily available. We describe an alternative semantical view of update in which observations are incorporated into a belief set by: (a) explaining the observation in terms of a set of plausible events that might have caused that observation; and (b) predicting further consequences of those explanations. We also allow the possibility of conditional explanations. We show that this picture naturally induces an update operator conforming to the KM postulates under certain assumptions. However, we argue that these assumptions are not always reasonable, and they restrict our ability to integrate update with other forms of revision when reasoning about action.}
}
@article{LIEMHETCHARAT201441,
title = {Weighted synergy graphs for effective team formation with heterogeneous ad hoc agents},
journal = {Artificial Intelligence},
volume = {208},
pages = {41-65},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213001306},
author = {Somchaya Liemhetcharat and Manuela Veloso},
keywords = {Multi-agent, Multi-robot, Heterogeneous, Capability, Synergy, Ad hoc, Team formation},
abstract = {Previous approaches to select agents to form a team rely on single-agent capabilities, and team performance is treated as a sum of such known capabilities. Motivated by complex team formation situations, we address the problem where both single-agent capabilities may not be known upfront, e.g., as in ad hoc teams, and where team performance goes beyond single-agent capabilities and depends on the specific synergy among agents. We formally introduce a novel weighted synergy graph model to capture new interactions among agents. Agents are represented as vertices in the graph, and their capabilities are represented as Normally-distributed variables. The edges of the weighted graph represent how well the agents work together, i.e., their synergy in a team. We contribute a learning algorithm that learns the weighted synergy graph using observations of performance of teams of only two and three agents. Further, we contribute two team formation algorithms, one that finds the optimal team in exponential time, and one that approximates the optimal team in polynomial time. We extensively evaluate our learning algorithm, and demonstrate the expressiveness of the weighted synergy graph in a variety of problems. We show our approach in a rich ad hoc team formation problem capturing a rescue domain, namely the RoboCup Rescue domain, where simulated robots rescue civilians and put out fires in a simulated urban disaster. We show that the weighted synergy graph outperforms a competing algorithm, thus illustrating the efficacy of our model and algorithms.}
}
@article{THIMM2016120,
title = {On the expressivity of inconsistency measures},
journal = {Artificial Intelligence},
volume = {234},
pages = {120-151},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300108},
author = {Matthias Thimm},
keywords = {Inconsistency measures, Inconsistency management},
abstract = {We survey recent approaches to inconsistency measurement in propositional logic and provide a comparative analysis in terms of their expressivity. For that, we introduce four different expressivity characteristics that quantitatively assess the number of different knowledge bases that a measure can distinguish. Our approach aims at complementing ongoing discussions on rationality postulates for inconsistency measures by considering expressivity as a desirable property. We evaluate 16 different measures on the proposed characteristics and conclude that the distance-based measure IdalalÎ£ from Grant and Hunter (2013) [8] and the proof-based measure IPm from Jabbour and Raddaoui (2013) [16] have maximal expressivity along all considered characteristics. In our study, we discovered several interesting relationships of inconsistency measurement to e.g. set theory and Boolean functions and we also report these findings.}
}
@article{RUSSELL199757,
title = {Rationality and intelligence},
journal = {Artificial Intelligence},
volume = {94},
number = {1},
pages = {57-77},
year = {1997},
note = {Economic Principles of Multi-Agent Systems},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00026-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029700026X},
author = {Stuart J. Russell},
keywords = {Philosophical foundations, Intelligence, Rationality, Bounded rationality, Bounded optimality},
abstract = {The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. The concept of rational agency has long been considered a leading candidate to fulfill this role. This paper outlines a gradual evolution in the formal conception of rationality that brings it closer to our informal conception of intelligence and simultaneously reduces the gap between theory and practice. Some directions for future research are indicated.}
}
@article{LI20061,
title = {RCC8 binary constraint network can be consistently extended},
journal = {Artificial Intelligence},
volume = {170},
number = {1},
pages = {1-18},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S000437020500127X},
author = {Sanjiang Li and Huaiqing Wang},
keywords = {Qualitative spatial reasoning, Computational complexity, Region connection calculus, Binary constraint network, Extensionality, Path-consistency},
abstract = {The RCC8 constraint language developed by Randell et al. has been popularly adopted by the Qualitative Spatial Reasoning and GIS communities. The recent observation that RCC8 composition table describes only weak composition instead of composition raises questions about Renz and Nebel's maximality results about the computational complexity of reasoning with RCC8. This paper shows that any consistent RCC8 binary constraint network (RCC8 network for short) can be consistently extended. Given Î˜, an RCC8 network, and z, a fresh variable, suppose xTyâˆˆÎ˜ and T is contained in the weak composition of R and S. This means that we can add two new constraints xRz and zSy to Î˜ without changing the consistency of the network. The result guarantees the applicability to RCC8 of one key technique, (Theorem 5) of [J. Renz, B. Nebel, On the complexity of qualitative spatial reasoning: A maximal tractable fragment of the Region Connection Calculus. Artificial Intelligence 108 (1999) 69â€“123], which allows the transfer of tractability of a set of RCC8 relations to its closure under composition, intersection, and converse.}
}
@article{KHARDON1996187,
title = {Reasoning with models},
journal = {Artificial Intelligence},
volume = {87},
number = {1},
pages = {187-213},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00006-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000069},
author = {Roni Khardon and Dan Roth},
keywords = {Knowledge representation, Common-sense reasoning, Automated reasoning},
abstract = {We develop a model-based approach to reasoning, in which the knowledge base is represented as a set of models (satisfying assignments) rather than a logical formula, and the set of queries is restricted. We show that for every propositional knowledge base (KB) there exists a set of characteristic models with the property that a query is true in KB if and only if it is satisfied by the models in this set. We characterize a set of functions for which the model-based representation is compact and provides efficient reasoning. These include cases where the formula-based representation does not support efficient reasoning. In addition, we consider the model-based approach to abductive reasoning and show that for any propositional KB, reasoning with its model-based representation yields an abductive explanation in time that is polynomial in its size. Some of our technical results make use of the monotone theory, a new characterization of Boolean functions recently introduced. The notion of restricted queries is inherent in our approach. This is a wide class of queries for which reasoning is efficient and exact, even when the model-based representation KB provides only an approximate representation of the domain in question. Moreover, we show that the theory developed here generalizes the model-based approach to reasoning with Horn expressions and captures even the notion of reasoning with Horn approximations.}
}
@article{JIN20071,
title = {Iterated belief revision, revised},
journal = {Artificial Intelligence},
volume = {171},
number = {1},
pages = {1-18},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206001378},
author = {Yi Jin and Michael Thielscher},
keywords = {Iterated belief revision, Implicit dependence, Conditional beliefs},
abstract = {The AGM postulates for belief revision, augmented by the DP postulates for iterated belief revision, provide widely accepted criteria for the design of operators by which intelligent agents adapt their beliefs incrementally to new information. These postulates alone, however, are too permissive: They support operators by which all newly acquired information is canceled as soon as an agent learns a fact that contradicts some of its current beliefs. In this paper, we present a formal analysis of the deficiency of the standard postulates alone, and we show how to solve the problem by an additional postulate of independence. We give a representation theorem for this postulate and prove that it is compatible with AGM and DP.}
}
@article{LOUREIRO2022103661,
title = {LMMS reloaded: Transformer-based sense embeddings for disambiguation and beyond},
journal = {Artificial Intelligence},
volume = {305},
pages = {103661},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103661},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000017},
author = {Daniel Loureiro and AlÃ­pio {MÃ¡rio Jorge} and Jose Camacho-Collados},
keywords = {Semantic representations, Neural language models},
abstract = {Distributional semantics based on neural approaches is a cornerstone of Natural Language Processing, with surprising connections to human meaning representation as well. Recent Transformer-based Language Models have proven capable of producing contextual word representations that reliably convey sense-specific information, simply as a product of self-supervision. Prior work has shown that these contextual representations can be used to accurately represent large sense inventories as sense embeddings, to the extent that a distance-based solution to Word Sense Disambiguation (WSD) tasks outperforms models trained specifically for the task. Still, there remains much to understand on how to use these Neural Language Models (NLMs) to produce sense embeddings that can better harness each NLM's meaning representation abilities. In this work we introduce a more principled approach to leverage information from all layers of NLMs, informed by a probing analysis on 14 NLM variants. We also emphasize the versatility of these sense embeddings in contrast to task-specific models, applying them on several sense-related tasks, besides WSD, while demonstrating improved performance using our proposed approach over prior work focused on sense embeddings. Finally, we discuss unexpected findings regarding layer and model performance variations, and potential applications for downstream tasks.}
}
@article{SADEH1995455,
title = {Backtracking techniques for the job shop scheduling constraint satisfaction problem},
journal = {Artificial Intelligence},
volume = {76},
number = {1},
pages = {455-480},
year = {1995},
note = {Planning and Scheduling},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00078-S},
url = {https://www.sciencedirect.com/science/article/pii/000437029500078S},
author = {Norman Sadeh and Katia Sycara and Yalin Xiong},
abstract = {This paper studies a version of the job shop scheduling problem in which some operations have to be scheduled within non-relaxable time windows (i.e. earliest/latest possible start time windows). This problem is a well-known NP-complete Constraint Satisfaction Problem (CSP). A popular method for solving this type of problems involves using depth-first backtrack search. In our earlier work, we focused on the development of consistency enforcing techniques and variable/value ordering heuristics that improve the efficiency of this search procedure. In this paper, we combine these techniques with new look-back schemes that help the search procedure recover from so-called deadend search states (i.e. partial solutions that cannot be completed without violating some constraints). More specifically, we successively describe three â€œintelligentâ€ backtracking schemes: (1) Dynamic Consistency Enforcement dynamically identifies critical subproblems and determines how far to backtrack by selectively enforcing higher levels of consistency among variables participating in these critical subproblems, (2) Learning Ordering From Failure dynamically modifies the order in which variables are instantiated based on earlier conflicts, and (3) Incomplete Backjumping Heuristic abandons areas of the search space that appear to require excessive computational efforts. These schemes are shown to (1) further reduce the average complexity of the backtrack search procedure, (2) enable our system to efficiently solve problems that could not be solved otherwise due to excessive computation cost, and (3) be more effective at solving job shop scheduling problems than other look-back schemes advocated in the literature.}
}
@article{DAI201352,
title = {POMDP-based control of workflows for crowdsourcing},
journal = {Artificial Intelligence},
volume = {202},
pages = {52-85},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437021300057X},
author = {Peng Dai and Christopher H. Lin and  Mausam and Daniel S. Weld},
keywords = {Partially-Observable Markov Decision Process, POMDP, Planning under uncertainty, Crowdsourcing},
abstract = {Crowdsourcing, outsourcing of tasks to a crowd of unknown people (â€œworkersâ€) in an open call, is rapidly rising in popularity. It is already being heavily used by numerous employers (â€œrequestersâ€) for solving a wide variety of tasks, such as audio transcription, content screening, and labeling training data for machine learning. However, quality control of such tasks continues to be a key challenge because of the high variability in worker quality. In this paper we show the value of decision-theoretic techniques for the problem of optimizing workflows used in crowdsourcing. In particular, we design AI agents that use Bayesian network learning and inference in combination with Partially-Observable Markov Decision Processes (POMDPs) for obtaining excellent cost-quality tradeoffs. We use these techniques for three distinct crowdsourcing scenarios: (1) control of voting to answer a binary-choice question, (2) control of an iterative improvement workflow, and (3) control of switching between alternate workflows for a task. In each scenario, we design a Bayes net model that relates worker competency, task difficulty and worker response quality. We also design a POMDP for each task, whose solution provides the dynamic control policy. We demonstrate the usefulness of our models and agents in live experiments on Amazon Mechanical Turk. We consistently achieve superior quality results than non-adaptive controllers, while incurring equal or less cost.}
}
@article{DVORAK2012157,
title = {Augmenting tractable fragments of abstract argumentation},
journal = {Artificial Intelligence},
volume = {186},
pages = {157-173},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000239},
author = {Wolfgang DvoÅ™Ã¡k and Sebastian Ordyniak and Stefan Szeider},
keywords = {Abstract argumentation, Backdoors, Computational complexity, Parameterized complexity, Fixed-parameter tractability},
abstract = {We present a new approach to the efficient solution of important computational problems that arise in the context of abstract argumentation. Our approach makes known algorithms defined for restricted fragments generally applicable, at a computational cost that scales with the distance from the fragment. Thus, in a certain sense, we gradually augment tractable fragments. Surprisingly, it turns out that some tractable fragments admit such an augmentation and that others do not. More specifically, we show that the problems of Credulous and Skeptical Acceptance are fixed-parameter tractable when parameterized by the distance from the fragment of acyclic argumentation frameworksâ€”for most semantics. Other tractable fragments such as the fragments of symmetrical and bipartite frameworks seem to prohibit an augmentation: the acceptance problems are already intractable for frameworks at distance 1 from the fragments. For our study we use a broad setting and consider several different semantics. For the algorithmic results we utilize recent advances in fixed-parameter tractability.}
}
@article{GRANT201072,
title = {An AGM-style belief revision mechanism for probabilistic spatio-temporal logics},
journal = {Artificial Intelligence},
volume = {174},
number = {1},
pages = {72-104},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020900109X},
author = {John Grant and Francesco Parisi and Austin Parker and V.S. Subrahmanian},
keywords = {Belief revision, Reasoning about motion, Spatio-temporal logics, Uncertainty management},
abstract = {There is now extensive interest in reasoning about moving objects. A probabilistic spatio-temporal (PST) knowledge base (KB) contains atomic statements of the form â€œObject o is/was/will be in region r at time t with probability in the interval [â„“,u]â€. In this paper, we study mechanisms for belief revision in PST KBs. We propose multiple methods for revising PST KBs. These methods involve finding maximally consistent subsets and maximal cardinality consistent subsets. In addition, there may be applications where the user has doubts about the accuracy of the spatial information, or the temporal aspects, or about the ability to recognize objects in such statements. We study belief revision mechanisms that allow changes to the KB in each of these three components. Finally, there may be doubts about the assignment of probabilities in the KB. Allowing changes to the probability of statements in the KB yields another belief revision mechanism. Each of these belief revision methods may be epistemically desirable for some applications, but not for others. We show that some of these approaches cannot satisfy AGM-style axioms for belief revision under certain conditions. We also perform a detailed complexity analysis of each of these approaches. Simply put, all belief revision methods proposed that satisfy AGM-style axioms turn out to be intractable with the exception of the method that revises beliefs by changing the probabilities (minimally) in the KB. We also propose two hybrids of these basic approaches to revision and analyze the complexity of these hybrid methods.}
}
@article{KARUNATILLAKE2009935,
title = {Dialogue games that agents play within a society},
journal = {Artificial Intelligence},
volume = {173},
number = {9},
pages = {935-981},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000174},
author = {Nishan C. Karunatillake and Nicholas R. Jennings and Iyad Rahwan and Peter McBurney},
keywords = {Dialogue game protocols, Multi-agent negotiation, Social conflict resolution, Argument schemes},
abstract = {Human societies have long used the capability of argumentation and dialogue to overcome and resolve conflicts that may arise within their communities. Today, there is an increasing level of interest in the application of such dialogue games within artificial agent societies. In particular, within the field of multi-agent systems, this theory of argumentation and dialogue games has become instrumental in designing rich interaction protocols and in providing agents with a means to manage and resolve conflicts. However, to date, much of the existing literature focuses on formulating theoretically sound and complete models for multi-agent systems. Nonetheless, in so doing, it has tended to overlook the computational implications of applying such models in agent societies, especially ones with complex social structures. Furthermore, the systemic impact of using argumentation in multi-agent societies and its interplay with other forms of social influences (such as those that emanate from the roles and relationships of a society) within such contexts has also received comparatively little attention. To this end, this paper presents a significant step towards bridging these gaps for one of the most important dialogue game types; namely argumentation-based negotiation (ABN). The contributions are three fold. First, we present a both theoretically grounded and computationally tractable ABN framework that allows agents to argue, negotiate, and resolve conflicts relating to their social influences within a multi-agent society. In particular, the model encapsulates four fundamental elements: (i) a scheme that captures the stereotypical pattern of reasoning about rights and obligations in an agent society, (ii) a mechanism to use this scheme to systematically identify social arguments to use in such contexts, (iii) a language and a protocol to govern the agent interactions, and (iv) a set of decision functions to enable agents to participate in such dialogues. Second, we use this framework to devise a series of concrete algorithms that give agents a set of ABN strategies to argue and resolve conflicts in a multi-agent task allocation scenario. In so doing, we exemplify the versatility of our framework and its ability to facilitate complex argumentation dialogues within artificial agent societies. Finally, we carry out a series of experiments to identify how and when argumentation can be useful for agent societies. In particular, our results show: a clear inverse correlation between the benefit of arguing and the resources available within the context; that when agents operate with imperfect knowledge, an arguing approach allows them to perform more effectively than a non-arguing one; that arguing earlier in an ABN interaction presents a more efficient method than arguing later in the interaction; and that allowing agents to negotiate their social influences presents both an effective and an efficient method that enhances their performance within a society.}
}
@article{SRIHARI2008300,
title = {Automatic scoring of short handwritten essays in reading comprehension tests},
journal = {Artificial Intelligence},
volume = {172},
number = {2},
pages = {300-324},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001129},
author = {Sargur Srihari and Jim Collins and Rohini Srihari and Harish Srinivasan and Shravya Shetty and Janina Brutt-Griffler},
keywords = {Automatic essay scoring, Contextual handwriting recognition, Reading comprehension, Latent semantic analysis, Artificial neural networks},
abstract = {Reading comprehension is largely tested in schools using handwritten responses. The paper describes computational methods of scoring such responses using handwriting recognition and automatic essay scoring technologies. The goal is to assign to each handwritten response a score which is comparable to that of a human scorer even though machine handwriting recognition methods have high transcription error rates. The approaches are based on coupling methods of document image analysis and recognition together with those of automated essay scoring. Document image-level operations include: removal of pre-printed matter, segmentation of handwritten text lines and extraction of words. Handwriting recognition is based on a fusion of analytic and holistic methods together with contextual processing based on trigrams. The lexicons to recognize handwritten words are derived from the reading passage, the testing prompt, answer rubric and student responses. Recognition methods utilize children's handwriting styles. Heuristics derived from reading comprehension research are employed to obtain additional scoring features. Results with two methods of essay scoringâ€”both of which are based on learning from a human-scored setâ€”are described. The first is based on latent semantic analysis (LSA), which requires a reasonable level of handwriting recognition performance. The second uses an artificial neural network (ANN) which is based on features extracted from the handwriting image. LSA requires the use of a large lexicon for recognizing the entire response whereas ANN only requires a small lexicon to populate its features thereby making it practical with current word recognition technology. A test-bed of essays written in response to prompts in statewide reading comprehension tests and scored by humans is used to train and evaluate the methods. End-to-end performance results are not far from automatic scoring based on perfect manual transcription, thereby demonstrating that handwritten essay scoring has practical potential.}
}
@article{COOPER19963,
title = {A systematic methodology for cognitive modelling},
journal = {Artificial Intelligence},
volume = {85},
number = {1},
pages = {3-44},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00112-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370295001123},
author = {R. Cooper and J. Fox and J. Farringdon and T. Shallice},
abstract = {The development and testing of computational models of cognition is typically ad hoc: few generally agreed methodological principles guide the process. Consequently computational models often conflate empirically justified mechanisms with pragmatic implementation details, and essential theoretical aspects of theories are frequently hard to identify. We argue that attempts to construct cognitive theories would be considerably assisted by the availability of appropriate languages for specifying cognitive models. Such languages should: (1) be syntactically clear and succinct; (2) be operationally well defined; (3) be executable; and (4) explicitly support the division between theory and implementation detail. In support of our arguments we introduce Sceptic, an executable specification language which goes some way towards satisfying these requirements. Sceptic has been successfully used to implement a number of cognitive models including Soar, and details of the Sceptic specification of Soar are included in a technical appendix. The simplicity of Sceptic Soar permits the essentials of the underlying cognitive theory to be seen, and aids investigation of alternative theoretical assumptions. We demonstrate this by reporting three computational experiments involving modifications to the functioning of working memory within Soar. Although our focus is on Soar, the thrust of the work is more concerned with general methodological issues in cognitive modelling.}
}
@article{GELSEY199835,
title = {Using modeling knowledge to guide design space search},
journal = {Artificial Intelligence},
volume = {101},
number = {1},
pages = {35-62},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00012-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000125},
author = {Andrew Gelsey and Mark Schwabacher and Don Smith},
keywords = {Design, Engineering, Model, Optimization, Physics, Search, Aircraft, Constraint, Numerical},
abstract = {Automated search of a space of candidate designs is an attractive way to improve the traditional engineering design process. To make this approach work, however, an automated design system must include both knowledge of the modeling limitations of the method used to evaluate candidate designs and an effective way to use this knowledge to influence the search process. We argue that a productive approach is to include this knowledge by implementing a set of model constraint functions which measure how much each modeling assumption is violated. The search is then guided by using the values of these model constraint functions as constraint inputs to a standard constrained nonlinear optimization numerical method. A key result of our work is a successful demonstration of the application of AI techniques to an important engineering problem. In an empirical study of parametric conceptual aircraft design, we observed a cost improvement of two orders of magnitude. The principal contribution of our work is a new design optimization methodology which makes explicit the interaction between models of artifacts, and validity models of artifact models.}
}
@article{RASTEGARI2011441,
title = {Revenue monotonicity in deterministic, dominant-strategy combinatorial auctions},
journal = {Artificial Intelligence},
volume = {175},
number = {2},
pages = {441-456},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001438},
author = {Baharak Rastegari and Anne Condon and Kevin Leyton-Brown},
keywords = {Mechanism design, Combinatorial auctions, Revenue},
abstract = {In combinatorial auctions using VCG, a seller can sometimes increase revenue by dropping bidders. In this paper we investigate the extent to which this counterintuitive phenomenon can also occur under other deterministic, dominant-strategy combinatorial auction mechanisms. Our main result is that such failures of â€œrevenue monotonicityâ€ can occur under any such mechanism that is weakly maximalâ€”meaning roughly that it chooses allocations that cannot be augmented to cause a losing bidder to win without hurting winning biddersâ€”and that allows bidders to express arbitrary known single-minded preferences. We also give a set of other impossibility results as corollaries, concerning revenue when the set of goods changes, false-name-proofness, and the core.11We previously published a six-page preliminary version of our main result at a conference (Rastegari et al., 2007) [24]. Among other differences, this work considered a very limited version of our weak maximality condition that can be understood as requiring Pareto efficiency with respect to bidder valuations (i.e., ignoring payments).}
}
@article{RAHWAN2007897,
title = {Laying the foundations for a World Wide Argument Web},
journal = {Artificial Intelligence},
volume = {171},
number = {10},
pages = {897-921},
year = {2007},
note = {Argumentation in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000768},
author = {Iyad Rahwan and Fouad Zablith and Chris Reed},
keywords = {Argument schemes, Argumentation, Tools, E-democracy, Argument interchange format, Semantic web},
abstract = {This paper lays theoretical and software foundations for a World Wide Argument Web (WWAW): a large-scale Web of inter-connected arguments posted by individuals to express their opinions in a structured manner. First, we extend the recently proposed Argument Interchange Format (AIF) to express arguments with a structure based on Walton's theory of argumentation schemes. Then, we describe an implementation of this ontology using the RDF Schema Semantic Web-based ontology language, and demonstrate how our ontology enables the representation of networks of arguments on the Semantic Web. Finally, we present a pilot Semantic Web-based system, ArgDF, through which users can create arguments using different argumentation schemes and can query arguments using a Semantic Web query language. Manipulation of existing arguments is also handled in ArgDF: users can attack or support parts of existing arguments, or use existing parts of an argument in the creation of new arguments. ArgDF also enables users to create new argumentation schemes. As such, ArgDF is an open platform not only for representing arguments, but also for building interlinked and dynamic argument networks on the Semantic Web. This initial public-domain tool is intended to seed a variety of future applications for authoring, linking, navigating, searching, and evaluating arguments on the Web.}
}
@article{GOLDSZTEJN2015105,
title = {Variable symmetry breaking in numerical constraint problems},
journal = {Artificial Intelligence},
volume = {229},
pages = {105-125},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001216},
author = {Alexandre Goldsztejn and Christophe Jermann and Vicente {Ruiz de Angulo} and Carme Torras},
keywords = {Constraint programming, Symmetries, Numerical constraints, Variable symmetries},
abstract = {Symmetry breaking has been a hot topic of research in the past years, leading to many theoretical developments as well as strong scaling strategies for dealing with hard applications. Most of the research has however focused on discrete, combinatorial, problems, and only few considered also continuous, numerical, problems. While part of the theory applies in both contexts, numerical problems have specificities that make most of the technical developments inadequate. In this paper, we present the rlex constraints, partial symmetry-breaking inequalities corresponding to a relaxation of the famous lex constraints extensively studied in the discrete case. They allow (partially) breaking any variable symmetry and can be generated in polynomial time. Contrarily to lex constraints that are impractical in general (due to their overwhelming number) and inappropriate in the continuous context (due to their form), rlex constraints can be efficiently handled natively by numerical constraint solvers. Moreover, we demonstrate their pruning power on continuous domains is almost as strong as that of lex constraints, and they subsume several previous work on breaking specific symmetry classes for continuous problems. Their experimental behavior is assessed on a collection of standard numerical problems and the factors influencing their impact are studied. The results confirm rlex constraints are a dependable counterpart to lex constraints for numerical problems.}
}
@article{MANURANGSI201996,
title = {Computing a small agreeable set of indivisible items},
journal = {Artificial Intelligence},
volume = {268},
pages = {96-114},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218300808},
author = {Pasin Manurangsi and Warut Suksompong},
keywords = {Agreeability, Indivisible items, Resource allocation, Social choice},
abstract = {We study the problem of assigning a small subset of indivisible items to a group of agents so that the subset is agreeable to all agents, meaning that all agents value the subset as least as much as its complement. For an arbitrary number of agents and items, we derive a tight worst-case bound on the number of items that may need to be included in such a set. We then present polynomial-time algorithms that find an agreeable set whose size matches the worst-case bound when there are two or three agents. We also show that finding small agreeable sets is possible even when we only have access to the agents' preferences on single items. Furthermore, we investigate the problem of efficiently computing an agreeable set whose size approximates the size of the smallest agreeable set for any given instance. We consider two well-known models for representing the preferences of the agentsâ€”the value oracle model and additive utilitiesâ€”and establish tight bounds on the approximation ratio that can be obtained by algorithms running in polynomial time in each of these models.}
}
@article{AZIZ201571,
title = {Fair assignment of indivisible objects under ordinal preferences},
journal = {Artificial Intelligence},
volume = {227},
pages = {71-92},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000880},
author = {Haris Aziz and Serge Gaspers and Simon Mackenzie and Toby Walsh},
keywords = {Fair division, Resource allocation, Envy-freeness, Proportionality},
abstract = {We consider the discrete assignment problem in which agents express ordinal preferences over objects and these objects are allocated to the agents in a fair manner. We use the stochastic dominance relation between fractional or randomized allocations to systematically define varying notions of proportionality and envy-freeness for discrete assignments. The computational complexity of checking whether a fair assignment exists is studied for these fairness notions. We also characterize the conditions under which a fair assignment is guaranteed to exist. For a number of fairness concepts, polynomial-time algorithms are presented to check whether a fair assignment exists. Our algorithmic results also extend to the case of unequal entitlements of agents. Our NP-hardness result, which holds for several variants of envy-freeness, answers an open question posed by Bouveret, Endriss, and Lang (ECAI 2010). We also propose fairness concepts that always suggest a non-empty set of assignments with meaningful fairness properties. Among these concepts, optimal proportionality and optimal weak proportionality appear to be desirable fairness concepts.}
}
@article{ROSSI2015129,
title = {Confidence-based reasoning in stochastic constraint programming},
journal = {Artificial Intelligence},
volume = {228},
pages = {129-152},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001058},
author = {Roberto Rossi and Brahim Hnich and S. Armagan Tarim and Steven Prestwich},
keywords = {Confidence-based reasoning, Stochastic constraint programming, Sampled SCSP, ()-solution, ()-solution set, Confidence interval analysis, Global chance constraint},
abstract = {In this work we introduce a novel approach, based on sampling, for finding assignments that are likely to be solutions to stochastic constraint satisfaction problems and constraint optimisation problems. Our approach reduces the size of the original problem being analysed; by solving this reduced problem, with a given confidence probability, we obtain assignments that satisfy the chance constraints in the original model within prescribed error tolerance thresholds. To achieve this, we blend concepts from stochastic constraint programming and statistics. We discuss both exact and approximate variants of our method. The framework we introduce can be immediately employed in concert with existing approaches for solving stochastic constraint programs. A thorough computational study on a number of stochastic combinatorial optimisation problems demonstrates the effectiveness of our approach.}
}
@article{KALECH2007491,
title = {On the design of coordination diagnosis algorithms for teams of situated agents},
journal = {Artificial Intelligence},
volume = {171},
number = {8},
pages = {491-513},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000483},
author = {Meir Kalech and Gal A. Kaminka},
keywords = {Diagnosis, Multi-agent systems, Situated agents},
abstract = {Teamwork demands agreement among team-members in order to collaborate and coordinate effectively. When a disagreement between teammates occurs (due to failures), team-members should ideally diagnose its causes, to resolve the disagreement. Such diagnosis of social failures can be expensive in communication and computation, challenges which previous work has not addressed. We present a novel design space of diagnosis algorithms, distinguishing several phases in the diagnosis process, and providing alternative algorithms for each phase. We then combine these algorithms in different ways to empirically explore specific design choices in a complex domain, on thousands of failure cases. The results show that different phases of diagnosis affect communication and computation overhead. In particular, centralizing the diagnosis disambiguation process is a key factor in reducing communications, while runtime is affected mainly by the amount of reasoning about other agents. These results contrast with previous work in disagreement detection (without diagnosis), in which distributed algorithms reduce communications.}
}
@article{BONANNO20091194,
title = {Rational choice and AGM belief revision},
journal = {Artificial Intelligence},
volume = {173},
number = {12},
pages = {1194-1203},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000617},
author = {Giacomo Bonanno},
keywords = {Rational choice, Revealed preference, Belief revision, Partial belief revision function, Choice function, Arrow's axiom},
abstract = {We establish a correspondence between the rationalizability of choice studied in the revealed preference literature and the notion of minimal belief revision captured by the AGM postulates. A choice frame consists of a set of alternatives Î©, a collection E of subsets of Î© (representing possible choice sets) and a function f:Eâ†’2Î© (representing choices made). A choice frame is rationalizable if there exists a total pre-order R on Î© such that, for every EâˆˆE, f(E) coincides with the best elements of E relative to R. We re-interpret choice structures in terms of belief revision. An interpretation is obtained by adding a valuation V that assigns to every atom p the subset of Î© at which p is true. Associated with an interpretation is an initial belief set and a partial belief revision function. A choice frame is AGM-consistent if, for every interpretation of it, the associated partial belief revision function can be extended to a full-domain belief revision function that satisfies the AGM postulates. It is shown that a finite choice frame is AGM-consistent if and only if it is rationalizable.}
}
@article{CIMATTI20151,
title = {An SMT-based approach to weak controllability for disjunctive temporal problems with uncertainty},
journal = {Artificial Intelligence},
volume = {224},
pages = {1-27},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000405},
author = {Alessandro Cimatti and Andrea Micheli and Marco Roveri},
keywords = {Weak controllability, Temporal problems, Satisfiability modulo theory, Strategy synthesis},
abstract = {The framework of temporal problems with uncertainty (TPU) is useful to express temporal constraints over a set of activities subject to uncertain (and uncontrollable) duration. In this work, we focus on the most general class of TPU, namely disjunctive TPU (DTPU), and consider the case of weak controllability, that allows one to model problems arising in practical scenarios (e.g. on-line scheduling). We first tackle the decision problem, i.e. whether there exists a schedule of the activities that, depending on the uncertainty, satisfies all the constraints. We propose a logical approach, based on the reduction to a problem of Satisfiability Modulo Theories (SMT), in the theory of Linear Real Arithmetic with Quantifiers. This results in the first implemented solver for weak controllability of DTPUs. Then, we tackle the problem of synthesizing control strategies for scheduling the activities. We focus on strategies that are amenable for efficient execution. We prove that linear strategies are not always sufficient, even in the sub-case of simple TPU (STPU), while piecewise-linear strategies, that are multiple conditionally-applied linear strategies, are always sufficient. We present several algorithms for the synthesis of linear and piecewise-linear strategies, in case of STPU and of DTPU. All the algorithms are implemented on top of SMT solvers. We provide experimental evidence of the scalability of the proposed techniques, with dramatic speed-ups in strategy execution compared to on-line reasoning.}
}
@article{SCHWARZ199539,
title = {In search of a â€œtrueâ€ logic of knowledge: the nonmonotonic perspective},
journal = {Artificial Intelligence},
volume = {79},
number = {1},
pages = {39-63},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00067-0},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000670},
author = {Grigori Schwarz},
abstract = {Modal logics are currently widely accepted as a suitable tool of knowledge representation, and the question what logics are better suited for representing knowledge is of particular importance. Usually, some axiom list is given, and arguments are presented justifying that suggested axioms agree with intuition. The question why the suggested axioms describe all the desired properties of knowledge remains answered only partially, by showing that the most obvious and popular additional axioms would violate the intuition. We suggest the general paradigm of maximal logics and demonstrate how it can work for nonmonotonic modal logics. Technically, we prove that each of the modal logics KD45, SW5, S4F and S4.2 is the strongest modal logic among the logics generating the same nonmonotonic logic. These logics have already found important applications in knowledge representation, and the obtained results contribute to the explanation of this fact.}
}
@article{JONSSON2017115,
title = {An initial study of time complexity in infinite-domain constraint satisfaction},
journal = {Artificial Intelligence},
volume = {245},
pages = {115-133},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300061},
author = {Peter Jonsson and Victor Lagerkvist},
keywords = {Constraint satisfaction, Infinite domain, Time complexity},
abstract = {The constraint satisfaction problem (CSP) is a widely studied problem with numerous applications in computer science and artificial intelligence. For infinite-domain CSPs, there are many results separating tractable and NP-hard cases while upper and lower bounds on the time complexity of hard cases are virtually unexplored. Hence, we initiate a study of the worst-case time complexity of such CSPs. We analyze backtracking algorithms and determine upper bounds on their time complexity. We present asymptotically faster algorithms based on enumeration techniques and we show that these algorithms are applicable to well-studied problems in, for instance, temporal reasoning. Finally, we prove non-trivial lower bounds applicable to many interesting CSPs, under the assumption that certain complexity-theoretic assumptions hold. The gap between upper and lower bounds is in many cases surprisingly small, which suggests that our upper bounds cannot be significantly improved.}
}
@article{BAUMGARTNER2008591,
title = {The model evolution calculus as a first-order DPLL method},
journal = {Artificial Intelligence},
volume = {172},
number = {4},
pages = {591-632},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001336},
author = {Peter Baumgartner and Cesare Tinelli},
keywords = {DPLL procedure, First-order logic, Sequent calculi, Model generation},
abstract = {The DPLL procedure is the basis of some of the most successful propositional satisfiability solvers to date. Although originally devised as a proof-procedure for first-order logic, it has been used almost exclusively for propositional logic so far because of its highly inefficient treatment of quantifiers, based on instantiation into ground formulas. The FDPLL calculus by Baumgartner was the first successful attempt to lift the procedure to the first-order level without resorting to ground instantiations. FDPLL lifts to the first-order case the core of the DPLL procedure, the splitting rule, but ignores other aspects of the procedure that, although not necessary for completeness, are crucial for its effectiveness in practice. In this paper, we present a new calculus loosely based on FDPLL that lifts these aspects as well. In addition to being a more faithful lifting of the DPLL procedure, the new calculus contains a more systematic treatment of universal literals, which are crucial to achieve efficiency in practice. The new calculus has been implemented successfully in the Darwin system, described elsewhere. The main results of this paper are theoretical, showing the soundness and completeness of the new calculus. In addition, the paper provides a high-level description of a proof procedure for the calculus, as well as a comparison with other calculi.}
}
@article{BARAL20081429,
title = {Maintenance goals of agents in a dynamic environment: Formulation and policy construction},
journal = {Artificial Intelligence},
volume = {172},
number = {12},
pages = {1429-1469},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000416},
author = {Chitta Baral and Thomas Eiter and Marcus BjÃ¤reland and Mutsumi Nakamura},
keywords = {Maintenance goals, k-maintainability, Agent control, Computational complexity of agent design, Answer set programming, Horn theories, SAT solving, Discrete event dynamic systems, Self-stabilization},
abstract = {The notion of maintenance often appears in the AI literature in the context of agent behavior and planning. In this paper, we argue that earlier characterizations of the notion of maintenance are not intuitive to characterize the maintenance behavior of certain agents in a dynamic environment. We propose a different characterization of maintenance and distinguish it from earlier notions such as stabilizability. Our notion of maintenance is more sensitive to a good-natured agent which struggles with an â€œadversaryâ€ environment, which hinders her by unforeseeable events to reach her goals (not in principle, but in case). It has a parameter k, referring to the length of non-interference (from exogenous events) needed to maintain a goal; we refer to this notion as k-maintainability. We demonstrate the notion on examples, and address the important but non-trivial issue of efficient construction of maintainability control functions. We present an algorithm which in polynomial time constructs a k-maintainable control function, if one exists, or tells that no such control is possible. Our algorithm is based on SAT Solving, and employs a suitable formulation of the existence of k-maintainable control in a fragment of SAT which is tractable. For small k (bounded by a constant), our algorithm is linear time. We then give a logic programming implementation of our algorithm and use it to give a standard procedural algorithm, and analyze the complexity of constructing k-maintainable controls, under different assumptions such as k=1, and states described by variables. On the one hand, our work provides new concepts and algorithms for maintenance in dynamic environment, and on the other hand, a very fruitful application of computational logic tools. We compare our work with earlier works on control synthesis from temporal logic specification and relate our work to Dijkstra's notion of self-stabilization and related notions in distributed computing.}
}
@article{LARSSON199629,
title = {Diagnosis based on explicit means-end models},
journal = {Artificial Intelligence},
volume = {80},
number = {1},
pages = {29-93},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00043-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000433},
author = {Jan Eric Larsson},
abstract = {This article describes three diagnostic methods for use with industrial processes. They are measurement validation, i.e., consistency checking of sensor and measurement values using any redundancy of instrumentation; alarm analysis, i.e., analysis of multiple alarm situations to find which alarms are directly connected to primary faults and which alarms are consequential effects of the primary ones; and fault diagnosis, i.e., a search for the causes of and remedies for faults. The three methods use multilevel flow models (MFM), to describe the target process. They have been implemented in the real-time expert system tool G2, in C, and in Common Lisp, and successfully tested on simulations of several processes. The knowledge representation ontology used is based on the notion of flows, of mass, energy, and information, which are used to describe physical systems. The relationships between structure and function of a system is described by teleological relations, which connect the flow structures into a graph, built at model construction time. This allows the diagnostic reasoning to be implemented as searches in a static graph structure, and it can thus be performed extremely rapidly. As with other model-based approaches, general algorithms are used over a representation with generative capacities. The representation gains strength from being functional with a very abstract physical level, more abstract than most qualitative physics models. It works well with systems that can be described using flows, while it currently lacks the capability of capturing important aspects of other types of systems, for example, electronic circuits.}
}
@article{JAEGER201330,
title = {Type Extension Trees for feature construction and learning in relational domains},
journal = {Artificial Intelligence},
volume = {204},
pages = {30-55},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437021300074X},
author = {Manfred Jaeger and Marco Lippi and Andrea Passerini and Paolo Frasconi},
keywords = {Statistical relational learning, Inductive logic programming, Feature discovery},
abstract = {Type Extension Trees are a powerful representation language for â€œcount-of-countâ€ features characterizing the combinatorial structure of neighborhoods of entities in relational domains. In this paper we present a learning algorithm for Type Extension Trees (TET) that discovers informative count-of-count features in the supervised learning setting. Experiments on bibliographic data show that TET-learning is able to discover the count-of-count feature underlying the definition of the h-index, and the inverse document frequency feature commonly used in information retrieval. We also introduce a metric on TET feature values. This metric is defined as a recursive application of the Wassersteinâ€“Kantorovich metric. Experiments with a k-NN classifier show that exploiting the recursive count-of-count statistics encoded in TET values improves classification accuracy over alternative methods based on simple count statistics.}
}
@article{ABREU20101481,
title = {Diagnosing multiple intermittent failures using maximum likelihood estimation},
journal = {Artificial Intelligence},
volume = {174},
number = {18},
pages = {1481-1497},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001529},
author = {Rui Abreu and Arjan J.C. {van Gemund}},
keywords = {Fault diagnosis, Bayesian reasoning, Maximum likelihood estimation},
abstract = {In fault diagnosis intermittent failure models are an important tool to adequately deal with realistic failure behavior. Current model-based diagnosis approaches account for the fact that a component cj may fail intermittently by introducing a parameter gj that expresses the probability the component exhibits correct behavior. This component parameter gj, in conjunction with a priori fault probability, is used in a Bayesian framework to compute the posterior fault candidate probabilities. Usually, information on gj is not known a priori. While proper estimation of gj can be critical to diagnostic accuracy, at present, only approximations have been proposed. We present a novel framework, coined Barinel, that computes estimations of the gj as integral part of the posterior candidate probability computation using a maximum likelihood estimation approach. Barinel's diagnostic performance is evaluated for both synthetic systems, the Siemens software diagnosis benchmark, as well as for real-world programs. Our results show that our approach is superior to reasoning approaches based on classical persistent failure models, as well as previously proposed intermittent failure models.}
}
@article{OOMMEN20051,
title = {A formal analysis of why heuristic functions work},
journal = {Artificial Intelligence},
volume = {164},
number = {1},
pages = {1-22},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2002.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000214},
author = {B. John Oommen and Luis G. Rueda},
keywords = {A* algorithms, Heuristic algorithms, Pattern recognition, Optimization},
abstract = {Many optimization problems in computer science have been proven to be NP-hard, and it is unlikely that polynomial-time algorithms that solve these problems exist unless P=NP. Alternatively, they are solved using heuristics algorithms, which provide a sub-optimal solution that, hopefully, is arbitrarily close to the optimal. Such problems are found in a wide range of applications, including artificial intelligence, game theory, graph partitioning, database query optimization, etc. Consider a heuristic algorithm, A. Suppose that A could invoke one of two possible heuristic functions. The question of determining which heuristic function is superior, has typically demanded a yes/no answerâ€”one which is often substantiated by empirical evidence. In this paper, by using Pattern Classification Techniques (PCT), we propose a formal, rigorous theoretical model that provides a stochastic answer to this problem. We prove that given a heuristic algorithm, A, that could utilize either of two heuristic functions H1 or H2 used to find the solution to a particular problem, if the accuracy of evaluating the cost of the optimal solution by using H1 is greater than the accuracy of evaluating the cost using H2, then H1 has a higher probability than H2 of leading to the optimal solution. This unproven conjecture has been the basis for designing numerous algorithms such as the A* algorithm, and its variants. Apart from formally proving the result, we also address the corresponding database query optimization problem that has been open for at least two decades. To validate our proofs, we report empirical results on database query optimization techniques involving a few well-known histogram estimation methods.}
}
@article{OLSSON199555,
title = {Inductive functional programming using incremental program transformation},
journal = {Artificial Intelligence},
volume = {74},
number = {1},
pages = {55-81},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00042-Y},
url = {https://www.sciencedirect.com/science/article/pii/000437029400042Y},
author = {Roland Olsson},
abstract = {The paper presents a system, ADATE, for automatic functional programming. ADATE uses specifications that contain few constraints on the programs to be synthesized and that allow a wide range of correct programs. ADATE can generate novel and unexpected recursive programs with automatic invention of recursive auxiliary functions. Successively better programs are developed using incremental program transformations. A key to the success of ADATE is the exact design of these transformations, and how to systematically search for appropriate transformation sequences.}
}
@article{NETZER2012186,
title = {Concurrent forward bounding for distributed constraint optimization problems},
journal = {Artificial Intelligence},
volume = {193},
pages = {186-216},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212001087},
author = {Arnon Netzer and Alon Grubshtein and Amnon Meisels},
keywords = {Distributed constraint optimization problems, Algorithms, ConcFB},
abstract = {A distributed search algorithm for solving Distributed Constraints Optimization Problems (DCOPs) is presented. The new algorithm scans the search space by using multiple search processes (SPs) that run on all agents concurrently. SPs search in non-intersecting parts of the global search space and perform Branch & Bound search. Each search process (SP) uses the mechanism of forward bounding (FB) to prune efficiently its part of the global search space. The Concurrent Forward-Bounding (ConcFB) algorithm enables all SPs to share their upper bound across all parts of the global search space. The number of concurrent SPs is controlled dynamically by the ConcFB algorithm, by performing dynamic splitting. Within each SP a dynamic variable ordering is employed in order to help control the balance of computational load among all agents and across different SPs. The ConcFB algorithm is evaluated experimentally and compared to all state of the art DCOP algorithms. The number of Non-Concurrent Logical Operations, Non-Concurrent Steps, the total number of messages sent and CPU time are used as performance metrics. The evaluation procedure considers different DCOP problem types with a varying number of agents and different constraint graphs. As problems become larger and denser, ConcFB is shown to outperform all other evaluated algorithms by 2â€“3 orders of magnitude in all performance measures. Further evaluations comparing different variants of ConcFB provide important insights into the working of the algorithm and reveals the contribution of its different components.}
}
@article{LECOUTRE20151,
title = {STR3: A path-optimal filtering algorithm for table constraints},
journal = {Artificial Intelligence},
volume = {220},
pages = {1-27},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437021400143X},
author = {Christophe Lecoutre and Chavalit Likitvivatanavong and Roland H.C. Yap},
keywords = {Constraint satisfaction problems, Generalized arc consistency, Non-binary constraints, Backtracking search},
abstract = {Constraint propagation is a key to the success of Constraint Programming (CP). The principle is that filtering algorithms associated with constraints are executed in sequence until quiescence is reached. Many such algorithms have been proposed over the years to enforce the property called Generalized Arc Consistency (GAC) on many types of constraints, including table constraints that are defined extensionally. Recent advances in GAC algorithms for extensional constraints rely on directly manipulating tables during search. This is the case with a simple approach called Simple Tabular Reduction (STR), which systematically maintains tables of constraints to their relevant lists of tuples. In particular, STR2, a refined STR variant is among the most efficient GAC algorithms for positive table constraints. In this paper, we revisit this approach by proposing a new GAC algorithm called STR3 that is specifically designed to enforce GAC during backtrack search. By indexing tables and reasoning from deleted values, we show that STR3 can avoid systematically iterating over the full set of current tuples, contrary to STR2. An important property of STR3 is that it can completely avoid unnecessary traversal of tables, making it optimal along any path of the search tree. We also study a variant of STR3, based on an optimal circular way for traversing tables, and discuss the relationship of STR3 with two other optimal GAC algorithms introduced in the literature, namely, GAC4 and AC5TC-Tr. Finally, we demonstrate experimentally how STR3 is competitive with the state-of-the-art. In particular, our extensive experiments show that STR3 is generally faster than STR2 when the average size of tables is not reduced too drastically during search, making STR3 complementary to STR2.}
}
@article{WILSON20111053,
title = {Computational techniques for a simple theory of conditional preferences},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1053-1091},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002079},
author = {Nic Wilson},
keywords = {Conditional preferences, Comparative preferences,  preferences, CP-nets, TCP-nets, Constrained optimisation, Lexicographic preferences},
abstract = {A simple logic of conditional preferences is defined, with a language that allows the compact representation of certain kinds of conditional preference statements, a semantics and a proof theory. CP-nets and TCP-nets can be mapped into this logic, and the semantics and proof theory generalise those of CP-nets and TCP-nets. The system can also express preferences of a lexicographic kind. The paper derives various sufficient conditions for a set of conditional preferences to be consistent, along with algorithmic techniques for checking such conditions and hence confirming consistency. These techniques can also be used for totally ordering outcomes in a way that is consistent with the set of preferences, and they are further developed to give an approach to the problem of constrained optimisation for conditional preferences.}
}
@article{EPSTEIN1998275,
title = {Pragmatic navigation: reactivity, heuristics, and search},
journal = {Artificial Intelligence},
volume = {100},
number = {1},
pages = {275-322},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00083-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000830},
author = {Susan L. Epstein},
keywords = {AI architectures, Heuristic search, Machine learning, Multistrategy learning, Navigation, Problem solving, Satisficing, Situation-based search, Spatial representation},
abstract = {FORR (FOr the Right Reasons) is an architecture for learning and problem solving that integrates a possibly incomplete and overlapping set of solution methods to address complex problems. Each method, although it represents some facet of domain expertise, may vary in reliability and speed. The principal contribution of this paper is the extension of FORR to include situation-based behavior (the serial testing of known, triggered techniques for problem solving in a domain) with reactivity and heuristic reasoning. FORR categorizes methods as reactive, heuristic, or situationbased, and addresses problem solving with one category of methods at a time. A hierarchical reasoner first has the opportunity to react correctly. If no ready reaction is computed, the reasoner activates a set of reactive triggers for time-limited search procedures tailored to specific situations. If they, too, fail to produce a response, the reasoner resorts to collaboration among heuristic rationales. All three components reference knowledge learned from experience. In a series of experiments, this architecture is shown to be effective and efficient. Ablation experiments demonstrate how each component plays an important role in problem solving. Additional contributions of this paper include a FORR-based, pragmatic, cognitively plausible approach to navigation with learned heuristic approximations that describe two-dimensional territory and travel experience through it, and a careful study of how situation-based behavior, reactivity, and heuristics interact there. Empirical evidence demonstrates that the resultant system is both effective and efficient, and guidelines for generalization to other domains are provided.}
}
@article{DUNNE2011457,
title = {Weighted argument systems: Basic definitions, algorithms, and complexity results},
journal = {Artificial Intelligence},
volume = {175},
number = {2},
pages = {457-486},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001542},
author = {Paul E. Dunne and Anthony Hunter and Peter McBurney and Simon Parsons and Michael Wooldridge},
keywords = {Argumentation, Handling inconsistency, Computational complexity},
abstract = {We introduce and investigate a natural extension of Dung's well-known model of argument systems in which attacks are associated with a weight, indicating the relative strength of the attack. A key concept in our framework is the notion of an inconsistency budget, which characterises how much inconsistency we are prepared to tolerate: given an inconsistency budget Î², we would be prepared to disregard attacks up to a total weight of Î². The key advantage of this approach is that it permits a much finer grained level of analysis of argument systems than unweighted systems, and gives useful solutions when conventional (unweighted) argument systems have none. We begin by reviewing Dung's abstract argument systems, and motivating weights on attacks (as opposed to the alternative possibility, which is to attach weights to arguments). We then present the framework of weighted argument systems. We investigate solutions for weighted argument systems and the complexity of computing such solutions, focussing in particular on weighted variations of grounded extensions. Finally, we relate our work to the most relevant examples of argumentation frameworks that incorporate strengths.}
}
@article{POLLOCK1998267,
title = {The logical foundations of goal-regression planning in autonomous agents},
journal = {Artificial Intelligence},
volume = {106},
number = {2},
pages = {267-334},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00100-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298001003},
author = {John L. Pollock},
keywords = {Autonomous agents, Defeasible reasoning, Goal regression, OSCAR, Planning},
abstract = {This paper addresses the logical foundations of goal-regression planning in autonomous rational agents. It focuses mainly on three problems. The first is that goals and subgoals will often be conjunctions, and to apply goal-regression planning to a conjunction we usually have to plan separately for the conjuncts and then combine the resulting subplans. A logical problem arises from the fact that the subplans may destructively interfere with each other. This problem has been partially solved in the AI literature (e.g., in SNLP and UCPOP), but the solutions proposed there work only when a restrictive assumption is satisfied. This assumption pertains to the computability of threats. It is argued that this assumption may fail for an autonomous rational agent operating in a complex environment. Relaxing this assumption leads to a theory of defeasible planning. The theory is formulated precisely and an implementation in the OSCAR architecture is discussed. The second problem is that goal-regression planning proceeds in terms of reasoning that runs afoul of the Frame Problem. It is argued that a previously proposed solution to the Frame Problem legitimizes goal-regression planning, but also has the consequence that some restrictions must be imposed on the logical form of goals and subgoals amenable to such planning. These restrictions have to do with temporal-projectibility. The third problem is that the theory of goal-regression planning found in the AI literature imposes restrictive syntactical constraints on goals and subgoals and on the relation of logical consequence. Relaxing these restrictions leads to a generalization of the notion of a threat, related to collective defeat in defeasible reasoning. Relaxing the restrictions also has the consequence that the previously adequate definition of â€œexpectable-resultâ€ no longer guarantees closure under logical consequence, and must be revised accordingly. That in turn leads to the need for an additional rule for goal-regression planning. Roughly, the rule allows us to plan for the achievement of a goal by searching for plans that will achieve states that â€œcauseâ€ the goal. Such a rule was not previously necessary, but becomes necessary when the syntactical constraints are relaxed. The final result is a general semantics for goal-regression planning and a set of procedures that is provably sound and complete. It is shown that this semantics can easily handle concurrent actions, quantified preconditions and effects, creation and destruction of objects, and causal connections embodying complex temporal relationships.}
}
@article{BIENVENU20111308,
title = {Specifying and computing preferred plans},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1308-1345},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002109},
author = {Meghyn Bienvenu and Christian Fritz and Sheila A. McIlraith},
keywords = {Knowledge representation, Preferences, Planning with preferences},
abstract = {In this paper, we address the problem of specifying and computing preferred plans using rich, qualitative, user preferences. We propose a logical language for specifying preferences over the evolution of states and actions associated with a plan. We provide a semantics for our first-order preference language in the situation calculus, and prove that progression of our preference formulae preserves this semantics. This leads to the development of PPlan, a bounded best-first search planner that computes preferred plans. Our preference language is amenable to integration with many existing planners, and beyond planning, can be used to support a diversity of dynamical reasoning tasks that employ preferences.}
}
@article{CARENINI2006925,
title = {Generating and evaluating evaluative arguments},
journal = {Artificial Intelligence},
volume = {170},
number = {11},
pages = {925-952},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S000437020600066X},
author = {Giuseppe Carenini and Johanna D. Moore},
keywords = {Natural language generation, User tailoring, Preferences, Empirical evaluation},
abstract = {Evaluative arguments are pervasive in natural human communication. In countless situations people attempt to advise or persuade their interlocutors that something is desirable (vs. undesirable) or right (vs. wrong). With the proliferation of on-line systems serving as personal advisors and assistants, there is a pressing need to develop general and testable computational models for generating and presenting evaluative arguments. Previous research on generating evaluative arguments has been characterized by two major limitations. First, researchers have tended to focus only on specific aspects of the generation process. Second, the proposed approaches were not empirically tested. The research presented in this paper addresses both limitations. We have designed and implemented a complete computational model for generating evaluative arguments. For content selection and organization, we devised an argumentation strategy based on guidelines from argumentation theory. For expressing the content in natural language, we extended and integrated previous work in computational linguistics on generating evaluative arguments. The key knowledge source for both tasks is a quantitative model of user preferences. To empirically test critical aspects of our generation model, we have devised and implemented an evaluation framework in which the effectiveness of evaluative arguments can be measured with real users. Within the framework, we have performed an experiment to test two basic hypotheses on which the design of the computational model is based; namely, that our proposal for tailoring an evaluative argument to the addressee's preferences increases its effectiveness, and that differences in conciseness significantly influence argument effectiveness. The second hypothesis was confirmed in the experiment. In contrast, the first hypothesis was only marginally confirmed. However, independent testing by other researchers has recently provided further support for this hypothesis.}
}
@article{KAKAS201149,
title = {Modular-E and the role of elaboration tolerance in solving the qualification problem},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {49-78},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000445},
author = {Antonis Kakas and Loizos Michael and Rob Miller},
keywords = {Reasoning about actions, Elaboration tolerance, Qualification problem, Default reasoning},
abstract = {We describe Modular-E (ME), a specialized, model-theoretic logic for reasoning about actions. ME is able to represent non-deterministic domains involving concurrency, static laws (constraints), indirect effects (ramifications), and narrative information in the form of action occurrences and observations along a time line. We give formal results which characterize ME's high degree of modularity and elaboration tolerance, and show how these properties help to separate out, and provide principled solutions to, different aspects of the qualification problem. In particular, we identify the endogenous qualification problem as the problem of properly accounting for highly distributed, and potentially conflicting, causal knowledge when reasoning about the effects of actions. We show how a comprehensive solution to the endogenous qualification problem helps simplify the exogenous qualification problem â€” the problem of reconciling conflicts between predictions about what should be true at particular times and actual observations. More precisely, we describe how ME is able to use straightforward default reasoning techniques to solve the exogenous qualification problem largely because its robust treatments of the frame, ramification and endogenous qualification problems combine into a particular characteristic of elaboration tolerance that we formally encapsulate as a notion of â€œfree willâ€.}
}
@article{KOMPELLA2017313,
title = {Continual curiosity-driven skill acquisition from high-dimensional video inputs for humanoid robots},
journal = {Artificial Intelligence},
volume = {247},
pages = {313-335},
year = {2017},
note = {Special Issue on AI and Robotics},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S000437021500017X},
author = {Varun Raj Kompella and Marijn Stollenga and Matthew Luciw and Juergen Schmidhuber},
keywords = {Reinforcement learning, Artificial curiosity, Skill acquisition, Slow feature analysis, Continual learning, Incremental learning, iCub},
abstract = {In the absence of external guidance, how can a robot learn to map the many raw pixels of high-dimensional visual inputs to useful action sequences? We propose here Continual Curiosity driven Skill Acquisition (CCSA). CCSA makes robots intrinsically motivated to acquire, store and reuse skills. Previous curiosity-based agents acquired skills by associating intrinsic rewards with world model improvements, and used reinforcement learning to learn how to get these intrinsic rewards. CCSA also does this, but unlike previous implementations, the world model is a set of compact low-dimensional representations of the streams of high-dimensional visual information, which are learned through incremental slow feature analysis. These representations augment the robot's state space with new information about the environment. We show how this information can have a higher-level (compared to pixels) and useful interpretation, for example, if the robot has grasped a cup in its field of view or not. After learning a representation, large intrinsic rewards are given to the robot for performing actions that greatly change the feature output, which has the tendency otherwise to change slowly in time. We show empirically what these actions are (e.g., grasping the cup) and how they can be useful as skills. An acquired skill includes both the learned actions and the learned slow feature representation. Skills are stored and reused to generate new observations, enabling continual acquisition of complex skills. We present results of experiments with an iCub humanoid robot that uses CCSA to incrementally acquire skills to topple, grasp and pick-place a cup, driven by its intrinsic motivation from raw pixel vision.}
}
@article{NASTASE201362,
title = {Transforming Wikipedia into a large scale multilingual concept network},
journal = {Artificial Intelligence},
volume = {194},
pages = {62-85},
year = {2013},
note = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000781},
author = {Vivi Nastase and Michael Strube},
keywords = {Knowledge base, Multilinguality, Knowledge acquisition},
abstract = {A knowledge base for real-world language processing applications should consist of a large base of facts and reasoning mechanisms that combine them to induce novel and more complex information. This paper describes an approach to deriving such a large scale and multilingual resource by exploiting several facets of the on-line encyclopedia Wikipedia. We show how we can build upon WikipediaÊ¼s existing network of categories and articles to automatically discover new relations and their instances. Working on top of this network allows for added information to influence the network and be propagated throughout it using inference mechanisms that connect different pieces of existing knowledge. We then exploit this gained information to discover new relations that refine some of those found in the previous step. The result is a network containing approximately 3.7 million concepts with lexicalizations in numerous languages and 49+ million relation instances. Intrinsic and extrinsic evaluations show that this is a high quality resource and beneficial to various NLP tasks.}
}
@article{LATOUR2022103650,
title = {Exact stochastic constraint optimisation with applications in network analysis},
journal = {Artificial Intelligence},
volume = {304},
pages = {103650},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103650},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221002010},
author = {Anna L.D. Latour and Behrouz Babaki and DaniÃ«l Fokkinga and Marie Anastacio and Holger H. Hoos and Siegfried Nijssen},
keywords = {Constraint programming, Probabilistic inference, Stochastic constraints, Ordered binary decision diagrams, Monotonic probability distributions, Global constraints, Automated algorithm configuration, Probabilistic networks},
abstract = {We present an extensive study of methods for exactly solving stochastic constraint (optimisation) problems (SCPs) in network analysis. These problems are prevalent in science, governance and industry. The first method we study is generic and decomposes stochastic constraints into a multitude of smaller local constraints that are solved using a constraint programming (CP) or mixed-integer programming (MIP) solver. However, many SCPs are formulated on probability distributions with a monotonic property, meaning that adding a positive decision to a partial solution to the problem cannot cause a decrease in solution quality. The second method is specifically designed for solving global stochastic constraints on monotonic probability distributions (SCMDs) in CP. Both methods use knowledge compilation to obtain a decision diagram encoding of the relevant probability distributions, where we focus on ordered binary decision diagrams (OBDDs). We discuss theoretical advantages and disadvantages of these methods and evaluate them experimentally. We observed that global approaches to solving SCMDs outperform decomposition approaches from CP, and perform complementarily to MIP-based decomposition approaches, while scaling much more favourably with instance size. Both methods have many alternative design choices, as both knowledge compilation and constraint solvers are used in a single pipeline. To identify which configurations work best, we apply programming by optimisation. Specifically, we show how an automated algorithm configurator can be used to find optimised configurations of our pipeline. After configuration, our global SCMD solving pipeline outperforms its closest competitor (a MIP-based decomposition pipeline) on all test sets we considered by up to two orders of magnitude in terms of PAR10 scores.}
}
@article{HACHEY2013130,
title = {Evaluating Entity Linking with Wikipedia},
journal = {Artificial Intelligence},
volume = {194},
pages = {130-150},
year = {2013},
note = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000446},
author = {Ben Hachey and Will Radford and Joel Nothman and Matthew Honnibal and James R. Curran},
keywords = {Named Entity Linking, Disambiguation, Information extraction, Wikipedia, Semi-structured resources},
abstract = {Named Entity Linking (nel) grounds entity mentions to their corresponding node in a Knowledge Base (kb). Recently, a number of systems have been proposed for linking entity mentions in text to Wikipedia pages. Such systems typically search for candidate entities and then disambiguate them, returning either the best candidate or nil. However, comparison has focused on disambiguation accuracy, making it difficult to determine how search impacts performance. Furthermore, important approaches from the literature have not been systematically compared on standard data sets. We reimplement three seminal nel systems and present a detailed evaluation of search strategies. Our experiments find that coreference and acronym handling lead to substantial improvement, and search strategies account for much of the variation between systems. This is an interesting finding, because these aspects of the problem have often been neglected in the literature, which has focused largely on complex candidate ranking algorithms.}
}
@article{DONALD1995217,
title = {On information invariants in robotics},
journal = {Artificial Intelligence},
volume = {72},
number = {1},
pages = {217-304},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00024-U},
url = {https://www.sciencedirect.com/science/article/pii/000437029400024U},
author = {Bruce Randall Donald},
abstract = {We consider the problem of determining the information requirements to perform robot tasks, using the concept of information invariants. This paper represents our attempt to characterize a family of complicated and subtle issues concerned with measuring robot task complexity. We also provide a first approximation to a purely operational theory that addresses a narrow but interesting special case. We discuss several measures for the information complexity of a task: (a) How much internal state should the robot retain? (b) How many cooperating agents are required, and how much communication between them is necessary? (c) How can the robot change (side-effect) the environment in order to record state or sensory information to perform a task? (d) How much information is provided by sensors? and (e) How much computation is required by the robot? We consider how one might develop a kind of â€œcalculusâ€ on (a)â€“(e) in order to compare the power of sensor systems analytically. To this end, we attempt to develop a notion of information invariants. We develop a theory whereby one sensor can be â€œreducedâ€ to another (much in the spirit of computation-theoretic reductions), by adding, deleting, and reallocating (a)â€“(e) among collaborating autonomous agents.}
}
@article{HAMMOND2023103919,
title = {Reasoning about causality in games},
journal = {Artificial Intelligence},
volume = {320},
pages = {103919},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103919},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000656},
author = {Lewis Hammond and James Fox and Tom Everitt and Ryan Carey and Alessandro Abate and Michael Wooldridge},
keywords = {Causality, Game theory, Graphical models},
abstract = {Causal reasoning and game-theoretic reasoning are fundamental topics in artificial intelligence, among many other disciplines: this paper is concerned with their intersection. Despite their importance, a formal framework that supports both these forms of reasoning has, until now, been lacking. We offer a solution in the form of (structural) causal games, which can be seen as extending Pearl's causal hierarchy to the game-theoretic domain, or as extending Koller and Milch's multi-agent influence diagrams to the causal domain. We then consider three key questions:i)How can the (causal) dependencies in games â€“ either between variables, or between strategies â€“ be modelled in a uniform, principled manner?ii)How may causal queries be computed in causal games, and what assumptions does this require?iii)How do causal games compare to existing formalisms? To address question i), we introduce mechanised games, which encode dependencies between agents' decision rules and the distributions governing the game. In response to question ii), we present definitions of predictions, interventions, and counterfactuals, and discuss the assumptions required for each. Regarding question iii), we describe correspondences between causal games and other formalisms, and explain how causal games can be used to answer queries that other causal or game-theoretic models do not support. Finally, we highlight possible applications of causal games, aided by an extensive open-source Python library.}
}
@article{HOWARTH19985,
title = {Interpreting a dynamic and uncertain world: task-based control},
journal = {Artificial Intelligence},
volume = {100},
number = {1},
pages = {5-85},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00004-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000046},
author = {Richard J. Howarth},
keywords = {High-level computer vision, Surveillance, Attention, Event reasoning, Visual behaviour},
abstract = {In this paper we show that it can be beneficial to have a high-level vision component that guides the reasoning of the whole vision system when interpreting a dynamic and uncertain world. This guidance is provided by an attentional mechanism that exploits knowledge of the specific problem being solved. Here we develop a general framework for such an attentional mechanism and its application to understanding dynamic scenes. This attentional mechanism can enable a vision system to perform a given domain task while expending minimal resources. We have developed a component that uses Bayesian networks combined with a deictic representation to select what, when and how to use processed data from a fixed camera. We apply two forms of Bayesian network, which (1) create a dynamic structure to reflect the spatial organisation of the data and (2) measure task relatedness. Together these give attentional focus making the reasoning performed relevant to the task.}
}
@article{VANDERHOEK200581,
title = {On the logic of cooperation and propositional control},
journal = {Artificial Intelligence},
volume = {164},
number = {1},
pages = {81-119},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000032},
author = {Wiebe {van der Hoek} and Michael Wooldridge},
keywords = {Multi-agent systems, Cooperation logic, Logics for control},
abstract = {Cooperation logics have recently begun to attract attention within the multi-agent systems community. Using a cooperation logic, it is possible to represent and reason about the strategic powers of agents and coalitions of agents in game-like multi-agent systems. These powers are generally assumed to be implicitly defined within the structure of the environment, and their origin is rarely discussed. In this paper, we study a cooperation logic in which agents are each assumed to control a set of propositional variablesâ€”the powers of agents and coalitions then derive from the allocation of propositions to agents. The basic modal constructs in this Coalition Logic of Propositional Control (cl-pc) allow us to express the fact that a group of agents can cooperate to bring about a certain state of affairs. After motivating and introducing cl-pc, we provide a complete axiom system for the logic, investigate the issue of characterising control in cl-pc with respect to the underlying power structures of the logic, and formally investigate the relationship between cl-pc and Pauly's Coalition Logic. We then show that the model checking and satisfiability problems for cl-pc are both pspace-complete, and conclude by discussing our results and how cl-pc sits in relation to other logics of cooperation.}
}
@article{DARWICHE19971,
title = {On the logic of iterated belief revision},
journal = {Artificial Intelligence},
volume = {89},
number = {1},
pages = {1-29},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00038-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000380},
author = {Adnan Darwiche and Judea Pearl},
keywords = {Iterated revision, AGM postulates, Conditional beliefs, Probabilistic conditioning, Epistemic states, Qualitative probability},
abstract = {We show in this paper that the AGM postulates are too weak to ensure the rational preservation of conditional beliefs during belief revision, thus permitting improper responses to sequences of observations. We remedy this weakness by proposing four additional postulates, which are sound relative to a qualitative version of probabilistic conditioning. Contrary to the AGM framework, the proposed postulates characterize belief revision as a process which may depend on elements of an epistemic state that are not necessarily captured by a belief set. We also show that a simple modification to the AGM framework can allow belief revision to be a function of epistemic states. We establish a model-based representation theorem which characterizes the proposed postulates and constrains, in turn, the way in which entrenchment orderings may be transformed under iterated belief revision.}
}
@article{SCHAERF1995249,
title = {Tractable reasoning via approximation},
journal = {Artificial Intelligence},
volume = {74},
number = {2},
pages = {249-310},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00009-P},
url = {https://www.sciencedirect.com/science/article/pii/000437029400009P},
author = {Marco Schaerf and Marco Cadoli},
abstract = {Problems in logic are well known to be hard to solve in the worst case. Two different strategies for dealing with this aspect are known from the literature: language restriction and theory approximation. In this paper we are concerned with the second strategy. Our main goal is to define a semantically well-founded logic for approximate reasoning, which is justifiable from the intuitive point of view, and to provide fast algorithms for dealing with it even when using expressive languages. We also want our logic to be useful to perform approximate reasoning in different contexts. We define a method for the approximation of decision reasoning problems based on multivalued logics. Our work expands and generalizes, in several directions, ideas presented by other researchers. The major features of our technique are: (1) approximate answers give semantically clear information about the problem at hand; (2) approximate answers are easier to compute than answers to the original problem; (3) approximate answers can be improved, and eventually they converge to the right answer; (4) both sound approximations and complete ones are described. The method we propose is flexible enough to be applied to a wide range of reasoning problems. In our research we considered approximation of several decidable problems with different worstcase complexity, involving both propositional and first-order languages. In particular we defined approximation techniques for: propositional logic, fragments of first-order logic (concept description languages) and modal logic. In our research we also addressed the issue of representing the knowledge of a reasoner with limited resources and how to use such a knowledge for approximate reasoning purposes.}
}
@article{PAN201339,
title = {Transfer learning in heterogeneous collaborative filtering domains},
journal = {Artificial Intelligence},
volume = {197},
pages = {39-55},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2013.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370213000143},
author = {Weike Pan and Qiang Yang},
keywords = {Transfer learning, Collaborative filtering, Missing ratings},
abstract = {A major challenge for collaborative filtering (CF) techniques in recommender systems is the data sparsity that is caused by missing and noisy ratings. This problem is even more serious for CF domains where the ratings are expressed numerically, e.g. as 5-star grades. We assume the 5-star ratings are unordered bins instead of ordinal relative preferences. We observe that, while we may lack the information in numerical ratings, we sometimes have additional auxiliary data in the form of binary ratings. This is especially true given that users can easily express themselves with their preferences expressed as likes or dislikes for items. In this paper, we explore how to use these binary auxiliary preference data to help reduce the impact of data sparsity for CF domains expressed in numerical ratings. We solve this problem by transferring the rating knowledge from some auxiliary data source in binary form (that is, likes or dislikes), to a target numerical rating matrix. In particular, our solution is to model both the numerical ratings and ratings expressed as like or dislike in a principled way. We present a novel framework of Transfer by Collective Factorization (TCF), in which we construct a shared latent space collectively and learn the data-dependent effect separately. A major advantage of the TCF approach over the previous bilinear method of collective matrix factorization is that we are able to capture the data-dependent effect when sharing the data-independent knowledge. This allows us to increase the overall quality of knowledge transfer. We present extensive experimental results to demonstrate the effectiveness of TCF at various sparsity levels, and show improvements of our approach as compared to several state-of-the-art methods.}
}
@article{WAH2006187,
title = {Constraint partitioning in penalty formulations for solving temporal planning problems},
journal = {Artificial Intelligence},
volume = {170},
number = {3},
pages = {187-231},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205001128},
author = {Benjamin W. Wah and Yixin Chen},
keywords = {Constraint partitioning, Extended saddle-point condition, Penalty function, Local search, Mixed space planning, Nonlinear constraints, Temporal planning},
abstract = {In this paper, we study the partitioning of constraints in temporal planning problems formulated as mixed-integer nonlinear programming (MINLP) problems. Constraint partitioning is attractive because it leads to much easier subproblems, where each is a significant relaxation of the original problem. Moreover, each subproblem is very similar to the original problem and can be solved by any existing solver with little or no modification. Constraint partitioning, however, introduces global constraints that may be violated when subproblems are evaluated independently. To reduce the overhead in resolving such global constraints, we develop in this paper new conditions and algorithms for limiting the search space to be backtracked in each subproblem. Using a penalty formulation of a MINLP where the constraint functions of the MINLP are transformed into non-negative functions, we present a necessary and sufficient extended saddle-point condition (ESPC) for constrained local minimization. When the penalties are larger than some thresholds, our theory shows a one-to-one correspondence between a constrained local minimum of the MINLP and an extended saddle point of the penalty function. Hence, one way to find a constrained local minimum is to increase gradually the penalties of those violated constraints and to look for a local minimum of the penalty function using any existing algorithm until a solution to the constrained model is found. Next, we extend the ESPC to constraint-partitioned MINLPs and propose a partition-and-resolve strategy for resolving violated global constraints across subproblems. Using the discrete-space ASPEN and the mixed-space MIPS planners to solve subproblems, we show significant improvements on some planning benchmarks, both in terms of the quality of the plans generated and the execution times to find them.}
}
@article{COOPER20111555,
title = {Hybrid tractability of valued constraint problems},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1555-1569},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000348},
author = {Martin C. Cooper and Stanislav Å½ivnÃ½},
keywords = {Constraint optimisation, Computational complexity, Tractability, Soft constraints, Valued constraint satisfaction problems, Graphical models, Forbidden substructures},
abstract = {The constraint satisfaction problem (CSP) is a central generic problem in computer science and artificial intelligence: it provides a common framework for many theoretical problems as well as for many real-life applications. Valued constraint problems are a generalisation of the CSP which allow the user to model optimisation problems. Considerable effort has been made in identifying properties which ensure tractability in such problems. In this work, we initiate the study of hybrid tractability of valued constraint problems; that is, properties which guarantee tractability of the given valued constraint problem, but which do not depend only on the underlying structure of the instance (such as being tree-structured) or only on the types of valued constraints in the instance (such as submodularity). We present several novel hybrid classes of valued constraint problems in which all unary constraints are allowed, which include a machine scheduling problem, constraint problems of arbitrary arities with no overlapping nogoods, and the SoftAllDiff constraint with arbitrary unary valued constraints. An important tool in our investigation will be the notion of forbidden substructures.}
}
@article{GOLDING1996215,
title = {Improving accuracy by combining rule-based and case-based reasoning},
journal = {Artificial Intelligence},
volume = {87},
number = {1},
pages = {215-254},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00120-4},
url = {https://www.sciencedirect.com/science/article/pii/0004370295001204},
author = {Andrew R. Golding and Paul S. Rosenbloom},
abstract = {An architecture is presented for combining rule-based and case-based reasoning. The architecture is intended for domains that are understood reasonably well, but still imperfectly. It uses a set of rules, which are taken to be only approximately correct, to obtain a preliminary answer for a given problem; it then draws analogies from cases to handle exceptions to the rules. Having rules together with cases not only increases the architecture's domain coverage, it also allows innovative ways of doing case-based reasoning: the same rules that are used for rule-based reasoning are also used by the case-based component to do case indexing and case adaptation. The architecture was applied to the task of name pronunciation, and, with minimal knowledge engineering, was found to perform almost at the level of the best commercial systems. Moreover, its accuracy was found to exceed what it could have achieved with rules or cases alone, thus demonstrating the accuracy improvement afforded by combining rule-based and case-based reasoning.}
}
@article{MOHR1995213,
title = {Understanding positioning from multiple images},
journal = {Artificial Intelligence},
volume = {78},
number = {1},
pages = {213-238},
year = {1995},
note = {Special Volume on Computer Vision},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00035-6},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000356},
author = {Roger Mohr and Boubakeur Boufama and Pascal Brand},
abstract = {It is possible to recover the three-dimensional structure of a scene using only correspondences between images taken with uncalibrated cameras (faugeras 1992). The reconstruction obtained this way is only defined up to a projective transformation of the 3D space. However, this kind of structure allows some spatial reasoning such as finding a path. In order to perform more specific reasoning, or to perform work with a robot moving in Euclidean space, Euclidean or affine constraints have to be added to the camera observations. Such constraints arise from the knowledge of the scene: location of points, geometrical constraints on lines, etc. First, this paper presents a reconstruction method for the scene, then it discusses how the framework of projective geometry allows symbolic or numerical information about positions to be derived, and how knowledge about the scene can be used for computing symbolic or numerical relationships. Implementation issues and experimental results are discussed.}
}
@article{SKOWRON2016191,
title = {Finding a collective set of items: From proportional multirepresentation to group recommendation},
journal = {Artificial Intelligence},
volume = {241},
pages = {191-216},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216301096},
author = {Piotr Skowron and Piotr Faliszewski and JÃ©rÃ´me Lang},
keywords = {Proportional representation, Ordered weighted average, Chamberlinâ€“Courant rule, Computational complexity, Approximation, Elections, Voting},
abstract = {We consider the following problem: There is a set of items (e.g., movies) and a group of agents (e.g., passengers on a plane); each agent has some intrinsic utility for each of the items. Our goal is to pick a set of K items that maximize the total derived utility of all the agents (i.e., in our example we are to pick K movies that we put on the plane's entertainment system). However, the actual utility that an agent derives from a given item is only a fraction of its intrinsic one, and this fraction depends on how the agent ranks the item among the chosen, available, ones. We provide a formal specification of the model and provide concrete examples and settings where it is applicable. We show that the problem is hard in general, but we show a number of tractability results for its natural special cases.}
}
@article{GOTTLOB201442,
title = {The price of query rewriting in ontology-based data access},
journal = {Artificial Intelligence},
volume = {213},
pages = {42-59},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000459},
author = {Georg Gottlob and Stanislav Kikot and Roman Kontchakov and Vladimir Podolskii and Thomas Schwentick and Michael Zakharyaschev},
keywords = {Ontology, Datalog, Conjunctive query, Query rewriting, Succinctness, Boolean circuit, Monotone complexity},
abstract = {We give a solution to the succinctness problem for the size of first-order rewritings of conjunctive queries in ontology-based data access with ontology languages such as OWLâ€‰2â€‰QL, linear DatalogÂ± and sticky DatalogÂ±. We show that positive existential and nonrecursive datalog rewritings, which do not use extra non-logical symbols (except for intensional predicates in the case of datalog rewritings), suffer an exponential blowup in the worst case, while first-order rewritings can grow superpolynomially unless NPâŠ†P/poly. We also prove that nonrecursive datalog rewritings are in general exponentially more succinct than positive existential rewritings, while first-order rewritings can be superpolynomially more succinct than positive existential rewritings. On the other hand, we construct polynomial-size positive existential and nonrecursive datalog rewritings under the assumption that any data instance contains two fixed constants.}
}
@article{FICHTE201564,
title = {Backdoors to tractable answer set programming},
journal = {Artificial Intelligence},
volume = {220},
pages = {64-103},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214001428},
author = {Johannes Klaus Fichte and Stefan Szeider},
keywords = {Answer set programming, Backdoors, Computational complexity, Parameterized complexity, Kernelization},
abstract = {Answer Set Programming (ASP) is an increasingly popular framework for declarative programming that admits the description of problems by means of rules and constraints that form a disjunctive logic program. In particular, many AI problems such as reasoning in a nonmonotonic setting can be directly formulated in ASP. Although the main problems of ASP are of high computational complexity, complete for the second level of the Polynomial Hierarchy, several restrictions of ASP have been identified in the literature, under which ASP problems become tractable. In this paper we use the concept of backdoors to identify new restrictions that make ASP problems tractable. Small backdoors are sets of atoms that represent â€œclever reasoning shortcutsâ€ through the search space and represent a hidden structure in the problem input. The concept of backdoors is widely used in theoretical investigations in the areas of propositional satisfiability and constraint satisfaction. We show that it can be fruitfully adapted to ASP. We demonstrate how backdoors can serve as a unifying framework that accommodates several tractable restrictions of ASP known from the literature. Furthermore, we show how backdoors allow us to deploy recent algorithmic results from parameterized complexity theory to the domain of answer set programming.}
}
@article{MOFFITT20111390,
title = {On the modelling and optimization of preferences in constraint-based temporal reasoning},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1390-1409},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002055},
author = {Michael D. Moffitt},
keywords = {Preferences, Overconstrained problems, Constraint satisfaction, Optimization, Branch and bound, Temporal reasoning},
abstract = {In this paper, we consider both the modelling and optimization of preferences in problems of constraint-based temporal reasoning. The Disjunctive Temporal Problems with Preferences (DTPP) â€“ a formulation that combines the rich expressive power of the Disjunctive Temporal Problem with the introduction of metric preference functions â€“ is studied, and transformed into a corresponding constraint system that we name the Valued DTP (VDTP). We show that for a broad family of optimization criteria, the VDTP can express the same solution space as the DTPP, under the assumption of arbitrary piecewise-constant preference functions. We then generalize the powerful search strategies from decision-based DTP literature to accomplish the efficient optimization of temporal preferences. In contrast to the previous state-of-the-art system (which addresses the optimization of temporal preferences using a SAT formulation), we instead employ a meta-CSP search space that has traditionally been used to solve DTPs without preferences. Our approach supports a variety of objective functions (such as utilitarian optimality or maximin optimality) and can accommodate any compliant valuation structure. We also demonstrate that key pruning techniques commonly used for temporal satisfiability (particularly, the removal of subsumed variables and semantic branching) are naturally suited to prevent the exploration of redundant search nodes during optimization that may otherwise be encountered when resolving a typical VDTP derived from a DTPP. Finally, we present empirical results showing that an implementation of our approach consistently outperforms prior algorithms by orders of magnitude.}
}
@article{ZISSERMAN1995239,
title = {3D object recognition using invariance},
journal = {Artificial Intelligence},
volume = {78},
number = {1},
pages = {239-288},
year = {1995},
note = {Special Volume on Computer Vision},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00023-2},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000232},
author = {Andrew Zisserman and David Forsyth and Joseph Mundy and Charlie Rothwell and Jane Liu and Nic Pillow},
abstract = {The systems and concepts described in this paper document the evolution of the geometric invariance approach to object recognition over the last five years. Invariance overcomes one of the fundamental difficulties in recognising objects from images: that the appearance of an object depends on viewpoint. This problem is entirely avoided if the geometric description is unaffected by the imaging transformation. Such invariant descriptions can be measured from images without any prior knowledge of the position, orientation and calibration of the camera. These invariant measurements can be used to index a library of object models for recognition and provide a principled basis for the other stages of the recognition process such as feature grouping and hypothesis verification. Object models can be acquired directly from images, allowing efficient construction of model libraries without manual intervention. A significant part of the paper is a summary of recent results on the construction of invariants for 3D objects from a single perspective view. A proposed recognition architecture is described which enables the integration of multiple general object classes and provides a means for enforcing global scene consistency. Various criticisms of the invariant approach are articulated and addressed.}
}
@article{KANG201670,
title = {Diffusion centrality: A paradigm to maximize spread in social networks},
journal = {Artificial Intelligence},
volume = {239},
pages = {70-96},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300716},
author = {Chanhyun Kang and Sarit Kraus and Cristian Molinaro and Francesca Spezzano and V.S. Subrahmanian},
keywords = {Social networks, Diffusion model, Logic programming, Quantitative logic},
abstract = {We propose Diffusion Centrality (DC) in which semantic aspects of a social network are used to characterize vertices that are influential in diffusing a property p. In contrast to classical centrality measures, diffusion centrality of vertices varies with the property p, and depends on the diffusion model describing how p spreads. We show that DC applies to most known diffusion models including tipping, cascade, and homophilic models. We present a hypergraph-based algorithm (HyperDC) with many optimizations to exactly compute DC. However, HyperDC does not scale well to huge social networks (millions of vertices, tens of millions of edges). For scaling, we develop methods to coarsen a network and propose a heuristic algorithm called â€œCoarsened Back and Forthâ€ (CBAF) to compute the top-k vertices (having the highest diffusion centrality). We report on experiments comparing DC with classical centrality measures in terms of runtime and the â€œspreadâ€ achieved by the k most central vertices (using 7 real-world social networks and 3 different diffusion models). Our experiments show that DC produces higher quality results and is comparable to several centrality measures in terms of runtime.}
}
@article{DICKMANNS199849,
title = {Vehicles capable of dynamic vision: a new breed of technical beings?},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {49-76},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00071-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029800071X},
author = {Ernst D. Dickmanns},
keywords = {Machine vision, Autonomous vehicles, Mobile robots, Dynamic scene understanding, Image processing},
abstract = {A survey is given on two decades of developments in the field, encompassing an increase in computing power by four orders of magnitude. The â€˜4-D approachâ€™ integrating expectation-based methods from systems dynamics and control engineering with methods from AI has allowed to create vehicles with unprecedented capabilities in the technical realm: autonomous road vehicle guidance in public traffic on freeways at speeds beyond 130 km/h, on-board-autonomous landing approaches of aircraft, and landmark navigation for AGV's, for road vehicles including turn-offs onto cross-roads, and for helicopters in low-level flight (real-time, hardware-in-the-loop simulations in the latter case).}
}
@article{TEIJEIRO2018163,
title = {On the adoption of abductive reasoning for time series interpretation},
journal = {Artificial Intelligence},
volume = {262},
pages = {163-188},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218303163},
author = {T. Teijeiro and P. FÃ©lix},
keywords = {Abduction, Interpretation, Time series, Temporal abstraction, Temporal reasoning, Non-monotonic reasoning, Signal abstraction},
abstract = {Time series interpretation aims to provide an explanation of what is observed in terms of its underlying processes. The present work is based on the assumption that the common classification-based approaches to time series interpretation suffer from a set of inherent weaknesses, whose ultimate cause lies in the monotonic nature of the deductive reasoning paradigm. In this document we propose a new approach to this problem, based on the initial hypothesis that abductive reasoning properly accounts for the human ability to identify and characterize the patterns appearing in a time series. The result of this interpretation is a set of conjectures in the form of observations, organized into an abstraction hierarchy and explaining what has been observed. A knowledge-based framework and a set of algorithms for the interpretation task are provided, implementing a hypothesize-and-test cycle guided by an attentional mechanism. As a representative application domain, interpretation of the electrocardiogram allows us to highlight the strengths of the proposed approach in comparison with traditional classification-based approaches.}
}
@article{BONET2010245,
title = {Conformant plans and beyond: Principles and complexity},
journal = {Artificial Intelligence},
volume = {174},
number = {3},
pages = {245-269},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209001234},
author = {Blai Bonet},
keywords = {Planning, Complexity of planning, Partially-observable planning, Non-deterministic planning, Modal logic},
abstract = {Conformant planning is used to refer to planning for unobservable problems whose solutions, like classical planning, are linear sequences of operators called linear plans. The term â€˜conformantâ€™ is automatically associated with both the unobservable planning model and with linear plans, mainly because the only possible solutions for unobservable problems are linear plans. In this paper we show that linear plans are not only meaningful for unobservable problems but also for partially-observable problems. In such case, the execution of a linear plan generates observations from the environment which must be collected by the agent during the execution of the plan and used at the end in order to determine whether the goal had been achieved or not; this is the typical case in problems of diagnosis in which all the actions are knowledge-gathering actions. Thus, there are substantial differences about linear plans for the case of unobservable or fully-observable problems, and for the case of partially-observable problems: while linear plans for the former model must conform with properties in state space, linear plans for partially-observable problems must conform with properties in belief space. This differences surface when the problems are allowed to express epistemic goals and conditions using modal logic, and place the plan-existence decision problem in different complexity classes. Linear plans is one extreme point in a discrete spectrum of solution forms for planning problems. The other extreme point is contingent plans in which there is a branch point for every possible observation at each time step, and thus the number of branch points is not bounded a priori. In the middle of the spectrum, there are plans with a bounded number of branch points. Thus, linear plans are plans with zero branch points and contingent plans are plans with unbounded number of branch points. In this work, we lay down foundations and principles for the general treatment of linear plans and plans of bounded branching, and provide exact complexity results for novel decision problems. We also show that linear plans for partially-observable problems are not only of theoretical interest since some challenging real-life problems can be dealt with them.}
}
@article{KIM20111722,
title = {Algorithms and complexity results for persuasive argumentation},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1722-1736},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000373},
author = {Eun Jung Kim and Sebastian Ordyniak and Stefan Szeider},
keywords = {Abstract argumentation, Value-based argumentation frameworks, Computational complexity, Graphical models, Bounded treewidth},
abstract = {The study of arguments as abstract entities and their interaction as introduced by Dung (1995) [1] has become one of the most active research branches within Artificial Intelligence and Reasoning. A main issue for abstract argumentation systems is the selection of acceptable sets of arguments. Value-based argumentation, as introduced by Bench-Capon (2003) [8], extends DungÊ¼s framework. It takes into account the relative strength of arguments with respect to some ranking representing an audience: an argument is subjectively accepted if it is accepted with respect to some audience, it is objectively accepted if it is accepted with respect to all audiences. Deciding whether an argument is subjectively or objectively accepted, respectively, are computationally intractable problems. In fact, the problems remain intractable under structural restrictions that render the main computational problems for non-value-based argumentation systems tractable. In this paper we identify nontrivial classes of value-based argumentation systems for which the acceptance problems are polynomial-time tractable. The classes are defined by means of structural restrictions in terms of the underlying graphical structure of the value-based system. Furthermore we show that the acceptance problems are intractable for two classes of value-based systems that where conjectured to be tractable by Dunne (2007) [12].}
}
@article{DUNNE201020,
title = {Solving coalitional resource games},
journal = {Artificial Intelligence},
volume = {174},
number = {1},
pages = {20-50},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209001076},
author = {Paul E. Dunne and Sarit Kraus and Efrat Manisterski and Michael Wooldridge},
keywords = {Coalitional games, NTU games, Solution concepts, The core, Bargaining, Algorithms, Complexity},
abstract = {Coalitional Resource Games (crgs) are a form of Non-Transferable Utility (ntu) game, which provide a natural formal framework for modelling scenarios in which agents must pool scarce resources in order to achieve mutually satisfying sets of goals. Although a number of computational questions surrounding crgs have been studied, there has to date been no attempt to develop solution concepts for crgs, or techniques for constructing solutions. In this paper, we rectify this omission. Following a review of the crg framework and a discussion of related work, we formalise notions of coalition structures and the core for crgs, and investigate the complexity of questions such as determining nonemptiness of the core. We show that, while such questions are in general computationally hard, it is possible to check the stability of a coalition structure in time exponential in the number of goals in the system, but polynomial in the number of agents and resources. As a consequence, checking stability is feasible for systems with small or bounded numbers of goals. We then consider constructive approaches to generating coalition structures. We present a negotiation protocol for crgs, give an associated negotiation strategy, and prove that this strategy forms a subgame perfect equilibrium. We then show that coalition structures produced by the protocol satisfy several desirable properties: Pareto optimality, dummy player, and pseudo-symmetry.}
}
@article{YANG1995121,
title = {Multilevel enhancement and detection of stereo disparity surfaces},
journal = {Artificial Intelligence},
volume = {78},
number = {1},
pages = {121-145},
year = {1995},
note = {Special Volume on Computer Vision},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00028-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000283},
author = {Yibing Yang and Alan L. Yuille},
abstract = {The problem of stereo vision has been of increasing interest to the computer vision community over the past decade. This paper presents a new computational framework for matching a pair of stereo images arising from viewing the same object from two different positions. In contrast to previous work, this approach formulates the matching problem as detection of a â€œbrightâ€, coherent disparity surface in a 3D image called the spatio-disparity space (SDS) image. The SDS images represents the goodness of each and every possible match. A nonlinear filter is proposed for enhancing the disparity surface in the SDS image and for suppressing the noise. This filter is used to construct a hyperpyramid representation of the SDS image. Then the disparity surface is detected using a coarse-to-fine control structure. The proposed method is robust to photometric and geometric distortions in the stereo images, and has a number of computational advantages. It produces good results for complex scenes.}
}
@article{FRIEDMAN1997257,
title = {Modeling belief in dynamic systems, part I: Foundations},
journal = {Artificial Intelligence},
volume = {95},
number = {2},
pages = {257-316},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00040-4},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000404},
author = {Nir Friedman and Joseph Y. Halpern},
keywords = {Belief change, Belief revision, Milimal change, Logic of knowledge, Logic of belief, Logic of time, Plausibility measure, AGM postulates},
abstract = {Belief change is a fundamental problem in AI: Agents constantly have to update their beliefs to accommodate new observations. In recent years, there has been much work on axiomatic characterizations of belief change. We claim that a better understanding of belief change can be gained from examining appropriate semantic models. In this paper we propose a general framework in which to model belief change. We begin by defining belief in terms of knowledge and plausibility: an agent believes Î¦ if he knows that Î¦ is more plausible than Â¬Î¦. We then consider some properties defining the interaction between knowledge and plausibility, and show how these properties affect the properties of belief. In particular, we show that by assuming two of the most natural properties, belief becomes a KD45 operator. Finally, we add time to the picture. This gives us a framework in which we can talk about knowledge, plausibility (and hence belief), and time, which extends the framework of Halpern and Fagin for modeling knowledge in multi-agent systems. We then examine the problem of â€œminimal changeâ€. This notion can be captured by using prior plausibilities, an analogue to prior probabilities, which can be updated by â€œconditioningâ€. We show by example that conditioning on a plausibility measure can capture many scenarios of interest. In a companion paper, we show how the two best-studied scenarios of belief change, belief revision and belief update, fit into our framework.}
}
@article{GENT2008738,
title = {Solving quantified constraint satisfaction problems},
journal = {Artificial Intelligence},
volume = {172},
number = {6},
pages = {738-771},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001932},
author = {Ian P. Gent and Peter Nightingale and Andrew Rowley and Kostas Stergiou},
keywords = {Quantified constraint satisfaction problems, Quantified Boolean formulas, Arc consistency, Search algorithms, Random problems},
abstract = {We make a number of contributions to the study of the Quantified Constraint Satisfaction Problem (QCSP). The QCSP is an extension of the constraint satisfaction problem that can be used to model combinatorial problems containing contingency or uncertainty. It allows for universally quantified variables that can model uncertain actions and events, such as the unknown weather for a future party, or an opponent's next move in a game. In this paper we report significant contributions to two very different methods for solving QCSPs. The first approach is to implement special purpose algorithms for QCSPs; and the second is to encode QCSPs as Quantified Boolean Formulas and then use specialized QBF solvers. The discovery of particularly effective encodings influenced the design of more effective algorithms: by analyzing the properties of these encodings, we identify the features in QBF solvers responsible for their efficiency. This enables us to devise analogues of these features in QCSPs, and implement them in special purpose algorithms, yielding an effective special purpose solver, QCSP-Solve. Experiments show that this solver and a highly optimized QBF encoding are several orders of magnitude more efficient than the initially developed algorithms. A final, but significant, contribution is the identification of flaws in simple methods of generating random QCSP instances, and a means of generating instances which are not known to be flawed.}
}
@article{GARCIAOSORIO2010410,
title = {Democratic instance selection: A linear complexity instance selection algorithm based on classifier ensemble concepts},
journal = {Artificial Intelligence},
volume = {174},
number = {5},
pages = {410-441},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000123},
author = {CÃ©sar GarcÃ­a-Osorio and Aida {de Haro-GarcÃ­a} and NicolÃ¡s GarcÃ­a-Pedrajas},
keywords = {Instance selection, Instance-based learning, Ensembles, Huge problems},
abstract = {Instance selection is becoming increasingly relevant due to the huge amount of data that is constantly being produced in many fields of research. Although current algorithms are useful for fairly large datasets, scaling problems are found when the number of instances is in the hundreds of thousands or millions. When we face huge problems, scalability becomes an issue, and most algorithms are not applicable. Thus, paradoxically, instance selection algorithms are for the most part impracticable for the same problems that would benefit most from their use. This paper presents a way of avoiding this difficulty using several rounds of instance selection on subsets of the original dataset. These rounds are combined using a voting scheme to allow good performance in terms of testing error and storage reduction, while the execution time of the process is significantly reduced. The method is particularly efficient when we use instance selection algorithms that are high in computational cost. The proposed approach shares the philosophy underlying the construction of ensembles of classifiers. In an ensemble, several weak learners are combined to form a strong classifier; in our method several weak (in the sense that they are applied to subsets of the data) instance selection algorithms are combined to produce a strong and fast instance selection method. An extensive comparison of 30 medium and large datasets from the UCI Machine Learning Repository using 3 different classifiers shows the usefulness of our method. Additionally, the method is applied to 5 huge datasets (from three hundred thousand to more than a million instances) with good results and fast execution time.}
}
@article{BESNARD20091406,
title = {Encoding deductive argumentation in quantified Boolean formulae},
journal = {Artificial Intelligence},
volume = {173},
number = {15},
pages = {1406-1423},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000782},
author = {Philippe Besnard and Anthony Hunter and Stefan Woltran},
keywords = {Argument systems, Argumentation, Classical logic, Inconsistency, Quantified Boolean formulae, Conflicting knowledge},
abstract = {There are a number of frameworks for modelling argumentation in logic. They incorporate a formal representation of individual arguments and techniques for comparing conflicting arguments. A common assumption for logic-based argumentation is that an argument is a pair ã€ˆÎ¦,Î±ã€‰ where Î¦ is minimal subset of the knowledge-base such that Î¦ is consistent and Î¦ entails the claim Î±. Different logics provide different definitions for consistency and entailment and hence give us different options for argumentation. Classical propositional logic is an appealing option for argumentation but the computational viability of generating an argument is an issue. To better explore this issue, we use quantified Boolean formulae to characterise an approach to argumentation based on classical logic.}
}
@article{GAL20101460,
title = {Agent decision-making in open mixed networks},
journal = {Artificial Intelligence},
volume = {174},
number = {18},
pages = {1460-1480},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001451},
author = {Ya'akov Gal and Barbara Grosz and Sarit Kraus and Avi Pfeffer and Stuart Shieber},
keywords = {Humanâ€“Computer decision-making, Negotiation},
abstract = {Computer systems increasingly carry out tasks in mixed networks, that is in group settings in which they interact both with other computer systems and with people. Participants in these heterogeneous humanâ€“computer groups vary in their capabilities, goals, and strategies; they may cooperate, collaborate, or compete. The presence of people in mixed networks raises challenges for the design and the evaluation of decision-making strategies for computer agents. This paper describes several new decision-making models that represent, learn and adapt to various social attributes that influence people's decision-making and presents a novel approach to evaluating such models. It identifies a range of social attributes in an open-network setting that influence people's decision-making and thus affect the performance of computer-agent strategies, and establishes the importance of learning and adaptation to the success of such strategies. The settings vary in the capabilities, goals, and strategies that people bring into their interactions. The studies deploy a configurable system called Colored Trails (CT) that generates a family of games. CT is an abstract, conceptually simple but highly versatile game in which players negotiate and exchange resources to enable them to achieve their individual or group goals. It provides a realistic analogue to multi-agent task domains, while not requiring extensive domain modeling. It is less abstract than payoff matrices, and people exhibit less strategic and more helpful behavior in CT than in the identical payoff matrix decision-making context. By not requiring extensive domain modeling, CT enables agent researchers to focus their attention on strategy design, and it provides an environment in which the influence of social factors can be better isolated and studied.}
}
@article{KUSHMERICK1995239,
title = {An algorithm for probabilistic planning},
journal = {Artificial Intelligence},
volume = {76},
number = {1},
pages = {239-286},
year = {1995},
note = {Planning and Scheduling},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00087-H},
url = {https://www.sciencedirect.com/science/article/pii/000437029400087H},
author = {Nicholas Kushmerick and Steve Hanks and Daniel S. Weld},
abstract = {We define the probabilistic planning problem in terms of a probability distribution over initial world states, a boolean combination of propositions representing the goal, a probability threshold, and actions whose effects depend on the execution-time state of the world and on random chance. Adopting a probabilistic model complicates the definition of plan success: instead of demanding a plan that provably achieves the goal, we seek plans whose probability of success exceeds the threshold. In this paper, we present buridan, an implemented least-commitment planner that solves problems of this form. We prove that the algorithm is both sound and complete. We then explore buridan's efficiency by contrasting four algorithms for plan evaluation, using a combination of analytic methods and empirical experiments. We also describe the interplay between generating plans and evaluating them, and discuss the role of search control in probabilistic planning.}
}
@article{SERVICE20112061,
title = {Randomized coalition structure generation},
journal = {Artificial Intelligence},
volume = {175},
number = {16},
pages = {2061-2074},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000889},
author = {Travis Service and Julie Adams},
keywords = {Coalition structure generation, Coalition formation, Characteristic function game},
abstract = {Randomization can be employed to achieve constant factor approximations to the coalition structure generation problem in less time than all previous approximation algorithms. In particular, this manuscript presents a new randomized algorithm that can generate a 23 approximate solution in O(n2.587n) time, improving upon the previous algorithm that required O(n2.83n) time to guarantee the same performance. Also, the presented new techniques allow a 14 approximate solution to be generated in the optimal time of O(2n) and improves on the previous best approximation ratio obtainable in O(2n) time of 18. The presented algorithms are based upon a careful analysis of the sizes and numbers of coalitions in the smallest optimal coalition structures. An empirical analysis of the new randomized algorithms compared to their deterministic counterparts is provided. We find that the presented randomized algorithms generate solutions with utility comparable to what is returned by their deterministic counterparts (in some cases producing better results on average). Moreover, a significant speedup was found for most approximation ratios for the randomized algorithms over the deterministic algorithms. In particular, the randomized 12 approximate algorithm runs in approximately 22.4% of the time required for the deterministic 12 approximation algorithm for problems with between 20 and 27 agents.}
}
@article{GUNS20111951,
title = {Itemset mining: A constraint programming perspective},
journal = {Artificial Intelligence},
volume = {175},
number = {12},
pages = {1951-1983},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211000646},
author = {Tias Guns and Siegfried Nijssen and Luc {De Raedt}},
keywords = {Data mining, Itemset mining, Constraint programming},
abstract = {The field of data mining has become accustomed to specifying constraints on patterns of interest. A large number of systems and techniques has been developed for solving such constraint-based mining problems, especially for mining itemsets. The approach taken in the field of data mining contrasts with the constraint programming principles developed within the artificial intelligence community. While most data mining research focuses on algorithmic issues and aims at developing highly optimized and scalable implementations that are tailored towards specific tasks, constraint programming employs a more declarative approach. The emphasis lies on developing high-level modeling languages and general solvers that specify what the problem is, rather than outlining how a solution should be computed, yet are powerful enough to be used across a wide variety of applications and application domains. This paper contributes a declarative constraint programming approach to data mining. More specifically, we show that it is possible to employ off-the-shelf constraint programming techniques for modeling and solving a wide variety of constraint-based itemset mining tasks, such as frequent, closed, discriminative, and cost-based itemset mining. In particular, we develop a basic constraint programming model for specifying frequent itemsets and show that this model can easily be extended to realize the other settings. This contrasts with typical procedural data mining systems where the underlying procedures need to be modified in order to accommodate new types of constraint, or novel combinations thereof. Even though the performance of state-of-the-art data mining systems outperforms that of the constraint programming approach on some standard tasks, we also show that there exist problems where the constraint programming approach leads to significant performance improvements over state-of-the-art methods in data mining and as well as to new insights into the underlying data mining problems. Many such insights can be obtained by relating the underlying search algorithms of data mining and constraint programming systems to one another. We discuss a number of interesting new research questions and challenges raised by the declarative constraint programming approach to data mining.}
}
@article{HAZON20121,
title = {On the evaluation of election outcomes under uncertainty},
journal = {Artificial Intelligence},
volume = {189},
pages = {1-18},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000574},
author = {Noam Hazon and Yonatan Aumann and Sarit Kraus and Michael Wooldridge},
keywords = {Computational social choice, Voting rules},
abstract = {We investigate the extent to which it is possible to compute the probability of a particular candidate winning an election, given imperfect information about the preferences of the electorate. We assume that for each voter, we have a probability distribution over a set of preference orderings. Thus, for each voter, we have a number of possible preference orderings â€“ we do not know which of these orderings actually represents the preferences of the voter, but for each ordering, we know the probability that it does. For the case where the number of candidates is a constant, we are able to give a polynomial time algorithm to compute the probability that a given candidate will win. We present experimental results obtained with an implementation of the algorithm, illustrating how the algorithmÊ¼s performance in practice is better than its predicted theoretical bound. However, when the number of candidates is not bounded, we prove that the problem becomes #P-hard for the Plurality, k-approval, Borda, Copeland, and Bucklin voting rules. We further show that even evaluating if a candidate has any chance of winning is NP-complete for the Plurality voting rule in the case where voters may have different weights. With unweighted voters, we give a polynomial algorithm for Plurality, and show that the problem is hard for many other voting rules. Finally, we give a Monte Carlo approximation algorithm for computing the probability of a candidate winning in any settings, with an error that is as small as desired.}
}
@article{MOTIK20091275,
title = {Representing ontologies using description logics, description graphs, and rules},
journal = {Artificial Intelligence},
volume = {173},
number = {14},
pages = {1275-1309},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000678},
author = {Boris Motik and Bernardo {Cuenca Grau} and Ian Horrocks and Ulrike Sattler},
keywords = {Knowledge representation, Description logics, Structured objects, Ontologies},
abstract = {Description logics (DLs) are a family of state-of-the-art knowledge representation languages, and their expressive power has been carefully crafted to provide useful knowledge modeling primitives while allowing for practically effective decision procedures for the basic reasoning problems. Recent experience with DLs, however, has shown that their expressivity is often insufficient to accurately describe structured objectsâ€”objects whose parts are interconnected in arbitrary, rather than tree-like ways. DL knowledge bases describing structured objects are therefore usually underconstrained, which precludes the entailment of certain consequences and causes performance problems during reasoning. To address this problem, we propose an extension of DL languages with description graphsâ€”a knowledge modeling construct that can accurately describe objects with parts connected in arbitrary ways. Furthermore, to enable modeling the conditional aspects of structured objects, we also extend DLs with rules. We present an in-depth study of the computational properties of such a formalism. In particular, we first identify the sources of undecidability of the general, unrestricted formalism. Based on that analysis, we then investigate several restrictions of the general formalism that make reasoning decidable. We present practical evidence that such a logic can be used to model nontrivial structured objects. Finally, we present a practical decision procedure for our formalism, as well as tight complexity bounds.}
}
@article{NAUMOV2018279,
title = {Together we know how to achieve: An epistemic logic of know-how},
journal = {Artificial Intelligence},
volume = {262},
pages = {279-300},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S000437021830328X},
author = {Pavel Naumov and Jia Tao},
keywords = {Strategy, Game theory, Knowledge, Formal epistemology, Logic, Axiomatization, Completeness, Imperfect information},
abstract = {The existence of a coalition strategy to achieve a goal does not necessarily mean that the coalition has enough information to know how to follow the strategy. Neither does it mean that the coalition knows that such a strategy exists. The article studies an interplay between the distributed knowledge, coalition strategies, and coalition â€œknow-howâ€ strategies. The main technical result is a sound and complete trimodal logical system that describes the properties of this interplay.}
}
@article{ZICK201974,
title = {Cooperative games with overlapping coalitions: Charting the tractability frontier},
journal = {Artificial Intelligence},
volume = {271},
pages = {74-97},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370219300219},
author = {Yair Zick and Georgios Chalkiadakis and Edith Elkind and Evangelos Markakis},
keywords = {Cooperative games, Overlapping coalition formation, Core, Treewidth, Arbitration functions},
abstract = {The framework of cooperative games with overlapping coalitions (OCF games), which was proposed by Chalkiadakis et al. [1], generalizes classic cooperative games to settings where agents may belong to more than one coalition. OCF games can be used to model scenarios where agents distribute resources, such as time or energy, among several tasks, and then divide the payoffs generated by these tasks in a fair and/or stable manner. As the framework of OCF games is very expressive, identifying settings that admit efficient algorithms for computing â€˜goodâ€™ outcomes of OCF games is a challenging task. In this work, we put forward two approaches that lead to tractability results for OCF games. First, we propose a discretized model of overlapping coalition formation, where each agent i has a weight WiâˆˆN and may allocate an integer amount of weight to any task. Within this framework, we focus on the computation of outcomes that are socially optimal and/or stable. We discover that the algorithmic complexity of this task crucially depends on the amount of resources that each agent possesses, the maximum coalition size, and the pattern of communication among the agents. We identify several constraints that lead to tractable subclasses of discrete OCF games, and supplement our tractability results by hardness proofs, which clarify the role of our constraints. Second, we introduce and analyze a natural class of (continuous) OCF gamesâ€”the Linear Bottleneck Games. We show that such games always admit a stable outcome, even assuming a large space of feasible deviations, and provide an efficient algorithm for computing such outcomes.}
}
@article{BLUM1997245,
title = {Selection of relevant features and examples in machine learning},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {245-271},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00063-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000635},
author = {Avrim L. Blum and Pat Langley},
keywords = {Relevant features, Relevant examples, Machine learning},
abstract = {In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area.}
}
@article{REECE1995397,
title = {Control of perceptual attention in robot driving},
journal = {Artificial Intelligence},
volume = {78},
number = {1},
pages = {397-430},
year = {1995},
note = {Special Volume on Computer Vision},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00029-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000291},
author = {Douglas A. Reece and Steve A. Shafer},
abstract = {Computer vision research aimed at performing general scene understanding has proven to be conceptually difficult and computationally complex. Active vision is a promising approach to solving this problem. Active vision systems use optimized sensor settings, reduced fields of view, and relatively simple algorithms to efficiently extract specific information from a scene. This approach is only appropriate in the context of a task that motivates the selection of the information to extract. While there has been a fair amount of research that describes the extraction processes, there has been little work that investigates how active vision could be used for a realistic task in a dynamic domain. We are studying such a task: driving an autonomous vehicle in traffic. In this paper we present a method for controlling visual attention as part of the reasoning process for driving, and analyze the efficiency gained in doing so. We first describe a model of driving and the driving environment, and estimate the complexity of performing the required sensing with a general driving-scene understanding system. We then introduce three programs that use increasingly sophisticated perceptual control techniques to select perceptual actions. The first program, called Ulysses-1, uses perceptual routines, which use known reference objects to guide the search for new objects. The second program, Ulysses-2, creates an inference tree to infer the effect of uncertain input data on action choices, and searches this tree to decide which data to sense. Finally, Ulysses-3 uses domain knowledge to reason about how dynamic objects will move or change over time; objects that do not move enough to affect the robot's decisions are not selected as perceptual targets. For each technique we have run experiments in simulation to measure the cost savings realized by using selective perception. We estimate that the techniques included in Ulysses-3 reduce the computational cost of perception by 9 to 12 orders of magnitude when compared to a general perception system.}
}
@article{NISHIDA19953,
title = {Qualitative analysis of behavior of systems of piecewise linear differential equations with two state variables},
journal = {Artificial Intelligence},
volume = {75},
number = {1},
pages = {3-29},
year = {1995},
note = {AI Research in Japan},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00062-6},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000626},
author = {Toyoaki Nishida and Shuji Doshita},
abstract = {The set of all solution curves in the phase space for a system of ordinary differential equations (ODEs) is called the phase portrait. A phase portrait provides global qualitative information about how a given system of ODEs behaves under different initial conditions. We are developing a method called Topological Flow Analysis (TFA) for automating analysis of the topological structure of the phase portrait of systems of ODEs. In this paper, we describe the first version of TFA for systems of piecewise linear ODEs with two state variables. TFA has several novel features that have not been achieved before. Firstly, TFA enables to grasp characteristics of all behaviors of a given system of ODEs. Secondly, TFA allows to symbolically represent behaviors in terms of critical geometric features in the phase space. Finally, TFA integrates qualitative and quantitative analysis. The current version of TFA has been implemented as a program called PSX2PWL using Common Lisp.}
}
@article{STERN20141,
title = {Potential-based bounded-cost search and Anytime Non-Parametric AâŽ},
journal = {Artificial Intelligence},
volume = {214},
pages = {1-25},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000551},
author = {Roni Stern and Ariel Felner and Jur {van den Berg} and Rami Puzis and Rajat Shah and Ken Goldberg},
keywords = {Heuristic search, Anytime algorithms, Robotics},
abstract = {This paper presents two new search algorithms: Potential Search (PTS) and Anytime Potential Search/Anytime Non-Parametric AâŽ (APTS/ANAâŽ). Both algorithms are based on a new evaluation function that is easy to implement and does not require user-tuned parameters. PTS is designed to solve bounded-cost search problems, which are problems where the task is to find as fast as possible a solution under a given cost bound. APTS/ANAâŽ is a non-parametric anytime search algorithm discovered independently by two research groups via two very different derivations. In this paper, co-authored by researchers from both groups, we present these derivations: as a sequence of calls to PTS and as a non-parametric greedy variant of Anytime Repairing AâŽ. We describe experiments that evaluate the new algorithms in the 15-puzzle, KPP-COM, robot motion planning, gridworld navigation, and multiple sequence alignment search domains. Our results suggest that when compared with previous anytime algorithms, APTS/ANAâŽ: (1) does not require user-set parameters, (2) finds an initial solution faster, (3) spends less time between solution improvements, (4) decreases the suboptimality bound of the current-best solution more gradually, and (5) converges faster to an optimal solution when reachable.}
}
@article{STAHOVICH1998211,
title = {Generating multiple new designs from a sketch},
journal = {Artificial Intelligence},
volume = {104},
number = {1},
pages = {211-264},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00058-7},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000587},
author = {Thomas F. Stahovich and Randall Davis and Howard Shrobe},
keywords = {Sketch understanding, Design generalization, Mechanical design, Qualitative geometric reasoning},
abstract = {We describe a program called SketchIT that transforms a single sketch of a mechanical device into multiple families of new designs. It represents each of these families with a â€œBEP-Modelâ€, a parametric model augmented with constraints that ensure the device produces the desired behavior. The program is based on qualitative configuration space (qc-space), a novel representation that captures mechanical behavior while abstracting away its implementation. The program employs a paradigm of abstraction and resynthesis: it abstracts the initial sketch into qc-space, then uses a library of primitive mechanical interactions to map from qc-space to new implementations.}
}
@article{SMITH199999,
title = {Fast Bayes and the dynamic junction forest},
journal = {Artificial Intelligence},
volume = {107},
number = {1},
pages = {99-124},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00103-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298001039},
author = {J.Q. Smith and K.N. Papamichail},
keywords = {Dynamic models, Hellinger metric, Influence diagrams, Junction trees, Probabilistic expert systems, Multivariate state space models},
abstract = {It has been shown that junction tree algorithms can provide a quick and efficient method for propagating probabilities in complex multivariate problems when they can be described by a fixed conditional independence structure. In this paper we formalise and illustrate with two practical examples how these probabilistic propagation algorithms can be applied to high dimensional processes whose conditional independence structure, as well as their underlying distributions, are augmented through the passage of time.}
}
@article{KUPCSIK2017415,
title = {Model-based contextual policy search for data-efficient generalization of robot skills},
journal = {Artificial Intelligence},
volume = {247},
pages = {415-439},
year = {2017},
note = {Special Issue on AI and Robotics},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214001374},
author = {Andras Kupcsik and Marc Peter Deisenroth and Jan Peters and Ai Poh Loh and Prahlad Vadakkepat and Gerhard Neumann},
keywords = {Robotics, Reinforcement learning, Contextual policy search, Model-based policy search, Robot skill generalization, Gaussian processes, Movement primitives, Robot table tennis, Robot hockey},
abstract = {In robotics, lower-level controllers are typically used to make the robot solve a specific task in a fixed context. For example, the lower-level controller can encode a hitting movement while the context defines the target coordinates to hit. However, in many learning problems the context may change between task executions. To adapt the policy to a new context, we utilize a hierarchical approach by learning an upper-level policy that generalizes the lower-level controllers to new contexts. A common approach to learn such upper-level policies is to use policy search. However, the majority of current contextual policy search approaches are model-free and require a high number of interactions with the robot and its environment. Model-based approaches are known to significantly reduce the amount of robot experiments, however, current model-based techniques cannot be applied straightforwardly to the problem of learning contextual upper-level policies. They rely on specific parametrizations of the policy and the reward function, which are often unrealistic in the contextual policy search formulation. In this paper, we propose a novel model-based contextual policy search algorithm that is able to generalize lower-level controllers, and is data-efficient. Our approach is based on learned probabilistic forward models and information theoretic policy search. Unlike current algorithms, our method does not require any assumption on the parametrization of the policy or the reward function. We show on complex simulated robotic tasks and in a real robot experiment that the proposed learning framework speeds up the learning process by up to two orders of magnitude in comparison to existing methods, while learning high quality policies.}
}
@article{BARONI2007675,
title = {On principle-based evaluation of extension-based argumentation semantics},
journal = {Artificial Intelligence},
volume = {171},
number = {10},
pages = {675-700},
year = {2007},
note = {Argumentation in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000744},
author = {Pietro Baroni and Massimiliano Giacomin},
keywords = {Argumentation frameworks, Argumentation semantics, Skepticism},
abstract = {The increasing variety of semantics proposed in the context of Dung's theory of argumentation makes more and more inadequate the example-based approach commonly adopted for evaluating and comparing different semantics. To fill this gap, this paper provides two main contributions. First, a set of general criteria for semantics evaluation is introduced by proposing a formal counterpart to several intuitive notions related to the concepts of maximality, defense, directionality, and skepticism. Then, the proposed criteria are applied in a systematic way to a representative set of argumentation semantics available in the literature, namely grounded, complete, preferred, stable, semi-stable, ideal, prudent, and CF2 semantics.}
}
@article{HUANG199675,
title = {ALX, an action logic for agents with bounded rationality},
journal = {Artificial Intelligence},
volume = {82},
number = {1},
pages = {75-127},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00090-5},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000905},
author = {Zhisheng Huang and Michael Masuch and LÃ¡szlÃ³ PÃ³los},
abstract = {We propose a modal action logic that combines ideas from H.A. Simon's bounded rationality, S. Kripke's possible world semantics, G.H. von Wright's preference logic, Pratt's dynamic logic, Stalnaker's minimal change and more recent approaches to update semantics. ALX (the xth action logic) is sound, complete and decidable, making it the first complete logic for two-place preference operators. ALX avoids important drawbacks of other action logics, especially the counterintuitive necessitation rule for goals (every theorem must be a goal) and the equally counterintuitive closure of goals under logical implication.}
}
@article{LEE2006160,
title = {Loop formulas for circumscription},
journal = {Artificial Intelligence},
volume = {170},
number = {2},
pages = {160-185},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S000437020500144X},
author = {Joohyung Lee and Fangzhen Lin},
keywords = {Nonmonotonic reasoning, Commonsense reasoning, Knowledge representation, Circumscription, Clark's completion, Loop formulas, Logic programming},
abstract = {Clark's completion is a simple nonmonotonic formalism and a special case of several nonmonotonic logics. Recently there has been work on extending completion with â€œloop formulasâ€ so that general cases of nonmonotonic logics such as logic programs (under the answer set semantics) and McCainâ€“Turner causal logic can be characterized by propositional logic in the form of â€œcompletion + loop formulasâ€. In this paper, we show that the idea is applicable to McCarthy's circumscription in the propositional case, with Lifschitz's pointwise circumscription playing the role of completion. We also show how to embed propositional circumscription in logic programs and in causal logic, inspired by the uniform characterization of â€œcompletion + loop formulasâ€.}
}
@article{SHIN2005194,
title = {Processes and continuous change in a SAT-based planner},
journal = {Artificial Intelligence},
volume = {166},
number = {1},
pages = {194-253},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000524},
author = {Ji-Ae Shin and Ernest Davis},
keywords = {SAT-based planning, LPSAT, Continuous time, Metric quantities, Processes},
abstract = {The TM-LPSAT planner can construct plans in domains containing atomic actions and durative actions; events and processes; discrete, real-valued, and interval-valued fluents; reusable resources, both numeric and interval-valued; and continuous linear change to quantities. It works in three stages. In the first stage, a representation of the domain and problem in an extended version of PDDL+ is compiled into a system of Boolean combinations of propositional atoms and linear constraints over numeric variables. In the second stage, a SAT-based arithmetic constraint solver, such as LPSAT or MathSAT, is used to find a solution to the system of constraints. In the third stage, a correct plan is extracted from this solution. We discuss the structure of the planner and show how planning with time and metric quantities is compiled into a system of constraints. The proofs of soundness and completeness over a substantial subset of our extended version of PDDL+ are presented.}
}
@article{LIAU1996163,
title = {Possibilistic reasoningâ€”a mini-survey and uniform semantics},
journal = {Artificial Intelligence},
volume = {88},
number = {1},
pages = {163-193},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00013-6},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000136},
author = {Churn-Jung Liau and Bertrand I-Peng Lin},
keywords = {Nonclassical logics, Possibility theory, Conditional possibility, Modal logic, Conditional logic},
abstract = {In this paper, we survey some quantitative and qualitative approaches to uncertainty management based on possibility theory and present a logical framework to integrate them. The semantics of the logic is based on the Dempster's rule of conditioning for possibility theory. It is then shown that classical modal logic, conditional logic, possibilistic logic, quantitative modal logic and qualitative possibilistic logic are all sublogics of the presented logical framework. In this way, we can formalize and generalize some well-known results about possibilistic reasoning in a uniform semantics. Moreover, our uniform framework is applicable to nonmonotonic reasoning, approximate consequence relation formulation, and partial consistency handling.}
}
@article{UGARTE201748,
title = {Skypattern mining: From pattern condensed representations to dynamic constraint satisfaction problems},
journal = {Artificial Intelligence},
volume = {244},
pages = {48-69},
year = {2017},
note = {Combining Constraint Solving with Mining and Learning},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S000437021500065X},
author = {Willy Ugarte and Patrice Boizumault and Bruno CrÃ©milleux and Alban Lepailleur and Samir Loudni and Marc Plantevit and Chedy RaÃ¯ssi and Arnaud Soulet},
keywords = {Skypatterns, Pattern mining, Constraint programming, Dynamic CSP, User preferences},
abstract = {Data mining is the study of how to extract information from data and express it as useful knowledge. One of its most important subfields, pattern mining, involves searching and enumerating interesting patterns in data. Various aspects of pattern mining are studied in the theory of computation and statistics. In the last decade, the pattern mining community has witnessed a sharp shift from efficiency-based approaches to methods which can extract more meaningful patterns. Recently, new methods adapting results from studies of economic efficiency and multi-criteria decision analyses such as Pareto efficiency, or skylines, have been studied. Within pattern mining, this novel line of research allows the easy expression of preferences according to a dominance relation. This approach is useful from a user-preference point of view and tends to promote the use of pattern mining algorithms for non-experts. We present a significant extension of our previous work [1], [2] on the discovery of skyline patterns (or â€œskypatternsâ€) based on the theoretical relationships with condensed representations of patterns. We show how these relationships facilitate the computation of skypatterns and we exploit them to propose a flexible and efficient approach to mine skypatterns using a dynamic constraint satisfaction problems (CSP) framework. We present a unified methodology of our different approaches towards the same goal. This work is supported by an extensive experimental study allowing us to illustrate the strengths and weaknesses of each approach.}
}
@article{JENSEN2008103,
title = {State-set branching: Leveraging BDDs for heuristic search},
journal = {Artificial Intelligence},
volume = {172},
number = {2},
pages = {103-139},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001075},
author = {Rune M. Jensen and Manuela M. Veloso and Randal E. Bryant},
keywords = {Heuristic search, BDD-based search, Boolean representation},
abstract = {In this article, we present a framework called state-set branching that combines symbolic search based on reduced ordered Binary Decision Diagrams (BDDs) with best-first search, such as A* and greedy best-first search. The framework relies on an extension of these algorithms from expanding a single state in each iteration to expanding a set of states. We prove that it is generally sound and optimal for two A* implementations and show how a new BDD technique called branching partitioning can be used to efficiently expand sets of states. The framework is general. It applies to any heuristic function, evaluation function, and transition cost function defined over a finite domain. Moreover, branching partitioning applies to both disjunctive and conjunctive transition relation partitioning. An extensive experimental evaluation of the two A* implementations proves state-set branching to be a powerful framework. The algorithms outperform the ordinary A* algorithm in almost all domains. In addition, they can improve the complexity of A* exponentially and often dominate both A* and blind BDD-based search by several orders of magnitude. Moreover, they have substantially better performance than BDDA*, the currently most efficient BDD-based implementation of A*.}
}
@article{WALKER1996181,
title = {The effect of resource limits and task complexity on collaborative planning in dialogue},
journal = {Artificial Intelligence},
volume = {85},
number = {1},
pages = {181-243},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00114-X},
url = {https://www.sciencedirect.com/science/article/pii/000437029500114X},
author = {Marilyn A. Walker},
abstract = {This paper shows how agents' choice in communicative action can be designed to mitigate the effect of their resource limits in the context of particular features of a collaborative planning task. I first motivate a number of hypotheses about effective language behavior based on a statistical analysis of a corpus of natural collaborative planning dialogues. These hypotheses are then tested in a dialogue testbed whose design is motivated by the corpus analysis. Experiments in the testbed examine the interaction between (1) agents' resource limits in attentional capacity and inferential capacity; (2) agents' choice in communication; and (3) features of communicative tasks that affect task difficulty such as inferential complexity, degree of belief coordination required, and tolerance for errors. The results show that good algorithms for communication must be defined relative to the agents' resource limits and the features of the task. Algorithms that are inefficient for inferentially simple, low coordination or fault tolerant tasks are effective when tasks require coordination or complex inferences, or are fault intolerant. The results provide an explanation for the occurrence of utterances in human dialogues that, prima facie, appear inefficient, and provide the basis for the design of effective algorithms for communicative choice for resource limited agents.}
}
@article{BERNARDINI20181,
title = {Extracting mutual exclusion invariants from lifted temporal planning domains},
journal = {Artificial Intelligence},
volume = {258},
pages = {1-65},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218300377},
author = {Sara Bernardini and Fabio Fagnani and David E. Smith},
keywords = {Automated planning, Temporal planning, Mutual exclusion invariants, Automatic domain analysis},
abstract = {We present a technique for automatically extracting mutual exclusion invariants from temporal planning instances. It first identifies a set of invariant templates by inspecting the lifted representation of the domain and then checks these templates against properties that assure invariance. Our technique builds on other approaches to invariant synthesis presented in the literature but departs from their limited focus on instantaneous actions by addressing temporal domains. To deal with time, we formulate invariance conditions that account for the entire temporal structure of the actions and the possible concurrent interactions between them. As a result, we construct a more comprehensive technique than previous methods, which is able to find not only invariants for temporal domains but also a broader set of invariants for sequential domains. Our experimental results provide evidence that our domain analysis is effective at identifying a more extensive set of invariants, which results in the generation of fewer multi-valued state variables. We show that, in turn, this reduction in the number of variables reflects positively on the performance of the temporal planners that use a variable/value representation.}
}
@article{RINTANEN201245,
title = {Planning as satisfiability: Heuristics},
journal = {Artificial Intelligence},
volume = {193},
pages = {45-86},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212001014},
author = {Jussi Rintanen},
keywords = {Planning, SAT, Heuristics},
abstract = {Reduction to SAT is a very successful approach to solving hard combinatorial problems in Artificial Intelligence and computer science in general. Most commonly, problem instances reduced to SAT are solved with a general-purpose SAT solver. Although there is the obvious possibility of improving the SAT solving process with application-specific heuristics, this has rarely been done successfully. In this work we propose a planning-specific variable selection strategy for SAT solving. The strategy is based on generic principles about properties of plans, and its performance with standard planning benchmarks often substantially improves on generic variable selection heuristics, such as VSIDS, and often lifts it to the same level with other search methods such as explicit state-space search with heuristic search algorithms.}
}
@article{PARK1995355,
title = {Topological direction-giving and visual navigation in large environments},
journal = {Artificial Intelligence},
volume = {78},
number = {1},
pages = {355-395},
year = {1995},
note = {Special Volume on Computer Vision},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00030-5},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000305},
author = {Il-Pyung Park and John R. Kender},
abstract = {In this paper, we propose and investigate a new model for robot navigation in large unstructured environments. Current models, which depend on metric information, have to deal with inherent mechanical and sensory errors. Instead we supply the navigator with qualitative information. Our model consists of two parts, a map-maker and a navigator. Given a source and a goal, the mapmaker derives a navigational path based on the topological relationships between landmarks. A navigational path is generated as a combination of â€œparkwayâ€ and â€œtrajectoryâ€ paths, both of which are abstractions of the real world into topological data structures. Traversing within a parkway enables the navigator to follow landmarks that are continuously visible. Traversing on a trajectory enables the navigator to move reliably into featureless space, based on local headings formed by visible landmarks that are robust to positional and orientational errors. Reliability measures of parkway and trajectory traversals are defined by appropriate error models that account for the sensory errors of the navigator, the population of neighboring objects, and the rotational and translational errors of the navigator. The optimal path is further abstracted into a â€œcustom mapâ€, which consists of a list of symbolic directional instructions, the vocabulary of which is defined by our environmental description language. Based on the custom map generated by the map-maker, the navigating robot looks for events that are characterized by spatial properties of the environment. The map-maker and the navigator are implemented using two cameras, an IBM 7575 robot arm, and a PIPE (Pipelined Image Processing Engine.)}
}
@article{DEGIACOMO2013106,
title = {Automatic behavior composition synthesis},
journal = {Artificial Intelligence},
volume = {196},
pages = {106-142},
year = {2013},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212001658},
author = {Giuseppe {De Giacomo} and Fabio Patrizi and Sebastian SardiÃ±a},
keywords = {Knowledge representation and reasoning, Intelligent agents, Reasoning about actions and change, Automated planning, Synthesis of reactive systems},
abstract = {The behavior composition problem amounts to realizing a virtual desired module (e.g., a surveillance agent system) by suitably coordinating (and re-purposing) the execution of a set of available modules (e.g., a video camera, vacuum cleaner, a robot, etc.). In particular, we investigate techniques to synthesize a controller implementing a fully controllable target behavior by suitably coordinating available partially controllable behaviors that are to execute within a shared, fully observable, but partially predictable (i.e., non-deterministic), environment. Both behaviors and environment are represented as arbitrary finite state transition systems. The technique we propose is directly based on the idea that the controller job is to coordinate the concurrent execution of the available behaviors so as to â€œmimicâ€ the target behavior. To this end, we exploit a variant of the formal notion of simulation to formally capture the notion of â€œmimickingâ€, and we show that the technique proposed is sound and complete, optimal with respect to computational complexity, and robust for different kind of system failures. In addition, we demonstrate that the technique is well suited for highly efficient implementation based on synthesis by model checking technologies, by relating the problem to that of finding a winning strategy in a special safety game and explaining how to actually solve it using an existing verification tool.}
}
@article{ZHOU20122291,
title = {Multi-instance multi-label learning},
journal = {Artificial Intelligence},
volume = {176},
number = {1},
pages = {2291-2320},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211001123},
author = {Zhi-Hua Zhou and Min-Ling Zhang and Sheng-Jun Huang and Yu-Feng Li},
keywords = {Machine learning, Multi-instance multi-label learning, MIML, Multi-label learning, Multi-instance learning},
abstract = {In this paper, we propose the MIML (Multi-Instance Multi-Label learning) framework where an example is described by multiple instances and associated with multiple class labels. Compared to traditional learning frameworks, the MIML framework is more convenient and natural for representing complicated objects which have multiple semantic meanings. To learn from MIML examples, we propose the MimlBoost and MimlSvm algorithms based on a simple degeneration strategy, and experiments show that solving problems involving complicated objects with multiple semantic meanings in the MIML framework can lead to good performance. Considering that the degeneration process may lose information, we propose the D-MimlSvm algorithm which tackles MIML problems directly in a regularization framework. Moreover, we show that even when we do not have access to the real objects and thus cannot capture more information from real objects by using the MIML representation, MIML is still useful. We propose the InsDif and SubCod algorithms. InsDif works by transforming single-instances into the MIML representation for learning, while SubCod works by transforming single-label examples into the MIML representation for learning. Experiments show that in some tasks they are able to achieve better performance than learning the single-instances or single-label examples directly.}
}
@article{LAW2018110,
title = {The complexity and generality of learning answer set programs},
journal = {Artificial Intelligence},
volume = {259},
pages = {110-146},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S000437021830105X},
author = {Mark Law and Alessandra Russo and Krysia Broda},
keywords = {Non-monotonic logic-based learning, Answer Set Programming, Complexity of non-monotonic learning},
abstract = {Traditionally most of the work in the field of Inductive Logic Programming (ILP) has addressed the problem of learning Prolog programs. On the other hand, Answer Set Programming is increasingly being used as a powerful language for knowledge representation and reasoning, and is also gaining increasing attention in industry. Consequently, the research activity in ILP has widened to the area of Answer Set Programming, witnessing the proposal of several new learning frameworks that have extended ILP to learning answer set programs. In this paper, we investigate the theoretical properties of these existing frameworks for learning programs under the answer set semantics. Specifically, we present a detailed analysis of the computational complexity of each of these frameworks with respect to the two decision problems of deciding whether a hypothesis is a solution of a learning task and deciding whether a learning task has any solutions. We introduce a new notion of generality of a learning framework, which enables us to define a framework to be more general than another in terms of being able to distinguish one ASP hypothesis solution from a set of incorrect ASP programs. Based on this notion, we formally prove a generality relation over the set of existing frameworks for learning programs under answer set semantics. In particular, we show that our recently proposed framework, Context-dependent Learning from Ordered Answer Sets, is more general than brave induction, induction of stable models, and cautious induction, and maintains the same complexity as cautious induction, which has the highest complexity of these frameworks.}
}
@article{WILSON1998237,
title = {Geometric reasoning about assembly tools},
journal = {Artificial Intelligence},
volume = {98},
number = {1},
pages = {237-279},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00062-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000623},
author = {Randall H. Wilson},
keywords = {Assembly, Geometric reasoning, Planning, Manufacturing, Robotics},
abstract = {Planning for assembly requires reasoning about various tools used by humans, robots, or other automation to manipulate, attach, and test parts and subassemblies. This paper presents a general framework to represent and reason about geometric accessibility issues for a wide variety of such assembly tools. Central to the framework is a use volume encoding a minimum space that must be free in an assembly state to apply a given tool, and placement constraints on where that volume must be placed relative to the parts on which the tool acts. Determining whether a tool can be applied in a given assembly state is then reduced to an instance of the FINDPLACE problem (Lozano-PÃ©rez, 1983). In addition, we present more efficient methods to integrate the framework into assembly planning. For tools that are applied either before or after their target parts are mated, one method preprocesses a single tool application for all possible states of assembly of a product in polynomial time, reducing all later state-tool queries to evaluations of a simple expression. For tools applied after their target parts are mated, a complementary method guarantees polynomial-time assembly planning. We present a wide variety of tools that can be described adequately using the approach, and survey tool catalogs to determine coverage of standard tools. Finally, we describe an implementation of the approach in an assembly planning system and experiments with a library of over one hundred manual and robotic tools and several complex assemblies.}
}
@article{GOTTLOB201242,
title = {On minimal constraint networks},
journal = {Artificial Intelligence},
volume = {191-192},
pages = {42-60},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000926},
author = {Georg Gottlob},
keywords = {Constraints, Minimal network, Complexity, Join decomposition, Structure identification, Database theory, Knowledge compilation},
abstract = {In a minimal binary constraint network, every tuple of a constraint relation can be extended to a solution. The tractability or intractability of computing a solution to such a minimal network was a long standing open question. Dechter conjectured this computation problem to be NP-hard. We prove this conjecture. We also prove a conjecture by Dechter and Pearl stating that for kâ©¾2 it is NP-hard to decide whether a single constraint can be decomposed into an equivalent k-ary constraint network. We show that this holds even in case of bi-valued constraints where kâ©¾3, which proves another conjecture of Dechter and Pearl. Finally, we establish the tractability frontier for this problem with respect to the domain cardinality and the parameter k.}
}
@article{ZHANG20081873,
title = {On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias},
journal = {Artificial Intelligence},
volume = {172},
number = {16},
pages = {1873-1896},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001008},
author = {Jiji Zhang},
keywords = {Ancestral graphs, Automated causal discovery, Bayesian networks, Causal models, Markov equivalence, Latent variables},
abstract = {Causal discovery becomes especially challenging when the possibility of latent confounding and/or selection bias is not assumed away. For this task, ancestral graph models are particularly useful in that they can represent the presence of latent confounding and selection effect, without explicitly invoking unobserved variables. Based on the machinery of ancestral graphs, there is a provably sound causal discovery algorithm, known as the FCI algorithm, that allows the possibility of latent confounders and selection bias. However, the orientation rules used in the algorithm are not complete. In this paper, we provide additional orientation rules, augmented by which the FCI algorithm is shown to be complete, in the sense that it can, under standard assumptions, discover all aspects of the causal structure that are uniquely determined by facts of probabilistic dependence and independence. The result is useful for developing any causal discovery and reasoning system based on ancestral graph models.}
}
@article{JOSHI20112198,
title = {Decision-theoretic planning with generalized first-order decision diagrams},
journal = {Artificial Intelligence},
volume = {175},
number = {18},
pages = {2198-2222},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370211001081},
author = {Saket Joshi and Kristian Kersting and Roni Khardon},
keywords = {Knowledge representation, Automated reasoning, First order logic, Model checking, Markov decision process, Dynamic programming, Decision theoretic planning},
abstract = {Many tasks in AI require representation and manipulation of complex functions. First-Order Decision Diagrams (FODD) are a compact knowledge representation expressing functions over relational structures. They represent numerical functions that, when constrained to the Boolean range, use only existential quantification. Previous work has developed a set of operations for composition and for removing redundancies in FODDs, thus keeping them compact, and showed how to successfully employ FODDs for solving large-scale stochastic planning problems through the formalism of relational Markov decision processes (RMDP). In this paper, we introduce several new ideas enhancing the applicability of FODDs. More specifically, we first introduce Generalized FODDs (GFODD) and composition operations for them, generalizing FODDs to arbitrary quantification. Second, we develop a novel approach for reducing (G)FODDs using model checking. This yields â€“ for the first time â€“ a reduction that maximally reduces the diagram for the FODD case and provides a sound reduction procedure for GFODDs. Finally we show how GFODDs can be used in principle to solve RMDPs with arbitrary quantification, and develop a complete solution for the case where the reward function is specified using an arbitrary number of existential quantifiers followed by an arbitrary number of universal quantifiers.}
}
@article{MODGIL2009901,
title = {Reasoning about preferences in argumentation frameworks},
journal = {Artificial Intelligence},
volume = {173},
number = {9},
pages = {901-934},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000162},
author = {Sanjay Modgil},
keywords = {Argumentation, Dung, Preferences, Non-monotonic reasoning, Logic programming},
abstract = {The abstract nature of Dung's seminal theory of argumentation accounts for its widespread application as a general framework for various species of non-monotonic reasoning, and, more generally, reasoning in the presence of conflict. A Dung argumentation framework is instantiated by arguments and a binary conflict based attack relation, defined by some underlying logical theory. The justified arguments under different extensional semantics are then evaluated, and the claims of these arguments define the inferences of the underlying theory. To determine a unique set of justified arguments often requires a preference relation on arguments to determine the success of attacks between arguments. However, preference information is often itself defeasible, conflicting and so subject to argumentation. Hence, in this paper we extend Dung's theory to accommodate arguments that claim preferences between other arguments, thus incorporating meta-level argumentation based reasoning about preferences in the object level. We then define and study application of the full range of Dung's extensional semantics to the extended framework, and study special classes of the extended framework. The extended theory preserves the abstract nature of Dung's approach, thus aiming at a general framework for non-monotonic formalisms that accommodate defeasible reasoning about as well as with preference information. We illustrate by formalising argument based logic programming with defeasible priorities in the extended theory.}
}
@article{BARONI2011791,
title = {On the resolution-based family of abstract argumentation semantics and its grounded instance},
journal = {Artificial Intelligence},
volume = {175},
number = {3},
pages = {791-813},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001943},
author = {P. Baroni and P.E. Dunne and M. Giacomin},
keywords = {Abstract argumentation semantics, Argumentation framework, Grounded semantics, Semantics evaluation criteria, Computational complexity},
abstract = {This paper introduces a novel parametric family of semantics for abstract argumentation called resolution-based and analyzes in particular the resolution-based version of the traditional grounded semantics, showing that it features the unique property of satisfying a set of general desirable properties recently introduced in the literature. Additionally, an investigation of its computational complexity properties reveals that resolution-based grounded semantics is satisfactory also from this perspective.}
}
@article{MICHELS20151,
title = {A new probabilistic constraint logic programming language based on a generalised distribution semantics},
journal = {Artificial Intelligence},
volume = {228},
pages = {1-44},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215001009},
author = {Steffen Michels and Arjen Hommersom and Peter J.F. Lucas and Marina Velikova},
keywords = {Probabilistic logic programming, Imprecise probabilities, Continuous probability distributions, Exact probabilistic inference},
abstract = {Probabilistic logics combine the expressive power of logic with the ability to reason with uncertainty. Several probabilistic logic languages have been proposed in the past, each of them with their own features. We focus on a class of probabilistic logic based on Sato's distribution semantics, which extends logic programming with probability distributions on binary random variables and guarantees a unique probability distribution. For many applications binary random variables are, however, not sufficient and one requires random variables with arbitrary ranges, e.g. real numbers. We tackle this problem by developing a generalised distribution semantics for a new probabilistic constraint logic programming language. In order to perform exact inference, imprecise probabilities are taken as a starting point, i.e. we deal with sets of probability distributions rather than a single one. It is shown that given any continuous distribution, conditional probabilities of events can be approximated arbitrarily close to the true probability. Furthermore, for this setting an inference algorithm that is a generalisation of weighted model counting is developed, making use of SMT solvers. We show that inference has similar complexity properties as precise probabilistic inference, unlike most imprecise methods for which inference is more complex. We also experimentally confirm that our algorithm is able to exploit local structure, such as determinism, which further reduces the computational complexity.}
}
@article{KALDELI201630,
title = {Domain-independent planning for services in uncertain and dynamic environments},
journal = {Artificial Intelligence},
volume = {236},
pages = {30-64},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437021630025X},
author = {Eirini Kaldeli and Alexander Lazovik and Marco Aiello},
keywords = {AI planning, Web service composition},
abstract = {Research in automated planning provides novel insights into service composition and contributes towards the provision of automatic compositions which adapt to changing user needs and environmental conditions. Most of the existing planning approaches to aggregating services, however, suffer from one or more of the following limitations: they are not domain-independent, cannot efficiently deal with numeric-valued variables, especially sensing outcomes or operator inputs, and they disregard recovery from runtime contingencies due to erroneous service behavior or exogenous events that interfere with plan execution. We present the RuGPlanner, which models the planning task as a Constraint Satisfaction Problem. In order to address the requirements put forward by service domains, the RuGPlanner is endowed with a number of special features. These include a knowledge-level representation to model uncertainty about the initial state and the outcome of sensing actions, and efficient handling of numeric-valued variables, inputs to actions or observational effects. In addition, it generates plans with a high level of parallelism, it supports a rich declarative language for expressing extended goals, and allows for continual plan revision to deal with sensing outputs, failures, long response times or timeouts, as well as the activities of external agents. The proposed planning framework is evaluated based on a number of scenarios to demonstrate its feasibility and efficiency in several planning domains and execution circumstances which reflect concerns from different service environments.}
}
@article{KNORR20111528,
title = {Local closed world reasoning with description logics under the well-founded semantics},
journal = {Artificial Intelligence},
volume = {175},
number = {9},
pages = {1528-1554},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2011.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S000437021100021X},
author = {Matthias Knorr and JosÃ© JÃºlio Alferes and Pascal Hitzler},
keywords = {Knowledge representation, Description logics and ontologies, Non-monotonic reasoning, Logic programming, Semantic Web},
abstract = {An important question for the upcoming Semantic Web is how to best combine open world ontology languages, such as the OWL-based ones, with closed world rule-based languages. One of the most mature proposals for this combination is known as hybrid MKNF knowledge bases (Motik and Rosati, 2010 [52]), and it is based on an adaptation of the Stable Model Semantics to knowledge bases consisting of ontology axioms and rules. In this paper we propose a well-founded semantics for nondisjunctive hybrid MKNF knowledge bases that promises to provide better efficiency of reasoning, and that is compatible with both the OWL-based semantics and the traditional Well-Founded Semantics for logic programs. Moreover, our proposal allows for the detection of inconsistencies, possibly occurring in tightly integrated ontology axioms and rules, with only little additional effort. We also identify tractable fragments of the resulting language.}
}
@article{BULLING201697,
title = {Norm-based mechanism design},
journal = {Artificial Intelligence},
volume = {239},
pages = {97-142},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300789},
author = {Nils Bulling and Mehdi Dastani},
keywords = {Norms, Multi-agent systems, Mechanism design},
abstract = {The increasing presence of autonomous (software) systems in open environments in general, and the complex interactions taking place among them in particular, require flexible control and coordination mechanisms to guarantee desirable overall system level properties without limiting the autonomy of the involved systems. In artificial intelligence, and in particular in the multi-agent systems research field, social laws, norms, and sanctions have been widely proposed as flexible means for coordinating the behaviour of autonomous agents in multi-agent settings. Recently, many languages have been proposed to specify and implement norm-based environments where the behaviour of autonomous agents is monitored, evaluated based on norms, and possibly sanctioned if norms are violated. In this paper, we first introduce a formal setting of multi-agent environments based on concurrent game structures which abstracts from concrete specification languages. We extend this formal setting with norms and sanctions, and show how concepts from mechanism design can be used to formally analyse and verify whether a specific behaviour can be enforced (or implemented) if agents follow their subjective preferences. We relate concepts from mechanism design to our setting, where agents' preferences are modelled by linear time temporal logic (LTL) formulae. This proposal bridges the gap between norms and mechanism design allowing us to formally study and analyse the effect of norms and sanctions on the behaviour of rational agents. The proposed machinery can be used to check whether specific norms and sanctions have the designer's expected effect on the rational agents' behaviour or if a set of norms and sanctions that realise the effect exists at all. We investigate the computational complexity of our framework, focusing on its implementation in Nash equilibria and we show that it is located at the second and third level of the polynomial hierarchy. Despite this high complexity, on the positive side, these results are in line with existing complexity results of related problems. Finally, we propose a concrete executable specification language that can be used to implement multi-agent environments. We show that the proposed specification language generates specific concurrent game structures and that the abstract multi-agent environment setting can be applied to study and analyse the behaviour of multi-agent programs with and without norms.}
}
@article{TRAN201753,
title = {Hierarchical semi-Markov conditional random fields for deep recursive sequential data},
journal = {Artificial Intelligence},
volume = {246},
pages = {53-85},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300231},
author = {Truyen Tran and Dinh Phung and Hung Bui and Svetha Venkatesh},
keywords = {Deep nested sequential processes, Hierarchical semi-Markov conditional random field, Partial labelling, Constrained inference, Numerical scaling},
abstract = {We present the hierarchical semi-Markov conditional random field (HSCRF), a generalisation of linear-chain conditional random fields to model deep nested Markov processes. It is parameterised as a conditional log-linear model and has polynomial time algorithms for learning and inference. We derive algorithms for partially-supervised learning and constrained inference. We develop numerical scaling procedures that handle the overflow problem. We show that when depth is two, the HSCRF can be reduced to the semi-Markov conditional random fields. Finally, we demonstrate the HSCRF on two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. The HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases.}
}
@article{EBENDT20091310,
title = {Weighted Aâˆ— search â€“ unifying view and application},
journal = {Artificial Intelligence},
volume = {173},
number = {14},
pages = {1310-1342},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S000437020900068X},
author = {RÃ¼diger Ebendt and Rolf Drechsler},
keywords = {Planning, Search, Heuristic search, , Weighted , BDD, STRIPS},
abstract = {The Aâˆ— algorithm is a well-known heuristic best-first search method. Several performance-accelerated extensions of the exact Aâˆ— approach are known. Interesting examples are approximate algorithms where the heuristic function used is inflated by a weight (often referred to as weighted Aâˆ—). These methods guarantee a bounded suboptimality. As a technical contribution, this paper presents the previous results related to weighted Aâˆ— from authors like Pohl, Pearl, Kim, Likhachev and others in a more condensed and unifying form. With this unified view, a novel general bound on suboptimality of the result is derived. In the case of avoiding any reopening of expanded states, for Ïµ>0, this bound is (1+Ïµ)âŒŠN2âŒ‹ where N is an upper bound on an optimal solution length. Binary Decision Diagrams (BDDs) are well-known to AI, e.g. from set-based exploration of sparse-memory and symbolic manipulation of state spaces. The problem of exact or approximate BDD minimization is introduced as a possible new challenge for heuristic search. Like many classical AI domains, this problem is motivated by real-world applications. Several variants of weighted Aâˆ— search are applied to problems of BDD minimization and the more classical domains like blocksworld and sliding-tile puzzles. For BDD minimization, the comparison of the evaluated methods also includes previous heuristic and simulation-based methods such as Rudell's hill-climbing based sifting algorithm, Simulated Annealing and Evolutionary Algorithms. A discussion of the results obtained in the different problem domains gives our experiences with weighted Aâˆ—, which is of value for the AI practitioner.}
}
@article{SHAPIRO2011165,
title = {Iterated belief change in the situation calculus},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {165-192},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000391},
author = {Steven Shapiro and Maurice Pagnucco and Yves LespÃ©rance and Hector J. Levesque},
keywords = {Knowledge representation and reasoning, Reasoning about action and change, Situation calculus, Belief change},
abstract = {John McCarthy's situation calculus has left an enduring mark on artificial intelligence research. This simple yet elegant formalism for modelling and reasoning about dynamic systems is still in common use more than forty years since it was first proposed. The ability to reason about action and change has long been considered a necessary component for any intelligent system. The situation calculus and its numerous extensions as well as the many competing proposals that it has inspired deal with this problem to some extent. In this paper, we offer a new approach to belief change associated with performing actions that addresses some of the shortcomings of these approaches. In particular, our approach is based on a well-developed theory of action in the situation calculus extended to deal with belief. Moreover, by augmenting this approach with a notion of plausibility over situations, our account handles nested belief, belief introspection, mistaken belief, and handles belief revision and belief update together with iterated belief change.}
}
@article{GEIB20091101,
title = {A probabilistic plan recognition algorithm based on plan tree grammars},
journal = {Artificial Intelligence},
volume = {173},
number = {11},
pages = {1101-1132},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000459},
author = {Christopher W. Geib and Robert P. Goldman},
keywords = {Plan recognition, Bayesian methods, Probabilistic grammars, Task tracking, Intent inference, Goal recognition, Action grammars},
abstract = {We present the PHATT algorithm for plan recognition. Unlike previous approaches to plan recognition, PHATT is based on a model of plan execution. We show that this clarifies several difficult issues in plan recognition including the execution of multiple interleaved root goals, partially ordered plans, and failing to observe actions. We present the PHATT algorithm's theoretical basis, and an implementation based on tree structures. We also investigate the algorithm's complexity, both analytically and empirically. Finally, we present PHATT's integrated constraint reasoning for parametrized actions and temporal constraints.}
}
@article{MU201852,
title = {Measuring inconsistency with constraints for propositional knowledge bases},
journal = {Artificial Intelligence},
volume = {259},
pages = {52-90},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218300523},
author = {Kedian Mu},
keywords = {Inconsistency, Constraints, Causality, Bipartite graph},
abstract = {Measuring inconsistency has been considered as a necessary starting point to understand the nature of inconsistency in a knowledge base better. For practical applications, however, we often have to face some constraints on resolving inconsistency. In this paper, we propose a graph-based approach to measuring the inconsistency for a propositional knowledge base with one or both of two typical types of constraints on modifying formulas. Here the first type of constraint, called the hard constraint, describes a pair of sets of formulas such that all the formulas in the first set should be protected from being modified on the condition that all the formulas in the second set must be modified in order to restore the consistency of that base, while the second type, called the soft constraint, describes a set of pairs of formulas that are not allowed to be modified together. At first, we use a bipartite graph to represent the relation between formulas and minimal inconsistent subsets of a knowledge base. Then we show that such a graph-based representation allows us to characterize the inconsistency with constraints in a concise way. Based on this characterization, we thus propose measures for the degree of inconsistency and for the responsibility of each formula for the inconsistency of a knowledge base with constraints, respectively. Finally, we show that these measures can be well explained based on Halpern and Pearl's causal model and Chockler and Halpern's notion of responsibility.}
}
@article{GROVE1995311,
title = {Naming and identity in epistemic logic part II: a first-order logic for naming},
journal = {Artificial Intelligence},
volume = {74},
number = {2},
pages = {311-350},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)98593-D},
url = {https://www.sciencedirect.com/science/article/pii/000437029598593D},
author = {Adam J. Grove},
abstract = {Modal epistemic logics for many agents sometimes ignore or simplify the distinction between the agents themselves, and the names these agents use when reasoning about each other. We consider problems motivated by practical computer science applications, and show that the simplest theories of naming are often inadequate. The issues we raise are related to some well-known philosophical concerns, such as indexical descriptions, de re knowledge, and the problem of referring to nonexistent objects. However, our emphasis is on epistemic logic as a descriptive tool for distributed systems and artificial intelligence applications, which leads to some nonstandard solutions. The main technical result of this paper is a first-order modal logic, specified both axiomatically and semantically (by a variant of possible-worlds semantics), that is expressive enough to cope with all the difficulties we discuss.}
}
@article{BESSIERE20091054,
title = {Range and Roots: Two common patterns for specifying and propagating counting and occurrence constraints},
journal = {Artificial Intelligence},
volume = {173},
number = {11},
pages = {1054-1078},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209000332},
author = {Christian Bessiere and Emmanuel Hebrard and Brahim Hnich and Zeynep Kiziltan and Toby Walsh},
keywords = {Constraint programming, Constraint satisfaction, Global constraints, Open global constraints, Decompositions},
abstract = {We propose Range and Roots which are two common patterns useful for specifying a wide range of counting and occurrence constraints. We design specialised propagation algorithms for these two patterns. Counting and occurrence constraints specified using these patterns thus directly inherit a propagation algorithm. To illustrate the capabilities of the Range and Roots constraints, we specify a number of global constraints taken from the literature. Preliminary experiments demonstrate that propagating counting and occurrence constraints using these two patterns leads to a small loss in performance when compared to specialised global constraints and is competitive with alternative decompositions using elementary constraints.}
}
@article{EITER20101172,
title = {Updating action domain descriptions},
journal = {Artificial Intelligence},
volume = {174},
number = {15},
pages = {1172-1221},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001001},
author = {Thomas Eiter and Esra Erdem and Michael Fink and JÃ¡n Senko},
keywords = {Knowledge representation, Reasoning about actions and change, Theory change, Action languages, Preference-based semantics},
abstract = {Incorporating new information into a knowledge base is an important problem which has been widely investigated. In this paper, we study this problem in a formal framework for reasoning about actions and change. In this framework, action domains are described in an action language whose semantics is based on the notion of causality. Unlike the formalisms considered in the related work, this language allows straightforward representation of non-deterministic effects and indirect effects of (possibly concurrent) actions, as well as state constraints; therefore, the updates can be more general than elementary statements. The expressivity of this formalism allows us to study the update of an action domain description with a more general approach compared to related work. First of all, we consider the update of an action description with respect to further criteria, for instance, by ensuring that the updated description entails some observations, assertions, or general domain properties that constitute further constraints that are not expressible in an action description in general. Moreover, our framework allows us to discriminate amongst alternative updates of action domain descriptions and to single out a most preferable one, based on a given preference relation possibly dependent on the specified criteria. We study semantic and computational aspects of the update problem, and establish basic properties of updates as well as a decomposition theorem that gives rise to a divide and conquer approach to updating action descriptions under certain conditions. Furthermore, we study the computational complexity of decision problems around computing solutions, both for the generic setting and for two particular preference relations, viz. set-inclusion and weight-based preference. While deciding the existence of solutions and recognizing solutions are PSPACE-complete problems in general, the problems fall back into the polynomial hierarchy under restrictions on the additional constraints. We finally discuss methods to compute solutions and approximate solutions (which disregard preference). Our results provide a semantic and computational basis for developing systems that incorporate new information into action domain descriptions in an action language, in the presence of additional constraints.}
}
@article{HOFFART201328,
title = {YAGO2: A spatially and temporally enhanced knowledge base from Wikipedia},
journal = {Artificial Intelligence},
volume = {194},
pages = {28-61},
year = {2013},
note = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000719},
author = {Johannes Hoffart and Fabian M. Suchanek and Klaus Berberich and Gerhard Weikum},
keywords = {Ontologies, Knowledge bases, Spatio-temporal facts, Information extraction},
abstract = {We present YAGO2, an extension of the YAGO knowledge base, in which entities, facts, and events are anchored in both time and space. YAGO2 is built automatically from Wikipedia, GeoNames, and WordNet. It contains 447 million facts about 9.8 million entities. Human evaluation confirmed an accuracy of 95% of the facts in YAGO2. In this paper, we present the extraction methodology, the integration of the spatio-temporal dimension, and our knowledge representation SPOTL, an extension of the original SPO-triple model to time and space.}
}
@article{ANGIULLI20081837,
title = {Outlier detection using default reasoning},
journal = {Artificial Intelligence},
volume = {172},
number = {16},
pages = {1837-1872},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208000982},
author = {Fabrizio Angiulli and Rachel {Ben-Eliyahu â€“ Zohary} and Luigi Palopoli},
keywords = {Default logic, Disjunctive logic programming, Knowledge representation, Nonmonotonic reasoning, Computational complexity, Data mining, Outlier detection},
abstract = {Default logics are usually used to describe the regular behavior and normal properties of domain elements. In this paper we suggest, conversely, that the framework of default logics can be exploited for detecting outliers. Outliers are observations expressed by sets of literals that feature unexpected semantical characteristics. These sets of literals are selected among those explicitly embodied in the given knowledge base. Hence, essentially we perceive outlier detection as a knowledge discovery technique. This paper defines the notion of outlier in two related formalisms for specifying defaults: Reiter's default logic and extended disjunctive logic programs. For each of the two formalisms, we show that finding outliers is quite complex. Indeed, we prove that several versions of the outlier detection problem lie over the second level of the polynomial hierarchy. We believe that a thorough complexity analysis, as done here, is a useful preliminary step towards developing effective heuristics and exploring tractable subsets of outlier detection problems.}
}
@article{BASILI199659,
title = {An empirical symbolic approach to natural language processing},
journal = {Artificial Intelligence},
volume = {85},
number = {1},
pages = {59-99},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00116-6},
url = {https://www.sciencedirect.com/science/article/pii/0004370295001166},
author = {Roberto Basili and Maria Teresa Pazienza and Paola Velardi},
abstract = {Empirical methods in the field of natural language processing (NLP) are usually based on a probabilistic model of language. These methods recently gained popularity because of the claim that they provide a better coverage of language phenomena. Though this claim is not entirely proved, empirical methods certainly outperform in this regard rationalist, or symbolic, methods. However, empirical methods provide a probabilistic, not conceptual, explanation of the analyzed linguistic phenomena. Probabilistic systems do â€œworkâ€ in real applications, and this is meritorious, but in our view they are intrinsically unable to provide insight into the mechanisms of human communication, because the output is represented by plain words, or word clusters, with attached probabilities. Eventually, a human analyst must make sense of these data. In the past few years, we explored the possibility of combining the advantages of empirical and rationalist approaches in NLP. Our objective was to define methods for lexical knowledge acquisition that are both scalable and linguistically â€œappealingâ€, that is, amenable to a theoretically founded analysis of language. In this paper we describe and evaluate the results of a large-scale lexical learning system, ARIOSTO_LEX, that uses a combination of probabilistic and knowledge-based methods for the acquisition of selectional restrictions of words in sublanguages. We present many experimental data obtained from different corpora in different domains and languages, and show that the acquired lexical data not only have practical applications in NLP, but they are indeed useful for a comparative analysis of sublanguages. Importantly, ARIOSTO_LEX shed light on recurrent linguistic phenomena that have a problematic impact on the large-scale applicability of commonly used NLP techniques.}
}
@article{LAKEMEYER2011142,
title = {A semantic characterization of a useful fragment of the situation calculus with knowledge},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {142-164},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S000437021000041X},
author = {Gerhard Lakemeyer and Hector J. Levesque},
keywords = {Knowledge representation, Reasoning about action},
abstract = {The situation calculus, as proposed by McCarthy and Hayes, and developed over the last decade by Reiter and co-workers, is reconsidered. A new logical variant called ES is proposed that captures much of the expressive power of the original, but where certain technical results are much more easily proved. This is illustrated using two existing non-trivial results: the determinacy of knowledge theorem of Reiter and the regression theorem, which reduces reasoning about the future to reasoning about the initial situation. Furthermore, we show the correctness of our approach by embedding ES in Reiter's situation calculus.}
}
@article{SANDHLOM199799,
title = {Coalitions among computationally bounded agents},
journal = {Artificial Intelligence},
volume = {94},
number = {1},
pages = {99-137},
year = {1997},
note = {Economic Principles of Multi-Agent Systems},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00030-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000301},
author = {Tuomas W. Sandhlom and Victor R.T Lesser},
keywords = {Distributed AI, Multiagent systems, Coalition formation, Negotiation, Bounded rationality, Resource-bounded reasoning, Game theory},
abstract = {This paper analyzes coalitions among self-interested agents that need to solve combinatorial optimization problems to operate efficiently in the world. By colluding (coordinating their actions by solving a joint optimization problem) the agents can sometimes save costs compared to operating individually. A model of bounded rationality is adopted where computation resources are costly. It is not worthwhile solving the problems optimally: solution quality is decision-theoretically traded off against computation cost. A normative, application- and protocol-independent theory of coalitions among bounded-rational agents is devised. The optimal coalition structure and its stability are significantly affected by the agents' algorithms' performance profiles and the cost of computation. This relationship is first analyzed theoretically. Then a domain classification including rational and bounded-rational agents is introduced. Experimental results are presented in vehicle routing with real data from five dispatch centers. This problem is NP-complete and the instances are so large thatâ€”with current technologyâ€”any agent's rationality is bounded by computational complexity.}
}
@article{FLATI201666,
title = {MultiWiBi: The multilingual Wikipedia bitaxonomy project},
journal = {Artificial Intelligence},
volume = {241},
pages = {66-102},
year = {2016},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216300959},
author = {Tiziano Flati and Daniele Vannella and Tommaso Pasini and Roberto Navigli},
keywords = {Taxonomy extraction, Taxonomy induction, Machine learning, Natural language processing, Collaborative resources, Wikipedia},
abstract = {We present MultiWiBi, an approach to the automatic creation of two integrated taxonomies for Wikipedia pages and categories written in different languages. In order to create both taxonomies in an arbitrary language, we first build them in English and then project the two taxonomies to other languages automatically, without the help of language-specific resources or tools. The process crucially leverages a novel algorithm which exploits the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show that the taxonomical information in MultiWiBi is characterized by a higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet, LHD and WikiTaxonomy, also across languages. MultiWiBi is available online at http://wibitaxonomy.org/multiwibi.}
}
@article{ROY2005170,
title = {Semiotic schemas: A framework for grounding language in action and perception},
journal = {Artificial Intelligence},
volume = {167},
number = {1},
pages = {170-205},
year = {2005},
note = {Connecting Language to the World},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205001037},
author = {Deb Roy},
keywords = {Grounding, Representation, Language, Situated, Embodied, Semiotic, Schemas, Meaning, Perception, Action, Cross-modal, Multimodal},
abstract = {A theoretical framework for grounding language is introduced that provides a computational path from sensing and motor action to words and speech acts. The approach combines concepts from semiotics and schema theory to develop a holistic approach to linguistic meaning. Schemas serve as structured beliefs that are grounded in an agent's physical environment through a causal-predictive cycle of action and perception. Words and basic speech acts are interpreted in terms of grounded schemas. The framework reflects lessons learned from implementations of several language processing robots. It provides a basis for the analysis and design of situated, multimodal communication systems that straddle symbolic and non-symbolic realms.}
}
@article{MINDOLIN20111092,
title = {Contracting preference relations for database applications},
journal = {Artificial Intelligence},
volume = {175},
number = {7},
pages = {1092-1121},
year = {2011},
note = {Representing, Processing, and Learning Preferences: Theoretical and Practical Challenges},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002006},
author = {Denis Mindolin and Jan Chomicki},
keywords = {Preference contraction, Preference change, Preference query},
abstract = {The binary relation framework has been shown to be applicable to many real-life preference handling scenarios. Here we study preference contraction: the problem of discarding selected preferences. We argue that the property of minimality and the preservation of strict partial orders are crucial for contractions. Contractions can be further constrained by specifying which preferences should be protected. We consider preference relations that are finite or finitely representable using preference formulas. We present algorithms for computing minimal and preference-protecting minimal contractions for finite as well as finitely representable preference relations. We study relationships between preference change in the binary relation framework and belief change in the belief revision theory. We evaluate the proposed algorithms experimentally and present the results.}
}
@article{SCHAUB19981,
title = {Prolog technology for default reasoning: proof theory and compilation techniques},
journal = {Artificial Intelligence},
volume = {106},
number = {1},
pages = {1-75},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00092-7},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000927},
author = {Torsten Schaub and Stefan BrÃ¼ning},
keywords = {Default reasoning, Automated reasoning, Default Logic, Model elimination, PTTP, Model-based consistency checking},
abstract = {The aim of this work is to show how Prolog technology can be used for efficient implementation of query answering in default logics. The idea is to translate a default theory along with a query into a Prolog program and a Prolog query such that the original query is derivable from the default theory iff the Prolog query is derivable from the Prolog program. In order to comply with the goal-oriented proof search of this approach, we focus on default theories supporting local proof procedures, as exemplified by so-called semi-monotonic default theories. Although this does not capture general default theories under Reiter's interpretation, it does so under alternative ones'. For providing theoretical underpinnings, we found the resulting compilation techniques upon a top-down proof procedure based on model elimination. We show how the notion of a model elimination proof can be refined to capture default proofs and how standard techniques for implementing and improving model elimination theorem provers (regularity, lemmas) can be adapted and extended to default reasoning. This integrated approach allows us to push the concepts needed for handling defaults from the underlying calculus onto the resulting compilation techniques. This method for default theorem proving is complemented by a model-based approach to incremental consistency checking. We show that the crucial task of consistency checking can benefit from keeping models in order to restrict the attention to ultimately necessary consistency checks. This is supported by the concept of default lemmas that allow for an additional avoidance of redundancy.}
}
@article{ENDRISS201786,
title = {Graph aggregation},
journal = {Artificial Intelligence},
volume = {245},
pages = {86-114},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300024},
author = {Ulle Endriss and Umberto Grandi},
keywords = {Social choice theory, Collective rationality, Impossibility theorems, Graph theory, Modal logic, Preference aggregation, Belief merging, Consensus clustering, Argumentation theory},
abstract = {Graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). In this paper, we introduce a formal framework for graph aggregation grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. We consider both common properties of graphs, such as transitivity and reflexivity, and arbitrary properties expressible in certain fragments of modal logic. Our results establish several connections between the types of properties preserved under aggregation and the choice-theoretic axioms satisfied by the rules used. The most important of these results is a powerful impossibility theorem that generalises Arrow's seminal result for the aggregation of preference orders to a large collection of different types of graphs.}
}
@article{GRATCH1996101,
title = {A statistical approach to adaptive problem solving},
journal = {Artificial Intelligence},
volume = {88},
number = {1},
pages = {101-142},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00011-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000112},
author = {Jonathan Gratch and Gerald DeJong},
abstract = {Domain independent general purpose problem solving techniques are desirable from the standpoints of software engineering and human computer interaction. They employ declarative and modular knowledge representations and present a constant homogeneous interface to the user, untainted by the peculiarities of the specific domain of interest. Unfortunately, this very insulation from domain details often precludes effective problem solving behavior. General approaches have proven successful in complex real-world situations only after a tedious cycle of manual experimentation and modification. Machine learning offers the prospect of automating this adaptation cycle, reducing the burden of domain specific tuning and reconciling the conflicting needs of generality and efficacy. A principal impediment to adaptive techniques is the utility problem: even if the acquired information is accurate and is helpful in isolated cases, it may degrade overall problem solving performance under difficult to predict circumstances. We develop a formal characterization of the utility problem and introduce COMPOSER, a statistically rigorous learning approach which avoids the utility problem. COMPOSER has been successfully applied to learning heuristics for planning and scheduling systems. This article includes theoretical results and an extensive empirical evaluation. The approach is shown to outperform significantly several other leading approaches to the utility problem.}
}
@article{LIKHACHEV2009696,
title = {Probabilistic planning with clear preferences on missing information},
journal = {Artificial Intelligence},
volume = {173},
number = {5},
pages = {696-721},
year = {2009},
note = {Advances in Automated Plan Generation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001860},
author = {Maxim Likhachev and Anthony Stentz},
keywords = {Planning with uncertainty, Planning with missing information, Partially Observable Markov Decision Processes, Planning, Heuristic search},
abstract = {For many real-world problems, environments at the time of planning are only partially-known. For example, robots often have to navigate partially-known terrains, planes often have to be scheduled under changing weather conditions, and car route-finders often have to figure out paths with only partial knowledge of traffic congestions. While general decision-theoretic planning that takes into account the uncertainty about the environment is hard to scale to large problems, many such problems exhibit a special property: one can clearly identify beforehand the best (called clearly preferred) values for the variables that represent the unknowns in the environment. For example, in the robot navigation problem, it is always preferred to find out that an initially unknown location is traversable rather than not, in the plane scheduling problem, it is always preferred for the weather to remain a good flying weather, and in route-finding problem, it is always preferred for the road of interest to be clear of traffic. It turns out that the existence of the clear preferences can be used to construct an efficient planner, called PPCP (Probabilistic Planning with Clear Preferences), that solves these planning problems by running a series of deterministic low-dimensional A*-like searches. In this paper, we formally define the notion of clear preferences on missing information, present the PPCP algorithm together with its extensive theoretical analysis, describe several useful extensions and optimizations of the algorithm and demonstrate the usefulness of PPCP on several applications in robotics. The theoretical analysis shows that once converged, the plan returned by PPCP is guaranteed to be optimal under certain conditions. The experimental analysis shows that running a series of fast low-dimensional searches turns out to be much faster than solving the full problem at once since memory requirements are much lower and deterministic searches are orders of magnitude faster than probabilistic planning.}
}
@article{SARIBATUR2021103563,
title = {Abstraction for non-ground answer set programs},
journal = {Artificial Intelligence},
volume = {300},
pages = {103563},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103563},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221001144},
author = {Zeynep G. Saribatur and Thomas Eiter and Peter SchÃ¼ller},
keywords = {Abstraction, Answer set programming, Declarative problem solving, Knowledge representation and reasoning, Nonmonotonic formalisms, Explaining unsatisfiability, Counterexample-guided abstraction and refinement},
abstract = {Abstraction is an important technique utilized by humans in model building and problem solving, in order to figure out key elements and relevant details of a world of interest. This naturally has led to investigations of using abstraction in AI and Computer Science to simplify problems, especially in the design of intelligent agents and automated problem solving. By omitting details, scenarios are reduced to ones that are easier to deal with and to understand, where further details are added back only when they matter. Despite the fact that abstraction is a powerful technique, it has not been considered much in the context of nonmonotonic knowledge representation and reasoning, and specifically not in Answer Set Programming (ASP), apart from some related simplification methods. In this work, we introduce a notion for abstracting from the domain of an ASP program such that the domain size shrinks while the set of answer sets (i.e., models) of the program is over-approximated. To achieve the latter, the program is transformed into an abstract program over the abstract domain while preserving the structure of the rules. We show in elaboration how this can be also achieved for single or multiple sub-domains (sorts) of a domain, and in case of structured domains like grid environments in which structure should be preserved. Furthermore, we introduce an abstraction-&-refinement methodology that makes it possible to start with an initial abstraction and to achieve automatically an abstraction with an associated abstract answer set that matches an answer set of the original program, provided that the program is satisfiable. Experiments based on prototypical implementations reveal the potential of the approach for problem analysis, by its ability to focus on the parts of the program that cause unsatisfiability and by achieving concrete abstract answer sets that merely reflect relevant details. This makes domain abstraction an interesting topic of research whose further use in important areas like Explainable AI remains to be explored.}
}
@article{GOGATE201238,
title = {Importance sampling-based estimation over AND/OR search spaces for graphical models},
journal = {Artificial Intelligence},
volume = {184-185},
pages = {38-77},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000148},
author = {Vibhav Gogate and Rina Dechter},
keywords = {Probabilistic inference, Approximate inference, Graphical models, Importance sampling, Bayesian networks, Constraint networks, Markov networks, Model counting, Variance reduction},
abstract = {It is well known that the accuracy of importance sampling can be improved by reducing the variance of its sample mean and therefore variance reduction schemes have been the subject of much research. In this paper, we introduce a family of variance reduction schemes that generalize the sample mean from the conventional OR search space to the AND/OR search space for graphical models. The new AND/OR sample means allow trading time and space with variance. At one end is the AND/OR sample tree mean which has the same time and space complexity as the conventional OR sample tree mean but has smaller variance. At other end is the AND/OR sample graph mean which requires more time and space to compute but has the smallest variance. Theoretically, we show that the variance is smaller in the AND/OR space because the AND/OR sample mean is defined over a larger virtual sample size compared with the OR sample mean. Empirically, we demonstrate that the AND/OR sample mean is far closer to the true mean than the OR sample mean.}
}
@article{YAZDANI2013176,
title = {Computing text semantic relatedness using the contents and links of a hypertext encyclopedia},
journal = {Artificial Intelligence},
volume = {194},
pages = {176-202},
year = {2013},
note = {Artificial Intelligence, Wikipedia and Semi-Structured Resources},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000744},
author = {Majid Yazdani and Andrei Popescu-Belis},
keywords = {Text semantic relatedness, Distance metric learning, Learning to rank, Random walk, Text classification, Text similarity, Document clustering, Information retrieval, Word similarity},
abstract = {We propose a method for computing semantic relatedness between words or texts by using knowledge from hypertext encyclopedias such as Wikipedia. A network of concepts is built by filtering the encyclopediaÊ¼s articles, each concept corresponding to an article. Two types of weighted links between concepts are considered: one based on hyperlinks between the texts of the articles, and another one based on the lexical similarity between them. We propose and implement an efficient random walk algorithm that computes the distance between nodes, and then between sets of nodes, using the visiting probability from one (set of) node(s) to another. Moreover, to make the algorithm tractable, we propose and validate empirically two truncation methods, and then use an embedding space to learn an approximation of visiting probability. To evaluate the proposed distance, we apply our method to four important tasks in natural language processing: word similarity, document similarity, document clustering and classification, and ranking in information retrieval. The performance of the method is state-of-the-art or close to it for each task, thus demonstrating the generality of the knowledge resource. Moreover, using both hyperlinks and lexical similarity links improves the scores with respect to a method using only one of them, because hyperlinks bring additional real-world knowledge not captured by lexical similarity.}
}
@article{GERASIMOVA2022103738,
title = {A tetrachotomy of ontology-mediated queries with a covering axiom},
journal = {Artificial Intelligence},
volume = {309},
pages = {103738},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103738},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000789},
author = {Olga Gerasimova and Stanislav Kikot and Agi Kurucz and Vladimir Podolskii and Michael Zakharyaschev},
keywords = {Ontology-mediated query, Description logic, Datalog, Disjunctive datalog, First-order rewritability, Data complexity},
abstract = {Our concern is the problem of efficiently determining the data complexity of answering queries mediated by description logic ontologies and constructing their optimal rewritings to standard database queries. Originated in ontology-based data access and datalog optimisation, this problem is known to be computationally very complex in general, with no explicit syntactic characterisations available. In this article, aiming to understand the fundamental roots of this difficulty, we strip the problem to the bare bones and focus on Boolean conjunctive queries mediated by a simple covering axiom stating that one class is covered by the union of two other classes. We show that, on the one hand, these rudimentary ontology-mediated queries, called disjunctive sirups (or d-sirups), capture many features and difficulties of the general case. For example, answering d-sirups is Î 2p-complete for combined complexity and can be in Image 1 or L-, NL-, P-, or coNP-complete for data complexity (with the problem of recognising FO-rewritability of d-sirups being 2ExpTime-hard); some d-sirups only have exponential-size resolution proofs, some only double-exponential-size positive existential FO-rewritings and single-exponential-size nonrecursive datalog rewritings. On the other hand, we prove a few partial sufficient and necessary conditions of FO- and (symmetric/linear-) datalog rewritability of d-sirups. Our main technical result is a complete and transparent syntactic Image 1/NL/P/coNP tetrachotomy of d-sirups with disjoint covering classes and a path-shaped Boolean conjunctive query. To obtain this tetrachotomy, we develop new techniques for establishing P- and coNP-hardness of answering non-Horn ontology-mediated queries as well as showing that they can be answered in NL.}
}
@article{DEJONG1997173,
title = {Permissive planning: extending classical planning to uncertain task domains},
journal = {Artificial Intelligence},
volume = {89},
number = {1},
pages = {173-217},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00031-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000318},
author = {Gerald F. DeJong and Scott W. Bennett},
keywords = {Planning, Learning, Uncertainty, Machine learning, Explanation-based learning, Planning bias},
abstract = {Uncertainty, inherent in most real-world domains, can cause failure of apparently sound classical plans. On the other hand, reasoning with representations that explicitly reflect uncertainty can engender significant, even prohibitive, additional computational costs. This paper contributes a novel approach to planning in uncertain domains. The approach is an extension of classical planning. Machine learning is employed to adjust planner bias in response to execution failures. Thus, the classical planner is conditioned towards producing plans that tend to work when executed in the world. The planner's representations are simple and crisp; uncertainty is represented and reasoned about only during learning. The user-supplied domain theory is left intact. The operator definitions and the planner's projection ability remain as the domain expert intended them. Some structuring of the planner's bias space is required. But with suitable structuring the approach scales well. The learning converges using no more than a polynomial number of examples. The system then probabilistically guarantees that either the plans produced will achieve their goal when executed or that adequate planning is not possible with the domain theory provided. An implemented robotic system is described.}
}
@article{LEVY1997351,
title = {Automated model selection for simulation based on relevance reasoning},
journal = {Artificial Intelligence},
volume = {96},
number = {2},
pages = {351-394},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00056-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000568},
author = {Alon Y. Levy and Yumi Iwasaki and Richard Fikes},
keywords = {Model formulation, Relevance reasoning, Qualitative reasoning, Simulation},
abstract = {Constructing an appropriate model is a crucial step in performing the reasoning required to successfully answer a query about the behavior of a physical situation. In the compositional modeling approach of Falkenhainer and Forbus (1991), a system is provided with a library of composable pieces of knowledge about the physical world called model fragments. The model construction problem involves selecting appropriate model fragments to describe the situation. Model construction can be considered either for static analysis of a single state or for simulation of dynamic behavior over a sequence of states. The latter is significantly more difficult than the former since one must select model fragments without knowing exactly what will happen in the future states. The model construction problem in general can advantageously be formulated as a problem of reasoning about relevance of knowledge that is available to the system using a general framework for reasoning about relevance described by Levy (1993) and Levy and Sagiv (1993). In this paper, we present a model formulation procedure based on that framework for selecting model fragments efficiently for the case of simulation. For such an algorithm to be useful, the generated model must be adequate for answering the given query and, at the same time, as simple as possible. We define formally the concepts of adequacy and simplicity and show that the algorithm in fact generates an adequate and simplest model.}
}
@article{SKIADOPOULOS200591,
title = {On the consistency of cardinal direction constraints},
journal = {Artificial Intelligence},
volume = {163},
number = {1},
pages = {91-135},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0004370204001730},
author = {Spiros Skiadopoulos and Manolis Koubarakis},
keywords = {Cardinal direction relations, Spatial constraints, Consistency checking, Qualitative spatial reasoning},
abstract = {We present a formal model for qualitative spatial reasoning with cardinal directions utilizing a co-ordinate system. Then, we study the problem of checking the consistency of a set of cardinal direction constraints. We introduce the first algorithm for this problem, prove its correctness and analyze its computational complexity. Utilizing the above algorithm, we prove that the consistency checking of a set of basic (i.e., non-disjunctive) cardinal direction constraints can be performed in O(n5) time. We also show that the consistency checking of a set of unrestricted (i.e., disjunctive and non-disjunctive) cardinal direction constraints is NP-complete. Finally, we briefly discuss an extension to the basic model and outline an algorithm for the consistency checking problem of this extension.}
}
@article{PRZYMUSINSKI1997115,
title = {Autoepistemic logic of knowledge and beliefs},
journal = {Artificial Intelligence},
volume = {95},
number = {1},
pages = {115-154},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00032-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000325},
author = {Teodor C. Przymusinski},
keywords = {Nonmonotonic reasoning, Logics of knowledge and beliefs, Semantics of logic programs and deductive databases},
abstract = {In recent years, various formalizations of nonmonotonic reasoning and different semantics for normal and disjunctive logic programs have been proposed, including autoepistemic logic, circumscription, CWA, GCWA, ECWA, epistemic specifications, stable, well-founded, stationary and static semantics of normal and disjunctive logic programs. In this paper we introduce a simple nonmonotonic knowledge representation framework which isomorphically contains all of the above-mentioned nonmonotonic formalisms and semantics as special cases and yet is significantly more expressive than each one of these formalisms considered individually. The new formalism, called the Autoepistemic Logic of Knowledge and Beliefs, AELB, is obtained by augmenting Moore's autoepistemic logic,AEL, already employing the knowledge operator, L, with an additional belief operator, B. As a result, we are able to reason not only about formulae F which are known to be true (i.e., those for which LF holds) but also about those which are only believed to be true (i.e., those for which BF holds). The proposed logic constitutes a powerful new formalism which can serve as a unifying frame-work for several major nonmonotonic formalisms. It allows us to better understand mutual relationships existing between different formalisms and semantics and enables us to provide them with simpler and more natural definitions. It also naturally leads to new, even more expressive, flexible and modular formalizations and semantics.}
}
@article{TADEPALLI1998177,
title = {Model-based average reward reinforcement learning},
journal = {Artificial Intelligence},
volume = {100},
number = {1},
pages = {177-224},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00002-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000022},
author = {Prasad Tadepalli and DoKyeong Ok},
keywords = {Machine learning, Reinforcement learning, Average reward, Model-based, Exploration, Bayesian networks, Linear regression, AGV scheduling},
abstract = {Reinforcement Learning (RL) is the study of programs that improve their performance by receiving rewards and punishments from the environment. Most RL methods optimize the discounted total reward received by an agent, while, in many domains, the natural criterion is to optimize the average reward per time step. In this paper, we introduce a model-based Averagereward Reinforcement Learning method called H-learning and show that it converges more quickly and robustly than its discounted counterpart in the domain of scheduling a simulated Automatic Guided Vehicle (AGV). We also introduce a version of H-learning that automatically explores the unexplored parts of the state space, while always choosing greedy actions with respect to the current value function. We show that this â€œAuto-exploratory H-Learningâ€ performs better than the previously studied exploration strategies. To scale H-learning to larger state spaces, we extend it to learn action models and reward functions in the form of dynamic Bayesian networks, and approximate its value function using local linear regression. We show that both of these extensions are effective in significantly reducing the space requirement of H-learning and making it converge faster in some AGV scheduling tasks.}
}
@article{BONATTI2020103389,
title = {Real-time reasoning in OWL2 for GDPR compliance},
journal = {Artificial Intelligence},
volume = {289},
pages = {103389},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2020.103389},
url = {https://www.sciencedirect.com/science/article/pii/S0004370220301399},
author = {Piero A. Bonatti and Luca Ioffredo and Iliana M. Petrova and Luigi Sauro and Ida R. Siahaan},
keywords = {Tractable OWL2 fragments, Structural subsumption, Import-by-query, Knowledge compilation, Semantic policy languages, GDPR},
abstract = {This paper shows how knowledge representation and reasoning techniques can be used to support organizations in complying with the GDPR, that is, the new European data protection regulation. This work is carried out in a European H2020 project called SPECIAL. Data usage policies, the consent of data subjects, and selected fragments of the GDPR are encoded in a fragment of OWL2 called PL (policy language); compliance checking and policy validation are reduced to subsumption checking and concept consistency checking. This work proposes a satisfactory tradeoff between the expressiveness requirements on PL posed by the modeling of the GDPR, and the scalability requirements that arise from the use cases provided by SPECIAL's industrial partners. Real-time compliance checking is achieved by means of a specialized reasoner, called PLR, that leverages knowledge compilation and structural subsumption techniques. The performance of a prototype implementation of PLR is analyzed through systematic experiments, and compared with the performance of other important reasoners. Moreover, we show how PL and PLR can be extended to support richer ontologies, by means of import-by-query techniques. We prove novel tractability and intractability results related to PL, and some negative results about the restrictions posed on ontology import.}
}
@article{KOEHLER1996145,
title = {Planning from second principles},
journal = {Artificial Intelligence},
volume = {87},
number = {1},
pages = {145-186},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00113-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370295001131},
author = {Jana Koehler},
abstract = {Planning from second principles by reusing and modifying plans is one way of improving the efficiency of planning systems. In this paper, we study it in the general framework of deductive planning and develop a logical formalization of planning from second principles, which relies on a systematic decomposition of the planning process. Deductive inference processes with clearly defined semantics formalize each of the subtasks a second principles planner has to address. Plan modification, which comprises matching and adaptation tasks, is based on a deductive approach yielding provably correct modified plans. Description logics are introduced as query languages to plan libraries, which leads to a novel and efficient solution to the indexing problem in case-based reasoning. Apart from sequential plans, this approach enables a planner to reuse and modify complex plans containing control structures like conditionals and loops.}
}
@article{LIU2010951,
title = {Reasoning about cardinal directions between extended objects},
journal = {Artificial Intelligence},
volume = {174},
number = {12},
pages = {951-983},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000834},
author = {Weiming Liu and Xiaotong Zhang and Sanjiang Li and Mingsheng Ying},
keywords = {Qualitative spatial reasoning, Cardinal direction calculus, Connected regions, Consistency checking, Maximal canonical solution},
abstract = {Direction relations between extended spatial objects are important commonsense knowledge. Recently, Goyal and Egenhofer proposed a relation model, known as the cardinal direction calculus (CDC), for representing direction relations between connected plane regions. The CDC is perhaps the most expressive qualitative calculus for directional information, and has attracted increasing interest from areas such as artificial intelligence, geographical information science, and image retrieval. Given a network of CDC constraints, the consistency problem is deciding if the network is realizable by connected regions in the real plane. This paper provides a cubic algorithm for checking the consistency of complete networks of basic CDC constraints, and proves that reasoning with the CDC is in general an NP-complete problem. For a consistent complete network of basic CDC constraints, our algorithm returns a â€˜canonicalâ€™ solution in cubic time. This cubic algorithm is also adapted to check the consistency of complete networks of basic cardinal constraints between possibly disconnected regions.}
}
@article{MUSCETTOLA19985,
title = {Remote Agent: to boldly go where no AI system has gone before},
journal = {Artificial Intelligence},
volume = {103},
number = {1},
pages = {5-47},
year = {1998},
note = {Artificial Intelligence 40 years later},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00068-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029800068X},
author = {Nicola Muscettola and P.Pandurang Nayak and Barney Pell and Brian C. Williams},
keywords = {Autonomous agents, Architectures, Constraint-based planning, Scheduling, Execution, Reactive systems, Diagnosis, Recovery, Model-based reasoning},
abstract = {Renewed motives for space exploration have inspired NASA to work toward the goal of establishing a virtual presence in space, through heterogeneous fleets of robotic explorers. Information technology, and Artificial Intelligence in particular, will play a central role in this endeavor by endowing these explorers with a form of computational intelligence that we call remote agents. In this paper we describe the Remote Agent, a specific autonomous agent architecture based on the principles of model-based programming, on-board deduction and search, and goal-directed closed-loop commanding, that takes a significant step toward enabling this future. This architecture addresses the unique characteristics of the spacecraft domain that require highly reliable autonomous operations over long periods of time with tight deadlines, resource constraints, and concurrent activity among tightly coupled subsystems. The Remote Agent integrates constraintbased temporal planning and scheduling, robust multi-threaded execution, and model-based mode identification and reconfiguration. The demonstration of the integrated system as an on-board controller for Deep Space One, NASA's first New Millennium mission, is scheduled for a period of a week in mid 1999. The development of the Remote Agent also provided the opportunity to reassess some of AI's conventional wisdom about the challenges of implementing embedded systems, tractable reasoning, and knowledge representation. We discuss these issues, and our often contrary experiences, throughout the paper.}
}
@article{BENCHCAPON200742,
title = {Audiences in argumentation frameworks},
journal = {Artificial Intelligence},
volume = {171},
number = {1},
pages = {42-71},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S000437020600110X},
author = {Trevor J.M. Bench-Capon and Sylvie Doutre and Paul E. Dunne},
keywords = {Argumentation frameworks, Practical reasoning, Dialogue},
abstract = {Although reasoning about what is the case has been the historic focus of logic, reasoning about what should be done is an equally important capacity for an intelligent agent. Reasoning about what to do in a given situationâ€”termed practical reasoning in the philosophical literatureâ€”has important differences from reasoning about what is the case. The acceptability of an argument for an action turns not only on what is true in the situation, but also on the values and aspirations of the agent to whom the argument is directed. There are three distinctive features of practical reasoning: first, that practical reasoning is situated in a context, directed towards a particular agent at a particular time; second, that since agents differ in their aspirations there is no right answer for all agents, and rational disagreement is always possible; third, that since no agent can specify the relative priority of its aspirations outside of a particular context, such prioritisation must be a product of practical reasoning and cannot be used as an input to it. In this paper we present a framework for practical reasoning which accommodates these three distinctive features. We use the notion of argumentation frameworks to capture the first feature. An extended form of argumentation framework in which values and aspirations can be represented is used to allow divergent opinions for different audiences, and complexity results relating to the extended framework are presented. We address the third feature using a formal description of a dialogue from which preferences over values emerge. Soundness and completeness results for these dialogues are given.}
}
@article{RAMCHURN2007805,
title = {Negotiating using rewards},
journal = {Artificial Intelligence},
volume = {171},
number = {10},
pages = {805-837},
year = {2007},
note = {Argumentation in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000756},
author = {Sarvapali D. Ramchurn and Carles Sierra and LluÃ­s Godo and Nicholas R. Jennings},
keywords = {Persuasive negotiation, Repeated negotiations, Negotiation tactics, Bargaining, Bilateral negotiation},
abstract = {Negotiation is a fundamental interaction mechanism in multi-agent systems because it allows self-interested agents to come to mutually beneficial agreements and partition resources efficiently and effectively. Now, in many situations, the agents need to negotiate with one another many times and so developing strategies that are effective over repeated interactions is an important challenge. Against this background, a growing body of work has examined the use of Persuasive Negotiation (PN), which involves negotiating using rhetorical arguments (such as threats, rewards, or appeals), in trying to convince an opponent to accept a given offer. Such mechanisms are especially suited to repeated encounters because they allow agents to influence the outcomes of future negotiations, while negotiating a deal in the present one, with the aim of producing results that are beneficial to both parties. To this end, in this paper, we develop a comprehensive PN mechanism for repeated interactions that makes use of rewards that can be asked for or given to. Our mechanism consists of two parts. First, a novel protocol that structures the interaction by capturing the commitments that agents incur when using rewards. Second, a new reward generation algorithm that constructs promises of rewards in future interactions as a means of permitting agents to reach better agreements, in a shorter time, in the present encounter. We then go on to develop a specific negotiation tactic, based on this reward generation algorithm, and show that it can achieve significantly better outcomes than existing benchmark tactics that do not use such inducements. Specifically, we show, via empirical evaluation in a Multi-Move Prisoners' Dilemma setting, that our tactic can lead to a 26% improvement in the utility of deals that are made and that 21 times fewer messages need to be exchanged in order to achieve this.}
}
@article{LANG2010799,
title = {Reasoning under inconsistency: A forgetting-based approach},
journal = {Artificial Intelligence},
volume = {174},
number = {12},
pages = {799-823},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000676},
author = {JÃ©rÃ´me Lang and Pierre Marquis},
keywords = {Knowledge representation, Reasoning under inconsistency, Forgetting},
abstract = {In this paper, a fairly general framework for reasoning from inconsistent propositional bases is defined. Variable forgetting is used as a basic operation for weakening pieces of information so as to restore consistency. The key notion is that of recoveries, which are sets of variables whose forgetting enables restoring consistency. Several criteria for defining preferred recoveries are proposed, depending on whether the focus is laid on the relative relevance of the atoms or the relative entrenchment of the pieces of information (or both). Our framework encompasses several previous approaches as specific cases, including reasoning from preferred consistent subsets, and some forms of information merging. Interestingly, the gain in flexibility and generality offered by our framework does not imply a complexity shift compared to these specific cases.}
}
@article{FOX200659,
title = {Robot introspection through learned hidden Markov models},
journal = {Artificial Intelligence},
volume = {170},
number = {2},
pages = {59-113},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S000437020500113X},
author = {Maria Fox and Malik Ghallab and Guillaume Infantes and Derek Long},
keywords = {Stochastic learning, Hidden Markov models, Robot behaviour},
abstract = {In this paper we describe a machine learning approach for acquiring a model of a robot behaviour from raw sensor data. We are interested in automating the acquisition of behavioural models to provide a robot with an introspective capability. We assume that the behaviour of a robot in achieving a task can be modelled as a finite stochastic state transition system. Beginning with data recorded by a robot in the execution of a task, we use unsupervised learning techniques to estimate a hidden Markov model (HMM) that can be used both for predicting and explaining the behaviour of the robot in subsequent executions of the task. We demonstrate that it is feasible to automate the entire process of learning a high quality HMM from the data recorded by the robot during execution of its task. The learned HMM can be used both for monitoring and controlling the behaviour of the robot. The ultimate purpose of our work is to learn models for the full set of tasks associated with a given problem domain, and to integrate these models with a generative task planner. We want to show that these models can be used successfully in controlling the execution of a plan. However, this paper does not develop the planning and control aspects of our work, focussing instead on the learning methodology and the evaluation of a learned model. The essential property of the models we seek to construct is that the most probable trajectory through a model, given the observations made by the robot, accurately diagnoses, or explains, the behaviour that the robot actually performed when making these observations. In the work reported here we consider a navigation task. We explain the learning process, the experimental setup and the structure of the resulting learned behavioural models. We then evaluate the extent to which explanations proposed by the learned models accord with a human observer's interpretation of the behaviour exhibited by the robot in its execution of the task.}
}
@article{ZHUO20101540,
title = {Learning complex action models with quantifiers and logical implications},
journal = {Artificial Intelligence},
volume = {174},
number = {18},
pages = {1540-1569},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001566},
author = {Hankz Hankui Zhuo and Qiang Yang and Derek Hao Hu and Lei Li},
keywords = {Action model learning, Machine learning, Knowledge engineering, Automated planning},
abstract = {Automated planning requires action models described using languages such as the Planning Domain Definition Language (PDDL) as input, but building action models from scratch is a very difficult and time-consuming task, even for experts. This is because it is difficult to formally describe all conditions and changes, reflected in the preconditions and effects of action models. In the past, there have been algorithms that can automatically learn simple action models from plan traces. However, there are many cases in the real world where we need more complicated expressions based on universal and existential quantifiers, as well as logical implications in action models to precisely describe the underlying mechanisms of the actions. Such complex action models cannot be learned using many previous algorithms. In this article, we present a novel algorithm called LAMP (Learning Action Models from Plan traces), to learn action models with quantifiers and logical implications from a set of observed plan traces with only partially observed intermediate state information. The LAMP algorithm generates candidate formulas that are passed to a Markov Logic Network (MLN) for selecting the most likely subsets of candidate formulas. The selected subset of formulas is then transformed into learned action models, which can then be tweaked by domain experts to arrive at the final models. We evaluate our approach in four planning domains to demonstrate that LAMP is effective in learning complex action models. We also analyze the human effort saved by using LAMP in helping to create action models through a user study. Finally, we apply LAMP to a real-world application domain for software requirement engineering to help the engineers acquire software requirements and show that LAMP can indeed help experts a great deal in real-world knowledge-engineering applications.}
}
@article{LUCAS1998295,
title = {Analysis of notions of diagnosis},
journal = {Artificial Intelligence},
volume = {105},
number = {1},
pages = {295-343},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00081-2},
url = {https://www.sciencedirect.com/science/article/pii/S0004370298000812},
author = {Peter J.F. Lucas},
keywords = {Diagnostic systems, Semantics of diagnosis, Formal theory of diagnosis},
abstract = {Various formal theories have been proposed in the literature to capture the notions of diagnosis underlying diagnostic programs. Examples of such notions are: heuristic classification, which is used in systems incorporating empirical knowledge, and model-based diagnosis, which is used in diagnostic systems based on detailed domain models. Typically, such domain models include knowledge of causal, structural, and functional interactions among modelled objects. In this paper, a new set-theoretical framework for the analysis of diagnosis is presented. Basically, the framework distinguishes between â€˜evidence functionsâ€™, which characterize the net impact of knowledge bases for purposes of diagnosis, and â€˜notions of diagnosisâ€™, which define how evidence functions are to be used to map findings observed for a problem case to diagnostic solutions. This set-theoretical framework offers a simple, yet powerful tool for comparing existing notions of diagnosis, as well as for proposing new notions of diagnosis. A theory of flexible notions of diagnosis, called refinement diagnosis, is proposed and defined in terms of this framework. Relationships with notions of diagnosis known from the literature are investigated.}
}
@article{TU201179,
title = {Approximation of action theories and its application to conformant planning},
journal = {Artificial Intelligence},
volume = {175},
number = {1},
pages = {79-119},
year = {2011},
note = {John McCarthy's Legacy},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000433},
author = {Phan Huy Tu and Tran Cao Son and Michael Gelfond and A. Ricardo Morales},
keywords = {Reasoning about action and change, Knowledge representation, Planning, Incomplete information, Answer set programming},
abstract = {This paper describes our methodology for building conformant planners, which is based on recent advances in the theory of action and change and answer set programming. The development of a planner for a given dynamic domain starts with encoding the knowledge about fluents and actions of the domain as an action theory D of some action language. Our choice in this paper is AL â€“ an action language with dynamic and static causal laws and executability conditions. An action theory D of AL defines a transition diagram T(D) containing all the possible trajectories of the domain. A transition ã€ˆs,a,sâ€²ã€‰ belongs to T(D) iff the execution of the action a in the state s may move the domain to the state sâ€². The second step in the planner development consists in finding a deterministic transition diagram Tlp(D) such that nodes of Tlp(D) are partial states of D, its arcs are labeled by actions, and a path in Tlp(D) from an initial partial state Î´0 to a partial state satisfying the goal Î´f corresponds to a conformant plan for Î´0 and Î´f in T(D). The transition diagram Tlp(D) is called an â€˜approximationâ€™ of T(D). We claim that a concise description of an approximation of T(D) can often be given by a logic program Ï€(D) under the answer sets semantics. Moreover, complex initial situations and constraints on plans can be also expressed by logic programming rules and included in Ï€(D). If this is possible then the problem of finding a parallel or sequential conformant plan can be reduced to computing answer sets of Ï€(D). This can be done by general purpose answer set solvers. If plans are sequential and long then this method can be too time consuming. In this case, Ï€(D) is used as a specification for a procedural graph searching conformant planning algorithm. The paper illustrates this methodology by building several conformant planners which work for domains with complex relationship between the fluents. The efficiency of the planners is experimentally evaluated on a number of new and old benchmarks. In addition we show that for a subclass of action theories of AL our planners are complete, i.e., if in Tlp(D) we cannot get from Î´0 to a state satisfying the goal Î´f then there is no conformant plan for Î´0 and Î´f in T(D).}
}
@article{BERLINER199697,
title = {Bâˆ— probability based search},
journal = {Artificial Intelligence},
volume = {86},
number = {1},
pages = {97-156},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00092-5},
url = {https://www.sciencedirect.com/science/article/pii/0004370295000925},
author = {Hans J. Berliner and Chris McConnell},
keywords = {Probabilistic B, Computer chess, Selective search, Two-person games},
abstract = {We describe a search algorithm for two-player games that relies on selectivity rather than brute-force to achieve success. The key ideas behind the algorithm are: 1.(1) stopping when one alternative is clearly better than all the others, and2.(2) focusing the search on the place where the most progress can likely be made toward stopping. Critical to this process is identifying uncertainty about the ultimate value of any move. The lower bound on uncertainty is the best estimate of the real value of a move. The upper bound is its optimistic value, based on some measure of unexplored potential. This provides an I-have-optimism-that-needs-to-be-investigated attitude that is an excellent guiding force. Uncertainty is represented by probability distributions. The search develops those parts of the tree where moving existing bounds would be most likely to succeed and would make the most progress toward terminating the search. Termination is achieved when the established real value of the best move is so good that the likelihood of this being achieved by any other alternative is minimal. The Bâˆ— probability based search algorithm has been implemented on the chess machine Hitech. En route we have developed effective techniques for: &#x02022;â€¢ producing viable optimistic estimates to guide the search,&#x02022;â€¢ producing cheap probability distribution estimates to measure goodness,&#x02022;â€¢ dealing with independence of alternative moves, and&#x02022;â€¢ dealing with the graph history interaction problem. The report describes the implementation, and the results of tests including games played against brute-force programs. Test data indicate that Bâˆ— Hitech is better than any searcher that expands its whole tree based on selectivity. Further, analysis of the data indicates that should additional power become available, the Bâˆ— technique will scale up considerably better than brute-force techniques.}
}
@article{HOWE1995125,
title = {Understanding planner behavior},
journal = {Artificial Intelligence},
volume = {76},
number = {1},
pages = {125-166},
year = {1995},
note = {Planning and Scheduling},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00083-D},
url = {https://www.sciencedirect.com/science/article/pii/000437029400083D},
author = {Adele E. Howe and Paul R. Cohen},
abstract = {As planners and their environments become increasingly complex, planner behavior becomes increasingly difficult to understand. We often do not understand what causes them to fail, so that we can debug their failures, and we may not understand what allows them to succeed, so that we can design the next generation. This paper describes a partially automated methodology for understanding planner behavior over long periods of time. The methodology, called Dependency Interpretation, uses statistical dependency detection to identify interesting patterns of behavior in execution traces and interprets the patterns using a weak model of the planner's interaction with its environment to explain how the patterns might be caused by the planner. Dependency Interpretation has been applied to identify possible causes of plan failures in the Phoenix planner. By analyzing four sets of execution traces gathered from about 400 runs of the Phoenix planner, we showed that the statistical dependencies describe patterns of behavior that are sensitive to the version of the planner and to increasing temporal separation between events, and that dependency detection degrades predictably as the number of available execution traces decreases and as noise is introduced in the execution traces. Dependency Interpretation is appropriate when a complete and correct model of the planner and environment is not available, but execution traces are available.}
}
@article{FRISCH2006803,
title = {Propagation algorithms for lexicographic ordering constraints},
journal = {Artificial Intelligence},
volume = {170},
number = {10},
pages = {803-834},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000385},
author = {Alan M. Frisch and Brahim Hnich and Zeynep Kiziltan and Ian Miguel and Toby Walsh},
keywords = {Artificial intelligence, Constraints, Constraint programming, Constraint propagation, Lexicographic ordering, Symmetry, Symmetry breaking, Generalized arc consistency, Matrix models},
abstract = {Finite-domain constraint programming has been used with great success to tackle a wide variety of combinatorial problems in industry and academia. To apply finite-domain constraint programming to a problem, it is modelled by a set of constraints on a set of decision variables. A common modelling pattern is the use of matrices of decision variables. The rows and/or columns of these matrices are often symmetric, leading to redundancy in a systematic search for solutions. An effective method of breaking this symmetry is to constrain the assignments of the affected rows and columns to be ordered lexicographically. This paper develops an incremental propagation algorithm, GACLexLeq, that establishes generalised arc consistency on this constraint in O(n) operations, where n is the length of the vectors. Furthermore, this paper shows that decomposing GACLexLeq into primitive constraints available in current finite-domain constraint toolkits reduces the strength or increases the cost of constraint propagation. Also presented are extensions and modifications to the algorithm to handle strict lexicographic ordering, detection of entailment, and vectors of unequal length. Experimental results on a number of domains demonstrate the value of GACLexLeq.}
}
@article{DECHTER200773,
title = {AND/OR search spaces for graphical models},
journal = {Artificial Intelligence},
volume = {171},
number = {2},
pages = {73-106},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S000437020600138X},
author = {Rina Dechter and Robert Mateescu},
keywords = {Search, AND/OR search, Decomposition, Graphical models, Bayesian networks, Constraint networks},
abstract = {The paper introduces an AND/OR search space perspective for graphical models that include probabilistic networks (directed or undirected) and constraint networks. In contrast to the traditional (OR) search space view, the AND/OR search tree displays some of the independencies present in the graphical model explicitly and may sometimes reduce the search space exponentially. Indeed, most algorithmic advances in search-based constraint processing and probabilistic inference can be viewed as searching an AND/OR search tree or graph. Familiar parameters such as the depth of a spanning tree, treewidth and pathwidth are shown to play a key role in characterizing the effect of AND/OR search graphs vs. the traditional OR search graphs. We compare memory intensive AND/OR graph search with inference methods, and place various existing algorithms within the AND/OR search space.}
}
@article{LEVY199783,
title = {Speeding up inferences using relevance reasoning: a formalism and algorithms},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {83-136},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00049-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000490},
author = {Alon Y. Levy and Richard E. Fikes and Yehoshua Sagiv},
keywords = {Relevance reasoning, Meta-level reasoning, Static analysis, Horn rules, Constraints, Knowledge representation},
abstract = {Irrelevance reasoning refers to the process in which a system reasons about which parts of its knowledge are relevant (or irrelevant) to a specific query. Aside from its importance in speeding up inferences from large knowledge bases, relevance reasoning is crucial in advanced applications such as modeling complex physical devices and information gathering in distributed heterogeneous systems. This article presents a novel framework for studying the various kinds of irrelevance that arise in inference and efficient algorithms for relevance reasoning. We present a proof-theoretic framework for analyzing definitions of irrelevance. The framework makes the necessary distinctions between different notions of irrelevance that are important when using them for speeding up inferences. We describe the query-tree algorithm which is a sound, complete and efficient algorithm for automatically deriving certain kinds of irrelevance claims for Horn-rule knowledge bases and several extensions. Finally, we describe experimental results that show that significant speedups (often orders of magnitude) are obtained by employing the query-tree in inference.}
}
@article{DEBONA2017118,
title = {Localising iceberg inconsistencies},
journal = {Artificial Intelligence},
volume = {246},
pages = {118-151},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2017.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370217300255},
author = {Glauber {De Bona} and Anthony Hunter},
keywords = {Propositional logic, Inconsistency management, Inconsistency analysis, Inconsistency localisation},
abstract = {In artificial intelligence, it is important to handle and analyse inconsistency in knowledge bases. Inconsistent pieces of information suggest questions like â€œwhere is the inconsistency?â€ and â€œhow severe is it?â€. Inconsistency measures have been proposed to tackle the latter issue, but the former seems underdeveloped and is the focus of this paper. Minimal inconsistent sets have been the main tool to localise inconsistency, but we argue that they are like the exposed part of an iceberg, failing to capture contradictions hidden under the water. Using classical propositional logic, we develop methods to characterise when a formula is contributing to the inconsistency in a knowledge base and when a set of formulas can be regarded as a primitive conflict. To achieve this, we employ an abstract consequence operation to â€œlook beneath the water levelâ€, generalising the minimal inconsistent set concept and the related free formula notion. We apply the framework presented to the problem of measuring inconsistency in knowledge bases, putting forward relaxed forms for two debatable postulates for inconsistency measures. Finally, we discuss the computational complexity issues related to the introduced concepts.}
}
@article{LUKASIEWICZ2005119,
title = {Weak nonmonotonic probabilistic logics},
journal = {Artificial Intelligence},
volume = {168},
number = {1},
pages = {119-161},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000809},
author = {Thomas Lukasiewicz},
keywords = {Probabilistic logic, Default reasoning from conditional knowledge bases, Entailment in System , Entailment in System , Lexicographic entailment, Nonmonotonic probabilistic logics, Inconsistency handling, Algorithms, Computational complexity},
abstract = {We present an approach where probabilistic logic is combined with default reasoning from conditional knowledge bases in Kraus et al.'s System P, Pearl's System Z, and Lehmann's lexicographic entailment. The resulting probabilistic generalizations of default reasoning from conditional knowledge bases allow for handling in a uniform framework strict logical knowledge, default logical knowledge, as well as purely probabilistic knowledge. Interestingly, probabilistic entailment in System P coincides with probabilistic entailment under g-coherence from imprecise probability assessments. We then analyze the semantic and nonmonotonic properties of the new formalisms. It turns out that they all are proper generalizations of their classical counterparts and have similar properties as them. In particular, they all satisfy the rationality postulates of System P and some Conditioning property. Moreover, probabilistic entailment in System Z and probabilistic lexicographic entailment both satisfy the property of Rational Monotonicity and some Irrelevance property, while probabilistic entailment in System P does not. We also analyze the relationships between the new formalisms. Here, probabilistic entailment in System P is weaker than probabilistic entailment in System Z, which in turn is weaker than probabilistic lexicographic entailment. Moreover, they all are weaker than entailment in probabilistic logic where default sentences are interpreted as strict sentences. Under natural conditions, probabilistic entailment in System Z and lexicographic entailment even coincide with such entailment in probabilistic logic, while probabilistic entailment in System P does not. Finally, we also present algorithms for reasoning under probabilistic entailment in System Z and probabilistic lexicographic entailment, and we give a precise picture of its complexity.}
}
@article{KAYSER20091154,
title = {From the textual description of an accident to its causes},
journal = {Artificial Intelligence},
volume = {173},
number = {12},
pages = {1154-1193},
year = {2009},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020900054X},
author = {Daniel Kayser and Farid Nouioua},
keywords = {Natural language understanding, Causal reasoning, Norms, Inference-based semantics, Semi-normal defaults},
abstract = {Every human being, reading a short report concerning a road accident, gets an idea of its causes. The work reported here attempts to enable a computer to do the same, i.e. to determine the causes of an event from a textual description of it. It relies heavily on the notion of norm for two reasons:â€¢The notion of cause has often been debated but remains poorly understood: we postulate that what people tend to take as the cause of an abnormal event, like an accident, is the fact that a specific norm has been violated.â€¢Natural Language Processing has given a prominent place to deduction, and for what concerns Semantics, to truth-based inference. However, norm-based inference is a much more powerful technique to get the conclusions that human readers derive from a text. The paper describes a complete chain of treatments, from the text to the determination of the cause. The focus is set on what is called â€œlinguisticâ€ and â€œsemantico-pragmaticâ€ reasoning. The former extracts so-called â€œsemantic literalsâ€ from the result of the parse, and the latter reduces the description of the accident to a small number of â€œkernel literalsâ€ which are sufficient to determine its cause. Both of them use a non-monotonic reasoning system, viz. LPARSE and SMODELS. Several issues concerning the representation of modalities and time are discussed and illustrated by examples taken from a corpus of reports obtained from an insurance company.}
}
@article{EITER1997177,
title = {Semantics and complexity of abduction from default theories},
journal = {Artificial Intelligence},
volume = {90},
number = {1},
pages = {177-223},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00040-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000409},
author = {Thomas Eiter and Georg Gottlob and Nicola Leone},
keywords = {Abduction, Default logic, Algorithms and complexity, Tractability},
abstract = {Abductive reasoning (roughly speaking, find an explanation for observations out of hypotheses) has been recognized as an important principle of common-sense reasoning. Since logical knowledge representation is commonly based on nonclassical formalisms like default logic, autoepistemic logic, or circumscription, it is necessary to perform abductive reasoning from theories (i.e., knowledge bases) of nonclassical logics. In this paper, we investigate how abduction can be performed from theories in default logic. In particular, we present a basic model of abduction from default theories. Different modes of abduction are plausible, based on credulous and skeptical default reasoning; they appear useful for different applications such as diagnosis and planning. Moreover, we thoroughly analyze the complexity of the main abductive reasoning tasks, namely finding an explanation, deciding relevance of a hypothesis and deciding necessity of a hypothesis. These problems are intractable even in the prepositional case and we locate them into the appropriate slots of the polynomial hierarchy. However, we also present known classes of default theories for which abduction is tractable. Moreover, we also consider first-order default theories, based on domain closure and the unique names assumption. In this setting, the abduction tasks are decidable, but have exponentially higher complexity than in the propositional case.}
}
@article{SARNE2008541,
title = {Managing parallel inquiries in agents' two-sided search},
journal = {Artificial Intelligence},
volume = {172},
number = {4},
pages = {541-569},
year = {2008},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207001300},
author = {David Sarne and Sarit Kraus},
keywords = {Multi-agent systems, Autonomous agents, Equilibrium analysis, Matching},
abstract = {In this paper we address the problem of agents engaged in a distributed costly two-sided search for pairwise partnerships in Multi-Agent Systems (MAS). While traditional two-sided search mechanisms are based on a purely sequential search of all searchers, our mechanism integrates an ability of some of the agents to maintain several search efforts in parallel at each search stage. We show that in many environments the transition to the new mechanism is inevitable since the adoption of the parallel-interactions based search suggests a greater utility for the searching agents. By exploring the appropriate model equations, we present the new dynamics that drive the equilibrium when using such a mechanism in MAS environments. Complementary algorithms are offered, based on the unique equilibria characteristics found, for facilitating the extraction of the agents' strategies. The analysis methodology used supplies a comprehensive solution to a self contained model, and also offers a great value for future work concerning distributed two-sided mechanisms for MAS. Towards the end of the paper we review two of these important models that can benefit from the proposed analysis.}
}
@article{XIANG1996295,
title = {A probabilistic framework for cooperative multi-agent distributed interpretation and optimization of communication},
journal = {Artificial Intelligence},
volume = {87},
number = {1},
pages = {295-342},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(95)00110-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370295001107},
author = {Y. Xiang},
abstract = {Multiply sectioned Bayesian networks for single-agent systems are extended into a framework for cooperative multi-agent distributed interpretation systems. Each agent is represented as a Bayesian subnet. We show that the semantics of the joint probability distribution of such a system is well defined under reasonable conditions. Unlike in single-agent systems where evidence is entered one subnet at a time, multiple agents may acquire evidence asynchronously in parallel. New communication operations are thus proposed to maintain global consistency. It may not be practical to maintain such consistency constantly due to the inter-agent â€œdistanceâ€. We show that, if the new operations are followed, between two successive communications, answers to queries from an agent are consistent with all local evidence and are consistent with all global evidence gathered up to the last communication. During a communication operation, each agent is not available to process evidence for a period of time (called off-line time). Two criteria for the minimization of the off-line time, which may commonly be used, are considered. We derive, under each criterion, the optimal schedules when the communication is initiated from an arbitrarily selected agent.}
}
@article{BERTOLI2010316,
title = {Automated composition of Web services via planning in asynchronous domains},
journal = {Artificial Intelligence},
volume = {174},
number = {3},
pages = {316-361},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2009.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370209001489},
author = {Piergiorgio Bertoli and Marco Pistore and Paolo Traverso},
keywords = {Planning, Web services, Automated program synthesis},
abstract = {The service-oriented paradigm promises a novel degree of interoperability between business processes, and is leading to a major shift in way distributed applications are designed and realized. While novel and more powerful services can be obtained, in such setting, by suitably orchestrating existing ones, manually developing such orchestrations is highly demanding, time-consuming and error-prone. Providing automated service composition tools is therefore essential to reduce the time to market of services, and ultimately to successfully enact the service-oriented approach. In this paper, we show that such tools can be realized based on the adoption and extension of powerful AI planning techniques, taking the â€œplanning via model-checkingâ€ approach as a stepping stone. In this respect, this paper summarizes and substantially extends a research line that started early in this decade and has continued till now. Specifically, this work provides three key contributions. First, we describe a novel planning framework for the automated composition of Web services, which can handle services specified and implemented using industrial standard languages for business processes modeling and execution, like ws-bpel. Since these languages describe stateful Web services that rely on asynchronous communication primitives, a distinctive aspect of the presented framework is its ability to model and solve planning problems for asynchronous domains. Second, we formally spell out the theory underlying the framework, and provide algorithms to solve service composition in such framework, proving their correctness and completeness. The presented algorithms significantly extend state-of-the-art techniques for planning under uncertainty, by allowing the combination of asynchronous domains according to behavioral requirements. Third, we provide and discuss an implementation of the approach, and report extensive experimental results which demonstrate its ability to scale up to significant cases for which the manual development of ws-bpel composed services is far from trivial and time consuming.}
}
@article{DAGOSTINO201815,
title = {Classical logic, argument and dialectic},
journal = {Artificial Intelligence},
volume = {262},
pages = {15-51},
year = {2018},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218302807},
author = {M. D'Agostino and S. Modgil},
keywords = {Classical logic, Argumentation, Dialectic, Rationality postulates, Natural deduction, Preferred subtheories, Bounded reasoning},
abstract = {A well studied instantiation of Dung's abstract theory of argumentation yields argumentation-based characterisations of non-monotonic inference over possibly inconsistent sets of classical formulae. This provides for single-agent reasoning in terms of argument and counter-argument, and distributed non-monotonic reasoning in the form of dialogues between computational and/or human agents. However, features of existing formalisations of classical logic argumentation (Cl-Arg) that ensure satisfaction of rationality postulates, preclude applications of Cl-Arg that account for real-world dialectical uses of arguments by resource-bounded agents. This paper formalises dialectical classical logic argumentation that both satisfies these practical desiderata and is provably rational. In contrast to standard approaches to Cl-Arg we: 1) draw an epistemic distinction between an argument's premises accepted as true, and those assumed true for the sake of argument, so formalising the dialectical move whereby arguments' premises are shown to be inconsistent, and avoiding the foreign commitment problem that arises in dialogical applications; 2) provide an account of Cl-Arg suitable for real-world use by eschewing the need to check that an argument's premises are subset minimal and consistent, and identifying a minimal set of assumptions as to the arguments that must be constructed from a set of formulae in order to ensure that the outcome of evaluation is rational. We then illustrate our approach with a natural deduction proof theory for propositional classical logic that allows measurement of the â€˜depthâ€™ of an argument, such that the construction of depth-bounded arguments is a tractable problem, and each increase in depth naturally equates with an increase in the inferential capabilities of real-world agents. We also provide a resource-bounded argumentative characterisation of non-monotonic inference as defined by Brewka's Preferred Subtheories.}
}
@article{BUCHHEIT1998209,
title = {A refined architecture for terminological systems: Terminology = Schema + Views},
journal = {Artificial Intelligence},
volume = {99},
number = {2},
pages = {209-260},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00079-9},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000799},
author = {M. Buchheit and F.M. Donini and W. Nutt and A. Schaerf},
keywords = {Domain modelling, Knowledge representation systems, Description logics, Knowledge-base architectures, Subumption of concepts},
abstract = {Traditionally, the core of a Terminological Knowledge Representation System (TKRS) consists of a TBox or terminology, where concepts are introduced, and an ABox or world description, where facts about individuals are stated in terms of concept memberships. This design has a drawback because in most applications the TBox has to meet two functions at a time: On the one handâ€”similarly to a database schemaâ€”frame-like structures with type information are introduced through primitive concepts and primitive roles; on the other hand, views on the objects in the knowledge base are provided through defined concepts. We propose to account for this conceptual separation by partitioning the TBox into two components for primitive and defined concepts, which we call the schema and the view part. We envision the two parts to differ with respect to the language for concepts, the statements allowed, and the semantics. We argue that this separation achieves more conceptual clarity about the role of primitive and defined concepts and the semantics of terminological cycles. Two case studies show the computational benefits to be gained from the refined architecture.}
}
@article{HADDAWY1996243,
title = {A logic of time, chance, and action for representing plans},
journal = {Artificial Intelligence},
volume = {80},
number = {2},
pages = {243-308},
year = {1996},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00070-0},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000700},
author = {Peter Haddawy},
abstract = {This paper integrates logical and probabilistic approaches to the representation of planning problems by developing a first-order logic of time, chance, and action. We start by making explicit and precise commonsense notions about time, chance, and action central to the planning problem. We then develop a logic, the semantics of which incorporates these intuitive properties. The logical language integrates both modal and probabilistic constructs and allows quantification over time points, probability values, and domain individuals. Probability is treated as a sentential operator in the language, so it can be arbitrarily nested and combined with other logical operators. The language can represent the chance that facts hold and events occur at various times. It can represent the chance that actions and other events affect the future. The model of action distinguishes between action feasibility, executability, and effects. We present a proof theory for the logic and show how the logic can be used to describe actions in such a way that the action descriptions can be composed to infer properties of plans via the proof theory.}
}
@article{DEJONG1997225,
title = {The computer revolution in science: steps towards the realization of computer-supported discovery environments},
journal = {Artificial Intelligence},
volume = {91},
number = {2},
pages = {225-256},
year = {1997},
note = {Scientific Discovery},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00011-8},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000118},
author = {Hidde {de Jong} and Arie Rip},
keywords = {Scientific discovery, Computer-supported discovery environments, Social studies of science and technology},
abstract = {The tools that scientists use in their search processes together form so-called discovery environments. The promise of artificial intelligence and other branches of computer science is to radically transform conventional discovery environments by equipping scientists with a range of powerful computer tools including large-scale, shared knowledge bases and discovery programs. We will describe the future computer-supported discovery environments that may result, and illustrate by means of a realistic scenario how scientists come to new discoveries in these environments. In order to make the step from the current generation of discovery tools to computer-supported discovery environments like the one presented in the scenario, developers should realize that such environments are large-scale sociotechnical systems. They should not just focus on isolated computer programs, but also pay attention to the question how these programs will be used and maintained by scientists in research practices. In order to help developers of discovery programs in achieving the integration of their tools in discovery environments, we will formulate a set of guidelines that developers could follow.}
}
@article{ARTIKIS2007776,
title = {An executable specification of a formal argumentation protocol},
journal = {Artificial Intelligence},
volume = {171},
number = {10},
pages = {776-804},
year = {2007},
note = {Argumentation in Artificial Intelligence},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000707},
author = {Alexander Artikis and Marek Sergot and Jeremy Pitt},
keywords = {Argumentation, Disputation, Protocol, Norm, Multi-agent system, Specification, Action language},
abstract = {We present a specification, in the action language C+, of Brewka's reconstruction of a theory of formal disputation originally proposed by Rescher. The focus is on the procedural aspects rather than the adequacy of this particular protocol for the conduct of debate and the resolution of disputes. The specification is structured in three separate levels, covering (i) the physical capabilities of the participant agents, (ii) the rules defining the protocol itself, specifying which actions are â€˜properâ€™ and â€˜timelyâ€™ according to the protocol and their effects on the protocol state, and (iii) the permissions, prohibitions, and obligations of the agents, and the sanctions and enforcement strategies that deal with non-compliance. Also included is a mechanism by which an agent may object to an action by another participant, and an optional â€˜silence implies consentâ€™ principle. Although comparatively simple, Brewka's protocol is thus representative of a wide range of other more complex argumentation and dispute resolution procedures that have been proposed. Finally, we show how the â€˜Causal Calculatorâ€™ implementation of C+ can be used to animate the specification and to investigate and verify properties of the protocol.}
}
@article{BAIER2009593,
title = {A heuristic search approach to planning with temporally extended preferences},
journal = {Artificial Intelligence},
volume = {173},
number = {5},
pages = {593-618},
year = {2009},
note = {Advances in Automated Plan Generation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001975},
author = {Jorge A. Baier and Fahiem Bacchus and Sheila A. McIlraith},
keywords = {Planning with preferences, Temporally extended preferences, PDDL3},
abstract = {Planning with preferences involves not only finding a plan that achieves the goal, it requires finding a preferred plan that achieves the goal, where preferences over plans are specified as part of the planner's input. In this paper we provide a technique for accomplishing this objective. Our technique can deal with a rich class of preferences, including so-called temporally extended preferences (TEPs). Unlike simple preferences which express desired properties of the final state achieved by a plan, TEPs can express desired properties of the entire sequence of states traversed by a plan, allowing the user to express a much richer set of preferences. Our technique involves converting a planning problem with TEPs into an equivalent planning problem containing only simple preferences. This conversion is accomplished by augmenting the inputed planning domain with a new set of predicates and actions for updating these predicates. We then provide a collection of new heuristics and a specialized search algorithm that can guide the planner towards preferred plans. Under some fairly general conditions our method is able to find a most preferred planâ€”i.e., an optimal plan. It can accomplish this without having to resort to admissible heuristics, which often perform poorly in practice. Nor does our technique require an assumption of restricted plan length or make-span. We have implemented our approach in the HPlan-P planning system and used it to compete in the 5th International Planning Competition, where it achieved distinguished performance in the Qualitative Preferences track.}
}
@article{PENCOLE2005121,
title = {A formal framework for the decentralised diagnosis of large scale discrete event systems and its application to telecommunication networks},
journal = {Artificial Intelligence},
volume = {164},
number = {1},
pages = {121-170},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000044},
author = {Yannick PencolÃ© and Marie-Odile Cordier},
keywords = {Model-based diagnosis, Discrete event systems, Decentralised model, Distributed artificial intelligence, Telecommunication networks, Fault propagation},
abstract = {We address the problem of diagnosing large discrete event systems. Given a flow of observations from the system, the goal is to explain these observations on-line by identifying and localising possible failures and their consequences across the system. Model-based diagnosis approaches deal with this problem but, apart very recent proposals, either they require the computation of a global model of the system which is not possible with large discrete event systems, or they cannot perform on-line diagnosis. The contribution of this paper is the description and the implementation of a formal framework for the on-line decentralised diagnosis of such systems, framework which is based on the â€œdivide and conquerâ€ principle and does not require the global model computation. This paper finally describes the use of this framework in the monitoring of a real telecommunication network.}
}
@article{WU2011487,
title = {Online planning for multi-agent systems with bounded communication},
journal = {Artificial Intelligence},
volume = {175},
number = {2},
pages = {487-511},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001578},
author = {Feng Wu and Shlomo Zilberstein and Xiaoping Chen},
keywords = {Decentralized POMDPs, Cooperation and collaboration, Planning under uncertainty, Communication in multi-agent systems},
abstract = {We propose an online algorithm for planning under uncertainty in multi-agent settings modeled as DEC-POMDPs. The algorithm helps overcome the high computational complexity of solving such problems offline. The key challenges in decentralized operation are to maintain coordinated behavior with little or no communication and, when communication is allowed, to optimize value with minimal communication. The algorithm addresses these challenges by generating identical conditional plans based on common knowledge and communicating only when history inconsistency is detected, allowing communication to be postponed when necessary. To be suitable for online operation, the algorithm computes good local policies using a new and fast local search method implemented using linear programming. Moreover, it bounds the amount of memory used at each step and can be applied to problems with arbitrary horizons. The experimental results confirm that the algorithm can solve problems that are too large for the best existing offline planning algorithms and it outperforms the best online method, producing much higher value with much less communication in most cases. The algorithm also proves to be effective when the communication channel is imperfect (periodically unavailable). These results contribute to the scalability of decision-theoretic planning in multi-agent settings.}
}
@article{BARONI2005162,
title = {SCC-recursiveness: a general schema for argumentation semantics},
journal = {Artificial Intelligence},
volume = {168},
number = {1},
pages = {162-210},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000962},
author = {Pietro Baroni and Massimiliano Giacomin and Giovanni Guida},
keywords = {Argumentation semantics, Extensions, Defeat cycles},
abstract = {In argumentation theory, Dung's abstract framework provides a unifying view of several alternative semantics based on the notion of extension. In this context, we propose a general recursive schema for argumentation semantics, based on decomposition along the strongly connected components of the argumentation framework. We introduce the fundamental notion of SCC-recursiveness and we show that all Dung's admissibility-based semantics are SCC-recursive, and therefore a special case of our schema. On these grounds, we argue that the concept of SCC-recursiveness plays a fundamental role in the study and definition of argumentation semantics. In particular, the space of SCC-recursive semantics provides an ideal basis for the investigation of new proposals: starting from the analysis of several examples where Dung's preferred semantics gives rise to questionable results, we introduce four novel SCC-recursive semantics, able to overcome the limitations of preferred semantics, while differing in other respects.}
}
@article{BUFFET2009722,
title = {The factored policy-gradient planner},
journal = {Artificial Intelligence},
volume = {173},
number = {5},
pages = {722-747},
year = {2009},
note = {Advances in Automated Plan Generation},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2008.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0004370208001859},
author = {Olivier Buffet and Douglas Aberdeen},
keywords = {Concurrent probabilistic temporal planning, Reinforcement learning, Policy-gradient, AI planning},
abstract = {We present an any-time concurrent probabilistic temporal planner (CPTP) that includes continuous and discrete uncertainties and metric functions. Rather than relying on dynamic programming our approach builds on methods from stochastic local policy search. That is, we optimise a parameterised policy using gradient ascent. The flexibility of this policy-gradient approach, combined with its low memory use, the use of function approximation methods and factorisation of the policy, allow us to tackle complex domains. This factored policy gradient (FPG) planner can optimise steps to goal, the probability of success, or attempt a combination of both. We compare the FPG planner to other planners on CPTP domains, and on simpler but better studied non-concurrent non-temporal probabilistic planning (PP) domains. We present FPG-ipc, the PP version of the planner which has been successful in the probabilistic track of the fifth international planning competition.}
}
@article{DOWNEY2010726,
title = {Analysis of a probabilistic model of redundancy in unsupervised information extraction},
journal = {Artificial Intelligence},
volume = {174},
number = {11},
pages = {726-748},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000688},
author = {Doug Downey and Oren Etzioni and Stephen Soderland},
keywords = {Information extraction, Unsupervised, World Wide Web},
abstract = {Unsupervised Information Extraction (UIE) is the task of extracting knowledge from text without the use of hand-labeled training examples. Because UIE systems do not require human intervention, they can recursively discover new relations, attributes, and instances in a scalable manner. When applied to massive corpora such as the Web, UIE systems present an approach to a primary challenge in artificial intelligence: the automatic accumulation of massive bodies of knowledge. A fundamental problem for a UIE system is assessing the probability that its extracted information is correct. In massive corpora such as the Web, the same extraction is found repeatedly in different documents. How does this redundancy impact the probability of correctness? We present a combinatorial â€œballs-and-urnsâ€ model, called Urns, that computes the impact of sample size, redundancy, and corroboration from multiple distinct extraction rules on the probability that an extraction is correct. We describe methods for estimating Urns's parameters in practice and demonstrate experimentally that for UIE the model's log likelihoods are 15 times better, on average, than those obtained by methods used in previous work. We illustrate the generality of the redundancy model by detailing multiple applications beyond UIE in which Urns has been effective. We also provide a theoretical foundation for Urns's performance, including a theorem showing that PAC Learnability in Urns is guaranteed without hand-labeled data, under certain assumptions.}
}
@article{YANG2007107,
title = {Learning action models from plan examples using weighted MAX-SAT},
journal = {Artificial Intelligence},
volume = {171},
number = {2},
pages = {107-143},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206001408},
author = {Qiang Yang and Kangheng Wu and Yunfei Jiang},
keywords = {Learning action models, Automated planning, Statistical relational learning},
abstract = {AI planning requires the definition of action models using a formal action and plan description language, such as the standard Planning Domain Definition Language (PDDL), as input. However, building action models from scratch is a difficult and time-consuming task, even for experts. In this paper, we develop an algorithm called ARMS (action-relation modelling system) for automatically discovering action models from a set of successful observed plans. Unlike the previous work in action-model learning, we do not assume complete knowledge of states in the middle of observed plans. In fact, our approach works when no or partial intermediate states are given. These example plans are obtained by an observation agent who does not know the logical encoding of the actions and the full state information between the actions. In a real world application, the cost is prohibitively high in labelling the training examples by manually annotating every state in a plan example from snapshots of an environment. To learn action models, ARMS gathers knowledge on the statistical distribution of frequent sets of actions in the example plans. It then builds a weighted propositional satisfiability (weighted MAX-SAT) problem and solves it using a MAX-SAT solver. We lay the theoretical foundations of the learning problem and evaluate the effectiveness of ARMS empirically.}
}
@article{THIELSCHER1997317,
title = {Ramification and causality},
journal = {Artificial Intelligence},
volume = {89},
number = {1},
pages = {317-364},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00033-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000331},
author = {Michael Thielscher},
keywords = {Temporal reasoning, Reasoning about actions, Ramification problem, Causality, Fluent calculus},
abstract = {In formal systems for reasoning about actions, the ramification problem denotes the problem of handling indirect effects. These effects are not explicitly represented in action specifications but follow from general laws describing dependencies among components of the world state. An adequate treatment of indirect effects requires a suitably weakened version of the general law of persistence. It also requires a method to avoid unintuitive changes suggested by the aforementioned dependency laws. We propose a solution to the ramification problem that uses directed relations between two single effects, stating the circumstances under which the occurrence of the first causes the second. We argue for the necessity of an approach based on causality by elaborating the limitations of common paradigms employed to handle ramificationsâ€”the principle of categorization and the policy of minimal change. Our abstract solution is realized on the basis of a particular action calculus, namely, the fluent calculus.}
}
@article{AMATI1997169,
title = {Definability and commonsense reasoning},
journal = {Artificial Intelligence},
volume = {93},
number = {1},
pages = {169-199},
year = {1997},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(96)00049-5},
url = {https://www.sciencedirect.com/science/article/pii/S0004370296000495},
author = {Gianni Amati and Luigia Carlucci Aiello and Fiora Pirri},
keywords = {Commonsense reasoning, Definability, Fixed points, Logic of provability, Default logic, Contextual reasoning, Self-reference},
abstract = {The definition of concepts is a central problem in commonsense reasoning. Many themes in nonmonotonic reasoning concern implicit and explicit definability. Implicit definability in nonmonotonic logic is always relative to the contextâ€”the current theory of the world. We show that fixed point equations provide a generalization of explicit definability, which correctly captures the relativized context. Theories expressed within this logical framework provide implicit definitions of concepts. Moreover, it is possible to derive these fixed points entirely within the logic.}
}
@article{BARRETT2017132,
title = {Making friends on the fly: Cooperating with new teammates},
journal = {Artificial Intelligence},
volume = {242},
pages = {132-171},
year = {2017},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2016.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370216301266},
author = {Samuel Barrett and Avi Rosenfeld and Sarit Kraus and Peter Stone},
keywords = {Ad hoc teamwork, Multiagent systems, Multiagent cooperation, Reinforcement learning, Pursuit domain, RoboCup soccer},
abstract = {Robots are being deployed in an increasing variety of environments for longer periods of time. As the number of robots grows, they will increasingly need to interact with other robots. Additionally, the number of companies and research laboratories producing these robots is increasing, leading to the situation where these robots may not share a common communication or coordination protocol. While standards for coordination and communication may be created, we expect that robots will need to additionally reason intelligently about their teammates with limited information. This problem motivates the area of ad hoc teamwork in which an agent may potentially cooperate with a variety of teammates in order to achieve a shared goal. This article focuses on a limited version of the ad hoc teamwork problem in which an agent knows the environmental dynamics and has had past experiences with other teammates, though these experiences may not be representative of the current teammates. To tackle this problem, this article introduces a new general-purpose algorithm, PLASTIC, that reuses knowledge learned from previous teammates or provided by experts to quickly adapt to new teammates. This algorithm is instantiated in two forms: 1) PLASTIC-Model â€“ which builds models of previous teammates' behaviors and plans behaviors online using these models and 2) PLASTIC-Policy â€“ which learns policies for cooperating with previous teammates and selects among these policies online. We evaluate PLASTIC on two benchmark tasks: the pursuit domain and robot soccer in the RoboCup 2D simulation domain. Recognizing that a key requirement of ad hoc teamwork is adaptability to previously unseen agents, the tests use more than 40 previously unknown teams on the first task and 7 previously unknown teams on the second. While PLASTIC assumes that there is some degree of similarity between the current and past teammates' behaviors, no steps are taken in the experimental setup to make sure this assumption holds. The teammates were created by a variety of independent developers and were not designed to share any similarities. Nonetheless, the results show that PLASTIC was able to identify and exploit similarities between its current and past teammates' behaviors, allowing it to quickly adapt to new teammates.}
}
@article{DAVIS200581,
title = {Knowledge and communication: A first-order theory},
journal = {Artificial Intelligence},
volume = {166},
number = {1},
pages = {81-139},
year = {2005},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2005.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370205000767},
author = {Ernest Davis},
keywords = {Communication, Knowledge, Logic, Paradox},
abstract = {This paper presents a theory of informative communications among agents that allows a speaker to communicate to a hearer truths about the state of the world; the occurrence of events, including other communicative acts; and the knowledge states of any agentâ€”speaker, hearer, or third partiesâ€”any of these in the past, present, or futureâ€”and any logical combination of these, including formulas with quantifiers. We prove that this theory is consistent, and compatible with a wide range of physical theories. We examine how the theory avoids two potential paradoxes, and discuss how these paradoxes may pose a danger when this theory are extended.}
}
@article{HERZIG2007951,
title = {Metatheory of actions: Beyond consistency},
journal = {Artificial Intelligence},
volume = {171},
number = {16},
pages = {951-984},
year = {2007},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2007.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0004370207000781},
author = {Andreas Herzig and Ivan Varzinczak},
keywords = {Reasoning about actions, Action theory, Modularity, Ramifications},
abstract = {Traditionally, consistency is the only criterion for the quality of a theory in logic-based approaches to reasoning about actions. This work goes beyond that and contributes to the metatheory of actions by investigating what other properties a good domain description should have. We state some metatheoretical postulates concerning this sore spot. When all postulates are satisfied we call the action theory modular. Besides being easier to understand and more elaboration tolerant in McCarthy's sense, modular theories have interesting properties. We point out the problems that arise when the postulates about modularity are violated, and propose algorithmic checks that can help the designer of an action theory to overcome them.}
}
@article{WOOLDRIDGE2006835,
title = {On the computational complexity of coalitional resource games},
journal = {Artificial Intelligence},
volume = {170},
number = {10},
pages = {835-871},
year = {2006},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2006.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370206000397},
author = {Michael Wooldridge and Paul E. Dunne},
keywords = {Coalitional games, Resources, Computational complexity, Multi-agent systems},
abstract = {We study Coalitional Resource Games (crgs), a variation of Qualitative Coalitional Games (qcgs) in which each agent is endowed with a set of resources, and the ability of a coalition to bring about a set of goals depends on whether they are collectively endowed with the necessary resources. We investigate and classify the computational complexity of a number of natural decision problems for crgs, over and above those previously investigated for qcgs in general. For example, we show that the complexity of determining whether conflict is inevitable between two coalitions with respect to some stated resource bound (i.e., a limit value for every resource) is co-np-complete. We then investigate the relationship between crgs and qcgs, and in particular the extent to which it is possible to translate between the two models. We first characterise the complexity of determining equivalence between crgs and qcgs. We then show that it is always possible to translate any given crg into a succinct equivalent qcg, and that it is not always possible to translate a qcg into an equivalent crg; we establish some necessary and some sufficient conditions for a translation from qcgs to crgs to be possible, and show that even where an equivalent crg exists, it may have size exponential in the number of goals and agents of its source qcg.}
}
@article{DVORAK20121,
title = {Towards fixed-parameter tractable algorithms for abstract argumentation},
journal = {Artificial Intelligence},
volume = {186},
pages = {1-37},
year = {2012},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2012.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0004370212000264},
author = {Wolfgang DvoÅ™Ã¡k and Reinhard Pichler and Stefan Woltran},
keywords = {Abstract argumentation, Fixed-parameter tractability, Tree-width, Dynamic programming, Complexity},
abstract = {Abstract argumentation frameworks have received a lot of interest in recent years. Most computational problems in this area are intractable but several tractable fragments have been identified. In particular, Dunne showed that many problems can be solved in linear time for argumentation frameworks of bounded tree-width. However, these tractability results, which were obtained via CourcelleÊ¼s Theorem, do not directly lead to efficient algorithms. The goal of this paper is to turn the theoretical tractability results into efficient algorithms and to explore the potential of directed notions of tree-width for defining larger tractable fragments. As a by-product, we will sharpen some known complexity results.}
}