{
    "TRAINING_DATA": [
        [
            "Artificial Intelligence 172 (2008) 1837–1872Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintOutlier detection using default reasoning ✩Fabrizio Angiulli a, Rachel Ben-Eliyahu – Zohary b,∗,1, Luigi Palopoli aa DEIS, Università della Calabria, Via Pietro Bucci 41C, 87036 Rende (CS), Italyb Ben-Gurion University and Jerusalem College of Engineering, Beer-Sheva/Jerusalem, Israela r t i c l ei n f oa b s t r a c tArticle history:Received 5 March 2007Received in revised form 9 July 2008Accepted 29 July 2008Available online 15 August 2008Keywords:Default logicDisjunctive logic programmingKnowledge representationNonmonotonic reasoningComputational complexityData miningOutlier detection1. IntroductionDefault logics are usually used to describe the regular behavior and normal properties ofdomain elements. In this paper we suggest, conversely, that the framework of default logicscan be exploited for detecting outliers. Outliers are observations expressed by sets of literalsthat feature unexpected semantical characteristics. These sets of literals are selected amongthose explicitly embodied in the given knowledge base. Hence, essentially we perceiveoutlier detection as a knowledge discovery technique. This paper defines the notion ofoutlier in two related formalisms for specifying defaults: Reiter’s default logic and extendeddisjunctive logic programs. For each of the two formalisms, we show that finding outliers isquite complex. Indeed, we prove that several versions of the outlier detection problem lieover the second level of the polynomial hierarchy. We believe that a thorough complexityanalysis, as done here, is a useful preliminary step towards developing effective heuristicsand exploring tractable subsets of outlier detection problems.© 2008 Elsevier B.V. All rights reserved.This paper is about detecting outliers. In this work, outliers are unexpected observations, e.g., strange characteristics ofindividuals, in a given application domain. Exceptionality is determined here with respect to a given trustable knowledgebase, with which a given set of elements does not comply. The issue that we address is how to locate such unusual elementsautomatically.A first step towards automatically detecting outliers is to state their formal definition. In this work, it is assumed that thegiven knowledge base is expressed using a default reasoning language and hence we formalize our definition of outliers inthis framework. The languages mainly dealt with are propositional default logics and extended disjunctive logic programs.Default logic was originally developed as a tool for working with incomplete knowledge. Default rules allow one todescribe a normal behavior of a system and to draw consequent conclusions. As such, default rules can also be exploited fordetecting outliers—observations that are unexpected according to the default theory at hand. This is the basic idea behindthis paper. We refer to outliers as sets of observations that demonstrate some properties contrasting with those that canbe logically “justified” according to the given knowledge base. Along with outliers, their “witnesses” are singled out—thoseunexpected properties that characterize outliers.To illustrate, some informal application examples for outlier detection are described below.✩This manuscript is an extended and comprehensive report of results of which part have appeared in IJCAI-03 under the title “Outlier Detection UsingDefault Logic” and in ECAI-04 under the title “Outlier Detection Using Disjunctive Logic Programming”.* Corresponding author.E-mail addresses: f.angiulli@deis.unical.it (F. Angiulli), rachel@bgu.ac.il (R. Ben-Eliyahu – Zohary), palopoli@deis.unical.it (L. Palopoli).1 Part of this work was done while the author was a visiting scholar in the Division of Engineering and Applied Sciences, Harvard University, Cambridge,Massachusetts.0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.07.004\f1838F. Angiulli et al. / Artificial Intelligence 172 (2008) 1837–1872Using outliers for diagnosis of computer hardware. Suppose that it usually takes about four seconds to download a giga-byte file from a server, but one day the system becomes slower, instead, eight seconds are needed to perform thesame task. While eight seconds may indicate a good performance, it is, nonetheless, helpful to find the source ofthe delay in order to prevent more critical faults in the future. In this case, the download operation is the outlierwhile the delay is its witness.Mechanical failure. Assume that someone’s car brakes are making a strange noise. Although they seem to be functioningproperly, this is not a normal behavior and the car is brought in for servicing. In this case, the car brakes are theoutlier and the noise is a witness for it.Knowledge base integrity.If an abnormal property is discovered in a database, the source that reported this informationwould have to be checked. Detecting abnormal properties, that is, detecting outliers, can also lead to an update ofdefault rules in a knowledge base. For example, suppose we have the rule that birds fly, and we observe a bird thatdoes not fly. This occurrence of such an outlier in the theory would be reported to the knowledge engineer. Theengineer investigates the case, finds out that the bird is actually a penguin, therefore he updates the knowledgebase with the default “penguins do not fly.”According to our approach, exceptions are not explicitly recorded in the knowledge base as “abnormals,” as is often donein logical-based abduction [16,23,47]. Rather, their “abnormality\" is singled out precisely because some of the propertiescharacterizing them cannot be justified within the given theory.In this paper we formally define outliers in both the related formalisms of Reiter’s default logic and Extended disjunctivelogic programming (EDLP).Reiter’s Default Logic is a powerful nonmonotonic formalism to deal with incomplete information, while logic program-ming is a practical tool that is widely employed in KR&R. The paper mostly deals with the propositional fragment of theselogics. However, first-order default theories shall be also briefly discussed in the paper (see Section 5 below).In the logic programming framework, we focus on Answer Set Semantics, which is used in most advanced systems forknowledge representation [38,40,43]. Extended logic programs (ELP) under Answer Set Semantics allow both negation as fail-ure and classical negation to be used. These programs can be naturally embedded into default theories and therefore canbe considered as a subset of default logic. As a consequence, our results for default theories carry over quite simply to ELPs.However, unlike ELP, extended disjunctive logic programs (EDLP) under Answer Set Semantics, in which also head-disjunctionis allowed, cannot be viewed as a subset of default logic, although default logic in its full volume does include disjunction.Indeed, part of the motivation for developing disjunctive logic programming lies in the limitations of default logic in han-dling disjunctive knowledge (see the paper by Poole [47]). In this context, EDLP can be considered as a convenient tool forrepresenting and manipulating complex knowledge [38] due to its declarativity and expressive power.In what follows, we first introduce our formal definition of outliers. Then, we analyze the complexities involved inincorporating the outlier detection mechanism into knowledge bases expressed in default logic and extended disjunctivelogic programs. We believe that a thorough complexity analysis is a useful step towards singling out the more complexsubtasks involved in outlier detection. This first step is conducive to designing effective algorithms for implementationpurposes.According to the view adopted in this work, the witness that an observation is an outlier is a property or a behaviorthat is explicitly the opposite of what is expected. Representing such contradicting properties requires the usage of classicalnegation. Both default logic and extended logic programs make use of classical negation. Hence, these two languages repre-sent a natural setting for outlier detection. A different approach, which does not require that the negation of the exceptionalproperty is explicitly inferred but, rather, that it is simply not entailed by a logic program, is taken in [5]. As explainedthoroughly in this paper, the anomalies that can be singled out by the definition of [5] are quite different than the outliersdetected by the work presented here. This is mirrored in the different complexity figures we obtained: most of the outlierdetection problems investigated here lie at the third level of the polynomial hierarchy, whereas the most complex of theproblems considered in [5] are contained in its second level. In the sequel we will further elaborate on these differences.The rest of this paper is organized as follows: Section 2 provides preliminary definitions and Section 3 defines outliersand related notions. Section 4 discusses the complexity of finding outliers in general propositional as well as in disjunction-free default logics. Section 5 deals with first-order defaults. Section 6 discusses related work—in particular, the relationshipbetween outlier detection and abduction. Finally, Section 7 draws conclusions.2. Preliminary definitionsIn this section we briefly review preliminary definitions used in default logic and extended (disjunctive) logic programs.Note that only the propositional fragment of these logics is considered here. Outlier detection in first-order default languagesshall be briefly discussed in Section 5. Thus, whenever a default theory or a logic program with variables is used, it isreferred to as an abbreviation of its grounded version.\fF. Angiulli et al. / Artificial Intelligence 172 (2008) 1837–187218392.1. Default logicDefault logic was first introduced by Reiter in a first-o",
            {
                "entities": [
                    [
                        3997,
                        4025,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 175 (2011) 142–164Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA semantic characterization of a useful fragment of the situation calculuswith knowledgeGerhard Lakemeyer a,∗, Hector J. Levesque ba Dept. of Computer Science, RWTH Aachen, 52056 Aachen, Germanyb Dept. of Computer Science, University of Toronto, Toronto, Ontario, Canada M5S 3A6a r t i c l ei n f oa b s t r a c tArticle history:Available online 3 April 2010Keywords:Knowledge representationReasoning about action1. IntroductionThe situation calculus, as proposed by McCarthy and Hayes, and developed over the lastis reconsidered. A new logical variant called ES isdecade by Reiter and co-workers,proposed that captures much of the expressive power of the original, but where certaintechnical results are much more easily proved. This is illustrated using two existingnon-trivial results: the determinacy of knowledge theorem of Reiter and the regressiontheorem, which reduces reasoning about the future to reasoning about the initial situation.Furthermore, we show the correctness of our approach by embedding ES in Reiter’ssituation calculus.© 2010 Elsevier B.V. All rights reserved.Among the many contributions of John McCarthy, the formalism of the situation calculus has proved to be an extremelyuseful tool for reasoning precisely about action and change. It was originally proposed in [29,30] as a dialect of first-orderlogic. A second-order refinement of the language, developed by Reiter and his colleagues [36], forms the theoretical andimplementation foundation for Golog [27], a language for the high-level control of robots and other agents (see, for example,[2,31]). Over the past decade, a number of extensions have been proposed to deal with issues such as time, natural actions,knowledge of agents, numerical uncertainty, or utilities (see [36] and the references therein).As a formalism, the situation calculus is based on axioms. In Reiter’s formulation, which is also our starting point, thesetake the form of so-called basic action theories. These consist of a number of foundational axioms, which define the space ofsituations, unique-name axioms for actions, axioms describing action preconditions and effects, and axioms about the initialsituation.What makes basic action theories particularly useful is the formulation of action effects in terms of successor state axioms,which not only provide a simple solution to the frame problem [35] but also allow the use of regression-based reasoning,which has been used in planning [8] and forms the core of every Golog interpreter, for example. Derivations using regressionare simple, clear, and computationally feasible.Since the situation calculus is defined axiomatically, no special semantics is needed. Tarskian models suffice, providedthey satisfy the foundational axioms. When the focus is on logical entailments, which is the case in the execution of Gologprograms, for example, this approach seems perfectly adequate.However, when we wish to consider theoretical questions about basic action theories that are not direct entailmentquestions, problems arise. For example, suppose we are doing an analysis of our system, and want to know, if wheneverTheory1 entails Formula1, is it also true that Theory2 entails Formula2? Here we can run into serious complications in an* Corresponding author.E-mail addresses: gerhard@cs.rwth-aachen.de (G. Lakemeyer), hector@cs.toronto.edu (H.J. Levesque).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.005\fG. Lakemeyer, H.J. Levesque / Artificial Intelligence 175 (2011) 142–164143Fig. 1. A simple robot.axiomatic setting unless there are ways to take derivations of Formula1 from Theory1 and convert them into derivations ofFormula2 from Theory2. Similar issues arise with consistency questions.For instance, consider the epistemic extension of the situation calculus, as introduced by Moore and later extended byScherl and Levesque [32,39]. If Know( A) entails (Know(B) ∨ Know(C)) in this theory, is it also true that Know( A) entailsKnow(B) or Know( A) entails Know(C)? For restricted A, B, C , the answer is yes, but the proof by Reiter requires a multi-page argument using considerable proof–theoretic machinery, including Craig’s Interpolation Lemma [37].One might wonder whether a semantic proof using Tarski structures would be any easier. The answer, in short, is no. Theproblem is that different Tarski structures can have different domains and considerable effort is required to standardize thedomains, identify the situations, and amalgamate multiple structures into a single structure that satisfies the foundationalaxioms. While certainly possible, the argument is again long and complicated.In contrast, in the epistemic logic KL [22], the semantic proof of the above determinacy of knowledge theorem is simple,clear and direct. One reason for this is the use of a semantic formulation involving possible worlds for knowledge [12,7].Typical of these formalisms, situations and possible worlds are not reified in the language itself. Beyond this, however,a major factor in the simplicity of proofs in KL (and its extension, OL) is the use of standard names, which allows a substi-tutional interpretation of the first-order quantifiers.1 While there have been philosophical arguments against substitutionalquantification [18], our experience has been that its technical simplicity has been of tremendous help in tackling issuessuch as quantifying-in [15], which are rarely addressed in other formalisms.Since KL only deals with static knowledge bases, an amalgamation of KL and the situation calculus was previously pro-posed [19]. However, this formalization kept situations reified, did not allow substitutional quantification, and the definitionof knowledge required second-order logic, all of which again complicated the proofs considerably, even semantic ones.In this paper, we propose a rather different amalgamation of KL and the situation calculus called ES. The idea is tokeep the simplicity of KL, and while dropping some of the expressiveness of the ordinary situation calculus, retain its mainbenefits, like successor state axioms to solve the frame problem and regression-based reasoning. In particular, we will use apossible-world semantics where situations are part of the semantics but do not appear as terms in the language. In order torepresent what is true in a situation after a number of actions have occurred, we use special modal operators. For example,we will have formulas like those of traditional dynamic logic [33,10], such as[forward] [forward] distance = 4to say that a robot is four units away from a wall after moving forward twice (see Fig. 1 for an illustration). In contrastto other modal approaches such as [3,11,5] but similar to [6], we also allow formulas of the form ∀a, x.([a](distance = x) ≡φ), where modalities contain (action) variables. This feature will be key in reconstructing Reiter’s basic action theories inour language. Moreover, unlike standard modal logics (including dynamic logics), we will be able to use a substitutionalinterpretation for first-order quantifiers. This is perhaps the main reason why we cannot afford situation terms as part ofour language. The epistemic situation calculus requires us to consider an uncountable number of initial situations (see [26]for a second-order foundational axiom that makes this explicit). In a language with only countably many situation terms,this would preclude a substitutional interpretation of quantifiers.Yielding much simpler proofs (like the determinacy of knowledge and the correctness of regression) still leaves open thequestion of the overall correctness of the approach. In other words, is ES really a faithful reconstruction of the situationcalculus? We will prove that it is by providing an embedding of ES in Reiter’s version of the situation calculus, showingthat the valid sentences of ES can be cast as entailments in Reiter’s original version (modulo some modest assumptions).This shows that ES is a notational variant for a fragment of the situation calculus that can be given a clean and work-able semantics. In addition, this result allows us to automatically transfer results obtained for ES to that fragment of thesituation calculus, which is expressive enough to formulate basic action theories, and more.The rest of the paper is organized as follows. In the next section we introduce the syntax and semantics of ES anddiscuss some of the properties of knowledge. In Section 3, we introduce the ES-version of Reiter’s basic action theories,1 Roughly speaking, this amounts to assuming at the outset that the domain of quantification is countably infinite and that there is a set of specialconstants called standard names uniquely denoting each element of the domain. A first-order universal sentence then ends up being true iff every instanceof the sentence, where a standard name substitutes for the variable, is true.\f144G. Lakemeyer, H.J. Levesque / Artificial Intelligence 175 (2011) 142–164followed by regression theorems for the non-epistemic and epistemic case. Section 5 provides the results on embedding ESin the situation calculus. We end the paper with a discussion of related work, only-knowing, and concluding remarks.2. The logic ESThe language is a second-order modal dialect with equality and sorts of type object and action. Before presenting theformal details, here are the main features:• Standard names: Unlike other languages (but similar to KL), the language includes (countably many) standard names forboth objects and actions. These can be thought of as special extra constants that satisfy the unique name assumptionand an infinitary version of domain closure. This allows first-order quantification to be understood substitutionally.Equality can also be given a simpler treatment: every ground term will have a coreferring standard ",
            {
                "entities": [
                    [
                        3579,
                        3607,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 241 (2016) 66–102Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintMultiWiBi: The multilingual Wikipedia bitaxonomy projectTiziano Flati∗, Daniele Vannella, Tommaso Pasini, Roberto NavigliDipartimento di Informatica, Sapienza Università di Roma, Italya r t i c l e i n f oa b s t r a c tArticle history:Received 14 May 2015Received in revised form 10 August 2016Accepted 15 August 2016Available online 8 September 2016Keywords:Taxonomy extractionTaxonomy inductionMachine learningNatural language processingCollaborative resourcesWikipedia1. IntroductionWe present MultiWiBi, an approach to the automatic creation of two integrated taxonomies for Wikipedia pages and categories written in different languages. In order to create both taxonomies in an arbitrary language, we first build them in English and then project the two taxonomies to other languages automatically, without the help of language-specific resources or tools. The process crucially leverages a novel algorithm which exploits the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show that the taxonomical information in MultiWiBi is characterized by a higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet, LHD and WikiTaxonomy, also across languages. MultiWiBi is available online at http :/ /wibitaxonomy.org /multiwibi.© 2016 Elsevier B.V. All rights reserved.Over recent decades, knowledge has increasingly become the fundamental “lubricant” of our society. The Web today is by far the largest repository of knowledge in history and, as it gradually creeps into all aspects of our everyday lives, the ability to manipulate and control its knowledge concerns everyone, both the great mass of general users and researchers [1–3], and the big industry players [4,5] that are called upon to process and deliver information in an efficient and accurate manner. With the exception of rare cases, such as WordNet [6], for which knowledge has been manually encoded, the building of big repositories of knowledge requiring human intervention, and the extended development times this entails, has now become, unfortunately, no longer feasible. Such approaches simply cannot cope with the high volume of information, its heterogeneity and the need to have knowledge available in as many languages as possible. Nevertheless, having such large repositories of knowledge embedded into intelligent systems would positively impact several Natural Language Processing (NLP) tasks, such as question answering [7–10], machine reading [11], entity linking [12,13], information extraction [14,15]and automatic reasoning [16–18]. For example, traditional open-domain Question Answering systems might not be able to answer questions such as “Which architect designed the Shard London Bridge?”. Even in the case where the answer is in effect provided within the text (e.g., “Renzo Piano designed many skyscrapers, among which is the Shard London Bridge”), additional information is usually needed at the semantic type level [19] (e.g., “Renzo Piano” is an architect and “Shard London Bridge” is a skyscraper). As a further demonstration of concept, Word Sense Disambiguation [20,21] might also receive a significant boost. Consider, for instance, the sentence “The woman lit a match”: by combining the information that i) a match can either be a lighter or a contest (contribution from the taxonomy) with ii) the fact that only lighters are usually lit (contribution from the disambiguation system), it should be possible to achieve higher disambiguation performance. Because of the usefulness of taxonomies, researchers and industrial stakeholders have been seeking for decades to design * Corresponding author.E-mail addresses: flati@di.uniroma1.it (T. Flati), vannella@di.uniroma1.it (D. Vannella), pasini@di.uniroma1.it (T. Pasini), navigli@di.uniroma1.it (R. Navigli).http://dx.doi.org/10.1016/j.artint.2016.08.0040004-3702/© 2016 Elsevier B.V. All rights reserved.\fT. Flati et al. / Artificial Intelligence 241 (2016) 66–10267novel mechanisms capable of automatically extracting valuable information which is both broad and accurate at the same time. This goal has been pursued in many different ways. In the early days (but such approaches remain as alive as ever) there was the conviction and the desire to extract knowledge from linguistic textual repositories alone: methods based on distributional word cooccurrence and statistical analysis over linguistic patterns relied on nothing but free text. Given the limited size and source of the textual corpora on which these systems relied, however, even when they proved to be accurate, they failed to serve as true general domain data providers. As time went by, though, collaborative efforts started to sprout spontaneously, with the aim of developing true encyclopedic stores in which users could actively contribute by enhancing the resource with additional information. Wikipedia, started in 2001, is one of the biggest such movements and currently the most active one, with knowledge available in 294 languages at the time of writing. A real added value brought by Wikipedia is the possibility to enrich text with hyperlinks: this feature, combined with the availability of tabular information, makes it possible to extract semi-structured information on a large scale [22,23].Over time, systems have targeted very different types of relation, sometimes very general or open-domain (TextRunner [24], ReVerb [25], and approaches at the syntactic-semantic interface like [26] and DefIE [14]), sometimes very specific or bound to a particular domain. Semantic relations encode a large number of linguistic aspects, spanning from general relatedness (as is the case for links across Wikipedia pages) up to specific types, such as hypernymy, holonymy, meronymy, and so on [27]. It became increasingly clear that hypernymy relations represented one of the most important types which could be used to boost current artificial intelligent systems. Starting from the eighties, a whole branch of research had focused on this type of semantic relation, with the pioneering work of Hearst [28] laying the foundation for the forthcoming literature. Hearst’s patterns, however, were designed to be applicable only to free text and did not exploit any specific feature of the collaborative machine-readable repositories yet to come. One of the first attempts to extract is-a information from Wikipedia dates back to WikiTaxonomy [29] which transformed the noisy network of Wikipedia categories into a structured taxonomy of concepts. Subsequently, the example of WikiTaxonomy inspired a full line of research, including YAGO [30], WikiNet [31], MENTA [32], and more recently LHD [33].Many of the above-mentioned taxonomies are focused on English and do not easily scale to dozens of languages, due to their dependency on English corpora and tools. Nonetheless, the multilinguality issue has been addressed in some of the existing taxonomies in a number of ways: DBpedia is based on manual mappings of Wikipedia infoboxes across languages to concepts in a small upper ontology, MENTA combines the taxonomical information from WordNet with information coming from several elements of Wikipedia, such as infoboxes and categories. LHD relies on a simple, though general, linking approach based on string-matching rules. However, the type of knowledge extracted by these approaches is either partial (is-a information is provided only for Wikipedia pages or Wikipedia categories), incomplete (lacking full coverage) or heterogeneous (i.e., not drawn from a shared, standard repository).In contrast, in this paper we present an approach to the automatic creation of an integrated bitaxonomy of Wikipedia pages and categories for multiple languages, called MultiWiBi, which addresses all the above-mentioned issues:1. First, it does not focus on Wikipedia pages or categories on their own but taxonomizes the two sides together, showing that they are mutually beneficial for inducing a wide-coverage and fine-grained integrated taxonomy. In particular, hypernyms are returned in a coherent manner, such that a Wikipedia page (category) has a Wikipedia page (category) as hypernym. The rationale behind this decision is that not only has Wikipedia been designed with these two separate but interconnected structures in mind, but also the nature of the two sides of Wikipedia is very different, in that pages encode concepts and named entities while categories group pages into coherent sets. For these reasons it would be unnatural to merge these two types of item.2. Second, our method is able to taxonomize Wikipedias in any language, in a way that is fully independent of additional resources. At the core of our approach, in fact, lies the idea that the English version of Wikipedia can be linguistically exploited as a pivot to project the taxonomic information to any other language offered by Wikipedia, in order to produce a bitaxonomy in arbitrary languages. English has been chosen as pivoting language because i) the quality of other Wikipedia languages is not comparable to the English version (see Section 12); ii) it is the language with the provable highest-performance syntactic parser, thus leading to the best hypernym lemmas; iii) English is the language which features by far the greatest number of pages in Wikipedia.1 Nonetheless, our method can potentially pivot on any language, not only English; we chose English as pivoting language because it is the language with the highest amount of data and, presumably, also the highest quality.3. Third, we prove that our approach overcomes the language barrier by extracting not only hypernyms for projectable concepts, but also for those concepts which do not have an English counterpart and therefore represent culture-specific bits of knowledge.2. Background an",
            {
                "entities": [
                    [
                        4019,
                        4047,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 167 (2005) 170–205www.elsevier.com/locate/artintSemiotic schemas:A framework for grounding languagein action and perceptionDeb RoyCognitive Machines Group, The Media Laboratory, Massachusetts Institute of Technology, USAReceived 5 August 2004; received in revised form 21 February 2005; accepted 14 April 2005Available online 8 August 2005AbstractA theoretical framework for grounding language is introduced that provides a computational pathfrom sensing and motor action to words and speech acts. The approach combines concepts fromsemiotics and schema theory to develop a holistic approach to linguistic meaning. Schemas serve asstructured beliefs that are grounded in an agent’s physical environment through a causal-predictivecycle of action and perception. Words and basic speech acts are interpreted in terms of groundedschemas. The framework reflects lessons learned from implementations of several language process-ing robots. It provides a basis for the analysis and design of situated, multimodal communicationsystems that straddle symbolic and non-symbolic realms. 2005 Published by Elsevier B.V.Keywords: Grounding; Representation; Language; Situated; Embodied; Semiotic; Schemas; Meaning;Perception; Action; Cross-modal; Multimodal1. Language and meaningThe relationship between words and the physical world, and consequently our ability touse words to refer to entities in the world, provides the foundations for linguistic commu-nication. Current approaches to the design of language processing systems are missing thisE-mail address: dkroy@media.mit.edu (D. Roy).0004-3702/$ – see front matter  2005 Published by Elsevier B.V.doi:10.1016/j.artint.2005.04.007\fD. Roy / Artificial Intelligence 167 (2005) 170–205171critical connection, which is achieved through a process I refer to as grounding—a term Iwill define in detail. A survey of contemporary textbooks on natural language processingreveals a rich diversity of data structures and algorithms concerned solely with manipula-tion of human-interpretable symbols (in either text or acoustic form) without any seriouseffort to connect semantic representations to the physical world.Is this a problem we should really care about? Web search engines and word processorsseem to work perfectly fine—why worry about distant connections between language andthe physical world? To see why, consider the problem of building natural language process-ing systems which can in principled ways interpret the speaker’s meaning in the followingeveryday scenarios:An elderly woman asks her aide, “Please push that chair over to me”.A man says to his waiter, “This coffee is cold!”.A child asks her father, “What is that place we visited yesterday?”.How might we build a robot that responds appropriately in place of the aide or waiter?How might a web search engine be designed to handle the child’s query? These are ofcourse not questions that are typically considered part of natural language processing, butthese are basic questions that every human language user handles with deceiving ease. Thewords in each of these examples refer to the physical world in very direct ways. The listenercannot do the right thing unless he/she (it?) knows something about the particular physicalsituation to which the words refer, and can assess the speaker’s reasons for choosing thewords as they have. A complete treatment of the meaning of these utterances—involvingboth physical and social dynamics—is beyond the framework presented in this paper.The focus here is on sub-symbolic representations and processes that connect symboliclanguage to the physical world with the ultimate aim of modeling situated language usedemonstrated by these examples.In recent years, several strands of research have emerged that begin to address the prob-lem of connecting language to the world [4,6,12,15,21,22,29,44,51,56,58,62,69,70,74] (seealso the other papers in this volume). Our own efforts have led to several implementedconversational robots and other situated language systems [25,60,61,63–65]. For example,one of our robots is able to translate spoken language into actions for object manipulationguided by visual and haptic perception [64]. Motivated by our previous implementations,and building upon a rich body of schema theory [2,19,34,43,45,50,68] and semiotics [20,42,47,49], I present a theoretical framework for language grounding that provides a com-putational path from embodied, situated, sensory-motor primitives to words and speechacts—from sensing and acting to symbols.The gist of the framework is as follows. Agents translate between speech acts, percep-tual acts, and motor acts. For example, an agent that sees a fly or hears the descriptivespeech act, “There is a fly here” is able to translate either observation into a commonrepresentational form. Upon hearing the directive speech act, “Swat that fly!”, an agentforms a mental representation that guides its sensory-motor planning mechanisms towardsthe intended goal. Signs originate from patterns in the physical world which are sensedand interpreted by agents to stand for entities (objects, properties, relations, actions, situa-tions, and, in the case of certain speech acts, goals). Speech acts, constructed from lexical\f172D. Roy / Artificial Intelligence 167 (2005) 170–205units, are one class of signs that can be observed by agents. Sensor-grounded perceptionleads to two additional classes of signs which indicate, roughly, the “what” and “where”information regarding an entity. To interpret signs, agents activate structured networks ofbeliefs1 called schemas. Schemas are made of continuous and discrete elements that arelinked through six types of projections. Two of these projection types, sensor and actionprojections, provide links between an agent’s internal representations and the external en-vironment. These links are shaped by the specific physical embodiment of the agent. Thefour remaining projection types are used for internal processes of attention, categorization,inference, and prediction.The primary focus of the framework in its current form is the interface between wordsand physical environments, and how an agent can understand speech acts that are about theenvironment. There are many important issues that are beyond the scope of this paper. I willnot address language generation, conceptual learning, language learning, or the semanticsof social or abstract domains. These topics are clearly of great importance, and will moti-vate future work that takes the framework presented here as a starting point. Learning inparticular deserves further comment. I firmly believe that to scale grounded language sys-tems, statistical machine learning will be required. Without appropriately structured biaseson what is learnable, however, the rich structures underlying situated language use willbe hopelessly out of reach of purely bottom-up data-driven learning systems. The frame-work presented here may provide useful structural constraints for future machine learningsystems.Taxonomic distinctions made in the theory are motivated by recurring distinctions thathave emerged in our implementations—distinctions which in turn were driven by practicalengineering concerns. Although the theory is incomplete and evolving, I believe it will beof value to those interested in designing physically embedded natural language processingsystems. The theory may also be of value from a cognitive modeling perspective althoughthis is not the focus of the paper (see [62]).Connecting language to the world is of both theoretical and practical interest. In practi-cal terms, people routinely use language to talk about concrete stuff that machines cannotmake sense of because machines have no way to jointly represent words and stuff. We talkabout places we are trying to find, about the action and characters of video games, about theweather, about the clothes we plan to buy, the music we like, and on and on. How can webuild machines that can converse about such everyday matters? From a theoretical perspec-tive, I believe that language rests upon deep non-linguistic roots. Any attempt to representnatural language semantics without proper consideration of these roots is fundamentallylimited.1 Although this paper deals with topics generally referred to as knowledge representation in AI, my focus willbe on beliefs. From an agent’s point of view, all that exists are beliefs about the world marked with degrees ofcertainty. Admittedly, as a robot designer, I share the intuition that a robot’s belief that x is true just in the casesfor which the corresponding situation x is the case—a correspondence that I as the designer can verify (Bickhardcalls this “designer semantics” [10]). True beliefs may be called knowledge in cases where the robot can in somesense justify its belief. However, as a starting point I prefer to model beliefs rather than knowledge so that thenotion of correspondence can be explained rather than assumed.\fD. Roy / Artificial Intelligence 167 (2005) 170–205173Fig. 1. A network of definitions extracted from Webster’s Dictionary containing circularities. To make use of suchsymbolic networks, non-linguistic knowledge is essential to ground basic terms of linguistic definitions.Inherent to current natural language processing (NLP) systems is the practice of con-structing representations of meaning that bottom out in symbolic descriptions of the worldas conceived by human designers. As a result, computers are trapped in sensory deprivationtanks, cut off from direct contact with the physical world. Semantic networks, meaning pos-tulates, and various representations encoded in first order predicate calculus all take objectsand relations as representational primitives that are assigned symbolic names. Without ad-ditional means to unpack the meaning of symbols, the machine is caught in circular chainsof dictionary-like definitions such as those shown in Fig. 1 [27]. Efforts to encode knowl-edge usin",
            {
                "entities": [
                    [
                        1672,
                        1700,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 175 (2011) 1092–1121Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintContracting preference relations for database applications ✩Denis Mindolin, Jan Chomicki∗Department of Computer Science and Engineering, 201 Bell Hall, University at Buffalo, Buffalo, NY 14260-2000, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 27 February 2009Received in revised form 17 September2010Accepted 17 September 2010Available online 1 December 2010Keywords:Preference contractionPreference changePreference query1. IntroductionThe binary relation framework has been shown to be applicable to many real-lifepreference handling scenarios. Here we study preference contraction: the problem ofdiscarding selected preferences. We argue that the property of minimality and thepreservation of strict partial orders are crucial for contractions. Contractions can be furtherconstrained by specifying which preferences should be protected. We consider preferencerelations that are finite or finitely representable using preference formulas. We presentalgorithms for computing minimal and preference-protecting minimal contractions forfinite as well as finitely representable preference relations. We study relationships betweenpreference change in the binary relation framework and belief change in the belief revisiontheory. We evaluate the proposed algorithms experimentally and present the results.© 2010 Elsevier B.V. All rights reserved.A large number of preference handling frameworks have been developed [16,7,20]. In this paper, we work with thebinary relation preference framework [10,22]. Preferences are represented as ordered pairs of tuples, and sets of preferencesform preference relations. Preference relations are required to be strict partial orders (SPO): transitive and irreflexive binaryrelations. The SPO properties are believed to capture the rationality of preferences [16]. This framework can deal with finiteas well as infinite preference relations, the latter represented using finite preference formulas.Working with preferences in any framework, it is naive to expect that they never change. Preferences can change overtime: if one likes something now, it does not mean one will still like it in the future. Preference change is an active topicof current research [11,17]. It was argued [15] that along with the discovery of sources of preference change and elicitationof the change itself, it is important to preserve the correctness of the preference model in the presence of change. In thebinary relation framework, a natural correctness criterion is the preservation of SPO properties of preference relations.An operation of preference change – preference revision – has been proposed in [11]. We note that when a preferencerelation is changed using a revision operator, new preferences are “semantically combined” with the original preferencerelation. However, combining new preferences with the existing ones is not the only way people change their preferences inreal life. Another very common operation of preference change is “semantic subtraction” from a set of preferences anotherset of preferences one used to hold, if the reasons for holding the contracted preferences are no longer valid. That is, we aregiven an initial preference relation (cid:3) and a subset CON of (cid:3) (called here a base contractor) which should not hold. We wantto change (cid:3) in such a way that CON does not hold in it. This is exactly opposite to the way the preference revision operatorschange preference relations. Hence, such a change cannot be captured by the existing preference revision operators.In multi-agent scenarios, a negotiation between different agents may involve giving up individual agents’ preferences [1]. Inmore complex scenarios, preferences may be added as well as given up.✩Research partially supported by NSF grant IIS-0307434. This paper is an extended version of Mindolin and Chomicki (2008) [23].* Corresponding author.E-mail addresses: mindolin@buffalo.edu (D. Mindolin), chomicki@buffalo.edu (J. Chomicki).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.11.011\fD. Mindolin, J. Chomicki / Artificial Intelligence 175 (2011) 1092–11211093Fig. 1. Example 1. Mary’s preferences.Another reason for contracting user preferences in real-life applications is the need for widening preference query results.In many database applications, preference relations are used to compute sets of the best (i.e. the most preferred) tuples,according to user’s preferences. Such tuples may represent objects like cars, books, cameras, etc. The operator which is usedin the binary relation framework to compute such sets is called winnow [10] (or BMO in [22]). The winnow operator isdenoted as w(cid:3)(r), where r is the original set of tuples, and (cid:3) is a preference relation. If the preference relation (cid:3) is large(i.e. the user has many preferences), the result of w(cid:3)(r) may be too narrow. One way to widen the result is by discardingsome preferences in (cid:3). Those may be the preferences which do not hold any more or are not longer important.In this paper, we address the problem of contraction of preference relations. We consider it for finitely representableand finite preference relations. We illustrate now preference contraction for finite (Example 1) and finitely representable(Example 2) preference relations.Example 1. Assume a car dealer has a web site showing his inventory of cars, and Mary is a customer interested in buying acar. Assume also that Mary has a previous purchase history with the dealer, so her preferences (possibly outdated) over carsare known: she prefers every car ti to every car t j (denoted ti (cid:3)1 t j ) with i > j (i, j ∈ [1, 5]). Let the inventory r1 consist offour cars (r1 = {t1, t3, t4, t5}), while t2 is currently missing. The preference relation is illustrated in Fig. 1 by the set of alledges, where an edge from ti to t j shows that ti is preferred to t j . The set of the best cars according to Mary’s preferencerelation is w(cid:3)1 (r1) = {t1}.Assume that the dealer observes that while Mary is browsing the web site, she indicates equal interest in three cars: t1(as expected according to (cid:3)1), t3, and t5. As a result, her preference relation (cid:3)1 has to be changed so that t1, t3, and t5are all among the best cars, i.e., they must not be dominated by any car in the inventory. That implies that the preferencesin the set CON1 consisting of the following preferences: the preference of t1 over t3, and the preference of t1, t3, and t4over t5 do not hold any more and need to be contracted (removed from (cid:3)1). Those preferences are shown as dashed arrowsin Fig. 1. Notice that since t2 is not in the inventory, and Mary has not explicitly provided any information regarding herpreferences involving t2, the preferences of t1 over t2 and t2 over t3, t4 and t5 remain unchanged.In the example above, we showed a simple scenario of preference contraction. The user preference relation there is afinite relation; and preferences to be contracted are elicited from the user-provided feedback. Variations of this scenarioare possible. First, the user’s preference relation may be infinite but representable by a finite preference formula. Second,a possibly infinite set of preferences to discard may also be defined by a formula.Example 2. Assume that Bob prefers newer cars, and given two cars made in the same year, the cheaper one is preferred.t (cid:3)2 t(cid:5) ≡ t.year > t(cid:5).year ∨ t.year = t(cid:5).year ∧ t.price < t(cid:5).pricewhere >, < denote the standard orderings of rational numbers, the attribute “year” defines the year when the car wasmade, and the attribute “price” – its price. The information about all cars which are in stock now is shown in the table r2below:idt1t3t4t5makeKiaVWKiaVWyear2007200720062006price12 00015 00015 0007000Then the set of the most preferred cars according to (cid:3)2 is w(cid:3)2 (r2) = {t1}. Assume that having observed the set w(cid:3)2 (r2),Bob understands that it is too narrow. He decides that the car t3 is not really worse than t1. He generalizes that by statingthat the cars made in 2007 which cost 12 000 are not better than the cars made in 2007 costing 15 000. Hence, the set ofpreferences the user wants to discard can be represented by the relation CON2(cid:3)(cid:5)(cid:2)t, tCON2≡ t.year = t(cid:5).year = 2007 ∧ t.price = 12 000 ∧ t(cid:5).price = 15 000The scenarios illustrated in the examples above have the following in common: we have a (finite or finitely representableinfinite) SPO preference relation (cid:3) and a set CON, finite of infinite, of preferences to discard. Our goal is to modify (cid:3), sothat the resulting preference relation is an SPO, and the preferences in CON do not hold anymore.\f1094D. Mindolin, J. Chomicki / Artificial Intelligence 175 (2011) 1092–1121Another important property of preference relation change is minimality. Indeed, a simple way of removing a subset of apreference relation without violating its SPO properties is to remove all the preferences from this relation. However, mostlikely it is not what the user expects. Hence, it is important to change the preference relation minimally.Fig. 2. Example 3.Example 3. Take Mary’s preferences from Example 1. A naive way to discard CON1 (CON1 = {t1t3, t1t5, t3t5, t4t5}) from (cid:3)1 isto represent the contracted preference relation as (cid:3)(cid:5)1 is not transitive1(and thus not an SPO): t1 (cid:3)(cid:5)1 t3, but t1 (cid:2)(cid:5)1 t5. Hence, this change does not preserveSPO. To make the changed preference relation transitive, some other preferences have to be discarded in addition to CON1.At the same time, discarding too many preferences is not a good solution since some of them may be important. Therefore,we need to discard a minimal subset of (cid:3)1 which contains CON1 and preserves SPO in t",
            {
                "entities": [
                    [
                        4192,
                        4220,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 245 (2017) 86–114Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintGraph aggregation ✩Ulle Endriss a, Umberto Grandi ba ILLC, University of Amsterdam, The Netherlandsb IRIT, University of Toulouse, Francea r t i c l e i n f oa b s t r a c tArticle history:Received 20 June 2016Received in revised form 6 January 2017Accepted 8 January 2017Keywords:Social choice theoryCollective rationalityImpossibility theoremsGraph theoryModal logicPreference aggregationBelief mergingConsensus clusteringArgumentation theory1. IntroductionGraph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). In this paper, we introduce a formal framework for graph aggregation grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. We consider both common properties of graphs, such as transitivity and reflexivity, and arbitrary properties expressible in certain fragments of modal logic. Our results establish several connections between the types of properties preserved under aggregation and the choice-theoretic axioms satisfied by the rules used. The most important of these results is a powerful impossibility theorem that generalises Arrow’s seminal result for the aggregation of preference orders to a large collection of different types of graphs.© 2017 Elsevier B.V. All rights reserved.Suppose each of the members of a group of autonomous agents provides us with a different directed graph that is defined on a common set of vertices. Graph aggregation is the task of computing a single graph over the same set of vertices that, in some sense, represents a good compromise between the various individual views expressed by the agents. Graphs are ubiquitous in computer science and artificial intelligence (AI). For example, in the context of decision support systems, an edge from vertex x to vertex y might indicate that alternative x is preferred to alternative y. In the context of modelling interactions taking place on an online debating platform, an edge from x to y might indicate that argument xThis work refines and extends papers presented at COMSOC-2012 [1] and ECAI-2014 [2]. We are grateful for the extensive feedback received from ✩Davide Grossi, Sylvie Doutre, Weiwei Chen, several anonymous reviewers, and the audiences at the SSEAC Workshop on Social Choice and Social Software held in Kiel in 2012, the Dagstuhl Seminar on Computation and Incentives in Social Choice in 2012, the KNAW Academy Colloquium on Dependence Logic held at the Royal Netherlands Academy of Arts and Sciences in Amsterdam in 2014, a course on logical frameworks for multiagent aggregation given at the 26th European Summer School in Logic, Language and Information (ESSLLI-2014) in Tübingen in 2014, the Lorentz Center Workshop on Clusters, Games and Axioms held in Leiden in 2015, the SEGA Workshop on Shared Evidence and Group Attitudes held in Prague in 2016, and lectures delivered at Sun Yat-Sen University in Guangzhou in 2014 as well as École Normale Supérieure and Pierre & Marie Curie University in Paris in 2016. This work was partly supported by COST Action IC1205 on Computational Social Choice. It was completed while the first author was hosted at the University of Toulouse in 2015 as well as Paris-Dauphine University, Pierre & Marie Curie University, and the London School of Economics in 2016.E-mail addresses: ulle.endriss@uva.nl (U. Endriss), umberto.grandi@irit.fr (U. Grandi).http://dx.doi.org/10.1016/j.artint.2017.01.0010004-3702/© 2017 Elsevier B.V. All rights reserved.\fU. Endriss, U. Grandi / Artificial Intelligence 245 (2017) 86–11487undercuts or otherwise attacks argument y. And in the context of social network analysis, an edge from x to y might express that person x is influenced by person y. How to best perform graph aggregation is a relevant question in these three domains, as well as in any other domain where graphs are used as a modelling tool and where particular graphs may be supplied by different agents or originate from different sources. For example, in an election, i.e., in a group decision making context, we have to aggregate the preferences of several voters. In a debate, we sometimes have to aggregate the views of the individual participants in the debate. And when trying to understand the dynamics within a community, we sometimes have to aggregate information coming from several different social networks.In this paper, we introduce a formal framework for studying graph aggregation in general abstract terms and we dis-cuss in detail how this general framework can be instantiated to specific application scenarios. We introduce a number of concrete methods for performing aggregation, but more importantly, our framework provides tools for evaluating what con-stitutes a “good” method of aggregation and it allows us to ask questions regarding the existence of methods that meet a certain set of requirements. Our approach is inspired by work in social choice theory [3], which offers a rich framework for the study of aggregation rules for preferences—a very specific class of graphs. In particular, we adopt the axiomatic methodused in social choice theory, as well as other parts of economic theory, to identify intuitively desirable properties of aggre-gation methods, to define them in mathematically precise terms, and to systematically explore their logical consequences.An aggregation rule maps any given profile of graphs, one for each agent, into a single graph, which we are often going to refer to as the collective graph. The central concept we focus on in this paper is the collective rationality of aggregation rules with respect to certain properties of graphs. Suppose we consider an agent rational only if the graph she provides has certain properties, such as being reflexive or transitive. Then we say that a given aggregation rule F is collectively rational with respect to that property of interest if and only if F can guarantee that that property is preserved during aggregation. For example, if we aggregate individual graphs by computing their union (i.e., if we include an edge from x to y in our collective graph if at least one of the individual graphs includes that edge), then it is easy to see that the property of reflexivity will always transfer. On the other hand, the property of transitivity will not always transfer. For example, if we aggregate two graphs over the set of vertices V = {x, y, z}, one consisting only of the edge (x, y) and one consisting only of the edge ( y, z), then although each of these two graphs is (vacuously) transitive, their union is not, as it is missing the edge (x, z). Thus, the union rule is collectively rational with respect to reflexivity, but not with respect to transitivity.We study collective rationality with respect to some such well-known and widely used properties of graphs, but also with respect to large families of graph properties that satisfy certain meta-properties. We explore both a semantic and a syntactic approach to defining such meta-properties. In our semantic approach, we identify three high-level features of graph properties that determine the kind of aggregation rules that are collectively rational with respect to them. For example, transitivity is what we call a “contagious” property: under certain circumstances, namely in the presence of edge ( y, z), inclusion of (x, y) spreads to (x, z). Transitivity also satisfies a second meta-property, which we call “implicativeness”: the inclusion of two specific edges, namely (x, y) and ( y, z), implies the inclusion of a third edge, namely (x, z). The third meta-property we introduce, “disjunctiveness”, expresses that, under certain circumstances, at least one of two specific edges has to be accepted. This is satisfied, for instance, by the property of completeness: every two vertices x and y need to be connected in at least one of the two possible directions. In our syntactic approach, we consider graph properties that can be expressed in particular syntactic fragments of a logical language. To this end, we make use of the language of modal logic [4]. This allows us to establish links between the syntactic properties of the language used to express the integrity constraints we would like to see preserved during aggregation and the axiomatic properties of the rules used.We prove both possibility and impossibility results. A possibility result establishes that every aggregation rule belonging to a certain class of rules (typically defined in terms of certain axioms) is collectively rational with respect to all graph properties that satisfy a certain meta-property. An impossibility result, on the other hand, establishes that it is impossible to define an aggregation rule belonging to a certain class that would be collectively rational with respect to any graph property that meets a certain meta-property—or that the only such aggregation rules would be clearly very unattractive for other reasons. Our main result is such an impossibility theorem. It is a generalisation of Arrow’s seminal result for preference aggregation [5], which we shall recall in Section 3.1. Our approach of working with meta-properties has two advantages. First, it permits us to give conceptually simple proofs for powerful results with a high degree of generality. Second, it makes i",
            {
                "entities": [
                    [
                        4115,
                        4143,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 696–721Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintProbabilistic planning with clear preferences on missing informationMaxim Likhachev a,∗, Anthony Stentz ba Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USAb The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 8 October 2007Received in revised form 27 October 2008Accepted 29 October 2008Available online 25 November 2008Keywords:Planning with uncertaintyPlanning with missing informationPartially Observable Markov DecisionProcessesPlanningHeuristic searchFor many real-world problems, environments at the time of planning are only partially-known. For example, robots often have to navigate partially-known terrains, planes oftenhave to be scheduled under changing weather conditions, and car route-finders oftenhave to figure out paths with only partial knowledge of traffic congestions. While generaldecision-theoretic planning that takes into account the uncertainty about the environmentis hard to scale to large problems, many such problems exhibit a special property: onecan clearly identify beforehand the best (called clearly preferred) values for the variablesthat represent the unknowns in the environment. For example, in the robot navigationproblem, it is always preferred to find out that an initially unknown location is traversablerather than not, in the plane scheduling problem, it is always preferred for the weather toremain a good flying weather, and in route-finding problem, it is always preferred for theroad of interest to be clear of traffic. It turns out that the existence of the clear preferencescan be used to construct an efficient planner, called PPCP (Probabilistic Planning with ClearPreferences), that solves these planning problems by running a series of deterministic low-dimensional A*-like searches.In this paper, we formally define the notion of clear preferences on missing information,present the PPCP algorithm together with its extensive theoretical analysis, describe severaluseful extensions and optimizations of the algorithm and demonstrate the usefulnessof PPCP on several applications in robotics. The theoretical analysis shows that onceconverged, the plan returned by PPCP is guaranteed to be optimal under certain conditions.The experimental analysis shows that running a series of fast low-dimensional searchesturns out to be much faster than solving the full problem at once since memoryrequirements are much lower and deterministic searches are orders of magnitude fasterthan probabilistic planning.© 2008 Elsevier B.V. All rights reserved.1. IntroductionA common source of uncertainty in planning problems is lack of full information about the environment. A robot maynot know the traversability of the terrain it has to traverse, an air traffic management system may not be able to forecastwith certainty future weather conditions, a car route-finder may not be able to predict well future traffic congestions oreven be sure about present traffic conditions, a shopping planner may not know whether a particular item will be on saleat one of the stores it considers. Ideally, in all of these situations, to produce a plan, a planner needs to reason over theprobability distribution over all the possible instances of the environment. Such planning is known to be hard [1,2].* Corresponding author.E-mail address: maximl@seas.upenn.edu (M. Likhachev).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.10.014\fM. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721697For many of these problems, however, one can clearly name beforehand the “best” values of the variables that representthe unknowns in the environment. We call such values clearly preferred values. Thus, in the robot navigation problem, it isalways preferred to find out that an initially unknown location is traversable rather than not. In the air traffic managementproblem it is always preferred to have a good flying weather. In the problem of route planning under partially-known trafficconditions, it is always preferred to find out that there is no traffic on the road of interest. And finally, in the shoppingplanning example, it is always preferred for a store to hold a sale on the item of interest. These are just few of what webelieve to be a large class of planning problems that exhibit clear preferences on missing information. One of the reasonsfor this is that the knowledge of clear preferences on missing information is not the same as the knowledge of a best actionat a state or the value of an optimal policy. Instead, we often know at intuitive level what would be the best event for us(i.e., no traffic congestion, sale, etc.), independently of whether we choose to make use of this event or not. All the otheroutcomes, on the other hand, are of less preference to us. This intuitive information can be used in planning.In this paper we present an algorithm called PPCP (Probabilistic Planning with Clear Preferences) that is able to scaleup to very large problems by exploiting the fact that these preferences exist. PPCP constructs and refines a plan by runninga series of deterministic A*-like searches. Furthermore, by making an approximating assumption that it is not necessaryto retain information about the variables whose values were discovered to be clearly preferred values, PPCP keeps thecomplexity of each search low and independent of the amount of the missing information. Each search is extremely fast,and running a series of fast low-dimensional searches turns out to be much faster than solving the full problem at once sincethe memory requirements are much lower and deterministic searches can often be many orders of magnitude faster thanprobabilistic planning techniques. While the assumption PPCP makes does not need to hold for the algorithm to converge,the returned plan is guaranteed to be optimal if the assumption does hold.The paper is organized as follows. We first briefly go over A* search and explain how it can be used to find least-costpaths in graphs. We then explain how a planning problem changes when some of the information about the environmentis missing. In Section 4, we introduce the notion of clear preferences on missing information and briefly talk about theproblems that exhibit them. In Section 5, we explain the PPCP algorithm and how it makes use of the clear preferences. Thesame section gives an extensive theoretical analysis of PPCP that includes the correctness of the algorithm, some complexityresults as well as the conditions for the optimality of the plan returned by PPCP. In Section 6 of the paper, we describe twouseful extensions of the algorithm such as how one can interleave PPCP planning and execution. In the same section, wealso give two optimizations of the algorithm which at least for some problems can speed it up by more than a factor offour. On the experimental side, Section 7 shows how PPCP enabled us to successfully solve the path clearance problem, animportant problem in defense robotics. The experimental results in Section 8.1, on the other hand, evaluate the performanceof PPCP on the problem of robot navigation in partially-known terrains. They show that in the environments small enoughto be solved with methods guaranteed to converge to an optimal solution (such as Real-Time Dynamic Programming [3]),PPCP always returns an optimal policy while being much faster. The results also show that PPCP is able to scale up to large(costmaps of size 500 by 500 cells) environments with thousands of initially unknown locations. The experimental resultsin Section 8.2, on the other hand, show that PPCP can also solve large instances of path clearance problem and results insubstantial benefits over other alternatives. We finally conclude the paper with a short survey of related work, discussion,and conclusions.2. Backward A* search for planning with complete informationNotations. Let us first consider a planning problem that can be represented as a search for a path in a fully knowndeterministic graph G. The fact that the graph G is completely known at the time of planning means that there is nomissing information about the domain (i.e., environment). We use S to denote a state (a vertex, in the graph terminology)in the graph G. State Sstart refers to the state of the agent at the time of planning, while state Sgoal refers to the desiredstate of the agent. We use A(S) to represent a set of actions available to the agent at state S ∈ G. Each action a ∈ A(S)corresponds to a transition (i.e., an edge) in the graph G from state S to the successor state denoted by succ(S, a). Eachsuch transition is associated with the cost c(S, a, succ(S, a)). The costs need to be bounded from below by a (small) positiveconstant.Backward A* search. The goal of shortest path search algorithms such as A* search [4] is to find a path from S start toSgoal for which the cumulative cost of the transitions along the path is minimal. The PPCP algorithm we present in thispaper is based on running a series of deterministic searches. Each of these searches is a modified backward A* search—theA* search that searches from Sgoal towards Sstart by reversing all the edges in the graph. In the following, we thereforebriefly describe the operation of a backward A* search.∗(S).Suppose for every state S ∈ G we knew the cost of a least-cost path from S to S goal. Let us denote such cost by gThen a least-cost path from Sstart to Sgoal can be easily followed by starting at Sstart and always executing such action∗(succ(S, a)). Consequently, A* search tries to computea ∈ A(S) at any state S that a = arg mina∈ A(S)(c(S, a, succ(S, a)) + g∗-values. In particular, A* maintains g-values for each state it has visited so far. g(S) is always the cost of the best pathgfound so fa",
            {
                "entities": [
                    [
                        3634,
                        3662,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 300 (2021) 103563Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAbstraction for non-ground answer set programs ✩Zeynep G. Saribatur∗, Thomas Eiter, Peter SchüllerInstitute of Logic and Computation, TU Wien, Favoritenstraße 9-11, A-1040 Vienna, Austriaa r t i c l e i n f oa b s t r a c tArticle history:Received 20 December 2019Received in revised form 11 May 2021Accepted 21 July 2021Available online 28 July 2021Keywords:AbstractionAnswer set programmingDeclarative problem solvingKnowledge representation and reasoningNonmonotonic formalismsExplaining unsatisfiabilityCounterexample-guided abstraction and refinementAbstraction is an important technique utilized by humans in model building and problem solving, in order to figure out key elements and relevant details of a world of interest. This naturally has led to investigations of using abstraction in AI and Computer Science to simplify problems, especially in the design of intelligent agents and automated problem solving. By omitting details, scenarios are reduced to ones that are easier to deal with and to understand, where further details are added back only when they matter. Despite the fact that abstraction is a powerful technique, it has not been considered much in the context of nonmonotonic knowledge representation and reasoning, and specifically not in Answer Set Programming (ASP), apart from some related simplification methods. In this work, we introduce a notion for abstracting from the domain of an ASP program such that the domain size shrinks while the set of answer sets (i.e., models) of the program is over-approximated. To achieve the latter, the program is transformed into an abstract program over the abstract domain while preserving the structure of the rules. We show in elaboration how this can be also achieved for single or multiple sub-domains (sorts) of a domain, and in case of structured domains like grid environments in which structure should be preserved. Furthermore, we introduce an abstraction-&-refinement methodology that makes it possible to start with an initial abstraction and to achieve automatically an abstraction with an associated abstract answer set that matches an answer set of the original program, provided that the program is satisfiable. Experiments based on prototypical implementations reveal the potential of the approach for problem analysis, by its ability to focus on the parts of the program that cause unsatisfiability and by achieving concrete abstract answer sets that merely reflect relevant details. This makes domain abstraction an interesting topic of research whose further use in important areas like Explainable AI remains to be explored.© 2021 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).1. IntroductionAbstraction is a technique applied in human reasoning and understanding, by reasoning over the models of the world that are built mentally [30,68]. Although its meaning comes from “to draw away”, there is no precise definition that is capable of covering all meanings that abstraction has in its utilizations. There is a variety of interpretations in different Some of the results in this article were presented in preliminary form at JELIA 2019 [113] and XAI 2019 [45]. This work has been partially supported ✩by the Austrian Science Fund (FWF) grant W-1255 and by the EU’s H2020 research and innovation programme under grant agreements 825619 (AI4EU) and 952026 (HumanE-AI Net).* Corresponding author.E-mail addresses: zeynep@kr.tuwien.ac.at (Z.G. Saribatur), eiter@kr.tuwien.ac.at (T. Eiter), ps@kr.tuwien.ac.at (P. Schüller).https://doi.org/10.1016/j.artint.2021.1035630004-3702/© 2021 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fZ.G. Saribatur, T. Eiter and P. SchüllerArtificial Intelligence 300 (2021) 10356322blue14536red145green36red76 5981249(a) 3-coloring of a graph(b) SudokuFig. 1. Use of abstraction.disciplines such as Philosophy, Cognitive Science, Art, Mathematics and Artificial Intelligence, with the shared consensus of the aim to “distill the essential” [108]. Among them is the capability of abstract thinking, which is achieved by removing irrelevant details and identifying the “essence” of a problem [71]. The notion of relevance is especially important in problem solving, as a problem may become too complex to solve if every detail is taken into account. A good strategy to solve a complex problem is to start with a coarse solution and then refine it by adding back more details. When planning a trip, for instance, one may first pick the destination and determine a coarse travel plan; fleshing out the precise details of the travel, such as taking the subway to the airport, comes later. This may be done in a hierarchy of levels of abstraction, with the lowest level containing all of the details. Another view of abstraction is the generalization aspect, which is singling out the relevant features and properties shared by objects. For example, features of an airplane such as color and cargo capacity with their possible differences may be irrelevant to the travel plan; we are (mostly) only interested in the fact that there is an airplane that takes us from Vienna to New York, say. Overall, the general aim of abstraction is to simplify the problem at hand to one that is easier to understand and deal with.For solving combinatorial problems and figuring out the key elements, humans arguably employ abstraction. In Artificial Intelligence, such problems vary from planning problems like in which order to move blocks to achieve a final configuration, to solving constraint problems such as finding an admissible coloring of the nodes of a given graph. In the latter problem, for instance, isolated nodes can be viewed as a single node and colored the same without thinking about the specific details (Fig. 1a). If a given graph is non-colorable, then we may try to find some subgraph (e.g., a clique) which causes the unsolvability, and we would not care about other nodes in the graph. Similarly with the blocks: if the labels are not important, we would disregard them when figuring out the actions. If the goal configuration cannot be achieved from the initial one, we would aim to find out the particular blocks that cause this.Notably, such disregard of detail also occurs for problems with multi-dimensional structures such as grid-cells in the well-known Sudoku problem, where a partially filled 9 × 9 board must be completed by filling in numbers 1..9 into the empty cells under constraints. If an instance is unsolvable, the reason can only be meaningfully grasped by a human by focusing on the relevant sub-regions, as looking at the whole grid is too complex. For illustration, Fig. 1b shows the sub-regions of an instance that contain the reason why no solution exists: as 6 and 7 occur in the middle column, they must appear in the sub-region below in the left column, which is unfeasible as there is only one empty cell. All these examples demonstrate abstraction abilities of humans that come naturally.Due to its important role in knowledge representation and in reasoning, abstraction has been explored in AI research early on as a useful tool for problem solving: solve a problem at hand first in an abstracted space, and then use the abstract solution as a heuristic to guide the search for a solution in the original space [70,92,106]. This approach was used in planning for speeding up the solving [64] and especially for computing heuristic functions to guide the plan search in the state space. Several abstraction methods were introduced towards this direction, especially to automatically compute abstractions that give a good heuristic [38,61,116]. However, it is well known that the success in solving a problem relies on how “good” the abstraction is. For this, theoretical approaches for defining abstractions with desired properties have been investigated [59,90]. Apart from gaining efficiency (which however may not always materialize [8,63]), abstraction forms a basis to obtain high-level explanations and an understanding of a problem. For more details on these and other related works see Section 7.3.Abstraction has been studied in other areas of AI and Computer Science as well, among them model-based diagnosis [23,89], constraint satisfaction [13,52], theorem proving [102], to name a few. Particularly fruitful were applications in model checking, which is a highly successful approach to computer aided verification [27], to tackle the state explosion problem by property preserving abstractions [26,32,82]. Furthermore, the seminal counterexample guided abstraction refinement (CEGAR) method [25] allows for automatic generation of such abstractions, by starting from an initial abstraction that over-approximates the behavior of a system to verify, and then stepwise refining the abstraction as long as needed, i.e., as long as spurious (false) counterexamples exist.Abstraction for Answer Set Programming. Answer Set Programming (ASP) [18,79] is a declarative problem solving paradigm that is rooted in knowledge representation, logic programming, and nonmonotonic reasoning. A problem is represented by a non-monotonic logic program whose answer sets (also called “stable models” [57]) correspond to the solutions of the problem. Thanks to the availability of efficient solvers and the expressiveness of the formalism, ASP has been gaining 2\fZ.G. Saribatur, T. Eiter and P. SchüllerArtificial Intelligence 300 (2021) 103563popularity for applications in many areas of AI and beyond, cf. [47–49] and references therein, from combinatorial search problems (e.g. configuration, diagnosis, planning) over system modeling (e.g., behavior of dynamic systems, beliefs and actions of agents) to knowledge-in",
            {
                "entities": [
                    [
                        3761,
                        3789,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 184–185 (2012) 38–77Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintImportance sampling-based estimation over AND/OR search spacesfor graphical modelsVibhav Gogate a,∗, Rina Dechter a,ba Department of Computer Science, The University of Texas at Dallas, Richardson, TX 75080, USAb Donald Bren School of Information and Computer Sciences, University of California, Irvine, CA 92697, USAa r t i c l ei n f oa b s t r a c tIt is well known that the accuracy of importance sampling can be improved by reducingthe variance of its sample mean and therefore variance reduction schemes have beenthe subject of much research. In this paper, we introduce a family of variance reductionschemes that generalize the sample mean from the conventional OR search space to theAND/OR search space for graphical models. The new AND/OR sample means allow tradingtime and space with variance. At one end is the AND/OR sample tree mean which has thesame time and space complexity as the conventional OR sample tree mean but has smallervariance. At other end is the AND/OR sample graph mean which requires more time andspace to compute but has the smallest variance. Theoretically, we show that the varianceis smaller in the AND/OR space because the AND/OR sample mean is defined over a largervirtual sample size compared with the OR sample mean. Empirically, we demonstrate thatthe AND/OR sample mean is far closer to the true mean than the OR sample mean.© 2012 Elsevier B.V. All rights reserved.Article history:Received 23 March 2010Received in revised form 24 February 2012Accepted 1 March 2012Available online 3 March 2012Keywords:Probabilistic inferenceApproximate inferenceGraphical modelsImportance samplingBayesian networksConstraint networksMarkov networksModel countingVariance reduction1. IntroductionImportance sampling [1,2] is a general scheme that can be used to approximate various weighted counting tasks definedover graphical models such as computing the probability of evidence in a Bayesian network, computing the partition func-tion of a Markov network and counting the number of solutions of a constraint network. The main idea is to transform thecounting or the summation task into an expectation using a special distribution called the proposal distribution. Then, thealgorithm generates samples from the proposal distribution and approximates the expectation by a weighted average overthe samples. The weighted average is often called the sample mean. It is well known that the accuracy of the estimate isinversely proportional to the variance of the sample mean and therefore significant research has focused on reducing itsvariance [2,3]. To this effect, in this paper, we propose a family of variance reduction schemes in the context of graphicalmodels called AND/OR importance sampling.The central idea in AND/OR importance sampling is to exploit problem decomposition introduced by the conditional in-dependencies in the graphical model. Recently, graph-based problem decompositions were introduced for systematic searchin graphical models [4,5] and captured using the notion of AND/OR search spaces [6]. The usual way of performing search isto systematically go over all possible instantiations of the variables, which can be organized in an OR search tree. In AND/ORsearch, additional AND nodes are interleaved with OR nodes to capture decomposition into conditionally independent sub-problems.* Corresponding author.E-mail addresses: vgogate@hlt.utdallas.edu (V. Gogate), dechter@ics.uci.edu (R. Dechter).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2012.03.001\fV. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7739We propose to organize the generated samples as a partial cover of a full AND/OR search tree yielding an AND/OR sampletree. Likewise, the OR sample tree is the portion of a full OR search tree that is covered by the samples. The main intuitionin moving from the OR space to the AND/OR space is that at the AND nodes, we can combine samples in independentcomponents to yield a virtual increase in the sample size. For example, if X is conditionally independent of Y given Z , wecan consider N samples of X independently from those of Y given the same value of Z , thereby yielding an effective orvirtual sample size of N 2 instead of the input N. Since the variance reduces as the number of samples increases (cf. [2,3]),the sample mean computed over the AND/OR sample tree has smaller variance than the one computed over the OR sampletree.We can take this idea a step further and look at the AND/OR search graph [6] as the target for compiling the givenset of samples. Since the AND/OR search graph captures more conditional independencies than the AND/OR search tree,its partial cover corresponding to the generated samples, yields an even larger virtual sample size. As a result, the samplemean computed over the AND/OR sample graph has smaller variance than the one computed over the AND/OR sample tree.∗) time-wise and a factor ofHowever, computing the AND/OR sample graph mean is more expensive, by a factor of O (wO (N) space wise, wbeing the treewidth and N being the number of samples. Thus, the AND/OR sample tree and graphmeans allow trading time and space with accuracy.∗We provide a thorough empirical evaluation comparing the impact of exploiting varying levels of problem decom-positions via AND/OR tree and AND/OR graph on a variety of probabilistic and deterministic benchmark networks. Ourexperiments demonstrate that the AND/OR sample tree mean is slightly better than the (conventional) OR sample treemean in terms of accuracy and that the AND/OR sample graph mean is clearly superior to the AND/OR sample tree mean.The rest of the paper is organized as follows. In the next section, we present preliminaries and background. In Section 3we define the AND/OR sample tree mean and in Section 4 we prove that it has smaller variance than the OR sample treemean. The AND/OR sample graph mean is defined in Section 5. Section 6 presents empirical results and we conclude inSection 7. The research presented in this paper is based in part on Gogate and Dechter [7,8].2. Preliminaries and backgroundWe denote variables by upper case letters (e.g., X, Y , . . .) and values of variables by lower case letters (e.g., x, y, . . .).Sets of variables are denoted by bold upper case letters (e.g., X = { X1, . . . , Xn}). We denote by D( Xi) the set of possiblevalues of Xi . D( Xi) is also called the domain of Xi . Xi = xi or simply xi when the variable is clear from the context, denotesthe assignment of xi ∈ D( Xi) to Xi while X = x (or simply x) denotes a sequence of assignments to all variables in X,namely x = ( X1 = x1, X2 = x2, . . . , Xn = xn). D(X) denotes the Cartesian product of the domains of all variables in X, namelyD(X) = D( X1) × · · · × D( Xn). We denote the projection of an assignment x to a set S ⊆ X by xS. Given an assignment y andz to the partition Y and Z of X, x = (y, z) denotes the composition of assignments to the two subsets.(cid:2)(cid:2)x∈D(X)x∈D(X) asExQ [ X] of a random variable X with respect to a distribution Q is defined as: ExQ [ X] =VarQ [ X] of X is defined as: VarQ [ X] =VarQ [ X] as Var[ X], when the identity of Q is clear from the context.· · ·x∈X. The expected valuex∈ X xQ (x). The variancex∈ X (x − ExQ [ X])2 Q (x). To simplify, we will write ExQ [ X] as Ex[ X] andx∈D(X) denotes the sum over all possible configurations of variables in X, namely,(cid:2)xn∈D( Xn). For brevity, we will abuse notation and writexi ∈D( Xi ) asx2∈D( X2)x1∈D( X1)=(cid:2)(cid:2)xi ∈ Xiand(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)We denote (discrete) functions by upper case letters (e.g. F , H , C , I , etc.), and the scope (set of arguments) of a functionF by scope(F ). Given an assignment y to a superset Y of scope(F ), we will abuse notation and write F (yscope(F )) as F (y).Definition 1 (Graphical models or Markov networks). A discrete graphical model or a Markov network denoted by G is a 3-tuple (cid:5)X, D, F(cid:6) where X = { X1, . . . , Xn} is a finite set of variables, D = {D( X1), . . . , D( Xn)} is a finite set of domains whereD( Xi) is the domain of variable Xi and F = {F 1, . . . , Fm} is a finite set of discrete-valued non-negative functions (also calledpotentials). The graphical model represents a joint distribution P G over X defined as:P G(x) = 1Zm(cid:3)i=1F i(x)where Z is a normalization constant, often called the partition function. It is given by:Z =(cid:4)m(cid:3)x∈Xi=1F i(x)We will often refer to Z as weighted counts.(1)(2)The primary queries over Markov networks are computing the partition function (or the weighted counts) and computingThe marginal probability P G ( Xi = xi) is a ratio of two weighted counts. Formally, let I xi be a Dirac-delta function withthe marginal probability P G ( Xi = xi).scope Xi , defined as follows:\f40V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77(cid:5)I xi (x) =1 if x = xi0 otherwiseThen, by definition, P G (xi) is given by:(cid:2)P G(xi) =I xi (x)P G(x) =(cid:4)x∈X(cid:6)mj=1 F j(x)x∈X I xi (x)Z(3)Notice that the numerator of Eq. (3) is the weighted counts of a graphical model obtained by augmenting G with I xi . Thusalgorithms for computing the weighted counts can be used for computing P G (xi).Each graphical model is associated with a primal graph which captures the dependencies present in the model.Definition 2 (Primal graph). The primal graph of a graphical model G = (cid:5)X, D, F(cid:6) is an undirected graph G(X, E) which hasvariables of G as its vertices and an edge between two variables that appear in the scope of a function.2.1. Bayesian and constraint networksDefinition 3 (Bayesian or belief networks). A Bayesian network is a graphical model B = (cid:5)X, D, G, P(cid:6) where G = (X, E) isa directed acyclic graph over the set of variables X. Each function P",
            {
                "entities": [
                    [
                        3671,
                        3699,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 309 (2022) 103738Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA tetrachotomy of ontology-mediated queries with a covering axiomOlga Gerasimova a, Stanislav Kikot b, Agi Kurucz c,∗Michael Zakharyaschev ea HSE University, Moscow, Russiab Institute for Information Transmission Problems, Moscow, Russiac Department of Informatics, King’s College London, UKd Steklov Mathematical Institute, Moscow, Russiae Department of Computer Science and Information Systems, Birkbeck, University of London, UK, Vladimir Podolskii d,a, a r t i c l e i n f oa b s t r a c tArticle history:Received 19 July 2020Received in revised form 4 May 2022Accepted 5 May 2022Available online 13 May 2022Keywords:Ontology-mediated queryDescription logicDatalogDisjunctive datalogFirst-order rewritabilityData complexityOur concern is the problem of efficiently determining the data complexity of answering queries mediated by description logic ontologies and constructing their optimal rewritings to standard database queries. Originated in ontology-based data access and datalog optimisation, this problem is known to be computationally very complex in general, with no explicit syntactic characterisations available. In this article, aiming to understand the fundamental roots of this difficulty, we strip the problem to the bare bones and focus on Boolean conjunctive queries mediated by a simple covering axiom stating that one class is covered by the union of two other classes. We show that, on the one hand, these rudimentary ontology-mediated queries, called disjunctive sirups (or d-sirups), capture many features and difficulties of the general case. For example, answering d-sirups is (cid:2)p2 -complete for combined complexity and can be in AC0 or L-, NL-, P-, or coNP-complete for data complexity (with the problem of recognising FO-rewritability of d-sirups being 2ExpTime-hard); some d-sirups only have exponential-size resolution proofs, some only double-exponential-size positive existential FO-rewritings and single-exponential-size nonrecursive datalog rewritings. On the other hand, we prove a few partial sufficient and necessary conditions of FO- and (symmetric/linear-) datalog rewritability of d-sirups. Our main technical result is a complete and transparent syntactic AC0/NL/P/coNP tetrachotomy of d-sirups with disjoint covering classes and a path-shaped Boolean conjunctive query. To obtain this tetrachotomy, we develop new techniques for establishing P- and coNP-hardness of answering non-Horn ontology-mediated queries as well as showing that they can be answered in NL.© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).* Corresponding author.(V. Podolskii), michael@dcs.bbk.ac.uk (M. Zakharyaschev).E-mail addresses: ogerasimova@hse.ru (O. Gerasimova), staskikotx@gmail.com (S. Kikot), agi.kurucz@kcl.ac.uk (A. Kurucz), podolskii@mi.ras.ruhttps://doi.org/10.1016/j.artint.2022.1037380004-3702/© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fO. Gerasimova, S. Kikot, A. Kurucz et al.Artificial Intelligence 309 (2022) 1037381. Introduction1.1. The ultimate questionThe general research problem we are concerned with in this article can be formulated as follows: for any given ontology-mediated query (OMQ, for short) Q = (O, q) with a description logic ontology O and a conjunctive query q,(data complexity) determine the computational complexity of answering Q over any input data instance A under the open (rewritability) reduce the task of finding certain answers to Q over any input A to the task of evaluating a conventional is then called a rewriting of the OMQ with optimal data complexity directly over A (the query Q(cid:3)(cid:3)world semantics and, if possible,database query QQ ).Ontology-based data access Answering queries mediated by a description logic (DL) ontology has been known as an im-portant reasoning problem in knowledge representation since the early 1990s [1]. The proliferation of DLs and their applications [2,3], the development of the (DL-underpinned) Web Ontology Language OWL,1 and especially the paradigm of ontology-based data access (OBDA) [4–6] (proposed in the mid 2000s and recently rebranded to the virtual knowledge graph (VKG) paradigm [7]), have made theory and practice of answering ontology-mediated queries (OMQs) a hot research area lying at the crossroads of Knowledge Representation and Reasoning, Semantic Technologies and the Semantic Web, Knowledge Graphs, and Database Theory and Technologies.In a nutshell, the idea underlying OBDA is as follows. The users of an OBDA system (such as Mastro2 or Ontop3) may assume that the data they want to query is given in the form of a directed graph whose nodes are labelled with concepts (unary predicates or classes) and whose edges are labelled with roles (binary predicates or properties)—even though, in reality, the data can be physically stored in different and possibly heterogeneous data sources—hence the moniker VKG. The concept and role labels come from an ontology, designed by a domain expert, and should be familiar to the intended users who, on the other hand, do not have to know anything about the real data sources. Apart from providing a user-friendly vocabulary for queries and a high-level conceptual view of the data, an important role of the ontology is to enrich possibly incomplete data with background knowledge. To illustrate, imagine that we are interested in the life of ‘scientists’ and would like to satisfy our curiosity by querying the data available on the Web (it may come from the universities’ databases, publishing companies, personal web pages, social networks, etc.). An ontology O about scientists, provided by an OBDA system, might contain the following ‘axioms’ (given, for readability, both as DL concept inclusions and first-order sentences):BritishScientist (cid:4) ∃ affiliatedWith.UniversityInUK∀x [BritishScientist(x) → ∃ y (affiliatedWith(x, y) ∧ UniversityInUK( y))]∃ worksOnProject (cid:4) Scientist∀x [∃ y worksOnProject(x, y) → Scientist(x)]Scientist (cid:9) ∃ affiliatedWith.UniversityInUK (cid:4) BritishScientist∀x [(Scientist(x) ∧ ∃ y (affiliatedWith(x, y) ∧ UniversityInUK( y))) → BritishScientist(x)]BritishScientist (cid:4) Brexiteer (cid:10) Remainer∀x [BritishScientist(x) → (Brexiteer(x) ∨ Remainer(x))]Now, to find, for example, British scientists, we could execute a simple OMQ Q (x) = (O, q(x)) with the query(1)(2)(3)(4)q(x) = BritishScientist(x)mediated by the ontology O. The OBDA system is expected to return the members of the concept BritishScientist that are extracted from the original datasets by ‘mappings’ (database queries connecting the data with the ontology vocabulary and virtually populating its concepts and roles) and also deduced from the data and axioms in O such as (3). It is this latter reasoning task that makes OMQ answering non-trivial and potentially intractable both in practice and from the complexity-theoretic point of view.1 https://www.w3 .org /TR /owl2 -overview/.2 https://www.obdasystems .com.3 https://ontopic .biz.2\fO. Gerasimova, S. Kikot, A. Kurucz et al.Artificial Intelligence 309 (2022) 103738Uniform approach To ensure theoretical and practical tractability, the OBDA paradigm presupposes that the users’ OMQs are reformulated—or rewritten—by the OBDA system into conventional database queries over the original data sources, which have proved to be quite efficiently evaluated by the existing database management systems. Whether or not such a rewriting is possible and into which query language naturally depends on the OMQ in question. One way to uniformlyguarantee the desired rewritability is to delimit the language for OMQ ontologies and queries. Thus, the DL-Lite family of description logics [5] and the OWL 2 QL profile4 of OWL 2 were designed so as to guarantee rewritability of all OMQs with a DL-Lite ontology and a conjunctive query (CQ) into first-order (FO) queries, that is, essentially SQL queries [8]. In complexity-theoretic terms, FO-rewritability of an OMQ means that it can be answered in LogTime uniform AC0, one of the smallest complexity classes [9]. In our example above, only axioms (1) and (2) are allowed by OWL 2 QL. Various dialects of tuple-generating dependencies (tgds), aka datalogor existential rules, that admit FO-rewritability and extend OWL 2 QLhave also been identified; see, e.g., [10–13].±Any OMQ with an EL, OWL 2 EL or HornSHIQ ontology is datalog-rewritable [14–17], and so can be answered inP—polynomial time in the size of data—using various datalog engines, say GraphDB,5 LogicBlox6 or RDFox.7 Axioms (1)–(3)are admitted by the EL syntax. On the other hand, OMQs with an ALC (a notational variant of the multimodal logicKn [18]) ontology and a CQ are in general coNP-complete [1], and so often regarded as intractable and not suitable for OBDA, though they can be rewritten to disjunctive datalog [19–21] supported by systems such as DLV8 or clasp.9 For example, coNP-complete is the OMQ ({(4)}, q1) with the CQ= ∃w, x, y, z [Brexiteer(w) ∧ hasCoAuthor(w, x) ∧ Remainer(x) ∧q1hasCoAuthor(x, y) ∧ Brexiteer( y) ∧ hasCoAuthor( y, z) ∧ Remainer(z)](see also the representation of q1 as a labelled graph below). It might be of interest to note that by making the role hasCoAuthor symmetric using, for example, the role inclusion axiomhasCoAuthor (cid:4) hasCoAuthor−∀x, y [hasCoAuthor(x, y) → hasCoAuthor( y, x)](5)we obtain the OMQ ({(4), (5)}, q1), which is rewritable to a symmetric datalog query, and so can be answered by a highly parallelisable algorithm in the complexity class L (logarithmic space).For various reasons, many existing ontologies do not comply with the restrictions imposed by the standard languages for OBDA. Notable examples include the large-sca",
            {
                "entities": [
                    [
                        3031,
                        3059,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 163 (2005) 91–135www.elsevier.com/locate/artintOn the consistency of cardinal direction constraints ✩Spiros Skiadopoulos a,∗, Manolis Koubarakis ba Knowledge and Database Systems Laboratory, School of Electrical and Computer Engineering,National Technical University of Athens, Zographou, 157 73 Athens, Greeceb Intelligent Systems Laboratory, Department of Electronic and Computer Engineering,Technical University of Crete, Chania, 731 00 Crete, GreeceReceived 3 December 2003; accepted 18 October 2004Available online 15 December 2004AbstractWe present a formal model for qualitative spatial reasoning with cardinal directions utilizing a co-ordinate system. Then, we study the problem of checking the consistency of a set of cardinal directionconstraints. We introduce the first algorithm for this problem, prove its correctness and analyze itscomputational complexity. Utilizing the above algorithm, we prove that the consistency checking of aset of basic (i.e., non-disjunctive) cardinal direction constraints can be performed in O(n5) time. Wealso show that the consistency checking of a set of unrestricted (i.e., disjunctive and non-disjunctive)cardinal direction constraints is NP-complete. Finally, we briefly discuss an extension to the basicmodel and outline an algorithm for the consistency checking problem of this extension. 2004 Elsevier B.V. All rights reserved.Keywords: Cardinal direction relations; Spatial constraints; Consistency checking; Qualitative spatial reasoning✩ This is a greatly revised and extended version of a paper which appears in Proc. of CP-02, Lecture Notes inComput. Sci., vol. 2470, Springer, Berlin, 2002, pp. 341–355.* Corresponding author.E-mail addresses: spiros@dblab.ece.ntua.gr (S. Skiadopoulos), manolis@intelligence.tuc.gr(M. Koubarakis).URLs: http://www.dblab.ece.ntua.gr/~spiros (S. Skiadopoulos), http://www.intelligence.tuc.gr/~manolis(M. Koubarakis).0004-3702/$ – see front matter  2004 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2004.10.010\f92S. Skiadopoulos, M. Koubarakis / Artificial Intelligence 163 (2005) 91–1351. IntroductionQualitative spatial reasoning has received a lot of attention in the areas of GeographicInformation Systems [13–15], Artificial Intelligence [6,8,13,29,36–38], Databases [32] andMultimedia [44]. Qualitative spatial reasoning problems have recently been posed as con-straint satisfaction problems and solved using traditional algorithms, e.g., path-consistency[38]. One of the most important problems in this area is the identification of useful andtractable classes of spatial constraints and the study of efficient algorithms for consistencychecking, minimal network computation and so on [38]. Several kinds of useful spatialconstraints have been studied so far, e.g., topological constraints [5,6,12,13,36–38], cardi-nal direction constraints [17,25,41] and qualitative distance constraints [14,48].In this paper, we concentrate on cardinal direction constraints [17,25,32]. Cardinal di-rection constraints describe how regions of space are placed relative to one another utilizinga co-ordinate system (e.g., region a is north of region b). Currently, the model of Goyaland Egenhofer [16,17] and Skiadopoulos and Koubarakis [40,42] is one of the most ex-pressive models for qualitative reasoning with cardinal directions. The model that we willpresent in this paper is closely related to the above model but there is a significant differ-ence. The model of [16,17,40,42] basically deals with extended regions that are connectedand have connected boundaries while our approach allows regions to be disconnected andhave holes. The regions that we consider are very common in Geography, Multimedia andImage Databases [4,7,44]. For example, countries are made up of separations (islands, ex-claves, external territories) and holes (enclaves) [7].We will study the problem of checking the consistency of a given set of cardinal direc-tion constraints in our model. Checking the consistency of a set of constraints in a modelof spatial information is a fundamental problem and has received a lot of attention in theliterature [25,32,38]. Algorithms for consistency checking are of immediate use in varioussituations including:• Propagating relations and detecting inconsistencies in a given set of spatial relations[25,38].• Preprocessing spatial queries so that inconsistent queries are detected or the searchspace is pruned [31].The technical contributions of this paper can be summarized as follows:1. We present a formal model for qualitative reasoning about cardinal directions. Thismodel is related to the model of [16,17,40,42] and is currently one of the most expres-sive models for qualitative reasoning with cardinal directions. The proposed modelformally defines cardinal direction relations on extended regions that can be discon-nected and have holes. The definition of a cardinal direction relation uses two types ofconstraints: order constraints (e.g., a < b) and set-union constraints (e.g., a = a1 ∪ a2).2. We use our formal framework to study the problem of checking the consistency ofa given set of cardinal direction constraints in the proposed model. We present thefirst algorithm for this problem and prove its correctness. The algorithm is interestingand has a non-trivial step where we show how to avoid using explicitly the obvious\fS. Skiadopoulos, M. Koubarakis / Artificial Intelligence 163 (2005) 91–13593but computational costly set-union constraints resulting from the definition of cardinaldirection relations.3. We present an analysis of the computational complexity of the consistency checkingproblem for cardinal direction constraints. We show that the aforementioned problemfor a given set of basic (i.e., non-disjunctive) cardinal direction constraints in n vari-ables can be solved in O(n5) time. Moreover, we prove that checking the consistencyof a set of unrestricted (i.e., disjunctive and non-disjunctive) cardinal direction con-straints is NP-complete.4. Finally, we consider the consistency checking problem of a set of cardinal directionconstraints expressed in an interesting extension of the basic model and outline analgorithm for this task. This extension considers not only extended regions but alsopoints and lines.The rest of the paper is organized as follows. In Section 2, we survey related work.Section 3 presents the cardinal direction relations and constraints of our model. In Sec-tion 4, we discuss the consistency checking of a set of basic cardinal direction constraints(expressed in the model of Section 3) and we present the first algorithm for this task. Sec-tion 5 studies the computational complexity of the consistency checking problem of basicand unrestricted sets of cardinal directions constraints. In Section 6, we outline algorithmsfor the consistency checking for an interesting extension of the basic cardinal directionmodel that we have already completed. Finally, Section 7 offers conclusions and proposesfuture directions.2. Related workQualitative spatial reasoning forms an important part of the commonsense reasoningrequired for building successful intelligent systems [10]. Most researchers in qualitativespatial reasoning have dealt with three main classes of spatial information: topological,directional and distance. Topological constraints describe how the boundaries, the interiorsand the exteriors of two regions relate [5,6,12,36–38]. For instance, if a and b are regionsthen a includes b and a externally connects with b are topological constraints. Directional(or orientation) constraints describe where regions are placed relative to one another [1,13,15,17,25,32,40,41]. For instance, a north b and a southeast b are directional constraints.Finally, distance constraints describe the relative distance of two regions [14,48]. For in-stance, a is far from b and a is close to b are distance constraints.In this paper, we concentrate on cardinal direction constraints [17,25,32,40]. Earlierqualitative models for cardinal direction relations approximate a spatial region by a repre-sentative point (most commonly the centroid) or by a representative box (most commonlythe minimum bounding box) [14,15,20,25,29,32].Depending on the particular spatial configuration these approximations may be toocrude [16,17]. Thus, expressing direction relations on these approximations can be mis-leading and contradictory (related observations are made in [28,34,45]). For instance, withrespect to the point-based approximation Spain is northeast of Portugal. Most people wouldagree that “northeast” does not describe accurately the relation between Spain and Portu-\f94S. Skiadopoulos, M. Koubarakis / Artificial Intelligence 163 (2005) 91–135Fig. 1. Problems with point and minimum bounding box approximations.gal on a map (see Fig. 1(a)). Similar examples are very common in geography. Consideralso the direction relation between Ireland and the UK (Fig. 1(b)). Summarizing, there is ademand for the formulation of a model that expresses direction relations between extendedobjects that overcomes the limitations of the point-based and box-based approximationmodels.With the above problem in mind, Goyal and Egenhofer [16,17] and Skiadopoulos andKoubarakis [40,42] presented a model in which we can express the cardinal direction re-lation of a region a with respect to a region b, by approximating b (using its minimumbounding box) while using the exact shape of a. Informally, the above model divides thespace around the reference region b, using its minimum bounding box, into nine areas andrecords the areas where the primary region a falls into (Fig. 1(c)). This gives a directionrelation between the primary and the reference region. Relations in the above model areclearly more expressive than point and box-based models. The model of [17,40] deals withconnected regions with a connected boundary. The model that we will present in Section 3,is a variation of the original model of [17,40] th",
            {
                "entities": [
                    [
                        2007,
                        2035,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 951–983Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintReasoning about cardinal directions between extended objects ✩Weiming Liu a, Xiaotong Zhang b, Sanjiang Li a,b,∗, Mingsheng Ying a,ba Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology, Sydney, Australiab State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology,Department of Computer Science and Technology, Tsinghua University, Beijing 100084, Chinaa r t i c l ei n f oa b s t r a c tArticle history:Received 6 July 2009Received in revised form 18 May 2010Accepted 18 May 2010Keywords:Qualitative spatial reasoningCardinal direction calculusConnected regionsConsistency checkingMaximal canonical solution1. IntroductionDirection relations between extended spatial objects are important commonsense knowl-edge. Recently, Goyal and Egenhofer proposed a relation model, known as the cardinaldirection calculus (CDC), for representing direction relations between connected plane re-gions. The CDC is perhaps the most expressive qualitative calculus for directional infor-mation, and has attracted increasing interest from areas such as artificial intelligence,geographical information science, and image retrieval. Given a network of CDC constraints,the consistency problem is deciding if the network is realizable by connected regions in thereal plane. This paper provides a cubic algorithm for checking the consistency of completenetworks of basic CDC constraints, and proves that reasoning with the CDC is in general anNP-complete problem. For a consistent complete network of basic CDC constraints, our al-gorithm returns a ‘canonical’ solution in cubic time. This cubic algorithm is also adapted tocheck the consistency of complete networks of basic cardinal constraints between possiblydisconnected regions.© 2010 Elsevier B.V. All rights reserved.Representing and reasoning with spatial information is of particular importance in areas such as artificial intelligence(AI), geographical information systems (GISs), robotics, computer vision, image retrieval, natural language processing, etc.While the numerical quantitative approach prevails in robotics and computer vision, it is widely acknowledged in AI andGIS that the qualitative approach is more attractive (see e.g. [6]).A predominant part of spatial information is represented by relations between spatial objects. In general, spatial relationsare classified into three categories: topological, directional, and metric (e.g. size, distance, shape, etc.). The RCC8 constraintlanguage [34] is the principal topological formalism in AI, and has been extensively investigated by many researchers (seee.g. [37,35,43,7,47,46,24,25,23]). When restricted to simple plane regions, RCC8 is equivalent to the 9-Intersection Model(9IM) [9], which is a very influential relation model in GIS.Unlike for topological relations, there are several competitive models for direction relations [10,11,2]. Most of thesemodels approximate a spatial object by a point (e.g. its centroid) or a box. This is too crude in real-world applicationssuch as describing directional information between two countries, say, Portugal and Spain. Recently, Goyal and Egenhofer[16,15] proposed a relation model, known as the cardinal direction calculus (CDC), for representing direction relationsbetween connected plane regions. In the CDC the reference object is approximated by a box, while the primary object is✩This paper is an extended version of Xiaotong Zhang, Weiming Liu, Sanjiang Li, Mingsheng Ying, Reasoning with cardinal directions: An efficientalgorithm, in: AAAI 2008, pp. 387–392.* Corresponding author at: Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University ofTechnology, Sydney, P.O. Box 123, Broadway, NSW 2007, Australia.E-mail address: sanjiang.li@uts.edu.au (S. Li).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.05.006\f952W. Liu et al. / Artificial Intelligence 174 (2010) 951–983not approximated. This means that the exact geometry of the primary object could be used in the representation of thedirection. This calculus has 218 basic relations, which is quite large when compared with the RCC8 and Allen’s IntervalAlgebra [1]. Due to its expressiveness, the CDC has attracted increasing interest from areas such as AI [40,41,31], GIS [17],database [39], and image retrieval [19].One basic criterion for evaluating a spatial relation model is the proper balance between its representation expressivityand reasoning complexity. While the reasoning complexity of the point-based and the box-based model of direction relationshas been investigated in depth (see [26] and [2]), there are few works discussing the complexity of reasoning with the CDC.One central reasoning problem with the CDC (and any other qualitative calculus) is the consistency (or satisfaction) prob-lem. Other reasoning problems such as deriving new knowledge from the given information, updating the given knowledge,or finding a minimal representation can be easily transformed into the consistency problem [6]. In particular, given a com-plete network of CDC constraintsN = {v iδi j v j}ni, j=1(each δi j is a CDC relation)(1)over n spatial variables v 1, . . . , vn, the consistency problem is deciding if N is realizable by a set of n connected regions inthe real plane. The consistency problem over the CDC is an open problem. Before this work, we did not know if there areefficient algorithms deciding if a set of CDC constraints are realizable. Even worse, we did not know if this is a decidableproblem. Furthermore, we did not know how to construct a realization for a satisfiable set of CDC constraints.This paper is devoted to solving these problems. We first show each consistent CDC network has a ‘canonical’ solution(Theorem 3) and then devise a cubic algorithm for checking if a complete network of basic CDC constraints is consistent.When the network is consistent, this algorithm also generates a canonical solution. We further show that deciding theconsistency of an arbitrary network of CDC constraints is an NP-Complete problem. This implies in particular that reasoningwith the CDC is decidable.Some restricted versions of the consistency problem have been discussed in the literature. Cicerone and di Felice [3]discussed the pairwise consistency problem, which decides when a pair of basic CDC relations (δ, δ(cid:3)) is consistent, i.e. when{v 1δv 2, v 2δ(cid:3)v 1} is consistent. Skiadopoulos and Koubarakis [40] investigated the weak composition problem [7,24] of theCDC, which is closely related to the consistency problem of basic CDC networks involving only three variables.The CDC algebra is defined over connected regions. A variant of the CDC was proposed in [41], where cardinal directionsbetween possibly disconnected regions are defined in the same way. This calculus, termed the CDCd in this paper, contains511 basic relations. An O (n5) algorithm1 was proposed in [41] for checking the consistency of basic constraints in the CDCd,but the consistency problem over the CDC is still open. Recently, Navarrete et al. [31] tried to adapt the approach used in[41] to cope with connected regions, but their approach turns out to be incorrect (see Remark 3 in Section 6.1 of this paper).The remainder of this paper proceeds as follows. Section 2 recalls basic notions in qualitative spatial/temporal reasoningand introduces the well-known Interval Algebra (IA) [1]. We introduce the CDC algebra in Section 3, where the connectionbetween CDC and IA relations is established in a natural way. Section 4 introduces the notion of canonical solution of aconsistent basic CDC network. Section 5 first proposes an intuitive O (n4) algorithm for consistency checking of completebasic networks and then improves it to O (n3). In Section 6, we first show local consistency is insufficient to decide theconsistency of even basic CDC networks, and then apply our main algorithm to the pairwise consistency problem and theweak composition problem. In Section 7 we adapt the main algorithm for connected regions to solve consistency checkingin two variants of the CDC. Section 8 discusses related work on the computational properties of other qualitative directioncalculi. Conclusions are given in the last section.Codes of the main algorithm are available via http://sites.google.com/site/lisanjiang/cdc, where we also provide illustra-tions for all 757 different consistent pairs of CDC basic relations and the illustration of the weak composition of S W : Wand N E : E. Interested readers may consult that webpage for detailed proofs of some minor results that are omitted in thepresent paper.Table 1 summaries notations used in this paper.2. Qualitative calculi: Basic notions and examplesSince Allen’s Interval Algebra, the study of qualitative calculi or relation models has been a central topic in qualitativespatial and temporal reasoning. This section introduces basic notions and important examples of qualitative calculi.2.1. Basic notionsLet D be a universe of temporal or spatial or spatial-temporal entities. We use small Greek symbols for representingrelations on D. For a relation α on D and two elements x, y in D, we write (x, y) ∈ α or xα y to indicate that (x, y) is aninstance of α. For two relations α, β on D, we define the complement of α, the intersection, and the union of α and β asfollows.1 We note that this algorithm applies to any (possibly incomplete) set of basic constraints.\fW. Liu et al. / Artificial Intelligence 174 (2010) 951–983953Table 1Notations.NotationsMeaningsα, β, γ , δ, θv i , v jNa, b, cI x(a), I y (a)M(a)χχ (a)a = {ai }ni=1ιx(δ), ι y (δ)ιx(δ, γ )ρ xi jρ yi jNx, N y{Ii }nmiS(a)C(a)ci jpi j , p, p(k)aribiciB(a)i",
            {
                "entities": [
                    [
                        4143,
                        4171,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 42–71www.elsevier.com/locate/artintAudiences in argumentation frameworksTrevor J.M. Bench-Capon, Sylvie Doutre, Paul E. Dunne ∗Department of Computer Science, University of Liverpool, Liverpool L69 7ZF, UKReceived 11 October 2006; received in revised form 16 October 2006; accepted 17 October 2006Available online 20 November 2006AbstractAlthough reasoning about what is the case has been the historic focus of logic, reasoning about what should be done is an equallyimportant capacity for an intelligent agent. Reasoning about what to do in a given situation—termed practical reasoning in thephilosophical literature—has important differences from reasoning about what is the case. The acceptability of an argument for anaction turns not only on what is true in the situation, but also on the values and aspirations of the agent to whom the argument isdirected. There are three distinctive features of practical reasoning: first, that practical reasoning is situated in a context, directedtowards a particular agent at a particular time; second, that since agents differ in their aspirations there is no right answer for allagents, and rational disagreement is always possible; third, that since no agent can specify the relative priority of its aspirationsoutside of a particular context, such prioritisation must be a product of practical reasoning and cannot be used as an input to it.In this paper we present a framework for practical reasoning which accommodates these three distinctive features. We use thenotion of argumentation frameworks to capture the first feature. An extended form of argumentation framework in which valuesand aspirations can be represented is used to allow divergent opinions for different audiences, and complexity results relating to theextended framework are presented. We address the third feature using a formal description of a dialogue from which preferencesover values emerge. Soundness and completeness results for these dialogues are given.© 2006 Elsevier B.V. All rights reserved.Keywords: Argumentation frameworks; Practical reasoning; Dialogue1. IntroductionReasoning about what should be done in a particular situation—termed practical reasoning in the philosophicalliterature—is carried out through a process of argumentation. Argumentation is essential because no completely com-pelling answer can be given: whereas in matters of belief, we at least should be constrained by what is actually thecase, in matters of action no such constraints apply—we can choose what we will attempt to make the case. Even anorm as universal and deep seated as thou shalt not kill is acknowledged to permit of exceptions in circumstances ofself defence and war. Thus whether arguments justifying or urging a course of action are acceptable will depend onthe aspirations and values of the agent to which they are addressed: the audience for the argument. The importance ofthe audience for arguments was recognised and advocated by Perelman [28].* Corresponding author.E-mail address: ped@csc.liv.ac.uk (P.E. Dunne).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.10.013\fT.J.M. Bench-Capon et al. / Artificial Intelligence 171 (2007) 42–7143Arguments in practical reasoning provide presumptive reasons for performing an action. These presumptive argu-ments are then subject to a process of challenge, called critical questioning in [31]. These critical questions may takethe form of other arguments, which can in turn be challenged, or may be answered by further arguments, resultingin a set of arguments constructed as the debate develops. An extension of Walton’s account of practical reasoning isgiven in [2,6], which proposes an elaborated argument scheme for practical reasoning, which incorporates the valuepromoted by acceptance of the argument, and identifies all the ways in which it can be challenged. Although mostof our discussion will treat arguments at a very abstract level, where we have need of a more particular structure forarguments, we will have this account in mind.In this paper we will propose and explore a framework for the representation and evaluation of arguments in prac-tical reasoning. Any such framework must account for some important phenomena associated with such reasoning.We will review these features in this section, and will structure the development of our framework in the remainder ofthis paper around them.First, as is clear from the brief sketch of practical reasoning above, arguments cannot be considered in isolation.Whether an argument is acceptable or not depends on whether it can withstand or counter the other arguments putforward in the debate. Once the relevant arguments have been identified, whether a given argument is acceptable willdepend on its belonging to a coherent subset of the arguments put forward which is able to defend itself against allattackers. We will call such a coherent subset a position. This notion of the acceptability of an argument deriving frommembership of a defensible position has been explored in AI through the use of argumentation frameworks [9,19],and our account will be based on a framework of this sort. Dung’s framework [19] will be recapitulated in Section 2,and then extended as the paper proceeds. The reasoning involved in constructing argumentation frameworks andidentifying positions within them is naturally modelled as a dialogue between a proponent and a critic. Dialogues forthis purpose have been proposed in [8,13,22], and we will make use of the way of exploring argument frameworks.Dialogues are discussed in Section 5.A second important feature of practical reasoning is that rational disagreement is possible, the acceptability ofan argument depending in part on the audience to which it is addressed. Within Dung’s framework it is possiblefor disagreement to be represented since argumentation frameworks may contain multiple incompatible defensiblepositions. The abstract nature of arguments, however, means that there is no information that can be used to motivatethe choice of one option over another. Searle states the need to recognise that disagreement in practical reasoningcannot be eliminated as follows [29]:Assume universally valid and accepted standards of rationality, assume perfectly rational agents operating withperfect information, and you will find that rational disagreement will still occur; because, for example, the rationalagents are likely to have different and inconsistent values and interests, each of which may be rationally acceptable.What distinguishes different audiences are their values and interests, and in order to relate the positions acceptableto a given audience to the values and interests of that audience we need a way of relating arguments to such valuesand interests. Hunter [25] makes a proposal in terms of what he calls resonance, but we will build on Value BasedArgumentation Frameworks (VAFs) proposed in [9], in which every argument is explicitly associated with a valuepromoted by its acceptance, and audiences are characterised by the relative ranking they give to these values. Wewill describe VAFs in Section 3, their properties in Section 4, and discuss the relationship between our proposal andHunter’s in Section 7.The above machinery can allow us to explain disagreement in terms of differences in the rankings of values betweendifferent audiences, but it does not allow us to explain these rankings. This brings us to the third feature of practicalreasoning for which we wish to account—that we cannot assume that the parties to a debate will come with a clearranking of values: rather these rankings appear to emerge during the course of the debate. We may quote Searle again:This answer [that we can rank values in advance] while acceptable as far as it goes [as an ex post explanation],mistakenly implies that the preferences are given prior to practical reasoning, whereas, it seems to me, they aretypically the product of practical reasoning. And since ordered preferences are typically products of practicalreason, they cannot be treated as its universal presupposition. [29]\f44T.J.M. Bench-Capon et al. / Artificial Intelligence 171 (2007) 42–71The question of how value orders emerge during debate is explored in Section 6, in which we define a dialogueprocess for evaluating the status of arguments in a VAF, and in which we show how this process can be used toconstruct positions. In the course of constructing a position, the ordering of values will be determined.Although it is not reasonable to assume that participants in a debate come with a firm value order, and so we wishto account for the emergence of such an order, neither do participants usually come to an debate with a completelyopen mind. Usually there will be some actions they are predisposed to perform, and others which they are reluctantto perform, and they will have a tendency to prefer arguments which match these predispositions. For example apolitician forming a political programme may recognise that raising taxation is electorally inexpedient and so mustexclude any arguments with the conclusion that taxes should be raised from the manifesto, while ensuring that argu-ments justifying actions bringing about core objectives are present: other arguments are acceptable in so far as theyenable this. This kind of initial intuitive response to arguments will be used to drive the construction of positions andformation of a value order. A similar technique for constructing positions on the basis of Dung’s framework has beenproposed in [11]. Because this treatment does not make use of values, however, it cannot use these reasons for actionto motivate choices, and there is no relation between the arguments which can be exploited to demand that choicesare made in a consistent and coherent manner. Our extensions to include values enable us to impose this requirementof moral consistency on the reasoners.Our overall aim is ",
            {
                "entities": [
                    [
                        3156,
                        3184,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 805–837www.elsevier.com/locate/artintNegotiating using rewardsSarvapali D. Ramchurn a,∗, Carles Sierra b, Lluís Godo b,Nicholas R. Jennings aa IAM Group, School of Electronics and Computer Science, University of Southampton, UKb IIIA—Artificial Intelligence Research Institute, CSIC, Bellaterra, SpainReceived 8 November 2006; received in revised form 2 April 2007; accepted 2 April 2007Available online 6 May 2007AbstractNegotiation is a fundamental interaction mechanism in multi-agent systems because it allows self-interested agents to cometo mutually beneficial agreements and partition resources efficiently and effectively. Now, in many situations, the agents need tonegotiate with one another many times and so developing strategies that are effective over repeated interactions is an importantchallenge. Against this background, a growing body of work has examined the use of Persuasive Negotiation (PN), which involvesnegotiating using rhetorical arguments (such as threats, rewards, or appeals), in trying to convince an opponent to accept a givenoffer. Such mechanisms are especially suited to repeated encounters because they allow agents to influence the outcomes of futurenegotiations, while negotiating a deal in the present one, with the aim of producing results that are beneficial to both parties. Tothis end, in this paper, we develop a comprehensive PN mechanism for repeated interactions that makes use of rewards that can beasked for or given to. Our mechanism consists of two parts. First, a novel protocol that structures the interaction by capturing thecommitments that agents incur when using rewards. Second, a new reward generation algorithm that constructs promises of rewardsin future interactions as a means of permitting agents to reach better agreements, in a shorter time, in the present encounter. We thengo on to develop a specific negotiation tactic, based on this reward generation algorithm, and show that it can achieve significantlybetter outcomes than existing benchmark tactics that do not use such inducements. Specifically, we show, via empirical evaluationin a Multi-Move Prisoners’ Dilemma setting, that our tactic can lead to a 26% improvement in the utility of deals that are madeand that 21 times fewer messages need to be exchanged in order to achieve this.© 2007 Elsevier B.V. All rights reserved.Keywords: Persuasive negotiation; Repeated negotiations; Negotiation tactics; Bargaining; Bilateral negotiation1. IntroductionNegotiation is a fundamental concept in multi-agent systems (MAS) because it enables self-interested agents tofind agreements and partition resources efficiently and effectively. In most cases, such negotiation proceeds as a seriesof offers and counter-offers [20]. These offers generally indicate the preferred outcome for the proponent and theopponent may either accept them, counter-offer a more beneficial outcome, or reject them. Now, in many cases, the* Corresponding author.E-mail addresses: sdr@ecs.soton.ac.uk (S.D. Ramchurn), sierra@iiia.csic.es (C. Sierra), godo@iiia.csic.es (L. Godo), nrj@ecs.soton.ac.uk(N.R. Jennings).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.014\f806S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837agents involved need to negotiate with one another many times. However, such repeated encounters have rarely beendealt with in the multi-agent systems literature (see Section 7 for more details). One of the main reasons for this isthat repeated encounters require additional mechanisms and structures, over and above those required for single shotencounters, to fully take into account the repeated nature of the interaction. In particular, offers that are generatedshould not only influence the present encounter, but also future ones, so that better deals can be found in the long run[9,25]. To this end, argument-based negotiation (ABN), in which arguments are used to support offers and persuadean opponent to accept them, has been advocated as an effective means to achieve this [30,36] and, therefore, this isthe approach we explore in this paper.In more detail, ABN techniques aim to enable agents to achieve better agreements faster by allowing them toexplore a larger space of possible solutions and/or to express, update, or evolve their preferences in single or multipleshot interactions [21]. They do this by providing additional explanations that justify the offer [1], identifying othergoals satisfied by the offer that the opponent might not be aware of [31], or offering additional incentives conditionalupon the acceptance of the offer [2,22,39]. While all these approaches capture, in one way or another, the notionof persuasiveness, a number of them have focused specifically on the use of rhetorical arguments such as threats,rewards, and appeals [3,28,41,44]. To be clear, here, we categorise such argument acts as persuasive elements thataim to force, entice, or convince an opponent to accept a given offer (see Section 7 for more details). In particular,we categorise such approaches under the general term of Persuasive Negotiation (PN) to denote the fact that these tryto find additional incentives (as opposed to justifying or elaborating on the goals of an offer) to move an opponent toaccept a given offer [30,36].In order to implement a PN mechanism, it is critical that the exchanges between the negotiating agents follow agiven pattern (i.e. ensuring that agents are seen to execute what they propose and that the negotiation terminates) andthat the agents are endowed with appropriate techniques to generate such exchanges (i.e. they can evaluate offers andcounter-offers during the negotiation process). These requirements can be met through the specification of a protocolthat dictates what agents are allowed to offer or commit to execute and a reasoning mechanism that allows agentsto make sense of the offers exchanged and accordingly determine their best response [30]. Given this, we present anovel protocol and reasoning mechanism for pairs of agents to engage in PN in the context of repeated games, inwhich the participating agents have to negotiate over a number of issues many times. In particular, we focus on theexchange of rewards (as opposed to threats or appeals). We do so because rewards have a clear benefit for the agentreceiving it, and entail a direct commitment by the agent giving it, to continue a long term relationship which is likelyto be beneficial to both participating agents.1 In addition to the standard use of rewards as something that is offeredas a prize or gift, our model also allows agents to ‘ask’ for rewards in an attempt to secure better outcomes in thefuture, while conceding in the current encounter and therefore closing the deal more quickly. This latter perspective iscommon in human-to-human negotiations where one of the participants may ask for a subsequent favour in return foragreeing to concede in the current round [17,33].Being more specific still, our PN mechanism constructs possible rewards in terms of constraints on issues to benegotiated in future encounters and our protocol extends Rubinstein’s [37] alternating offers protocol to allow agentsto negotiate by exchanging arguments along with their offers (in the form of promises of future rewards or requestsfor such promises in future encounters).Example. A car seller may reward a buyer who prefers red cars with a promise (or the buyer might ask for the reward)of a discount of at least 10% (i.e. a constraint on the price the seller can propose next time) on the price of her yearlycar servicing if she agrees to buy a blue one instead at the demanded price (as the buyer’s asking price for the red caris too low for the seller). Now, if the buyer accepts, it is a better outcome for both parties; the buyer benefits becauseshe is able to make savings in future that match her preference for the red car and the seller benefits in that he reduceshis stock and obtains immediate profit.1 The use of appeals and threats poses a number of problems. For example, the use of appeals usually assumes agents implement the samedeductive mechanism (an overly constraining assumption in most cases) because appeals impact directly on an agent’s beliefs or goals whichmeans that such appeals need to adopt a commonly understood belief and goal representation [1,3,22]. Threats, in turn, tend to break relationshipsdown and are not guaranteed to be enforced, which makes them harder to assess in a negotiation encounter [19].\fS.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837807We believe such promises are important in repeated interactions for a number of reasons. First, agents may beable to reach an agreement faster in the present game by providing some guarantees over the outcome of subsequentgames. Thus, agents may find the current offer and the reward worth more than a counter-offer (which only delaysthe agreement and future games). Second, by involving issues from future negotiations in the present game (as inthe cost of servicing in the example above), we effectively expand the negotiation space considered and, therefore,provide more possibilities for finding (better) agreements in the long run [20]. For example, agents that value futureoutcomes more (because of their lower discount factors) than their opponent are able to obtain a higher utility infuture games, while the opponent who values immediate rewards can take them more quickly. Thirdly, if the rewardguarantees the range of possible outcomes in the next game, the corresponding negotiation space is constrained bythe reward, which should reduce the number of offers exchanged to search the space and hence the time elapsedbefore an agreement is reached. Continuing the above example, the buyer starts off with an advantage next time shewants to negotiate the price to service her car and she may then not need to negotiate for long to get a rea",
            {
                "entities": [
                    [
                        3220,
                        3248,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 799–823Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintReasoning under inconsistency: A forgetting-based approach ✩Jérôme Lang a, Pierre Marquis b,∗a LAMSADE-CNRS / Université Paris-Dauphine, Place du Maréchal de Lattre de Tassigny, 75775 Paris Cedex 16, Franceb CRIL-CNRS / Université d’Artois, rue Jean Souvraz, S.P. 18, 62307 Lens Cedex, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 15 September 2009Received in revised form 23 April 2010Accepted 23 April 2010Available online 29 April 2010Keywords:Knowledge representationReasoning under inconsistencyForgetting1. IntroductionIn this paper, a fairly general framework for reasoning from inconsistent propositionalbases is defined. Variable forgetting is used as a basic operation for weakening pieces ofinformation so as to restore consistency. The key notion is that of recoveries, which are setsof variables whose forgetting enables restoring consistency. Several criteria for definingpreferred recoveries are proposed, depending on whether the focus is laid on the relativerelevance of the atoms or the relative entrenchment of the pieces of information (orboth). Our framework encompasses several previous approaches as specific cases, includingreasoning from preferred consistent subsets, and some forms of information merging.Interestingly, the gain in flexibility and generality offered by our framework does not implya complexity shift compared to these specific cases.© 2010 Elsevier B.V. All rights reserved.Reasoning from inconsistent pieces of information, represented as logical formulas, is an important issue in ArtificialIntelligence. Thus, there are at least two very different contexts where inconsistent sets of formulas have to be dealt with.The first one is when the formulas express beliefs about the real world, that may stem from different sources. In thiscase, inconsistency means that some of the formulas are just wrong. The second one is when the input formulas expresspreferences (or goals, desires) expressed by different agents (or by a single agent according to different criteria). In this case,inconsistency does not mean that anything is incorrect, but that some preferences will not be able to be fulfilled. Even ifthe nature of the problems is very different whether we are in one context or the other one, many notions and techniquesthat can be employed to reason from inconsistent sets of formulas are similar.Whatever the nature of the information represented, classical reasoning is inadequate to derive significant consequencesfrom inconsistent formulas, since it trivializes in this situation (ex falso quodlibet sequitur). This calls for other inference rela-tions which avoid the trivialization problem (namely, paraconsistent inference relations) but there is no general consensusabout what such relations should be. Actually, both the complexity of the problem of reasoning under inconsistency and itssignificance are reflected by the number of approaches that have been developed for decades and can be found in the liter-ature under various names, like paraconsistent logics, belief revision, argumentative inference, information merging, modelfitting, arbitration, knowledge integration, knowledge purification, etc. (see [7,5] for surveys).Corresponding to these approaches, many different mechanisms to avoid trivialization can be exploited. A first taxonomyallows for distinguishing between active approaches, where inconsistency is removed by identifying wrong pieces of beliefthrough knowledge-gathering actions (see e.g. [34,36,28]) or by the group of agents agreeing on some goals to be given✩This is an extended and revised version of a paper that appeared in the Proceedings of the 8th International Conference on Knowledge Representationand Reasoning (KR’02), 2002, pp. 239–250.* Corresponding author.E-mail addresses: lang@lamsade.dauphine.fr (J. Lang), marquis@cril.univ-artois.fr (P. Marquis).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.023\f800J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823up after a negotiation process, from passive approaches, where inconsistency is dealt with. In this latter case, trivializationis avoided by weakening the set of consequences that can be derived from the given base (a set of formulas). In thepropositional case, this can be achieved by two means:(1) By weakening the consequence relation of classical logic while keeping the base intact. Such an approximation by below ofclassical entailment can be achieved, which typically leads to paraconsistent logics.(2) By weakening the input base while keeping classical entailment. The pieces of information from the initial base are weakenedsuch that their conjunction becomes consistent. This technique is at work in so-called coherence-based approaches toparaconsistent inference (see e.g. [46,24,25,11,3,41,44,4] for some of the early references), where weakening the inputbase consists in inhibiting some of the pieces of information it contains (by removing them). It is also at work in beliefmerging (see e.g. [37,47,31,40] for some of the early references). Belief merging, and especially distance-based mergingconsists in weakening the pieces of information by dilating them: the piece of information φ, instead of expressing thatthe real world is for sure among the models of φ, now expresses that it is close to be a model of φ (the further a worldω from the models of φ, the less plausible it is that ω is the real world).The above dichotomy between (1) and (2) is reminiscent of the dichotomy between actual and potential contradictions, asdiscussed in [7,5]. Actual contradictions tolerate inconsistency by reasoning with a set of inconsistent statements, whereaspotential contradictions are prevented from arising by putting individually consistent yet jointly inconsistent informationtogether.In the rest of this paper we deal with potential inconsistencies and focus on the class of approaches, consisting inweakening the input base. While existing weakening-based approaches work well on some families of problems, there aretypical examples that they fail to handle in a satisfactory way (see Section 6 for a detailed discussion), the reason being thatwhile some of these approaches take account for the relative importance of pieces of information (or of the correspondingsources), they do not handle the relative importance of atoms in the problem at hand. This is problematic in many situationswhere some atoms are less central than others, especially when some atoms are meaningful only in the presence of others.For instance, it makes little sense to reason about whether John’s car is grey if there is some strong conflict about whetherJohn actually has a car. Or, in a preference merging context, suppose that a group of co-owners of a residence tries to agreeabout whether a tennis court or a swimming pool should be built: if there is no agreement about whether the swimmingpool is to be constructed, any preference concerning its colour must be (at least temporarily) ignored (this would not bethe case for its size, however, because it influences its price in a dramatic way).More generally, it is sometimes the case that ignoring a small set of propositional atoms of the formulas from an incon-sistent set renders it consistent. When reasoning from inconsistent beliefs, this allows for giving some useful informationabout the other atoms (those that are not forgotten); information about other atoms can be processed further (for instancethrough knowledge-gathering actions) if these atoms are important enough. When trying to reach a common decision froman inconsistent set of preferences, ignoring small sets of atoms allows for making a decision about all other atoms; thedecision about the few remaining atoms can then take place after a negotiation process among agents.In the following, we define a framework for reasoning from inconsistent propositional bases, using forgetting [10,39,33]as a basic operation for weakening formulas. Belief (or preference) bases are viewed as (finite) vectors of propositional for-mulas, conjunctively interpreted. Without loss of generality, each formula is assumed to be issued from a specific source ofinformation (or a specific agent). Forgetting a set X of atoms in a formula consists in replacing it by its logically strongestconsequence which is independent of X , in the sense that it is equivalent to a formula in which no atom from X occurs [33].The key notion of our approach is that of recoveries, which are sets of atoms whose forgetting enables restoring consistency.The intuition of this simple principle is that if a collection of pieces of information is jointly inconsistent, weakening it byignoring some atoms (for instance the least important ones) can help restoring consistency and derive reasonable conclu-sions. Several criteria for defining preferred recoveries are proposed, depending on whether the focus is laid on the relativerelevance of the atoms or the relative entrenchment of the pieces of information (or both).Our contributions are composed of the following models and results. We first define a general model for using variableforgetting in order to reason under inconsistency. We show that our model is general enough to encompass several classesof paraconsistent inference relations, including reasoning from preferred consistent subbases (Propositions 4.1 and 4.2) andvarious types of belief merging (Propositions 4.3, 4.4 and 4.5). Our framework does not only recover known approaches asspecific cases (which would make its interest rather limited) but it allows for new families of paraconsistent inferences,including homogeneous inferences, where propositional variables have to be forgotten in a homogeneous way from thedifferent sources, and abstraction-based inferences, where the most specific variables a",
            {
                "entities": [
                    [
                        4088,
                        4116,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 170 (2006) 59–113www.elsevier.com/locate/artintRobot introspection through learned hiddenMarkov modelsMaria Fox a,∗, Malik Ghallab b, Guillaume Infantes b, Derek Long aa Department of Computer and Information Sciences, University of Strathclyde, 26 Richmond Street,Glasgow, G1 1XH, UKb LAAS-CNRS, 7 Avenue du Colonel Roche, 31500 Toulouse, FranceReceived 1 December 2004; accepted 6 May 2005Available online 1 September 2005AbstractIn this paper we describe a machine learning approach for acquiring a model of a robot behaviourfrom raw sensor data. We are interested in automating the acquisition of behavioural models toprovide a robot with an introspective capability. We assume that the behaviour of a robot in achievinga task can be modelled as a finite stochastic state transition system.Beginning with data recorded by a robot in the execution of a task, we use unsupervised learningtechniques to estimate a hidden Markov model (HMM) that can be used both for predicting andexplaining the behaviour of the robot in subsequent executions of the task. We demonstrate that it isfeasible to automate the entire process of learning a high quality HMM from the data recorded bythe robot during execution of its task.The learned HMM can be used both for monitoring and controlling the behaviour of the robot.The ultimate purpose of our work is to learn models for the full set of tasks associated with a givenproblem domain, and to integrate these models with a generative task planner. We want to show thatthese models can be used successfully in controlling the execution of a plan. However, this paperdoes not develop the planning and control aspects of our work, focussing instead on the learningmethodology and the evaluation of a learned model. The essential property of the models we seekto construct is that the most probable trajectory through a model, given the observations made bythe robot, accurately diagnoses, or explains, the behaviour that the robot actually performed whenmaking these observations. In the work reported here we consider a navigation task. We explain* Corresponding author.E-mail address: maria.fox@cis.strath.ac.uk (M. Fox).0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.05.007\f60M. Fox et al. / Artificial Intelligence 170 (2006) 59–113the learning process, the experimental setup and the structure of the resulting learned behaviouralmodels. We then evaluate the extent to which explanations proposed by the learned models accordwith a human observer’s interpretation of the behaviour exhibited by the robot in its execution of thetask. 2005 Elsevier B.V. All rights reserved.Keywords: Stochastic learning; Hidden Markov models; Robot behaviour1. IntroductionThe goal of the work described in this paper is to automate the process of learning howa given robot executes a task in a particular class of dynamic environments. We want tolearn an abstract model of the behaviour of the robot when executing its task solely on thebasis of the sensed data that the robot records when performing the task. Having learnedan execution model of this task we want to use the model to reliably predict and explainthe behaviour of the robot carrying out that same task in any other environment belongingto the class. This paper describes how we have approached this goal in the context of anindoor navigation task, and how successful we have been in learning a reliable behaviouralmodel.1.1. MotivationThe work presented here illustrates that it can be advantageous to approach a complexartifact, such as an autonomous robot, not from the usual viewpoint in robotics of thedesigner, but from the observer’s point of view. Instead of the typical engineering questionof “how do I design my robot to behave according to some specifications”, here we addressthe different issue of “how do I model the observed behaviour of my robot”, ignoring, inthis process, the intricacy of its design.It may sound strange for a roboticist to engage in observing and modelling what a ro-bot is doing, since this should be inferrable from the roboticist’s own design. However,a modular design of a complex artifact develops only local models which are combinedon the basis of some composition principle of these models; it seldom provides globalbehaviour models. The design usually relies on some reasonable assumptions about theenvironment and does not model explicitly a changing, open-ended environment with hu-man interaction. Hence, a precise observation model of a robot behaviour in a varying andopen environment can be essential for understanding how the robot operates within thatenvironment.We are proposing in this paper a machine learning approach for acquiring a particu-lar class of behaviour models of a robot. The main motivation for this work is to buildmodels of robot task execution that are intermediate between the high level representationsused in deliberative reasoning, such as planning, and the low level representations usedin sensory-motor functions. A high-level action model, such as a collection of planningoperators with abstract preconditions and effects, is certainly needed in high level missionplanning. However, it is of limited use in monitoring and controlling the execution of plans.\fM. Fox et al. / Artificial Intelligence 170 (2006) 59–11361These functions need a more detailed model of how an action breaks down, depending onthe environment and the context, into low-level concurrent and sequential sensory-motorprimitives, and how these primitives are controlled. On the other hand, the representationsused for designing and modelling sensory-motor functions are necessarily too detailed.They are far too complex to be dealt with at a planning level, or even for execution moni-toring. The latter requires intermediate level models, either hand-programmed, learned, orrefined through specification and learning.Other authors have considered how intermediate level descriptions of task executionmight be used for designing a robot, i.e., how the corresponding models might be encodedand exploited within a plan execution framework. We are not concerned with programmingthe low level control of the robot but with providing the means by which a robot can intro-spect about the development of its behaviour in the execution of a task. We rely on hiddenMarkov models (HMMs) [25] as the intermediate level representation of this behaviour.Since these models are built empirically, they take into account the dynamics and uncer-tainty of the real execution environment. The resulting behavioural models provide a wayin which the controller can reason about the robot behaviour in the context of executing atask.Our focus here is not on learning topological or metric maps for robot navigation. Oth-ers have considered this problem in depth [1–4] and shown that navigation with respectto a given environment can be dynamically improved as the robot interacts with its envi-ronment. The use of stochastic learning techniques to improve robot navigation in a givenenvironment is therefore quite well-understood. We are concerned with learning abstractmodels of how a robot performs a compound task, whatever that task might be. Navigationis an example of such a compound task.1.2. ApproachOur objective is to be able to predict and explain the robot’s behaviour as it undertakes acompound task in the uncertain real world. In reality the robot passes through a number ofabstract behavioural states, some of which can be distinguished and identified by a humanobserver. For example, when picking up an object in its grippers a robot might be in thestate of positioning with respect to the object, approaching it, grasping it, knocking into it,lifting it, and so on.To illustrate the kind of model we are interested in learning, Fig. 1 shows a high levelstate transition model of a pickup task (this is an artificially simplified example that wasnot learned from real data). Time is abstracted out of the model and it is assumed that amonitoring process tracks how often the robot revisits the same state.It can be seen that, according to the model, the probability of knocking into the objectis 0.2 when the robot is positioning itself and when it is in the approaching state, havingpositioned itself ready to grasp the object. The probability of looping on the positioningstate is high, suggesting that the robot often fumbles to get into a good grasping position.The trajectories through this model that are actually followed by the robot might revisit thepositioning state multiply often and it might be that the state of knocking into the objectis entered most frequently when this is the case. Using the HMM to identify the mostprobable trajectory leading out of the current state provides a monitoring system with a\f62M. Fox et al. / Artificial Intelligence 170 (2006) 59–113Fig. 1. The compound task of picking up an object.powerful ability to determine the most likely outcome of the robot’s current behaviour. InSection 7 we discuss how the structure of the HMM can be exploited by such a monitoringsystem.The behavioural states of the model are hidden, because they cannot be sensed directlyby the robot. The robot is equipped with noisy sensors from which it can obtain only anestimate of its state. A hidden Markov model (HMM) represents the association betweenthese noisy sensor readings and the possible behavioural states of the system, as well as theprobabilities of transitioning between pairs of states. The HMM is therefore ideally suitedto our objectives. Our approach is to learn a HMM that relates the sensor readings made bythe robot to the hidden real states it traverses when executing its task, in order to equip therobot with the capacity to monitor its progress during subsequent executions of the sametask.Our work makes several innovations. First, we address the problem of learning the struc-ture as well as the parameters of the HMM, using a structural le",
            {
                "entities": [
                    [
                        2259,
                        2287,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 1540–1569Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintLearning complex action models with quantifiers and logical implicationsHankz Hankui Zhuo a,b, Qiang Yang b,∗, Derek Hao Hu b, Lei Li aa Department of Computer Science, Sun Yat-sen University, Guangzhou, China, 510275b Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clearwater Bay, Kowloon, Hong Konga r t i c l ei n f oa b s t r a c tArticle history:Received 2 January 2010Received in revised form 4 September 2010Accepted 5 September 2010Available online 29 September 2010Keywords:Action model learningMachine learningKnowledge engineeringAutomated planningAutomated planning requires action models described using languages such as the PlanningDomain Definition Language (PDDL) as input, but building action models from scratch is avery difficult and time-consuming task, even for experts. This is because it is difficult toformally describe all conditions and changes, reflected in the preconditions and effects ofaction models. In the past, there have been algorithms that can automatically learn simpleaction models from plan traces. However, there are many cases in the real world wherewe need more complicated expressions based on universal and existential quantifiers,implications in action models to precisely describe the underlyingas well as logicalmechanisms of the actions. Such complex action models cannot be learned using manyprevious algorithms. In this article, we present a novel algorithm called LAMP (LearningAction Models from Plan traces), to learn action models with quantifiers and logicalimplications from a set of observed plan traces with only partially observed intermediatestate information. The LAMP algorithm generates candidate formulas that are passed to aMarkov Logic Network (MLN) for selecting the most likely subsets of candidate formulas.The selected subset of formulas is then transformed into learned action models, which canthen be tweaked by domain experts to arrive at the final models. We evaluate our approachin four planning domains to demonstrate that LAMP is effective in learning complex actionmodels. We also analyze the human effort saved by using LAMP in helping to create actionmodels through a user study. Finally, we apply LAMP to a real-world application domain forsoftware requirement engineering to help the engineers acquire software requirements andshow that LAMP can indeed help experts a great deal in real-world knowledge-engineeringapplications.© 2010 Elsevier B.V. All rights reserved.1. IntroductionAutomated planning systems achieve goals by producing sequences of actions from the given action models that areprovided as input [14]. A typical way to describe the action models is to use action languages such as the Planning DomainDefinition Language (PDDL) [13,11,14] in which one can specify the precedence and consequence of actions. A traditionalway of building action models is to ask domain experts to analyze a task domain and manually construct a domain de-scription that includes a set of complete action models. Planning systems can then proceed to generate action sequences toachieve goals.However, it is very difficult and time-consuming to manually build action models in a given task domain, even forexperts. This is a typical problem of the knowledge-engineering bottleneck, where experts often find it difficult to articulatetheir experiences formally and completely. Because of this, researchers have started to explore ways to reduce the human* Corresponding author.E-mail addresses: zhuohank@mail.sysu.edu.cn (H.H. Zhuo), qyang@cse.ust.hk (Q. Yang), derekhh@cse.ust.hk (D.H. Hu), lnslilei@mail.sysu.edu.cn (L. Li).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.09.007\fH.H. Zhuo et al. / Artificial Intelligence 174 (2010) 1540–15691541effort of building action models by learning from observed examples or plan traces. Some researchers have developedmethods to learn action models from complete state information before and after an action in some example plan traces [4,15,31,50]. Others, such as Yang et al. [51,39] have proposed to learn action models from plan examples with only incompletestate information. Yang et al. [51,52] have developed an approach known as Action Relation Modeling System (ARMS) to learnaction models in a STRIPS (STanford Research Institute Problem Solver) [10] representation using a weighted MAXSAT-based(Maximum Satisfiability) approach. Shahaf et al. [39] have proposed an algorithm called Simultaneous Learning and Filtering(SLAF) to learn more expressive action models using consistency-based algorithms.Despite the success of these learning systems, in the real world, there are many applications where actions should beexpressed using a more expressive representation, namely, quantifiers and logical implications. For instance, consider thecase where there are different cases in a briefcase1 planning domain, such that a briefcase should not be moved to a placewhere there is another briefcase with the same color. We can model the action move in PDDL as follows.2action: move(?c1 - case ?l1 ?l2 - location)pre:(:and (at ?c1 ?l1)(forall ?c2 - case (imply (samecolor ?c2 ?c1)(not (at ?c2 ?l2)))))effect:(:and (at ?c1 ?l2) (not (at ?c1 ?l1)))That is, if we want to move the case c1 from the location l1 to l2, c1 should be at l1 first, and every other case c2 whosecolor is the same with c1 should not be at l2. After the action move, c1 will be at l2 instead of at l1. Likewise, consider apilot could not fly to a place where there are enemies. We can model the action model fly as follows.action:pre:fly(?p1 - pilot ?l1 ?l2 - location)(:and (at ?p1 ?l1)(forall ?p2 - person (imply (enemy ?p2 ?p1)(not (at ?p2 ?l2)))))effect:(:and (at ?p1 ?l2) (not (at ?p1 ?l1)))We can see that in these examples, we need universal quantifiers as well as logical implications in the precondition part ofthe action to precisely represent this action and compress the action model in a compact form.As another example, consider a driver who intends to drive a train. Before he can start, he should make sure all thepassengers have gotten on the train. After that, if there is a seat vacant, then he can start to drive the train. We representthis drive-train action model in PDDL as follows.action:pre:effect:drive-train(?d - driver ?t - train)(free ?d) (forall ?p - passenger (in ?p ?t))(:and (when (exist ?s - seat (vacant ?s)) (available ?t))(driving ?d ?t)(not (free ?d)))That is, if a driver ?d makes sure all the passengers ?p are in the train ?t and is free at that time, then he can drive thetrain ?t. Furthermore, if there is a seat ?s vacant, as a consequence of this action drive-train, the train will be set as availableto show that more passengers can take this train. Besides, the driver ?d will be in the state of driving the train, i.e., (driving?d ?t), and not free. Such an action model needs a universal quantifier in describing its preconditions and an existentialquantifier for the condition “(exist ?s - seat (vacant ?s))” of the conditional effect “(when (exist ?s - seat (vacant ?s))(available?t))”. More examples that require the use of quantifiers and logical implications can be found in many action models inrecent International Planning Competitions, such as the domains in IPC-53: trucks, openstacks, etc. These complex actionmodels can be represented by PDDL, but cannot be learned by existing algorithms proposed for action model learning.Our objective is to develop a new algorithm for learning complex action models with quantifiers (including conditionaleffects) and logical implications, from a collection of given example plan traces. The input of our algorithm includes: (1)a set of observed plan traces with partially observed intermediate state information between actions; (2) a list of actionheadings, each of which is composed of an action name and a list of parameters, but is not provided with preconditions oreffects; (3) a list of predicates along with their corresponding parameters. Our algorithm is called LAMP (Learn Action Modelsfrom Plan traces), which outputs a set of action models with quantifiers and implications. These action models ‘summarizes’the plan traces as much as possible, and can be used by domain experts, who need to spend only a small amount of time, inrevising parts of the action models that are incorrect or incomplete, before finalizing the action models for planning usage.Compared to many previous approaches, our main contributions are: (1) LAMP can learn quantifiers that conform to thePDDL definition [13,11], where the latter article shows that action models in PDDL can have quantifiers. (2) LAMP can learnaction models with implications as preconditions, which improves the expressiveness of learned action models. We require1 http://www.informatik.uni-freiburg.de/~koehler/ipp/pddl-domains.tar.gz.2 A symbol with a prefix “?” suggests that the symbol is a variable; e.g. “?c1” suggests that “c1” is a variable that can take on certain constants as values.3 http://zeus.ing.unibs.it/ipc-5/domains.html.\f1542H.H. Zhuo et al. / Artificial Intelligence 174 (2010) 1540–1569that existential quantifiers can only be used to quantify the left-hand side of a conditional effect, which is consistent withthe constraints used in PDDL1.2 and PDDL2.1 [13,11]. (3) Similar to some previous systems such as the ARMS system [51,52], LAMP can learn action models from plan traces with partially observed intermediate state information. This is importantbecause in many real world situations, what we can record between two actions in a plan trace is likely to be incomplete;e.g., when using only a small number of sensors, we can record a subset of what happens after each action is executed.In such a case, the number of sensors cannot cover all possible new information sources and ",
            {
                "entities": [
                    [
                        3866,
                        3894,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 175 (2011) 79–119Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintApproximation of action theories and its applicationto conformant planningPhan Huy Tu a, Tran Cao Son b,∗, Michael Gelfond c, A. Ricardo Morales ca Microsoft Corporation, 1 Microsoft Way, Redmond, WA 98052, USAb Computer Science Department, New Mexico State University, Las Cruces, NM 88003, USAc Computer Science Department, Texas Tech University, Lubbock, TX 79409, USAa r t i c l ei n f oa b s t r a c tArticle history:Available online 3 April 2010Keywords:Reasoning about action and changeKnowledge representationPlanningIncomplete informationAnswer set programmingThis paper describes our methodology for building conformant planners, which is basedon recent advances in the theory of action and change and answer set programming. Thedevelopment of a planner for a given dynamic domain starts with encoding the knowledgeabout fluents and actions of the domain as an action theory D of some action language.Our choice in this paper is AL – an action language with dynamic and static causal lawsand executability conditions. An action theory D of AL defines a transition diagram T (D)(cid:4)(cid:5) belongs to T (D)containing all the possible trajectories of the domain. A transition (cid:3)s, a, s(cid:4)iff the execution of the action a in the state s may move the domain to the state s.The second step in the planner development consists in finding a deterministic transitiondiagram T lp(D) such that nodes of T lp(D) are partial states of D, its arcs are labeled byactions, and a path in T lp(D) from an initial partial state δ0 to a partial state satisfying thein T (D). The transition diagramgoal δ f corresponds to a conformant plan for δ0 and δ fT lp(D) is called an ‘approximation’ of T (D). We claim that a concise description of anapproximation of T (D) can often be given by a logic program π (D) under the answersets semantics. Moreover, complex initial situations and constraints on plans can be alsoexpressed by logic programming rules and included in π (D). If this is possible then theproblem of finding a parallel or sequential conformant plan can be reduced to computinganswer sets of π (D). This can be done by general purpose answer set solvers. If plans aresequential and long then this method can be too time consuming. In this case, π (D) isused as a specification for a procedural graph searching conformant planning algorithm.The paper illustrates this methodology by building several conformant planners whichwork for domains with complex relationship between the fluents. The efficiency of theplanners is experimentally evaluated on a number of new and old benchmarks. In additionwe show that for a subclass of action theories of AL our planners are complete, i.e., if inT lp(D) we cannot get from δ0 to a state satisfying the goal δ f then there is no conformantplan for δ0 and δ fin T (D).© 2010 Elsevier B.V. All rights reserved.1. IntroductionA conformant planner is a program that generates a sequence of actions, which achieves a goal from any possible initialstate of the world, given the information about the initial state and the possible effects of actions. Such sequences arenormally referred to as conformant plans. In this paper we describe our methodology for the design and implementation of* Corresponding author.E-mail addresses: tuphan@microsoft.com (P.H. Tu), tson@cs.nmsu.edu (T.C. Son), mgelfond@cs.ttu.edu (M. Gelfond), ricardo@cs.ttu.edu (A.R. Morales).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.007\f80P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119conformant planners. The methodology is rooted in the ideas of declarative programming [42] and utilizes recent advancesin answer set programming and the theory of action and change. This allows the designer to guarantee a substantiallyhigher degree of trust in the planners’ correctness, as well as a greater degree of elaboration tolerance [43].The design of a declarative solution of a problem P normally involves the selection of a logical language capable ofrepresenting knowledge relevant to P . We base our methodology on representing such knowledge in action languages– formal models of parts of natural language used for reasoning about actions and their effects. A theory in an actionlanguage (often called an action description) is used to succinctly describe the collection of all possible trajectories of agiven dynamic domain. Usually this is done by defining the transition diagram, T (D), of an action description D. The statesof T (D) correspond to possible physical states of the domain represented by D. Arcs of T (D) are labeled by actions.A transition (cid:3)s, a, s. In someaction languages, actions are elementary (or atomic). In some others, an action a is viewed as a finite non-empty collectionof elementary actions. Intuitively, execution of an action a = {e1, . . . , en}, where the ei ’s are elementary actions, correspondsto the simultaneous execution of every ei ∈ a.(cid:4)(cid:5) ∈ T (D) if the execution of the action a in the state s may move the domain to the state s(cid:4)There are by now a large number of action languages (see for instance [8,24,25,32,41,67]) capturing different aspectsof dynamic domains. Our choice in this paper is AL [8] – an action language with dynamic causal laws describing directeffects of actions, impossibility conditions stating the conditions under which an action cannot be executed, and static causallaws (a.k.a. state constraints) describing static relations between fluents. For example the statement “putting a block A on topof block B causes A to be on top of B” can be viewed as a dynamic causal law describing the direct effect of action put( A, B).The statement “a block A cannot be put on B if there is a block located on A or on B” represents an impossibility condition.The statement “block A is above block C if A is on C or it is on B and B is above C” is an example of a (recursive) static causallaw. Note that static causal laws can cause actions to have indirect effects. Consider for instance the effects of executing theaction put( A, B) in a state in which both A and B are clear and B is located above some block C . The direct effects ofthis action (described by the dynamic causal law above) is on( A, B). An indirect effect, above( A, C), is obtained from ourstatic causal law. The problem of determining such indirect effects, known as the ramification problem, remained open fora comparatively long time. In the last decade, several solutions to this problem have been proposed, for example [5,38,28,37,41,53,54,46,64]. One of these solutions [41] is incorporated in the semantics of AL. The ability to represent causal lawsmakes AL a powerful modeling language. It was successfully used for instance to model the reactive control system of thespace shuttle [4]. The system consists of fuel and oxidizer tanks, valves and other plumbing needed to provide propellantto the maneuvering jets of the shuttle. It also includes electronic circuitry; both to control the valves in the fuel lines andto prepare the jets to receive firing commands. Overall, the system is rather complex, in that it includes 12 tanks, 44 jets,66 valves, 33 switches, and around 160 computer commands (computer-generated signals). The use of static causal laws(including recursive ones) was crucial for modeling the system and for the development of industrial size planning anddiagnostic applications.While static causal laws have been intensively studied by researchers interested in knowledge representation, they haverarely been considered by the mainstream planning community. Although the original specification of the Planning DomainDescription Language (PDDL) – a language frequently used for the specification of planning problems by the planning com-munity – includes axioms1 (which correspond to non-recursive static causal laws in our terminology) [27], most of theplanning domains investigated by this community, including those used for planning competitions [1,17,40] do not includeaxioms. This is partly due to the fact that the semantics for PDDL with axioms is not clearly specified, and partly to the(somewhat mistaken but apparently widespread) belief that static causal laws can always be replaced by dynamic causallaws. There is fortunately also an opposing view. For instance, in [63], the authors argue that the use of axioms not onlyincreases the expressiveness and elegance of the problem representation but also improves the performance of planners. Itis known that the complexity of the conformant planning problem is much higher than classical planning in deterministicdomains (Σ P2 vs. NP-complete) [6,68], and hence the question of efficiency becomes even more important.An action description D of AL describing the corresponding dynamic domain can be used for multiple purposes in-cluding classical planning and diagnostics (see for instance [3,4,7,35]). One way to attack this problem is to replace thetransition diagram T (D) by a deterministic transition diagram T lp(D) such that nodes of T lp(D) are partial states of D, itsarcs are labeled by actions, and a path in T lp(D) from an initial partial state δ0 to a partial state satisfying δ f correspondsto a conformant plan for δ0 and δ f in T (D). The transition diagram T lp(D) is called an approximation of T (D). Even thoughT lp(D) normally has many more states than T (D) does, validating whether a given sequence of actions is a conformant planusing T lp(D) is much easier. As pointed out in [6] the use of an approximation can substantially help reduce the complexityof the planning problem. Indeed, an approximation in domains with incomplete information and static causal laws has beendeveloped and applied successfully in the context of conditional and conformant planning in [66]. Of course a drawback ofthis approach is the p",
            {
                "entities": [
                    [
                        3619,
                        3647,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 170 (2006) 803–834www.elsevier.com/locate/artintPropagation algorithms for lexicographic ordering constraintsAlan M. Frisch a,∗, Brahim Hnich b, Zeynep Kiziltan c, Ian Miguel d, Toby Walsh ea Department of Computer Science, University of York, UKb Faculty of Computer Science, Izmir University of Economics, Turkeyc DEIS, University of Bologna, Italyd School of Computer Science, University of St Andrews, UKe National ICT Australia and Department of CS & E, UNSW, AustraliaReceived 12 April 2005; received in revised form 24 March 2006; accepted 27 March 2006AbstractFinite-domain constraint programming has been used with great success to tackle a wide variety of combinatorial problems inindustry and academia. To apply finite-domain constraint programming to a problem, it is modelled by a set of constraints on a setof decision variables. A common modelling pattern is the use of matrices of decision variables. The rows and/or columns of thesematrices are often symmetric, leading to redundancy in a systematic search for solutions. An effective method of breaking thissymmetry is to constrain the assignments of the affected rows and columns to be ordered lexicographically. This paper develops anincremental propagation algorithm, GACLexLeq, that establishes generalised arc consistency on this constraint in O(n) operations,where n is the length of the vectors. Furthermore, this paper shows that decomposing GACLexLeq into primitive constraintsavailable in current finite-domain constraint toolkits reduces the strength or increases the cost of constraint propagation. Alsopresented are extensions and modifications to the algorithm to handle strict lexicographic ordering, detection of entailment, andvectors of unequal length. Experimental results on a number of domains demonstrate the value of GACLexLeq.© 2006 Elsevier B.V. All rights reserved.Keywords: Artificial intelligence; Constraints; Constraint programming; Constraint propagation; Lexicographic ordering; Symmetry; Symmetrybreaking; Generalized arc consistency; Matrix models1. IntroductionConstraints are a natural means of knowledge representation. For instance: the maths class must be timetabledbetween 9 and 11am on Monday; the helicopter can carry up to four passengers; the sum of the variables must equal100. This generality underpins the success with which finite-domain constraint programming has been applied to awide variety of disciplines [27]. To apply finite-domain constraint programming to a given domain, a problem mustfirst be characterised or modelled by a set of constraints on a set of decision variables, which its solutions must satisfy.A common pattern arising in the modelling process is the use of matrices of decision variables, so-called matrixmodels [9]. For example, it is simple to represent many types of functions and relations in this way [15].* Corresponding author.E-mail addresses: frisch@cs.york.ac.uk (A.M. Frisch), brahim.hnich@ieu.edu.tr (B. Hnich), zkiziltan@deis.unibo.it (Z. Kiziltan),ianm@dcs.st-and.ac.uk (I. Miguel), tw@cse.unsw.edu.au (T. Walsh).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.03.002\f804A.M. Frisch et al. / Artificial Intelligence 170 (2006) 803–834Concomitant with the selection of a matrix model is the possibility that the rows and/or columns of the matrixare symmetric. Consider, for instance, a matrix model of a constraint problem that requires finding a relation R onA × B where A and B are n-element and m-element sets of interchangeable objects respectively. The matrix, M, hasn columns and m rows to represent the elements of A and B. Each decision variable Ma,b can be assigned either 1 or0 to indicate whether (cid:3)a, b(cid:4) ∈ A × B is in R. Symmetry has been introduced because the matrix, whose columns androws are indexed by A and B, distinguishes the position of the elements of the sets, whereas A and B did not. Givena (non-)solution to this problem instance, a (non-)solution can be obtained by permuting columns of assignmentsand/or permuting rows of assignments. This is known as row and column symmetry [8]. Since similar behaviour canbe found in multidimensional matrices of decision variables it is known more generally as index symmetry. As is welldocumented, symmetry can lead to a great deal of redundancy in systematic search [8].As reviewed in Section 2.5 of this paper, lexicographic ordering constraints have been shown to be an effectivemethod of breaking index symmetry. This paper describes a constraint propagation algorithm, GACLexLeq, that en-forces this constraint. Given a lexicographic ordering constraint c, the propagation algorithm removes values from thedomains of the constrained variables that cannot be part of any solution to c. This paper also shows that GACLexLeqestablishes a property called generalised arc consistency,—that is it removes all infeasible values—while only re-quiring a number of operations linear in the number of variables constrained. The GACLexLeq algorithm is alsoincremental; if the domain of a variable is reduced the algorithm can re-establish generalised arc consistency withoutworking from scratch.Although the examples and experiments in the paper employ the lexicographic ordering constraint to break indexsymmetry, we note that lexicographic ordering can be used to break any symmetry that operates on the variables ofan instance. The lex-leader method [5] breaks all symmetry by identifying a representative among the elements of theequivalence class of symmetries of an instance and adding a lexicographic ordering constraint for each other elementof the equivalence class to ensure that only the representative is allowed.The paper is organised as follows. Section 2 introduces the necessary background while Section 3 describes a num-ber of applications used to evaluate our approach. Section 4 presents a propagation algorithm for the lexicographicordering constraint. Then Section 5 discusses the complexity of the algorithm, and proves that the algorithm is soundand complete. Section 6 extends the algorithm to propagate a strict ordering constraint, to detect entailment, and tohandle vectors of different lengths. Alternative approaches to propagating the lexicographic ordering constraint arediscussed in Section 7. Section 8 demonstrates that decomposing a chain of lexicographic ordering constraints intolexicographic ordering constraints between adjacent or all pairs of vectors hinders constraint propagation. Computa-tional results are presented in Section 9. Finally, we conclude and outline some future directions in Section 10.2. BackgroundAn instance of the finite-domain constraint satisfaction problem (CSP) consists of:• a finite set of variables X ;• for each variable X ∈ X , a finite set D(X) of values (its domain); and• a finite set C of constraints on the variables, where each constraint c(X1, . . . , Xn) ∈ C is defined over the variablesX1, . . . , Xn by a subset of D(X1) × · · · × D(Xn) giving the set of allowed combinations of values. That is, c isan n-ary relation.A variable assignment maps every variable in a given instance of CSP to a member of its domain. A variableassignment A is said to satisfy a constraint c(X1, . . . , Xn) if and only if (cid:3)A(X1), . . . , A(Xn)(cid:4) is in the relation denotedby c. A solution to an instance of CSP is a variable assignment that satisfies all the constraints. An instance is said to besatisfiable if it has a solution; otherwise it is unsatisfiable. Typically, we are interested in finding one or all solutions,or an optimal solution given some objective function. In the presence of an objective function, a CSP instance is aninstance of the constraint optimisation problem.To impose total ordering constraints on variables and vectors of variables there must be an underlying total orderingon domains. If the domain of interest is not totally ordered, a total order can be imposed. And now, since domainsare always finite, every domain is isomorphic to a finite set of integers. So we shall simplify the presentation byconsidering all domains to be finite sets of integers.\fA.M. Frisch et al. / Artificial Intelligence 170 (2006) 803–834805The minimum element in the domain of variable X is min(X), and the maximum is max(X). Throughout, vars(c)is used to denote the set of variables constrained by constraint c.If a variable X has a singleton domain {v} we say that v is assigned to X, or simply that X is assigned. If two.= X(cid:6)). If v is.= X(cid:6), otherwise we write ¬(Xvariables X and X(cid:6) are assigned the same value, then we write Xassigned to X and v(cid:6) is assigned to X(cid:6) and v < v(cid:6) then we write X (cid:2) X(cid:6).A constraint c is entailed if all assignments of values to vars(c) satisfy c. If a constraint can be shown to be entailedthen running the (potentially expensive) propagation algorithm can be avoided. Similarly, a constraint c is disentailedwhen all assignments of values to vars(c) violate c. Observe that if a constraint in a CSP instance can be shown to bedisentailed then the instance has no solution.2.1. Generalised arc consistencyThis paper focuses on solving the CSP by searching for a solution in a space of assignments to subsets of thevariables. Solution methods of this type use propagation algorithms that make inferences based on the domains ofthe constrained variables and the assignments that satisfy the constraint. These inferences are typically recordedas reductions in variable domains, where the elements removed cannot form part of any assignment satisfying theconstraint, and therefore any solution. At each node in the search, constraint propagation algorithms are used toestablish a local consistency property. A common example is generalised arc consistency (see [19]).Definition 1 (Generalised arc consistency). A constraint c is generalised arc consistent (or GAC), written GAC(c), ifand only if for every X ∈ vars(c) and every v ∈ D(X)",
            {
                "entities": [
                    [
                        3169,
                        3197,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 246 (2017) 118–151Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintLocalising iceberg inconsistenciesGlauber De Bona∗, Anthony HunterDepartment of Computer Science, University College London, WC1E 6BT, UKa r t i c l e i n f oa b s t r a c tArticle history:Received 3 August 2016Received in revised form 16 February 2017Accepted 19 February 2017Available online 28 February 2017Keywords:Propositional logicInconsistency managementInconsistency analysisInconsistency localisationIn artificial intelligence, it is important to handle and analyse inconsistency in knowledge bases. Inconsistent pieces of information suggest questions like “where is the inconsis-tency?” and “how severe is it?”. Inconsistency measures have been proposed to tackle the latter issue, but the former seems underdeveloped and is the focus of this paper. Minimal inconsistent sets have been the main tool to localise inconsistency, but we argue that they are like the exposed part of an iceberg, failing to capture contradictions hidden under the water. Using classical propositional logic, we develop methods to characterise when a formula is contributing to the inconsistency in a knowledge base and when a set of formulas can be regarded as a primitive conflict. To achieve this, we employ an abstract consequence operation to “look beneath the water level”, generalising the minimal inconsistent set concept and the related free formula notion. We apply the framework presented to the problem of measuring inconsistency in knowledge bases, putting forward relaxed forms for two debatable postulates for inconsistency measures. Finally, we discuss the computational complexity issues related to the introduced concepts.© 2017 Elsevier B.V. All rights reserved.1. IntroductionThe occurrence of inconsistencies in data and knowledge is an important issue for the application of knowledge repre-sentation and reasoning technologies that are based on standard logics. To develop ways of dealing with an inconsistent set of formulas, it is important to understand the inconsistency, analysing its properties. Given an inconsistent knowledge base (a set of formulas), natural questions that arise are “where is the inconsistency?” and “how severe is it?”. To answer the second question in a qualitative way, inconsistent knowledge bases were classified by the severity of their inconsistency [17]. Recently, to numerically quantify the extent to which a knowledge base is inconsistent, many inconsistency measures have been proposed [29,24,25,19,28,27,20,42,43]. In contrast, the first question appears quite underdeveloped, and it is the subject of the present work.Inconsistency localisation can mean different things. One may want for instance to spot which part of the language is “contaminated” by the inconsistency, looking for the logical variables involved in contradictions (see e.g. [22,25]). Alterna-tively, one might assign numeric inconsistency values for formulas in a knowledge base, indicating the extent to which they are involved in the inconsistency, according to a given definition (e.g. [23,25]). In this paper, we focus on localising the inconsistency in a knowledge base, showing how it unfolds among the formulas.1 That is, given an inconsistent knowledge * Corresponding author.E-mail addresses: glauberbona@gmail.com (G. De Bona), anthony.hunter@ucl.ac.uk (A. Hunter).1 Note that logically closed theories are equal to the whole logical language when inconsistent, hence we focus on (possibly non-closed) knowledge bases.http://dx.doi.org/10.1016/j.artint.2017.02.0050004-3702/© 2017 Elsevier B.V. All rights reserved.\fG. De Bona, A. Hunter / Artificial Intelligence 246 (2017) 118–151119base, we are interested in discovering which subsets of formulas are contributing to the inconsistency, being its causes, and which formulas are not involved whatsoever.Fig. 1. Inconsistency as icebergs.1.1. MotivationWhen a knowledge base is inconsistent, it is not necessarily the case that its inconsistency is spread over all its formulas. For example, consider the set formed by the propositions: “Alice is a cat”, “Alice is not a cat” and “Bob is a dog”. Even though the whole set is inconsistent, intuition tends towards regarding the first two propositions as controversial and the third one as free of inconsistency somehow. To capture such intuition, minimal inconsistent sets (inconsistent sets whose all proper subsets are consistent) have been construed as the “purest form of inconsistency” [24,25]. Accordingly, a formula not contained in any minimal inconsistent set — a free formula — has been regarded as “uncontroversial”. As the first two propositions are already contradicting each other, the whole base is not a minimal inconsistent set. Furthermore, the third proposition contradicts neither the first nor the second proposition, hence “Bob is a dog” is indeed technically free, for not being in a minimal inconsistent set. Such a simple solution to the problem of localising the inconsistency probably is the reason for the lack of a systematic investigation of this issue. Nonetheless, the situation is more complex than might at first appear, since minimal inconsistent sets are alike the exposed part of the iceberg, ignoring all the inconsistency hidden under the water, as illustrated in Fig. 1.The recognition of these iceberg inconsistencies can find application in different areas where inconsistent pieces of information have to be dealt with. For instance, in software engineering, requirements extraction might reveal users’ expec-tations that cannot hold together, calling for a method for localising the conflicts. In data integration/fusion, as well as in belief merging, the proper identification of the sources of information, or the agents, that are conflicting each other allows one to narrow its attention to the focus of the problem, ignoring uncontroversial data/beliefs. In formal argumentation, in-consistency can be localised in order to show how a set of arguments is conflicting. Inconsistency localisation may also bring important clues in fraud investigation, for instance in the analysis of contradicting tax forms of a given taxpayer. In general, any decision making under inconsistent information might benefit from localising the inconsistency. For example, a physi-cian facing several different medical tests of a given patient with inconsistent results might need to choose which ones should be performed again. Example 1.1 brings a concrete situation where a decision can be influenced by inconsistency localisation.Example 1.1. The police is investigating a robbery on a jewellery shop that occurred on a weekday, during working hours. The investigators have taken testimony from all employees that were working on the day of the crime. The witnesses’ statements include the following:• salesperson: “I did not open the safe, and the criminals carried no guns!”• security chief: “Only the manager or the salesperson could have opened the safe, and the criminals carried guns.”• manager: “I did not open the safe.”As the police conceives the possibility of some of the employees having been complicit, they look for contradictions among the versions given. Inconsistent testimonies would imply some witnesses are lying, raising suspicions of complicity against them. The security chief and the salesperson are clearly contradicting each other, but is the manager involved in some contradiction? From the statements above, can one infer that it is possible that the manager is lying?To answer the questions raised in the example above, we need a tool to tell the “uncontroversial” from the “controversial” formulas in a knowledge base, since we are only interested in knowing whether the manager’s testimony is involved in the inconsistency, raising suspicion that he/she lied. This can be regarded as the relaxed form of the problem of localising inconsistency, whose solution is a partition of the inconsistent knowledge base into “controversial” and “uncontroversial” formulas. Free formulas are intended to encompass all and only “uncontroversial” formulas in a knowledge base, but we \f120G. De Bona, A. Hunter / Artificial Intelligence 246 (2017) 118–151shall argue that they are not suitable for all contexts. For instance, in the example above, the manager’s testimony is free (because it is not in any minimal inconsistent set), but it also seems to contradict the others in some way.A harder problem is identifying the atomic inconsistencies, or the primitive conflicts, in a knowledge base and can be illustrated by the following situation:Example 1.2. A university has hired a company to design a library management software to be used by all its members. In order to extract the design specifications, the company has collected requirements from the head of each department, which include:• Ecology: “The software should be open source, contributing to the whole academic community.”• Marketing: “It can’t be freely available, we need to keep our university edge in IT systems as a differential that attracts new students.”• Philosophy: “Both graduate and undergraduate students shall have the same rights in the system and it must be re-motely accessible.”• Economy: “Due to their different demands, graduate students need some privileges. If the system is to be remotely accessible, its software should not be open source, otherwise it could be vulnerable.”• Theology: “Department heads shall have no exclusive privileges.”• Arts: “I have no specific requirements.”The project manager, while reading such requirements, notes two contradictions: one between the heads of the Ecology and Marketing departments, on whether the software should be open source, and another between the heads of Philosophy and Economy departments, about the graduate and undergraduate students rights. The manager plans to arrange meetings with the department heads to discuss — and maybe relax — ",
            {
                "entities": [
                    [
                        3605,
                        3633,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 168 (2005) 119–161www.elsevier.com/locate/artintWeak nonmonotonic probabilistic logics ✩Thomas Lukasiewicz 1Dipartimento di Informatica e Sistemistica, Università di Roma “La Sapienza”,Via Salaria 113, 00198 Rome, ItalyReceived 1 October 2004; accepted 31 May 2005Available online 5 July 2005AbstractWe present an approach where probabilistic logic is combined with default reasoning from condi-tional knowledge bases in Kraus et al.’s System P , Pearl’s System Z, and Lehmann’s lexicographicentailment. The resulting probabilistic generalizations of default reasoning from conditional knowl-edge bases allow for handling in a uniform framework strict logical knowledge, default logicalknowledge, as well as purely probabilistic knowledge. Interestingly, probabilistic entailment in Sys-tem P coincides with probabilistic entailment under g-coherence from imprecise probability assess-ments. We then analyze the semantic and nonmonotonic properties of the new formalisms. It turnsout that they all are proper generalizations of their classical counterparts and have similar propertiesas them. In particular, they all satisfy the rationality postulates of System P and some Conditioningproperty. Moreover, probabilistic entailment in System Z and probabilistic lexicographic entailmentboth satisfy the property of Rational Monotonicity and some Irrelevance property, while probabilis-tic entailment in System P does not. We also analyze the relationships between the new formalisms.Here, probabilistic entailment in System P is weaker than probabilistic entailment in System Z,which in turn is weaker than probabilistic lexicographic entailment. Moreover, they all are weakerthan entailment in probabilistic logic where default sentences are interpreted as strict sentences.Under natural conditions, probabilistic entailment in System Z and lexicographic entailment evencoincide with such entailment in probabilistic logic, while probabilistic entailment in System P does✩ This paper is a significantly extended and revised version of a paper in: Proceedings of the 9th InternationalConference on Principles of Knowledge Representation and Reasoning (KR2004), Whistler, Canada, June 2004,AAAI Press, 2004, pp. 141–151.E-mail address: lukasiewicz@dis.uniroma1.it (T. Lukasiewicz).1 Alternate address: Institut für Informationssysteme, Technische Universität Wien, Favoritenstraße 9-11, 1040Vienna, Austria; e-mail: lukasiewicz@kr.tuwien.ac.at.0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.05.005\f120T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161not. Finally, we also present algorithms for reasoning under probabilistic entailment in System Z andprobabilistic lexicographic entailment, and we give a precise picture of its complexity. 2005 Elsevier B.V. All rights reserved.Keywords: Probabilistic logic; Default reasoning from conditional knowledge bases; Entailment in System P ;Entailment in System Z; Lexicographic entailment; Nonmonotonic probabilistic logics; Inconsistencyhandling; Algorithms; Computational complexity1. IntroductionDuring the recent decades, reasoning about probabilities has started to play an importantrole in AI. In particular, reasoning about interval restrictions for conditional probabilities,also called conditional constraints [49], has been a subject of extensive research efforts.Roughly, a conditional constraint is of the form (ψ|φ)[l, u], where ψ and φ are events, and[l, u] is a subinterval of the unit interval [0, 1]. It encodes that the conditional probabilityof ψ given φ lies in [l, u].An important approach for handling conditional constraints is probabilistic logic, whichhas its origin in philosophy and logic, and whose roots can be traced back to alreadyBoole in 1854 [12]. There is a wide spectrum of formal languages that have been exploredin probabilistic logic, ranging from constraints for unconditional and conditional eventsto rich languages that specify linear inequalities over events (see especially the work byNilsson [54,55], Fagin et al. [19], Dubois and Prade et al. [2,13,16,17], Frisch and Had-dawy [21], and the author [48,49,51]; see also the survey on sentential probability logic byHailperin [35]). The main decision and optimization problems in probabilistic logic are de-ciding satisfiability, deciding logical consequence, and computing tight logically entailedintervals. Recently, column generation techniques from operations research have been suc-cessfully used to solve large problem instances in probabilistic logic (see especially thework by Jaumard et al. [37] and Hansen et al. [36]).Example 1.1 (Eagles). A simple collection of conditional constraints KB may encode thestrict logical knowledge “all eagles are birds” and “all birds have feathers” as well asthe purely probabilistic knowledge “birds fly with a probability of at least 0.95” (cf. Ex-ample 2.1). This collection of conditional constraints KB is satisfiable, and some logicalconsequences in probabilistic logic from KB are “all birds have feathers”, “birds fly with aprobability of at least 0.95”, “all eagles have feathers”, and “eagles fly with a probabilitybetween 0 and 1”; in fact, these are the tightest intervals that follow from KB (cf. Exam-ple 2.2). That is, we especially cannot conclude anything from KB about the ability to flyof eagles.A closely related research area is default reasoning from conditional knowledge bases,which consist of a collection of strict statements in classical logic and a collection of defea-sible rules, also called defaults. The former must always hold, while the latter are rules ofthe kind ψ ← φ, which read as “generally, if φ then ψ”. Such rules may have exceptions,which can be handled in different ways.\fT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161121The literature contains several different proposals for default reasoning from conditionalknowledge bases and extensive work on its desired properties. The core of these proper-ties are the rationality postulates of System P by Kraus, Lehmann, and Magidor [40],which constitute a sound and complete axiom system for several classical model-theoreticentailment relations under uncertainty measures on worlds. They characterize classicalmodel-theoretic entailment under preferential structures [40,64], infinitesimal probabili-ties [1,57], possibility measures [14], and world rankings [33,65]. As shown by Friedmanand Halpern [20], many of these uncertainty measures on worlds are expressible as plausi-bility measures. The postulates of System P also characterize an entailment relation basedon conditional objects [15]. A survey of the above relationships is given in [6,22].Mainly to solve problems with irrelevant information, the notion of rational closure asa more adventurous notion of entailment was introduced by Lehmann [45,47]. It is equiva-lent to entailment in System Z by Pearl [58], to the least specific possibility entailment byBenferhat et al. [5], and to a conditional (modal) logic-based entailment by Lamarre [44].Finally, mainly to solve problems with property inheritance from classes to exceptionalsubclasses, the maximum entropy approach to default entailment was proposed by Gold-szmidt et al. [31]; lexicographic entailment was introduced by Lehmann [46] and Benferhatet al. [4]; conditional entailment was proposed by Geffner [24,26]; and an infinitesimal be-lief function approach was suggested by Benferhat et al. [7]. The following example due toGoldszmidt and Pearl [34] illustrates default reasoning from conditional knowledge bases.Example 1.2 (Penguins). A conditional knowledge base KB may encode the strict logicalknowledge “all penguins are birds” and the default logical knowledge “generally, birds fly”,“generally, penguins do not fly”, and “generally, birds have wings”. Some desirable con-clusions from KB [34] are “generally, birds fly” and “generally, birds have wings” (whichboth belong to KB), “generally, penguins have wings” (since the set of all penguins is asubclass of the set of all birds, and thus penguins should inherit all properties of birds),“generally, penguins do not fly” (since properties of more specific classes should overrideinherited properties of less specific classes), and “generally, red birds fly” (since “red” isnot mentioned at all in KB and thus should be considered irrelevant to the ability to fly ofbirds).There are several works in the literature on probabilistic foundations for default reason-ing from conditional knowledge bases [1,11,31,57], on combinations of Reiter’s defaultlogic [63] with statistical inference [43,67], and on a rich first-order formalism for deriv-ing degrees of belief from statistical knowledge including default statements [3]. However,there has been no work so far that extends probabilistic logic by the capability of handlingdefaults as in conditional knowledge bases.In this paper, we try to fill this gap. We present extensions of probabilistic logic bydefaults as in conditional knowledge bases under Kraus et al.’s System P [40], Pearl’sSystem Z [58], and Lehmann’s lexicographic entailment [46]. The new formalisms allowfor expressing in a uniform framework strict logical knowledge and purely probabilisticknowledge from probabilistic logic, as well as default logical knowledge from default rea-soning from conditional knowledge bases. Informally, strict logical knowledge representssentences that must always hold, while purely probabilistic (resp., default logical) knowl-\f122T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161edge encodes sentences that may have exceptions, which is expressed in a quantitative(resp., qualitative) way.Example 1.3 (Ostriches). Consider the strict logical knowledge “all ostriches are birds”,the default logical knowledge “generally, birds have legs” and “generally, birds fly”, andthe purely probabilistic knowledge “ostriches fly with a probability of at most 0.05”. Ob-viously, some desired",
            {
                "entities": [
                    [
                        2541,
                        2569,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 1154–1193Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintFrom the textual description of an accident to its causesDaniel Kayser∗, Farid Nouioua 1Laboratoire d’Informatique de Paris-Nord, UMR 7030 du C.N.R.S. – Institut Galilée, Université Paris 13, 99 avenue Jean-Baptiste Clément, F 93430 – Villetaneuse, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 29 September 2008Received in revised form 7 April 2009Accepted 23 April 2009Available online 6 May 2009Keywords:Natural language understandingCausal reasoningNormsInference-based semanticsSemi-normal defaultsEvery human being, reading a short report concerning a road accident, gets an idea ofits causes. The work reported here attempts to enable a computer to do the same, i.e. todetermine the causes of an event from a textual description of it. It relies heavily on thenotion of norm for two reasons:• The notion of cause has often been debated but remains poorly understood: wepostulate that what people tend to take as the cause of an abnormal event, like anaccident, is the fact that a specific norm has been violated.• Natural Language Processing has given a prominent place to deduction, and for whatconcerns Semantics, to truth-based inference. However, norm-based inference is amuch more powerful technique to get the conclusions that human readers derive froma text.The paper describes a complete chain of treatments, from the text to the determinationof the cause. The focus is set on what is called “linguistic” and “semantico-pragmatic”reasoning. The former extracts so-called “semantic literals” from the result of the parse,and the latter reduces the description of the accident to a small number of “kernel literals”which are sufficient to determine its cause. Both of them use a non-monotonic reasoningsystem, viz. LPARSE and SMODELS.Several issues concerning the representation of modalities and time are discussed andillustrated by examples taken from a corpus of reports obtained from an insurancecompany.© 2009 Elsevier B.V. All rights reserved.1. Motivation1.1. Basic postulatesThe work described here is grounded on two postulates:• what is perceived as the cause of an event is:– the norm itself, if the event is perceived as normal,– and the violation of some norm, if the event is considered abnormal;• the semantics of natural language (NL) is not based on the notion of truth, but on norms.* Corresponding author.E-mail addresses: Daniel.Kayser@lipn.univ-paris13.fr (D. Kayser), Farid.Nouioua@lipn.univ-paris13.fr, Farid.nouioua@univ-cezanne.fr (F. Nouioua).1 Now at Laboratoire des Sciences de l’Information et des Systèmes, UMR 6168 du C.N.R.S. Université Paul Cézanne (Aix-Marseille 3), Avenue EscadrilleNormandie–Niemen 13397 Marseille Cedex 20, France.0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.04.002\fD. Kayser, F. Nouioua / Artificial Intelligence 173 (2009) 1154–11931155The notion of norm, which plays a central role in this paper, has both a descriptive and a prescriptive meaning.2 In thedescriptive sense, norms are just what explains the difference between what is perceived as normal or not. In the prescriptivesense, norms build a corpus on the basis of which an agent is considered, legally or otherwise, entitled or not to performan action.In Artificial Intelligence, these two meanings have given rise to two rather separate fields of study. On the one hand,non-monotonic logics have been developed in order to derive conclusions that are considered normal, in the absence ofany specific circumstance invalidating this derivation. On the other hand, deontic logics have the purpose of formalizing thereasoning of agents respecting normative prescriptions [8,17,48].In this paper, norms will generally be taken in their descriptive sense but clearly, as it is normal to follow the rules, thisacceptation of norms includes as a special case the prescriptive sense, and we will also have to deal with duties, which arenormative.Our first postulate concerns a very old and very controversial issue: when is it sensible to say that A causes B? Deter-mining the essence of the notion of cause is not in the agenda of Artificial Intelligence. However, commonsense reasoningmakes an intensive use of causation, e.g. for diagnosing, planning, predicting; and therefore AI cannot (and does not, seee.g. [33,55]) completely ignore the debate concerning this notion. What AI needs, however, does not concern the meta-physics of cause, but only how people reason causally. And, even if observation reveals that we use the word cause to meanrather different things, i.e. that this word is polysemic, in a vast majority of cases we take as causal for an abnormal eventthe fact that some agent has violated a norm. We report in the paper a psychological experiment showing which violation(s)are selectively chosen as cause(s) of the event.Consider now our second postulate: dealing with a sentence such as:The car before me braked suddenly.A truth-based approach (see e.g. [13,27,35]) will derive all the conclusions C that logically follow from the existence of atime t and a car c, such that the two following propositions are true at t: (i) c brakes suddenly and (ii) c is located in frontof the writer. Clearly, several other propositions, none of them being valid in the logical sense, come to the mind of a humanreader, e.g. (iii) at t, the writer was driving, (iv) s/he was driving in the same direction as c, (v) no vehicle was betweenc and him/her, (vi) s/he had to act quickly, in order to prevent an accident, and so on. Subsequent information may forcethe reader to retract some of these conclusions. Nonetheless, as they are likely to be present in the mind of every personwho shares our culture, there is no necessity to consider separately the propositions C derived by means of truth-preservingoperations (the only ones that are said to pertain to semantics, according to the prevailing theories), from the propositionsderived by means of norms (generally said to be the result of pragmatics).Knowing the norms of a domain is absolutely necessary to understand the texts of that domain. But there exists noexhaustive list of the norms ruling any given domain: the rules and regulations are only the visible part of the iceberg ofall we know, and keep implicit, about the domain. An indirect consequence of our study is to point out that examining howpeople ascribe causation to the events happening in a domain is a powerful means to reveal its implicit norms.1.2. Specification of the goalIn order to validate our postulates, we need to focus on a domain where the number of norms remains reasonable, whereabnormal events are frequent, where these events are reported in natural language, and where it is easy to ask people whatthey take as being the cause of the events reported.We selected the domain of road accidents, for the following reasons:• A large number of short texts exists, describing such accidents: every insurance company receives daily a number offorms filled by drivers, describing what happened to them.• Most people of our culture know enough about car crashes to give sensible answers, after having read a text, whenthey are asked what caused the accident.• Each report describes some facts, but clearly implies also a number of other facts, which are not logically entailed. Wecan therefore see whether our postulates work, i.e. check whether a reasoning based on norms captures the kind ofreasoning used by the reader.• An accident by itself is an anomaly, and the text generally goes back to another anomaly that allegedly explains why ithappened. We can thus test which anomaly, if any, is taken to be the cause of the accident, and confirm or infirm ourfirst postulate.• The number of norms involved is neither too large nor too small. They are clearly not limited to those listed in theHighway Code.• The corpus on which we perform our study has been studied from different points of views. (See for example [60,19,34].The last work presents a system that produces automatically a 3D scene of an accident from its textual description.3)2 Von Wright [62, Chap. 1] discusses in much greater details the various meanings of the word norm.3 Another study [63] concerns British records of road accidents: the authors use a description logic in order to check the consistency between a naturallanguage report and coded data which are both components of the record.\f1156D. Kayser, F. Nouioua / Artificial Intelligence 173 (2009) 1154–1193However, a drawback of this corpus is worth a mention. Most of the reports sent by a driver to an insurance company are,for obvious reasons, biased in order to lessen the author’s responsibility; the rhetorical aspect of a plea interferes with themore basic descriptive part of the report, be it truthful or not. The fact that the texts do not necessarily reflect what reallyhappened is not per se a problem: the norms of a domain are not better revealed from “true” descriptions, whatever thismay mean, than from biased ones. The unwanted consequence of the choice of this corpus is that it requires some effortto identify (and most of the time, to ignore) what has been added for pure argumentative reasons. A study is currentlyin progress that examines specifically the argumentative strategies used by the authors to highlight or to minimize someaspects of the accident, in order to convey the best possible impression of their behavior [9]. As we will see, this dimensionof the reports does not affect much the results of the present work.We obtained, by courtesy of MAIF, an insurance company, a number of reports written in French. Some of them areunclear, and only the accompanying drawings make them understandable. We discarded them and kept only those whichare self-contained, i.e. those on the basis of which we understood enough of what happened to build a hypothesis that",
            {
                "entities": [
                    [
                        2915,
                        2943,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 316–361Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAutomated composition of Web services via planning in asynchronousdomainsPiergiorgio Bertoli∗, Marco Pistore, Paolo TraversoFondazione Bruno Kessler, via Sommarive 18, 38100 Povo (Tn), Italya r t i c l ei n f oa b s t r a c tArticle history:Received 9 April 2008Received in revised form 13 November 2009Accepted 26 November 2009Available online 16 December 2009Keywords:PlanningWeb servicesAutomated program synthesisThe service-oriented paradigm promises a novel degree ofinteroperability betweenbusiness processes, and is leading to a major shift in way distributed applications aredesigned and realized. While novel and more powerful services can be obtained, in suchsetting, by suitably orchestrating existing ones, manually developing such orchestrationsis highly demanding,time-consuming and error-prone. Providing automated servicecomposition tools is therefore essential to reduce the time to market of services, andultimately to successfully enact the service-oriented approach.In this paper, we show that such tools can be realized based on the adoption and extensionof powerful AI planning techniques, taking the “planning via model-checking” approachas a stepping stone. In this respect, this paper summarizes and substantially extends aresearch line that started early in this decade and has continued till now. Specifically, thiswork provides three key contributions.First, we describe a novel planning framework for the automated composition of Webservices, which can handle services specified and implemented using industrial standardlike ws-bpel. Since theselanguages for business processes modeling and execution,languages describe stateful Web services that rely on asynchronous communicationprimitives, a distinctive aspect of the presented framework is its ability to model andsolve planning problems for asynchronous domains.Second, we formally spell out the theory underlying the framework, and provide algorithmsto solve service composition in such framework, proving their correctness andcompleteness. The presented algorithms significantly extend state-of-the-art techniquesfor planning under uncertainty, by allowing the combination of asynchronous domainsaccording to behavioral requirements.Third, we provide and discuss an implementation of the approach, and report extensiveexperimental results which demonstrate its ability to scale up to significant cases forwhich the manual development of ws-bpel composed services is far from trivial and timeconsuming.© 2009 Elsevier B.V. All rights reserved.1. IntroductionSince its inception, the Web has maintained a fast growth rate in terms of quantity and variety of contained information,becoming a reference information source for billions of users and business entities world-wide. In particular, in the last fewyears, the economical impact of the Web has grown substantially, due to the fact that the Web is not used anymore just topresent static information, but, more and more, to expose services with which a Web user (or a different Web-exposed ser-* Corresponding author.E-mail addresses: bertoli@fbk.eu (P. Bertoli), pistore@fbk.eu (M. Pistore), traverso@fbk.eu (P. Traverso).0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.12.002\fP. Bertoli et al. / Artificial Intelligence 174 (2010) 316–361317vice) can actively interact. This has enacted a range of Web-based solutions for commercial, learning and health-care activ-ities, and gave a substantial push to e-Commerce, e-Learning, and e-Health initiatives (see e.g. [99,51,35,55,92,80,52,33,68]).This has been the starting point for the emergence of a Service Oriented Computing paradigm, which envisages theadoption of standards for the publication and access of services over the Web, so to allow the interoperability of inde-pendently developed procedures over the Web. In such a setting, existing services can be suitably combined by means ofWeb-based “orchestration” services, so to realize novel and more complex procedures that satisfy some given user or busi-ness requirement. For instance, different services taking care of specific aspects related to the organization of a trip (e.g.flight booking, lodging, bank payment, and so on) can be suitably coordinated by an integrated “trip adviser” service, whoseadoption may save a customer considerable time and effort in setting up a trip. Indeed, being able to build new services bycomposing existing ones is crucial to the actual enactment of the service-oriented paradigm. However, the task of manuallydeveloping such orchestrations is extremely difficult, time-consuming and error-prone, even for experienced designers andprogrammers. This calls for the design of effective support techniques and automated tools capable of synthesizing serviceorchestrations starting from suitable high-level composition requirements.In this context, planning has proved to be one of the most promising techniques for the automated composition of Webservices. Several works in planning have addressed different aspects of this problem, see, e.g., [100,59,34,65,83,17,73,8,84,4,43]. In these works, automated composition is described as a planning problem: services that are available and published onthe Web, the component services, are used to construct the planning domain, composition requirements can be formalized asplanning goals, and planning algorithms can be used to generate composed services, i.e., plans that compose the componentservices. These works, which provide different technical solutions, share the conception of services as stateless entities,which enact simple query–response protocols.An even more difficult challenge for planning is the automated composition of Web services at the process level, i.e., thecomposition of component services that consist of stateful business processes, capable to establish complex multi-phaseinteractions with their partners. Indeed, in the large majority of real cases, services cannot be considered simply as atomiccomponents, which, given some inputs, return some outputs, in a single request–response step. On the contrary, in mostapplication domains, they need to be represented as stateful processes that realize interaction protocols which may involvedifferent sequential, conditional, and iterative steps. For instance, we cannot in general interact with a “flight booking” ser-vice in an atomic step. The service may require a sequence of different operations including an authentication, a submissionof a specific request for a flight, the possibility to submit iteratively different requests, acceptance (or refusal) of the offer,and finally, a payment procedure. In these cases, the process, i.e. the published interaction flow, is the key aspect to beconsidered when (automatically) composing services.The planning problem corresponding to the automated composition of services that are published as processes is farfrom trivial. First, component services cannot be simply represented as atomic actions of the planning domain. As a con-sequence, it is not obvious, like in the case of atomic component services, which is the planning domain that correspondsto the composition problem. Second, in realistic cases, component services publish nondeterministic and partially observ-able behaviors, since, in general, the outputs of a service cannot be predicted a priori and its internal status is not fullyavailable to external services. For instance, whether a payment transaction will succeed cannot be known a priori of itsexecution, and whether there are still seats available on a flight cannot be known until a specific request is submitted tothe service. Third, the plan that coordinates the component services cannot be simply a sequence of actions that call atomiccomponents; rather, it needs to interleave the (partial execution of) component services with typical programming languageconstructs such as conditionals and loops. Finally, Web service interactions are typically asynchronous: each process evolvesindependently and with unpredictable speed, and interacts with the other processes only through asynchronous messageexchanges. Message queues are used in practical implementations to guarantee that processes do not lose messages thatthey are not ready to receive.As a consequence of all these characteristics of Web services, it is far from obvious how their automated compositioncan be adequately represented as a planning problem. Moreover, their nondeterministic, partially observable, and asyn-chronous behavior poses strong requirements and introduce novel problems for the planning techniques that can be used.This has led the authors of this paper and their colleagues to investigate a research line on service oriented composition,that started with [71,95], and continued with [78,79,74,61,62], up to date [60,63,72,64,21]. While distinct for the technicalsolutions and degree of maturity, these works share two general ideas. The first consists in taking, as an algorithmic andtechnological baseline, the “planning via model checking” approach devised by the authors of this paper together with othercolleagues [16,77,32,15]. Such approach combines state-of-the-art performance with the ability to deal with general formsof nondeterminism, therefore tackling effectively one of the critical aspects implied by the nature of Web services. Thesecond idea is to face actual composition problems by pragmatically considering services expressed using de-facto standardlanguages such as ws-bpel [1,31]. While this choice renders the problem further complex, it is strongly motivated by theobjective to provide usable tools.This paper summarizes and significantly extends a large portion of the corpus of work presented in [71,95,78] and [79,74,61,62,60,63,72,64], providing for the first time both a comprehensive survey of the framework und",
            {
                "entities": [
                    [
                        3374,
                        3402,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 262 (2018) 15–51Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintClassical logic, argument and dialecticM. D’Agostino a, S. Modgil b,∗a Department of Philosophy, University of Milan, Italyb Department of Informatics, King’s College London, United Kingdoma r t i c l e i n f oa b s t r a c tArticle history:Received 10 July 2017Received in revised form 10 May 2018Accepted 24 May 2018Available online 20 June 2018Keywords:Classical logicArgumentationDialecticRationality postulatesNatural deductionPreferred subtheoriesBounded reasoningA well studied instantiation of Dung’s abstract theory of argumentation yields argumenta-tion-based characterisations of non-monotonic inference over possibly inconsistent sets of classical formulae. This provides for single-agent reasoning in terms of argument and counter-argument, and distributed non-monotonic reasoning in the form of dialogues between computational and/or human agents. However, features of existing formalisations of classical logic argumentation (Cl-Arg) that ensure satisfaction of rationality postulates, preclude applications of Cl-Arg that account for real-world dialectical uses of arguments by resource-bounded agents. This paper formalises dialectical classical logic argumentation that both satisfies these practical desiderata and is provably rational. In contrast to standard approaches to Cl-Arg we: 1) draw an epistemic distinction between an argument’s premises accepted as true, and those assumed true for the sake of argument, so formalising the dialectical move whereby arguments’ premises are shown to be inconsistent, and avoiding the foreign commitment problem that arises in dialogical applications; 2) provide an account of Cl-Arg suitable for real-world use by eschewing the need to check that an argument’s premises are subset minimal and consistent, and identifying a minimal set of assumptions as to the arguments that must be constructed from a set of formulae in order to ensure that the outcome of evaluation is rational. We then illustrate our approach with a natural deduction proof theory for propositional classical logic that allows measurement of the ‘depth’ of an argument, such that the construction of depth-bounded arguments is a tractable problem, and each increase in depth naturally equates with an increase in the inferential capabilities of real-world agents. We also provide a resource-bounded argumentative characterisation of non-monotonic inference as defined by Brewka’s Preferred Subtheories.© 2018 Elsevier B.V. All rights reserved.1. IntroductionArgumentation is a form of reasoning that makes explicit the reasons for the conclusions that are drawn and how conflicts between reasons are resolved. While informal studies of argumentation have a rich tradition, recent years have witnessed intensive study of logic-based models of argumentation and their use in formalising agent reasoning, decision making, and inter-agent dialogue [11,53]. Much of this work builds on Dung’s seminal theory of abstract argumentation [26], and the theory’s provision of argumentative characterisations of nonmonotonic inference. Given a possibly inconsistent set of logical formulae (base) one defines the arguments and a binary attack relation denoting that one argument is a * Corresponding author.E-mail address: sanjay.modgil@kcl.ac.uk (S. Modgil).https://doi.org/10.1016/j.artint.2018.05.0030004-3702/© 2018 Elsevier B.V. All rights reserved.\f16M. D’Agostino, S. Modgil / Artificial Intelligence 262 (2018) 15–51counter-argument to another. Various developments of Dung’s theory additionally accommodate a preference relation over arguments, which is used to determine which attacks succeed as defeats [1,9,43]. The resulting directed graph of arguments related by defeats, referred to as an argumentation framework (AF), is then said to be ‘instantiated’ by the base. Evaluation of the justified arguments is then based on the intuitive principle that an argument is justified if all its defeaters are themselves defeated by justified arguments. The conclusions of justified arguments identify the ‘argumentation defined’ non-monotonic inferences from the instantiating base.The widespread impact of Dung’s theory is in large part due to this characterisation of non-monotonic inference in terms of the dialectical use of arguments and counter-arguments familiar in everyday reasoning and debate. The theory thus provides foundations for reasoning by individual computational and human agents, and distributed non-monotonic reasoning involving agents resolving conflicts amongst beliefs or deciding amongst alternative actions, or negotiating alloca-tions of resources (e.g., [2,3,8,38,40,44,45,48,58]). These ‘monological’ and ‘dialogical’ applications have motivated the study of rationality postulates for logical instantiations of A F s [15,16], as well as desiderata for practical applications [27,44].This paper focuses on classical logic instantiations of A F s (Cl-Arg) [1,33,43]. Features of the current paradigm have been shown to provide sufficient conditions for satisfaction of the rationality postulates. However, these features preclude satisfaction of practical desiderata that account for modelling real-world uses of arguments by resource-bounded agents. This paper therefore aims at an account of Cl-Arg that satisfies both practical desiderata and the rationality postulates.In Section 2 we review Dung’s theory, Cl-Arg, and the rationality postulates. In Section 3 we argue that monological and dialogical applications of Dung’s theory require formalisation of real-world uses of argument suitable for resource-bounded agents. However, current approaches to Cl-Arg tacitly assume that all arguments defined by a base can be constructed and included in an A F , and that prior to inclusion the legitimacy of each constructed argument is verified by checking that its premises are consistent and not redundant in the strong sense that their conclusion is not entailed by any proper subset of the premises. These assumptions are computationally unfeasible (even in the propositional case) for real-world uses of argument by resource-bounded agents. However, they are proposed as sufficient conditions for satisfaction of the consistency and closure postulates [15] for first order Cl-Arg with preferences [43],1 and of the ‘non-contamination’ postulates [16] for propositional Cl-Arg without preferences. Moreover, checking the legitimacy of arguments prior to inclusion in an A F fails to account for real-world uses of argument. Firstly, in real-world uses the inconsistency of arguments’ premises is typically demonstrated dialectically. Secondly, agents do not interrogate premises for subset minimality. Rather, it is the specific proof-theoretic means for constructing arguments that determines whether or not premises are redundantly used in deriving the conclusion; that is, redundant in the obvious sense that they are syntactically disjoint from the remaining premises and the conclusion.Section 3 then presents a new account of first order Cl-Arg that satisfies practical desiderata. Our approach introduces a new notion of argument that distinguishes amongst the premises accepted as true and those supposed true ‘for the sake of argument’. We can therefore model a ubiquitous feature of dialectical practice, whereby the inconsistency of a set of premises (cid:2) is shown dialectically, by defeats from arguments that claim that a contradiction is implied if one supposes (for the sake of argument) the truth of (cid:2). The distinction also solves the so called foreign commitment problem that arises in dialogical applications when agents are forced to commit to the premises of their interlocutors in order to challenge their arguments [17]. We also drop the computationally demanding checks on the legitimacy of arguments, and define ‘partially instantiated’ A F s that include subsets of the arguments defined by a base. We thus accommodate real-world uses of argument in which agents do not (or may not have sufficient resources to) construct all arguments from a base when determining whether arguments are justified. We show that our account satisfies standard properties of Dung’s theory. We also show that despite dropping the legitimacy checks on arguments and making minimal assumptions as to the arguments defined by a base for inclusion in an A F , the consistency and closure postulates are satisfied (where the latter are adapted to account for the fact that not all defined arguments may be included in the A F ). Moreover, in contrast with [43], these postulates are satisfied assuming any preference relation over arguments.Finally, in Section 3 we identify the notion of an argument whose use of obviously redundant (in the sense described above) premises can be excluded proof-theoretically, in contrast with the use of impractical subset-minimality checks We generalise the ‘non-contamination’ postulates defined for propositional instantiations of A F s in [16], to first order instanti-ations. We then show that despite dropping consistency and subset minimality checks on arguments’ premises, our account of first order Cl-Arg satisfies these postulates under the assumption that preference relations are ‘coherent’.Standard accounts of Cl-Arg typically leave implicit the specific proof theoretic means by which one entails a conclusion from a set of premises. In Section 4 we illustrate use of our dialectical account of argumentation by formalising arguments as intelim trees: a new natural deduction formalism for propositional classical logic [20,21] that allows measurement of the ‘depth’ of an argument such that the construction of depth-bounded arguments is a tractable problem, and each increase in depth naturally equates with an increase in the inferential capabilities of real-world agents. We then show that A F s instantiated by arguments up to any given depth",
            {
                "entities": [
                    [
                        3430,
                        3458,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 776–804www.elsevier.com/locate/artintAn executable specification of a formal argumentation protocolAlexander Artikis a,∗, Marek Sergot b, Jeremy Pitt ca Institute of Informatics & Telecommunications, NCSR “Demokritos”, Athens, 15310, Greeceb Department of Computing, Imperial College London, SW7 2AZ, UKc Department of Electrical & Electronic Engineering, Imperial College London, SW7 2BT, UKReceived 8 November 2006; received in revised form 3 April 2007; accepted 16 April 2007Available online 29 April 2007AbstractWe present a specification, in the action language C+, of Brewka’s reconstruction of a theory of formal disputation originallyproposed by Rescher. The focus is on the procedural aspects rather than the adequacy of this particular protocol for the conduct ofdebate and the resolution of disputes. The specification is structured in three separate levels, covering (i) the physical capabilitiesof the participant agents, (ii) the rules defining the protocol itself, specifying which actions are ‘proper’ and ‘timely’ according tothe protocol and their effects on the protocol state, and (iii) the permissions, prohibitions, and obligations of the agents, and thesanctions and enforcement strategies that deal with non-compliance. Also included is a mechanism by which an agent may objectto an action by another participant, and an optional ‘silence implies consent’ principle. Although comparatively simple, Brewka’sprotocol is thus representative of a wide range of other more complex argumentation and dispute resolution procedures that havebeen proposed. Finally, we show how the ‘Causal Calculator’ implementation of C+ can be used to animate the specification andto investigate and verify properties of the protocol.© 2007 Elsevier B.V. All rights reserved.Keywords: Argumentation; Disputation; Protocol; Norm; Multi-agent system; Specification; Action language1. IntroductionOne of the main tasks in the formal specification and analysis of (open) multi-agent systems (MAS) is the represen-tation of the protocols and procedures for agent interactions, and the norms of behaviour that govern these interactions.Examples include protocols for exchanging information, for negotiation, and for resolving disputes.It has been argued that a specification of systems of this type should satisfy at least the following two requirements:first, the interactions of the members should be governed by a formal, declarative, verifiable and meaningful seman-tics [64]; and second, to cater for the possibility that agent behaviour may deviate from what is prescribed, agentinteractions can usefully be described in terms of permissions and obligations [26].We have been developing a theoretical framework for the executable specification of open agent systems thataddresses the aforementioned requirements [3–5]. We adopt the perspective of an external observer, thus taking intoaccount only externally observable behaviours and not the internal architectures of the individual agents, and view* Corresponding author.E-mail addresses: a.artikis@acm.org (A. Artikis), mjs@doc.ic.ac.uk (M. Sergot), j.pitt@imperial.ac.uk (J. Pitt).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.008\fA. Artikis et al. / Artificial Intelligence 171 (2007) 776–804777agent systems as instances of normative systems [26] whereby constraints on agents’ behaviour (or social constraints)are specified in terms of their permissions, their institutional power to effect changes and bring about certain states ofaffairs, and their rights and obligations to one another. We employ an action formalism to specify the social constraintsgoverning the behaviour of the members and then use a computational framework to animate the specification andinvestigate its properties. For the action formalism, we have employed the Event Calculus [29], the action languageC+ [22], and an extended form of C+ specifically designed for modelling the institutional aspects of agent systems[57–59].In this paper we demonstrate how the theoretical and computational frameworks can be used with the languageC+ to specify and execute an argumentation protocol based on Brewka’s reconstruction [8], in the Situation Calculus[52], of a theory of formal disputation originally proposed by Rescher [53]. We presented a preliminary formulationin an earlier paper [4]. This present paper is a refined and much extended version.We are focusing here on the procedural aspects of the protocol rather than on the underlying logic of disputationemployed by Brewka or on the adequacy of this particular protocol for the conduct of debate and the resolution ofdisputes. The features of Brewka’s protocol are representative of a wide range of other more complex argumentationand dispute resolution procedures that have been proposed in the literature, and to which the methods of this papercan be similarly applied.The specification of the argumentation protocol is structured into three separate levels, covering:(i) the physical capabilities of the participant agents (in the present context, the messages/utterances each agent isactually capable of transmitting);(ii) the rules defining the protocol itself, specifying which actions are ‘proper’ and ‘timely’ according to the protocoland their effects on the protocol state;(iii) the permissions, prohibitions and obligations of the agents, and the sanctions and enforcement strategies that dealwith non-compliance.In any given implementation of the protocol, it may or may not be permitted for an agent to perform an action that isnot proper or timely; conversely, there may be protocol actions that are proper and timely but that are nevertheless notpermitted under certain circumstances, because, for instance, they lead to protocol runs with undesirable properties.The rules comprising level (ii) of the specification correspond to constitutive norms that define the meaning of theprotocol actions. Levels (i) and (iii), respectively, can be seen as representing the physical and normative environmentwithin which the protocol is executed. We have also been concerned with the concept of social role. Briefly, a role isassociated with a set of (role) preconditions that agents must satisfy in order to be eligible to occupy that role and aset of (role) constraints that govern the behaviour of the agents once they occupy that role. We will not discuss roleassignment in this paper. For the example in this paper, we will assume for simplicity that the participant agents arealready assigned to certain roles, and that these roles do not change during an execution of the protocol.A note on terminology. In the earlier version of this paper [4], and in the treatment of other examples, we defineda protocol by specifying the conditions under which an action was said to be ‘valid’ according to the protocol. Here,we have employed a finer structure, further classifying ‘valid’ actions as proper or timely, in line with suggestionsthat have also been made by Prakken et al. [45,49]. A ‘valid’ action in our earlier terminology is one that is bothproper and timely. Other terminology in common use employs the term ‘successful’ where we say ‘valid’: one thendistinguishes between an action, such as an utterance or the transmission of a message of a certain form, which isan ‘attempt’ to make a claim, say, and the conditions under which the attempt to claim is ‘successful’ (sometimes,‘effective’). We prefer to avoid the term ‘successful’ since even an unsuccessful ‘attempt’ can have effects on theprotocol state. We also avoid use of the term ‘legal’ for ‘valid’ or ‘successful’ since it is ambiguous as to whetherit refers to the constitutive element of the protocol (level (ii) of our specification) or the normative environment inwhich the protocol is executed (level (iii)). Also related is the concept of institutional (or ‘institutionalised’) power(sometimes, ‘competence’ or ‘capacity’). This refers to the characteristic feature of institutions—legal systems, formalorganisations, or informal groupings—whereby designated agents, often when acting in specific roles, are empoweredto create or modify facts of special significance in that institution—institutional facts in the terminology of Searle [56].(See e.g. [27,35] for further discussion and references to the literature.) Thus in the present example it is natural tosay that, under certain circumstances, an agent acting in a certain role has power (competence, capacity) to declarethe dispute resolved in favour of one or other of the protagonists; or that in certain circumstances an agent has power\f778A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804to object to an action by one of the other participants; or more generally, that the argumentation protocol definesthe conditions under which an agent has the power to perform one of the argumentation actions. We will not referexplicitly to power in the specification of the argumentation protocol presented here. The classification of actions intoproper and timely already provides a more detailed specification.In this paper we use the language C+ to formulate the specification. An advantage of C+, compared with otheraction formalisms, is that it can be given an explicit semantics in terms of transition systems. This enables us toanalyse and prove properties of the protocol. The concluding sections of the paper present some illustrative examples.The paper is structured as follows. First, we briefly describe the C+ language. Second, we present the ‘Causal Cal-culator’ software implementation, a computational framework for executing specifications formalised in C+. Third,we summarise Brewka’s reconstruction of Rescher’s theory of formal disputation. Fourth, we specify, prove propertiesof, and execute (a form of) Brewka’s argumentation protocol with the use of C+ and the Causal Calculator. Finally,we discuss related research, summarise the presented work, and",
            {
                "entities": [
                    [
                        3248,
                        3276,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 593–618Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA heuristic search approach to planning with temporally extendedpreferencesJorge A. Baier a,b,∗, Fahiem Bacchus a, Sheila A. McIlraith aa Department of Computer Science, University of Toronto, Canadab Department of Computer Science, Pontificia Universidad Católica de Chile, Chilea r t i c l ei n f oa b s t r a c tArticle history:Received 27 October 2007Received in revised form 28 November 2008Accepted 28 November 2008Available online 6 December 2008Keywords:Planning with preferencesTemporally extended preferencesPDDL3Planning with preferences involves not only finding a plan that achieves the goal,itrequires finding a preferred plan that achieves the goal, where preferences over plansare specified as part of the planner’s input. In this paper we provide a technique foraccomplishing this objective. Our technique can deal with a rich class of preferences,including so-called temporally extended preferences (TEPs). Unlike simple preferences whichexpress desired properties of the final state achieved by a plan, TEPs can express desiredproperties of the entire sequence of states traversed by a plan, allowing the user to expressa much richer set of preferences. Our technique involves converting a planning problemwith TEPs into an equivalent planning problem containing only simple preferences. Thisconversion is accomplished by augmenting the inputed planning domain with a new set ofpredicates and actions for updating these predicates. We then provide a collection of newheuristics and a specialized search algorithm that can guide the planner towards preferredplans. Under some fairly general conditions our method is able to find a most preferredplan—i.e., an optimal plan. It can accomplish this without having to resort to admissibleheuristics, which often perform poorly in practice. Nor does our technique require anassumption of restricted plan length or make-span. We have implemented our approachin the HPlan-P planning system and used it to compete in the 5th International PlanningCompetition, where it achieved distinguished performance in the Qualitative Preferencestrack.© 2008 Elsevier B.V. All rights reserved.1. IntroductionClassical planning requires a planner to find a plan that achieves a specified goal. In practice, however, not every planthat achieves the goal is equally desirable. Preferences allow the user to provide the planner with information that it canuse to discriminate between successful plans; this information allows the planner to distinguish successful plans based onplan quality.Planning with preferences involves not just finding a plan that achieves the goal, it requires finding one that achieves thegoal while also optimizing the user’s preferences. Unfortunately, finding an optimal plan can be computationally expensive.In such cases, we would at least like the planner to direct its search towards a reasonably preferred plan.In this paper we provide a technique for accomplishing this objective. Our technique is able to deal with a rich classof preferences. Most notably this class includes temporally extended preferences (TEPs). The difference between a TEP and aso-called simple preference is that a simple preference expresses some desired property of the final state achieved by the* Corresponding author at: Department of Computer Science, University of Toronto, Canada.E-mail addresses: jabaier@cs.toronto.edu (J.A. Baier), fbacchus@cs.toronto.edu (F. Bacchus), sheila@cs.toronto.edu (S.A. McIlraith).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.11.011\f594J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618plan, while a TEP expresses a desired property of the sequence of states traversed by the plan. For example, a preferencethat a shift worker work no more than 2 overtime shifts in a week is a temporally extended preference. It expresses acondition on a sequence of daily schedules that might be constructed in a plan. Planning with TEPs has been the subject ofrecent research (e.g. [6,12,35]). It was also a theme of the 5th International Planning Competition (IPC-5).The technique we provide in this paper is able to plan with a class of preferences that includes those that can bespecified in the planning domain definition language PDDL3 [23]. PDDL3 was specifically designed for IPC-5. It extendsPDDL2.2 to include, among other things, facilities for expressing both temporally extended and simple preferences, wherethe temporally extended preferences are described by a subset of linear temporal logic (LTL). It also supports quantifying thevalue of achieving different preferences through the specification of a metric function. The metric function assigns to eachplan a value that is dependent on the specific preferences the plan satisfies. The aim in solving a PDDL3 planning instanceis to generate a plan that satisfies the hard goals and constraints while achieving the best possible metric value, optimizingthis value if possible or at least returning a high value plan if optimization is infeasible.Our technique is a two part approach. The first part exploits existing work [2] to convert planning problems with TEPsto equivalent problems containing only simple preferences defined over an extended planning domain. The second part,and main contribution of our work, is to develop a set of new heuristics, and a search algorithm that can exploit theseheuristics to guide the planner towards preferred plans. Many of our heuristics are extracted from a relaxed planninggraph a technique that has previously been used to compute heuristics in classical planning. Previous heuristics for classicalplanning, however, are not well suited to planning with preferences. The heuristics we present here are specifically designedto address the tradeoffs that arise when planning to achieve preferences.Our search algorithm is also very different from previous algorithms used in planning. As we will show, it has a numberof attractive properties, including the ability to find optimal plans without having to resort to admissible heuristics. This isimportant because admissible heuristics generally lead to unacceptable search performance. Our method is also able to findoptimal plans without requiring a restriction on plan length or make-span. This is important because such restrictions donot generally allow the planner to find a globally optimal plan. In addition, the search algorithm is incremental in that itfinds a sequence of plans each one improving on the previous. This is important because in practice it is often necessary totrade off computation time with plan quality. The first plans in this sequence of plans can often be generated fairly quicklyand provide the user with at least a working plan if they must act immediately. If more time is available the algorithmcan continue to search for a better plan. The incremental search process also employs a pruning technique to make eachincremental search more efficient. The heuristics and search algorithm presented here can easily be employed in otherplanning systems.An additional contribution of the paper is that we have brought all of these ideas together into a working planningsystem called HPlan-P. Our planner is built as an extension of the TLPlan system [1]. The basic TLPlan system uses LTLformulae to express domain control knowledge; thus, LTL formulae serve to prune the search space. However, TLPlan has nomechanism for providing heuristic guidance to the search. In contrast, our implementation extends TLPlan with a heuristicsearch mechanism that guides the planner towards plans that satisfy TEPs, while still pruning those partial plans that violatehard constraints. We also exploit TLPlan’s ability to evaluate quantified formulae to avoid having to convert the preferencestatements (many of which are quantified) into a collection of ground instances. This is important because grounding thepreferences can often yield intractably large domain descriptions. We use our implementation to evaluate the performanceof our algorithm and to analyze the relative performance of different heuristics on problems from both the IPC-5 Simple andQualitative Preferences tracks.In the rest of the paper we first provide some necessary background. This includes a brief description of the featuresof PDDL3 that our approach can handle. In Section 3 we describe the first part of our approach—a method for compilinga domain with temporally extended preferences into one that is solely in terms of simple (i.e., final state) preferences.Section 4 describes the heuristics and search algorithm we have developed. It also presents a number of formal propertiesof the algorithm, including characterizing various conditions under which the algorithm is guaranteed to return optimalplans. Section 5 presents an extensive empirical evaluation of the technique, including an analysis of the effectiveness ofvarious combinations of the heuristics presented in Section 4. Section 6 presents a discussion of the approach and Section 7summarizes our contributions and discusses related work after which we provide some final conclusions.2. BackgroundThis section reviews the background needed to understand this paper. Section 2.1 presents some basic planning defi-nitions and a brief description of the planning domain definition language PDDL. Section 2.2 describes a variation of thewell-known approach to computing domain-independent heuristics based on the computation of relaxed plans that is usedby our planner to compute heuristics. As opposed to most well-known approaches, our method is able to handle ADL do-mains directly without having to pre-compile the domain into a STRIPS domain. Section 2.3 describes the planning domaindefinition language PDDL3, a recent version of PDDL that enables the definition of hard constraints, preferences, and",
            {
                "entities": [
                    [
                        3685,
                        3713,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 175 (2011) 487–511Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintOnline planning for multi-agent systems with bounded communicationFeng Wu a,b,∗, Shlomo Zilberstein b, Xiaoping Chen aa School of Computer Science, University of Science and Technology of China, Jinzhai Road 96, Hefei, Anhui 230026, Chinab Department of Computer Science, University of Massachusetts at Amherst, 140 Governors Drive, Amherst, MA 01003, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 2 February 2010Received in revised form 22 September2010Accepted 26 September 2010Available online 29 September 2010Keywords:Decentralized POMDPsCooperation and collaborationPlanning under uncertaintyCommunication in multi-agent systemsWe propose an online algorithm for planning under uncertainty in multi-agent settingsmodeled as DEC-POMDPs. The algorithm helps overcome the high computationalcomplexity of solving such problems offline. The key challenges in decentralized operationare to maintain coordinated behavior with little or no communication and, whencommunication is allowed, to optimize value with minimal communication. The algorithmaddresses these challenges by generating identical conditional plans based on commonknowledge and communicating only when history inconsistency is detected, allowingcommunication to be postponed when necessary. To be suitable for online operation,the algorithm computes good local policies using a new and fast local search methodimplemented using linear programming. Moreover, it bounds the amount of memory usedat each step and can be applied to problems with arbitrary horizons. The experimentalresults confirm that the algorithm can solve problems that are too large for the bestexisting offline planning algorithms and it outperforms the best online method, producingmuch higher value with much less communication in most cases. The algorithm also provesto be effective when the communication channel is imperfect (periodically unavailable).These results contribute to the scalability of decision-theoretic planning in multi-agentsettings.© 2010 Elsevier B.V. All rights reserved.1. IntroductionA multi-agent system (MAS) consists of multiple independent agents that interact in a domain. Each agent is a decisionmaker that is situated in the environment and acts autonomously, based on its own observations and domain knowledge, toaccomplish a certain goal. A multi-agent system design can be beneficial in many AI domains, particularly when a systemis composed of multiple entities that are distributed functionally or spatially. Examples include multiple mobile robots(such as space exploration rovers) or sensor networks (such as weather tracking radars). Collaboration enables the differentagents to work more efficiently and to complete activities they are not able to accomplish individually. Even in domains inwhich agents can be centrally controlled, a MAS can improve performance, robustness and scalability by selecting actions inparallel. In principle, the agents in a MAS can have different, even conflicting, goals. We are interested in fully-cooperativeMAS, in which all the agents share a common goal.In a cooperative setting, each agent selects actions individually, but it is the resulting joint action that produces theoutcome. Coordination is therefore a key aspect in such systems. The goal of coordination is to ensure that the individualdecisions of the agents result in (near-)optimal decisions for the group as a whole. This is extremely challenging especially* Corresponding author at: Department of Computer Science, University of Massachusetts at Amherst, 140 Governors Drive, Amherst, MA 01003, USA.Tel.: +1 413 545 1985; fax: +1 413 545 1249.E-mail addresses: wufeng@mail.ustc.edu.cn (F. Wu), shlomo@cs.umass.edu (S. Zilberstein), xpchen@ustc.edu.cn (X. Chen).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.09.008\f488F. Wu et al. / Artificial Intelligence 175 (2011) 487–511when the agents operate under high-level uncertainty. For example, in the domain of robot soccer, each robot operatesautonomously, but is also part of a team and must cooperate with the other members of the team to play successfully.The sensors and actuators used in such systems introduce considerable uncertainty. What makes such problems particularlychallenging is that each agent gets a different stream of observations at runtime and has a different partial view of thesituation. And while the agents may be able to communicate with each other, sharing all their information all the time isnot possible. Besides, agents in such domains may need to perform a long sequence of actions in order to reach the goal.Different mathematical models exist to specify sequential decision-making problems. Among them, decision-theoreticmodels for planning under uncertainty have been studied extensively in artificial intelligence and operations research sincethe 1950’s. Decision-theoretic planning problems can be formalized as Markov decision processes (MDPs), in which a singleagent repeatedly interacts with a stochastically changing environment and tries to optimize a performance measure basedon rewards or costs. Partially-observable Markov decision processes (POMDPs) extend the MDP model to handle sensor un-certainty by incorporating observations and a probabilistic model of their occurrence. In a MAS, however, each individualagent may have different partial information about the other agents and about the state of the world. Over the last decade,different formal models for this problem have been proposed. We adopt decentralized partially-observable Markov deci-sion processes (DEC-POMDPs) to model a team of cooperative agents that interact within a stochastic, partially-observableenvironment.It has been proved that decentralized control of multiple agents is significantly harder than single agent control andprovably intractable. In particular, the complexity of solving a two-agent finite-horizon DEC-POMDP is NEXP-complete [12].In the last few years, several promising approximation techniques have been developed [3,11,17,19,46,47]. The vast majorityof these algorithms work offline and compute, prior to the execution, the best action to execute for all possible situations.While these offline algorithms can achieve very good performance, they often take a very long time due to the doubleexponential policy space that they explore. For example, PBIP-IPG – the state-of-the-art MBDP-based offline algorithm –takes 3.85 hours to solve a small problem such as Meeting in a 3×3 grid that involves 81 states, 5 actions and 9 observations[3]. Online algorithms, on the other hand, plan only one step at a time and they do so given all the currently availableinformation. The potential for achieving good scalability is more promising with online algorithms. But it is extremelychallenging to keep agents coordinated over a long period of time with no offline planning. Recent developments in onlinealgorithms suggest that combining online techniques with selective communication – when communication is possible –may be the most efficient way to tackle large DEC-POMDP problems. The main goal of this paper is to present, analyze,and evaluate online methods with bounded communication, and show that they present an attractive alternative to offlinetechniques for solving large DEC-POMDPs.The main contributions of this paper include: (1) a fast method for searching policies online, (2) an innovative way foragents to remain coordinated by maintaining a shared pool of histories, (3) an efficient way for bounding the number ofpossible histories agents need to consider, and (4) a new communication strategy that can cope with bounded or unreliablecommunication channels. In the presence of multiple agents, each agent must cope with limited knowledge about theenvironment and the other agents, and must reason about all the possible beliefs of the other agents and how that affectstheir decisions. Therefore, there are still many possible situations to consider even for selecting just one action given thecurrent knowledge. We present a new linear program formulation to search the space of policies very quickly. Anotherchallenge is that the number of possible histories (situations) grows very rapidly over time steps, and agents could runout of memory very quickly. We introduce a new approach to merging histories and thus bound the size of the pool ofhistories, while preserving solution quality. Finally, it is known that appropriate amounts of communication can improvethe tractability and performance of multi-agent systems. When communication is bounded, which is true in many real-world applications, it is difficult to decide how to utilize the limited communication resource efficiently. In our work,agents communicate when history inconsistency is detected. This presents a new effective way to initiate communicationdynamically at runtime.The rest of the paper is organized as follows. In Section 2, we provide the background by introducing the formal modeland discussing the offline and online algorithms as well as the communication methods in the framework of decentralizedPOMDPs. In Section 3, we present the multi-agent online planning with communication algorithms including the generalframework, policy search, history merging, communication strategy and implementation issues. In Section 4, we report theexperimental results on several common benchmark problems and a more challenging problem named grid soccer. We alsoreport the results for the cooperative box pushing domain with imperfect communication settings. In Section 5, we surveythe various existing online approaches with communications that have been applied to decentralized POMDPs, and discusstheir strengths and drawbacks. Finally, we summarize the contributions and discuss the limitations and open questions inthis work.2. BackgroundI",
            {
                "entities": [
                    [
                        3970,
                        3998,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 726–748Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAnalysis of a probabilistic model of redundancy in unsupervisedinformation extractionDoug Downey a,∗, Oren Etzioni b, Stephen Soderland ba Northwestern University, 2133 Sheridan Road, Evanston, IL 60208, United Statesb University of Washington, Box 352350, Seattle, WA 98195, United Statesa r t i c l ei n f oa b s t r a c tArticle history:Received 7 July 2009Received in revised form 14 April 2010Accepted 26 April 2010Available online 29 April 2010Keywords:Information extractionUnsupervisedWorld Wide WebUnsupervised Information Extraction (UIE) is the task of extracting knowledge fromtext without the use of hand-labeled training examples. Because UIE systems do notrequire human intervention, they can recursively discover new relations, attributes, andinstances in a scalable manner. When applied to massive corpora such as the Web, UIEsystems present an approach to a primary challenge in artificial intelligence: the automaticaccumulation of massive bodies of knowledge.A fundamental problem for a UIE system is assessing the probability that its extractedinformation is correct. In massive corpora such as the Web, the same extraction is foundrepeatedly in different documents. How does this redundancy impact the probability ofcorrectness?We present a combinatorial “balls-and-urns” model, called Urns, that computes the impactof sample size, redundancy, and corroboration from multiple distinct extraction ruleson the probability that an extraction is correct. We describe methods for estimatingUrns’s parameters in practice and demonstrate experimentally that for UIE the model’slog likelihoods are 15 times better, on average, than those obtained by methods used inprevious work. We illustrate the generality of the redundancy model by detailing multipleapplications beyond UIE in which Urns has been effective. We also provide a theoreticalfoundation for Urns’s performance, including a theorem showing that PAC Learnability inUrns is guaranteed without hand-labeled data, under certain assumptions.© 2010 Elsevier B.V. All rights reserved.1. IntroductionAutomatically extracting knowledge from text is the task of Information Extraction (IE). When applied to the Web, IEpromises to radically improve Web search engines, allowing them to answer complicated questions by synthesizing infor-mation across multiple Web pages. Further, extraction from the Web presents a new approach to a fundamental challengein artificial intelligence: the automatic accumulation of massive bodies of knowledge.IE on the Web is particularly challenging due to the variety of different concepts expressed. The strategy employed forprevious, small-corpus IE is to hand-label examples for each target concept, and uses the examples to train an extractor [19,38,7,9,29,27]. On the Web, hand-labeling examples of each concept are intractable—the number of concepts of interestis simply far too large. IE without hand-labeled examples is referred to as Unsupervised Information Extraction (UIE). UIE* Corresponding author.E-mail addresses: ddowney@eecs.northwestern.edu (D. Downey), etzioni@cs.washington.edu (O. Etzioni), soderlan@cs.washington.edu (S. Soderland).URLs: http://www.cs.northwestern.edu/~ddowney/ (D. Downey), http://www.cs.washington.edu/homes/etzioni/ (O. Etzioni),http://www.cs.washington.edu/homes/soderlan/ (S. Soderland).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.024\fD. Downey et al. / Artificial Intelligence 174 (2010) 726–748727systems such as KnowItAll [16–18] and TextRunner [3,4] have demonstrated that at Web scale, automatically-generatedtextual patterns can perform UIE for millions of diverse facts. As a simple example, an occurrence of the phrase “C such asx” suggests that the string x is a member of the class C, as in the phrase “films such as Star Wars” [22].1However, all extraction techniques make errors, and a key problem for an IE system is determining the probability thatextracted information is correct. Specifically, given a corpus, and a set of extractions XC for a class C , we wish to estimateP (x ∈ C|corpus) for each x ∈ XC . In UIE, where hand-labeled examples are unavailable, the task is particularly challenging.How can we automatically assign probabilities of correctness to extractions for arbitrary target concepts, without hand-labeled examples?This paper presents a solution to the above question that applies across a broad spectrum of UIE systems and techniques.It relies on the KnowItAll hypothesis, which states that extractions that occur more frequently in distinct sentences in a corpusare more likely to be correct.KnowItAll hypothesis: Extractions drawn more frequently from distinctsentences in a corpus are more likely to be correct.The KnowItAll hypothesis holds on the Web. Intuitively, we would expect the KnowItAll hypothesis to hold because althoughextraction errors occur (e.g., KnowItAll erroneously extracts California as a City name from the phrase “states con-taining large cities, such as California”), errors occurring in distinct sentences tend to be different.2 Thus, typically a givenerroneous extraction is repeated only a limited number of times. Further, while the Web does contain some misinformation(for example, the statement “Elvis killed JFK” appears almost 200 times on the Web according to a major search engine),this tends to be the exception (the correct statement “Oswald killed JFK” occurs over 3000 times).At Web-scale, the KnowItAll hypothesis can identify many correct extractions due to redundancy: individual facts areoften repeated many times, and in many different ways. For example, consider the TextRunner Web information extractionsystem, which extracts relational statements between pairs of entities (e.g., from the phrase “Edison invented the light bulb,”TextRunner extracts the relational statement Invented(Edison, light bulb)). In an experiment with a set of about500 million Web pages, ignoring the extractions occurring only once (which tend to be errors), TextRunner extracted 829million total statements, of which only 218 million were unique (on average, 3.8 repetitions per statement). Well-knownfacts can be repeated many times. According to a major search engine, the Web contains over 10,000 statements thatThomas Edison invented the light bulb, and this fact is expressed in dozens of different ways (“Edison invented the lightbulb,” “The light bulb, invented by Thomas Edison,” ”Thomas Edison, after ten thousand trials, invented a workable lightbulb,” etc.).Although the KnowItAll hypothesis is simply stated, leveraging it to assess extractions is non-trivial. For example, the10,000th most frequently extracted Film is dramatically more likely to be correct than the 10,000th most frequently ex-tracted US President, due to the relative sizes of the target sets. In UIE, this distinction must be identified without anyhand-labeled data. This paper shows that a probabilistic model of the KnowItAll hypothesis, coupled with the redundancyof the Web, can power UIE for arbitrary target concepts. The primary contributions are discussed below.1.1. The urns model of redundancy in textThe KnowItAll hypothesis states that the probability that an extraction is correct increases with its repetition. But byhow much? How can we precisely quantify our confidence in an extraction given the available textual evidence?We present an answer to these questions in the form of the Urns model—an instance of the classic “balls-and-urns”model from combinatorics. In Urns, extractions are represented as draws from an urn, where each ball in the urn is labeledwith either a correct extraction, or an error—and different labels can be repeated on different numbers of balls. Giventhe frequency distribution in the urn for labels in the target set and error set, we can compute the probability that anobserved label is a target element based on how many times it is drawn. A key insight of Urns is that when the frequencydistributions have predictable structure (for example, in textual corpora the distributions tend to the Zipfian), they can beestimated without hand-labeled data.We prove that when the frequency of each label in the urn is drawn from a mixture of two Zipfian distributions (onefor the target class and another for errors), the parameters of Urns can be learned without hand-labeled data. When thedata exhibits a certain separability criterion, PAC learnability is guaranteed. We also demonstrate that Urns is effective inpractice. In experiments with UIE on the Web, the probabilities produced by the model are shown to be 15 times better, onaverage, when compared with techniques from previous work [14].1 Here, the term class may also refer to relations between multiple strings, e.g. the ordered pair (Chicago, Illinois) is a member of the Locate-dIn class.2 Two sentences are distinct when they are not comprised of exactly the same word sequence. We stipulate that sentences be distinct to avoid placingundue credence in content that is simply duplicated across many different pages, a common occurrence on the Web.\f728D. Downey et al. / Artificial Intelligence 174 (2010) 726–7481.2. Paper outlineThe paper proceeds as follows. We describe the Urns model in Section 2, experimentally demonstrate its effectivenessin UIE, and detail applications beyond UIE in which the model has been employed. The theoretical results characterizing theUrns model are presented in Section 3. We discuss future work in Section 4, and conclude.2. The URNS modelIn this section, we describe the Urns model for assigning probabilities of correctness to extractions. We begin by formallyintroducing the model, then describe our implementation and a set of experiments establishing the model’s effectivenessfor UIE.The Urns model takes the form of a classic “balls-and-urns” model from combinatorics",
            {
                "entities": [
                    [
                        3544,
                        3572,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 107–143www.elsevier.com/locate/artintLearning action models from plan examples usingweighted MAX-SATQiang Yang a,∗, Kangheng Wu a,b, Yunfei Jiang ba Department of Computer Science and Engineering, Hong Kong University of Science and Technology,Clearwater Bay, Kowloon, Hong Kong, Chinab Software Institute, Zhongshan University (Sun Yat-Sen University), Guangzhou, ChinaReceived 21 October 2005; received in revised form 14 November 2006; accepted 27 November 2006Available online 25 January 2007AbstractAI planning requires the definition of action models using a formal action and plan description language, such as the stan-dard Planning Domain Definition Language (PDDL), as input. However, building action models from scratch is a difficult andtime-consuming task, even for experts. In this paper, we develop an algorithm called ARMS (action-relation modelling system)for automatically discovering action models from a set of successful observed plans. Unlike the previous work in action-modellearning, we do not assume complete knowledge of states in the middle of observed plans. In fact, our approach works when noor partial intermediate states are given. These example plans are obtained by an observation agent who does not know the logicalencoding of the actions and the full state information between the actions. In a real world application, the cost is prohibitivelyhigh in labelling the training examples by manually annotating every state in a plan example from snapshots of an environment.To learn action models, ARMS gathers knowledge on the statistical distribution of frequent sets of actions in the example plans. Itthen builds a weighted propositional satisfiability (weighted MAX-SAT) problem and solves it using a MAX-SAT solver. We laythe theoretical foundations of the learning problem and evaluate the effectiveness of ARMS empirically.© 2006 Elsevier B.V. All rights reserved.Keywords: Learning action models; Automated planning; Statistical relational learning1. IntroductionAI planning systems require the definition of action models, an initial state and a goal. In the past, various actionmodelling languages have been developed. Some examples are STRIPS [13], ADL [12] and PDDL [14,17]. With theselanguages, a domain expert sits down and writes a complete set of domain action representation. These representationsare then used by planning systems as input to generate plans.However, building action models from scratch is a task that is exceedingly difficult and time-consuming even fordomain experts. Because of this difficulty, various approaches [4,5,18,34,37,42,43] have been explored to learn action* Corresponding author.E-mail address: qyang@cse.ust.hk (Q. Yang).URL: http://www.cse.ust.hk/~qyang.0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.11.005\f108Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143models from examples. In knowledge acquisition for planning, many state of the art systems for acquiring actionmodels are based on a procedure in which a computer system interacts with a human expert, as illustrated by Blytheet al. [5] and McCluskey et al. [29]. A common feature of these works is that they require states just before or aftereach action to be known. Statistical and logical inferences can then be made to learn the actions’ preconditions andeffects.In this paper, we take one step towards automatically acquiring action models from observed plans in practice.The resultant algorithm is called ARMS, which stands for Action-Relation Modelling System. We assume that eachobserved plan consists of a sequence of action names together with the objects that each action uses. The intermediatestates between actions can be partially known; that is, between every adjacent pair of actions, the truth of a literal canbe totally unknown. This means that our input can be in the form of action names and associated parameter list withno state information, which is much more practical than previous systems that learn action models. Suppose that wehave several observed plans as input. From this incomplete knowledge, our ARMS system automatically guesses theapproximately correct and concise action models that can explain most of the observed plans. This action model isnot guaranteed to be completely correct, but it can serve to provide important initial guidance for human knowledgeeditors.Consider an example input and output of our algorithm in the Depot problem domain from an AI Planning com-petition [14,17]. As part of the input, we are given relations such as (clear ?x:surface) to denote that ?x is clear on topand that ?x is of type “surface”, relation (at ?x:locatable ?y:place) to denote that a locatable object ?x is located at aplace ?y. We are also given a set of plan examples consisting of action names along with their parameter list, such asdrive(?x:truck ?y:place ?z:place), and then lift(?x:hoist ?y:crate ?z:surface ?p:place). We call the pair consisting of anaction name and the associated parameter list an action signature; an example of an action signature is drive(?x:truck?y:place ?z:place). Our objective is to learn an action model for each action signature, such that the relations in thepreconditions and postconditions are fully specified.A complete description of the example is shown in Table 1, which lists the actions to be learned, and Table 2, whichdisplays the training examples. From the examples in Table 2, we wish to learn the preconditions, add and delete listsof all actions. Once an action is given with the three lists, we say that it has a complete action model. Our goal is tolearn an action model for every action in a problem domain in order to “explain” all training examples successfully.An example output from our learning algorithms for the load(?x ?y ?z ?p) action signature is:actionload(?x:hoist ?y:crate ?z:truck ?p:place)pre:del:add:(at ?x ?p), (at ?z ?p), (lifting ?x ?y)(lifting ?x ?y)(at ?y ?p), (in ?y ?z), (available ?x), (clear ?y)Table 1Input domain description for Depot planning domainDomaintypesrelationsactionsDepotplace locatable - objectdepot distributor - placetruck hoist surface - locatablepallet crate - surface(at ?x:locatable ?y:place)(on ?x:crate ?y:surface)(in ?x:crate ?y:truck)(lifting ?x:hoist ?y:crate)(available ?x:hoist)(clear ?x:surface)drive(?x:truck ?y:place ?z:place)lift(?x:hoist ?y:crate ?z:surface ?p:place)drop(?x:hoist ?y:crate ?z:surface ?p:place)load(?x:hoist ?y:crate ?z:truck ?p:place)unload(?x:hoist ?y:crate ?z:truck ?p:place)\fQ. Yang et al. / Artificial Intelligence 171 (2007) 107–143109Table 2Three plan traces as part of the training examplesInitialStep1StateStep2Step3StateStep4StateStep5Step6Step7Step8Step9GoalPlan1I1lift(h1 c0 p1 ds0),drive(t0 dp0 ds0)load(h1 c0 t0 ds0)drive(t0 ds0 dp0)(available h1)unload(h0 c0 t0 dp0)(lifting h0 c0)drop (h0 c0 p0 dp0)(on c0 p0)Plan2I2lift(h1 c1 c0 ds0)(lifting h1 c1)load(h1 c1 t0 ds0)lift(h1 c0 p1 ds0)load(h1 c0 t0 ds0)drive(t0 ds0 dp0)unload(h0 c1 t0 dp0)drop(h0 c1 p0 dp0)unload(h0 c0 t0 dp0)drop(h0 c0 c1 dp0)(on c1 p0)(on c0 c1)Plan3I3lift(h2 c1 c0 ds0)load(h2 c1 t1 ds0)lift(h2 c0 p2 ds0),drive(t1 ds0 dp1)unload(h1 c1 t1 dp1),load(h2 c0 t0 ds0)drop(h1 c1 p1 dp1),drive(t0 ds0 dp0)unload(h0 c0 t0 dp0)drop(h0 c0 p0 dp0)(on c0 p0)(on c1 p1)I1: (at p0 dp0), (clear p0), (available h0), (at h0 dp0), (at t0 dp0), (at p1 ds0), (clear c0), (on c0 p1), (available h1), (at h1 ds0).I2: (at p0 dp0), (clear p0), (available h0), (at h0 dp0), (at t0 ds0), (at p1 ds0), (clear c1), (on c1 c0), (on c0 p1), (available h1), (at h1 ds0).I3: (at p0 dp0), (clear p0), (available h0), (at h0 dp0), (at p1 dp1), (clear p1), (available h1), (at h1 dp1), (at p2 ds0), (clear c1), (on c1 c0), (on c0p2), (available h2), (at h2 ds0), (at t0 ds0), (at t1 ds0).We wish to emphasize an important feature of our problem definition, which is the relaxation of observable stateinformation requirements. The learning problem would be easier if we knew the states just before and after eachaction. However, in reality, what is observed before and after each action may just be partially known. In this paper,we address the situation where we know little about the states surrounding the actions. Thus we do not know for sureexactly what is true just before each of load(h1 c0 t0 ds0), drive(t0 ds0 dp0), unload(h0 c0 t0 dp0), drop(h0 c0 p0dp0) as well as the complete state just before the goal state in the first plan in Table 2. Part of the difficulty in learningaction models is due to the uncertainty in assigning state relations to actions. In each plan, any relation such as (on c0p0) in the goal conditions might be established by the first action, the second, or any of the rest. It is this uncertaintythat causes difficulties for many previous approaches that depend on knowing states precisely.In our methodology, the training plans can be obtained through monitoring devices such as sensors and cameras, orthrough a sequence of recorded commands through a computer system such as UNIX. These action models can thenbe revised using interactive systems such as GIPO.It is thus intriguing to ask whether we can approximate an action model in an application domain if we are given aset of recorded action signatures as well as partial or even no intermediate state information. In this paper, we take afirst step towards answering this question by presenting an algorithm known as ARMS. ARMS proceeds in two phases.In phase one of the algorithm, ARMS finds frequent action sets from plans that share a common set of parameters.In addition, ARMS finds some frequent relation-action pairs with the help of the initial state and the goal state. Theserelation-action pairs give us an initial guess on the preconditions, add lists and delete lists of actions in this subset.These action subsets and pairs are used to obtain a set of constraints that must hold in order to make the plans correct.We then transform the con",
            {
                "entities": [
                    [
                        2849,
                        2877,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 242 (2017) 132–171Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintMaking friends on the fly: Cooperating with new teammates ✩Samuel Barrett a,∗,1, Avi Rosenfeld b, Sarit Kraus c,d, Peter Stone ea Cogitai, Inc., Anaheim, CA 92808, USAb Dept. of Industrial Engineering, Jerusalem College of Technology, Jerusalem, 9116001, Israelc Department of Computer Science, Bar-Ilan University, Ramat Gan, 5290002, Israeld Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USAe Dept. of Computer Science, The University of Texas at Austin, Austin, TX 78712, USAa r t i c l e i n f oa b s t r a c tArticle history:Received 28 February 2015Received in revised form 10 October 2016Accepted 17 October 2016Available online 21 October 2016Keywords:Ad hoc teamworkMultiagent systemsMultiagent cooperationReinforcement learningPursuit domainRoboCup soccerRobots are being deployed in an increasing variety of environments for longer periods of time. As the number of robots grows, they will increasingly need to interact with other robots. Additionally, the number of companies and research laboratories producing these robots is increasing, leading to the situation where these robots may not share a common communication or coordination protocol. While standards for coordination and communication may be created, we expect that robots will need to additionally reason intelligently about their teammates with limited information. This problem motivates the area of ad hoc teamwork in which an agent may potentially cooperate with a variety of teammates in order to achieve a shared goal. This article focuses on a limited version of the ad hoc teamwork problem in which an agent knows the environmental dynamics and has had past experiences with other teammates, though these experiences may not be representative of the current teammates. To tackle this problem, this article introduces a new general-purpose algorithm, PLASTIC, that reuses knowledge learned from previous teammates or provided by experts to quickly adapt to new teammates. This algorithm is instantiated in two forms: 1) PLASTIC-Model – which builds models of previous teammates’ behaviors and plans behaviors online using these models and 2) PLASTIC-Policy – which learns policies for cooperating with previous teammates and selects among these policies online. We evaluate PLASTIC on two benchmark tasks: the pursuit domain and robot soccer in the RoboCup 2D simulation domain. Recognizing that a key requirement of ad hoc teamwork is adaptability to previously unseen agents, the tests use more than 40 previously unknown teams on the first task and 7 previously unknown teams on the second. While PLASTIC assumes that there is some degree of similarity between the current and past teammates’ behaviors, no steps are taken in the experimental setup to make sure this assumption holds. The teammates were created by a variety of independent developers and were not designed to share any similarities. Nonetheless, the results show that PLASTIC was able to identify and exploit similarities between its current and past teammates’ behaviors, allowing it to quickly adapt to new teammates.© 2016 Elsevier B.V. All rights reserved.✩This article contains material from 4 prior conference papers [11–14].* Corresponding author.E-mail addresses: sam@cogitai.com (S. Barrett), rosenfa@jct.ac.il (A. Rosenfeld), sarit@cs.biu.ac.il (S. Kraus), pstone@cs.utexas.edu (P. Stone).1 This work was performed while Samuel Barrett was a graduate student at the University of Texas at Austin.http://dx.doi.org/10.1016/j.artint.2016.10.0050004-3702/© 2016 Elsevier B.V. All rights reserved.\fS. Barrett et al. / Artificial Intelligence 242 (2017) 132–1711331. IntroductionRobots are becoming cheaper and more durable and are therefore being deployed in more environments for longer periods of time. As robots continue to proliferate in this way, many of them will encounter and interact with a variety of other kinds of robots. In many cases, these interacting robots will share a set of common goals, in which case it will be desirable for them to cooperate with each other. In order to effectively perform in new environments and with changing teammates, they should observe their teammates and adapt to achieve their shared goals. For example, after a disaster, it is helpful to use robots to search the site and rescue survivors. However, the robots may come from a variety of sources and may not be designed to cooperate with each other, such as in the response to the 2011 Tohoku earthquake and tsunami [43,55,56,58]. If these robots are not pre-programmed to cooperate, they may not share information about which areas have been searched; or worse, they may unintentionally impede their teammates’ efforts to rescue survivors. Therefore, in the future, it is desirable for robots to be designed to observe their teammates and adapt to them, forming a cohesive team that quickly searches the area and rescues the survivors.This idea epitomizes the spirit of ad hoc teamwork. In ad hoc teamwork settings, agents encounter a variety of teammates and try to accomplish a shared goal. In ad hoc teamwork research, researchers focus on designing a single agent or subset of agents that can cooperate with a variety of teammates. The desire is for agents designed for ad hoc teamwork to quickly learn about these teammates and determine how they should act on this new team to achieve their shared goals. Agents that reason about ad hoc teamwork will be robust to changes in teammates in addition to changes in the environment. This article focuses on a limited version of the ad hoc teamwork problem. Specifically, this article investigates how an agent should adapt to new teammates given that it has previously interacted with other teammates and learned from these interactions. However, these past interactions may not be representative of the current teammates.In this article, the word “agent” refers to an entity that repeatedly senses its environment and takes actions that affect this environment, shown visually in Fig. 1a. As a shorthand, the terms ad hoc team agent and ad hoc agent are used in this article to refer to an agent that reasons about ad hoc teamwork. The environment includes the dynamics of the world the agent interacts with, as well as defining the observations received by the agent. We treat the other agents in the domain as teammates because they share a set of common goals; they are fully cooperative in the terminology of game theory.Previous work on teamwork has largely assumed that all agents in the domain will act as a unified team and are designed to work with their specific teammates [25,36,66,68]. Methods for coordinating multiagent teams largely rely on specifying standardized protocols for communication as well as shared algorithms for coordination. These approaches do not directly apply to ad hoc teams due to their strong assumptions about this sharing of prior knowledge, which is violated in the ad hoc teamwork scenario. This view of multiagent teams is shown in Fig. 1b.On the other hand, this article will focus on creating a single agent that cooperates with teammates coming from a variety of sources without directly altering the behavior of these teammates. However, all of the agents still share a set of common goals, so it is desirable for them to act as a team. In addition, rather than focusing on a single task, these agents may face a variety of tasks, where a task refers to both the environment other than the team’s agents as well as the team’s shared goals.The differences of this article from prior work are presented visually in Fig. 1. Another existing area of research into how agents should behave is reinforcement learning (RL). Generally, RL problems revolve around a single agent learning by interacting with its environment. In RL problems, agents receive sparse feedback about the quality of sequences of actions. Generally, RL algorithms either model other agents as part of the environment and try to learn the best policy for the single agent given this environment or they consider the case where the whole team is under a single designer’s control. In addition, RL algorithms usually learn from scratch in each new environment, ignoring information coming from previous environments. However, there is a growing body of work on applying transfer learning to RL to allow agents to reuse prior experiences on new domains [69]. Fig. 1a shows the standard RL view of an agent interacting with its environment. Fig. 1b represents a common multiagent view of a unified team interacting with the environment where the agents model their teammates as being separate from the environment. In this case, the team is designed before being deployed to cooperate with these specific agents to interact with a fixed environment. However, these agents rely on knowing their teammates and usually require an explicit communication and/or coordination protocol to be shared among the whole team [36,53,73]. On the other hand, this article will focus on ad hoc teams drawn from a set of possible teammates, where the team tackles a variety of possible environments as shown in Fig. 1c. In this case, the teammates are not programmed to cooperate with this specific ad hoc agent, and they must be treated as given and inalterable. Instead, this research focuses on enabling the ad hoc agent to cooperate with a variety of teammates in a range of possible environments.In an ad hoc team, agents need to be able to cooperate with a variety of previously unseen teammates. Rather than developing protocols for coordinating an entire team, ad hoc team research focuses on developing agents that cooperate with teammates in the absence of such explicit protocols. Therefore, we consider a single agent cooperating with teammates that may or may not adapt to its behavior. In this scenario, w",
            {
                "entities": [
                    [
                        3656,
                        3684,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 166 (2005) 81–139www.elsevier.com/locate/artintKnowledge and communication:A first-order theory ✩Ernest DavisCourant Institute, New York University, New York, NY 10012, USAReceived 1 July 2004; accepted 2 May 2005Available online 22 June 2005AbstractThis paper presents a theory of informative communications among agents that allows a speaker tocommunicate to a hearer truths about the state of the world; the occurrence of events, including othercommunicative acts; and the knowledge states of any agent—speaker, hearer, or third parties—any ofthese in the past, present, or future—and any logical combination of these, including formulas withquantifiers. We prove that this theory is consistent, and compatible with a wide range of physical the-ories. We examine how the theory avoids two potential paradoxes, and discuss how these paradoxesmay pose a danger when this theory are extended. 2005 Elsevier B.V. All rights reserved.Keywords: Communication; Knowledge; Logic; Paradox1. IntroductionIn constructing a formal theory of communications between agents, the issue of expres-sivity enters at two different levels: the scope of what can be said about the communica-tions, and the scope of what can be said in the communications. Other things being equal,it is obviously desirable to make both of these as extensive as possible. Ideally, a theoryshould allow a speaker to communicate to a hearer truths about the state of the world;the occurrence of events, including other communicative acts; the knowledge states of any✩ The research reported in this paper was supported in part by NSF grant IIS-0097537.E-mail address: davise@cs.nyu.edu (E. Davis).0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.05.002\f82E. Davis / Artificial Intelligence 166 (2005) 81–139agent—speaker, hearer, or third parties; any of these in the past, present, or future; and anylogical combination of these. This paper presents a theory that achieves pretty much that.A few examples of what can be expressed in our representation:(1) Alice tells Bob that all her children are asleep.(2) Alice tells Bob that she does not know whether he locked the door.(3) Alice tells Bob that if he finds out who was in the kitchen at midnight, then he willknow who killed Colonel Mustard.(4) Alice tells Bob that no one had ever told her she had a sister.(5) Alice tells Bob that he has never told her anything she did not already know.The above examples illustrate many of the expressive features of our representation:• Example 1 shows that the content of a communication may be a quantified formula.• Example 2 shows that the content of a communication may refer to knowledge andignorance of past actions.• Example 3 shows that the content of a communication may be a complex formulainvolving both past and future events and states of knowledge.• Examples 4 and 5 show that the content of a communication may refer to other com-munications. They also show that the language supports quantification over agents andover the content of a communication, and thus allows the content to be partially char-acterized, rather than fully specified.Moreover, our theory supports basic inference from these kinds of representations. Forexample, given that Alice tells Bob that no one has ever told her that she has a sister, andthat Alice knows that, if she did have a sister, someone would have told her, it is possibleto infer that Alice knows that she does not have a sister. The proof from our theory of thisand similar sample inferences and the representation of the five statements above is givenin Section 4.Following the research programme of [8,21,24,25], the primary purpose of this paper isto develop a representation for expressing commonsense knowledge about knowledge andcommunication, with the ultimate intention that this representation or something similar,could be used to carry out symbolic reasoning in this domain. A secondary purpose isto develop an object-level theory, expressible in the language, that will justify as broad arange as possible of commonsensically obvious inference in the domain, while entailing asfew as possible commonsensically absurd consequences. The success of the language andtheory is demonstrated in terms of their ability to capture a large number and a broad rangeof examples of commonsensically obvious inferences. We are not here concerned withspecialized applications, such as distributed systems; with subtle philosophical nuance; orwith efficiency of inference in an implemented reasoning engine. Potentially, this theorycould find practical application as a logical foundation either for planning communicationsin a multi-agent system, or for a theory of speech acts to be used in interpreting dialogueor engaging in dialogue with human speakers.Since our theory allows communications that refer to other communications, and evencommunications that refer to themselves, there is clearly a danger of running into para-\fE. Davis / Artificial Intelligence 166 (2005) 81–13983doxes of vicious self-reference. It is therefore particularly important to establish that thetheory is consistent. We prove a meta-theorem that the theory is indeed consistent; in fact,that it is consistent with a wide range of domain-specific physical theories and axiomsof knowledge acquisition. We discuss two particular apparent paradoxes—an analogue ofRussell’s paradox, and the “unexpected hanging” paradox—and we show how our theorymanages to side-step these.We should note at the outset the limitations of our theory. The theory deals only withinformative acts (and not, for example, with requests) and assumes that the following con-ditions are true and universally known: If AS communicates Q to AH, then(1) AS knows that Q is true at the time that he initiates the communication.(2) From the time that he initiates the communication, AS knows that he is carrying out acommunication; he knows that the content is Q; and he knows that the recipient is AH.(3) Similarly, when the communication is complete, AH knows that he has received a com-munication; he knows that the content was Q; and he knows that the sender was AS.(4) When the communication is complete, AS knows that the communication is completeand AH knows the time at which the communication was initiated.The paradigmatic example of a form of communication satisfying conditions (2), (3),and (4) is direct speech.1 Another example could be mail, assuming that• All messages are time-stamped with the time of sending, and signed by the sender.• There is a universally known maximal delay D between the time of sending and thetime of receiving a message. (“Receiving” here means the time when the hearer readsthe message, not the time that it arrives in his mailbox.)In this case, if we define a communication to be “complete” at the time of sending plus D,then the above conditions are met.Many aspects of the theory can be applied to communications that do not meet condi-tion (4), but I have not been able to find a plausible axiomatization of this more generalcase that I can prove to be consistent. Also, I cannot prove that the theory is consistentunless time is taken to be discrete. These are discussed further in Section 8.The paper proceeds as follows: Section 2 reviews the theories of time and of knowledge,which are not new here. Section 3 presents our language and axioms of communica-tion. Section 4 is the core of this paper; it illustrates the power of the theory by showinghow it supports the representation of the five sample statements above and three examplecommonsense inferences. Sections 5 and 6 describe two apparent paradoxes—a paradoxanalogous to Russell’s paradox and the “unexpected hanging” paradox—and explain whythese do not cause inconsistencies in the theory. Section 7 gives the statement of Theo-rems 1 and 2, which assert that the theory is internally consistent and compatible with awide range of physical theories. Sections 8 and 9 discuss related work. Section 10 dis-1 Under assumptions that are reasonable, though not universally valid: e.g. that the speaker knows what hewill say when he begins speaking, and that the speaker and hearer have common knowledge that the hearer willcorrectly understand the message.\f84E. Davis / Artificial Intelligence 166 (2005) 81–139cusses open problems and summarizes our conclusions. Appendix A gives the proofs ofTheorems 1 and 2.2. FrameworkWe use a situation-based, branching theory of time; an interval-based theory of multi-agent actions; and a possible-worlds theory of knowledge. This is all well known, so thedescription below is brief.2.1. Time and actionWe use a situation-based theory of time. Time can be either continuous2 or discrete, butit must be branching, like the situation calculus. The branching structure is described bythe partial ordering “S1 < S2”, meaning that there is a timeline containing S1 and S2 andS1 precedes S2. It is convenient to use the abbreviations “S1 (cid:1) S2” and “ordered(S1, S2).”The predicate “holds(S, Q)” means that fluent Q holds in situation S.Each agent has, in various situations, a choice about what action to perform next, andthe time structure includes a separate branch for each such choice. Thus, the statement thataction E is feasible in situation S is expressed by asserting that E occurs from S to S1 forsome S1 > S.Following McDermott [26], actions are represented as occurring over an interval; thepredicate occurs(E, S1, S2) states that action E occurs starting in S1 and ending in S2.However, the whole theory could be recast without substantial change into the situationcalculus extended to permit multiple agents, after the style of Reiter [36]. The advantageof using the “occurs” representation is the much greater ease of extensibility. The situationcalculus was developed for domains where a single agent executes a single atomic actionin each situation to bring about the next situation; and extending the situatio",
            {
                "entities": [
                    [
                        1758,
                        1786,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 951–984www.elsevier.com/locate/artintMetatheory of actions: Beyond consistencyAndreas Herzig, Ivan Varzinczak ∗IRIT – Université Paul Sabatier, 118 route de Narbonne, 31062, Toulouse Cedex 9, FranceReceived 25 April 2006; received in revised form 15 March 2007; accepted 23 April 2007Available online 29 April 2007AbstractTraditionally, consistency is the only criterion for the quality of a theory in logic-based approaches to reasoning about actions.This work goes beyond that and contributes to the metatheory of actions by investigating what other properties a good domaindescription should have. We state some metatheoretical postulates concerning this sore spot. When all postulates are satisfied wecall the action theory modular. Besides being easier to understand and more elaboration tolerant in McCarthy’s sense, modulartheories have interesting properties. We point out the problems that arise when the postulates about modularity are violated, andpropose algorithmic checks that can help the designer of an action theory to overcome them.© 2007 Elsevier B.V. All rights reserved.Keywords: Reasoning about actions; Action theory; Modularity; Ramifications1. IntroductionIn logic-based approaches to knowledge representation, a given domain is described by a set of logical formulasT , which we call a (non-logical) theory. That is also the case for reasoning about actions, where we are interested intheories describing particular actions (or, more precisely, action types). We call such theories action theories.A priori consistency is the only criterion that formal logic provides to check the quality of such descriptions. Inthe present work we go beyond that, and argue that we should require more than the mere existence of a model for agiven theory.Our starting point is the fact that in reasoning about actions one usually distinguishes several kinds of logicalformulas. Among these are effect axioms, precondition axioms, and boolean axioms. In order to distinguish such non-logical axioms from logical axioms, we prefer to speak of effect laws, executability laws, and static laws, respectively.Moreover we single out those effect laws whose effect is ⊥, and call them inexecutability laws.Given these types of laws, suppose the language is powerful enough to state conditional effects of actions. Forexample, suppose that action a is inexecutable in contexts where ϕ1 holds, and executable in contexts where ϕ2 holds.It follows that there can be no context where ϕ1 ∧ ϕ2 holds. Now ¬(ϕ1 ∧ ϕ2) is a static law that does not mention a.It is natural to expect that ¬(ϕ1 ∧ ϕ2) follows from the static laws alone. By means of examples we show that when* Corresponding author.E-mail addresses: herzig@irit.fr (A. Herzig), ivan@irit.fr (I. Varzinczak).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.013\f952A. Herzig, I. Varzinczak / Artificial Intelligence 171 (2007) 951–984this is not the case, then unexpected conclusions might follow from the theory T , even in the case T is logicallyconsistent.This motivates postulates requiring that the different laws of an action theory should be arranged modularly, i.e.,in separated components, and in such a way that interactions between them are limited and controlled. In essence,we argue that static laws may entail new effects of actions (that cannot be inferred from the effect laws alone), whileeffect laws and executability laws should never entail new static laws that do not follow from the set of static lawsalone. We here formulate postulates that make these requirements precise. It will turn out that in all existing accountsthat allow for these four kinds of laws [1–6], consistent action theories can be written that violate these postulates.In this work we give algorithms that allow one to check whether an action theory satisfies the postulates or not. Withsuch algorithms, the task of correcting flawed action theories can be made easier.Although we here use the syntax of propositional dynamic logic (PDL) [7], all we shall say applies as well tofirst-order formalisms, in particular to the Situation Calculus [8]. All postulates we are going to present can be statedas well for other frameworks, in particular for action languages such as A, AR [9–11] and others, and for SituationCalculus based approaches. In [12] we have given a Situation Calculus version of our analysis, while in [13] wepresented a similar notion for ontologies in Description Logics [14]. The present work is the complete version of theone first appeared in [15].This text is organized as follows: after some background definitions (Section 2) we state some postulates concerningaction theories (Section 3). In Sections 4 and 5, we study the two most important of these postulates, giving algorithmicmethods to check whether an action theory satisfies them or not. We then generalize our postulates (Section 6) anddiscuss possible strengthening of them (Section 7). In Section 8 we show interesting features of modular actiontheories. Before concluding, we assess related work found in the literature on metatheory of actions (Section 9).2. Preliminaries2.1. Dynamic logicHere we establish an ontology of dynamic domains. As our base formalism we use ∗-free PDL, i.e., PDL withoutthe iteration operator ∗. For more details on PDL, see [7,16].Let Act = {a1, a2, . . .} be the set of all atomic action constants of a given domain. Our running example is in termsof the Walking Turkey Scenario [4]. There, the atomic actions are load, shoot and tease. We use a as a variable foratomic actions. To each atomic action a there is an associated modal operator [a]. Here we suppose that the underlyingmultimodal logic is independently axiomatized (i.e., the logic is a fusion and there is no interaction between the modaloperators [17,18]).Prop = {p1, p2, . . .} denotes the set of all propositional constants, also called fluents or atoms. Examples of thoseare loaded, alive and walking. We use p as a variable for propositional constants.We here suppose that both Act and Prop are nonempty and finite.We use small Greek letters ϕ, ψ, . . . to denote classical formulas, also called boolean formulas. They are recursivelydefined in the following way:ϕ ::= p | (cid:5) | ⊥ | ¬ϕ | ϕ ∧ ϕ | ϕ ∨ ϕ | ϕ → ϕ | ϕ ↔ ϕ.Fml is the set of all classical formulas.Examples of classical formulas are walking → alive and ¬(bachelor ∧ married).A classical formula is classically consistent if there is at least one valuation in classical propositional logic thatmakes it true. Given ϕ ∈ Fml, valuations(ϕ) denotes the set of all valuations of ϕ. We note |=CPL the logical conse-quence in classical propositional logic.The set of all literals is Lit = Prop ∪ {¬p: p ∈ Prop}. Examples of literals are alive and ¬walking. (cid:4) will be usedas a variable for literals. If (cid:4) = ¬p, then we identify ¬(cid:4) with p.A clause χ is a disjunction of literals. We say that a literal (cid:4) appears in a clause χ , written (cid:4) ∈ χ , if (cid:4) is a disjunctof χ .We denote complex formulas (possibly with modal operators) by capital Greek letters Φ1, Φ2, . . . They are recur-sively defined in the following way:Φ ::= ϕ | [a]Φ | (cid:11)a(cid:12)Φ | ¬Φ | Φ ∧ Φ | Φ ∨ Φ | Φ → Φ | Φ ↔ Φ\fA. Herzig, I. Varzinczak / Artificial Intelligence 171 (2007) 951–984953where Φ denotes a complex formula. (cid:11)a(cid:12) is the dual operator of [a], defined by: (cid:11)a(cid:12)Φ =def ¬[a]¬Φ. Sequentialcomposition of actions is defined by the abbreviation [a1; a2]Φ =def [a1][a2]Φ. Examples of complex formulas areloaded → [shoot]¬alive and hasGun → (cid:11)load; shoot(cid:12)(¬alive ∧ ¬loaded).If T is a set of formulas (modal or classical), atm(T ) returns the set of all atoms occurring in T . For instance,atm({¬¬¬p1, [a]p2}) = {p1, p2}.For parsimony’s sake, whenever there is no confusion we identify a set of formulas with the conjunction of itselements. The semantics is that for multimodal K [19,20].Definition 1. A PDL-model is a tuple M = (cid:11)W, R(cid:12) where W is a set of valuations (alias possible worlds), andR : Act −→ 2W ×W a function mapping action constants a to accessibility relations Ra ⊆ W × W .As an example, for Act = {a1, a2} and Prop = {p1, p2}, we have the PDL-model M = (cid:11)W, R(cid:12), whereW =(cid:2){p1, p2}, {p1, ¬p2}, {¬p1, p2}(cid:3),(cid:4)(cid:2)(cid:6)R(a1) =R(a2) =({p1, p2}, {p1, ¬p2}), ({p1, p2}, {¬p1, p2}),({¬p1, p2}, {¬p1, p2}), ({¬p1, p2}, {p1, ¬p2})(cid:7)(cid:3)(cid:6){p1, ¬p2}, {p1, ¬p2}{p1, p2}, {p1, ¬p2}.(cid:7),(cid:5),Fig. 1 gives a graphical representation of M.Given M = (cid:11)W, R(cid:12), a ∈ Act, and w, w(cid:14) ∈ W , we write Ra instead of R(a), and wRaw(cid:14) instead of w(cid:14) ∈ Ra(w).Definition 2. Given a PDL-model M = (cid:11)W, R(cid:12), the satisfaction relation is defined as the smallest relation satisfying:w p (p is true at world w of model M) if p ∈ w;• |=M[a]Φ if for every w(cid:14) such that wRaw(cid:14), |=M• |=Mw• the usual truth conditions for the other connectives.w(cid:14) Φ; andDefinition 3. A PDL-model M is a model of Φ (noted |=M Φ) if and only if for all w ∈ W , |=Ma set of formulas T (noted |=M T ) if and only if |=M Φ for every Φ ∈ T .w Φ. M is a model ofIn the model depicted in Fig. 1, we have |=M p1 → [a2]¬p2 and |=M p1 ∨ p2.Definition 4. A formula Φ is a consequence of the set of global axioms T in the class of all PDL-models (notedT |=PDL Φ) if and only if for every PDL-model M, if |=M T , then |=M Φ.1We here suppose that the logic under consideration is compact [21].Fig. 1. Example of a PDL-model for Act = {a1, a2}, and Prop = {p1, p2}.1 Instead of global consequence, in [5] local consequence is considered. For that reason, a further modal operator (cid:2) had to be introduced, givinga logic that is multimodal K plus monomodal S4 for (cid:2), and where axiom schema (cid:2)Φ → [a]Φ holds.\f954A. Herzig, I. Varzinczak / Artificial Intelligence 171 (2007) 951–98",
            {
                "entities": [
                    [
                        2877,
                        2905,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 170 (2006) 835–871www.elsevier.com/locate/artintOn the computational complexity ofcoalitional resource gamesMichael Wooldridge, Paul E. Dunne ∗Department of Computer Science, University of Liverpool, Liverpool L69 7ZF, United KingdomReceived 15 July 2005; received in revised form 22 February 2006; accepted 29 March 2006Available online 2 May 2006AbstractWe study Coalitional Resource Games (CRGs), a variation of Qualitative Coalitional Games (QCGs) in which each agent isendowed with a set of resources, and the ability of a coalition to bring about a set of goals depends on whether they are collectivelyendowed with the necessary resources. We investigate and classify the computational complexity of a number of natural decisionproblems for CRGs, over and above those previously investigated for QCGs in general. For example, we show that the complexityof determining whether conflict is inevitable between two coalitions with respect to some stated resource bound (i.e., a limit valuefor every resource) is co-NP-complete. We then investigate the relationship between CRGs and QCGs, and in particular the extent towhich it is possible to translate between the two models. We first characterise the complexity of determining equivalence betweenCRGs and QCGs. We then show that it is always possible to translate any given CRG into a succinct equivalent QCG, and that it isnot always possible to translate a QCG into an equivalent CRG; we establish some necessary and some sufficient conditions for atranslation from QCGs to CRGs to be possible, and show that even where an equivalent CRG exists, it may have size exponential inthe number of goals and agents of its source QCG.© 2006 Elsevier B.V. All rights reserved.Keywords: Coalitional games; Resources; Computational complexity; Multi-agent systems1. IntroductionThe questions of why and how self-interested agents might choose to cooperate are central to several researchareas, of which multi-agent systems is an important recent example [4,44,46]. One problem that has received particularattention is that of coalition formation [20,34–36,39,40]. The main question in coalition formation is that of whichcoalition an agent should join: the main answer to this question is that an agent should join a coalition that is stable,that is, one such that no subset of agents from the coalition would have any rational incentive to defect from it [27,p. 255].In previous work, we introduced a model of coalitional games in which agents were assumed to cooperate withone another in order that they can mutually accomplish their goals [47]. Such Qualitative Coalitional Games (QCGs)seem a useful framework for modelling goal-oriented multi-agent systems. The basic idea in QCGs is that each agent* Corresponding author.E-mail address: ped@csc.liv.ac.uk (P.E. Dunne).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.03.003\f836M. Wooldridge, P.E. Dunne / Artificial Intelligence 170 (2006) 835–871desires to achieve one of a set of goals, and every coalition has available to it a set of choices, where each choiceintuitively represents one way that the coalition could choose to cooperate. A choice is modelled as a set of goals,which would be achieved if the coalition chose to cooperate in the corresponding way. The incentive for an agent tojoin a coalition is that the individual choices available to this agent may not result in the satisfaction of its goals, butby cooperating, a coalition can achieve a set of goals to their mutual satisfaction. In [47], we presented a systematicsurvey of the complexity of decision problems associated with QCGs, and also defined an efficient representation forthem, based on propositional logic.Although QCGs seems appropriate for modelling and understanding the abstract properties of cooperation in goal-oriented multi-agent systems, they do not consider the origin of the choices available to coalitions. These choicesare simply ascribed to coalitions via a characteristic function, in much the same way as in conventional coalitionalgames [27, p. 257]. In this paper, we consider a special case of QCGs, which provides one answer to the question ofhow these choices arise. In a Coalitional Resource Game (CRG), the choices available to a coalition are dependenton the resources available to its members and the resources required to achieve goals. Thus, in CRGs, we assumethat agents have goals that they desire to achieve, exactly as in QCGs; but each agent is also assumed to have a fixedendowment of resources, while to achieve any given goal requires the expenditure of a certain profile of resources.A coalition will then form in order to pool resources to achieve a set of goals that satisfies all members of the coalition.Defined in this way, every CRG can also be understood as a QCG: given any CRG, it is possible to construct a QCG that is“equivalent”, in the sense of the choices available to coalitions. Thus, given a CRG, we can ask all the questions relatingto coalitions, goals sets, and QCGs generally that were studied in [47]. But it also becomes possible to ask questionsrelating to (for example) resource consumption (e.g., is the consumption of a given resource strictly necessary in orderto satisfy the goals of a given coalition?) and resource contention (e.g., is it the case that two given coalitions cannotachieve their goals without consuming more than some stated resource bound?). In this sense, CRGs enable us to askmore fine grained questions about cooperation in the scenarios for which they are applicable than is possible usingQCGs.Many naturally occurring scenarios in contemporary computing and AI can be understood as CRGs. One of the mosttimely and important is that of virtual organisations, (VOs), particularly within emerging software infrastructures suchas the Grid:VOs have the potential to change dramatically the way we use computers to solve problems, much as the Web haschanged how we exchange information. [. . . The] need to engage in collaborative processes is fundamental to manydiverse disciplines and activities. It is because of this broad applicability of VO concepts that Grid technology isimportant. [14]VOs are of particular interest in collaborative science projects, where a number of partners cooperate by sharingresources (e.g., particle accelerators, super-computers or Grid networks, gene sequencers) in order to accomplishindividual goals. In such situations, profit is not the motivation; the VO participants are primarily interested in accom-plishing their specific goals. Such scenarios naturally map to CRGs. When the participants in a VO are software agents,then the computational questions associated with them—particularly the complexity of these questions—naturallycome to the fore.We believe that the focus on resources is very natural, given the concerns of multi-agent systems and relateddisciplines: resource limitations, and the need to efficiently manage and share resources in a multi-agent environment,provides one of the fundamental motivations for distributed AI and multi-agent systems [4, p. 9]. Most consideration ofresources in the multi-agent systems community has been directed at the resource allocation problem, i.e., the problemof determining which agent or agents should have access to some scarce resource [4, p. 15]. Economic mechanisms(such as auctions) are currently the focus of much attention with respect to resource allocation [24,33]. In this paper,we focus not on the resource allocation problem, but rather on the properties of such allocations, and in particular,how and what coalitions may form, given a specific allocation, and what the properties of such allocations are withrespect to resources.Overall, the paper makes the following three key contributions to the computational study of games played withresources:\fM. Wooldridge, P.E. Dunne / Artificial Intelligence 170 (2006) 835–871837• First, we present ten natural decision problems associated with CRGs, and classify their computational complex-ity.• Second, we investigate the relationship between CRGs and QCGs in detail. We define the notion of “equivalence”between a CRG and QCG, and show that the problem of deciding equivalence is co-NP-complete.• Third, we investigate a number of questions associated with “translating” between CRGs and QCGs. We estab-lish that, not only is it the case that any CRG can be represented by an equivalent QCG, but that for every CRG,there exists a succinct equivalent QCG. More precisely, we show that any CRG containing t resources, m goals,and n agents can be represented by a QCG of size O(bt (n + m + b)), where b is the number of bits used to en-code endowment and resource quantity values. The proof is constructive, in that we show how to build such anequivalent QCG. With respect to translating from QCGs to CRGs, we first show that in the general case, no suchtranslation is possible. We then define some necessary and some sufficient conditions for such a translation to bepossible, and show that, even when such a translation is possible, it may result in a CRG of size exponential inn + m.The paper also makes a more general contribution to the problem of how to represent coalitional games succinctly, aproblem which has attracted a number of researchers over the past five years or so (cf. [3,8,17]); see Section 6.The remainder of the paper is structured as follows. First, in Section 2, we motivate and introduce the formalframework of CRGs. Section 3 presents our main complexity results. Section 4 considers the relationship betweenCRGs and QCGs, Section 5 discusses variants of the CRG model in which some of the underlying assumptions arerelaxed, while Section 6 presents some related work. Finally, Section 7 presents some conclusions. We begin, in thefollowing subsection, with a summary of some key notational conventions and a very brief review of some relevantconcepts from complexity theory.1.1. No",
            {
                "entities": [
                    [
                        2918,
                        2946,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 186 (2012) 1–37Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintTowards fixed-parameter tractable algorithms for abstractargumentation ✩Wolfgang Dvoˇrák∗, Reinhard Pichler, Stefan WoltranInstitute of Information Systems, Vienna University of Technology, A-1040 Vienna, Austriaa r t i c l ei n f oa b s t r a c tAbstract argumentation frameworks have received a lot of interest in recent years. Mostcomputational problems in this area are intractable but several tractable fragments havebeen identified. In particular, Dunne showed that many problems can be solved in lineartime for argumentation frameworks of bounded tree-width. However, these tractabilityresults, which were obtained via Courcelle’s Theorem, do not directly lead to efficientalgorithms. The goal of this paper is to turn the theoretical tractability results into efficientalgorithms and to explore the potential of directed notions of tree-width for defining largertractable fragments. As a by-product, we will sharpen some known complexity results.© 2012 Elsevier B.V. All rights reserved.Article history:Received 16 June 2011Received in revised form 1 February 2012Accepted 11 March 2012Available online 13 March 2012Keywords:Abstract argumentationFixed-parameter tractabilityTree-widthDynamic programmingComplexity1. IntroductionArgumentation has evolved as an important field in AI with abstract argumentation frameworks (AFs, for short) asintroduced by Dung [20] being its most popular formalization. Meanwhile, a wide range of semantics for AFs has beenproposed (for an overview see [4]) and their complexity has been analyzed in depth. Most computational problems in thisarea are intractable (see e.g. [17,24,26]), but the importance of efficient algorithms for tractable fragments has been clearlyrecognized (see e.g. [18]). Such tractable fragments are, for instance, symmetric argumentation frameworks [12] or bipartiteargumentation frameworks [22].An interesting approach to dealing with intractable problems comes from parameterized complexity theory and is basedon the following observation: Many hard problems become tractable if some problem parameter is bounded by a fixedconstant. This property is referred to as fixed-parameter tractability (FPT). One important parameter of graphs is the tree-width, which measures the “tree-likeness” of a graph. Indeed, Dunne [22] showed that many problems in the area ofargumentation can be solved in linear time for argumentation frameworks of bounded tree-width. This FPT result wasshown via a seminal result by Courcelle [13]. However, as stated in [22], “rather than synthesizing methods indirectly fromCourcelle’s Theorem, one could attempt to develop practical direct methods”. The primary goal of this paper is therefore topresent new, direct algorithms for certain reasoning tasks in abstract argumentation.Clearly, the quest for FPT results in argumentation should not stop at the tree-width, and further parameters have to beanalyzed. This may of course also lead to negative results. For instance, considering as parameter the degree of an argument(i.e., the number of incoming and outgoing attacks), Dunne [22] showed that reasoning remains intractable, even if decision✩This work has been funded by the Vienna Science and Technology Fund (WWTF) through project ICT08-028 and by the Austrian Science Fund (FWF)under grant P20704-N18. A short version of this article appeared in the Proceedings of the 12th International Conference on Knowledge Representationand Reasoning (KR 2010), AAAI Press, 2010.* Corresponding author.E-mail addresses: dvorak@dbai.tuwien.ac.at (W. Dvoˇrák), pichler@dbai.tuwien.ac.at (R. Pichler), woltran@dbai.tuwien.ac.at (S. Woltran).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.03.005\f2W. Dvoˇrák et al. / Artificial Intelligence 186 (2012) 1–37problems are given over AFs with at most two incoming and two outgoing attacks. A number of further parameters ishowever, still unexplored. Hence, the second major goal of this paper is to explore the potential of further parametersfor identifying tractable fragments of argumentation. In particular, since AFs are directed graphs, it is natural to considerdirected notions of width to obtain larger classes of tractable AFs. To this end, we investigate the effect of bounded cycle-rank [28] on reasoning in AFs. We show that reasoning remains intractable even if we only consider AFs of cycle-rank 2.Actually, many further directed notions of width exist in the literature. However, it has been recently shown [6,33,31] thatproblems which are hard for bounded cycle-rank remain hard when several other directed variants of the tree-width arebounded. A notable exception is the related notion of clique-width [14] which (in contrast to tree-width) can be directlyextended to directed graphs. Moreover, meta-theorems for clique-width [15] show that Dunne’s result on tractability withrespect to bounded tree-width extend to AFs of bounded clique-width (for details, we refer to [27]).Still, the main focus of this paper is on novel algorithms for decision problems defined over the so-called preferredsemantics of AFs. Roughly speaking, the preferred extensions of an AF are maximal admissible sets of arguments, whereadmissible means that the selected arguments defend themselves against attacks. To be more precise, we present herealgorithms for the following three decision problems.• Credulous acceptance: deciding whether a given argument is contained in at least one preferred extension of a given AF.• Skeptical acceptance: deciding whether a given argument is contained in all preferred extensions of a given AF.• Ideal acceptance: deciding whether a given argument is contained in an admissible set which itself is a subset of eachpreferred extension of a given AF.The problem of ideal acceptance is better known as ideal semantics [21]. To the best of our knowledge, FPT results for idealsemantics have not been established yet, thus the algorithm that we present in the paper provides such a result as a by-product (one could alternatively use Courcelle’s meta-theorem to obtain that result). By its very nature, the running timesof our novel algorithms will heavily depend on the tree-width of the given AF, but are linear in the size of the AF. Thus forAFs of small tree-width, these algorithms are expected to be preferable over standard algorithms from the literature (seee.g. [19,38]).One reason why we have chosen the preferred semantics for our work here is that it is widely used. Moreover, admissi-bility and maximality are prototypical properties common in many other semantics, for instance complete and stable [20],stage [43], and semi-stable [10] semantics. Hence, we expect that the methods developed here can also be extended toother semantics.1.1. Summary of results• We first prove some negative results: we show that reasoning remains intractable in AFs of bounded cycle-rank [28].As has been mentioned above, this negative result carries over to many other directed notions of width. We also showthat the problem of skeptical acceptance is coNP-complete for AFs of cycle-rank 1.• We develop a dynamic programming approach to characterize admissible sets of AFs. The time complexity of our algo-rithm is linear in the size of the AFs (as expected by Courcelle’s Theorem) with a multiplicative constant that is singleexponential in the tree-width (which is in great contrast to algorithms derived via Courcelle’s Theorem). This algorithmcan be directly used to decide the problem of credulous acceptance.• This dynamic programming algorithm is then extended so as to cover also the preferred semantics, and thus to decideskeptical acceptance.• We finally show how to further adapt this algorithm to decide ideal acceptance.1.2. Structure of the paperIn Section 2, we recall some basic notions and results on AFs and discuss some width-measures for graphs. We thenshow in Section 3 some negative results for reasoning in AFs where some parameters of directed graphs are bounded. InSection 4.1, we first develop a dynamic programming approach for credulous acceptance in AFs of bounded tree-width. Thisalgorithm is then extended to cover also preferred semantics in Section 4.2 and adapted to ideal acceptance in Section 4.3.Section 5 provides some final conclusions as well as pointers to related and future work.2. BackgroundIn this section, we first introduce argumentation frameworks and then some graph measures we want to investigate forsuch frameworks.2.1. Argumentation frameworksWe start by introducing (abstract) argumentation frameworks [20], and then recall the preferred as well as the idealsemantics for such frameworks. Afterwards, we highlight some known complexity results for typical decision problemsassociated to such frameworks.\fW. Dvoˇrák et al. / Artificial Intelligence 186 (2012) 1–373Definition 1. An argumentation framework (AF) is a pair F = ( A, R) where A is a set of arguments and R ⊆ A × A is theattack relation. We sometimes use the notation a (cid:2) b instead of (a, b) ∈ R, in case no ambiguity arises. Further, for S ⊆ Aand a ∈ A, we write S (cid:2) a (resp. a (cid:2) S) iff there exists b ∈ S, such that b (cid:2) a (resp. a (cid:2) b). An argument a ∈ A is defended⊕ = {b ∈ A | S (cid:2) b}.by a set S ⊆ A iff for each b ∈ A, such that b (cid:2) a, also S (cid:2) b holds. Finally, for a set S ⊆ A we define SAn AF can naturally be represented as a directed graph.Example 1. Let F = ( A, R) with A = {a, b, c, d, e, f , g} and R = {(a, b), (c, b), (c, d), (d, c), (d, e), (e, g), ( f , e), (g, f )}. Thegraph representation of F is given as follows.We continue with a few basic concepts and the definition of preferred extensions as introduced in Dung’s seminalpaper [20] as well as the concept of ideal sets as proposed by Dung, Mancarella and Toni [21].Definition 2. Let F = ( A, R",
            {
                "entities": [
                    [
                        3863,
                        3891,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "NIH Public AccessAuthor ManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.Published in final edited form as:Neurocomputing. 2011 June 1; 74(12-13): 2184–2192. doi:10.1016/j.neucom.2011.02.014.Physical Activity Recognition Based on Motion in ImagesAcquired by a Wearable CameraHong Zhanga,*, Lu Lia,b, Wenyan Jiab, John D. Fernstromc, Robert J. Sclabassib, Zhi-HongMaod, and Mingui Sunb,d,*a Image Processing Center, Beihang University, Beijing 100191, Chinab Department of Neurosurgery, University of Pittsburgh, PA 15213, USAc Department of Psychiatry, University of Pittsburgh, PA 15261, USAd Department of Electrical and Computer Engineering, University of Pittsburgh, PA 15261, USAAbstractA new technique to extract and evaluate physical activity patterns from image sequences capturedby a wearable camera is presented in this paper. Unlike standard activity recognition schemes, thevideo data captured by our device do not include the wearer him/herself. The physical activity ofthe wearer, such as walking or exercising, is analyzed indirectly through the camera motionextracted from the acquired video frames. Two key tasks, pixel correspondence identification andmotion feature extraction, are studied to recognize activity patterns. We utilize a multiscaleapproach to identify pixel correspondences. When compared with the existing methods such as theGood Features detector and the Speed-up Robust Feature (SURF) detector, our technique is moreaccurate and computationally efficient. Once the pixel correspondences are determined whichdefine representative motion vectors, we build a set of activity pattern features based on motionstatistics in each frame. Finally, the physical activity of the person wearing a camera is determinedaccording to the global motion distribution in the video. Our algorithms are tested using differentmachine learning techniques such as the K-Nearest Neighbor (KNN), Naive Bayesian and SupportVector Machine (SVM). The results show that many types of physical activities can be recognizedfrom field acquired real-world video. Our results also indicate that, with a design of specificmotion features in the input vectors, different classifiers can be used successfully with similarperformances.KeywordsActivity recognition; classification; feature extraction; feature matching; motion histogram;multiscaleI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscript1. IntroductionVideo based activity recognition has been an active field of research in computer vision andmultimedia systems [1-9]. Although numerous algorithms have been developed, a© 2011 Elsevier B.V. All rights reserved.*Corresponding author. dmrzhang@buaa.edu.cn drsun@pitt.edu.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to ourcustomers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review ofthe resulting proof before it is published in its final citable form. Please note that during the production process errors may bediscovered which could affect the content, and all legal disclaimers that apply to the journal pertain.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 2fundamental requirement has been that the target object engaging in a certain activity mustappear in the video, which is not achievable in many practical cases where the object neverappears in the video because the camera can only be mounted on the target object itself.Examples of such objects include a spaceship, an aircraft, a submarine, a vehicle, a robot, ananimal, or a person. For example, if the goal is to study an individual's physical activity overan entire day in a free-living environment, it is unrealistic to track the person with videocameras. Alternatively, with today's technological advancement, a subject can comfortablywear a small camera for the entire day. Although he/she does not appear in the recordedvideo, physical activity can be recognized indirectly by observing the recorded backgroundscene. In general, a specific activity will result in a specific motion of the camera since it ismounted on the human body and the background scene will change accordingly when thecamera is moved.We have been investigating the use of a wearable video device to monitor food intakecontinuously in obese individuals [10]. However, modification of diet (energy input)represents only half of the energy balance equation of the human body. The other half isphysical activity (energy output). A worn device that unobtrusively and automaticallyrecords physical activity will provide a powerful tool for the development of individualizedobesity treatment programs that help people lose weight and keep it off.Wearable sensors that objectively measure body motion and dynamics have been developed[11-14]. One common approach is to use accelerometers attached at multiple locations of thebody to measure both acceleration and orientation [11, 12]. The accuracy of physical activityrecognition by accelerometer-based systems generally improves as the number ofaccelerometers increases. However, the obtrusiveness of such systems makes it inconvenientto wear and use in daily life. We have thus developed a new wearable device, whichcontains a video camera and other sensors, to monitor both food intake and physical activity(see Fig. 1). The device is mounted in front of the chest using a pin, a lanyard or a pair ofmagnets, allowing it to measure the trunk motion of the upper body. While the generaldesign of the device and its food intake measurement function are published elsewhere [10,15, 16], this paper describes the algorithms utilized to process the acquired video data andrecognize several common types of physical motion and activity.Numerous vision algorithms are available for activity recognition using features extractedfrom the observed target directly[2, 3]. Unfortunately, these algorithms do not apply to ourcase where the target (a person) does not appear in the video. The key problem is thereforeto find descriptions of the physical activity in the recorded video without direct observationof the target. These descriptions can be obtained if the following two assumptions aresatisfied: 1) the motion profiles of the activities to be recognized differ from each other, and2) the background scene is rich enough so that sufficient image features can be extractedreliably.We approach the indirect activity recognition problem by investigating camera motion anddeveloping an activity detection scheme based on 2D image features. Considering that thecamera in the wearable device is usually not controlled intentionally and, as a result, theacquired images are often blurry, we match correspondent points between adjacent framesusing multiscale local features. In order to reduce errors in pixel-pair matching, we imposeuniqueness and epipolar constraints which eliminate ambiguous pixel pairs. After thecorrespondence selection process, a motion histogram is defined according to motionvectors obtained from the selected pixel pairs. For each activity video, an accumulation ofmotion vectors is evaluated based on a set of motion histograms to obtain global motioncharacteristics which finally lead to physical activity recognition.Neurocomputing. Author manuscript; available in PMC 2012 June 1.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 3This paper is organized as follows. Section 2 provides an overview of our algorithms.Detailed descriptions of these algorithms are presented in Section 3. Experimental results areprovided in Section 4. In Sections 5 and 6, we summarize our work and discuss futuredirections on physical activity recognition using the wearable camera approach.2. OutlineOur framework for recognizing physical activity is shown in Fig. 2. It consists of three dataprocessing steps: feature extraction, motion representation and activity recognition. In thefirst step, local image features are extracted from which a set of “salient key points” aredetermined. In the second step, we match these key points between neighboring frames. Thematched points define a set of motion vectors. A motion histogram is then defined accordingto these vectors. In the last step, the cumulative motion over all frame pairs is evaluated.Physical activity is recognized based on motion histograms and global motioncharacteristics.3. Methods3.1. Feature ExtractionObtaining reliable correspondences of features is essential in our activity recognition systembecause inaccurate correspondences produce ambiguous motion estimation. We use localimage features which are widely investigated [17-26]. We prefer local features to globalfeatures because the local ones can be detected and represented more easily. The keyproblem here is to find salient points in each image. Shi and Tomasi [22] described a methodcalled “Good Features”, which computes the minimum eigenvalue of the covariance matrixinstead of the cost function defined in the Harris detector [23]; Lowe [17] presented a ScaleInvariant Feature Transform (SIFT) method using scale space analysis. This method isinvariant to scale, orientation and affine distortion [18-20]. Bay et al. [24, 25] proposed aSpeed-Up Robust Features (SURF) method using the 2D Haar wavelet.Although the existing methods have been well studied for activity recognition from directlyrecorded images as the input, these methods usually require these images to have reasonablyhigh quality. In our case, however, the wearable device is uncontrolled and thus the imagesacquired are often blurred. We have found that the blur of our images resulted from manyfactors, and it is hence difficult for us to choose the most suitable model for de-blurring.Occasionally, an incorrect model even aggravates noise. Hence we use a multiscale detectorto capture motion features in an ”overlooking” scale in wh",
            {
                "entities": [
                    [
                        187,
                        215,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "llOPEN ACCESSPerspectiveMemetics and neural models of conspiracy theoriesW1odzis1aw Duch1,*1Department of Informatics, Faculty of Physics, Astronomy and Informatics, and Neurocognitive Laboratory, Center for ModernInterdisciplinary Technologies, Nicolaus Copernicus University, Toru(cid:1)n, Poland*Correspondence: wduch@umk.plhttps://doi.org/10.1016/j.patter.2021.100353THE BIGGER PICTURE Conspiracy theories are widespread. So far, research in this area has been focusedon psychological, sociological, and political science perspectives. Brain processes facilitating formation ofconspiracy theories are largely unknown. In neural systems, a meme may be represented by a quasi-stableassociative memory network attractor state. Creation of memes with numerous fake associations distorts re-lations between stable memory states. Simulations of neural network models trained with competitiveHebbian learning (CHL) on stationary and non-stationary input data show the formation of distorted memorystates. In non-stationary situations, rapid learning with high plasticity followed by stepwise decrease of plas-ticity leads to many states with overlapping attraction basins, distorting patterns in associative memory.Such system-level models may be used to understand conditions under which memplexes with distortedmemory patterns arise, representing deeply settled conspiracy beliefs.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYMemetics has so far been developing in social sciences, but to fully understand memetic processes it shouldbe linked to neuroscience models of learning, encoding, and retrieval of memories in the brain. Attractor neu-ral networks show how incoming information is encoded in memory patterns, how it may become distorted,and how chunks of information may form patterns that are activated by many cues, forming the foundation ofconspiracy theories. The rapid freezing of high neuroplasticity (RFHN) model is offered as one plausiblemechanism of such processes. Illustrations of distorted memory formation based on simulations of compet-itive learning neural networks are presented as an example. Linking memes to attractors of neurodynamicsshould help to give memetics solid foundations, show why some information is easily encoded and propa-gated, and draw attention to the need to analyze neural mechanisms of learning and memory that lead toconspiracies.INTRODUCTIONConspiracy theories are part of a much wider subject: formationof beliefs, creation of memes, distorted memories, twistedworldviews, or in general investigating ways in which learningfails to represent the data faithfully. In recent article by Seitzand Angel ‘‘Belief formation – A driving force for brain evolu-tion,’’1 the authors write: ‘‘The topic of belief has been neglectedin the natural sciences for a long period of time’’. They dividebeliefs into empirical, relational, and conceptual, discussinglarge brain areas involved in the formation of beliefs. Bayesianmodels of belief propagation are used to model details ofperceptual processes and relate them to connectomes.2 Theartificial neural network community has focused on faithfullearning methods, but there is another, neglected side of learningand memory formation. When the training data are not learnedperfectly, what types of errors may one expect, and how willthey influence the performance of an artificial system? Can anal-ysis of artificial systems help to understand how biological brainslearn incoming information, transforming it into memes that arelikely to be transmitted in a distorted form to other brains? Theworld view that we use to guide our behavior is based on anetwork of associative memory states. Consolidation of newmemory states in the neocortex may occur quite quickly if theyare well connected to other memory states.3 Several lines ofresearch lead to this conclusion: animal studies, association ofplaces with items in mnemotechnics, behavioral studies on theuse of schemas for rapid learning, and building of cognitivemaps. Neural models of schemas and sequences of associa-tions may be based on attractor states in neural networks.4Each episodic or semantic memory state is based on activationsof synchronized, distributed networks of brain regions. It isPatterns 2, November 12, 2021 ª 2021 The Author(s). 1This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\fllOPEN ACCESSencoded in relation to the existing activation patterns and maybe modified when new patterns are learned.internalUsing functional magnetic resonance imaging (fMRI) evokedby natural movies, Huth et al.5,6 have created a ‘‘semantic atlas,’’showing patterns of brain activations for categories of hundredsof objects and actions. These patterns are evoked by stimuli thatprovide sufficient cues to recall specific objects, such as bodyparts, animals, furniture, or types of actions. This process maybe described using the language of dynamical systems for net-works of elements representing neurons. The Hopfield network7was the simplest associative memory model encoding informa-tion in activation patterns of network nodes. In such recurrentnetworks,feedback changes activity patterns withtime; this process is referred to as neurodynamics. All kinds ofmemory states (semantic, episodic, procedural, and working)are called attractors of neurodynamics4 because initial patternsof neural network activations are attracted by the network dy-namics toward one of the quasi-stable memory patterns. Usuallyonly a small subset of neurons are highly active in each pattern,synchronizing their activity sending signals through strongIn biologically motivated attractor net-mutual connections.works, memory states are not stable, and neural noise, fatigue,and other processes lead to desynchronization, decrease activ-ity of some neurons, and recruit others, forming different neuralpatterns. Transitions between neural patterns define trajectoryof brain state changes in the space of neural activations. In arti-ficial systems we can visualize it to observe neurodynamics ofmodel networks8 and transform it to dimensions that are mean-ingful at the mental level.9 fMRI scans provide snapshots of thewhole brain activation with temporal resolution of about 1 sand spatial resolution of about 1 mm, while measurement ofelectric potentials using electroencephalographic or magneto-encephalographic techniques provides millisecond temporalresolution but spatial resolution that is less than 1 cm.Seitz et al.10 presented a general theoretical model of forma-tions of empirically grounded and metaphysical beliefs. In theirview,the process of attraction is described by the verb‘‘believing,’’ and the endpoint, the final activation quasi-stablestate, is called a ‘‘belief’’ and is interpreted as a mental construct.Beliefs are based on sensory perception and attribution of a per-sonal value in an emotionally loaded process. High-level formularelates beliefs to incoming signals, ambient noise, current andprevious valuation, learning, and prediction errors. Changes ofneural activation in real brains depend on current knowledgeschemas, history of previous activations (priming), generalemotional state, specific context cues that invoke memories,and many other factors. Transitions that happen frequently in-crease probability of association between different activationpatterns11 and may not only create strong associations butdistort or even completely blend different memories, creatingfalse memories.12,13 Understanding abnormal belief formationin neuropsychological disorders is an important challenge,14but neuropsychiatry needs precise hypotheses and models atthe level of neural networks.In some cases, memories may become easily activated invarious contexts, leading to false associations and schemasthat develop into conspiracy theories. While there is a largebody of literature on conspiracy theories written by historians,philosophers, psychologists, sociologists, or political scientists2 Patterns 2, November 12, 2021Perspective(Routledge has a whole series of books on conspiracy theories;see also the review by Douglas et al.15), our understanding of thebrain mechanisms is completely lacking. The best explanationsthat we have relate beliefs in conspiracy theories to personalitytraits, mental disorders, or the need to find a simple satisfactoryexplanation.Memetics, introduced in the 1976 book The Selfish Gene byRichard Dawkins,16 tried to explain cultural information transferand persistence of certain ideas in societies. Memes may be un-derstood as sequences or information structures that tend toreplicate in a society. Despite great initial popularity of memeticideas, and the desperate need of mathematical theories to un-derpin social science, theories connecting neuroscience andmemetics have never been developed. The Journal of Memeticswas discontinued in 2005 after 8 years of electronic publishing.Memetic ideas were relegated into a set of vague philosophicaland psychological concepts of little interest to neuroscience. Inevolutionary computing, memetic ideas have inspired manynew developments, combining global search with focus on inter-esting local regions.17 The Memetic Computing journal was es-tablished in 1989, a whole series of books on Advances inMemetic Algorithms. Studies in Fuzziness and Soft Computingis published by Springer. Research on memetic computing isfocused on optimization problems, while here we are interestedin the process of formation of memories.The lack of efforts to understand distortions of informationtransmission and memory storage in biological learning systemsis certainly related to the lack of theoretical models, and to theexperimental difficulties in searching for memes in brain activity.McNamara18 has argued that neuroimaging technology may beused to trace memes in the brain and to measure how theychange over time. Following Heylighen and Chie",
            {
                "entities": [
                    [
                        343,
                        371,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Understanding common human driving semanticsfor autonomous vehiclesArticleHighlightsd Reveal human auditory cortex activation during drivingd Discover the hierarchical structure of human drivingunderstandingd Propose a neural-informed semantics-driven drivingunderstanding modeld Address long-term contextual dependency of drivingbehaviorsAuthorsYingji Xia, Maosi Geng, Yong Chen, ...,Bing Zhang, Ziyou Gao,Xiqun (Michael) ChenCorrespondencechenxiqun@zju.edu.cnIn briefAutonomous vehicles will share roadswith human-driven vehicles and bringwith them problems regardingbidirectional understanding of drivingbehavior. Based on cerebral neurologicalfindings from the human process forunderstanding driving, a novel neural-inspired semantics-driven drivingunderstanding model is proposed forautonomous vehicles. The model imitatesthe way humans understand driving andcan interpret long-term driving behaviorevolutions like human drivers.Xia et al., 2023, Patterns 4, 100730June 9, 2023 ª 2023 The Author(s).https://doi.org/10.1016/j.patter.2023.100730ll\fPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSArticleUnderstanding common human drivingsemantics for autonomous vehiclesYingji Xia,1 Maosi Geng,1,2 Yong Chen,1 Sudan Sun,3 Chenlei Liao,1 Zheng Zhu,1,4,10 Zhihui Li,5Washington Yotto Ochieng,6 Panagiotis Angeloudis,6 Mireille Elhajj,6 Lei Zhang,7 Zhenyu Zeng,7 Bing Zhang,7 Ziyou Gao,8and Xiqun (Michael) Chen1,4,9,10,11,*1Institute of Intelligent Transportation Systems, College of Civil Engineering and Architecture, Zhejiang University, Hangzhou 310058, China2Polytechnic Institute & Institute of Intelligent Transportation Systems, Zhejiang University, Hangzhou 310015, China3School of Medicine, Zhejiang University, Hangzhou 310058, China4Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, Hangzhou 310027, China5School of Transportation, Jilin University, Changchun 130022, China6Department of Civil and Environmental Engineering, Imperial College London, South Kensington Campus, London SW7 2AZ, UK7Alibaba Group, Hangzhou 310052, China8School of Traffic and Transportation, Beijing Jiaotong University, Beijing 100044, China9Zhejiang University/University of Illinois Urbana-Champaign (ZJU-UIUC) Institute, Zhejiang University, Haining 314400, China10Zhejiang Provincial Engineering Research Center for Intelligent Transportation, Hangzhou 310058, China11Lead contact*Correspondence: chenxiqun@zju.edu.cnhttps://doi.org/10.1016/j.patter.2023.100730THE BIGGER PICTURE ‘‘Driving like humans’’ is the ultimate goal of autonomous driving. Hence, human-like driving understanding ability is required for autonomous vehicles to better understand the driving be-haviors of surrounding human-driven vehicles. In this study, we investigated human driving neural responseand subsequently built a biologically plausible model to interpret driving behaviors like humans. This studypioneers the design of bio-inspired, human-like autonomous vehicles and can ultimately benefit futureresearch of human-machine interactions.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYAutonomous vehicles will share roads with human-driven vehicles until the transition to fully autonomoustransport systems is complete. The critical challenge of improving mutual understanding between bothvehicle types cannot be addressed only by feeding extensive driving data into data-driven models but byenabling autonomous vehicles to understand and apply common driving behaviors analogous to humandrivers. Therefore, we designed and conducted two electroencephalography experiments for comparingthe cerebral activities of human linguistics and driving understanding. The results showed that driving acti-vates hierarchical neural functions in the auditory cortex, which is analogous to abstraction in linguistic un-derstanding. Subsequently, we proposed a neural-informed, semantics-driven framework to understandcommon human driving behavior in a brain-inspired manner. This study highlights the pathway of fusingneuroscience into complex human behavior understanding tasks and provides a computational neural modelto understand human driving behaviors, which will enable autonomous vehicles to perceive and think like hu-man drivers.INTRODUCTIONAutonomous vehicles (AVs) continue to receive significant atten-tion worldwide because they have the potential to realize a safer,faster, and more efficient mode of transportation. Every day,almost 2,700 people are killed globally in traffic crashes6; fataland non-fatal crash injuries are estimated to cost approximately1.8 trillion US dollars between 2015 and 2030.7 By shiftingvehicle control from the human driver to machines via AVs,driver-related road crashes can be eliminated and thus savePatterns 4, 100730, June 9, 2023 ª 2023 The Author(s). 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSACBDArticleEFigure 1. Four-stage development to understand the driving behavior of AVs(A) The surrounding vehicles are considered moving obstacles without self-consciousness.(B) The velocity of the surrounding vehicles is predicted using probability distribution outputs of discrete choice models.(C) The potential maneuvers of surrounding vehicles are surmised by recurrently applying short-time trajectory prediction models.(D) The intentions of the surrounding vehicles are understood from their contextual driving behaviors.(E) The mechanistic and biological requirements for AV development.lives.8 However, until the transition to fully autonomous transportis completed, AVs will inevitably share roads with human-drivenvehicles. During this transitory phase, AVs and human-driven ve-hicles need to share mutually interactive behaviors.9,10 Given thiscontext, it is impossible to expect every human driver to accom-modate certain traits and attributes of AVs, such as inconsistentor stilted driving behaviors (e.g., aggressive car following, jerk-ing, sudden braking, or unexpected mandatory lane changing).Existing studies have revealed that a lack of transparency inAV decision making creates a psychological barrier that affectshuman drivers’ trust in AVs11; human drivers expect AVs tomimic their driving behaviors to become trustworthy.12 A moreplausible approach is for AVs to acquire the ability to drive likehuman drivers, which would make it easier for other road usersto interpret their driving behaviors and react appropriately. Thiswould subsequently rebuild the driver’s trust and increase thesocial acceptability of AVs.13,14In recent years, various types of AVs were developed andtested in urban road scenarios, and they yielded promising re-sults and applications.15–17 Given that vehicular sensing andnavigation technologies are relatively mature,18 major concernswith AV adoption are related to whether AVs can interact appro-priately with the human-driven vehicles in the surrounding areas.However, research studies on understanding common drivingbehaviors and designing AVs to operate while following hu-man-like principles or brain-inspired mechanisms remain lack-ing. Therefore, we developed a method to understand AV drivingbehaviors as shown in Figure 1, where the red vehicle representsan AV and the blue vehicles represent the surrounding human-driven vehicles in a typical driving scenario.Unlike the classical vehicular trajectory prediction and routeplanning models19–21 widely accepted in the robotics field(Figure 1A) or the various discrete-choice driving models22,232 Patterns 4, 100730, June 9, 2023developed in the traffic engineering discipline (Figure 1B), recentresearch24–26 showed that subjective individual human drivingfactors are critical and cannot be neglected in the developmentof human-like AVs (Figure 1C). Unfortunately, as human drivingbehaviors evolve with an indefinite temporal dependency,state-of-the-art machine learning-based methods that partiallyimitate the nature of the human driving decision-making processmay lose the ability to adapt and generalize. For example, stand-alone data-driven intention recognizers (e.g., deep neural net-works) employed in machine learning-based methods are likelyto be trapped in the following dilemma: those with increasedtemporal inputs carry a more significant risk of overfitting localfeatures and the output confusing driving intentions, whereasthose with shorter temporal dependencies are too myopic to fullyunderstand the driving intentions of surrounding drivers, such ashuman drivers.Understanding common human driving behaviors that followthe human cerebral driving thinking mechanisms of humandrivers (Figure 1D) is necessary to address this dilemma.27–29As human driving behaviors are generated by humans ratherthan machines, AV development needs to be mechanisticallyand biologically plausible (Figure 1E).Our research is motivated by the fact that talking while drivingcan cause severe distractions because both behaviors or activ-ities share the same cerebral resources30,31 (right parietal re-sources32 and working memory in the prefrontal cortex33,34).Therefore, we attempt to fuse neuroscience and robotics in aneuroengineering manner35 in this study. To this end, we de-signed two separate electroencephalography (EEG) experi-ments to reveal the formation of cerebral driving thinking andevolution using well-studied linguistic analyses. We subse-quently present a semantics-driven method to understand com-mon driving behaviors for developing AVs in a brain-inspired\fPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous ",
            {
                "entities": [
                    [
                        1020,
                        1048,
                        "DOI"
                    ],
                    [
                        1204,
                        1232,
                        "DOI"
                    ],
                    [
                        2604,
                        2632,
                        "DOI"
                    ],
                    [
                        5265,
                        5293,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNIH Public AccessAuthor ManuscriptPattern Recognit. Author manuscript; available in PMC 2012 June 1.Published in final edited form as:Pattern Recognit. 2012 June 1; 45(6): 2050–2063. doi:10.1016/j.patcog.2011.04.033.Phase Ambiguity Correction and Visualization Techniques forComplex-Valued ICA of Group fMRI DataPedro A. Rodrigueza,1,*, Vince D. Calhounb, and Tülay Adalıa,1,**Vince D. Calhoun: vcalhoun@mrn.orga University of Maryland, Baltimore County, Department of CSEE, Baltimore, MD 21250b The Mind Research Network, Albuquerque, NM 87106 and University of New Mexico,Department of ECE, Albuquerque, NM 87131AbstractAnalysis of functional magnetic resonance imaging (fMRI) data in its native, complex form hasbeen shown to increase the sensitivity both for data-driven techniques, such as independentcomponent analysis (ICA), and for model-driven techniques. The promise of an increase insensitivity and specificity in clinical studies, provides a powerful motivation for utilizing both thephase and magnitude data; however, the unknown and noisy nature of the phase poses a challenge.In addition, many complex-valued analysis algorithms, such as ICA, suffer from an inherent phaseambiguity, which introduces additional difficulty for group analysis. We present solutions forthese issues, which have been among the main reasons phase information has been traditionallydiscarded, and show their effectiveness when used as part of a complex-valued group ICAalgorithm application. The methods we present thus allow the development of new fully complexdata-driven and semi-blind methods to process, analyze, and visualize fMRI data.We first introduce a phase ambiguity correction scheme that can be either applied subsequent toICA of fMRI data or can be incorporated into the ICA algorithm in the form of prior informationto eliminate the need for further processing for phase correction. We also present a Mahalanobisdistance-based thresholding method, which incorporates both magnitude and phase informationinto a single threshold, that can be used to increase the sensitivity in the identification of voxels ofinterest. This method shows particular promise for identifying voxels with significantsusceptibility changes but that are located in low magnitude (i.e. activation) areas. We demonstratethe performance gain of the introduced methods on actual fMRI data.Keywordsphase; fMRI; ICA; group analysis; ambiguity; visualization1. IntroductionFunctional magnetic resonance imaging (fMRI) is a technique that provides the opportunityto study brain function non-invasively and is a powerful tool utilized in both research andclinical arenas since the early 90s [1]. FMRI data are natively acquired as complex-valuedspatio-temporal images; however, usually only the magnitude images are used for analysis.The phase images are usually discarded, primarily because their unfamiliar and noisy natureposes a challenge for successful study of fMRI [2]. However, recent studies have identified*Principal corresponding author: rodripe1@umbc.edu (Pedro A. Rodriguez ). **Corresponding author: adali@umbc.edu (Tülay Adalı).1This work is supported by the NSF grants NSF-CCF 0635129 and NSF-IIS 0612076.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptRodriguez et al.Page 2the presence of novel information in the phase, which can be utilized to better understandbrain function [3, 4, 5, 6, 7, 8]. One exciting implication of this fact would be increasedsensitivity in diagnosis when studying differences in control and patient groups when thephase information is utilized.Both model-based approaches, such as general linear model (GLM), and data-drivenapproaches, such as independent component analysis (ICA), can be used for studyingcomplex-valued fMRI data. By using a simple generative model based on linear mixing,ICA can minimize the constraints imposed on the temporal—or the spatial—dimension ofthe fMRI data, and therefore is particularly attractive for studying paradigms for whichreliable models of brain activity are not available. Hence, ICA is particularly promising forthe analysis of the phase of the complex-valued fMRI data, since it does not make anystrong assumptions about its unfamiliar nature. A number of recent applications of ICA tocomplex fMRI data have demonstrated the promise of ICA for the analysis of fMRI data inits native complex form [2, 9, 10, 11], but have also underlined the importance ofpreprocessing and visualization techniques to fully take advantage of the additionalinformation presented by the phase.Successful application of both the model-based and data-driven approaches for analysis ofcomplex-valued fMRI data requires the development of pre-and post-processing techniquesthat can help exploit the phase information and therefore improve the usability of thesealgorithms. In this paper, we introduce a phase ambiguity correction scheme and aMahalanobis distance-based thresholding method that can significantly help improve theanalysis and visualization of complex-valued fMRI data. We show the effectiveness of thesemethods when used in the analysis of actual fMRI group data using a complex-valued ICAalgorithm. The main idea for phase ambiguity correction with some preliminary results weregiven in [12]. These methods, together with a previously developed complex-valued de-noising algorithm [13, 14], form a successful framework that allows one to utilize the phaseinformation in fMRI data to enhance the sensitivity and specificity of ICA of fMRI studies.Fig. 1, shows the steps of the framework, including the two methods introduced in thispaper, and the de-noising method. The phase correction step can be incorporated directlyinto the ICA step as we discuss in the paper. The Mahalanobis distance-based visualizationand de-noising methods can be used for model-based approaches, as well as data-drivenones. The rest of the steps in Fig. 1 are typical steps in the pipeline, such as using PCA fordata dimensionality reduction and the use of the MATLAB Toolbox for StatisticalParametric Mapping (SPM)2 for motion correction.In Section 3, we describe the phase ambiguity correction scheme that solves the inherentphase scaling ambiguity found in complex-valued data-driven algorithms, such as ICA. Thephase ambiguity correction scheme can be either applied subsequent to ICA of fMRI or canbe incorporated into the ICA algorithm in the form of prior information—when available—to eliminate the need for further processing.In Section 4, we describe the novel visualization technique, based on the Mahalanobisdistance-based metric, which can be used to identify voxels of interest in complex-valuedfMRI images and which provides a single threshold based upon both the phase andmagnitude data. In Section 5, we present results of the methods introduced here whenapplied to actual fMRI group data.2SPM, URL: http://www.3l.ion.ucl.ac.uk/spm/software/spm5Pattern Recognit. Author manuscript; available in PMC 2012 June 1.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptRodriguez et al.2. Background2.1. Spatial ICA of Complex-valued fMRI DataPage 3Independent component analysis has emerged as an attractive analysis tool for discoveringhidden factors in observed data and has been successfully applied for data analysis in a widearray of applications [15, 16, 17]. Especially in the case of fMRI analysis, it has provenparticularly fruitful [18, 19].In the ICA analysis of fMRI data, we assume independence of spatial brain activations (forspatial ICA), and write the complex ICA model aswhere x is the random vector of observation, A is the mixing matrix and s is the independentsource vector. For the volume image data of v voxels and n time points, we can write X =[x1, x2, ···, xv] and S = [s1, s2, ···, sv] where X ∈ ℂn×v is formed using the fMRI data suchthat the kth row is formed by flattening the volume image data of v voxels into a row. Therows of X are indexed as a function of time from one to n and the kth row vector of S is thekth spatial map. The assumed kth mixing column vector of A represents the time course forthe kth spatial map.ICA achieves demixing by estimating a weight matrix W such that ŝ = Wx = PΛs. Here, P, apermutation matrix, represents the permutation ambiguity and Λ, a diagonal matrix,represents the scaling ambiguity of ICA, which has a magnitude and phase term in thecomplex-valued implementation of ICA.Thus, spatial ICA finds systematically nonoverlapping, temporally coherent brain regionswithout constraining the temporal domain. A principal advantage of this approach is itsapplicability to cognitive paradigms for which detailed a priori models of brain activity arenot available. Following its first application by McKeown et al. [19], ICA has beensuccessfully used in a number of exciting fMRI applications, especially in those that haveproven challenging with the standard regression-type approaches. These includeidentification of various signal types (e.g., task-related, transiently task-related, andphysiology-related signals) in the spatial or temporal domain, analysis of multisubject fMRIdata, incorporation of a priori information to improve the estimates, clinical applications,and for the analysis of complex-valued fMRI data. A comprehensive review of ICAapproaches for fMRI data along with main references in the area is given in [20] and in [21].In spatial ICA, the number of estimated independent components (ICs) correspond to thenumber of time-points, which in general are in the order of 100s, and for temporal ICA, theycorrespond to the number of voxels that are much higher. Hence, in both cases, a principalcomponent analysis (PCA) stage traditionally precedes the ICA algorithm that is used towhiten the data and to determine the effective model order. Information theoretic criteriasuch as Akaike’s information criterion, and the minimum description length (MDL)—or theBayesian ",
            {
                "entities": [
                    [
                        253,
                        281,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "1 Three practical field normalised alternative indicator formulae for research evaluation1 Mike Thelwall, Statistical Cybermetrics Research Group, University of Wolverhampton, UK. Although altmetrics and other web-based alternative indicators are now commonplace in publishers’ websites, they can be difficult for research evaluators to use because of the time or expense of the data, the need to benchmark in order to assess their values, the high proportion of zeros in some alternative indicators, and the time taken to calculate multiple complex indicators. These problems are addressed here by (a) a field normalisation formula, the Mean Normalised Log-transformed Citation Score (MNLCS) that allows simple confidence limits to be calculated and is similar to a proposal of Lundberg, (b) field normalisation formulae for the proportion of cited articles in a set, the Equalised Mean-based Normalised Proportion Cited (EMNPC) and the Mean-based Normalised Proportion Cited (MNPC), to deal with mostly uncited data sets, (c) a sampling strategy to minimise data collection costs, and (d) free unified software to gather the raw data, implement the sampling strategy, and calculate the indicator formulae and confidence limits. The approach is demonstrated (but not fully tested) by comparing the Scopus citations, Mendeley readers and Wikipedia mentions of research funded by Wellcome, NIH, and MRC in three large fields for 2013-2016. Within the results, statistically significant differences in both citation counts and Mendeley reader counts were found even for sets of articles that were less than six months old. Mendeley reader counts were more precise than Scopus citations for the most recent articles and all three funders could be demonstrated to have an impact in Wikipedia that was significantly above the world average. 1 Introduction Citation analysis is now a standard part of the research evaluation toolkit. Citation-based indicators are relatively straightforward to calculate and are inexpensive compared to peer review. Cost is a key issue for evaluations designed to inform policy decisions because these tend to cover large numbers of publications but may have a restricted budget. For example, reports on government research policy or national research performance can include citation indicators (e.g., Elsevier, 2013; Science-Metrix, 2015), as can programme evaluations by research funders (Dinsmore, Allen, & Dolby, 2014). Although funding programme evaluations can be conducted by aggregating end-of-project reviewer scores (Hamilton, 2011), this does not allow benchmarking against research funded by other sources in the way that citation counts do. The increasing need for such evaluations is driven by a recognition that public research funding must be accountable (Jaffe, 2002) and for charitable organisations to monitor their effectiveness (Hwang & Powell, 2009). The use of citation-based indicators has many limitations. Some well discussed issues, such as the existence of negative citations, systematic failures to cite important influences and field differences (MacRoberts & MacRoberts, 1996; Seglen, 1998; MacRoberts & MacRoberts, 2010), can be expected to average out when using appropriate indicators and comparing large enough collections of articles (van Raan, 1998). Other 1 Thelwall, M. (2017). Three practical field normalised alternative indicator formulae for research evaluation. Journal of Informetrics, 11(1), 128–151. doi: 10.1016/j.joi.2016.12.002 ©2016 This manuscript version is made available under the CC-BY-NCND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/                             \f2 problems are more difficult to deal with, such as language biases within the citation databases used for the raw data (Archambault, Vignola-Gagne, Côté, Larivière, & Gingras, 2006; Li, Qiao, Li, & Jin, 2014). More fundamentally, the ultimate purpose of research, at least from the perspective of many funders, is not to understand the world but to help shape it (Gibbons, Limoges, Nowotny, Schwartzman, Scott, & Trow, 1994). An important limitation of citations is therfore that they do not directly measure the commercial, cultural, social or health impacts of research. This has led to the creation and testing of many alternative types of indicators, such as patent citation counts (Jaffe, Trajtenberg, & Henderson, 1993; Narin, 1994), webometrics/web metrics (Thelwall, & Kousha, 2015a) and altmetrics/social media metrics (Priem, Taraborelli, Groth, & Neylon, 2010; Thelwall, & Kousha, 2015b). These indicators can exploit information created by non-scholars, such as industrial inventors’ patents, and may therefore reflect non-academic types of impacts, such as commercial value. A practical problem with many alternative indicators (i.e., those not based on citation counts) is that there is no simple cheap source for them. It can therefore be time-consuming or expensive for organisations to obtain, say, a complete list of the patent citation counts for all of their articles. This problem is exacerbated if an organisation needs to collect the same indicators for other articles so that they can benchmark their performance against the world average or against other similar organisations. Even if the cost is the same as for citation counts, alternative indicators need to be calculated in addition to, rather than instead of, citation counts (e.g., Dinsmore, Allen, & Dolby, 2014; Thelwall, Kousha, Dinsmore, & Dolby, 2016) and so their costs can outweigh their value. This can make it impractical to calculate a range of alternative indicators to reflect different types of impacts, despite this seeming to be a theoretically desirable strategy. The problem is also exacerbated by alternative indicator data usually being much sparser than citation counts (Kousha & Thelwall, 2008; Thelwall, Haustein, Larivière, & Sugimoto, 2013; Thelwall & Kousha, 2008). For example, in almost all Scopus categories, over 90% of articles have no patent citations (Kousha & Thelwall, in press-b). These low values involved make it more important to use statistical methods to detect whether differences between groups of articles are significant. Finally, the highly skewed nature of citation counts and most alternative indicator data causes problems with simple methods of averaging to create indicators, such as the arithmetic mean, and complicate the task of identifying the statistical significance of differences between groups of articles. This article addresses the above problems and introduces a relatively simple and practical strategy to calculate a set of alternative indicators for a collection of articles in an informative way. The first component of the strategy is the introduction of a new field normalisation formula, the Mean Normalised Log-transformed Citation Score (MNLCS) for benchmarking against the world average. As argued below, this is simpler and more coherent than a previous similar field normalisation approach to deal with skewed indicator data. The second component is the introduction of a second new field normalisation formula, the Equalised Mean-based Normalised Proportion Cited (EMNPC), that targets sparse data, and an alternative, the Mean-based Normalised Proportion Cited (MNPC). The third component is a simple sampling strategy to reduce the amount of data needed for effective field normalisation. The final component integrated software environment for collecting and analysing the data so that evaluators can create their own alternative indicator reports for a range of indicators with relative ease. The methods are illustrated with a comparative evaluation of the impact of the research of three large is a single, \f3 medical funders using three types of data: Scopus citation counts; Mendeley reader counts; and Wikipedia citations. 2 Mean Normalised Log-transformed Citation Score The citation count of an article must be compared to the citation counts of other articles in order to be assessed. The same is true for collections of articles and a simple solution would be to calculate the average number of citations per article for two or more collections so that the values can be compared. This is a flawed approach for the following reasons that have led to the creation of improved methods. Older articles tend to be more cited than younger articles (Wallace, Larivière, & Gingras, 2009) and so it is not fair to compare averages between sets of articles of different ages. Similarly, different fields attract citations at different rates and so comparing averages between sets of articles from different mixes of fields would also be unfair (Schubert & Braun, 1986). One solution would be to segment each collection of articles into separate sets, one for each field and year, and only compare corresponding sets between collections. Although this may give useful fine grained information, it is often impractical because each set may contain too few articles to reveal informative or statistically significant differences. The standard solution to field differences in citation counts is to use a field (and year) normalised indicator. The Mean Normalised Citation Score (MNCS), for example, adjusts each citation count by dividing it by the average for the world in its field and year. After this, the arithmetic mean of the normalised citation counts is the MNCS value (Waltman, van Eck, van Leeuwen, Visser, & van Raan, 2011ab). This can reasonably be compared between different collections of articles or against the world average, which is always exactly 1, as long as all articles are classified in a single field. If some articles are in multiple fields then weighting articles and citations with the reciprocal of the number of fields containing the article ensures that the world average is 1 (Waltman et al., 2011a). A limitation of the MNCS is that the arithmetic mean is inappropriate for citation counts and most alternative indicators because they are hi",
            {
                "entities": [
                    [
                        3480,
                        3505,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNIH Public AccessAuthor ManuscriptComput Med Imaging Graph. Author manuscript; available in PMC 2013 October 01.Published in final edited form as:Comput Med Imaging Graph. 2012 October ; 36(7): 542–551. doi:10.1016/j.compmedimag.2012.06.004.Improving the Correction of Eddy Current-Induced Distortion inDiffusion-Weighted Images by Excluding Signals from theCerebral Spinal FluidWei Liua,b, Xiaozheng Liua,b, Guang Yanga, Zhenyu Zhoub, Yongdi Zhouc, Gengying Lia,Marc Dubind, Ravi Bansalb, Bradley. S. Petersonb, and Dongrong Xub,*aKey Laboratory of Brain Functional Genomics, Ministry of Education & Shanghai Key Laboratoryof Brain Functional Genomics, Shanghai Key Laboratory of Magnetic Resonance, East ChinaNormal University, Shanghai, China 200062bMRI Unit, Columbia University Dept of Psychiatry, New York State Psychiatric Institute, NYSPIUnit 74, 1051 Riverside Drive, New York, NY 10032, U.S.AcDepartment of Neurosurgery, Johns Hopkins University, Baltimore, MD 21287, U.S.AdWeill Cornell Medical College, Cornell University, New York, NY 10065, U.S.AAbstractIterative Cross-Correlation (ICC) is the most popularly used schema for correcting eddy current(EC)-induced distortion in diffusion-weighted imaging data, however, it cannot process dataacquired at high b-values. We analyzed the error sources and affecting factors in parameterestimation, and propose an efficient algorithm by expanding the ICC framework with a number oftechniques: (1) Pattern recognition for excluding brain ventricles; (2) ICC with the extractedventricle for parameter initialization; (3) Gradient-based Entropy Correlation Coefficient (GECC)for optimal and finer registration. Experiments demonstrated that our method is robust with highaccuracy and error tolerance, and outperforms other ICC-family algorithms and popularapproaches currently in use.Keywordsdiffusion weighted imaging; diffusion tensor imaging; eddy current; distortion correction; iterativecross-correlation; mutual information1. IntroductionDiffusion Tensor Imaging (DTI) is a powerful imaging technique for the non-invasivecharacterization of the microstructure of normal and pathological tissues. DTI data,however, are vulnerable to geometric distortion caused by eddy current (EC). DTI datatypically are acquired using echo-planar imaging (EPI) pulse sequences. Consequently,rapid switches between the strong gradients of diffusion-sensitizing magnetic fields© 2012 Elsevier Ltd. All rights reserved.*Corresponding author: Dongrong Xu Ph.D., 1051 Riverside Drive, NYSPI Unit 74, New York, NY 10032, Tel. (212)543-5495 Fax.(212)543-0522, dx2103@columbia.edu.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to ourcustomers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review ofthe resulting proof before it is published in its final citable form. Please note that during the production process errors may bediscovered which could affect the content, and all legal disclaimers that apply to the journal pertain.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptLiu et al.Page 2inevitably create residual gradients that produce EC-induced distortion. Often the bandwidthof the phase-encoding direction is routinely much narrower than the other two (readout-encoding and frequency-encoding) directions, making the phase-encoding direction of thediffusion-weighted (DW) image more sensitive to contamination of EC-induced distortionscaused by the DW gradient. Furthermore, because the amplitude and direction of thediffusion gradients determine the magnitude of the EC-induced distortion, DW imaging(DWI) data acquired along different directions of gradient contain different distortion.Misregistration among DWI data along differing gradient directions will produce errors inthe reconstruction of the diffusion tensors in virtually every voxel of an image, andsubsequently in the derived measurements. These include the apparent diffusion coefficient(ADC)[1], fractional anisotropy (FA)[2] or ellipsoidal area ratio (EAR)[3], and theprocedure of fiber tracking. Therefore, EC-induced distortions must be corrected in all DWimages before a diffusion tensor is reconstructed. Many algorithms have been developed forcorrecting EC-induced distortions, and can be classified into three categories. The firstcategory are those algorithms that rely on improving MRI pulse sequences, including thosecompensating ECs by changing the shape of the gradient amplitude envelope [4] and thosedesigning special pulse sequences [5–7]. For example, the bipolar gradient imaging schema[6] minimized EC-induced distortion by using pairs of bipolar gradients, which requires aprolonged diffusion-weighted time and thus results in an obvious signal attenuation due totransverse relaxation. Most investigators, however, do not have easy access to theproprietary computer codes required to program pulse sequences.The second category of algorithms for correcting EC-induced distortions generally operateson DWI data in k-space [8–10]. The processing and computations involved in the k-spacecorrection algorithms, however, are often complex. For example, one representative work[10] proposed first measuring the phase evolution caused by EC fields along a set ofreference DW gradient directions that are parallel to the coordinate axes, using thesemeasurements to estimate the true phase evolution along particular spatial directions otherthan those that are parallel to the coordinate axes, and then using those estimates to removeEC-induced distortions from the k-space data. The third category refers to post-processingmethods that usually coregister the DW images to a reference image. In most cases, thereference image is one set of the baseline images that is in theory acquired without theapplication of a DW gradient and is therefore considered to be free of EC-induceddistortions by DW gradients. The methods in this third category are appealing because theyare generally flexible in permitting the correction of imaging data off-line.Among these various post-processing methods, the Iterative Cross-Correlation (ICC)algorithm [11], is one of the most popularly adopted. It estimates EC-induced distortion inDW images by cross-correlating the DWI data with the reference images along the phase-encoding direction. Unfortunately, ICC is known to correct accurately only DWI data thatare acquired at b-values lower than 300s/mm2 [12]. This is because DW signals derivingfrom the regions of cerebrospinal fluid (CSF) within the images, when measured at higher b-values, are much weaker than those measured in the reference images, thereby underminingthe accuracy of the cross-correlation of those DW images with the reference [11]. A numberof remedies have been proposed to address this limitation of the ICC algorithm [12–15],such as using a fluid attenuated inversion recovery (FLAIR) sequence to suppress signalsfrom CSF [13] or acquiring an additional set of images using gradients of opposite polarityto estimate the EC-induced distortion present in the original dataset [15]. These remedies,however, generally require either the acquisition of additional imaging data or themanipulation of specific pulse sequences, thereby significantly increasing the alreadylengthy scan time. Moreover, the need to acquire additional imaging data means that theserevised ICC methods no longer involve only post-processing procedures that can be at theconvenience of the investigator off-line.Comput Med Imaging Graph. Author manuscript; available in PMC 2013 October 01.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptLiu et al.Page 3Because the primary determinant of the errors in estimating distortion using the ICCalgorithm derives from the difference in image contrast between the reference and DWimages in the regions containing CSF, excluding the signal of the CSF when estimatingdistortion should immediately and dramatically improve the performance of an ICC-basedalgorithm for correcting EC-induced distortion in DWI data. Compared with the currentlyavailable methods for suppressing signals from CSF [13], this purely postprocessingapproach would not require extra pulse sequence or imaging data.We therefore propose to improve the ICC algorithm by excluding the CSF signal from thedistortion estimation. We first segment the brain and coarsely identify the regions of CSF(including both cortical and ventricular CSF) in a binary mask that excludes the CSFregions. This mask is then used within the conventional ICC algorithm to estimate the initialparameters of distortion. As the segmentation step may not have accurately outlined theregions of CSF, we then use Mutual Information (MI) instead of cross-correlation in the costfunction to refine the estimation of distortion for coregistering the reference with the DWimages. We use MI because it has been shown to be a robust and accurate measure ofsimilarity for registration of multimodal images in many studies [16–19]. With this measure,we use a 3-paramter affine transformation and an algorithm called Limited memory BroydenFletcher Goldfarb Shanno with bound constraint (L-BFGS-B) to find the optimal parametersfor correcting distortion (see the Method section)[20]. L-BFGS-B algorithm can performoptimization without computing the Hessian matrix, which is known to be difficult to obtainin numerical calculation [20].2. PreliminaryIterative Cross-CorrelationThe ICC model estimates distortion parameters by calculating the cross-correlation betweenthe DW and reference images along each corresponding column of their reconstructedimages at a higher resolution. The distortion is characterized by three parameters, M, T andS within the plane of an image slice (for convenience we use the notation that the image liesin the XY plane, with X, Y, and Z b",
            {
                "entities": [
                    [
                        273,
                        306,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptCurr Opin Syst Biol. Author manuscript; available in PMC 2022 December 01.Published in final edited form as:Curr Opin Syst Biol. 2021 December ; 28: . doi:10.1016/j.coisb.2021.100363.Design and development of engineered receptors for cell and tissue engineeringShwan B. Javdan1, Tara L. Deans1,*1Department of Biomedical Engineering, University of Utah, Salt Lake City, UTAbstractAdvances in synthetic biology have provided genetic tools to reprogram cells to obtain desired cellular functions that include tools to enable the customization of cells to sense an extracellular signal and respond with a desired output. These include a variety of engineered receptors capable of transmembrane signaling that transmit information from outside of the cell to inside when specific ligands bind to them. Recent advances in synthetic receptor engineering have enabled the reprogramming of cell and tissue behavior, controlling cell fate decisions, and providing new vehicles for therapeutic delivery.KeywordsSynthetic biology; receptor engineering; synthetic receptors; cell and tissue engineeringIntroductionA traditional focus of synthetic biologists is to apply engineering principles to reprogram cells with desired functions by assembling individual genetic parts into more complex gene circuits to produce switches [1–6], oscillators [7–10], and biosensors [11–16]. A newer approach is to engineer custom receptors to change which extracellular cues cells recognize to produce a desired cellular response. The concept that “the whole is greater than the sum of its parts” has become ubiquitous across a variety of disciplines, especially in systems biology where small-scale molecular interactions are engineered in ways that impact large-scale cellular and tissue functions. The advent of synthetic biology has enabled the engineering of cells and tissues to perform novel biological behaviors, often with the use of genetic circuits and designer synthetic receptors [17–22]. Additionally, cellular *Corresponding author (tara.deans@utah.edu). Conflict of interest statement:Nothing declared.Declaration of interestsThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptJavdan and DeansPage 2reprogramming has been utilized for controlling cell fate decisions [23–26], as well as for drug delivery applications [27–31].Customizing cell sensing with a triggered biological response has enabled an entirely new therapeutic approach that can be specifically targeted to sites of disease or injury. The clinical potential of cell therapy became clear with the recent FDA-approval of chimeric antigen receptor (CAR)-T cell therapy for large B-cell lymphoma [32,33], a landmark achievement that represented a new paradigm in the treatment of human diseases. In short, patients’ T cells are engineered to produce a genetically modified receptor that endows them with the ability to target specific cancer proteins [20,34,35]. These efforts have highlighted how the engineering of receptors can lead to new cell signaling pathways beyond those found in nature. In this review, we discuss recent advances among several major classes of engineered receptors (Figure 1) as well as compare the methods used in their design and development. We also highlight the cell and tissue types modified by these engineered receptors, some potential challenges to future progress, as well as potential avenues for further synthetic receptor development.Receptors activated solely by synthetic ligands (RASSLs)G-protein coupled receptors (GPCRs) are a group of membrane receptors in eukaryotes that bind to a diverse number of extracellular molecules that cause a conformational change in the receptor, triggering second messenger molecules that initiate and coordinate intracellular signaling pathways. These receptors act as signaling switches for a vast array of physiological responses in the body, making them popular receptors for engineering synthetic signaling systems [36–38]. Many studies have sought to engineer GPCRs to be activated by pharmacologically inert small molecule drugs, a family collectively known as receptors activated solely by synthetic ligands (RASSLs) (Figure 1a). These engineered receptors have taken many forms, primarily designer receptors exclusively activated by designer drugs (DREADDs) [39–41].DREADDs were initially developed by directed molecular evolution of human muscarinic receptors, which are involved in the parasympathetic nervous system, as a tool to study receptor-specific functions [39], and have since expanded to modulate the downstream GPCR pathways for several tissue engineering applications. The methods for designing novel RASSLs have spanned approaches in directed molecular evolution, directed mutagenesis, and structure-guided chimera development (Figure 2a–c). The earliest DREADDs were generated in yeast (Figure 2a) [39,42], however, more modern approaches have since been developed in mammalian cells, such as the Viral Evolution of Genetically Actuating Sequences (VEGAS) platform, which paves the way for evolving mammalian GPCRs that otherwise could not be successfully expressed in yeast [43]. Another common design approach has involved the substitution of conserved residues in the GPCR binding pockets with alanines by site-directed mutagenesis, allowing for fine tuning of the receptor’s binding affinity and potency (Figure 2b) [44,45]. Alternatively, chimeric GPCRs have been developed by structure-guided protein design principles, for example, the intracellular loops of one GPCR have been replaced with another while keeping the extracellular and transmembrane domains intact (Figure 2c) [46]. This approach allowed for the generation of Curr Opin Syst Biol. Author manuscript; available in PMC 2022 December 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptJavdan and DeansPage 3GPCRs with a unique input, in this case light instead of a ligand, while retaining the native receptor’s output signals inside the cell.DREADDs have recently been used for the synthetic control of chondrocyte calcium signaling in order to enhance the compositional and mechanical properties of engineered cartilage [47]. Additionally, they have been engineered as ultrasound-responsive neuromodulations for the noninvasive control of memory formation in the mouse hippocampus [48], and in hepatocytes to stimulate cyclic AMP signaling and control glucose levels to study hyperglycemia [49]. Finally, DREADD-based therapeutics have been proposed for a variety of diseases, including Parkinson’s disease and drug addiction [40,50,51]. Data from these studies suggested that pharmacogenetics may enhance overall cellular signaling in reprogrammed cells, and could be used in cell replacement therapy for patients whose own cells have signaling deficiencies. Overall, DREADDs are an exciting avenue for synthetic biologists to rewire endogenous cellular signaling pathways in response to synthetic ligands to obtain desired cell behavior. For example, these receptors have been engineered to direct the migration of cells in response to the inert drug-like molecule, clozapine-N-oxide (CNO) [52]. This work opens the possibility to engineer therapeutic cells with the capabilities of directing therapeutic cells to a user-specified location in the body.Chimeric antigen receptors (CARs)Chimeric antigen receptors (CARs) are a class of synthetic receptors that combine antigen­specificity and T cell (CAR-T) activating properties in a single fusion molecule [53,54]. CAR-T cells were the first FDA-approved genetically-modified cell-based therapy to induce complete remission of hematological malignancies in some patients where traditional chemotherapy has failed [32,33]. CARs are fusion proteins with an extracellular antigen recognition domain typically comprised of single-chain variable fragments (scFvs) from a monoclonal antibody, a spacer/hinge region, a transmembrane domain, and an intracellular signaling domain (Figure 1b) [53,55,56]. In contrast with many other synthetic receptors, CARs are often engineered by rational design principles, and are created as chimeric proteins (Figure 2c). Computational modeling and docking simulation data have also aided in the design and development of novel CARs, improving their overall specificity (Figure 2d) [57–59].Since their inception, there have been multiple generations of CARs, with the first generation lacking any costimulatory domains resulting in the cells not proliferating or surviving upon repeated antigen exposure. However, once costimulatory properties were incorporated into subsequent generations of CARs, they were shown to have a greater strength of antigen-induced signaling that also enabled T cell proliferation and survival upon repeated exposure to antigen [53,56]. Additional engineering of CARs have also been reported that can control on-off switches to enhance T cell function [60,61], contain therapeutic payloads [62], and implement Boolean operations to improve on-target off-tumor toxicity [63–66]. Most recently, the field has seen the development of universal CARs to allow for a variety of antigens to be targeted without requiring the immune cells to be re-engineered [61,66,67].Curr Opi",
            {
                "entities": [
                    [
                        253,
                        280,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "© <2021>. This manuscript version is made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/     The definitive publisher version is available online at https://doi.org/ 10.1016/j.knosys.2021.106994 \fPlease cite as: Zhang, Y., Wu, M., Tian, G. Y., Zhang, G. & Lu, J. 2021, Ethics and privacy of artificial intelligence: Understandings from bibliometrics, Knowledge-based Systems, DOI: 10.1016/j.knosys.2021.106994 Ethics and privacy of artificial intelligence: Understandings from bibliometrics Yi Zhang1, Mengjia Wu1, George Yijun Tian2, Guangquan Zhang1, Jie Lu1 1Australian Artificial Intelligence Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Australia 2Faculty of Law, University of Technology Sydney, Australia Email: yi.zhang@uts.edu.au; mengjia.wu@student.uts.edu.au; yijun.tian@uts.edu.au; guangquan.zhang@uts.edu.au; jie.lu@uts.edu.au ORCID: 0000-0002-7731-0301 (Yi Zhang); 0000-0003-3956-7808 (Mengjia Wu); 0000-0003-4472-5428 (George Yijun Tian); 0000-0003-3960-0583 (Guangquan Zhang); 0000-0003-0690-4732 (Jie Lu). Abstract Artificial intelligence (AI) and its broad applications are disruptively transforming the daily lives of human beings and a discussion of the ethical and privacy issues surrounding AI is a topic of growing interest, not only among academics but also the general public. This review identifies the key entities (i.e., leading research institutions and their affiliated countries/regions, core research journals, and communities) that contribute to the research on the ethical and privacy issues in relation to AI and their intersections using co-occurrence analysis. Topic analyses profile the topical landscape of AI ethics using a topical hierarchical tree and the changing interest of society in AI ethics over time through scientific evolutionary pathways. We also paired 15 selected AI techniques with 17 major ethical issues and identify emerging ethical issues from a core set of the most recent articles published in Nature, Science, and Proceedings of the National Science Academy of the United States. These insights, bridging the knowledge base of AI techniques and ethical issues in the literature, are of interest to the AI community and audiences in science policy, technology management, and public administration. Keywords Artificial intelligence; Ethics; Privacy; Bibliometrics; Topic analysis.  \fHighlights • Articles on AI ethics cover 199 of the 254 Web of Science Categories, indicating a broad interest from the academia. • Research communities of computer science, business and management, medical science and law are playing a leading role on studies of AI ethics. • USA, UK, and China make the major contribution to AI ethics, with a relatively high level of domestic collaborations. • Key AI techniques raise ethical concerns, such as fairness, accountability, data privacy, responsibility, liability, and crimes.  \f1. Introduction A pandora’s box of artificial intelligence (AI) has been opened and these disruptive technologies are transforming the daily lives of human beings in relation to new ways of thinking and behavioral patterns, with enhanced capabilities and efficiency. There are many examples of AI applications in use today, such as smart homes [1] smart farming [2], precision medicine [3] and healthcare surveillance systems [4].The ethical and privacy issues surrounding the use of AI have been a topic of growing interest among diverse communities. For example, the general public has expressed concern about the impact of the increased use of robots on unemployment and inequality [5], social scientists have raised deep privacy concerns related to surveillance systems [6], and limited regulation of social media has raised debate with technical giants on the abuse of private data1. Despite these concerns, the AI community stands behind the efficiency and robustness of their AI models and there is an urgent need to guide the research community to understand these ethical and privacy challenges. Bibliometrics, which is a set of approaches for analyzing scientific documents (e.g., research articles, patents, and academic proposals), has been widely used as a tool for science, technology and innovation studies [7], such as identifying technological topics [8], discovering latent relationships [9], and predicting potential future trends [10]. Recently, AI has received recognition in bibliometrics as an emerging topic for empirical investigation [11, 12]. These investigations either align with the interest in technology management (e.g., using AI as a representative case in digital transformation) or emphasize its role in examining the reliability of the proposed methods. However, from a practical perspective, a bibliometric guide which summarizes ideas, assumptions, and debate in the literature would bring significant benefits to the AI community, not only by highlighting the ethical and privacy concerns raised by the public but also by identifying the potential conflicts between AI techniques and these issues of concern. To address these concerns, this paper reports on a bibliometric study to comprehensively profile the key ethical and privacy issues discussed in the research articles and to trace how such issues have changed over the past few decades. We integrated a set of intelligent bibliometric approaches within a framework for diverse analyses. To identify the key entities, i.e., the leading research institutions and their affiliated countries and regions, and the core research journals and their behind research communities, which report the ethical and privacy issues surrounding AI, we used co-occurrence statistics with diverse bibliographical indicators (e.g., authors, affiliations, and sources). With specific foci in topic analysis, we initially retrieved terms from the combined titles and abstracts of collected articles and used a term clumping process [13] to remove noisy terms and consolidate technical synonyms. In parallel, we represented each word in the combined field with titles and abstracts as a vector using the Word2Vec model [14] and combined the word vectors into term vectors by matching the core terms refined in the term clumping process. We answered the 1 More information can be found on the website: https://www.bbc.com/news/business-49099364 \fquestions as to what is the topical landscape and how have these topics evolved over time, using an approach of scientific evolutionary pathways [15]. We also targeted a core set of articles published in three world-leading multi-disciplinary journals, namely Science, Nature, and Proceedings of the National Academy of Sciences (PNAS) of the United States of America, and identified cutting-edge issues that might either focus attention on emergent ethical and privacy issues in the current AI age or lead to novel developments in AI models to address any potential negative impacts. We anticipate that the empirical insights identified in this study will motivate the AI community to extensively and comprehensively discuss the ethical and privacy issues surrounding AI and will guide the implementation of AI in line with an ethical framework. The rest of this paper is organized as follows: Section 2 presents a review of the related work on AI ethics, privacy, and bibliometrics; Section 3 introduces the data and methodologies used in this study; Section 4 presents the results, and our key findings and Section 5 concludes the study and suggests future research directions. 2. Related work In this section, we review the current debate on the ethical and privacy issues surrounding AI and then briefly introduce the bibliometrics and topic analysis used in this study. 2.1. Ethics, ethical dilemma, and AI ethics In philosophy, ethics describes “what is good for the individual and for society”, as well as the essence of “duties that people owe themselves and one another” [16], while ethical dilemma refers to certain ethical problems can be extremely complicated and the challenges they bring cannot be easily solved. Ever-improving technologies bring along with multiple advantages to human society, but they may also “generate downside risks and challenges, including more complicated ethical dilemma2. This is true with AI technologies. With the rapid growth in AI techniques in recent decades, there has been increasing controversy over the impact of AI on the daily lives of human beings, for example, the potential for robots to replace human labor [17], the accountability and accident risk of driverless vehicles [18], the self-awareness and behavior autonomy of robotics [19], and possible fraud caused by deep-fake videos and photos [20]. Such concerns in relation to the ethics around AI has attracted attention from global federal governments and corporations, in particular, tech giants such as Google and SAP, when those corporations are willing to form national and industrial committee to formulate AI ethics guidelines [21]. An increasing number of international organizations have also started to take actions to address the ethical challenges brought by AI technology. As one of the most recent developments, the United Nations Educational, Scientific and Cultural Organization (UNESCO) has issued its first draft of Recommendation on the Ethics of 2 the United Nations Educational, Scientific and Cultural Organization, Elaboration of a Recommendation on the ethics of artificial intelligence at https://en.unesco.org/artificial-intelligence/ethics  \fArtificial Intelligence (Recommendations) 3 in September 2020, which sets up ten important Principles of the Ethics of AI, including: proportionality and do no hard, safety and security, fairness and non-discrimination, sustainability, privacy, human oversight and determination, transparency and expandability, responsibly and accountability, awareness and literacy, and multi-stakeholder and adaptive governance and collaboratio",
            {
                "entities": [
                    [
                        213,
                        241,
                        "DOI"
                    ],
                    [
                        428,
                        456,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "ArticleConnectome-based machine learning models arevulnerable to subtle data manipulationsGraphical abstractAuthorsMatthew Rosenblatt,Raimundo X. Rodriguez,Margaret L. Westwater, ...,R. Todd Constable, Stephanie Noble,Dustin ScheinostCorrespondencematthew.rosenblatt@yale.edu (M.R.),dustin.scheinost@yale.edu (D.S.)In briefImperceptible data manipulations candrastically increase or decreaseperformance in machine learning modelsthat use high-dimensional neuroimagingdata. These manipulations could achievenearly any desired predictionperformance without noticeable changesto the data or any changes in otherdownstream analyses. The feasibility ofdata manipulations highlights thesusceptibility of data sharing andscientific machine learning pipelines tofraudulent behavior.Highlightsd Enhancement attacks falsely improve the performance ofconnectome-based modelsd Adversarial attacks degrade the performance ofconnectome-based modelsd Subtle data manipulations lead to large changes inperformanceRosenblatt et al., 2023, Patterns 4, 100756July 14, 2023 ª 2023 The Author(s).https://doi.org/10.1016/j.patter.2023.100756ll\fPlease cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSArticleConnectome-based machine learning modelsare vulnerable to subtle data manipulationsMatthew Rosenblatt,1,9,* Raimundo X. Rodriguez,2 Margaret L. Westwater,3 Wei Dai,4 Corey Horien,2 Abigail S. Greene,2R. Todd Constable,1,2,3,5 Stephanie Noble,3 and Dustin Scheinost1,2,3,6,7,8,*1Department of Biomedical Engineering, Yale School of Engineering and Applied Science, New Haven, CT 06510, USA2Interdepartmental Neuroscience Program, Yale School of Medicine, New Haven, CT 06510, USA3Department of Radiology & Biomedical Imaging, Yale School of Medicine, New Haven, CT 06510, USA4Department of Biostatistics, Yale School of Public Health, New Haven, CT 06510, USA5Department of Neurosurgery, Yale School of Medicine, New Haven, CT 06510, USA6Department of Statistics & Data Science, Yale University, New Haven, CT 06510, USA7Child Study Center, Yale School of Medicine, New Haven, CT 06510, USA8Wu Tsai Institute, Yale University, New Haven, CT 06510, USA9Lead contact*Correspondence: matthew.rosenblatt@yale.edu (M.R.), dustin.scheinost@yale.edu (D.S.)https://doi.org/10.1016/j.patter.2023.100756THE BIGGER PICTURE In recent years, machine learning models using brain functional connectivity havefurthered our knowledge of brain-behavior relationships. The trustworthiness of these models has notyet been explored, and determining the extent to which data can be manipulated to change the results isa crucial step in understanding their trustworthiness. Here, we showed that only minor manipulations ofthe data could lead to drastically different performance. Although this work focuses on machine learningmodels using brain functional connectivity data, the concepts investigated here apply to any scientificresearch that uses machine learning, especially with high-dimensional data. As machine learning becomesincreasingly popular in many fields of scientific research, data manipulations may become a major obstacleto the integrity of scientific machine learning.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYNeuroimaging-based predictive models continue to improve in performance, yet a widely overlookedaspect of these models is ‘‘trustworthiness,’’ or robustness to data manipulations. High trustworthinessis imperative for researchers to have confidence in their findings and interpretations. In this work, weused functional connectomes to explore how minor data manipulations influence machine learning predic-tions. These manipulations included a method to falsely enhance prediction performance and adversarialnoise attacks designed to degrade performance. Although these data manipulations drastically changedmodel performance, the original and manipulated data were extremely similar (r = 0.99) and did not affectother downstream analysis. Essentially, connectome data could be inconspicuously modified to achieveany desired prediction performance. Overall, our enhancement attacks and evaluation of existingadversarial noise attacks in connectome-based models highlight the need for counter-measures thatimprove the trustworthiness to preserve the integrity of academic research and any potential translationalapplications.INTRODUCTIONHuman neuroimaging studies have increasingly used machinelearning approaches to identify brain-behavior associationsthat generalize to novel samples.1,2 They do so by aggregatingweak yet informative signals occurring throughout the brain.3,4Machine learning models for functional connectomes (‘‘con-nectome-based models’’)5–7 are among the most popularPatterns 4, 100756, July 14, 2023 ª 2023 The Author(s). 1This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).PATTER 100756\fPlease cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSmethods for establishing brain-behavior relationships, andthey have successfully characterized the neural correlates ofvarious clinically relevant processes,8 including general cogni-tive ability,9 psychiatric disorders,7,10 affective states,11 andabstinence in individuals with substance use disorder.12Recent work has uncovered bias, or lack of fairness acrossgroups, in connectome-based models,13–15 including predic-tion failure in individuals who defy stereotypes.15 Although im-provements in accuracy6 and fairness (i.e., race, age, orgender bias)13–15 of connectome-based models are crucialfor improving the quality of academic studies and the potentialfor clinical translation, accurate and bias-free models are notenough. Connectome-based models should also have hightrustworthiness, which we define as robustness to data ma-nipulations.In other words, the output or performance ofa trustworthy model remains similar despite minor changesto the input (i.e., X data). Without a high degree of trustworthi-ness, researchers may not be able to have confidence in theirfindings and ensuing interpretations, as even minor modifica-tions to the data could dramatically alter results.Although trustworthiness has been explored from variousperspectives in the machine learning literature, including pri-vacy16 and explainability,17 here we examine trustworthinessthrough the lens of robustness to data manipulations.18 A pop-ular form of data manipulation specific to machine learning isadversarial noise (i.e., adversarial attacks), where a pattern(or ‘‘noise’’) deliberately designed to trick a machine learningmodel is added to data to cause misclassification.19,20 Theseattacks have been investigated in various contexts, includingcybersecurity,21,22 image recognition,20,23 and medical imagingor recordings.24–26 For neuroimaging, adversarial attacks maybecome problematic in the more distant future (e.g., in clinicalapplications25,27).A more immediate concern is the potential for data manipula-tions to falsely enhance prediction performance in researchstudies. Although the majority of scientific researchers seek toperform ethical research, data manipulations are more commonthan one might expect.28–33 For example, an analysis by Bik et al.showed that about 2% of biology papers contained a figure withevidence of intentional data manipulation.31 Furthermore, 2%of scientists admitted to fabrication/falsification, and 14%admitted to seeing their colleagues fabricate/falsify in a survey.32As data manipulation can result in wasted grant money andfuture research endeavors, determining themisdirection ofextent to which the prediction performance of connectome-based models can be falsely enhanced or diminished via datamanipulations is crucial.In this work, we investigated the trustworthiness of connec-tome-based predictive models. Specifically, we introduce thefor connectome-based‘‘performance enhancement attack’’models, where data are injected with small,inconspicuouspatterns to falsely improve the prediction performance ofa specific phenotype. We also explore the effectiveness of ad-versarial noise attacks on connectome-based models. Whereasadversarial noise attacks manipulate only the test data to changea particular prediction, enhancement attacks modify the entiredataset (i.e., training and test data) to falsely improve perfor-mance. In both cases—enhancement attacks and adversarialnoise attacks—we find that subtle manipulations drastically2 Patterns 4, 100756, July 14, 2023Articlechange predictions in four large datasets. Overall, our findingsdemonstrate that currentimplementations of connectome-based models are highly susceptible to data manipulations,which points toward the need for preventive measures built into study designs and data sharing practices.RESULTSFunctional MRI data were obtained from the Adolescent BrainCognitive Development (ABCD) study,34 the Human Connec-tome Project (HCP),35 the Philadelphia NeurodevelopmentalCohort (PNC),36 and the Southwest University LongitudinalImaging Multimodal (SLIM) study.37 The first three datasets(ABCD, HCP, and PNC) were used to demonstrate enhance-ment and adversarial attacks for prediction of IQ and self-re-ported sex. SLIM was introduced to demonstrate enhance-ment with a clinically relevant measure (state anxiety). Allanalyses were conducted on resting-state data. For SLIM,we downloaded fully preprocessed functional connectomes.For ABCD and PNC, raw data were registered to commonspace as previously described.38,39 For HCP, we startedwith the minimally preprocessed data.40 Next, standard, iden-tical preprocessing steps were performed across all datasetsusing BioImage Suite41 (see experi",
            {
                "entities": [
                    [
                        1091,
                        1119,
                        "DOI"
                    ],
                    [
                        1297,
                        1325,
                        "DOI"
                    ],
                    [
                        2409,
                        2437,
                        "DOI"
                    ],
                    [
                        5265,
                        5293,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "llOPEN ACCESSReviewModern views of machine learningfor precision psychiatryZhe Sage Chen,1,2,3,4,* Prathamesh (Param) Kulkarni,5 Isaac R. Galatzer-Levy,1,6 Benedetta Bigio,1 Carla Nasca,1,3and Yu Zhang7,8,*1Department of Psychiatry, New York University Grossman School of Medicine, New York, NY 10016, USA2Department of Neuroscience and Physiology, New York University Grossman School of Medicine, New York, NY 10016, USA3The Neuroscience Institute, New York University Grossman School of Medicine, New York, NY 10016, USA4Department of Biomedical Engineering, New York University Tandon School of Engineering, Brooklyn, NY 11201, USA5Headspace Health, San Francisco, CA 94102, USA6Meta Reality Lab, New York, NY, USA7Department of Bioengineering, Lehigh University, Bethlehem, PA 18015, USA8Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA 18015, USA*Correspondence: zhe.chen@nyulangone.org (Z.S.C.), yuzi20@lehigh.edu (Y.Z.)https://doi.org/10.1016/j.patter.2022.100602THE BIGGER PICTURE Mental health issues are an epidemic in the United States and the world and haveimposed a tremendous burden to the healthcare system and society. To date, there is still a lack of bio-markers and individualized treatment guidelines for mental illnesses. In recent years, machine learning(ML) and artificial intelligence (AI) have become increasingly popular in analyzing complex patterns of neuraland behavioral data for psychiatry. We provide a comprehensive review of ML methodologies and applica-tions in precision psychiatry. We argue that advances in ML-powered modern technologies will create a para-digm shift in the current practice in diagnosis, prognosis, monitoring, and treatment of mental illnesses. Wediscuss conceptual and practical challenges in precision psychiatry and highlight future research opportu-nities in ML.SUMMARYIn light of the National Institute of Mental Health (NIMH)’s Research Domain Criteria (RDoC), the advent offunctional neuroimaging, novel technologies and methods provide new opportunities to develop preciseand personalized prognosis and diagnosis of mental disorders. Machine learning (ML) and artificial intelli-gence (AI) technologies are playing an increasingly critical role in the new era of precision psychiatry.Combining ML/AI with neuromodulation technologies can potentially provide explainable solutions in clinicalpractice and effective therapeutic treatment. Advanced wearable and mobile technologies also call for thenew role of ML/AI for digital phenotyping in mobile mental health. In this review, we provide a comprehensivereview of ML methodologies and applications by combining neuroimaging, neuromodulation, and advancedmobile technologies in psychiatry practice. We further review the role of ML in molecular phenotyping andcross-species biomarker identification in precision psychiatry. We also discuss explainable AI (XAI) and neu-romodulation in a closed human-in-the-loop manner and highlight the ML potential in multi-media informa-tion extraction and multi-modal data fusion. Finally, we discuss conceptual and practical challenges in pre-cision psychiatry and highlight ML opportunities in future research.INTRODUCTIONMental health issues are an epidemic in the United States and theworld. According to the NationalInstitute of Mental Health(NIMH), nearly one in five American adults suffer from a form ofmental(www.nimh.nih.gov/health/statistics/). According to the Centers for Disease Controlillness or psychiatric disorderand Prevention (CDC), the COVID-19 pandemic has witnessed asignificant impact on our lifestyle and considerably elevatedadverse mental health conditions caused by fear, worry, and un-certainty.1 Increased suicide rates, opioid abuse, and anti-depressant usage have been observed in both adults and teen-agers. The diagnosis and treatment of mental health hasimposed a burden to the healthcare system and society. In thePatterns 3, November 11, 2022 ª 2022 The Author(s). 1This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\fllOPEN ACCESSABCFigure 1. ML research in mental health and categorization ofneuroimaging(A) The number of PubMed publications with keywords ‘‘machine learning orAI’’ and ‘‘psychiatry or mental health’’ in the title or abstract (years 2000–2021).(B) Growth of mental health tech funding in the US market (years 2017–2021;data source: https://www.cbinsights.com).(C) Human neuroimaging at various spatial and temporal resolution (copyrightIEEE; figure reproduced from Thukral et al.13 with permission).United States alone, the economic burden of depression alone isestimated to be at least $210 billion annually.2 Precision medi-cine (or personalized medicine) is an innovative approach totailoring disease prevention, diagnosis, and treatment that ac-counts for the differences in subjects’ genes, environments,and lifestyles. The goal of precision medicine is to target timelyand accurate diagnosis/prognosis/therapeutics for the individu-alized patient’s health problem and to further provide feedbackinformation to patients and surrogate decision-makers. Recentdecades have witnessed various degrees of successes in preci-sion medicine, especially in oncology3. Traditional diagnoses ofmental illnesses rely on physical exams, lab tests, and psycho-logical and behavioral evaluations. Meanwhile, precision psychi-atry has increasingly received its deserved attention.4,5 Althoughpsychiatry has not yet benefited fully from the advanced diag-nostic and therapeutic technologies that have an impact on otherclinical specialties, these technologies have the potential totransform the future psychiatric landscape.The NIMH’s Research Domain Criteria (RDoC) initiative aims toillness and provide aaddress the heterogeneity of mentalbiology-based (as opposed to symptom-based) framework forunderstanding these mental illnesses in terms of varying degrees2 Patterns 3, November 11, 2022Reviewof dysfunction in psychological or neurobiological systems; it at-tempts to bridge the power of multi-disciplinary (such as thegenetics, neuroscience, and behavioral science) research ap-proaches.6,7 The current gold standard for diagnosis and treat-ment outcome in mental disorders—the Diagnostic and Statisti-cal Manual of Mental Disorders (DSM), maintained by theAmerican Psychiatric Association (APA)—is often based on theclinician’s observations, behavioral symptoms, and patient re-porting, which are all susceptible to a high degree of variability.Therefore, it is imperative to develop quantitative neurobiologicalmarkers for mental disorders while accounting for their hetero-geneity and comorbidity.One important goal in neuropsychiatry research is to identifythe relationship between neurobiological/neurophysiologicalfindings and clinical behavioral/self-report observations. Ma-chine learning (ML) and artificial intelligence (AI) have generatedgrowing interests in psychiatry because of their strong predictivepower and generalization ability for prognosis and diagnosis ap-plications.8–10 The interest of applying ML/AI in psychiatry hasgrown steadily in the past two decades, as reflected in the num-ber of PubMed publications (Figure 1A). To improve mentalhealth outcomes with digital technologies, the so-called ‘‘digitalpsychiatry’’ focuses on developing ML/AI methods for assess-ing, diagnosing, and treating mental health issues.11 A recentglobal survey has indicated that psychiatrists were somewhatskeptical that AI could replace human empathy, but many pre-dicted that ‘‘man and machine’’ would increasingly collaboratein undertaking clinical decisions, and psychiatrists were opti-mistic that AI might improve efficiencies and access to mentalcare and reduce costs.12The past two decades have witnessed substantial growth ofML applications for psychiatry in the literature, reflected inmany applications and reviews.17–27 Although multiple reviewsof ML for psychiatry are available, the majority of reviews arerestricted to relatively narrow scopes. In this paper, we try to pro-vide a comprehensive review of ML and ML-powered technolo-gies in mental health applications. Our view is ‘‘modern’’ in thesense that the development of new technologies, consumermarket demand, and public health crises (such as COVID-19)have constantly redefined the role of ML and reshaped ourthinking in precision psychiatry. Specifically, we will coverstate-of-the-art methodological developments in ML, multi-modal neuroimaging, large-scale circuit modeling, neuromodu-lation, and human-machine interface. Due to space limitations,our reviewed literature is by no means exhaustive. To distinguishour review from others, we will focus on several issues central tothe ML applications for psychiatry: generalizability, interpret-ability, causality, and clinical and behavioral integration.Our view about this emerging field is cautiously optimistic forseveral reasons. First, with an increasing amount of data andcomputational power, there is a growing demand for psychia-trists to use ML to reevaluate clinical, behavioral, and neuroi-maging data. The interests in mental health funding from the in-dustry have also grown substantially (Figure 1B). Second, it isbecoming increasingly important to leverage the power of MLand develop explainable AI (XAI) tools for unbiased risk diag-nosis, personalized medicine recommendation, and preciseneurostimulation. The integration of ML with neuroimaging canpotentially help us identify and validate biomarkers in diagnosis\fReviewand treatment of mental illnesses. Third, there is a growing de-mand for psychiatrists in the United States, and the shortage iseven more acute in poorer countries.28 ML/AI technologiesmay change the practice of psychiatry for both clinicians and pa-tients. Finally, advanced technologies such as social media,multi-media, and mobile and wearable devices also call for thedevelopment of ML/AI tools to assist the assessment, diagno",
            {
                "entities": [
                    [
                        977,
                        1005,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Unsupervised visual feature learning withspike-timing-dependent plasticity: How far are we fromtraditional feature learning approaches?Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne, PierreBouletTo cite this version:Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne, Pierre Boulet. Unsupervisedvisual feature learning with spike-timing-dependent plasticity: How far are we from traditional featurelearning approaches?. Pattern Recognition, 2019, 93, pp.418-429.￿10.1016/j.patcog.2019.04.016￿.￿hal-02146284￿HAL Id: hal-02146284https://hal.science/hal-02146284Submitted on 22 Oct 2021HAL is a multi-disciplinary open accessarchive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come fromteaching and research institutions in France orabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL, estdestinée au dépôt et à la diffusion de documentsscientifiques de niveau recherche, publiés ou non,émanant des établissements d’enseignement et derecherche français ou étrangers, des laboratoirespublics ou privés.Distributed under a Creative Commons Attribution - NonCommercial| 4.0 InternationalLicense\fVersion of Record: https://www.sciencedirect.com/science/article/pii/S0031320319301621Manuscript_e9bbab9223355c7c3fec8d7737d3ac25Unsupervised Visual Feature Learning withSpike-timing-dependent Plasticity: How Far are wefrom Traditional Feature Learning Approaches?Pierre Faleza, Pierre Tirillyb,∗, Ioan Marius Bilascoa, Philippe Deviennea,Pierre BouletaaUniv. Lille, CNRS, Centrale Lille, UMR 9189 - CRIStAL - Centre de Recherche enInformatique Signal et Automatique de Lille, F-59000 Lille, FrancebUniv. Lille, CNRS, Centrale Lille, IMT Lille Douai, UMR 9189 - CRIStAL - Centre deRecherche en Informatique Signal et Automatique de Lille, F-59000 Lille, FranceAbstractSpiking neural networks (SNNs) equipped with latency coding and spike-timingdependent plasticity rules offer an alternative to solve the data and energy bot-tlenecks of standard computer vision approaches: they can learn visual featureswithout supervision and can be implemented by ultra-low power hardware ar-chitectures. However, their performance in image classification has never beenevaluated on recent image datasets. In this paper, we compare SNNs to auto-encoders on three visual recognition datasets, and extend the use of SNNs tocolor images. The analysis of the results helps us identify some bottlenecks ofSNNs: the limits of on-center/off-center coding, especially for color images, andthe ineffectiveness of current inhibition mechanisms. These issues should beaddressed to build effective SNNs for image recognition.Keywords:feature learning, unsupervised learning, spiking neural networks,spike-timing dependent plasticity, auto-encoders, image recognition.1. IntroductionMachine learning algorithms require good data representations to be effec-tive [1]. Good data representations can capture underlying correlations of the∗Corresponding authorPreprint submitted to Pattern RecognitionMarch 30, 2019© 2019 published by Elsevier. This manuscript is made available under the CC BY NC user licensehttps://creativecommons.org/licenses/by-nc/4.0/\fdata, provide invariance properties and help disentangle the data to make itlinearly separable.In computer vision, much effort has been put historicallyinto engineering the right visual features for recognizing, organizing and inter-preting visual contents [2; 3]. More recently, and especially since the rise of deeplearning, those features tend to be learned by algorithms rather than designedby human effort. Learned features have shown their superiority on a numberof tasks, such as image classification [4], image segmentation [5], and actionrecognition [6]. Although effective, feature learning has two major drawbacks:• it is data-consuming, as supervised learning algorithms – especially deeplearning ones – require large amounts of annotated data to be trained;• it is energy-consuming, as training large models, e.g., using gradient descent-based algorithms, has a high computational cost, which increases with theamount of training data. These algorithms are usually run on dedicatedhardware (typically GPU) that are power-intensive.The first issue – data consumption – can be mitigated by the use of unsu-pervised learning models. Unsupervised representation learning is recognized asone of the major challenges in machine learning [1] and is receiving a growinginterest in computer vision applications [7; 8; 9]. A number of unsupervisedmodels have been developed through the years, notably auto-encoders [10] andrestricted Boltzmann machines (RBMs) [11], and their multi-layer counterparts,stacked auto-encoders [12] and deep belief networks (DBN) [13]. Other lines ofwork include sparse coding [14] and the use of semi- or weakly supervised learn-ing algorithms [15]. Moreover, in the case of neural networks, initializing a deepneural network with features learned without supervision before training canyield better generalization capabilities than purely supervised training [12].The second issue – energy consumption – is addressed much less frequently inthe literature, but several authors acknowledge its importance [16; 17; 18] whichis bound to grow more and more as machine learning becomes overwhelminglypresent in a large range of applications: marketing, medicine, finance, education,administration, etc. Most hardware vendors have proposed dedicated machine2\flearning processor architectures (based on GPU, FPGA, etc.)recently [19].These hardware improvements help reduce the energy consumption by a smallfactor (typically one order of magnitude). Reducing further the energy con-sumption of learning algorithms requires to define new learning models andassociated ultra-low power architectures [18; 20; 21]. One promising model isspiking neural networks (SNNs). In this model, artificial neurons communicateinformation through spikes, as natural neurons do. Initially studied in neuro-science as a model of the brain, SNNs receive constant attention in the fieldsof machine learning and pattern recognition, from both the theoretical [22] andthe applicative [17; 23; 24; 25] perspectives. Dedicated hardware implementingthis model can be very energy-efficient [20]. SNNs have already shown theirability to provide near-state-of-the-art results in image classification, but onlywhen they are trained by transferring parameters from pre-trained deep neuralnetworks [21] or by variants of back-propagation [26]. In terms of energy effi-ciency, the first option is not viable as it still requires to train a standard deepneural network, which is exactly what should be avoided; the second option isnot suited either as back-propagation is a global, centralized algorithm – the er-ror must be propagated from the output to all units –, whereas the efficiency ofSNNs lies in their ability to perform highly decentralized, parallel processing onsparse spike data. The alternative is to use bio-inspired learning rules, such asHebbian rules. Among those, rules based on spike-timing dependent plasticity(STDP) [27] have shown promising results for learning visual features; how-ever, they have only been evaluated on datasets with limited challenges (rigidobjects, limited number of object instances, uncluttered backgrounds. . . ) suchas MNIST, 3D-object, ETH-80 or NORB [28; 29; 30; 31; 32], or on two-classdatasets [31; 32]. How they perform on more complex image datasets, what isthe performance gap between them and standard approaches, and what needsto be done to bridge this gap is yet to be established.Aims and scope. In this paper, we evaluate the ability of SNNs equipped withlatency coding and STDP to learn features for visual recognition on three stan-3\fdard datasets (CIFAR-10, CIFAR-100, and STL-10). Our goal is to identifysome of the factors that prevent STDP-based SNNs to reach state-of-the-artresults on actual computer vision tasks. First, we compare the performanceof SNNs on grayscale and color images (Section 5.4), then we compare themto one standard unsupervised feature learning algorithm, sparse auto-encoders(Section 5.5). The resulting models are analyzed with respect to different fac-tors (Section 6): input pre-processing, feature sparsity, feature coherence, andobjective functions.It allows us to identify some bottlenecks that should betackled to bridge the gap from SNNs to state-of-the-art models. In the conclu-sion (Section 7), we suggest some solutions to help address these bottlenecks.In this work, we consider only single-layer architectures because multi-layerSNNs with unsupervised STDP are only very recent and difficult to train, dueto the loss of spiking activity across layers [33; 31]. To our knowledge, this is thefirst work that evaluates features learned by unsupervised STDP-based SNNson recent benchmarks for object recognition and on color images, making onestep towards their use for actual vision applications.2. Unsupervised visual feature learningA visual feature extractor can be modeled as a function f : Rh×w → Rnfthat maps an image or image region of size h × w to a real vector of dimensionnf . It defines a dictionary of features of size nf . In the remaining of the paper,f will denote either the feature extractor function or the resulting dictionary,depending on the context. Early visual feature extractors were handcraftedto capture specific types of visual information (e.g., distributions of edges [3]).Recent approaches rather rely on machine learning to produce features thatbetter fit the data and that can be optimized towards a specific application.A typical learning-based feature extractor can be seen as a function fθ whoseparameters θ are optimized towards a specific goal by a learning algorithm. Thegeneral shape of fθ can be specified explicitly (e.g., a linear transform [34]), orimplicitly, based on the learning alg",
            {
                "entities": [
                    [
                        503,
                        531,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fInformation Fusion 64 (2020) 149–187 Contents lists available at ScienceDirect Information Fusion journal homepage: www.elsevier.com/locate/inffus Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation Yu-Dong Zhang a , b , ∗ , Zhengchao Dong c , d , Shui-Hua Wang b , f , g , Xiang Yu a , Xujing Yao a , Qinghua Zhou a , Hua Hu c , e , Min Li c , h , Carmen Jiménez-Mesa i , Javier Ramirez i , Francisco J. Martinez i , Juan Manuel Gorriz i , j a School of Informatics, University of Leicester, Leicester, LE1 7RH, Leicestershire, UK b Department of Information Systems, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah 21589, Saudi Arabia c Department of Psychiatry, Columbia University, USA d New York State Psychiatric Institute, New York, NY 10032, USA e Department of Neurology, The Second Affiliated Hospital of Soochow University, China f School of Architecture Building and Civil engineering, Loughborough University, Loughborough, LE11 3TU, UK g School of Mathematics and Actuarial Science, University of Leicester, LE1 7RH, UK h School of Internet of Things, Hohai University, Changzhou, China i Department of Signal Theory, Networking and Communications, University of Granada, Granada, Spain j Department of Psychiatry, University of Cambridge, Cambridge CB21TN, UK a r t i c l e i n f o a b s t r a c t Keywords: Multimodal data fusion Neuroimaging Magnetic resonance imaging PET SPECT Fusion rules Assessment Applications Partial volume effect 1. Introduction Multimodal fusion in neuroimaging combines data from multiple imaging modalities to overcome the fundamen- tal limitations of individual modalities. Neuroimaging fusion can achieve higher temporal and spatial resolution, enhance contrast, correct imaging distortions, and bridge physiological and cognitive information. In this study, we analyzed over 450 references from PubMed, Google Scholar, IEEE, ScienceDirect, Web of Science, and var- ious sources published from 1978 to 2020. We provide a review that encompasses (1) an overview of current challenges in multimodal fusion (2) the current medical applications of fusion for specific neurological diseases, (3) strengths and limitations of available imaging modalities, (4) fundamental fusion rules, (5) fusion quality assessment methods, and (6) the applications of fusion for atlas-based segmentation and quantification. Overall, multimodal fusion shows significant benefits in clinical diagnosis and neuroscience research. Widespread edu- cation and further research amongst engineers, researchers and clinicians will benefit the field of multimodal neuroimaging. Neuroimaging has been playing pivotal roles in clinical diagnosis and basic biomedical research in the past decades. As described in the follow- ing section, the most widely used imaging modalities are magnetic res- onance imaging (MRI), computerized tomography (CT), positron emis- sion tomography (PET), and single-photon emission computed tomog- raphy (SPECT). Among them, MRI itself is a non-radioactive, non- invasive, and versatile technique that has derived many unique imaging modalities, such as diffusion-weighted imaging, diffusion tensor imag- ing, susceptibility-weighted imaging, and spectroscopic imaging. PET is also versatile, as it may use different radiotracers to target different molecules or to trace different biologic pathways of the receptors in the body. Therefore, these individual imaging modalities (the use of one imag- ing modality), with their characteristics in signal sources, energy lev- els, spatial resolutions, and temporal resolutions, provide complemen- tary information on anatomical structure, pathophysiology, metabolism, structural connectivity, functional connectivity, etc. Over the past decades, everlasting efforts have been made in developing individual modalities and improving their technical performance. Directions of im- provements include data acquisition and data processing aspects to in- crease spatial and/or temporal resolutions, improve signal-to-noise ratio and contrast to noise ratio, and reduce scan time. On application aspects, ∗ Corresponding author. E-mail addresses: yudongzhang@ieee.org (Y.-D. Zhang), zhengchao.dong@nyspi.columbia.edu (Z. Dong), shuihuawang@ieee.org (S.-H. Wang), xy144@le.ac.uk (X. Yu), xy147@le.ac.uk (X. Yao), qz105@le.ac.uk (Q. Zhou), huhua8775@suda.edu.cn (H. Hu), limin@hhu.edu.cn (M. Li), carmenj@ugr.es (C. Jiménez-Mesa), javierrp@ugr.es (J. Ramirez), fjesusmartinez@ugr.es (F.J. Martinez), gorriz@ugr.es (J.M. Gorriz). https://doi.org/10.1016/j.inffus.2020.07.006 Received 30 April 2020; Received in revised form 6 July 2020; Accepted 14 July 2020 Available online 17 July 2020 1566-2535/© 2020 Elsevier B.V. All rights reserved. \fY.-D. Zhang, Z. Dong and S.-H. Wang et al. Information Fusion 64 (2020) 149–187 the strength and limitations of the neuroimaging modalities and the cor- responding analysis methods, and in particular, the needs for improved image fusion methods and (2) we will review recent methodological development in data preprocessing and data fusion in multimodal neu- roimaging. We note that although we tried to cover all neuroimaging modalities, we inevitably paid more attention to MRI modalities. This is not only due to the most practical application and versatility of the MRI but also due to the limitations of our expertise. Fig. 2 shows the taxonomy of this review. The main contents of the paper are organized as follows. Chapter 2 will give a brief introduction to neuroimaging, and challenges of multi- modal imaging; Chapter 3 introduces the commonly used neuroimaging modalities, which include computerized tomography, positron emission tomography, single-proton emission computed tomography, and mag- netic resonance imaging, which has many modalities in its own right. For each modality, we will concisely describe its signal source, energy level, spatial resolution, temporal resolution, and major applications; Chapter 4 describe applications of neuroimaging in three major areas: the developing brains, the degenerative brains, and mental disorders. In each part, we will first briefly describe what the clinical and/or biomed- ical problems are, we then review recent papers on how neuroimaging has been used to address these problems, and we point out what the unmet needs and challenges; Chapters 5 to 9 are devoted to the multimodal neuroimaging fusion, covering some important procedures in data fusion. The topics are not necessarily complete and their order of presentation is not necessarily coherent with the pipeline of fusion processing. Chapter 5 reviews the fundamental methods, which covers types, rules, atlas-based segmenta- tion, decomposition, reconstruction, and quantification; Chapter 6 re- views subjective and objective assessment of data fusion in multimodal neuroimaging; Chapter 7 reviews the advantages of data fusion in im- proving the spatial/temporal resolution, distortion correction, and con- trast; it also reviews the benefits of these advantages in fusing structural and functional images; Chapter 8 reviews atlas-based segmentations in multimodal imaging fusion; Chapter 9 reviews the quantification in mul- timodal neuroimaging fusion. While the focus of this part is given to PET and SPECT, some of the approaches and principles discussed here, such as partial volume correction and attenuation (relaxation), can be applied to quantitative MRI modalities, such as DTI, ASL, quantitative susceptibility mapping (QSM), etc. Chapter 10 concludes the paper. 2. Multimodal imaging data fusion: challenges in neuroimaging In this part, we will review the current challenges of neuroimag- ing, including limited spatial/temporal resolution, lack of quantifica- tion, and imaging distortions. These challenges often create fundamental limitations on individual modalities of neuroimaging, while some chal- lenges also exist in current multi-modal neuroimaging. This part will mainly cover the challenges of individual neuroimaging modalities that led to the development and ongoing research of multimodal neuroimag- ing methods. 2.1. Individual modality imaging Neuroimaging can be divided into structural imaging and functional imaging according to the imaging mode. Structural imaging is used to show the structure of the brain to aid the diagnosis of some brain dis- eases, such as brain tumors or brain trauma. Functional imaging is used to show how the brain metabolizes while carrying out certain tasks, in- cluding sensory, motor, and cognitive functions. Functional imaging is mainly used in neuroscience and psychological research, but it is grad- ually becoming a new way of clinical-neurological diagnosis [10] . The amount of information obtainable through single-mode imag- ing is limited and often cannot reflect the complex specificity of organ- isms. For instance, although CT imaging is effective in identifying nor- mal structures and abnormal diseased tissues according to their density Fig. 1. Numbers of peer-reviewed papers with the keywords of “neuroimaging ”or “brain imaging ” in titles (the numbers and the bar graph were generated by PubMed in Feb 2020).",
            {
                "entities": [
                    [
                        5383,
                        5411,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "This document is downloaded from DR‑NTU (https://dr.ntu.edu.sg)Nanyang Technological University, Singapore.Behavioral responses of nursing home residentsto a robotic pet dog with a customizableinteractive kitSheba, Jaichandar Kulandaidaasan; Salman, Adnan Ahmed; Kumar, Sampath; Phuc, LeTan; Elara, Mohan Rajesh; Martínez‑García, Edgar2018Sheba, J. K., Salman, A. A., Kumar, S., Phuc, L. T., Elara, M. R., & Martínez‑García, E. (2018).Behavioral Responses of Nursing Home Residents to a Robotic Pet Dog with a CustomizableInteractive Kit. Procedia Computer Science, 133, 409‑416. doi:10.1016/j.procs.2018.07.050https://hdl.handle.net/10356/89268https://doi.org/10.1016/j.procs.2018.07.050© 2018 The Author(s). Published by Elsevier Ltd. This is an open access article under the CCBY‑NC‑ND license(http://creativecommons.org/licenses/by‑nc‑nd/4.0/).Downloaded on 04 Jul 2023 07:44:17 SGT\fAvailable online at www.sciencedirect.comAvailable online at www.sciencedirect.comScienceDirectScienceDirectProcedia Computer Science 00 (2018) 000–000Procedia Computer Science 00 (2018) 000–000Procedia Computer Science 133 (2018) 409–416www.elsevier.com/locate/procediawww.elsevier.com/locate/procediaInternational Conference on Robotics and Smart Manufacturing (RoSMa2018)International Conference on Robotics and Smart Manufacturing (RoSMa2018)Behavioral Responses of Nursing Home Residents to a Robotic PetBehavioral Responses of Nursing Home Residents to a Robotic PetDog with a Customizable Interactive KitDog with a Customizable Interactive KitJaichandar Kulandaidaasan Shebaa*, Adnan Ahmed Salmana, Sampath Kumara, Le Tan Jaichandar Kulandaidaasan Shebaa*, Adnan Ahmed Salmana, Sampath Kumara, Le Tan Phucb, Mohan Rajesh Elarac, Edgar Martínez-GarcíadPhucb, Mohan Rajesh Elarac, Edgar Martínez-GarcíadaSchool of Electrical and Electronics Engineering, Singapore Polytechnic, Singapore aSchool of Electrical and Electronics Engineering, Singapore Polytechnic, Singapore bNanyang Technological University, Singapore bNanyang Technological University, Singapore cEngineering Product Development, Singapore University of Technology and Design cEngineering Product Development, Singapore University of Technology and Design dInstitute of Engineering and Technology, Universidad Autónoma de Ciudad Juárez dInstitute of Engineering and Technology, Universidad Autónoma de Ciudad Juárez Abstract Abstract Robot therapy for the elderly had been a novel advent for the past decade and the efficacy of such therapeutic procedures had Robot therapy for the elderly had been a novel advent for the past decade and the efficacy of such therapeutic procedures had similar benefits to pets in improving health outcomes. But there had been experiments which resulted in showing the loss of similar benefits to pets in improving health outcomes. But there had been experiments which resulted in showing the loss of interest over time due to limited levels of interaction. This paper regards that if a collection of customizable interactive games interest over time due to limited levels of interaction. This paper regards that if a collection of customizable interactive games comes along with the robotic pet dog, the long-term interest will sustain and in effect, long-term psychological benefits would be comes along with the robotic pet dog, the long-term interest will sustain and in effect, long-term psychological benefits would be rendered. Here in this study, we investigate the effect of elderly interacting with pet robot thought multimodal peripheral devices rendered. Here in this study, we investigate the effect of elderly interacting with pet robot thought multimodal peripheral devices with a different level of cognitive challenges using questionnaire, facial temperature, EMG and EEG. with a different level of cognitive challenges using questionnaire, facial temperature, EMG and EEG. © 2018 The Authors. Published by Elsevier Ltd.© 2018 The Authors. Published by Elsevier Ltd. © 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/). This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/). Peer-review under responsibility of the scientific committee of the International Conference on Robotics and Smart Manufacturing.Keywords: human-robot interaction; elderly care; robotic therapy; behavioral assessment Keywords: human-robot interaction; elderly care; robotic therapy; behavioral assessment 1. Introduction1. IntroductionExtensive research has been undertaken around the world towards the use of robot therapy to treat loneliness,Extensive research has been undertaken around the world towards the use of robot therapy to treat loneliness,depression and dementia in elderly patients. The presence of pet robots can reduce stress and improve health depression and dementia in elderly patients. The presence of pet robots can reduce stress and improve health outcomes and robotic animals can be as effective as real animals [1, 2, 3, and 4]. It has been shown that the elderly outcomes and robotic animals can be as effective as real animals [1, 2, 3, and 4]. It has been shown that the elderly with dementia are attracted to robots, raising the promise that appropriately designed robots with an interactive with dementia are attracted to robots, raising the promise that appropriately designed robots with an interactive stimulation features could play an important role in their treatment [5].stimulation features could play an important role in their treatment [5].* Corresponding author. Tel.: +65-64886303; fax: +65-67721974. * Corresponding author. Tel.: +65-64886303; fax: +65-67721974. E-mail address:jai@sp.edu.sg E-mail address:jai@sp.edu.sg 1877-0509© 2018 The Authors. Published by Elsevier Ltd. 1877-0509© 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/). This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/). 1877-0509 © 2018 The Authors. Published by Elsevier Ltd.This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)Peer-review under responsibility of the scientific committee of the International Conference on Robotics and Smart Manufacturing.10.1016/j.procs.2018.07.05010.1016/j.procs.2018.07.0501877-0509ScienceDirectAvailable online at www.sciencedirect.com\f2 410 Jaichandar Kulandaidaasan Sheba et.al / Procedia Computer Science 00 (2018) 000–000 Jaichandar Kulandaidaasan Sheba et al. / Procedia Computer Science 133 (2018) 409–416Abbreviations MAV Mean Absolute value AS SW Time taken in secs for 10 cycles using Dumbbell integrated with ActSense Time taken in secs for 10 cycles using Standard Weight without ActSense There are many research studies done to show how pet robots are helpful during therapy among elderly and how acceptance is measured [6]. Seal like therapeutic robots for instance PARO [7], are been widely used in nursing homes. Several healthcare robots like AIBO a dog like robot, NeCoRo a cat like robot, PARO a seal like robot are developed, however, not all are well accepted due to various expectations from the range of stakeholders [8,9,10,11]. In robot therapy, people feelings has to be stimulated through interactions and interest to engage with pet robots has to be sustained for long term therapies. For pet robots, shapes, feeling of touch (texture), response mimicking animals has to be carefully designed. A single perfect design which will be accepted by the elderly is unlikely. Robots used for cognitive training should engender mental effects, such as comfort, pleasure and relaxation. Actions that manifest themselves during interactions with elderly can be interpreted as if the robots had hearts and feelings [12]. After carefully accessing individual needs and preferences we are proposing a customizable therapeutic pet robot kit. The kit will allow robot’s functionality to be modified or extended based on the individual needs and thereby enable greater acceptance. By using ‘user matching strategy’ robots are matched to human expectation. The quality and quantitative study aimed to access response of elderly people to the dog robot, SNOWY, in an elderly home setting. 2. Experimental Design The study was conducted at Lions home for elderly, Singapore. Residents capable of completing the study by staff were approached and informed about the trial. 12 residents were recruited for the study. The researcher asked residents to make themselves comfortable and rest for ten minutes. The initial facial thermography reading was taken. Residents were briefed about the study and wireless EMG and EEG sensors are connected. The researcher brought SNOWY into the room and turned it on. He placed SNOWY on a table in front of them so they could cuddle it. He explained how to interact with SNOWY using ActSense (an interactive dumbbell), glove and how to play game using reminiscence and music therapy kit. More details of the interactive kit is described in Fig. 1. Fig. 1. Complete set of interactive therapeutic pet robot kit The measurement criteria for the reaction of the subjects were in the form of EEG & EMG data evaluation, face score, a holistic questionnaire and thermal activity levels before and after interaction analysis. The experiment was to be run with 12 elderlies with age ranging from 65-91. Subjects were all females. One set of SNOWY was introduced and the interaction sequence of each device is in the order of the dumbbell, the card game, the glove and the memory game. The interaction time would be 10 minutes per elderly. They were not subjected into the same room together. A different chamber was",
            {
                "entities": [
                    [
                        584,
                        611,
                        "DOI"
                    ],
                    [
                        661,
                        688,
                        "DOI"
                    ],
                    [
                        6569,
                        6596,
                        "DOI"
                    ],
                    [
                        6596,
                        6623,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Future Generation Computer Systems 144 (2023) 291–306Contents lists available at ScienceDirectFuture Generation Computer Systemsjournal homepage: www.elsevier.com/locate/fgcsDeep learning for understanding multilabel imbalanced Chest X-raydatasetsHelena Liz a,b,∗David Camacho aa Computer Systems Engineering Department, Universidad Politécnica de Madrid, Alan Turing s/n, Madrid, 28031, Spainb Department of Computer Sciences, Universidad Rey Juan Carlos, Tulipán s/n, Móstoles, 28933, Spainc Computer Science Department, Universidad Autónoma de Madrid, Madrid, 28049, Spaind TECNALIA Basque Research & Technology Alliance (BRTA), P. Tecnologico 700, Derio, Bizkaia, 48160, Spaine University of the Basque Country (UPV/EHU), Bilbao, 48013, Spain, Javier Huertas-Tato a, Manuel Sánchez-Montañés c, Javier Del Ser d,e,a r t i c l ei n f oa b s t r a c tArticle history:Received 29 July 2022Received in revised form 28 December 2022Accepted 4 March 2023Available online 6 March 2023Keywords:Convolutional neural networksChest X-raysExplainable AIEnsemble MethodologyOver the last few years, convolutional neural networks (CNNs) have dominated the field of computervision thanks to their ability to extract features and their outstanding performance in classificationproblems, for example in the automatic analysis of X-rays. Unfortunately, these neural networks areconsidered black-box algorithms, i.e. it is impossible to understand how the algorithm has achieved thefinal result. To apply these algorithms in different fields and test how the methodology works, we needto use eXplainable AI techniques. Most of the work in the medical field focuses on binary or multiclassclassification problems. However, in many real-life situations, such as chest X-rays, radiological signsof different diseases can appear at the same time. This gives rise to what is known as \"multilabelclassification problems\". A disadvantage of these tasks is class imbalance, i.e. different labels do nothave the same number of samples. The main contribution of this paper is a Deep Learning methodologyfor imbalanced, multilabel chest X-ray datasets. It establishes a baseline for the currently underutilisedPadChest dataset and a new eXplainable AI technique based on heatmaps. This technique also includesprobabilities and inter-model matching. The results of our system are promising, especially consideringthe number of labels used. Furthermore, the heatmaps match the expected areas, i.e. they mark theareas that an expert would use to make a decision.© 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-NDlicense (http://creativecommons.org/licenses/by-nc-nd/4.0/).1. IntroductionIn recent years, the field of medicine has faced two relevantproblems that hinder patient care: staff workload and subjectiv-ity in the interpretation of tests [1,2]. These problems have noeasy solution, which is especially dangerous in medicine becauseprocedural errors can lead to serious health complications. Firstly,overwork in medicine, aggravated in recent times by the globalCOVID-19 pandemic, can lead to errors and delays in diagnosisand treatment. As mentioned above, there is also subjectivity inthe interpretation of some medical tests. The expert analysingthese tests, for example X-rays, may arrive at an erroneous di-agnosis due to, for example, the existence of signs of differentdiseases to different degrees [3]. This type of imaging test is oneof the most common in various diagnoses due to its low cost,∗Corresponding author at: Computer Systems Engineering Department,Universidad Politécnica de Madrid, Alan Turing s/n, Madrid, 28031, Spain.E-mail addresses: helena.liz@urjc.es (H. Liz), javier.huertas.tato@upm.es(J. Huertas-Tato), manuel.smontanes@uam.es (M. Sánchez-Montañés),javier.delser@tecnalia.com (J. Del Ser), david.camacho@upm.es (D. Camacho).speed of acquisition and the fact that it does not require muchpreparation [4]. Chest X-rays are useful for detecting a variety ofdiseases of the chest related to different organs such as the heart,lungs or bones. The features of X-rays make them suitable foranalysis with convolutional neural networks (CNN) [5]. The com-bination of AI algorithms and medical knowledge can improve theperformance of medical staff [6] and could also reduce patientwaiting times by speeding up the diagnostic process and reducingthe workload of doctors.CNNs have been a breakthrough in computer vision due totheir ability to extract features from images. These architecturesare composed of different layers. The first has convolutional lay-ers that are inspired by the notion of cells in visual neuroscience.The architectures are based on the visual cortex of animals. Themain reason why these architectures have stood out is their greatcapacity to extract patterns from data, improving the perfor-mance of previous systems based on Machine Learning models.This advantage has made them a benchmark in Deep Learningdue to their high performance in a wide range of tasks, such asspeech recognition, computer vision or text analysis [7].https://doi.org/10.1016/j.future.2023.03.0050167-739X/© 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\fH. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306The properties of chest X-rays make them susceptible to beanalysed by this type of algorithms. Some of the main advantagesof CNNs over traditional techniques are that it is not necessary tomanually extract image features or perform segmentation, andthat by being able to learn from large volumes of data theycan identify patterns that are difficult for the human eye to de-tect. Although in this article we focus on classification problems,other problems can be solved, such as X-ray segmentation [8],localisation, regression (such as predicting drug dosage), amongothers. CNNs are a potential tool for the analysis of chest ra-diographs. However, most of the work in this field focuses onbinary and multiclass classification problems. Actual problemsare usually more complex than the above; they tend to be mul-tilabel classification problems, i.e. the different labels are notmutually exclusive, whereas in binary and multiclass classifica-tion problems there is only one label per radiograph [9]. To solvemultilabel problems, we need to explore new strategies. Adaptingalgorithms can interpret this kind of problem by transformingthem into simpler problems that can be solved by traditionalalgorithms, i.e., transforming them into binary problems [10]. Inthe field of chest X-rays we can find samples without labels,healthy patients and samples with radiological signs of severaldiseases at the same time. On the other hand, there are a largenumber of different radiological signs in chest X-rays, so if wewant to build and validate a system that approximates realisticconditions, we have to use a dataset with a large number ofmutually non-exclusive labels. This is the case of the PadChestdatabase [11], which has 174 different radiological signs, substan-tially increasing the degree of realism and the complexity of theproblem.Many machine learning algorithms, including CNNs, work bestwhen the classes in the dataset are balanced. However, in reallife it is common to find datasets where this condition is notmet; they are imbalanced datasets, where one or more classeshave substantially more examples than the rest. As a conse-quence, with such datasets, machine learning algorithms learna bias towards the majority class, even though the minorityclass is often more relevant. Therefore, it is necessary to applydifferent methods to improve the recognition rate [12]. Thereare several options to overcome this difficulty: (a) modify thedataset, reducing the samples from the majority class or increas-ing the number of samples from the minority class; (b) modifythe algorithms to alleviate their bias towards the majority class,e.g. weighted learners [13]. The problem of unbalanced databasesis exacerbated in multilabel classification problems, where mul-tiple minority classes may appear, making this challenge moredifficult to solve. In medicine, it is widespread because eachdisease has a different incidence in the population. Heart disor-ders top the list of the deadliest diseases, followed by chronicobstructive pulmonary disease, which causes more than 6 milliondeaths a year. In contrast, other diseases such as lung cancerare the sixth leading cause of death with less than 2 milliondeaths, according to the World Health Organization.1 As a result,most radiographic datasets are imbalanced; a clear example isPadChest, the dataset used in this article, where the number ofsamples in each class approximates the incidence published bythe World Health Organization.These algorithms, like many other Deep Learning and MachineLearning methods, are considered ‘‘black box’’ algorithms becauseend users can only analyse the input and output, but the inferenceprocess is opaque, which reduces confidence in these algorithms.To alleviate this problem, explainable AI techniques have beendeveloped, such as saliency maps, which produce heatmaps thatFig. 1. Visualmethodology.representation ofthe problem and the objective ofthehighlight the pixels with the greatest influence on the final pre-diction [14]. This problem is serious in medicine, where errorscan be dangerous for patients [15]. For this reason, explainableAI techniques are essential, as they allow users to understandhow the system has arrived at the final result and use it to helpdiagnose [16]. However, the combination of medical knowledgeand AI has many advantages, such as helping to reduce medicalerrors and speeding up diagnostic processes, leading to improvedpatient care, as doctors would have more time to attend patients.The contribution of this manuscript is a methodology, seeFig. 1, for classi",
            {
                "entities": [
                    [
                        5114,
                        5142,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptNeurocomputing. Author manuscript; available in PMC 2016 January 05.Published in final edited form as:Neurocomputing. 2015 January 5; 147: 485–491. doi:10.1016/j.neucom.2014.06.037.Motion sequence analysis in the presence of figural cuesPawan Sinha1 and Lucia M. Vaina1,2,31Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA2Departments of Biomedical Engineering, Neuroscience and Neurology, Boston University3Department of Neurology, Harvard Medical School, Boston, MAAbstractThe perception of 3D structure in dynamic sequences is believed to be subserved primarily through the use of motion cues. However, real-world sequences contain many figural shape cues besides the dynamic ones. We hypothesize that if figural cues are perceptually significant during sequence analysis, then inconsistencies in these cues over time would lead to percepts of non-rigidity in sequences showing physically rigid objects in motion. We develop an experimental paradigm to test this hypothesis and present results with two patients with impairments in motion perception due to focal neurological damage, as well as two control subjects. Consistent with our hypothesis, the data suggest that figural cues strongly influence the perception of structure in motion sequences, even to the extent of inducing non-rigid percepts in sequences where motion information alone would yield rigid structures. Beyond helping to probe the issue of shape perception, our experimental paradigm might also serve as a possible perceptual assessment tool in a clinical setting.Keywordsmotion; form-cues; structure-from-motion; stroke patients; 3D-structure-from-motion; figural cues; motion-impaired-patientsIntroductionMotion of objects in the environment induces complex transformations in their images. The human visual system can recover the 3-D structure of the viewed objects and their motion in space by interpreting these image transformations [5, 36]. As early as 1953, Wallach and O’Connell demonstrated humans’ capacity to interpret structure from motion while studying what they termed the ‘kinetic depth effect’ [41. In their experiments, an unfamiliar object © 2014 Elsevier B.V. All rights reserved.Correspondence should be addressed to: Professor Lucia M. Vaina, Brain and Vision Research Laboratory, Department of Biomedical Engineering, Boston University, Boston, MA, 02215, USA, Ph: 617-353-2455; Fax: 617-353-6766; vaina@bu.edu. Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptSinha and VainaPage 2was rotated behind a translucent screen with its shadow being observed from the other side of the screen. In most cases, the viewers were able to describe correctly the 3-D shape of the hidden object and its motion, even when each static shadow projection of the object was unrecognizable and contained no 3-D information.Any vision system that attempts to compute 3-D structure from motion must contend with the problem that the recovery of structure is under-constrained; there are infinitely many 3-D structures consistent with a given pattern of motion in the changing 2-D image. Additional constraint is required to establish a unique interpretation. Computational studies have used the rigidity assumption to derive a unique 3-D structure and motion; they assume that if it is possible to interpret the changing 2-D image as the projection of a rigid 3-D object in motion, then such an interpretation should be chosen [3,7, 11, 14,18,22,23,24,25,26, 35,36,37,45]. The rigidity assumption was suggested by perceptual studies that described a tendency for the human visual system to choose a rigid interpretation of moving elements [9,15,16,41].The rigidity assumption has proven to be a powerful constraint, one that appears sufficient to explain how the human visual system solves the structure from motion problem in general settings. However, some interesting perceptual effects suggest that this notion of sufficiency might need to be revisited, at least insofar as modeling human performance is concerned. According to the rigidity assumption, a rigid object in motion should necessarily be perceived as rigid. But, a few studies have reported instances where displays of rigid objects in motion can give rise to the perception of distorting objects [4,6,10,42,46]. As detailed below, we suggest that these breakdowns of rigidity perception hint at a significant contribution from figural shape cues in the perceptual analysis of dynamic sequences. In this paper, we develop the hypothesis of a role for figural cues in sequence analysis, and present experiments designed to test this hypothesis.The remainder of this paper is organized as follows: we first briefly review the current state of research related to the recovery of 3-D structure from motion and static image attributes; subsequently we present a hypothesis regarding interactions between shape information derived using motion and that derived from figural cues; and in Methods we describe the psychophysical experiment and in Results we present evidence from normal observers as well as stroke patients with perceptual impairments in support of the basic thesis of this paper.Possible strategies for the analysis of dynamic sequencesIn a laboratory setting, it is possible to generate motion stimuli that cleanly dissociate between static and dynamic sources of 3-D information. For instance, the random clusters of dots often used in perceptual studies of recovering structure from motion are carefully controlled so that no single frame has any discernible static organization that may provide hints about the 3-D configuration of the dots. This has been the dominant paradigm of structure from motion research so far. Since there is no static 3-D information in such displays, the question of how statically and dynamically derived 3-D shape estimates interact with each other has been sidestepped.Neurocomputing. Author manuscript; available in PMC 2016 January 05.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptSinha and VainaPage 3In the real world, however, the static and dynamic shape cues are almost always confounded. The objects we see moving, e.g. cars, people and airplanes have non-random static configurations that may be used to derive good 3-D shape estimates. Since these estimates are available simultaneously with those from motion cues, we are faced with the question of whether they play a role in determining the eventual 3D percepts.Let us consider two extreme scenarios. In scenario 1, the visual system uses only motion trajectories of feature points to estimate structure (‘features’ are defined as points of high curvature or other punctuate discontinuities). This has been the typical approach for sequence analysis. The reason for the popularity of this approach is its ability to yield unique structure interpretations based on motion information from only a few frames and features. For instance, Ullman [36] has shown that under orthographic projection, three views of four non-coplanar points are sufficient to guarantee a unique 3-D solution. Longuet-Higgins and Prazdny [18] proved that the instantaneous velocity field and its first and second spatial derivatives at a point admit at most three different 3-D interpretations. Tsai and Huang [35] showed that two perspective views of seven points are also usually sufficient to guarantee uniqueness.While scenario 1 is mathematically elegant and powerful, we should consider whether the human visual system does in fact adopt such a strategy, or perhaps it might incorporate other, non motion-based, cues as well in its analysis of dynamic sequences. This leads us to scenario 2. Here, we treat each frame of a motion sequence as an entity to be analyzed on its own, in terms of the figural cues it contains. These cues provide 3D estimates on a frame-by-frame basis, rather than requiring the use of feature motion trajectories, as prescribed by scenario 1. Several figural cues, such as shading, or texture gradients can provide 3D structure information [7,12, 33,47]. Even in the absence of such gradients, global contour based cues provide powerful constraints for 3-D shape recovery [8,13,17,19,43,44] as demonstrated by computational schemes for the recovery of 3D structures from simple 2D line drawings [20,29,30].Is human analysis of dynamic sequences more akin to scenario 1 (predominant use of motion information), or scenario 2 (predominant use of figural information, when such information is available)? Addressing this question presents a challenge in that for most dynamic sequences, both scenarios tend to produce identical results. For instance, a moving wire-frame cube would be seen as a cube irrespective of whether one uses structure from motion algorithms on the vertex trajectories, or applies shape from contour algorithms on individual frames. In order to overcome this constraint, we need dynamic sequences where the motion based and figural content-based strategies yield different results.Here we use dynamic sequences showing rigid wire-frames in motion, where the wire-frame objects are specially constructed so that their different views suggest different 3D shapes. Conventional structure from motion algorithms would easily recover the true rigid 3D structure of these objects. However, the use of figural cues on a frame by frame basis woul",
            {
                "entities": [
                    [
                        250,
                        278,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Information Fusion 16 (2014) 3–17Contents lists available at SciVerse ScienceDirectInformation Fusionj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / i n f f u sA survey of multiple classifier systems as hybrid systemsMichał Woz´ niak a,⇑, Manuel Graña b, Emilio Corchado ca Department of Systems and Computer Networks, Wroclaw University of Technology, Wroclaw, Polandb Computational Intelligence Group, University of the Basque Country, San Sebastian, Spainc Departamento de Informática y Automática, University of Salamanca, Salamanca, Spaina r t i c l ei n f oa b s t r a c tArticle history:Available online 29 April 2013Keywords:Combined classifierMultiple classifier systemClassifier ensembleClassifier fusionHybrid classifierA current focus of intense research in pattern classification is the combination of several classifier sys-tems, which can be built following either the same or different models and/or datasets buildingapproaches. These systems perform information fusion of classification decisions at different levels over-coming limitations of traditional approaches based on single classifiers. This paper presents an up-to-date survey on multiple classifier system (MCS) from the point of view of Hybrid Intelligent Systems.The article discusses major issues, such as diversity and decision fusion methods, providing a vision ofthe spectrum of applications that are currently being developed.(cid:2) 2013 Elsevier B.V. All rights reserved.1. IntroductionHybrid Intelligent Systems offer many alternatives for unortho-dox handling of realistic increasingly complex problems, involvingambiguity, uncertainty and high-dimensionality of data. They al-low to use both a priori knowledge and raw data to compose inno-vative solutions. Therefore, there is growing attention to thismultidisciplinary research field in the computer engineering re-search community. Hybridization appears in many domains of hu-man activity. It has an immediate natural inspiration in the humanbiological systems, such as the Central Nervous System, which is ade facto hybrid composition of many diverse computational units,as discussed since the early days of computer science, e.g., byvon Neumann [1] or Newell [2]. Hybrid approaches seek to exploitthe strengths of the individual components, obtaining enhancedperformance by their combination. The famous ‘‘no free lunch’’ the-orem [3] stated by Wolpert may be extrapolated to the point ofsaying that there is no single computational view that solves allproblems. Fig. 1 is a rough representation of the computational do-mains covered by the Hybrid Intelligent System approach. Some ofthem deal with the uncertainty and ambiguity in the data by prob-abilistic or fuzzy representations and feature extraction. Othersdeal with optimization problems appearing in many facets of the⇑ Corresponding author.E-mail addresses: michal.wozniak@pwr.wroc.pl (M. Woz´ niak), ccpgrrom@g-mail.com (M. Graña), escorchado@usal.es (E. Corchado).1566-2535/$ - see front matter (cid:2) 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.inffus.2013.04.006intelligent system design and problem solving, either following anature inspired or a stochastic process approach. Finally, classifiersimplementing the intelligent decision process are also subject tohybridization by various forms of combination. In this paper, wefocus in this specific domain, which is in an extraordinary efferves-cence nowadays, under the heading of Multi-Classifier Systems(MCS). Referring to classification problems, Wolpert’s theoremhas an specific lecture: there is not a single classifier modeling ap-proach which is optimal for all pattern recognition tasks, sinceeach has its own domain of competence. For a given classificationtask, we expect the MCS to exploit the strengths of the individualclassifier models at our disposal to produce the high quality com-pound recognition system overcoming the performance of individ-ual classifiers. Summarizing:(cid:2) Hybrid Intelligent Systems (HIS) are free combinations of compu-tational intelligence techniques to solve a given problem, cover-ing al computational phases from data normalization up to finaldecision making. Specifically, they mix heterogeneous funda-mental views blending them into one effective working system.(cid:2) Information Fusion covers the ways to combine informationsources in a view providing new properties that may allow tosolve better or more efficiently the proposed problem. Informa-tion sources can be the result of additional computationalprocesses.(cid:2) Multi-Classifier Systems (MCS) focus on the combination ofclassifiers form heterogenous or homogeneous modeling back-grounds to give the final decision. MCS are therefore a subcate-gory of HIS.\f4M. Woz´niak et al. / Information Fusion 16 (2014) 3–17Fig. 1. Domains of hybrid intelligent systems.on three well known academic search sites. The growth in the num-ber of publications has an exponential trend. The last entry of theHistorical perspective. The concept of MCS was first presented byChow [4], who gave conditions for optimality of the joint decision1of independent binary classifiers with appropriately defined weights.In 1979 Dasarathy and Sheela combined a linear classifier and one k-NN classifier [6], suggesting to identify the region of the featurespace where the classifiers disagree. The k-NN classifier gives the an-swer of the MCS for the objects coming from the conflictive regionand by the linear one for the remaining objects. Such strategy signif-icantly decreases the exploitation cost of whole classifier system.This was the first work introducing a classifier selection concept,however the same idea was developed independently in 1981 byRastrigin and Erenstein [7] performing first a feature space partition-ing and, second, assigning to each partition region an individual clas-sifier that achieves the best classification accuracy over it. Otherearly relevant works formulated conclusions regarding MCS ’s classi-fication quality, such as [8] who considered a neural network ensem-ble, [9] with majority voting applied to handwriting recognition,Turner in 1996 [10] showed that averaging outputs of an infinitenumber of unbiased and independent classifiers can lead to the sameresponse as the optimal Bayes classifier, Ho [11] underlined that adecision combination function must receive useful representationof each classifier’s decision. Specifically, they considered severalmethod based on decision ranks, such as Borda count. Finally, thelandmark works devoted introducing bagging [12] and boosting[13,14] which are able to produce strong classifiers [15], in the (Prob-ably Approximately Correct) theory [16] sense, on the basis of theweak one. Nowadays MCS, are highlighted by review articles as ahot topic and promising trend in pattern recognition [17–21]. Thesereviews include the books by Kuncheva [22], Rokach [23], Seni andEdler [24], and Baruque and Corchado [25]. Even leading-edge gen-eral machine learning handbooks such as [26–28] include extensivepresentations of MCS concepts and architectures. The popularity ofthis approach is confirmed by the growing trend in the number ofpublications shown in Fig. 2. The figure reproduces the evolutionof the number of references retrieved by the application of specifickeywords related to MCS since 1990. The experiment was repeated1 We can retrace decision combination long way back in history. Perhaps the firstworthy reference is the Greek democracy (meaning government of the people) rulingthat full citizens have an equal say in any decision that affects their life. Greeksbelieved in the community wisdom, meaning that the rule of the majority willproduce the optimal joint decision. In 1785 Condorcet formulated the Jury Theoremabout the misclassification probability of a group ofindependent voters [5]],providing the first result measuring the quality of classifier committee.Fig. 2. Evolution of the number of publications per year ranges retrieved from thekeywords specified in the plot legend. Each plot corresponds to searching site: thetop to Google Scholar; the center to the Web of Knowledge, the bottom to Scopus.The first entry of the plots is for publications prior to 1990. The last entry is only forthe last 2 years.\fM. Woz´niak et al. / Information Fusion 16 (2014) 3–175plots corresponds to the last 2 years, and some of the keywords giveas many references as in the previous 5 years.Advantages. Dietterich [29] summarized the benefits of MCS: (a)allowing to filter out hypothesis that, though accurate, might beincorrect due to a small training set, (b) combining classifierstrained starting from different initial conditions could overcomethe local optima problem, and (c) the true function may be impos-sible to be modeled by any single hypothesis, but combinations ofhypotheses may expand the space of representable functions.Rephrasing it, there is widespread acknowledgment of the follow-ing advantages of MCS:(cid:2) MCS behave well in the two extreme cases of data availability:when we have very scarce data samples for learning, and whenwe have a huge amount of them at our disposal. In the scarcitycase, MCS can exploit bootstrapping methods, such as baggingor boosting. Intuitive reasoning justifies that the worst classifierwould be out of the selection by this method [30], e.g., by indi-vidual classifier output averaging [31]. In the event of availabil-ity of a huge amount of learning data samples, MCS allow totrain classifiers on dataset’s partitions and merge their decisionusing appropriate combination rule [20].(cid:2) Combined classifier can outperform the best individual classi-fier [32]. Under some conditions (e.g., majority voting by agroup of independent classifiers) this improvement has beenproven analytically [10].(cid:2) Many machine learning algorithms are de facto heuristic searchalgorithms. For example the popular decision tree inductionmethod C4.5 [33] uses a",
            {
                "entities": [
                    [
                        3099,
                        3127,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fPattern Recognition 123 (2022) 108403 Contents lists available at ScienceDirect Pattern Recognition journal homepage: www.elsevier.com/locate/patcog Fitbeat: COVID-19 estimation based on wristband heart rate using a contrastive convolutional auto-encoder Shuo Liu a , ∗, Jing Han a , b , Estela Laporta Puyal c , d , Spyridon Kontaxis c , d , Shaoxiong Sun e , Patrick Locatelli f , Judith Dineley a , Florian B. Pokorny a , g , Gloria Dalla Costa h , Letizia Leocani h , Ana Isabel Guerrero i , Carlos Nos i , Ana Zabalza i , Per Soelberg Sørensen j , Mathias Buron j , Melinda Magyari j , Yatharth Ranjan e , Zulqarnain Rashid e , Pauline Conde e , Callum Stewart e , Amos A Folarin e , k , Richard JB Dobson e , k , Raquel Bailón c , d , Srinivasan Vairavan l , Nicholas Cummins a , e , Vaibhav A Narayan l , Matthew Hotopf m , n , Giancarlo Comi o , Björn Schuller a , p , RADAR-CNS Consortium q a EIHW – Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany b Department of Computer Science and Technology, University of Cambridge, Cambridge, United Kingdom c BSICoS Group, Aragón Institute of Engineering Research (I3A), IIS Aragón, University of Zaragoza, Zaragoza, Spain d CIBER of Bioengineering, Biomaterials and Nanomedicine (CIBER-BNN), Madrid, Spain e The Department of Biostatistics and Health informatics, Institute of Psychiatry, Psychology and Neuroscience, King’s College London, London, UK f Department of Engineering and Applied Science, University of Bergamo, Bergamo, Italy g Division of Phoniatrics, Medical University of Graz, Graz, Austria h Università Vita Salute San Raffaele and Experimental Neurophysiology Unit, Institute of Experimental Neurology, Scientific Institute Hospital San Raffaele, Milan, Italy i Multiple Sclerosis Centre of Catalonia (Cemcat), Department of NeurologyNeuroimmunology, Hospital Universitari Vall dH ´ebron, Universitat Autónoma de Barcelona, Barcelona, Spain j Danish Multiple Sclerosis Centre, Department of Neurology, Copenhagen University Hospital Rigshospitalet, Copenhagen, Denmark k Institute of Health Informatics, University College London, London, United Kingdom l Janssen Research and Development LLC, Titusville, NJ, USA m The Department of Psychological Medicine, Institute of Psychiatry, Psychology and Neuroscience, King’s College London, London, United Kingdom n South London and Maudsley National Health Service Foundation Trust, London, United Kingdom o Università Vita Salute San Raffaele, Casa di Cura Privata del Policlinico, Milan, Italy p GLAM – Group on Language, Audio, & Music, Imperial College London, London, United Kingdom q The RADAR-CNS Consortium, London, United Kingdom a r t i c l e i n f o a b s t r a c t Article history: Received 5 April 2021 Revised 30 August 2021 Accepted 24 October 2021 Available online 26 October 2021 Keywords: COVID-19 Respiratory tract infection Anomaly detection Contrastive learning Convolutional auto-encoder This study proposes a contrastive convolutional auto-encoder (contrastive CAE), a combined architecture of an auto-encoder and contrastive loss, to identify individuals with suspected COVID-19 infection using heart-rate data from participants with multiple sclerosis (MS) in the ongoing RADAR-CNS mHealth re- search project. Heart-rate data was remotely collected using a Fitbit wristband. COVID-19 infection was either confirmed through a positive swab test, or inferred through a self-reported set of recognised symp- toms of the virus. The contrastive CAE outperforms a conventional convolutional neural network (CNN), a long short-term memory (LSTM) model, and a convolutional auto-encoder without contrastive loss (CAE). On a test set of 19 participants with MS with reported symptoms of COVID-19, each one paired with a participant with MS with no COVID-19 symptoms, the contrastive CAE achieves an unweighted average recall of 95 . 3% , a sensitivity of 100% and a specificity of 90 . 6% , an area under the receiver operating char- acteristic curve (AUC-ROC) of 0.944, indicating a maximum successful detection of symptoms in the given heart rate measurement period, whilst at the same time keeping a low false alarm rate. © 2021 Elsevier Ltd. All rights reserved. ∗ Corresponding author. E-mail address: shuo.liu@informatik.uni-augsburg.com (S. Liu). URL: http://www.radar-cns.org (R.-C. Consortium) https://doi.org/10.1016/j.patcog.2021.108403 0031-3203/© 2021 Elsevier Ltd. All rights reserved. \fS. Liu, J. Han, E.L. Puyal et al. 1. Introduction Remote passive monitoring of physiological and behavioural characteristics using smartphones and wearable devices can be used to rapidly collect a variety of data in huge volumes with min- imal effort from the wearer. Such data has the potential to improve our understanding of the interplay between a variety of health conditions at individual and population level, if rigorously collected and validated [1] . Passive data collection is typically implemented with a high temporal resolution [2] . Wearable fitness trackers, for example, estimate parameters such as heart rate up to every sec- ond and up to 24 hours a day. Monitoring individuals with a range of health states, lifestyles, and demographic variables in combina- tion with data artefacts and missing data leads to high variability, while multiple data streams, from heart rate and physical activity to GPS-based location, can be collected. Therefore, studies using wearables and smartphones in this way exhibit several vs of big data: velocity, volume, variability and variety. As such, advanced analysis methodologies such as deep learning can potentially make a significant contribution [3] , particularly in the context of infec- tious diseases, such as COVID-19, the disease caused by the novel corona virus (SARS-CoV-2). Specific applications include individual screening and population-level monitoring that minimise contact with infected individuals [4,5] . Since the outbreak of the COVID-19 pandemic in 2020, sev- eral deep learning methodologies have been applied to computed tomography (CT) scans [6] and 2D X-ray images [7] to detect COVID-19. These methods require specific clinical equipment and the patient must attend a clinical facility. Consequently, it cannot achieve early, automatic detection when COVID-19 symptoms first appear. In contrast, heart rate can be measured remotely and non- intrusively using wearable devices. Heart rate is a biomarker of particular value in such appli- cations. Patterns in heart rate fluctuations over time have been found to provide clinically relevant information about the integrity of the physiological system generating these dynamics. Previous studies have not only revealed an altered heart rate variability in a number of medical conditions [8] , but also demonstrated that the degree of short-term heart rate alteration correlates with ill- ness severity. Analysis of the autonomic regulation of heart rate has also been discussed as a promising approach for detecting infections earlier than conventional clinical methods and making prognoses [9] . Wearables such as Fitbit fitness trackers 1 provide indirect measurements of the heart rate through pulse rate estimates made using photoplethysmography (PPG). In the ongoing DETECT 2 study [5] , researchers are focusing on monitoring outbreaks of vi- ral infections including COVID-19 based on the resting heart rate collected in this way [10] . Other similar ongoing endeavours in- clude the German project Corona-Datenspende 3 , which has a co- hort of over 50 0 0 0 0 volunteers, and the TemPredict study in the US 4 . Applied to such data sets, deep learning has the potential to au- tomatically identify individuals with COVID-19 purely on the basis of data passively acquired by means of wearable devices [5,11,12] . To the best of our knowledge, the present study is the first to com- pare deep learning approaches in predicting the presence or ab- sence of COVID-19-like symptoms using Fitbit-measured heart rate data. We aim to exploit state-of-the-art methods to represent the 1 2 3 4 https://www.fitbit.com/ [as of 03 August 2021]. http://detectstudy.org/ [as of 03 August 2021]. http://corona-datenspende.de/science/en/ [as of 03 August 2021]. http://osher.ucsf.edu/research/current-research-studies/tempredict [as of 03 Au- gust 2021]. 2 Pattern Recognition 123 (2022) 108403 problem by feature maps, including convolutional neural networks (CNNs) and a convolutional auto-encoder (CAE) [13] . Considering the deficiency of class information in training a standard CAE, in some previous works, the class information was applied to latent attribute layers, leading to the supervised auto- encoder introduced in [14] . Cross-entropy losses are used to min- imise the difference between predicted labels from latent at- tributes and true labels. This approach provides a certain preser- vation of the reconstructed feature map, taking the cross-entropy loss as a regularisation method. The reconstruction error and cross entropy loss are jointly optimised. However, the optimisa- tion of the joint loss requires a proper combination factor in or- der to balance the optimisation on reconstruction",
            {
                "entities": [
                    [
                        5183,
                        5211,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "ArticleDeep forecasting of translational impact in medicalresearchHighlightsd Deep learning models of biomedical paper content canaccurately predict translationd Deep content models substantially outperform traditionalcitation metricsd Models trained on patent inclusion transfer to predictingNobel Prize-preceding papersAuthorsAmy P.K. Nelson, Robert J. Gray,James K. Ruffle, ..., Bryan Williams,Geraint E. Rees, Parashkev NachevCorrespondenceamy.nelson@ucl.ac.uk (A.P.K.N.),p.nachev@ucl.ac.uk (P.N.)d Science policy is potentially better informed by deep contentmodels than by citationsIn briefAnalyzing 43.3 million biomedical papersfrom 1990–2019, we show that deeplearning models of publication title andabstract content can predict inclusion in apatent, guideline, or policy documentwith far greater fidelity than citationmetrics alone. If judgments of thetranslational potential of science are to bebased on objective metrics, then complexmodels of paper content should bepreferred over citations.Nelson et al., 2022, Patterns 3, 100483May 13, 2022 ª 2022 The Authors.https://doi.org/10.1016/j.patter.2022.100483ll\fllOPEN ACCESSArticleDeep forecasting of translationalimpact in medical researchAmy P.K. Nelson,1,7,* Robert J. Gray,1 James K. Ruffle,1 Henry C. Watkins,1 Daniel Herron,2 Nick Sorros,3 Danil Mikhailov,3M. Jorge Cardoso,4 Sebastien Ourselin,4 Nick McNally,2 Bryan Williams,2,5 Geraint E. Rees,1,6 and Parashkev Nachev1,*1High Dimensional Neurology Group, UCL Queen Square Institute of Neurology, University College London, Russell Square House,Bloomsbury, London WC1B 5EH, UK2Research & Development, NIHR University College London Hospitals Biomedical Research Centre, London WC1E 6BT, UK3Wellcome Data Labs, Wellcome Trust, London NW1 2BE, UK4School of Biomedical Engineering & Imaging Sciences, King’s College London, London WC2R 2LS, UK5UCL Institute of Cardiovascular Sciences, University College London, London WC1E 6BT, UK6Faculty of Life Sciences, University College London, Gower Street, London WC1E 6BT, UK7Lead contact*Correspondence: amy.nelson@ucl.ac.uk (A.P.K.N.), p.nachev@ucl.ac.uk (P.N.)https://doi.org/10.1016/j.patter.2022.100483THE BIGGER PICTURE The relationship of scientific activity to real-world impact is hard to describe andeven harder to quantify. Analyzing 43.3 million biomedical papers from 1990–2019, we show that deeplearning models of publication, title, and abstract content can predict inclusion of a scientific paper in a pat-ent, guideline, or policy document. We show that the best of these models, incorporating the richest infor-mation, substantially outperforms traditional metrics of paper success—citations per year—and transfersto the task of predicting Nobel Prize-preceding papers. If judgments of the translational potential of scienceare to be based on objective metrics, then complex models of paper content should be preferred over ci-tations. Our approach is naturally extensible to richer scientific content and diverse measures of impact. Itswider application could maximize the real-world benefits of scientific activity in the biomedical realm andbeyond.Development/Pre-production: Data science output has beenrolled out/validated across multiple domains/problemsSUMMARYThe value of biomedical research—a $1.7 trillion annual investment—is ultimately determined by its down-stream, real-world impact, whose predictability from simple citation metrics remains unquantified. Here wesought to determine the comparative predictability of future real-world translation—as indexed by inclusion inpatents, guidelines, or policy documents—from complex models of title/abstract-level content versus citationsand metadata alone. We quantify predictive performance out of sample, ahead of time, across major domains,using the entire corpus of biomedical research captured by Microsoft Academic Graph from 1990–2019, encom-passing 43.3 million papers. We show that citations are only moderately predictive of translational impact. Incontrast, high-dimensional models of titles, abstracts, and metadata exhibit high fidelity (area under the receiveroperating curve [AUROC] > 0.9), generalize across time and domain, and transfer to recognizing papers of Nobellaureates. We argue that content-based impact models are superior to conventional, citation-based measuresand sustain a stronger evidence-based claim to the objective measurement of translational potential.INTRODUCTIONScientometrics has existed for only a small fraction of the historyof science itself, sparked by the logical empiricists of the ViennaCircle in their philosophical quest to construct a unified languageof science.1 Developed into the familiar, citation-centered formthrough arduous manual extraction in the mid-20th century,2,3its indicators have proliferated in the Internet age. They nowdominate the research landscape, routinely informing majorfunding decisions and academic staff recruitment worldwide.4–8Patterns 3, 100483, May 13, 2022 ª 2022 The Authors. 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fllOPEN ACCESSThe importance of the original goal has become magnifiedover time: to measure scientific progress regardless of fundingor ideology, uncolored by the reputations of individuals or insti-tutions. But the fundamental focus of its current solution—thevolume and density of discussion in print—is detached fromthe ultimate, real-world objective and subject to familiar distor-tions, such as the popularity of papers notable only for beingspectacularly wrong.9–11These concerns are amplified in medical science, whose pri-mary focus is not merely knowledge but impact on patient health:necessarily a consequence rather than a constitutive character-istic of research activity, neither easily benchmarked nor directlyoptimized. And there is no doubt that optimization is needed;over the past 60 years, the number of new drug approvals perunit R&D spend has consistently halved every 9 years, whereaspublished medical research has doubled with the same period-icity,12 and only 0.004% of basic research findings ultimatelylead to clinically useful treatments.13 The critical pre-requisitefor all research—funding—shows substantial randomness in itsdistribution,14 enough for at least one major healthcare funderto award grants by lottery.15 Any decision function based onrandom chance, or a process demonstrably not much betterthan random chance, leaves room for improvement, particularlywhen commanding approximately $1.7 trillion global annual in-vestment across the United States, Japan, South Korea, andthe European Union.16Is this state of affairs partially caused by over-reliance onmisleading scientometrics, have we simply not found the rightmetrics yet, or is the relation between scientific activity andconsequent impact opaque to objective analysis? To addressthese crucial questions, we need a fully inclusive survey ofpublished medical research that relates its characteristics toan independently measured translational outcome as close toreal-world impact as can be quantified. This relationshipmust be explored with models of sufficient expressivity todetect complex relations between many candidate predictivefactors beyond paper-to-paper citations. The extant literatureis largely limited to modeling keywords or simple representa-tions of semantic content,17–21 within specific subdomains, orcomparatively restricted bibliographic databases,22–26 andwithout exploration of the impact of data dimensionality andmodel flexibility.Here we provide the first comprehensive, field-wide analysisof translational impact measured by its most widely acceptedproximalindices—patents, guidelines, or policies—based on29 years of data from the medical field encompassing 43.3million published papers. We quantify the ability to predict inclu-sion in future patents, guidelines, or policies from conventionalage-normalized citation counts and compare this with the pre-dictive fidelity of deep learning models incorporating more com-plex features extracted from metadata, titles, and abstracts. Weevaluate the performance of the best model across time and the-matic domain and in transfer to the task of recognizing papers ofNobel laureates. We derive succinct, surveyable representationsof paper title and abstract content with deep autoencoding oftransformer-based text embeddings and of publication meta-data with stochastic block models. The breadth and depth ofanalysis allow us to draw strong conclusions about the compar-ative fidelity of conventional bibliographic and novel semantic2 Patterns 3, 100483, May 13, 2022Articlepredictors of translational impact, with substantial implicationsfor research policy.RESULTSCitationsOver the period from January 1990 to March 2019, only 17.1million of the 43.3 million published papers categorized as med-ical by Microsoft Academic Graph were cited at least once. Ofthese, 964,403 were included in a patent and 16,752 in a guide-line or a policy document. Included papers were more frequentlycited, but the numbers of citations and inclusions were weaklycorrelated (Pearson’s r = 0.094 for guidelines or policies, r =0.248 for patents; Figure 1). The mean time delay from paperpublication to first patent inclusion was 4.73 years (SD 4.54;Figure S1).Predictive performanceA series of models was developed to investigate the relativecontribution of three data modalities—annual paper citations,metadata only, and the combination of metadata and abstract/ti-tle embeddings—in predicting two translational outcomes: apaper’s inclusion in a patent or policy/guideline reference list. At-tempting to predict inclusion in a guideline or policy documentfrom the traditional measure of impact—annual paper cita-tions—yielded a mean cross-validated area under the receiveroperating curve (AUROC) of 0.766 with univariable logisticregression (Citations-LogisticRegression) and 0.767 with anoptimized univariable multilayer perceptron (MLP) model (",
            {
                "entities": [
                    [
                        1091,
                        1119,
                        "DOI"
                    ],
                    [
                        2140,
                        2168,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Computers in Biology and Medicine journal homepage: www.elsevier.com/locate/compbiomed A review of deep learning-based detection methods for COVID-19 Nandhini Subramanian *, Omar Elharrouss, Somaya Al-Maadeed, Muhammed Chowdhury Qatar University College of Engineering, Computer Science and Engineering, Qatar  A R T I C L E I N F O  A B S T R A C T  Keywords: COVID-19 detection DL-Based COVID-19 detection Lung image classification Coronavirus pandemic Medical image processing COVID-19 is a fast-spreading pandemic, and early detection is crucial for stopping the spread of infection. Lung images are used in the detection of coronavirus infection. Chest X-ray (CXR) and computed tomography (CT) images are available for the detection of COVID-19. Deep learning methods have been proven efficient and better performing in many computer vision and medical imaging applications. In the rise of the COVID pandemic, researchers are using deep learning methods to detect coronavirus infection in lung images. In this paper, the currently available deep learning methods that are used to detect coronavirus infection in lung images are surveyed. The available methodologies, public datasets, datasets that are used by each method and evaluation metrics are summarized in this paper to help future researchers. The evaluation metrics that are used by the methods are comprehensively compared.  1. Introduction The World Health Organization (WHO) declared the spread of the coronavirus infection a pandemic in March 2020, which is called the coronavirus pandemic or COVID-19 pandemic. The coronavirus pandemic is caused by severe acute respiratory syndrome coronavirus 2 (SARS CoV 2). The outbreak originally started in Wuhan, China, and later spread to every country in the world [1]. The coronavirus spreads through respiratory droplets of the infected person that are produced through cough or sneeze. These droplets can further contaminate the surfaces increasing the spread. Coronavirus-infected persons may suffer from mild to severe respiratory illness and may require ventilation support [2]. Older people and people with chronological disorders are easily prone to coronavirus infection. Thus, many governments have closed their borders and locked down people to break the cycle and prevent the spread of the pandemic [3]. With the sequencing of ribonucleic acid (RNA) from the coronavirus, many vaccines are being developed worldwide. The developed vaccines use both traditional and next-generation technology with six vaccine platforms, namely, live attenuated virus, inactivated virus, protein or subunit, viral vector-based, messenger RNA (mRNA), and deoxy-ribonucleic acid (DNA). Although vaccines can reduce the rapid spread and facilitate the development of immunity via the production of suit-able antibodies, the efficacy of the vaccines is still 95%. Many issues are encountered in administering the vaccine, such as supply chain logistical challenges, vaccine hesitancy, and vaccine complacency. A vaccine is a prevention measure rather than a cure [4]. Even with the availability of the vaccine, early detection of the coronavirus is important, as it can facilitate tracing of the people who were in contact directly and indi-rectly. By tracing these people, further spread of the pandemic can be avoided. COVID-19 infection manifests as lung infection, and computed tomography (CT) and chest X-ray (CXR) images are primarily used in the detection of lung infection of any type [5]. Along with doctors and clinical personnel, researchers and technol-ogists are focusing their efforts on early detection of coronavirus in-fections. According to PubMed [6], 755 academic articles were published with the search term “coronavirus” in 2019, and this number rose to 1245 in the first 80 days of 2020. Artificial intelligence and deep learning methods are the most commonly used methods by researchers for the detection of coronavirus infection from CT and CXR images. Deep learning methods have shown significant performance in many research applications, such as computer vision [7], object tracking [8], gesture recognition [9], face recognition [10], and steganography [11–13]. Deep learning methods are widely used because of their improved per-formance compared to traditional methods. In contrast to traditional methods and machine learning methods, the features need not be hand-picked. By changing the parameters and configurations of the deep learning convolutional neural network (CNN) architecture, a model can be trained to learn the best possible features for the dataset in use. Re-searchers have used deep learning methods to explore the field of * Corresponding author. E-mail addresses: ns1808900@qu.edu.qa (N. Subramanian), elharrouss.omar@gmail.com (O. Elharrouss), S_alali@qu.edu.qa (S. Al-Maadeed), mchowdhury@qu. edu.qa (M. Chowdhury). https://doi.org/10.1016/j.compbiomed.2022.105233 Received 6 June 2021; Received in revised form 10 January 2022; Accepted 10 January 2022  ComputersinBiologyandMedicine143(2022)105233Availableonline29January20220010-4825/©2022QatarUniversity.PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fN. Subramanian et al.                                                                                                             medical imaging even before the coronavirus pandemic. With the recent pandemic, the use of deep learning methods for the detection of coro-navirus infection from images has increased tremendously. A detailed survey of the available deep learning approaches for the detection of coronavirus infection from images such as CT scans or CXR images is conducted in this paper. Although other surveys are available in the literature, most of them cover a wider scope. For example, Ulhaq et al. [14] surveyed all methods that address coronaviruses, such as medical image processing, data science methods for pandemic modeling, AI and the Internet of things (IoT), AI for text mining and natural language processing (NLP), and AI in computational biology and medicine. This provides an overall view of what is happening in the research world. A survey on the application of computer vision methods for COVID-19 [15] described the segmentation of lung images. This paper aims to exclusively describe coronavirus detection methods using deep learning methods. In the hope of helping researchers develop better coronavirus detection methods, this paper summarizes all the methods that have been reported in the literature. Along with the methods, the used datasets, commonly used metrics for evaluation and comparison are discussed and future direction are elaborated in this paper. 2. Background Before discussing the details of the available methods for coronavirus infection detection, it is essential to have a working knowledge of deep convolutional neural networks and popular CNN architectures. In this section, a brief overview of CNN architectures and main points on available CNN architectures are presented. 2.1. Convolutional neural networks Convolutional neural networks, specifically artificial neural net-works, are a branch of deep learning methods that are inspired by the natural visual perception mechanism of living organisms [16]. CNNs are nothing but stacked multilayered neural networks. There are three major categories of layers, namely, convolutional layers, pooling layers and fully connected layers. The first layer of any CNN model is an input layer, where the width, height and depth of the input image are specified as the input parameters. Immediately after the input layer, convolu-tional layers are defined with the number of filters, filter window size, stride, padding and activation as the parameters. Convolutional layers are used to extract meaningful feature maps for the input location by calculating the weighted sum [17,18]. Then, each feature map is passed through an activation function, and bias is added to form the output. Usually, rectilinear unit (ReLU) activation is used as the activation function [19]. Pooling layers are used to reduce the size of the output from the convolutional layers. As the model increases in size with an increasing number of filters in the convolutional layer, the output dimensionality also increases exponentially, which makes it hard for computers to handle. Pooling layers are added to reduce the dimensions for easy computation and sometimes to suppress noise. The pooling layer can be a max pooling, average pooling, global average pooling, or spatial pooling layer. The most commonly used pooling layer is a max pooling layer [20]. The output is flattened to form a single-array feature vector, which is fed to a fully connected layer. Finally, a classification layer is defined with activation functions such as sigmoid, softmax and tanh functions [21]. The number of classes is specified in this layer, and the extracted features are aggregated into class scores. Batch normalization layers are applied after the input layer or after the activation layers to standardize the learning process and reduce the training time [22]. Another important parameter is the loss function, which summarizes the error in the predictions during training and validation. The loss is backpropagated to the CNN model after each epoch to enhance the learning process [23]. 2.2. Transfer learning and fine-tuning After designing, creating and building a deep learning model, the number of epochs is set to start training. During training, random weights are initialized, which will be refined during each epoch to make the result closer to the classification score. However, in transfer learning, instead of using random weight values, the model can be initialized with weight values from pretrained models. Transfer learning performs best when there is a limited availability of training data. When performing transfer learning, the last layer of",
            {
                "entities": [
                    [
                        4927,
                        4959,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptNeurocomputing. Author manuscript; available in PMC 2017 January 29.Published in final edited form as:Neurocomputing. 2016 January 29; 175(Pt A): 40–46. doi:10.1016/j.neucom.2015.09.103.The general critical analysis for continuous-time UPPAM recurrent neural networks*Chen Qiao†, Wen-Feng Jing‡, Jian Fang§, and Yu-Ping Wang¶Chen Qiao: qiaochen@mail.xjtu.edu.cn, cqiao@tulane.edu; Wen-Feng Jing: wfjing@mail.xjtu.edu.cn; Jian Fang: jfang3@tulane.edu; Yu-Ping Wang: wyp@tulane.edu†School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, 710049, P.R. China and with the Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA‡School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, 710049, P.R. China§School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, 710049, P.R. China and with the Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA¶Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA and the Center of Genomics and Bioinformatics, Tulane University, New Orleans, LA, 70112, USAAbstractThe uniformly pseudo-projection-anti-monotone (UPPAM) neural network model, which can be considered as the unified continuous-time neural networks (CNNs), includes almost all of the known CNNs individuals. Recently, studies on the critical dynamics behaviors of CNNs have drawn special attentions due to its importance in both theory and applications. In this paper, we will present the analysis of the UPPAM network under the general critical conditions. It is shown that the UPPAM network possesses the global convergence and asymptotical stability under the general critical conditions if the network satisfies one quasi-symmetric requirement on the connective matrices, which is easy to be verified and applied. The general critical dynamics have rarely been studied before, and this work is an attempt to gain an meaningful assurance of general critical convergence and stability of CNNs. Since UPPAM network is the unified model for CNNs, the results obtained here can generalize and extend the existing critical conclusions for CNNs individuals, let alone those non-critical cases. Moreover, the easily verified conditions for general critical convergence and stability can further promote the applications of CNNs.KeywordsContinuous-time recurrent neural network; uniformly pseudo-projection-anti-monotone network; general critical condition; dynamical analysis*This research was supported by NSFC No. 11101327, No. 11471006 and No. 11171270, and was partially supported by NIH R01 GM109068 and R01 MH104680.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.1 IntroductionPage 2The two basic elements of a recurrent neural network (RNN) are: the synaptic connections among the neurons and the nonlinear activation functions deduced from the input-output properties of the involved neurons. For applications such as associative memory, synaptic connections among the neurons are designed to encode the memories we hope to recover. The activation functions are assumed to capture the complex, nonlinear response of neurons of the brain. For different purpose of simulations and applications, both of them are preassigned before use. So understanding their properties are very important, and especially exploring the characteristics of the activation functions are quite crucial to determine the performance of the RNNs. For the commonly used RNN individuals, the activation functions are monotonically nondecreasing and saturated. To study and apply RNNs only based on such two features is far from enough. To overcome the non-thorough descriptions of activation functions, many special cases of activation functions have been brought forward, resulting in many different RNNs individuals. Furthermore, in order to obtain more useful results of RNNs, e.g., the convergence and stability of those individuals, additional strict requirements are unavoidable to impose on the networks for the lack of in-depth descriptions on the activation functions. Obviously, since those individuals are studied separately, it’s inevitable that there exist large numbers of redundancy of analysis for those individual models. In order to reduce the superabundance, establishing a harmonization methodology is a challenging work.In [16], Xu and Qiao put forward two novel concepts: uniformly anti-monotone as well as the pseudo-projection properties of the activation functions, which discover more essential characteristics other than the nondecreasing and bounded properties of the commonly used activation functions. It is shown that the proposed uniformly pseudo-projection anti-monotone (UPPAM) operator can embody most of activation operators (the precise definition of uniformly pseudo-projection-anti-monotone operator will be given in Section II), e.g., nearest-point projection, linear saturating operator, signum operator, symmetric multi-valued step operator, multi-threshold operator, winner-take-all operator, etc. Thus, the UPPAM operator can be considered as a framework of formalizing most of the activation operators of RNNs.In this paper, we use the concept of UPPAM operators to establish a unified model for continuous-time RNNs. Let’s consider the following continuous-time UPPAM RNNs mdoel:(1)where x(t) = (x1(t), x2(t), ···, xN(t))T is the neural network state, G = (g1, g2, ···, gN)T is the nonlinear activation operator deduced from all the activation functions gi, and G owns the uniformly pseudo-projection-anti-monotone property. Both A and W are the connective weight matrices, b, q are two fixed external bias vectors and τ is the state feedback coefficient. The form of model (1) includes two basic kinds of continuous-time RNNs [17], i.e., the static RNNs and the local field RNNs. Furthermore, as proved in [16], most Neurocomputing. Author manuscript; available in PMC 2017 January 29.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 3activation operators are special cases of the UPPAM operator. So, model (1) can be considered as a unified model of continuous-time RNNs and can include almost all of the existing continuous-time RNNs specials [4], e.g., Hopfield-type neural networks, Brain-State-in-a-Box neural networks, Recurrent Back-propagation neural networks, Mean-field neural networks, Bound-constraints Optimization Solvers, Convex Optimization Solvers, Recurrent Correlation Associative Memories neural networks, Cellular neural networks, etc. In addition, since model (1) owns the essential characteristics of the activation functions, i.e., the uniformly anti-monotone as well as the pseudo-projection properties, it can be expected that the analysis of model (1), especially the dynamics analysis can give more in-depth results and provide the unified and concise characterization of the continuous-time RNNs models. The main purpose of this paper will focus on discovering some essential global convergence and stability for the unified model (1), i.e., the critical convergence and stability.For RNNs, one difficult problem of dynamics analysis lies in the critical analysis. Define a discriminant matrixwhere Γ is a positive definite diagonal matrix, P is a diagonal matrix defined by the network, and W and A are the weight matrices. If there exists a positive definite diagonal matrix Γ, such that S(Γ, 2Λ − B) > 0 (i.e., S(Γ, 2Λ − B) is positive definite), where Λ and B are the anti-monotone and pseudo-projection constant matrices of the network (the definitions of them are given in Section II), then RNNs have exponential stability [4]. Many stability results have been achieved for RNNs individuals under various specifications of S(Γ, 2Λ − B) > 0 (typically, when S(Γ, 2Λ − B) > 0 is an M-matrix), and they are called as the non-critical dynamical analysis [1]. On the other hand, if there exists a positive definite diagonal matrix Γ such that S(Γ, V) is negative definite, here V = diag{r1, r2, ···, rN} with each ri > 0 being the maximum inversely Lipschitz constant of gi (i.e., for all s, t ∈ ℛN, |gi(t) − gi(s)| ≥ ri|t − s|), then RNNs are globally exponentially unstable [1, 7]. Since S(Γ, 2Λ − B) > 0 is the sufficient condition on the globally exponential stability of RNNs, and S(Γ, V) ≥ 0 is the necessary condition for RNNs to be globally stable, it is quite natural to explore the gap between S(Γ, 2Λ − B) ≤ 0 (i.e., S(Γ, 2Λ − B) is negative semi-definite) and S(Γ, V) ≥ 0 (i.e., S(Γ, V) is positive definite). Such a gap is called the general critical condition, and the dynamics analysis of RNNs under such condition is referred as the general critical dynamics analysis.For any application and practical design of RNNs, such as pattern recognition, associative memories, or as optimization solvers, the convergence and stability of RNNs are both prerequisite. For instance, when an RNN is used in associative memory or pattern recognition, any pattern we hope to store has to be an equilibrium point of the RNN. In addition, to ensure that each stored pattern can be retrieved even with noises, each equilibrium point must possess the stability. When the RNN is employed as an optimization solver, the possible optimal solutions correspond to the equilibrium of the RNN, and the Neurocomputing. Author manuscript; available in PMC 2017 Janua",
            {
                "entities": [
                    [
                        255,
                        283,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Pattern Recognition 138 (2023) 109400 Contents lists available at ScienceDirect Pattern Recognition journal homepage: www.elsevier.com/locate/patcog Learning from multiple annotators for medical image segmentation Le Zhang a , b , 1 , Ryutaro Tanno d , 1 , Moucheng Xu b , Yawen Huang f , ∗, Kevin Bronik a , Chen Jin b , Joseph Jacob b , c , Yefeng Zheng f , Ling Shao g , Olga Ciccarelli a , Frederik Barkhof a , e , Daniel C. Alexander b , ∗a Queen Square Institute of Neurology, Faculty of Brain Sciences, University College London, London, WC1B 5EH, United Kingdom b Centre for Medical Image Computing, Department of Computer Science, University College London, London, WC1E 6BT, United Kingdom c UCL Respiratory, University College London, London, WC1E 6JF, United Kingdom d Healthcare Intelligence, Microsoft Research, Cambridge, CB1 2FB, United Kingdom e Amsterdam UMC, Vrije Universiteit Amsterdam, Department of Radiology and Nuclear Medicine, Amsterdam, Netherlands f Tencent Jarvis Lab, Shenzhen, China g Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates a r t i c l e i n f o a b s t r a c t Article history: Received 10 May 2022 Revised 18 December 2022 Accepted 5 February 2023 Available online 11 February 2023 Keywords: Multi-Annotator Label fusion Segmentation Supervised machine learning methods have been widely developed for segmentation tasks in recent years. However, the quality of labels has high impact on the predictive performance of these algorithms. This issue is particularly acute in the medical image domain, where both the cost of annotation and the inter-observer variability are high. Different human experts contribute estimates of the ”actual” segmen- tation labels in a typical label acquisition process, influenced by their personal biases and competency levels. The performance of automatic segmentation algorithms is limited when these noisy labels are used as the expert consensus label. In this work, we use two coupled CNNs to jointly learn, from purely noisy observations alone, the reliability of individual annotators and the expert consensus label distribu- tions. The separation of the two is achieved by maximally describing the annotator’s “unreliable behav- ior” (we call it “maximally unreliable”) while achieving high fidelity with the noisy training data. We first create a toy segmentation dataset using MNIST and investigate the properties of the proposed algorithm. We then use three public medical imaging segmentation datasets to demonstrate our method’s efficacy, including both simulated (where necessary) and real-world annotations: 1) ISBI2015 (multiple-sclerosis lesions); 2) BraTS (brain tumors); 3) LIDC-IDRI (lung abnormalities). Finally, we create a real-world mul- tiple sclerosis lesion dataset (QSMSC at UCL: Queen Square Multiple Sclerosis Center at UCL, UK) with manual segmentations from 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label). In all datasets, our method consistently outperforms competing methods and relevant baselines, especially when the number of annotations is small and the amount of disagreement is large. The studies also reveal that the system is capable of capturing the complicated spatial characteristics of annotators’ mistakes. © 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) 1. Introduction The performance of downstream supervised machine learning models is known to be influenced by substantial inter-reader vari- ability when segmenting anatomical structures in medical images [26] . This issue is especially acute in the medical image domain, ∗ Corresponding authors. E-mail addresses: yawenhuang@tencent.com (Y. Huang), d.alexander@ucl.ac.uk (D.C. Alexander) . 1 Authors contributed equally. where labelled data is commonly scarce due to the high cost of annotations. For instance, because of the heterogeneity in lesion lo- cation, size, shape, and anatomical variability across patients [29] , accurate identification of multiple sclerosis (MS) lesions in MRIs is difficult even for experienced experts. Another example [21] shows that glioblastoma (a kind of brain tumour) segmentation had an average inter-reader variability of 74–85%. Segmentation annota- tions of structures in medical image suffer from substantial anno- tation variations, which is exacerbated by disparities in biases and level of expertise [18] . As a result, despite the current quantity of medical imaging data due to almost two decades of digitisation, the world still lacks access to data with curated labels that can be https://doi.org/10.1016/j.patcog.2023.109400 0031-3203/© 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \fL. Zhang, R. Tanno, M. Xu et al. Pattern Recognition 138 (2023) 109400 used by machine learning [14] , necessitating the use of intelligent algorithms to learn robustly from such noisy annotations. Different pre-processing techniques are often used to curate segmentation annotations by fusing labels from different experts in order to minimise inter-reader differences. The most basic and widely used approach is based on a majority vote, with the most representative expert opinion being treated as the expert consen- sus label. In the aggregation of brain tumour segmentation labels, a smarter variant [21] that accounts for class similarity has proven effective. However, one major limitation with such approaches is that all experts are presumed to be equally trustworthy [25] . proposed a label fusion approach, which is called STAPLE. This method explic- itly models individual expert reliability and uses that knowledge to ”weight” their judgments in the label aggregation step. STAPLE has been the go-to label fusion method in the construction of pub- lic medical image segmentation datasets, such as ISLES [27] , MSSeg [11] , and Gleason’19 [12] datasets, after demonstrating its superior- ity over traditional majority-vote pre-processing in various appli- cations. Asman further extended this strategy in [4] by accounting for voxel-wise consensus to solve the issue of annotators’ reliabil- ity being under-estimated. Another extension [5] was proposed to model the annotator’s reliability across different pixels in images. More recently, STAPLE has been modified in numerous ways to en- code the information of the underlying images into the label aggre- gation process in the context of multi-atlas segmentation problems [2,16] where image registration is used to warp segments from la- belled images (”atlases”) onto a new scan. STEP, which is a way to further incorporate the local morphological similarity between atlases and target images in [8] , is a notable example, and sev- eral extensions of this approach, such as [1,6] , have subsequently been examined. However, all of the previous label fusion methods have one major limitation: they don’t have a way to integrate in- formation from distinct training images. This severely restricts the scope of applications to situations in which each image has a rea- sonable number of annotations from multiple experts, which can be prohibitively expensive in practise. Moreover, to model the re- lationship between observed noisy annotations, expert consensus label and reliability of experts, relatively simplistic functions are utilized, which may fail to capture complex characteristics of hu- man annotators. In this paper, we introduce and fully evaluate an unique end- to-end segmentation approach that predicts the reliability of mul- tiple human annotators and the expert consensus label based on noisy labels alone. We use the Morpho-MNIST framework [9] to perform morphometric operations on the MNIST dataset to simu- late a variety of annotator types for evaluation. We also demon- strate the potential in several public medical imaging datasets, namely (i) MS lesion segmentation dataset (ISBI2015) from the ISBI 2015 challenge [7] , (ii) Brain tumour segmentation dataset (BraTS) [21] and (iii) Lung nodule segmentation dataset (LIDC-IDRI) [3] . Furthermore, we create a practical MS lesion segmentation dataset with 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label) to evaluate our model’s performance in real-world data. Experiments on all datasets demonstrate that our method consistently leads to bet- ter segmentation performance compared to widely adopted label- fusion methods and other relevant baselines, especially when the number of available labels for each image is low and the degree of annotator disagreement is high. The main contributions of our approach are: (1) A novel deep CNN architecture is proposed for jointly learn- ing the expert consensus label and the annotator’s label. The pro- posed architecture ( Fig. 1 ) consists of two coupled CNNs where one estimates the expert consensus label probabilities and the other 2 models the characteristics of individual annotators (e.g., tendency to over-segmentation, mix-up between different classes, etc) by es- timating the pixel-wise confusion matrices (CMs) on a per image basis. Unlike STAPLE [25] and its variants, our method models, and disentangles with deep neural networks, the complex mappings from the input images to the annotator behaviours and to the ex- pert consensus label. (2) The parameters of our CNNs are “global variables” that are optimised across different image samples; this enables the model to disentangle robustly the annotators’ mistakes and the expert consensus label based on correlations between similar image sam- ples, even when the number of available annotations is small per image (e.g., a single annotation per image). In contrast, this would not be possible with STAPLE [25] and its variants [5,8] where the annotators’ pa",
            {
                "entities": [
                    [
                        4728,
                        4756,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Edith Cowan University Edith Cowan University Research Online Research Online Research outputs 2022 to 2026 1-1-2022 A review of arthritis diagnosis techniques in artificial intelligence A review of arthritis diagnosis techniques in artificial intelligence era: Current trends and research challenges era: Current trends and research challenges Maleeha Imtiaz Edith Cowan University Syed Afaq Ali Shah Edith Cowan University, afaq.shah@ecu.edu.au Zia ur Rehman Follow this and additional works at: https://ro.ecu.edu.au/ecuworks2022-2026 Part of the Diseases Commons 10.1016/j.neuri.2022.100079 Imtiaz, M., Shah, S. A. A., & ur Rahman, Z. (2022). A review of arthritis diagnosis techniques in artificial intelligence era: Current trends and research challenges. Neuroscience Informatics, 2, Article 100079. https://doi.org/10.1016/j.neuri.2022.100079 This Journal Article is posted at Research Online. https://ro.ecu.edu.au/ecuworks2022-2026/1430 \fNeuroscience Informatics 2 (2022) 100079Contents lists available at ScienceDirectNeuroscience Informaticswww.elsevier.com/locate/neuriArtificial Intelligence in Brain InformaticsA review of arthritis diagnosis techniques in artificial intelligence era: Current trends and research challengesMaleeha Imtiaz a, Syed Afaq Ali Shah b,∗a Joondalup Health Campus, Australiab Centre for AI and Machine Learning, Edith Cowan University, Australiac Cairns Hospital, Cairns, QLD, Australia, Zia ur Rehman ca r t i c l e i n f oa b s t r a c tArticle history:Received 2 January 2022Received in revised form 4 May 2022Accepted 12 May 2022Keywords:Machine learning (ML)Deep learningOsteoarthritisRheumatoid arthritisDeep learning, a branch of artificial intelligence, has achieved unprecedented performance in several domains including medicine to assist with efficient diagnosis of diseases, prediction of disease progression and pre-screening step for physicians. Due to its significant breakthroughs, deep learning is now being used for the diagnosis of arthritis, which is a chronic disease affecting young to aged population. This paper provides a survey of recent and the most representative deep learning techniques (published between 2018 to 2020) for the diagnosis of osteoarthritis and rheumatoid arthritis. The paper also reviews traditional machine learning methods (published 2015 onward) and their application for the diagnosis of these diseases. The paper identifies open problems and research gaps. We believe that deep learning can assist general practitioners and consultants to predict the course of the disease, make treatment propositions and appraise their potential benefits.© 2022 The Author(s). Published by Elsevier Masson SAS. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).1. IntroductionArthritis is a term which is used for various inflammatory con-ditions that affect different parts of the body such as joints, bones, and muscles. It can be of several types such as Osteoarthritis (OA), Rheumatoid Arthritis (RA), juvenile Arthritis, psoriatic arthri-tis, and gouty Arthritis, which can result in stiffness, pain, redness and swelling in the joints [47]. According to [5], it has been re-vealed that about 3.6 million (15%) of people are affected from arthritis which includes 17.9% females and 12.1% males. Moreover, 62% of patients affected from arthritis had Osteoarthritis, 12.7% had rheumatoid arthritis, and 32.1% had suffered from an unspeci-fied form of arthritis. One in every seven Australians has Arthritis [6]. The prevalence of arthritis rises with age, primarily affecting the females (ABS, 2017). Moreover, higher mortality risk is also recorded in patients with rheumatoid arthritis (RA) as compared to the general population [22], [52].Rheumatic diseases are chronic and fluctuating in nature, in-volving complicated and unclear etiology, which further intricates the treatment of this kind of arthritis [12], [57], [52]. Regardless, even from the invention of various biological and synthetic treat-ments for rheumatoid arthritis (RA), the decrease in disease pro-* Corresponding author.E-mail address: afaq.shah@ecu.edu.au (S.A.A. Shah).gression is achieved only in a small subset of patients [23], [36]. Moreover, the clinical experiments for another rheumatic disease that is Osteoarthritis (OA) are not very fruitful due to different disease phenotypes involved in the disease. Therefore, the dis-ease diagnosis at an early stage can slow down its progression, where diagnosis involves numerous imaging modalities such as X-rays, MRI and CT. However, diagnosis techniques, such as Kellgren-Lawrence (KL) grade suffer from subjectivity, as their accuracy heavily depends on the practitioner’s experience [65]. Table 1 pro-vides details of Kellgren and Lawrence grading for completion. In order to make the diagnosis process more systematic and reli-able, computer-aided analysis and predictive modelling is required to overcome the human errors and for early disease detection in places where there are fewer experts available.In addition, to advent an appropriate treatment for arthritis, a data-intensive investigation is essential where artificial intelligence (AI) can play a significant role in disease detection. Exclusively, ma-chine learning (ML), a subfield of AI aims to design data-driven predictive models which possess the ability to learn from the ex-perience regardless of the rules explicitly specified by individuals [23]. It uses methods, algorithms and processes to expose con-cealed associations within data and to produce prescriptive, de-scriptive and predictive tools in order to exploit these associations [27]. Additionally, the advancement of Machine Learning principles and Artificial Intelligence techniques has increased the productiv-https://doi.org/10.1016/j.neuri.2022.1000792772-5286/© 2022 The Author(s). Published by Elsevier Masson SAS. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\fM. Imtiaz, S.A.A. Shah and Z. ur RehmanNeuroscience Informatics 2 (2022) 100079Table 1Kellgren and Lawrence (KL) grading.KL gradeDiagnosis01234No features of osteophytes are presentNarrowing of joint space, doubtful OACertain narrowing of joint space, minor OAMultiple osteophytes, sure joint space narrowing and some sclerotic areas, moderate OALarge osteophytes, severe joint space narrowing, severe sclerosis and bone deformity, Severe OAity and effectiveness in medical imaging research [55]. Machine learning concepts, when applied to medical data, have great po-tential to improve disease diagnosis and early detection of diseases [11], [48], [57], [8]. In clinical settings, these techniques can help medical experts to analyse the disease in a better way to predict potential future issues and treat patients more effectively.Machine learning algorithms are capable of learning useful data representations automatically [40], [50]. They can deal with a vari-ety of data inputs such as genetic information, text e.g., electronic health records, patient cohorts and medical images. Furthermore, it can also learn from the knowledge available from clinical data and generate outcomes by recognising disease patterns, and features. Further, it can also help in optimising treatment strategies. Hence, it is quite evident that ML has helped in significantly filling the gap of automatic learning from clinical experience. Furthermore, Deep learning (DL), is a subfield of ML, which utilises multi-layered neu-ral networks, intensive computational algorithms and big data [23], [46]. Over the last decade, both ML and DL have been used in the field of medicine for medical imaging, and it has been depicted that ML-based decision-making is superior to physicians’ individ-ual clinical trial decisions [23].Inspired by the recent advent of artificial intelligence in medical field, this paper presents a survey of deep learning and traditional machine learning techniques for the diagnosis of osteoarthritis and rheumatoid arthritis. The paper also aims to identify the current challenges and open research problems in this area. In contrast to current review papers [23], [55], [24], [28] which mainly focus on a specific type of arthritis e.g., OA or RA and machine learning tech-niques only, this paper reviews deep learning as well as machine learning methods for the diagnosis of both OA and RA. In addition, this paper also provides detailed information about the publicly available datasets for RA and OA research (Section 4.2). This makes our survey paper different from the existing review articles.The rest of the paper is organised as follows. Section 2 discusses most common arthritis types. Overview of the most popular ma-chine and deep learning techniques is presented in Section 3. An overview of imaging techniques and arthritis datasets is provided in Section 4. Machine learning and deep learning approaches for the diagnosis of arthritis are presented in Section 5 and 6, respec-tively. Section 7 discusses some of the open research problems and research challenges. The paper is concluded in Section 8.2. Arthritis and its typesArthritis is a degenerative disorder associated with human joints that can result in disability. There are numerous types of arthritis such as rheumatoid arthritis, Osteoarthritis, Juvenile Arthritis, Psoriatic arthritis and gout arthritis. In the following, we will briefly discuss rheumatoid arthritis, osteoarthritis and Psori-atic arthritis.2.1. Rheumatoid arthritisRheumatoid Arthritis (RA) is an autoimmune inflammatory dis-order which involves multiple organs affecting one or more joints [27]. It is a disease with unclear etiology and a combination of ge-netic and environmental factors. The complex interactions of these factors affect disease development and progression [29]. In gen-eral, RA is categorised through morning stiffness and inflammation of joints that requires skills and experience f",
            {
                "entities": [
                    [
                        567,
                        594,
                        "DOI"
                    ],
                    [
                        823,
                        850,
                        "DOI"
                    ],
                    [
                        5834,
                        5861,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "llOPEN ACCESSPerspectivePatient and public involvement to build trust in artificialintelligence: A framework, tools, and case studiesSoumya Banerjee,1,* Phil Alsop,1,2 Linda Jones,1 and Rudolf N. Cardinal11University of Cambridge, Cambridge, UK2Cambridge, Cambridgeshire, UK*Correspondence: sb2333@cam.ac.ukhttps://doi.org/10.1016/j.patter.2022.100506THE BIGGER PICTURE Hype and negative news reports about artificial intelligence (AI) abound. Involving pa-tients in healthcare AI projects may help in adoption and acceptance of these technologies. We argue that AIalgorithms should be co-designed with patients and healthcare workers.We show examples of how to involve patients in AI research and how patients can build trust in algorithms.We share some best practices, case studies, a framework, and computational tools.Avenues for future work include guidelines for patient and public involvement in AI healthcare research forfunding bodies and regulatory agencies.An understanding of what AI can and cannot do, and a realistic appraisal of risks and benefits, may help inadoption and democratize access to AI for healthcare.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYArtificial intelligence (AI) is increasingly taking on a greater role in healthcare. However, hype and negativenews reports about AI abound. Integrating patient and public involvement (PPI) in healthcare AI projectsmay help in adoption and acceptance of these technologies.We argue that AI algorithms should also be co-designed with patients and healthcare workers.We specifically suggest (1) including patients with lived experience of the disease, and (2) creating a researchadvisory group (RAG) and using these group meetings to walk patients through the process of AI model build-ing, starting with simple (e.g., linear) models.We present a framework, case studies, best practices, and tools for applying participative data science tohealthcare, enabling data scientists, clinicians, and patients to work together. The strategy of co-designingwith patients can help set more realistic expectations for all stakeholders, since conventional narratives of AIrevolve around dystopia or limitless optimism.INTRODUCTIONMachine learning is increasingly becoming pervasive in health-care. Artificial intelligence (AI) is increasingly taking on a greaterrole in healthcare, especially during the current coronavirus dis-ease 2019 (COVID-19) pandemic.1 However, hype and negativenews reports about AI abound.People do not always understand or trust AI. This overlapswith other concerns people have, such as the security of theirdata. People are not always consulted about AI that might affectthem. Part of the solution is patient and public involvement (PPI).In PPI, the general public and patients are involved in research.The level of involvement varies from project to project. Beinginvolved in a project helps build trust. There is a rich history ofPPI in healthcare. However, it has not been done very much inthe context of modern AI.As misinformation spreads around AI, integrating patient andpublic involvement in healthcare AI projects and clinical trialsmay help in adoption and acceptance of these technologies.We argue that AI software should also be co-designed with pa-tients and that patients should be involved in discussions aroundAI research applied to healthcare.We advocate a collaborative approach where patients, carers,clinicians, and data scientists work together to decide what datawill be used as inputs to computer programs and understandwhy these algorithms made a particular prediction. Recent studieshave raised awareness about designing AI algorithms in closecollaboration with healthcare workers.1 Machine learning re-searchers alone may not be able to appreciate the broader impactof their work and there is a need to involve other stakeholders.2We give examples of work we have done in this area as casestudies (in the section ‘‘case studies: examples of data-focusedPatterns 3, June 10, 2022 ª 2022 The Author(s). 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fllOPEN ACCESSresearch via a research advisory group’’), and make some gen-eral recommendations (in the sections ‘‘framework for buildingtrust and typical patient concerns’’ and ‘‘recommendations’’).We suggest a framework of how patients can build trust in AIand we share tools and resources that can be used to explainthe basics of AI to patients. We developed tools to demonstratekey concepts to the public (section ‘‘tools for outreach andinvolvement’’). We also review the current literature on trust inAI (section subsection ‘‘trust in AI and the role of PPI’’). Wehope that the approach of involving patients, clinicians, anddata scientists in a virtuous cycle of co-design will be used infuture AI projects in healthcare.CASE STUDIES: EXAMPLES OF DATA-FOCUSEDRESEARCH VIA A RESEARCH ADVISORY GROUPIn this section, we describe two projects as case studies. In latersections, we reflect on these projects and present our generalrecommendations.We were conducting research in this area, so we recruited pa-tients and formed a research advisory group (RAG). The RAGmet regularly and discussed data-focused research projectsrelated to severe mental illness. Additional details on the RAGare available in the section ‘‘framework for building trust andtypical patient concerns.’’Analysis of the effect of lithium medication on kidneyfunctionIn this section we describe a patient-led project. Patients withbipolar disorder are sometimes prescribed lithium. Lithium isan effective medication, but long-term use may lead to kidneydamage. A patient in the RAG had suggested looking at hos-pital data to investigate if discontinuing lithium can helprecover kidney function in patients with bipolar disorder takinglithium.The patient was involved in all stages of a researchproject. Our aim was to predict whether stopping lithiumintake, in patients with bipolar disorder (who have been onthe medication for a long time), is associated with reversalof drug-induced renal damage.We used observational from hospital electronic healthcare re-cords systems to answer these questions. We outline the variousdatasets that were used in this work:(1) EPIC prescription data. This is an electronic patient recordsystem operationalin Cambridge University Hospitals (CUH)from October 2014 until present. This system captures all CUH ac-tivity during its period of operation. This includes laboratory testsand prescriptions (these are recorded typically for inpatientsonly) and structured diagnostic codes (for a subset of patientsand a subset of diagnoses). This has features like age, gender,and ethnicity.(2) Meditech data. This is a laboratory system operational inCUH from 1995 until present. This system captures all laboratoryinvestigations data from CUH during its period of operation. Thishas laboratory results like creatinine.Patients with records in both EPIC and Meditech had their re-cords cross-matched before anonymization.We used the following linear mixed effects model (in the R pro-gramming language notation):2 Patterns 3, June 10, 2022PerspectiveeGFR = e0 + boff toff + bonton + ð1j pid + toff + tonÞ(Equation 1)where eGFR is the estimated glomerular filtration rate (eGFR)and is calculated from creatinine, age, gender, and ethnicity(data available from hospital electronic healthcare recordssystem) using the CKD-EPI formula.3 pid is the unique patientidentification numberin the electronic healthcare recordsystem. bon is the rate at which eGFR declines when a patientis on lithium. toffis the cumulative time spent off lithium, andton is the cumulative time spent on lithium. boffis the rate atwhich eGFR is declining for patients off lithium, and bon is therate at which eGFR is declining for patients on lithium. e0,bon, boff , ton, and toff are parameters that are estimated fromthe data.However, using these data on a few thousand patients, the re-sults were inconclusive. This motivated the need to go back tothe RAG and explain the need for more data. We took feedbackfrom patients as to whether we should apply for access to moredata. We also built a tool that explains how, in some cases,having more data can help in estimating parameters of statisticalmodels (see sections ‘‘tools for outreach and involvement’’ and‘‘framework for building trust and typical patient concerns’’).This process of performing research and getting inconclusiveresults also showed patients how research always takes timeand can lead to unexpected roadblocks.We also took the time to explain how to build statistical models.For example, we tried other, simpler formulations before we arrivedat the final model (see Equation 2). This showed patients how re-searchers always incrementally build more complex models.eGFR = e0 + boff toff + bonton + ð1j pidÞ(Equation 2)We explained these models using an example of a simplerlinear model:y = a,x + b(Equation 3)This is a linear model where the value of x is used to predict y(say eGFR). a and b are the parameters of the model, and thesecan be estimated. We explained that estimating means deter-mining the values of a and b from data. Once we explained theconcepts of a linear model, we progressed to more advancedconcepts like confidence intervals.We are currently validating ourresults in an additionalindependent cohort of patients (the Clinical Practice ResearchDatalink [CPRD] research database, which has general practicerecords from the United Kingdom4).At this stage, we also communicated to the patients a numberof caveats. Lithium is an effective medication for managingbipolar disorder,5 and the chances of patients developing renalcomplications is quite small.6 The benefit of discontinuing lithiumshould be carefully weighed against the risk of relapse of thepsychiatric disorder, as has been documented in case studies7and suggested in meta-analysis studies.6Our ",
            {
                "entities": [
                    [
                        323,
                        351,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 984–1006Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintUnderstanding the scalability of Bayesian network inference using cliquetree growth curvesOle J. MengshoelCarnegie Mellon University, NASA Ames Research Center, Mail Stop 269-3, Moffett Field, CA 94035, United Statesa r t i c l ei n f oa b s t r a c tArticle history:Received 16 January 2009Received in revised form 18 May 2010Accepted 18 May 2010Available online 25 May 2010Keywords:Probabilistic reasoningBayesian networksClique tree clusteringClique tree growthC/V -ratioContinuous approximationGompertz growth curvesControlled experimentsRegressionOne of the main approaches to performing computation in Bayesian networks (BNs) isclique tree clustering and propagation. The clique tree approach consists of propagationin a clique tree compiled from a BN, and while it was introduced in the 1980s, there isstill a lack of understanding of how clique tree computation time depends on variationsin BN size and structure. In this article, we improve this understanding by developingan approach to characterizing clique tree growth as a function of parameters that canbe computed in polynomial time from BNs, specifically: (i) the ratio of the number of aBN’s non-root nodes to the number of root nodes, and (ii) the expected number of moraledges in their moral graphs. Analytically, we partition the set of cliques in a clique treeinto different sets, and introduce a growth curve for the total size of each set. For thespecial case of bipartite BNs, there are two sets and two growth curves, a mixed cliquegrowth curve and a root clique growth curve. In experiments, where random bipartiteBNs generated using the BPART algorithm are studied, we systematically increase theout-degree of the root nodes in bipartite Bayesian networks, by increasing the numberof leaf nodes. Surprisingly, root clique growth is well-approximated by Gompertz growthcurves, an S-shaped family of curves that has previously been used to describe growthprocesses in biology, medicine, and neuroscience. We believe that this research improvesthe understanding of the scaling behavior of clique tree clustering for a certain class ofBayesian networks; presents an aid for trade-off studies of clique tree clustering usinggrowth curves; and ultimately provides a foundation for benchmarking and developingimproved BN inference and machine learning algorithms.© 2010 Elsevier B.V. All rights reserved.1. IntroductionBayesian networks (BNs) play a central role in a wide range of automated reasoning applications, including in diagnosis,sensor validation, probabilistic risk analysis, information fusion, and decoding of error-correcting codes [64,6,59,38,37,60,43,58]. A crucial issue in reasoning using BNs, as well as in other forms of model-based reasoning, is that of computationalscalability. Most BN inference problems are computationally hard in the general case [10,63,61,1], thus there may be rea-son to be concerned about scalability. One can make progress on the scalability question by studying classes of probleminstances analytically and experimentally. Such problem instances may come from applications or they may be randomlygenerated. In the area of application BNs, both encouraging and discouraging scalability results have been reported. Forexample, a prominent bipartite BN for medical diagnosis is known to be intractable using current technology [64]. Decodingof error-correcting codes, which can be understood as BN inference, is also not tractable but has empirically been found tobe solvable with high reliability using inexact BN inference [20,37]. On the other hand, it is well-known that BNs that areE-mail address: Ole.Mengshoel@sv.cmu.edu.0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.05.007\fO.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006985tree-structured, including the so-called naive Bayes model, are solvable in polynomial time using exact inference algorithms.There are also encouraging empirical results for application BNs that are “close” to being tree-structured or more generallyfor application BNs that are not highly connected [26,43].Clique tree clustering, where inference takes the form of propagation in a clique tree compiled from a BN, is currentlyamong the most prominent BN inference algorithms [33,2,62]. The performance of tree clustering algorithms depends on aBN’s treewidth or the optimal maximal clique size of a BN’s induced clique tree [16,11,15]. The performance of other exactBN inference algorithms also depends on treewidth. A key research question is, then, how the size of a clique tree generatedfrom a BN (and consequently, inference time) depends on structural measures of BNs. One way to investigate this is throughthe use of random generation from distributions of problem instances [66,5,11,52,23]. Taking this approach, and increasingthe ratio C/V between the number of leaf nodes C and the number of root nodes V in bipartite BNs, an easy-hard-harderpattern along with approximately exponential growth have previously been observed for clique tree clustering for a certainclass of BNs, namely BPART BNs [45].In this article, we develop a more precise understanding of this easy-hard-harder pattern. This is done by formulatingmacroscopic and approximate models of clique tree growth by means of restricted growth curves, which we illustrate byusing bipartite BNs created by the BPARTalgorithm [45]. For the sake of this work, we assume that a clique tree propagationalgorithm, operating on a clique tree compiled from a BN, is executed in order to answer probabilistic queries of interest.We introduce a random variable for total clique tree size. This random variable is, for the case of bipartite BNs, the sum oftwo random variables, one for the size of root cliques and one for the size of mixed cliques. Reflecting the random variablefor total clique tree size, we introduce a continuous growth curve for total clique tree size which is the sum of growthcurves for the size of root cliques and mixed cliques. Of particular interest is the growth curve for root clique size, whereGompertz curves of the form g(∞)e, where g(∞), ζ , and γ are parameters, turn out to be useful. A key findingis that Gompertz growth curves are justified on theoretical grounds and also fit very well to experimental data generatedusing the BPART algorithm [45]. While we emphasize bipartite BNs in this article, we also discuss how to generalize toarbitrary BNs, by using multiple growth curves or translating arbitrary BNs to bipartite BNs via factor graphs [32,70].−ζ e−γ xFor experimentation, we sampled bipartite BNs using an implementation of the BPART algorithm. For the number ofroot nodes, V , we used V = 20 and V = 30. The number of leaf nodes was also varied, thereby creating BNs of varyinghardness; 100 BNs per C/V -level were randomly generated. A clique tree inference system, employing the minimum fill-inweight heuristic, was used to generate clique trees for the sampled BNs. Let W be a random variable representing thenumber of moral edges in moral graphs induced by random BNs. In addition to x = C/V , we consider x = E(W ) as anindependent variable. In experiments, we compared different growth curves and investigated x = C/V versus x = E(W ) asindependent variables for Gompertz growth curves. Linear regression was used to obtain values for the parameters ζ and γbased on a linear form of the Gompertz growth curve; values for g(∞) were obtained by analysis. Gompertz growth curvesare common in biological, medical, and neuroscience research [4,35,17], but have not previously been used to characterizeclique tree growth (except for in our earlier conference paper [41] which this article extends). We provide improved resultscompared to previous research, where an easy-hard-harder pattern and approximately exponential growth of upper boundson optimal maximal clique size as a function of C/V -ratio were established [45].We believe this research is significant for the following reasons. First, analytical growth curves improve the understand-ing of clique tree clustering’s performance for a certain class of BNs, namely BPART BNs. Consider Kepler’s three laws ofplanetary motion, developed using Brahe’s observational data of planetary movement. There is a need to develop similarlaws for clique tree clustering’s performance, and in this article we obtain such laws in the form of Gompertz growth curvesfor BPART BNs [45]. While they admittedly have a strong empirical basis, these Gompertz growth curves give significantlybetter fit to the raw data than alternative curves. Consequently, they provide better insight into the underlying mechanismsof the clique tree clustering algorithm and may be used to approximately predict the performance of the algorithm. Sincethe performance of other exact BN inference algorithms — including conditioning [55,11] and elimination algorithms [34,71,14] — also depends on optimal maximal clique size, our results may have significance for these algorithms as well. A sec-ond benefit of growth curves is that they can be used to summarize performance of different BN inference algorithms ordifferent implementations of the same algorithm on benchmark sets of problem instances, and thereby aid in evaluations.1Suppose that the growth curves g1(x) and g2(x) were obtained by benchmarking slightly different clique tree algorithms.Compared to looking at and evaluating potentially large amounts of raw data, it may be easier to understand the perfor-mance difference between the two algorithms by studying their curves g1(x) and g2(x) or by comparing their respectiveGompertz curve parameter values ζ1 and γ1 versus ζ2 and γ2. A third benefit is that growth curves provide estimates ofresource consumption in terms of clique tree size, estimates that can easily be translated into requirements on memo",
            {
                "entities": [
                    [
                        3856,
                        3884,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fContents lists available at ScienceDirect Computers in Biology and Medicine journal homepage: www.elsevier.com/locate/compbiomed Computational screening of 645 antiviral peptides against the receptor-binding domain of the spike protein in SARS-CoV-2 Md Minhas Hossain Sakib a, Aktiya Anjum Nishat a, Mohammad Tarequl Islam a, Mohammad Abu Raihan Uddin a, Md Shahriar Iqbal a, Farhan Fuad Bin Hossen a, Mohammad Imran Ahmed a, Md Samiul Bashir a, Takbir Hossain a, Umma Sumia Tohura a, Saiful Islam Saif a, Nabilah Rahman Jui a, Mosharaf Alam a, Md Aminul Islam a, Md Mehadi Hasan a, Md Abu Sufian b, Md Ackas Ali a, Rajib Islam a, Mohammed Akhter Hossain c, Mohammad A. Halim d, 1,* a Division of Infectious Diseases and Division of Computer-Aided Drug Design, The Red-Green Research Centre, BICCB, 16 Tejkunipara, Tejgaon, Dhaka, 1215, Bangladesh b School of Pharmacy, Temple University, Philadelphia, PA, 19140, USA c Florey Institute of Neuroscience and Mental Health, University of Melbourne, Melbourne, Victoria 3010, Australia d Department of Physical Sciences, University of Arkansas-Fort Smith, Fort Smith, Arkansas 72913, USA  A R T I C L E I N F O  A B S T R A C T  Keywords: SARS-CoV-2 Antiviral peptide Molecular dynamics simulation Receptor-binding domain Angiotensin converting enzyme 2 The receptor-binding domain (RBD) of SARS-CoV-2 spike (S) protein plays a vital role in binding and inter-nalization through the alpha-helix (AH) of human angiotensin-converting enzyme 2 (hACE2). Thus, it is a po-tential target for designing and developing antiviral agents. Inhibition of RBD activity of the S protein may be achieved by blocking RBD interaction with hACE2. In this context, inhibitors with large contact surface area are preferable as they can form a potentially stable complex with RBD of S protein and would not allow RBD to come in contact with hACE2. Peptides represent excellent features as potential anti-RBD agents due to better efficacy, safety, and tolerability in humans compared to that of small molecules. The present study has selected 645 antiviral peptides known to inhibit various viruses and computationally screened them against the RBD of SARS- CoV-2 S protein. In primary screening, 27 out of 645 peptides exhibited higher affinity for the RBD of S protein compared to that of AH of the hACE2 receptor. Subsequently, AVP1795 appeared as the most promising candidate that could inhibit hACE2 recognition by SARS-CoV 2 as was predicted by the molecular dynamics simulation. The critical residues in RBD found for protein-peptide interactions are TYR 489, GLY 485, TYR 505, and GLU 484. Peptide-protein interactions were substantially influenced by hydrogen bonding and hydrophobic interactions. This comprehensive computational screening may provide a guideline to design the most effective peptides targeting the spike protein, which could be studied further in vitro and in vivo for assessing their anti- SARS CoV-2 activity.  1. Introduction A novel coronavirus is causing widespread respiratory tract in-fections and posing a serious threat to public life and health. Following a devastating first wave, it has emerged as even more dangerous and is causing havoc upon lives around the world. The international committee on taxonomy of viruses (ICTV) officially designated 2019 novel coronavirus (2019-nCov) as SARS-CoV-2 and the disease as COVID-19 [1,2]. This is the third time an animal to human transmission of deadly viruses has been witnessed in the past two decades [3]. The number of affected people and death toll due to COVID-19 are increasing day by day. As of July 26, 2021, 195 million people have been infected and 4.1 million are killed by this deadly virus [4]. SARS-CoV-2 is a positive sense, single-stranded, enveloped, non- * Corresponding author. E-mail address: mhalim1@kennesaw.edu (M.A. Halim).  1 Present Address: Department of Chemistry and Biochemistry, Kennesaw State University, Kennesaw, GA 30144, USA. https://doi.org/10.1016/j.compbiomed.2021.104759 Received 4 May 2021; Received in revised form 3 August 2021; Accepted 7 August 2021  ComputersinBiologyandMedicine136(2021)104759Availableonline10August20210010-4825/©2021ElsevierLtd.Allrightsreserved.\fM.M.H. Sakib et al.                                                                                                              segmented RNA virus belonging to the coronaviridae family [5]. The genome size of SARS-CoV-2 ranges from 29.8 kb to 29.9 kb [6], which encodes four main structural proteins, comprising of spike (S), envelop (E), membrane (M), nucleocapsid (N) proteins, and 16 non-structural proteins (nsp) [5,7]. S protein is a highly N-glycosylated trimeric pro-tein that covers the outer surface of SARS-CoV-2. Each monomer of S protein has a molecular weight of 180 kDa and consists of S1 and S2 subunits [8–10]. The S protein is involved in receptor recognition, membrane fusion, as well as the entry of the virus to host cells. It binds with human angiotensin-converting enzyme 2 (hACE2), which serves as the port of entry for the virus to host lung epithelial cells. hACE2 is found on the surface of many other cell types including epithelial tissues of The upper receptor-binding-domain (RBD) of S1 subunit binds with an alpha helix of the peptidase domain (PD) of the angiotensin-converting enzyme 2 (ACE2) [13,14], which subsequently triggers the fusion of viral and host cellular membrane by S2 subunit [15]. The RBD of S1 protein contains antiparallel β sheets (β1, β2, β3, β4 and β7) with short joining helices and loops forming the core. The shorter β5 and β6 strands, α4 and α5 helices, and loops are inserted between the β4 and β7 strands [16]. respiratory [11,12]. lower tracts and Drug design to counter COVID-19 can be directed towards targeting viral proteins or host-cell proteins. Designing drugs to target viral pro-teins have several benefits since they could be highly specific against the virus while maintaining minimal detrimental effects on host cells. Among the four structural proteins of SARS-CoV-2, design of inhibitors against S1 protein is an effective choice to arrest viral entry to the host cells, which is a key step in the virus infection cycle [17]. Evidently, SARS-CoV-2 exhibits a high nucleotide sequence similarity with SARS-CoV-1 (79.5%) as well as MERS (50%) [18]. Closest relatives, RaTG13-CoV and RmYN02-CoV, share 96.3% nucleotide identity in the whole genome sequence, ~97% nucleotide identity in the long 1 ab open reading frame (ORF1ab), respectively. Notably, S protein from SARS-CoV-1 and SARS-CoV-2 share critical residues within the RBD of the S1 subunit and bind to the same receptor, hACE2, for internalization [16,19,20]. Given the sequence similarity among different viruses, in-hibitors of one virus show great promise as potential therapeutics against others. Moreover, with similarities between critical residues of RBDs, inhibitors known to arrest SARS-CoV-1 entry offer high potential to halt SARS-CoV-2 entry as well. to superior The binding interface of SARS-CoV-2 and hACE2 shares a high contact surface and contains a hydrogen bonding network as well as a hydrophobic region [21]. Therefore, as potential inhibitors of RBD S1 protein, peptides are small molecules because peptide-protein interaction (PPI) has a large contact surface area [22]. As peptides have a molecular weights in between small molecules (<500 Da) and biologics (up to 150,000 Da), they exhibit some unique chemical features. Peptides are also amenable to chemical adjustment and can bind with PPIs that are therapeutically relevant [23]. This class of therapeutics have many advantages over small-molecule medications because they are highly selective, well-tolerated, have fewer side effects, and go through a shorter clinical development and FDA approval process [27,28]. Despite the challenges of short half-lives, rapid clearance, cost, and intravenous administration, several groups including ours are looking into the possibility of using an antiviral peptide to treat covid-19 [24–27]. In this regard, peptide like molecules can be an ideal solution to inhibit RBD S1 and potentially inhibit RBD-hACE2 interaction. In a previous study, the effectiveness of peptides against the S1 protein of SARS-CoV-1 was established [28]. In another study, a corresponding hexapeptide to the ACE-interacting domain of SARS-CoV-2 (AIDS) has been found to disrupt the association between RBD-ACE2 in mice [29]. Karoyan and colleagues designed peptide-mimics which has been found to inhibit S protein with inhibitory concentration (IC50) in nanomolar range [30]. In a previous work, we have computationally demonstrated that peptides known to inhibit RBD of SARS-CoV-1 S1 protein shows great promise against SARS-CoV-2 [31]. In this research, we extended our search to include 645 peptides, experimentally known to inhibit a wide variety of viruses, and compu-tationally screened them against the RBD of SARS-CoV-2 S1 protein. Only RBD of S1 was chosen as a therapeutic target instead of RBD- hACE2 complex, with the primary hypothesis that peptide inhibitors with stronger affinity for RBD than that of hACE2 would prevent the virus from host cellular ent",
            {
                "entities": [
                    [
                        4765,
                        4797,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptFuture Gener Comput Syst. Author manuscript; available in PMC 2022 February 01.Published in final edited form as:Future Gener Comput Syst. 2021 February ; 115: 610–618. doi:10.1016/j.future.2020.09.040.Estimation of laryngeal closure duration during swallowing without invasive X-raysShitong Maoa, Aliaa Sabryb, Yassin Khalifaa, James L Coyleb, Ervin Sejdica,c,*aDepartment of Electrical and Computer Engineering, Swanson School of Engineering, University of Pittsburgh, Pittsburgh, PA 15260 USAbDepartment of Communication Science and Disorders, School of Health and Rehabilitation Sciences, University of Pittsburgh, Pittsburgh, PA 15260 USAcDepartment of Bioengineering, Swanson School of Engineering Department of Biomedical Informatics, School of Medicine Intelligent Systems Program, School of Computing and Information, University of Pittsburgh, Pittsburgh, PA 15260 USAAbstractLaryngeal vestibule (LV) closure is a critical physiologic event during swallowing, since it is the first line of defense against food bolus entering the airway. Identifying the laryngeal vestibule status, including closure, reopening and closure duration, provides indispensable references for assessing the risk of dysphagia and neuromuscular function. However, commonly used radiographic examinations, known as videofluoroscopy swallowing studies, are highly constrained by their radiation exposure and cost. Here, we introduce a non-invasive sensor-based system, that acquires high-resolution cervical auscultation signals from neck and accommodates advanced deep learning techniques for the detection of LV behaviors. The deep learning algorithm, which combined convolutional and recurrent neural networks, was developed with a dataset of 588 swallows from 120 patients with suspected dysphagia and further clinically tested on 45 samples from 16 healthy participants. For classifying the LV closure and opening statuses, our method achieved 78.94% and 74.89% accuracies for these two datasets, suggesting the feasibility of implementing sensor signals for LV prediction without traditional videofluoroscopy screening methods. The sensor supported system offers a broadly applicable computational approach for *Correponding author: esejdic@pitt.edu (Ervin Sejdic).Shitong Mao: Methodology, Formal analysis, Software, Writing- Original draft preparation. Aliaa Sabry: Data curation, Resources, Investigation, Writing- Original draft preparation. Yassin Khalifa: Data Curation, Methodology, Investigation. James L Coyle: Conceptualization, Data Curation, Resources, Writing-Reviewing and Editing. Ervin Sejdic: Writing-Reviewing and Editing, Supervision, Project administration, Funding acquisition.Declaration of competing interestWe declare we have no competing interests.Declaration of interests□ The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptMao et al.Page 2clinical diagnosis and biofeedback purposes in patients with swallowing disorders without the use of radiographic examination.Graphical AbstractKeywordsLaryngeal vestibule closure; High resolution cervical auscultation (HRCA); Deep learning; Dysphagia; Health-care1. IntroductionFor humans, the respiration and digestive systems share the same entrance, therefore, protecting the airway from food bolus entering the trachea or lungs is a fundamental requirement for safe swallowing. Laryngeal vestibule (LV) closure has been considered the first line of defense against swallowed material entering the airway [1, 2, 3]. Likewise, the duration of LV closure is a predictor of airway invasion during swallowing. If the laryngeal closure is absent or its duration is too short, this can lead to aspiration/penetration[4, 5]. Aspiration has been considered as a major concern for individuals with dysphagia (swallowing disorders), especially in neurologic and neurodegenerative diseases, where aspiration-related respiratory infections are a leading cause of death[6]. Therefore, proper evaluation of LV closure and duration could provide an objective outcome measure to improve the assessment of swallowing safety, provide clinical evidence of increased risk of airway compromise during swallowing, and guide the instigation of appropriate compensatory interventions.The videofluoroscopy swallowing study (VFSS) is the only instrumental assessment technique that can visualize the event of LV closure and determine its duration during swallowing through the kinematic analysis of radiographic images[6, 7, 8]. However, practical issues will raise when the VFSS is implemented: it exposes patients to radiation, and is not feasible in all facilities without x-ray departments or qualified clinicians to perform and interpret the VFSS images.[9, 10]. Additionally, it is not suitable for the cases in which patients prefer not to undergo x-ray testing or when patients are unable to participate in the examination protocols[10, 11, 12, 13].Future Gener Comput Syst. Author manuscript; available in PMC 2022 February 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptMao et al.Page 3Furthermore, there are certain limitations of the ordinary clinical setting preventing the more frequent temporal analysis of swallowing events using VFSS images which provides quantification of LV closure at baseline and assessment of treatment efficacy. Frame-by-frame review of VFSS video is time-consuming and some clinicians may not have the ability to record VFSS images for secondary review due to lack of equipment or limited access to archived materials. Clinicians tend to comment on whether and at what phase of the swallow, the material enters the laryngeal vestibule without determining whether LV closure itself was shortened, further limiting inferences that can lead to treatment decisions[14].Because of the previously mentioned drawbacks and limitations of VFSS in the detection of LV closure and reopening, it would be practically beneficial for patients and clinicians to investigate an alternative, non-invasive tool. High-resolution cervical auscultation (HRCA) is a promising non-invasive method for dysphagia screening assessment and management[15]. It uses high-resolution accelerometers and microphones, attached to patients’ necks to record vibratory and acoustic signals during swallowing[16, 17]. The advantages of such a sensor supported approach include mobility, cost-effectiveness, non-invasiveness, and suitability for. day-to-day and even minute-to-minute monitoring[15, 18]. To investigate the relationship between the signals and the LV shifting, previous studies postulated the cardiac analogy hypothesis that explained the elusive physiologic cause of swallowing sounds[19]. This theory suggested that cervical auscultation acoustic signals are generated via vibrations caused by valve and pump systems within the vocal tract. Moreover, HRCA signal features have been found to be associated with LV closure onset and LV reopening[20]. The slapping of the epiglottis and aryepiglottic fold may provide the valve activity that generates swallowing sounds and neck vibration which can be recorded with HRCA.All these studies indicated the possibility of detecting the LV closure and reopening, and a method of determining the closure duration solely based on the HRCA signals. However, no studies attempted to quantitatively implement such an idea. The main challenge was that the explicit dependencies between the signal features and the LV behaviors were not mathematically established. In this study, we sought to investigate the ability of HRCA signals to identify LV status with an advanced deep learning model, which approximated the relationship with training examples. We hypothesized that the computer-aided algorithm with HRCA signals which were acquired from the neck was able to detect the event of LV switching, and estimate the duration of LV closure.The machine learning and deep learning methods have already become powerful tools in the health-care applications and widely employed in the computer-assisted diagnosis for swallowing, laryngeal and neck disorders or disease[21]. Based on larynx contact endoscopic video images, Esmaeili et al. attempted to apply support vector machine, k-nearest neighbor, and random forest to classify benign and malignant lesions on the superficial layers of laryngeal mucosa[22]. For early stage diagnosis of laryngeal squamous cell carcinoma, Moccia et al. implemented a support vector machine classifier with features extracted from the laryngeal endoscopic frames, and they achieved 93% sensitivity[23]. For the similar purpose, Araújo et al. applied transfer learning with pre-trained Convolutional Neural Network (CNN) models to process laryngoscopy images, and they achieved state-of-Future Gener Comput Syst. Author manuscript; available in PMC 2022 February 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptMao et al.Page 4art performance[24]. Our previous study also performed multi-scale CNN filers for hyoid bone detection on the VFSS images[25]. All those studies were conducted based on images as model input. Only several studies attempted to use signals in time series as input and build up deep learning models to serve the swallowing or laryngeal ap",
            {
                "entities": [
                    [
                        271,
                        299,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Procedia Computer ScienceVolume 53, 2015, Pages 345–3552015 INNS Conference on Big DataA classification algorithm for high-dimensional data Asim Roy1 1Department of Information Systems, Arizona State University, Tempe, Arizona, USA Asim.Roy@asu.edu Abstract With the advent of high-dimensional stored big data and streaming data, suddenly machine learning on a very large scale has become a critical need. Such machine learning should be extremely fast, should scale up easily with volume and dimension, should be able to learn from streaming data, should automatically perform dimension reduction for high-dimensional data, and should be deployable on hardware. Neural networks are well positioned to address these challenges of large scale machine learning. In this paper, we present a method that can effectively handle large scale, high-dimensional data. It is an online method that can be used for both streaming and large volumes of stored big data. It primarily uses Kohonen nets, although only a few selected neurons (nodes) from multiple Kohonen nets are actually retained in the end; we discard all Kohonen nets after training. We use Kohonen nets both for dimensionality reduction through feature selection and for building an ensemble of classifiers using single Kohonen neurons. The method is meant to exploit massive parallelism and should be easily deployable on hardware that implements Kohonen nets. Some initial computational results are presented. Keywords: Kohonen nets; classification algorithm; online learning; feature selection; high-dimensional data 1 Introduction The arrival of big and streaming data is forcing major changes to the machine learning field. In this new era, there are significantly more demands on machine learning systems - from the need to handle very large volumes of data and fast learning to the need for automation of machine learning that requires less expert assistance and for hardware deployment. Traditional artificial neural network algorithms have many properties that can meet these demands of big data and thus can certainly play a key role in the major transformations that are taking place. For example, many neural net algorithms are based on the concept of online, incremental learning that does not require simultaneous access to large volumes of data. This mode of learning not only resolves many computational issues, it also Selection and peer-review under responsibility of the Scientific Programme Committee of INNS-BigData2015c(cid:2) The Authors. Published by Elsevier B.V.345doi: 10.1016/j.procs.2015.07.311    \fA classification algorithm for high-dimensional dataAsim Royremoves the headache of correctly sampling from large volumes of data. It also makes neural net algorithms highly scalable (i.e. they can easily handle large volumes of data without running into computer memory limitations and learning (processing) time scales up linearly with data volume because of incremental learning) and provides them the capability to learn from all of the data. Neural network algorithms also have the advantage that they use simple computations that can be highly parallelized. These algorithms are already being implemented on hardware that allows parallel computations (Oh & Jung, 2004) and more powerful hardware is on the way (Monroe, 2014; Poon & Zhou, 2011; Furber et al., 2013). All these features of neural network algorithms positions the field to become the backbone of machine learning applications in the era of big and streaming data. In this paper, we present a new neural network learning method that (1) can be parallelized at different levels of granularity, (2) addresses the issue of high-dimensional data through class-based feature selection, (3) learns an ensemble of classifiers using selected Kohonen neurons (nodes) from different Kohonen nets (Kohonen, 2001), and (4) can be easily implemented on hardware. For dimensionality reduction through feature selection, we train a number of Kohonen nets in parallel with streaming data to create some representative data points. (Note that stored data can also be streamed.) Using these Kohonen nets, we perform class-based feature selection (Roy et al., 2013). The basic criteria for feature selection are to select features for each class that (1) makes the class more compact, and (2) at the same time, maximize the average distance from the other classes. Once class-based feature selection is complete, we discard these Kohonen nets. In the second phase, we construct several new Kohonen nets in parallel in different feature spaces, again from streaming data, based on the selected features. Once trained, we then extract just the active neurons from these different Kohonen nets, add class labels to them and create an ensemble of Kohonen neurons for classification. In the end, we just retain a set of dangling active Kohonen neurons from different Kohonen nets in different feature spaces and discard all Kohonen nets. The paper is organized as follows. Section 2 provides an overview of the concept of class-based feature selection and separability index of features. Section 3 has the algorithm for class-based feature selection from streaming data using Kohonen nets in parallel. Sections 4 provide details on how an ensemble classifier is constructed using neurons from different Kohonen nets. Section 5 has computational results for several high-dimensional problems and the conclusions are in Section 6. 2 Class-specific feature selection, separability index of features and dimensionality reduction A fundamental challenge for machine learning is learning from high-dimensional data. A number of new methods have been developed for both online feature selection and feature extraction for high-dimensional streaming data (Yan et al., 2006; Hoi et al., 2012; Wu et al., 2010; Law et al., 2006). However, none of them are for class-specific feature selection. Since 1997, at various conferences, Roy had proposed methods that use a subset of the original features in class-specific classifiers and Roy et al. (2013) presents one such method. However, the method in Roy et al. (2013) does not work for streaming data. In class-specific feature selection, we find separate feature sets for each class such that they are the best ones to separate that class from the rest of the classes. The concept we use is identical to the one used by LDA and Maximum Margin Criterion (MMC) methods (Li et al. 2006) that maximize the between-class scatter and minimize the within-class scatter. In other words, those methods try to maximize the distance between different class centers and at the same time make the data points in the same class as close as possible. Our method, although not a feature extraction method, is based on the same concept. 346   \fA classification algorithm for high-dimensional dataAsim RoyIn an offline mode, where a collection of data points is available, it is fairly easy to select features that maximize the average distance of data points of one class from the rest of the classes and also, at the same time, minimize the average distance of data points within that class. Roy et al. (2013) ranks and selects features on this basis and computational experiments show that it works quite well. However, that method cannot be used for streaming data where no data is stored. In the proposed method, we use the same concept for feature selection, but train multiple Kohonen nets from streaming data to do so. By training multiple Kohonen nets, we essentially create some representative data points for each class and that’s how we resolve the dilemma of not having access to a collection of data points. Once we have a collection of representative data points (represented by certain Kohonen neurons in the Kohonen nets), it is easy to use the class-based feature selection method proposed in Roy et al. (2013). We train many different Kohonen nets, of different grid sizes and for different feature subsets, and we train them in parallel on a distributed computing platform. We use Apache Spark (2015) as our distributed computing platform, but other similar platforms can be used. A Kohonen net forms clusters and the cluster centers (that is, the active Kohonen net nodes or neurons) are equivalent to representative examples of the streaming data. We then use these representative examples to select features by class.  in is the average distance between patterns within class k for feature n, and dknSuppose there are kc total classes. Our basic feature ranking principles are that (1) a good feature for class k should produce good separation between patterns in class k and those not in class k, k = 1…kc, and (2) also make the patterns in class k more compact. Roy et al. (2013) uses a measure called the separability index that is based on these concepts and can rank features for each class. out the Suppose dknaverage distance between the patterns in class k and those not in class k for feature n.  Roy et al. (2013) uses the Euclidean distance for distance measure, but other distance measures could be used. in. Roy et al. (2013) uses this separability index rkn to The separability index is given by rkn = dknrank order features of class k where a higher ratio implies a higher rank. The sense of this measure is out increases the that a feature n with a lower dknseparation of class k from the other classes. Thus, higher the ratio rkn for a feature n, greater is its ability to separate class k from the other classes and better is the feature. in makes class k more compact and with a higher dknout / dkn2.1 Why class-based feature selection? An example Gene Number Separability Indices by Class AML ALL Good Good AML Features 758 1809 4680 ALL Features 2288 760 6182 82.53 75.25 39.73 0.85 0.93 0.8 2.49 1.85 2.82 114.75 98.76 34.15 Table 2.1 – Separability indices for a few features in the AMLALL gene expression dataset 347      \fA classification algorithm for high-dimensional dataAsim RoyWe solved a number o",
            {
                "entities": [
                    [
                        2551,
                        2578,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Manuscript version: Author’s Accepted Manuscript The version presented in WRAP is the author’s accepted manuscript and may differ from the published version or Version of Record. Persistent WRAP URL: http://wrap.warwick.ac.uk/125464              How to cite: Please refer to published version for the most recent bibliographic citation information. If a published version is known of, the repository item page linked to above, will contain details on accessing it. Copyright and reuse: The Warwick Research Archive Portal (WRAP) makes this work by researchers of the University of Warwick available open access under the following conditions. © 2019 Elsevier. Licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International http://creativecommons.org/licenses/by-nc-nd/4.0/. Publisher’s statement: Please refer to the repository item page, publisher’s statement section, for further information. For more information, please contact the WRAP Team at: wrap@warwick.ac.uk. warwick.ac.uk/lib-publications          \fA Convolutional Neural Network Approach to Detect Congestive Heart Failure Mihaela Porumb1, Ernesto Iadanza2, Sebastiano Massaro3 and Leandro Pecchia1* 1 University of Warwick, School of Engineering, Coventry CV4 7AL, UK 2 University of Florence, v. S. Marta, 3, Florence, IT (E-mail: Ernesto.Iadanza@unifi.it) 3 The Organizational Neuroscience Laboratory, London WC1N 3AX, UK; University of Surrey, Guildford, GU2 7XH, UK (E-mail: sebastiano.massaro@theonelab.org) How to Cite: “Porumb, M., Iadanza, E., Massaro, S., & Pecchia, L. (2020). A convolutional neural network approach to detect congestive heart failure. Biomedical Signal Processing and Control, 55, 101597. DOI: https://doi.org/10.1016/j.bspc.2019.101597” Link: https://www.sciencedirect.com/science/article/pii/S1746809419301776 *Corresponding author: E-mail address: L.Pecchia@warwick.ac.uk, University of Warwick, School of Engineering, Coventry CV4 7AL, UK Abstract Congestive Heart Failure (CHF) is a severe pathophysiological condition associated with high prevalence, high mortality rates, and sustained healthcare costs, therefore demanding efficient methods for its detection. Despite recent research has provided methods focused on advanced signal processing and machine learning, the potential of applying Convolutional Neural Network (CNN) approaches to the automatic detection of CHF has been largely overlooked thus far. This study addresses this important gap by presenting a CNN model that accurately identifies CHF on the basis of one raw electrocardiogram (ECG) heartbeat only, also juxtaposing existing methods typically grounded on Heart Rate Variability. We trained and tested the model on publicly available ECG datasets, comprising a total of 490,505 heartbeats, to achieve 100% CHF detection accuracy. Importantly, the model also identifies those heartbeat   \fsequences and ECG’s morphological characteristics which are class-discriminative and thus prominent for CHF detection. Overall, our contribution substantially advances the current methodology for detecting CHF and caters to clinical practitioners’ needs by providing an accurate and fully transparent tool to support decisions concerning CHF detection. Keywords: Convolutional Neural Networks, Congestive Heart Failure, Machine Learning 1. INTRODUCTION Congestive Heart Failure (CHF) is a pathophysiological condition responsible for the failure of the heart in pumping blood in the body [1] which has encountered widespread research and societal attention [2]. According to the European Society of Cardiology, around 26 million people worldwide are affected by a form of heart failure [3]. CHF is a strongly degenerative condition, and its prevalence increases quickly with age [4], [5]. The mortality rate is closely associated with the degree of severity, reaching peaks of 40% in the most serious events [e.g.,  New York Heart Association (NYHA) classes III-IV] [6]. CHF is also one of the foremost reasons for hospitalization in the elderly, and it is characterized by a resilient relapse rate, with half of the outpatients readmitted within a few months from hospital discharge [7]. Moreover, just among the most industrialized countries, the healthcare expenditure for CHF consumes 2-3% of the healthcare budgets, with the cost of hospitalization being the greatest proportion of the spending [8]–[10]. Thus, with a worldwide aging population and sustained pressures on healthcare systems and resources, there is the compelling demand—among patients, healthcare providers, policymakers, and the society as a whole—to address this scenario by identifying highly accurate methods to improve detection of heart failures [11] and in turn enable early and more efficient diagnoses. Recently, research has made significant progress in these areas. In particular, given the quantity and complexity of data involved, machine learning techniques and classsifiers (e.g., SVM, MLP, k-NN, CART, Random Forest) [12]–[18] have been successfully applied to analyze, detect, and classify heart failures, as we discussed and showed in previous works [12], [13], [19]–[21]. These approaches able to distinguish between heart failure and healthy subjects are mostly based on Heart Rate Variability (HRV)—the variation over time of the period between consecutive heartbeats extracted from electrocardiographic (ECG) signals [22]—, showing that depressed HRV patterns represent accurate markers for detecting the condition. However, building accurate HRV-based models is time-consuming and prone to error steps, due to the preprocessing and the iterative process of manually selecting appropriate features. \fMoreover, the best performing HRV-based models generally require either long-term signals (i.e., 24h) or at least the combination of short-term HRV with non-standard long-term HRV features, as shown in [23]. To tackle these issues, we present a novel framework of CHF detection that does not rely on HRV features, rather it uses raw ECG signals only. This method is based on a 1-D Convolutional Neural Network (CNN). CNNs are hierarchical neural networks that mimic the human visual system and have proven to be effective in recognizing patterns and structures of input data in image classification, localization, and detection tasks, among others [24]–[26]. Moreover, they have been extensively used for time series analysis in classification tasks. Some successful examples include, among others: general time series classification [27]–[29], speech recognition tasks [30], arrhythmia detection [31], [32], and multivariate diagnostic measurements modeling [33], [34]. Inspired by this growing body of research, we aim to detect CHF through a 1-D CNN approach on the ECG signals. Adding to the significant benefit of building upon raw physiological data, this method enables visualization of the input time series subsequences that are class discriminative (i.e., CHF vs. healthy subjects). This feature considerably improves the interpretability of the CNN model and represents a crucial aspect to ensure the ‘transparency’ of the method [35], [36]. Such a transparency is fundamental to help researchers explain how conclusions are reached, and to aid professionals in better understanding correlations of pathophysiological behaviors and properties revealed by the model. As we shall explain, we used a form of class activation mapping (Grad-CAM) [37] that highlights the class discriminative regions in the input data. In other words, Grad-CAM uses the model’s last activation maps to originate a heat map that can be overlapped with the input, thus showing what regions in the input data contribute most to CHF detection. Overall, the proposed framework puts forward several developments for CHF detection, such as refraining from using hefty preprocessing and features selection steps of HRV-based models, as well as enabling visualization of the subsequences in the input time series which are used to reach certain clinically-relevant conclusions. 2.1 Data 2. METHODS We performed a retrospective analysis on two publicly available datasets. The data for the normal subjects (i.e., control group) were retrieved from the MIT-BIH Normal Sinus Rhythm Database [38] included in PhysioNet [39]. This dataset includes 18 long-term ECG recordings of normal healthy not-arrhythmic subjects (Females=13; age \frange: 20 to 50). The data for the CHF group were retrieved from the BIDMC Congestive Heart Failure Database [40] from PhysioNet [39]. This dataset includes long-term ECG recordings of 15 subjects with severe CHF (i.e., NYHA classes III-IV) (Females =4; age range: 22 to 63). Altogether, the pool of data available for this study consists of about 20 hours of ECG recordings per each subject; it contains two-channel ECG signals sampled at 250 samples per second for the BIDMC dataset, and 128 samples per second for the MIT-BIH dataset. The two datasets used in this study were published by the same laboratory, the Beth Israel Deaconess Medical Center, and were digitized using the same procedures according to the signal specification line in the header file. These datasets have been widely used in several studies concerning CHF detection, as explained in [11]. Indeed, it is an acknowledged practice to build and validate CHF classification models on these sources to ensure opportunities for reproducibility. 2.2 ECG preprocessing The ECG recordings from the BIDMC dataset were down-sampled in order to match the sampling frequency of the ECG signals from the MIT-BIH dataset (i.e., 128 Hz). Records from both databases were already made available with computed beat annotations, which we used to isolate and extract individual heartbeats. We considered a window of 235 ms before the annotated R peak (equivalent to 30 samples=128 samples/s*0.235 s), and a window of 390 ms after the R peak (equivalent to 50 samples). Only the beats that were annotated as normal (N) were retained for further analy",
            {
                "entities": [
                    [
                        1733,
                        1759,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Pattern Recognition 115 (2021) 107899 Contents lists available at ScienceDirect Pattern Recognition journal homepage: www.elsevier.com/locate/patcog Pruning by explaining: A novel criterion for deep neural network pruning Seul-Ki Yeom a , i , Philipp Seegerer a , h , Sebastian Lapuschkin c , Alexander Binder d , e , Simon Wiedemann c , Klaus-Robert Müller a , f , g , b , ∗, Wojciech Samek c , b , ∗a Machine Learning Group, Technische Universität Berlin, 10587 Berlin, Germany b BIFOLD – Berlin Institute for the Foundations of Learning and Data, Berlin, Germany c Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany d ISTD Pillar, Singapore University of Technology and Design, Singapore 487372, Singapore e Department of Informatics, University of Oslo, 0373 Oslo, Norway f Department of Artificial Intelligence, Korea University, Seoul 136–713, Korea g Max Planck Institut für Informatik, 66123 Saarbrücken, Germany h Aignostics GmbH, 10557 Berlin, Germany i Nota AI GmbH, 10117 Berlin, Germany a r t i c l e i n f o a b s t r a c t Article history: Received 18 December 2019 Revised 28 January 2021 Accepted 8 February 2021 Available online 22 February 2021 Keywords: Pruning Layer-wise relevance propagation (LRP) Convolutional neural network (CNN) Interpretation of models Explainable AI (XAI) The success of convolutional neural networks (CNNs) in various applications is accompanied by a sig- nificant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource- constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning. © 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) 1. Introduction Deep CNNs have become an indispensable tool for a wide range of applications [1] , such as image classification, speech recognition, natural language processing, chemistry, neuroscience, medicine and even are applied for playing games such as Go, poker or Su- per Smash Bros. They have achieved high predictive performance, ∗ Corresponding authors. E-mail addresses: yeom@tu-berlin.de (S.-K. Yeom), philipp.seegerer@tu- berlin.de (P. Seegerer), sebastian.lapuschkin@hhi.fraunhofer.de (S. Lapuschkin), alexabin@uio.no (A. Binder), simon.wiedemann@hhi.fraunhofer.de (S. Wiedemann), klaus-robert.mueller@tu-berlin.de (K.-R. Müller), wojciech.samek@hhi.fraunhofer.de (W. Samek). at times even outperforming humans. Furthermore, in specialized domains where limited training data is available, e.g., due to the cost and difficulty of data generation (medical imaging from fMRI, EEG, PET etc.), transfer learning can improve the CNN performance by extracting the knowledge from the source tasks and applying it to a target task which has limited training data. However, the high predictive performance of CNNs often comes at the expense of high storage and computational costs, which are related to the energy expenditure of the fine-tuned network. These deep architectures are composed of millions of parameters to be trained, leading to overparameterization (i.e. having more pa- rameters than training samples) of the model [2] . The run-times are typically dominated by the evaluation of convolutional layers, while dense layers are cheap but memory-heavy [3] . For instance, https://doi.org/10.1016/j.patcog.2021.107899 0031-3203/© 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \fS.-K. Yeom, P. Seegerer, S. Lapuschkin et al. Pattern Recognition 115 (2021) 107899 the VGG-16 model has approximately 138 million parameters, tak- ing up more than 500MB in storage space, and needs 15.5 bil- lion floating-point operations (FLOPs) to classify a single image. ResNet50 has approx. 23 million parameters and needs 4.1 bil- lion FLOPs. Note that overparameterization is helpful for an ef- ficient and successful training of neural networks, however, once the trained and well generalizing network structure is established, pruning can help to reduce redundancy while still maintaining good performance [4] . Reducing a model’s storage requirements and computational cost becomes critical for a broader applicability, e.g., in embedded systems, autonomous agents, mobile devices, or edge devices [5] . Neural network pruning has a decades long history with inter- est from both academia and industry [6] aiming to eliminate the subset of network units (i.e. weights or filters) which is the least important w.r.t. the network’s intended task. For network prun- ing, it is crucial to decide how to identify the “irrelevant” subset of the parameters meant for deletion. To address this issue, pre- vious researches have proposed specific criteria based on Taylor expansion, weight, gradient, and others, to reduce complexity and computation costs in the network. Related works are introduced in Section 2 . From a practical point of view, the full capacity (in terms of weights and filters) of an overparameterized model may not be re- quired, e.g., when (1) parts of the model lie dormant after training (i.e., are per- manently ”switched off”), (2) a user is not interested in the model’s full array of possible outputs, which is a common scenario in transfer learning (e.g. the user only has use for 2 out of 10 available network outputs), or (3) a user lacks data and resources for fine-tuning and running the overparameterized model. In these scenarios the redundant parts of the model will still occupy space in memory, and information will be propagated through those parts, consuming energy and increasing runtime. Thus, criteria able to stably and significantly reduce the com- putational complexity of deep neural networks across applications are relevant for practitioners. In this paper, we propose a novel pruning framework based on Layer-wise Relevance Propagation (LRP) [7] . LRP was originally de- veloped as an explanation method to assign importance scores, so called relevance , to the different input dimensions of a neural net- work that reflect the contribution of an input dimension to the model’s decision, and has been applied to different fields of com- puter vision (e.g., [8–10] ). The relevance is backpropagated from the output to the input and hereby assigned to each unit of the deep model. Since relevance scores are computed for every layer and neuron from the model output to the input, these relevance scores essentially reflect the importance of every single unit of a model and its contribution to the information flow through the network — a natural candidate to be used as pruning criterion. The LRP criterion can be motivated theoretically through the concept of Deep Taylor Decomposition (DTD) (c.f. [11–13] ). Moreover, LRP is scalable and easy to apply, and has been implemented in software frameworks such as iNNvestigate [14] . Furthermore, it has linear computational cost in terms of network inference cost, similar to backpropagation. We systematically evaluate the compression efficacy of the LRP criterion compared to common pruning criteria for two different scenarios. Scenario 1 : We prune pre-trained CNNs followed by subse- quent fine-tuning. This is the usual setting in CNN pruning and requires a sufficient amount of data and computational power. Scenario 2 : In this scenario a pretrained model needs to be transferred to a related problem as well, but the data available for the new task is too scarce for a proper fine-tuning and/or the time consumption, computational power or energy consumption is con- strained. Such transfer learning with restrictions is common in mo- bile or embedded applications. Our experimental results on various benchmark datasets and four different popular CNN architectures show that the LRP crite- rion for pruning is more scalable and efficient, and leads to bet- ter performance than existing criteria regardless of data types and model architectures if retraining is performed (Scenario 1). Especially, if retraining is prohibited due to external constraints after pruning, the LRP criterion clearly outperforms previous crite- ria on all datasets (Scenario 2). Finally, we would like to note that our proposed pruning framework is not limited to LRP and image data, but can be also used with other explanation techniques and data types. The rest of this paper is organized as follows: Section 2 sum- marizes related works for network compression and introduces the typical criteria for network pruning. Section 3 describes the frame- work and details of our approach. The experimental results are il- lustrated and discussed in Section 4 ",
            {
                "entities": [
                    [
                        4664,
                        4692,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptNeurocomputing. Author manuscript; available in PMC 2017 February 12.Published in final edited form as:Neurocomputing. 2016 February 12; 177: 75–88. doi:10.1016/j.neucom.2015.11.008.Dictionary Pruning with Visual Word Significance for Medical Image RetrievalFan Zhanga,b, Yang Songa, Weidong Caia, Alexander G. Hauptmannc, Sidong Liua, Sonia Pujolb, Ron Kikinisb, Michael J Fulhamd,e, David Dagan Fenga,f, and Mei Cheng,haSchool of Information Technologies, University of Sydney, AustraliabDept of Radiology, Brigham & Womens Hospital, Harvard Medical School, United StatescSchool of Computer Science, Carnegie Mellon University, United StatesdDept of PET and Nuclear Medicine, Royal Prince Alfred Hospital, AustraliaeSydney Medical School, University of Sydney, AustraliafMed-X Research Institute, Shanghai Jiaotong University, ChinagDept of Informatics, University of Albany State University of New York, United StateshRobotics Institute, Carnegie Mellon University, United StatesAbstractContent-based medical image retrieval (CBMIR) is an active research area for disease diagnosis and treatment but it can be problematic given the small visual variations between anatomical structures. We propose a retrieval method based on a bag-of-visual-words (BoVW) to identify discriminative characteristics between different medical images with Pruned Dictionary based on Latent Semantic Topic description. We refer to this as the PD-LST retrieval. Our method has two main components. First, we calculate a topic-word significance value for each visual word given a certain latent topic to evaluate how the word is connected to this latent topic. The latent topics are learnt, based on the relationship between the images and words, and are employed to bridge the gap between low-level visual features and high-level semantics. These latent topics describe the images and words semantically and can thus facilitate more meaningful comparisons between the words. Second, we compute an overall-word significance value to evaluate the significance of a visual word within the entire dictionary. We designed an iterative ranking method to measure overall-word significance by considering the relationship between all latent topics and words. The words with higher values are considered meaningful with more significant discriminative power in differentiating medical images. We evaluated our method on two public medical imaging datasets and it showed improved retrieval accuracy and efficiency.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.KeywordsMedical image retrieval; BoVW; Dictionary pruning1. IntroductionPage 2Content-based medical image retrieval (CBMIR), which retrieves a subset of images that are visually similar to the query from a large image database, is the focus of intensive research (Müller et al., 2004; Akgül et al., 2011; Kumar et al., 2013). CBMIR provides the potential of having an efficient tool for disease diagnosis, by finding related pre-diagnosed cases and it can be used for disease treatment planning and management. In the past three decades, but in particular in the last decade, medical image data have expanded rapidly due to the pivotal role of imaging in patient management and the growing range of image modalities (Duncan and Ayache, 2000; Menze et al., 2014). Traditional text-based retrieval, which manually indexes the images with alphanumerical keywords, is unable to sufficiently meet the increased demand from this growth. At the same time, advances in computer-aided content-based medical image analysis systems mean that there are methods that can automatically extract the rich visual properties/features to characterize the images efficiently (El-Naqa et al., 2004; Lehmann et al., 2004; Napel et al., 2010; Avni et al., 2011; André et al., 2012a; Xu et al., 2012; Zhang et al., 2015c).In CBMIR research, the main challenge is to design an effective image representation so that images with visually similar anatomical structures are closely correlated. A number of research groups are working in this area (Müller et al., 2004; Zhang et al., 2010; Akgül et al., 2011; Kumar et al., 2013), and there is a trend to use a bag-of-visual-words (BoVW) for medical image representation (Castellani et al., 2010; Cruz-Roa et al., 2012; Kwitt et al., 2012; Foncubierta-Rodríguez et al., 2013; Liu et al., 2013a; Depeursinge et al., 2014). The BoVW model represents an image with a visual word frequency histogram that is obtained by assigning the local visual features to the closest visual words in the dictionary. Rather than matching the visual feature descriptors directly, BoVW retrieval approaches compare the images according to the visual words that are assumed to have higher discriminative power (Foncubierta-Rodríguez et al., 2012; Tamaki et al., 2013). The BoVW model was proposed by Sivic and Zisserman (Sivic and Zisserman, 2003) and has been adopted by many researchers in non-medical domains such as computer vision (Li and Pietro, 2005; Yang et al., 2007; Bosch et al., 2008), showing the advantages of describing local patterns over using global features only. This model has recently been applied to tackle the large-scale medical image retrieval problem (Jiang et al., 2015; Zhang et al., 2015d). In this study, we focus on a new BoVW-based retrieval for better retrieval accuracy and efficiency.1.1. Related workThe aim of CBMIR is to extract visual characteristics of images to identify the level of similarity between two images. Feature extraction can be categorized into global-(GFM) and local-feature (LFM) models based on the scope of descriptors (Bannour et al., 2009). The GFM extracts a single feature vector from the whole image and the LFM partitions the image into a collection of smaller regions, namely patches, and considers that each patch has Neurocomputing. Author manuscript; available in PMC 2017 February 12.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 3its own importance in describing the whole image (Avni et al., 2011). This patch-based model is particularly useful in medical image analysis since different image regions can represent the anatomical structures that play different and essential roles in medical imaging diagnosis (Tong et al., 2014; Zhang et al., 2014).The BoVW representation builds upon the LFM. Visually similar patches from different images are assigned to the same code in a codebook. Then, the patch-code co-occurrence assignment can be used to describe the image features and to compute the similarity between images. The workflow of BoVW-based image retrieval can be generalized into three steps (Caicedo et al., 2009): feature extraction, BoVW construction and similarity calculation. Specifically, the LFM is used to extract a collection of local patch features from each image. The entire patch feature set computed from all images in the database is then grouped into clusters, with each cluster regarded as a visual word and the whole cluster collection considered as the visual dictionary. Then, all patch features in one image are assigned to visual words, generating a visual word frequency histogram to represent this image. Finally, the similarity between images is computed based on these frequency histograms for retrieval.In this workflow, an important issue is the dictionary construction. The visual word in the dictionary corresponds to a group of visually similar patches. Normally, these words are obtained within the local patch feature space using unsupervised clustering methods, e.g., k-means (André et al., 2011; Yang et al., 2012). These approaches often generate a redundant and noisy dictionary since they tend to accommodate all local patch feature patterns (Foncubierta-Rodríguez et al., 2013), thus reducing the effects of the most crucial words and increasing the computational cost. Hence, it is preferable to remove the visual words that are less essential for the BoVW representation.To ensure that only the meaningful feature patterns are included, the supervised clustering method of Bilenko et al (Bilenko et al., 2004) can be used to regulate the construction of dictionary, but the method adaptability is limited because prior knowledge is required for the learning process. Another approach is to analyze the discriminative power of visual words (Caicedo et al., 2009), but the weighting scheme also requires supervised classifiers. Some researchers have suggested that the most frequent visual words in images are ‘stop words’, which occur widely but have little influence on differentiating images, and need to be removed from the dictionary (Sivic and Zisserman, 2003). Yang et al., however, showed that ranking the visual words based on their occurrences in the different images only was not sufficient to evaluate the importance of visual words (Yang et al., 2007). Term frequency-inverse document frequency (TF-IDF) (Jones, 1972) relies on the inverse frequency weighting and has demonstrated its benefits on visual word evaluation. Nevertheless, it merely utilizes the direct co-occurrence relationship between the images and visual words. Jiang et al. (Jiang et al., 2015) proposed an unsupervised approach to refine the weights of visual words within the vocabulary tree and showed the advantages of using the correlations among the visual words. We suggest that this relationshi",
            {
                "entities": [
                    [
                        251,
                        279,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Available online at www.sciencedirect.com Available online at www.sciencedirect.com Procedia Computer Science 00 (2017) 000–000 Procedia Computer Science 00 (2017) 000–000 ScienceDirect ScienceDirect Procedia Computer Science 110 (2017) 498–503 www.elsevier.com/locate/procedia  www.elsevier.com/locate/procedia The 4th International Symposium on Emerging Inter-networks, Communication and Mobility The 4th International Symposium on Emerging Inter-networks, Communication and Mobility (EICM 2017) (EICM 2017) On the use of Networks in Biomedicine On the use of Networks in Biomedicine Eugenio Vocaturoa*, Pierangelo Veltrib Eugenio Vocaturoa*, Pierangelo Veltrib a Department of Computer Science, Modeling, Electronic and Systems Engineering (DIMES), University of Calabria, Italy a Department of Computer Science, Modeling, Electronic and Systems Engineering (DIMES), University of Calabria, Italy b Bioinformatics Laboratory Surgical and Medical Science Department University Magna Graecia of Catanzaro, Italy b Bioinformatics Laboratory Surgical and Medical Science Department University Magna Graecia of Catanzaro, Italy Abstract Abstract The concept of “neural network” emerges by electronic models inspired to the neural structure of human brain. Neural networks aim to The concept of “neural network” emerges by electronic models inspired to the neural structure of human brain. Neural networks aim to solve problems currently out of computer’s calculation capacity, trying to mimic the role of human brain. Recently, the number of biological solve problems currently out of computer’s calculation capacity, trying to mimic the role of human brain. Recently, the number of biological based applications using neural networks is growing up. Biological networks represent correlations, extracted from sets of clinical data, based applications using neural networks is growing up. Biological networks represent correlations, extracted from sets of clinical data, diseases, mutations, and patients, and many other types of clinical or biological features. Biological networks are used to model both the state diseases, mutations, and patients, and many other types of clinical or biological features. Biological networks are used to model both the state of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events. of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events. The study of biological networks, their analysis and modeling are important tasks in life sciences. Most biological networks are still far from The study of biological networks, their analysis and modeling are important tasks in life sciences. Most biological networks are still far from being complete and they are often difficult to interpret due to the complexity of relationships and the peculiarities of the data. Starting from being complete and they are often difficult to interpret due to the complexity of relationships and the peculiarities of the data. Starting from preliminary notions about neural networks, we focus on biological networks and discuss some well-known applications, like protein-protein preliminary notions about neural networks, we focus on biological networks and discuss some well-known applications, like protein-protein interaction networks, gene regulatory networks (DNA-protein interaction networks), metabolic networks, signaling networks, neuronal interaction networks, gene regulatory networks (DNA-protein interaction networks), metabolic networks, signaling networks, neuronal network, phylogenetic trees and special networks. Finally, we consider the use of biological network inside a proposed model to map health network, phylogenetic trees and special networks. Finally, we consider the use of biological network inside a proposed model to map health related data. related data. © 2017 The Authors. Published by Elsevier B.V. © 2017 The Authors. Published by Elsevier B.V.© 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs. Peer-review under responsibility of the Conference Program Chairs.Peer-review under responsibility of the Conference Program Chairs. Keywords: Biological Networks; Protein-Protein Interaction Networks (PPIn); Gene Regulatory Networks (GRN); Metabolic Networks; Signaling Networks; Keywords: Biological Networks; Protein-Protein Interaction Networks (PPIn); Gene Regulatory Networks (GRN); Metabolic Networks; Signaling Networks; Neuronal Networks; Food Webs; Phylogenetic Trees, Special Networks and Hierarchies; Health Care Model. Neuronal Networks; Food Webs; Phylogenetic Trees, Special Networks and Hierarchies; Health Care Model. 1. Introduction 1. Introduction The human brain has the capacity of processing information and making decisions instantaneously. Many researchers have The human brain has the capacity of processing information and making decisions instantaneously. Many researchers have shown that human brain performs calculations in a different way than computers, hence the aspiration to solve problems whose shown that human brain performs calculations in a different way than computers, hence the aspiration to solve problems whose complexity is beyond the current computing power, has prompted the scientific community to the neural networks. For complexity is beyond the current computing power, has prompted the scientific community to the neural networks. For biological network is meant any network applied to a biological systems. biological network is meant any network applied to a biological systems. A network, in a broad sense, identifies a system, which is characterized by interconnected sub-units. Biological networks are A network, in a broad sense, identifies a system, which is characterized by interconnected sub-units. Biological networks are types of important applicable model in various contexts; complex biological systems can be represented and analyzed by types of important applicable model in various contexts; complex biological systems can be represented and analyzed by computable networks. Like the computer networks, the high complexity degree of biological networks is generated by a simple computable networks. Like the computer networks, the high complexity degree of biological networks is generated by a simple mechanism. Bioinformatics really shifted its focus from individual genes, proteins, structures and search algorithms for large mechanism. Bioinformatics really shifted its focus from individual genes, proteins, structures and search algorithms for large networks; even more biologists are discovering the links between Internet and metabolic pathways, interactions of proteins networks; even more biologists are discovering the links between Internet and metabolic pathways, interactions of proteins through a network topology or a scale-free network. through a network topology or a scale-free network. * * * Corresponding author. Tel.: +039-0984-4799. * Corresponding author. Tel.: +039-0984-4799. E-mail address: e.vocaturo@dimes.unical.it E-mail address: e.vocaturo@dimes.unical.it 1877-0509 © 2017 The Authors. Published by Elsevier B.V. 1877-0509 © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs. Peer-review under responsibility of the Conference Program Chairs. 1877-0509 © 2017 The Authors. Published by Elsevier B.V.Peer-review under responsibility of the Conference Program Chairs.10.1016/j.procs.2017.06.13210.1016/j.procs.2017.06.1321877-0509ScienceDirectAvailable online at www.sciencedirect.com          \f2 Eugenio Vocaturo/ Procedia Computer Science 00 (2017) 000–000 Eugenio Vocaturo et al. / Procedia Computer Science 110 (2017) 498–503 499A neural network is composed of a set of parallel and distributed processing units, referred as nodes or neurons; they are arranged in layers, and are interconnected by unidirectional or bidirectional connections (see Fig. 1). Typically, a neural network has a set of N input nodes, whose generic element is related with, and each node is interconnected to others through weighted arcs. The products of input and weight are simply summed and feed through (Activation Function) to generate the output (see Fig. 2).              Fig. 1. Typical Structure of Neural Network                              Fig. 2. Activation Functions Neural network design typically consists of Topology, Transfer Function and Learning Algorithm. The neural network topologies are actually classified by the directions of interconnection in the layer; so the most referred topologies are, Feed Forward Topology and Recurrent Topology. In feed forward topology (FFT) network, the nodes are “hierarchically arranged” in layers starting with the input layers and ending with output layers. The number of hidden layers provides most of the network computational power. In literature typical application of this topology are the multilayer perception network and radial basic function network. The nodes in each layers are connected to next layer through unidirection paths starting from one layer (source) and ending at the subsequently layer (sink). The output of a given layer feeds the nodes of the following layer in a forward direction and does not allow feedback flow of information12. Unlike the FFT, in the recurring topology (RNT) the flow of information between connected nodes is bidirectional. Typical applications of RNT, for example, are Hopfield Network1 and time delay neural network (TDNN)2. A recurrent network structure has a sort of memory, which helps storing information in output nodes through dynamic states. Biological networks shapes both the state of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events14. In neural networks, the basic unit are the neurons that work like simple processors. Any neuron takes th",
            {
                "entities": [
                    [
                        7558,
                        7585,
                        "DOI"
                    ],
                    [
                        7585,
                        7612,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Journal of Informetrics 15 (2021) 101171 Contents lists available at ScienceDirect Journal of Informetrics journal homepage: www.elsevier.com/locate/joi Gender-based homophily in research: A large-scale study of man-woman collaboration Marek Kwiek a , Wojciech Roszka b a Institute for Advanced Studies in Social Sciences and Humanities (IAS), UNESCO Chair in Institutional Research and Higher Education Policy, Adam Mickiewicz University in Poznan, Poland b Poznan University of Economics and Business, Poznan, Poland a r t i c l e i n f o a b s t r a c t Keywords: Research collaboration co-authorships gender gap sociology of science homophily scientific careers publishing patterns probabilistic record linkage sex differences 1. Introduction We examined the male-female collaboration practices of all internationally visible Polish uni- versity professors (N = 25,463) based on their Scopus-indexed publications from 2009–2018 (158,743 journal articles). We merged a national registry of 99,935 scientists (with full admin- istrative and biographical data) with the Scopus publication database, using probabilistic and deterministic record linkage. Our unique biographical, administrative, publication, and citation database ( “The Polish Science Observatory ”) included all professors with at least a doctoral de- gree employed in 85 research-involved universities. We determined what we term an “individual publication portfolio ” for every professor, and we examined the respective impacts of biological age, academic position, academic discipline, average journal prestige, and type of institution on the same-sex collaboration ratio. The gender homophily principle (publishing predominantly with scientists of the same sex) was found to apply to male scientists —but not to females. The majority of male scientists collaborate solely with males; most female scientists, in contrast, do not collab- orate with females at all. Across all age groups studied, all-female collaboration is marginal, while all-male collaboration is pervasive. Gender homophily in research-intensive institutions proved stronger for males than for females. Finally, we used a multi-dimensional fractional logit regres- sion model to estimate the impact of gender and other individual-level and institutional-level independent variables on gender homophily in research collaboration. Science is a collaborative enterprise, with (male and female) scientists collaborating internationally, nationally, and institutionally ( Wuchty, Jones, & Uzzi, 2007 ; Wagner, 2018 ). However, this is not our topic: our focus is on male–male, female–female, and male–female (or mixed-sex) research collaboration rather than collaboration across countries and institutions. The dominating view in liter- ature is that, on average, males collaborate more often with males, and females collaborate more often with females ( Jadidi, Karimi, Lietz, & Wagner, 2018 ; Lerchenmueller, Hoisl, & Schmallenbach, 2019 ; Wang, Lee, West, Bergstrom, & Erosheva, 2019 ; Holman & Morandin, 2019 ; Boschini & Sjögren, 2007 ; McDowell & Smith, 1992 ). This hypothesis is being tested using a large-scale dataset with unique variables. According to the homophily principle, “similarity breeds connection ”; consequently, personal networks are homogeneous with regard to many sociodemographic and personal characteristics (such as age, ethnic origin, class origin, wealth, education, and gender). On the positive side, homophily is reported to simplify communication ( McPherson, Smith-Lovin, & Cook, 2001 ; Kegen, 2013 ). However, on the negative side, homophily may “limit people’s social worlds in a way that has powerful implications for the information E-mail addresses: kwiekm@amu.edu.pl (M. Kwiek), wojciech.roszka@ue.poznan.pl (W. Roszka). https://doi.org/10.1016/j.joi.2021.101171 Received 6 June 2020; Received in revised form 29 April 2021; Accepted 5 May 2021 1751-1577/© 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \fM. Kwiek and W. Roszka Journal of Informetrics 15 (2021) 101171 they receive, the attitudes they form, and the interactions they experience ” ( McPherson et al., 2001 ). As science is increasingly collaborative, the homophily principle may increasingly influence academic careers. Research collaboration in science (or gender co-authorship patterns) provides fertile ground to test the homophily principle. Man–woman research collaboration patterns in science are contrasted in this paper through six lenses: biological age, academic position, academic discipline, gender-defined research collaboration type, journal prestige, and institutional research intensity. The individual scientist, rather than the individual article, is the unit of analysis. The key innovative methodological step is the determi- nation of what we term an “individual publication portfolio ” (for the decade of 2009–2018) for every internationally visible Polish scientist (N = 25,463 university professors from 85 universities, grouped into 27 disciplines, along with their 164,908 international collaborators, who together authored 158,743 Scopus-indexed publications). Co-authorships are used for the operationalization of research collaboration, following standard bibliometric practice. The individual publication portfolio reflects the distribution of gender-defined research collaboration types (same-sex collaboration and mixed-sex collaboration) for every individual scientist. Team formation in academia, understood as publishing with coauthors of varying numbers and different genders, is voluntary ( McDowell & Smith, 1992 ): researchers team up when they think that they are better off collaborating than publishing alone. The teams formed, or the articles published, are likely to reflect “individual tastes and perceptions of the returns to collaboration, as well as the costs of coordination ” ( Boschini & Sjögren, 2007 , p. 327). Some male scientists collaborate predominantly with other males, and some female scientists collaborate predominantly with other females. Still, others prefer to publish in mixed-sex collaborations (or to author individually). We examine the same-sex collaboration ratio at an individual level of every internationally visible Polish scientist (i.e., only authors with Scopus-indexed publications) and generalize the results from the individual level to the level of the national higher education system. 2. Literature review 2.1. The gender context of science The gender context of academic science has changed substantially in the past few decades ( Huang, Gates, Sinatra, & Barabàsi, 2020 ; Larivière, Ni, Gingras, Cronin, & Sugimoto, 2013 ), with more female scientists entering the higher education sector ( Elsevier, 2018 ) and occupying high academic positions ( Zippel, 2017 ; Diezmann & Grieshaber, 2019 ). Male and female scientists often pursued or were pushed onto somewhat different career tracks and were located in different academic structures, with “differential access to valuable resources ” ( Xie & Shauman, 2003 , p. 193). Females, as new entrants into a traditionally male-dominated academic profession, initially did not have equal access to professional networks ( McDowell, Singell, & Stater, 2006 ). But the academic world is changing. New bibliometric literatures applying the various gender-determination methods to authors and authorships ( Halevi, 2019 ; Elsevier, 2020 ) bring new data-driven insights to gender disparities in science, and literatures have become much less based on anecdotal and localized studies ( Larivière et al., 2013 ). Women are plugging into networks over time as the profession becomes more gender representative (as shown for academic economists by McDowell et al., 2006 , p. 154). However, somewhat paradoxically, the increased participation of women in STEM disciplines is reported to have been accompanied by an increase in gender differences regarding both productivity and impact ( Huang et al., 2020 , p. 8; Elsevier, 2018 , p. 16). As recent literature highlights, female scientists occupy more junior positions and receive lower salaries, are more often in non- tenure-track and teaching-only positions, are promoted more slowly, are less likely to be listed as either first or last author on a paper, and are allocated less research funding from national research councils. Women also tend to be less involved in international collaboration; female collaborations are more domestically oriented than are the collaborations of males from the same country; and females have less-prestigious collaborations and fewer collaborations overall (see Holman & Morandin, 2019 ; Halevi, 2019 ; Larivière et al., 2013 ; Larivière et al., 2011 ; Aksnes, Rørstad, Piro, & Sivertsen, 2011 ; Aksnes, Piro, & Rørstad, 2019 ; Huang et al., 2020 ; Maddi, Larivière, & Gingras, 2019 ; Fell & König, 2016 ; van den Besselaar & Sandström, 2016 ; Nielsen, 2016 ). In every country studied recently in Elsevier (2020) and Elsevier (2018) , the percentage of women who publish internationally is lower than the percentage of men who do so; for Poland, which is not included in the Elsevier reports, these publishing patterns are confirmed for various collaboration intensity levels and for various age groups; see Kwiek & Roszka, 2020 , on gender disparities in international collaboration). Female scientists in Poland constitute a substantial, highly productive, and highly internationalized part of the academic work- force, which is often the case in formerly communist European countries, which exhibit greater gender parity than the world and the OECD averages ( Larivière et al., 2013 , p. 212). Poland has a higher proportion of professors than any country studied in Larivière et al. (2013) or in Diezmann and Grieshaber (2019) , reaching 29.82% in 2018 ( GUS, 2019 , p. 220), even though there is a clear “the higher the fewer ” pattern acro",
            {
                "entities": [
                    [
                        3813,
                        3838,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Tilburg UniversityA vulnerability analysisKrupiy, TetyanaPublished in:Computer Law and Security ReviewDOI:10.1016/j.clsr.2020.105429Publication date:2020Document VersionPublisher's PDF, also known as Version of recordLink to publication in Tilburg University Research PortalCitation for published version (APA):Krupiy, T. (2020). A vulnerability analysis: Theorising the impact of artificial intelligence decision-makingprocesses on individuals, society and human diversity from a social justice perspective. Computer Law andSecurity Review, 38(September), 1-25. [105429]. https://doi.org/10.1016/j.clsr.2020.105429General rightsCopyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright ownersand it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.      • Users may download and print one copy of any publication from the public portal for the purpose of private study or research.      • You may not further distribute the material or use it for any profit-making activity or commercial gain      • You may freely distribute the URL identifying the publication in the public portalTake down policyIf you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediatelyand investigate your claim.Download date: 04. jul.. 2023  \fcomputer law & security review 38 (2020) 105429 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR A vulnerability analysis: Theorising the impact of artificial intelligence decision-making processes on individuals, society and human diversity from a social justice perspective Tetyana (Tanya) Krupiy Tilburg University, Montesquieu Building, Room 808, Prof. Cobbenhagenlaan 221, Tilburg, North Brabant, 5037 DE, the Netherlands a r t i c l e i n f o a b s t r a c t Keywords: Artificial intelligence Data science Decision-making process Social justice Human diversity Vulnerability theory Feminism Queer legal theory Critical disability theory The article examines a number of ways in which the use of artificial intelligence technolo- gies to predict the performance of individuals and to reach decisions concerning the enti- tlement of individuals to positive decisions impacts individuals and society. It analyses the effects using a social justice lens. Particular attention is paid to the experiences of individ- uals who have historically experienced disadvantage and discrimination. The article uses the university admissions process where the university utilises a fully automated decision- making process to evaluate the capability or suitability of the candidate as a case study. The article posits that the artificial intelligence decision-making process should be viewed as an institution that reconfigures the relationships between individuals, and between indi- viduals and institutions. Artificial intelligence decision-making processes have institutional elements embedded within them that result in their operation disadvantaging groups who have historically experienced discrimination. Depending on the manner in which an artifi- cial intelligence decision-making process is designed, it can produce solidarity or segrega- tion between groups in society. There is a potential for the operation of artificial intelligence decision-making processes to fail to reflect the lived experiences of individuals and as a re- sult to undermine the protection of human diversity. Some of these effects are linked to the creation of an ableist culture and to the resurrection of eugenics-type discourses. It is con- cluded that one of the contexts in which human beings should reach decisions is where the decision involves representing and evaluating the capabilities of an individual. The legisla- ture should respond accordingly by identifying contexts in which it is mandatory to employ human decision-makers and by enacting the relevant legislation. © 2020 Tetyana˜(Tanya) Krupiy. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) E-mail address: t.krupiy@uvt.nl https://doi.org/10.1016/j.clsr.2020.105429 0267-3649/© 2020 Tetyana˜(Tanya) Krupiy. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) \f2 computer law & security review 38 (2020) 105429 Erica Curtis, a former admissions evaluator at Brown Uni- versity in the United States, has noted that she evaluated each student’s application consisting of standardised test scores, the transcript, the personal statement, and multiple supple- mental essays within a twelve-minute timeframe.1 Arguably, this is a very short period of time within which an admissions officer can evaluate the applicant’s personality and academic qualities holistically.2 The time constraints create a possibility that the admissions officer may fail to detect the applicants’ capabilities or how societal barriers diminished their ability to realise their potential. Another concern with human decision- making is that the decision-maker officer may act arbitrar- ily in the course of exercising discretion 3 by putting differ- ent weight on comparable attributes that cannot be measured. What is more, an admissions officer could treat applicants on an unequal basis due to being influenced by conscious or unconscious biases.4 Advances in artificial intelligence (hereinafter AI) technology give rise to a discussion whether organisations should use AI systems to select applicants for admission to university.5 Technology companies market AI systems with a capability to predict the candidates’ perfor- mance and to follow a decision-making procedure as possess- ing the capacity to eliminate bias and to improve decision- making.6 The computer science community is now working on embedding values, such as fairness, into the AI decision- Acknowledgments: I would like to thank Professor Corien Prins for her feedback on the draft version of this article. I am grateful to Atieno Samandari, Stu Marvel, Professor Martha Albertson Fine- man, Professor Nicole Morris and Professor Paul Myers for their feedback on a presentation which formed the foundation for this article. Additionally, I wish to thank scholars who asked stimulat- ing questions during the Ethics of Data Science: Addressing the Future Use and Misuse of Our Data Conference, the BIAS in Artifi- cial Intelligence and Neuroscience Transdisciplinary Conference, and the Media & Space: The Regulation of Digital Platforms, New Media & Technologies Symposium where I presented my ongoing work. 1 Joel Butterly, ‘7 Admissions Officers Share the Things They Never Tell Applicants’ (Insider Inc., 2018) < https: //www.businessinsider.com/7- things- college- admissions- officers-wishevery-applicant-knew-2018-2?international= true&r=US&IR=T > accessed 26 June 2019 2 3 4 5 6 Ibid Mark Bovens and Stavros Zouridis, ‘From Street-Level to System-Level Bureaucracies: How Information and Communica- tion Technology is Transforming Administrative Discretion and Constitutional Control’ (2002) 62 Public Administration Review 174, 181 Josh Wood, ‘“The Wolf of Racial Bias\": the Admissions Lawsuit Rocking Harvard’ The Guardian (London 18 October < https://www.theguardian.com/education/2018/oct/18/ 2018) harvard-affirmative-action-trial-asian-american-students > accessed 10 March 2019 Moritz Hardt, How Big Data is Unfair: Understanding Unintended Sources of Unfairness in Data Driven Decision-making (Medium Cor- poration 2014) Ekta Dokania, ‘Can AI Help Humans Overcome Bias?’ The Seat- tle Globalist (Seattle 22 May 2019) < https://www.seattleglobalist. com/2019/05/22/can- ai- help- humans- overcome- bias/83957 > ac- cessed 3 March 2019 making procedure.7 Daniel Greene and colleagues view the focus on achieving fairness by incorporating values into the design of the system as short-sighted.8 The attention on how to embed fairness into the decision-making procedure of a technical system side-lines the discussion how the employ- ment of AI decision-making processes impacts on achieving social goals, such as social justice and ‘equitable human flour- ishing.’ 9 Virginia Eubank’s work underscores the importance of investigating how the use of AI decision-making processes impacts individuals and society. Her interviews with affected individuals who applied to access state benefits in the state of Indiana in the United States 10 demonstrate that the employ- ment of AI decision-making processes can lead to the deepen- ing of inequality,11 and to social division.13 The enquiry is particularly pertinent given the fact that not all sources report adverse outcomes. The British Universities and Colleges Admissions Service asserts that in its pilot project an algorithmic process selected the same pool of applicants to be admitted to universities as admissions officers; the organisa- tion did not reveal the algorithm’s design and operation pro- cedure.14 to social sorting 12 The present paper explores some of the hitherto unre- solved longstanding societal problems and new issues the employment of AI decision-making processes raises. It con- tributes to existing literature by proposing that an AI decision- making process should be understood as an institution. The AI decision-making process reconfigures relationships between individuals as well as between individuals and institutions. The paper examines some of the values and types of institu- tional arrangements the employment of AI decision-making processes embeds into society. This issue is significant. The Council of Europe Committee of Ministers stated that when data-driven technologies operate ‘at scale’ their operation prioritises certain values over others.15 The assertion of the Council of Europe Committee of Ministers that data-d",
            {
                "entities": [
                    [
                        106,
                        132,
                        "DOI"
                    ],
                    [
                        589,
                        615,
                        "DOI"
                    ],
                    [
                        4275,
                        4301,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "NIH Public AccessAuthor ManuscriptMed Image Anal. Author manuscript; available in PMC 2011 December 1.Published in final edited form as:Med Image Anal. 2010 December ; 14(6): 770–783. doi:10.1016/j.media.2010.06.002.Detection of Neuron Membranes in Electron Microscopy Imagesusing a Serial Neural Network ArchitectureElizabeth Jurrusa,b, Antonio R. C. Paivaa, Shigeki Watanabec, James R. Andersone, BryanW. Jonese, Ross T. Whitakera,b, Erik M. Jorgensenc, Robert E. Marce, and TolgaTasdizena,daScientific Computing and Imaging InstitutebSchool of Computing, University of UtahcDepartment of Biology, University of UtahdDepartment of Electrical Engineering, University of UtaheMoran Eye Center, University of Utah School of MedicineAbstractStudy of nervous systems via the connectome, the map of connectivities of all neurons in that system,is a challenging problem in neuroscience. Towards this goal, neurobiologists are acquiring largeelectron microscopy datasets. However, the shear volume of these datasets renders manual analysisinfeasible. Hence, automated image analysis methods are required for reconstructing the connectomefrom these very large image collections. Segmentation of neurons in these images, an essential stepof the reconstruction pipeline, is challenging because of noise, anisotropic shapes and brightness,and the presence of confounding structures. The method described in this paper uses a series ofartificial neural networks (ANNs) in a framework combined with a feature vector that is composedof image intensities sampled over a stencil neighborhood. Several ANNs are applied in seriesallowing each ANN to use the classification context provided by the previous network to improvedetection accuracy. We develop the method of serial ANNs and show that the learned context doesimprove detection over traditional ANNs. We also demonstrate advantages over previous membranedetection methods. The results are a significant step towards an automated system for thereconstruction of the connectome.KeywordsMachine Learning; Membrane Detection; Auto-Context; Artificial Neural Networks; Filter Bank;Contour Completion; Neural Circuit ReconstructionI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscript1. IntroductionNeural circuit reconstruction, i.e. the connectome [1], is currently one of the grand challengesfacing neuroscientists. Similarly, the National Academy of Engineering has listed reverse-engineering the brain as one its grand challenges 1. While neural circuits are central to the study© 2010 Elsevier B.V. All rights reserved.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customerswe are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resultingproof before it is published in its final citable form. Please note that during the production process errors may be discovered which couldaffect the content, and all legal disclaimers that apply to the journal pertain.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptJurrus et al.Page 2of the nervous system, relatively little is known about differences in existing neuronal classes,patterns, and connections. Electron microscopy (EM) is an unique modality for scientistsattempting to map the anatomy of individual neurons and their connectivity because it has aresolution that is high enough to identify synaptic contacts and gap junctions. These areimportant indicators for types of neuron topology and are required for neural circuitreconstruction. Several researchers have undertaken extensive EM imaging projects in orderto create detailed maps of neuronal structure and connectivity [2, 3]. Early work in this area,by White et al. [4], includes the complete mapping of the nematode C. elegans nervous system.This is a simple organism, containing just over 300 neurons and 6000 synapses, yet it tooknearly a decade to identify all the relevant structures and reconstruct the connectivity 2. Incomparison, newer imaging techniques are producing much larger volumes of very complexorganisms, with thousands of neurons and millions of synapses [5,6]. Thus, automating thereconstruction process is of paramount importance.The ability to reconstruct neural circuitry at ultrastructural resolution is of substantial clinicalimportance. Retinal degenerative diseases, including pigmentosa and macular degeneration,result from a loss of photoreceptors. Photoreceptor cell stress and death induces subsequentchanges in the neural circuitry of the retina resulting in corruption of the surviving retinal cellclass circuitry. Ultrastructural examination of the cell identity and circuitry reveal substantialchanges to retinal circuitry with implications for vision rescue strategies [7,8,9,10,11,12,13].These findings in retinal degenerative disease mirror findings in epilepsy where neural circuitsalso undergo remodeling in presumed response to abnormal electrical activity clinicallymanifested as seizures. Scientists are interested in examining normal and pathological synapticconnectivities and how neuronal remodeling contributes to neuronal pathophysiology [14,15,16]. Examination of synaptic and dendritic spine formation during development provide insightinto the adaptivity of neural circuits [17,18]. Ultrastructural evaluation of multiple canonicalvolumes of neural tissue are critical to evaluate differences in connectivity between wild typeand mutants. The complexity and size of the these datasets, often approaching 17 terabytes,makes human segmentation of the complex textural information of electron microscopicimagery a difficult task. Moreover, population or screening studies become unfeasible sincefully manual segmentation and analysis would require multiple years of manual effort perspecimen. As a result, better image processing techniques are needed to help with automatedsegmentation of EM data including identification of neurons and the connections.1.1. Serial-section transmission electron microscopyThe modality we have chosen for reconstructing the connectome at the individual cell level isserial-section transmission electron microscopy (TEM). It provides scientists with images thatcapture the relevant structures; however, it poses some interesting challenges for imageprocessing. Most importantly, serial-section TEM offers a relatively wide field of view toidentify large sets of cells that may wander significantly as they progress through the sections.It also has an in-plane resolution that is high enough for identifying synapses. In collectingimages through TEM, sections are cut from a specimen and suspended so that an electron beamcan pass through it creating a projection. The projection can be captured on a piece of film andscanned or captured directly as a digital image. An important trade-off occurs with respect tothe section thickness. Thinner sections are preferable from an image analysis point of viewbecause structures are more easily identifiable due to less averaging. However, from anacquisition point of view, thinner sections are harder to handle and impose a limit on the areaof the section that can be cut. For instance, in the rabbit retina, scientists need to study sections1William Perry, Farouk El-Baz, Wesley Harris, Calestous Juma, Raymond Kurzweil, and Robert Langer, The unveiling of the grandchallenges for engineering, in AAAS Meeting, Feb 2008.2Emily Singer, A wiring diagram of the brain, Technology Review, Nov 2007.Med Image Anal. Author manuscript; available in PMC 2011 December 1.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptJurrus et al.Page 3with areas as large as 250µm in diameter to gain a sufficient understanding of neuralconnectivity patterns. Sections of this size can be reliably cut at 50 – 90nm thickness with thecurrent serial section TEM technology. This leads to an extremely anisotropic resolution, 2 –5nm in-plane compared to 50 – 90nm out-of-plane, and poses two image processing challenges.First, the cell membranes can range from solid dark curves for neurons that run approximatelyperpendicular to the cutting-plane, to grazed grey swaths for others which run more obliquelyand suffer more from the averaging effect. Consequently, segmentations of neurons in these2-D images, are difficult given the change in membrane contrast and thickness. Second, dueto the large physical separation between sections, shapes and positions of neurons can changesignificantly between adjacent sections.There are alternative specimen preparation and EM imaging techniques that can be used forneural circuit reconstruction such as Serial-Block Face Scanning Electron Microscopy.Briggman and Denk proposed a specimen preparation which only highlights extracellularspaces removing almost all contrast from intracellular structures [5]. However, it is not possibleto identify synapses with that approach. Identification of synapses is an important part of neuralcircuit reconstruction because it determines which cells are communicating, and where in thecircuitry they connect. To highlight synapses in TEM, scientists must use a stain that alsohighlights intracellular structures, such as vesicles and mitochondria, as well as neuronmembranes. Therefore, image segmentation techniques must account for these datacharacteristics in order to identify and successfully track neurons across hundreds of sections.1.2. Neuron segmentationThere are two general approaches for neuron segmentation. One approach focuses first on thedetection of neuron membranes in each 2-D section [19,20,21]. These boundaries can be usedto identify individual neurons, which are then linked across sections to form a complete neuron.Unfortunately, accurate detection of neuron membranes in EM is a difficult problem given thepresence of intracellular structures. This makes simple thresholding, edge detection (i.e.,Canny), and region growing method",
            {
                "entities": [
                    [
                        188,
                        215,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptComput Biol Med. Author manuscript; available in PMC 2023 January 31.Published in final edited form as:Comput Biol Med. 2021 May ; 132: 104353. doi:10.1016/j.compbiomed.2021.104353.In-silico development and assessment of a Kalman filter motor decoder for prosthetic hand controlMai Gamala,b, Mohamed H. Mousac, Seif Eldawlatlyb,d, Sherif M. Elbasiounyc,e,*aCenter for Informatics Science, Nile University, Giza, EgyptbComputer Science and Engineering Department, Faculty of Media Engineering and Technology, German University in Cairo, Cairo, EgyptcDepartment of Biomedical, Industrial, and Human Factors Engineering, Wright State University, Dayton, OH, USAdComputer and Systems Engineering Department, Faculty of Engineering, Ain Shams University, Cairo, EgypteDepartment of Neuroscience, Cell Biology, and Physiology, Wright State University, Dayton, OH, USAAbstractUp to 50% of amputees abandon their prostheses, partly due to rapid degradation of the control systems, which require frequent recalibration. The goal of this study was to develop a Kalman filter-based approach to decoding motoneuron activity to identify movement kinematics and thereby provide stable, long-term, accurate, real-time decoding. The Kalman filter-based decoder was examined via biologically varied datasets generated from a high-fidelity computational model of the spinal motoneuron pool. The estimated movement kinematics controlled a simulated MuJoCo prosthetic hand. This clear-box approach showed successful estimation of hand movements under eight varied physiological conditions with no retraining. The mean correlation coefficient of 0.98 and mean normalized root mean square error of 0.06 over these eight datasets provide proof of concept that this decoder would improve long-term integrity of performance while performing new, untrained movements. Additionally, the decoder operated in real-time (~0.3 ms). Further results include robust performance of the Kalman filter when re-trained to more severe post-amputation limitations in the type and number of motoneurons remaining. An additional analysis shows that the decoder achieves better accuracy when using the firing of individual motoneurons as input, compared to using aggregate pool firing. Moreover, the decoder demonstrated robustness to noise affecting both the trained decoder parameters and the decoded motoneuron activity. These results demonstrate the utility of a proof of concept Kalman filter decoder that can support prosthetics’ control systems to maintain accurate and stable real-time movement performance.*Corresponding author. 3640 Colonel Glenn Hwy, Dayton, OH, 45435, USA., sherif.elbasiouny@wright.edu (S.M. Elbasiouny). Declaration of competing interestThe authors do not have any conflict of interest.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptGamal et al.KeywordsMotoneurons; Firing rate; Decoding; Kalman filter; Prosthetic control1. IntroductionPage 2Limb loss dramatically limits the lifestyle of an amputee, and upper-limb prostheses have helped amputees to overcome this functional disability [1–3]. Muscle-based electromyographic (EMG) signals and neural-based electroneurographic (ENG) signals [4] have all been used to drive prosthesis control. The peripheral EMG and ENG signals particularly have potential to provide naturalistic control, as they contain detailed low-level information about the neural drive to the muscles [5]. This information, encoded via motoneuron action potentials or spike trains, provides a fine resolution of movement intention [6]. Thus, motor unit activity has been recorded in amputees from ENG signals through electrodes implanted in peripheral nerves [4,7–12]. Numerous spike-sorting algorithms have been applied to peripheral nerve recordings to extract individual motor unit activity [13]. Motor unit activity can also be indirectly measured by decoding from EMG activity using decomposition algorithms [4–6,14–22]. Such algorithms achieved better performance in movement estimation than did the conventional amplitude-based features of EMG activity [6,19–23]. More recently, the activity of individual motor units, indirectly measured from EMG with high-fidelity pin electrodes, has been demonstrated to provide more responsive, smooth, and proportional control compared to the conventional EMG features [23].However, several limitations and difficulties still impede amputees from attaining full, natural movement with their prostheses [1,24,25]. While state-of-the-art prostheses can perform complex movements, the control algorithms for prosthetic motion are a major reported factor in limited prosthesis functionality [1,24,25]. Thus, enhanced control systems are needed to provide more naturalistic control of prostheses [25,26]. Specific needs include improved prosthesis movement accuracy and response time [4,27]. In addition, amputees need the control system to respond accurately to new and different intended movements without retraining of the control algorithm [4,27]. Most importantly, amputees require improved longevity of prosthetic control systems. Currently, motor decoders become unreliable after a short period of real-life use. This greatly hinders their usefulness outside the laboratory [5], probably contributing to the abandonment of their upper-limb prostheses by up to 50% of amputees [25]. While the variety of decoding approaches and the limited duration of clinical testing [13] has made it difficult to quantitate the extent of this issue, some studies report neural features used by decoders degrading in just a few days [5] or that training is required on a weekly or even daily basis [9]. In consequence, amputees are burdened by either frequent recalibrations of their prostheses or a rapid decline in their limbs’ responsiveness.Such degraded performance occurs because the amputee’s physiological state changes continually, yet the limb is calibrated to one or few states and tested in one, or a few limited, conditions. For instance, one’s neuromodulatory state (the excitability level of one’s Comput Biol Med. Author manuscript; available in PMC 2023 January 31.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptGamal et al.Page 3motoneurons) fluctuates throughout the day in response to the intensity of motion required [28]. Second, decoders are also generally designed to mimic a limited range of “normal” motoneuron output, which result primarily from orderly recruitment (in which motoneurons are recruited from smallest to largest). However, reversed and mixed recruitment orders have been observed in animals and humans [29–31]. Third, on a longer timescale, amputation commonly causes neurodegeneration and ongoing shifts in motoneuron numbers and their electrical properties. These changes continue well beyond the original injury and lead to fewer and loss of certain types of motoneurons as well as higher heterogeneity in the electrical properties of remaining MNs [32]. These changes also allow procedures like bionic reconstruction and targeted muscle reinnervation (TMR) to take place leading to pool compartmentalization and changes in MN firing characteristic [33,34]. Also, hardware problems, such as electrode and wire breakage, lead to fewer cells or data channels to record from to feed the decoder with incoming information [5,35,36]. Together, these issues continue to represent critical barriers to the development of prosthesis control systems that provide long-term reliability and accuracy.Efforts to produce robustly accurate prosthetic control algorithms include pattern recognition and non-pattern recognition methods [4,27,37,38]. However, the ability of such approaches to generate naturalistic movements is limited [37,38]. Here, we propose to use the Kalman filter, which has been extensively utilized in neural decoding for prosthesis control [39,40] due to its high accuracy and fast computational speed [40]. Moreover, it is known to generate stable output from noisy input signals, which occur in motor decoding problems [5,9,41]. Importantly, the Kalman filter was also shown to be robust even without large amounts of training data [9]. Multiple studies used the Kalman filter in movement estimation by decoding data recorded either from the motor cortex [41–44], ENG signals [9,45,46], EMG signals [47–49], or by combining peripheral neural and EMG signals [50,51]. Together, these studies suggest that the Kalman filter has potential to enhance the performance of prosthetic control algorithms.Therefore, the objective of this study was to develop a motoneuron-based Kalman filter decoder to support advanced prosthetic hand control systems that maintain long-term, accurate, real-time performance, in the context of both new, untrained movements and ongoing physiological changes. To accomplish this, we employed a recently developed multi-scale, high-fidelity computational model of the motor pool by Ref. [52] to input simulated motoneuron spiking activity into our proposed decoder. The model, developed in our prior work, provides high-fidelity, 3D representations of all three types of motoneurons in the pool, and has been shown to provide a highly accurate simulation of firing behaviors [53]. The Kalman-based algorithm decodes the simulated activity to estimate the underlying synaptic input (i.e., excitation level) that drives the firing of the modeled motoneurons. The estimated synaptic input is then mapped to specific movements of a simulated prosthetic hand (by MuJoCo software). Using simulated input allows us to control, alter, and know the exact biological conditions in which the decoder is performing. Further, by using a simulated prosthetic hand, we can accurately compare hand movements resulting from decoded signals against hand movements resulting from the actual synaptic inputs, which can be directly input to the simulated prost",
            {
                "entities": [
                    [
                        246,
                        278,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Published in \"Medical image analysis\", 2018, vol. 43, pp. 66-84, which should be cited to refer to this work.DOI: 10.1016/j.media.2017.09.007Large-scale Retrieval for Medical Image Analytics: AComprehensive ReviewZhongyu Lia, Xiaofan Zhanga, Henning M¨ullerb, Shaoting Zhanga,∗aDepartment of Computer Science, University of North Carolina at Charlotte, USAbUniversity of Applied Sciences Western Switzerland (HES-SO), Sierre, SwitzerlandAbstractOver the past decades, medical image analytics was greatly facilitated by theexplosion of digital imaging techniques, where huge amounts of medical im-ages were produced with ever-increasing quality and diversity. However, con-ventional methods for analyzing medical images have achieved limited suc-cess, as they are not capable to tackle the huge amount of image data. In thispaper, we review state-of-the-art approaches for large-scale medical imageanalysis, which are mainly based on recent advances in computer vision, ma-chine learning and information retrieval. Specifically, we first present the gen-eral pipeline of large-scale retrieval, summarize the challenges/opportunitiesof medical image analytics on a large-scale. Then, we provide a comprehen-sive review of algorithms and techniques relevant to major processes in thepipeline, including feature representation, feature indexing, searching, etc.On the basis of existing work, we introduce the evaluation protocols andmultiple applications of large-scale medical image retrieval, with a variety ofexploratory and diagnostic scenarios. Finally, we discuss future directions oflarge-scale retrieval, which can further improve the performance of medicalimage analysis.Keywords: Medical image analysis, information retrieval, large scale,computer aided diagnosis∗Corresponding author, rutgers.shaoting@gmail.comPreprint submitted to Medical Image AnalysisSeptember 21, 2018\f123456789101112131415161718192021222324252627282930313233343536371. IntroductionMedical image analytics plays a central role in clinical diagnosis, image-guided surgery and pattern discovery. Many protocols and modalities ofdigital imaging techniques have been adopted to generate medical images,including magnetic resonance imaging (MRI) (Slichter, 2013), computed to-mography (CT) (Hsieh, 2009), photon emission tomography (PET) (Baileyet al., 2005), ultrasound (Szabo, 2004), fluorescence microscopy (Lichtmanand Conchello, 2005), X-ray (Lewis, 2004) and others. Generally, these med-ical images reflect specific aspects (anatomy, function) of tissue types/organsthat require an accurate interpretation and analysis from either domain ex-perts or computer-aided decision support. In comparison with domain ex-pert analysis that is labor intensive and time-consuming, computer-aidedapproaches are efficient and its accuracy has increased continuously withthe rapid development of computer vision, machine learning and relatedfields (Doi, 2014; Katouzian et al., 2012; May, 2010). To support computer-aided medical image analytics, one important task is content-based imageretrieval (CBIR) (Akg¨ul et al., 2011; Lehmann et al., 2004; M¨uller et al.,2004), i.e., indexing and mining images that contain a similar visual con-tent (e.g., shape, morphology, structure, etc). For a new medical image tobe analyzed, a CBIR system can first retrieve visually similar images in anexisting dataset. Then, its high-level descriptions and interpretations can beexplored based on the retrieved images.Over the past 25 years, CBIR has been one of the most vivid researchtopics in the field of computer vision. Many CBIR methods were developedfor accurate and efficient image retrieval. Especially in recent years, withthe ever-increasing number of digital images (e.g., ImageNet (Russakovskyet al., 2015), COCO (Lin et al., 2014), PASCAL VOC (Everingham et al.,2010), etc), CBIR has moved towards the era of big data. Massive amounts ofimages can provide rich information for comparison and analysis, and thusfacilitate the generation of new algorithms and techniques that can tackleIn general, large-scale image retrievalimage retrieval in large databases.can be divided into two stages, i.e., feature extraction to represent imagesand feature indexing. Deep learning (LeCun et al., 2015) is one of the mostpopular methods for feature representation that is particularly suitable forlarge image databases, where massive amounts of data can boost the retrievalperformance by training deep and complex neural networks with millions ofparameters (Babenko and Lempitsky, 2015; Wan et al., 2014). For the feature2\f38394041424344454647484950515253545556575859606162636465666768697071727374indexing at a large-scale, the key problem is computational efficiency, i.e.,similarity searching in millions of images with thousand dimensional featuresvectors. Methods such as vocabulary trees (Nister and Stewenius, 2006) andhashing (Wang et al., 2016) can efficiently tackle this problem, either throughchanging the indexing structure or compressing the original features.Despite the current large-scale methods having achieved many successesin generic image retrieval problems, how to best tackle the retrieval in large-scale medical image databases is still a very challenging topic (Zhang andMetaxas, 2016). On the one hand, the meaning of large-scale in the medicalimage field is somewhat different from large-scale in the generic image do-main. Generally, each patient can generate hundreds to thousands of imageslices using different protocols, modalities (e.g., CT, MRI, X-ray) and multi-ple dimensions (e.g., volumetric 3D, time series). These volumes are usuallystored in many single images (as slices) in the DICOM (Digitla Imaging andCommunications in Medicine) format (Kahn et al., 2007). Besides this, thesize of some medical images can be extremely large. For example, the whole-slide histopathological images can include more than 100, 000×100, 000 pixelsand thus each is usually split into millions of small patches for processing.On the other hand, medical images are usually more difficult to analyze com-pared to generic images. The complex imaging parameters (contrast agents,machine settings), anatomic difference and interactions between different dis-eases result in a more complex analysis compared with natural images, wherebroad object categories are recognized and used for similarity calculations.The relevant changes of some medical images can be very subtle, which re-quire more fine-grained and detailed analysis. Therefore, directly employingtraditional CBIR methods may not suitable for the large-scale medical imageretrieval problem. In recent years, many efforts have been made to achievelarge-scale medical image analytics, aiming to improve the efficiency andaccuracy of image retrieval.1.1. Related WorkThere have been multiple reviews focusing on content-based medical im-age retrieval. The first review in the field was (Tang et al., 1999) but thetext only contained few systems with a limited scope. Muller et al. (M¨ulleret al., 2004) presented a first complete review that concentrates on imageretrieval in the medical domain, where the techniques used in medical im-age retrieval, including visual feature extraction, image comparison, systemevaluation, etc. are summarized. Subsequently, Long et al. (Long et al.,3\f757677787980818283848586878889909192939495969798991001011021031041051061071081091101112009) introduced four medical CBIR systems, i.e., CervigramFinder (Xueet al., 2008), SPIRS (Hsu et al., 2007), IRMA (IRMA), SPIRS-IRMA (An-tani et al., 2007). The authors also discussed future directions of medicalimage retrieval. Akgul et al. (Akg¨ul et al., 2011) presented a comprehensivereview about recent techniques of content-based image retrieval in radiol-ogy until 2011, including image features/descriptors, similarity measures andstate-of-the-art systems. Additionally, they discussed challenges and futuredirections for the coming decade. Hwang et al. (Hwang et al., 2012) reviewedboth text-based and content-based medical image retrieval systems, drawinga conclusion that the image retrieval service will be more effective if CBIRand semantic systems are combined. In 2013, Kumar et al. (Kumar et al.,2013) surveyed several applications and approaches to medical CBIR thatfocus on clinical imaging data that are multidimensional or acquired usingmultiple modalities such as combined PET-CT images.Besides the abovementioned survey articles, the image retrieval task ofthe Conference and Labs of the Evaluation Forum, named ImageCLEF (Im-ageCLEF; M¨uller et al., 2010), has held several medical image retrieval tasksfrom 2004-2014. ImageCLEF provides a platform for research groups sub-mitting results and competing on the performance of their medical imageretrieval methods. After each ImageCLEF medical image retrieval task, anoverview is provided to summarize the methods and results of each compe-tition groups (de Herrera et al., 2013; Kalpathy-Cramer et al., 2015, 2011;M¨uller et al., 2012), which demonstrates the state-of-the-art in the medicalimage retrieval field. A benchmark for case-based retrieval including full vol-umetric images of more than 300 patients was run as part of the VISCERALbenchmark Jimenez-del-Toro et al. (2015).1.2. Contributions and Organization of this ArticleThis survey provides a structured and extensive overview of large-scaleretrieval for medical image analytics. Despite existing reviews having sum-marized varieties of medical retrieval systems and methods, none of themfocused on the retrieval techniques for large-scale medical data, which is cur-rently the main challenge in the field of medical analytics. This survey offersa focused overview of the retrieval approaches for the large-scale medical im-age data by expanding multidisciplinary components that involve a nexusof the idea from machine learning, computer vision, information retrieval,and bioinformatics. It explains the entire process from scratch and presentsa comprehensi",
            {
                "entities": [
                    [
                        114,
                        141,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Internet Interventions 18 (2019) 100265Contents lists available at ScienceDirectInternet Interventionsjournal homepage: www.elsevier.com/locate/inventA usability study of an internet-delivered behavioural intervention tailoredfor children with residual insomnia symptoms after obstructive sleep apneatreatmentMatthew Orra, Jason Isaacsa, Roger Godboutb, Manisha Witmansc, Penny Corkuma,⁎Ta Department of Psychology and Neuroscience, Dalhousie University, Canadab Department of Psychiatry, Université de Montréal, Canadac Department of Pediatrics, University of Alberta, CanadaA R T I C L E I N F OA B S T R A C TKeywords:Obstructive sleep apneaInsomniaChildInternet interventioneHealthSleep disordersBetter Nights, Better Days (BNBD) is a 5-session online intervention designed to treat insomnia in 1–10-year-oldchildren (Corkum et al. 2016). Obstructive sleep apnea (OSA) and insomnia commonly occur in children and,after surgical treatment for OSA, it is estimated that up to 50% of children may continue to suffer from insomniasymptoms. Access to insomnia interventions following OSA treatment is limited as there are few programsavailable, few trained practitioners to deliver these programs, and limited recognition that these problems exist.The current study involved the usability testing of an internet-based parent-directed session of BNBD tailoredtowards the needs of children (ages 4–10 years) who experience residual insomnia symptoms after treatment ofOSA. This new session was added to the BNBD program. Participants (n = 43) included 6 parents, 17 sleepexperts, and 20 front-line healthcare providers who completed and provided feedback on the new session.Participants completed a feedback questionnaire, with both quantitative and qualitative questions, after re-viewing the session. Quantitative responses analyzed via descriptive statistics suggested that the session wasprimarily viewed as helpful by most participants, and open-ended qualitative questions analyzed by contentanalyses generated a mix of positive and constructive feedback. The results provide insights on how to optimallytailor the BNBD program to meet the needs of the target population and suggest that testing the session on alarger scale would be beneficial.1. IntroductionInsomnia has a significant impact on children's daily functioningand development (Curcio et al., 2006; Paavonen et al., 2000) andprevious studies suggest that there are significant barriers to accessingpediatric behavioural sleep interventions (Boerner et al., 2013; Honaker& Meltzer, 2016). Better Nights, Better Days (BNBD) is an interactive 5-session, parent-directed eHealth program intended to share psychoe-ducation and behavioural strategies about insomnia in 1–10-year-oldchildren (Corkum et al., 2016). Sessions focus on the importance ofsleep and consequences of poor sleep (Session 1), healthy sleep prac-tices (Session 2), falling asleep independently (Session 3), stayingasleep through the night, reducing early morning awakenings, andensuring adequate napping for younger children (Session 4), and a re-view of progress combined with future goal setting (Session 5). Thefocus of the current study was a usability test of a new session of BNBDtailored for the specific needs of parents of children ages 4–10 yearswho had previously been surgically treated for Obstructive Sleep Apnea(OSA).OSA involves a narrowing or obstruction of the upper airway duringsleep resulting in ventilatory disturbances, caused either by adeno-tonsillar hypertrophy (Capdevila et al., 2008) or obesity (Capdevilaet al., 2008; Gaines et al., 2018; Patinkin et al., 2017), and results insleep disturbance and daytime impairment (Rosen, 2010). Snoring,gasping, and choking are the most common nighttime symptoms of OSAfor children (Dehlink and Tan, 2016; Trosman, 2013). The con-sequences of OSA are significant and can include psychosocial problemssuch as behavioural dysregulation (Blechner and Williamson, 2016;Owens, 2009), cognitive and school-related problems (Gozal, 2009;Hunter et al., 2016; Xanthopoulos et al., 2015), as well as increased riskof physical problems such as diabetes, cardiovascular disease, andobesity (Framnes and Arble, 2018).OSA is often thought of as an adult disorder; however, the disorderis common among children, with a prevalence of 1–5% (Lumeng and⁎Corresponding author at: 1355 Oxford Street, P.O. Box 15000, Halifax, NS B3H 4R2, Canada.E-mail address: penny.corkum@dal.ca (P. Corkum).https://doi.org/10.1016/j.invent.2019.100265Received 16 April 2019; Received in revised form 18 June 2019; Accepted 4 July 2019Available online 15 August 20192214-7829/ © 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/BY-NC-ND/4.0/).\fM. Orr, et al.Internet Interventions 18 (2019) 100265Table 1Overview of content for the OSA-Insomnia session and each of the original BNBD sessions.Session nameSession contentOSA-Insomnia Pre-session: navigating insomnia afterOSA treatmentSession 1: introduction to Better Nights, Better Days(BNBD)Session 2: healthy sleep practicesSession 3: settling to sleepSession 4: going back to sleepSession 5: looking back and aheadduring OSA may become habitual even after OSA treatment)(cid:129) Education about OSA etiology, symptoms, consequences, and primary treatments(cid:129) Understanding the relationship between OSA and insomnia(cid:129) Investigating the behaviourally-based connection between the two disorders (e.g., how night wakings that arose(cid:129) Education about how sleep works and the consequences of poor sleep with a focus on insomnia(cid:129) Introduction to the BNBD team(cid:129) Information about healthy sleep practices and the ways in which they can lead to better sleep(cid:129) Focus on settling independently at bedtime (i.e., self-soothing)(cid:129) Addresses what to do if a child wakes during the night or is up too early in the morning(cid:129) Review the progress made over the previous sessions(cid:129) Revisit goals and address how to maintain them in the futureChervin, 2008; Meltzer et al., 2010). While the primary risk factor foradult OSA is male gender and obesity (Gaines, Vgontzas, Fernandez-Mendoza, & Bixler, 2018; Qaseem et al., 2014), the primary contributorto pediatric OSA is enlarged adenoids or tonsils (i.e., adenotonsillarhypertrophy) (Erler and Paditz, 2004; Rosen, 2010). Various other riskfactors have been described including asthma, allergies, prematurity,ethnicity, and exposure to environmental tobacco smoke (Erler andPaditz, 2004; Redline et al., 1999; Rosen, 2010). Prevalence rates arehighest when children are 3–6 years old, a period during which thetonsils and adenoids are largest relative to the size of the upper airway(Ahn, 2010; Li et al., 2016; Sheldon et al., 2014). Due to the risks as-sociated with childhood OSA, early identification and treatment areessential (Marcus et al., 2012). Adenotonsillectomy (AT; surgical re-moval of adenoids and tonsils) is recommended as the first-line treat-ment for childhood OSA (Cielo and Gungor, 2016; Tan et al., 2016;Trosman, 2013).Previous research indicates that many children continue to havesleep disturbances after adenotonsillectomy. This can in part be un-derstood by the high comorbidity rate between OSA and insomnia (Al-Jawder and BaHammam, 2012; Lack and Sweetman, 2016). Insomniainvolves problems initiating sleep, staying asleep, or waking too early(American Academy of Sleep Medicine, 2014). Preliminary researchindicates a rate of comorbidity among children of approximately 30%(Kukwa et al., 2016; Owens et al., 1998); however, to date, little re-search has been conducted on this comorbidity in children (Byars et al.,2011). Children with comorbid OSA and insomnia, like children withjust OSA, tend to undergo adenotonsillectomy as a first-line treatmentfor their sleep-related issues. While adenotonsillectomy may success-fully treat OSA, it has been reported that > 50% of children with co-morbid insomnia and OSA may continue to have insomnia symptomsafter recovering from surgery (i.e., six months post-OSA surgery)(Chervin et al., 2014), meaning that approximately 15% of all kids withOSA may continue to have insomnia after treatment for OSA. The causeof this is likely multifaceted, but one potential cause is that apnea-re-lated episodes can result in difficulties falling asleep and contribute tonight awakenings, establishing a behavioural pattern which continueseven after adenotonsillectomy (Byars et al., 2011).Even though it is common for insomnia to persist after OSA treat-ment, research on the effectiveness of treatments for residual insomniais limited (Björnsdóttir et al., 2013; Manickam et al., 2015). Insomnia isless likely to be screened for in the context of OSA (Byars et al., 2011),and as such, insomnia may continue to present itself even after OSA hasbeen treated. Thus, treatments should be explored for children withtreated OSA who continue to suffer from insomnia after recoveringfrom adenotonsillectomy.Behaviouralinterventions are the recommended treatmentforchildren with insomnia (Vriend and Corkum, 2011; Meltzer andMindell, 2014; Morgenthaler et al., 2006; Taylor and Roane, 2010),however, in-person treatment is often difficult to access due to factorssuch as finances, transportation, or a general lack of available services(i.e., behavioural(Speth et al., 2015).eHealth interventionsinterventions delivered via the internet) have become increasinglypopular over the last decade, as they improve accessibility. eHealthinterventions designed for a wide range of children's mental and phy-sical health issues have been shown to be an effective and cost-savingalternative to providing children with in-person treatment (Cushing andSteele, 2010). While there are no published eHealth interventions forinsomnia in school-aged children, there is preliminary evidence de-monstrating that distally provided behavioural interventions can ",
            {
                "entities": [
                    [
                        4489,
                        4517,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Computers in Biology and Medicine 104 (2019) 339–351Contents lists available at ScienceDirectComputers in Biology and Medicinejournal homepage: www.elsevier.com/locate/compbiomedRethinking multiscale cardiac electrophysiology with machine learning andpredictive modellingChris D. Cantwella,b,∗Charles Houstona,c, Rasheda A. Chowdhurya,c, Fu Siong Nga,c, Anil A. Bharatha,d,Nicholas S. Petersa,ca ElectroCardioMaths Group, Imperial College Centre for Cardiac Engineering, Imperial College London, London, UKb Department of Aeronautics, Imperial College London, South Kensington Campus, London, UKc National Heart and Lung Institute, Imperial College London, South Kensington Campus, London, UKd Department of Bioengineering, Imperial College London, South Kensington Campus, London, UK, Yumnah Mohamieda,c, Konstantinos N. Tzortzisa,c, Stef Garastoa,d,TA R T I C L E I N F OA B S T R A C TKeywords:Cardiac electrophysiologyCardiac arrhythmiaElectrogramMachine learningPredictive modellingDeep learningWe review some of the latest approaches to analysing cardiac electrophysiology data using machine learning andpredictive modelling. Cardiac arrhythmias, particularly atrial fibrillation, are a major global healthcare chal-lenge. Treatment is often through catheter ablation, which involves the targeted localised destruction of regionsof the myocardium responsible for initiating or perpetuating the arrhythmia. Ablation targets are either ana-tomically defined, or identified based on their functional properties as determined through the analysis ofcontact intracardiac electrograms acquired with increasing spatial density by modern electroanatomic mappingsystems. While numerous quantitative approaches have been investigated over the past decades for identifyingthese critical curative sites, few have provided a reliable and reproducible advance in success rates. Machinelearning techniques, including recent deep-learning approaches, offer a potential route to gaining new insightfrom this wealth of highly complex spatio-temporal information that existing methods struggle to analyse.Coupled with predictive modelling, these techniques offer exciting opportunities to advance the field and pro-duce more accurate diagnoses and robust personalised treatment. We outline some of these methods and il-lustrate their use in making predictions from the contact electrogram and augmenting predictive modellingtools, both by more rapidly predicting future states of the system and by inferring the parameters of these modelsfrom experimental observations.1. IntroductionCardiac arrhythmias, particularly atrial fibrillation (AF), are a majorglobal healthcare challenge in the developed world. Arrhythmias de-scribe the abnormal and self-perpetuating propagation of action po-tentials (AP) within the myocardium. Their initiation and maintenanceare incompletely understood and this has hindered the development ofeffective and reliable therapy. Treatment for AF is often through ca-theter ablation, where the regions of myocardium determined to beresponsible for initiating or perpetuating the disturbance are targetedand made electrically inactive through the localised application ofradio-frequency energy or freezing. For paroxysmal AF, catheter abla-tion delivers relatively good outcomes, with success rates in the regionof 80–90 percent [1]. However, outcomes of catheter ablation in pa-tients with persistent AF remain disappointing, and is effective in onlyapproximately 50 percent of patients, despite all forms of adjunctiveablation strategies [2].Identifying the critical sites responsible for abnormal AF main-tenance has been a major focus of research, with a number of drivingmechanisms, including rotors [3], multiple wavelets [4] and epi-endodisassociation [5], being proposed. Recent clinical studies, such as theCONFIRM study [6], have tested the new approaches of catheter ab-lation by targeting the foci of rotational drivers, with initially promisingresults showing that 86% of 101 cases achieved AF termination orslowing. However, subsequent studies suggest more moderate outcomeswith Steinberg et al. [7] reporting only 4.7% of 47 cases achieved AFtermination, while 60% documented recurrence within 12 months. Theefficacy of this technique may be in part due to the poor spatial re-solution of the global mapping catheter used [8]. Techniques involvingthe targeting of complex fractionated atrial electrograms (CFAE) [9],∗Corresponding author. Department of Aeronautics, Imperial College London, South Kensington Campus, London, UK.E-mail address: c.cantwell@imperial.ac.uk (C.D. Cantwell).https://doi.org/10.1016/j.compbiomed.2018.10.015Received 29 June 2018; Received in revised form 4 October 2018; Accepted 14 October 20180010-4825/ © 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/BY/4.0/).\fC.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351high dominant frequency (DF) [10] and singularities identified duringphase mapping [11] have each been used as strategies for terminatingarrhythmias. However, none of these adjunctive ablation strategieshave been shown to add any value to the conventional approach ofelectrically isolating the pulmonary veins [2]. Part of the reason for thismay be that they each discard a large proportion of the informationcontent of the acquired electrogram signals or their spatio-temporalassociation during analysis. Additionally, not all identified sites may becritical to the perpetuation of the arrhythmia, leading to excessive ab-lation. The complexity of the underlying electro-architecture of myo-cardium therefore requires a more sophisticated, personalised andmulti-faceted approach to address the challenge of treating AF.The principle data modality used clinically for the treatment of AF isthe contact electrogram, which arises from the superposition of electricfields induced by charged ions moving across cell membranes in themyocardium. It is the electrical signature of action potential propaga-tion through tissue which implicitly encodes the functional and struc-tural characteristics of the local substrate. The electrogram thereforeprovides a wealth of information which is rarely fully utilised in currentclinical practice. Electrograms are normally only broadly categorised bybinary descriptors – such as simple or complex, early or late [12,13],fractionated or non-fractionated – with much of the signal content ef-fectively discarded. Despite a number of studies based on interpretingclinical electrogram data [14,15], these do not investigate how elec-trogram morphology is influenced by individual electro-architecturalfactors. Our knowledge about the direct effects of electrical remodellingon electrogram morphology is consequently poor, considering thenumber of these abnormalities related to cardiac diseases [16]. Lever-aging the electrogram to infer electroarchitectural properties of themyocardium may therefore provide new direct insight in locating cri-tical sites for ablation.Multiple concurrently recorded electrograms may be combined toevaluate the spatio-temporal propagation patterns occurring in thetissue. This activity can also be inferred from the surface of the body[17]. More recently, predictive modelling of action potential propaga-tion is emerging as a potential tool for personalised testing and opti-misation of interventions [18], but this technology is heavily dependenton the accuracy of the underlying calibration of parameters. This canonly be achieved by fully leveraging the huge wealth of informationnow available clinically. The data science revolution in the form ofsophisticated machine learning algorithms and increasing availabilityof computing power, opens up possibilities to manage this data over-load, both in terms of learning from the data, inferring model para-meters and consequently making increasingly accurate predictions.1.1. Machine learning in cardiac electrophysiologyMachine learning describes a class of algorithms which learn modelparameters from a set of training data (for which outcomes may, or maynot, be known) with the purpose of accurately predicting outcomes forpreviously unseen data. Training data that includes associated outcomelabels can be used for supervised learning in which the algorithm usesthis knowledge to directly improve its prediction. In contrast, un-supervised learning seeks patterns in the data with more limited gui-dance, of which clustering is a common example. Although there isconsiderable overlap, machine learning methods are considered todiffer from more conventional statistical modelling, such as regression,in that they are more concerned with the predictive accuracy of theresulting model rather than the ability to explain the reasoning behindits parameters and determining concrete relationships between thedata. The high accuracy of some of the more recent machine learningmethods – which are virtually impossible to analyse analytically – hasjustified this lack of transparency.All machine learning algorithms seek some form of mapping thatmodels the relationship between input data and outcome. In an abstractcontext, we suppose that we have a model f, governed by one or moreparameterslation, which maps an inputto some output, under the re-(1)The form and dimensions ofin Equation (1) are a functionandof the particular problem under consideration. For example, may be alarge one-dimensional vector (time-series) in the case of a music-clas-sification problem, or a two-dimensional image in the case of objectrecognition. For regressions, the output may be a prediction of thedependent quantity, while for classification problems,is usually alabel which assigns the corresponding input to a single class. The size ofdepends on both the problem and also on the chosen model. For ex-ample, for a linear regression between two variables would consist ofonly two values (namely the slope and intercept)",
            {
                "entities": [
                    [
                        4630,
                        4662,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptNeurocomputing. Author manuscript; available in PMC 2022 May 14.Published in final edited form as:Neurocomputing. 2021 May 14; 436: 22–34. doi:10.1016/j.neucom.2020.12.130.Information Capacity of a Stochastically Responding NeuronAssemblyI. Smyrnakis1, M. Papadopouli1,2, G. Pallagina3, S. Smirnakis3,41Institute of Computer Science, Foundation for Research & Technology-Hellas2Department of Computer Science, University of Crete, Heraklion, Greece3Department of Neurology, Brigham and Womens Hospital, Harvard Medical School, Boston MA021154Jamaica Plain VA Hospital, Harvard Medical SchoolAbstractIn this work, certain aspects of the structure of the overlapping groups of neurons encodingspecific signals are examined. Individual neurons are assumed to respond stochastically to inputsignal. Identification of a particular signal is assumed to result from the aggregate activity of agroup of neurons, which we call information pathway. Conditions for definite response and fornon-interference of pathways are derived. These conditions constrain the response properties ofindividual neurons and the allowed overlap among pathways. Under these constrains, and underthe simplifying assumption that all pathways have similar structure, the information capacity ofthe system is derived. Furthermore, we show that there is a definite advantage in the informationcapacity if pathway neurons areinterspersed among the neuron assembly.1 IntroductionVisual cortex neurons fire action potentials when visual stimuli appear within their receptivefields, and visual information is encoded in real time via the joint firing of multiple neurons.Although much is known about the properties of single neuronal units, the rules by whichcortical neurons coordinate their activity to represent information about stimuli remainelusive. To understand why, one must consider that the responses of single units are bothnoisy and ambiguous [1], [2], with large trial-to-trial variability in response strength andprobability [3], [4]. In other words, repeated responses to the same stimulus varyconsiderably, and responses to multiple different stimuli can be the same. To achieve optimalreal-time performance, these ambiguities must be resolved at the level of neuronalpopulations by the coordinated firing of distinct neuronal ensembles. However, it is not clearhow these ensembles must behave in order to allow stable unambiguous percepts to emerge.In contrast to the typical variability of single neuron firing, typical stimulus induced percepts(e.g., barring bi-stability phenomena and other ambiguous types of stimuli) appear to bedefinite, effectively noise-free, representations of the stimulus. This suggests that thecoordinated firing of appropriate neuronal ensembles, here called”information pathways”, isable to represent and transmit information about a wide class of stimuli with low degree ofuncertainty.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptSmyrnakis et al.Page 2One important question is how the pathways, where definite information about the stimulusis encoded, are implemented in the brain. Existence of super-specialized cells that representa specific stimulus class or even unique stimuli has been postulated. Such “grandmother-likecells” that respond reliably to increasingly complex arrangements of the stimuli (objects) arefound in higher associative cortical areas of primates [5], [6]. They are thought to receiveunreliable input from multiple lower-level neurons and integrate it into a definiterepresentation. However, such cells are not found in early visual areas, such as area V1, andthey are elusive even in higher areas. Hence, it is likely that earlier visual areas representdefinite information in the aggregate, nearly simultaneous, firing of ensembles of neuronswhose collective output reliably represents a given stimulus (e.g., an oriented bar in areaV1). In agreement with this, it has been shown that recurring recruitment of feature-selectivecells into co-firing neuronal ensembles occurs during natural visual stimulation by visualscenes in area V1 [7], [8]. Moreover, neurons with similar feature-selectivity have increasedprobability to be wired together even though in rodents they are distributed in a salt andpepper fashion [9],[10]. Such neuronal ensembles likely represent, in part,”informationpathways” that encode specific visual stimulus features.In addition to variability, the neurons in V1 have other three computationally importantproperties. Firstly, their activity under natural visual conditions is sparse with low noisecorrelations [11], [12], [13]. Secondly, V1 possesses a retinotopic map, where nearbyneurons share receptive field locations. Finally, in rodents, despite topografic organization,the feature-selectivity and direction / orientation tuning are not organized in columns, but aredistributed across V1 in salt-and-pepper manner [9], [10]. Diffuse localization of sparselyfiring feature-selective cells may enable efficient encoding of local features in the scene. Thesparsification resulting in low correlated noise enables the fine feature discrimination eventhough the tuning of individual units is relatively broad [3]. In what follows, we assume thatneurons in a pathway respond probabilistically to a stimulus encoded by the pathway, anddifferent pathways form overlapping information representing sets. Neuron responses areconsidered to be independent, reflecting the low value of noise correlations reported in manyvertebrates [14], [15], [13]. Early theoretical considerations [17], [16] suggest thatoptimality is achieved when there is little or no coordinated response of neurons other thanthe one induced by the jointly independent response of neurons to a particular signal. Theseassumptions are based on properties of V1 neuronal responses, which are known to besparse under natural visual stimulation conditions, with low noise correlations and relativelyhigh response variability [11], [12], [13]. Sparsification resulting in low correlated noiseenables fine feature discrimination, even though the tuning of individual units may berelatively broad [3],[18], [19]. Also, it should be added, there is evidence of increasedneuronal decorrelation of pyramidal neurons in mice at adulthood [20], suggesting that asmore information is encoded in the brain, neurons decorrelate.In what follows, we use simple models to identify the basic principles underlying theinformation content of information pathways. Specifically, we explore the computationalproperties that may allow overlapping sparsely firing neuronal assemblies to createunambiguous definite representations of visual stimuli. To achieve this, two conditions haveto be satisfied: (i) The encoding must be definite: the probability of an information pathwayto be active when a stimulus is present should be close to 1, while in the absence of stimulus,Neurocomputing. Author manuscript; available in PMC 2022 May 14.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptSmyrnakis et al.Page 3the probability should be close to 0. (ii) There should be no significant interference betweendifferent overlapping pathways. We examine the implications of conditions (i) and (ii) on thedegree of pathway overlap and neuronal assembly architecture. Further, we evaluate theinformation capacity of three incrementally more plausible architectures of the informationpathways, and find that, in the most plausible architecture, the number of non-interferingdefinitely responding pathways that can co-exist, increases exponentially in the maximumallowed overlap. This in turn is determined by the response probabilities of the individualneurons. Finally, we examine the validity of our analysis by analyzing a dataset obtained bytwo-photon imaging of layer 2/3 neurons in area V1 of adult mice.The structure of the paper is the following: In Section 2, we analyse the conditions ofdefinite response and of non-interference of overlapping bimodal pathways. In Section 3,three models for overlapping pathway organization are examined, namely the “DenseNeighbourhood Pathway Model”, the “Random Selection Model”, and the “LocalityPreserving Random Selection Pathway Model”. The implications of the organizationprinciple on the information capacity of the system are then assessed. In Section 4, we testthe main findings of the previous sections in the context of the Interneuron PyramidalPartner Groups in adult mouse V1 cortical area, as identified by [21], using the “LocalityPreserving Random Selection Pathway Model” (Section 3), since it appears to be the mostrelevant for a topographically mapped area like V1. Section 5 concludes with our mainremarks. In the appendix, we analyse a variant of the Locality Preserving Random SelectionPathway Model, where the probability that a neuron belongs to a given pathway variesaccording to the distance from the pathway center.2 Overlapping Information Pathways with Bimodal ProbabilisticallyResponding NeuronsIn this section, two important requirements of overlapping pathways are examined, namelythe conditions of definite response to a preferred signal, encoded by each pathway, and thecondition of non-interference among overlapping pathways. These conditions are examinedin the abscense of spontaneous firing as well as in the presence of spontaneous firing.2.1 No Spontaneous FiringHere we assume no spontaneous firing and derive two necessary constraints that allow amulti neuronal ensemble (pathway) to effectively transmit information. The first is that thepathway needs to give a definite response when the signal it is supposed to transmit ispresent despite the fact that its individual constituent units often do not. The second is thatthe activation of a pathway should not induce activation in another pathway whose signal isnot present.Two overlapping information pathways: Let us suppose t",
            {
                "entities": [
                    [
                        241,
                        269,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptPattern Recognit. Author manuscript; available in PMC 2018 March 01.Published in final edited form as:Pattern Recognit. 2017 March ; 63: 531–541. doi:10.1016/j.patcog.2016.09.019.Brain Atlas Fusion from High-Thickness Diagnostic Magnetic Resonance Images by Learning-Based Super-ResolutionJinpeng Zhang#,a, Lichi Zhang#,a,c, Lei Xianga, Yeqin Shaob, Guorong Wuc, Xiaodong Zhoud, Dinggang Shen*,c,e, and Qian Wang*,aaMed-X Research Institute, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200240, ChinabNantong University, Nantong, Jiangsu 226019, ChinacDepartment of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United StatesdShanghai United Imaging Healthcare Co., Ltd., Shanghai 201815, ChinaeDepartment of Brain and Cognitive Engineering, Korea University, Seoul 02841, Republic of KoreaAbstractIt is fundamentally important to fuse the brain atlas from magnetic resonance (MR) images for many imaging-based studies. Most existing works focus on fusing the atlases from high-quality MR images. However, for low-quality diagnostic images (i.e., with high inter-slice thickness), the problem of atlas fusion has not been addressed yet. In this paper, we intend to fuse the brain atlas from the high-thickness diagnostic MR images that are prevalent for clinical routines. The main idea of our works is to extend the conventional groupwise registration by incorporating a novel super-resolution strategy. The contribution of the proposed super-resolution framework is two-fold. First, each high-thickness subject image is reconstructed to be isotropic by the patch-based sparsity learning. Then, the reconstructed isotropic image is enhanced for better quality through the random-forest-based regression model. In this way, the images obtained by the super-resolution strategy can be fused together by applying the groupwise registration method to construct the required atlas. Our experiments have shown that the proposed framework can effectively solve the problem of atlas fusion from the low-quality brain MR images.KeywordsBrain atlas; super-resolution; image enhancement; sparsity learning; random forest regression; groupwise registrationEmail addresses: jinpengzhangsjtu@gmail.com (Jinpeng Zhang#), lichizhang@sjtu.edu.cn (Lichi Zhang#), dgshen@med.unc.edu (Dinggang Shen*), wang.qian@sjtu.edu.cn (Qian Wang*)Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.1. IntroductionPage 2Medical resonance (MR) imaging has become a pivotally important tool in many brain-related clinical applications and studies. Without introducing hazardous ionizing radiation, the technique allows researchers to observe in-vivo neural structures and functions in a non-invasive way. Large-scale studies are thus enabled for (early) brain development (Thompson et al., 2000; Casey et al., 2000; Lenroot and Giedd, 2006), maturation (Sowell et al., 1999; Paus et al., 2001), and aging (Resnick et al., 2000; Raz et al., 2005). The technique has also provided a unique perspective to investigate disease anomalies (Frisoni et al., 2010; Polman et al., 2011) and to assess the effects of pharmacological interventions (Mulnard et al., 2000; Jack et al., 2004). In general, MR imaging has played a key role in the field of neuroscience as well as translational medicine. Challenged by the studies of larger scales, a lot of efforts have been devoted toward computer-assisted automatic analysis of brain MR images.The brain atlas, which can often be fused from individual brain MR images, has attracted a lot of interest (Mazziotta et al., 1995; Joshi et al., 2004). Given a group of subjects, the atlas encodes the common morphological information within the group. To this end, researchers can compare the atlases of two individual groups (e.g., the diseased group and the normal control group) and then reveal the subtle difference that might be connected with the disease. Meanwhile, the atlas provides a common space where the inter-subject variation within a population can be measured quantitatively. For example, after being registered with the atlas, each subject owns a deformation field that is typically regarded as the pathway between the subject and the atlas. Since the deformation pathways of all images in the group are established upon the same common space defined by the atlas, comparing the estimated deformation fields of all images can capture the inter-subject variation within the group. Obviously, it is important to properly designate a high-quality atlas in advance for many similar studies.However, it is yet rare and difficult to fuse the atlas from the low-quality diagnostic MR images (i.e., with high inter-slice thickness). Currently most aforementioned studies are focusing on high-quality and (nearly) isotropic imaging data, which has identical resolutions in all dimensions. The acquisitions are often conducted on designated MR scanners and may cost high. The resources required for high-quality imaging, however, are not always available. In developing countries such as China, most diagnostic MR images are still scanned with high inter-slice thickness, partially due to concerns on costs of radiation examinations and limited medical resources per capita. For the real clinical data with high inter-slice thickness, the challenge to fuse the brain atlas is yet unresolved. The lack of the atlas fusion method apparently undermines the efforts to incorporate the low-quality diagnostic imaging data into clinical researches.In this work, we intend to apply learning-based super-resolution to real clinical low-quality brain MR images and then fuse the atlas in the groupwise manner (Joshi et al., 2004). Our super-resolution consists of two stages. First, since the subject images are heterogeneous with high inter-slice thickness, we adopt the non-local patch-based strategy and utilize sparsity learning to reconstruct the subject images in the isotropic space. Second, we turn to random forest and learn the regression model for image enhancement, such that the Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 3reconstructed isotropic image (with relatively low quality) is mapped to be of higher quality (i.e., by suppressing incorrect local anatomical patterns). With all subject images processed through super-resolution, we apply groupwise registration and then fuse the atlas. Specifically, for the iteratively updated group mean image, we can apply the aforementioned forest regression model to enhance its quality. The enhanced group mean image provides better guidance in groupwise registration and leads to the atlas with higher-quality essentially.The rest of this paper is organized as follows. In Section 2, we survey the recent development of the relevant works, especially atlas fusion and super-resolution. In Section 3, we provide details of the proposed framework, including the learning-based super-resolution technique and subsequent groupwise registration. In Section 4, we demonstrate the feasibility of our novel framework through experiments. Finally, we provide the discussions with conclusions in Section 5.2. Related Works2.1. Atlas FusionIn the literature, it is popular to select a third-party standard atlas and then conduct group-level statistical analysis. For example, MNI-152 is one of the most widely accepted atlases (Mazziotta et al., 1995). The fusion of the MNI-152 atlas clearly demonstrates two important steps when fusing the brain atlas: (1) 152 individual subjects are registered with a certain template; (2) all registered images are averaged to produce the desired atlas. The MNI-152 atlas was later adopted by International Consortium for Brain Mapping (ICBM) and became part of Statistical Parametric Mapping (SPM), in which it provides automatic parcellation of neural regions-of-interest (ROI) (Tzourio-Mazoyer et al., 2002). The parcellation facilitates numerous studies upon brain morphology and structural/functional connectivity.Though simple and convenient, it is not necessarily proper to select the atlas manually for the atlas-based analysis. The anatomical variation across human brains is typically high, implying that a single and external atlas cannot fully account for individual subjects (Toga and Thompson, 2001). The (pairwise) registration between the subject(s) and the atlas can also introduce systemic bias into subsequent statistical analysis. For example, it is known that the Alzheimer’s Disease (AD) can cause brain tissue atrophy. When examining the impact of AD upon brain morphology, a large-scale group of both patients and normal controls is usually recruited for scanning the anatomical structures. If the atlas was corresponding to normal control, then it would be relatively easier to register the normal control images with the atlas than to register the patient images. In this way, the overall quality of the registered patient data would become less reliable, resulting in the imbalanced signal-to-noise ratios of the patients and the normal controls.To attain increased signal-to-noise ratio and unbiased statistical analysis, the atlas is better to be fused from all subject images in the data-driven way. Groupwise registration has provided such an alternative solution for atlas fusion o",
            {
                "entities": [
                    [
                        248,
                        276,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "478ANDERSON et al., Issues in Research Data ManagementResearch Paper (cid:1)Issues in Biomedical Research Data Management and Analysis:Needs and BarriersNICHOLAS R. ANDERSON, MS, E. SALLY LEE, MS, J. SCOTT BROCKENBROUGH, PHD, MARK E. MINIE, PHD,SHERRILYNNE FULLER, PHD, JAMES BRINKLEY, MD, PHD, PETER TARCZY-HORNOCH, MDA b s t r a c t Objectives: A. Identify the current state of data management needs of academic biomedicalresearchers. B. Explore their anticipated data management and analysis needs. C. Identify barriers to addressingthose needs.Design: A multimodal needs analysis was conducted using a combination of an online survey and in-depth one-on-one semi-structured interviews. Subjects were recruited via an e-mail list representing a wide range ofacademic biomedical researchers in the Pacific Northwest.Measurements: The results from 286 survey respondents were used to provide triangulation of the qualitativeanalysis of data gathered from 15 semi-structured in-depth interviews.Results: Three major themes were identified: 1) there continues to be widespread use of basic general-purposeapplications for core data management; 2) there is broad perceived need for additional support in managing andanalyzing large datasets; and 3) the barriers to acquiring currently available tools are most commonly related tofinancial burdens on small labs and unmet expectations of institutional support.Conclusion: Themes identified in this study suggest that at least some common data management needs will bestbe served by improving access to basic level tools such that researchers can solve their own problems.Additionally, institutions and informaticians should focus on three components: 1) facilitate and encourage the useof modern data exchange models and standards, enabling researchers to leverage a common layer ofinteroperability and analysis; 2) improve the ability of researchers to maintain provenance of data and models asthey evolve over time though tools and the leveraging of standards; and 3) develop and support informationmanagement service cores that could assist in these previous components while providing researchers with uniquedata analysis and information design support within a spectrum of informatics capabilities.(cid:1) J Am Med Inform Assoc. 2007;14:478 – 488. DOI 10.1197/jamia.M2114.IntroductionRapid advances in analytical technology coupled with wide-spread access to large amounts of highly detailed, heter-ogeneous and often public biomedical research data havedramatically increased the difficulties faced by biomedicalAffiliations of the authors: Division of Biomedical and HealthInformatics, Department of Medical Education and BiomedicalInformatics (NRA, ESL, MEM, SF, JB, PT-H); Department of Biolog-ical Structure (JSB, JB); Health Sciences Libraries and InformationCenter (MEM, SF); Department of Health Services, School of PublicHealth and Community Medicine (SF); Department of Pediatrics(PT-H); Department of Computer Science and Engineering (JB,PT-H), University of Washington, Seattle, WA.The authors would like to thank and acknowledge National Libraryof Medicine Training grant (Biomedical Health Informatics trainingprogram) T15LM07442, the BioMediator grant R01-HG02288, BISTIplanning grant P20-LM007714, and the Human Brain Project grantDC02310 for providing the funding to support parts of this work.Correspondences and reprints: Nicholas Anderson, University ofWashington, Department of Medical Education and BiomedicalInformatics, Boxe-mail:357240,(cid:1)nicka@u.washington.edu(cid:2).Seattle, WA 98195-7420;Received for review: 3/29/2006; accepted for publication: 3/27/2007.investigators in acquiring, archiving, annotating, and ana-lyzing data.1 Recognition of this fact is reflected in a numberof large scale initiatives by the major U.S. funding institu-tions as well as a profusion of software tools designed forbiomedical research data management and analysis.1–5 Overthe past several years we have met with many academicbiomedical researchers to discuss solutions to their datahandling problems as part of our own data integrationefforts.6 –12 Through informal discussions, we have beenstruck by the frequency with which they stated that: a) adata handling problem had become a major barrier to theprogress of their research, b) available computational solu-tions were prohibitively expensive, c) available solutionswere too complex for their needs, and/or d) computationalsolutions to their problem did not exist at all. In addition, wehave noticed that the needs of investigators can be extremelydynamic, often changing on a weekly basis. From a biomed-ical informatics standpoint, these issues raise several funda-mental questions:• How are researchers coping with managing these quicklyevolving information management problems in practice?• What obstacles are faced by researchers seeking individ-ual solutions to data management and analysis needs?lDownoadedfromhttps://iacademc.oup.com/jli////amaartice144478788143byguest/on03Juy2023l    \fJournal of the American Medical Informatics Association Volume 14 Number 4July / August 2007479• To what extent can biomedical research data handlingneeds be generalized across more than one lab (or evenmore than one project within a lab)?• What core design issues must be addressed in designingand implementing informatics solutions to aid biomedi-cal researchers in their data management and analysis?To address these questions, we have embarked on a project toidentify the data management and analysis needs of academicbiomedical researchers at the University of Washington.BackgroundInformatics journals report a steady stream of freely avail-able analytic and archiving tools with the potential tostreamline data analysis and integration tasks.6,8,10,13,14 Yetbiomedical researchers continue to struggle with increasingvolume and complexity of their own datasets. Accurate andthorough needs analysis is widely recognized as one of theearliest and most crucial events in virtually all softwaredevelopment cycles. For example, needs analyses for avariety of applications are frequently reported in softwareengineering15–20 as well in the medical fields.21–26 However,few attempts to assess the needs of biomedical research existin print27 despite recent calls for increased evaluation-basedscience to support informatics research.28 –30 We feel thatevaluation-based assessment of data management and anal-ysis needs of biomedical research is a crucial informaticsresearch area.Through our examination of existing methodologies de-scribed in the literature,21,31–37 we have concluded thatmutually supportive data resulting from a mixed methodsapproach has the greatest potential to support a comprehen-sive biomedical research needs assessment. Our approach isto use broad web-based surveys followed by personalizedin-depth interviews. The surveys were targeted toward alarge population of biomedical researchers to provide broadoverviews of generalized needs; however, surveys are lim-ited in that they don’t allow the elicitation of informationthat was not understood or imagined by the authors of thesurvey but is important to the respondents. Therefore, inaddition to the quantitative survey data, we gathered highlydetailed and context-specific qualitative data from individ-ual interviews. The semi-structured interview data not onlyprovided detailed contextual information, but helped revealideas that can be transferred to other domains.17,18,21,38 – 40Combining qualitative and quantitative methods has al-ready been successfully used in the discovery of user issuesassociated with the implementation of clinical ElectronicMedical Records (EMR) systems.36,37,41In this paper, we present the results of the survey and theinterviews in a combined analysis framework that we hopeto use in future biomedical research needs assessments.Using this multi-modal method, we describe the needs ofbiomedical investigators affiliated with the University ofWashington. The UW is an internationally recognized re-search university that was recently ranked #17 in the worldby the Economist newspaper.42 UW research supported over7,400 full-time equivalent positions in fiscal year 2005.43 Asa result, we feel that this study, though limited to one univer-sity and its local collaborator research institutes, can be appli-cable to other academic biomedical research settings.MethodsWe focused our assessment on data management and analysisneeds, including: a) current strategies for management andanalysis of experimental data, and b) obstacles to data man-agement and data sharing. A survey of 286 faculty, researchstaff, and students yielded quantifiable and moderately de-tailed data about informatics software needs. Fifteen volun-teers from this population were the subjects of semi-structuredinterviews. We conducted qualitative analysis on the interviewdata that represented in-depth views of individual needs.Human SubjectsTo ensure the safety and anonymity of the participants, allaspects of this research including participants in both thesurvey and the subsequent interviews were approved by theHuman Subjects Committee of the University of WashingtonInternal Review Board (IRB).SurveyThe survey consisted of two separate sections that togethertotaled 36 questions (See Appendix A, available as an on-linedata supplement at www.jamia.org). Twenty-three of thesequestions addressed a variety of library and informationscience issues and built on previous UW work from Yarfitzet al. involving library-based bioinformatics services.31 Thissurvey is part of a process of continuous evaluation ofbioresearch needs from both the academic research andinstitutional support perspectives. Of the 13 questions focus-ing on biomedical research information management needs,four questions related to subject demographics, with theremainder focusing on high-level overviews of generalizedneeds across biomedical research disciplines. Here we reportprimarily on data from the needs-related ",
            {
                "entities": [
                    [
                        2304,
                        2323,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Neurocomputing 222 (2017) 204–216Contents lists available at ScienceDirectNeurocomputingjournal homepage: www.elsevier.com/locate/neucomIntegration of touch attention mechanisms to improve the robotic hapticexploration of surfacesRicardo Martinsa,b,⁎, João Filipe Ferreiraa, Miguel Castelo-Brancob, Jorge Diasa,ca ISR-UC, Institute of Systems and Robotics, Department of Electrical and Computer Engineering - University of Coimbra Polo II, 3030-290 Coimbra,Portugalb IBILI-UC, Institute for Biomedical Imaging and Life Sciences, Faculty of Medicine, University of Coimbra, Coimbra, Portugalc Robotics Institute, Khalifa University, Abu Dhabi, UAEcrossmarkA R T I C L E I N F OA B S T R A C TCommunicated by Grana ManuelKeywords:Touch attentionArtificial perceptionBayesian modellingPath planningHaptic explorationProbabilistic grid mapsThis text presents the integration of touch attention mechanisms to improve the efficiency of the action-perception loop, typically involved in active haptic exploration tasks of surfaces by robotic hands. Theprogressive inference of regions of the workspace that should be probed by the robotic system uses informationrelated with haptic saliency extracted from the perceived haptic stimulus map (exploitation) and a “curiosity”-inducing prioritisation based on the reconstruction's inherent uncertainty and inhibition-of-return mechanisms(exploration), modulated by top-down influences stemming from current task objectives, updated at eachexploration iteration. This work also extends the scope of the top-down modulation of information presented ina previous work, by integrating in the decision process the influence of shape cues of the current explorationpath. The Bayesian framework proposed in this work was tested in a simulation environment. A scenario madeof three different materials was explored autonomously by a robotic system. The experimental results show thatthe system was able to perform three different haptic discontinuity following tasks with a good structuralaccuracy, demonstrating the selectivity and generalization capability of the attention mechanisms. Theseexperiments confirmed the fundamental contribution of the haptic saliency cues to the success and accuracy ofthe execution of the tasks.1. IntroductionIn an attempt to capitalise on the same advantages that havinghands benefit human beings, researchers have recently put a lot ofeffort into the development of dexterous robotic hands, due to themechanical (high number of degrees-of-freedom) and sensory (tactile,force, torque, heat) capabilities that they provide. These devices allowrobotic platforms to perform precise manipulation of objects (reaching,grasping, transportation, in-hand reorientation) [1], as well as hapticexploration of surfaces using different patterns of movements (lateralmotion, press-and-release, static contact),thereby promoting theextraction and integration of different haptic properties (contours,texture, compliance, temperature) of the materials these surfaces arecomposed of [2].The contributions presented in this work are related with therobotic haptic exploration of surfaces, following three essential as-sumptions: (1) no other type of sensors are used besides haptics (i.e.exploration is “blind”); (2) exploration paths are not predefined; (3) thesurface geometry is unknown to the robot. The objectives of theexploration tasks concern haptic discontinuity/contour following.Haptic discontinuities are defined by the transition/border regionsbetween surfaces with different haptic properties. During hapticexploration, the interaction of the robotic platform with the probedsurface provides multiple simultaneous streams of data over itsgeometry and the properties of its composing materials relayed by anensemble of haptic sensors. This data is potentially uncertain due tosensor noise and the unknown nature of the surface.To tackle these challenges, we propose a Bayesian framework toimplement autonomous haptic exploration of surfaces that implementsan action-perception loop architecture. The Bayesian formalism pro-vides a principled way of implementing the integration of the multi-modal sensory data supplied by the haptics ensemble while properlydealing with their inherent uncertainty. The proposed action-percep-tion loop architecture integrates touch attention mechanisms (i.e.stimulus-driven processes modulated by task-relevant top-down influ-ences) to optimise the exploration strategy. This in turn promotes⁎ Corresponding author at: University of Coimbra Polo II, Department of Electrical and Computer Engineering, ISR-UC, Institute of Systems and Robotics, 3030-290 Coimbra,Portugal.E-mail addresses: rmartins@isr.uc.pt (R. Martins), jfilipe@isr.uc.pt (J.F. Ferreira), mcbranco@fmed.uc.pt (M. Castelo-Branco), jorge@isr.uc.pt (J. Dias).http://dx.doi.org/10.1016/j.neucom.2016.10.027Received 21 February 2016; Received in revised form 11 July 2016; Accepted 17 October 2016Available online 26 October 20160925-2312/ © 2016 Elsevier B.V. All rights reserved.\fR. Martins et al.Neurocomputing 222 (2017) 204–216adaptive behaviour due to the exploration and exploitation traits ofsuch mechanisms.The haptic exploration of surfaces plays a fundamental role inreduced visibility scenarios (i.e.: underwater robotic manipulation,smoky and foggy disaster environments, partial or complete occlusionof elements in the scenario). Although this work only addresses theimplementation of haptic exploration strategies, the proposed Bayesianframework allows the integration of additional sensory sources such asvision (depth, color) and laser to infer the robotic exploration path. Theapproach proposed in this work can be used to complement methodsalready available to explore surfaces using exclusively non-hapticsensory inputs [4–6].The structure of the manuscript and an overview of the Bayesianmodels proposed in this work are presented in Section 1.1.1.1. Problem formulation and approach overviewIn the application scenarios used in this work, the exploration taskis performed on top of a table – a workspace defined by a planarsurface – and using a generic robotic system with manipulationcapability. The internal structure and configuration of the workspaceis unknown a priori to the robotic system. The solution to the hapticexploration task is described in two-dimensional Cartesian space,progressively unfolding a sequence of regions of the workspace to beprobed by the robotic platform during task execution.As in previous reported work, the 2D-Cartesian space is partitionedusing a planar isometric 2D grid (square cells), as represented in Fig. 1b). Each cell vk has a side of length ε and is described by a 2D Cartesianlocation (x,y) expressed in the inertial world referential {}. Thesetesselations of space have been used extensively in robotics as inferencegrids in many applications [9].The methods presented follow the principles and architecture of thehuman somatosensory processing pipeline and human cognition. Aconceptual overview of our solution is presented in Fig. 2; thecorresponding detailed diagram is given in Fig. 3, including a repre-sentation of data flow. Haptic sensory inputs are acquired during thelocal interaction of the robotic exploratory elements with the environ-ment at region vk. Haptic features such as texture, compliance,temperature are extracted from the haptic sensory inputs. Thesefeatures are integrated and used to discriminate the different classesof materials in the workspace. These processes are modelled by theBayesian model πper presented in Section 3.Next in the sensory processing pipeline, the robotic system uses theupdated perceptual representation of the workspace to infer the nextregion that should be explored. The mechanisms involved in thisprocess are implemented by the Bayesian model πtar and described inSection 5. Touch attention is modelled by integrating the following:(cid:129) Stimulus-driven processes – concurrent mechanisms that pro-Fig. 2. Conceptual representation of the action-perception loop [7] involved in thehaptic exploration of surfaces [8]. In this work, the objectives of the task andcorresponding solution is represented in two levels: symbolic and mid-level.mote both exploitation behaviour concerning perceptual represen-tations of stimuli in the form of haptic saliency and shape cues(determined by the Bayesian model πobj, Section 4), and explora-tion behaviours fuelled by spatial distribution of perceptual uncer-tainty and also inhibition-of-return mechanisms.(cid:129) Goal-directed modulation – mechanisms that influence theweights of stimulus-driven processes through top-down influencesinformed by current task objectives.The experimental setup used in this work is described in Section 6.The impact of the integration of the touch attention mechanisms in theaction-perception loop and generalization capability of the explorationstrategies inferred from the proposed Bayesian models are tested insimulation environment, Section 6. The main conclusions of this workand formulation of the main guidelines for future developments of thisapproach are presented in Section 7.1.2. Path planning of the global haptic exploration strategyThe framework conceptually represented in Fig. 2 and detailed inFig. 3 implements a haptic exploration path planning method, whichinfers a series of global via-points in the workspace that should beFig. 1. a) Results from a previous work [3], demonstrating a haptic discontinuity following task: straight line geometry. In this work, the haptic exploration tasks are more challenging:three materials and discontinuities with other geometries than straight lines. b) Illustration of a 2D isometric grid partitioning a real world workspace area. Each cell v has a dimension εand is described by position (x,y) expressed in {}.205\fR. Martins et al.Neurocomputing 222 (2017) 204–216Fig. 3. Detailed diagram of the architecture of the proposed system. The main contributions of this work are ident",
            {
                "entities": [
                    [
                        4837,
                        4865,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fContents lists available at ScienceDirect Children and Youth Services Review journal homepage: www.elsevier.com/locate/childyouth The child abuse reporting guideline compliance in Korean newspapers Serim Lee , Jieun Lee , JongSerl Chun * Department of Social Welfare, Ewha Womans University, Republic of Korea  A R T I C L E I N F O  A B S T R A C T  Keywords: Child abuse Reporting guidelines Newspapers South Korea The rate of child abuse has sharply increased worldwide, especially during the COVID-19 pandemic. As the media’s role in addressing child abuse cases is crucial, several international and formal organizations have established child abuse reporting guidelines. This study investigated how closely journalists follow reporting guidelines in addressing child abuse cases. Five major Korean presses and 189 articles from January 1, 2018, to January 31, 2021, were selected using the keyword “child abuse.” Each article was analyzed using a guideline framework consisting of 13 items regarding the five principles of the Korean Ministry of Health and Welfare and Central Child Protection Agency reporting guidelines. This study identified a radical growth in media reporting on child abuse cases in South Korea; almost 60% of the articles analyzed came from 2020 and 2021. More than 80% of the articles analyzed did not provide abuse resources, and 70% did not provide factual information. 57.1% of the articles instigated negative stereotypes, and about 30% explicitly mentioned certain family types in the headlines. Nearly 20% of the articles provided excessive details about the method used. Approximately 16% exposed victims’ identities. Some articles (7.9%) also described victims as sharing responsibility for the abuse. This study indicates that the media reports of child abuse in South Korea did not follow the guidelines in many facets. The present study discusses the limitations of the current guidelines and suggests future directions for the news media in reporting on child abuse cases nationwide.  1. Introduction According to the Korean Ministry of Health and Welfare (2020), the number of child abuse cases in South Korea has been increasing annu-ally. Especially during the COVID-19 pandemic, the risk of child abuse has increased owing to the heightened stress and social isolation of children and their families (Rosenthal & Thompson, 2020). The number of child abuse cases has increased sharply in South Korea since the beginning of the pandemic. According to the Korean National Policy Agency (2020), the number of child abuse reports in families between February and March 2020 was 1558, up 13.8 % from 2019. Additionally, according to the Korean Ministry of Health and Welfare’s (2022) anal-ysis of child abuse cases in Korea the total number of reported child abuse cases tallied in 2021 was 53,932, a significant increase of about 27.6 % compared to the previous year. Of the 37,605 cases judged as child abuse, the age range of 13–15 years accounted for the largest portion of victims with 8693 cases (23.1 %), followed by those aged 10–12 years with 8657 cases (23.0 %), and 7–9 years old with 7219 cases (19.2 %) (Korean Ministry of Health and Welfare, 2022). In terms of family types of child abuse victims, 23,838 cases (63.4 %) occurred in families with biological parents, 4618 cases (12.3 %) in mother-and-child families, 3707 cases (9.9 %) in father-and- child families, and 1980 cases (5.3 %) in remarried families (Korean Ministry of Health and Welfare, 2022). Regarding the relationship be-tween assailants and victims, parents accounted for the highest number of cases, with 31,486 cases (83.7 %), followed by 3609 cases (9.6 %) involving surrogate caregivers, and 1517 cases (4.0 %) involving rela-tives (Korean Ministry of Health and Welfare, 2022). Among the confirmed reports of child abuse, 45.1% were reported by fathers (16,944 cases), 35.6% by mothers (13,380 cases), and 3.2% by childcare teachers (1221 cases) (Korean Ministry of Health and Welfare, 2022). Regarding types of child abuse, 16,026 cases (42.6 %) involved multiple types of abuse (Korean Ministry of Health and Welfare, 2022) This was followed by 12,351 cases of emotional abuse (32.8 %), 5780 Abbreviations: WHO, World Health Organization; UNICEF, United Nations Children’s Fund; CDC, Centers for Disease Control and Prevention; NAPAC, National Association for People Abused in Childhood; PRISMA, Preferred Reporting Items for Systematic Reviews and Meta-Analyses; CCTV, Closed-circuit television. * Corresponding author at: 52, Ewhayeodae-gil, Seodaemun-gu, Seoul 03760, Republic of Korea. E-mail address: jschun@ewha.ac.kr (J. Chun). https://doi.org/10.1016/j.childyouth.2023.107037 Received 19 September 2021; Received in revised form 29 September 2022; Accepted 25 May 2023  ChildrenandYouthServicesReview151(2023)107037Availableonline30May20230190-7409/©2023ElsevierLtd.Allrightsreserved.\fS. Lee et al.                                                                                                                    cases of physical abuse (15.4 %), 2793 cases of neglect (7.4 %), and 655 cases of sexual abuse (1.7 %) (Korean Ministry of Health and Welfare, 2022). Among the cases involving multiple forms of abuse, the highest prevalence was observed for physical abuse and emotional abuse, ac-counting for 13,538 cases (36.0 %). Additionally, emotional abuse and neglect comprised 1011 cases (2.7 %); physical abuse, emotional abuse, and neglect comprised 798 cases (2.1 %); and 16 cases (0.0 %) included all types of abuse (Korean Ministry of Health and Welfare, 2022). Thus, the issue of child abuse is severe, and it is a global concern. Worldwide, child abuse reporting has almost doubled from 8 % to 17 % since the school closures due to COVID-19 (Save the Children, 2020). Therefore, child abuse cases have received significant media attention. Naturally, the media’s role in addressing child abuse cases is growing. However, the more media reports there are about child abuse cases, the more likely they are to trigger copycat crimes (Jung & Lee, 2017). Moreover, these reports often undermine the human rights of victims (Lee & Jung, 2016), an issue observed in relation to suicide (Kim et al., 2015). According to Kim et al. (2015), newspapers have a sig-nificant influence than television in inducing copycat suicides because of their easy accessibility and reproduction through various media outlets (Stack, 2002). Therefore, newspapers reporting on child abuse must exercise greater caution. However, some studies have suggested that media coverage of child abuse increase the public awareness on the issue (Saint-Jacques et al., 2012). Moreover, the increase in reporting helps individuals recognize the importance of reporting child abuse (Saint- Jacques et al., 2012). While there is a press rule requiring respect for the human rights of suspects for the benefit of the public interest and directing the media to only report necessary facts, reporters often violate this rule when a press competition begins (Lee & Jung, 2016). Media coverage of a particular crime does not necessarily reflect its true reality as it tends to distort, artificially construct, and simulate reality when reporting social issues such as crime (Payne et al., 2008). Lee and Kim (2008) presented the characteristics of and problems with crime reporting in the Korean media, including crime-reporting trends, disclosure-oriented reporting trends, sensational trends, and violations of human rights in crime reporting. Additionally, they noted that crime reports in the Korean media tend to overemphasize the investigation stage because the police and prosecution are the primary sources of coverage (Lee & Kim, 2008). That is, problems with media coverage of child abuse cases include emphasizing the brutality or deviance of criminals or perpetrators, a lack of in-depth coverage of the causes of child abuse, and provocative and sensational reporting trends (Lee & Jung, 2016). Additionally, a significant issue arises when reporting incidents involving children, as the media often fails to respect the human rights of children and lack of comprehensive awareness of these rights (Lee & Jung, 2016). Several researchers have argued that special efforts are necessary to protect human rights in crime reports, especially when they involve minors and children (Hove et al., 2013; Mejia et al., 2012; Niner et al., 2013). Crime reports can have a tremendous emotional and cognitive impact on children, and exposure to violent crime reports increases their fear of crime (Yoo et al., 2016). Further, Lee and Jung (2016) reported that children exposed to violent crime news could overestimate the possibility of crime and suffer psychological trauma as if crime scenes are persistent. This finding shows that crime reports cause significant mental and emotional damage to children. However, the number of studies on journalism and its accuracy in reporting child abuse cases is limited. For example, a study comparing newspaper reports of child abuse and neglect and data from real cases in England and Wales demonstrated that sexual abuse cases received the most media coverage, despite neglec",
            {
                "entities": [
                    [
                        5457,
                        5489,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Information Processing and Management journal homepage: www.elsevier.com/locate/infoproman A little bird told me your gender: Gender inferences in social media E. Fosch-Villaronga a,*, A. Poulsen b, R.A. Søraa c, B.H.M. Custers a a eLaw Center for Law and Digital Technologies, Leiden University, the Netherlands b School of Computing and Mathematics, Charles Sturt University, Australia c Researcher at Department of Interdisciplinary Studies of Culture and Department Neuromedicine and Movement Science, Norwegian University of Science and Technology (NTNU), Norway  A R T I C L E I N F O  A B S T R A C T  Keywords: Gender Twitter Social media Inference Gender classifier Automated gender recognition system Privacy Algorithmic bias Discrimination LGBTQAI+Gender stereotyping Online Behavioral Advertising Online and social media platforms employ automated recognition methods to presume user preferences, sensitive attributes such as race, gender, sexual orientation, and opinions. These opaque methods can predict behaviors for marketing purposes and influence behavior for profit, serving attention economics but also reinforcing existing biases such as gender stereotyping. Although two international human rights treaties include explicit obligations relating to harmful and wrongful stereotyping, these stereotypes persist online and offline. By identifying how inferential analytics may reinforce gender stereotyping and affect marginalized communities, opportunities for addressing these concerns and thereby increasing privacy, diversity, and in-clusion online can be explored. This is important because misgendering reinforces gender ste-reotypes, accentuates gender binarism, undermines privacy and autonomy, and may cause feelings of rejection, impacting people’s self-esteem, confidence, and authenticity. In turn, this may increase social stigmatization. This study brings into view concerns of discrimination and exacerbation of existing biases that online platforms continue to replicate and that literature starts to highlight. The implications of misgendering on Twitter are investigated to illustrate the impact of algorithmic bias on inadvertent privacy violations and reinforcement of social preju-dices of gender through a multidisciplinary perspective, including legal, computer science, and critical feminist media-studies viewpoints. An online pilot survey was conducted to better un-derstand how accurate Twitter’s gender inferences of its users’ gender identities are. This served as a basis for exploring the implications of this social media practice.  1. Introduction Online and social media platform providers use attributes of their users, including their name, age, and gender, to improve user experience and online behavioral advertising (OBA). For instance, the social media platform Twitter infers gender from a wide variety of sources.1 By processing user attributes, companies can target or exclude certain groups more easily, tailor their services to users, and increase their attention levels (Ur, Leon, Cranor, Shay & Wang, 2012). In this way, profiling makes marketing more precise and * Corresponding author. E-mail addresses: e.fosch.villaronga@law.leidenuniv.nl (E. Fosch-Villaronga), apoulsen@csu.edu.au (A. Poulsen), roger.soraa@ntnu.no (R.A. Søraa), b.h.m.custers@law.leidenuniv.nl (B.H.M. Custers).  1 See https://help.twitter.com/en/rules-and-policies/data-processing-legal-bases. https://doi.org/10.1016/j.ipm.2021.102541 Received 29 October 2020; Received in revised form 7 January 2021; Accepted 2 February 2021  InformationProcessingandManagement58(2021)102541Availableonline18February20210306-4573/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fE. Fosch-Villaronga et al.                                                                                               effective. However, a growing concern is the increasing use of transparent inferential analytics that reveal sensitive user traits that serve attention economics,2 (Davenport & Beck, 2001) and that may reinforce existing biases that, although not explicit, can be very influential in exacerbating discrimination (Caliskan, Bryson & Narayanan, 2017; Custers, 2018). One bias is gender stereotyping, which ’refers to the practice of ascribing to an individual woman or man specific attributes, characteristics, or roles by reason only of her or his membership in the social group of women or men’ (OHCHR, 2020). However, gender stereotyping is a complex process that, although based on strong beliefs of what gender is and should be, is understood too simplistically (Kachel, Steffens & Niedlich, 2016). For instance, Sink, Mastro and Dragojevic (2018): 592) investigated how television character perceptions often judged effeminate gay men negatively. They found out that ’straight-acting’ or hyper-masculine gay men are evaluated more favorably for conforming to and even mastering heteronormative gender roles. Also, \"gay men who are perceived to be more feminine would map onto traditional female stereotypes (i.e., warm but less competent)\" (ibid). Two international human rights treaties include explicit obligations relating to harmful and wrongful stereotyping (mainly Art. 5 of the Convention on the Elimination of All Forms of Discrimination against Women and Art. 8(1)(b) of the Convention on the Rights of Persons with Disabilities).3 Although States are usually the recipients of human rights treaties, the United Nations Human Rights Council has shown growing attention to the responsibility that corporations, sectors, and industries worldwide have for respecting human rights (OHCHR, 2012). Still, these stereotypes persist online and offline (Grant, Grey & van Hell, 2020; Hentschel, Heilman & Peus, 2019), as if platforms failed to understand—or deliberately choose to ignore—that gender is not merely being a ’man’ or a ’woman,’ but a social construct (Butler, 1990). In the words of De Beauvoir (1949), \"one is not born, but rather becomes a woman\". It is now becoming clear that these practices continue to exist online, for instance, on social media platforms. As described below, Twitter often assigns you to be a ‘woman’ if you’re a gay man. This socio-technical construction of gender shows the intricate gendered way of how humans shape their own identities through the technology they use and interact with (Søraa, 2017). In this contribution, the impact of inferential analytics on inadvertent privacy violations and the reinforcement of social prejudices of gender and sexuality is investigated by means of a specific case study (i.e., Twitter) through a multidisciplinary perspective, including legal, computer science, and queer media viewpoint. Therefore, the central research question of this contribution is: What are the implications of any inaccurate gender inferences by Twitter? This research question is only relevant if Twitter actually performs gender inferences and if these inferences are sometimes inaccurate. As is shown in a pilot survey presented in this contribution, there is evidence supporting this. In a broader perspective, addressing this research question illustrates concerns of discrimination, mis-gendering, and exacerbation of existing biases that online platforms persist in replicating and that literature has started to highlight very recently (Hamidi, Scheuerman & Branham, 2018; Keyes, 2018). The rationale behind the research question is, first, that gender is a co-shaped, changing part of human-identity tied into the socio- materiality of gendered relations often treated as a binary dichotomy. The assumption that gender is physiologically-rooted harms transgender people by essentializing the body as the source of gender and also harms non-binary people, who cannot be accurately classified (Keyes, 2018). The categories ‘female’ and ‘male’ do not reflect who they are (Fergus, 2020). Second, that platform providers no longer have to learn sensitive details about a particular user or to correctly group users into categories for advertising to be effective, as advertising has a high tolerance for classification errors (Wachter, 2020). Nevertheless, not considering gender and sexuality in social media platforms can be socially harmful and expensive, as the unconscious bias of technology usage and implementation may lead to further exacerbation of existing biases, for gender and race if only (Bray, 2007; Hao, 2019a; Schiebinger, 2014). In this article, we start with providing some background information on the mechanics of inferential analytics to elucidate how companies infer specific user attributes, including gender, and how these techniques may harm users’ rights in Section 2. Also, we introduce the technical rationale of gender classifiers. In Section 3, we explain how we conducted this study, which is based on configuring an online pilot survey geared towards understanding how accurate Twitters’ gender inferences of its users’ gender identities are and, with the support of survey data, explore what implications this social media practice has. Our results, presented in Section 4, already anticipate the binary understanding of gender from Twitter, which excludes those not fitting the category ‘male’ and ‘female,’ that inferring gender is part of the Twitter’s personalization trade-off, and that in nearly 20% of the cases it misgendered its users. After presenting the data, we discuss the social implications of gender inference on social media platforms in Section 5, including the lack of diversity in social media platforms and the role designers play in accounting for inclusivity and diversity. We also argue that platforms use a form of scapegoating to get away with the inference of sensitive user traits without user awareness. Consequences for data protection and discrimination are also discussed. We conclude the article in Section 6 by su",
            {
                "entities": [
                    [
                        3492,
                        3517,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Delft University of TechnologyA healthy debateExploring the views of medical doctors on the ethics of artificial intelligenceMartinho, Andreia; Kroesen, Maarten; Chorus, CasparDOI10.1016/j.artmed.2021.102190Publication date2021Document VersionFinal published versionPublished inArtificial Intelligence in MedicineCitation (APA)Martinho, A., Kroesen, M., & Chorus, C. (2021). A healthy debate: Exploring the views of medical doctors onthe ethics of artificial intelligence. Artificial Intelligence in Medicine, 121, [102190].https://doi.org/10.1016/j.artmed.2021.102190Important noteTo cite this publication, please use the final published version (if applicable).Please check the document version above.CopyrightOther than for strictly personal use, it is not permitted to download, forward or distribute the text or part of it, without the consentof the author(s) and/or copyright holder(s), unless the work is under an open content license such as Creative Commons.Takedown policyPlease contact us and provide details if you believe this document breaches copyrights.We will remove access to the work immediately and investigate your claim.This work is downloaded from Delft University of Technology.For technical reasons the number of authors shown on this cover page is limited to a maximum of 10. \fContents lists available at ScienceDirect Artificial Intelligence In Medicine journal homepage: www.elsevier.com/locate/artmed A healthy debate: Exploring the views of medical doctors on the ethics of artificial intelligence Andreia Martinho *, Maarten Kroesen, Caspar Chorus Delft University of Technology, Delft, the Netherlands  A R T I C L E I N F O  A B S T R A C T  Keywords: Artificial intelligence Healthcare Medicine Ethics Q-methodology Artificial Intelligence (AI) is moving towards the health space. It is generally acknowledged that, while there is great promise in the implementation of AI technologies in healthcare, it also raises important ethical issues. In this study we surveyed medical doctors based in The Netherlands, Portugal, and the U.S. from a diverse mix of medical specializations about the ethics surrounding Health AI. Four main perspectives have emerged from the data representing different views about this matter. The first perspective (AI is a helpful tool: Let physicians do what they were trained for) highlights the efficiency associated with automation, which will allow doctors to have the time to focus on expanding their medical knowledge and skills. The second perspective (Rules & Regulations are crucial: Private companies only think about money) shows strong distrust in private tech companies and emphasizes the need for regulatory oversight. The third perspective (Ethics is enough: Private companies can be trusted) puts more trust in private tech companies and maintains that ethics is sufficient to ground these corporations. And finally the fourth perspective (Explainable AI tools: Learning is necessary and inevitable) emphasizes the importance of explainability of AI tools in order to ensure that doctors are engaged in the technological progress. Each perspective provides valuable and often contrasting insights about ethical issues that should be operationalized and accounted for in the design and development of AI Health.  1. Introduction Artificial Intelligence (AI) is moving towards the health space. Given the abundance of data generated by health systems as a result of digi-tization efforts made over the last decade, a new data-driven approach to implement AI in healthcare has emerged. In contrast with previous and somewhat failed rule-based approaches to implement AI in healthcare [1,2], this new approach relies heavily on algorithms that detect pat-terns in data from clinical practice (e.g. medical imaging and electronic health records), clinical trials, genomics studies, and insurance, phar-maceutical, and pharmacy benefits management operations [3]. There is an expectation that these state-of-the-art-data-driven AI methods and algorithms will be able to use such data to address the complex problems of health systems [4,3]. The implementation of AI in healthcare holds great promise for expanding the medical knowledge and providing optimal yet cost- effective healthcare solutions [5,6]. In the clinical domain, expected results include identification of individuals at high risk for a disease, improved diagnosis and matching of effective personalized treatment, and out-of-hospital monitoring of therapy response [4,7]. Despite the projected benefits associated with Health AI, it also raises important ethical issues [8,9]. It is well known that AI has the potential to threaten values such as Autonomy, Privacy, and Safety [10], which are core values in Medicine [11,12]. Therefore, in order for AI to promote quality of care and minimize potentially disruptive effects [13], its deployment must take ethics into account. An important step towards ethical deployment of disruptive AI technologies is to learn the views of practitioners about such technologies. This information allows a better operationalization of the ethical issues associated with AI in a particular domain, which eventually is expected to lead to more meaningful debates and robust policies. The current academic literature provides interesting and valuable information on the perspectives of practitioners about the impact of AI technologies in the medical profession [14,15,16,17]. Most of these studies are particularly suited to medical fields with a strong image processing component, which is adequate for automated analysis, such as radiology [18,19,20,21,22,23,24], pathology [25], and dermatology * Corresponding author. E-mail address: a.m.martinho@tudelft.nl (A. Martinho). https://doi.org/10.1016/j.artmed.2021.102190 Received 12 February 2021; Received in revised form 22 September 2021; Accepted 29 September 2021  ArtificialIntelligenceInMedicine121(2021)102190Availableonline12October20210933-3657/©2021DelftUniversityofTechnology.PublishedbyElsevierB.V.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fA. Martinho et al.                                                                                                                [26,27]. However, there is little knowledge on the views of medical doctors about the ethical issues associated with the implementation of AI in healthcare. The aim of this study is to gain insight into the reasoning patterns and moral opinions about Health AI from those involved in the medical practice. By surveying medical doctors in The Netherlands, Portugal, and U.S. on the ethical issues associated with the implementation of AI in healthcare, we expect to enrich existing literature on the impact of AI technologies in medicine and provide valuable knowledge for the operationalization of Health AI Ethics. We first provide a brief commentary about the ethics of AI in healthcare. Subsequently we explain the methods used in this research by outlining the basic steps of q-methodology and explaining how we established these steps in this study. Later we present the results of the study by describing the four different perspectives that have emerged from the data. These results are further analyzed and discussed. Finally we draw conclusions and present directions for further research. 2. The ethics of health AI The empirical work about AI in healthcare that has been reported in the literature focuses mainly on issues directly related to the medical practice and career, such as Future of Employment, Education about AI, and Accountability. It has been reported that medical students and practitioners under-stand the increasing importance of AI in healthcare and have positive attitudes towards the clinical use of AI [20,26,17], but mainly as a supportive system for diagnosis [18,19,26,27,25,24]. Despite the positive attitudes towards AI, it has also been reported that students and medical doctors are poorly trained on these technol-ogies [20,28,29,30]. One study indicated that, although a small cohort of UK medical students who received AI teaching felt more confident in working with AI in the future compared to students that did not receive teaching, a significant number of taught students still felt inadequately prepared [20]. In order to take full advantage of these technologies, scholars seem to agree that medical school training on AI should be expanded and improved [18,20,21,26,25]. Regarding the impact of AI on career choice and reputation, it was reported that AI has an impact in the career intentions of students with respect to radiology [20], but radiologists would still choose this spe-cialty if given that choice [21]. These specialists have, however, revealed concerns that AI might diminish their professional reputation [24]. Contrary to the perceptions of the general public that AI will completely or partially replace human doctors [31], medical students and doctors in general are not concerned about job replacement [18,26,17,32,24]. Another important issue related to medical practice and career is liability. In a study in which pathologists were surveyed, it was reported that, with respect to medico-legal responsibility for diagnostic errors made by a human/AI combination, opinions were split between those who believed that the platform vendor and pathologist should be held equally liable, and others who believed responsibility remains primarily that of the human, with only a minority reporting that the platform vendor should primarily be liable [25]. Clearly, the ethics surrounding implementation of AI in healthcare goes beyond issues related to medical practice and career. Health AI gives rise to higher level ethical issues such as Autonomy, Fairness, or Privacy [33,10] but, with the exception of fairness, these issues have received less attention in the scientific literature. Fairness concerns related to racial and gender bias in AI-powered medical applications have t",
            {
                "entities": [
                    [
                        179,
                        207,
                        "DOI"
                    ],
                    [
                        540,
                        568,
                        "DOI"
                    ],
                    [
                        5764,
                        5792,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Vrije Universiteit BrusselVulnerable Data SubjectsMalgieri, GianclaudioPublished in:Computer Law & Security ReviewDOI:10.1016/j.clsr.2020.105415Publication date:2020License:CC BYDocument Version:Final published versionLink to publicationCitation for published version (APA):Malgieri, G. (2020). Vulnerable Data Subjects. Computer Law & Security Review, 37, 1-22. [105415].https://doi.org/10.1016/j.clsr.2020.105415CopyrightNo part of this publication may be reproduced or transmitted in any form, without the prior written permission of the author(s) or other rightsholders to whom publication rights have been transferred, unless permitted by a license attached to the publication (a Creative Commonslicense or other), or unless exceptions to copyright law apply.Take down policyIf you believe that this document infringes your copyright or other rights, please contact openaccess@vub.be, with details of the nature of theinfringement. We will investigate the claim and if justified, we will take the appropriate steps.Download date: 04. jul. 2023 \fcomputer law & security review 37 (2020) 105415 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR Vulnerable data subjects Gianclaudio Malgieri a , 1 , J ˛edrzej Niklas b , 1 , ∗a b Vrije Universiteit Brussel, Belgium Cardiff University, United Kingdom a r t i c l e i n f o a b s t r a c t Keywords: Data protection Vulnerability Vulnerable groups Discrimination AI Research ethics Discussion about vulnerable individuals and communities spread from research ethics to consumer law and human rights. According to many theoreticians and practitioners, the framework of vulnerability allows formulating an alternative language to articulate prob- lems of inequality, power imbalances and social injustice. Building on this conceptualisa- tion, we try to understand the role and potentiality of the notion of vulnerable data subjects. The starting point for this reflection is wide-ranging development, deployment and use of data-driven technologies that may pose substantial risks to human rights, the rule of law and social justice. Implementation of such technologies can lead to discrimination system- atic marginalisation of different communities and the exploitation of people in particularly sensitive life situations. Considering those problems, we recognise the special role of per- sonal data protection and call for its vulnerability-aware interpretation. This article makes three contributions. First, we examine how the notion of vulnerability is conceptualised and used in the philosophy, human rights and European law. We then confront those findings with the presence and interpretation of vulnerability in data protection law and discourse. Second, we identify two problematic dichotomies that emerge from the theoretical and prac- tical application of this concept in data protection. Those dichotomies reflect the tensions within the definition and manifestation of vulnerability. To overcome limitations that arose from those two dichotomies we support the idea of layered vulnerability, which seems com- patible with the GDPR and the risk-based approach. Finally, we outline how the notion of vulnerability can influence the interpretation of particular provisions in the GDPR. In this process, we focus on issues of consent, Data Protection Impact Assessment, the role of Data Protection Authorities, and the participation of data subjects in the decision making about data processing. © 2020 Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) ∗ Corresponding author: J ˛edrzej Niklas, Cardiff University, United Kingdom. 1 E-mail addresses: Gianclaudio.Malgieri@vub.be (G. Malgieri), niklasj@cardiff.ac.uk (J. Niklas). Both the authors contributed equally to each paragraph. The authors would also like to thank the anonymous reviewer whose sug- gestions have greatly improved this article. The open access version of this research was funded by the EU Commission, H2020 SWAFS Programme, PANELFIT Project, research grant number 788039. https://doi.org/10.1016/j.clsr.2020.105415 0267-3649/© 2020 Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) \f2 computer law & security review 37 (2020) 105415 1. Introduction For decades, experts in research ethics have assumed that some research participants and communities are more likely to be mistreated, abused, exploited or harmed.2 Such groups seem to possess a level of vulnerability, which generates cer- tain obligations and responsibilities for researchers and over- sight entities. The principle of special treatment of “vulner- able groups” was incorporated into various declarations and guidelines that regulate especially clinical research, like the Belmont Report or the Declaration of Helsinki.3 Those docu- ments predominantly focus on the issue of consent and in- formed participation, highlighting problems of autonomy and integrity. Nevertheless, some other interpretations add more elaborated understanding of vulnerability and raise issues of power imbalance and political and economic disadvantage.4 In other words, the language of vulnerability in research ethics allows greater sensitivity and responsiveness to equity, dis- crimination and different socio-historical contexts. However, the notion of vulnerability is also discussed in other fields. From human rights to political philosophy, the concept is seen as a framework that enables the articulation of broad issues that fill into the category of social justice and uncover human exposure to harms, pain and suffering.5 As it will be argued below, human vulnerability is also (to some extent) present in the discussions about data protection, privacy and data-driven technologies. Calo, a prominent voice in this debate, argues that the rationale for privacy protec- tion is precisely addressing vulnerability of individuals.6 Put it differently, privacy and data protection regimes are man- ifestations of the idea that all individuals are vulnerable to the power imbalances created by data-driven technologies. 2 3 4 5 Carol Levine et al., “The Limitations of ‘Vulnerability’ as a Pro- tection for Human Research Participants,” The American Journal of Bioethics 4, no. 3 (August 2004): 44–49, https://doi.org/10.1080/ 15265160490497083 . Phoebe Friesen et al., “Rethinking the Belmont Report?,” The American Journal of Bioethics 17, no. 7 (July 3, 2017): 15–21, https: //doi.org/10.1080/15265161.2017.1329482 . Dearbhail Bracken-Roche et al., “The Concept of ‘Vulnerability’ in Research Ethics: An in-Depth Analysis of Policies and Guide- lines,” Health Research Policy and Systems 15, no. 1 (December 2017): 8, https://doi.org/10.1186/s12961- 016- 0164- 6 . Lourdes Peroni and Alexandra Timmer, “Vulnerable Groups: The Promise of an Emerging Concept in European Human Rights Convention Law,” International Journal of Constitutional Law 11, no. 4 (October 1, 2013): 1056–85, https://doi.org/10.1093/icon/mot042 ; Rebecca Hewer, “A Gossamer Consensus: Discourses of Vulner- ability in the Westminster Prostitution Policy Subsystem,” Social & Legal Studies 28, no. 2 (April 2019): 227–49, https://doi.org/10. 1177/0964663918758513 ; Isabelle Bartkowiak-Théron and Nicole L. Asquith, “Conceptual Divides and Practice Synergies in Law En- forcement and Public Health: Some Lessons from Policing Vulner- ability in Australia,” Policing and Society 27, no. 3 (April 3, 2017): 276–88, https://doi.org/10.1080/10439463.2016.1216553 , Martha Albert- son Fineman, “The Vulnerable Subject: Anchoring Equality in the Human Condition,” Yale Journal of Law and Feminism 20 (2008): 23; Ju- dith Butler, Precarious Life: The Powers of Mourning and Violence (Lon- don ; New York: Verso, 2004). Ryan Calo, “Privacy, Vulnerability, and Affordance,” DePaul L. 6 Rev. 66 (2017): 592–593. detect children anxiety and depression 9 Additionally, different scholars explain how data-driven tech- nologies can lead to discrimination, social marginalisation or affect human autonomy and dignity and exploit particular communities.7 Such controversial cases in the data-driven re- search concern automated systems that identify sexual orien- tation,8 or predict and prevent suicide.10 Finally, the notion of vulnerability appears in the discussion about ethics and regulation of Artificial In- telligence. Here some of the guidelines and ethical policies call for the governance frameworks that recognise the situation of vulnerable groups such as women, persons with disabilities, ethnic minorities, children, and consumers.11 It seems to us that the issue of human vulnerability should be an important topic in the data protection debate, consid- ering the new risks of individual exploitation in the algorith- mic environment. Involving vulnerability as a “heuristic tool”could emphasise existing inequalities between different data subjects and specify in a more systematic and consolidated way that the exercise of data rights is conditioned by many factors such as health, age, gender or social status. However, the scholarly discussion about vulnerable data subjects is still largely underdeveloped. Accordingly, in this article, we try to understand and conceptualise how the notion of vulnerable individuals finds its way in the data protection debate. More precisely, when human vulnerability can influence the way we are interpreting data protection regimes. We are aware that it is not possible to address this com- plex topic in one article satisfactorily. Our modest goal here is to initiate a discussion about this topic and its problem- atic aspects, suggesting some first interpretative paths, while calling for further analysis and research. To do this, we first investigate the meaning of “vulnerable individuals”, look- ing in particular at the theoretical discussion about vulner- ability ( Section 2 ). Taking ",
            {
                "entities": [
                    [
                        118,
                        144,
                        "DOI"
                    ],
                    [
                        388,
                        414,
                        "DOI"
                    ],
                    [
                        4142,
                        4168,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect International Journal of Information Management journal homepage: www.elsevier.com/locate/ijinfomgt Research Article Emotional reactions to robot colleagues in a role-playing experiment Nina Savela a,*, Atte Oksanen a, Max Pellert b, c,d, David Garcia b,c, d a Faculty of Social Sciences, Tampere University b Institute of Interactive Systems and Data Science, Department of Computer Science and Biomedical Engineering, Graz University of Technology c Complexity Science Hub Vienna d Center for Medical Statistics, Informatics and Intelligent Systems, Medical University of Vienna  A R T I C L E I N F O  A B S T R A C T  Keywords: Robot Work Sentiment Role-play Experiment We investigated how people react emotionally to working with robots in three scenario-based role-playing survey experiments collected in 2019 and 2020 from the United States (Study 1: N = 1003; Study 2: N = 969, Study 3: N = 1059). Participants were randomly assigned to groups and asked to write a short post about a scenario in which we manipulated the number of robot teammates or the size of the social group (work team vs. organi-zation). Emotional content of the corpora was measured using six sentiment analysis tools, and socio- demographic and other factors were assessed through survey questions and LIWC lexicons and further analyzed in Study 4. The results showed that people are less enthusiastic about working with robots than with humans. Our findings suggest these more negative reactions stem from feelings of oddity in an unusual situation and the lack of social interaction.  1. Introduction People have been using automation and working with robots in in-dustry fields such as manufacturing for many years. Researchers suggest that the exceptional situation caused by COVID-19 and social distancing guidelines will further increase the use of advanced information sys-tems, such as robots, at work (Coombs, 2020; He, Zhang, & Li, 2021). Due to the development of more interactive, collaborative, and social robots, people are more likely to be in situations in which they must work and interact with robots as coworkers or teammates (Dwivedi et al., 2021; Haidegger et al., 2013; M¨ortl et al., 2012). As a result, new-generation robots will create new social and psychological chal-lenges that could impact work life profoundly. There is a sufficient body of evidence confirming that social psy-chological processes such as attitudes and trust are essential factors in successful collaboration with robots and ultimately accepting them in everyday life (Hancock et al., 2011; Schaefer, Straub, Chen, Putney, & Evans, 2017; Sheridan, 2016; Yusif, Soar, & Hafeez-Baig, 2016). In addition to these extensively researched factors, robotization is likely to arouse both positive and negative emotional reactions in human workers. Introducing advanced technology such as social robots as co-workers in the same organization or work team presents human workers with a new situation. Adapting to this could be more challenging to some workers than others, causing negative attitudes and emotions that could have an unwanted effect on emotional well-being. In addition to examining acceptance of robots through attitudes and trust, researchers have investigated emotional attachment to companion robots (Friedman, Kahn, & Hagman, 2003); emotional reactions to ill-treatment of robots (Rosenthal-von der Pütten, Kr¨amer, Hoffmann, Sobieraj, & Eimler, 2013); and the connection between negative emo-tions, such as anxiety, and negative attitudes (Nomura, Kanda, & Suzuki, 2006). Even though working closely with robots has been argued to arouse negative attitudinal and emotional reactions in human workers (Groom & Nass, 2007), we do not currently know how people would respond emotionally to working with robots on the same work team or in the workplace community with robots. In addition to explicit methods of measuring attitudes and emotions, such as surveys, emotional and attitudinal reactions toward robot co-workers can be investigated through more implicit means such as examining textual data collected from role-playing scenarios. Computer- aided analysis methods have generated the massive new field of affec-tive computing, which offers fast and quantitative means of analyzing large amounts of text with the help of emotional lexicons (Piryani, Madhavi, & Singh, 2017). Our study was designed to fill the research gap through analysis of textual data collected from three role-playing experiments that involved * Corresponding author at: Faculty of Social Sciences, 33014 Tampere University, Tampere, Finland. E-mail address: nina.savela@tuni.fi (N. Savela). https://doi.org/10.1016/j.ijinfomgt.2021.102361 Received 5 August 2020; Received in revised form 6 May 2021; Accepted 7 May 2021  InternationalJournalofInformationManagement60(2021)102361Availableonline23May20210268-4012/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).Konstanzer Online-Publikations-System (KOPS) URL: http://nbn-resolving.de/urn:nbn:de:bsz:352-2-1cnionttcewjv7\fN. Savela et al.                                                                                                                 introduction of robots as work team members or as coworkers within a workplace. We focused on emotional reactions to the hypothetical sit-uations, as identified via sentiment analysis, in three studies and further investigated the associated factors in a fourth study. Computational social scientific analysis methods combined with an experimental design and online role-playing data collection method generated a unique multi-methodological approach that has not previously been utilized to investigate the acceptance of robots. 2. Literature review The concept of emotion has a long and complex history in philosophy and psychology, and it has traditionally been used as a metaconcept that combines different words describing feelings and attitudes (Dixon, 2012). One empirical study considered emotion as an intense mental state with hedonic content (Cabanac, 2002). There is no consensus on the definition, process, or hierarchical levels of emotion among multiple emotion theories, but most support some form of connection between emotion and cognitive appraisal (Barnard & Teasdale, 1991; Moors, 2009). Theories of attitudes often include both cognitive and emotional perspectives, and this is specifically manifested in a multicomponent model of attitude (Zanna & Rempel, 2008). In the context of technology, researchers have investigated possible connections between cognitive and emotional constructs in the framework of the technology acceptance model (TAM) and its extensions (Kulviwat, Bruner, Kumar, Nasco, & Clark, 2007; Lee, Xiong, & Hu, 2012; Saad´e & Kira, 2006; Venkatesh, 2000). For example, in a model called consumer acceptance of tech-nology, affective and cognitive attitude dimensions explain the behav-ioral attitude toward adoption, which then predicts adoption intention (Kulviwat et al., 2007). According to a literature review about the his-tory of TAM (Maranguni´c & Grani´c, 2015), further integration of emo-tions into TAM is still needed. In research focused on the advanced technology of robots specif-ically, attitudes and emotions have often overlapped, especially in research measuring and focusing on negative emotions, such as anxiety, and negative attitudes (Nomura et al., 2006). TAM and its extensions have also been used in research on human–robot interaction and user studies, but some researchers have stressed caution when applying it to interactive technology such as robots (Young, Hawkins, Sharlin, & Igarashi, 2009). For this reason and because this research area is an emerging field, the tools used to measure different social and psycho-logical constructs have varied. Because emotion is linked to attitudes and behavior (Gursoy, Chi, Lu, & Nunkoo, 2019; Kulviwat et al., 2007), and because the cognitive measures of attitude have their weaknesses (Peters & Slovic, 2007), investigating emotional responses in acceptance of emerging technologies such as robots is an important research avenue. Evidence that humans can feel empathy and get emotionally attached to artificial beings confirms that artificial entities such as ro-bots can arouse emotional reactions (Kr¨amer, Eimler, von der Pütten, & Payr, 2011; Rosenthal-von der Pütten et al., 2013). Other researchers suggested that even imagined contact with a robot can affect emotions toward robots (Wullenkord, Fraune, Eyssel, & ˇSabanovi´c, 2016). The examination of emotions toward robots is essential because they affect social processes such as identification and play an important role in human behavior (DeSteno, Dasgupta, Bartlett, & Cajdric, 2004; DeS-teno, Petty, Rucker, Wegener, & Braverman, 2004). This has conse-quences for the intended use and possible benefits gained from larger utilization of robots in work life. Emotional detection literature offers different ways to examine emotions from facial expressions, speech, and writing (Cowie & Cor-nelius, 2003; Russell, Bachorowski, & Fern´andez-Dols, 2003). For example, females and older people are more likely to express positivity in writing (Pennebaker & Stone, 2003; Thelwall, Wilkinson, & Uppal, 2010), neurotic people are likely to use negative language, and extraverted and agreeable people are more likely to use positive words (Yarkoni, 2010). However, different associations could emerge in the context of robots. The more traditional research literature on robot acceptance gives some information about the expected associations and factors to consider when studying emotional expressions in written re-actions toward robots. Some literature has suggested a difference in attitudes toward robots based on age and gender, with young individuals and males being more willing to accept robots (Flandorfer, 2012). H",
            {
                "entities": [
                    [
                        4716,
                        4747,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Futures journal homepage: www.elsevier.com/locate/futures Moral circle expansion: A promising strategy to impact the far future Jacy Reese Anthis a,b,*, Eze Paez c,d a The University of Chicago, USA b Sentience Institute, USA c Pompeu Fabra University, Spain d Centre de Recherche en ´Ethique, Universit´e de Montr´eal, Canada  A R T I C L E I N F O  A B S T R A C T  Keywords: Existential risk Effective altruism Artificial intelligence Animal ethics Longtermism 1. Introduction Many sentient beings suffer serious harms due to a lack of moral consideration. Importantly, such harms could also occur to a potentially astronomical number of morally considerable future beings. This paper argues that, to prevent such existential risks, we should prioritise the strategy of expanding humanity’s moral circle to include, ideally, all sentient beings. We present empirical evidence that, at micro- and macro-levels of society, increased concern for members of some outlying groups facilitates concern for others. We argue that the perspective of moral circle expansion can reveal and clarify important issues in futures studies, particularly regarding animal ethics and artificial intelligence. While the case for moral circle expansion does not hinge on specific moral criteria, we focus on sentience as the most recommendable policy when deciding, as we do, under moral uncertainty. We also address various nuances of adjusting the moral circle, such as the risk of over-expansion.  There are currently around 8 billion humans (109) on Earth. There are over 100 billion domestic animals (1011)—primarily chickens and fishes used in the food industry. There may be trillions of wild birds and mammals (1012) and over a quintillion tiny wild animals such as insects (1018) (Tomasik, 2009). These moral stakes are difficult to conceptualise, yet they pale in comparison to the potential long-term scope of human civilisation in the distant future, such as with interstellar expansion.1 Some moral philosophers and futures scholars are beginning to consider our impact on the far future as a serious and perhaps overwhelmingly important consideration that we should account for in present decisions (see, for example, Beard, Rowe, & James, 2020; Bostrom, 2014; Liu, Lauta, & Maas, 2018; Kareiva & Carranza, 2018; Moynihan, 2020; Parfit, 1992). Such work is part of a broader project of prescriptive futures studies, asking questions such as how to encourage thoughtful contemplation of the future, how to design future-oriented public policy, and how to account for the indirect effects of our actions on the future (Ahvenharju, * Corresponding author. E-mail addresses: jacy@uchicago.edu (J.R. Anthis), joseezequiel.paez@upf.edu (E. Paez).  1 For example, suppose humanity expanded to the Virgo Supercluster. Then the human population could be one hundred undecillion (1038). See Bostrom (2003). Notice, however, that the same interstellar resources could fuel many more minds of less complexity, and therefore less energy cost, than the human mind. Thus, assuming minds less complex than the human mind can be sentient, the total number of possible future sentient in-dividuals is proportionally higher. https://doi.org/10.1016/j.futures.2021.102756 Received 22 March 2021; Received in revised form 23 April 2021; Accepted 26 April 2021  Futures130(2021)102756Availableonline30April20210016-3287/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fJ.R. Anthis and E. Paez                                                                                               Minkkinen, & Lalot, 2018; Dorsser, Walker, Taneja, & Marchau, 2018; Ng, 2020). Restricting our concern to present or near-future generations of human beings may be a significant normative blindspot, neglecting the majority of individuals who will, at any time, exist. While work on ‘existential risk’ has focused on extinction risk in particular, recent work has highlighted the importance of ‘suffering risks’, which are ‘astronomical in scope and hellish in severity’ (Kovic, 2021). Human history is riddled with atrocities committed against contemporary members of our own species, and if species-based discrimination is wrong, then history is also riddled with serious harms committed against nonhuman animals. Thus, the risk that we commit atrocities on an interstellar scale is a serious danger—perhaps one of the greatest dangers that exist on the horizon of our species. In this article we present a novel approach to reducing such existential risks and bettering the future. We build on an interdisci-plinary literature including animal ethics, environmental ethics, ethical theory, history, and social psychology. We defend two claims. The first is that to minimise the danger of existential risks, particularly suffering risks, we should prioritise a strategy that we call: Moral circle expansion: a community’s moral circle has expanded with respect to some previous time t if, and only if, a number of entities which used to be given less than full moral consideration at t are now given more moral consideration. Clearly laying out this definition allows us to describe the following familiar phenomenon as a case of moral circle expansion: a community of moral agents concludes that a feature on which they previously based the moral inconsiderability of some individuals is not morally relevant. Thus, they extend moral consideration to all members of the set of entities that possess that feature. This definition is completely agnostic about what attributes—sentience, rationality, being alive, etc.—a community uses to construct its moral circle. In addition, it is not a moralised definition. Thus, according to our definition, in order to identify some shift in a com-munity’s sphere of concern as an instance of moral circle expansion, it is not necessary to judge whether they are deploying a more justified criteria of moral considerability. This has several advantages. First, we do not need to endorse the same criteria in order to agree that a community’s moral circle has expanded. Second, our definition allows us to identify instances of moral circle expansion in a society and retain the ability to criticise them (i.e., one can believe a specific instance of moral circle expansion was a mistake). Note, finally, that this is a definition of, so to speak, gross expansions of the moral circle. It could be modified to accommodate calculations of net expansion. This would consist of something like the number of new entities included with respect to t minus the number of entities excluded with respect to t, given an appropriate specification of ‘minus’ that clarifies the subtractivity of ‘sets’.2 We will argue that explicitly discussing and analysing moral circle expansion is a revealing yet underexplored perspective for understanding the nature of moral progress. While much scholarly work can fit under the umbrella of moral inclusion and exclusion (e. g., prejudice, discrimination), anything more than a passing reference to the moral circle is rare, and we argue there is much to be gained by direct exploration of moral circle expansion itself. Our second claim is that humanity’s moral circle ought to be expanded to include, ideally, all sentient beings. That is, all beings with a capacity for positive and negative experiences.3 In this paper, we will not assume that sentience is necessary for moral con-siderability. However, looking back on historical atrocities, it seems that many could have been prevented or mitigated if communities had extended their moral concern to other groups of sentient beings, such as humans of various races, ethnicities, sexualities, genders, or nationalities—or animals of various species who share this planet with humankind (Judge & Wilson, 2015; Wright, 2018). Thus, even if one is unsure what exactly the future of the moral circle should look like (e.g., the moral patienthood of artificial intelligence), pushing on the current frontiers of the moral circle (e.g., farmed animals) or otherwise engendering expansion towards other kinds of sentient beings is a compelling moral priority not only for utilitarians and others concerned with doing the most good but, more generally, for those concerned with preventing serious wrongs. Artificial intelligence researcher Paul Christiano (2013) argues that, for those trying to do the most good, ‘changing long-term social values’ should not be a priority for several reasons, particularly the changing nature of moral values upon reflection and the potentially zero-sum nature of competing moral values. In addition, there are many other pressing issues demanding our limited attention, such as the reduction of extinction risk (Bostrom, 2003), the mitigation of the effects of anthropogenic climate change, and the alleviation of global poverty.4 Thus, the claim that moral circle expansion should be prioritised, among all of those compelling projects, is in need of some defence. 2 As we mentioned, a society’s expansion of its moral circle usually occurs in terms of sets, rather than individuals. That is, increased moral consideration is usually given to members of the set of individuals who possess a particular attribute. This is consistent with prominent accounts of discrimination in terms of the exclusion of societal groups (see, for example, Lippert-Rasmussen, 2013). We decided to define gross moral circle expansion in terms of individuals, rather than in terms of set, because of the difficulties the latter generates. First, for a given situation, there may be various equally acceptable criteria for carving up sets and there seems to be no value-neutral way to decide which to choose. Employing different criteria, the same societal shift may be described either as an expansion or a contraction. Second, even if there were reasons to prefer some set constructi",
            {
                "entities": [
                    [
                        3259,
                        3288,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fContents lists available at ScienceDirect Computers in Human Behavior journal homepage: http://www.elsevier.com/locate/comphumbeh Full length article Machine learning techniques and older adults processing of online information and misinformation: A covid 19 study Jyoti Choudrie a,*, Snehasish Banerjee b, Ketan Kotecha c, Rahee Walambe c, Hema Karende c, Juhi Ameta c a University of Hertfordshire, Hertfordshire Business School, DeHavilland Campus, Hatfield. Herts, AL109EU, UK b York Management School, University of York, Freboys Lane, YO10 5GD, UK c Symbiosis Centre for Applied Artificial Intelligence (SCAAI), Symbiosis Institute of Technology, Pune. Symbiosis (Deemed University), Pune, Maharashtra, 412115, India  A R T I C L E I N F O  A B S T R A C T  Keywords: AI Machine learning techniques COVID-19 pandemic Older adult Interview Information-misinformation This study is informed by two research gaps. One, Artificial Intelligence’s (AI’s) Machine Learning (ML) tech-niques have the potential to help separate information and misinformation, but this capability has yet to be empirically verified in the context of COVID-19. Two, while older adults can be particularly susceptible to the virus as well as its online infodemic, their information processing behaviour amid the pandemic has not been understood. Therefore, this study explores and understands how ML techniques (Study 1), and humans, particularly older adults (Study 2), process the online infodemic regarding COVID-19 prevention and cure. Study 1 employed ML techniques to classify information and misinformation. They achieved a classification accuracy of 86.7% with the Decision Tree classifier, and 86.67% with the Convolutional Neural Network model. Study 2 then investigated older adults’ information processing behaviour during the COVID-19 infodemic period using some of the posts from Study 1. Twenty older adults were interviewed. They were found to be more willing to trust traditional media rather than new media. They were often left confused about the veracity of online content related to COVID-19 prevention and cure. Overall, the paper breaks new ground by highlighting how humans’ information processing differs from how algorithms operate. It offers fresh insights into how during a pandemic, older adults—a vulnerable demographic segment—interact with online information and misinformation. On the methodological front, the paper represents an intersection of two very disparate paradigms—ML techniques and interview data analyzed using thematic analysis and concepts drawn from grounded theory to enrich the scholarly understanding of human interaction with cutting-edge technologies.  1. Introduction When the internet was introduced to daily life, it was meant to offer immensely diverse knowledge and information (Ratchford et al., 2001). The internet has however also led to a growth of ignorance in various forms and guises that are labelled using terms such as fake news, disinformation and misinformation. This study specifically uses the term ‘misinformation’. Access to the internet is now, often, access to resources that reinforce biases, ignorance, prejudgments, and absurdity. Parallel to a right to information, some researchers believe that there is a right to ignorance (Froehlich, 2017). Meanwhile, a pandemic, COVID-19, has exposed several difficulties with the present global health care system. A societal concern for healthcare organizations and the World Health Organization (WHO) has been the spread of online misinformation that can exacerbate the impact of the pandemic (Ali, 2020). Almost 90% of Internet users seek online health information as one of the first tasks after experiencing a health concern (Chua & Banerjee, 2017). Therefore, regarding the pandemic COVID-19, where there is little a priori information and knowledge, individuals are likely to explore the online avenue. However, when searching for such information on the internet and social media, one is faced with an avalanche of information referred to as an ‘infodemic’, which includes a mixture of facts and hoaxes that are difficult to separate from one another (WHO, 2020a). If a hoax related to COVID-19 prevention and cure is mistaken as a fact, there could be serious ramifications on people’s health and well-being. Conceivably, * Corresponding author. E-mail address: j.choudrie@herts.ac.uk (J. Choudrie). https://doi.org/10.1016/j.chb.2021.106716 Received 31 July 2020; Received in revised form 10 January 2021; Accepted 22 January 2021  ComputersinHumanBehavior119(2021)106716Availableonline30January20210747-5632/©2021ElsevierLtd.Allrightsreserved.\fJ. Choudrie et al.                                                                                                                healthcare organizations and public health authorities are keen to ensure that people are not deceived by COVID-19-related misinforma-tion that has been circulating online. This is reflected in their propensity to submit misinformation-exposing posts on their social media channels (Raamkumar et al., 2020). Social media, also known as online social networks (OSN), have now emerged as contemporary ways to reach the consumer market. Artificial Intelligence (AI)—traditionally referring to an artificial creation of human-like intelligence that can learn, reason, plan, perceive, or process natural language (Russell & Norvig, 2009)—is associated with social media. It is “an area of computer science that aims to decipher data from the natural world often using cognitive technologies designed to un-derstand and complete tasks that humans have taken on in the past” (Ball, 2018, para. 4). The adoption of AI, a cutting-edge technology, has been propelled to an unprecedented level in the wake of the pandemic. With regards to health, AI-enabled mobile applications are now widely used for infection detection and contact-tracing (Fong et al., 2020). Even with regards to the infodemic, AI’s ML techniques can play a crucial role. Research has shown that ML techniques can help separate information from misinformation (Katsaros et al., 2019; Kinsora et al., 2017; Shu et al., 2017; Tacchini et al., 2017). However, despite the hype and enthusiasm around AI and social media, there is still a lack of under-standing in terms of how consumers interact and engage with these technologies (Ameen et al., 2020; Capatina et al., 2020; Rai, 2020; Wesche & Sonderegger, 2019). The extent to which algorithms can help detect misinformation amid information related to COVID-19 is there-fore worth investigating. Older adults constitute a consumer demographic group that is particularly susceptible to COVID-19 (WHO, 2020b). The pandemic causes pneumonia and symptoms such as fever, cough and shortness of breath among older adults (Adler, 2020), who usually exert maximal pressure on healthcare systems (WHO, 2020b). Moreover, ceteris par-ibus, older adults can also be susceptible to the ‘infodemic’. They are less confident than younger individuals in tackling the challenges that the online setting has to offer (Xie et al., 2021). Hence, older adults are more willing to trust the traditional media rather than what AI feeds them through social media (Media Insight Project, 2018). Still, they often end up becoming a victim of online misinformation (Guess et al., 2019; Seo et al., 2021). To protect this segment of the population from misinformation about COVID-19 prevention and cure, health care organizations would require a systematic understanding of not only the ‘infodemic’, but also of how older adults respond to it. Both are issues on which the literature has shed little light. To fill this gap, the aim of this study is: To explore and understand how AI’s ML techniques (Study 1) and older adults (Study 2) process the infodemic regarding COVID-19 prevention and cure. With this overarching research aim, the objective of this study is two- fold. First, it investigates the extent to which algorithms can distinguish between information and misinformation related to COVID-19 preven-tion and cure (Study 1). For this purpose, a supervised ML framework was developed to classify facts and hoaxes. Second, the study investigates older adults’ information processing behaviour in the face of the COVID-19 infodemic (Study 2). Informed by the results of Study 1 along with the theoretical lenses of misinforma-tion, information processing and trust, 20 older adults were interviewed to understand how they had been coping with the infodemic associated with COVID-19 prevention and cure in their daily lives. This study is important and timely for several reasons. First, The World Health Organization (WHO) declared that besides finding pre-ventions and cures for the pandemic, it was also concerned about the online infodemic. By addressing the infodemic problem from both the computational and behavioral perspectives, the study represents a timely endeavour in the aftermath of the COVID-19 outbreak. Second, the study introduces a machine learning framework to classify infor-mation and misinformation related to COVID-19 prevention and cure. As will be shown later, the classification performance was generally promising. Third",
            {
                "entities": [
                    [
                        5201,
                        5226,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Available online at www.sciencedirect.com Available online at www.sciencedirect.com ScienceDirect ScienceDirect Procedia Computer Science 00 (2022) 000–000 Procedia Computer Science 00 (2022) 000–000 Procedia Computer Science 211 (2022) 36–46www.elsevier.com/locate/procedia www.elsevier.com/locate/procedia 15th International Conference on Current Research Information Systems 15th International Conference on Current Research Information Systems Research Information Systems and Ethics relating to Open Science Research Information Systems and Ethics relating to Open Science Joachim Schöpfela*, Otmane Azeroualb, Pablo de Castroc Joachim Schöpfela*, Otmane Azeroualb, Pablo de Castroc aGERiiCO-Labor, University of Lille, 59650 Villeneuve-d’Ascq, France aGERiiCO-Labor, University of Lille, 59650 Villeneuve-d’Ascq, France bGerman Centre for Higher Education Research and Science Studies (DZHW), Schützenstraße 6a, 10117 Berlin, Germany bGerman Centre for Higher Education Research and Science Studies (DZHW), Schützenstraße 6a, 10117 Berlin, Germany cUniversity of Strathclyde, 101 St James Road, Glasgow G4 0NS, United Kingdom cUniversity of Strathclyde, 101 St James Road, Glasgow G4 0NS, United Kingdom Abstract Abstract Current research information systems (CRIS) evaluate research performance and are intended to contribute to the continuous Current research information systems (CRIS) evaluate research performance and are intended to contribute to the continuous improvement of research. Based on former research on the ethical dimensions of CRIS, our paper presents the results of a survey improvement of research. Based on former research on the ethical dimensions of CRIS, our paper presents the results of a survey with a small sample of representatives of ethics committees from different European countries on ethical aspects of CRIS. Ethics with a small sample of representatives of ethics committees from different European countries on ethical aspects of CRIS. Ethics committees and experts are rarely associated with CRIS-related projects. However, their opinion on ethical indicators and the committees and experts are rarely associated with CRIS-related projects. However, their opinion on ethical indicators and the implementation and use of a CRIS is undoubtedly essential for the future development and management of these systems. Against implementation and use of a CRIS is undoubtedly essential for the future development and management of these systems. Against this background, our purpose is to provide a deeper understanding of the ethical aspects in the field of research information this background, our purpose is to provide a deeper understanding of the ethical aspects in the field of research information management, to show how CRIS represent ethical dimensions of scientific research and to suggest some adjustment of their management, to show how CRIS represent ethical dimensions of scientific research and to suggest some adjustment of their development, implementation and use. development, implementation and use. © 2022 The Authors. Published by Elsevier B.V.© 2022 The Authors. Published by ELSEVIER B.V. © 2022 The Authors. Published by ELSEVIER B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information SystemsInformation Systems Information Systems Keywords: Current research information systems; CRIS; research information management; research ethics; research integrity; research Keywords: Current research information systems; CRIS; research information management; research ethics; research integrity; research infrastructures; ethics committees; infraethics. infrastructures; ethics committees; infraethics. 1. The challenge of ethics 1. The challenge of ethics Research ethics, as a field of applied ethics, provides concepts and recommendations of “right” and “wrong” scientific Research ethics, as a field of applied ethics, provides concepts and recommendations of “right” and “wrong” scientific practice, especially norms of conduct that distinguish between acceptable (responsible) and unacceptable scientific practice, especially norms of conduct that distinguish between acceptable (responsible) and unacceptable scientific behavior. Many different research organizations and associations have adopted specific codes, rules and policies behavior. Many different research organizations and associations have adopted specific codes, rules and policies relating to research ethics, such as the Nuremberg Code of 1947, the 2010 Singapore Statement on Research Integrity relating to research ethics, such as the Nuremberg Code of 1947, the 2010 Singapore Statement on Research Integrity * Corresponding author. E-mail address: joachim.schopfel@univ-lille.fr * Corresponding author. E-mail address: joachim.schopfel@univ-lille.fr 1877-0509 © 2022 The Authors. Published by ELSEVIER B.V. 1877-0509 © 2022 The Authors. Published by ELSEVIER B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information Systems Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information Systems 1877-0509 © 2022 The Authors. Published by Elsevier B.V.This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information Systems10.1016/j.procs.2022.10.17410.1016/j.procs.2022.10.1741877-05092 Joachim Schöpfel et al. / Procedia Computer Science 00 (2022) 000–000 (Resnik & Shamoo, 2011), the European Code of Conduct for Research Integrity (2017) or the recent National Science Foundation’s Manual of “Conflicts of Interest and Standards of Ethical Conduct”. Due to the changing research environment “with new and complex technologies, increased pressure to publish, greater competition in grant applications, increased university-industry collaborative programs, and growth in international collaborations” but also to “highly publicized cases of misconduct” (Armond et al., 2021), academic interest in research ethics and research integrity is steadily increasing, above all in medical and health sciences. Main issues are falsification and fabrication of research data, informed consent, patient safety, plagiarism and conflict of interest. Research ethics “is a matter of debate” (Corvol, 2017). Even when researchers agree that research ethics is important, they do not agree on a common meaning but rather adopt divergent meanings that reflect their priorities, which stem from their personal needs, professional demands, or roles in society. Broadly speaking, research ethics can be defined as “doing good science in a good manner” according to shared and accepted standards of excellence and in compliance with all the steps necessary to meet the rules of responsible research conduct (appropriate data storage, conflict of interest management, protection of human and animal participants, laboratory safety etc.) (DuBois & Antes, 2018). Regarding new technologies and infrastructures, a growing body of research revealed essential and recurrent themes and dimensions of ethics, such as privacy, security, autonomy, justice, human dignity, control of technology and balance of power; some of these issues have been addressed through legal adjustments and new rules and obligations while “for other ethical issues (...) such as discrimination, autonomy, human dignity and unequal balance of power, the supervision is hardly organized” (Royakkers et al., 2018). In the field of big data and artificial intelligence, ethically-aligned technology has been defined as “that which is (a) beneficial to, and respectful of, people and the environment (beneficence); (b) robust and secure (non-maleficence); (c) respectful of human values (autonomy); (d) fair (justice); and (e) explainable, accountable and understandable (explicability)” (Morley et al., 2020). Regarding artificial intelligence, Morley et al. (2021) observed “that a significant gap exists between the theory of AI ethics principles and the practical design of AI systems”. Does the same observation apply to current research information systems (CRIS)? Do we need new tools and methods designed to help CRIS developers, engineers, and designers translate ethical principles into practice? In spite of the growing body of research on ethics in the field of technology, big data, artificial intelligence and so on, so far there are but few papers on ethics in the field of research information systems. For CRIS, ethics is a double challenge: to contribute to the development of responsible research in the context of open science, and to be able to measure the practices and performances recommended by these rules of responsible conduct. In other words, CRIS must respect the regulatory framework and the good practices, principles and values of scientific communities (Diener & Crandall, 1978; Guillemin & Gillam, 2004). But, at the same time, and this is indeed the particularity of these systems,",
            {
                "entities": [
                    [
                        6360,
                        6387,
                        "DOI"
                    ],
                    [
                        6387,
                        6414,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Journal of Strategic Information Systems 29 (2020) 101600Contents lists available at ScienceDirectJournal of Strategic Information Systemsjournal homepage: www.elsevier.com/locate/jsisThe strategic impacts of Intelligent Automation for knowledge andservice work: An interdisciplinary reviewCrispin Coombsa,⁎a School of Business and Economics, Loughborough University, Ashby Road, Loughborough LE11 3TU, UKb Business School, University of Aberdeen, Aberdeen AB24 5UA, UKc Faculty of Social Sciences, University of Nottingham, University Park, Nottingham NG7 2RD, UK, Donald Hislopb, Stanimira K. Tanevac, Sarah BarnardaTA R T I C L E I N F OA B S T R A C TKeywords:Artificial intelligenceAutomationBusiness valueComputerisationMachine learningMobile roboticsIntroductionA significant recent technological development concerns the automation of knowledge and ser-vice work as a result of advances in Artificial Intelligence (AI) and its sub-fields. We use the termIntelligent Automation to describe this phenomenon. This development presents organisationswith a new strategic opportunity to increase business value. However, academic research con-tributions that examine these developments are spread across a wide range of scholarly dis-ciplines resulting in a lack of consensus regarding key findings and implications. We conduct thefirst interdisciplinary literature review that systematically characterises the intellectual state anddevelopment of Intelligent Automation technologies in the knowledge and service sectors. Basedon this review, we provide three significant contributions. First, we conceptualise IntelligentAutomation and its associated technologies. Second, we provide a business value-based model ofIntelligent Automation for knowledge and service work and identify twelve research gaps thathinder a complete understanding of the business value realisation process. Third, we provide aresearch agenda to address these gaps.Analysts and commentators have forecast mass unemployment from the automation of a wide range of job roles that involvepredictable, repetitive work (Grace et al., 2018; Makridakis, 2017). The McKinsey Global Institute has claimed that 60% of jobs couldbecome 30% automated by the early 2020s (Chui et al., 2016), while Frey and Osborne (2017) argue that automation could eliminate47% of jobs in the United States economy by 2033. Researchers also predict that Artificial Intelligence (AI) will outperform humans inmany activities in the next ten years (Grace et al., 2018), thereby becoming a practical alternative to human labour (Makridakis,2017). These claims are based on a recent step change in the technological advance of AI. AI is the broad suite of technologies thatcan match or surpass human capabilities, particularly those involving cognition such as learning and problem solving (DeCanio,2016). Applications of AI are wide-ranging and include knowledge reasoning, machine learning, natural language processing,computer vision, and robotics. For clarity, we use the term AI to refer to all these technologies.Advances in AI and its sub-fields have enabled the development of a new form of automation that we describe as IntelligentAutomation1 (the application of AI in ways that can learn, adapt and improve over time to automate tasks that were formally⁎Corresponding author.E-mail addresses: c.r.coombs@lboro.ac.uk (C. Coombs), donald.hislop@abdn.ac.uk (D. Hislop),Stanimira.Taneva@nottingham.ac.uk (S.K. Taneva), s.h.barnard@lboro.ac.uk (S. Barnard).1 We use the term Intelligent Automation throughout this paper, rather than the abbreviation IA, to avoid confusion with AI.https://doi.org/10.1016/j.jsis.2020.101600Received 1 July 2017; Received in revised form 29 January 2020; Accepted 3 February 2020Available online 09 March 20200963-8687/ © 2020 The Authors. Published by Elsevier B.V.\fC. Coombs, et al.Journal of Strategic Information Systems 29 (2020) 101600undertaken by a human). Frey and Osborne (2017) observe that algorithms are being developed that would allow cognitive tasks tobe automated. They also state that the application of AI in mobile robotics has extended the opportunity for automation of manualtasks. Cognitive and manual tasks are commonly found in knowledge and service work (Davenport and Kirby, 2016a). Knowledgework is defined as work which is intellectual, creative, and non-routine, and which involves the utilisation and creation of knowledge(Hislop et al., 2018). Knowledge work includes work in a wide range of professional areas, such as information and communication,consulting, pharmacology, and education (Kuusisto and Meyer, 2003). Service work can be defined as the process of using one'sresources (e.g., knowledge) for someone's (self or other) benefit (Barrett et al., 2015). It includes jobs as diverse as working in retail,security, office cleaning, and more knowledge-intensive work such as consulting. Our definition of service work thus includes (white-collar) office and administrative work.Until recently, knowledge and service work tasks have been considered too difficult to automate because they require a highdegree of cognitive flexibility and physical adaptability (Lacity and Willcocks, 2016b). However, the scope and capability of AI hasrecently expanded and is likely to continue to grow (Brynjolfsson and McAfee, 2016). For example, applications of AI are predicted tosignificantly reduce the need for humans to translate languages (by 2024), drive a truck (by 2027), work in retail (by 2031), and workas surgeons (by 2053) (Grace et al., 2018). Frey and Osborne (2017) predict that most office and administrative support work, as wellas a substantial proportion of service work in the US, is likely to be automated. In which case, the advance of AI will create dramaticchanges to the supply of knowledge and service work (Loebbecke and Picot, 2015). It is this impact on knowledge and service workthat sets this change apart from previous technological revolutions, such as the industrialisation of factory work in the 19th century,or the adoption of transactional computers for administrative and service work in the late 20th century (Davenport and Kirby,2016b). This review focuses on knowledge and service work to examine the transformational effects of Intelligent Automation insectors that have previously been relatively untouched by automation compared to other industries, such as manufacturing(Brynjolfsson and McAfee, 2011).The transformation of knowledge and service work presents organisations with a new strategic opportunity to increase businessvalue. Recent advances in AI could enable organisations to create new business value opportunities through the application ofIntelligent Automation to middle-income cognitive jobs (Manyika et al., 2017). Alternatively, organisations may opt to substitute newAI capital for high-skilled labour or choose to reassign high-skilled workers to focus more exclusively on the most complex, non-routine cognitive tasks (Davenport and Kirby, 2016a). However, there is considerable disagreement regarding the possible impacts ofAI on knowledge and service work. Makridakis (2017) identified four contrasting perspectives: optimists that predict a utopian futureof AI (e.g., Kurzweil, 2005); pessimists that predict a dystopian future where AI reduces humans to a second rate status (e.g., Bostrom,2014); pragmatists that predict AI will augment human skills (e.g., Markoff, 2016); and doubters that predict that AI will never beable to replicate human intelligence (e.g., Jankel, 2015). This lack of consensus means that there is little coherent guidance regardingthe new strategies that need to be developed to realise business value from Intelligent Automation. Thus, there is a pressing need forresearch that examines the latest advances in AI and considers their impact on the application of Intelligent Automation for businessvalue.A valuable source of guidance for strategic perspectives on Intelligent Automation is current academic knowledge. Numerousstudies, many employing sound rigorous methods, consider the potential impacts of AI on work. However, these contributions aresituated in a wide range of scholarly disciplines that draw on contrasting research paradigms, theories, methods, and perspectives,resulting in a lack of consensus regarding critical findings and implications. Operating at the intersection of many scholarly dis-ciplines, considering both social and technical aspects, IS researchers are well placed to assemble a cohesive understanding of thisemerging research challenge. Thus, this paper aims to inform researchers of the current state (state of the art) of research relating tothe application of Intelligent Automation for knowledge and service work.To assist the IS research community in navigating this complex domain, this paper provides a scoping review of existing academicliterature (Paré et al., 2015). Scoping reviews focus on breadth rather than depth of coverage in the literature. They describe andsummarise the size and nature of the literature on a particular topic and allow researchers to identify research gaps in the extantliterature (see, for example, Smith et al., 2011). The advantage of this approach is to offer a comprehensive view of the researchlandscape. This review explores the potential impacts of Intelligent Automation through the classification of AI research related toknowledge and service work published between January 2011 and December 2017. We focused our review on knowledge and servicework for two reasons. First, the most significant developments associated with the work-related use of AI have been in occupationsthat have hitherto made little use of them, such as the knowledge and service industries (Brynjolfsson and McAfee, 2011; Loebbeckeand Picot, 2015). Second, the late 20th century and the start of this century has witnessed a significant growth of employment inknowledge and service work, and a decline in jobs in manufacturing sectors in",
            {
                "entities": [
                    [
                        3647,
                        3673,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect International Journal of Information Management journal homepage: www.elsevier.com/locate/ijinfomgt “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy☆ Yogesh K. Dwivedi a, b, *, Nir Kshetri c, Laurie Hughes a, Emma Louise Slade d, Anand Jeyaraj e, Arpan Kumar Kar f, g, Abdullah M. Baabdullah h, Alex Koohang i, Vishnupriya Raghavan j, Manju Ahuja k,1, Hanaa Albanna l, 1, Mousa Ahmad Albashrawi m,1, Adil S. Al-Busaidi n,o, 1, Janarthanan Balakrishnan p, 1, Yves Barlette q, 1, Sriparna Basu r, 1, Indranil Bose s, 1, Laurence Brooks t, 1, Dimitrios Buhalis u,1, Lemuria Carter v,1, Soumyadeb Chowdhury w, 1, Tom Crick x, 1, Scott W. Cunningham y, 1, Gareth H. Davies z, 1, Robert M. Davison aa, 1, Rahul D´e ab,1, Denis Dennehy a, Yanqing Duan ac,1, Rameshwar Dubey ad,ae,1, Rohita Dwivedi af,1, John S. Edwards ag,1, Carlos Flavi´an ah, 1, Robin Gauld ai, 1, Varun Grover aj, 1, Mei-Chih Hu ak,1, Marijn Janssen al, 1, Paul Jones am, 1, Iris Junglas an,1, Sangeeta Khorana ao,1, Sascha Kraus ap, 1, Kai R. Larsen aq, 1, Paul Latreille ar,1, Sven Laumer as,1, F. Tegwen Malik at,1, Abbas Mardani au,1, Marcello Mariani av,aw,1, Sunil Mithas ax,1, Emmanuel Mogaji ay, 1, Jeretta Horn Nord az,1, Siobhan O’Connor ba,1, Fevzi Okumus bb, bc,1, Margherita Pagani bd, 1, Neeraj Pandey be,1, Savvas Papagiannidis bf, 1, Ilias O. Pappas bg, bh,1, Nishith Pathak bi,1, Jan Pries-Heje bj, 1, Ramakrishnan Raman bk,1, Nripendra P. Rana bl, 1, Sven-Volker Rehm bm,1, Samuel Ribeiro-Navarrete bn, 1, Alexander Richter bo,1, Frantz Rowe bp,1, Suprateek Sarker bq, 1, Bernd Carsten Stahl br,1, Manoj Kumar Tiwari be, 1, Wil van der Aalst bs,1, Viswanath Venkatesh bt,1, Giampaolo Viglia bu,bv,1, Michael Wade bw,1, Paul Walton bx,1, Jochen Wirtz by,1, Ryan Wright bq, 1 a Digital Futures for Sustainable Business & Society Research Group, School of Management, Swansea University, Bay Campus, Fabian Bay, Swansea, Wales, UK b Department of Management, Symbiosis Institute of Business Management, Pune & Symbiosis International (Deemed University), Pune, Maharashtra, India c Bryan School of Business and Economics, University of North Carolina at Greensboro, USA d University of Bristol Business School, University of Bristol, BS8 1SD, UK e Professor of Information Systems, Raj Soin College of Business, Wright State University, 3640 Colonel Glenn Highway, Dayton, OH 45435, USA f School of Artificial Intelligence, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India g Department of Management Studies, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India h Department of Management Information Systems, Faculty of Economics and Administration, King Abdulaziz University, Jeddah, Saudi Arabia i School of Computing, Middle Georgia State University, Macon, GA, USA j Client Advisory and Transformation, Stackroute, NIIT Limited, India k Department of Information Systems, Analytics and Operations, College of Business, University of Louisville, USA l Northumbria University London, UK m IRC-FDE, KFUPM, Saudi Arabia, ISOM, KFUPM Business School, KFUPM, Saudi Arabia n Innovation and Technology Transfer Center, Sultan Qaboos University, Oman o Department of Business Communication, Sultan Qaboos University, Oman p Department of Management Studies, National Institute of Technology, Tiruchirappalli, India ☆This editorial opinion paper provides a subjective viewpoint on the potential impact of generative AI technologies such as ChatGPT in the domains of education, business, and society. Its objective is to offer initial guidance on the opportunities, challenges, and implications associated with these technologies. It is worth noting that, given its nature as an editorial opinion piece, this submission has not undergone a formal double-blind review process but has been reviewed informally by appropriate experts. * Corresponding author at: Digital Futures for Sustainable Business & Society Research Group, School of Management, Swansea University, Bay Campus, Fabian Bay, Swansea, Wales, UK. E-mail address: y.k.dwivedi@swansea.ac.uk (Y.K. Dwivedi). https://doi.org/10.1016/j.ijinfomgt.2023.102642 Received 1 March 2023; Accepted 1 March 2023  InternationalJournalofInformationManagement71(2023)102642Availableonline11March20230268-4012/©2023TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).\fY.K. Dwivedi et al.                                                                                                               q Montpellier Business School (MBS), Montpellier, France r FORE School of Management, New Delhi, India s Indian Institute of Management Ahmedabad, Vastrapur, Ahmedabad 380015, India t Information School, University of Sheffield, UK u Bournemouth University Business School, Poole, UK v School of Information Systems and Technology Management, University of New South Wales, Sydney, Australia w Information, Operations and Management Sciences Department, TBS Business School, 1 Place Alphonse Jourdain, 31068 Toulouse, France x Department of Education & Childhood Studies, Swansea University, Swansea, United Kingdom y Faculty of Humanities and Social Sciences, University of Strathclyde, Glasgow G1 1XQ, United Kingdom z School of Management, Swansea University, Swansea, UK aa Department of Information Systems, City University of Hong Kong, Hong Kong Special Administrative Region ab Indian Institute of Management Bangalore, India ac Business and Management Research Institute, University of Bedfordshire, UK ad Montpellier Business School, Montpellier, France ae Liverpool Business School, Liverpool John Moores University, UK af Prin. L. N. Welingkar Institute of Management Development and Research, Mumbai, India ag Operations & Information Management Department, Aston Business School, UK ah Department of Marketing and Marketing Management, Faculty of Economics and Business, University of Zaragoza, Zaragoza, Spain ai Otago Business School, Co-Director, Centre for Health Systems and Technology, University of Otago, Dunedin, New Zealand aj Distinguished Professor and George & Boyce Billingsley Endowed Chair of Information Systems, IS Doctoral Program, Walton College of Business, University of Arkansas, Room 216, Fayetteville, AR 72703, USA ak Institute of Technology Management, National Tsing Hua University, Hsinchu 300, Taiwan al Faculty of Technology, Policy and Management, Delft University of Technology, the Netherlands am School of Management, Swansea University, United Kingdom an College of Charleston, School of Business, USA ao Bournemouth University Business School, Bournemouth University, UK ap Free University of Bozen-Bolzano, Italy & University of Johannesburg, South Africa aq Leeds School of Business, Boulder, University of Colorado, Boulder, USA ar Sheffield University Management School, The University of Sheffield, UK as Sch¨oller-Endowed Chair of Information Systems, Institute of Information Systems Nürnberg, School of Business, Economics and Society, Friedrich-Alexander University Erlangen-Nuremberg, Germany at School of Management, Swansea University Bay Campus, Swansea, SA1 8EN Wales, UK au Business School, Worcester Polytechnic Institute, Worcester, MA 01609-2280, USA av Henley Business School, University of Reading, Henley-on-Thames, Oxfordshire, UK aw Department of Management, University of Bologna, Bologna, Italy ax School of Information Systems and Management, University of South Florida, Tampa, FL, USA ay Greenwich Business School, University of Greenwich, London SE10 9LS, UK az Management Science and Information Systems, Spears School of Business, Oklahoma State University, Stillwater, OK 74078, USA ba Division of Nursing, Midwifery and Social Work, School of Health Sciences, The University of Manchester, Manchester, United Kingdom bb Rosen College of Hospitality Management University of Central Florida 9907 Universal Boulevard, Orlando, FL 32819, USA bc Department of Business, WSB University, Wrocław, Poland bd SKEMA Research Center for Artificial Intelligence, SKEMA Business School, 5 quai Marcel Dassault – Suresnes, France be National Institute of Industrial Engineering (NITIE), Mumbai, India bf Newcastle University Business School, Newcastle upon Tyne, United Kingdom bg Department of Information Systems, University of Agder, Norway bh Department of Computer Science, Norwegian University of Science and Technology, Norway bi Microsoft AI MVP and Microsoft Regional DirectorGlobal lead - Innovation and Architecture at DXC Technologies India bj Department of People and Technology, Roskilde University, Denmark bk Symbiosis Institute of Business Management, Pune & Symbiosis International (Deemed University), Pune, India bl Department of Management and Marketing, College of Business and Economics, Qatar University, P.O. Box 2713, Doha, Qatar bm HuManiS Research Center – Humans and Management in Society, UR 7308, Universit´e de Strasbourg – EM Strasbourg Business School, France bn ESIC University, Madrid, Spain and University of Economics and Human Sciences, Warsaw, Poland bo Wellington School of Business and Government, Rutherford House, 23 Lambton Quay, Wellington, New Zealand bp Nantes University, LEMNA, and SKEMA Business School, France bq Rolls-Royce Commonwealth Commerce, McIntire School of Commerce, University of Virginia, USA br School of Computer Science, The University of Nottingham, UK bs Process and Data Science, RWTH Aachen University, Ahornstraße 55, Aachen 52074, North Rhine-Westphalia, Germany bt Eminent Scholar and Verizon Chair, Director of Executive PhD in Business, Pamplin College of Business, Virginia Tech, Blacksburg, Virginia, USA bu University of Portsmouth, Department of Strategy, Marketing and Innovation, Richmond Building, Portsmouth, United Kingdom bv Department of Eco",
            {
                "entities": [
                    [
                        4256,
                        4287,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fEuropean Journal of Operational Research 291 (2021) 906–917 Contents lists available at ScienceDirect European Journal of Operational Research journal homepage: www.elsevier.com/locate/ejor The responsibility of social media in times of societal and political manipulation Ulrike Reisach Department of Information Management, Prof. Dr. Ulrike Reisach, Neu-Ulm University of Applied Sciences, Wiley-Street 1, D-89231 Neu-Ulm, Germany a r t i c l e i n f o a b s t r a c t Article history: Received 17 December 2019 Accepted 16 September 2020 Available online 22 September 2020 Keywords: Ethics in OR Decision-making Artificial intelligence Behavioural OR Education The way electorates were influenced to vote for the Brexit referendum, and in presidential elections both in Brazil and the USA, has accelerated a debate about whether and how machine learning techniques can influence citizens’ decisions. The access to balanced information is endangered if digital political ma- nipulation can influence voters. The techniques of profiling and targeting on social media platforms can be used for advertising as well as for propaganda: Through tracking of a person’s online behaviour, al- gorithms of social media platforms can create profiles of users. These can be used for the provision of recommendations or pieces of information to specific target groups. As a result, propaganda and dis- information can influence the opinions and (election) decisions of voters much more powerfully than previously. In order to counter disinformation and societal polarization, the paper proposes a responsibility-based approach for social media platforms in diverse political contexts. Based on the implementation require- ments of the “Ethics Guidelines for Trustworthy Artificial Intelligence” of the European Commission, the eth- ical principles will be operationalized, as far as they are directly relevant for the safeguarding of demo- cratic societies. The resulting suggestions show how the social media platform providers can minimize risks for societies through responsible action in the fields of human rights, education and transparency of algorithmic decisions. © 2020 The Author. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) 1. Introduction and research methodology 1.1. Aims and research question During the Corona-crisis in 2020, the degree of disinformation has reached a level which could endanger the proper functioning of democratic decision-making. Crises have always been a time of rising emotions and anxiety. These seem to culminate on social media platforms, where citizens and self-proclaimed experts give unsubstantiated advice dealing with Covid-19, or try to identify as- sumingly guilty parties and fabricate conspiracy theories, through amalgamating facts and false interpretations. The more exciting, the even more weird ideas that are shared, including vaccine anx- iety, and doubtful, or even potentially lethal health recipes. In the United States, these developments have fuelled the long- standing debate on whether misrepresentations and recommen- dations still fall under the freedom of speech, or should be ac- companied, e.g. with a fact check advice, or should be filtered out and deleted. The European Commission (EC) aims at combat- ing disinformation and appeals to the social media platforms to install a transparent and consistent moderation of disinformation ( EC, 2020a , 2020b ). Bell’s observation ( 2018 ) that “… techniques for fabricating, editing, and reframing news in harmful ways develop faster than they can be detected and countered …” describes the current situ- ation (p. 5). C. West Churchman asks “which end results are good in an objective sense?” ( Churchman, 1970 ). This question is ap- plies to the current issues of social media platforms: The paper asks whether and how social media platforms can (practically) and should (ethically), deal with risks of societal and political manipu- lation. The EC’s Action Plan on Disinformation ( 2019 ) defines disinfor- mation as is verifiably false or misleading information created, pre- sented and disseminated for economic gain, or to intentionally de- ceive the public. The reasons given for their action are: (a) the potential for far-reaching consequences such as public harm, (b) threats to democratic political and policy-making processes, (c) the risk of endangering the protection of EU citizens’ health, E-mail address: ulrike.reisach@hnu.de security and their environment. https://doi.org/10.1016/j.ejor.2020.09.020 0377-2217/© 2020 The Author. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) \fU. Reisach European Journal of Operational Research 291 (2021) 906–917 This research focuses on the threats to democratic decision- making processes. Based on the developments in 2019 and 2020, the EC (2020a) expresses concerns which relate to the main rea- sons for the research: “Disinformation erodes trust in institutions and in digital and traditional media and harms our democracies by hampering the ability of citizens to take informed decisions. It can polarise debates, create or deepen tensions in society and un- dermine electoral systems, and have a wider impact on European security. It impairs freedom of opinion and expression, a funda- mental right enshrined in the Charter of Fundamental Rights of the European Union.” This paper analyses how disinformation endan- gers democratic decision-making and how social media platforms could contribute to tackling those challenges. 1.2. Research methodology Reflecting on the reasons and impacts of disinformation on so- cial media, a hermeneutic process of understanding and interpret- ing the ethical and societal consequences has been chosen. What can be expected is a set of ethically grounded suggestions for ac- tions which could be discussed and implemented by the platform owners. Hermeneutics confronts “quantitative methodologies with qualitative questions” ( van Dijck, 2014 : p. 206). The tools for this process are rooted in the humanities and in social science. Critical reflectivity ( Gregory, 20 0 0 ) is one such method that could be ap- plied to a topic that raises societal and political questions. The re- sults cannot resolve the existing problems of political manipulation completely, due to societal complexity ( DeTombe, 2002 ), associated with almost ubiquitous social media. The philosopher Wilhelm Dilthey (1833–1911) established hermeneutics in the humanities as interpretative sciences in their own right. He studied the rela- tionships between personal experience, its realization in creative expression, and the reflective understanding of this experience; and, finally, the logical development from these to the understand- ing of social groups and historical processes ( Dilthey, 2013 ). Using the concept of hermeneutics and reflective understand- ing, the operation and impacts of social media with regard to the current societal trends are discussed in Part 2. Examples for ma- nipulation are given in Part 3. Concepts of digital media ethics and responsibility are presented in Part 4, and compared with the tra- ditional media’s accountability approaches. Ethics codes for AI are shown and suggestions for responsible action of social media are provided. Based on those, a reflective and strategic corporate re- sponsibility of digital media is introduced as a new concept. Part 5 gives some reflections and limitations. Finally, the conclusion in part 6 offers a global outlook on societal responsibilities. 2. The role of social media platforms and their impacts on societies To explain why societal and political manipulation is a spe- cial issue for social media, the goals, functioning and legal sta- tus of the respective corporate actors need to be clarified. Social media platforms such as Facebook, Twitter, Instagram, YouTube and TikTok facilitate an interactive one-to-few or many-to-many- communication in an international scale. “In the web 2.0 era, an infinite “crowd” of users can anonymously and with almost no cost “voice” criticism and protest, via Twitter and Facebook” ( Fengler, 2012 , p. 184). 2.1. The business model of social media platforms In their official prospectus for the computerized US stock mar- ket NASDAQ, Facebook claimed their vision was “t o make the world more open and connected ” ( Facebook, 2012 ). Nevertheless, their business model is commercial. When going public they disclaim that their goal is profit and shareholder value: “Advertisers can en- gage with users … on Facebook or subsets of our users based on in- formation they have chosen to share with us such as their age, lo- cation, gender, or interests. We offer advertisers a unique combina- tion of reach, relevance, social context, and engagement to enhance the value of their ads.” ( Facebook, 2012 ). Users do not pay but give their data to Facebook who then sells them to their customers, the advertisers. Dwyer and Martin (2017) explain that data cap- ture, data-mining and behavioural advertising are typical activi- t",
            {
                "entities": [
                    [
                        5373,
                        5399,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Machine learning fairness notions: Bridging the gapwith real-world applicationsKarima Makhlouf, Sami Zhioua, Catuscia PalamidessiTo cite this version:Karima Makhlouf, Sami Zhioua, Catuscia Palamidessi. Machine learning fairness notions: Bridg-Information Processing and Management, 2021, 58 (5),ing the gap with real-world applications.￿10.1016/j.ipm.2021.102642￿. ￿hal-03624025￿HAL Id: hal-03624025https://hal.science/hal-03624025Submitted on 13 Jun 2023HAL is a multi-disciplinary open accessarchive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come fromteaching and research institutions in France orabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL, estdestinée au dépôt et à la diffusion de documentsscientifiques de niveau recherche, publiés ou non,émanant des établissements d’enseignement et derecherche français ou étrangers, des laboratoirespublics ou privés.Distributed under a Creative Commons Attribution - NonCommercial| 4.0 InternationalLicense\fVersion of Record: https://www.sciencedirect.com/science/article/pii/S0306457321001321Manuscript_fc01a0c56074f6184ab81f45e475e037Machine Learning Fairness Notions:Bridging the Gap with Real-world ApplicationsKarima Makhloufa, Sami Zhiouab, Catuscia Palamidessic,∗aUniversité du Québec à Montréal, Québec, CanadabHigher Colleges of Technology, Dubai, UAEcInria, École Polytechnique, IPP, Paris, FranceAbstractFairness emerged as an important requirement to guarantee that Machine Learning (ML) predictivesystems do not discriminate against specific individuals or entire sub-populations, in particular,minorities. Given the inherent subjectivity of viewing the concept of fairness, several notions offairness have been introduced in the literature. This paper is a survey that illustrates the subtletiesbetween fairness notions through a large number of examples and scenarios. In addition, unlikeother surveys in the literature, it addresses the question of “which notion of fairness is most suitedto a given real-world scenario and why?”. Our attempt to answer this question consists in (1)identifying the set of fairness-related characteristics of the real-world scenario at hand, (2) analyzingthe behavior of each fairness notion, and then (3) fitting these two elements to recommend the mostsuitable fairness notion in every specific setup. The results are summarized in a decision diagramthat can be used by practitioners and policy makers to navigate the relatively large catalogue of MLfairness notions.Keywords: Fairness, Machine learning, Discrimination, Survey , Systemization of Knowledge (SoK)1. IntroductionDecisions in several domains are increasingly taken by “machines”. These machines try to takethe best decisions based on relevant historical data and using Machine Learning (ML) algorithms.∗Corresponding author.Email addresses: karima.makhlouf@courrier.uqam.ca (Karima Makhlouf), szhioua@hct.ac.ae (Sami Zhioua),catuscia@lix.polytechnique.fr (Catuscia Palamidessi)Preprint submitted to Journal of Information Processing and ManagementApril 9, 2021© 2021 published by Elsevier. This manuscript is made available under the CC BY NC user licensehttps://creativecommons.org/licenses/by-nc/4.0/\fOverall, ML-based decision-making (MLDM)1 is beneficial as it allows to take into considerationorders of magnitude more factors than humans do and hence outputting decisions that are more5informed and less subjective. However, in their quest to maximize efficiency, ML algorithms cansystemize discrimination against a specific group of population, typically, minorities. As an example,consider the automated candidates selection system of St. George Hospital Medical School [1, 2].The aim of the system was to help screening for the most promising candidates for medical studies.The automated system was built using records of manual screenings from previous years. Duringthose manual screening years, applications with grammatical mistakes and misspellings were rejectedby human evaluators as they indicate a poor level of English. As non-native English speakers aremore likely to send applications with grammatical and misspelling mistakes than native Englishspeakers do, the automated screening system built on that historical data ended up correlating race,birthplace, and address with a lower likelihood of acceptance. Later, while the overall English levelof non-native speakers improved, the race and ethnicity bias persisted in the system to the extentthat an excellent candidate may be rejected simply for her birthplace or address.Given that MLDM can have a significant impact in the lives and safety of human beings, it isno surprise that social and political organization are becoming very concerned with the possibleconsequences of biased MLDM, and the related issue of lack of explanation and interpretability ofML-based decisions. The European Union has been quite active in this respect. Already in theGeneral Data Protection Regulation (GDPR) there were directives concerning Automated DecisionMaking: for instance, Article 22 states that “The data subject shall have the right not to be subjectto a decision based solely on automated processing.” Other initiatives include the European Union’sEthics Guidelines for Trustworthy AI (April 2019), and OECD’s Council Recommendation onArtificial Intelligence (May 2019).In the scientific community, the issue of fairness in machine learning has become one of the mostpopular topics in recent years. The number of publications and conferences in this field has literallyexploded, and a huge number of different notions of fairness have been proposed, leading sometimesto possible confusion. This paper, like other surveys in the literature (cf. Section 1), attempts toclassify and systematize these notions. The characteristic of our work, however, consists in our point10152025301We focus on automated decision-making system supported by ML algorithms. In the rest of the paper we refer tosuch systems as MLDM.2\fof view, which is that the very reason for having different fairness notions is how suitable each oneof them is for specific real-world scenarios. We feel that none of the existing surveys has addressedthis aspect specifically. Discussion about the suitability (and sometimes the applicability) of thefairness notions is very limited and scattered through several papers [3, 4, 5, 6, 7, 8]. In this survey35paper we show that each MLDM system can be different based on a set of criteria such as: whetherthe ground-truth exists, difference in base-rates between sub-groups, the cost of misclassification,the existence of a government regulation that needs to be enforced, etc. We then revisit exhaustivelythe list of fairness notions and discuss the suitability and applicability of each one of them based onthe list of criteria.40Another set of results from the literature which is particularly related to the applicability problemwe are addressing in this paper is the tensions that exist between some definitions of fairness. Severalpapers in the literature provide formal proofs of the impossibility to satisfy several fairness definitionssimultaneously [3, 6, 8, 9, 10]. These results are revisited and summarized as they are related to theapplicability of fairness notions.45The results of this survey are finally summarized in a decision diagram that hopefully can helpresearchers, practitioners, and policy makers to identify the subtleties of the MLDM system at handand to choose the most appropriate fairness notion to use, or at least rule out notions that can leadto wrong fairness/discrimination result.50The paper is organized as follows. Section 3 lists notable real-world MLDMs where fairness iscritical. Section 4 identifies a set of fairness-related characteristics of MLDMs that will be used in thesubsequent sections to recommend and/or discourage the use of fairness notions. Fairness notions arelisted and described in the longest section of the survey, Section 5. Section 6 discusses relaxations ofthe strict definitions of fairness notions. Section 7 describes classification and tensions that existbetween some fairness notions. The decision diagram is provided and discussed in Section 8.552. Related Work and ScopeWith the increasing fairness concerns in the field of automated decision making and machinelearning, several survey papers have been published in the literature in the few previous years. Thissection revisits these survey papers and highlights how this proposed survey deviates from them.60In 2015, Zliobaite compiled a survey about fairness notions that have been introduced previ-ously [11]. He classified fairness notions into four categories, namely, statistical tests, absolute3\fmeasures, conditional measures, and structural measures. Statistical tests indicate only the presenceor absence of discrimination. Absolute and conditional measures quantify the extent of discriminationwith the difference that conditional measures consider legitimate explanations for the discrimination.6570These three categories correspond to the group fairness notions in this survey. Structural measurescorrespond to individual fairness notions2. Most of the fairness notions listed by Zliobaite arevariants of the group fairness notions in this survey. For instance, difference of means test (Section4.1.2 in [11]) is a variant of balance for positive class (Section 5.7 in this paper). Although, hededicated one category for individual notions (structural measures), Zliobaite did not mentionimportant notions, in particular fairness through awareness. Regarding the applicability of notions,the only criterion considered was the type of variables (e.g. binary, categorical, numerical, etc.).The survey of Berk et al. [12] listed only group fairness notions that are defined using theconfusion matrix. Similar to this survey, they used simple examples based on the confusion matrix tohighlight relationshi",
            {
                "entities": [
                    [
                        337,
                        362,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "UvA-DARE (Digital Academic Repository)In defense of offense: information security research under the right to sciencevan Daalen, O.DOI10.1016/j.clsr.2022.105706Publication date2022Document VersionFinal published versionPublished inComputer Law and Security ReviewLicenseArticle 25fa Dutch Copyright Act Article 25fa Dutch Copyright Act(https://www.openaccess.nl/en/in-the-netherlands/you-share-we-take-care)Link to publicationCitation for published version (APA):van Daalen, O. (2022). In defense of offense: information security research under the right toscience. Computer Law and Security Review, 46, [105706].https://doi.org/10.1016/j.clsr.2022.105706General rightsIt is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s)and/or copyright holder(s), other than for strictly personal, individual use, unless the work is under an opencontent license (like Creative Commons).Disclaimer/Complaints regulationsIf you believe that digital publication of certain material infringes any of your rights or (privacy) interests, pleaselet the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the materialinaccessible and/or remove it from the website. Please Ask the Library: https://uba.uva.nl/en/contact, or a letterto: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. Youwill be contacted as soon as possible.Download date:04 jul. 2023UvA-DARE is a service provided by the library of the University of Amsterdam (https://dare.uva.nl)\fcomputer law & security review 46 (2022) 105706 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR In defense of offense: information security research under the right to science ✩ ∗Ot van Daalen University of Amsterdam a r t i c l e i n f o a b s t r a c t Keywords: Information security Coordinated vulnerability disclosure Right to science Communications freedom Duty to disclose Vulnerabilities Information security research Information security is something you do , not something you have . It’s a recurring process of finding weaknesses and fixing them, only for the next weakness to be discovered, and fixed, and so on. Yet, European Union rules in this field are not built around this cycle of making and breaking: doing offensive information security research is not always legal, and doubts about its legality can have a chilling effect. At the same time, the results of such research are sometimes not used to allow others to take defensive measures, but instead are used to attack. In this article, I review whether states have an obligation under the right to science and the right to communications freedom to develop governance which addresses these two issues. I first discuss the characteristics of this cycle of making and breaking. I then discuss the rules in the European Union with regard to this cycle. Then I discuss how the right to science and the right to communications freedom under the European Convention for Human Rights , the EU Charter of Fundamental Rights and the International Covenant on Economic, Social and Cultural Rights apply to this domain. I then conclude that states must recognise a right to research information security vulnerabilities, but that this right comes with a duty of researchers to disclose their findings in a way which strengthens information security. © 2022 Ot van Daalen. Published by Elsevier Ltd. All rights reserved. 1. Introduction Information security is something you do , not something you have . It’s a recurring process of finding weaknesses and fix- ing them, only for the next weakness to be discovered, and fixed, and so on. Yet, European Union rules in this field are not built around this cycle of making and breaking: doing of- fensive information security research is not always legal, and doubts about its legality can have a chilling effect. At the same time, the results of such research are sometimes not used to allow others to take defensive measures, but instead are used to attack. In this article, I review whether states have an obli- gation under the right to science and the right to communica- tions freedom to develop governance which addresses these two issues. I first discuss the characteristics of this cycle of making and breaking. I then discuss the rules in the Euro- pean Union with regard to this cycle. Then I discuss how the right to science and the right to communications freedom un- der the European Convention for Human Rights (the Conven- tion), the EU Charter of Fundamental Rights (the Charter) and the International Covenant on Economic, Social and Cultural Rights (the Covenant) apply to this domain. I then conclude that states must recognise a right to research information se- ✩ This work was supported by the Netherlands Organisation for Scientific Research (NWO/OCW), as part of the Quantum Software Con- sortium programme (project number 024.003.037/3368). It is based on a forthcoming PhD on information security, encryption, quantum computing and human rights by the author. The author would like to thank his PhD supervisors, Joris van Hoboken and Mireille van Eechoud, for their comments on earlier drafts. ∗ Corresponding author: Mr Ot van Daalen, Institute for Information Law, Netherlands E-mail address: o.l.vandaalen@uva.nl https://doi.org/10.1016/j.clsr.2022.105706 0267-3649/© 2022 Ot van Daalen. Published by Elsevier Ltd. All rights reserved. \f2 computer law & security review 46 (2022) 105706 curity vulnerabilities, but that this right comes with a duty of researchers to disclose their findings in a way which strength- ens information security. Information security as a cycle of making 2. and breaking In the literature, information security is often framed in terms of desirable security properties, such as confidentiality, in- tegrity and availability.2 And information security measures are intended to safeguard these properties against attacks. Many organisations will, at some point in their development, take these kind of information security measures. But it can be difficult to determine which measures make the most sense. Over the past decades, standard practices have emerged to help organisations make the best choices, even in the face of changing circumstances. These are generally subsumed un- der the plan-do-check-act cycle.3 In the first phase of the cycle, the plan -phase, an organisa- tion will decide which measures are necessary in view of the risks. These decisions are usually laid out in an information security policy. In the second phase, the do -phase, the organi- sation then implements these measures. That does not mean, of course, that these measures are always sufficient. That’s why information security policies need to be tested and re- viewed periodically – the check and act phases of the cycle. You periodically check whether the measures are commensurate with the risks, then adjust as needed. This approach reflects the reality of the continuous cycle of making and breaking. An important part of this cycle centres around the “vul- nerability” or weakness, in software or hardware. An attacker can exploit such a vulnerability to make a system act in a way which it is not supposed to do, or to be more precise: to violate a security policy . And while you might in theory be able to make software and hardware without vulnerabilities, in practice it’s virtually impossible. It is not doable to independently verify all components of a system.4 And even such verification only provides limited assurance that a component can actually be trusted.5 2 3 4 5 See Axel M. Arnbak, Securing Private Communications: Protecting Private Communications Security in EU Law: Fundamental Rights, Func- tional Value Chains, and Market Incentives (Kluwer Law International 2016) ch 5 for an in depth discussion of these concepts; and A. J. Menezes, Paul C. Van Oorschot and Scott A. Vanstone, Handbook of Applied Cryptography (CRC Press 1997) 4 for the definition of the first two. See for example Regulation (EU) 2016/679 of the European Par- liament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (2016 OJ L 119/1), Art. 32(1)(d). Edlyn V. Levine, “The Die Is Cast: Hardware Security Is Not As- sured” (2020) 18 ACMqueue. See Ken Thompson, “Reflections on Trusting Trust” (August) 1984 Communications of the ACM 761 for a principled argument; and Georg T. Becker and others, “Stealthy Dopant-Level Hardware Trojans: Extended Version” (2014) 4 Journal of Cryptographic Engi- neering 19 for a practical example. 2.1. Research and discovery As a result, many vulnerabilities are found in most widely used products after they are shipped, and even if they’re shipped without vulnerabilities, the deployment by users might create new weaknesses. One particular type of vulnera- bility is called the “zero day vulnerability”, or simply the “zero day”. It’s a weak spot for which no fix has been created yet, usually because the vendor doesn’t know of its existence. Of all vulnerabilities, zero days are the most coveted by attack- ers, because by definition there is not yet a direct defence for them. And that’s why zero days can also wreak the most havoc. After inception, these vulnerabilities will often lie dormant, undiscovered for months, if not years. But researchers are con- tinuously on the lookout for bugs, and they generally have the upper hand. They hunt by disassembling hardware, trawling through lines of code and remotely testing online services. This used to be done manually, and still often is. However, many tools and services have come available which enable au- tomatic testing.6 And since the information security of a sys- tem is as strong as its weakest link, attackers generally only need one vulnera",
            {
                "entities": [
                    [
                        134,
                        160,
                        "DOI"
                    ],
                    [
                        629,
                        655,
                        "DOI"
                    ],
                    [
                        5393,
                        5419,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Debating Darwin at the CapeLivingstone, D. (2016). Debating Darwin at the Cape. Journal of Historical Geography, 52, 1-15.https://doi.org/10.1016/j.jhg.2015.12.002Published in:Journal of Historical GeographyDocument Version:Peer reviewed versionQueen's University Belfast - Research Portal:Link to publication record in Queen's University Belfast Research PortalPublisher rights© Elsevier 2016.This is an open access article published under a Creative Commons Attribution-NonCommercial-NoDerivs License(https://creativecommons.org/licenses/by-nc-nd/4.0/), which permits distribution and reproduction for non-commercial purposes, provided theauthor and source are cited.General rightsCopyright for the publications made accessible via the Queen's University Belfast Research Portal is retained by the author(s) and / or othercopyright owners and it is a condition of accessing these publications that users recognise and abide by the legal requirements associatedwith these rights.Take down policyThe Research Portal is Queen's institutional repository that provides access to Queen's research output. Every effort has been made toensure that content in the Research Portal does not infringe any person's rights, or applicable UK laws. If you discover content in theResearch Portal that you believe breaches copyright or violates any law, please contact openaccess@qub.ac.uk.Open AccessThis research has been made openly available by Queen's academics and its Open Research team. We would love to hear how access tothis research benefits you. – Share your feedback with us: http://go.qub.ac.uk/oa-feedbackDownload date:04. jul. 2023\fDebating Darwin at the Cape On 31st May 1836, the Royal Navy’s surveying barque, HMS Beagle, dropped anchor at Simon’s Bay near Cape Town. On deck was the young Charles Darwin who, nearly four and a half years earlier, had stepped aboard the vessel as a budding geologist and table companion to Captain Robert Fitzroy who had been assigned the task of charting the coastline of South America and determining meridian distances in the southern hemisphere. The Royal Observatory outside Cape Town was a crucial port of call, and with Sir John Herschel, Britain’s highly distinguished astronomer, currently residing in Cape Colony on a four-year project to catalogue the stars, clusters and nebulae of the southern skies, the Beagle’s crew found themselves in the Cape for eighteen days – a longer stay than anywhere else on the whole voyage save for the Galápagos Islands. For all that, Darwin was remarkably silent about the Cape.1 For the fact of the matter is that Darwin did not take to the colony much at all. In his diary entry for 4th June he confessed “I saw so very little worth seeing, that I have scarcely anything to say”. The landscape he found “bleak and desolate”, its aspect “cheerless” and the Ruggensveld region devoid of interest.2 His private notes on Paarl Rock never saw the public light of day and his reflections on the Sea Point granite-slate contacts were reduced to the briefest of remarks in his 1844 Geological Observations on Volcanic Islands.3 But if Darwin more or less entirely ignored the Cape in his writings – though he did remain in touch with a number of correspondents there – the same cannot be said of the Cape’s reaction to his theories. For during the late 1860s and 1870s, when controversy surrounding the theory of evolution by natural selection was bursting into full flame, the Cape Monthly Magazine in particular carried a spate of articles subjecting Darwinism to sustained scrutiny. The Cape Monthly had come into being in 1857 under the editorship of Roderick Noble who taught at the South African College, and was designed to advance the virtues of intellectual enlightenment, social progress, and the spread of civilization in the Cape.4 As Saul Dubow remarks, the “Monthly combined the seriousness of purpose characteristic of the highbrow British quarterlies … and lay at the center of an 1 Wilhelm S. Barnard, “Darwin at the Cape”, South African Journal of Science 100 (2004): 243-48. 2 See entries for June 1836, in Charles Darwin’s Beagle Diary ed. Richard Darwin Keynes (Cambridge: Cambridge University Press, 1988). See also Barnard, “Darwin at the Cape”, p. 245. 3 On Darwin’s account of the Sea Point contacts see Sharad Master, “Darwin as a Geologist in Africa – Dispelling the Myths and Unravelling a Confused Knot”, South African Journal of Science 108 No 9/10 (2012): 1-5. 4 See the discussion in Saul Dubow, A Commonwealth of Knowledge: Science, Sensibility, and White South Africa 1820-2000 (Oxford: Oxford University Press, 2006), chapter 2. 1                           \finterlocking network of associated colonial institutions and societies such as the South African Library, Museum, the Art Gallery and the University of the Cape of Good Hope”.5 Aspiring to involve itself in the global scientific conversation, its editors kept their eyes “firmly fixed on developments in the imperial centres of London and Edinburgh”.6 Thus while much original work on the local geography and anthropology of the Cape itself graced the Monthly’s pages, its tone was, by and large, that of a liberal intelligentsia seeking a place at the international scientific table during a time when the colony was absorbed with railway construction, diamond mining, and the establishment of ‘Responsible Government’ with the appointment of its own Prime Minister in 1872. As elsewhere, the Darwinian debates in the Cape really only surfaced during the late 1860s and 1870s, and progressively intensified as the new decade wore on owing, in large measure, to the appearance of the Descent of Man in 1871 which directly applied the theory of evolution by natural selection to the human race, and to the furore surrounding John Tyndall’s infamous presidential address to the 1874 meeting of the British Association for the Advancement of Science in Belfast. Taken in the round, exchanges over Darwin’s proposals were conducted with notable civility, certainly compared with other venues, though worries over materialism were increasingly voiced in the aftermath of Tyndall’s incursion. There were, too, novel mobilisations of Darwinism for purposes of immediate cultural relevance to the colony – especially in the fields of legislation and linguistics – which had significant racial resonances. Charting something of these engagements in the cultural space marked out by the English-speaking network that congregated around the Cape Monthly Magazine, the African Library and the like, is my ambition in what follows.7 This inquiry is intended to further contribute to the growing literature on the geographies of scientific knowledge in general, and the historical geography of Darwinism more particularly, by tracing in some detail the ways in which Darwin’s theory was talked about and acted upon in the Cape during the decades around 1900. By examining the practices of science and the responses of the Cape’s intellectual elite to the latest theoretical proposals, it is intended to make a contribution to understanding something of the nature of scientific culture in a colonial setting. At the same time, by inspecting the diverse range of spheres into which evolutionary thinking was drawn – 5 Saul Dubow, “Earth History, Natural History, and Prehistory at the Cape, 1860-1785”, Comparative Studies in Society and History 46 (2004): 107-133, on p. 109. 6 Dubow, Commonwealth of Knowledge, p. 71. 7 How Afrikaner culture engaged with Darwin’s proposals in this period, so far as I am aware, remains to be explored. 2                          \fphilology, natural history, anthropology, religion, philosophy, geology, law – it demonstrates just how wide-ranging the Darwinian debates were in the Colony’s public square. What also emerges from this analysis is the complex geography of exchange between Europe and the Cape with the circulation of people, print and opinion across the imperial domain rendering local scientific cultures a compound product of both ‘here’ and ‘there’. Early Encounters Initial reactions to Darwin at the Cape were articulated in a setting already favourably disposed to scientific inquiry. The Scottish-born physical scientist, Roderick Noble, Professor at the South African College, public lecturer and editor of the Cape Monthly,8 for example, had expressed his views on the science of geology in a lecture delivered to the Mechanics Institute in 1854. Noble was deeply religious – he had studied for the ministry in Edinburgh – and was well acquainted with the tradition of Scottish Common Sense philosophy, lecturing on such figures as Dugald Stewart and Thomas Reid. Such predilections favourably disposed him to the scientific enterprise and his lecture Geology: Its Relation to Scripture was sculpted in dialogue with his theological heritage. Here no trace of literalist scriptural geology surfaced.9 Instead, calling on the authority of such figures as Thomas Chalmers, John Pye Smith, Hugh Miller, and Edward Hitchcock, not to mention Cardinal Wiseman and Archbishop Whately, he argued that they had developed a variety of hermeneutic schemes – basically harmonising strategies – showing how a lengthy earth-history was entirely compatible with enlightened readings of the Genesis narrative. Geology’s compatibility with popular religious sentiment was a different matter; but Noble assured his audience that “no such antagonism or irreconcilableness does in reality hold”.10 Later in 1868, in another public lecture, this time to the South African Public Library, an institution renowned for its rich manuscript resources, he insisted there was no inevitable conflict between Darwinian evolution and Divine 8 See William Beinart, The Rise of Conservation in South Africa: Settlers, Livestock and the Environment 1770-1950 (Oxford: Oxford University Press, 2008); W.J. De Kock (ed.) Dictionary of South African Biography (Cape Town: Tafelberg, 1968-1981) Vol. II, p. 518-519. 9 On scr",
            {
                "entities": [
                    [
                        138,
                        163,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Vrije Universiteit BrusselAutomated decision-making in the EU Member States: The right to explanation and other“suitable safeguards” in the national legislationsMalgieri, GianclaudioPublished in:Computer Law & Security ReviewDOI:10.1016/j.clsr.2019.05.002Publication date:2019License:CC BYDocument Version:Final published versionLink to publicationCitation for published version (APA):Malgieri, G. (2019). Automated decision-making in the EU Member States: The right to explanation and other“suitable safeguards” in the national legislations. Computer Law & Security Review, 35(5), [105327].https://doi.org/10.1016/j.clsr.2019.05.002CopyrightNo part of this publication may be reproduced or transmitted in any form, without the prior written permission of the author(s) or other rightsholders to whom publication rights have been transferred, unless permitted by a license attached to the publication (a Creative Commonslicense or other), or unless exceptions to copyright law apply.Take down policyIf you believe that this document infringes your copyright or other rights, please contact openaccess@vub.be, with details of the nature of theinfringement. We will investigate the claim and if justified, we will take the appropriate steps.Download date: 04. jul. 2023 \fcomputer law & security review 35 (2019) 105327 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR Automated decision-making in the EU Member States: The right to explanation and other “suitable safeguards” in the national legislations ∗Gianclaudio Malgieri Vrije Universiteit Brussel, Pleinlaan 2, 1020 Brussels, Belgium a r t i c l e i n f o a b s t r a c t Keywords: Right to explanation Automated decision-making AI Legibility Suitable safeguards Data Protection GDPR Article 22 Right to contest Algorithmic impact assessment The aim of this paper is to analyse the very recently approved national Member States’ laws that have implemented the GDPR in the field of automated decision-making (prohi- bition, exceptions, safeguards): all national legislations have been analysed and in partic- ular 9 Member States Law address the case of automated decision making providing spe- cific exemptions and relevant safeguards, as requested by Article 22(2)(b) of the GDPR (Bel- gium, The Netherlands, France, Germany, Hungary, Slovenia, Austria, the United Kingdom, Ireland). The approaches are very diverse: the scope of the provision can be narrow (just auto- mated decisions producing legal or similarly detrimental effects) or wide (any decision with a significant impact) and even specific safeguards proposed are very diverse. After this overview, this article will also address the following questions: are Member States free to broaden the scope of automated decision-making regulation? Are ‘positive decisions’ allowed under Article 22, GDPR, as some Member States seem to affirm? Which safeguards can better guarantee rights and freedoms of the data subject? In particular, while most Member States refers just to the three safeguards mentioned at Article 22(3) (i.e. subject’s right to express one’s point of view; right to obtain human in- tervention; right to contest the decision), three approaches seem very innovative: a) some States guarantee a right to legibility/explanation about the algorithmic decisions (France and Hungary); b) other States (Ireland and United Kingdom) regulate human intervention on algorithmic decisions through an effective accountability mechanism (e.g. notification, ex- planation of why such contestation has not been accepted, etc.); c) another State (Slovenia) require an innovative form of human rights impact assessments on automated decision- making. © 2019 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) ∗ Corresponding author: Gianclaudio Malgieri, Vrije Universiteit Brussel, Pleinlaan 2, 1020 Brussels, Belgium. E-mail address: gianclaudio.malgieri@vub.ac.be https://doi.org/10.1016/j.clsr.2019.05.002 0267-3649/© 2019 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) \f2 computer law & security review 35 (2019) 105327 1. Introduction and methodology The aim of this paper is to analyse the very recently ap- proved national Member States’ laws that have implemented the GDPR in the field of automated decision-making (prohibi- tion, exceptions, safeguards). The EU General Data Protection Regulation has tried to ad- dress the risks of the automated decision-making through different tools: a right to receive meaningful information about logics, significance and envisaged effects of automated decision-making; the right not to be subject to automated decision-making with several safeguards and restrains for the limited cases in which automated decisions are permitted. In a previous article it was suggested that the dualism be- tween right to ex post explanation vs. right to ex ante general information should be overcome: transparency and compre- hensibility should merge in the concept of “legibility”.1 One re- maining problem is the exact meaning of “suitable measures to safeguard the data subject’s rights and freedoms and legiti- mate interests” that should be taken, e.g. when the automated decision-making is authorised by Union or Member State law. Member States laws implementing the GDPR are, thus, an important reference when discussing automated decision- making and suitable safeguards to protect individuals against such decisions: Article 22(2) lett. b explicitly refers to Mem- ber States laws that should also adopt ‘suitable safeguards’ for protecting individuals. Section 2 will analyse the relevant GDPR provisions in terms of automated decision-making and “suitable safe- guards”; while Section 3 will briefly mention the debate around the right to an explanation of the algorithmic decision-making. Consequently, Section 4 will analyse pos- sible ‘suitable safeguards’ against adverse effects of auto- mated decision-making on individuals; while Section 5 will analyse the nine Member States whose data protection laws have explicitly regulated automated decision-making. Finally, Section 6 will summarize and compare some of the most rel- evant provisions of Member States Law and Section 7 will propose some preliminary conclusions, analysing advantages and disadvantages of the most innovative national regula- tions. Some preliminary remarks about methodology are also necessary. All Member States Law implementing the GDPR have been analysed here, through the official versions avail- able in different national online repositories of the approved legislation (e.g. www.gesetze- im- internet.de for German law, www.legislation.gov.uk for UK law, etc.).2 Sometimes the offi- cial language is already English (UK, Ireland, Malta), in other cases the English translation is publicly available (it is the case 1 2 Gianclaudio Malgieri and Giovanni Comandé, ‘Why a Right to Legibility of Automated Decision-Making Exists in the General Data Protection Regulation’, International Data Privacy Law 7, no. 4 (1 November 2017): 243–65, https://doi.org/10.1093/idpl/ipx019 . A useful summary of all national online repositories for different national legislations can be found here: https://iapp. org/resources/article/eu- member- state- gdpr- implementation- laws- and- drafts/ (last access, 1 December 2018). The list of links for each Member State Law will be in the following footnotes. Danish law,4 of German law,3 ). In the other cases, the author has profited from national experts who have specifically translated in English for him the relevant provi- sions regarding automated decision-making. Romanian Law 5 In addition, to improve the quality of legal comparison among different legal texts, the author has taken in due con- sideration the wording of the GDPR and the official transla- tions in all different languages of the EU: 6 it has allowed to understand whether national laws have strictly respected the GDPR wording, or, as an alternative, have proposed more orig- inal implementations. 2. The problem of automated-decision making and the GDPR (Articles 15 and 22) Profiling algorithms and automated decision-making are a growing reality in the actual data-driven society. Policy- makers, scholars and commentators are more and more con- cerned with the risks of black box society 7 in several fields: fi- nance, insurance, housing, police investigations, e-commerce, work life, etc. The GDPR has tried to provide a solution through different tools: a right to receive/access meaningful information about logics, significance and envisaged effects of the automated decision-making processes (Articles 13(2), lett. f; 14(2), lett. g; and 15(1), lett. h). In addition, Article 22(1) states that “the data subject shall have the right not to be subject to a decision based solely on au- tomated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her ”. This right shall not apply in only three cases: a. the decision “is necessary for entering into, or performance of, a contract between the data subject and a data con- troller”; 3 https://www.gesetze- im- internet.de/englisch _ bdsg/ englisch _ bdsg.html#p0310 . https://www.datatilsynet.dk/media/6894/danish-data- protection-act.pdf. https://www.privacyone.ro/files/Romanian-GDPR- implementation- law- English- translation.pdf. See the official versions here: https://eur-lex.europa.eu/ legal-content/EN/TXT/?uri=CELEX:32016R0679 . Frank Pasquale, The Black Box Society: The Secret Algorithms That Control Money and Information , Cambridge-London, 2015. Vir- ginia Eubanks, Automating Inequality – How High Tech Tools Pro- file, Police, and Punish the Poor , St. Martin Press, New York, 2018. See also, e.g., Joshua A. Kroll, Joanna Huey, Solon Barocas, Ed- ward W. Felten, Joe",
            {
                "entities": [
                    [
                        229,
                        255,
                        "DOI"
                    ],
                    [
                        607,
                        633,
                        "DOI"
                    ],
                    [
                        4031,
                        4057,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "ReligionISSN: 0048-721X (Print) 1096-1151 (Online) Journal homepage: https://www.tandfonline.com/loi/rrel20Daniel Dubuisson, The Western Construction ofReligionSteven Engler & Dean MillerTo cite this article: Steven Engler & Dean Miller (2006) Daniel Dubuisson, The WesternConstruction of Religion , Religion, 36:3, 119-178, DOI: 10.1016/j.religion.2006.08.001To link to this article: https://doi.org/10.1016/j.religion.2006.08.001Published online: 19 Oct 2011.Submit your article to this journal Article views: 2069View related articles Citing articles: 1 View citing articles Full Terms & Conditions of access and use can be found athttps://www.tandfonline.com/action/journalInformation?journalCode=rrel20\fReligion 36 (2006) 119e178www.elsevier.com/locate/religionReview symposiumDaniel Dubuisson, The WesternConstruction of ReligionSteven Engler a,*, Dean Miller ba Department of Humanities, Mount Royal College, Calgary, Alberta T3N 6K6, Canadab Emeritus, Department of History, University of Rochester,10848 South Hoyne Avenue, Chicago, IL 60643, USAAbstractIn The Western Construction of Religion Daniel Dubuisson argues that the concept of ‘religion’ is toohistorically and culturally contingent to serve as the basis for a comparative discipline. The concept isindigenous to Western culture and is inherently theological and phenomenological. He argues for a con-structionist view of the discipline and proposes the concept ‘cosmographic formations’ as a replacementfor ‘religion’. Religious phenomena should be taken as discursive constructions that link embodied individ-uals to the social, cultural and cosmic orders. The following reviews evaluate Dubuisson’s arguments,relating them to broader currents in the theory of religion. Daniel Dubuisson responds to each of thereviews.1(cid:2) 2006 Elsevier Ltd. All rights reserved.* Corresponding author.E-mail address: sjengler@gmail.com (S. Engler).1 The reviews by Engler, Hughes and Segal were presented in a session of the Critical Theory and Discourses onReligion Group of the American Academy of Religion meeting in San Antonio, Texas, in November 2005. Ann Taves’contribution is adapted from her response to these commentators at that AAR Session. An additional review fromthat session, by Gustavo Benavides, is not included here because portions were previously committed for publicationelsewhere. Dean Miller gathered the three additional reviews. Dubuisson’s responses were edited and translated bySteven Engler.0048-721X/$ - see front matter (cid:2) 2006 Elsevier Ltd. All rights reserved.doi:10.1016/j.religion.2006.08.001\f120S. Engler, D. Miller / Religion 36 (2006) 119e178Agency, order and time in the human science of religionSteven EnglerDepartment of Humanities, Mount Royal College, Calgary, Alberta T3N 6K6, CanadaE-mail address: sjengler@gmail.comIn The Western Construction of Religion (2003) Daniel Dubuisson criticises religious studies onthe grounds that its axiomatic category, ‘religion’, is too historically and culturally contingent toserve as the basis for a comparative discipline. He argues that ‘the West invented religion’ (a leg-acy of Christian concepts and nineteenth-century colonial scholarship) (p. 12).2 He faults scholarsof religion for propagating this deception: ‘the history of religions should not have exported thissingular notion, found nowhere else, and issuing from a history that took its own unique course,without having subjected it beforehand to a rigorous critical examination’ (p. 191).In two respects Dubuisson goes beyond similar arguments by others, such as Talad Asad, Rus-sell McCutcheon and Timothy Fitzgerald. First, he argues that ‘religion’ is not just another con-struct. Rather, the concept has filled an ‘architectonic function’it has‘supplied the nucleus about which the West has constructed its own universe of values and repre-sentations’ (pp. 117, 39). Second, he proposes as a replacement the concept ‘cosmographic forma-tions’, which he roots in a universal instinct for creating conceptions that relate cosmic, culturaland social orders.in Western culture:The value of Dubuisson’s book lies not just in its critique of religion but also in the threads thathe draws upon in beginning to suggest a way to move our discussions forward. I will argue fourrelated claims. (1) Dubuisson’s discursive link between the themes of religion and order is veryuseful. (2) An ambiguous appeal to science presents a misleading dichotomy, leaving us to choosebetween naively essentialist and radically constructionist views of the study of religion, withDubuisson throwing his weight behind radical constructionism. (3) The claim that ‘cosmographicformations’ offer a truly universal category for cross-cultural comparison needs further clarifica-tion. (4) A clearer conception of agency can reframe and supplement the argument, reclaiming thepractical dimension of religion.The key question is, What is to be gained by replacing the concept of ‘religion’? For Dubuisson,religion is too limited a construct. If ‘religion’ is a construct, then a substitution is possible. If ‘re-ligion’ is a limited concept, then a substitution is desirable.The concept of religion, according to Dubuisson, is limited in three related ways. First, it drawson metaphysical presuppositions. Dubuisson suggests an alternative, because using ‘religion’ as ananalytical tool imports insider concepts, resulting in circularity: ‘A regrettable confusion con-stantly arises between religious ideas and ideas about religion’ (p. 55). ‘How could it leave thismagic circle when the object that it is supposed to study is supplied by its own cultural traditionwhich also surreptitiously imposes on it the means and frameworks for its inquiry?’ (p. 192).Second, ‘religion’ is culturally specific: ‘Religion, that is, the word, the idea, and above all theparticular domain that they all designate represents an entirely original creation that the Westalone conceived and developed after having converted to Christianity’ (p. 190). The ‘history of2 All unattributed page references in all reviews are to Dubuisson, 2003.\fS. Engler, D. Miller / Religion 36 (2006) 119e178121religion thus reveals itself to be not only a Western discipline but a science born of the closingdecades of the nineteenth century’ (p. 155).Third, and most significant as a motivation for replacing it, the concept of religion hasplayed a foundational role in generating the ‘major paradigm’ of the West that has constrainednot just the history of religions but Western thought in general: religion plays a ‘decisive rolein the constitution of Western culture.. [R]eligion is at the heart of our ‘world’’ (p. 190).Religion is ‘the most ideological of Western creations’ (p. 147). This increases the risk of ac-cepting the concept uncritically: ‘religion’ is so foundational to the biases of Western intellectualperspectives that we can only leave behind its distorting influence by abandoning the conceptentirely.Dubuisson’s constructionism asserts that Western culture, in the wake of Christianity, is con-strained by an underlying conceptual framework:Western thought . (is) disposed of only a small number of theses and models.. a huge sys-tem of fractal shapes . dominated by the incessant activity of polemic and controversy. .This immemorial movement, inscribed in our oldest intellectual tradition, . offer(s) ourfields of knowledge the system of references and coordinates in which they inscribe them-selves. Every new idea or hypothesis immediately generates its antithesis, whose positionis a priori predictable.. We are advancing in a familiar, well-mapped universe, in whichthe same ideas never cease to be revived and recombined with one another. (pp. 132e3)The study of religion works itself out within this ‘tacit contract that a priori binds every Westernthinker to the vast complex formed by its interpretive grids’ (p. 144).Dubuisson turns to the relation between religion and science. There is a misleading ambiguityhere. Usually, he uses ‘science’ to refer to the social, or ‘human’, sciences, but occasionally he usesthe term to refer to the natural sciences. Dubuisson clearly holds that the two are different: ‘Un-like the natural sciences, the various human sciences never have to deal with raw data’ (p. 175).However, this distinction and its implications are not evoked consistently. Talk of ‘raw data’offers a misleadingly positivistic view of the natural sciences. It is all too easy to hold that thehuman sciences cannot live up to this ideal, but the portrayal is distorted, lending support bydefault to a constructionist view of the human sciences: ‘Contrary to the received opinion implicitin common parlance and embodied in a kind of scientific positivism, people do not live the THEworld, . since each human group lives only in its world (p. 204).Dubuisson distinguishes two levels at which the concepts ‘science’ and ‘religion’ work. At onelevel we find ‘the canonical opposition, religion versus science’, but ‘on another more global level,religious explanation and scientific explanation offer . undeniable affinities and amusing similar-ities’ (p. 150). Dubuisson means the human sciences here, and the affinity is constructionism. ForDubuisson, the human sciences are like religion, because both are constructs: scientific knowledgereflects the perspective of a specific culture; it is not universal and objective (see p. 203). And itmanifests inevitable progress (see p. 161).Dubuisson offers us a stark choice in conceptions of science: radical constructionism or simplis-tic realism. But this is made possible because in these passages he is referring to ‘the human sci-ences’. This excessively stark view of the natural sciences is easily discounted as a model for thesocial or human sciences (see Engler, 2004). He thus neglects the possibility of a trans-culturalmiddle ground, neither naively objective nor purely Western. I take Dubuisson’s term ‘human\f122S. Engler, D. Miller / Religion 36 (20",
            {
                "entities": [
                    [
                        330,
                        360,
                        "DOI"
                    ],
                    [
                        401,
                        431,
                        "DOI"
                    ],
                    [
                        2563,
                        2593,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "computer law & security review 47 (2022) 105737 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR What post-mortem privacy may teach us about privacy ✩ Uta Kohl University of Southampton, United Kingdom a b s t r a c t This paper approaches the debate about the protection of digital legacies through a medical confidentiality lens, capitalising on its outlier status in common law jurisdictions as a privacy-type duty that survives the death of the rightsholder. The discussion takes the case law in England and Wales and by the European Court of Human Rights on post-mortem medical confidentiality as a springboard for interrogating how these judgments navigate the traditional objections to post-mortem privacy. Whilst the legal duty of medical confidentiality, drawing on the professional duty of the Hippocratic Oath, acts in the first place as a trust mechanism between doctor and patient based on a reciprocity of interests, its incidental effect of protecting not just the rightsholder but also duty bearers and the industry, signals more complex operational dynamics. The post-mortem continuation of that duty in turn brings these other relationships to the surface. Indeed, the post-mortemness amplifies that confidentialities – and by extension information privacy - can rarely be located in an isolated, singular binary relationship between a duty bearer and a rightsholder but is entangled in the great messy sociality of life that involves multiple overlapping, interdependent relationships of relative trust. These may upon the death of the primary rightsholder – make an appearance as concurrent or competing claims on her legacies and incidentally also carry her post-mortem privacy. © 2022 Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) 1. Introduction The persistence of data and information in global online net- works has tested the functional and temporal boundaries of privacy protection. Such persistence can have dire personal and professional consequences for individuals if unfavourable personal information enters the online domain. Digital mem- ory lacks the ‘natural’ forgetfulness of humans as a protec- tive mechanism upon which privacy law has previously re- lied. Some relief is now provided in the EU through the right- to-be-forgotten under the General Data Protection Regulation ✩ University of Southampton. Many thanks to Remigius Nwabueze and the anonymous referees for their helpful comments on an earlier draft. E-mail address: U.Kohl@soton.ac.uk https://doi.org/10.1016/j.clsr.2022.105737 0267-3649/© 2022 Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \f2 computer law & security review 47 (2022) 105737 [GDPR] 1 which allows individuals to request data controllers such as search engines to ‘forget’ outdated or inaccurate per- sonal information in the results produced in response to name searches, and thereby gives them some control over their per- sonal narratives in the public domain.2 Yet, the problematic of informational persistence also continues after death pre- cisely because personal data and information remains unal- tered in situ, and the rightsholder is no longer there to give directions. Whilst the issue of personal legacies is not inher- ently new, digital remains exceed, in depth and breadth, the amount and sensitivity of previous analogue records. This has led to renewed discussions of the merits, or otherwise, of post- mortem privacy protection.3 Such post-mortem privacy would address itself, in the first place, to ‘digital legacies’ or ‘digital social media accounts,5 remains’ – such as email histories,4 documents and files, search histories, personal DNA or health profiles and digital footprints more generally. Post-mortem privacy has also been called upon by relatives to prevent public access to death scene images, against the threat of potentially large online circulations .6 Furthermore, personal big data has created entirely new post-mortem possibilities with privacy implications. For example, deepfakes, that is AI-generated im- personations based on existing personal digital footage, can bring the deceased ’back to life’, with both innocuous and abu- sive potentials which once more fall within the possible ambit of privacy protection.7 Copyright and other intellectual prop- 1 2 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016on the protection of natural persons with regard to the processing of personal data and on the freemove- ment of such data. Google Spain SL and Google Inc v Agencia Española de Protección de Datos (AEPD) and Mario Costeja González C-131/12 (CJEU, GC, 13 May 2014) EU:C:2014:317, interpreting the Data Protection Direc- tive 95/46/EC; now Art 17 of the GDPR. NNG de Andrade, ‘Oblivion: the right to be different … from oneself: re-proposing the right to be forgotten’ in Alessia Ghezzi, Ângela Guimarães Pereira, Lucia Vesnic-Alujevic L (eds) The Ethics of Memory in a Digital Age – In- terrogating the Right to Be Forgotten (New York: Palgrave Macmillan, 2014) 65. For some limited US authority to the same effect, see e.g. Briscoe v Reader’s Digest Association 4 Cal 3d 532 (1971) concerning a newspaper revelation of the claimant’s criminal past which had the effect of alienating his daughter and friends from him. Jason Mazzone, ‘Facebook’s Afterlife’ (2012) 90 North Carolina Law Review 1643; Lilian Edwards, Edina Harbinja, ‘Protecting Post- Mortem Privacy: Reconsidering the Privacy Interests of the De- ceased in a Digital World’ (2013) 32 (1) Cardozo Arts & Entertainment Law Journal 101; Natalie M Banta, ‘Death and Privacy in the Digi- tal Age’ (2016) 94 North Carolina Law Review 927; Floris Tomasini, Remembering and Disremembering the Dead (Palgrave, 2017) esp Ch3. Estate of Maria Cecilia Quadri v Parisi (2021) WL 3544783 (Fla Cir 4 3 5 6 Ct); see also Ajemian v Yahoo! Inc 84 NE3d 766 (Mass 2017). Digital Inheritance III ZR 183/17 (12 July 2018) BGH. In National Archives and Records Administration v Favish 541 US 157 (2004) the US Supreme Court allowed the family’s privacy claim in respect of death scene images of the deceased as an ex- emption to freedom of information requests (here made by jour- nalists): ‘The well-established cultural tradition of acknowledging a family’s control over the body and the deceased’s death images has long been recognized at common law.’ Marisa McVey, ‘Deepfakes and the Dead: The Case of An- thony Bourdain’ (10 August 2021) Modern Technologies, Privacy Law and the Dead https://thefutureofprivacylaw.wordpress.com/2021/ 08/10/deepfakes- and- the- dead- the- case- of- anthony- bourdain/ 7 erty rights which serve economic interests, outlast the death of the rightsholder, but should privacy underwritten by digni- tary concerns also remain intact post-mortem and, if so, how long and guarded by whom? In some civil law jurisdictions, such as Germany, person- ality rights do not automatically expire upon death,8 which gives those jurisdictions a solid foundation for navigating the raised stakes of personal digital remains. Meanwhile common law countries are especially ill-equipped to deal with the new phenomenon, given the absence of a developed personality rights jurisprudence and their adherence (but for some statu- tory interventions) to the long-standing maxim actio person- alis moritur cum persona , that is personal, as opposed to propri- etary, actions die with the person.9 This maxim which goes, at least, as far back as Hambly v Trott (1776) 10 is based on the idea that ‘personal rights’ are by their very design attached to the person and thus not transferrable. By implication, any post- mortem recovery would be ‘in the nature of a penalty rather than compensatory.’ 11 The maxim captures injuries to intan- gible interests of personality, of which privacy is an example par excellence : ‘[i]t is well settled that the right to privacy is purely a personal one; it cannot be asserted by anyone other than the person whose privacy has been invaded.’ 12 This paper examines the call for post-mortem privacy in respect of digital legacies by mapping one of the few con- texts where even common law jurisdictions have recognised the possibility of privacy surviving the death of the rightsh- older, that is in respect of medical confidences.13 Whilst its almost unique status may suggest an outlier case the ratio- nale of which is not transferrable to other subject-matters,14 the argument here is that the animating forces behind post- mortem medical confidentiality are instructive about post- mortem privacy generally. Thus the microcosm of medical confidentiality delivers an opportunity for interrogating why and how the law recognises post-mortem medical privacy de- 8 9 Mephisto 30/173 (24 February 1971) BVergG; Marlene Dietrich 1 ZR 49/97 (1 December 1999) BGH; Wilhelm Kaisen 1 BvR 932/94 (5 April 20012) BVerfG; see also § 189 of the German Criminal Code (offence of defiling the memory of the dead). Note, for example, Recital 27 of the GDPR which provides that ‘This Regulation does not apply to the personal data of deceased persons. Member States may provide for rules regarding the pro- cessing of personal data of deceased persons.’ In the UK data pro- tection is limited to living individuals: s.3(2) of the Data Protection Act 2018. 10 Hambly v Trott (1776) 1 Cowper 371 (where the court acknowl- edged the limited application of the maxim to torts); but the Com- mon Law Procedure Act 1833, replaced by the Law Reform (Miscel- laneous Provisions) Act 1934 allowed for the deceased’s estate to recover some losses that arose before their death. 11 Alvin E Evans, ‘Survival of Claims For and Against Executors and Administrator’ (1931) 19 Kentucky Law Journal 195, 206. 12 James v Screen Gems Inc (1959) 174 CalApp 2d 650, 653; Hendrick-",
            {
                "entities": [
                    [
                        2626,
                        2652,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "University of Southern DenmarkDesign research education and global concernsWilde, DaniellePublished in:She Ji: The Journal of Design, Economics, and InnovationDOI:10.1016/j.sheji.2020.05.003Publication date:2020Document version:Final published versionDocument license:CC BY-NC-NDCitation for pulished version (APA):Wilde, D. (2020). Design research education and global concerns. She Ji: The Journal of Design, Economics,and Innovation, 6(2), 170-212. https://doi.org/10.1016/j.sheji.2020.05.003Go to publication entry in University of Southern Denmark's Research PortalTerms of useThis work is brought to you by the University of Southern Denmark.Unless otherwise specified it has been shared according to the terms for self-archiving.If no other license is stated, these terms apply:      • You may download this work for personal use only.      • You may not further distribute the material or use it for any profit-making activity or commercial gain      • You may freely distribute the URL identifying this open access versionIf you believe that this document breaches copyright please contact us providing details and we will investigate your claim.Please direct all enquiries to puresupport@bib.sdu.dkDownload date: 04. jul.. 2023    \fDesign Research Education and Global ConcernsDanielle WildeKeywordsPost-disciplinarityAnticipationCapacity-buildingGlobal challengesResearch educationTransformative designReceivedOctober 31, 2019AcceptedMay 5, 2020DANIELLE WILDE Department of Design and Communication,University of Southern Denmark, Denmark(corresponding author)d@daniellewilde.comAbstract If the ecosystems that we are part of and rely on are to flourish, we must urgently transform how we live, and how we imagine living. Design educa-tion has a critical role to play in this transformation, as design is a materially engaged, world-building activity. Design is complicit in the problems we are facing, and informs and shapes how people live. In this article, I seed ideas about design research education for global challenges. I speak to the merits of post-disciplinary and hybrid strategies, and look to science for clues about how to respond to twenty-first-century challenges through design. I posit sustainability brokering as a new pathway for design, and anticipating al-ternative futures as a critical step in developing transformative innovation. I then propose participatory research through design as a foundational meth-odology; describe four pillars of practice to scaffold sophisticated research at undergraduate and master’s level; and lay out a work plan for building research capacity in a doctoral school. Through this process, I articulate core skills that design researchers will likely require if they are to contribute to global challenges constructively. My aim is to seed fruitful regenerative dis-cussion with these propositions.Copyright © 2020, Tongji University and Tongji University Press. Publishing services by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Peer review under responsibility of Tongji University and Tongji University Press.http://www.journals.elsevier.com/she-ji-the-journal-of-design-economics-and-innovation https://doi.org/10.1016/j.sheji.2020.05.003\f1711 IPCC, “Global Warming of 1.5°C,” special Introductionreport from IPCC, 2018, accessed May 18, 2020, http://www.ipcc.ch/report/sr15/; IPCC, “Special Report on Climate Change and Land,” special report from IPCC, 2020, accessed May 18, 2020, https://www.ipcc.ch/srccl/; IPBES, “Introducing IPBES’ 2019 Global Assessment Report on Biodiversity and Ecosystem Services,” The Intergovern-mental Science-Policy Platform on Biodi-versity and Ecosystem Services, May 2019, accessed May 18, 2020, https://www.ipbes.net/news/ipbes-global-assessment-pre-view; Walter Willett et al., “Food in the Anthropocene: The EAT–Lancet Commission on Healthy Diets from Sustainable Food Systems,” The Lancet 393, no. 10170 (2019): 447–92, DOI: https://doi.org/10.1016/S0140-6736(18)31788-4; Independent Group of Scientists appointed by the Secretary- General, Global Sustainable Development Report 2019: The Future Is Now — Science for Achieving Sustainable Development (New York: United Nations, 2019), http://sustain-abledevelopment.un.org.2 Christiana Figueres and Tom Rivett-Carnac, The Future We Choose: Surviving the Climate Crisis (London, UK: Manilla Press, 2020); Ann Light, Irina Shklovski, and Alison Powell, “Design for Existential Crisis,” in CHI Conference Extended Abstracts on Human Factors in Computing Systems (New York: ACM, 2017), 722–34, https://doi.org/10.1145/3027063.3052760; Arturo Escobar, Designs for the Pluriverse: Radical Interdependence, Autonomy, and the Making of Worlds (Durham, NC: Duke University Press, 2018).3 Victor Papanek, Design for the Real World: Human Ecology and Social Change (Thames and Hudson London, 1972).4 Tomás Maldonado, Design, Nature, and Rev-olution: Toward a Critical Ecology (Minne-apolis, MN: University of Minnesota Press, 2019 [1972]); Papanek, Design for the Real World; Thomas T. K. Zeung, ed., Buckminster Fuller: Anthology for the New Millennium (New York: Macmillan, 2001); Tony Fry, A New Design Philosophy: An Introduction to Defuturing (Sydney: UNSW Press, 1999); Tony Fry, Design Futuring: Sustainabilitym Ethics and New Practice (Sydney: UNSW Press, 2009), 71–77; Tony Fry, Design as Politics (Oxford: Berg Publishers, 2010); Ezio Manzini, Design, When Everybody Designs: An Introduction to Design for Social Inno-vation (Cambridge, MA: MIT Press, 2015); Light et al., “Design for Existential Crisis”; Escobar, Designs for the Pluriverse.5 Melissa Leach et al., “Transforming In-novation for Sustainability,” Ecology and Society 17, no. 2 (2012): 11, DOI: https://doi.org/10.5751/ES-04933-170211.With increasing urgency, global and intergovernmental reports inform us that we must transform how we live if human society and the planetary ecosystem that we are both part of and rely on are to flourish.1 Such trans-formation requires radical shifts in the beliefs, attitudes, values, and systems that guide, shape, and constrain our behaviors. It requires culture change, in society broadly, but also in design education, as design shapes our world. Humans — impactful species that we are — must become more mindful of how intertwined we are with nature, and with each other across the globe, and the broad-reaching impact of our situated (material, social, cultural, political, and ecological) practices. We need to make room for more diverse stories — a plurality of experiences and perspectives — and start choosing vibrant, regenerative futures that consider diverse, more-than-human con-cerns.2 Design education is necessarily a part of this transformation. Design and designers have contributed in profound ways to the problems we face, and continue to shape our world. For design to contribute constructively, design education must be continually renewed. Rather than “teaching skills related to processes and working methods of an age that has ended,”3 we need to teach for the inherent instability of the circumstances at hand. We need to equip designers to respond not only to urgent crises such as COVID-19, climate change, ecosystem collapse, social and environmental injustices, war, mass migration, poverty, food scarcity, and more; but also to as-yet- unknown possibilities. This is not a new story. For decades, Tomás Maldonado, Victor Papanek, Buckminster Fuller, Tony Fry, Ezio Manzini, Ann Light and her colleagues, Eli Blevis, Arturo Escobar, and more have been telling this story.4 And yet, it still urgently requires our attentiveness and care.In this article, I focus on design research education for global challenges. I make a series of propositions: taking a post-disciplinary approach, using hybrid strategies, and looking to science and creativity for clues about how design research education might become fit for twenty-first-century chal-lenges. I posit sustainability brokering — a proposition of resilience and sustainability studies to transform innovation for sustainability5 — as a new pathway for design. From this vantage point, I outline the value of anticipa-tion if we want to develop radically different ways of living — more sustain-able, nourishing, and regenerative. I then lay out the practices and principles that guide my design research pedagogy. I describe a participatory, applied action-reflection approach to research-through-design plus four pillars of practice that I find essential for designers to contribute to transformational innovation. These can scaffold sophisticated research at the undergraduate and master’s level. I then describe a work plan for building research capacity in an art and design doctoral school. This process results in a list of core skills that (I believe) design researchers require if they are to contribute to global challenges constructively. The ideas I present are not new. However, I find that they are not always successfully disseminated or assimilated into practice, and when intro-duced to fledgling design researchers, these approaches often prove game-changers. Further, my understanding of their value has become more pointed during the COVID-19 pandemic (unfolding as I write), as design students Wilde: Design Research Education and Global Concerns\f1726 John Dewey, Art as Experience (New York: Penguin Group, 2005); John Dewey, “De-mocracy in Education,” The Elementary School Teacher 4, no. 4 (1903): 193–204, available at https://www.journals.uchi-cago.edu/doi/pdfplus/10.1086/453309; John Dewey, The Public and Its Problems: An Essay in Political Inquiry (New York: Holt, 1927).7 Jane Addams, Democracy and Social Ethics (Chicago: University of Illinois Press, 2002).8 George Herbert Mead, Mind, Self and Society, vol. 111 (Chicago: University of Chicago Press, 1934).9 Paulo Freire, Pedagogy of the Oppressed, trans. Myra Bergman Macedo (New York: Bl",
            {
                "entities": [
                    [
                        163,
                        190,
                        "DOI"
                    ],
                    [
                        468,
                        495,
                        "DOI"
                    ],
                    [
                        3266,
                        3293,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Internet Interventions journal homepage: www.elsevier.com/locate/invent Accessibility of mental health support in China and preferences on web-based services for mood disorders: A qualitative study Yuxi Tan a,b,c, Emily G. Lattie d, Yan Qiu a, b, c, Ziwei Teng a,b, c, Chujun Wu a, b, c, Hui Tang a,b, c, Jindong Chen a, b,c, * a Department of Psychiatry, The Second Xiangya Hospital, Central South University, Changsha, Hunan, China b National Clinical Research Center for Mental Disorders, Changsha, Hunan, China c Institute of Mental Health, Central South University, Changsha, Hunan, China d Department of Medical Social Sciences, Northwestern University, Chicago, IL, United States  A R T I C L E I N F O  A B S T R A C T  Keywords: Mental health Web-based health service Mood disorder Recovery Background: The fast development of mobile technologies provides promising opportunities to fulfill the largely unmet needs of treatment and recovery for mood disorders in China. However, with limited research from China, the development of acceptable and usable web-based mental health services that are based on preference of patients from China still remains a challenge. Objective: The aims of this paper were to (1) understand the experience of patients with mood disorders on current accessibility of mental health support in China; and (2) to get insights on patients' preferences on web- based mental health services, so as to provide suggestions for the future development of web-based mental health services for mood disorders in China. Methods: Semi-structured interviews were conducted with 10 female participants diagnosed with depression and 7 with bipolar disorder (5 female and 2 male) via the audio chat function of WeChat. The interviews were 60–90 min long and were audio-recorded and transcribed verbatim. Thematic analysis was conducted using QSR NVivo 12 to identify and establish themes and sub-themes. Results: Two major sections of results with a total of 5 themes were identified. The first section was participants' treatment and recovery experience, which included three main themes: (1) professional help seeking experience; (2) establishment of self-help strategies; and (3) complex experiences from various source of social support. The second section was focused on preferences for web-based services, which were divided into two themes: (1) preferred support and features, with three sub-themes: as channels to access professionals, as databases for self- help resources, and as sources of social support; and (2) preferred modality. Conclusions: The access to mental health support for personal recovery of mood disorders in China was perceived by participants as not sufficient. Web-based mental health services that include professional, empathetic social support from real humans, and recovery-oriented, personalized self-help resources are promising to bridge the gap. The advantages of social media like WeChat were emphasized for patients in China. More user-centered research based on social, economic and cultural features are needed for the development of web-based mental health services in China.  1. Introduction Mood disorders are common in China, with a lifetime prevalence of 7.4% (Huang et al., 2019). With the episodic nature of these disorders, the risk of relapse is high (Gitlin and Miklowitz, 2017; Hardeveld et al., 2013). Management of subthreshold symptoms by providing adjunctive personalized psychological interventions appears vital to preventing disability and enhancing quality of life (Bonnín et al., 2012; Miziou et al., 2015). However, despite the high prevalence, only one fifth of patients with mood disorders have ever been in contact with mental health-care providers in their lifetime (Patel et al., 2016). The reasons for the large, unmet mental health needs in China are abundant, and are often explained by the unbalanced allocation of resources between * Corresponding author at: Department of Psychiatry, The Second Xiangya Hospital, Central South University, 139 Middle Renmin Road, Changsha, Hunan 410011, China. E-mail address: chenjindong@csu.edu.cn (J. Chen). https://doi.org/10.1016/j.invent.2021.100475 Received 19 January 2021; Received in revised form 20 October 2021; Accepted 30 October 2021  InternetInterventions26(2021)100475Availableonline4November20212214-7829/©2021TheAuthors.PublishedbyElsevierB.V.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).\fY. Tan et al.                                                                                                                   Abbreviations None.  major cities and rural areas, limited mental health workforce, especially the lack of professional social workers and psychological therapists (Liang et al., 2018). The lack of medical insurance support also contributed to the high threshold for psychological therapy (Zhang et al., 2017). In recent years, the 686 project, also called the Central Government Support for the Local Management and Treatment of Serious Mental Illness Project, has been benefiting many patients with severe mental diseases, especially schizophrenia, to receive more access to more convenient even free treatment and recovery service (Liang et al., 2018). To achieve better mental health at the population level, much more attention is needed to bridge the gap faced by patients with non-psychotic disorders. With the development of mobile technology, the Internet has become a common way for patients to search for information, seek mental health resources, or obtain social support (Rahal et al., 2018). Using mobile platforms to provide mental health services has become a feasible way to bridge the gap. Currently, there are many types of web-based mental health services, such as provision of psychological education and self- management strategies based on cognitive behavioral therapy or mindfulness (O'Connor et al., 2018; Rathbone et al., 2017), active or passive symptom monitoring and feedback using smartphone sensors or wearable devices (Simblett et al., 2018), or real-time interaction via chatbots or conversational agents (Vaidyam et al., 2019). A number of reviews have demonstrated that web-based mental health services have the advantages in preventing mental illness, alleviating symptoms and improving quality of life, especially with human support (Ebert et al., 2018; O'Connor et al., 2018). Evidence showed that patients generally have high acceptance of mental health services provided on the Internet, but are prudent about their effectiveness (Apolinario-Hagen et al., 2017; Tan et al., 2020). In addition, there are several challenges worth con-cerning for future practice and implication, including low engagement, lack of evidence support, limited implication on significant clinical symptoms (Frank et al., 2018; Torous et al., 2018). There are also numerous qualitative research studies that have demonstrated the importance of mobile technologies to facilitate the treatment and recovery of mood disorders. For instance, several studies have identified the role of mobile app use on helping with patients' social interconnectivity, skill acquisition, access and management of needs, and bridge the disconnection with health professionals (Fulford et al., 2019; Pung et al., 2018). Other studies have revealed factors that might influence the effectiveness or motivation of mobile mental health ser-vices usage, such as mental health awareness, appropriate content and medium, functionality, human support, reminders or incentives (Eccles et al., 2020; Simblett et al., 2018). Barriers can be divided into personal barriers, such as lack of time, high stress level, and barriers directly related to the program, such as complex content, functionality and privacy issues (Eccles et al., 2021; Ebert et al., 2018) However, the vast majority of research on web-based mental health services were conducted in western counties (Lal and Adair, 2014), and there is still a gap in the literature, especially qualitative researches on the needs and preferences of people living in China. How to design more acceptable and usable web-based mental health services that are truly based on patients' needs and effectively promote mental health in China still remains a challenge. Given these gaps, along with the lack of service availability in China, the use of well-designed web-based mental health intervention as auxiliary means to expand the accessibility of existing mental health resources, rather than replacing the traditional face-to- face medical services (Ebert et al., 2018), could be particularly prom-ising in China. The purposes of this study are to: (1) understand the experience of patients with mood disorders regarding the current accessibility of mental health support, and (2) identify patients' preferences on web- based mental health services, so as to provide suggestions for the future development of web-based mental health services for mood dis-orders in China. 2. Methods 2.1. Study design Single session semi-structured interviews were conducted with par-ticipants diagnosed with depression or bipolar disorder via remote audio chat function provided by WeChat. WeChat is one of the most commonly used social media platforms in China (Montag et al., 2018). Audio chat was selected over video conferencing due to the sensitivity of interview content, and belief that patients may feel more comfortable discussing personal stories and feelings in a more anonymous manner. Moreover, the study was conducted during the Coronavirus Disease 2019 (COVID- 19) pandemic, which further required the interviews to be remote. 2.2. Recruitment The criteria for recruitment are: (1) age > 18; (2) have a diagnosis of major depression disorder or bipolar disorder for at least 6 months; (3) not currently experiencing suicidal ideation or manic symptoms. Par-ticipants were drawn from a pool of indivi",
            {
                "entities": [
                    [
                        4199,
                        4227,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "This is a repository copy of What motivates building repair-maintenance practitioners to include or avoid energy efficiency measures? Evidence from three studies in the United Kingdom.White Rose Research Online URL for this paper:https://eprints.whiterose.ac.uk/170121/Version: Accepted VersionArticle:Murtagh, N, Owen, A orcid.org/0000-0002-1240-9319 and Simpson, K (2021) What motivates building repair-maintenance practitioners to include or avoid energy efficiency measures? Evidence from three studies in the United Kingdom. Energy Research and Social Science, 73 (1). 101943. ISSN 2214-6296 https://doi.org/10.1016/j.erss.2021.101943© 2021, Elsevier. This manuscript version is made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/.Reuse This article is distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs (CC BY-NC-ND) licence. This licence only allows you to download this work and share it with others as long as you credit the authors, but you can’t change the article in any way or use it commercially. More information and the full terms of the licence here: https://creativecommons.org/licenses/ Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. eprints@whiterose.ac.ukhttps://eprints.whiterose.ac.uk/\fWhat motivates building repair-maintenance practitioners to include or avoid energy efficiency measures? Evidence from three studies in the United Kingdom Niamh Murtagh1, Alice M. Owen2, Kate Simpson3 1 The Bartlett School of Construction and Project Management, University College London (UCL), 1-19 Torrington Place, London WC1E 7HB. Email: n.murtagh@ucl.ac.uk 2 Sustainability Research Institute, School of Earth & Environment, University of Leeds, Leeds, LS2 9JT, UK 3 University Centre Scunthorpe, North Lindsey College, Scunthorpe, DN17 1AJ. Now at: School of Design Engineering, Imperial College London, Kensington, London SW7 1AL Acknowledgements Study B data collection was made possible via funding from University Centre Scunthorpe as part of the Association of Colleges Scholarship project under the HEFCE Catalyst fund, and the work of Aaron Flannagan who co-created interview scripts and data collection within a supervised internship. Study C data collection was funded by the Sainsbury’s Charitable Trusts’ Climate Collaboration and the UKRI through the UK Energy Research Centre’s flexible fund project “GLIDER”. The funders had no involvement in any aspect of the research.     \f1. Introduction Homes must become more energy efficient to meet the UK policy target of net-zero CO2 emissions by 2050 [1]. The UK residential sector alone accounts for 22% of end-use greenhouse gas emissions, a contribution which has changed little over the last 15 years [2]. The UK Industrial Strategy Construction Sector Deal [3] set an ambition to halve the emissions of new buildings by 2030, through developing innovative energy and low carbon technologies. The strategic agenda on construction is focused on new-build, with particular interest in modern technologies such as robotics, artificial intelligence, off-site and modular construction methods. However, evidence demonstrates that while UK strategy has tended to focus on new build, the primary problem lies elsewhere. An estimated 87% of existing residential building stock is expected to be in use in 2050 [4]. To meet the legally binding target of net zero CO2 emissions by 2050 [1], energy efficiency measures are required on 25 million existing homes throughout the UK [5]. This is already technologically achievable: for example, high performance insulation and window systems are readily available. But how can essential stakeholders be engaged to deliver the steep decarbonisation required? Focusing on how to engage key actors in the domain of energy-efficiency retrofit, the paper offers a novel theoretical contribution, applying an established framework from the field of behaviour change. We first briefly present an overview of the repair, maintenance and improvement sector. We consider relevant policy initiatives and review previous research in this area, before examining how energy efficiency fits within the sector. We then outline the background to the theoretical framework, COM-B (Capability, Opportunity, Motivation, Behaviour) [6] and provide a short explanation. After presenting the method and findings of the empirical study, the Discussion section considers the implications, forming the basis for recommendations in the Conclusion. 1.1 The repair-maintenance-improvement sector The construction sector in Great Britain is composed primarily of micro and small enterprise builders and tradespeople: 77.1% (1.7 million) are self-employed or work in businesses employing fewer than 50 people, and 39.9% (926,000) are sole operators [7]. Work on the existing housing stock, referred to as Repair-Maintenance-Improvement (RMI), constitutes 17% of construction output [8]. RMI work includes all forms of construction and maintenance activity on existing homes, from window replacement to adding extensions. Factors relating to energy efficiency are not typically a primary objective of RMI work [9] and the individuals already making a living within repair and maintenance may not see benefit in expanding to cover energy-related retrofit. Innovating in the technologies and techniques used in order to ensure homes and buildings become more energy efficient can be a risk for a sole tradesperson or small-to-medium enterprise (SME) who, by contrast, understand well the products they install on a regular basis [10, 11]. In sum, the core actors in the sector may not be motivated to engage with the actions necessary to deliver zero-carbon homes. Previous research on domestic energy retrofit has tended to focus on policy [12], technology [13] and performance [14]. The people involved have been less well-served in academic studies. While there has been more attention paid to householders and building occupants in recent years [15-17], the practitioners themselves have had relatively little focus. There are exceptions, such as a valuable contribution to the literature that distinguished intermediary roles from that of ‘middle actors’ [18], shifting the focus from intermediate organisations involved in policy implementation [19] towards individual practitioners. Some of the few studies which have investigated RMI or retrofit professionals include a study of the perspectives of building performance evaluation practitioners in the UK [20], an investigation of attitudes and approaches of practitioners in the sector in Ireland [21] and barriers to retrofit for the fuel poor in the UK [22]. However, the studies did not clarify if their \fresearch participants were drawn from the micro- and small-enterprises which characterise the sector nor did they draw on established theory. The role of these SMEs, and micro-enterprises in particular, has been largely neglected in policy development debates. Further, their voices are rarely heard and they are often considered to be beyond the reach of policy [23]. This invisibility has been noted too regarding heating engineers, one of the important groups of practitioners influencing domestic refurbishment and often operating as sole traders or in micro-firms [24]. Today’s RMI actors are crucial to the delivery of energy efficiency in the residential building stock but are not well-understood or represented. Nonetheless, builders are a main source of information for householders on possible retrofit options [25, 26] and finding a trusted builder has been found to be a prerequisite for many householders before undertaking retrofit activity [27]. There is evidence too for the effectiveness of shared learning between domestic heating installers and homeowners, particularly where the installers are certified and receive regular information from the certifying organisation [24]. Despite this, previous policies have failed to introduce vocational training schemes for builders, tradespeople and others to equip them with the necessary knowledge relating to energy efficiency. Although the Federation of Master Builders, a body representing the interests of small and medium-sized construction firms, has been involved in guidelines on delivering sustainability in new homes, more generally, UK government policy has been criticised for inhibiting skills development [28]. For example, in 2015, the ‘Each Home Counts’ review was launched to consider issues concerning consumer advice, protection and standards in relation to UK home energy efficiency and renewable measures. Its recommendations to government touched on certification and quality standards, consumer trust and improved training [29]. However, its remit was consumer-focused and did not address the perspectives, attitudes or knowledge needs of the actors who will deliver energy-efficient retrofit. A previous policy package intended to stimulate demand for energy-efficiency refurbishment of UK homes, the Green Deal, was launched in 2013 but closed in 2015 due largely to its failure to attract demand from householders [12]. Many installers who had registered as Green Deal accredited installers then exited the scheme, leading to the collapse of the residential energy efficiency market in the UK [18]. Energy-efficiency programmes are necessary to deliver the government commitment to zero-carbon by 2050 [30] but a lack of trust from builders’ prior experiences or lack of incentive to become involved may demotivate and result in the essential actors working against the desired outcomes. Policy has long recognised the need for substantial change in the construction industry but current industrial strategy for the sector forms part of a wider agenda to advance technological innovation, particularly for new-",
            {
                "entities": [
                    [
                        613,
                        639,
                        "DOI"
                    ]
                ]
            }
        ],
        [
            "Fossil fuel violence and visual practices on Indigenous landCitation for published version:Spiegel, SJ 2021, 'Fossil fuel violence and visual practices on Indigenous land: Watching, witnessing andresisting settler-colonial injustices', Energy Research and Social Science, vol. 79, 102189.https://doi.org/10.1016/j.erss.2021.102189Digital Object Identifier (DOI):10.1016/j.erss.2021.102189Link:Link to publication record in Edinburgh Research ExplorerDocument Version:Publisher's PDF, also known as Version of recordPublished In:Energy Research and Social ScienceGeneral rightsCopyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s)and / or other copyright owners and it is a condition of accessing these publications that users recognise andabide by the legal requirements associated with these rights.Take down policyThe University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorercontent complies with UK legislation. If you believe that the public display of this file breaches copyright pleasecontact openaccess@ed.ac.uk providing details, and we will remove access to the work immediately andinvestigate your claim.Download date: 04. jul.. 2023   Edinburgh Research Explorer                   \fContents lists available at ScienceDirect Energy Research & Social Science journal homepage: www.elsevier.com/locate/erss Fossil fuel violence and visual practices on Indigenous land: Watching, witnessing and resisting settler-colonial injustices Samuel J. Spiegel School of Social and Political Science, University of Edinburgh, 19 George Square, Edinburgh EH8 9LD, Scotland, United Kingdom  A R T I C L E I N F O  A B S T R A C T  Keywords: Oil pipelines Decolonising energy Environmental injustice Indigenous sovereignty Visual politics Counter-watching While controversial plans for fossil fuel pipeline-building continue across Indigenous lands without consent, how are visual practices – including watching and witnessing – serving as modes of resistance? Drawing on a participant-observation ethnography over the 2018–2021 period with environmental defenders on Coast Salish land, in what is colonially called ‘British Columbia, Canada’, this article offers a lens for exploring visual realms of resistance amid expanding extractivism, police surveillance and reconfigured pipeline opposition during the COVID-19 pandemic. Grassroots photography in land-based monitoring, artistic communication in and around courtrooms and other visual practices have been serving as powerful inflection points, countering multiple facets of petro-colonialism – ecological destruction, health threats, and moral and legal transgressions by companies and state institutions. They have also been stimulating new collective actions, some led by Indigenous land protectors extending longstanding traditions of protecting human and non-human life. As ‘more-than-repre-sentational’, visual encounters can be active players in constructing knowledge, challenging structures of dispossession, genocide and ecocide, and cultivating understandings of care, sovereignty, climate justice and anti-colonial solidarity from heterogeneous vantage points. Some environmental defenders’ visual creativities invite deep reflection on ontologies rooted in reciprocity and respect that are thoroughly ignored in extractivist settler-colonial cultures. The article situates visual strategies in fraught political contexts of ramped-up police and corporate surveillance targeting Indigenous land protectors and other environmental defenders, under-scoring critical concern about superficial optical allyship and hollow gestures by state actors responding to racism and state violence on Indigenous land. It calls for attention to dialectical relationships amongst state visual tactics and counter-hegemonic visual practices in struggles to resist colonial energy regimes and to cultivate efforts towards alternative, less destructive energy futures.  1. Introduction: seeing and watching in the age of fossil fuel violence Visual encounters – including practices of seeing, watching, wit-nessing and representing – can play powerful roles in struggles over fossil fuel extraction regimes, reflecting diverse values, interests and cultural processes. Some practices of visual representation mislead, stigmatize and marginalise, reinforcing racist interactions and cultural hegemonies [1-4]. In contrast, however, others might contribute to vi-sual sovereignty and resistance in Indigenous communities amid efforts to oppose unwanted oil pipelines and colonial power, as Brígido-Cor-ach´an (2017) considered in the context of Standing Rock in the United States [5]. Practices of visual resistance constitute important yet often under-explored and under-theorized dimensions of longstanding strug-gles over fossil fuel pipelines, at times as a nexus for re-imagining ideas and discourses of identity across multiple scales [6]. From critical ‘watching back’ against land degradation and colonial oppression as police and corporate surveillance intensify, to the witnessing of envi-ronmental defenders prosecuted in courtrooms, I suggest here a lens through which to understand dynamic fossil fuel contestations and their linkages as well as the reconfigurations during the COVID pandemic. Extending literatures on the use of visual surveillance to criminalize environmental defenders and the performativity of images, this article calls for attention to how visual practices are embedded in the ever- changing dynamics of ongoing colonisation and anti-colonial strug-gles, drawing upon experiences from early 2018 to the beginning of 2021 on Indigenous lands – in what is now colonially called ‘British Columbia’ (by the Government of Canada to describe the western-most province of its federation). My overarching aim here is to explore how critical visual thinking E-mail address: sam.spiegel@ed.ac.uk. https://doi.org/10.1016/j.erss.2021.102189 Received 21 January 2021; Received in revised form 28 June 2021; Accepted 30 June 2021  EnergyResearch&SocialScience79(2021)102189Availableonline16July20212214-6296/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).\fS.J. Spiegel                                                                                                                    around energy regimes may – in some cases – be part of ‘activism that resists settling on colonial ways of knowing place’ [7]. Beyond unpacking discrete visual practices, the interrogating of multiple visual encounters as connected in systems of oppression and resistance offers a lens into dialectical relations that shape hegemonic and counter- hegemonic worlds of seeing. From state surveillance to monitor envi-ronmental defenders who oppose energy megaprojects [8–10] to the advertisement campaigns of the fossil fuel industry [6], visuals often play consequential roles amplifying agendas of corporations and prac-tices of what Finn (2013) calls ‘seeing surveillantly’ [11]. Visual prac-tices may also disorient and re-orient. Works of art may make hidden spaces visible, engendering attention to emotion and affect [12]. Acts of witnessing, watching, seeing and representing are, of course, never just acts; they are embedded in constantly moving relations with diverse possibilities for tapping into colonial and/or anti-colonial politics, and diverse theories of change, ethics and subjectivities at play. There is now growing interest in visual practices as a nexus for seeing how power asymmetries are inscribed in spaces of environmental politics [13]. Yet, while much scholarly writing has addressed fossil fuel impacts by exploring visual representations of climate change and sometimes shocking environmental degradation shown in news media [14], the interplay of multiple (radical as well as less radical) visual practices in contexts of contested fossil fuel projects has, in many milieus, received surprisingly little attention. This article particularly focuses on experiences unfolding on ílwetaʔ/Selilwitulh (Tsleil- CSkwxwú7mesh (Squamish), St´o:l¯o and SelWaututh), xʷmeθkʷey̓ em (Musqueam), Secw´epemc and Wet’suwet’en lands. Its approach focuses on three distinct periods of counter- watching, witnessing and making injustices visible, particularly in re-gard to the Trans Mountain Pipeline Expansion (TMX), a megaproject designed to triple the amount of heavy oil (bitumen) transported from the Alberta tar sands across more than 140 Indigenous Nations’ terri-tory, presenting threats to planetary wellbeing due to climate impacts as well as local health, ecosystems, economies and cultural rights [15]1. While my own field research focused on place-based struggles around TMX, the analysis is contextualized by also exploring the simultaneous efforts of Indigenous-led movements to resist the Coastal Gaslink pipe-line which is set to move fracked gas through the land of the Wet’su-wet’en people further north in BC, in direct affront to their sovereignty [16,17].2 I focus first on the burgeoning anti-pipeline resistance move-ments in BC between March 2018 and January 2020; then on a period of rapid growth of these movements in February 2020 right after mil-itarised police raids and before the COVID pandemic hit; and, finally, on transformations after the pandemic struck in March 2020 – including analysis of the (non)performativity of images and ‘optical allyship’ [20] of state actors in this period. Each of these periods was punctuated by various visual practices that reflect historically situated experiences, with various cultural politics, affects and relations at play. The 2018–2021 era constitutes a critical time span for grappling with systemic racism that propels unjust energy systems [21] and conceptualising, visually, the sometimes more distant 1 A map of the pipeline and timeline of key events in th",
            {
                "entities": [
                    [
                        304,
                        330,
                        "DOI"
                    ],
                    [
                        362,
                        388,
                        "DOI"
                    ],
                    [
                        5981,
                        6007,
                        "DOI"
                    ]
                ]
            }
        ]
    ]
}