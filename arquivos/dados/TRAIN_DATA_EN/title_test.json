{
    "TRAINING_DATA": [
        [
            "Artificial Intelligence 259 (2018) 110–146Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintThe complexity and generality of learning answer set programsMark Law∗, Alessandra Russo, Krysia BrodaDepartment of Computing, Imperial College London, SW7 2AZ, United Kingdoma r t i c l e i n f oa b s t r a c tArticle history:Received 2 September 2016Received in revised form 20 February 2018Accepted 15 March 2018Available online 21 March 2018Keywords:Non-monotonic logic-based learningAnswer Set ProgrammingComplexity of non-monotonic learningTraditionally most of the work in the field of Inductive Logic Programming (ILP) has ad-dressed the problem of learning Prolog programs. On the other hand, Answer Set Program-ming is increasingly being used as a powerful language for knowledge representation and reasoning, and is also gaining increasing attention in industry. Consequently, the research activity in ILP has widened to the area of Answer Set Programming, witnessing the pro-posal of several new learning frameworks that have extended ILP to learning answer set programs. In this paper, we investigate the theoretical properties of these existing frame-works for learning programs under the answer set semantics. Specifically, we present a detailed analysis of the computational complexity of each of these frameworks with re-spect to the two decision problems of deciding whether a hypothesis is a solution of a learning task and deciding whether a learning task has any solutions. We introduce a new notion of generality of a learning framework, which enables us to define a framework to be more general than another in terms of being able to distinguish one ASP hypothesis solution from a set of incorrect ASP programs. Based on this notion, we formally prove a generality relation over the set of existing frameworks for learning programs under answer set se-mantics. In particular, we show that our recently proposed framework, Context-dependent Learning from Ordered Answer Sets, is more general than brave induction, induction of stable models, and cautious induction, and maintains the same complexity as cautious induction, which has the highest complexity of these frameworks.© 2018 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).1. IntroductionOver the last two decades there has been a growing interest in Inductive Logic Programming (ILP) [1], where the goal is to learn a logic program called a hypothesis, which together with a given background knowledge base, explains a set of examples. The main advantage that ILP has over traditional statistical machine learning approaches is that the learned hypotheses can be easily expressed in plain English and explained to a human user, so facilitating a closer interaction be-tween humans and machines. Traditional ILP frameworks have focused on learning definite logic programs [1–6] and normal logic programs [7,8]. On the other hand, Answer Set Programming [9] is a powerful language for knowledge representation and reasoning. ASP is closely related to other declarative paradigms such as SAT, SMT and Constraint Programming, which have each been used for inductive reasoning [10–12]. Compared with these other paradigms, due to its non-monotonicity, ASP is particularly suited for common-sense reasoning [13–15]. Because of its expressiveness and efficient solving, ASP is * Corresponding author at: Department of Computing, Huxley Building, 180 Queen’s Gate, Imperial College London, London, SW7 2AZ, United Kingdom.E-mail address: mark.law09 @imperial .ac .uk (M. Law).https://doi.org/10.1016/j.artint.2018.03.0050004-3702/© 2018 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fM. Law et al. / Artificial Intelligence 259 (2018) 110–146111also increasingly gaining attention in industry [16]; for example, in decision support systems [17], in e-tourism [18] and in product configuration [19]. Consequently, the scope of ILP has recently been extended to learning answer set programs from examples of partial solutions of a given problem, with the intention being to provide algorithms that support automated learning of complex declarative knowledge. Learning ASP programs allows us to learn a variety of declarative non-monotonic, common-sense theories, including for instance Event Calculus [20] theories [21] and theories for scheduling problems and agents’ preference models, both from real user data [22] and from synthetic data [23,24].Learning ASP programs has several advantages when compared to learning Prolog programs. Firstly, when learning Prolog programs, the goal directed SLDNF procedure of Prolog must be taken into account. Specifically, when learning programs with negation, it must be ensured that the programs are stratified, or otherwise the learned program may loop under certain queries. As ASP is declarative, no such consideration need be taken into account when learning ASP programs. A second, more fundamental advantage of learning ASP programs, is that the theory learned can be expressed using extra types of rules that are not available in Prolog, such as choice rules and weak constraints. Learning choice rules allows us to learn non-deterministic concepts; for instance, we may learn that a coin may non-deterministically land on either heads or tails, but never both. This could be achieved by learning the simple choice rule 1{heads, tails}1. Learning choice rules is different from probabilistic ILP settings such as [25–27] where, in similar coins problems the focus would be on learning the probabilities of the two outcomes of are coin. Learning weak constraints enables a natural extension of ILP to preference learning [23], which has resulted to be effective in problem domains such as learning preference models for scheduling [23]and for urban mobility [24].Several algorithms, aimed at learning under the answer set semantics, and different frameworks for learning ASP pro-grams have been recently introduced in the literature. [28] presented the notions of brave induction (I L P b) and cautious induction (I L P c ), based respectively on the well established notions of entailment under the answer set semantics [13,29]of brave entailment (when an atom is true in at least one answer set) and cautious entailment (when and an atom is true in all answer sets). In brave induction, at least one answer set must cover the examples, whereas in cautious induction, every answer set must cover the examples. Brave induction is actually a special case of an earlier learning framework, called in-duction of stable models (I L P sm) [30], in which examples are partial interpretations. A hypothesis is a solution of an induction of stable models task if for each of the example partial interpretations, there is an answer set of the hypothesis combined with the background knowledge, that covers that partial interpretation. Brave induction is equivalent to induction of stable models with exactly one (partial interpretation) example.Each of the above frameworks for learning ASP programs is unable to learn some types of ASP programs [31]; for exam-ple, brave induction alone cannot learn programs containing hard constraints. In [31], we presented a learning framework, called Learning from Answer Sets (I L P L A S ), which unifies brave and cautious induction and is able to learn ASP programs containing normal rules, choice rules and hard constraints. In spite of the increased expressivity, none of the above ap-proaches can learn weak constraints, which are able to capture preference learning. Informally, learning weak constraints consists on identifying conditions for ordering answer sets. The learning task in this case would require examples of or-derings over partial interpretations. To tackle this aspect of learning ASP programs, we have extended the Learning from Answer Sets framework to Learning from Ordered Answer Sets (I L P L O A S ) [23] and demonstrated that our algorithm1 is able to learn preferences in a scheduling domain. More recently, we have extended the I L P L O A S framework to I L P contextL O A S , with context-dependent examples, which come together with extra contextual information [24].In this paper, we explore both the expressive power and the computational complexity of each framework. The former is important, as it allows us to identify the class of problems that each framework can solve, whereas the latter gives an indication of the price paid for using each framework. We characterise the expressive power of a framework in terms of new notions called one-to-one-distinguishability, one-to-many-distinguishability and many-to-many-distinguishability. The intuition of one-to-one-distinguishability is that, given some fixed background knowledge B and sufficient examples, the framework should be able to distinguish a target hypotheses H 1 from another, unwanted, hypotheses H 2. This means that there should be at least one task T (of the given framework) with background knowledge B, such that H 1 is a solution 1(F )) as the set of of T , and H2 is not. We characterise the one-to-one-distinguishability class of a framework F (written D1tuples (cid:3)B, H1, H2(cid:4) for such B’s, H1’s and H2’s, and state that a framework F1 is more D11 -general than another F2 if F2’s one-to-one-distinguishability class is a strict subset of F1’s one-to-one-distinguishability class.One-to-many-distinguishability relates to the task of finding a single target hypothesis from within a set of possi-ble hypotheses. It upgrades the notion of one-to-one-distinguishability classes to one-to-many-distinguishability classes. These are tuples of the form (cid:3)B, H, S(cid:4) for which a framework has at least one task that includes H and none of the (unwanted) hypotheses in S as an inductive solution. Many",
            {
                "entities": [
                    [
                        136,
                        197,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 98 (1998) 237-279 Artificial Intelligence Geometric reasoning about assembly tools Randall H. Wilson * Intelligent Systems and Robotics Center, Sandia National Laboratories, Albuquerque, NM 87185-1008, USA Received January 1997 Abstract Planning for assembly requires reasoning about various tools used by humans, robots, or other automation to manipulate, attach, and test parts and subassemblies. This paper presents a general framework to represent and reason about geometric accessibility issues for a wide variety of such assembly tools. constraints Central to the framework is a use volwne encoding a minimum space that must be free in an on where that volume must be assembly state to apply a given tool, and placement placed relative to the parts on which the tool acts. Determining whether a tool can be applied in a given assembly state is then reduced to an instance of the FINDPLACE problem (Lozano-Perez, 1983). In addition, we present more efficient methods to integrate the framework into assembly planning. For tools that are applied either before or after their target parts are mated, one method preprocesses a single tool application for all possible states of assembly of a product in polynomial time, reducing all later state-tool queries to evaluations of a simple expression. For tools applied after their target parts are mated, a complementary method guarantees polynomial-time assembly planning. We present a wide variety of tools that can be described adequately using the approach, and survey tool catalogs to determine coverage of standard tools. Finally, we describe an implementa- tion of the approach in an assembly planning system and experiments with a library of over one hundred manual and robotic tools and several complex assemblies. @ 1998 Elsevier Science B.V. Keywords: Assembly; Geometric reasoning; Planning; Manufacturing; Robotics * Present address: Eastman Kodak Company, 2901 Juan T&o NE, Suite 210, Albuquerque, NM 87112-1881, USA. Email: Iwilson@kodak.com. 0004-3702/98/$19.00 @ 1998 Elsevier Science B.V. All rights reserved. PIISOOO4-3702(97)00062-3 \f238 R.H. Wihon/Art$cial Intelligence 98 (1998) 237-279 1. Introduction is a tool-using “Man all.” - Thomas Carlyle, Sartor Resartus, book I, 1833. . . . . Without animal tools he is nothing, with tools he is and disassembly of a product for that product. In addition, is a critical assembly planning step can to help designers process important servicing, for assembly, realization Planning in the design supply feedback standpoint. Computer-aided to produce assembly plans while need straints on assembly straints. This paper proposes an approach and describes system. an implementation to manipulate, increasing assembly planning of the approach attach, and test parts and subassemblies plans, methods must be developed improve promises to reduce the design their quality and completeness. from a manufacturing the labor required the con- these con- to cover a wide variety of such constraints planning to reason about in a prototype to important assembly Since leads We focus on representing and reasoning about the geometric in assembly. Assembly tools are implements test parts and subassemblies during the processes of assembly in this sense include manual tools such as screwdrivers and hammers, requirements used to manipulate, of applying attach, and disassembly. robotic items such as laser such as welders and bolt drivers, and a number of other riveters, and part-manipulation traditional robotic cameras, coordinate measuring machines, devices used include these may seem like a diverse group of implements, in more automation. and human they all share in a common way. We call the that allow them to be reasoned about from the need to use various tools in assembly on assembly plans deriving tool constraints. tools various and Tools tooling welders, Inspection eyesight. Although certain aspects constraints or disassembly tools to apply reference constraints in assembly to a canonical tools by whether reference this representation, the tools act. A use volume encodes a minimum We present a framework resulting the tool, and placement constraints determine where frame at the position of required a tool can be applied frame. A particular application the tool acts upon and places the tool’s of to represent and reason about the geometric accessibility in necessary on assembly plans. We begin by they are used before, during, or after mating of the parts space that must be free that tools classifying upon which in a subassembly volume must be placed relative of the tool then defines which parts of a product canonical Given placement that satisfies not collide with any parts in the subassembly. This is an instance of the FINDPLACE problem accessibility their target parts are mated polynomial-time methods assembly of a product, Moreover, extension assembly planning with tools. [ 281. However, a typical assembly planner will make many queries about tool for a single tool application. For tools that are applied either before or after the great majority of tools), we describe for all possible states of tool use. in a given subassembly constraints that are applied after their target parts are mated, we present an to evaluations of a simple expression. to preprocess a single tool application for tools to previous of its use volume exists reducing all later queries only if a and does that guarantees polynomial-time the placement techniques assembly planning includes (which \fR. H. Wilson /Art$cial Intelligence 98 (1998) 237-279 239 While issues, to geometric accessibility for a tools. We present a number of examples and survey catalogs of limited wide variety of assembly typical assembly standard mechanical tools. We also describe in an assembly planning system and show experiments with a library of over one hundred manual and robotic tools and several complex assemblies. tools to determine how well the approach covers the approach provides coverage an implementation of the approach The next section describes tool constraints. and tivates our approach, previous work addressing work for approach constraints Iplanning assembly Finally, Section 9 discusses open issues, and concludes. are evaluated assumptions and describes the problem of planning with tools in greater detail, mo- Section 3 describes this and related problems. Sections 4 and 5 present our frame- the the results of our tool catalog surveys. Section 7 then shows how the into an Section 6 gives examples for single operations as well as efficiently system. Section 8 describes our implementation represented limitations. of tools using and integrated and experiments. future work and several aspects of the approach, describes 2. Problem and motivation “But “Economy”, lo! men have become 1854. the tools of their tools.” - Henry David Thoreau, We are primarily interested in representing and reasoning about assembly tools be- the assembly process and costly mistakes are of- is demanding the constraints and time-consuming, cause of their impact on assembly plans and planning. Planning for a complex product accurately ten made because in complex geometric enough or quickly enough. Determining situations in the design process unless costly (in time and money) prototypes are fabricated. Automated or partially automated meth- these ods problems. plans would help alleviate tool access constraints cannot be reasoned about and resolved for humans early to reason about tool constraints and assembly :is particularly difficult Althouglh assembly planning about tools, we hope areas where assembly visualization. We believe assembly planning on the tool representation that the framework developed here will be applicable tools are used, such as assembly plan simulation, is our main motivation and application area for reasoning to other and than validation, that at least some of these areas will be less demanding and reasoning methods. 2.1. Assembly planning An assembly is a product consisting of two or more independently-fabricated called pan’s; a subassembly is any nonempty assembly plan for a product transforms of an assembly, assembly planning is the problem of determining plan for it. Closely parts into the finished product. Given a complete description a feasible assembly plans and service plans, is a sequence of motions and manipulations to assembly plans are disassembly pieces, subset of the parts of an assembly. An of the parts that the individual related \f240 R.H. Wilson /Artijicial Intelligence 98 (1998) 237-279 the latter being plans for partial disassembly can differ somewhat assembly planning. planning. Most of the methods for these processes, but and re-assembly. The planning techniques in this paper we will mainly discuss and service to disassembly transfer easily In theory, a complete assembly plan down to the factory floor, including it together and full descriptions assembly plans are almost never written down line workers decide process abstractions: they specify the corresponding Automated itself is its only true description. their own body motions complete plan to be determined. planners Instead, includes every detail of the assembly process the motions of all parts, humans, and robots that put of the fixtures, jigs, tools, etc. that are used. In practice, least, the product. The assembly in that much detail; at the very to assemble the assembly plan down to a certain representations of assembly plans are level of detail, and leave assembly therefore must plan they don’t reason about the constraints below their abstraction level of level, abstraction. Because are not feasible. they often produce plans This paper to generate assembly plans in more detail that are therefore more likely to be feasible. Alternatively, the resulting planners will require input in an interactive assembly planning system. are considered, the abstraction hierarchy, as one step further down those constraints for assembly l",
            {
                "entities": [
                    [
                        66,
                        106,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 191–192 (2012) 42–60Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintOn minimal constraint networks ✩Georg GottlobDepartment of Computer Science and Oxford Man Institute, University of Oxford, Oxford OX1 3QD, UKa r t i c l ei n f oa b s t r a c tIn a minimal binary constraint network, every tuple of a constraint relation can beextended to a solution. The tractability or intractability of computing a solution tosuch a minimal network was a long standing open question. Dechter conjectured thiscomputation problem to be NP-hard. We prove this conjecture. We also prove a conjectureby Dechter and Pearl stating that for k (cid:2) 2 it is NP-hard to decide whether a singleconstraint can be decomposed into an equivalent k-ary constraint network. We showthat this holds even in case of bi-valued constraints where k (cid:2) 3, which proves anotherconjecture of Dechter and Pearl. Finally, we establish the tractability frontier for thisproblem with respect to the domain cardinality and the parameter k.© 2012 Elsevier B.V. All rights reserved.Article history:Received 12 May 2012Received in revised form 26 July 2012Accepted 28 July 2012Available online 31 July 2012Keywords:ConstraintsMinimal networkComplexityJoin decompositionStructure identificationDatabase theoryKnowledge compilation1. IntroductionThis paper deals with problems related to minimal constraint networks. First, the complexity of computing a solution toa minimal network is determined. Then, the problems of recognizing network minimality and network-decomposability arestudied.1.1. Minimal constraint networksIn his seminal 1974 paper [26], Montanari introduced the concept of a minimal constraint network. Roughly, a minimalnetwork is a constraint network where each partial instantiation corresponding to a tuple of a constraint relation can beextended to a solution. Each arbitrary binary network N having variables { X1, . . . , X v } can be transformed into an equivalentbinary minimal network M(N) by computing the set sol(N) of all solutions to N and creating for 1 (cid:2) i < j (cid:2) v a constraintci j whose scope is ( Xi, X j) and whose constraint relation consists of the projection of sol(N) to ( Xi, X j), and for 1 (cid:2) i (cid:2) va unary constraint ci whose scope is ( Xi) and whose constraint relation is the projection of sol(N) over ( Xi). The minimalnetwork M(N) is unique, and its solutions are exactly those of the original network, i.e., sol(N) = sol(M(N)).An example of a binary constraint network N is given in Fig. 1(a). This network has four variables X1, . . . , X4 which,for simplicity, all range over the same numerical domain {1, 2, 3, 4, 5}. Its solution, sol(N), which is the join of all relationsof N, is shown in Fig. 1(b). The minimal network M(N) is shown in Fig. 1(c).Obviously, M(N), which can be regarded as an optimally pruned version of N, is hard to compute. But computing M(N)may result in a quite useful knowledge compilation [21,5]. In fact, with M(N) at hand, we can answer a number of queries✩This paper is a significantly extended version of a paper with the same title presented at the 17th International Conference on Principles and Practiceof Constraint Programming (Gottlob, 2011, [17]). The present paper contains new results in addition to those of Gottlob (2011) [17]. Possible future updateswill be made available on CORR at http://arxiv.org/abs/1103.1604.E-mail address: georg.gottlob@cs.ox.ac.uk.0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.07.006\fG. Gottlob / Artificial Intelligence 191–192 (2012) 42–6043Fig. 1. A binary constraint network N, it solution sol(N), and its minimal network M(N).in polynomial time that would otherwise be NP-hard. Typically, these are queries that involve one or two variables only,for example, the queries “is there a solution for which X4 (cid:2) 3?” or “does N have a solution for which X2 < X1?” are affirmativelyanswered by a simple lookup in the relevant tables of M(N). For the latter query, for example, one just has to look intothe first relation table of M(N), whose tuple (cid:2)2, 1(cid:3) constitutes a witness. In contrast, in our example, the query “is therea solution for which X1 < X4?” is immediately recognized to have a negative answer, as the fourth relation of M(N) hasno tuple witnessing this inequality. An example of a slightly more involved non-Boolean two-variable query that can bepolynomially answered using M(N) is: “what is the maximal value of X2 such that X4 is minimum over all solutions?”. Again, onecan just “read off” the answer from the single relation of M(N) whose variables are those of the query. In our example inFig. 1, it is the penultimate relation of M(N), that can be easily used to deduce that the answer is 2.1.2. Computing solutions to minimal constraint networksIn applications such as computer-supported interactive product configuration, such queries arise frequently, but it wouldbe useful to be able to exhibit at the same time a full solution together with the query answer, that is, an assignment ofvalues to all variables witnessing this answer. However, it was even unclear whether the following problem is tractable:Given a non-empty minimal network M(N), compute an arbitrary solution to it. Gaur [11] formulated this as an openproblem. He showed that a stronger version of the problem, where solutions restricted by specific value assignments to apair of variables are sought, is NP-hard, but speculated that finding arbitrary solutions could be tractable. However, since theintroduction of minimal networks in 1974, no one came up with a polynomial-time algorithm for this task. This led Dechterto conjecture that this problem is hard [8]. Note that this problem deviates in two ways from classical decision problems:First, it is a search problem rather than a decision problem, and second, it is a promise problem, where it is “promised” that\f44G. Gottlob / Artificial Intelligence 191–192 (2012) 42–60the input networks, which constitute our problem instances, are indeed minimal – a promise whose verification is itselfNP-hard (see Section 4.1). We therefore have to clarify what NP-hardness means, when referring to such problems. Thesimplest and probably cleanest definition is the following: The problem is NP-hard if any polynomial algorithms solving itwould imply the existence of a polynomial-time algorithm for NP-hard decision problems, and would thus imply P = NP. Inthe light of this, Dechter’s conjecture reads as follows:Conjecture 1.1. (See Dechter [8].) Unless P = NP, computing a single solution to a non-empty minimal constraint network cannot bedone in polynomial time.While the problem has interested a number of researchers, it has not been solved until recently. Some progress was madeby Bessiere in 2006. In his well-known handbook article “Constraint Propagation” [4], he used results of Cros [6] to showthat no backtrack-free algorithm for computing a solution from a minimal network can exist unless the Polynomial Hierarchycollapses to its second level (more precisely, unless Σ p2 ). However, this does not mean that the problem is intractable.2A backtrack-free algorithm according to Bessiere must be able to recognize each partial assignment that is extensible toa solution. In a sense, such an algorithm, even if it computes only one solution, must have the potential to compute allsolutions just by changing the choices of the variable-instantiations made at the different steps. In more colloquial terms,backtrack-free algorithms according to Bessiere must be fair to all solutions. Bessiere’s result does not preclude the existenceof a less general algorithm that computes just one solution, while being unable to recognize all partial assignments, andthus being unfair to some solutions.= Π pThe simple example in Fig. 1, by the way, shows that the following naïve backtrack-free strategy is doomed to fail: Pickan arbitrary tuple from the first relation of M(N), expand it by a suitable tuple of the second relation, and so on. In fact,if we just picked the first tuple (cid:2)1, 1(cid:3) of the first relation, we could combine it with the first tuple (cid:2)1, 1(cid:3) of the secondrelation and obtain the partial instantiation X1 = X2 = X3 = 1. However, this partial instantiation is not part of a solution,as it cannot be expanded to match any tuple of the third relation. While this naïve strategy fails, one may still imagine theexistence of a more sophisticated backtrack-free strategy, that pre-computes in polynomial time some helpful data structurebefore embarking on choices. However, as we show in this paper, such a strategy cannot exist unless NP = P.In the first part of this paper, we prove Dechter’s conjecture by showing that every polynomial-time search algorithm A∗that computes a single solution to a minimal network can be transformed into a polynomial-time decision algorithm Afor the classical satisfiability problem 3SAT. The proof is carried-out in Section 3. We first show that each SAT instance canbe transformed in polynomial time into an equivalent one that is highly symmetric (Section 3.1). Such symmetric instances,which we call k-supersymmetric, are then polynomially reduced to the problem of computing a solution to a minimal binaryconstraint network (Section 3.2). We further consider the case of bounded domains, that is, when the input instances aresuch that the cardinality of the overall domain of all values that may appear in the constraint relation is bounded by somefixed constant c. By a simple modification of the proof of the general case, it is easily seen that even in the bounded domaincase, the problem of computing a single solution remains NP-hard (Section 3.3).Our hardness results for computing relations can be reformulated in terms of database theory. Every constraint net-work N can be seen as a relational database instance,",
            {
                "entities": [
                    [
                        147,
                        177,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 901–934Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintReasoning about preferences in argumentation frameworksSanjay Modgil∗Department of Computer Science, Kings College London, Strand, London WC2R 2LS, UKa r t i c l ei n f oa b s t r a c tArticle history:Received 17 December 2007Received in revised form 23 December 2008Accepted 3 February 2009Available online 7 February 2009Keywords:ArgumentationDungPreferencesNon-monotonic reasoningLogic programmingThe abstract nature of Dung’s seminal theory of argumentation accounts for its widespreadapplication as a general framework for various species of non-monotonic reasoning, and,more generally, reasoning in the presence of conflict. A Dung argumentation frameworkis instantiated by arguments and a binary conflict based attack relation, defined by someunderlying logical theory. The justified arguments under different extensional semanticsare then evaluated, and the claims of these arguments define the inferences of theunderlying theory. To determine a unique set of justified arguments often requires apreference relation on arguments to determine the success of attacks between arguments.However, preference information is often itself defeasible, conflicting and so subjectin this paper we extend Dung’s theory to accommodateto argumentation. Hence,arguments that claim preferences between other arguments, thus incorporating meta-level argumentation based reasoning about preferences in the object level. We then defineand study application of the full range of Dung’s extensional semantics to the extendedframework, and study special classes of the extended framework. The extended theorypreserves the abstract nature of Dung’s approach, thus aiming at a general framework fornon-monotonic formalisms that accommodate defeasible reasoning about as well as withpreference information. We illustrate by formalising argument based logic programmingwith defeasible priorities in the extended theory.© 2009 Elsevier B.V. All rights reserved.1. Introduction1.1. BackgroundThe formal study of argumentation has come to be increasingly central as a core study within Artificial Intelligence [12].Logic based models of argumentation are being applied to formalisation of defeasible reasoning and conflict resolutionover beliefs and goals, and to decision making over actions [1,8,24,25,35]. The inherently dialectical nature of these modelshave provided foundations for formalisation of argumentation-based dialogues [5], where, for example, one agent seeksto persuade another to adopt a belief it does not already hold to be true [33], or when agents deliberate about whatactions to execute [21], or negotiate over resources [4]. Furthermore, recent major research projects [6,7] are developinggeneral models of argumentation based inference, decision making and dialogue, and implementations of these models fordeployment in agent and semantic grid applications.Many of the above theoretical and practical developments build on Dung’s seminal theory of argumentation [19]. A Dungargumentation framework is a directed graph consisting of a set of arguments Args and a binary conflict based attack re-lation R on Args. The extensions, and so justified arguments of a framework are then defined under different semantics,* Tel.: +44 (0)207 8481631.E-mail address: sanjay.modgil@kcl.ac.uk.0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.02.001\f902S. Modgil / Artificial Intelligence 173 (2009) 901–934where the choice of semantics equates with varying degrees of scepticism or credulousness. Extensions are defined throughapplication of an ‘acceptability calculus’, whereby an argument X ∈ Args is said to be acceptable with respect to S ⊆ Args iffany argument Y that attacks X is itself attacked by some argument Z in S (intuitively, any such Z is said to reinstate X ).For example, if S is a maximal (under set inclusion) set such that all its contained arguments are acceptable with respectto S, then S is said to be an extension under the preferred semantics.Recent years have witnessed intensive formal study, development and application of Dungs ideas in various directions.This can be attributed to the abstract nature of a Dung argumentation framework, and the encoding of intuitive, genericprinciples of commonsense reasoning through application of the acceptability calculus. The underlying logic, and defini-tion of the logic’s constructed arguments Args and attack relation R is left unspecified, thus enabling instantiation of aframework by various logical formalisms. A theory’s inferences can then be defined in terms of the claims of the justifiedarguments constructed from the theory (an argument essentially being a proof of a candidate inference – the argument’sclaim – in the underlying logic). Dung’s theory can therefore be understood as a semantics for non-monotonic reasoning. Inthis view, what appropriately accounts for the correctness of an inference is that it can be shown to rationally prevail in theface of opposing inferences, where it is application of the acceptability calculus that encodes logic neutral, rational meansfor establishing such standards of correctness. Indeed, many logic programming formalisms and non-monotonic logics (e.g.default, auto-epistemic, defeasible, non-monotonic modal logics and certain instances of circumscription) have been shownto conform to Dung’s semantics [13,18–20].Dung’s extensional semantics may yield multiple extensions. The sceptically justified arguments are those that appearin every extension. However, one may then be faced with the problem of how to choose between conflicting credulouslyjustified arguments that belong to at least one, but not all extensions. To illustrate, consider two individuals P and Qexchanging arguments A, B . . . about the weather forecast:P1: “Today will be dry in London since the BBC forecast sunshine” = A.Q1: “Today will be wet in London since CNN forecast rain” = B.A and B claim contradictory conclusions and so attack each other (symmetrically attack). Under Dung’s preferred semantics,there are two extensions: { A} and {B}. Neither argument is sceptically justified. One solution is to provide some means forpreferring one argument to another. Some works (e.g., [32,34]) formalise the role of preferences in the underlying logicalformalisms that instantiate a Dung framework. For example, in [32], if X undercuts Y (where ‘undercut’ denotes a certaintype of conflict based interaction), then XRY if Y is not stronger than (preferred to) X . Other approaches formalise therole of preferences at an abstract level. In Amgoud and Cayrol’s Preference based Argumentation Frameworks [3], a Dungframework is augmented with a preference ordering on Args, so that an attack by X on Y is successful only if Y is notpreferred to X . In Bench-Capon’s Value based Argumentation Frameworks (VAFs) [11], a Dung framework is augmented withvalues and value orderings, so that an attack by X on Y is successful only if the value promoted by Y is not ranked higherthan the value promoted by X according to a given ordering on values.Examining the role of preferences in the above weather example, one might reason that A is preferred to B because theBBC are deemed more trustworthy than CNN. Hence B does not successfully attack A, and so this attack can be removedand we are only left with the successful attack from A to B. Thus only { A} will be a preferred extension; A is scepticallyjustified. This example illustrates resolution of an argumentation framework obtained by replacing symmetric attacks withasymmetric attacks. Properties relating frameworks and their resolutions have been studied in [9] and [26]. Properties offrameworks obtained through removal of asymmetric attacks have also been studied in the context of VAFs.1.2. Overview of paperInformation required to determine the success of an attack is often assumed pre-specified, as a given preference or valueordering. However, preference information may be contradictory, given that preferences may vary according to context, anddistinct sources may valuate the strengths of arguments using different criteria, or indeed assign different valuations for anygiven criterion. Thus, one often needs to reason, and indeed argue about, as well as with, defeasible and possibly conflictingpreference information. To illustrate, consider continuation of the above dialogue about the weather:P2: “But the BBC are more trustworthy than CNN” = C .Q2: “However, statistically CNN are more accurate forecasters than the BBC” = CQ3: “And basing a comparison on statistics is more rigorous and rational than basing a comparison on your instinctsabout their relative trustworthiness” = E..(cid:5)(cid:5)expresses a preference for B over A. C and C(cid:5)Argument C is not an argument that attacks B; rather it is an argument expressing a preference for A over B. However,attack each other since they express contradictory preferences. Finally,Csuccessfully attacks C . We thus have a sceptically justifiedargument E claims that Cargument Cclaiming that B is preferred to A, and so B is sceptically justified.is preferred to C , and so only C(cid:5)(cid:5)(cid:5)\fS. Modgil / Artificial Intelligence 173 (2009) 901–934903In this paper we are thus motivated to extend Dung’s theory to accommodate argumentation about preferences. Thepaper is organised as follows. In Section 2 we review Dung’s theory of argumentation. In the sections that follow wedescribe our contributions to the formal study and application of argumentation:• In Sections 3 and 4, Dung’s theory of argumentation is extended to integrate ‘metalevel’ argumentation about pref-erences between arguments. The extended theory preserves the abstract nature of Dung’s approach. Arguments expressingpreferences (preference arguments) are simply nodes in a graph, and application of preferences is abstra",
            {
                "entities": [
                    [
                        136,
                        191,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 228 (2015) 1–44Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA new probabilistic constraint logic programming language based on a generalised distribution semantics ✩Steffen Michels a,∗a Radboud University, Institute for Computing and Information Sciences, Nijmegen, The Netherlandsb Embedded Systems Innovation by TNO, The Netherlands, Arjen Hommersom a, Peter J.F. Lucas a, Marina Velikova ba r t i c l e i n f oa b s t r a c tArticle history:Received 7 April 2014Received in revised form 2 May 2015Accepted 26 June 2015Available online 4 July 2015Keywords:Probabilistic logic programmingImprecise probabilitiesContinuous probability distributionsExact probabilistic inferenceProbabilistic logics combine the expressive power of logic with the ability to reason with uncertainty. Several probabilistic logic languages have been proposed in the past, each of them with their own features. We focus on a class of probabilistic logic based on Sato’s distribution semantics, which extends logic programming with probability distributions on binary random variables and guarantees a unique probability distribution. For many applications binary random variables are, however, not sufficient and one requires random variables with arbitrary ranges, e.g. real numbers. We tackle this problem by developing a generalised distribution semantics for a new probabilistic constraint logic programminglanguage. In order to perform exact inference, imprecise probabilities are taken as a starting point, i.e. we deal with sets of probability distributions rather than a single one. It is shown that given any continuous distribution, conditional probabilities of events can be approximated arbitrarily close to the true probability. Furthermore, for this setting an inference algorithm that is a generalisation of weighted model counting is developed, making use of SMT solvers. We show that inference has similar complexity properties as precise probabilistic inference, unlike most imprecise methods for which inference is more complex. We also experimentally confirm that our algorithm is able to exploit local structure, such as determinism, which further reduces the computational complexity.© 2015 Elsevier B.V. All rights reserved.1. IntroductionProbabilistic logics are gaining popularity as modelling and reasoning tools, since they combine the power of logic to represent knowledge with the ability of probability theory to deal with uncertainty. In addition, in the field of statistical relational learning (SRL) [1], powerful machine learning methods have been developed using probabilistic logical languages as their basis.The need for those methods emerges from the fact that in many areas more and more data become available, which does not only imply uncertainty, but often provides rich structure in terms of relations between entities. Probabilistic logics and SRL methods have been applied to a wide range of problem domains. Examples include: link and node prediction in This publication was supported by the Dutch national program COMMIT. The research work was carried out as part of the Metis project under the ✩responsibility of Embedded Systems Innovation by TNO, with Thales Nederland B.V. as the carrying industrial partner.* Corresponding author.E-mail addresses: s.michels@science.ru.nl (S. Michels), arjenh@cs.ru.nl (A. Hommersom), peterl@cs.ru.nl (P.J.F. Lucas), marina.velikova@tno.nl(M. Velikova).http://dx.doi.org/10.1016/j.artint.2015.06.0080004-3702/© 2015 Elsevier B.V. All rights reserved.\f2S. Michels et al. / Artificial Intelligence 228 (2015) 1–44metabolic networks [2], discovering interactions between genes [3], dealing with a potentially unknown number of relations between multiple objects by a robot [3] and fusing information about vessels on the North Sea by modelling relations between those objects and heterogeneous pieces of information [4].Combining logic and probability theory is challenging and involves dealing with a trade-off between expressivity and efficiency of inference. The research described in this article focuses on probabilistic logics adhering to Sato’s distribution semantics [5]. This semantics guarantees a single unambiguously defined probability distribution and supports efficient in-ference. In this semantic framework, logic programming (LP) is used to define a probability distribution over a set of binary facts. Examples of languages based on that semantics are ProbLog [6], PRISM [7], ICL [8], and CP-logic [9]. The choice for this kind of semantics is motivated by the fact that it allows one to use probabilities with local meaning, which provides the modularity needed for knowledge representation, similar to the widely used Bayesian networks [10]. Some alternatives, in particular the popular Markov Logic Networks, make use of weights that only can be interpreted in the context of the entire theory [11].For many real-world knowledge-representation problems binary random variables are not convenient or not sufficient; very often, random variables taking values within arbitrary ranges are needed, e.g. integers and real numbers. In fact, virtually all deterministic real-world models include such variables. Examples are: data models involving the age or height of persons, temporal models that use integer or real-valued time, and physical models expressing most quantities as real numbers. Since in all such domains uncertainty is present, the provision of a probabilistic representation of such models is essential. Furthermore, most domains are typically relational. An example involving relations and uncertainty, as well as real-valued variables, is inferring a map of the indoor environment based on observations by robots [12].While finite-ranged discrete random variables can be represented by sets of binary random variables, random variables with an infinite number of values, such as continuous variables, have significant impact on the semantics of probabilistic logics and the complexity of the inference problem. Some previous attempts to extend probabilistic logic programming with real-valued variables heavily restrict the use of such variables [13], such that they are not powerful enough to model many real world problems. Other approaches resort to approximate methods for inference, such as sampling [14,12]. For many problems current sampling techniques are effective, but such approaches may fail and guarantees about the result’s quality are weak. This is especially problematic for problems in which wrong decisions have a huge impact.In this article, we propose an alternative to using exact inference and sampling with some inherent advantages; we can represent distributions for which exact inference is infeasible and at the same time provide more guarantees than offered by existing approximation methods. We provide a powerful, general theoretic foundation for probabilistic logic programs, which we refer to as a generalised distribution semantics, and a practically useful language based on the semantic framework. The theory supports approximating probability distributions with arbitrarily-ranged random variables, both continuous and discrete, and is equipped with an efficient method for inference.The main contribution of our work is an expressive logical language that defines events in terms of constraints on the random variable’s values, e.g. inequalities on real-valued random variables. The logical part of the language is inspired by constraint logic programming (CLP) [15]. To allow exact computation of probabilities, we make probabilities imprecise by introducing credal sets on top of the generalised distribution semantics. Given these credal sets, it is ensured that the bounds on marginals can be computed exactly. Moreover, by virtue of the distribution semantics, this new framework also ensures that there is always at least one consistent distribution, in contrast to some other probabilistic logics with imprecisions, e.g. the probabilistic logic of Nilsson [16].Based on this semantics we introduce a new probabilistic constraint logic programming (PCLP) language, in which inde-pendent probability distributions of random variables are defined by means of a set of probability-constraint pairs. Similar to deterministic constraint logic programming (CLP) [15], PCLP is a family of languages allowing one to use arbitrary con-straint theories, unlike most formalisms which are restricted to certain kinds of random variables, e.g. finite discrete and real-valued ones. This is possible thanks to the general semantic foundation. As examples we discuss discrete constants (PCLP(D)) and real numbers (PCLP(R)). To our knowledge this is the first usable framework that combines imprecise prob-abilities with continuous variables. An earlier version of the language was already presented at a previous occasion [17]. In the present paper we place the language in the context of the general semantic foundation and provide proofs of more general properties.We finally present an inference algorithm that is a generalisation of recently emerged probabilistic inference methods based on translating inference problems to weighted model counting (WMC) problems. This has been shown to be an effi-cient inference method for propositional formalisms such as Bayesian networks [18]. In addition, it has been shown to be applicable to probabilistic logics based on distribution semantics [19]. We show that exact inference in our new language can be dealt with by a generalisation of this method.This paper also shows that inference in our framework has similar complexity characteristics as precise probabilistic inference. In fact, we show that it is in the same complexity class as precise inference, unlike most other imprecise ap-proaches. Furthermore, the complexity can be bounded in terms of the problem structure, similar to WMC. Our generalised framework can also exploit additional structur",
            {
                "entities": [
                    [
                        133,
                        236,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 236 (2016) 30–64Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintDomain-independent planning for services in uncertain and dynamic environmentsEirini Kaldeli a,b, Alexander Lazovik b, Marco Aiello ba School of Electrical and Computer Engineering, National Technical University of Athens, Zographou Campus, 15780 Athens, Greeceb Johann Bernoulli Institute, University of Groningen, Nijenborgh 9, 9747 AG Groningen, The Netherlandsa r t i c l e i n f oa b s t r a c tArticle history:Received 12 May 2014Received in revised form 6 March 2016Accepted 9 March 2016Available online 14 March 2016Keywords:AI planningWeb service compositionResearch in automated planning provides novel insights into service composition and contributes towards the provision of automatic compositions which adapt to changing user needs and environmental conditions. Most of the existing planning approaches to aggregating services, however, suffer from one or more of the following limitations: they are not domain-independent, cannot efficiently deal with numeric-valued variables, especially sensing outcomes or operator inputs, and they disregard recovery from runtime contingencies due to erroneous service behavior or exogenous events that interfere with plan execution. We present the RuGPlanner, which models the planning task as a Constraint Satisfaction Problem. In order to address the requirements put forward by service domains, the RuGPlanner is endowed with a number of special features. These include a knowledge-level representation to model uncertainty about the initial state and the outcome of sensing actions, and efficient handling of numeric-valued variables, inputs to actions or observational effects. In addition, it generates plans with a high level of parallelism, it supports a rich declarative language for expressing extended goals, and allows for continual plan revision to deal with sensing outputs, failures, long response times or timeouts, as well as the activities of external agents. The proposed planning framework is evaluated based on a number of scenarios to demonstrate its feasibility and efficiency in several planning domains and execution circumstances which reflect concerns from different service environments.© 2016 Elsevier B.V. All rights reserved.1. IntroductionSoftware service infrastructures enable the large scale integration of heterogeneous systems and solve a number of interoperability issues. A prototypical example is that of Web Services (WS) where programmatic access to Web resources is guaranteed via standardized XML interfaces, such as those defined by the Web Service Description Language (WSDL) (www.w3.org/TR/wsdl). Automated planning can contribute to the realization of service infrastructures that go beyond basic interoperation and ad hoc process specifications, offering highly automated functionalities that are adaptable to changing user needs and environmental conditions. The goal is to compose and interact automatically with several service providers in order to offer value added functionalities. More precisely, a service composition is a combination of operations provided by different services to satisfy complex objectives which cannot be fulfilled by a single service instance. Planning is the process of “choosing and organizing actions by anticipating their expected outcomes,” with the aim of achieving a pre-stated E-mail address: kaldeli@gmail.com (E. Kaldeli).http://dx.doi.org/10.1016/j.artint.2016.03.0020004-3702/© 2016 Elsevier B.V. All rights reserved.\fE. Kaldeli et al. / Artificial Intelligence 236 (2016) 30–6431goal [42]. The analogies between the problem of Web Service composition (WSC) and automated planning are evident and have been exploited before (e.g., [1,108,98,110,15]): actions correspond to functionalities offered by different services, and the goal is derived from a user request or inferred by a situation that calls for a combination of services.The composition method advocated herein is driven by the general aim of combining services automatically and on-demand, relying solely on individual descriptions of loosely-coupled software components, and a declarative goal language. The idea is to maintain a generic and modular repository that comprises a number of atomic service operations, from book-ing flights to arranging appointments with a doctor, and that can serve a variety of objectives with minimal request-specific configuration. We use domain-independent planning and propose an extended language for expressing complex goals in a declarative fashion, detached from the particularities and interdependencies of the available services. This is unlike most previous approaches that restrict the applicability of the domain to a set of anticipated user needs, predefined in the form of some procedural template, e.g., [90,108].1.1. Characteristics of service domainsService domains are data intensive, i.e., they deal with variables ranging over very large domains, such as prices, dates, product quantities, and so on. If the behavior of a service operation is to be modeled by a planning operator, then this must quite frequently involve fluents with numeric-valued input arguments. For example, an operation to reserve an airline ticket is parametrized by data associated with the flight. Whether or not the application of such an operation has the intended effect depends on the choice of the correct value of the input parameters. One must also take into account numeric properties (e.g., the temperature must be greater than 0) both in preconditions and in the goal, and be able to apply arithmetic operations (e.g., increase an account balance by a certain amount). Things become more complicated when one considers that numeric information is very often the output of sensing operations. Due to the many operators involving variables taking values in domains with large cardinality, either as input parameters or as outputs, the number of ground operators and, as a consequence, the size of the search space can increase enormously.In a service environment, the planner must deal with uncertainty about the initial state, i.e., consider that there is a number of possibilities about the actual values of certain variables. The actual value of an unknown variable can be returned after the invocation of a sensing operator, referred to as knowledge-gathering or observational, which returns exactly one outcome out of a (possibly very large) set of choices. In fact, marketplaces consisting of services publicly available on the Web are usually dominated by services that are data sources [31], which in a planning context are modeled as sensing operators. A successful plan may be conditioned on the outcomes of such actions, e.g., the user may want to go ahead with buying a book from amazon.com if this costs less than a maximum price. In a domain-independent setting, the planning agent is required to plan for sensing; the planner should be able to identify which knowledge it lacks for satisfying the goal, and reason about how to find it, instead of relying on pre-specified queries [110,76,4]. The planner should also proactively see to the data flow of the plan, i.e., the way the service return values are used by subsequent actions. For example, a user may want to mail a parcel to someone whose address he does not already know. The plan should thus first consult a white pages service, and then give the order for posting by passing the right address value to the respective input parameter. For data intensive service domains, determining the parameters for an action can be just as difficult as determining which actions belong to the plan. Since almost all state-of-the-art planners resort to some kind of pre-processing for compiling the PDDL [87] domain into a fully grounded encoding, on-the-fly handling of runtime outputs is difficult to implement.Uncertainty applies, however, not only to the initial state and the non-determinism of the many possible values of the outputs of sensing actions. In fact, a service invocation may behave in unexpected ways: i mat return a failure, not respond at all, or even act in a way different from the one prescribed by its description. The actual outcomes of a service invocation only become visible at runtime, when the plan is exposed to the actual environmental conditions. Thus, the problem of uncertainty is directly related to the interaction between planning and execution.The state of a service domain may not only change as a result of the deliberate actions executed by the planning agent, but also due to the activity of other exogenous agents which are active in the same environment. The activity of other actors may have repercussions for the composition-plan under execution, and render it invalid, either because information own which it relies has meanwhile become obsolete, or because certain scheduled actions are no longer applicable under the new circumstances. For example, considering a partially executed composition which involves circumnavigation of a robot, if an external actor puts an obstacle in the robot’s way in the middle of execution, the robot may fall unless it revises its decisions about how to move. With a few notable exceptions [4,17,73], context dynamicity has largely been overlooked by existing planning approaches to WSC. Moreover, in many service environments, dynamicity also applies to the availability of information and services; e.g., the services offered by a mobile phone may appear and disappear depending on the location of the phone.1.2. ApproachWe choose to work directly at the schematic level of the planning domain, i.e., without performing any grounding. Following the so-called Multi-Valued Task (MPT) paradigm [53], we use state variables rather than predicates as the basic elements for describing the world. According to that view, a state is a tuple of values to variables, leading to a more",
            {
                "entities": [
                    [
                        134,
                        212,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 246 (2017) 53–85Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintHierarchical semi-Markov conditional random fields for deep recursive sequential dataTruyen Tran a,∗a Center for Pattern Recognition and Data Analytics, Deakin University Geelong, Australiab Adobe Research, Adobe, USA, Dinh Phung a, Hung Bui b, Svetha Venkatesh aa r t i c l e i n f oa b s t r a c tArticle history:Received 20 January 2015Received in revised form 12 February 2017Accepted 14 February 2017Available online 24 February 2017Keywords:Deep nested sequential processesHierarchical semi-Markov conditional random fieldPartial labellingConstrained inferenceNumerical scaling1. IntroductionWe present the hierarchical semi-Markov conditional random field (HSCRF), a generalisation of linear-chain conditional random fields to model deep nested Markov processes. It is parameterised as a conditional log-linear model and has polynomial time algorithms for learning and inference. We derive algorithms for partially-supervised learning and constrained inference. We develop numerical scaling procedures that handle the overflow problem. We show that when depth is two, the HSCRF can be reduced to the semi-Markov conditional random fields. Finally, we demonstrate the HSCRF on two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. The HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases.© 2017 Elsevier B.V. All rights reserved.Modelling hierarchical depth in complex stochastic processes is important in many application domains. In a deep hier-archy, each level is an abstraction of lower level details [1–4]. This paper studies recursively sequential processes, in that each level is a sequence and each node in a sequence can be decomposed further into a sub-sequence of finer grain [2].Consider, for example, a frequent activity performed by human ‘eat-breakfast’. It may include a series of more specific activities like ‘enter-kitchen’, ‘go-to-cupboard’, ‘take-cereal’, ‘wash-dishes’ and ‘leave-kitchen’. Each specific activity can be decomposed into finer details. Similarly, in natural language processing (NLP) syntax trees are inherently hierarchical. In a partial parsing task known as noun-phrase (NP) chunking [5], there are three syntactic levels: the sentence, noun-phrases and part-of-speech (POS) tags. In this setting, the sentence is a sequence of NPs and non-NPs, and each phrase is a sub-sequence of POS tags.A popular approach to deal with hierarchical data is to build a cascaded model where each level is modelled separately, and the output of the lower level is used as the input of the level right above it (e.g. see [6]). For instance, in NP chunking this approach first builds a POS tagger and then constructs a chunker that incorporates the output of the tagger. This approach is sub-optimal because the POS tagger takes no information of the NPs and the chunker is not aware of the reasoning of the tagger. In contrast, a noun-phrase is often very informative to infer the POS tags belonging to the phrase. As a result, this layered approach may suffer from the so-called cascading error problem in that errors introduced from the lower layer propagate to higher levels.* Corresponding author.E-mail address: truyen.tran@deakin.edu.au (T. Tran).http://dx.doi.org/10.1016/j.artint.2017.02.0030004-3702/© 2017 Elsevier B.V. All rights reserved.\f54T. Tran et al. / Artificial Intelligence 246 (2017) 53–85A more holistic approach is to build a joint representation of all the levels. Formally, given a data sequence z we need to model and infer about the deep, nested semantic x. The main problem is to choose an appropriate representation of xso that inference can be efficient. An important class of representation is hierarchical hidden Markov model (HHMM) [2]. An HHMM is a nested hidden Markov network (HMM) in the sense that each state is also a sub HMM. Although HMMs represent only first-order Markov processes, HHMMs offer higher-order interaction. HHMMs are generative models with joint distribution Pr(x, z), where the data generating distribution Pr(z | x) must be simplified for efficient inference about the semantic Pr(x | z). An alternative is to model the discriminative distribution Pr(x | z) directly without modelling the data Pr(z). This can be more effective since arbitrary long-range and interdependent data features can be incorporated into the model.The most popular class of probabilistic structured output methods are conditional random fields (CRFs) [7], but the early models are flat. Deep variants have been introduced the past decade, including dynamic CRFs (DCRF) [8], hierarchical CRFs [9,10]), and stacked CRFs [11]. However, these methods require a fixed pre-defined hierarchy, and thus are not suitable for problems with automatically inferred topologies.To this end, we construct a novel discriminative model called Hierarchical Semi-Markov Conditional Random Field (HSCRF).1The HSCRF offers nested semantic similar to that by the HHMM but is parameterised as an undirected log-linear model. The HSCRF generalises linear-chain CRFs [7] and semi-Markov CRFs [13].To be more concrete, let us return to the NP chunking example. The problem can be modelled as a three-level HSCRF, where the root represents the sentence, the second level the NP process, and the bottom level the POS process. The root and the two processes are conditioned on the sequence of words in the sentence. Under discriminative modelling, rich contextual information can be simply encoded as features including starting and ending of a phrase, phrase length, and distribution of words falling inside the phrase can be effectively encoded. On the other hand, such encoding is much more difficult for HHMMs.We then proceed to address important issues. First, we show how to represent HSCRFs using a dynamic graphical model (e.g. see [14]) which effectively encodes hierarchical and temporal semantics. For parameter learning, an efficient algorithm based on the Asymmetric Inside–Outside of [15] is introduced. For inference, we generalise the Viterbi algorithm to decode the semantics from an observational sequence.The common assumptions in discriminative learning and inference are that the training data in learning is fully labelled, and the test data during inference is not labelled. We propose to relax these assumptions in that training labels may only be partially available. Likewise, when some labels are given during inference, the algorithm should automatically adjust to meet the new constraints.We demonstrate the effectiveness of HSCRFs in two applications: (i) segmenting and labelling activities of daily living (ADLs) in an indoor environment and (ii) jointly modelling noun-phrases and part-of-speeches in shallow parsing. Our experimental results in the first application show that the HSCRFs are capable of learning rich, hierarchical activities with good accuracy and exhibit better performance when compared to DCRFs and flat-CRFs. Results for the partially supervised case also demonstrate that significant reduction of training labels still results in models that perform reasonably well. We also show that observing a small amount of labels can significantly increase the accuracy during decoding. In shallow parsing, the HSCRFs can achieve higher accuracy than standard CRF-based techniques and the recent DCRFs.To summarise, in this paper we claim the following contributions:• Introducing a novel Hierarchical Semi-Markov Conditional Random Field (HSCRF) to model complex hierarchical and nested Markovian processes in a discriminative framework.• Developing an efficient generalised Asymmetric Inside–Outside (AIO) algorithm for full-supervised learning.• Generalising the Viterbi algorithm for decoding the most probable semantic labels and structure given an observational sequence.• Addressing the problem of partially-supervised learning and constrained inference.• Constructing a numerical scaling algorithm to prevent numerical overflow.• Demonstration of the applicability of the HSCRFs for modelling human activities in the domain of home video surveil-lance and shallow parsing of English.The rest of the paper is organised as follows. Section 2 reviews Conditional Random Fields and Hierarchical Hidden Markov Models. Section 3 continues with the HSCRF model definition. Section 4 defines building blocks required for common in-ference tasks. Section 5 presents the generalised Viterbi algorithm. Parameterisation and estimation follow in Section 6. Learning and inference with partially available labels are addressed in Section 7. Section 8 presents a method for numerical scaling to prevent numerical overflow. Section 9 documents experimental results. Section 11 concludes the paper.1 Preliminary version was published in NIPS’08 [12].\fT. Tran et al. / Artificial Intelligence 246 (2017) 53–8555Table 1Notations used in this paper.NotationDescription(cid:3)(cid:3)xd:di: jed:di: jζ d,si: jci, j, tτ dr, s, u, v, wRd,s,zi: jπ d,su,iAd,s,zu,v,iEd,s,zu,i(cid:5) [ζ, z]Sd(cid:6)d,si: jˆ(cid:6)d,si: j(cid:7)d,si: jˆ(cid:7)d,si: jαd,si: j (u)λd,si: j (u)δ [·], I [·]ψ(·),ϕ(·)(cid:3)and starting from time i and ending at time j, inclusive(cid:3)Subset of state variables from level d down to level dSubset of ending indicators from level d down to level dSet of state variables and ending indicators of a sub model rooted at sd, level d, spanning a sub-string [i, j]Contextual cliqueTime indicesSet of all ending time indices, e.g. if i ∈ τ d then edand starting from time i and ending at time j, inclusive= 1iStateState-persistence potential of state s, level d, spanning [i, j]Initialisation potential of state s at level d, time i initialising sub-state uTransition at level d, time i from state u to v under the same pare",
            {
                "entities": [
                    [
                        134,
                        219,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 175 (2011) 165–192Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintIterated belief change in the situation calculus ✩Steven Shapiro a,∗, Maurice Pagnucco b, Yves Lespérance c, Hector J. Levesque aa Department of Computer Science, University of Toronto, Toronto, ON M5S 3G4, Canadab ARC Centre of Excellence for Autonomous Systems and National ICT Australia, School of Computer Science and Engineering, The University of New South Wales,Sydney, NSW 2052, Australiac Department of Computer Science and Engineering, York University, Toronto, ON M3J 1P3, Canadaa r t i c l ei n f oa b s t r a c tArticle history:Available online 3 April 2010Keywords:Knowledge representation and reasoningReasoning about action and changeSituation calculusBelief changeintelligenceJohn McCarthy’s situation calculus has left an enduring mark on artificialresearch. This simple yet elegant formalism for modelling and reasoning about dynamicsystems is still in common use more than forty years since it was first proposed. The abilityto reason about action and change has long been considered a necessary component forany intelligent system. The situation calculus and its numerous extensions as well as themany competing proposals that it has inspired deal with this problem to some extent. Inthis paper, we offer a new approach to belief change associated with performing actionsthat addresses some of the shortcomings of these approaches. In particular, our approach isbased on a well-developed theory of action in the situation calculus extended to deal withbelief. Moreover, by augmenting this approach with a notion of plausibility over situations,our account handles nested belief, belief introspection, mistaken belief, and handles beliefrevision and belief update together with iterated belief change.© 2010 Elsevier B.V. All rights reserved.The work of John McCarthy has had a profound and lasting effect on artificial intelligence research. One of his moreenduring contributions has been the introduction of the situation calculus [2,3]. This simple yet elegant formalism for mod-elling and reasoning about dynamic systems is still in common use more than forty years since it was first proposed. Theability to reason about action and change has long been considered a necessary component for any intelligent system. Anagent acting in its environment must be capable of reasoning about the state of its environment and keeping track of anychanges to the environment as actions are performed. Various theories have been developed to give an account of howthis can be achieved. Foremost among these are theories of belief change and theories for reasoning about action. Whileoriginating from different motivations, the two are united in their aim to have agents maintain a model of the environmentthat matches the actual environment as closely as possible given the available information. An important consideration isthe ability to deal with a succession of changes; known as the problem of iterated belief change.In this paper, we consider a new approach for modelling iterated belief change using the language of the situationcalculus [2,3]. While our approach is in some ways limited in its applicability, we feel that it is conceptually very simpleand offers a number of useful features not found in other approaches:• It is completely integrated with a well-developed theory of action in the situation calculus [4] and its extension tohandle knowledge expansion [5,6]. Specifically, the manner in which beliefs change in our account is simply a special✩An earlier version of this paper appeared in Shapiro et al. (2000) [1].* Corresponding author.E-mail addresses: steven@cs.toronto.edu (S. Shapiro), morri@cse.unsw.edu.au (M. Pagnucco), lesperan@cse.yorku.ca (Y. Lespérance), hector@ai.toronto.edu(H.J. Levesque).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.003\f166S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192case of how other fluents change as the result of actions, and thus among other things, we inherit a solution to theframe problem.• Like Scherl and Levesque [5,6], our theory accommodates both belief update and belief expansion. The former concernsbeliefs that change as the result of the realization that the world has changed; the latter concerns beliefs that changeas the result of newly acquired information.• Unlike Scherl and Levesque, however, our theory is not limited to belief expansion; rather it deals with the more generalcase of belief revision. It will be possible in our model for an agent to believe some formula φ, acquire information thatcauses it to change its mind and believe ¬φ (without believing the world has changed), and later go back to believing φagain. In Scherl and Levesque and in other approaches based on this work such as [7,8], new information that contradictsprevious beliefs cannot be consistently accommodated.• Because belief change in our model is always the result of action, our account naturally supports iterated belief change.This is simply the result of a sequence of actions. Moreover, each individual action can potentially cause both an update(by changing the world) and a revision (by providing sensing information) in a seamless way.• Like Scherl and Levesque and unlike many previous approaches to belief change, e.g., [9,10], our approach supportsbelief introspection: an agent will know what it believes and does not believe. Furthermore, it has information aboutthe past, and so will also know what it used to believe and not believe. Finally, an agent will be able to predict what itwill believe in the future after it acquires information through sensing.• Unlike Scherl and Levesque, our agents will be able to introspectively tell the difference between an update and arevision as they move from believing φ to believing ¬φ. In the former case, the agent will believe that it believed φ inthe past, and that it was correct to do so; in the latter case, it will believe that it believed φ in the past but that it wasmistaken.• One important lesson learned is that not only does our method for iterated belief change in the situation calculuspossess interesting properties but attempting to use more sophisticated schemes that involve modifying plausibilities ofpossible worlds, leads to unintuitive introspection properties when applied to situations.The rest of the paper is organized as follows: in the next section, we briefly review the situation calculus including theScherl and Levesque [5,6] model of belief expansion, and we review the most popular accounts of belief revision, beliefupdate, and iterated belief change; in Section 3, we motivate and define a new belief operator as a modification to the oneused by Scherl and Levesque; in Section 4, we prove some properties of this operator, justifying the points made above;in Section 5, we show the operator in action on a simple example, and how an agent can change its mind repeatedly;in Section 6, we analyze the extent to which our framework satisfies revision, update, and iterated revision postulates; inSection 7, we compare our framework to some of the existing approaches to belief change; and in the final section, we drawsome conclusions and discuss future work.1. BackgroundThe basis of our framework for belief change is an action theory [4] based on the situation calculus [2,3], and extendedto include a belief operator [5,6]. In this section, we begin with a brief overview of the situation calculus and follow it witha short review of belief change in sufficient detail to understand the contributions made in this paper.1.1. The situation calculusThe situation calculus is a predicate calculus language for representing dynamically changing domains. A situation repre-sents a snapshot of the domain. There is a set of initial situations corresponding to the ways the agent1 believes the domainmight be initially. The actual initial state of the domain is represented by the distinguished initial situation constant, S 0,which may or may not be among the set of initial situations believed possible by the agent. The term do(a, s) denotes theunique situation that results from the agent performing action a in situation s. Thus, the situations can be structured into aset of trees, where the root of each tree is an initial situation and the arcs are actions.Predicates and functions whose value may change from situation to situation (and whose last argument is a situation)are called fluents. For instance, we use the fluent InR1(s) to represent that the agent is in room R1 in situation s. The effectsof actions on fluents are defined using successor state axioms [4], which provide a succinct representation for both effectaxioms and frame axioms [2,3]. For example, assume that there are only two rooms, R 1 and R2, and that the action leavetakes the agent from the current room to the other room. Then, the successor state axiom for InR1 is2:(cid:2)InR1(s) ∧ a (cid:6)= leaveThis axiom asserts that the agent will be in R1 after doing some action if and only if either the agent is in R2 (¬InR1(s))and leaves it or the agent is currently in R1 and the action is anything other than leaving it.(cid:3)¬InR1(s) ∧ a = leave(cid:3)do(a, s)InR1(cid:3)(cid:3)(cid:2)(cid:2)≡∨(cid:2).1 The situation calculus can accommodate multiple agents, but for the purposes of this paper we assume that there is a single agent, and all actions areperformed by that agent.2 We adopt the convention that unbound variables are universally quantified in the widest scope.\fS. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192167Moore [11] defined a possible-worlds semantics for a modal logic of knowledge in the situation calculus by treatingsituations as possible worlds. Scherl and Levesque [5,6] adapted the semantics to the action theories of Reiter [4]. The ideais to have an accessibility relation on situations, B(si",
            {
                "entities": [
                    [
                        136,
                        184,
                        "TITLE"
                    ],
                    [
                        6198,
                        6246,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 1101–1132Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA probabilistic plan recognition algorithm based on plan tree grammarsChristopher W. Geib a,∗, Robert P. Goldman ba University of Edinburgh, School of Informatics 2 Buccleuch Place, Edinburgh, EH8 9LW, United Kingdomb SIFT LLC, 211 N. First St. Suite 300, Minneapolis, MN 55401, USAa r t i c l ei n f oa b s t r a c tWe present the PHATT algorithm for plan recognition. Unlike previous approaches to planrecognition, PHATT is based on a model of plan execution. We show that this clarifiesseveral difficult issues in plan recognition including the execution of multiple interleavedroot goals, partially ordered plans, and failing to observe actions. We present the PHATTalgorithm’s theoretical basis, and an implementation based on tree structures. We alsoinvestigate the algorithm’s complexity, both analytically and empirically. Finally, we presentPHATT’s integrated constraint reasoning for parametrized actions and temporal constraints.© 2009 Elsevier B.V. All rights reserved.Article history:Received 28 November 2007Received in revised form 31 October 2008Accepted 21 January 2009Available online 24 March 2009Keywords:Plan recognitionBayesian methodsProbabilistic grammarsTask trackingIntent inferenceGoal recognitionAction grammars1. IntroductionThere is an increasing need for automated systems that understand the goals and plans of their human users. Applica-tions that need such understanding include everything from assistive systems for the elderly, to computer network security,to insider threat detection, to agent based systems. As we develop such systems, we find that much of the early work onplan recognition made simplifying assumptions that are too restrictive for effective application in these domains. Some suchsimplifying assumptions include:• The observed agent is only pursuing a single plan at a time.• The observed agent’s plans are totally ordered.• Failing to observe an action means it will never be observed or the observer will see an arbitrary subset of the actualactions.• The actions within a plan have no explicit temporal relations.• The plan representation is purely propositional. That is, actions do not have parameters.1While some of these limitations have been addressed individually, our new plan recognition system, PHATT, is the firstsystem to provide a solution to all of these issues within a single framework. PHATT takes a very different approach to theproblem of intent inference2 from other systems, and it is this approach that allows it to address all of these issues at thesame time.* Corresponding author.E-mail addresses: cgeib@inf.ed.ac.uk (C.W. Geib), rpgoldman@sift.info (R.P. Goldman).1 For example, one may have an action that goes from home to the train station, and an action that goes from the train station to home, but not anaction that moves between two arbitrary locations.2 Plan recognition is sometimes also referred to as “task tracking,” “intent recognition” or “intent inference.” We will use these terms interchangeably.0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.01.003\f1102C.W. Geib, R.P. Goldman / Artificial Intelligence 173 (2009) 1101–1132Most, if not all, early work on plan recognition treated plans as patterns to be matched against data, rather than asrecipes for actions to be executed. Unlike such approaches, PHATT is based on a model of plan execution, which moresimply and elegantly captures key aspects of the plan recognition problem. The critical observation behind this approachis that goal driven agents will take those actions that are consistent with their goals and are enabled by the actions theyhave already taken. We call the set of actions that the agent could execute next, given their goals and the actions they havealready performed, the pending set. Putting the execution of plans and pending sets at the center of our plan recognitionmodel, we can build a stochastic model of plan execution and from it develop a probabilistic algorithm for recognizingplans.Like much of the prior work, we will be assuming that the agents being observed are not actively deceitful. Deceitfulagents might attempt to dissemble, misdirect, or otherwise take actions to deliberately confuse an observing agent ratherthan directly to achieve goals. We will not be discussing how to address these issues here.The rest of this paper has the following structure. We first present an example domain and a short review of priorwork, with particular attention to limitations of this previous work that PHATT addresses (Sections 2 and 3). Section 4describes the intuitions behind the PHATT algorithm followed by the theoretical core of the paper in Sections 5 to 8.Section 5 formalizes plan libraries in terms of leftmost plan trees. Section 6 provides an abstract, top-down algorithm thatclosely parallels the generative model, providing a smooth transition to the probability model, and a first step to the actualimplementation. Section 7 provides a formal probability model for use with the explanations produced using the plan trees.Section 8 gives a bottom-up algorithm that approximates the top-down algorithm and then discusses its implementationand limitations.The rest of the paper covers evaluating the PHATT algorithm and extensions. Section 9 discusses the algorithm’s formalcomplexity, while Section 10 covers empirical complexity results and some studies of the algorithm’s scalability. Section 11explains PHATT’s use of variables and temporal constraints to improve its efficiency. Finally, Section 12 concludes this paperwith a discussion of topics that we think are particularly interesting areas for future work.2. BackgroundMost plan recognition algorithms require as input a plan library which implicitly specifies the set of plans that are tobe recognized. PHATT [20,22,25] is based on a model of the execution of simple hierarchical plans [16]. In this framework,plan libraries are partially ordered AND/OR trees. AND-nodes represent methods for achieving a particular task: all of thechildren of an AND-node must be performed in order to perform the parent task. The children may be further constrainedto be performed in a particular order (or one of a set of possible orders), by annotating them with pairwise orderingconstraints.As an example, Fig. 1 displays a small example plan library taken from a computer network security domain. In this case,the attacker is motivated by one of three top-level AND-node goals: bragging (Brag) (being able to boast of his/her successto other crackers); theft of information (Theft); or denial of service (DoS) (attacking a network by consuming its resourcesso that it can no longer serve its legitimate objectives). Attackers who wish to achieve bragging rights will first scan thetarget network for vulnerabilities (scan), and then attempt to gain control (get-ctrl). They are not motivated by exploitingthe control they gain. On the other hand, attackers who wish to steal information will scan for vulnerabilities, get controlof the target, and then exploit that control to steal data (get-data). Finally, an attacker who wishes to DoS the target needonly scan to identify a vulnerability, and then carry out his DoS attack (dos-attack).OR-nodes in the plan library represent places where the agent may choose one of a number of alternate methods toachieve a task. Only one of the children of an OR-node need be performed in order for the parent action to be achieved. Forthis reason, ordering constraints between the children of an OR-node are not allowed. For example, in Fig. 1 the OR-nodedos-attack has three possible children: synflood (syn-flood), bind DoS attack (bind-DoS), and the ping of Death (ping-of-death), but only one of them must be executed to perform a dos-attack. Since OR-nodes represent choices within the planwe will also refer to them as choice points for the plan, and will use these two terms interchangeably.Fig. 1. An example set of plans: In this figure, AND-nodes are represented by an undirected arc across the lines connecting the parent node to its children.OR-nodes do not have this arc. Ordering constraints in the plans are represented by directed arcs between the ordered actions. For example action scanmust be executed before get-ctrl which must be executed before get-data to perform Theft.\fC.W. Geib, R.P. Goldman / Artificial Intelligence 173 (2009) 1101–11321103Our representation of plans as partially ordered AND/OR trees is similar to the Hierarchical Task Network (HTN) represen-tation in Ghallab, Nau, and Traverso’s recent textbook [24, pp. 244–245], but does not take into account action preconditionsand postconditions. Note that our example plan library, while displaying a variety of the phenomena that we will be inter-ested in discussing, is not a full, up-to-date, or even realistic plan library for this computer security domain. This librarysimply illustrates the use of method decomposition (represented by AND-nodes), choice points (represented by OR-nodes),and ordering constraints between sibling actions. We will use this plan library as a running example throughout this article.3. Previous work in plan recognitionAttempts to perform plan recognition are almost as old as artificial intelligence itself, and over the years a large numberof methods have been applied to plan recognition. Some of the methods used include rule-based systems, parsing (bothconventional and stochastic), graph-covering, Bayesian nets, cost-based abduction, etc. Early approaches paid little atten-tion to choosing between different explanatory hypotheses. Either the problem was not isolated as a separate problem ofparticular interest (as in early rule-based approaches), or it was finessed (as in graph-covering approaches). Our Bayesianapproach addresses this issue directly (as did earlier Bayesian approaches). ",
            {
                "entities": [
                    [
                        138,
                        208,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 259 (2018) 52–90Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintMeasuring inconsistency with constraints for propositional knowledge basesKedian MuSchool of Mathematical Sciences, Peking University, Beijing 100871, PR Chinaa r t i c l e i n f oa b s t r a c tArticle history:Received 16 May 2016Received in revised form 12 February 2018Accepted 20 February 2018Available online 2 March 2018Keywords:InconsistencyConstraintsCausalityBipartite graphMeasuring inconsistency has been considered as a necessary starting point to understand the nature of inconsistency in a knowledge base better. For practical applications, however, we often have to face some constraints on resolving inconsistency. In this paper, we propose a graph-based approach to measuring the inconsistency for a propositional knowledge base with one or both of two typical types of constraints on modifying formulas. Here the first type of constraint, called the hard constraint, describes a pair of sets of formulas such that all the formulas in the first set should be protected from being modified on the condition that all the formulas in the second set must be modified in order to restore the consistency of that base, while the second type, called the soft constraint, describes a set of pairs of formulas that are not allowed to be modified together. At first, we use a bipartite graph to represent the relation between formulas and minimal inconsistent subsets of a knowledge base. Then we show that such a graph-based representation allows us to characterize the inconsistency with constraints in a concise way. Based on this characterization, we thus propose measures for the degree of inconsistency and for the responsibility of each formula for the inconsistency of a knowledge base with constraints, respectively. Finally, we show that these measures can be well explained based on Halpern and Pearl’s causal model and Chockler and Halpern’s notion of responsibility.© 2018 Elsevier B.V. All rights reserved.1. IntroductionInconsistency is one of the important issues in knowledge and information systems. Techniques for inconsistency han-dling have been given much attention in the community of artificial intelligence and its application domains. Recently, measuring inconsistency has been considered as a useful way of better understanding the nature of inconsistency, and then provides a promising starting point to promote the process of inconsistency handling in knowledge and information systems in many applications such as requirements engineering [22,23], network security and intrusion detection [18,19], and med-ical experts systems [29]. A growing number of inconsistency measures have been proposed so far. Hunter et al. classified these inconsistency measures into two categories, i.e., base-level measures and formula-level ones [8]. Roughly speaking, the base-level measures focus on describing how inconsistent a knowledge base is, while the formula-level ones aim to grasp the responsibility (or contribution) of each formula of a knowledge base for the inconsistency in that base.In particular, minimal inconsistent subsets of a knowledge base are attractive to measuring inconsistency in applications of syntax-based inconsistency handling [7]. Here a minimal inconsistent subset (MIS for short) refers to an inconsistent E-mail address: mukedian @math .pku .edu .cn.https://doi.org/10.1016/j.artint.2018.02.0030004-3702/© 2018 Elsevier B.V. All rights reserved.\fK. Mu / Artificial Intelligence 259 (2018) 52–9053subset without an inconsistent proper subset. Please note that minimal inconsistent subsets of a knowledge base may be considered as a natural characterization of inconsistency in that base, since we only need to remove one formula from each minimal inconsistent subset in order to restore the consistency of that base [30]. In this sense, inconsistency measures built upon minimal inconsistent subsets may help us link measuring inconsistency with resolving inconsistency in a natural way. Along this line, minimal inconsistent subsets have been used to develop base-level inconsistency measures [7,8,24,26,9] as well as formula-level measures [7,8,21,9].Theoretically, removing any formula of a minimal inconsistent subset can break the minimal inconsistent subset. Then removing a minimal part that contains at least one formula for each minimal inconsistent subset may be considered as a potential proposal for resolving the inconsistency. However, not all of such proposals for resolving inconsistency are interesting to a given practical application. For example, in requirements engineering, changing different sets of software requirements may involve different stakeholders with their own demands and benefit expectations, then a final proposal is often a trade-off between different stakeholders [22]. Generally, domain experts and users often have a good sense of which proposals are more appropriate for resolving inconsistency in that application. They may also have a sense of “conditions” for acceptable proposals for the application domain, which would rule out the proposals that they know would not be of interest. Thus, a good heuristic is to specify such intuition or expectations on resolving inconsistency as constraints to facilitate inconsistency handling in practical applications. For example, the integrity constraints in merging an inconsistent multiset of information from different sources are used to characterize the behavior that any expected merging operator has to obey [15]. In requirements engineering, essential requirements are not allowed to be involved in any feasible proposal for resolving contradictions between requirements in general case [27].Besides that such constraints can help us to select proposals we want, they may be pushed deep into the process of inconsistency handling to improve the effectiveness of related activities involved in resolving inconsistency. In particular, incorporating such constraints in measuring inconsistency can help us establish more practical relations between measuring inconsistency and resolving inconsistency.In this paper, we focus on two typical kinds of constraints on modifying formulas of a knowledge base. A constraint of the first type is a pair of sets of formulas such that all the formulas in the first set should be protected from being modified, on the condition that all the formulas in the second must be changed in resolving inconsistency. We call such a constraint a hard constraint. Generally, a hard constraint represents some partial compromise on resolving inconsistency in practical applications. In contrast, a constraint of the second type is given as a set of pairs of formulas that are not allowed to be modified together in resolving inconsistency. We call such a constraint a soft constraint.(cid:2)(cid:2)As mentioned above, minimal inconsistent subsets can be considered as a promising starting point to connect inconsis-tency measures and syntax-based inconsistency handling. However, selecting formulas that have to be modified to break minimal inconsistent subsets from their own respective perspectives does not necessarily lead to an effective proposal for resolving inconsistency. Intuitively, overlaps between minimal inconsistent subsets are often of interest to breaking all the minimal inconsistent subsets by removing as few formulas as possible. Then such two overlapping minimal inconsistent subsets should be associated with each other when we want to break them. Along this line, given a minimal inconsistent subset, any two minimal inconsistent subsets that have their own respective overlaps with the minimal inconsistent subset will be also associated with each other even if the two subsets have no overlap. More generally, two minimal inconsistent are associated with each other if there is a chain of minimal inconsistent subsets M1, M2, · · · , Mn with subsets M and MM1 = M and Mn = Msuch that Mi+1 and Mi overlap each other for all i = 1, 2, · · · , n − 1. Evidently, these associations bring a partition of the set of minimal inconsistent subsets such that only minimal inconsistent subsets in the same part (called a cluster in this paper) are associated with one another (but not necessarily overlap), moreover, they should be broken as a whole instead of from their own respective perspectives. Then we need to know how the minimal inconsistent subsets in a cluster are associated with each other in order to break them together. That is, we need to capture both the interconnection relation between minimal inconsistent subsets and formulas that play important roles in the interconnection so as to help us understand the role of each formula in causing the inconsistency from a perspective of causality. To address this, we con-struct a bipartite graph for a knowledge base, which represents both the inner structure of each minimal inconsistent subset and the interconnection between minimal inconsistent subsets due to their overlaps. Then we show that such a graph-based representation allows us to incorporate the two types of constraints in characterizing inconsistency in a concise way. Based on this incorporation, we propose approaches to measuring inconsistency with one or both of the two types of constraints, respectively. In particular, both our base-level and formula-level measures can be reduced to their respective corresponding measures presented in [21] when there is no constraint. Some intuitive logical properties and complexity issues for these inconsistency measures are also discussed, respectively.On the other hand, it is often expected that an inconsistency measure under development will be interpretable. That is, inconsistency measures may need to be tied in with some specific interpretations that can help us gain an intuitive insight into the inconsistency. Generally, causality plays an important role in analyzing and resolving inc",
            {
                "entities": [
                    [
                        134,
                        208,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 74 (1995) 31 l-350 Artificial Intelligence Naming and identity in epistemic logic Part II: a first-order logic for naming Adam J. Grove * Stanford University, Stanford, CA 94305, USA Received January 1994; revised March 1994 Abstract Modal epistemic logics for many agents sometimes ignore or simplify the distinction between the agents themselves, and the names these agents use when reasoning about each other. We consider problems motivated by practical computer science applications, and show that the simplest theories of naming are often inadequate. The issues we raise are related to some well-known philosophical concerns, such as indexical descriptions, de re knowledge, and the problem of referring to nonexistent objects. However, our emphasis is on epistemic logic as a descriptive tool for distributed systems and artificial intelligence applications, which leads to some nonstandard solutions. The main technical result of this paper is a first-order modal logic, specified both axiomatically that is expressive enough to cope (by a variant of possible-worlds semantics), and semantically with all the difficulties we discuss. 1. Introduction to individual the world. These logics formalisms There are several well-known modal that can represent an agent’s knowledge to ascribe help clarify what agents, and they can suggest or verify correct patterns of is to model systems of the state of the interacting it also reasons about what other agents know about the world, what these agents reasoning or beliefs about “knowledge” reasoning. Another promising many world, know about other agents’ knowledge, and so on. In particular, such multi-agent can involve subtle for epistemic application agents. Then not only does an agent issues of naming. That is, how does one agent refer to others? logic reason about it means *E-mail: grove&esearch.nj.nec.com. Princeton NJ 08540, USA. Current address: NEC Research Institute, 4 Independence Way, 0004-3702/95/$09.50 @ 1995 Elsevier Science B.V. All rights reserved ssD10004-3702(94)00015 \f312 A.J. Gmve/Ar~ificial Intelliger~ce 74 (1995) 31 I-350 Many treatments of multi-agent epistemic logic have all but ignored this question; we that include a useful and general concept of naming. Our approach research, from multiple-agent in which we develop epistemic is to find then and systems is the second of two papers explain why below. This logics simple examples investigate AI and distributed that arise. names the problems concerning logics epistemic (although in propositional The first paper, [ 111, investigated naming logics. The logic we develop the issues raised there are important this paper we look at first-order expressive complex), quantification, more interesting In this paper we discuss naming, scope and multiple ways of referring. In Section 3.1 we introduce in detail. It is difficult in contrast, logics. Some of for this work, and we review these in Section 2. In in Section 4.2 is far more it is correspondingly more functions, equality, there are also relating to these issues logic; than any of our propositional In part this is for the usual reasons, and the much more realistic to deal these issues logic we develop in a general way within propositional is well suited to handling both problems. is important in introduction, we discuss why “naming” logic, and the two problems of scope and multiple ways of referring. Some repeats part I ( [ 111)) and we refer the reader to that paper for more first-order semantics. However, two quite subtle problems to do with predicates, the remainder the first-order reasons. of this In epistemic of this discussion details. single-agent modal In a traditional that “It is known happens with more agents? One straightforward for every agent a in the system, and make minimal changes instance, (say K for .“). The general question we ask in this paper and in [ 111 is: what is to include one operator Ka answer to the rest of the logic. For logic there is just one modal operator taken in [ 131. However, observe this is the approach that: is fixed, and so the set of agents l The language l The only way we can refer to a is by means of the operator Ka. If by “name” we to an agent, we see that each agent has just one is fixed also. simply mean any way of referring name. l Conversely, l The composition each name denotes just one agent. of the system and the names of the agents knowledge so that every agent knows them, etc. them, and knows in it are common that every agent knows These assumptions are often quite reasonable, particularly for instance, for applications involving see [ 3,6,12, 13,15,30,33, among a small, fixed set of agents; interactions 34,391. But there are equally many situations where they are much part I, we noted that: too restrictive. In l Sometimes the system changes and reintroduce know what the composition various possibilities. there is no fixed set of agents. This happens when the composition of (e.g., we remove robots from a factory when they break down, repaired). This also occurs when an agent does not is, and so the agent must reason about of the system them when l We often need a way to refer to groups of agents; for instance, “every node in the network knows that the power will be shut off soon.” that is, names l We need non-rigid names, in different possible worlds. An agents can reason using these names, such as “the manager”, that denote different agents \fA.J. Grove/Artijcial Intelligence 74 (1995) 31 I-350 313 “the nodes in the network that haven’t failed”, “the most reliable robot”, even when it doesn’t know who the name “really” refers to. Note that it is not necessarily common knowledge who these names refer to. l Sometimes agents must reason about themselves, as in “I know that I must be at work before eight”. This becomes an especially important, and nontrivial, issue in anonymous systems. l Often we need ways of referring to agents that are relative to ourselves (or to whoever is making an assertion). Typical examples are “the node to my left”, “the person behind me”, “everyone within 10 meters of me”. Also known as in&&al reference, this has becomes an increasingly prominent issue in AI planning research 11,261. In [ 111, we present simple propositional logics that have all of these features. How- ever, these logics are deficient in other significant, and quite subtle, ways. Let us intro- duce the problems with two examples. First, consider an unreliable network in which some processes may have failed. Let the formula 4p stand for “pt knows that all correct processes received pt ‘s last message”. Suppose that, in fact, the correct processes are pt , ~2, and ps, and p1 knows that all three of these processes actually did receive the mes- sage. Then, under one reading, we could argue that Q is true: p1 does know that all the processes which are in fact correct did receive its message. On the other hand, p1 may not know that these processes are the only correct ones (and, perhaps, may spend more for additional acknowledgments). This observation leads to time-fruitlessly-waiting a second reading which makes 4p false at w. All the propositional logics we considered assumed that the second of these interpretations is the correct one. Both readings are meaningful, however, and we may need to reason about either. referent determined-just This example shows that there are different scopes that can be used in evaluating names. This can also be seen by thinking about the possible-worlds semantics that are often used for epistemic logic. The non-rigid name “the correct processes” can be evaluated-its once in the actual world, or else be re-evaluated in each situation that p1 considers possible. We call these possibilities outermost scope and innermost scope, respectively. ’ Somewhat informally, we may look at the general issue in the following way. Although we ask about the truth of some sentence like Q at a particular world (w, say), every knowledge operator we encounter in 4p will force us to consider possible worlds other than w. If n is a name occurring in cp, which world(s) should we evaluate n at? Because n is potentially non-rigid, the meaning of the formula will depend on this decision. The problem is that there seems to be several reasonable answers: for example, we can evaluate n at w (we call this outermost scope) ; or at the most “recent” worlds we have considered (innermost scope); or at any other world that has been introduced by a modal operator in whose scope the occurrence of n lies. If n is within m such operators, there will be m + 1 choices. In the following, we shall refer to all but innermost scope as being varieties of outer-scope reference. In the example above, the only modality was “‘~1 knows” and so two scopes were possible for the interpretation of “the correct processes”. ’ These two possibilities for the example are sometimes describe.d as being de re and de ditto reference, respectively. We will discuss this terminology in much more detail later. \fthat he (B) that “I 314 A.J. Grove/ArtQicial intelligence 74 (I 995) 31 I-350 This observation about scope does not explain all the difficulties names. It is not enough interpreting of scope addresses-because illustrate this with another example. to know who a name refers to-which the reference we must also decide how that can arise in the question is made. We Suppose there are two robotic agents, A and B, and A has just broken down. He sends a cry for help over a public broadcast for dealing with such matters, may or may not have heard. So A’s subsequent depends on whether he can deduce (if this is true, he can just wait, but otherwise he should system. B, who is the agent responsible action that I need help” that “I (A) know that B knows try something else). that I need help”? That should A try to deduce from what he knows? The logic we review in the formula K, KB (Zneed_heZp) , which certainly is really much more complex, because a fo",
            {
                "entities": [
                    [
                        67,
                        145,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 1172–1221Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintUpdating action domain descriptions ✩Thomas Eiter a, Esra Erdem b, Michael Fink a,∗, Ján Senko aa Institute of Information Systems, Vienna University of Technology, Vienna, Austriab Faculty of Engineering and Natural Sciences, Sabancı University, Istanbul, Turkeya r t i c l ei n f oa b s t r a c tArticle history:Received 28 November 2008Received in revised form 4 June 2010Accepted 27 June 2010Available online 17 July 2010Keywords:Knowledge representationReasoning about actions and changeTheory changeAction languagesPreference-based semanticsIncorporating new information into a knowledge base is an important problem which hasbeen widely investigated. In this paper, we study this problem in a formal frameworkfor reasoning about actions and change. In this framework, action domains are describedin an action language whose semantics is based on the notion of causality. Unlikethe formalisms considered in the related work, this language allows straightforwardrepresentation of non-deterministic effects and indirect effects of (possibly concurrent)actions, as well as state constraints; therefore, the updates can be more general thanelementary statements. The expressivity of this formalism allows us to study the updateof an action domain description with a more general approach compared to related work.First of all, we consider the update of an action description with respect to furthercriteria, for instance, by ensuring that the updated description entails some observations,assertions, or general domain properties that constitute further constraints that are notexpressible in an action description in general. Moreover, our framework allows us todiscriminate amongst alternative updates of action domain descriptions and to single outa most preferable one, based on a given preference relation possibly dependent on thespecified criteria. We study semantic and computational aspects of the update problem,and establish basic properties of updates as well as a decomposition theorem that givesrise to a divide and conquer approach to updating action descriptions under certainconditions. Furthermore, we study the computational complexity of decision problemsaround computing solutions, both for the generic setting and for two particular preferencerelations, viz. set-inclusion and weight-based preference. While deciding the existence ofsolutions and recognizing solutions are PSPACE-complete problems in general, the problemsfall back into the polynomial hierarchy under restrictions on the additional constraints. Wefinally discuss methods to compute solutions and approximate solutions (which disregardpreference). Our results provide a semantic and computational basis for developing systemsthat incorporate new information into action domain descriptions in an action language, inthe presence of additional constraints.© 2010 Elsevier B.V.Open access under CC BY-NC-ND license.1. IntroductionAs we live in a world where knowledge and information is in flux, updating knowledge bases is an important issuethat has been widely studied in the area of knowledge representation and reasoning (see e.g. [67,12,20,61] and references✩This paper is a revised and significantly extended version of a preliminary paper that appeared in: Proc. 19th International Joint Conference on ArtificialIntelligence (IJCAI 2005), pp. 418–423.* Corresponding author.E-mail addresses: eiter@kr.tuwien.ac.at (T. Eiter), esraerdem@sabanciuniv.edu (E. Erdem), michael@kr.tuwien.ac.at (M. Fink), jan@kr.tuwien.ac.at(J. Senko).0004-3702 © 2010 Elsevier B.V.doi:10.1016/j.artint.2010.07.004Open access under CC BY-NC-ND license.\fT. Eiter et al. / Artificial Intelligence 174 (2010) 1172–12211173therein). However, the problem is far from trivial and many different methods have been proposed to incorporate newinformation, be it affirmative or prohibitive, which are based on different formal and philosophical underpinnings, cf. [67,39,57]. It appears that there is no general purpose method that would work well in all settings, which is partly due to thefact that an update method is also dependent to some extent on the application domain.In particular, in reasoning about actions and change, the dynamicity of the world is a part of the domain theory, andrequires special attention in update methods. For various approaches to formal action theories, including the prominentsituation calculus, event calculus, and action languages that emerged from the research on non-monotonic reasoning, theproblem of change has been widely studied and different methods have been proposed (see [64] for background and refer-ences, and Section 8.1 for a more detailed discussion).To give a simple example, consider an agent having the following knowledge, K TV , about a TV with remote control:(TV1) If the power is off, pushing the power button on the TV turns the power on.(TV2) If the power is on, pushing the power button on the TV turns the power off.(TV3) The TV is on whenever the power is on.1(TV4) The TV is off whenever the power is off.Now assume that the agent does not know how a remote control works (e.g., she does not know the effect of pushing thepower button on the remote control). Suppose that later she obtains the following information, K RC , about remote controls:(RC1) If the power is on and the TV is off, pushing the power button on the remote control turns the TV on.(RC2) If the TV is on, pushing the power button on the remote control turns the TV off.The task is now to incorporate this new knowledge into the current knowledge base K TV . In this particular case, thisseems unproblematic, as upon simply adding K RC to KTV the resulting stock of knowledge is consistent; in general, however,it might be inconsistent, and a major issue is how to overcome this inconsistency.We study the incorporation problem in the context of action languages [30]. In these formalisms, actions and change aredescribed by “causal laws.” For instance, in the action language C [32], the direct effect of the action of pushing the powerbutton on the TV, stated in (TV1), is described by the causal lawcaused PowerON after PushPBTV ∧ ¬PowerON,(1)which expresses that this action, represented by PushPBTV , causes the value of the fluent PowerON to change from false totrue; the indirect effect of this action that is stated in (TV3) is described by the causal lawcaused TvON if PowerON,(2)which expresses that if the fluent PowerON is caused to be true, then the fluent TvON is caused to be true as well.Action description languages are quite expressive to easily handle non-determinism, concurrency, ramifications, qualifica-tions, etc. The meaning of an action description can be represented by a “transition diagram”—a directed graph whose nodescorrespond to states and whose edges correspond to action occurrences; Fig. 1 below (Section 2) shows an example. Thereare reasoning systems, like CCalc2 and DLVK,3 that accept domain descriptions in an action language, like C or K respec-tively, and support various kinds of reasoning tasks over these descriptions, including planning, prediction and postdictionin CCcalc and computing different kinds of plans in DLVK.As far as action languages are concerned, the update problem was studied to a remarkably little extent. For the basicaction language A (see [30]), which is far less expressive than C, the update problem has been considered, e.g., in [44,47]. Both works focused on updates that consist of elementary statements (i.e., essentially facts) over time, and presentedspecific update methods, focusing on the contents of the knowledge base. We address the update problem from a moregeneral perspective in the following ways:• We consider a richer language (i.e., a fragment of C) to study the update problem, and updates are represented interms of a set of arbitrary causal laws.• We view the update problem from a more general perspective. Sometimes, ensuring consistency is not sufficient: wemight want to ensure also that the updated action description entails some scenarios, conditions, or general properties ofthe domain that cannot be expressed by causal laws. In our update framework, such further knowledge could be taken intoaccount.For example, for the effective use of the TV system in the above scenario, the following constraint might be imposed:(C) Pushing the power button on the remote control is always possible.41 Note that the statement is wrong; its defectiveness is observed and resolved upon update.2 http://www.cs.utexas.edu/users/tag/cc/.3 http://www.dbai.tuwien.ac.at/proj/dlv/K/.4 Note the conceptual difference between (C) and (TV2): (C) expresses an executability condition, whereas (TV2) captures a causal relationship.\f1174T. Eiter et al. / Artificial Intelligence 174 (2010) 1172–1221If KRC is simply added to KTV , then (C) is not satisfied by KRC ∪ KTV : when the power and the TV are on, pushing the powerbutton on the remote control is not possible, since (RC2) and (TV3) contradict. The question is then how the agent canupdate KTV by incorporating KRC relative to (C); note that (C) is not expressible by causal laws in the action language C.To represent constraints like (C), we use formulas for “queries” in action languages like in [30]; here, the formulaALWAYS executable {PushPBRC}(3)has to evaluate to true, where {PushPBRC} stands for the concrete action of pushing the power button on the remote control.Similarly, consider the following scenario that we might want the updated action description to entail:(S) Sometimes, when the power is on, pushing the power button on TV turns the power off, and after that if we push thepower button on the TV then the power is on again.This scenario cannot be expressed by means of causal laws either; however, it can be expressed by a formulaSOMETIMES evolves PowerON; {PushPBTV };¬PowerON; {PushPBTV }; PowerON.• Some",
            {
                "entities": [
                    [
                        138,
                        173,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 172 (2008) 1837–1872Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintOutlier detection using default reasoning ✩Fabrizio Angiulli a, Rachel Ben-Eliyahu – Zohary b,∗,1, Luigi Palopoli aa DEIS, Università della Calabria, Via Pietro Bucci 41C, 87036 Rende (CS), Italyb Ben-Gurion University and Jerusalem College of Engineering, Beer-Sheva/Jerusalem, Israela r t i c l ei n f oa b s t r a c tArticle history:Received 5 March 2007Received in revised form 9 July 2008Accepted 29 July 2008Available online 15 August 2008Keywords:Default logicDisjunctive logic programmingKnowledge representationNonmonotonic reasoningComputational complexityData miningOutlier detection1. IntroductionDefault logics are usually used to describe the regular behavior and normal properties ofdomain elements. In this paper we suggest, conversely, that the framework of default logicscan be exploited for detecting outliers. Outliers are observations expressed by sets of literalsthat feature unexpected semantical characteristics. These sets of literals are selected amongthose explicitly embodied in the given knowledge base. Hence, essentially we perceiveoutlier detection as a knowledge discovery technique. This paper defines the notion ofoutlier in two related formalisms for specifying defaults: Reiter’s default logic and extendeddisjunctive logic programs. For each of the two formalisms, we show that finding outliers isquite complex. Indeed, we prove that several versions of the outlier detection problem lieover the second level of the polynomial hierarchy. We believe that a thorough complexityanalysis, as done here, is a useful preliminary step towards developing effective heuristicsand exploring tractable subsets of outlier detection problems.© 2008 Elsevier B.V. All rights reserved.This paper is about detecting outliers. In this work, outliers are unexpected observations, e.g., strange characteristics ofindividuals, in a given application domain. Exceptionality is determined here with respect to a given trustable knowledgebase, with which a given set of elements does not comply. The issue that we address is how to locate such unusual elementsautomatically.A first step towards automatically detecting outliers is to state their formal definition. In this work, it is assumed that thegiven knowledge base is expressed using a default reasoning language and hence we formalize our definition of outliers inthis framework. The languages mainly dealt with are propositional default logics and extended disjunctive logic programs.Default logic was originally developed as a tool for working with incomplete knowledge. Default rules allow one todescribe a normal behavior of a system and to draw consequent conclusions. As such, default rules can also be exploited fordetecting outliers—observations that are unexpected according to the default theory at hand. This is the basic idea behindthis paper. We refer to outliers as sets of observations that demonstrate some properties contrasting with those that canbe logically “justified” according to the given knowledge base. Along with outliers, their “witnesses” are singled out—thoseunexpected properties that characterize outliers.To illustrate, some informal application examples for outlier detection are described below.✩This manuscript is an extended and comprehensive report of results of which part have appeared in IJCAI-03 under the title “Outlier Detection UsingDefault Logic” and in ECAI-04 under the title “Outlier Detection Using Disjunctive Logic Programming”.* Corresponding author.E-mail addresses: f.angiulli@deis.unical.it (F. Angiulli), rachel@bgu.ac.il (R. Ben-Eliyahu – Zohary), palopoli@deis.unical.it (L. Palopoli).1 Part of this work was done while the author was a visiting scholar in the Division of Engineering and Applied Sciences, Harvard University, Cambridge,Massachusetts.0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.07.004\f1838F. Angiulli et al. / Artificial Intelligence 172 (2008) 1837–1872Using outliers for diagnosis of computer hardware. Suppose that it usually takes about four seconds to download a giga-byte file from a server, but one day the system becomes slower, instead, eight seconds are needed to perform thesame task. While eight seconds may indicate a good performance, it is, nonetheless, helpful to find the source ofthe delay in order to prevent more critical faults in the future. In this case, the download operation is the outlierwhile the delay is its witness.Mechanical failure. Assume that someone’s car brakes are making a strange noise. Although they seem to be functioningproperly, this is not a normal behavior and the car is brought in for servicing. In this case, the car brakes are theoutlier and the noise is a witness for it.Knowledge base integrity.If an abnormal property is discovered in a database, the source that reported this informationwould have to be checked. Detecting abnormal properties, that is, detecting outliers, can also lead to an update ofdefault rules in a knowledge base. For example, suppose we have the rule that birds fly, and we observe a bird thatdoes not fly. This occurrence of such an outlier in the theory would be reported to the knowledge engineer. Theengineer investigates the case, finds out that the bird is actually a penguin, therefore he updates the knowledgebase with the default “penguins do not fly.”According to our approach, exceptions are not explicitly recorded in the knowledge base as “abnormals,” as is often donein logical-based abduction [16,23,47]. Rather, their “abnormality\" is singled out precisely because some of the propertiescharacterizing them cannot be justified within the given theory.In this paper we formally define outliers in both the related formalisms of Reiter’s default logic and Extended disjunctivelogic programming (EDLP).Reiter’s Default Logic is a powerful nonmonotonic formalism to deal with incomplete information, while logic program-ming is a practical tool that is widely employed in KR&R. The paper mostly deals with the propositional fragment of theselogics. However, first-order default theories shall be also briefly discussed in the paper (see Section 5 below).In the logic programming framework, we focus on Answer Set Semantics, which is used in most advanced systems forknowledge representation [38,40,43]. Extended logic programs (ELP) under Answer Set Semantics allow both negation as fail-ure and classical negation to be used. These programs can be naturally embedded into default theories and therefore canbe considered as a subset of default logic. As a consequence, our results for default theories carry over quite simply to ELPs.However, unlike ELP, extended disjunctive logic programs (EDLP) under Answer Set Semantics, in which also head-disjunctionis allowed, cannot be viewed as a subset of default logic, although default logic in its full volume does include disjunction.Indeed, part of the motivation for developing disjunctive logic programming lies in the limitations of default logic in han-dling disjunctive knowledge (see the paper by Poole [47]). In this context, EDLP can be considered as a convenient tool forrepresenting and manipulating complex knowledge [38] due to its declarativity and expressive power.In what follows, we first introduce our formal definition of outliers. Then, we analyze the complexities involved inincorporating the outlier detection mechanism into knowledge bases expressed in default logic and extended disjunctivelogic programs. We believe that a thorough complexity analysis is a useful step towards singling out the more complexsubtasks involved in outlier detection. This first step is conducive to designing effective algorithms for implementationpurposes.According to the view adopted in this work, the witness that an observation is an outlier is a property or a behaviorthat is explicitly the opposite of what is expected. Representing such contradicting properties requires the usage of classicalnegation. Both default logic and extended logic programs make use of classical negation. Hence, these two languages repre-sent a natural setting for outlier detection. A different approach, which does not require that the negation of the exceptionalproperty is explicitly inferred but, rather, that it is simply not entailed by a logic program, is taken in [5]. As explainedthoroughly in this paper, the anomalies that can be singled out by the definition of [5] are quite different than the outliersdetected by the work presented here. This is mirrored in the different complexity figures we obtained: most of the outlierdetection problems investigated here lie at the third level of the polynomial hierarchy, whereas the most complex of theproblems considered in [5] are contained in its second level. In the sequel we will further elaborate on these differences.The rest of this paper is organized as follows: Section 2 provides preliminary definitions and Section 3 defines outliersand related notions. Section 4 discusses the complexity of finding outliers in general propositional as well as in disjunction-free default logics. Section 5 deals with first-order defaults. Section 6 discusses related work—in particular, the relationshipbetween outlier detection and abduction. Finally, Section 7 draws conclusions.2. Preliminary definitionsIn this section we briefly review preliminary definitions used in default logic and extended (disjunctive) logic programs.Note that only the propositional fragment of these logics is considered here. Outlier detection in first-order default languagesshall be briefly discussed in Section 5. Thus, whenever a default theory or a logic program with variables is used, it isreferred to as an abbreviation of its grounded version.\fF. Angiulli et al. / Artificial Intelligence 172 (2008) 1837–187218392.1. Default logicDefault logic was first introduced by Reiter in a first-o",
            {
                "entities": [
                    [
                        138,
                        179,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 94 (1997) 99-137 Artificial Intelligence Coalitions among computationally bounded agents Tuomas W. Sandholmav*, Victor R. Lesser b,l a Department of Computer Science, Washington University, One Brookings Drive, Campus Box 1045, St. Louis, MO 63130-4899, USA h Department of Computer Science, University of Massachusetts, Amherst, MA 01003, USA Abstract solving to operate efficiently the agents can sometimes in the world. By colluding (coordinating save costs compared This paper analyzes coalitions among self-interested agents that need to solve combinatorial op- their actions by to operating timization problems solving a joint optimization problem) individually. A model of bounded rationality is adopted where computation is not worthwhile the problems optimally: off against computation tions among bounded-rational are significantly putation. This relationship rational and bounded-rational routing with real data from five dispatch centers. This problem are so large that-with complexity. @ 1997 Elsevier Science B.V. resources are costly. It traded theory of coali- is devised. The optimal coalition structure and its stability profiles and the cost of com- including in vehicle and the instances is bounded by computational theoretically. Then a domain classification results are presented cost. A normative, application- and protocol-independent affected by the agents’ algorithms’ performance solution quality is decision-theoretically is introduced. Experimental agent’s rationality is first analyzed is NP-complete technology-any current agents agents Keywords: Distributed AI; Multiagent systems; Coalition formation; Negotiation; Bounded rationality; Resource-bounded reasoning; Game theory 1. Introduction Automated negotiation systems with self-interested agents are becoming increasingly important. munication One reason this infrastructure-Internet, for is the technology WWW, NII, EDI, KQML push of a growing standardized com- [ 81, FIPA, Telescript * Corresponding author. Email: sandholm@cs.wustl.edu. ’ Email: lesser@cs.umass.edu. 0004-3702/97/$17.00 PII SOOO4-3702 (97)00030-l @ 1997 Elsevier Science B.V. All rights reserved. \f100 lIW Sandholm, VR. .f.esser/Arti&ial Inteliigence 94 (1997) 99-137 [ 141, Java, etc.-over which separately designed agents belonging to different or- ganizations can interact in an open environment in real-time and safely carry out transactions. The second reason is strong application pull for computer support for negotiation at the operative decision making level. For example, we are witnessing the advent of small tr~saction commerce on the Internet for purchasing goods, in- formation, and communication bandwidth [ 21,3 11. There is also an industrial trend toward virtual enterprises: dynamic alliances of small, agile enterprises which together can take advantage of economies of scale when available (e.g., respond to more di- verse orders than individual agents can), but do not suffer from diseconomies of scale. Multiagent technology facilitates the automated formation of such dynamic coalitions at the operative decision making level. This automation can save labor time of human negotiators, but in addition, other savings are possible because computational agents can be more effective at finding beneficial short-term coalitions than humans are in strategically and combinatorially complex settings. This paper discusses coalition formation in inherently distributed combinatorial prob- resource and task allocation and multiagent planning and scheduling-in lems-e.g., situations where agents may have different goals, and each agent is trying to maximize its own good without concern for the global good. Such self-interest naturally prevails in negotiations among independent businesses or individu~s. In building computer support for coalition formation in such settings, the issue of self-interest has to be dealt with. In cooperative distributed problem solving [7,5], the system designer imposes an interaction protocol* and a strategy (a mapping from state history to action; a way to use the protocol) for each agent. The approach is usually descriptive: the main question is what social outcomes follow given the protocol and assuming that the agents use the the imposed strategies. On the other hand, in m~ltiugent systems [ 35,23,5,48,42,45], agents are provided with an interaction protocol, but each agent will choose its own strategy. A self-interested agent will choose the best strategy for itself, which cannot be explicitly imposed from outside. The protocols need to be designed normatively: the main question is what social outcomes follow given a protocol which guarantees that each agent’s desired local strategy is best for that agent-and thus the agent will use it. The normative approach is required in designing robust non-manipulable multiagent systems where the agents may be constructed by separate designers and/ or may represent different real world parties. Interactions of self-motivated agents have been widely studied in microeconomics- especially in game theory [29,11,24,34]. Most of that work assumes perfect rationality of the agents [ 50,181, e.g., flawless and costless deduction. We extend the normative approach of game theory to settings where the agents lack full rationality because they cannot enumerate or evaluate all alternative solutions to a 2 By protocol we do not mean a low the possible actions determines the sealed-bid $rst-price aucrion, where each bidder task, which mechanism in game theory is awarded [ 11,241. level ~ommunica~on protocoi, but a negotiation that agents can take at any point of the negotiation. An example protocol is free to submit one bid to take responsibility protocol which is for a is called a to the lowest price bidder at the price of his bid. The analog of a protocol \fTU? Sandholm, VR. Lesser/Artificial Intelligence 94 (1997) 99-137 101 coalition’s optimization incurs expenses search attempt to find optimal solutions traded off against problem. 3 Instead, in terms of CPU the cost of computation. they have to search for good solutions. Such to time. Therefore Instead, solution quality needs to be it is unreasonably costly to hard problems. 1.1. Example application: distributed vehicle routing The methods presented and there is an underlying because two characteristics dispatch centers, manufacturing these rationality self-interested, the agents’ Applications with independent agile enterprises, meeting scheduling, classroom multiagent information in multi-provider multi-consumer developed concrete, the paper. problem the problem in this paper are needed intractable include distributed planning scheduling of patient in settings where combinatorial cannot be solved optimally the agents are that limits in practice. routing among among multiple across hospitals, planning software projects, gathering on the World Wide Web, and allocating bandwidth to name just a few. The methods to make the concepts more throughout and scheduling treatments independent. However, computer networks, of multi-contractor and scheduling scheduling, vehicle the distributed vehicle routing problem will be used as an example in this paper are domain The distributed vehicle for certain deliveries and has a certain number of vehicles is structured that we study routing problem in terms of a dispersed dispatch centers of different companies. Each center to take care of its own vehicles and delivery routing a dispatch center-has is a heterogeneous fleet multi-depot number of geographically is responsible them. So teach agent-representing tasks. The problem with the following constraints. local problem of each agent 0 0 l Each vehicle has to begin and end its tour at the depot of its center the pickup nor the drop-off Each vehicle has a maximum Each vehicle has a maximum vehicles. load weight constraint. These differ among vehicles. load volume constraint. These also differ among locations of the orders need to be at the depot). (but neither l Each vehicle has the same maximum l Every delivery has to be included route length (prescribed by law). in the route of some vehicle. is to minimize The objective route lengths of the vehicles transportation in the solution the domain cost is the sum of the costs: that has been reached. The problem reduced is NP-hard, because dTSP can be trivially the cost and feasibility of a solution can easily be checked instances because Thus, the problem are so large that even the smallest ones are too hard to solve optimally-unlike in Fig. 1. is NP-complete. Moreover, the problem It is in NP, time. to it.4 in polynomial in our experiments the one 3 Others in game theory have examined the effects of computational agents play a combinatorially game, see e.,g. [ 321. 4 ATSP is :I Traveling Salesman Problem where trivial game, but complexity limits on rational play in settings where repetitions of that same stems from numerous the distances between cities satisfy the triangle inequality. \fTX Sandholm, VR. .kwer/Art@ciai intelligence Q4 (1997) 99-137 Fig. 1. Small example problem instance of distributed vehicfe routing. This instance has three dispatch centers represented in the figure by computer operators. They receive the delivery orders and route the vehicles. Each parcel is numbered according to the dispatch center that is responsible for delivering it. No routing solution is shown. it can often be less expensively to handle a delivery. The cost of handling The geographical operation areas of the centers overlap. This creates the potential multiple centers because ones while honoring be incorporated with lowest cost into the routing solution of the center have adjacent make cost savings for it may vary between agents than remote routes length constraints. So it can often to costs among agents for handling a delivery often tasks among agents. This allows considerable integrated the weight, volume and route routes. The asymmetric to realloca",
            {
                "entities": [
                    [
                        65,
                        112,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 241 (2016) 66–102Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintMultiWiBi: The multilingual Wikipedia bitaxonomy projectTiziano Flati∗, Daniele Vannella, Tommaso Pasini, Roberto NavigliDipartimento di Informatica, Sapienza Università di Roma, Italya r t i c l e i n f oa b s t r a c tArticle history:Received 14 May 2015Received in revised form 10 August 2016Accepted 15 August 2016Available online 8 September 2016Keywords:Taxonomy extractionTaxonomy inductionMachine learningNatural language processingCollaborative resourcesWikipedia1. IntroductionWe present MultiWiBi, an approach to the automatic creation of two integrated taxonomies for Wikipedia pages and categories written in different languages. In order to create both taxonomies in an arbitrary language, we first build them in English and then project the two taxonomies to other languages automatically, without the help of language-specific resources or tools. The process crucially leverages a novel algorithm which exploits the information available in either one of the taxonomies to reinforce the creation of the other taxonomy. Our experiments show that the taxonomical information in MultiWiBi is characterized by a higher quality and coverage than state-of-the-art resources like DBpedia, YAGO, MENTA, WikiNet, LHD and WikiTaxonomy, also across languages. MultiWiBi is available online at http :/ /wibitaxonomy.org /multiwibi.© 2016 Elsevier B.V. All rights reserved.Over recent decades, knowledge has increasingly become the fundamental “lubricant” of our society. The Web today is by far the largest repository of knowledge in history and, as it gradually creeps into all aspects of our everyday lives, the ability to manipulate and control its knowledge concerns everyone, both the great mass of general users and researchers [1–3], and the big industry players [4,5] that are called upon to process and deliver information in an efficient and accurate manner. With the exception of rare cases, such as WordNet [6], for which knowledge has been manually encoded, the building of big repositories of knowledge requiring human intervention, and the extended development times this entails, has now become, unfortunately, no longer feasible. Such approaches simply cannot cope with the high volume of information, its heterogeneity and the need to have knowledge available in as many languages as possible. Nevertheless, having such large repositories of knowledge embedded into intelligent systems would positively impact several Natural Language Processing (NLP) tasks, such as question answering [7–10], machine reading [11], entity linking [12,13], information extraction [14,15]and automatic reasoning [16–18]. For example, traditional open-domain Question Answering systems might not be able to answer questions such as “Which architect designed the Shard London Bridge?”. Even in the case where the answer is in effect provided within the text (e.g., “Renzo Piano designed many skyscrapers, among which is the Shard London Bridge”), additional information is usually needed at the semantic type level [19] (e.g., “Renzo Piano” is an architect and “Shard London Bridge” is a skyscraper). As a further demonstration of concept, Word Sense Disambiguation [20,21] might also receive a significant boost. Consider, for instance, the sentence “The woman lit a match”: by combining the information that i) a match can either be a lighter or a contest (contribution from the taxonomy) with ii) the fact that only lighters are usually lit (contribution from the disambiguation system), it should be possible to achieve higher disambiguation performance. Because of the usefulness of taxonomies, researchers and industrial stakeholders have been seeking for decades to design * Corresponding author.E-mail addresses: flati@di.uniroma1.it (T. Flati), vannella@di.uniroma1.it (D. Vannella), pasini@di.uniroma1.it (T. Pasini), navigli@di.uniroma1.it (R. Navigli).http://dx.doi.org/10.1016/j.artint.2016.08.0040004-3702/© 2016 Elsevier B.V. All rights reserved.\fT. Flati et al. / Artificial Intelligence 241 (2016) 66–10267novel mechanisms capable of automatically extracting valuable information which is both broad and accurate at the same time. This goal has been pursued in many different ways. In the early days (but such approaches remain as alive as ever) there was the conviction and the desire to extract knowledge from linguistic textual repositories alone: methods based on distributional word cooccurrence and statistical analysis over linguistic patterns relied on nothing but free text. Given the limited size and source of the textual corpora on which these systems relied, however, even when they proved to be accurate, they failed to serve as true general domain data providers. As time went by, though, collaborative efforts started to sprout spontaneously, with the aim of developing true encyclopedic stores in which users could actively contribute by enhancing the resource with additional information. Wikipedia, started in 2001, is one of the biggest such movements and currently the most active one, with knowledge available in 294 languages at the time of writing. A real added value brought by Wikipedia is the possibility to enrich text with hyperlinks: this feature, combined with the availability of tabular information, makes it possible to extract semi-structured information on a large scale [22,23].Over time, systems have targeted very different types of relation, sometimes very general or open-domain (TextRunner [24], ReVerb [25], and approaches at the syntactic-semantic interface like [26] and DefIE [14]), sometimes very specific or bound to a particular domain. Semantic relations encode a large number of linguistic aspects, spanning from general relatedness (as is the case for links across Wikipedia pages) up to specific types, such as hypernymy, holonymy, meronymy, and so on [27]. It became increasingly clear that hypernymy relations represented one of the most important types which could be used to boost current artificial intelligent systems. Starting from the eighties, a whole branch of research had focused on this type of semantic relation, with the pioneering work of Hearst [28] laying the foundation for the forthcoming literature. Hearst’s patterns, however, were designed to be applicable only to free text and did not exploit any specific feature of the collaborative machine-readable repositories yet to come. One of the first attempts to extract is-a information from Wikipedia dates back to WikiTaxonomy [29] which transformed the noisy network of Wikipedia categories into a structured taxonomy of concepts. Subsequently, the example of WikiTaxonomy inspired a full line of research, including YAGO [30], WikiNet [31], MENTA [32], and more recently LHD [33].Many of the above-mentioned taxonomies are focused on English and do not easily scale to dozens of languages, due to their dependency on English corpora and tools. Nonetheless, the multilinguality issue has been addressed in some of the existing taxonomies in a number of ways: DBpedia is based on manual mappings of Wikipedia infoboxes across languages to concepts in a small upper ontology, MENTA combines the taxonomical information from WordNet with information coming from several elements of Wikipedia, such as infoboxes and categories. LHD relies on a simple, though general, linking approach based on string-matching rules. However, the type of knowledge extracted by these approaches is either partial (is-a information is provided only for Wikipedia pages or Wikipedia categories), incomplete (lacking full coverage) or heterogeneous (i.e., not drawn from a shared, standard repository).In contrast, in this paper we present an approach to the automatic creation of an integrated bitaxonomy of Wikipedia pages and categories for multiple languages, called MultiWiBi, which addresses all the above-mentioned issues:1. First, it does not focus on Wikipedia pages or categories on their own but taxonomizes the two sides together, showing that they are mutually beneficial for inducing a wide-coverage and fine-grained integrated taxonomy. In particular, hypernyms are returned in a coherent manner, such that a Wikipedia page (category) has a Wikipedia page (category) as hypernym. The rationale behind this decision is that not only has Wikipedia been designed with these two separate but interconnected structures in mind, but also the nature of the two sides of Wikipedia is very different, in that pages encode concepts and named entities while categories group pages into coherent sets. For these reasons it would be unnatural to merge these two types of item.2. Second, our method is able to taxonomize Wikipedias in any language, in a way that is fully independent of additional resources. At the core of our approach, in fact, lies the idea that the English version of Wikipedia can be linguistically exploited as a pivot to project the taxonomic information to any other language offered by Wikipedia, in order to produce a bitaxonomy in arbitrary languages. English has been chosen as pivoting language because i) the quality of other Wikipedia languages is not comparable to the English version (see Section 12); ii) it is the language with the provable highest-performance syntactic parser, thus leading to the best hypernym lemmas; iii) English is the language which features by far the greatest number of pages in Wikipedia.1 Nonetheless, our method can potentially pivot on any language, not only English; we chose English as pivoting language because it is the language with the highest amount of data and, presumably, also the highest quality.3. Third, we prove that our approach overcomes the language barrier by extracting not only hypernyms for projectable concepts, but also for those concepts which do not have an English counterpart and therefore represent culture-specific bits of knowledge.2. Background an",
            {
                "entities": [
                    [
                        135,
                        191,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 175 (2011) 1092–1121Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintContracting preference relations for database applications ✩Denis Mindolin, Jan Chomicki∗Department of Computer Science and Engineering, 201 Bell Hall, University at Buffalo, Buffalo, NY 14260-2000, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 27 February 2009Received in revised form 17 September2010Accepted 17 September 2010Available online 1 December 2010Keywords:Preference contractionPreference changePreference query1. IntroductionThe binary relation framework has been shown to be applicable to many real-lifepreference handling scenarios. Here we study preference contraction: the problem ofdiscarding selected preferences. We argue that the property of minimality and thepreservation of strict partial orders are crucial for contractions. Contractions can be furtherconstrained by specifying which preferences should be protected. We consider preferencerelations that are finite or finitely representable using preference formulas. We presentalgorithms for computing minimal and preference-protecting minimal contractions forfinite as well as finitely representable preference relations. We study relationships betweenpreference change in the binary relation framework and belief change in the belief revisiontheory. We evaluate the proposed algorithms experimentally and present the results.© 2010 Elsevier B.V. All rights reserved.A large number of preference handling frameworks have been developed [16,7,20]. In this paper, we work with thebinary relation preference framework [10,22]. Preferences are represented as ordered pairs of tuples, and sets of preferencesform preference relations. Preference relations are required to be strict partial orders (SPO): transitive and irreflexive binaryrelations. The SPO properties are believed to capture the rationality of preferences [16]. This framework can deal with finiteas well as infinite preference relations, the latter represented using finite preference formulas.Working with preferences in any framework, it is naive to expect that they never change. Preferences can change overtime: if one likes something now, it does not mean one will still like it in the future. Preference change is an active topicof current research [11,17]. It was argued [15] that along with the discovery of sources of preference change and elicitationof the change itself, it is important to preserve the correctness of the preference model in the presence of change. In thebinary relation framework, a natural correctness criterion is the preservation of SPO properties of preference relations.An operation of preference change – preference revision – has been proposed in [11]. We note that when a preferencerelation is changed using a revision operator, new preferences are “semantically combined” with the original preferencerelation. However, combining new preferences with the existing ones is not the only way people change their preferences inreal life. Another very common operation of preference change is “semantic subtraction” from a set of preferences anotherset of preferences one used to hold, if the reasons for holding the contracted preferences are no longer valid. That is, we aregiven an initial preference relation (cid:3) and a subset CON of (cid:3) (called here a base contractor) which should not hold. We wantto change (cid:3) in such a way that CON does not hold in it. This is exactly opposite to the way the preference revision operatorschange preference relations. Hence, such a change cannot be captured by the existing preference revision operators.In multi-agent scenarios, a negotiation between different agents may involve giving up individual agents’ preferences [1]. Inmore complex scenarios, preferences may be added as well as given up.✩Research partially supported by NSF grant IIS-0307434. This paper is an extended version of Mindolin and Chomicki (2008) [23].* Corresponding author.E-mail addresses: mindolin@buffalo.edu (D. Mindolin), chomicki@buffalo.edu (J. Chomicki).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.11.011\fD. Mindolin, J. Chomicki / Artificial Intelligence 175 (2011) 1092–11211093Fig. 1. Example 1. Mary’s preferences.Another reason for contracting user preferences in real-life applications is the need for widening preference query results.In many database applications, preference relations are used to compute sets of the best (i.e. the most preferred) tuples,according to user’s preferences. Such tuples may represent objects like cars, books, cameras, etc. The operator which is usedin the binary relation framework to compute such sets is called winnow [10] (or BMO in [22]). The winnow operator isdenoted as w(cid:3)(r), where r is the original set of tuples, and (cid:3) is a preference relation. If the preference relation (cid:3) is large(i.e. the user has many preferences), the result of w(cid:3)(r) may be too narrow. One way to widen the result is by discardingsome preferences in (cid:3). Those may be the preferences which do not hold any more or are not longer important.In this paper, we address the problem of contraction of preference relations. We consider it for finitely representableand finite preference relations. We illustrate now preference contraction for finite (Example 1) and finitely representable(Example 2) preference relations.Example 1. Assume a car dealer has a web site showing his inventory of cars, and Mary is a customer interested in buying acar. Assume also that Mary has a previous purchase history with the dealer, so her preferences (possibly outdated) over carsare known: she prefers every car ti to every car t j (denoted ti (cid:3)1 t j ) with i > j (i, j ∈ [1, 5]). Let the inventory r1 consist offour cars (r1 = {t1, t3, t4, t5}), while t2 is currently missing. The preference relation is illustrated in Fig. 1 by the set of alledges, where an edge from ti to t j shows that ti is preferred to t j . The set of the best cars according to Mary’s preferencerelation is w(cid:3)1 (r1) = {t1}.Assume that the dealer observes that while Mary is browsing the web site, she indicates equal interest in three cars: t1(as expected according to (cid:3)1), t3, and t5. As a result, her preference relation (cid:3)1 has to be changed so that t1, t3, and t5are all among the best cars, i.e., they must not be dominated by any car in the inventory. That implies that the preferencesin the set CON1 consisting of the following preferences: the preference of t1 over t3, and the preference of t1, t3, and t4over t5 do not hold any more and need to be contracted (removed from (cid:3)1). Those preferences are shown as dashed arrowsin Fig. 1. Notice that since t2 is not in the inventory, and Mary has not explicitly provided any information regarding herpreferences involving t2, the preferences of t1 over t2 and t2 over t3, t4 and t5 remain unchanged.In the example above, we showed a simple scenario of preference contraction. The user preference relation there is afinite relation; and preferences to be contracted are elicited from the user-provided feedback. Variations of this scenarioare possible. First, the user’s preference relation may be infinite but representable by a finite preference formula. Second,a possibly infinite set of preferences to discard may also be defined by a formula.Example 2. Assume that Bob prefers newer cars, and given two cars made in the same year, the cheaper one is preferred.t (cid:3)2 t(cid:5) ≡ t.year > t(cid:5).year ∨ t.year = t(cid:5).year ∧ t.price < t(cid:5).pricewhere >, < denote the standard orderings of rational numbers, the attribute “year” defines the year when the car wasmade, and the attribute “price” – its price. The information about all cars which are in stock now is shown in the table r2below:idt1t3t4t5makeKiaVWKiaVWyear2007200720062006price12 00015 00015 0007000Then the set of the most preferred cars according to (cid:3)2 is w(cid:3)2 (r2) = {t1}. Assume that having observed the set w(cid:3)2 (r2),Bob understands that it is too narrow. He decides that the car t3 is not really worse than t1. He generalizes that by statingthat the cars made in 2007 which cost 12 000 are not better than the cars made in 2007 costing 15 000. Hence, the set ofpreferences the user wants to discard can be represented by the relation CON2(cid:3)(cid:5)(cid:2)t, tCON2≡ t.year = t(cid:5).year = 2007 ∧ t.price = 12 000 ∧ t(cid:5).price = 15 000The scenarios illustrated in the examples above have the following in common: we have a (finite or finitely representableinfinite) SPO preference relation (cid:3) and a set CON, finite of infinite, of preferences to discard. Our goal is to modify (cid:3), sothat the resulting preference relation is an SPO, and the preferences in CON do not hold anymore.\f1094D. Mindolin, J. Chomicki / Artificial Intelligence 175 (2011) 1092–1121Another important property of preference relation change is minimality. Indeed, a simple way of removing a subset of apreference relation without violating its SPO properties is to remove all the preferences from this relation. However, mostlikely it is not what the user expects. Hence, it is important to change the preference relation minimally.Fig. 2. Example 3.Example 3. Take Mary’s preferences from Example 1. A naive way to discard CON1 (CON1 = {t1t3, t1t5, t3t5, t4t5}) from (cid:3)1 isto represent the contracted preference relation as (cid:3)(cid:5)1 is not transitive1(and thus not an SPO): t1 (cid:3)(cid:5)1 t3, but t1 (cid:2)(cid:5)1 t5. Hence, this change does not preserveSPO. To make the changed preference relation transitive, some other preferences have to be discarded in addition to CON1.At the same time, discarding too many preferences is not a good solution since some of them may be important. Therefore,we need to discard a minimal subset of (cid:3)1 which contains CON1 and preserves SPO in t",
            {
                "entities": [
                    [
                        138,
                        196,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 245 (2017) 86–114Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintGraph aggregation ✩Ulle Endriss a, Umberto Grandi ba ILLC, University of Amsterdam, The Netherlandsb IRIT, University of Toulouse, Francea r t i c l e i n f oa b s t r a c tArticle history:Received 20 June 2016Received in revised form 6 January 2017Accepted 8 January 2017Keywords:Social choice theoryCollective rationalityImpossibility theoremsGraph theoryModal logicPreference aggregationBelief mergingConsensus clusteringArgumentation theory1. IntroductionGraph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). In this paper, we introduce a formal framework for graph aggregation grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. We consider both common properties of graphs, such as transitivity and reflexivity, and arbitrary properties expressible in certain fragments of modal logic. Our results establish several connections between the types of properties preserved under aggregation and the choice-theoretic axioms satisfied by the rules used. The most important of these results is a powerful impossibility theorem that generalises Arrow’s seminal result for the aggregation of preference orders to a large collection of different types of graphs.© 2017 Elsevier B.V. All rights reserved.Suppose each of the members of a group of autonomous agents provides us with a different directed graph that is defined on a common set of vertices. Graph aggregation is the task of computing a single graph over the same set of vertices that, in some sense, represents a good compromise between the various individual views expressed by the agents. Graphs are ubiquitous in computer science and artificial intelligence (AI). For example, in the context of decision support systems, an edge from vertex x to vertex y might indicate that alternative x is preferred to alternative y. In the context of modelling interactions taking place on an online debating platform, an edge from x to y might indicate that argument xThis work refines and extends papers presented at COMSOC-2012 [1] and ECAI-2014 [2]. We are grateful for the extensive feedback received from ✩Davide Grossi, Sylvie Doutre, Weiwei Chen, several anonymous reviewers, and the audiences at the SSEAC Workshop on Social Choice and Social Software held in Kiel in 2012, the Dagstuhl Seminar on Computation and Incentives in Social Choice in 2012, the KNAW Academy Colloquium on Dependence Logic held at the Royal Netherlands Academy of Arts and Sciences in Amsterdam in 2014, a course on logical frameworks for multiagent aggregation given at the 26th European Summer School in Logic, Language and Information (ESSLLI-2014) in Tübingen in 2014, the Lorentz Center Workshop on Clusters, Games and Axioms held in Leiden in 2015, the SEGA Workshop on Shared Evidence and Group Attitudes held in Prague in 2016, and lectures delivered at Sun Yat-Sen University in Guangzhou in 2014 as well as École Normale Supérieure and Pierre & Marie Curie University in Paris in 2016. This work was partly supported by COST Action IC1205 on Computational Social Choice. It was completed while the first author was hosted at the University of Toulouse in 2015 as well as Paris-Dauphine University, Pierre & Marie Curie University, and the London School of Economics in 2016.E-mail addresses: ulle.endriss@uva.nl (U. Endriss), umberto.grandi@irit.fr (U. Grandi).http://dx.doi.org/10.1016/j.artint.2017.01.0010004-3702/© 2017 Elsevier B.V. All rights reserved.\fU. Endriss, U. Grandi / Artificial Intelligence 245 (2017) 86–11487undercuts or otherwise attacks argument y. And in the context of social network analysis, an edge from x to y might express that person x is influenced by person y. How to best perform graph aggregation is a relevant question in these three domains, as well as in any other domain where graphs are used as a modelling tool and where particular graphs may be supplied by different agents or originate from different sources. For example, in an election, i.e., in a group decision making context, we have to aggregate the preferences of several voters. In a debate, we sometimes have to aggregate the views of the individual participants in the debate. And when trying to understand the dynamics within a community, we sometimes have to aggregate information coming from several different social networks.In this paper, we introduce a formal framework for studying graph aggregation in general abstract terms and we dis-cuss in detail how this general framework can be instantiated to specific application scenarios. We introduce a number of concrete methods for performing aggregation, but more importantly, our framework provides tools for evaluating what con-stitutes a “good” method of aggregation and it allows us to ask questions regarding the existence of methods that meet a certain set of requirements. Our approach is inspired by work in social choice theory [3], which offers a rich framework for the study of aggregation rules for preferences—a very specific class of graphs. In particular, we adopt the axiomatic methodused in social choice theory, as well as other parts of economic theory, to identify intuitively desirable properties of aggre-gation methods, to define them in mathematically precise terms, and to systematically explore their logical consequences.An aggregation rule maps any given profile of graphs, one for each agent, into a single graph, which we are often going to refer to as the collective graph. The central concept we focus on in this paper is the collective rationality of aggregation rules with respect to certain properties of graphs. Suppose we consider an agent rational only if the graph she provides has certain properties, such as being reflexive or transitive. Then we say that a given aggregation rule F is collectively rational with respect to that property of interest if and only if F can guarantee that that property is preserved during aggregation. For example, if we aggregate individual graphs by computing their union (i.e., if we include an edge from x to y in our collective graph if at least one of the individual graphs includes that edge), then it is easy to see that the property of reflexivity will always transfer. On the other hand, the property of transitivity will not always transfer. For example, if we aggregate two graphs over the set of vertices V = {x, y, z}, one consisting only of the edge (x, y) and one consisting only of the edge ( y, z), then although each of these two graphs is (vacuously) transitive, their union is not, as it is missing the edge (x, z). Thus, the union rule is collectively rational with respect to reflexivity, but not with respect to transitivity.We study collective rationality with respect to some such well-known and widely used properties of graphs, but also with respect to large families of graph properties that satisfy certain meta-properties. We explore both a semantic and a syntactic approach to defining such meta-properties. In our semantic approach, we identify three high-level features of graph properties that determine the kind of aggregation rules that are collectively rational with respect to them. For example, transitivity is what we call a “contagious” property: under certain circumstances, namely in the presence of edge ( y, z), inclusion of (x, y) spreads to (x, z). Transitivity also satisfies a second meta-property, which we call “implicativeness”: the inclusion of two specific edges, namely (x, y) and ( y, z), implies the inclusion of a third edge, namely (x, z). The third meta-property we introduce, “disjunctiveness”, expresses that, under certain circumstances, at least one of two specific edges has to be accepted. This is satisfied, for instance, by the property of completeness: every two vertices x and y need to be connected in at least one of the two possible directions. In our syntactic approach, we consider graph properties that can be expressed in particular syntactic fragments of a logical language. To this end, we make use of the language of modal logic [4]. This allows us to establish links between the syntactic properties of the language used to express the integrity constraints we would like to see preserved during aggregation and the axiomatic properties of the rules used.We prove both possibility and impossibility results. A possibility result establishes that every aggregation rule belonging to a certain class of rules (typically defined in terms of certain axioms) is collectively rational with respect to all graph properties that satisfy a certain meta-property. An impossibility result, on the other hand, establishes that it is impossible to define an aggregation rule belonging to a certain class that would be collectively rational with respect to any graph property that meets a certain meta-property—or that the only such aggregation rules would be clearly very unattractive for other reasons. Our main result is such an impossibility theorem. It is a generalisation of Arrow’s seminal result for preference aggregation [5], which we shall recall in Section 3.1. Our approach of working with meta-properties has two advantages. First, it permits us to give conceptually simple proofs for powerful results with a high degree of generality. Second, it makes i",
            {
                "entities": [
                    [
                        135,
                        152,
                        "TITLE"
                    ],
                    [
                        594,
                        611,
                        "TITLE"
                    ],
                    [
                        785,
                        802,
                        "TITLE"
                    ],
                    [
                        1218,
                        1235,
                        "TITLE"
                    ],
                    [
                        2142,
                        2159,
                        "TITLE"
                    ],
                    [
                        4447,
                        4464,
                        "TITLE"
                    ],
                    [
                        5125,
                        5142,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 88 (1996) 101-142 Artificial Intelligence A statistical approach to adaptive problem solving Jonathan Gratch ‘**, Gerald DeJong b,l a Information Sciences Institute, University of Southern California, 4676 Admiral@ Way, Marina de1 Rey, CA 90292, USA h Beckman Institute, University of Illinois, 405 N. Mathews, Urbana, IL 61801, USA Received May 1994; revised February 1995 Abstract Domain independent general purpose problem solving techniques are desirable from the stand- points of software engineering and human computer interaction. They employ declarative and modular knowledge representations and present a constant homogeneous interface to the user, untainted by the peculiarities of the specific domain of interest. Unfortunately, this very insulation from domain details often precludes effective problem solving behavior. General approaches have proven successful in complex real-world situations only after a tedious cycle of manual experi- mentation and modification. Machine learning offers the prospect of automating this adaptation cycle, reducing the burden of domain specific tuning and reconciling the conflicting needs of generality and efficacy. A principal impediment to adaptive techniques is the utility problem: even if the acquired information is accurate and is helpful in isolated cases, it may degrade overall problem solving performance under difficult to predict circumstances. We develop a formal char- acterization of the utility problem and introduce COMPOSER, a statistically rigorous learning approach which avoids the utility problem. COMPOSER has been successfully applied to learning heuristics for planning and scheduling systems. This article includes theoretical results and an extensive empirical evaluation. The approach is shown to outperform significantly several other leading approaches to the utility problem. 1. Introduction There is a wide gulf between general approaches and flective solving. Practical success has come from custom [ 52,661, or other application systems specific techniques techniques * Corresponding ’ E-mail: dejong@cs.uiuc.edu. author. E-mail: gratch@isi.edu. approaches to problem like expert systems, reactive that require extensive human 0004-3702/96/$15.00 PII SOOO4-3702(96)0001 l-2 Copyright @ 1996 Elsevier Science B.V. All rights reserved. \fto complete. AI researchers have also developed domain independent algo- and constraint satisfaction algorithms. Unfortunately, show success, systems, while derived from a general it is usually only after extensive domain specific technique, bear more investment rithms such as nonlinear planning when general approaches adjustments. The resulting resemblance Adaptive problem tradeoff performance solvers to work well all about performance irrelevant problem to real-world on unseen or unlikely problems, fact, machine problem solving performance, adaptive problem solving learning to the custom approaches. solving in repetitive problem for the problems on other hypothetical is a potential means solving they actually for circumventing this generality/ situations. We want our problem solving. Pragmatically, a system’s overall performance may be enhanced. encounter problems. Worst-case behavior and we care not at is largely by sacrificing good behavior In to enhance Nonetheless, the capacity techniques have successfully demonstrated although in limited contexts [ 18,46,57,65]. in any general sense. is still far from realized The principal impediment to adaptive problem introduced to machine hypothesized adaptation actually automatically formance. Steve Minton refer to this difficulty of insuring performance discussed via search control heuristics progress on this issue understood, solving performance [ 15,40,48,53], and can fail to improve performance, in the context of improving called control under certain circumstances. the problem results solving is characterizing when an in improved problem solving per- to the term utility problem the average problem [ 531. Minton originally solving speed learning improvements rules. While there has been considerable the proposed methods are often ad hoc, poorly or worse, actually degrade problem Adaptivity can be an effective method for improving problem solving performance in through that to deduce a priori, as in the blocksworld domain where a block can never represented. On the other hand, the that real-world problems are often constrained experience. On one hand, the domain specification may implicitly embed constraints are difficult be atop itself, though distribution of tasks embeds many constraints For example, greater that can only be induced application may never contain from experience. towers of height in ways that only become obvious a particular blocksworld is not explicitly this constraint than three. Exploiting these regularities from characterizations can lead to clear performance of past problems. Most distributions improvements. Of course, look into the future to anticipate particular problems. However, the problem solver cannot exhibit it can generalize peculiarities intractable that may be exploited once detected. Conversely, even worst-case algorithms may perform well under certain distributions. For example, Goldberg suggests solved in 0( n2) time [ 221. that naturally occurring [ 5,561 or devising Recent work has focused on characterizing [3]. techniques it is available Research optimization [45] exploit construction [ 51, pp. 252-2851 the expected distribution these easy distributions information when that can exploit specific distribution satisfiability problems are frequently the fact of a problem better expected performance. solver with substantially of tasks can allow into self-organizing and dynamic that learning systems the In the next section, we provide a formal characterization decision problem theoretic solving performance terms. This introduces and casts in the notion of expected utility as a metric of through a space of the learning of the utility problem as a search \fJ. Gratch, G. DeJong/Ar@cial Intelligence 88 (1996) 101-142 103 p&~!iEEJcy Fig. I. Learning transforms an initial problem solver into a new one. To accomplish this the learner must choose one of a set of possible transformations. several techniques transformations the COMPOSER algorithm, performs a probabilistic to ensure problem solving 3 describes problem. COMPOSER and incorporates describes an extensive evaluation of the approach for a domain compares average case analysis of the algorithm’s to be polynomial confidence favorably with existing approaches independent problem in the number of transformations a probabilistic for a problem solver with high expected utility. Section the utility approach space search through the efficiency of this process. Section 4 in the context of learning control rules that COMPOSER to avoiding the transformation solver. The experiments indicate complexity. COMPOSER’s to the utility problem. Section 5 provides an run time is shown in the statistical considered and required. Finally, we discuss some limitations and present conclusions. 2. A formal characterization of the utility problem algorithm learning Before a rigorous terize what the learning formal characterization makes precise and explicit should do, and it provides a structure and precise statements. can be constructed, we must explicitly charac- to achieve. In this section, we introduce a is attempting system of a general class of learning problem solvers. This formalism intuitive and often unstated notions of what a learning system for formal analysis, enabling us to make definitive Abstractly, a learning algorithm operates on an initial problem solver, transforming relative the effect of this change criterion and a pattern of tasks associated with the intended application. solver, where is assessed it to into a different problem some evaluation This is illustrated set of potential problem transformations To characterize utility [2] as a common argued for the merits of decision systems [ 41,64,67,7 1 ] and machine in Fig. 1 where a set of hypothesized transformations defines a solvers. The learning to adopt, where “good” outcomes, we use the decision the outcome of this decision framework for characterizing system must decide which hypothesized solver. theoretic notion of expected this decision problem. Doyle has is a new problem [ 131 and it has seen increasing theory as a standard for evaluating artificial acceptance, both in artificial intelligence intelligence at large learning in particular [ 30,35,45,69]. 2.1. Expected utility Decision theory relies on the observation can, under some natural assumptions, that preferences over different outcomes be characterized by a real-valued utility function. \f104 J. Gratch. G. DeJoq/Art@cial Intelligence RR (1996) 101-142 Outcome A is preferred to outcome B iff the utility of A is greater than the utility of B. the same outcome. Even In an uncertain world, a decision may not always produce the next. The correct the best decision policy may do very well one time and poorly decision policy under uncertainty maximizes is characterized by a set of outcomes and a probability distribution over this set, and the expected utility of a decision of occurrence. is the utility of all possible outcomes weighted by their probability expected utility: a decision For learning, the characteristics solvers) problem (transformed preferences with a utility of choosing application a transformation are preferred. With decision of the intended application determine what outcomes these theory, we represent to be cast as the decision problem the learning expected utility. However, we require function, allowing that increases to obey the following two restrictions. probability as a random selection of tasks according Fixed distribution assumption. The pattern of tasks in the problem ment must be characterizable unknown) that there is some probability of occurr",
            {
                "entities": [
                    [
                        75,
                        125,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 696–721Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintProbabilistic planning with clear preferences on missing informationMaxim Likhachev a,∗, Anthony Stentz ba Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USAb The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 8 October 2007Received in revised form 27 October 2008Accepted 29 October 2008Available online 25 November 2008Keywords:Planning with uncertaintyPlanning with missing informationPartially Observable Markov DecisionProcessesPlanningHeuristic searchFor many real-world problems, environments at the time of planning are only partially-known. For example, robots often have to navigate partially-known terrains, planes oftenhave to be scheduled under changing weather conditions, and car route-finders oftenhave to figure out paths with only partial knowledge of traffic congestions. While generaldecision-theoretic planning that takes into account the uncertainty about the environmentis hard to scale to large problems, many such problems exhibit a special property: onecan clearly identify beforehand the best (called clearly preferred) values for the variablesthat represent the unknowns in the environment. For example, in the robot navigationproblem, it is always preferred to find out that an initially unknown location is traversablerather than not, in the plane scheduling problem, it is always preferred for the weather toremain a good flying weather, and in route-finding problem, it is always preferred for theroad of interest to be clear of traffic. It turns out that the existence of the clear preferencescan be used to construct an efficient planner, called PPCP (Probabilistic Planning with ClearPreferences), that solves these planning problems by running a series of deterministic low-dimensional A*-like searches.In this paper, we formally define the notion of clear preferences on missing information,present the PPCP algorithm together with its extensive theoretical analysis, describe severaluseful extensions and optimizations of the algorithm and demonstrate the usefulnessof PPCP on several applications in robotics. The theoretical analysis shows that onceconverged, the plan returned by PPCP is guaranteed to be optimal under certain conditions.The experimental analysis shows that running a series of fast low-dimensional searchesturns out to be much faster than solving the full problem at once since memoryrequirements are much lower and deterministic searches are orders of magnitude fasterthan probabilistic planning.© 2008 Elsevier B.V. All rights reserved.1. IntroductionA common source of uncertainty in planning problems is lack of full information about the environment. A robot maynot know the traversability of the terrain it has to traverse, an air traffic management system may not be able to forecastwith certainty future weather conditions, a car route-finder may not be able to predict well future traffic congestions oreven be sure about present traffic conditions, a shopping planner may not know whether a particular item will be on saleat one of the stores it considers. Ideally, in all of these situations, to produce a plan, a planner needs to reason over theprobability distribution over all the possible instances of the environment. Such planning is known to be hard [1,2].* Corresponding author.E-mail address: maximl@seas.upenn.edu (M. Likhachev).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.10.014\fM. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721697For many of these problems, however, one can clearly name beforehand the “best” values of the variables that representthe unknowns in the environment. We call such values clearly preferred values. Thus, in the robot navigation problem, it isalways preferred to find out that an initially unknown location is traversable rather than not. In the air traffic managementproblem it is always preferred to have a good flying weather. In the problem of route planning under partially-known trafficconditions, it is always preferred to find out that there is no traffic on the road of interest. And finally, in the shoppingplanning example, it is always preferred for a store to hold a sale on the item of interest. These are just few of what webelieve to be a large class of planning problems that exhibit clear preferences on missing information. One of the reasonsfor this is that the knowledge of clear preferences on missing information is not the same as the knowledge of a best actionat a state or the value of an optimal policy. Instead, we often know at intuitive level what would be the best event for us(i.e., no traffic congestion, sale, etc.), independently of whether we choose to make use of this event or not. All the otheroutcomes, on the other hand, are of less preference to us. This intuitive information can be used in planning.In this paper we present an algorithm called PPCP (Probabilistic Planning with Clear Preferences) that is able to scaleup to very large problems by exploiting the fact that these preferences exist. PPCP constructs and refines a plan by runninga series of deterministic A*-like searches. Furthermore, by making an approximating assumption that it is not necessaryto retain information about the variables whose values were discovered to be clearly preferred values, PPCP keeps thecomplexity of each search low and independent of the amount of the missing information. Each search is extremely fast,and running a series of fast low-dimensional searches turns out to be much faster than solving the full problem at once sincethe memory requirements are much lower and deterministic searches can often be many orders of magnitude faster thanprobabilistic planning techniques. While the assumption PPCP makes does not need to hold for the algorithm to converge,the returned plan is guaranteed to be optimal if the assumption does hold.The paper is organized as follows. We first briefly go over A* search and explain how it can be used to find least-costpaths in graphs. We then explain how a planning problem changes when some of the information about the environmentis missing. In Section 4, we introduce the notion of clear preferences on missing information and briefly talk about theproblems that exhibit them. In Section 5, we explain the PPCP algorithm and how it makes use of the clear preferences. Thesame section gives an extensive theoretical analysis of PPCP that includes the correctness of the algorithm, some complexityresults as well as the conditions for the optimality of the plan returned by PPCP. In Section 6 of the paper, we describe twouseful extensions of the algorithm such as how one can interleave PPCP planning and execution. In the same section, wealso give two optimizations of the algorithm which at least for some problems can speed it up by more than a factor offour. On the experimental side, Section 7 shows how PPCP enabled us to successfully solve the path clearance problem, animportant problem in defense robotics. The experimental results in Section 8.1, on the other hand, evaluate the performanceof PPCP on the problem of robot navigation in partially-known terrains. They show that in the environments small enoughto be solved with methods guaranteed to converge to an optimal solution (such as Real-Time Dynamic Programming [3]),PPCP always returns an optimal policy while being much faster. The results also show that PPCP is able to scale up to large(costmaps of size 500 by 500 cells) environments with thousands of initially unknown locations. The experimental resultsin Section 8.2, on the other hand, show that PPCP can also solve large instances of path clearance problem and results insubstantial benefits over other alternatives. We finally conclude the paper with a short survey of related work, discussion,and conclusions.2. Backward A* search for planning with complete informationNotations. Let us first consider a planning problem that can be represented as a search for a path in a fully knowndeterministic graph G. The fact that the graph G is completely known at the time of planning means that there is nomissing information about the domain (i.e., environment). We use S to denote a state (a vertex, in the graph terminology)in the graph G. State Sstart refers to the state of the agent at the time of planning, while state Sgoal refers to the desiredstate of the agent. We use A(S) to represent a set of actions available to the agent at state S ∈ G. Each action a ∈ A(S)corresponds to a transition (i.e., an edge) in the graph G from state S to the successor state denoted by succ(S, a). Eachsuch transition is associated with the cost c(S, a, succ(S, a)). The costs need to be bounded from below by a (small) positiveconstant.Backward A* search. The goal of shortest path search algorithms such as A* search [4] is to find a path from S start toSgoal for which the cumulative cost of the transitions along the path is minimal. The PPCP algorithm we present in thispaper is based on running a series of deterministic searches. Each of these searches is a modified backward A* search—theA* search that searches from Sgoal towards Sstart by reversing all the edges in the graph. In the following, we thereforebriefly describe the operation of a backward A* search.∗(S).Suppose for every state S ∈ G we knew the cost of a least-cost path from S to S goal. Let us denote such cost by gThen a least-cost path from Sstart to Sgoal can be easily followed by starting at Sstart and always executing such action∗(succ(S, a)). Consequently, A* search tries to computea ∈ A(S) at any state S that a = arg mina∈ A(S)(c(S, a, succ(S, a)) + g∗-values. In particular, A* maintains g-values for each state it has visited so far. g(S) is always the cost of the best pathgfound so fa",
            {
                "entities": [
                    [
                        136,
                        204,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 300 (2021) 103563Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAbstraction for non-ground answer set programs ✩Zeynep G. Saribatur∗, Thomas Eiter, Peter SchüllerInstitute of Logic and Computation, TU Wien, Favoritenstraße 9-11, A-1040 Vienna, Austriaa r t i c l e i n f oa b s t r a c tArticle history:Received 20 December 2019Received in revised form 11 May 2021Accepted 21 July 2021Available online 28 July 2021Keywords:AbstractionAnswer set programmingDeclarative problem solvingKnowledge representation and reasoningNonmonotonic formalismsExplaining unsatisfiabilityCounterexample-guided abstraction and refinementAbstraction is an important technique utilized by humans in model building and problem solving, in order to figure out key elements and relevant details of a world of interest. This naturally has led to investigations of using abstraction in AI and Computer Science to simplify problems, especially in the design of intelligent agents and automated problem solving. By omitting details, scenarios are reduced to ones that are easier to deal with and to understand, where further details are added back only when they matter. Despite the fact that abstraction is a powerful technique, it has not been considered much in the context of nonmonotonic knowledge representation and reasoning, and specifically not in Answer Set Programming (ASP), apart from some related simplification methods. In this work, we introduce a notion for abstracting from the domain of an ASP program such that the domain size shrinks while the set of answer sets (i.e., models) of the program is over-approximated. To achieve the latter, the program is transformed into an abstract program over the abstract domain while preserving the structure of the rules. We show in elaboration how this can be also achieved for single or multiple sub-domains (sorts) of a domain, and in case of structured domains like grid environments in which structure should be preserved. Furthermore, we introduce an abstraction-&-refinement methodology that makes it possible to start with an initial abstraction and to achieve automatically an abstraction with an associated abstract answer set that matches an answer set of the original program, provided that the program is satisfiable. Experiments based on prototypical implementations reveal the potential of the approach for problem analysis, by its ability to focus on the parts of the program that cause unsatisfiability and by achieving concrete abstract answer sets that merely reflect relevant details. This makes domain abstraction an interesting topic of research whose further use in important areas like Explainable AI remains to be explored.© 2021 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).1. IntroductionAbstraction is a technique applied in human reasoning and understanding, by reasoning over the models of the world that are built mentally [30,68]. Although its meaning comes from “to draw away”, there is no precise definition that is capable of covering all meanings that abstraction has in its utilizations. There is a variety of interpretations in different Some of the results in this article were presented in preliminary form at JELIA 2019 [113] and XAI 2019 [45]. This work has been partially supported ✩by the Austrian Science Fund (FWF) grant W-1255 and by the EU’s H2020 research and innovation programme under grant agreements 825619 (AI4EU) and 952026 (HumanE-AI Net).* Corresponding author.E-mail addresses: zeynep@kr.tuwien.ac.at (Z.G. Saribatur), eiter@kr.tuwien.ac.at (T. Eiter), ps@kr.tuwien.ac.at (P. Schüller).https://doi.org/10.1016/j.artint.2021.1035630004-3702/© 2021 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fZ.G. Saribatur, T. Eiter and P. SchüllerArtificial Intelligence 300 (2021) 10356322blue14536red145green36red76 5981249(a) 3-coloring of a graph(b) SudokuFig. 1. Use of abstraction.disciplines such as Philosophy, Cognitive Science, Art, Mathematics and Artificial Intelligence, with the shared consensus of the aim to “distill the essential” [108]. Among them is the capability of abstract thinking, which is achieved by removing irrelevant details and identifying the “essence” of a problem [71]. The notion of relevance is especially important in problem solving, as a problem may become too complex to solve if every detail is taken into account. A good strategy to solve a complex problem is to start with a coarse solution and then refine it by adding back more details. When planning a trip, for instance, one may first pick the destination and determine a coarse travel plan; fleshing out the precise details of the travel, such as taking the subway to the airport, comes later. This may be done in a hierarchy of levels of abstraction, with the lowest level containing all of the details. Another view of abstraction is the generalization aspect, which is singling out the relevant features and properties shared by objects. For example, features of an airplane such as color and cargo capacity with their possible differences may be irrelevant to the travel plan; we are (mostly) only interested in the fact that there is an airplane that takes us from Vienna to New York, say. Overall, the general aim of abstraction is to simplify the problem at hand to one that is easier to understand and deal with.For solving combinatorial problems and figuring out the key elements, humans arguably employ abstraction. In Artificial Intelligence, such problems vary from planning problems like in which order to move blocks to achieve a final configuration, to solving constraint problems such as finding an admissible coloring of the nodes of a given graph. In the latter problem, for instance, isolated nodes can be viewed as a single node and colored the same without thinking about the specific details (Fig. 1a). If a given graph is non-colorable, then we may try to find some subgraph (e.g., a clique) which causes the unsolvability, and we would not care about other nodes in the graph. Similarly with the blocks: if the labels are not important, we would disregard them when figuring out the actions. If the goal configuration cannot be achieved from the initial one, we would aim to find out the particular blocks that cause this.Notably, such disregard of detail also occurs for problems with multi-dimensional structures such as grid-cells in the well-known Sudoku problem, where a partially filled 9 × 9 board must be completed by filling in numbers 1..9 into the empty cells under constraints. If an instance is unsolvable, the reason can only be meaningfully grasped by a human by focusing on the relevant sub-regions, as looking at the whole grid is too complex. For illustration, Fig. 1b shows the sub-regions of an instance that contain the reason why no solution exists: as 6 and 7 occur in the middle column, they must appear in the sub-region below in the left column, which is unfeasible as there is only one empty cell. All these examples demonstrate abstraction abilities of humans that come naturally.Due to its important role in knowledge representation and in reasoning, abstraction has been explored in AI research early on as a useful tool for problem solving: solve a problem at hand first in an abstracted space, and then use the abstract solution as a heuristic to guide the search for a solution in the original space [70,92,106]. This approach was used in planning for speeding up the solving [64] and especially for computing heuristic functions to guide the plan search in the state space. Several abstraction methods were introduced towards this direction, especially to automatically compute abstractions that give a good heuristic [38,61,116]. However, it is well known that the success in solving a problem relies on how “good” the abstraction is. For this, theoretical approaches for defining abstractions with desired properties have been investigated [59,90]. Apart from gaining efficiency (which however may not always materialize [8,63]), abstraction forms a basis to obtain high-level explanations and an understanding of a problem. For more details on these and other related works see Section 7.3.Abstraction has been studied in other areas of AI and Computer Science as well, among them model-based diagnosis [23,89], constraint satisfaction [13,52], theorem proving [102], to name a few. Particularly fruitful were applications in model checking, which is a highly successful approach to computer aided verification [27], to tackle the state explosion problem by property preserving abstractions [26,32,82]. Furthermore, the seminal counterexample guided abstraction refinement (CEGAR) method [25] allows for automatic generation of such abstractions, by starting from an initial abstraction that over-approximates the behavior of a system to verify, and then stepwise refining the abstraction as long as needed, i.e., as long as spurious (false) counterexamples exist.Abstraction for Answer Set Programming. Answer Set Programming (ASP) [18,79] is a declarative problem solving paradigm that is rooted in knowledge representation, logic programming, and nonmonotonic reasoning. A problem is represented by a non-monotonic logic program whose answer sets (also called “stable models” [57]) correspond to the solutions of the problem. Thanks to the availability of efficient solvers and the expressiveness of the formalism, ASP has been gaining 2\fZ.G. Saribatur, T. Eiter and P. SchüllerArtificial Intelligence 300 (2021) 103563popularity for applications in many areas of AI and beyond, cf. [47–49] and references therein, from combinatorial search problems (e.g. configuration, diagnosis, planning) over system modeling (e.g., behavior of dynamic systems, beliefs and actions of agents) to knowledge-in",
            {
                "entities": [
                    [
                        135,
                        181,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 309 (2022) 103738Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA tetrachotomy of ontology-mediated queries with a covering axiomOlga Gerasimova a, Stanislav Kikot b, Agi Kurucz c,∗Michael Zakharyaschev ea HSE University, Moscow, Russiab Institute for Information Transmission Problems, Moscow, Russiac Department of Informatics, King’s College London, UKd Steklov Mathematical Institute, Moscow, Russiae Department of Computer Science and Information Systems, Birkbeck, University of London, UK, Vladimir Podolskii d,a, a r t i c l e i n f oa b s t r a c tArticle history:Received 19 July 2020Received in revised form 4 May 2022Accepted 5 May 2022Available online 13 May 2022Keywords:Ontology-mediated queryDescription logicDatalogDisjunctive datalogFirst-order rewritabilityData complexityOur concern is the problem of efficiently determining the data complexity of answering queries mediated by description logic ontologies and constructing their optimal rewritings to standard database queries. Originated in ontology-based data access and datalog optimisation, this problem is known to be computationally very complex in general, with no explicit syntactic characterisations available. In this article, aiming to understand the fundamental roots of this difficulty, we strip the problem to the bare bones and focus on Boolean conjunctive queries mediated by a simple covering axiom stating that one class is covered by the union of two other classes. We show that, on the one hand, these rudimentary ontology-mediated queries, called disjunctive sirups (or d-sirups), capture many features and difficulties of the general case. For example, answering d-sirups is (cid:2)p2 -complete for combined complexity and can be in AC0 or L-, NL-, P-, or coNP-complete for data complexity (with the problem of recognising FO-rewritability of d-sirups being 2ExpTime-hard); some d-sirups only have exponential-size resolution proofs, some only double-exponential-size positive existential FO-rewritings and single-exponential-size nonrecursive datalog rewritings. On the other hand, we prove a few partial sufficient and necessary conditions of FO- and (symmetric/linear-) datalog rewritability of d-sirups. Our main technical result is a complete and transparent syntactic AC0/NL/P/coNP tetrachotomy of d-sirups with disjoint covering classes and a path-shaped Boolean conjunctive query. To obtain this tetrachotomy, we develop new techniques for establishing P- and coNP-hardness of answering non-Horn ontology-mediated queries as well as showing that they can be answered in NL.© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).* Corresponding author.(V. Podolskii), michael@dcs.bbk.ac.uk (M. Zakharyaschev).E-mail addresses: ogerasimova@hse.ru (O. Gerasimova), staskikotx@gmail.com (S. Kikot), agi.kurucz@kcl.ac.uk (A. Kurucz), podolskii@mi.ras.ruhttps://doi.org/10.1016/j.artint.2022.1037380004-3702/© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fO. Gerasimova, S. Kikot, A. Kurucz et al.Artificial Intelligence 309 (2022) 1037381. Introduction1.1. The ultimate questionThe general research problem we are concerned with in this article can be formulated as follows: for any given ontology-mediated query (OMQ, for short) Q = (O, q) with a description logic ontology O and a conjunctive query q,(data complexity) determine the computational complexity of answering Q over any input data instance A under the open (rewritability) reduce the task of finding certain answers to Q over any input A to the task of evaluating a conventional is then called a rewriting of the OMQ with optimal data complexity directly over A (the query Q(cid:3)(cid:3)world semantics and, if possible,database query QQ ).Ontology-based data access Answering queries mediated by a description logic (DL) ontology has been known as an im-portant reasoning problem in knowledge representation since the early 1990s [1]. The proliferation of DLs and their applications [2,3], the development of the (DL-underpinned) Web Ontology Language OWL,1 and especially the paradigm of ontology-based data access (OBDA) [4–6] (proposed in the mid 2000s and recently rebranded to the virtual knowledge graph (VKG) paradigm [7]), have made theory and practice of answering ontology-mediated queries (OMQs) a hot research area lying at the crossroads of Knowledge Representation and Reasoning, Semantic Technologies and the Semantic Web, Knowledge Graphs, and Database Theory and Technologies.In a nutshell, the idea underlying OBDA is as follows. The users of an OBDA system (such as Mastro2 or Ontop3) may assume that the data they want to query is given in the form of a directed graph whose nodes are labelled with concepts (unary predicates or classes) and whose edges are labelled with roles (binary predicates or properties)—even though, in reality, the data can be physically stored in different and possibly heterogeneous data sources—hence the moniker VKG. The concept and role labels come from an ontology, designed by a domain expert, and should be familiar to the intended users who, on the other hand, do not have to know anything about the real data sources. Apart from providing a user-friendly vocabulary for queries and a high-level conceptual view of the data, an important role of the ontology is to enrich possibly incomplete data with background knowledge. To illustrate, imagine that we are interested in the life of ‘scientists’ and would like to satisfy our curiosity by querying the data available on the Web (it may come from the universities’ databases, publishing companies, personal web pages, social networks, etc.). An ontology O about scientists, provided by an OBDA system, might contain the following ‘axioms’ (given, for readability, both as DL concept inclusions and first-order sentences):BritishScientist (cid:4) ∃ affiliatedWith.UniversityInUK∀x [BritishScientist(x) → ∃ y (affiliatedWith(x, y) ∧ UniversityInUK( y))]∃ worksOnProject (cid:4) Scientist∀x [∃ y worksOnProject(x, y) → Scientist(x)]Scientist (cid:9) ∃ affiliatedWith.UniversityInUK (cid:4) BritishScientist∀x [(Scientist(x) ∧ ∃ y (affiliatedWith(x, y) ∧ UniversityInUK( y))) → BritishScientist(x)]BritishScientist (cid:4) Brexiteer (cid:10) Remainer∀x [BritishScientist(x) → (Brexiteer(x) ∨ Remainer(x))]Now, to find, for example, British scientists, we could execute a simple OMQ Q (x) = (O, q(x)) with the query(1)(2)(3)(4)q(x) = BritishScientist(x)mediated by the ontology O. The OBDA system is expected to return the members of the concept BritishScientist that are extracted from the original datasets by ‘mappings’ (database queries connecting the data with the ontology vocabulary and virtually populating its concepts and roles) and also deduced from the data and axioms in O such as (3). It is this latter reasoning task that makes OMQ answering non-trivial and potentially intractable both in practice and from the complexity-theoretic point of view.1 https://www.w3 .org /TR /owl2 -overview/.2 https://www.obdasystems .com.3 https://ontopic .biz.2\fO. Gerasimova, S. Kikot, A. Kurucz et al.Artificial Intelligence 309 (2022) 103738Uniform approach To ensure theoretical and practical tractability, the OBDA paradigm presupposes that the users’ OMQs are reformulated—or rewritten—by the OBDA system into conventional database queries over the original data sources, which have proved to be quite efficiently evaluated by the existing database management systems. Whether or not such a rewriting is possible and into which query language naturally depends on the OMQ in question. One way to uniformlyguarantee the desired rewritability is to delimit the language for OMQ ontologies and queries. Thus, the DL-Lite family of description logics [5] and the OWL 2 QL profile4 of OWL 2 were designed so as to guarantee rewritability of all OMQs with a DL-Lite ontology and a conjunctive query (CQ) into first-order (FO) queries, that is, essentially SQL queries [8]. In complexity-theoretic terms, FO-rewritability of an OMQ means that it can be answered in LogTime uniform AC0, one of the smallest complexity classes [9]. In our example above, only axioms (1) and (2) are allowed by OWL 2 QL. Various dialects of tuple-generating dependencies (tgds), aka datalogor existential rules, that admit FO-rewritability and extend OWL 2 QLhave also been identified; see, e.g., [10–13].±Any OMQ with an EL, OWL 2 EL or HornSHIQ ontology is datalog-rewritable [14–17], and so can be answered inP—polynomial time in the size of data—using various datalog engines, say GraphDB,5 LogicBlox6 or RDFox.7 Axioms (1)–(3)are admitted by the EL syntax. On the other hand, OMQs with an ALC (a notational variant of the multimodal logicKn [18]) ontology and a CQ are in general coNP-complete [1], and so often regarded as intractable and not suitable for OBDA, though they can be rewritten to disjunctive datalog [19–21] supported by systems such as DLV8 or clasp.9 For example, coNP-complete is the OMQ ({(4)}, q1) with the CQ= ∃w, x, y, z [Brexiteer(w) ∧ hasCoAuthor(w, x) ∧ Remainer(x) ∧q1hasCoAuthor(x, y) ∧ Brexiteer( y) ∧ hasCoAuthor( y, z) ∧ Remainer(z)](see also the representation of q1 as a labelled graph below). It might be of interest to note that by making the role hasCoAuthor symmetric using, for example, the role inclusion axiomhasCoAuthor (cid:4) hasCoAuthor−∀x, y [hasCoAuthor(x, y) → hasCoAuthor( y, x)](5)we obtain the OMQ ({(4), (5)}, q1), which is rewritable to a symmetric datalog query, and so can be answered by a highly parallelisable algorithm in the complexity class L (logarithmic space).For various reasons, many existing ontologies do not comply with the restrictions imposed by the standard languages for OBDA. Notable examples include the large-sca",
            {
                "entities": [
                    [
                        135,
                        200,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 89 ( 1997) 173-217 Artif’icial Intelligence Permissive planning: extending classical planning to uncertain task domains Gerald F. DeJongayb-*, Scott W. Bennett c a Department of Computer Science, University of Illinois at Urbana-Champaign, 405 North Matthews Ave., Urbana, IL 61801, USA h Beckman Instihde, University of Illinois at Urbana-Champaign, 405 North Matthews Ave., Urbana, IL 61801, USA ’ SRA Corporation, 2000 15th St. North, Arlington, VA, USA Received May 1995; revised July 1996 Abstract Uncertainty, inherent significant, even prohibitive, plans. On the other hand, reasoning with representations engender novel approach is employed planning. Machine Thus, the classical planner is conditioned in the world. to planning learning in most real-world domains, can cause failure of apparently sound classical can a costs. This paper contributes additional computational reflect uncertainty that explicitly in uncertain domains. The approach is an extension of classical failures. to adjust planner bias in response to execution towards producing plans that tend to work when executed ability domain learning. The user-supplied The planner’s representations are simple and crisp; uncertainty only during the planner’s projection of the planner’s bias space The learning converges using no more than a polynomial number of examples. The system probabilistically that adequate planning system is represented and reasoned about theory is left intact. The operator definitions and them. Some structuring scales well. then that either the plans produced will achieve their goal when executed or robotic is required. But with suitable structuring theory provided. An implemented is not possible with the domain remain as the domain expert the approach is described. guarantees intended Keywords: Planning; Learning; Uncertainty; Machine learning; Explanation-based learning; Planning bias * Corresponding author. E-mail: dejong@cs.uiuc.edu. 0004-3702/97/$17.00 PIISOOO4-3702(96)00031-8 Copyright @ 1997 Elsevier Science B.V. All rights reserved \f174 G.E DeJong, SW! Bennett/Artijicial Intelligence 89 (1997) 173-217 1. Introduction We seek a formal bridge between classical planning and real-world is wide and deep. Classical planners hypothetical worlds to be spanned in micro-worlds, to achieve a micro-world ment. The chasm with behavior Classical plans are often proven when executed. We can trust that the micro-world if the planner’s only performance of the real world. This is seldom only approximate may conspire to invalidate a plan in the real world. goal only to produce an unacceptable thwarted by the real world; a sequence of actions may be state behavior is indicative of real-world perfectly capture all relevant details the planner’s domain representations the case. Often, representations real-world their real-world changes. Combinations of apparently innocuous flaws that can be flawlessly goal achieve- are concerned formalized. One cause of this difficulty is the qualification problem [46], which is succinctly stated by Genesereth and Nilsson [ 271: Most universally qualifications quantified statements will have to include an infinite number of if they are to be interpreted as accurate statements about the world. Classical operator definitions can, therefore, only imperfectly initial world state may also be imperfect. Thus, encounter are universally situations capture most real-world changes. Representations quantified first order expressions and of the that a planner may it is inescapable in which its conclusions will contradict the real world. ingredients: milk, Permissive planning is a learning approach and so on. There are many possible the simple problem of making breakfast includes many standard cooking to planning under uncertainty. To illustrate in the morning. The initial state it, consider flour, eggs, butter, of our kitchen of hot and cold cereal, syrup, a selection of fresh and dried fruit, etc. The containers include pouring, measuring, mixing, goal is to reduce our morning hunger. Operators heating, in principle produce for us. It could make pancakes, crepes, Belgian waffles, hot oatmeal, cold raisin bran, sausage and eggs, gravy and biscuits, eggs Benedict, etc. We call the set of all solutions for the problem. it will construct just one. This designated of the planner from the competence of planning, Suppose set we call the performance for a particular performance set the planner’s bias. As we shall see, bias is an inescapable is at the heart of permissive planning. element of the competence for the problem. We call the preference that could In fact, of course, the planner will not generate all solutions; item for the breakfast problem that the planner’s performance solutions our planner could in principle be produced set of the planner and its adjustment item item facet the cdmpetence is, of course, entailed in the real world may be quite unsatisfactory. A perfectly sauce reducing in lumpy and separated hollandaise tasteless mess. The problem eggs Benedict. Goal achievement the result plan might result dish to a disgusting internal Perhaps some initial state representations represented incomplete, believed or the beating may result or the butter or based upon representations are not sufficiently in the planner’s micro-world, is to prepare but looking reasonable the eggs Benedict is that one or more of the system’s to succeed. for the plan than are incorrect, eggs are warmer the operators faithful to reality are subtly wrong-the false suppositions-the blender may not beat as fast as in an unmodeled heating of the mixture. Most likely is too low a grade, or perhaps \fG.E DeJong, S. WI Bennett/Art@cial Intelligence 89 (1997) 173-217 the blame cannot be laid at a single representation. Usually multiple discrepancies, of which is slight, conspire the failure. to produce together 175 each Permissive planning, when confronted with an unacceptable the solutions planner bias. This shifts different eggs Benedict prepared. There are nearly always planning work; few problems admit to only a single solution. eggs Benedict It is foolish ignoring all the attractive other breakfast options. to different performance recipe might be attempted or perhaps a different dish entirely alternatives within the micro-world failure rate, adjusts the items. For breakfast a is frame- foul to persist making Why might preserving the micro-world be desirable? The system’s operator represen- the expert did his of conceptualization idiosyncratic problem assures us that imperfections upon one is to be expected and is hardly evidence tations are typically provided by a human domain expert. Presumably, best at capturing his own coherent although possibly world change. The qualification Stumbling prove upon the work of the domain expert. Furthermore, knowledge may corrupt latent inconsistencies is likely or no understanding planner’s the domain A major It is supplied by the planner of the domain. And yet each bias has grave representations. issue raised by permissive It may be more prudent in particular domains. to be quite arbitrary. its faithfulness is that bias implementor planning success to the expert’s conceptualization of a far worse nature. On the other hand, the planner’s and introduce intial bias and reflects little for the to alter the bias over implications are unavoidable. that the system can im- tinkering with human-supplied of the for at once. It may be that the solutions generated with the old bias were failures. Permissive the bias may shift the performance the plans now produced element produced is a characteristic to real-world lead some guarantee of global acceptable behavior. Much of our planner as a whole. Altering many problems quite acceptable planning must provide research concerns this point. but fall into includes Bayesian reasoning theoretic approaches Let us briefly examine how permissive planning certainty. These three general categories. provided with some explicit account of uncertainty general approach fuzzy logic [ 871, and decision in this general area are given representations than required by conventional characterization takes such variabilities can view each representation each representation is less likely cost in reasoning with augmented of possible discrepancies to other approaches relates In the first, reasoning and how it is propagated [ 15,661, planning with error balls to un- systems are [ 181. This [ 251, [ 19,30,3 1,741. Systems of the world that are more sophisticated some to be encountered. The planner in the world. We large disjunction of facts. Thus, there is in the real world. Importantly, to be violated representations which can be quite high in deciding how best to behave classical planning. These that are likely representations as specifying into account a potentially to planning include is employed In the second approach, machine sentations. As in classical planning, simple and precise but possibly through information into line with the observed world behavior. This can be seen as an induction problem: The hypothesis the initial domain repre- learning are the system’s action and object representations incorrect. As the system interacts with the world, it gains it alterations of real-world behaviors. The task is theory. Examples are the observed space consists of some (usually its observations. The domain theory is suitably modified large) set of well-formed to bring [ 731. to refine the offending \f176 G.E DeJong, S. W Bennett/Artijicial Intelligence 89 (1997) 173-217 the hypothesis to locate finement and discovery and refinement with theories of action (e.g. that best fits the examples. This may involve general (e.g. [ 28,35,83] theory re- learning ). The reasearch connecting belief revision and update ) or, more conservatively, [ 61,64,65,70] operator [ 11,23,40] is also relevant. the sequence of the planner-specified [ 1,12,49,60,76]. While conventional The third approach reli",
            {
                "entities": [
                    [
                        68,
                        143,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ELSEVI ER Artificial Intelligence 96 (1997) 351-394 Artificial Intelligence Automated model selection for simulation based on relevance reasoning Alon Y. Levy a~1, Yumi Iwasaki bp*, Richard Fikes c*2 a AT&T Bell Laboratories, 600 Mountain Ave., Room 2A-440, Murray Hill, NJ 07974, USA b Knowledge Systems Laboratory, Stanford University, Gates Bldg. 2A, Rm. 256, Stanford, CA 94305. USA ’ Knowledge Systems Laboratory, Stanford University, Gates Bldg. 2A. Rm. 246, Stanford, CA 94305, USA Received March 1996; revised August 1997 Abstract Constructing an appropriate model is a crucial step in performing the reasoning required to pieces of knowledge the behavior of a physical and Forbus successfully answer a query about modeling approach of Falkenhainer composable construction problem Model construction of dynamic behavior over a sequence of states. The latter is significantly more difficult former since one must select model fragments without knowing exactly what will happen future statles. is provided with a library of the physical world called model fragments. The model the situation. can be considered either for static analysis of a single state or for simulation than the in the selecting appropriate model In the compositional ( 1991), a system to describe fragments situation. involves about The model construction problem about relevance described by Levy reasoning about relevance of knowledge for reasoning paper, we present a model formulation procedure based on that framework fragments efficiently model must be adequate possible. We define formally in fact generates an adequate and simplest model. @ 1997 Published by Elsevier Science B.V. in general can advantageously be formulated as a problem of that is available to the system using a general framework ( 1993) and Levy and Sagiv ( 1993). In this for selecting model to be useful, the generated time, as simple as the concepts of adequacy and simplicity and show that the algorithm for the case of simulation. For such an algorithm the given query and, at the same for answering Keywords: Model formulation; Relevance reasoning; Qualitative reasoning; Simulation * Corresponding author. Email: iwasaki@ksl.stanford.edu. ’ Email: levy@research.art.com. 2 Email: filtes@ksl.stanford.edu. 0004-3702/97/$17.00 @ 1997 Published by Elsevier Science B.V. All rights reserved. PIISOOO4~-3702(97)00056-8 \f352 A. E Levy et al. /Artificial Intelligence 96 (1997) 351-394 1. Introduction study solving problem themselves for a problem the real world is too complex instead of studying endeavor, and constructing for various because are products of our intellectual Models are the conceptual objects humans is a challenging reasons. For example, Models priate model constructed constructed models of a device being designed are constructed of the device by experimenting with the model even before the device the real thing. an appro- task in itself. Models are simplified models of the real world are in its entirety, and the behavior is actually built. There are as many possible models of a given subject of study as there are reasons is no one “true” or “correct” model since any model and the goodness of a model depends on one’s goal, i.e., a model. the question detail. for for constructing models. There is necessarily the question one wishes For a model with sufficient precision Constructing answering and studying to answer too much unnecessary to comprehend in order to understand to by constructing information requires deciding what information and, therefore, should be included and accuracy without containing it must contain enough to find an answer could be relevant such a model an abstraction in the model. to be useful, the questions Thus, model formulation can be considered about and Genesereth problem of reasoning as Subramanian for reasoning of one of them, namely Levy’s, efficient procedure, purpose of simulation useful, same time, as simple as possible. of adequacy and simplicity and simplest model. about relevance of knowledge [ 301 and Levy relevance of knowledge. as a special case of a more general for a given goal. Researchers such [ 161 have proposed general frameworks In this paper, we present an application based on the general framework, for formulating of physical devices. For any model formulation to the problem of model formulation. We propose an for the to be the given query and, at the the concepts in fact generates an adequate In later sections, we will define formally and show that the procedure a model procedure for answering the generated model must be adequate 1.1. Motivations The ability to analyze a physical system using a model of its behavior skill required of engineers. Equally that is appropriate a model a good model is much formulated. Most computational rely on the user to construct a model. less understood for one’s purpose. However, important, if not more, is the ability is an important to formulate the problem of how to build it is to assist in analysis of model behavior than that of how to analyze a model once tools intended though simulation design, The ability to formulate systems greatly by making For example, in engineering involved If a system could quickly aspect of interest, perform in a readily understandable in formulating an appropriate model would enhance the utility of such it much easier to take advantage of their analysis capabilities. tool for evaluating design alternatives is a very useful a model, performing it is not currently used as freely as it could be because of the cost the results. and interpreting the particular for analyzing of the results and produce an interpretation form, a designer could much more easily analyze design a simulation, an appropriate model formulate the simulation, \fA.Z Levy et al./Art$cial Intelligence 96 (1997) 351-394 353 Fig. 1. An example circuit: SAl is a solar array and BAI is a rechargeable battery. alternatives. during the design process, quality of the final design. Such a capability would enable designers thereby improving to make better informed decisions the efficiency of the process as well as the 1.1.1. behavior modeling as modeling paradigm modeling [7] represented by an effective system of for automatically differential equations t!hat can adequately modeled a lumped-parameter sitional approach, a is provided a 3 In a physical compo- composable pieces fragment formu- model a library model fragments. or a by selecting process. The model fragments knowledge about one aspect physical world a component given physical lates model of them. composing main advantage fragments, describing compositional modeling a phenomenon, its modularity. is much easier model than composing complete model every possible and query. existing appropriate is also easier. Furthermore, context. model fragments reused fragments can in an a system systems automatically to aid given domain, in formulating in analyzing modeling model. However, must have mechanism model will order for for appropriate for promising behaviors of wide variety is approach compositional modeling such model fragments given analysis that a Also, because all the savings achieved by using an appropriate model for analysis will not be worthwhile if the cost of the selection process for the selection algorithm to have a reasonable itself is prohibitive. time complexity, to succeed, the it 11s imperative I. 1.2. Model formulation in compositional modeling is crucial when and different perspectives an appropriate model Selecting levels of abstractions various studied. #Suppose one is analyzing is rich with can be the design of an electrical power supply consisting of the problem domain at which phenomena 3 When spatial variation is not of interest, one speaks of the system as being lumped parameter [6]. \f354 A.I: Levy et al. /Artijicial Intelligence 96 (1997) 351-394 into account phenomena of the battery. Furthermore, in Fig. 1. If one is interested in the voltage a model battery and a solar array as shown that takes and discharging in the a rechargeable level supplied by the battery over the course of a day, one would variation such as solar power generation construct and charging the model would describe those phenomena with mostly electrical properties such as currents, voltages and charge in the charging capacity of the battery level. If, instead, one is interested such over several months, one would have to take into consideration as aging, whose effect becomes observable cycles. in the details of the Yet another possibility is interested In this case, an appropriate model would consist chemical processes of representations the electrodes of the battery. is that one that cause aging. of individual only after many charge-discharge chemical processes other phenomena in the variation the electrolyte in aging and involving and While the library of model fragments may contain knowledge can also be represented by several different model fragments, of physical phenomena, most of them may be irrelevant phenomenon different ways representation made, desired Therefore, to describe and the problem temporal and quantitative in the context of compositional modeling, to be solved, the same thing based on different including accuracy, and precision. about a wide variety for any given problem. Each assumptions representing about the such things as approximations the model formulation problem is: l Select an appropriate subset of the model fragment library and appropriate instan- tiations of that subset, given: l A description of a problem situation, and l An analysis goal, A more formal statement of the problem will be given in Section 4.1. The set of instantiations a model of the situation. of the selected subset comprises As discussed above, this selection process requires one to make two types of decisions, namely: l What phenomena l How to model each of the chosen phenomena? If the model to model? includes the most detailed model fragment about every phenomenon th",
            {
                "entities": [
                    [
                        76,
                        145,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 163 (2005) 91–135www.elsevier.com/locate/artintOn the consistency of cardinal direction constraints ✩Spiros Skiadopoulos a,∗, Manolis Koubarakis ba Knowledge and Database Systems Laboratory, School of Electrical and Computer Engineering,National Technical University of Athens, Zographou, 157 73 Athens, Greeceb Intelligent Systems Laboratory, Department of Electronic and Computer Engineering,Technical University of Crete, Chania, 731 00 Crete, GreeceReceived 3 December 2003; accepted 18 October 2004Available online 15 December 2004AbstractWe present a formal model for qualitative spatial reasoning with cardinal directions utilizing a co-ordinate system. Then, we study the problem of checking the consistency of a set of cardinal directionconstraints. We introduce the first algorithm for this problem, prove its correctness and analyze itscomputational complexity. Utilizing the above algorithm, we prove that the consistency checking of aset of basic (i.e., non-disjunctive) cardinal direction constraints can be performed in O(n5) time. Wealso show that the consistency checking of a set of unrestricted (i.e., disjunctive and non-disjunctive)cardinal direction constraints is NP-complete. Finally, we briefly discuss an extension to the basicmodel and outline an algorithm for the consistency checking problem of this extension. 2004 Elsevier B.V. All rights reserved.Keywords: Cardinal direction relations; Spatial constraints; Consistency checking; Qualitative spatial reasoning✩ This is a greatly revised and extended version of a paper which appears in Proc. of CP-02, Lecture Notes inComput. Sci., vol. 2470, Springer, Berlin, 2002, pp. 341–355.* Corresponding author.E-mail addresses: spiros@dblab.ece.ntua.gr (S. Skiadopoulos), manolis@intelligence.tuc.gr(M. Koubarakis).URLs: http://www.dblab.ece.ntua.gr/~spiros (S. Skiadopoulos), http://www.intelligence.tuc.gr/~manolis(M. Koubarakis).0004-3702/$ – see front matter  2004 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2004.10.010\f92S. Skiadopoulos, M. Koubarakis / Artificial Intelligence 163 (2005) 91–1351. IntroductionQualitative spatial reasoning has received a lot of attention in the areas of GeographicInformation Systems [13–15], Artificial Intelligence [6,8,13,29,36–38], Databases [32] andMultimedia [44]. Qualitative spatial reasoning problems have recently been posed as con-straint satisfaction problems and solved using traditional algorithms, e.g., path-consistency[38]. One of the most important problems in this area is the identification of useful andtractable classes of spatial constraints and the study of efficient algorithms for consistencychecking, minimal network computation and so on [38]. Several kinds of useful spatialconstraints have been studied so far, e.g., topological constraints [5,6,12,13,36–38], cardi-nal direction constraints [17,25,41] and qualitative distance constraints [14,48].In this paper, we concentrate on cardinal direction constraints [17,25,32]. Cardinal di-rection constraints describe how regions of space are placed relative to one another utilizinga co-ordinate system (e.g., region a is north of region b). Currently, the model of Goyaland Egenhofer [16,17] and Skiadopoulos and Koubarakis [40,42] is one of the most ex-pressive models for qualitative reasoning with cardinal directions. The model that we willpresent in this paper is closely related to the above model but there is a significant differ-ence. The model of [16,17,40,42] basically deals with extended regions that are connectedand have connected boundaries while our approach allows regions to be disconnected andhave holes. The regions that we consider are very common in Geography, Multimedia andImage Databases [4,7,44]. For example, countries are made up of separations (islands, ex-claves, external territories) and holes (enclaves) [7].We will study the problem of checking the consistency of a given set of cardinal direc-tion constraints in our model. Checking the consistency of a set of constraints in a modelof spatial information is a fundamental problem and has received a lot of attention in theliterature [25,32,38]. Algorithms for consistency checking are of immediate use in varioussituations including:• Propagating relations and detecting inconsistencies in a given set of spatial relations[25,38].• Preprocessing spatial queries so that inconsistent queries are detected or the searchspace is pruned [31].The technical contributions of this paper can be summarized as follows:1. We present a formal model for qualitative reasoning about cardinal directions. Thismodel is related to the model of [16,17,40,42] and is currently one of the most expres-sive models for qualitative reasoning with cardinal directions. The proposed modelformally defines cardinal direction relations on extended regions that can be discon-nected and have holes. The definition of a cardinal direction relation uses two types ofconstraints: order constraints (e.g., a < b) and set-union constraints (e.g., a = a1 ∪ a2).2. We use our formal framework to study the problem of checking the consistency ofa given set of cardinal direction constraints in the proposed model. We present thefirst algorithm for this problem and prove its correctness. The algorithm is interestingand has a non-trivial step where we show how to avoid using explicitly the obvious\fS. Skiadopoulos, M. Koubarakis / Artificial Intelligence 163 (2005) 91–13593but computational costly set-union constraints resulting from the definition of cardinaldirection relations.3. We present an analysis of the computational complexity of the consistency checkingproblem for cardinal direction constraints. We show that the aforementioned problemfor a given set of basic (i.e., non-disjunctive) cardinal direction constraints in n vari-ables can be solved in O(n5) time. Moreover, we prove that checking the consistencyof a set of unrestricted (i.e., disjunctive and non-disjunctive) cardinal direction con-straints is NP-complete.4. Finally, we consider the consistency checking problem of a set of cardinal directionconstraints expressed in an interesting extension of the basic model and outline analgorithm for this task. This extension considers not only extended regions but alsopoints and lines.The rest of the paper is organized as follows. In Section 2, we survey related work.Section 3 presents the cardinal direction relations and constraints of our model. In Sec-tion 4, we discuss the consistency checking of a set of basic cardinal direction constraints(expressed in the model of Section 3) and we present the first algorithm for this task. Sec-tion 5 studies the computational complexity of the consistency checking problem of basicand unrestricted sets of cardinal directions constraints. In Section 6, we outline algorithmsfor the consistency checking for an interesting extension of the basic cardinal directionmodel that we have already completed. Finally, Section 7 offers conclusions and proposesfuture directions.2. Related workQualitative spatial reasoning forms an important part of the commonsense reasoningrequired for building successful intelligent systems [10]. Most researchers in qualitativespatial reasoning have dealt with three main classes of spatial information: topological,directional and distance. Topological constraints describe how the boundaries, the interiorsand the exteriors of two regions relate [5,6,12,36–38]. For instance, if a and b are regionsthen a includes b and a externally connects with b are topological constraints. Directional(or orientation) constraints describe where regions are placed relative to one another [1,13,15,17,25,32,40,41]. For instance, a north b and a southeast b are directional constraints.Finally, distance constraints describe the relative distance of two regions [14,48]. For in-stance, a is far from b and a is close to b are distance constraints.In this paper, we concentrate on cardinal direction constraints [17,25,32,40]. Earlierqualitative models for cardinal direction relations approximate a spatial region by a repre-sentative point (most commonly the centroid) or by a representative box (most commonlythe minimum bounding box) [14,15,20,25,29,32].Depending on the particular spatial configuration these approximations may be toocrude [16,17]. Thus, expressing direction relations on these approximations can be mis-leading and contradictory (related observations are made in [28,34,45]). For instance, withrespect to the point-based approximation Spain is northeast of Portugal. Most people wouldagree that “northeast” does not describe accurately the relation between Spain and Portu-\f94S. Skiadopoulos, M. Koubarakis / Artificial Intelligence 163 (2005) 91–135Fig. 1. Problems with point and minimum bounding box approximations.gal on a map (see Fig. 1(a)). Similar examples are very common in geography. Consideralso the direction relation between Ireland and the UK (Fig. 1(b)). Summarizing, there is ademand for the formulation of a model that expresses direction relations between extendedobjects that overcomes the limitations of the point-based and box-based approximationmodels.With the above problem in mind, Goyal and Egenhofer [16,17] and Skiadopoulos andKoubarakis [40,42] presented a model in which we can express the cardinal direction re-lation of a region a with respect to a region b, by approximating b (using its minimumbounding box) while using the exact shape of a. Informally, the above model divides thespace around the reference region b, using its minimum bounding box, into nine areas andrecords the areas where the primary region a falls into (Fig. 1(c)). This gives a directionrelation between the primary and the reference region. Relations in the above model areclearly more expressive than point and box-based models. The model of [17,40] deals withconnected regions with a connected boundary. The model that we will present in Section 3,is a variation of the original model of [17,40] th",
            {
                "entities": [
                    [
                        71,
                        123,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 95 (1997) 115-154 Artificial Intelligence Autoepistemic logic of knowledge and beliefs l Teodor C. Przymusinski * Department of Computer Science, University of California, Riverside, CA 92521, USA Received November 1996; revised May 1997 Abstract reasoning and different semantics fotmalizations In recent years, various logic programs have been proposed, including autoepistemic logic, for normal and disjunctive circumscription, CWA, GCWA, ECWA, epistemic specifications, stable, well-founded, stationary and static semantics of normal and disjunctive logic programs. of nonmonotonic In this paper we introduce a simple nonmonotonic knowledge representation framework which isomorphically contains all of the above-mentioned nonmonotonic formalisms and semantics as special cases and yet is significantly more expressive than each one of these formalisms considered individually. The new formalism, called the Autoepistemic Logic of Knowledge and Beliefs, AELB, is obtained by augmenting Moore’s autoepistemic logic, AEL, already employing the knowledge operator, C, with an additional belief operator, t?. As a result, we are able to reason not only about formulae F which are known to be true (i.e., those for which CF holds) but also about those which are only believed to be true (i.e., those for which !3F holds). The proposed logic constitutes a powerful new formalism which can serve as a unifyingfrume- work for several major nonmonotonic formalisms. It allows us to better understand mutual re- lationships existing between different formalisms and semantics and enables us to provide them with simpler and more natural definitions. It also naturally leads to new, even more expressive, flexible and modular formalizations and semantics. @ 1997 Published by Elsevier Science B.V. Keywords: Nonmonotonic reasoning; Logics of knowledge and beliefs; Semantics of logic programs and deductive databases * Email: teodor@cs.ucr.edu. ’ The extended abstract of this paper appeared in: Proceedings c~f the Twelfth National Conference on (AAAI-94). Seattle, WA (1994), pp. 952-959. Partially supported by the National Artificial Intelligence Science Foundation grant #IRI-93 1306 I, 0004-3702/97/$17.00 @ 1997 Published by Elsevier Science B.V. All rights reserved. PIISOOO4-3702(97)00032-5 \f116 T.C. Pr~ymusinski/Art~ciul Intelligence 95 (1997) 115-154 1. Introduction expansion Moore’s autoepistemic logic, AEL [ 201, is obtained by augmenting logic with a modal operator L. The intended meaning of the modal atom LF classical proposi- tional in is “F is provable” or “F is logically derivable”. Thus a stable autoepistemic Moore’s modal operator L can be viewed as a “knowledge operator” which allows us to reason about formulae known to be true in some stable expansion.2 However, often times to be true we also that are only believed to be true, where what is need to reason about believed or not believed is determined by some specific nonmonotonicformalism. about statements which are known those statements to reasoning in addition For example, consider a scenario l You rent a movie a football game. if you believe in which: that you will neither go to a baseball game nor to l You do not buy tickets We could describe this scenario as follows: 3 to a game if you don’t know that you will go to watch it. &baseball A &football > rentmovie TLbaseball A TLfootball> dontbuy-tickets. Assuming that initially (circumscription), this is all you know and that your beliefs are based on minimal that you (i.e., both lbaseball and Ifootball hold in all minimal and you will not buy tickets because you don’t know that you will go to any you will likely rent a movie because you believe entailment will not go to watch any games models) of the games (i.e., neither baseball nor football is provable). Suppose now that you learn that you will either go to a baseball game or to a football game: baseball V football. In the new scenario you will no longer rent a movie (because Tbaseball A Ifootball no longer holds in all minimal models) but you will not buy any tickets either because (i.e., neither baseball nor you don’t know yet which game you are going football is provable). Finally, suppose learn that you in fact go to a baseball game: that you eventually to watch baseball. Clearly, you no longer believe in not buying tickets because you now know that you are going to watch a specific game (i.e., baseball is provable). Observe, that in the above example the roles played by the knowledge operators are quite different and one cannot be substituted we cannot the premise Bybaseball A B-football LTbaseball A Llfootball because that would result replace and belief by the other. In particular, by in the first true in first implication in rent-movie not being * Moore’s modal operator .L is usually referred to as a “belief operator”. In Section 2.1 we explain why we prefer to view it as a “knowledge operator”. 3 The second clause could be equivalently written as: buy-rickets 3 Lbaseball V Lfootball. \fTC. Przymusinski/Artifcial Intelligence 95 (1997) 115-154 117 called A 4football it by Xbaseball scenario.4 would result Similarly, we cannot in rent-movie being In order to be able to explicitly replace true in the second scenario. 5 reason about beliefs, we introduce a new nonmonotonic the Autoepisternic Logic of Knowledge and Beliefs, AELB, obtained the knowledge formalism, by augmenting Moore’s autoepistemic operator, C, with to reason not only about formulae F which are known to be true (i.e., those for which CF holds) but also about those which are only believed to be true (i.e., those for which t3F holds). belief operator, 23. As a result, we will be able logic, AEL, already employing the additional because that The resulting nonmonotonic knowledge representation framework turns out to be circumscription for normal and disjunctive and semantics into AELB. In particular [ 15,181; CWA [ 3 1 ] ; GCWA rather simple and yet quite powerful. We prove that several of the major nonmonotonic logic programs are isomorphically formalisms [ 201; propo- logic embedduble [ 131; epistemic sitional stationary and static semantics of normal and specifications disjunctive time Logic of Knowledge than each one of these formalisms logic constitutes the same is significantly more expressive [ lo]; stable, well-founded, and Beliefs, AELB, [ 11,12,26,29,34]. the Autoepistemic to autoepistemic logic programs [ 191; ECWA and flexible this applies individually. The proposed unifying framework understand mutual relationships enables us to provide leads to new, even more expressive and modular for several major nonmonotonic considered a powerful new formalism which can serve as a It allows us to better formalisms. existing between different formalisms and semantics and It also naturally them with simpler and more natural definitions. formalizations and semantics. At The paper is organized as follows: into AELB. are embeddable theories are limited the Autoepistemic Logic of Knowledge and Beliefs, AELB, its basic properties. We also show that both Moore’s Autoepistemic In-Section 2 we introduce and establish Logic, AEL, and McCarthy’s Circumscription study of the Autoepistemic Logic of Beliefs, Section 3 is devoted to a detailed AEB, a sub-logic of AELB whose to formulae which do not use the knowledge operator C. It turns out that the logic AEB has some very nice In particular, any such theory has the least static expansion and regular properties. which can be iteratively constructed as the fixed point of a natural minimal model operator. In Section 4 we demonstrate major semantics, This allows us to better understand mutual In Section 5 we show that also Gelfond’s represented byproduct we establish in Gelfond’s appearing can be easily in the logic AELB. As a the two operators between logic programs, under all in the logic AELB. and their semantics that normal and disjunctive translated theories the meaning of different as special knowledge can be equivalently a simple duality specifications relationship and belief epistemic relations. theories logic. into 4 Because neither -baseball ’ Because neither baseball norfiwfball nor +mrball is provable is provable. \f118 TC. Przymusinski/Artificial Intelligence 95 (1997) 115-154 l In Section 6 we illustrate how the meaning of theories the underlying suitably changing belief can be also changed by adding suitable axioms nonmonotonic sections we demonstrated is based. In previous to the logic. in AELB can be adjusted by formalism on which the notion of that the semantics of AELB l Section 7 contains concluding remarks and a brief discussion of other applications of the logic AELB and of its relationship to other proposed logics. 2. Autoepistemic Logic of Knowledge and Beliefs The language of the Autoepistemic Logic of Knowledge and Beliefs, AELB, is a propo- (V, A, 3, T), respectively. The atomic formulae of the form LF sitional modal language, letter l_ (denotingfalse) operators, F is an arbitrary atoms). The intended meaning of LF is “F is believed” BF belief atoms are jointly of introspective ICLJ, with standard connectives and two modal operators L and a, called knowledge and belief 23F), where belief is “F is known” while the intended meaning of and later on in this section). Knowledge that arbitrary nestings formula of Ic,c,n, are called knowledge atoms to as introspective atoms. Observe (we make referred atoms are allowed. the propositional (respectively, (respectively, it precise The formulae of IC,Q set of all such formulae which only C (respectively, theory T in the language beliefs, or, briefly, a knowledge and belief theory. only L?) occurs in which neither L nor B occurs are called objective and the the set of all formulae of ICC,J in is denoted by Ic. Similarly, Ice). Any theory of knowledge and ICL,B will be called an autoepistemic is denoted by icr. (respectively, (Knowledge and belief theories) By an autoepistemic Definition 2.1. edge",
            {
                "entities": [
                    [
                        66,
                        110,
                        "TITLE"
                    ],
                    [
                        936,
                        980,
                        "TITLE"
                    ],
                    [
                        7003,
                        7047,
                        "TITLE"
                    ],
                    [
                        8757,
                        8801,
                        "TITLE"
                    ],
                    [
                        8822,
                        8866,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 100 ( 1998) 177-224 Artificial Intelligence Model-based average reward reinforcement learning * Prasad Tadepalli ‘,*, DoKyeong Ok b*2 ’ Department of Computer Science, Oregon State University, Corvallis, OR 97331, USA ’ Korean Army Computer Center, NonSanSi DuMaMyeon BuNamRi, i?O.Box 29, ChungNam 320-919, South Korea Received 27 September 1996; revised 15 December 1997 Abstract from (RL) Reinforcement that improve their performance in many domains, than its discounted the natural criterion is the study of programs the environment. Most RL methods optimize Learning rewards and punishments by the dis- receiving is to total reward received by an agent, while, counted optimize the average reward per time step. In this paper, we introduce a model-based Average- reward Reinforcement Learning method called H-learning and show that it converges more quickly in the domain of scheduling a simulated Automatic and robustly Guided Vehicle explores to the the unexplored parts of the state space, while always choosing greedy actions with respect current value function. We show that this “Auto-exploratory H-Learning” performs better than the previously to larger state spaces, we extend it to learn action models and reward functions in the form of dynamic Bayesian networks, and approximate are effective faster in some AGV scheduling its value function using local linear regression. We show that both of these extensions it converge in significantly and making tasks. @ 1998 Published by Elsevier Science B.V. (AGV). We also introduce a version of H-learning strategies. To scale H-learning studied exploration that automatically counterpart of H-learning requirement the space reducing Keywords: Machine networks; Linear regression; AGV scheduling learning; Reinforcement learning; Average reward; Model-based; Exploration; Bayesian author. Email: * Corresponding ’ Most of the work was done when both the authors were at Oregon State University. ’ Email: okdo@unitel.co.kr. tadepall@cs.orst.edu. 0004-3702/98/$19.00 PII SOOO4-3702(98)00002-2 @ 1998 Published by Elsevier Science B.V. All rights reserved. \f178 P: 7hdepulli. D. Ok/Art$ificial Intelligence 100 (1998) 177-224 1. Introduction receives equivalent scheduling the learner is the study of programs rewards and punishments Reinforcement Learning (EU) task by receiving that improve their perfor- from the environment. RL tasks, for many and elevator scheduling optimization as money learning of good procedures including Q-learning [ 31, optimize in automatic tasks such as job-shop learning, (ARTDP) [ 181. In other words, a reward mance at some has been quite successful including some real-world [ 1 1,44,47]. Most approaches to reinforcement and Adaptive Real-Time Dynamic Programming counted reward after one time step is considered immediately. Discounted ward can be interpreted situation run will be terminated in which we would etary aspect or the probability in which we are is the average counted the average ward action sequences one. [ 461 the total dis- that is received to a fraction of the same reward received in which re- time step. Another that the at any given time for whatever reason. However, many domains the mon- in the time scales in such domains received per time step. Even so, many people have used dis- to optimize learning total re- two such the better algorithms reason is that the discounted sequence of actions and rewards. Hence, to choose to use reinforcement of immediate in. The natural in such domains, while aiming to do this criterion that can earn is when from a state can be compared by this criterion is finite even for an infinite learning do not have either at least to optimize termination, criterion is motivated by domains is a fixed probability that it is well-suited interest there [ 21,261. One reinforcement to model interested in each reward reward like with encourages the learner factor, discounting While mathematically convenient, in domains where these isn’t a natural interpretation long-term benefits reward decreases optimization when average-reward to sacrifice for the discount for short-term gains, since the impact of an action choice on long-term exponentially time. Hence, using discounted optimization be argued optimizes to 1 [ 21,261. This raises appropriate it can if that also nearly close the question whether and when discounted RL methods are that it is appropriate the average lead to suboptimal policies. Nevertheless, reward by using a discount is what is required could to optimize discounted the average reward. to use to optimize is sufficiently factor which total reward version the short-term is an undiscounted [ 31. We compare H-learning with In this paper, we describe an Average-reward RL (ARL) method called H-learning, Programming of Adaptive Real-Time Dynamic its discounted a simulated Automatic Guided Vehicle which (ARTDP) the task of scheduling dling robot used in manufacturing. Our results show that H-learning ARTDP when icy also optimizes are different, ARTDP either fails to converge converges counterpart, ARTDP, in (AGV) , a material han- is competitive with factor) optimal pol- the average reward. When short-term and long-term optimal policies policy or factor is high. Like ARTDP, and unlike Schwartz’s R-learning is model-based, [ 381, H-learning models. We show that in the AGV scheduling domain H-learning steps than R-learning [37] and Singh’s ARL algorithms in that it learns and uses explicit action and reward in fewer and is competitive with it in CPU time. This is consistent with the (discounted with a small discount to the optimal average-reward if the discount too slowly converges \fI? Tadepdli, D. Ok/Artificiul Intelligence 100 (1998) 177-224 179 previous RL [3,28]. results on the comparisons between model-based and model-free discounted Like most other RL methods, H-learning needs exploration to find an optimal policy. including (recency-based) [45]. Other meth- random actions, preferring (IE) method of Kaelbling strategies have been studied that are least recently executed occasionally in RL, to visit states that are least visited (counter-based) of reward functions used by Koenig and Simmons under uncertainty” A number of exploration executing or executing actions ods such as the Interval Estimation the idea incorporate representation the value function of states of “optimism with high values, and gradually decreasing to explore automatically, while always executing greedy actions. We introduce a version of H-learning exploring respect converges more quickly random, counter-based, to the current value function. We show that this “Auto-exploratory H-learning” to H-learning under strategies. the unexplored parts of the state space while always taking a greedy action with to a better average reward when compared and Boltzmann recency-based, in which the value function that uses optimism under uncertainty, and has the property of automatically them, these methods encourage [ 17,201. By initializing and the action-penalty ARL methods exploration the learner is stored as a table require to scale to large state spaces. Model-based methods too much space like H-learning time and training also have the additional the reward essential problem of having to explicitly functions, which is space-intensive. To scale ARL in a more compact store their action models and it is to such domains, form. the to design in such a these networks to fully specify the domain models. the to approximate its action models and value function Dynamic Bayesian networks have been successfully used in the past to represent [ 12,351. In many cases, it is possible action models way that a small number of parameters are sufficient We extended H-learning conditional probabilities for our method but also increases the action models dominates the learning time. so that it takes the network structure as input and learns in the network. This not only reduces the space requirements in domains where learning the speed of convergence learning is piecewise reinforcement of H-learning tasks, especially the performance linear. To take advantage of this, we implemented combines with learning of Bayesian network-based that can be described by small Bayesian networks, have a uniform those with action models and reward Many reward functions the optimal value func- structure over large regions of the state space. In such domains, tion of H-learning a value function approximation method based on local linear regression. Local linear regression action models and synergistically improves tasks. Combining Auto-exploratory H-learning with action model and value function approximation to even faster convergence The rest of the paper leads learning method. as follows: Section 2 introduces Markov Deci- that form the basis for RL, and motivates Average-reward RL. Section 3 it with ARTDP and R-learning sion Problems introduces H-learning and compares ing task. Section 4 introduces Auto-exploratory H-learning, previously Bayesian networks and local linear regression value function issues, and Section 7 is a summary. in an AGV schedul- it with some the use of dynamic the action models and the respectively. Section 6 is a discussion of related work and future research in some domains, producing a very effective is organized schemes. Section 5 demonstrates in many AGV scheduling studied exploration to approximate and compares \f180 P 72uiepdii. D. Ok/Artijicitrl Irltelligencr 100 (1998) 177-224 2. Background We assume that are applicable is described by a discrete that the learner’s environment is modeled by a Markov Decision Process (MDP). An MDP set S of n states, and a discrete set of i are denoted by U(i) and actions, A. The set of actions are called admissible. The Markovian assumption means that an action u in a given state i E S results in state j with some fixed probability P;,,i (u). There is a finite immediate reward r;,,;(u) a sequence of discrete steps",
            {
                "entities": [
                    [
                        77,
                        126,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 87 ( 1996) 145-I 86 Artificial Intelligence Planning from second principles Jana Koehler * Depariment of Computer Science, Albert Ludwigs University, Am Flughafen 17, D-791 10 Freiburg, Germany Received June 1994; revised June 1995 Abstract Planning from second principles by reusing and modifying plans is one way of improving the efficiency of planning systems. In this paper, we study it in the general framework of deductive planning and develop a logical formalization of planning from second principles, which relies on a systematic decomposition of the planning process. Deductive inference processes with clearly defined semantics formalize each of the subtasks a second principles planner has to address. Plan modification, which comprises matching and adaptation tasks, is based on a deductive approach yielding provably correct modified plans. Description logics are introduced as query languages to plan libraries, which leads to a novel and efficient solution to the indexing problem in case-based reasoning. Apart from sequential plans, this approach enables a planner to reuse and modify complex plans containing control structures like conditionals and loops. 1. Introduction actions and tries to construct to specified preconditions. A serious Planning from first principles generates plans from “scratch”. The planner its set of available with respect is the invariable nature of the planning process over time: If a planner the same planning problem, words, it is unable processes. inspects a plan the desired goal limitation of first principles planners receives exactly In other that can be drawn from previous planning the same planning operations. to benefit from experience it will repeat exactly that achieves Approaches to planning from second principles try to overcome and modifying of generated plans. From a this limitation from scratch by reusing previously planning * E-mail: koehler@informatik.uni-freiburg.de 0004-3702/96/$15.00 Copyright @ 1996 Elsevier Science B.V. All rights reserved SSDlOOO4-3702(95)00113-1 \fcomplexity case [44], easier reasonable theory point of view, WC: cannot hope to prove efficiency gains in the worst since the reuse of’ plans comprises that are not computationally subtasks than plan generation. But to reuse existing plans in many practical applications than generating a new one. The current state of the art comprises a variety of approaches it seems to be more that tackle the problems planning, from a cognitive point of view Ccf: 1341 for a summary framework of STRIPS-based In using a deductive cl‘. ( 2 I, 28.54). framework, we present a formal approach from and frame- the retrieval of candidate plans from a librar;i as well as the problem of second principles. which makes no commitments application domains. We formalize work including plan modi$cation. the whole reuse process formalisms logical to particular planning of approaches) or in the in a unique to planning Plan modification is based on deductive modified plans. It comprises planning problem task of re$ttirzg the reused plan plan modification. we discuss plans, in order to determine inference processes that yield provably correct two subtasks: First. the task of matching an old and a new the new requirements. As a new issue in in the reuse and refitting of control structures occurring and differences. Secondly, to accommodate their similarities like case analyses and loops. which introduce qualitatively new problems. library, we propose a hybrid knowledge As for the plan the planning logic with a description languages abstraction, linking as query arc introduced use leads to well-defined oretical and practical properties of interest. to develop efficient and complete [44 1, which are guaranteed problem. logic. In this approach, description to large knowledge bases or case retrieval, and update procedures In particular, description representation formalism logics libraries. Their that possess the- logics enable us for the matching problem that solve a planning approximation algorithms to retrieve all plans from the library Finally, the formal framework allows us to prove important properties ness and completeness provides developed as an integrated part of the PHI planner for the implemented of the underlying the foundation [S]. like the correct- inference procedures. Besides this, the approach plan reuse system MRL, ’ which has been The paper is organized as follows: We begin in Section 2 with a summary of the of second principles a four-phase model as the foundation that are used by MRL. The section also contains a short introduction logical formalisms as well as a short overview of the PHI planner. Section 3 into deductive planning planning. The introduces model supports a temporal view as well as a task specific view of the second principles planning the theoretical basis process. A logical for the system MRL that is described sections. Sections 4 and 6 are in the subsequent to the inference procedures working on the plan library, while in Section 5 the devoted deductive in Section 7 we review related work and propose a systematic categorization of the various principles and design decisions underlying the main properties of MRL in the light of this categorization. second principles planners. We summarize of each phase provides is presented. Finally, to plan modification formalization approach ’ MKL stands for modification and reuse 111 logic \fJ. Koehler/Ariijicial Intelligence 87 (1996) 145-186 147 2. Formal preliminaries Deductive planning is a longstanding variant of artificial origins go back to QA3 formal plan specifications specified condition, cf. [ 37, p. 141. Usually, the form [ 181. To generate plans deductively, are performed, i.e., “to construct one proves the existence of a state in which this requires us to constructively constructive intelligence planning, whose proofs of that will meet a is true”, of the condition prove plan specifications a plan v’s0 ‘Ja 32 Q[so,a,zl where SO denotes planvariable the initial state, a is an argument or input parameter, and z_ is a representing the plan term that has to be constructed [ 371. Two properties of deductive planners are particularly interesting when studying plan- from second principles: First, plans are provably correct. Once provided with a of a particular application domain and the actions which can be to can leads to a sound plan, that solves ning correct axiomatization performed work. Preserving we ensure the planning problem at hand? this property during plan modification a deductive planner generates plans reordering or adding actions is a real challenge-how that are guaranteed in this domain, that removing, Secondly, deductive planning has been closely like if-then-else and while loops. While from its origins. Plans are viewed as programs and consequently, structures generate such complex plans, many deductive planning trol structures in plans. The retrieval and modification and loops is therefore another challenge we are going to address. related to program right synthesis they contain control it is very rare that classical planners systems are able to generate con- of plans containing conditionals A deductive planning system-like complexity this means controlling a search space of enormous For a deductive planner, in such a way that plans can be constructed system to correctly “guess” appropriate with mechanisms The difficulty of this Many sacrifice soundness by ignoring complexity. inference task led to a temporary that decide which any other classical planning search and controlling instantiations the inferences automatically. This of planvariables to certain rules apply abandonment (but not all) classical planners, which have been developed in the underlying system-is faced with is a difficult problem. logic a it involves enabling and to provide logical of deductive formulae. techniques. in the meanwhile, in order to reduce search frame or ramification problems solution that allow for an efficient Recently, deductive planning logics devised, e.g., [ 8,471. On the other hand, new ways of controlling by using planning ative representation automatically giving up completeness also play an important has seen a renaissance. On one hand, various planning to the frame problem have been carefully a deductive planner in deductive the declar- so that plans can be this implies tactics tactics have been developed, is inspired by tactical theorem proving tractability. As we will demonstrate, plan modification. in order to purchase role in implementing [ 11,24,46]. Tactics support [8,53]. The use of tactics constructed when proving the plan specification. of control knowledge the inference In practice, and guide e.g., \fto use to tllustrate planning The examples we are going are to perform plan generation and plan taken from the PHI planner. PHI has been designed recognition e.g., software systems. It provides a logic-based kernel that can be used to develop intelligent help systems supporting users of PHI is the UNIX mail domain where objects of software. A prototype like messuges and mailboxes are manipulated like read, delete, and save. from second principles language environments, tasks in command application by actions logical The system uses the so-called language formalism. The logic LLP reflects the specific requirements for planning lying planning language environments. ementary like loops and conditionals complex user actions on the level of the logical formalism. For example, statements of the application [ 8 J as the under- of command the basic actions which occur in plans are the el- control structures system to describe language. Furthermore, are available 3.4 defined operators, which allows (LLP) 2.1. The logicul luttguage,ftir pluntting LLP temporal [49] with a tempo& logic with interval-based semantics, which combines LLP is a modal logic for programs tures of c/zo~~~ logic temporal formalisms logics have been proposed as ",
            {
                "entities": [
                    [
                        68,
                        99,
                        "TITLE"
                    ],
                    [
                        265,
                        296,
                        "TITLE"
                    ],
                    [
                        501,
                        532,
                        "TITLE"
                    ],
                    [
                        1773,
                        1804,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 951–983Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintReasoning about cardinal directions between extended objects ✩Weiming Liu a, Xiaotong Zhang b, Sanjiang Li a,b,∗, Mingsheng Ying a,ba Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology, Sydney, Australiab State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology,Department of Computer Science and Technology, Tsinghua University, Beijing 100084, Chinaa r t i c l ei n f oa b s t r a c tArticle history:Received 6 July 2009Received in revised form 18 May 2010Accepted 18 May 2010Keywords:Qualitative spatial reasoningCardinal direction calculusConnected regionsConsistency checkingMaximal canonical solution1. IntroductionDirection relations between extended spatial objects are important commonsense knowl-edge. Recently, Goyal and Egenhofer proposed a relation model, known as the cardinaldirection calculus (CDC), for representing direction relations between connected plane re-gions. The CDC is perhaps the most expressive qualitative calculus for directional infor-mation, and has attracted increasing interest from areas such as artificial intelligence,geographical information science, and image retrieval. Given a network of CDC constraints,the consistency problem is deciding if the network is realizable by connected regions in thereal plane. This paper provides a cubic algorithm for checking the consistency of completenetworks of basic CDC constraints, and proves that reasoning with the CDC is in general anNP-complete problem. For a consistent complete network of basic CDC constraints, our al-gorithm returns a ‘canonical’ solution in cubic time. This cubic algorithm is also adapted tocheck the consistency of complete networks of basic cardinal constraints between possiblydisconnected regions.© 2010 Elsevier B.V. All rights reserved.Representing and reasoning with spatial information is of particular importance in areas such as artificial intelligence(AI), geographical information systems (GISs), robotics, computer vision, image retrieval, natural language processing, etc.While the numerical quantitative approach prevails in robotics and computer vision, it is widely acknowledged in AI andGIS that the qualitative approach is more attractive (see e.g. [6]).A predominant part of spatial information is represented by relations between spatial objects. In general, spatial relationsare classified into three categories: topological, directional, and metric (e.g. size, distance, shape, etc.). The RCC8 constraintlanguage [34] is the principal topological formalism in AI, and has been extensively investigated by many researchers (seee.g. [37,35,43,7,47,46,24,25,23]). When restricted to simple plane regions, RCC8 is equivalent to the 9-Intersection Model(9IM) [9], which is a very influential relation model in GIS.Unlike for topological relations, there are several competitive models for direction relations [10,11,2]. Most of thesemodels approximate a spatial object by a point (e.g. its centroid) or a box. This is too crude in real-world applicationssuch as describing directional information between two countries, say, Portugal and Spain. Recently, Goyal and Egenhofer[16,15] proposed a relation model, known as the cardinal direction calculus (CDC), for representing direction relationsbetween connected plane regions. In the CDC the reference object is approximated by a box, while the primary object is✩This paper is an extended version of Xiaotong Zhang, Weiming Liu, Sanjiang Li, Mingsheng Ying, Reasoning with cardinal directions: An efficientalgorithm, in: AAAI 2008, pp. 387–392.* Corresponding author at: Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University ofTechnology, Sydney, P.O. Box 123, Broadway, NSW 2007, Australia.E-mail address: sanjiang.li@uts.edu.au (S. Li).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.05.006\f952W. Liu et al. / Artificial Intelligence 174 (2010) 951–983not approximated. This means that the exact geometry of the primary object could be used in the representation of thedirection. This calculus has 218 basic relations, which is quite large when compared with the RCC8 and Allen’s IntervalAlgebra [1]. Due to its expressiveness, the CDC has attracted increasing interest from areas such as AI [40,41,31], GIS [17],database [39], and image retrieval [19].One basic criterion for evaluating a spatial relation model is the proper balance between its representation expressivityand reasoning complexity. While the reasoning complexity of the point-based and the box-based model of direction relationshas been investigated in depth (see [26] and [2]), there are few works discussing the complexity of reasoning with the CDC.One central reasoning problem with the CDC (and any other qualitative calculus) is the consistency (or satisfaction) prob-lem. Other reasoning problems such as deriving new knowledge from the given information, updating the given knowledge,or finding a minimal representation can be easily transformed into the consistency problem [6]. In particular, given a com-plete network of CDC constraintsN = {v iδi j v j}ni, j=1(each δi j is a CDC relation)(1)over n spatial variables v 1, . . . , vn, the consistency problem is deciding if N is realizable by a set of n connected regions inthe real plane. The consistency problem over the CDC is an open problem. Before this work, we did not know if there areefficient algorithms deciding if a set of CDC constraints are realizable. Even worse, we did not know if this is a decidableproblem. Furthermore, we did not know how to construct a realization for a satisfiable set of CDC constraints.This paper is devoted to solving these problems. We first show each consistent CDC network has a ‘canonical’ solution(Theorem 3) and then devise a cubic algorithm for checking if a complete network of basic CDC constraints is consistent.When the network is consistent, this algorithm also generates a canonical solution. We further show that deciding theconsistency of an arbitrary network of CDC constraints is an NP-Complete problem. This implies in particular that reasoningwith the CDC is decidable.Some restricted versions of the consistency problem have been discussed in the literature. Cicerone and di Felice [3]discussed the pairwise consistency problem, which decides when a pair of basic CDC relations (δ, δ(cid:3)) is consistent, i.e. when{v 1δv 2, v 2δ(cid:3)v 1} is consistent. Skiadopoulos and Koubarakis [40] investigated the weak composition problem [7,24] of theCDC, which is closely related to the consistency problem of basic CDC networks involving only three variables.The CDC algebra is defined over connected regions. A variant of the CDC was proposed in [41], where cardinal directionsbetween possibly disconnected regions are defined in the same way. This calculus, termed the CDCd in this paper, contains511 basic relations. An O (n5) algorithm1 was proposed in [41] for checking the consistency of basic constraints in the CDCd,but the consistency problem over the CDC is still open. Recently, Navarrete et al. [31] tried to adapt the approach used in[41] to cope with connected regions, but their approach turns out to be incorrect (see Remark 3 in Section 6.1 of this paper).The remainder of this paper proceeds as follows. Section 2 recalls basic notions in qualitative spatial/temporal reasoningand introduces the well-known Interval Algebra (IA) [1]. We introduce the CDC algebra in Section 3, where the connectionbetween CDC and IA relations is established in a natural way. Section 4 introduces the notion of canonical solution of aconsistent basic CDC network. Section 5 first proposes an intuitive O (n4) algorithm for consistency checking of completebasic networks and then improves it to O (n3). In Section 6, we first show local consistency is insufficient to decide theconsistency of even basic CDC networks, and then apply our main algorithm to the pairwise consistency problem and theweak composition problem. In Section 7 we adapt the main algorithm for connected regions to solve consistency checkingin two variants of the CDC. Section 8 discusses related work on the computational properties of other qualitative directioncalculi. Conclusions are given in the last section.Codes of the main algorithm are available via http://sites.google.com/site/lisanjiang/cdc, where we also provide illustra-tions for all 757 different consistent pairs of CDC basic relations and the illustration of the weak composition of S W : Wand N E : E. Interested readers may consult that webpage for detailed proofs of some minor results that are omitted in thepresent paper.Table 1 summaries notations used in this paper.2. Qualitative calculi: Basic notions and examplesSince Allen’s Interval Algebra, the study of qualitative calculi or relation models has been a central topic in qualitativespatial and temporal reasoning. This section introduces basic notions and important examples of qualitative calculi.2.1. Basic notionsLet D be a universe of temporal or spatial or spatial-temporal entities. We use small Greek symbols for representingrelations on D. For a relation α on D and two elements x, y in D, we write (x, y) ∈ α or xα y to indicate that (x, y) is aninstance of α. For two relations α, β on D, we define the complement of α, the intersection, and the union of α and β asfollows.1 We note that this algorithm applies to any (possibly incomplete) set of basic constraints.\fW. Liu et al. / Artificial Intelligence 174 (2010) 951–983953Table 1Notations.NotationsMeaningsα, β, γ , δ, θv i , v jNa, b, cI x(a), I y (a)M(a)χχ (a)a = {ai }ni=1ιx(δ), ι y (δ)ιx(δ, γ )ρ xi jρ yi jNx, N y{Ii }nmiS(a)C(a)ci jpi j , p, p(k)aribiciB(a)i",
            {
                "entities": [
                    [
                        136,
                        196,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial intelligence 103 (199X) 5117 Artificial Intelligence Remote Agent: to boldly go where no AI system has gone before * Nicola Muscettola ’ , P. Pandurang Nayak 2, Barney Pell *, Brian C. Williams 3 NASA Ames Research Center, MS 269-2, Mq&tt Field, CA 94035. USA Abstract toward to work and Artificial Renewed motives fleets of robotic that takes a significant exploration in space, in particular, will play a central inspired NASA have through heterogeneous for space a virtual presence technology, intelligence the Remote Agent, a specific autonomous Intelligence these explorers with a form of computational the goal of explorers. establishing role in this Information that we call remote endeavor by endowing ayems. In this paper we describe agent architecture on-board deduction and search, and goal- based on the principles of model-based programming, commanding, directed closed-loop this future. This the unique characteristics of the spacecraft domain that require highly reliable architecture addresses and autonomous operations over long periods of time with tight deadlines, concurrent activity among integrates constraint- execution, and model-based mode based temporal planning and scheduling, system as an on-board identification for a period controller the opportunity of a week in mid 1999. The development to reassess some of AI’s conventional wisdom about the challenges of implementing embedded systems, these issues, and our often contrary experiences, tightly coupled subsystems. The Remote Agent robust multi-threaded tractable reasoning, and knowledge throughout for Deep Space One, NASA’s first New Millennium mission, the paper. 0 1998 Published by Elsevier Science B.V. and reconfiguration. The demonstration of the Remote Agent also provided representation. We discuss step toward enabling resource constraints, of the integrated is scheduled Kcyrtords: Autonomous systems; Diagnosis; Recovery; Model-based reasoning agents; Architectures; Constraint-based planning; Scheduling; Execution; Reactive Authors in alphabetical order. author. RIACS. Email: pellQptolemy.arc.nasa.gov. * Corresponding ’ Recom Technologies. Email: mus@ptolemy.arc.nasa.gov. ’ RIACS. Email: nayak@ptolemy.arc.nasa.gov. ’ Email: williams@ptolemy.arc.nasa.gov. 00043702/98/$ - see front matter 0 1998 Published by Elsevier Science B.V. All rights reserved Pll: SOOO4-3702(98)00068-X \f6 N. A4uscettola et al. /Artijicinl Intelligence 103 (1998) 547 1. Introduction imagination, The melding of space exploration particularly and robotic in its vision of the future. For example, intelligence has had an amazing hold on the science the public fiction classic “2001: A Space Odyssey” offered a future in which humankind was firmly and space-stations. At the established beyond Earth, within amply populated moon-bases through the impressive same time, intelligence was firmly established beyond humankind HAL9000 computer, created in Urbana, Illinois on January 12, 1997. In fact, January I2th, 1997 has passed without a moon base or HAL9000 computer Space Station will begin its launch However, this space station is far more modest in scope. into space this year, reaching completion in sight. The International by 2002. is far from our ambitious dreams is surprising us with a different future that is particularly this reality While exploration exploration, and for the information enabling this future: technology community for humans in space, space for robotic that will play a central role in exciting in NASA is to open the Space Frontier. When people Our vision think of rocket plumes and the space shuttle. But the future of space is in information technology. We must establish a virtual presence, in space, on planets, spacecraft. think of space, they in aircraft, and - Daniel S. Goldin, NASA Administrator, Sacramento, California, May 29, 1996. Providing a virtual human presence in the universe through the actual presence of a plethora of robotic probes requires a strong motive, mechanical means, and computational that motivate space exploration the scientific questions intelligence. We briefly consider and the mechanical means for exploring this paper on our progress intelligence computational these questions, and then focus the remainder of explorers with a form of towards endowing that we call remote agents. these mechanical The development of a remote agent under tight time constraints has forced us to re- examine, and in a few places call to question, some of Al’s conventional wisdom about the challenges of implementing systems, This topic is addressed in a variety of places throughout and representation. this paper. embedded reasoning tractable 1.1. Estublishing a virtuul presence in space is evidence, found during that suggest new possibilities Renewed motives for space exploration have recently been offered. A prime example is for life in space. The best that primitive the summer of 1996, suggesting than 3.6 billion years ago. More specifically, a series of scientific discoveries known example life might have existed on Mars more the recent discovery of extremely scientists evidence suggestive of “native microfossils, mineralogical and evidence of complex organic chemistry” or overturn and is more cost effective led the Martian meteorite AlH84001 at fine resolution, where they found features characteristic of life, to confirm that has higher performance than traditional missions. Traditional planetary missions, these findings requires a new means of exploration small bacteria on Earth, called nanobacteria, [47]. Extending a virtual presence to examine such \fN. Muscettala et al. /Art$cial Intelligencr 103 (1998) 547 Fig. 1. Planned and concept missions Return missions (courtesy of NASA Johnson Space Center); (2) cryobot and hydrobot exploration airplane (courtesy of NASA Ames Research Center). (courtesy of JPL); (3) DS3 formation to extend human virtual presence flying optical interferometer in the universe. (I) Mars Sample for Europa oceanographic (courtesy of JPL); (4) Mars solar as the Galileo Jupiter mission or the Cassini Saturn mission, have price tags in excess of a billion dollars, and ground crews ranging the entire introduced a paradigm shift within life of the mission. The Mars Pathfinder NASA towards lightweight, highly focused missions, at a tenth of the cost, and operated last by small ground summer when MPF landed on Mars and enabled [48] to become the first mobile robot to land on the surface of another planet. [ 141. The viability of this concept was vividly demonstrated from 100 to 300 personnel during the Sojourner micro-rover (MPF) mission teams Pathfinder and Sojourner demonstrate a to achieving to achieve the goals for its two month life span taxing for its small ground crew. Future Mars rovers are expected to operate the need for the development of remote agents that are able to virtual presence, but currently of more challenging missions. For example, operating Sojourner was extremely for over a year, emphasizing continuously an important mechanical means intelligence necessary interact with an uncertain environment. lack the on-board and robustly Rovers are not the only means of exploring Mars. Another is a solar airplane, under study at NASA Lewis and NASA Ames. Given the thin CO2 atmosphere innovative concept \f8 N. khscettolrr et al. /Art$cial Intelligencr 103 (1998) 5-47 feet above sea level. This height is like a terrestrial plane on Mars, a plane flying a few feet above the Martian surface the reach of all but flying more than 90000 survey Mars a Martian plane a few existing planes. Developing of the Martian climate, requires the the idiosyncrasies over long durations, while surviving development of remote agents that are able to accurately model and quickly adapt to their environment. is beyond that can autonomously A second example is the discovery of the first planet around another star, which raises the intriguing question of whether or not Earth-like planets exist elsewhere. To search for [ 161, such as Earth-like planets, NASA is developing a series of interferometric telescopes the New Millennium Deep Space Three (DS3) mission. These interferometers identify and in a star, induced by its orbiting planets. They categorize planets by measuring a wobble the are so accurate that, if pointed from California to Washington DC, they could measure three optical thickness of a single piece of paper. DS3 achieves this requirement by placing flying in tight formation up to a kilometer apart. This units on three separate spacecraft, extends tightly coordinated of multiple, the computational remote agents. to the development challenge frozen surface. A final example smooth surface and chunky In February of 1998, the Galileo mission to the idea that Europa may have subsurface oceans, hidden under a thin is the question of whether or not some form of life might exist identified beneath Europa’s ice rafts, that lend features on Europa, such as a relatively icy support this subsurface ocean is an layer. One of NASA’s most intriguing ice penetrator and a submarine, that could autonomously navigate beneath Europa’s surface. This hydrobot would need to operate autonomously within an environment Taken together, flying for developing these examples of small explorers, interferometers, including micro-rovers, provide remote agents that assist in establishing airplanes, an extraordinary formation opportunity space, on land, in the air and under the sea. called a cryobot and hydrobot, that is utterly unknown. a virtual presence and hydrobots, for exploring cryobots, concepts in 1.2. Requirements for building remote agents to enable the above missions The level of on-board autonomy necessary is unprece- is the fact that NASA will need to achieve this capability to the billion that cost under 100 million dollars, in 2-3 years, and operated by a small ground team. This ambitious goal is to be low cost, technol- probe, Deep Space One (DSl), has d",
            {
                "entities": [
                    [
                        64,
                        125,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 42–71www.elsevier.com/locate/artintAudiences in argumentation frameworksTrevor J.M. Bench-Capon, Sylvie Doutre, Paul E. Dunne ∗Department of Computer Science, University of Liverpool, Liverpool L69 7ZF, UKReceived 11 October 2006; received in revised form 16 October 2006; accepted 17 October 2006Available online 20 November 2006AbstractAlthough reasoning about what is the case has been the historic focus of logic, reasoning about what should be done is an equallyimportant capacity for an intelligent agent. Reasoning about what to do in a given situation—termed practical reasoning in thephilosophical literature—has important differences from reasoning about what is the case. The acceptability of an argument for anaction turns not only on what is true in the situation, but also on the values and aspirations of the agent to whom the argument isdirected. There are three distinctive features of practical reasoning: first, that practical reasoning is situated in a context, directedtowards a particular agent at a particular time; second, that since agents differ in their aspirations there is no right answer for allagents, and rational disagreement is always possible; third, that since no agent can specify the relative priority of its aspirationsoutside of a particular context, such prioritisation must be a product of practical reasoning and cannot be used as an input to it.In this paper we present a framework for practical reasoning which accommodates these three distinctive features. We use thenotion of argumentation frameworks to capture the first feature. An extended form of argumentation framework in which valuesand aspirations can be represented is used to allow divergent opinions for different audiences, and complexity results relating to theextended framework are presented. We address the third feature using a formal description of a dialogue from which preferencesover values emerge. Soundness and completeness results for these dialogues are given.© 2006 Elsevier B.V. All rights reserved.Keywords: Argumentation frameworks; Practical reasoning; Dialogue1. IntroductionReasoning about what should be done in a particular situation—termed practical reasoning in the philosophicalliterature—is carried out through a process of argumentation. Argumentation is essential because no completely com-pelling answer can be given: whereas in matters of belief, we at least should be constrained by what is actually thecase, in matters of action no such constraints apply—we can choose what we will attempt to make the case. Even anorm as universal and deep seated as thou shalt not kill is acknowledged to permit of exceptions in circumstances ofself defence and war. Thus whether arguments justifying or urging a course of action are acceptable will depend onthe aspirations and values of the agent to which they are addressed: the audience for the argument. The importance ofthe audience for arguments was recognised and advocated by Perelman [28].* Corresponding author.E-mail address: ped@csc.liv.ac.uk (P.E. Dunne).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.10.013\fT.J.M. Bench-Capon et al. / Artificial Intelligence 171 (2007) 42–7143Arguments in practical reasoning provide presumptive reasons for performing an action. These presumptive argu-ments are then subject to a process of challenge, called critical questioning in [31]. These critical questions may takethe form of other arguments, which can in turn be challenged, or may be answered by further arguments, resultingin a set of arguments constructed as the debate develops. An extension of Walton’s account of practical reasoning isgiven in [2,6], which proposes an elaborated argument scheme for practical reasoning, which incorporates the valuepromoted by acceptance of the argument, and identifies all the ways in which it can be challenged. Although mostof our discussion will treat arguments at a very abstract level, where we have need of a more particular structure forarguments, we will have this account in mind.In this paper we will propose and explore a framework for the representation and evaluation of arguments in prac-tical reasoning. Any such framework must account for some important phenomena associated with such reasoning.We will review these features in this section, and will structure the development of our framework in the remainder ofthis paper around them.First, as is clear from the brief sketch of practical reasoning above, arguments cannot be considered in isolation.Whether an argument is acceptable or not depends on whether it can withstand or counter the other arguments putforward in the debate. Once the relevant arguments have been identified, whether a given argument is acceptable willdepend on its belonging to a coherent subset of the arguments put forward which is able to defend itself against allattackers. We will call such a coherent subset a position. This notion of the acceptability of an argument deriving frommembership of a defensible position has been explored in AI through the use of argumentation frameworks [9,19],and our account will be based on a framework of this sort. Dung’s framework [19] will be recapitulated in Section 2,and then extended as the paper proceeds. The reasoning involved in constructing argumentation frameworks andidentifying positions within them is naturally modelled as a dialogue between a proponent and a critic. Dialogues forthis purpose have been proposed in [8,13,22], and we will make use of the way of exploring argument frameworks.Dialogues are discussed in Section 5.A second important feature of practical reasoning is that rational disagreement is possible, the acceptability ofan argument depending in part on the audience to which it is addressed. Within Dung’s framework it is possiblefor disagreement to be represented since argumentation frameworks may contain multiple incompatible defensiblepositions. The abstract nature of arguments, however, means that there is no information that can be used to motivatethe choice of one option over another. Searle states the need to recognise that disagreement in practical reasoningcannot be eliminated as follows [29]:Assume universally valid and accepted standards of rationality, assume perfectly rational agents operating withperfect information, and you will find that rational disagreement will still occur; because, for example, the rationalagents are likely to have different and inconsistent values and interests, each of which may be rationally acceptable.What distinguishes different audiences are their values and interests, and in order to relate the positions acceptableto a given audience to the values and interests of that audience we need a way of relating arguments to such valuesand interests. Hunter [25] makes a proposal in terms of what he calls resonance, but we will build on Value BasedArgumentation Frameworks (VAFs) proposed in [9], in which every argument is explicitly associated with a valuepromoted by its acceptance, and audiences are characterised by the relative ranking they give to these values. Wewill describe VAFs in Section 3, their properties in Section 4, and discuss the relationship between our proposal andHunter’s in Section 7.The above machinery can allow us to explain disagreement in terms of differences in the rankings of values betweendifferent audiences, but it does not allow us to explain these rankings. This brings us to the third feature of practicalreasoning for which we wish to account—that we cannot assume that the parties to a debate will come with a clearranking of values: rather these rankings appear to emerge during the course of the debate. We may quote Searle again:This answer [that we can rank values in advance] while acceptable as far as it goes [as an ex post explanation],mistakenly implies that the preferences are given prior to practical reasoning, whereas, it seems to me, they aretypically the product of practical reasoning. And since ordered preferences are typically products of practicalreason, they cannot be treated as its universal presupposition. [29]\f44T.J.M. Bench-Capon et al. / Artificial Intelligence 171 (2007) 42–71The question of how value orders emerge during debate is explored in Section 6, in which we define a dialogueprocess for evaluating the status of arguments in a VAF, and in which we show how this process can be used toconstruct positions. In the course of constructing a position, the ordering of values will be determined.Although it is not reasonable to assume that participants in a debate come with a firm value order, and so we wishto account for the emergence of such an order, neither do participants usually come to an debate with a completelyopen mind. Usually there will be some actions they are predisposed to perform, and others which they are reluctantto perform, and they will have a tendency to prefer arguments which match these predispositions. For example apolitician forming a political programme may recognise that raising taxation is electorally inexpedient and so mustexclude any arguments with the conclusion that taxes should be raised from the manifesto, while ensuring that argu-ments justifying actions bringing about core objectives are present: other arguments are acceptable in so far as theyenable this. This kind of initial intuitive response to arguments will be used to drive the construction of positions andformation of a value order. A similar technique for constructing positions on the basis of Dung’s framework has beenproposed in [11]. Because this treatment does not make use of values, however, it cannot use these reasons for actionto motivate choices, and there is no relation between the arguments which can be exploited to demand that choicesare made in a consistent and coherent manner. Our extensions to include values enable us to impose this requirementof moral consistency on the reasoners.Our overall aim is ",
            {
                "entities": [
                    [
                        70,
                        107,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 805–837www.elsevier.com/locate/artintNegotiating using rewardsSarvapali D. Ramchurn a,∗, Carles Sierra b, Lluís Godo b,Nicholas R. Jennings aa IAM Group, School of Electronics and Computer Science, University of Southampton, UKb IIIA—Artificial Intelligence Research Institute, CSIC, Bellaterra, SpainReceived 8 November 2006; received in revised form 2 April 2007; accepted 2 April 2007Available online 6 May 2007AbstractNegotiation is a fundamental interaction mechanism in multi-agent systems because it allows self-interested agents to cometo mutually beneficial agreements and partition resources efficiently and effectively. Now, in many situations, the agents need tonegotiate with one another many times and so developing strategies that are effective over repeated interactions is an importantchallenge. Against this background, a growing body of work has examined the use of Persuasive Negotiation (PN), which involvesnegotiating using rhetorical arguments (such as threats, rewards, or appeals), in trying to convince an opponent to accept a givenoffer. Such mechanisms are especially suited to repeated encounters because they allow agents to influence the outcomes of futurenegotiations, while negotiating a deal in the present one, with the aim of producing results that are beneficial to both parties. Tothis end, in this paper, we develop a comprehensive PN mechanism for repeated interactions that makes use of rewards that can beasked for or given to. Our mechanism consists of two parts. First, a novel protocol that structures the interaction by capturing thecommitments that agents incur when using rewards. Second, a new reward generation algorithm that constructs promises of rewardsin future interactions as a means of permitting agents to reach better agreements, in a shorter time, in the present encounter. We thengo on to develop a specific negotiation tactic, based on this reward generation algorithm, and show that it can achieve significantlybetter outcomes than existing benchmark tactics that do not use such inducements. Specifically, we show, via empirical evaluationin a Multi-Move Prisoners’ Dilemma setting, that our tactic can lead to a 26% improvement in the utility of deals that are madeand that 21 times fewer messages need to be exchanged in order to achieve this.© 2007 Elsevier B.V. All rights reserved.Keywords: Persuasive negotiation; Repeated negotiations; Negotiation tactics; Bargaining; Bilateral negotiation1. IntroductionNegotiation is a fundamental concept in multi-agent systems (MAS) because it enables self-interested agents tofind agreements and partition resources efficiently and effectively. In most cases, such negotiation proceeds as a seriesof offers and counter-offers [20]. These offers generally indicate the preferred outcome for the proponent and theopponent may either accept them, counter-offer a more beneficial outcome, or reject them. Now, in many cases, the* Corresponding author.E-mail addresses: sdr@ecs.soton.ac.uk (S.D. Ramchurn), sierra@iiia.csic.es (C. Sierra), godo@iiia.csic.es (L. Godo), nrj@ecs.soton.ac.uk(N.R. Jennings).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.014\f806S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837agents involved need to negotiate with one another many times. However, such repeated encounters have rarely beendealt with in the multi-agent systems literature (see Section 7 for more details). One of the main reasons for this isthat repeated encounters require additional mechanisms and structures, over and above those required for single shotencounters, to fully take into account the repeated nature of the interaction. In particular, offers that are generatedshould not only influence the present encounter, but also future ones, so that better deals can be found in the long run[9,25]. To this end, argument-based negotiation (ABN), in which arguments are used to support offers and persuadean opponent to accept them, has been advocated as an effective means to achieve this [30,36] and, therefore, this isthe approach we explore in this paper.In more detail, ABN techniques aim to enable agents to achieve better agreements faster by allowing them toexplore a larger space of possible solutions and/or to express, update, or evolve their preferences in single or multipleshot interactions [21]. They do this by providing additional explanations that justify the offer [1], identifying othergoals satisfied by the offer that the opponent might not be aware of [31], or offering additional incentives conditionalupon the acceptance of the offer [2,22,39]. While all these approaches capture, in one way or another, the notionof persuasiveness, a number of them have focused specifically on the use of rhetorical arguments such as threats,rewards, and appeals [3,28,41,44]. To be clear, here, we categorise such argument acts as persuasive elements thataim to force, entice, or convince an opponent to accept a given offer (see Section 7 for more details). In particular,we categorise such approaches under the general term of Persuasive Negotiation (PN) to denote the fact that these tryto find additional incentives (as opposed to justifying or elaborating on the goals of an offer) to move an opponent toaccept a given offer [30,36].In order to implement a PN mechanism, it is critical that the exchanges between the negotiating agents follow agiven pattern (i.e. ensuring that agents are seen to execute what they propose and that the negotiation terminates) andthat the agents are endowed with appropriate techniques to generate such exchanges (i.e. they can evaluate offers andcounter-offers during the negotiation process). These requirements can be met through the specification of a protocolthat dictates what agents are allowed to offer or commit to execute and a reasoning mechanism that allows agentsto make sense of the offers exchanged and accordingly determine their best response [30]. Given this, we present anovel protocol and reasoning mechanism for pairs of agents to engage in PN in the context of repeated games, inwhich the participating agents have to negotiate over a number of issues many times. In particular, we focus on theexchange of rewards (as opposed to threats or appeals). We do so because rewards have a clear benefit for the agentreceiving it, and entail a direct commitment by the agent giving it, to continue a long term relationship which is likelyto be beneficial to both participating agents.1 In addition to the standard use of rewards as something that is offeredas a prize or gift, our model also allows agents to ‘ask’ for rewards in an attempt to secure better outcomes in thefuture, while conceding in the current encounter and therefore closing the deal more quickly. This latter perspective iscommon in human-to-human negotiations where one of the participants may ask for a subsequent favour in return foragreeing to concede in the current round [17,33].Being more specific still, our PN mechanism constructs possible rewards in terms of constraints on issues to benegotiated in future encounters and our protocol extends Rubinstein’s [37] alternating offers protocol to allow agentsto negotiate by exchanging arguments along with their offers (in the form of promises of future rewards or requestsfor such promises in future encounters).Example. A car seller may reward a buyer who prefers red cars with a promise (or the buyer might ask for the reward)of a discount of at least 10% (i.e. a constraint on the price the seller can propose next time) on the price of her yearlycar servicing if she agrees to buy a blue one instead at the demanded price (as the buyer’s asking price for the red caris too low for the seller). Now, if the buyer accepts, it is a better outcome for both parties; the buyer benefits becauseshe is able to make savings in future that match her preference for the red car and the seller benefits in that he reduceshis stock and obtains immediate profit.1 The use of appeals and threats poses a number of problems. For example, the use of appeals usually assumes agents implement the samedeductive mechanism (an overly constraining assumption in most cases) because appeals impact directly on an agent’s beliefs or goals whichmeans that such appeals need to adopt a commonly understood belief and goal representation [1,3,22]. Threats, in turn, tend to break relationshipsdown and are not guaranteed to be enforced, which makes them harder to assess in a negotiation encounter [19].\fS.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837807We believe such promises are important in repeated interactions for a number of reasons. First, agents may beable to reach an agreement faster in the present game by providing some guarantees over the outcome of subsequentgames. Thus, agents may find the current offer and the reward worth more than a counter-offer (which only delaysthe agreement and future games). Second, by involving issues from future negotiations in the present game (as inthe cost of servicing in the example above), we effectively expand the negotiation space considered and, therefore,provide more possibilities for finding (better) agreements in the long run [20]. For example, agents that value futureoutcomes more (because of their lower discount factors) than their opponent are able to obtain a higher utility infuture games, while the opponent who values immediate rewards can take them more quickly. Thirdly, if the rewardguarantees the range of possible outcomes in the next game, the corresponding negotiation space is constrained bythe reward, which should reduce the number of offers exchanged to search the space and hence the time elapsedbefore an agreement is reached. Continuing the above example, the buyer starts off with an advantage next time shewants to negotiate the price to service her car and she may then not need to negotiate for long to get a rea",
            {
                "entities": [
                    [
                        72,
                        97,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 799–823Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintReasoning under inconsistency: A forgetting-based approach ✩Jérôme Lang a, Pierre Marquis b,∗a LAMSADE-CNRS / Université Paris-Dauphine, Place du Maréchal de Lattre de Tassigny, 75775 Paris Cedex 16, Franceb CRIL-CNRS / Université d’Artois, rue Jean Souvraz, S.P. 18, 62307 Lens Cedex, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 15 September 2009Received in revised form 23 April 2010Accepted 23 April 2010Available online 29 April 2010Keywords:Knowledge representationReasoning under inconsistencyForgetting1. IntroductionIn this paper, a fairly general framework for reasoning from inconsistent propositionalbases is defined. Variable forgetting is used as a basic operation for weakening pieces ofinformation so as to restore consistency. The key notion is that of recoveries, which are setsof variables whose forgetting enables restoring consistency. Several criteria for definingpreferred recoveries are proposed, depending on whether the focus is laid on the relativerelevance of the atoms or the relative entrenchment of the pieces of information (orboth). Our framework encompasses several previous approaches as specific cases, includingreasoning from preferred consistent subsets, and some forms of information merging.Interestingly, the gain in flexibility and generality offered by our framework does not implya complexity shift compared to these specific cases.© 2010 Elsevier B.V. All rights reserved.Reasoning from inconsistent pieces of information, represented as logical formulas, is an important issue in ArtificialIntelligence. Thus, there are at least two very different contexts where inconsistent sets of formulas have to be dealt with.The first one is when the formulas express beliefs about the real world, that may stem from different sources. In thiscase, inconsistency means that some of the formulas are just wrong. The second one is when the input formulas expresspreferences (or goals, desires) expressed by different agents (or by a single agent according to different criteria). In this case,inconsistency does not mean that anything is incorrect, but that some preferences will not be able to be fulfilled. Even ifthe nature of the problems is very different whether we are in one context or the other one, many notions and techniquesthat can be employed to reason from inconsistent sets of formulas are similar.Whatever the nature of the information represented, classical reasoning is inadequate to derive significant consequencesfrom inconsistent formulas, since it trivializes in this situation (ex falso quodlibet sequitur). This calls for other inference rela-tions which avoid the trivialization problem (namely, paraconsistent inference relations) but there is no general consensusabout what such relations should be. Actually, both the complexity of the problem of reasoning under inconsistency and itssignificance are reflected by the number of approaches that have been developed for decades and can be found in the liter-ature under various names, like paraconsistent logics, belief revision, argumentative inference, information merging, modelfitting, arbitration, knowledge integration, knowledge purification, etc. (see [7,5] for surveys).Corresponding to these approaches, many different mechanisms to avoid trivialization can be exploited. A first taxonomyallows for distinguishing between active approaches, where inconsistency is removed by identifying wrong pieces of beliefthrough knowledge-gathering actions (see e.g. [34,36,28]) or by the group of agents agreeing on some goals to be given✩This is an extended and revised version of a paper that appeared in the Proceedings of the 8th International Conference on Knowledge Representationand Reasoning (KR’02), 2002, pp. 239–250.* Corresponding author.E-mail addresses: lang@lamsade.dauphine.fr (J. Lang), marquis@cril.univ-artois.fr (P. Marquis).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.023\f800J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823up after a negotiation process, from passive approaches, where inconsistency is dealt with. In this latter case, trivializationis avoided by weakening the set of consequences that can be derived from the given base (a set of formulas). In thepropositional case, this can be achieved by two means:(1) By weakening the consequence relation of classical logic while keeping the base intact. Such an approximation by below ofclassical entailment can be achieved, which typically leads to paraconsistent logics.(2) By weakening the input base while keeping classical entailment. The pieces of information from the initial base are weakenedsuch that their conjunction becomes consistent. This technique is at work in so-called coherence-based approaches toparaconsistent inference (see e.g. [46,24,25,11,3,41,44,4] for some of the early references), where weakening the inputbase consists in inhibiting some of the pieces of information it contains (by removing them). It is also at work in beliefmerging (see e.g. [37,47,31,40] for some of the early references). Belief merging, and especially distance-based mergingconsists in weakening the pieces of information by dilating them: the piece of information φ, instead of expressing thatthe real world is for sure among the models of φ, now expresses that it is close to be a model of φ (the further a worldω from the models of φ, the less plausible it is that ω is the real world).The above dichotomy between (1) and (2) is reminiscent of the dichotomy between actual and potential contradictions, asdiscussed in [7,5]. Actual contradictions tolerate inconsistency by reasoning with a set of inconsistent statements, whereaspotential contradictions are prevented from arising by putting individually consistent yet jointly inconsistent informationtogether.In the rest of this paper we deal with potential inconsistencies and focus on the class of approaches, consisting inweakening the input base. While existing weakening-based approaches work well on some families of problems, there aretypical examples that they fail to handle in a satisfactory way (see Section 6 for a detailed discussion), the reason being thatwhile some of these approaches take account for the relative importance of pieces of information (or of the correspondingsources), they do not handle the relative importance of atoms in the problem at hand. This is problematic in many situationswhere some atoms are less central than others, especially when some atoms are meaningful only in the presence of others.For instance, it makes little sense to reason about whether John’s car is grey if there is some strong conflict about whetherJohn actually has a car. Or, in a preference merging context, suppose that a group of co-owners of a residence tries to agreeabout whether a tennis court or a swimming pool should be built: if there is no agreement about whether the swimmingpool is to be constructed, any preference concerning its colour must be (at least temporarily) ignored (this would not bethe case for its size, however, because it influences its price in a dramatic way).More generally, it is sometimes the case that ignoring a small set of propositional atoms of the formulas from an incon-sistent set renders it consistent. When reasoning from inconsistent beliefs, this allows for giving some useful informationabout the other atoms (those that are not forgotten); information about other atoms can be processed further (for instancethrough knowledge-gathering actions) if these atoms are important enough. When trying to reach a common decision froman inconsistent set of preferences, ignoring small sets of atoms allows for making a decision about all other atoms; thedecision about the few remaining atoms can then take place after a negotiation process among agents.In the following, we define a framework for reasoning from inconsistent propositional bases, using forgetting [10,39,33]as a basic operation for weakening formulas. Belief (or preference) bases are viewed as (finite) vectors of propositional for-mulas, conjunctively interpreted. Without loss of generality, each formula is assumed to be issued from a specific source ofinformation (or a specific agent). Forgetting a set X of atoms in a formula consists in replacing it by its logically strongestconsequence which is independent of X , in the sense that it is equivalent to a formula in which no atom from X occurs [33].The key notion of our approach is that of recoveries, which are sets of atoms whose forgetting enables restoring consistency.The intuition of this simple principle is that if a collection of pieces of information is jointly inconsistent, weakening it byignoring some atoms (for instance the least important ones) can help restoring consistency and derive reasonable conclu-sions. Several criteria for defining preferred recoveries are proposed, depending on whether the focus is laid on the relativerelevance of the atoms or the relative entrenchment of the pieces of information (or both).Our contributions are composed of the following models and results. We first define a general model for using variableforgetting in order to reason under inconsistency. We show that our model is general enough to encompass several classesof paraconsistent inference relations, including reasoning from preferred consistent subbases (Propositions 4.1 and 4.2) andvarious types of belief merging (Propositions 4.3, 4.4 and 4.5). Our framework does not only recover known approaches asspecific cases (which would make its interest rather limited) but it allows for new families of paraconsistent inferences,including homogeneous inferences, where propositional variables have to be forgotten in a homogeneous way from thedifferent sources, and abstraction-based inferences, where the most specific variables a",
            {
                "entities": [
                    [
                        136,
                        194,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 1540–1569Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintLearning complex action models with quantifiers and logical implicationsHankz Hankui Zhuo a,b, Qiang Yang b,∗, Derek Hao Hu b, Lei Li aa Department of Computer Science, Sun Yat-sen University, Guangzhou, China, 510275b Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clearwater Bay, Kowloon, Hong Konga r t i c l ei n f oa b s t r a c tArticle history:Received 2 January 2010Received in revised form 4 September 2010Accepted 5 September 2010Available online 29 September 2010Keywords:Action model learningMachine learningKnowledge engineeringAutomated planningAutomated planning requires action models described using languages such as the PlanningDomain Definition Language (PDDL) as input, but building action models from scratch is avery difficult and time-consuming task, even for experts. This is because it is difficult toformally describe all conditions and changes, reflected in the preconditions and effects ofaction models. In the past, there have been algorithms that can automatically learn simpleaction models from plan traces. However, there are many cases in the real world wherewe need more complicated expressions based on universal and existential quantifiers,implications in action models to precisely describe the underlyingas well as logicalmechanisms of the actions. Such complex action models cannot be learned using manyprevious algorithms. In this article, we present a novel algorithm called LAMP (LearningAction Models from Plan traces), to learn action models with quantifiers and logicalimplications from a set of observed plan traces with only partially observed intermediatestate information. The LAMP algorithm generates candidate formulas that are passed to aMarkov Logic Network (MLN) for selecting the most likely subsets of candidate formulas.The selected subset of formulas is then transformed into learned action models, which canthen be tweaked by domain experts to arrive at the final models. We evaluate our approachin four planning domains to demonstrate that LAMP is effective in learning complex actionmodels. We also analyze the human effort saved by using LAMP in helping to create actionmodels through a user study. Finally, we apply LAMP to a real-world application domain forsoftware requirement engineering to help the engineers acquire software requirements andshow that LAMP can indeed help experts a great deal in real-world knowledge-engineeringapplications.© 2010 Elsevier B.V. All rights reserved.1. IntroductionAutomated planning systems achieve goals by producing sequences of actions from the given action models that areprovided as input [14]. A typical way to describe the action models is to use action languages such as the Planning DomainDefinition Language (PDDL) [13,11,14] in which one can specify the precedence and consequence of actions. A traditionalway of building action models is to ask domain experts to analyze a task domain and manually construct a domain de-scription that includes a set of complete action models. Planning systems can then proceed to generate action sequences toachieve goals.However, it is very difficult and time-consuming to manually build action models in a given task domain, even forexperts. This is a typical problem of the knowledge-engineering bottleneck, where experts often find it difficult to articulatetheir experiences formally and completely. Because of this, researchers have started to explore ways to reduce the human* Corresponding author.E-mail addresses: zhuohank@mail.sysu.edu.cn (H.H. Zhuo), qyang@cse.ust.hk (Q. Yang), derekhh@cse.ust.hk (D.H. Hu), lnslilei@mail.sysu.edu.cn (L. Li).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.09.007\fH.H. Zhuo et al. / Artificial Intelligence 174 (2010) 1540–15691541effort of building action models by learning from observed examples or plan traces. Some researchers have developedmethods to learn action models from complete state information before and after an action in some example plan traces [4,15,31,50]. Others, such as Yang et al. [51,39] have proposed to learn action models from plan examples with only incompletestate information. Yang et al. [51,52] have developed an approach known as Action Relation Modeling System (ARMS) to learnaction models in a STRIPS (STanford Research Institute Problem Solver) [10] representation using a weighted MAXSAT-based(Maximum Satisfiability) approach. Shahaf et al. [39] have proposed an algorithm called Simultaneous Learning and Filtering(SLAF) to learn more expressive action models using consistency-based algorithms.Despite the success of these learning systems, in the real world, there are many applications where actions should beexpressed using a more expressive representation, namely, quantifiers and logical implications. For instance, consider thecase where there are different cases in a briefcase1 planning domain, such that a briefcase should not be moved to a placewhere there is another briefcase with the same color. We can model the action move in PDDL as follows.2action: move(?c1 - case ?l1 ?l2 - location)pre:(:and (at ?c1 ?l1)(forall ?c2 - case (imply (samecolor ?c2 ?c1)(not (at ?c2 ?l2)))))effect:(:and (at ?c1 ?l2) (not (at ?c1 ?l1)))That is, if we want to move the case c1 from the location l1 to l2, c1 should be at l1 first, and every other case c2 whosecolor is the same with c1 should not be at l2. After the action move, c1 will be at l2 instead of at l1. Likewise, consider apilot could not fly to a place where there are enemies. We can model the action model fly as follows.action:pre:fly(?p1 - pilot ?l1 ?l2 - location)(:and (at ?p1 ?l1)(forall ?p2 - person (imply (enemy ?p2 ?p1)(not (at ?p2 ?l2)))))effect:(:and (at ?p1 ?l2) (not (at ?p1 ?l1)))We can see that in these examples, we need universal quantifiers as well as logical implications in the precondition part ofthe action to precisely represent this action and compress the action model in a compact form.As another example, consider a driver who intends to drive a train. Before he can start, he should make sure all thepassengers have gotten on the train. After that, if there is a seat vacant, then he can start to drive the train. We representthis drive-train action model in PDDL as follows.action:pre:effect:drive-train(?d - driver ?t - train)(free ?d) (forall ?p - passenger (in ?p ?t))(:and (when (exist ?s - seat (vacant ?s)) (available ?t))(driving ?d ?t)(not (free ?d)))That is, if a driver ?d makes sure all the passengers ?p are in the train ?t and is free at that time, then he can drive thetrain ?t. Furthermore, if there is a seat ?s vacant, as a consequence of this action drive-train, the train will be set as availableto show that more passengers can take this train. Besides, the driver ?d will be in the state of driving the train, i.e., (driving?d ?t), and not free. Such an action model needs a universal quantifier in describing its preconditions and an existentialquantifier for the condition “(exist ?s - seat (vacant ?s))” of the conditional effect “(when (exist ?s - seat (vacant ?s))(available?t))”. More examples that require the use of quantifiers and logical implications can be found in many action models inrecent International Planning Competitions, such as the domains in IPC-53: trucks, openstacks, etc. These complex actionmodels can be represented by PDDL, but cannot be learned by existing algorithms proposed for action model learning.Our objective is to develop a new algorithm for learning complex action models with quantifiers (including conditionaleffects) and logical implications, from a collection of given example plan traces. The input of our algorithm includes: (1)a set of observed plan traces with partially observed intermediate state information between actions; (2) a list of actionheadings, each of which is composed of an action name and a list of parameters, but is not provided with preconditions oreffects; (3) a list of predicates along with their corresponding parameters. Our algorithm is called LAMP (Learn Action Modelsfrom Plan traces), which outputs a set of action models with quantifiers and implications. These action models ‘summarizes’the plan traces as much as possible, and can be used by domain experts, who need to spend only a small amount of time, inrevising parts of the action models that are incorrect or incomplete, before finalizing the action models for planning usage.Compared to many previous approaches, our main contributions are: (1) LAMP can learn quantifiers that conform to thePDDL definition [13,11], where the latter article shows that action models in PDDL can have quantifiers. (2) LAMP can learnaction models with implications as preconditions, which improves the expressiveness of learned action models. We require1 http://www.informatik.uni-freiburg.de/~koehler/ipp/pddl-domains.tar.gz.2 A symbol with a prefix “?” suggests that the symbol is a variable; e.g. “?c1” suggests that “c1” is a variable that can take on certain constants as values.3 http://zeus.ing.unibs.it/ipc-5/domains.html.\f1542H.H. Zhuo et al. / Artificial Intelligence 174 (2010) 1540–1569that existential quantifiers can only be used to quantify the left-hand side of a conditional effect, which is consistent withthe constraints used in PDDL1.2 and PDDL2.1 [13,11]. (3) Similar to some previous systems such as the ARMS system [51,52], LAMP can learn action models from plan traces with partially observed intermediate state information. This is importantbecause in many real world situations, what we can record between two actions in a plan trace is likely to be incomplete;e.g., when using only a small number of sensors, we can record a subset of what happens after each action is executed.In such a case, the number of sensors cannot cover all possible new information sources and ",
            {
                "entities": [
                    [
                        138,
                        210,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 105 (1998) 295-343 Artificial Intelligence Analysis of notions of diagnosis Peter J.F. Lucas * Department qf Computer Science, Utrecht Universiry, PO. Box 80.089, 3508 TB Utwcht, The Netherlands Received 27 November 1997 Abstract Various formal theories have been proposed in the literature to capture the notions of diagnosis framework for the analysis of diagnosis underlying diagnostic programs. Examples of such notions are: heuristic classification, which is used in systems incorporating empirical knowledge, and model-based diagnosis, which is used in diagnostic systems based on detailed domain models. Typically, such domain models include interactions among modelled objects. In this paper, knowledge of causal, structural, and functional a new set-theoretical the framework is presented. Basically, the net impact of knowledge bases distinguishes between functions are to for purposes of diagnosis, and ‘notions of diagnosis’, which define how evidence be used to map findings observed framework offers a simple, yet powerful as well as for proposing new notions of diagnosis. A theory of flexible notions of diagnosis, called refinement diagnosis, is proposed and defined in terms of this framework. Relationships with notions of diagnosis known from the literature are investigated. 0 1998 Elsevier Science B.V. All rights reserved. ‘evidence functions’, which characterize for a problem case to diagnostic existing notions of diagnosis, solutions. This set-theoretical tool for comparing Keywords: Diagnostic systems; Semantics of diagnosis; Formal theory of diagnosis 1. Introduction Diagnostic Intelligence. In the burgeoning computer programs were among in the field in the 1970s dealt differed In a sense, this was a consequence of the additional goal of the development and problem-solving of applied Artificial and 198Os, diagnostic with similar, or related, problem domains, considerably. of many of these, now classic, programs: methods. Only after researchers experienced the first systems developed field of expert systems these systems that developing reliable diagnostic systems to explore representation abound. Although their underlying applications frequently principles often * Email: lucas@cs.uu.nl. COO43702/98/$ - see front matter 0 1998 Elsevier Science B.V. All rights reserved. PII: SOOO4-3702(9X)0008 1-2 \f296 l?J.F: Lucas /Artificial Intelligence IO5 (1998) 295-343 was much more difficult underlying diagnosis were actually poorly understood. than previously thought, it was recognized that the principles formal aspects of diagnosis was undertaken, with into the nature of diagnostic problem solving. For example, Chandrasekaran Starting about halfway through the 1980s a significant amount of research on conceptual the aim of acquiring more and in terms of a small number behaviour of the diagnostic process conceptually tasks’ [6]. Instead of studying systems, other researchers have focussed on representation issues in diagnostic the problem-solving and insight colleagues have analysed ‘generic problem-solving diagnostic systems. Where in many early diagnostic captured approaches became (cf. [2,14]), and nonindustrial approach advocates of a problem domain. For example, interactions elements systems INTER [17], SOPHIE among components in a domain. Model-based in the form of empirical increasingly the construction of knowledge-based systems diagnostic knowledge from experts was classification popular for building diagnostic systems areas, such as medicine rules [4], in later systems model-based in both industrial (cf. [19,40]). The model-based systems based on explicit models the structural and functional among interactions in the early system, or the causal such models describe of a physical diagnosis was, in fact, already explored [43], and ABEL [27]. Although the introduction [3], CASNET of the model-based impact on the field of diagnosis, into the process of diagnosis. Real fundamental tions had a significant insight diagnostic process was yielded by subsequent diagnosis. approach to building diagnostic applica- it did not immediately provide deep of the nature of the the formal aspects of understanding research concerning An early formal theory of diagnosis was proposed by Reggia and colleagues causal knowledge of abnormality covering causal theory of diagnosis, theory, or parsimonious of set theory, called set-covering in the set-covering by means of a binary essentially by determining whether actually observed causal relation. Subsequent work has yielded several algorithms in practical applications diagnoses efficiently reasoning to be NP-hard is known theory and its variants have been performed by several researchers [29,38,44], although [5]. Experimental relation, which is employed in general to compute set-covering this type of diagnostic studies of set-covering [21,34,41]. findings can be predicted using the in terms theory [35]. Basically, is represented diagnoses, for computing The formal aspects of diagnosis employing using logic as the primary is formalized diagnosis represented as logical implications tool [ 11,13,30,32]. as reasoning of the form causal knowledge have also been studied, In the logical theory of abductive diagnosis, to causes, with causal knowledge from effects causes + effects abnormalities type of reasoning of the form above and under certain conditions, or faults, but where causes are usually situations. This abductive implications effects are conjunctions effects. Because for certain observed diagnosis as well. The logical covering in set-covering findings, theory, because they may also literals, would amount is contrasted with deduction, which include normal for like that given causes and to to find causes this theory may be viewed as a specific theory of abductive theory of abductive diagnosis than set- to explicitly is more expressive represent various types of interaction, theory causal relations are also exploited from causes to reasoning it is possible of positive \fI?J.E Lucas /Artificial Intelligence IO5 (1998) 295-343 297 set-covering theory. For example, in the original set-covering in the original leads which is not possible to express or more causes proposed several different versions of abductive diagnosis an implementation investigated reasoning it is not possible occurrence of two findings. Console and colleagues have [9,11], and have also developed of the theory as the CHECK system [40]. Poole and colleagues have abductive diagnosis using Theorist, to the masking of certain a theory and system that the simultaneous for hypothetical [30-321. theory logic-based Approximately at the same time, Reiter proposed yet another aiming at formally capturing diagnosis of abnormal behaviour diagnosis, system, using a model of normal structure and behaviour which was later extended by de Kleer and colleagues normal and abnormal behaviour based diagnosis. Basically, consistency-based components for a discrepancy device, possibly supplemented with predictions of abnormal behaviour, both according a domain model, and actually observed behaviour. The discrepancy a diagnosis inconsistency; and others to be normally theory of in a device or [36]. Nowadays, Reiter’s theory, to deal with knowledge of both to as the theory of consistency- to finding faulty device diagnosis amounts between predicted normal behaviour of a to is formalized as logical to be faulty is established when assuming particular components functioning restores consistency. [ 181, is usually that account referred The abductive and consistency-based theories of diagnosis implications, diagnosis based on empirical associations. When empirical associations then viewed as a classification logical be accomplished by logical deduction, computing The more procedurally refer to this type of diagnostic term heuristic clusszjkation reasoning oriented relation, establishing [8]. the closure of this classification are often contrasted with are represented as a diagnosis can relation. to employed is frequently It has been shown that abductive and consistency-based diagnosis can be mapped that diagnostic each other [36]. Furthermore, both types of diagnosis can be defined, ways, in terms of the logical entailment thought checking, abductive types of reasoning, than that [26]. it appears that characterizing reasoning, deductive diagnostic systems relation [ 12,331. Hence, although systems could be classified as being either based on consistency reasoning, or on a combination of these three is more complicated to in slightly different it was once The conclusion that there is not a unique way to characterize underlying the assumptions a particular abductive diagnosis, type of the diagnosis is logical concerning implication and heuristic raises questions classification. Does in abductive diagnosis, In this paper, it is argued that the formalization diagnosis logical notion of consistency-based consistency provide an appropriate basis for formalizing various notions of diagnosis, and relationships between causes the proper way to formalize similarly, and effects in heuristic and to formalize of diagnosis will benefit from classification? the modelling of interactions between to observable in the context of such a mapping. A set-theoretical aspects of diagnosis using known are taken as example domains. (1) the modelling of interactions from defects findings to express these in detail in Section 4 from the literature. Medicine and simple logic circuits at two levels of specification: the presence or absence of defects or faults, expressed by a mapping and (2) the modelling of an interpretation in Sections 2 and 3, and examined theories of diagnosis of observed is proposed associations framework empirical semantic findings, \f298 RJ.E Lucas /Artificial Intelligence 105 (1998) 295-343 As in many other theories of diagnosis, diagnostic problem solving is viewed as a problem, reasoning instance of hypothetical In s",
            {
                "entities": [
                    [
                        76,
                        108,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 76 (1995) 125-166 Artificial Intelligence Understanding planner behavior Adele E. Howe a**, Paul R. Cohen b~l a Computer Science Department, Colorado State Universiry, Fort Collins, CO 80523, USA b Computer Science Department, University of Massachusetts, Amherst, MA 01003, USA Received June 1993; revised March 1994 Abstract As planners and their environments become increasingly complex, planner behavior becomes increasingly difficult to understand. We often do not understand what causes them to fail, so that we can debug their failures, and we may not understand what allows them to succeed, so that we can design the next generation. This paper describes a partially automated methodology for understanding planner behavior over long periods of time. The methodology, calkd Dependency Interpretation, uses statistical dependency detection to identify interesting patterns of behavior in execution traces and interprets the patterns using a weak model of the planner’s interaction to explain how the patterns might be caused by the planner. Dependency with its environment Interpretation has been applied to identify possible causes of plan failures in the Phoenix planner. By analyzing four sets of execution traces gathered from about 400 runs of the Phoenix planner, we showed that the statistical dependencies describe patterns of behavior that are sensitive to the version of the planner and to increasing temporal separation between events, and that dependency detection degrades predictably as the number of available execution traces decreases and as noise is introduced in the execution traces. Dependency Interpretation is appropriate when a complete and correct model of the planner and environment is not available, but execution traces are available. 1. Introduction AI planners have long been introspective. They sit thinking about whether their actions will interact or their plans are efficient, and then they issue a plan. Some planners execute their plans, sometimes before planning is complete. The introspective nature of planners can make it difficult for us to understand why they act as they do, especially when * Corresponding author. Emaik howe@cs.colostakedu. 1 E-mail: cohen@cs.umass.edu. 0004-3702/95/.$09.50 @ 1995 Elsevier Science B.V. All rights reserved SSDIOOO4-3702(94)00083-2 \f126 A.E. Howe, i?R. CohedArtijicial Intelligence 76 (1995) 125-166 regularities the regularities. the planner and even more so when includes many interacting and acting are interleaved, their behavior, we frequently have trouble connecting components, planning to a dynamic environment. Although we know how our planners are designed, and we can record design combine represent that skips a staggering amount of detail and permits us to predict and explain of modifications Our approach traces. To understand planner behavior, we must traces in a way the effects to produce complex execution the internal workings of a planner and its external execution the two. A planner’s and actions and events in the environment to the planner, to understanding its tasks, and its environment. is to find statistical planner behavior is responsive in actions among actions-unexpectedly arise for many follow another in a strict order. Some dependencies, traces and then use a weak model of the planner to explain frequent or infrequent co- reasons, most very mundane. For example, trace because a frequently- in an execution execution First, we find dependencies occurrences. Dependencies one action might frequently however, alert includes both actions used subplan suggests us to problems; for example, trace, that the planner has become the trace, mundane or otherwise, we find out which plans were being executed during and we search the plans for structures of actions, such as a strict linear ordering between actions. When we have found all such structures, we of the dependencies. All these steps are use them to help us debug completely the failure-recovery a long sequence of obstacle-avoidance trapped. To explain dependencies to index a library of explanations that often result in co-occurrences automated or semi-automated, and they have been applied in an execution component of the Phoenix planner to find statistical dependencies relationships, to identify in execution It is straightforward slightly more difficult However, most dependencies is to show how weak knowledge interpret dependencies. Our approach fragments of execution traces in terms of detailed histories of internal and external states, but, rather, to explain in statistical plans. signify nothing of interest. The contribution such as overlap, between dependencies. of this paper can be used to about a planner and its environment in terms of general structures is not to explain particular over many planning traces, and only tendencies, episodes, [ 121. I. 1. Related research have taken the other approach Much of the research effort into understanding in particular, understanding why plans planner behavior has focused on plan fail so that the plan and the planner failures, can be debugged. Researchers two general positions on plan and planner debugging. One is that bugs or failures can be anticipated by looking at the structure is to uncover pathologies by simulating plan execution or of plans, actually executing plans. Sussman’s HACKER is the earliest example of the first approach [ 191: critics and detect look at the structure of a plan at each level of its development potential problems Instead of simulating would increasingly execution that to identify and satisfy among actions; again, without actually simulating plan lead to bugs. For several years, researchers built planners to discover bugs, HACKER could recognize structures (and opportunities), which inform the next level of plan development. complex constraints execution [ 181. \fA.E. Howe, P.R. Cohen/Art@cial Intelligence 76 (1995) 125-166 127 [ 161 debugs plans by simulating them with a causal model. GORDIUS Many subsequent efforts relied on simulation or execution, however. Hammond’s [ lo] simulates execution to produce an execution trace of the plan that includes CHEF relationships between plan actions and resulting states. CHEF chains backward through the execution trace to determine the steps that caused a failure, classifies the failure cause based on the explanation of what happened and indexes into a set of general repair strategies to fix the plan to avoid the observed failure. Like CHEF, Simmons’ traces GOFUXUS plan assumptions by regressing desired outcomes (values of states) through a causal dependency structure (generated through simulation of the plan) and identifying mis- matches. The system then repairs the faulty assumptions with one of a set of general repair strategies. Both Hammond’s and Simmons’ approaches assume that the simulator has a correct model of the domain; the approaches differ in the kinds of flaws they detect and the strategies they apply to repair the faults. Related efforts, all of which rely on causal models of the planner, include Hudlicka and Lesser’s work on diagnosing failures during execution [ 141, and Birnbaum et al.? proposal to enhance model-based diagnosis of plans [ 31. Most of the research addresses debugging plans, rather than debugging the planner. While the two are related (after all, explaining why a plan would fail is a step toward explaining why a planner should not favor such a plan), little has been done on planner debugging. The most notable exception is Hammond’s CHEF, which learned from its plan failures. CHEF would remember repaired plans and the bugs found in them so that subsequent planning could use the newly modified plans and account for the bugs found previously. Others have exploited the idea of model-based explanation of failures, bugs and errors that arise in execution traces to learn new plans (e.g., [ 251) . Our position is an amalgam of the structural and simulation/execution approaches focused on debugging the planner. We think it is too difficult to debug a planner by identifying an individual bug and explaining how it arose in terms of a history of states, variable bindings, environmental events and other details. Instead, we rely on statistical dependencies to point us to pieces of plans, and we look for structural features of those pieces that might explain the dependencies. After we have made some modifications, we test whether our explanations were correct by executing plans and seeing whether particular dependencies disappear or are reduced. Our approach has some parallels in software testing and program analysis. For example, Bates and Wileden [l] developed a language for describing salient abstractions of a system’s behavior; a module monitors the system’s behavior and extracts event traces based on the desired abstractions. Gupta describes a knowledge-based system for selectively collecting and analyzing traces of interprocess messages [ 91. In both of these systems, a human programmer must examine the resulting traces and localized failures to determine why the software failed and to debug it. As in our statistical approach, a technique for hardware fault diagnosis, called correspondence analysis, classifies failure modes into “causes” by analyzing contingency tables of system test results [ 151. As in our knowledge-based approach, the DAACS (Dump Analysis And Consulting System) takes a snapshot of a particular type of fatal program error (the contents of a minidump) and matches information from the dump to a belief network of canonical diagnoses [ 41. The result is a set of hypotheses about the source of the failure. \f128 A.E. Howe, RR. Cohen/Artij?cial Intelligence 76 (1995) 125-166 Most research in AI debugging explains particular failures in order to debug a plan; most research from software testing addresses finding patterns of behavior in order to debug a program. Our approach combines the two to explain patterns of behavior over tim",
            {
                "entities": [
                    [
                        66,
                        96,
                        "TITLE"
                    ],
                    [
                        725,
                        755,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 170 (2006) 803–834www.elsevier.com/locate/artintPropagation algorithms for lexicographic ordering constraintsAlan M. Frisch a,∗, Brahim Hnich b, Zeynep Kiziltan c, Ian Miguel d, Toby Walsh ea Department of Computer Science, University of York, UKb Faculty of Computer Science, Izmir University of Economics, Turkeyc DEIS, University of Bologna, Italyd School of Computer Science, University of St Andrews, UKe National ICT Australia and Department of CS & E, UNSW, AustraliaReceived 12 April 2005; received in revised form 24 March 2006; accepted 27 March 2006AbstractFinite-domain constraint programming has been used with great success to tackle a wide variety of combinatorial problems inindustry and academia. To apply finite-domain constraint programming to a problem, it is modelled by a set of constraints on a setof decision variables. A common modelling pattern is the use of matrices of decision variables. The rows and/or columns of thesematrices are often symmetric, leading to redundancy in a systematic search for solutions. An effective method of breaking thissymmetry is to constrain the assignments of the affected rows and columns to be ordered lexicographically. This paper develops anincremental propagation algorithm, GACLexLeq, that establishes generalised arc consistency on this constraint in O(n) operations,where n is the length of the vectors. Furthermore, this paper shows that decomposing GACLexLeq into primitive constraintsavailable in current finite-domain constraint toolkits reduces the strength or increases the cost of constraint propagation. Alsopresented are extensions and modifications to the algorithm to handle strict lexicographic ordering, detection of entailment, andvectors of unequal length. Experimental results on a number of domains demonstrate the value of GACLexLeq.© 2006 Elsevier B.V. All rights reserved.Keywords: Artificial intelligence; Constraints; Constraint programming; Constraint propagation; Lexicographic ordering; Symmetry; Symmetrybreaking; Generalized arc consistency; Matrix models1. IntroductionConstraints are a natural means of knowledge representation. For instance: the maths class must be timetabledbetween 9 and 11am on Monday; the helicopter can carry up to four passengers; the sum of the variables must equal100. This generality underpins the success with which finite-domain constraint programming has been applied to awide variety of disciplines [27]. To apply finite-domain constraint programming to a given domain, a problem mustfirst be characterised or modelled by a set of constraints on a set of decision variables, which its solutions must satisfy.A common pattern arising in the modelling process is the use of matrices of decision variables, so-called matrixmodels [9]. For example, it is simple to represent many types of functions and relations in this way [15].* Corresponding author.E-mail addresses: frisch@cs.york.ac.uk (A.M. Frisch), brahim.hnich@ieu.edu.tr (B. Hnich), zkiziltan@deis.unibo.it (Z. Kiziltan),ianm@dcs.st-and.ac.uk (I. Miguel), tw@cse.unsw.edu.au (T. Walsh).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.03.002\f804A.M. Frisch et al. / Artificial Intelligence 170 (2006) 803–834Concomitant with the selection of a matrix model is the possibility that the rows and/or columns of the matrixare symmetric. Consider, for instance, a matrix model of a constraint problem that requires finding a relation R onA × B where A and B are n-element and m-element sets of interchangeable objects respectively. The matrix, M, hasn columns and m rows to represent the elements of A and B. Each decision variable Ma,b can be assigned either 1 or0 to indicate whether (cid:3)a, b(cid:4) ∈ A × B is in R. Symmetry has been introduced because the matrix, whose columns androws are indexed by A and B, distinguishes the position of the elements of the sets, whereas A and B did not. Givena (non-)solution to this problem instance, a (non-)solution can be obtained by permuting columns of assignmentsand/or permuting rows of assignments. This is known as row and column symmetry [8]. Since similar behaviour canbe found in multidimensional matrices of decision variables it is known more generally as index symmetry. As is welldocumented, symmetry can lead to a great deal of redundancy in systematic search [8].As reviewed in Section 2.5 of this paper, lexicographic ordering constraints have been shown to be an effectivemethod of breaking index symmetry. This paper describes a constraint propagation algorithm, GACLexLeq, that en-forces this constraint. Given a lexicographic ordering constraint c, the propagation algorithm removes values from thedomains of the constrained variables that cannot be part of any solution to c. This paper also shows that GACLexLeqestablishes a property called generalised arc consistency,—that is it removes all infeasible values—while only re-quiring a number of operations linear in the number of variables constrained. The GACLexLeq algorithm is alsoincremental; if the domain of a variable is reduced the algorithm can re-establish generalised arc consistency withoutworking from scratch.Although the examples and experiments in the paper employ the lexicographic ordering constraint to break indexsymmetry, we note that lexicographic ordering can be used to break any symmetry that operates on the variables ofan instance. The lex-leader method [5] breaks all symmetry by identifying a representative among the elements of theequivalence class of symmetries of an instance and adding a lexicographic ordering constraint for each other elementof the equivalence class to ensure that only the representative is allowed.The paper is organised as follows. Section 2 introduces the necessary background while Section 3 describes a num-ber of applications used to evaluate our approach. Section 4 presents a propagation algorithm for the lexicographicordering constraint. Then Section 5 discusses the complexity of the algorithm, and proves that the algorithm is soundand complete. Section 6 extends the algorithm to propagate a strict ordering constraint, to detect entailment, and tohandle vectors of different lengths. Alternative approaches to propagating the lexicographic ordering constraint arediscussed in Section 7. Section 8 demonstrates that decomposing a chain of lexicographic ordering constraints intolexicographic ordering constraints between adjacent or all pairs of vectors hinders constraint propagation. Computa-tional results are presented in Section 9. Finally, we conclude and outline some future directions in Section 10.2. BackgroundAn instance of the finite-domain constraint satisfaction problem (CSP) consists of:• a finite set of variables X ;• for each variable X ∈ X , a finite set D(X) of values (its domain); and• a finite set C of constraints on the variables, where each constraint c(X1, . . . , Xn) ∈ C is defined over the variablesX1, . . . , Xn by a subset of D(X1) × · · · × D(Xn) giving the set of allowed combinations of values. That is, c isan n-ary relation.A variable assignment maps every variable in a given instance of CSP to a member of its domain. A variableassignment A is said to satisfy a constraint c(X1, . . . , Xn) if and only if (cid:3)A(X1), . . . , A(Xn)(cid:4) is in the relation denotedby c. A solution to an instance of CSP is a variable assignment that satisfies all the constraints. An instance is said to besatisfiable if it has a solution; otherwise it is unsatisfiable. Typically, we are interested in finding one or all solutions,or an optimal solution given some objective function. In the presence of an objective function, a CSP instance is aninstance of the constraint optimisation problem.To impose total ordering constraints on variables and vectors of variables there must be an underlying total orderingon domains. If the domain of interest is not totally ordered, a total order can be imposed. And now, since domainsare always finite, every domain is isomorphic to a finite set of integers. So we shall simplify the presentation byconsidering all domains to be finite sets of integers.\fA.M. Frisch et al. / Artificial Intelligence 170 (2006) 803–834805The minimum element in the domain of variable X is min(X), and the maximum is max(X). Throughout, vars(c)is used to denote the set of variables constrained by constraint c.If a variable X has a singleton domain {v} we say that v is assigned to X, or simply that X is assigned. If two.= X(cid:6)). If v is.= X(cid:6), otherwise we write ¬(Xvariables X and X(cid:6) are assigned the same value, then we write Xassigned to X and v(cid:6) is assigned to X(cid:6) and v < v(cid:6) then we write X (cid:2) X(cid:6).A constraint c is entailed if all assignments of values to vars(c) satisfy c. If a constraint can be shown to be entailedthen running the (potentially expensive) propagation algorithm can be avoided. Similarly, a constraint c is disentailedwhen all assignments of values to vars(c) violate c. Observe that if a constraint in a CSP instance can be shown to bedisentailed then the instance has no solution.2.1. Generalised arc consistencyThis paper focuses on solving the CSP by searching for a solution in a space of assignments to subsets of thevariables. Solution methods of this type use propagation algorithms that make inferences based on the domains ofthe constrained variables and the assignments that satisfy the constraint. These inferences are typically recordedas reductions in variable domains, where the elements removed cannot form part of any assignment satisfying theconstraint, and therefore any solution. At each node in the search, constraint propagation algorithms are used toestablish a local consistency property. A common example is generalised arc consistency (see [19]).Definition 1 (Generalised arc consistency). A constraint c is generalised arc consistent (or GAC), written GAC(c), ifand only if for every X ∈ vars(c) and every v ∈ D(X)",
            {
                "entities": [
                    [
                        72,
                        133,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 97 (1997) 83-136 Artificial Intelligence Speeding up inferences using relevance reasoning: a formalism and algorithms Alon Y. Levy a,*, Richard E. Fikes b~l, Yehoshua Sagiv cV2 B AT&T Bell Laboratories, 180 Park Ave., Room A283, Florham Park, NJ 07932, USA b KSL, Stanford University, Stanford, CA 94305, USA c Department of Computer Science, Hebrew University, Jerusalem, Israel Received September 1995; revised May 1996 Abstract reasoning Irrelevance to a specific query. Aside from its importance refers to the process in which a system reasons about which parts of in its knowledge are relevant (or irrelevant) speeding up inferences from large knowledge bases, relevance reasoning is crucial in advanced applications such as modeling complex physical devices and information gathering in distributed heterogeneous systems. This article presents a novel framework for studying the various kinds of irrelevance that arise in inference and efficient algorithms for relevance reasoning. We present a proof-theoretic framework for analyzing definitions of irrelevance. The framework makes the necessary distinctions between different notions of irrelevance that are important when using them for speeding up inferences. We describe the query-tree algorithm which is a sound, complete and efficient algorithm for automatically deriving certain kinds of irrelevance claims for Horn-rule knowledge bases and several extensions. Finally, we describe experimental results that show that significant speedups (often orders of magnitude) are obtained by employing the query-tree in inference. @ 1997 Elsevier Science B.V. Keywords: Relevance representation reasoning; Meta-level reasoning; Static analysis; Horn rules; Constraints; Knowledge author. Email: * Corresponding ’ Email: fikes@cs.stanford.edu. 2 Email: sagiv@cs.huji.ac.il. levy@research.att.com. 0004-3702/97/$17.00 PII SOOO4-3702( @ 1997 Elsevier Science B.V. All rights reserved. 97)00049-O \fbase formance of inference knowledge irrelevant - Irrelevant engine query. Consequently, information: 84 A.L Levy et al./ArtiJicial Intelligence 97 (1997) 83-136 1. Introduction Many the inability future applications large amounts of knowledge. it is essential or task. In fact, is a major obstacle process (or base or by exploiting is a specific knowledge about in the knowledge Irrelevance the domain. irrelevant) in which the system explicit form of meta-level of Artificial In order for a system to perform efficiently Intelligence (AI) will be in domains having in such a domain to any given query is relevant that it be able to determine which knowledge of current AI systems to ignore irrelevant information in scaling up such systems. Irrelevance reasoning reasons about which parts of its knowledge to a specific query, either by automatically inspecting refers to the are relevant the knowledge reasoning irrelevance-claims given by a user. Irrelevance reasoning base, as opposed reasoning [ 37,45,54], to using in which we reason about the the knowledge base to reason is important in several contexts: a Speeding up inferences in large knowledge bases: It is well known that the per- in AI systems degrades quickly as the size of the to are due increases. Two of the major sources of inefficiency engines facts in the knowledge base: In its search for a solution, in the knowledge considers many facts base the inference it spends significant - Irrelevant distinctions in the representation: A knowledge a variety of tasks. Therefore, accommodate must be detailed enough relations, objects, etc.). As a result, the representation for any given for all those task, thereby to inefficient tasks (i.e., leading reasoning. that are irrelevant to the effort pursuing useless solution paths. to is designed of the domain refined is likely to be too complex its conceptualization it must include many base l Modeling complex physical devices: Tasks such as diagnosis, design and simulation require a model of a given physical device a model depends heavily on the task for which with the most detailed model of the system would be intractable. Therefore, important (e.g.,[ 2,32,46] system are relevant the adequacy of directly it is create a model that is suited for a given query that we determine which aspects of the ), and doing so requires to a given query. to be able to automatically it is used, and reasoning [ 11,221. However, l Large scale distributed information systems: Current communications to many enables accessing the Internet) easy access this information (e.g., At the moment, body of work in AI has the goal of designing sources of information high thereby 3 1,33,38,5 1,60,65,90]. is the ability a given query posed by a user. to automatically and providing determine which A key issue that needs technology remote sources of information. can be mostly by browsing. A growing for integrating multiple them, [4, to be addressed by these systems to architectures level querying sources are relevant facilities over information sources freeing a user from the need to know about specific information Irrelevance reasoning also plays an important 711, belief revision using applications relevance reasoning of relevance [ 351 and learning (e.g., to speed up inferences role in nonmonotonic [ 7,36, [ 30,66,74] > . The focus of this paper is on in large knowledge bases. The other reasoning reasoning are discussed briefly in Section 6. \fA.Y Levy et al. /Art$cial Intelligence 97 (1997) 83-136 8.5 In order for relevance reasoning to be a viable method for controlling inference, issues need that are irrelevant detecting parts of a knowledge base to be addressed. First, we must develop eficient for to a query. The to these algorithms may vary; it could consist of certain parts (e.g., the rules) of on facts in the knowledge base) and, possibly, some irrelevance claims supplied irrelevant knowledge reasoning two questions, we need a formal several automatically input the knowledge base, some meta-claims the ground by the user. Second, we must and the tradeoff between meta-level about understanding about relevance and base-level these of the possible meanings of irrelevance reasoning for addressing the domain. As a basis about other parts (e.g., the utility of removing integrity constraints investigate algorithms claims. in research in many contexts The notion of irrelevance has appeared in AI and related fields. However, most of the time researchers use the term informally. Formal analyses of [ 47 J >, 1950 irrelevance have been discussed by philosophers [ 161) and 1978 (Gardenfors (Catnap thrust of these analyses was notions of irrelevance by a formal definition. Most to try to capture our common-sense of the work focuses on formulating and finding properties of the notion of irrelevance the work has not been concerned that satisfy definitions with how to use irrelevance detecting for speeding up inference or how to design algorithms those properties. Consequently, as early as 1921 (Keynes [ 341). The main irrelevance. for Within AI, the notion of irrelevance was investigated [53], [21,23,70] and used there to control reasoning In the context of logical knowledge bases, Subramanian several meyer of automatically irrelevance left largely open, and consequently effective way. investigated deriving inference claims and the utility of irrelevance relevance reasoning formal definitions of irrelevance. However, in the context of probabilistic in Bayesian belief networks. [ 881, and more recently Lake- the issues reasoning were in any has not been applied This article presents a framework studied. We present efficient algorithms and we describe the results of experiments In particular, we make the following contributions. in which various definitions of irrelevance irrelevance for automatically that validate the utility of relevance deriving can be claims, reasoning. deriving framework distinctions for analyzing the necessary l We present the definitions of irrelevance, a proof-theoretic between irrelevance claims vary as we move in the space. The framework encompasses definitions for the notion. We describe how properties of yielding a space of possible definitions irrelevance and sheds new light on previous definitions of irrelevance. We show that the framework makes the problem of automatically irrelevant facts. l We consider the problem of automatically knowledge bases and several extensions, tree, for that purpose. We identify and show that the query-tree provides a sound and complete irrelevance for such claims. Strong strongly often especially useful irrelevance claims for Horn-rule and present a novel tool, called the query- claims inference procedure that removing (and it in practice. First, irrelevance claims are derived by inspecting only facts is guaranteed not to slow down an inference engine the important class of strong-irrelevance claims and the utility of removing The query-tree has two properties claims also have the property it up significantly). to address that make irrelevant to speed deriving needed \f86 A. E Levy et al. /Artijicial Intelligence 97 (1997) 83-136 irrelevance the semantics of interpreted predicates appearing in deriving facts). Second, a small part of the knowledge base (e.g., inspecting and not the ground considers (e.g., order predicates, key role in many applications, query-tree programs it provides predicates. l We describe but is distinguished sort predicates, completeness experimental that show this is an important the rules in the knowledge base claims, the query-tree in the knowledge base interpreted predicates play a feature of the query-tree. The logic from previous work in that area in that can also be viewed as a tool for partial evaluation of constraint [ 15,67,85], in the presence of recursive rules and interpreted etc.). Since speedups also results that significant (often or- in inference. The it is used to determine which facts are ir- to a query. Based on that determination,",
            {
                "entities": [
                    [
                        65,
                        141,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 246 (2017) 118–151Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintLocalising iceberg inconsistenciesGlauber De Bona∗, Anthony HunterDepartment of Computer Science, University College London, WC1E 6BT, UKa r t i c l e i n f oa b s t r a c tArticle history:Received 3 August 2016Received in revised form 16 February 2017Accepted 19 February 2017Available online 28 February 2017Keywords:Propositional logicInconsistency managementInconsistency analysisInconsistency localisationIn artificial intelligence, it is important to handle and analyse inconsistency in knowledge bases. Inconsistent pieces of information suggest questions like “where is the inconsis-tency?” and “how severe is it?”. Inconsistency measures have been proposed to tackle the latter issue, but the former seems underdeveloped and is the focus of this paper. Minimal inconsistent sets have been the main tool to localise inconsistency, but we argue that they are like the exposed part of an iceberg, failing to capture contradictions hidden under the water. Using classical propositional logic, we develop methods to characterise when a formula is contributing to the inconsistency in a knowledge base and when a set of formulas can be regarded as a primitive conflict. To achieve this, we employ an abstract consequence operation to “look beneath the water level”, generalising the minimal inconsistent set concept and the related free formula notion. We apply the framework presented to the problem of measuring inconsistency in knowledge bases, putting forward relaxed forms for two debatable postulates for inconsistency measures. Finally, we discuss the computational complexity issues related to the introduced concepts.© 2017 Elsevier B.V. All rights reserved.1. IntroductionThe occurrence of inconsistencies in data and knowledge is an important issue for the application of knowledge repre-sentation and reasoning technologies that are based on standard logics. To develop ways of dealing with an inconsistent set of formulas, it is important to understand the inconsistency, analysing its properties. Given an inconsistent knowledge base (a set of formulas), natural questions that arise are “where is the inconsistency?” and “how severe is it?”. To answer the second question in a qualitative way, inconsistent knowledge bases were classified by the severity of their inconsistency [17]. Recently, to numerically quantify the extent to which a knowledge base is inconsistent, many inconsistency measures have been proposed [29,24,25,19,28,27,20,42,43]. In contrast, the first question appears quite underdeveloped, and it is the subject of the present work.Inconsistency localisation can mean different things. One may want for instance to spot which part of the language is “contaminated” by the inconsistency, looking for the logical variables involved in contradictions (see e.g. [22,25]). Alterna-tively, one might assign numeric inconsistency values for formulas in a knowledge base, indicating the extent to which they are involved in the inconsistency, according to a given definition (e.g. [23,25]). In this paper, we focus on localising the inconsistency in a knowledge base, showing how it unfolds among the formulas.1 That is, given an inconsistent knowledge * Corresponding author.E-mail addresses: glauberbona@gmail.com (G. De Bona), anthony.hunter@ucl.ac.uk (A. Hunter).1 Note that logically closed theories are equal to the whole logical language when inconsistent, hence we focus on (possibly non-closed) knowledge bases.http://dx.doi.org/10.1016/j.artint.2017.02.0050004-3702/© 2017 Elsevier B.V. All rights reserved.\fG. De Bona, A. Hunter / Artificial Intelligence 246 (2017) 118–151119base, we are interested in discovering which subsets of formulas are contributing to the inconsistency, being its causes, and which formulas are not involved whatsoever.Fig. 1. Inconsistency as icebergs.1.1. MotivationWhen a knowledge base is inconsistent, it is not necessarily the case that its inconsistency is spread over all its formulas. For example, consider the set formed by the propositions: “Alice is a cat”, “Alice is not a cat” and “Bob is a dog”. Even though the whole set is inconsistent, intuition tends towards regarding the first two propositions as controversial and the third one as free of inconsistency somehow. To capture such intuition, minimal inconsistent sets (inconsistent sets whose all proper subsets are consistent) have been construed as the “purest form of inconsistency” [24,25]. Accordingly, a formula not contained in any minimal inconsistent set — a free formula — has been regarded as “uncontroversial”. As the first two propositions are already contradicting each other, the whole base is not a minimal inconsistent set. Furthermore, the third proposition contradicts neither the first nor the second proposition, hence “Bob is a dog” is indeed technically free, for not being in a minimal inconsistent set. Such a simple solution to the problem of localising the inconsistency probably is the reason for the lack of a systematic investigation of this issue. Nonetheless, the situation is more complex than might at first appear, since minimal inconsistent sets are alike the exposed part of the iceberg, ignoring all the inconsistency hidden under the water, as illustrated in Fig. 1.The recognition of these iceberg inconsistencies can find application in different areas where inconsistent pieces of information have to be dealt with. For instance, in software engineering, requirements extraction might reveal users’ expec-tations that cannot hold together, calling for a method for localising the conflicts. In data integration/fusion, as well as in belief merging, the proper identification of the sources of information, or the agents, that are conflicting each other allows one to narrow its attention to the focus of the problem, ignoring uncontroversial data/beliefs. In formal argumentation, in-consistency can be localised in order to show how a set of arguments is conflicting. Inconsistency localisation may also bring important clues in fraud investigation, for instance in the analysis of contradicting tax forms of a given taxpayer. In general, any decision making under inconsistent information might benefit from localising the inconsistency. For example, a physi-cian facing several different medical tests of a given patient with inconsistent results might need to choose which ones should be performed again. Example 1.1 brings a concrete situation where a decision can be influenced by inconsistency localisation.Example 1.1. The police is investigating a robbery on a jewellery shop that occurred on a weekday, during working hours. The investigators have taken testimony from all employees that were working on the day of the crime. The witnesses’ statements include the following:• salesperson: “I did not open the safe, and the criminals carried no guns!”• security chief: “Only the manager or the salesperson could have opened the safe, and the criminals carried guns.”• manager: “I did not open the safe.”As the police conceives the possibility of some of the employees having been complicit, they look for contradictions among the versions given. Inconsistent testimonies would imply some witnesses are lying, raising suspicions of complicity against them. The security chief and the salesperson are clearly contradicting each other, but is the manager involved in some contradiction? From the statements above, can one infer that it is possible that the manager is lying?To answer the questions raised in the example above, we need a tool to tell the “uncontroversial” from the “controversial” formulas in a knowledge base, since we are only interested in knowing whether the manager’s testimony is involved in the inconsistency, raising suspicion that he/she lied. This can be regarded as the relaxed form of the problem of localising inconsistency, whose solution is a partition of the inconsistent knowledge base into “controversial” and “uncontroversial” formulas. Free formulas are intended to encompass all and only “uncontroversial” formulas in a knowledge base, but we \f120G. De Bona, A. Hunter / Artificial Intelligence 246 (2017) 118–151shall argue that they are not suitable for all contexts. For instance, in the example above, the manager’s testimony is free (because it is not in any minimal inconsistent set), but it also seems to contradict the others in some way.A harder problem is identifying the atomic inconsistencies, or the primitive conflicts, in a knowledge base and can be illustrated by the following situation:Example 1.2. A university has hired a company to design a library management software to be used by all its members. In order to extract the design specifications, the company has collected requirements from the head of each department, which include:• Ecology: “The software should be open source, contributing to the whole academic community.”• Marketing: “It can’t be freely available, we need to keep our university edge in IT systems as a differential that attracts new students.”• Philosophy: “Both graduate and undergraduate students shall have the same rights in the system and it must be re-motely accessible.”• Economy: “Due to their different demands, graduate students need some privileges. If the system is to be remotely accessible, its software should not be open source, otherwise it could be vulnerable.”• Theology: “Department heads shall have no exclusive privileges.”• Arts: “I have no specific requirements.”The project manager, while reading such requirements, notes two contradictions: one between the heads of the Ecology and Marketing departments, on whether the software should be open source, and another between the heads of Philosophy and Economy departments, about the graduate and undergraduate students rights. The manager plans to arrange meetings with the department heads to discuss — and maybe relax — ",
            {
                "entities": [
                    [
                        136,
                        170,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 168 (2005) 119–161www.elsevier.com/locate/artintWeak nonmonotonic probabilistic logics ✩Thomas Lukasiewicz 1Dipartimento di Informatica e Sistemistica, Università di Roma “La Sapienza”,Via Salaria 113, 00198 Rome, ItalyReceived 1 October 2004; accepted 31 May 2005Available online 5 July 2005AbstractWe present an approach where probabilistic logic is combined with default reasoning from condi-tional knowledge bases in Kraus et al.’s System P , Pearl’s System Z, and Lehmann’s lexicographicentailment. The resulting probabilistic generalizations of default reasoning from conditional knowl-edge bases allow for handling in a uniform framework strict logical knowledge, default logicalknowledge, as well as purely probabilistic knowledge. Interestingly, probabilistic entailment in Sys-tem P coincides with probabilistic entailment under g-coherence from imprecise probability assess-ments. We then analyze the semantic and nonmonotonic properties of the new formalisms. It turnsout that they all are proper generalizations of their classical counterparts and have similar propertiesas them. In particular, they all satisfy the rationality postulates of System P and some Conditioningproperty. Moreover, probabilistic entailment in System Z and probabilistic lexicographic entailmentboth satisfy the property of Rational Monotonicity and some Irrelevance property, while probabilis-tic entailment in System P does not. We also analyze the relationships between the new formalisms.Here, probabilistic entailment in System P is weaker than probabilistic entailment in System Z,which in turn is weaker than probabilistic lexicographic entailment. Moreover, they all are weakerthan entailment in probabilistic logic where default sentences are interpreted as strict sentences.Under natural conditions, probabilistic entailment in System Z and lexicographic entailment evencoincide with such entailment in probabilistic logic, while probabilistic entailment in System P does✩ This paper is a significantly extended and revised version of a paper in: Proceedings of the 9th InternationalConference on Principles of Knowledge Representation and Reasoning (KR2004), Whistler, Canada, June 2004,AAAI Press, 2004, pp. 141–151.E-mail address: lukasiewicz@dis.uniroma1.it (T. Lukasiewicz).1 Alternate address: Institut für Informationssysteme, Technische Universität Wien, Favoritenstraße 9-11, 1040Vienna, Austria; e-mail: lukasiewicz@kr.tuwien.ac.at.0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.05.005\f120T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161not. Finally, we also present algorithms for reasoning under probabilistic entailment in System Z andprobabilistic lexicographic entailment, and we give a precise picture of its complexity. 2005 Elsevier B.V. All rights reserved.Keywords: Probabilistic logic; Default reasoning from conditional knowledge bases; Entailment in System P ;Entailment in System Z; Lexicographic entailment; Nonmonotonic probabilistic logics; Inconsistencyhandling; Algorithms; Computational complexity1. IntroductionDuring the recent decades, reasoning about probabilities has started to play an importantrole in AI. In particular, reasoning about interval restrictions for conditional probabilities,also called conditional constraints [49], has been a subject of extensive research efforts.Roughly, a conditional constraint is of the form (ψ|φ)[l, u], where ψ and φ are events, and[l, u] is a subinterval of the unit interval [0, 1]. It encodes that the conditional probabilityof ψ given φ lies in [l, u].An important approach for handling conditional constraints is probabilistic logic, whichhas its origin in philosophy and logic, and whose roots can be traced back to alreadyBoole in 1854 [12]. There is a wide spectrum of formal languages that have been exploredin probabilistic logic, ranging from constraints for unconditional and conditional eventsto rich languages that specify linear inequalities over events (see especially the work byNilsson [54,55], Fagin et al. [19], Dubois and Prade et al. [2,13,16,17], Frisch and Had-dawy [21], and the author [48,49,51]; see also the survey on sentential probability logic byHailperin [35]). The main decision and optimization problems in probabilistic logic are de-ciding satisfiability, deciding logical consequence, and computing tight logically entailedintervals. Recently, column generation techniques from operations research have been suc-cessfully used to solve large problem instances in probabilistic logic (see especially thework by Jaumard et al. [37] and Hansen et al. [36]).Example 1.1 (Eagles). A simple collection of conditional constraints KB may encode thestrict logical knowledge “all eagles are birds” and “all birds have feathers” as well asthe purely probabilistic knowledge “birds fly with a probability of at least 0.95” (cf. Ex-ample 2.1). This collection of conditional constraints KB is satisfiable, and some logicalconsequences in probabilistic logic from KB are “all birds have feathers”, “birds fly with aprobability of at least 0.95”, “all eagles have feathers”, and “eagles fly with a probabilitybetween 0 and 1”; in fact, these are the tightest intervals that follow from KB (cf. Exam-ple 2.2). That is, we especially cannot conclude anything from KB about the ability to flyof eagles.A closely related research area is default reasoning from conditional knowledge bases,which consist of a collection of strict statements in classical logic and a collection of defea-sible rules, also called defaults. The former must always hold, while the latter are rules ofthe kind ψ ← φ, which read as “generally, if φ then ψ”. Such rules may have exceptions,which can be handled in different ways.\fT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161121The literature contains several different proposals for default reasoning from conditionalknowledge bases and extensive work on its desired properties. The core of these proper-ties are the rationality postulates of System P by Kraus, Lehmann, and Magidor [40],which constitute a sound and complete axiom system for several classical model-theoreticentailment relations under uncertainty measures on worlds. They characterize classicalmodel-theoretic entailment under preferential structures [40,64], infinitesimal probabili-ties [1,57], possibility measures [14], and world rankings [33,65]. As shown by Friedmanand Halpern [20], many of these uncertainty measures on worlds are expressible as plausi-bility measures. The postulates of System P also characterize an entailment relation basedon conditional objects [15]. A survey of the above relationships is given in [6,22].Mainly to solve problems with irrelevant information, the notion of rational closure asa more adventurous notion of entailment was introduced by Lehmann [45,47]. It is equiva-lent to entailment in System Z by Pearl [58], to the least specific possibility entailment byBenferhat et al. [5], and to a conditional (modal) logic-based entailment by Lamarre [44].Finally, mainly to solve problems with property inheritance from classes to exceptionalsubclasses, the maximum entropy approach to default entailment was proposed by Gold-szmidt et al. [31]; lexicographic entailment was introduced by Lehmann [46] and Benferhatet al. [4]; conditional entailment was proposed by Geffner [24,26]; and an infinitesimal be-lief function approach was suggested by Benferhat et al. [7]. The following example due toGoldszmidt and Pearl [34] illustrates default reasoning from conditional knowledge bases.Example 1.2 (Penguins). A conditional knowledge base KB may encode the strict logicalknowledge “all penguins are birds” and the default logical knowledge “generally, birds fly”,“generally, penguins do not fly”, and “generally, birds have wings”. Some desirable con-clusions from KB [34] are “generally, birds fly” and “generally, birds have wings” (whichboth belong to KB), “generally, penguins have wings” (since the set of all penguins is asubclass of the set of all birds, and thus penguins should inherit all properties of birds),“generally, penguins do not fly” (since properties of more specific classes should overrideinherited properties of less specific classes), and “generally, red birds fly” (since “red” isnot mentioned at all in KB and thus should be considered irrelevant to the ability to fly ofbirds).There are several works in the literature on probabilistic foundations for default reason-ing from conditional knowledge bases [1,11,31,57], on combinations of Reiter’s defaultlogic [63] with statistical inference [43,67], and on a rich first-order formalism for deriv-ing degrees of belief from statistical knowledge including default statements [3]. However,there has been no work so far that extends probabilistic logic by the capability of handlingdefaults as in conditional knowledge bases.In this paper, we try to fill this gap. We present extensions of probabilistic logic bydefaults as in conditional knowledge bases under Kraus et al.’s System P [40], Pearl’sSystem Z [58], and Lehmann’s lexicographic entailment [46]. The new formalisms allowfor expressing in a uniform framework strict logical knowledge and purely probabilisticknowledge from probabilistic logic, as well as default logical knowledge from default rea-soning from conditional knowledge bases. Informally, strict logical knowledge representssentences that must always hold, while purely probabilistic (resp., default logical) knowl-\f122T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161edge encodes sentences that may have exceptions, which is expressed in a quantitative(resp., qualitative) way.Example 1.3 (Ostriches). Consider the strict logical knowledge “all ostriches are birds”,the default logical knowledge “generally, birds have legs” and “generally, birds fly”, andthe purely probabilistic knowledge “ostriches fly with a probability of at most 0.05”. Ob-viously, some desired",
            {
                "entities": [
                    [
                        72,
                        110,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 1154–1193Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintFrom the textual description of an accident to its causesDaniel Kayser∗, Farid Nouioua 1Laboratoire d’Informatique de Paris-Nord, UMR 7030 du C.N.R.S. – Institut Galilée, Université Paris 13, 99 avenue Jean-Baptiste Clément, F 93430 – Villetaneuse, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 29 September 2008Received in revised form 7 April 2009Accepted 23 April 2009Available online 6 May 2009Keywords:Natural language understandingCausal reasoningNormsInference-based semanticsSemi-normal defaultsEvery human being, reading a short report concerning a road accident, gets an idea ofits causes. The work reported here attempts to enable a computer to do the same, i.e. todetermine the causes of an event from a textual description of it. It relies heavily on thenotion of norm for two reasons:• The notion of cause has often been debated but remains poorly understood: wepostulate that what people tend to take as the cause of an abnormal event, like anaccident, is the fact that a specific norm has been violated.• Natural Language Processing has given a prominent place to deduction, and for whatconcerns Semantics, to truth-based inference. However, norm-based inference is amuch more powerful technique to get the conclusions that human readers derive froma text.The paper describes a complete chain of treatments, from the text to the determinationof the cause. The focus is set on what is called “linguistic” and “semantico-pragmatic”reasoning. The former extracts so-called “semantic literals” from the result of the parse,and the latter reduces the description of the accident to a small number of “kernel literals”which are sufficient to determine its cause. Both of them use a non-monotonic reasoningsystem, viz. LPARSE and SMODELS.Several issues concerning the representation of modalities and time are discussed andillustrated by examples taken from a corpus of reports obtained from an insurancecompany.© 2009 Elsevier B.V. All rights reserved.1. Motivation1.1. Basic postulatesThe work described here is grounded on two postulates:• what is perceived as the cause of an event is:– the norm itself, if the event is perceived as normal,– and the violation of some norm, if the event is considered abnormal;• the semantics of natural language (NL) is not based on the notion of truth, but on norms.* Corresponding author.E-mail addresses: Daniel.Kayser@lipn.univ-paris13.fr (D. Kayser), Farid.Nouioua@lipn.univ-paris13.fr, Farid.nouioua@univ-cezanne.fr (F. Nouioua).1 Now at Laboratoire des Sciences de l’Information et des Systèmes, UMR 6168 du C.N.R.S. Université Paul Cézanne (Aix-Marseille 3), Avenue EscadrilleNormandie–Niemen 13397 Marseille Cedex 20, France.0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.04.002\fD. Kayser, F. Nouioua / Artificial Intelligence 173 (2009) 1154–11931155The notion of norm, which plays a central role in this paper, has both a descriptive and a prescriptive meaning.2 In thedescriptive sense, norms are just what explains the difference between what is perceived as normal or not. In the prescriptivesense, norms build a corpus on the basis of which an agent is considered, legally or otherwise, entitled or not to performan action.In Artificial Intelligence, these two meanings have given rise to two rather separate fields of study. On the one hand,non-monotonic logics have been developed in order to derive conclusions that are considered normal, in the absence ofany specific circumstance invalidating this derivation. On the other hand, deontic logics have the purpose of formalizing thereasoning of agents respecting normative prescriptions [8,17,48].In this paper, norms will generally be taken in their descriptive sense but clearly, as it is normal to follow the rules, thisacceptation of norms includes as a special case the prescriptive sense, and we will also have to deal with duties, which arenormative.Our first postulate concerns a very old and very controversial issue: when is it sensible to say that A causes B? Deter-mining the essence of the notion of cause is not in the agenda of Artificial Intelligence. However, commonsense reasoningmakes an intensive use of causation, e.g. for diagnosing, planning, predicting; and therefore AI cannot (and does not, seee.g. [33,55]) completely ignore the debate concerning this notion. What AI needs, however, does not concern the meta-physics of cause, but only how people reason causally. And, even if observation reveals that we use the word cause to meanrather different things, i.e. that this word is polysemic, in a vast majority of cases we take as causal for an abnormal eventthe fact that some agent has violated a norm. We report in the paper a psychological experiment showing which violation(s)are selectively chosen as cause(s) of the event.Consider now our second postulate: dealing with a sentence such as:The car before me braked suddenly.A truth-based approach (see e.g. [13,27,35]) will derive all the conclusions C that logically follow from the existence of atime t and a car c, such that the two following propositions are true at t: (i) c brakes suddenly and (ii) c is located in frontof the writer. Clearly, several other propositions, none of them being valid in the logical sense, come to the mind of a humanreader, e.g. (iii) at t, the writer was driving, (iv) s/he was driving in the same direction as c, (v) no vehicle was betweenc and him/her, (vi) s/he had to act quickly, in order to prevent an accident, and so on. Subsequent information may forcethe reader to retract some of these conclusions. Nonetheless, as they are likely to be present in the mind of every personwho shares our culture, there is no necessity to consider separately the propositions C derived by means of truth-preservingoperations (the only ones that are said to pertain to semantics, according to the prevailing theories), from the propositionsderived by means of norms (generally said to be the result of pragmatics).Knowing the norms of a domain is absolutely necessary to understand the texts of that domain. But there exists noexhaustive list of the norms ruling any given domain: the rules and regulations are only the visible part of the iceberg ofall we know, and keep implicit, about the domain. An indirect consequence of our study is to point out that examining howpeople ascribe causation to the events happening in a domain is a powerful means to reveal its implicit norms.1.2. Specification of the goalIn order to validate our postulates, we need to focus on a domain where the number of norms remains reasonable, whereabnormal events are frequent, where these events are reported in natural language, and where it is easy to ask people whatthey take as being the cause of the events reported.We selected the domain of road accidents, for the following reasons:• A large number of short texts exists, describing such accidents: every insurance company receives daily a number offorms filled by drivers, describing what happened to them.• Most people of our culture know enough about car crashes to give sensible answers, after having read a text, whenthey are asked what caused the accident.• Each report describes some facts, but clearly implies also a number of other facts, which are not logically entailed. Wecan therefore see whether our postulates work, i.e. check whether a reasoning based on norms captures the kind ofreasoning used by the reader.• An accident by itself is an anomaly, and the text generally goes back to another anomaly that allegedly explains why ithappened. We can thus test which anomaly, if any, is taken to be the cause of the accident, and confirm or infirm ourfirst postulate.• The number of norms involved is neither too large nor too small. They are clearly not limited to those listed in theHighway Code.• The corpus on which we perform our study has been studied from different points of views. (See for example [60,19,34].The last work presents a system that produces automatically a 3D scene of an accident from its textual description.3)2 Von Wright [62, Chap. 1] discusses in much greater details the various meanings of the word norm.3 Another study [63] concerns British records of road accidents: the authors use a description logic in order to check the consistency between a naturallanguage report and coded data which are both components of the record.\f1156D. Kayser, F. Nouioua / Artificial Intelligence 173 (2009) 1154–1193However, a drawback of this corpus is worth a mention. Most of the reports sent by a driver to an insurance company are,for obvious reasons, biased in order to lessen the author’s responsibility; the rhetorical aspect of a plea interferes with themore basic descriptive part of the report, be it truthful or not. The fact that the texts do not necessarily reflect what reallyhappened is not per se a problem: the norms of a domain are not better revealed from “true” descriptions, whatever thismay mean, than from biased ones. The unwanted consequence of the choice of this corpus is that it requires some effortto identify (and most of the time, to ignore) what has been added for pure argumentative reasons. A study is currentlyin progress that examines specifically the argumentative strategies used by the authors to highlight or to minimize someaspects of the accident, in order to convey the best possible impression of their behavior [9]. As we will see, this dimensionof the reports does not affect much the results of the present work.We obtained, by courtesy of MAIF, an insurance company, a number of reports written in French. Some of them areunclear, and only the accompanying drawings make them understandable. We discarded them and kept only those whichare self-contained, i.e. those on the basis of which we understood enough of what happened to build a hypothesis that",
            {
                "entities": [
                    [
                        138,
                        195,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 90 (1997) 177-223 Artificial Intelligence Semantics and complexity of abduction from default theories * Thomas Eiter a**, Georg Gottlob a~1, Nicola Leone avb*2 a Christian Doppler Laboratory for Expert Systems, Information Systems Department, TU Vienna, Paniglgasse 16, A-1040 Wien, Austria b Istituto per la Sistemistica e l’lnformatica - C.N.R., c/o DEIS - UNICAL, 87036 Rende, Italy Received December 1995; revised June 1996 Abstract Abductive reasoning (roughly speaking, find an explanation for observations out of hypothe- ses) has been recognized as an important principle of common-sense reasoning. Since logical is commonly based on nonclassical formalisms like default logic, au- knowledge representation toepistemic logic, or circumscription, it is necessary to perform abductive reasoning from theories logics. In this paper, we investigate how abduction can (i.e., knowledge bases) of nonclassical be performed from theories in default logic. In particular, we present a basic model of abduc- tion from default theories. Different modes of abduction are plausible, based on credulous and skeptical default reasoning; they appear useful for different applications such as diagnosis and planning. Moreover, we thoroughly analyze the complexity of the main abductive reasoning tasks, namely finding an explanation, deciding relevance of a hypothesis, and deciding necessity of a hypothesis. These problems are intractable even in the propositional case, and we locate them into the appropriate slots of the polynomial hierarchy. However, we also present known classes of default theories for which abduction is tractable. Moreover, we also consider first-order default theories, based on domain closure and the unique names assumption. In this setting, the abduction tasks are decidable, but have exponentially higher complexity than in the propositional case. @ 1997 Elsevier Science B.V. Keywords: Abduction; Default logic; Algorithms and complexity; Tractability *A short and preliminary version of this paper appeared International Joint Conference on Artificial Intelligence (IJCAI-95) in: C. Mellish, ed., Proceedings of the Fourteenth ( AAAI press, 1995) 870-876. author. E-mail: eiter@dbai.tuwien.ac.at. * Corresponding ’ E-mail: gottlob@dbai.tuwien.ac.at. leone@dbai.tuwien.ac.at, * E-mail: nik@si.deis.unical.it. 0004-3702/97/$17.00 PIISOOO4-3702(96)00040-9 @ 1997 Elsevier Science B.V. All rights reserved. \f178 7: Eiter et al. /Artificial Intelligence 90 (1997) 177-223 1. Introduction Abductive reasoning has been recognized as an important principle of common-sense speech recognition formalizations having applications [ 121. Various are well known. These fruitful [ 16,45,46], is represented. Roughly, by a function reasoning based diagnosis [ 341, and vision posed, among which set-covering-based [ 15,16,45,46] knowledge edge is represented are atomic entities i.e. observed tions, if X explains of X; tive explanation. On the other hand, edge potheses tions M. For a more detailed comparison see [22]. is an abductive is represented by a logical explanation, representing symptoms. The set e(X) all manifestations M, reasoning of abductive [ 301, maintenance [ 7,431 and logic-based approaches two types basically differ approach, in a number of areas such diverse as model- of database views have been pro- approaches in the way domain the domain knowl- subsets X of hypotheses, which to subsets e(X) of manifesta- power then X is an abduc- the domain knowl- language. A subset X of hy- can be seen as the explanation i.e., e(X) = M, approach in the set-covering e which maps all possible disorders, of these and other approaches by X derives the manifesta- to abduction, in the logic-based theory T in some if T augmented Until now, mainly logical abduction from theories of classical logic has been stud- ied. However, formalisms situations edge bases) as hybrid malisms [ 21. like default it is necessary of nonclassical reasoning, i.e., logical knowledge representation is commonly logic, autoepistemic logic, or circumscription. to perform logics; reasoning reasoning abductive in a sense, this on a knowledge from theories is orthogonal to what base built using different based on nonclassical Thus, in such (i.e., knowl- is known for- Since default logic [52] languages important fault motivating lead us towards a formal model of abduction that emerged to investigate how abduction In this paper, we address examples; they show that abductive logic. in the field of nonmonotonic is one of the most used logical knowledge (cf. theories [ 18,53]), (W, D) this problem. We start by first considering reasoning from can be performed representation it is in de- some logic is needed, and reasoning from default in default theories. Example 1.1. Consider edge about Bill’s skiing habits: the following set of default rules, which represent some knowl- D= ‘t : vkiing(Bil1) yskiing( Bill) weekend : -snowing skiing( Bill) ’ : -mowing ’ lsnowing ’ 1 intuitively state the following: Bill is usually out for skiing, unless The defaults weekend, Bill snowing. For the certain knowledge W = {weekend} Sunday), skiing( Bill). the default is usually not out for skiing; on the and usually, it is snowing; it is not that it is Saturday or (encoding -snowing and theory T = (W, D) has one extension which contains Suppose now that we observe (which is not consistent with the extension). Abduction means to identify a set of facts, chosen from a set of hypotheses, whose presence for this observation, that is, in the theory that Bill is not out for skiing to find an explanation \fT. Eiter et al. /Artificial Intelligence 90 (1997) 177-223 179 f C d i-i/ Fig. I. A computer network. e the observation at hand would derive the extension. We find such an explanation if we add snowing a single extension, which contains the observation lskiing(Bill), to W, we obtain for the default lskiing(BilZ), by adopting i.e., cause the hypothesis that +Gng(BiZl) snowing. is in Indeed, theory T’ = ({weekend, snowing}, D) is abduced from or that it is an abductive explanation of lskiing(Bill). lskiing( Bill). We say that snowing Observe default properties that the description of the above situation that cannot be represented properly requires in classical logic. the specification of some logic. The default about Example 1.2. Assume default setting knowledge connections established connections: between by a path following that information about a computer network theory T = (W, D) described next comprises relationships the status of a site (working between sites, and reachability of one site from another direct connections), together with is represented using in a simplified or not), (which can be about information / Vx. works(x) > path( x, x) , Vx, y. 7works( x) 3 7reaches( y, x) , conn(a,b>,conn(a,f), w= < conn(b,c),conn(b,d), conn(c,e>,conn(d,e>,conn(e,f), ‘dx. conn( x, x) , \\ Vx, y. conn( x, y) 3 conn(y, x) The first two formulas in W state that if a site works, if a site does not work, then then this site to itself, and from that from any node. The facts conn( s1, ~2) state direct connections venience, the respective Fig. 1.’ and symmetry of the connectivity axioms.3 A graphical representation reflexivity relationship of these connections between there is a trivial path it cannot be reached sites. For con- is taken care of by in is depicted ’ Negated facts omitted. If desired, : lconn(x,y)/~conn(x,y) they can be derived using in D. lconn( ~1, sz), stating that there is no direct connection the closed world assumption, between by sites $1 and ~2, are defaults introducing \f: worh( x) works(x) 180 T Eiter et al. /Artificial Intelligence 90 (1997) 177-223 The default rules are as follows. puth(x, y) : : -yuth(x, y) ’ reuche.s(x,y) ’ v-euches(x,y) ’ D= puth(x,y) Aconn(y,z) Aworks : . paw% z> The first rule states by adopting that a site works by default; the next two rules relate paths to that x reaches y if a path from x to y provably exists, and does it otherwise. The last rules states that a path can be extended by following for if the site at the other end is working; here, it is assumed reachability, not reach a direct connection, simplicity that the connections are reliable, i.e., no communication failures occur. In this representation, we implicitly adopt . V x = c,, where cl , . . . , c, are all the objects x = Cl v.. the underlying axiom of the unique names assumption /jiCj ci # cj, which expresses mentioned domain, which are those mentioned the common domain closure axiom Vx. of in T (in our case, all sites), and the that all objects (respectively individuals) In this setting, in T are different. the default and each site reaches any other site; notice path x = se, si,. andconn(sj_i,sj) We remark that the knowledge istrueforallj=l,...,n. theory T has a single extension E, in which all sites work iff there is a that reaches (x. y) is derivable . . , s, = y in the network such that worh( si) is true for all i = 0,. . . , n that transitive closure cannot be expressed between reachability sical logic. Indeed, it is well known [ l] ). Explicit (cf. vious disadvantages or its topology changes. However, default transitive closure and reachability. storage of inflexibility represented in T cannot be easily represented in clas- sites refers to the transitive closure of a graph; logic in terms of e.g. ground atoms con&( ~1, $2) has the ob- is extended cost, of logic allows for an elegant and maintenance if the network representation in classical first-order Suppose now we observe that site a works but site e is not reachable works(u) and veuches we look for an explanation we disallow an explanation; then immediately indeed, for assertion of facts puth( X, y) >, we find that-as from the formula Vx, y. -7works(x) 3 veuches(y, -veuches (a, e) , and, by the first default rule in D, works(u) from it, i.e., is not the case in the extension of T) . If in terms of sites that are down (whi",
            {
                "entities": [
                    [
                        75,
                        134,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 87 ( 1996) 295-342 Artificial Intelligence A probabilistic framework for cooperative multi-agent distributed interpretation and optimization of communication Y. Xiang” Department of Computer Science, University of Regina, Regina, Sask., Canada S4S OA2 Received September 1995 Abstract Multiply sectioned Bayesian networks for single-agent interpretation systems are extended into a framework for cooperative multi-agent as a Bayesian subnet. We show that the semantics of the joint probability distribution of such a system is well defined under reasonable conditions. systems. Each agent is represented distributed Unlike in single-agent systems where evidence is entered one subnet at a time, multiple agents may acquire evidence asynchronously in parallel. New communication operations are thus pro- posed to maintain global consistency. It may not be practical to maintain such consistency con- stantly due to the inter-agent “distance”. We show that, if the new operations are followed, between two successive communications, answers to queries from an agent are consistent with all local evidence, and are consistent with all global evidence gathered up to the last communication. During a communication operation, each agent is not available to process evidence for a period of time (called @line rime). Two criteria for the minimization of the off-line time, which may commonly be used, are considered. We derive, under each criterion, the optimal schedules when the communication is initiated from an arbitrarily selected agent. 1. Introduction Probabilistic a single-agent representation, reasoning paradigm. That updates in Bayesian networks (BNs) , as commonly applied, assumes is, a single processor accesses a single global network as over the network variables distribution the joint probability * E-mail: yxiang@cs.uregina.ca. 0004-3702/96/$15.00 Copyright @ 1996 Elsevier Science B.V. All rights reserved. SSDIOOO4-3702(95)00110-7 \f296 E Xumg/Arrificial Intellipvzce X7 (1996) 295-342 evidence becomes available and answers queries. Concurrency, marily aims at performance among multiple inference concurrent element junction and decentralization agents with multiple perspectives. The resultant is thus “ftne grained”, e.g., a node in a BN [ 151 or a clique to BNs, pri- [ 9,151, but not at modeling individual tree representation of a BN [ 91. as applied of control in the The single-agent paradigm is inadequate when uncertain are specialized to be addressed. A multi-agent (elements elements of a system between which temporal or semantic issues special each agent domain computational system’s goal cooperatively. that need is an autonomous accesses knowledge, intelligent some external reasoning differently) is performed by there is some “distance”, which may be spatial, [ I]. Such systems pose is thus required where its own partial some the subsystem. Each agent holds source and consumes information to achieve view resource. Each agent communicates with other agents Distributed artificial intelligence (DAI) addresses such “large-grained” coordinating multi-agent in DAI, e.g., blackboard systems [ 51, contract nets analyzing approaches [ 81 are essentially probabilistic approach in DAI. logic based. To our best knowledge, little has been reported the problems systems of designing and [ 2,6]. Main stream [ 31 and open systems to explore the probabilistic the problem of distributed in DAI. As defined originally system accepts evidence of objects and events reports our pilot study on applying reasoning. We address This paper erative multi-agent subclass of problems interpretation level descriptions system cation of all evidence include sensor networks, medical diagnosis by multiple complex artifacts and distributed of cooperative or self-interested paper. is needed when sensors to a centralized for collecting from some environment in the environment. A distributed evidence are distributed, interpretation and communi- site is undesirable. Examples of such systems of systems may consist in this agents image interpretation. Multi-agent agents. We consider only cooperative trouble-shooting specialists, approach interpretation, to coop- a [ 121, an and produces higher by Lesser and Erman inference for single-agent oriented and modular knowledge is based on multiply sectioned Bayesian networks Our representation which were developed and more efficient a natural extension the semantics of the joint probability distribution of a cooperative multi-agent well defined under that are used to maintain that optimize conditions. We propose new communication consistency. We derive communication the time efficiency of the communication. [ 181. We demonstrate into a multi-agent that the modularity of MSBNs allows In particular, we show that is system operations schedules representation, framework. reasonable inter-agent (MSBNs) reasoning [ 191, Section 2 briefly extension is represented semantic agent source, gathers are conditionally common distribution belief of every agent introduces BNs and single-agent MSBNs. Section 3 presents of single-agent MSBNs subnet to multi-agent MSBNs. Each cooperative that consumes its own evidence and can answer queries. When agents are cooperative, its own computational as a Bayesian re- the independent given the intersections of their subdomains and have a initial belief on their of the multi-agent intersections, system then it is shown that a joint probability is uniquely defined and is consistent with the in the system. \fY Xiang/Artijicial Intelligence 87 (1996) 295-342 291 Unlike single-agent systems where evidence is entered one subnet at a time, multiple in parallel. Section 4 discusses consistency- in the system will be globally consistent that arise from the extension. Section 5 adds new belief propagation issues to the set of single-agent MSBN operations agents may acquire evidence asynchronously related erations show that agents are performed. Inter-agent vent constant maintenance operations from an agent are consistent with all local evidence gathered tent with all global evidence gathered up to the last communication. an experimental distributed after the proposed operations cost may pre- consistency. We prove that when the proposed the answers to queries so far and are consis- Section 6 presents op- communication. We how a multi-agent MSBN may be used and the associated communication demonstration task. “distance” of inter-agent are used, between communications, two successive for inter-agent to perform a interpretation operation, each agent is not available time). Such non-availability the length of the off-line to process new evidence restriction on imposes time should be minimized. During a communication for a period of time (called @-line applications. Therefore, time-critical Section 7 defines two criteria commonly multi-agent all agents we abstract the factors the activities during that can be manipulated in the system. To facilitate be used. One is based on the total length of the off-line system. The other is based on the average for the minimization length of the off-line the study of the optimal communication of the off-line time which may time for the entire time across schedules, into a graphical model, and identify the communication in optimizing these schedules. Section 8 reduces scheduling the communication subproblems the duality of the two subproblems. the optimal schedules be derived by solving only one of the subproblems. Section 9 the communication the selected is initiated time when the communication into two independent from an arbitrarily result allows that yield schedules criterion, This for each minimization and establishes communication derives, minimum agent. off-line Section 10 discusses some general issues related to this work. Our presentation as- sumes a general terminology of graph theory. 2. Multiply sectioned Bayesian networks 2.1. Bayesian networks is a triplet A BN [9,11,13,15] (N, E, I’). N is a set of nodes. Each node is labeled with a variable associated with a space. We shall use “node” and “variable” interchange- ably. Therefore, N represents a problem domain. E is a set of arcs such that D = (N, E) is a directed acyclic graph (DAG) . We refer to D as the strucrure of the BN. The arcs signify directed dependencies the linked variables. For each node Ai E N, the strengths of the dependencies on the values of Ai’S parents. For probability any three sets X, Y and Z of variables, X and Y are said to be conditionally independent given Z under probability distribution P if P (XI YZ) = P (XI Z) whenever P (Yz) > 0. in BNs is that a variable The basic dependency between from its parent nodes 7~ are quantified by a conditional distribution p(Ail~i) of Ai conditioned is conditionally assumption embedded \f298 L Xiung/Artijicid Intelligence 87 (1996) 295-342 bcps pn apb mf7_a mmcb mcmp mupl Fig. MSBN I. Left: an example MSBN in the left. Right: a general hypertree for neural muscular diagnosis. Middle: structured MSBN. D’ @ D -:ji ND< D the hypertree organization of the independent probability of its non-descendants given its parents. This assumption allows P, the joint distribution (jpd), to be specified by the product P = nip(A;l’rri). 2.2. Single-agent oriented MSBNs To make the paper self-contained, we briefly introduce the single-agent oriented MS- BNs [ 18,191. An MSBN M consists of a set of interrelated Bayesian subnets. Each subnet represents in a large problem domain or total universe. Each subnet intersection of a subdomain dependencies shares a non-empty between each pair of subnets satisfies set of variables with at least one other subnet. The the d-sepset condition. Definition 1 (d-sepset). (N’ U N’, E’ U E2) and D’ Let D’ = (N’, E’) is a DAG. The intersection (i = I, 2) be two DAGs such that D = I = N’ n N* is a d-sepset between D’ if, for every Ai E I with its parents Z-; in D, either Z-; C: N’ or n-; C N2. It can be shown that, when",
            {
                "entities": [
                    [
                        76,
                        190,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 262 (2018) 15–51Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintClassical logic, argument and dialecticM. D’Agostino a, S. Modgil b,∗a Department of Philosophy, University of Milan, Italyb Department of Informatics, King’s College London, United Kingdoma r t i c l e i n f oa b s t r a c tArticle history:Received 10 July 2017Received in revised form 10 May 2018Accepted 24 May 2018Available online 20 June 2018Keywords:Classical logicArgumentationDialecticRationality postulatesNatural deductionPreferred subtheoriesBounded reasoningA well studied instantiation of Dung’s abstract theory of argumentation yields argumenta-tion-based characterisations of non-monotonic inference over possibly inconsistent sets of classical formulae. This provides for single-agent reasoning in terms of argument and counter-argument, and distributed non-monotonic reasoning in the form of dialogues between computational and/or human agents. However, features of existing formalisations of classical logic argumentation (Cl-Arg) that ensure satisfaction of rationality postulates, preclude applications of Cl-Arg that account for real-world dialectical uses of arguments by resource-bounded agents. This paper formalises dialectical classical logic argumentation that both satisfies these practical desiderata and is provably rational. In contrast to standard approaches to Cl-Arg we: 1) draw an epistemic distinction between an argument’s premises accepted as true, and those assumed true for the sake of argument, so formalising the dialectical move whereby arguments’ premises are shown to be inconsistent, and avoiding the foreign commitment problem that arises in dialogical applications; 2) provide an account of Cl-Arg suitable for real-world use by eschewing the need to check that an argument’s premises are subset minimal and consistent, and identifying a minimal set of assumptions as to the arguments that must be constructed from a set of formulae in order to ensure that the outcome of evaluation is rational. We then illustrate our approach with a natural deduction proof theory for propositional classical logic that allows measurement of the ‘depth’ of an argument, such that the construction of depth-bounded arguments is a tractable problem, and each increase in depth naturally equates with an increase in the inferential capabilities of real-world agents. We also provide a resource-bounded argumentative characterisation of non-monotonic inference as defined by Brewka’s Preferred Subtheories.© 2018 Elsevier B.V. All rights reserved.1. IntroductionArgumentation is a form of reasoning that makes explicit the reasons for the conclusions that are drawn and how conflicts between reasons are resolved. While informal studies of argumentation have a rich tradition, recent years have witnessed intensive study of logic-based models of argumentation and their use in formalising agent reasoning, decision making, and inter-agent dialogue [11,53]. Much of this work builds on Dung’s seminal theory of abstract argumentation [26], and the theory’s provision of argumentative characterisations of nonmonotonic inference. Given a possibly inconsistent set of logical formulae (base) one defines the arguments and a binary attack relation denoting that one argument is a * Corresponding author.E-mail address: sanjay.modgil@kcl.ac.uk (S. Modgil).https://doi.org/10.1016/j.artint.2018.05.0030004-3702/© 2018 Elsevier B.V. All rights reserved.\f16M. D’Agostino, S. Modgil / Artificial Intelligence 262 (2018) 15–51counter-argument to another. Various developments of Dung’s theory additionally accommodate a preference relation over arguments, which is used to determine which attacks succeed as defeats [1,9,43]. The resulting directed graph of arguments related by defeats, referred to as an argumentation framework (AF), is then said to be ‘instantiated’ by the base. Evaluation of the justified arguments is then based on the intuitive principle that an argument is justified if all its defeaters are themselves defeated by justified arguments. The conclusions of justified arguments identify the ‘argumentation defined’ non-monotonic inferences from the instantiating base.The widespread impact of Dung’s theory is in large part due to this characterisation of non-monotonic inference in terms of the dialectical use of arguments and counter-arguments familiar in everyday reasoning and debate. The theory thus provides foundations for reasoning by individual computational and human agents, and distributed non-monotonic reasoning involving agents resolving conflicts amongst beliefs or deciding amongst alternative actions, or negotiating alloca-tions of resources (e.g., [2,3,8,38,40,44,45,48,58]). These ‘monological’ and ‘dialogical’ applications have motivated the study of rationality postulates for logical instantiations of A F s [15,16], as well as desiderata for practical applications [27,44].This paper focuses on classical logic instantiations of A F s (Cl-Arg) [1,33,43]. Features of the current paradigm have been shown to provide sufficient conditions for satisfaction of the rationality postulates. However, these features preclude satisfaction of practical desiderata that account for modelling real-world uses of arguments by resource-bounded agents. This paper therefore aims at an account of Cl-Arg that satisfies both practical desiderata and the rationality postulates.In Section 2 we review Dung’s theory, Cl-Arg, and the rationality postulates. In Section 3 we argue that monological and dialogical applications of Dung’s theory require formalisation of real-world uses of argument suitable for resource-bounded agents. However, current approaches to Cl-Arg tacitly assume that all arguments defined by a base can be constructed and included in an A F , and that prior to inclusion the legitimacy of each constructed argument is verified by checking that its premises are consistent and not redundant in the strong sense that their conclusion is not entailed by any proper subset of the premises. These assumptions are computationally unfeasible (even in the propositional case) for real-world uses of argument by resource-bounded agents. However, they are proposed as sufficient conditions for satisfaction of the consistency and closure postulates [15] for first order Cl-Arg with preferences [43],1 and of the ‘non-contamination’ postulates [16] for propositional Cl-Arg without preferences. Moreover, checking the legitimacy of arguments prior to inclusion in an A F fails to account for real-world uses of argument. Firstly, in real-world uses the inconsistency of arguments’ premises is typically demonstrated dialectically. Secondly, agents do not interrogate premises for subset minimality. Rather, it is the specific proof-theoretic means for constructing arguments that determines whether or not premises are redundantly used in deriving the conclusion; that is, redundant in the obvious sense that they are syntactically disjoint from the remaining premises and the conclusion.Section 3 then presents a new account of first order Cl-Arg that satisfies practical desiderata. Our approach introduces a new notion of argument that distinguishes amongst the premises accepted as true and those supposed true ‘for the sake of argument’. We can therefore model a ubiquitous feature of dialectical practice, whereby the inconsistency of a set of premises (cid:2) is shown dialectically, by defeats from arguments that claim that a contradiction is implied if one supposes (for the sake of argument) the truth of (cid:2). The distinction also solves the so called foreign commitment problem that arises in dialogical applications when agents are forced to commit to the premises of their interlocutors in order to challenge their arguments [17]. We also drop the computationally demanding checks on the legitimacy of arguments, and define ‘partially instantiated’ A F s that include subsets of the arguments defined by a base. We thus accommodate real-world uses of argument in which agents do not (or may not have sufficient resources to) construct all arguments from a base when determining whether arguments are justified. We show that our account satisfies standard properties of Dung’s theory. We also show that despite dropping the legitimacy checks on arguments and making minimal assumptions as to the arguments defined by a base for inclusion in an A F , the consistency and closure postulates are satisfied (where the latter are adapted to account for the fact that not all defined arguments may be included in the A F ). Moreover, in contrast with [43], these postulates are satisfied assuming any preference relation over arguments.Finally, in Section 3 we identify the notion of an argument whose use of obviously redundant (in the sense described above) premises can be excluded proof-theoretically, in contrast with the use of impractical subset-minimality checks We generalise the ‘non-contamination’ postulates defined for propositional instantiations of A F s in [16], to first order instanti-ations. We then show that despite dropping consistency and subset minimality checks on arguments’ premises, our account of first order Cl-Arg satisfies these postulates under the assumption that preference relations are ‘coherent’.Standard accounts of Cl-Arg typically leave implicit the specific proof theoretic means by which one entails a conclusion from a set of premises. In Section 4 we illustrate use of our dialectical account of argumentation by formalising arguments as intelim trees: a new natural deduction formalism for propositional classical logic [20,21] that allows measurement of the ‘depth’ of an argument such that the construction of depth-bounded arguments is a tractable problem, and each increase in depth naturally equates with an increase in the inferential capabilities of real-world agents. We then show that A F s instantiated by arguments up to any given depth",
            {
                "entities": [
                    [
                        134,
                        173,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 91 ( 1997) 225-256 Artificial Intelligence The computer revolution in science: steps towards the realization of computer-supported discovery environments Hidde de Jong a**, Arie Rip b,l B Knowledge-Based Systems Group, Department of Computer Science, University of Twente, PO. Box 217, 7500 AE Enschede, Netherlands h School of Philosophy and Social Sciences, Universify of Twente, PO. Box 217, 7500 AE Enschede, Netherlands Received December 1995; revised September 1996 Abstract The tools that scientists use in their search processes together form so-called discovery environ- ments. The promise of artificial intelligence and other branches of computer science is to radically transform conventional discovery environments by equipping scientists with a range of powerful computer tools including large-scale, shared knowledge bases and discovery programs. We will describe the future computer-supported discovery environments that may result, and illustrate by means of a realistic scenario how scientists come to new discoveries in these environments. In order to make the step from the current generation of discovery tools to computer-supported discovery environments like the one presented in the scenario, developers should realize that such environments are large-scale sociotechnical systems. They should not just focus on isolated computer programs, but also pay attention to the question how these programs will be used and maintained by scientists in research practices. In order to help developers of discovery programs in achieving the integration of their tools in discovery environments, we will formulate a set of guidelines that developers could follow. @ 1997 Elsevier Science B.V. Keywords: Scientific discovery; Computer-supported technology discovery environments; Social studies of science and * Corresponding ’ E-mail: a.rip@wmw.utwente.nl. author. E-mail: hdejong@cs.utwente.nl. 0004.3702/97/$17.00 PII SOOO4-3702(97)0001 1-8 @ 1997 Elsevier Science B.V. All rights reserved. \f226 H. de Jong. A. Rip/Artificial Intelligence 91 (1997) 225-256 1. Introduction Knowledge collection of experimental hnd- explanatory models, and theories, which continuously in a scientific domain and patterns, the activities of scientists. Scientists are engaged is a heterogeneous search in which they construct new empirical phenomena, devise models accounting relating a wide range of empirical phenomena. in open-ended theories regularities through institutes, or in field work at distant ings, develops processes, for these findings and propose These search processes are local: in research a consequence processes convinced of the value of the results, edge in a domain may lead to further research and further discoveries, dynamic of science. In structuring the shared body of knowledge of the communication to relevant locations. The generality of science they take place at laboratory benches, behind desks is in local search are the claims are added to the shared body of knowl- [ 521. The extension of the body of knowledge with new discoveries thus giving rise to the characteristic audiences within a scientific community. of the discovery claims generated If these audiences systematized and rules are sometimes they work. These heuristics scientists make use of the heuristics of science have for instance described their activities, and the skills these activities exemplify, in their domain and performing disco- and rules of the research practice very activities, in which into ex- plicit methods and made available as tools to scientists grappling with similar problems, Philosophers and evaluated how experimenters reconstruct others can use in order to judge and replicate zation of scientific work and knowledge may of a practice, a specific, self-contained performing days measured by putting electrodes Within embodied. The that into procedures [ 21,471. The systemati- to the further material equipment into an apparatus its pH, is nowa- subtask. The acidity of a liquid, in the liquid and reading off the pH from a scale. are their findings lead such a piece of lab equipment for instance when heuristics a number of electrochemical and rules are embodied thus constructed, that have been regularities tools together form a discovery [ 141. Historians of discovery environments, that Boyle used for his pneumatic terial devices, their search processes descriptions instruments ern biological installations lab focusing on neuroendocrinological environment, in which scientists both methods and procedures and ma- can pursue and philosophers of science have given detailed such as the air-pump and related methods and around 1660 [ 541, a mod- [ 371, and the complex experiments research for running experiments in high-energy physics [ 201. The promise of AI, and of computer science in general, is to transform conventional and material equipment of a research practice. Systematic methods experiments environments and planning through widening discovery systematization tasks like designing of data are now being implemented atizations of bodies of knowledge, files with experimental databases existing supported discovery environments. This development and knowledge scientific discovery environments, results bases give rise (Fig. 1). The resulting the scope and increasing the depth of the for in large amounts system- in the form of handbooks or ordered in automatic discovery programs. Existing for instance and finding regularities computer in tools, to what we will call computer- to speculate about has led some integrated in office drawers, are transformed into machine-readable \fH. de Jong, A. Rip/Artijcial intelligence 91 (1997) 225-256 221 RESEARCH PRACTICE systematic scientific method scientific discovery program I. Systematization Fig. practice. and computerization of the body of knowledge and scientific activities in a research “radical, and perhaps surprising, (Allen Newell, quoted with new and promising environments, in [ 61) . How present, well-tested AI techniques, and what this means for science, techniques, can help create these computer-supported is the topic of this article. 2 transformations of the disciplinary structure of science” supplemented discovery understood then argue regulation mechanism are used by scientists the computer-supported discovery environments In Section 2, we will describe and reinforce our points, a realistic in the E. coli bacterium will be introduced. of the in their search future and the way these discovery environments scenario of the discovery processes. To illustrate In of a new genetic cannot Section 3 we will tools, but that we have to be adequately in a research practice. This widening of view them as sociotechnical tools: we should not just focus perspective has implications to the way on the task performance of isolated computer programs, but also pay attention in a discovery environment. Section 4 reviews how far we have come they are organized that toward the realization of computer-supported tools in research practices have the integration of discovery especially hardly been touched upon. In Section 5 we will therefore present a number of guidelines that computer-supported as a mere collection of computer systems embedded for the design of discovery discovery environments discovery environments issues surrounding and concludes * Amidst the enthusiasm about future computer-supported that the emergence of ideas on the large-scale techniques. however, with the availability of advanced computer age have shown much Leibniz, among others, propagated of an am inveniendi to derive new discoveries in systematizing interest from these histories. discovery systematization In fact, philosophers environments, we should not forget, of scientific practices did not coincide from the past as well as from our and activities. For example, Bacon and scientific knowledge the construction of natural and experimental histories and the development \f228 H. de Jong, A. Rip/Artificial Intelligence 91 (1997) 225-256 that developers other computer and offers some concluding could tools remarks. follow when in practices of scientists. Section 6 recapitulates they strive at embedding discovery programs and the main points 2. Search processes in computer-supported discovery environments technical and knowledge to propose a detailed In the computer-supported think of them as being built around relatively passive resources discovery environments systems will find a place alongside other tools already available architecture that are now envisaged, powerful to scientists. of discovery like like hypothesis gen- (as suggested by Fig, 1). tools will also form a part of the discovery of the future. In addition we will meet with the conventional measuring equipment discovery currently and retain ele- gradually develop discovery Although we do not intend systems, one could databases simulators, erators, process Telephone, e-mail, and other communication environments and analytic environments ments from them. computer-supported environments bases, and active discovery programs from existing discovery in laboratories; assistants revision theory found and across results obtained A computer-supported at laboratories discovery environment to a distributed database, which stores experimental is not just a loose aggregate of tools, related and adjusted, so that for instance sent in millions discovery environment but an integrated system. The tools are mutually the results of a bioassay can be processed by an analysis program and subsequently forward of bioassays performed computer-supported but is imposed by the research practice Through relationships established. These who know how to handle and combine Think, for example, of program manuals and operating and maintenance also of relationships connections. the world. The order and structure in a is not an inherent characteristic of the tools, is embedded. in the search processes of s",
            {
                "entities": [
                    [
                        76,
                        186,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 776–804www.elsevier.com/locate/artintAn executable specification of a formal argumentation protocolAlexander Artikis a,∗, Marek Sergot b, Jeremy Pitt ca Institute of Informatics & Telecommunications, NCSR “Demokritos”, Athens, 15310, Greeceb Department of Computing, Imperial College London, SW7 2AZ, UKc Department of Electrical & Electronic Engineering, Imperial College London, SW7 2BT, UKReceived 8 November 2006; received in revised form 3 April 2007; accepted 16 April 2007Available online 29 April 2007AbstractWe present a specification, in the action language C+, of Brewka’s reconstruction of a theory of formal disputation originallyproposed by Rescher. The focus is on the procedural aspects rather than the adequacy of this particular protocol for the conduct ofdebate and the resolution of disputes. The specification is structured in three separate levels, covering (i) the physical capabilitiesof the participant agents, (ii) the rules defining the protocol itself, specifying which actions are ‘proper’ and ‘timely’ according tothe protocol and their effects on the protocol state, and (iii) the permissions, prohibitions, and obligations of the agents, and thesanctions and enforcement strategies that deal with non-compliance. Also included is a mechanism by which an agent may objectto an action by another participant, and an optional ‘silence implies consent’ principle. Although comparatively simple, Brewka’sprotocol is thus representative of a wide range of other more complex argumentation and dispute resolution procedures that havebeen proposed. Finally, we show how the ‘Causal Calculator’ implementation of C+ can be used to animate the specification andto investigate and verify properties of the protocol.© 2007 Elsevier B.V. All rights reserved.Keywords: Argumentation; Disputation; Protocol; Norm; Multi-agent system; Specification; Action language1. IntroductionOne of the main tasks in the formal specification and analysis of (open) multi-agent systems (MAS) is the represen-tation of the protocols and procedures for agent interactions, and the norms of behaviour that govern these interactions.Examples include protocols for exchanging information, for negotiation, and for resolving disputes.It has been argued that a specification of systems of this type should satisfy at least the following two requirements:first, the interactions of the members should be governed by a formal, declarative, verifiable and meaningful seman-tics [64]; and second, to cater for the possibility that agent behaviour may deviate from what is prescribed, agentinteractions can usefully be described in terms of permissions and obligations [26].We have been developing a theoretical framework for the executable specification of open agent systems thataddresses the aforementioned requirements [3–5]. We adopt the perspective of an external observer, thus taking intoaccount only externally observable behaviours and not the internal architectures of the individual agents, and view* Corresponding author.E-mail addresses: a.artikis@acm.org (A. Artikis), mjs@doc.ic.ac.uk (M. Sergot), j.pitt@imperial.ac.uk (J. Pitt).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.008\fA. Artikis et al. / Artificial Intelligence 171 (2007) 776–804777agent systems as instances of normative systems [26] whereby constraints on agents’ behaviour (or social constraints)are specified in terms of their permissions, their institutional power to effect changes and bring about certain states ofaffairs, and their rights and obligations to one another. We employ an action formalism to specify the social constraintsgoverning the behaviour of the members and then use a computational framework to animate the specification andinvestigate its properties. For the action formalism, we have employed the Event Calculus [29], the action languageC+ [22], and an extended form of C+ specifically designed for modelling the institutional aspects of agent systems[57–59].In this paper we demonstrate how the theoretical and computational frameworks can be used with the languageC+ to specify and execute an argumentation protocol based on Brewka’s reconstruction [8], in the Situation Calculus[52], of a theory of formal disputation originally proposed by Rescher [53]. We presented a preliminary formulationin an earlier paper [4]. This present paper is a refined and much extended version.We are focusing here on the procedural aspects of the protocol rather than on the underlying logic of disputationemployed by Brewka or on the adequacy of this particular protocol for the conduct of debate and the resolution ofdisputes. The features of Brewka’s protocol are representative of a wide range of other more complex argumentationand dispute resolution procedures that have been proposed in the literature, and to which the methods of this papercan be similarly applied.The specification of the argumentation protocol is structured into three separate levels, covering:(i) the physical capabilities of the participant agents (in the present context, the messages/utterances each agent isactually capable of transmitting);(ii) the rules defining the protocol itself, specifying which actions are ‘proper’ and ‘timely’ according to the protocoland their effects on the protocol state;(iii) the permissions, prohibitions and obligations of the agents, and the sanctions and enforcement strategies that dealwith non-compliance.In any given implementation of the protocol, it may or may not be permitted for an agent to perform an action that isnot proper or timely; conversely, there may be protocol actions that are proper and timely but that are nevertheless notpermitted under certain circumstances, because, for instance, they lead to protocol runs with undesirable properties.The rules comprising level (ii) of the specification correspond to constitutive norms that define the meaning of theprotocol actions. Levels (i) and (iii), respectively, can be seen as representing the physical and normative environmentwithin which the protocol is executed. We have also been concerned with the concept of social role. Briefly, a role isassociated with a set of (role) preconditions that agents must satisfy in order to be eligible to occupy that role and aset of (role) constraints that govern the behaviour of the agents once they occupy that role. We will not discuss roleassignment in this paper. For the example in this paper, we will assume for simplicity that the participant agents arealready assigned to certain roles, and that these roles do not change during an execution of the protocol.A note on terminology. In the earlier version of this paper [4], and in the treatment of other examples, we defineda protocol by specifying the conditions under which an action was said to be ‘valid’ according to the protocol. Here,we have employed a finer structure, further classifying ‘valid’ actions as proper or timely, in line with suggestionsthat have also been made by Prakken et al. [45,49]. A ‘valid’ action in our earlier terminology is one that is bothproper and timely. Other terminology in common use employs the term ‘successful’ where we say ‘valid’: one thendistinguishes between an action, such as an utterance or the transmission of a message of a certain form, which isan ‘attempt’ to make a claim, say, and the conditions under which the attempt to claim is ‘successful’ (sometimes,‘effective’). We prefer to avoid the term ‘successful’ since even an unsuccessful ‘attempt’ can have effects on theprotocol state. We also avoid use of the term ‘legal’ for ‘valid’ or ‘successful’ since it is ambiguous as to whetherit refers to the constitutive element of the protocol (level (ii) of our specification) or the normative environment inwhich the protocol is executed (level (iii)). Also related is the concept of institutional (or ‘institutionalised’) power(sometimes, ‘competence’ or ‘capacity’). This refers to the characteristic feature of institutions—legal systems, formalorganisations, or informal groupings—whereby designated agents, often when acting in specific roles, are empoweredto create or modify facts of special significance in that institution—institutional facts in the terminology of Searle [56].(See e.g. [27,35] for further discussion and references to the literature.) Thus in the present example it is natural tosay that, under certain circumstances, an agent acting in a certain role has power (competence, capacity) to declarethe dispute resolved in favour of one or other of the protagonists; or that in certain circumstances an agent has power\f778A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804to object to an action by one of the other participants; or more generally, that the argumentation protocol definesthe conditions under which an agent has the power to perform one of the argumentation actions. We will not referexplicitly to power in the specification of the argumentation protocol presented here. The classification of actions intoproper and timely already provides a more detailed specification.In this paper we use the language C+ to formulate the specification. An advantage of C+, compared with otheraction formalisms, is that it can be given an explicit semantics in terms of transition systems. This enables us toanalyse and prove properties of the protocol. The concluding sections of the paper present some illustrative examples.The paper is structured as follows. First, we briefly describe the C+ language. Second, we present the ‘Causal Cal-culator’ software implementation, a computational framework for executing specifications formalised in C+. Third,we summarise Brewka’s reconstruction of Rescher’s theory of formal disputation. Fourth, we specify, prove propertiesof, and execute (a form of) Brewka’s argumentation protocol with the use of C+ and the Causal Calculator. Finally,we discuss related research, summarise the presented work, and",
            {
                "entities": [
                    [
                        72,
                        134,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 175 (2011) 487–511Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintOnline planning for multi-agent systems with bounded communicationFeng Wu a,b,∗, Shlomo Zilberstein b, Xiaoping Chen aa School of Computer Science, University of Science and Technology of China, Jinzhai Road 96, Hefei, Anhui 230026, Chinab Department of Computer Science, University of Massachusetts at Amherst, 140 Governors Drive, Amherst, MA 01003, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 2 February 2010Received in revised form 22 September2010Accepted 26 September 2010Available online 29 September 2010Keywords:Decentralized POMDPsCooperation and collaborationPlanning under uncertaintyCommunication in multi-agent systemsWe propose an online algorithm for planning under uncertainty in multi-agent settingsmodeled as DEC-POMDPs. The algorithm helps overcome the high computationalcomplexity of solving such problems offline. The key challenges in decentralized operationare to maintain coordinated behavior with little or no communication and, whencommunication is allowed, to optimize value with minimal communication. The algorithmaddresses these challenges by generating identical conditional plans based on commonknowledge and communicating only when history inconsistency is detected, allowingcommunication to be postponed when necessary. To be suitable for online operation,the algorithm computes good local policies using a new and fast local search methodimplemented using linear programming. Moreover, it bounds the amount of memory usedat each step and can be applied to problems with arbitrary horizons. The experimentalresults confirm that the algorithm can solve problems that are too large for the bestexisting offline planning algorithms and it outperforms the best online method, producingmuch higher value with much less communication in most cases. The algorithm also provesto be effective when the communication channel is imperfect (periodically unavailable).These results contribute to the scalability of decision-theoretic planning in multi-agentsettings.© 2010 Elsevier B.V. All rights reserved.1. IntroductionA multi-agent system (MAS) consists of multiple independent agents that interact in a domain. Each agent is a decisionmaker that is situated in the environment and acts autonomously, based on its own observations and domain knowledge, toaccomplish a certain goal. A multi-agent system design can be beneficial in many AI domains, particularly when a systemis composed of multiple entities that are distributed functionally or spatially. Examples include multiple mobile robots(such as space exploration rovers) or sensor networks (such as weather tracking radars). Collaboration enables the differentagents to work more efficiently and to complete activities they are not able to accomplish individually. Even in domains inwhich agents can be centrally controlled, a MAS can improve performance, robustness and scalability by selecting actions inparallel. In principle, the agents in a MAS can have different, even conflicting, goals. We are interested in fully-cooperativeMAS, in which all the agents share a common goal.In a cooperative setting, each agent selects actions individually, but it is the resulting joint action that produces theoutcome. Coordination is therefore a key aspect in such systems. The goal of coordination is to ensure that the individualdecisions of the agents result in (near-)optimal decisions for the group as a whole. This is extremely challenging especially* Corresponding author at: Department of Computer Science, University of Massachusetts at Amherst, 140 Governors Drive, Amherst, MA 01003, USA.Tel.: +1 413 545 1985; fax: +1 413 545 1249.E-mail addresses: wufeng@mail.ustc.edu.cn (F. Wu), shlomo@cs.umass.edu (S. Zilberstein), xpchen@ustc.edu.cn (X. Chen).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.09.008\f488F. Wu et al. / Artificial Intelligence 175 (2011) 487–511when the agents operate under high-level uncertainty. For example, in the domain of robot soccer, each robot operatesautonomously, but is also part of a team and must cooperate with the other members of the team to play successfully.The sensors and actuators used in such systems introduce considerable uncertainty. What makes such problems particularlychallenging is that each agent gets a different stream of observations at runtime and has a different partial view of thesituation. And while the agents may be able to communicate with each other, sharing all their information all the time isnot possible. Besides, agents in such domains may need to perform a long sequence of actions in order to reach the goal.Different mathematical models exist to specify sequential decision-making problems. Among them, decision-theoreticmodels for planning under uncertainty have been studied extensively in artificial intelligence and operations research sincethe 1950’s. Decision-theoretic planning problems can be formalized as Markov decision processes (MDPs), in which a singleagent repeatedly interacts with a stochastically changing environment and tries to optimize a performance measure basedon rewards or costs. Partially-observable Markov decision processes (POMDPs) extend the MDP model to handle sensor un-certainty by incorporating observations and a probabilistic model of their occurrence. In a MAS, however, each individualagent may have different partial information about the other agents and about the state of the world. Over the last decade,different formal models for this problem have been proposed. We adopt decentralized partially-observable Markov deci-sion processes (DEC-POMDPs) to model a team of cooperative agents that interact within a stochastic, partially-observableenvironment.It has been proved that decentralized control of multiple agents is significantly harder than single agent control andprovably intractable. In particular, the complexity of solving a two-agent finite-horizon DEC-POMDP is NEXP-complete [12].In the last few years, several promising approximation techniques have been developed [3,11,17,19,46,47]. The vast majorityof these algorithms work offline and compute, prior to the execution, the best action to execute for all possible situations.While these offline algorithms can achieve very good performance, they often take a very long time due to the doubleexponential policy space that they explore. For example, PBIP-IPG – the state-of-the-art MBDP-based offline algorithm –takes 3.85 hours to solve a small problem such as Meeting in a 3×3 grid that involves 81 states, 5 actions and 9 observations[3]. Online algorithms, on the other hand, plan only one step at a time and they do so given all the currently availableinformation. The potential for achieving good scalability is more promising with online algorithms. But it is extremelychallenging to keep agents coordinated over a long period of time with no offline planning. Recent developments in onlinealgorithms suggest that combining online techniques with selective communication – when communication is possible –may be the most efficient way to tackle large DEC-POMDP problems. The main goal of this paper is to present, analyze,and evaluate online methods with bounded communication, and show that they present an attractive alternative to offlinetechniques for solving large DEC-POMDPs.The main contributions of this paper include: (1) a fast method for searching policies online, (2) an innovative way foragents to remain coordinated by maintaining a shared pool of histories, (3) an efficient way for bounding the number ofpossible histories agents need to consider, and (4) a new communication strategy that can cope with bounded or unreliablecommunication channels. In the presence of multiple agents, each agent must cope with limited knowledge about theenvironment and the other agents, and must reason about all the possible beliefs of the other agents and how that affectstheir decisions. Therefore, there are still many possible situations to consider even for selecting just one action given thecurrent knowledge. We present a new linear program formulation to search the space of policies very quickly. Anotherchallenge is that the number of possible histories (situations) grows very rapidly over time steps, and agents could runout of memory very quickly. We introduce a new approach to merging histories and thus bound the size of the pool ofhistories, while preserving solution quality. Finally, it is known that appropriate amounts of communication can improvethe tractability and performance of multi-agent systems. When communication is bounded, which is true in many real-world applications, it is difficult to decide how to utilize the limited communication resource efficiently. In our work,agents communicate when history inconsistency is detected. This presents a new effective way to initiate communicationdynamically at runtime.The rest of the paper is organized as follows. In Section 2, we provide the background by introducing the formal modeland discussing the offline and online algorithms as well as the communication methods in the framework of decentralizedPOMDPs. In Section 3, we present the multi-agent online planning with communication algorithms including the generalframework, policy search, history merging, communication strategy and implementation issues. In Section 4, we report theexperimental results on several common benchmark problems and a more challenging problem named grid soccer. We alsoreport the results for the cooperative box pushing domain with imperfect communication settings. In Section 5, we surveythe various existing online approaches with communications that have been applied to decentralized POMDPs, and discusstheir strengths and drawbacks. Finally, we summarize the contributions and discuss the limitations and open questions inthis work.2. BackgroundI",
            {
                "entities": [
                    [
                        136,
                        202,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 89 ( 1997) 317-364 Artificial Intelligence Ramification and causality Michael Thielscher * International Computer Science Institute. 1947 Center Street, Berkeley, CA 94704-1198, USA Received December 1995; revised June 1996 Abstract In formal systems laws describing dependencies the problem in action specifications the ramification problem denotes represented among components for reasoning about actions, indirect effects. These effects are not explicitly from general of handling but follow of the world state. An adequate treatment of indirect effects requires a suitably weakened version of the general law of persistence. It also requires a method to avoid unintuitive changes suggested by the aforementioned dependency laws. We propose a solution to the ramification problem that uses directed relations between two single effects, stating the circumstances under which the occurrence the second. We argue for the necessity of an approach based on causality by of the first causes principle elaborating the limitations of common paradigms employed to handle ramifications-the of categorization and the policy of minimal change. Our abstract solution is realized on the basis of a particular action calculus, namely, the fluent calculus. Keywords: Temporal reasoning; Reasoning about actions; Ramification problem; Causality; Fluent calculus 1. Introduction The ability to reason about changing environments, which effects of one’s own actions and explaining observed phenomena, basis for understanding in their habitat. Formal approaches to model This research area was initiated by McCarthy actions plays a fundamental the world to an extent sufficient this ability have a long [ 3 11, who claimed role in common involves predicting the serves humans as a to survive and to act intelligently tradition that reasoning in AI. about Drawing conclusions about dynamic is grounded on formal specifica- tions of what effects are caused by the execution of a particular obviously to provide an exhaustive description defining infeasible action. Since the result of executing it is * On leave from FG Intellektik, TH Darmstadt. E-mail: michaelt@icsi.berkeley.edu. 0004.3702/97/$17.00 Copyright @ 1997 Elsevier Science B.V. All rights reserved. PUSOOO4-3702(96)00033-l sense. environments \f318 M. Thielscher/Artijiciai lntelli~ence 89 (I 997) 317-364 in each possible that they affect-while the rest of the world is subject state of the world, action specifications i.e., is assumed in complex domains should be restricted an action to the to the part of the world to remain stable. Yet even this approach becomes law of ,~e~~istence, if one tries to put all effects into a single, complete unmanageable an action may cause only a small number of direct changes, specification. Although that can be hard to foresee. they in turn may in the first place causes For instance, nothing but a change of the switch’s position. However, the switch may be part of an electric circuit so that, say, some light bulb is turned off as a side effect, which in turn room by running against a may cause someone chair that, as a consequence, the fire alarm and so on and so forth.’ initiate a long chain of indirect effects the action of toggling a switch, which set whose implosion activates in a suddenly darkened falls into a television to hurt himself consider The task, then, is to design a framework are not assumed specifications the ramification problem. 2 A satisfactory treatment of the following two major issues. to formalize action scenarios where action to completely describe all possible effects. This is called requires a successful to this problem solution First, we need an appropriately weakened version of the aforementioned law of persis- that are unaffected by the tence that applies only to those parts of the world description the to this problem, we suggest keeping action’s direct and indirect effects. As a solution through the world description obtained law of persistence as it stands while considering result. Indirect effects are then accommodated its application merely as an intermediate by further state obtains. This method accounts perfectly both for rigorous persistence of unaffected parts and for arbitrarily complex chains of indirect effects. until an overall satisfactory successor reasoning from Second, dependencies indirect effects typically are consequences between world description components of additional, general knowledge of domain-specific (usually called not all effects suggested puenrs)-but the from are desirable standpoint of causality. As an example, consider the simple electric circuit depicted in Fig. 1, which consists of a battery, two switches and a light bulb. The obvious these components may formally be described by the logic expression connection between SM/~ A SW? E light, i.e., the light is on if and only if both switches are on. Now suppose state displayed, where both WI and light we toggle in the particular are false while sw2 is true. Then, besides true, we that the light bulb turns on. This indirect effect is inspired by the formula also expect the direct effect of SWI becoming the first switch this perspective the above may not only be described as “the fire alarm the switch”, but also as, say, “the chair the distinction between in this context concerns step and those which deserve separate state transitions indirect effects occurring during a single (also called delayed effects). state after having hits the the next state transition)“. As a reasonable, albeit informal, guidance we suggest a single agent himself, has the summarize what happens until someone, e.g., is activated in the successor (and presumably in the successor is falling state ’ A crucial question world’s state transition E.g., toggled television set during state transition possibility should to intervene by acting again (stopping the chair from falling, the reasoning for instance) * The naming was suggested cation problem to exploit aim of restricting in exactly logical consequences search space. in [ I5 1, inspired by [ IO 1. The latter, however, was not devoted the above sense, contrary (called ramifications) to what is often claimed; of goal specifications to the ramifi- rather, this thesis describes how in planning problems, with the \fM. Thielscher/Artijcial Intelligence 89 (1997) 317-364 319 Fig. I. An electric circuit consisting of a battery, both switches are closed. The respective by a unique propositional state of each of the three dynamic components constant, where negation is denoted by a bar on top of the respective symbol. involved is described two switches, and a light bulb, which is on if and only if expected the intuitively the implication SWI A SW:! > light. However, despite result, the mere knowledge of the connection between this just mentioned, which includes being the switches and the bulb is not sufficient: Notice that the above formula, SWI ASW~ = light, that instead of the light also entails is that the second one being jumps turned on, the indirect effect of toggling its position-a sw1 A light > ~2, which suggests the first switch the implication result which contradicts our intuition. of merely taking into account formalizations The reason for the inadequacy logical as pure formulas-usually pendencies formulas do not include causal information. More precisely, -. sw2 IS clearly one observes false. However, exploiting true in any state and, therefore, contains evidential information, sw1 to be true and light to be false then it is safe to conclude called domain constraints-is the implication for reasoning about causality As a solution this implication to the second problem, we propose of de- that these swl A light > that is, if that sw2 is is misleading. 3 in the form of so-called cuusuZ relationships, which formalize statements to incorporate like causality A change of FFi to SWI causes a change of light to light, provided sw2 is true. the direct effects of executing an action in a particular state of the world, indirect effects one by one, to accommodate After computing we will apply suitable causal relationships, until a satisfactory each of which only caused by a direct one and also for indirect effects the relationship effects. To illustrate result obtains. Employing the latter, consider two particular relates effects, accounts a collection of single causal relationships, for several in turn causing indirect effects indirect further A change of light provided detector-activated is true to light causes a change of light-detector to light-detector, in addition specification an automated procedure information of a formal automatic generation to the one above. Since we do not expect the designer of a formal domain to create a complete set of suitable causal relationships, we will also present to extract them from given domain constraints plus some general influence other fluents. On the basis and their specifying which fluents may possibly theory of actions in Section 2)) causal relationships (to be defined are formally introduced in Section 3. 3 See [ 33 I for a general discussion on the different natures of causal and evidential implications. \f320 M. Ti~ielscher/Arti~cial Intelligence 89 (I 997) 317-364 Yet the purpose of this article is not only to suggest incorporating but also to supply evidence causal information the principle of categorization to cope with the ramification that something like problem. To this of existing paradigms aimed at and the policy of minimal that are fluents (e.g., a change of light is considered more that some resist any fluents is the assumption of minimal approaches distinguish trying namely, to change relationships is inevitable when the limited expressiveness by means of our causal this approach end, in Section 5 we illustrate ramifications, handling speaking, categorization-based change. Roughly likely more than other fluents than a change of sw2). However, we will show likely (Section",
            {
                "entities": [
                    [
                        67,
                        93,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 93 ( 1997) I69- 199 Artificial Intelligence Definability and commonsense reasoning Gianni Amati a**, Luigia Carlucci Aiello b,l, Fiora Pirri b-2 a Fo~dazione Ugo Bordoni, via 3alduss~~rre C~st~g~io~e 59, 00142 Roma, My h Dipurtjme~to di ~~farmat~ca e Sistemi~tjca, Universitii di Roma “La Sapienza”, via Suluria 1 J-3, 00198 Romu, Daly Received March 1996; revised September 1996 Abstract in commonsense is a central problem The definition of concepts in reasoning concern implicit and explicit definability. Implicit definability in non- nonmonotonic monotonic logic is always relative to the context-the current theory of the world. We show that fixed point equations provide a generalization of explicit definability, which correctly captures the relativized context. Theories expressed within this logical framework provide implicit definitions of concepts. Moreover, it is possible to derive these fixed points entirely within the logic. @ 1997 Elsevier Science B.V. reasoning. Many themes Kqw~rrds: Commonsense reasoning; Delinabiiity; Fixed points; Logic of provability; Default logic; Contextual reasoning; Self-reference 1. Introduction and motivations Concepts play a central role in commonsense of postulating in which the properties used as definientes are independent a definition of a concept, of the definiendum. reasoning. The classical view consists in terms of necessary and sufficient conditions, Positivists pointed out that if a concept cannot be defined via necessary and sufficient that there are concepts, do not seem to have a common core which it is not a scientific concept. noted by Wittgenstein-that It turns out, however conditions like game-as could be characterized by a set of necessary conditions (see [IS] for a discussion). * Corresponding author. E-mail: gba@fub.it. Work carried out in the framework of the agreement between the halian Ff Administration and the Fondazione Ugo Bordoni. ’ E-mail: aiello@dis.uniromal .it. *E-mail: pirri@dis.uniromal.it. 0004-3702/97/$17.00 @ 1997 Efsevier Science B.V. All rights reserved. PI1 s0004-3702(94)00049-5 \f170 G. Amati et al. /Art@cial Intelligence 93 (I 997) 169-199 natural kinds is one of the main proponents On the other hand, concepts involving necessary but not sufficient conditions. This point Eleanor Rosch view on “natural kinds”, Reiter [41] suggests resorting via some appeals patterns mirror of classical necessary conditions W and a set of “sufficient” default conditions D. like bird, lemon, etc., possess theory, where in prototype (see e.g. [ 321). In connection with this to sufficient conditions postulated “ typically”, or “assume by default”, which is relativized. And, in fact, these linguistic the role of defaults; namely, a default to which the definition linguistic pattern (lY D) accounts like “normally”, to the context is stressed for a set theory axiomatization in almost all cases reported Likewise, a circumscription definitions implicit sufficient condition tions) whereby amounts (by (e.g. [22,24,29] in the literature finding an equivalent ), solving theory) yields axiom acts like an (just as, for Reiter, the default rules act like sufficient condi- the concept. Solving for the predicates being minimized. the circumscription first-order for the predicate being minimized. The circumscription the theory is implicitly defining to finding an explicit definition logic theorem in first-order through Beth’s definability The notions of implicit and explicit definition of a predicate, with respect to a theory, [ 6]), from an implicit definition formula. in first-order are formalized which shows that the two notions are equivalent. Moreover, of a predicate an explicit definition can be found by constructing An immediate logic state we state that S t- V’x (RAVEN(X) = BLACK(X)), then talk about axioms, then by the monotonicity definition of the concept. Tarski discusses with definability if things we can for example by adding new the of Beth’s theorem of the predicate being defined. Therefore, logic, one cannot compatibly update consequence the uniqueness and those concerned with deduction in S are ravens. Whenever is that implicit definitions the connection between the notions concerned the context changes, the only black an interpolant for example, of first-order (see e.g. in (541. On the other hand, only “well-behaved” can be drawn. McCarthy’s circumscription we may lack either sufficient or necessary conditions no explicit definition strong behaviour of definability is weaker the explicit definition constraint because other words, there is no total commitment in first-order (see is achieved by minimization, than implicit definability it is relativized to define a concept; predicates are implicitly definable; in general in such cases this axiom [ 111 for a discussion). As noted above, since is no longer a strong In the circumscription [29] circumvents logic because is applied. uniqueness to the theory on which circumscription to the given definition. Consider, for example, the following theory T. ONTABLE(a)VONTABLE( b), vxRED(x) -ONTABLE( (1) The circumscription is separable (see [ 21]), that is of ONTABLE in T yields a first-order formula because ONTABLE (vxONTABLE(x) = (RED(x) i’X=U) AZ) v (vxONTABLE( x)E (FCED(x)VX= b) AZ’) (2) where Z and Z’ are formulae not containing ONTABLE (see [ 2 1, Theorem 1 ] ) \fG. Amati et al./Artijiicial fnielligence 93 (1997) 169-199 171 We note that in such a case we do not get an implicit definition as for it. Therefore we do not get of ONTABLE, to uniqueness. A first-order disjunction the lack of a classical explicit definition, a disju$z~tio~ of two de~nitio~s for the predicate ONTABLE. The shows is weaker that circumscription in Beth sense. Despite two minimal nonisomorphic models there are at least an explicit definition we have obtained example clearly and we are no longer committed is a nice generalization definition circumscription yield a very large disjunction Lin proposes a transformation state axioms. The interesting introducing the different effects of the performed action are realized. of successor that breaks contribution of definability, from weak of a theory, axiomatizing though implicit can be obtained the disjunction than Beth’s implicit definability of definitions it does not tell how a single explicit that the [23] conditions. Lin actions, may the effects of indeterminate this problem, state axioms. To overcome and yields different successor by in which is that his transfo~ation is perfo~ed shows a suitable predicate which is used, in a sense, to name the contexts As another example of how implicit and explicit definability that an approach reasoning, observe tonic negation as failure operator as a set of necessary ficient conditions shows how, in some cases, program. conditions yielding semantics to logic programming enters into nonmonotonic for the nonmono- is the Clark completion, which treats a logic program suf- and “completes” predicates. Reiter 1401 the this program by adding suitable is the result of circumscribing for the progr~ the Clark completion explicit definitions The upshot of the foregoing discussion implicit and explicit definability. of the definientes it seems clear how to weaken concern reasons for wanting explicit definitions, because by substitution However, while reasoning of how to get, in general, an explicit definition stated through a nonmonotonic one way to solve this problem. (via defauits or circumscription) is that many themes in nonmonotonic reasoning In fact there are very good computational theorem proving to be proved. then we can do efficient theorem in any for the definiendum implicit de~nability so far there is no solution from the implicit in nonmonotoni~ to the problem sufficient conditions theory. The main contribution of this paper is to show Returning to the above theory ( 1)) in order to obtain one of the disjuncts disregarding the others, we should be able to express sufficient conditions object a. Analogously contexts different context. should be the following: to a context C, the necessary and to be on the table are that either it is red or it is the is b. The role played by refer to a there is a context C’ where the object each definition in the disjunction for an object that, relative should To subsume a context of reasoning (or a context of discourse) state of affairs, we need a language that resorts a certain ability. That is, we need to formalize, about taken naturally hand. in that same language into account relies on a current is a cu~~~~nse~se in a theory axiomatizing to a kind of self-referential that can be reasoned in the language, expressions itself. A statement in which the context can be ex~~~~it~~ state~lent. And, in fact, commonsense reasoning state of affairs: the minimum resource of information at This form of relativization to the context to the belief set of an agent. So the self-referential statements is often carried out by binding commonsense ability is lifted from the \f172 G. Anzati et al. /Art+&1 Intelligence 93 (I 997) 169-l 99 language to one in which two distinct the one where the agent draws conclusions commonsense are fo~ulated: and general, in which the agent is reasoning. The following definition the conclusions structure, i.e., a computational is a metalogical the one where drawn are compared with (or even more) levels of reasoning from the initial assumptions the context, which, in object external to the language is, in this sense, paradigmatic: r E cn/, (I u (Off f -cd $8 F}) (3) f in the sense is a context, as “it is consistent the above schema the deductive closure-in theory then the nonmonotonic the above expression does not belong which says that if I is a nonmonotonic the logic n-of I are obtained by taking the formulae consistent with r, where Oa is, in fact, interpreted assume a in r”. Although belief set of an agent, When of discourse. One for the default compatible with Oa, and, f",
            {
                "entities": [
                    [
                        77,
                        115,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 242 (2017) 132–171Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintMaking friends on the fly: Cooperating with new teammates ✩Samuel Barrett a,∗,1, Avi Rosenfeld b, Sarit Kraus c,d, Peter Stone ea Cogitai, Inc., Anaheim, CA 92808, USAb Dept. of Industrial Engineering, Jerusalem College of Technology, Jerusalem, 9116001, Israelc Department of Computer Science, Bar-Ilan University, Ramat Gan, 5290002, Israeld Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USAe Dept. of Computer Science, The University of Texas at Austin, Austin, TX 78712, USAa r t i c l e i n f oa b s t r a c tArticle history:Received 28 February 2015Received in revised form 10 October 2016Accepted 17 October 2016Available online 21 October 2016Keywords:Ad hoc teamworkMultiagent systemsMultiagent cooperationReinforcement learningPursuit domainRoboCup soccerRobots are being deployed in an increasing variety of environments for longer periods of time. As the number of robots grows, they will increasingly need to interact with other robots. Additionally, the number of companies and research laboratories producing these robots is increasing, leading to the situation where these robots may not share a common communication or coordination protocol. While standards for coordination and communication may be created, we expect that robots will need to additionally reason intelligently about their teammates with limited information. This problem motivates the area of ad hoc teamwork in which an agent may potentially cooperate with a variety of teammates in order to achieve a shared goal. This article focuses on a limited version of the ad hoc teamwork problem in which an agent knows the environmental dynamics and has had past experiences with other teammates, though these experiences may not be representative of the current teammates. To tackle this problem, this article introduces a new general-purpose algorithm, PLASTIC, that reuses knowledge learned from previous teammates or provided by experts to quickly adapt to new teammates. This algorithm is instantiated in two forms: 1) PLASTIC-Model – which builds models of previous teammates’ behaviors and plans behaviors online using these models and 2) PLASTIC-Policy – which learns policies for cooperating with previous teammates and selects among these policies online. We evaluate PLASTIC on two benchmark tasks: the pursuit domain and robot soccer in the RoboCup 2D simulation domain. Recognizing that a key requirement of ad hoc teamwork is adaptability to previously unseen agents, the tests use more than 40 previously unknown teams on the first task and 7 previously unknown teams on the second. While PLASTIC assumes that there is some degree of similarity between the current and past teammates’ behaviors, no steps are taken in the experimental setup to make sure this assumption holds. The teammates were created by a variety of independent developers and were not designed to share any similarities. Nonetheless, the results show that PLASTIC was able to identify and exploit similarities between its current and past teammates’ behaviors, allowing it to quickly adapt to new teammates.© 2016 Elsevier B.V. All rights reserved.✩This article contains material from 4 prior conference papers [11–14].* Corresponding author.E-mail addresses: sam@cogitai.com (S. Barrett), rosenfa@jct.ac.il (A. Rosenfeld), sarit@cs.biu.ac.il (S. Kraus), pstone@cs.utexas.edu (P. Stone).1 This work was performed while Samuel Barrett was a graduate student at the University of Texas at Austin.http://dx.doi.org/10.1016/j.artint.2016.10.0050004-3702/© 2016 Elsevier B.V. All rights reserved.\fS. Barrett et al. / Artificial Intelligence 242 (2017) 132–1711331. IntroductionRobots are becoming cheaper and more durable and are therefore being deployed in more environments for longer periods of time. As robots continue to proliferate in this way, many of them will encounter and interact with a variety of other kinds of robots. In many cases, these interacting robots will share a set of common goals, in which case it will be desirable for them to cooperate with each other. In order to effectively perform in new environments and with changing teammates, they should observe their teammates and adapt to achieve their shared goals. For example, after a disaster, it is helpful to use robots to search the site and rescue survivors. However, the robots may come from a variety of sources and may not be designed to cooperate with each other, such as in the response to the 2011 Tohoku earthquake and tsunami [43,55,56,58]. If these robots are not pre-programmed to cooperate, they may not share information about which areas have been searched; or worse, they may unintentionally impede their teammates’ efforts to rescue survivors. Therefore, in the future, it is desirable for robots to be designed to observe their teammates and adapt to them, forming a cohesive team that quickly searches the area and rescues the survivors.This idea epitomizes the spirit of ad hoc teamwork. In ad hoc teamwork settings, agents encounter a variety of teammates and try to accomplish a shared goal. In ad hoc teamwork research, researchers focus on designing a single agent or subset of agents that can cooperate with a variety of teammates. The desire is for agents designed for ad hoc teamwork to quickly learn about these teammates and determine how they should act on this new team to achieve their shared goals. Agents that reason about ad hoc teamwork will be robust to changes in teammates in addition to changes in the environment. This article focuses on a limited version of the ad hoc teamwork problem. Specifically, this article investigates how an agent should adapt to new teammates given that it has previously interacted with other teammates and learned from these interactions. However, these past interactions may not be representative of the current teammates.In this article, the word “agent” refers to an entity that repeatedly senses its environment and takes actions that affect this environment, shown visually in Fig. 1a. As a shorthand, the terms ad hoc team agent and ad hoc agent are used in this article to refer to an agent that reasons about ad hoc teamwork. The environment includes the dynamics of the world the agent interacts with, as well as defining the observations received by the agent. We treat the other agents in the domain as teammates because they share a set of common goals; they are fully cooperative in the terminology of game theory.Previous work on teamwork has largely assumed that all agents in the domain will act as a unified team and are designed to work with their specific teammates [25,36,66,68]. Methods for coordinating multiagent teams largely rely on specifying standardized protocols for communication as well as shared algorithms for coordination. These approaches do not directly apply to ad hoc teams due to their strong assumptions about this sharing of prior knowledge, which is violated in the ad hoc teamwork scenario. This view of multiagent teams is shown in Fig. 1b.On the other hand, this article will focus on creating a single agent that cooperates with teammates coming from a variety of sources without directly altering the behavior of these teammates. However, all of the agents still share a set of common goals, so it is desirable for them to act as a team. In addition, rather than focusing on a single task, these agents may face a variety of tasks, where a task refers to both the environment other than the team’s agents as well as the team’s shared goals.The differences of this article from prior work are presented visually in Fig. 1. Another existing area of research into how agents should behave is reinforcement learning (RL). Generally, RL problems revolve around a single agent learning by interacting with its environment. In RL problems, agents receive sparse feedback about the quality of sequences of actions. Generally, RL algorithms either model other agents as part of the environment and try to learn the best policy for the single agent given this environment or they consider the case where the whole team is under a single designer’s control. In addition, RL algorithms usually learn from scratch in each new environment, ignoring information coming from previous environments. However, there is a growing body of work on applying transfer learning to RL to allow agents to reuse prior experiences on new domains [69]. Fig. 1a shows the standard RL view of an agent interacting with its environment. Fig. 1b represents a common multiagent view of a unified team interacting with the environment where the agents model their teammates as being separate from the environment. In this case, the team is designed before being deployed to cooperate with these specific agents to interact with a fixed environment. However, these agents rely on knowing their teammates and usually require an explicit communication and/or coordination protocol to be shared among the whole team [36,53,73]. On the other hand, this article will focus on ad hoc teams drawn from a set of possible teammates, where the team tackles a variety of possible environments as shown in Fig. 1c. In this case, the teammates are not programmed to cooperate with this specific ad hoc agent, and they must be treated as given and inalterable. Instead, this research focuses on enabling the ad hoc agent to cooperate with a variety of teammates in a range of possible environments.In an ad hoc team, agents need to be able to cooperate with a variety of previously unseen teammates. Rather than developing protocols for coordinating an entire team, ad hoc team research focuses on developing agents that cooperate with teammates in the absence of such explicit protocols. Therefore, we consider a single agent cooperating with teammates that may or may not adapt to its behavior. In this scenario, w",
            {
                "entities": [
                    [
                        136,
                        193,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 951–984www.elsevier.com/locate/artintMetatheory of actions: Beyond consistencyAndreas Herzig, Ivan Varzinczak ∗IRIT – Université Paul Sabatier, 118 route de Narbonne, 31062, Toulouse Cedex 9, FranceReceived 25 April 2006; received in revised form 15 March 2007; accepted 23 April 2007Available online 29 April 2007AbstractTraditionally, consistency is the only criterion for the quality of a theory in logic-based approaches to reasoning about actions.This work goes beyond that and contributes to the metatheory of actions by investigating what other properties a good domaindescription should have. We state some metatheoretical postulates concerning this sore spot. When all postulates are satisfied wecall the action theory modular. Besides being easier to understand and more elaboration tolerant in McCarthy’s sense, modulartheories have interesting properties. We point out the problems that arise when the postulates about modularity are violated, andpropose algorithmic checks that can help the designer of an action theory to overcome them.© 2007 Elsevier B.V. All rights reserved.Keywords: Reasoning about actions; Action theory; Modularity; Ramifications1. IntroductionIn logic-based approaches to knowledge representation, a given domain is described by a set of logical formulasT , which we call a (non-logical) theory. That is also the case for reasoning about actions, where we are interested intheories describing particular actions (or, more precisely, action types). We call such theories action theories.A priori consistency is the only criterion that formal logic provides to check the quality of such descriptions. Inthe present work we go beyond that, and argue that we should require more than the mere existence of a model for agiven theory.Our starting point is the fact that in reasoning about actions one usually distinguishes several kinds of logicalformulas. Among these are effect axioms, precondition axioms, and boolean axioms. In order to distinguish such non-logical axioms from logical axioms, we prefer to speak of effect laws, executability laws, and static laws, respectively.Moreover we single out those effect laws whose effect is ⊥, and call them inexecutability laws.Given these types of laws, suppose the language is powerful enough to state conditional effects of actions. Forexample, suppose that action a is inexecutable in contexts where ϕ1 holds, and executable in contexts where ϕ2 holds.It follows that there can be no context where ϕ1 ∧ ϕ2 holds. Now ¬(ϕ1 ∧ ϕ2) is a static law that does not mention a.It is natural to expect that ¬(ϕ1 ∧ ϕ2) follows from the static laws alone. By means of examples we show that when* Corresponding author.E-mail addresses: herzig@irit.fr (A. Herzig), ivan@irit.fr (I. Varzinczak).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.013\f952A. Herzig, I. Varzinczak / Artificial Intelligence 171 (2007) 951–984this is not the case, then unexpected conclusions might follow from the theory T , even in the case T is logicallyconsistent.This motivates postulates requiring that the different laws of an action theory should be arranged modularly, i.e.,in separated components, and in such a way that interactions between them are limited and controlled. In essence,we argue that static laws may entail new effects of actions (that cannot be inferred from the effect laws alone), whileeffect laws and executability laws should never entail new static laws that do not follow from the set of static lawsalone. We here formulate postulates that make these requirements precise. It will turn out that in all existing accountsthat allow for these four kinds of laws [1–6], consistent action theories can be written that violate these postulates.In this work we give algorithms that allow one to check whether an action theory satisfies the postulates or not. Withsuch algorithms, the task of correcting flawed action theories can be made easier.Although we here use the syntax of propositional dynamic logic (PDL) [7], all we shall say applies as well tofirst-order formalisms, in particular to the Situation Calculus [8]. All postulates we are going to present can be statedas well for other frameworks, in particular for action languages such as A, AR [9–11] and others, and for SituationCalculus based approaches. In [12] we have given a Situation Calculus version of our analysis, while in [13] wepresented a similar notion for ontologies in Description Logics [14]. The present work is the complete version of theone first appeared in [15].This text is organized as follows: after some background definitions (Section 2) we state some postulates concerningaction theories (Section 3). In Sections 4 and 5, we study the two most important of these postulates, giving algorithmicmethods to check whether an action theory satisfies them or not. We then generalize our postulates (Section 6) anddiscuss possible strengthening of them (Section 7). In Section 8 we show interesting features of modular actiontheories. Before concluding, we assess related work found in the literature on metatheory of actions (Section 9).2. Preliminaries2.1. Dynamic logicHere we establish an ontology of dynamic domains. As our base formalism we use ∗-free PDL, i.e., PDL withoutthe iteration operator ∗. For more details on PDL, see [7,16].Let Act = {a1, a2, . . .} be the set of all atomic action constants of a given domain. Our running example is in termsof the Walking Turkey Scenario [4]. There, the atomic actions are load, shoot and tease. We use a as a variable foratomic actions. To each atomic action a there is an associated modal operator [a]. Here we suppose that the underlyingmultimodal logic is independently axiomatized (i.e., the logic is a fusion and there is no interaction between the modaloperators [17,18]).Prop = {p1, p2, . . .} denotes the set of all propositional constants, also called fluents or atoms. Examples of thoseare loaded, alive and walking. We use p as a variable for propositional constants.We here suppose that both Act and Prop are nonempty and finite.We use small Greek letters ϕ, ψ, . . . to denote classical formulas, also called boolean formulas. They are recursivelydefined in the following way:ϕ ::= p | (cid:5) | ⊥ | ¬ϕ | ϕ ∧ ϕ | ϕ ∨ ϕ | ϕ → ϕ | ϕ ↔ ϕ.Fml is the set of all classical formulas.Examples of classical formulas are walking → alive and ¬(bachelor ∧ married).A classical formula is classically consistent if there is at least one valuation in classical propositional logic thatmakes it true. Given ϕ ∈ Fml, valuations(ϕ) denotes the set of all valuations of ϕ. We note |=CPL the logical conse-quence in classical propositional logic.The set of all literals is Lit = Prop ∪ {¬p: p ∈ Prop}. Examples of literals are alive and ¬walking. (cid:4) will be usedas a variable for literals. If (cid:4) = ¬p, then we identify ¬(cid:4) with p.A clause χ is a disjunction of literals. We say that a literal (cid:4) appears in a clause χ , written (cid:4) ∈ χ , if (cid:4) is a disjunctof χ .We denote complex formulas (possibly with modal operators) by capital Greek letters Φ1, Φ2, . . . They are recur-sively defined in the following way:Φ ::= ϕ | [a]Φ | (cid:11)a(cid:12)Φ | ¬Φ | Φ ∧ Φ | Φ ∨ Φ | Φ → Φ | Φ ↔ Φ\fA. Herzig, I. Varzinczak / Artificial Intelligence 171 (2007) 951–984953where Φ denotes a complex formula. (cid:11)a(cid:12) is the dual operator of [a], defined by: (cid:11)a(cid:12)Φ =def ¬[a]¬Φ. Sequentialcomposition of actions is defined by the abbreviation [a1; a2]Φ =def [a1][a2]Φ. Examples of complex formulas areloaded → [shoot]¬alive and hasGun → (cid:11)load; shoot(cid:12)(¬alive ∧ ¬loaded).If T is a set of formulas (modal or classical), atm(T ) returns the set of all atoms occurring in T . For instance,atm({¬¬¬p1, [a]p2}) = {p1, p2}.For parsimony’s sake, whenever there is no confusion we identify a set of formulas with the conjunction of itselements. The semantics is that for multimodal K [19,20].Definition 1. A PDL-model is a tuple M = (cid:11)W, R(cid:12) where W is a set of valuations (alias possible worlds), andR : Act −→ 2W ×W a function mapping action constants a to accessibility relations Ra ⊆ W × W .As an example, for Act = {a1, a2} and Prop = {p1, p2}, we have the PDL-model M = (cid:11)W, R(cid:12), whereW =(cid:2){p1, p2}, {p1, ¬p2}, {¬p1, p2}(cid:3),(cid:4)(cid:2)(cid:6)R(a1) =R(a2) =({p1, p2}, {p1, ¬p2}), ({p1, p2}, {¬p1, p2}),({¬p1, p2}, {¬p1, p2}), ({¬p1, p2}, {p1, ¬p2})(cid:7)(cid:3)(cid:6){p1, ¬p2}, {p1, ¬p2}{p1, p2}, {p1, ¬p2}.(cid:7),(cid:5),Fig. 1 gives a graphical representation of M.Given M = (cid:11)W, R(cid:12), a ∈ Act, and w, w(cid:14) ∈ W , we write Ra instead of R(a), and wRaw(cid:14) instead of w(cid:14) ∈ Ra(w).Definition 2. Given a PDL-model M = (cid:11)W, R(cid:12), the satisfaction relation is defined as the smallest relation satisfying:w p (p is true at world w of model M) if p ∈ w;• |=M[a]Φ if for every w(cid:14) such that wRaw(cid:14), |=M• |=Mw• the usual truth conditions for the other connectives.w(cid:14) Φ; andDefinition 3. A PDL-model M is a model of Φ (noted |=M Φ) if and only if for all w ∈ W , |=Ma set of formulas T (noted |=M T ) if and only if |=M Φ for every Φ ∈ T .w Φ. M is a model ofIn the model depicted in Fig. 1, we have |=M p1 → [a2]¬p2 and |=M p1 ∨ p2.Definition 4. A formula Φ is a consequence of the set of global axioms T in the class of all PDL-models (notedT |=PDL Φ) if and only if for every PDL-model M, if |=M T , then |=M Φ.1We here suppose that the logic under consideration is compact [21].Fig. 1. Example of a PDL-model for Act = {a1, a2}, and Prop = {p1, p2}.1 Instead of global consequence, in [5] local consequence is considered. For that reason, a further modal operator (cid:2) had to be introduced, givinga logic that is multimodal K plus monomodal S4 for (cid:2), and where axiom schema (cid:2)Φ → [a]Φ holds.\f954A. Herzig, I. Varzinczak / Artificial Intelligence 171 (2007) 951–98",
            {
                "entities": [
                    [
                        72,
                        113,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "llOPEN ACCESSPerspectiveMemetics and neural models of conspiracy theoriesW1odzis1aw Duch1,*1Department of Informatics, Faculty of Physics, Astronomy and Informatics, and Neurocognitive Laboratory, Center for ModernInterdisciplinary Technologies, Nicolaus Copernicus University, Toru(cid:1)n, Poland*Correspondence: wduch@umk.plhttps://doi.org/10.1016/j.patter.2021.100353THE BIGGER PICTURE Conspiracy theories are widespread. So far, research in this area has been focusedon psychological, sociological, and political science perspectives. Brain processes facilitating formation ofconspiracy theories are largely unknown. In neural systems, a meme may be represented by a quasi-stableassociative memory network attractor state. Creation of memes with numerous fake associations distorts re-lations between stable memory states. Simulations of neural network models trained with competitiveHebbian learning (CHL) on stationary and non-stationary input data show the formation of distorted memorystates. In non-stationary situations, rapid learning with high plasticity followed by stepwise decrease of plas-ticity leads to many states with overlapping attraction basins, distorting patterns in associative memory.Such system-level models may be used to understand conditions under which memplexes with distortedmemory patterns arise, representing deeply settled conspiracy beliefs.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYMemetics has so far been developing in social sciences, but to fully understand memetic processes it shouldbe linked to neuroscience models of learning, encoding, and retrieval of memories in the brain. Attractor neu-ral networks show how incoming information is encoded in memory patterns, how it may become distorted,and how chunks of information may form patterns that are activated by many cues, forming the foundation ofconspiracy theories. The rapid freezing of high neuroplasticity (RFHN) model is offered as one plausiblemechanism of such processes. Illustrations of distorted memory formation based on simulations of compet-itive learning neural networks are presented as an example. Linking memes to attractors of neurodynamicsshould help to give memetics solid foundations, show why some information is easily encoded and propa-gated, and draw attention to the need to analyze neural mechanisms of learning and memory that lead toconspiracies.INTRODUCTIONConspiracy theories are part of a much wider subject: formationof beliefs, creation of memes, distorted memories, twistedworldviews, or in general investigating ways in which learningfails to represent the data faithfully. In recent article by Seitzand Angel ‘‘Belief formation – A driving force for brain evolu-tion,’’1 the authors write: ‘‘The topic of belief has been neglectedin the natural sciences for a long period of time’’. They dividebeliefs into empirical, relational, and conceptual, discussinglarge brain areas involved in the formation of beliefs. Bayesianmodels of belief propagation are used to model details ofperceptual processes and relate them to connectomes.2 Theartificial neural network community has focused on faithfullearning methods, but there is another, neglected side of learningand memory formation. When the training data are not learnedperfectly, what types of errors may one expect, and how willthey influence the performance of an artificial system? Can anal-ysis of artificial systems help to understand how biological brainslearn incoming information, transforming it into memes that arelikely to be transmitted in a distorted form to other brains? Theworld view that we use to guide our behavior is based on anetwork of associative memory states. Consolidation of newmemory states in the neocortex may occur quite quickly if theyare well connected to other memory states.3 Several lines ofresearch lead to this conclusion: animal studies, association ofplaces with items in mnemotechnics, behavioral studies on theuse of schemas for rapid learning, and building of cognitivemaps. Neural models of schemas and sequences of associa-tions may be based on attractor states in neural networks.4Each episodic or semantic memory state is based on activationsof synchronized, distributed networks of brain regions. It isPatterns 2, November 12, 2021 ª 2021 The Author(s). 1This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\fllOPEN ACCESSencoded in relation to the existing activation patterns and maybe modified when new patterns are learned.internalUsing functional magnetic resonance imaging (fMRI) evokedby natural movies, Huth et al.5,6 have created a ‘‘semantic atlas,’’showing patterns of brain activations for categories of hundredsof objects and actions. These patterns are evoked by stimuli thatprovide sufficient cues to recall specific objects, such as bodyparts, animals, furniture, or types of actions. This process maybe described using the language of dynamical systems for net-works of elements representing neurons. The Hopfield network7was the simplest associative memory model encoding informa-tion in activation patterns of network nodes. In such recurrentnetworks,feedback changes activity patterns withtime; this process is referred to as neurodynamics. All kinds ofmemory states (semantic, episodic, procedural, and working)are called attractors of neurodynamics4 because initial patternsof neural network activations are attracted by the network dy-namics toward one of the quasi-stable memory patterns. Usuallyonly a small subset of neurons are highly active in each pattern,synchronizing their activity sending signals through strongIn biologically motivated attractor net-mutual connections.works, memory states are not stable, and neural noise, fatigue,and other processes lead to desynchronization, decrease activ-ity of some neurons, and recruit others, forming different neuralpatterns. Transitions between neural patterns define trajectoryof brain state changes in the space of neural activations. In arti-ficial systems we can visualize it to observe neurodynamics ofmodel networks8 and transform it to dimensions that are mean-ingful at the mental level.9 fMRI scans provide snapshots of thewhole brain activation with temporal resolution of about 1 sand spatial resolution of about 1 mm, while measurement ofelectric potentials using electroencephalographic or magneto-encephalographic techniques provides millisecond temporalresolution but spatial resolution that is less than 1 cm.Seitz et al.10 presented a general theoretical model of forma-tions of empirically grounded and metaphysical beliefs. In theirview,the process of attraction is described by the verb‘‘believing,’’ and the endpoint, the final activation quasi-stablestate, is called a ‘‘belief’’ and is interpreted as a mental construct.Beliefs are based on sensory perception and attribution of a per-sonal value in an emotionally loaded process. High-level formularelates beliefs to incoming signals, ambient noise, current andprevious valuation, learning, and prediction errors. Changes ofneural activation in real brains depend on current knowledgeschemas, history of previous activations (priming), generalemotional state, specific context cues that invoke memories,and many other factors. Transitions that happen frequently in-crease probability of association between different activationpatterns11 and may not only create strong associations butdistort or even completely blend different memories, creatingfalse memories.12,13 Understanding abnormal belief formationin neuropsychological disorders is an important challenge,14but neuropsychiatry needs precise hypotheses and models atthe level of neural networks.In some cases, memories may become easily activated invarious contexts, leading to false associations and schemasthat develop into conspiracy theories. While there is a largebody of literature on conspiracy theories written by historians,philosophers, psychologists, sociologists, or political scientists2 Patterns 2, November 12, 2021Perspective(Routledge has a whole series of books on conspiracy theories;see also the review by Douglas et al.15), our understanding of thebrain mechanisms is completely lacking. The best explanationsthat we have relate beliefs in conspiracy theories to personalitytraits, mental disorders, or the need to find a simple satisfactoryexplanation.Memetics, introduced in the 1976 book The Selfish Gene byRichard Dawkins,16 tried to explain cultural information transferand persistence of certain ideas in societies. Memes may be un-derstood as sequences or information structures that tend toreplicate in a society. Despite great initial popularity of memeticideas, and the desperate need of mathematical theories to un-derpin social science, theories connecting neuroscience andmemetics have never been developed. The Journal of Memeticswas discontinued in 2005 after 8 years of electronic publishing.Memetic ideas were relegated into a set of vague philosophicaland psychological concepts of little interest to neuroscience. Inevolutionary computing, memetic ideas have inspired manynew developments, combining global search with focus on inter-esting local regions.17 The Memetic Computing journal was es-tablished in 1989, a whole series of books on Advances inMemetic Algorithms. Studies in Fuzziness and Soft Computingis published by Springer. Research on memetic computing isfocused on optimization problems, while here we are interestedin the process of formation of memories.The lack of efforts to understand distortions of informationtransmission and memory storage in biological learning systemsis certainly related to the lack of theoretical models, and to theexperimental difficulties in searching for memes in brain activity.McNamara18 has argued that neuroimaging technology may beused to trace memes in the brain and to measure how theychange over time. Following Heylighen and Chie",
            {
                "entities": [
                    [
                        24,
                        73,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Understanding common human driving semanticsfor autonomous vehiclesArticleHighlightsd Reveal human auditory cortex activation during drivingd Discover the hierarchical structure of human drivingunderstandingd Propose a neural-informed semantics-driven drivingunderstanding modeld Address long-term contextual dependency of drivingbehaviorsAuthorsYingji Xia, Maosi Geng, Yong Chen, ...,Bing Zhang, Ziyou Gao,Xiqun (Michael) ChenCorrespondencechenxiqun@zju.edu.cnIn briefAutonomous vehicles will share roadswith human-driven vehicles and bringwith them problems regardingbidirectional understanding of drivingbehavior. Based on cerebral neurologicalfindings from the human process forunderstanding driving, a novel neural-inspired semantics-driven drivingunderstanding model is proposed forautonomous vehicles. The model imitatesthe way humans understand driving andcan interpret long-term driving behaviorevolutions like human drivers.Xia et al., 2023, Patterns 4, 100730June 9, 2023 ª 2023 The Author(s).https://doi.org/10.1016/j.patter.2023.100730ll\fPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSArticleUnderstanding common human drivingsemantics for autonomous vehiclesYingji Xia,1 Maosi Geng,1,2 Yong Chen,1 Sudan Sun,3 Chenlei Liao,1 Zheng Zhu,1,4,10 Zhihui Li,5Washington Yotto Ochieng,6 Panagiotis Angeloudis,6 Mireille Elhajj,6 Lei Zhang,7 Zhenyu Zeng,7 Bing Zhang,7 Ziyou Gao,8and Xiqun (Michael) Chen1,4,9,10,11,*1Institute of Intelligent Transportation Systems, College of Civil Engineering and Architecture, Zhejiang University, Hangzhou 310058, China2Polytechnic Institute & Institute of Intelligent Transportation Systems, Zhejiang University, Hangzhou 310015, China3School of Medicine, Zhejiang University, Hangzhou 310058, China4Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, Hangzhou 310027, China5School of Transportation, Jilin University, Changchun 130022, China6Department of Civil and Environmental Engineering, Imperial College London, South Kensington Campus, London SW7 2AZ, UK7Alibaba Group, Hangzhou 310052, China8School of Traffic and Transportation, Beijing Jiaotong University, Beijing 100044, China9Zhejiang University/University of Illinois Urbana-Champaign (ZJU-UIUC) Institute, Zhejiang University, Haining 314400, China10Zhejiang Provincial Engineering Research Center for Intelligent Transportation, Hangzhou 310058, China11Lead contact*Correspondence: chenxiqun@zju.edu.cnhttps://doi.org/10.1016/j.patter.2023.100730THE BIGGER PICTURE ‘‘Driving like humans’’ is the ultimate goal of autonomous driving. Hence, human-like driving understanding ability is required for autonomous vehicles to better understand the driving be-haviors of surrounding human-driven vehicles. In this study, we investigated human driving neural responseand subsequently built a biologically plausible model to interpret driving behaviors like humans. This studypioneers the design of bio-inspired, human-like autonomous vehicles and can ultimately benefit futureresearch of human-machine interactions.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYAutonomous vehicles will share roads with human-driven vehicles until the transition to fully autonomoustransport systems is complete. The critical challenge of improving mutual understanding between bothvehicle types cannot be addressed only by feeding extensive driving data into data-driven models but byenabling autonomous vehicles to understand and apply common driving behaviors analogous to humandrivers. Therefore, we designed and conducted two electroencephalography experiments for comparingthe cerebral activities of human linguistics and driving understanding. The results showed that driving acti-vates hierarchical neural functions in the auditory cortex, which is analogous to abstraction in linguistic un-derstanding. Subsequently, we proposed a neural-informed, semantics-driven framework to understandcommon human driving behavior in a brain-inspired manner. This study highlights the pathway of fusingneuroscience into complex human behavior understanding tasks and provides a computational neural modelto understand human driving behaviors, which will enable autonomous vehicles to perceive and think like hu-man drivers.INTRODUCTIONAutonomous vehicles (AVs) continue to receive significant atten-tion worldwide because they have the potential to realize a safer,faster, and more efficient mode of transportation. Every day,almost 2,700 people are killed globally in traffic crashes6; fataland non-fatal crash injuries are estimated to cost approximately1.8 trillion US dollars between 2015 and 2030.7 By shiftingvehicle control from the human driver to machines via AVs,driver-related road crashes can be eliminated and thus savePatterns 4, 100730, June 9, 2023 ª 2023 The Author(s). 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSACBDArticleEFigure 1. Four-stage development to understand the driving behavior of AVs(A) The surrounding vehicles are considered moving obstacles without self-consciousness.(B) The velocity of the surrounding vehicles is predicted using probability distribution outputs of discrete choice models.(C) The potential maneuvers of surrounding vehicles are surmised by recurrently applying short-time trajectory prediction models.(D) The intentions of the surrounding vehicles are understood from their contextual driving behaviors.(E) The mechanistic and biological requirements for AV development.lives.8 However, until the transition to fully autonomous transportis completed, AVs will inevitably share roads with human-drivenvehicles. During this transitory phase, AVs and human-driven ve-hicles need to share mutually interactive behaviors.9,10 Given thiscontext, it is impossible to expect every human driver to accom-modate certain traits and attributes of AVs, such as inconsistentor stilted driving behaviors (e.g., aggressive car following, jerk-ing, sudden braking, or unexpected mandatory lane changing).Existing studies have revealed that a lack of transparency inAV decision making creates a psychological barrier that affectshuman drivers’ trust in AVs11; human drivers expect AVs tomimic their driving behaviors to become trustworthy.12 A moreplausible approach is for AVs to acquire the ability to drive likehuman drivers, which would make it easier for other road usersto interpret their driving behaviors and react appropriately. Thiswould subsequently rebuild the driver’s trust and increase thesocial acceptability of AVs.13,14In recent years, various types of AVs were developed andtested in urban road scenarios, and they yielded promising re-sults and applications.15–17 Given that vehicular sensing andnavigation technologies are relatively mature,18 major concernswith AV adoption are related to whether AVs can interact appro-priately with the human-driven vehicles in the surrounding areas.However, research studies on understanding common drivingbehaviors and designing AVs to operate while following hu-man-like principles or brain-inspired mechanisms remain lack-ing. Therefore, we developed a method to understand AV drivingbehaviors as shown in Figure 1, where the red vehicle representsan AV and the blue vehicles represent the surrounding human-driven vehicles in a typical driving scenario.Unlike the classical vehicular trajectory prediction and routeplanning models19–21 widely accepted in the robotics field(Figure 1A) or the various discrete-choice driving models22,232 Patterns 4, 100730, June 9, 2023developed in the traffic engineering discipline (Figure 1B), recentresearch24–26 showed that subjective individual human drivingfactors are critical and cannot be neglected in the developmentof human-like AVs (Figure 1C). Unfortunately, as human drivingbehaviors evolve with an indefinite temporal dependency,state-of-the-art machine learning-based methods that partiallyimitate the nature of the human driving decision-making processmay lose the ability to adapt and generalize. For example, stand-alone data-driven intention recognizers (e.g., deep neural net-works) employed in machine learning-based methods are likelyto be trapped in the following dilemma: those with increasedtemporal inputs carry a more significant risk of overfitting localfeatures and the output confusing driving intentions, whereasthose with shorter temporal dependencies are too myopic to fullyunderstand the driving intentions of surrounding drivers, such ashuman drivers.Understanding common human driving behaviors that followthe human cerebral driving thinking mechanisms of humandrivers (Figure 1D) is necessary to address this dilemma.27–29As human driving behaviors are generated by humans ratherthan machines, AV development needs to be mechanisticallyand biologically plausible (Figure 1E).Our research is motivated by the fact that talking while drivingcan cause severe distractions because both behaviors or activ-ities share the same cerebral resources30,31 (right parietal re-sources32 and working memory in the prefrontal cortex33,34).Therefore, we attempt to fuse neuroscience and robotics in aneuroengineering manner35 in this study. To this end, we de-signed two separate electroencephalography (EEG) experi-ments to reveal the formation of cerebral driving thinking andevolution using well-studied linguistic analyses. We subse-quently present a semantics-driven method to understand com-mon driving behaviors for developing AVs in a brain-inspired\fPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous ",
            {
                "entities": [
                    [
                        1101,
                        1169,
                        "TITLE"
                    ],
                    [
                        5162,
                        5230,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "1 Three practical field normalised alternative indicator formulae for research evaluation1 Mike Thelwall, Statistical Cybermetrics Research Group, University of Wolverhampton, UK. Although altmetrics and other web-based alternative indicators are now commonplace in publishers’ websites, they can be difficult for research evaluators to use because of the time or expense of the data, the need to benchmark in order to assess their values, the high proportion of zeros in some alternative indicators, and the time taken to calculate multiple complex indicators. These problems are addressed here by (a) a field normalisation formula, the Mean Normalised Log-transformed Citation Score (MNLCS) that allows simple confidence limits to be calculated and is similar to a proposal of Lundberg, (b) field normalisation formulae for the proportion of cited articles in a set, the Equalised Mean-based Normalised Proportion Cited (EMNPC) and the Mean-based Normalised Proportion Cited (MNPC), to deal with mostly uncited data sets, (c) a sampling strategy to minimise data collection costs, and (d) free unified software to gather the raw data, implement the sampling strategy, and calculate the indicator formulae and confidence limits. The approach is demonstrated (but not fully tested) by comparing the Scopus citations, Mendeley readers and Wikipedia mentions of research funded by Wellcome, NIH, and MRC in three large fields for 2013-2016. Within the results, statistically significant differences in both citation counts and Mendeley reader counts were found even for sets of articles that were less than six months old. Mendeley reader counts were more precise than Scopus citations for the most recent articles and all three funders could be demonstrated to have an impact in Wikipedia that was significantly above the world average. 1 Introduction Citation analysis is now a standard part of the research evaluation toolkit. Citation-based indicators are relatively straightforward to calculate and are inexpensive compared to peer review. Cost is a key issue for evaluations designed to inform policy decisions because these tend to cover large numbers of publications but may have a restricted budget. For example, reports on government research policy or national research performance can include citation indicators (e.g., Elsevier, 2013; Science-Metrix, 2015), as can programme evaluations by research funders (Dinsmore, Allen, & Dolby, 2014). Although funding programme evaluations can be conducted by aggregating end-of-project reviewer scores (Hamilton, 2011), this does not allow benchmarking against research funded by other sources in the way that citation counts do. The increasing need for such evaluations is driven by a recognition that public research funding must be accountable (Jaffe, 2002) and for charitable organisations to monitor their effectiveness (Hwang & Powell, 2009). The use of citation-based indicators has many limitations. Some well discussed issues, such as the existence of negative citations, systematic failures to cite important influences and field differences (MacRoberts & MacRoberts, 1996; Seglen, 1998; MacRoberts & MacRoberts, 2010), can be expected to average out when using appropriate indicators and comparing large enough collections of articles (van Raan, 1998). Other 1 Thelwall, M. (2017). Three practical field normalised alternative indicator formulae for research evaluation. Journal of Informetrics, 11(1), 128–151. doi: 10.1016/j.joi.2016.12.002 ©2016 This manuscript version is made available under the CC-BY-NCND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/                             \f2 problems are more difficult to deal with, such as language biases within the citation databases used for the raw data (Archambault, Vignola-Gagne, Côté, Larivière, & Gingras, 2006; Li, Qiao, Li, & Jin, 2014). More fundamentally, the ultimate purpose of research, at least from the perspective of many funders, is not to understand the world but to help shape it (Gibbons, Limoges, Nowotny, Schwartzman, Scott, & Trow, 1994). An important limitation of citations is therfore that they do not directly measure the commercial, cultural, social or health impacts of research. This has led to the creation and testing of many alternative types of indicators, such as patent citation counts (Jaffe, Trajtenberg, & Henderson, 1993; Narin, 1994), webometrics/web metrics (Thelwall, & Kousha, 2015a) and altmetrics/social media metrics (Priem, Taraborelli, Groth, & Neylon, 2010; Thelwall, & Kousha, 2015b). These indicators can exploit information created by non-scholars, such as industrial inventors’ patents, and may therefore reflect non-academic types of impacts, such as commercial value. A practical problem with many alternative indicators (i.e., those not based on citation counts) is that there is no simple cheap source for them. It can therefore be time-consuming or expensive for organisations to obtain, say, a complete list of the patent citation counts for all of their articles. This problem is exacerbated if an organisation needs to collect the same indicators for other articles so that they can benchmark their performance against the world average or against other similar organisations. Even if the cost is the same as for citation counts, alternative indicators need to be calculated in addition to, rather than instead of, citation counts (e.g., Dinsmore, Allen, & Dolby, 2014; Thelwall, Kousha, Dinsmore, & Dolby, 2016) and so their costs can outweigh their value. This can make it impractical to calculate a range of alternative indicators to reflect different types of impacts, despite this seeming to be a theoretically desirable strategy. The problem is also exacerbated by alternative indicator data usually being much sparser than citation counts (Kousha & Thelwall, 2008; Thelwall, Haustein, Larivière, & Sugimoto, 2013; Thelwall & Kousha, 2008). For example, in almost all Scopus categories, over 90% of articles have no patent citations (Kousha & Thelwall, in press-b). These low values involved make it more important to use statistical methods to detect whether differences between groups of articles are significant. Finally, the highly skewed nature of citation counts and most alternative indicator data causes problems with simple methods of averaging to create indicators, such as the arithmetic mean, and complicate the task of identifying the statistical significance of differences between groups of articles. This article addresses the above problems and introduces a relatively simple and practical strategy to calculate a set of alternative indicators for a collection of articles in an informative way. The first component of the strategy is the introduction of a new field normalisation formula, the Mean Normalised Log-transformed Citation Score (MNLCS) for benchmarking against the world average. As argued below, this is simpler and more coherent than a previous similar field normalisation approach to deal with skewed indicator data. The second component is the introduction of a second new field normalisation formula, the Equalised Mean-based Normalised Proportion Cited (EMNPC), that targets sparse data, and an alternative, the Mean-based Normalised Proportion Cited (MNPC). The third component is a simple sampling strategy to reduce the amount of data needed for effective field normalisation. The final component integrated software environment for collecting and analysing the data so that evaluators can create their own alternative indicator reports for a range of indicators with relative ease. The methods are illustrated with a comparative evaluation of the impact of the research of three large is a single, \f3 medical funders using three types of data: Scopus citation counts; Mendeley reader counts; and Wikipedia citations. 2 Mean Normalised Log-transformed Citation Score The citation count of an article must be compared to the citation counts of other articles in order to be assessed. The same is true for collections of articles and a simple solution would be to calculate the average number of citations per article for two or more collections so that the values can be compared. This is a flawed approach for the following reasons that have led to the creation of improved methods. Older articles tend to be more cited than younger articles (Wallace, Larivière, & Gingras, 2009) and so it is not fair to compare averages between sets of articles of different ages. Similarly, different fields attract citations at different rates and so comparing averages between sets of articles from different mixes of fields would also be unfair (Schubert & Braun, 1986). One solution would be to segment each collection of articles into separate sets, one for each field and year, and only compare corresponding sets between collections. Although this may give useful fine grained information, it is often impractical because each set may contain too few articles to reveal informative or statistically significant differences. The standard solution to field differences in citation counts is to use a field (and year) normalised indicator. The Mean Normalised Citation Score (MNCS), for example, adjusts each citation count by dividing it by the average for the world in its field and year. After this, the arithmetic mean of the normalised citation counts is the MNCS value (Waltman, van Eck, van Leeuwen, Visser, & van Raan, 2011ab). This can reasonably be compared between different collections of articles or against the world average, which is always exactly 1, as long as all articles are classified in a single field. If some articles are in multiple fields then weighting articles and citations with the reciprocal of the number of fields containing the article ensures that the world average is 1 (Waltman et al., 2011a). A limitation of the MNCS is that the arithmetic mean is inappropriate for citation counts and most alternative indicators because they are hi",
            {
                "entities": [
                    [
                        2,
                        89,
                        "TITLE"
                    ],
                    [
                        3345,
                        3432,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptCurr Opin Syst Biol. Author manuscript; available in PMC 2022 December 01.Published in final edited form as:Curr Opin Syst Biol. 2021 December ; 28: . doi:10.1016/j.coisb.2021.100363.Design and development of engineered receptors for cell and tissue engineeringShwan B. Javdan1, Tara L. Deans1,*1Department of Biomedical Engineering, University of Utah, Salt Lake City, UTAbstractAdvances in synthetic biology have provided genetic tools to reprogram cells to obtain desired cellular functions that include tools to enable the customization of cells to sense an extracellular signal and respond with a desired output. These include a variety of engineered receptors capable of transmembrane signaling that transmit information from outside of the cell to inside when specific ligands bind to them. Recent advances in synthetic receptor engineering have enabled the reprogramming of cell and tissue behavior, controlling cell fate decisions, and providing new vehicles for therapeutic delivery.KeywordsSynthetic biology; receptor engineering; synthetic receptors; cell and tissue engineeringIntroductionA traditional focus of synthetic biologists is to apply engineering principles to reprogram cells with desired functions by assembling individual genetic parts into more complex gene circuits to produce switches [1–6], oscillators [7–10], and biosensors [11–16]. A newer approach is to engineer custom receptors to change which extracellular cues cells recognize to produce a desired cellular response. The concept that “the whole is greater than the sum of its parts” has become ubiquitous across a variety of disciplines, especially in systems biology where small-scale molecular interactions are engineered in ways that impact large-scale cellular and tissue functions. The advent of synthetic biology has enabled the engineering of cells and tissues to perform novel biological behaviors, often with the use of genetic circuits and designer synthetic receptors [17–22]. Additionally, cellular *Corresponding author (tara.deans@utah.edu). Conflict of interest statement:Nothing declared.Declaration of interestsThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptJavdan and DeansPage 2reprogramming has been utilized for controlling cell fate decisions [23–26], as well as for drug delivery applications [27–31].Customizing cell sensing with a triggered biological response has enabled an entirely new therapeutic approach that can be specifically targeted to sites of disease or injury. The clinical potential of cell therapy became clear with the recent FDA-approval of chimeric antigen receptor (CAR)-T cell therapy for large B-cell lymphoma [32,33], a landmark achievement that represented a new paradigm in the treatment of human diseases. In short, patients’ T cells are engineered to produce a genetically modified receptor that endows them with the ability to target specific cancer proteins [20,34,35]. These efforts have highlighted how the engineering of receptors can lead to new cell signaling pathways beyond those found in nature. In this review, we discuss recent advances among several major classes of engineered receptors (Figure 1) as well as compare the methods used in their design and development. We also highlight the cell and tissue types modified by these engineered receptors, some potential challenges to future progress, as well as potential avenues for further synthetic receptor development.Receptors activated solely by synthetic ligands (RASSLs)G-protein coupled receptors (GPCRs) are a group of membrane receptors in eukaryotes that bind to a diverse number of extracellular molecules that cause a conformational change in the receptor, triggering second messenger molecules that initiate and coordinate intracellular signaling pathways. These receptors act as signaling switches for a vast array of physiological responses in the body, making them popular receptors for engineering synthetic signaling systems [36–38]. Many studies have sought to engineer GPCRs to be activated by pharmacologically inert small molecule drugs, a family collectively known as receptors activated solely by synthetic ligands (RASSLs) (Figure 1a). These engineered receptors have taken many forms, primarily designer receptors exclusively activated by designer drugs (DREADDs) [39–41].DREADDs were initially developed by directed molecular evolution of human muscarinic receptors, which are involved in the parasympathetic nervous system, as a tool to study receptor-specific functions [39], and have since expanded to modulate the downstream GPCR pathways for several tissue engineering applications. The methods for designing novel RASSLs have spanned approaches in directed molecular evolution, directed mutagenesis, and structure-guided chimera development (Figure 2a–c). The earliest DREADDs were generated in yeast (Figure 2a) [39,42], however, more modern approaches have since been developed in mammalian cells, such as the Viral Evolution of Genetically Actuating Sequences (VEGAS) platform, which paves the way for evolving mammalian GPCRs that otherwise could not be successfully expressed in yeast [43]. Another common design approach has involved the substitution of conserved residues in the GPCR binding pockets with alanines by site-directed mutagenesis, allowing for fine tuning of the receptor’s binding affinity and potency (Figure 2b) [44,45]. Alternatively, chimeric GPCRs have been developed by structure-guided protein design principles, for example, the intracellular loops of one GPCR have been replaced with another while keeping the extracellular and transmembrane domains intact (Figure 2c) [46]. This approach allowed for the generation of Curr Opin Syst Biol. Author manuscript; available in PMC 2022 December 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptJavdan and DeansPage 3GPCRs with a unique input, in this case light instead of a ligand, while retaining the native receptor’s output signals inside the cell.DREADDs have recently been used for the synthetic control of chondrocyte calcium signaling in order to enhance the compositional and mechanical properties of engineered cartilage [47]. Additionally, they have been engineered as ultrasound-responsive neuromodulations for the noninvasive control of memory formation in the mouse hippocampus [48], and in hepatocytes to stimulate cyclic AMP signaling and control glucose levels to study hyperglycemia [49]. Finally, DREADD-based therapeutics have been proposed for a variety of diseases, including Parkinson’s disease and drug addiction [40,50,51]. Data from these studies suggested that pharmacogenetics may enhance overall cellular signaling in reprogrammed cells, and could be used in cell replacement therapy for patients whose own cells have signaling deficiencies. Overall, DREADDs are an exciting avenue for synthetic biologists to rewire endogenous cellular signaling pathways in response to synthetic ligands to obtain desired cell behavior. For example, these receptors have been engineered to direct the migration of cells in response to the inert drug-like molecule, clozapine-N-oxide (CNO) [52]. This work opens the possibility to engineer therapeutic cells with the capabilities of directing therapeutic cells to a user-specified location in the body.Chimeric antigen receptors (CARs)Chimeric antigen receptors (CARs) are a class of synthetic receptors that combine antigen­specificity and T cell (CAR-T) activating properties in a single fusion molecule [53,54]. CAR-T cells were the first FDA-approved genetically-modified cell-based therapy to induce complete remission of hematological malignancies in some patients where traditional chemotherapy has failed [32,33]. CARs are fusion proteins with an extracellular antigen recognition domain typically comprised of single-chain variable fragments (scFvs) from a monoclonal antibody, a spacer/hinge region, a transmembrane domain, and an intracellular signaling domain (Figure 1b) [53,55,56]. In contrast with many other synthetic receptors, CARs are often engineered by rational design principles, and are created as chimeric proteins (Figure 2c). Computational modeling and docking simulation data have also aided in the design and development of novel CARs, improving their overall specificity (Figure 2d) [57–59].Since their inception, there have been multiple generations of CARs, with the first generation lacking any costimulatory domains resulting in the cells not proliferating or surviving upon repeated antigen exposure. However, once costimulatory properties were incorporated into subsequent generations of CARs, they were shown to have a greater strength of antigen-induced signaling that also enabled T cell proliferation and survival upon repeated exposure to antigen [53,56]. Additional engineering of CARs have also been reported that can control on-off switches to enhance T cell function [60,61], contain therapeutic payloads [62], and implement Boolean operations to improve on-target off-tumor toxicity [63–66]. Most recently, the field has seen the development of universal CARs to allow for a variety of antigens to be targeted without requiring the immune cells to be re-engineered [61,66,67].Curr Opi",
            {
                "entities": [
                    [
                        281,
                        359,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "© <2021>. This manuscript version is made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/     The definitive publisher version is available online at https://doi.org/ 10.1016/j.knosys.2021.106994 \fPlease cite as: Zhang, Y., Wu, M., Tian, G. Y., Zhang, G. & Lu, J. 2021, Ethics and privacy of artificial intelligence: Understandings from bibliometrics, Knowledge-based Systems, DOI: 10.1016/j.knosys.2021.106994 Ethics and privacy of artificial intelligence: Understandings from bibliometrics Yi Zhang1, Mengjia Wu1, George Yijun Tian2, Guangquan Zhang1, Jie Lu1 1Australian Artificial Intelligence Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Australia 2Faculty of Law, University of Technology Sydney, Australia Email: yi.zhang@uts.edu.au; mengjia.wu@student.uts.edu.au; yijun.tian@uts.edu.au; guangquan.zhang@uts.edu.au; jie.lu@uts.edu.au ORCID: 0000-0002-7731-0301 (Yi Zhang); 0000-0003-3956-7808 (Mengjia Wu); 0000-0003-4472-5428 (George Yijun Tian); 0000-0003-3960-0583 (Guangquan Zhang); 0000-0003-0690-4732 (Jie Lu). Abstract Artificial intelligence (AI) and its broad applications are disruptively transforming the daily lives of human beings and a discussion of the ethical and privacy issues surrounding AI is a topic of growing interest, not only among academics but also the general public. This review identifies the key entities (i.e., leading research institutions and their affiliated countries/regions, core research journals, and communities) that contribute to the research on the ethical and privacy issues in relation to AI and their intersections using co-occurrence analysis. Topic analyses profile the topical landscape of AI ethics using a topical hierarchical tree and the changing interest of society in AI ethics over time through scientific evolutionary pathways. We also paired 15 selected AI techniques with 17 major ethical issues and identify emerging ethical issues from a core set of the most recent articles published in Nature, Science, and Proceedings of the National Science Academy of the United States. These insights, bridging the knowledge base of AI techniques and ethical issues in the literature, are of interest to the AI community and audiences in science policy, technology management, and public administration. Keywords Artificial intelligence; Ethics; Privacy; Bibliometrics; Topic analysis.  \fHighlights • Articles on AI ethics cover 199 of the 254 Web of Science Categories, indicating a broad interest from the academia. • Research communities of computer science, business and management, medical science and law are playing a leading role on studies of AI ethics. • USA, UK, and China make the major contribution to AI ethics, with a relatively high level of domestic collaborations. • Key AI techniques raise ethical concerns, such as fairness, accountability, data privacy, responsibility, liability, and crimes.  \f1. Introduction A pandora’s box of artificial intelligence (AI) has been opened and these disruptive technologies are transforming the daily lives of human beings in relation to new ways of thinking and behavioral patterns, with enhanced capabilities and efficiency. There are many examples of AI applications in use today, such as smart homes [1] smart farming [2], precision medicine [3] and healthcare surveillance systems [4].The ethical and privacy issues surrounding the use of AI have been a topic of growing interest among diverse communities. For example, the general public has expressed concern about the impact of the increased use of robots on unemployment and inequality [5], social scientists have raised deep privacy concerns related to surveillance systems [6], and limited regulation of social media has raised debate with technical giants on the abuse of private data1. Despite these concerns, the AI community stands behind the efficiency and robustness of their AI models and there is an urgent need to guide the research community to understand these ethical and privacy challenges. Bibliometrics, which is a set of approaches for analyzing scientific documents (e.g., research articles, patents, and academic proposals), has been widely used as a tool for science, technology and innovation studies [7], such as identifying technological topics [8], discovering latent relationships [9], and predicting potential future trends [10]. Recently, AI has received recognition in bibliometrics as an emerging topic for empirical investigation [11, 12]. These investigations either align with the interest in technology management (e.g., using AI as a representative case in digital transformation) or emphasize its role in examining the reliability of the proposed methods. However, from a practical perspective, a bibliometric guide which summarizes ideas, assumptions, and debate in the literature would bring significant benefits to the AI community, not only by highlighting the ethical and privacy concerns raised by the public but also by identifying the potential conflicts between AI techniques and these issues of concern. To address these concerns, this paper reports on a bibliometric study to comprehensively profile the key ethical and privacy issues discussed in the research articles and to trace how such issues have changed over the past few decades. We integrated a set of intelligent bibliometric approaches within a framework for diverse analyses. To identify the key entities, i.e., the leading research institutions and their affiliated countries and regions, and the core research journals and their behind research communities, which report the ethical and privacy issues surrounding AI, we used co-occurrence statistics with diverse bibliographical indicators (e.g., authors, affiliations, and sources). With specific foci in topic analysis, we initially retrieved terms from the combined titles and abstracts of collected articles and used a term clumping process [13] to remove noisy terms and consolidate technical synonyms. In parallel, we represented each word in the combined field with titles and abstracts as a vector using the Word2Vec model [14] and combined the word vectors into term vectors by matching the core terms refined in the term clumping process. We answered the 1 More information can be found on the website: https://www.bbc.com/news/business-49099364 \fquestions as to what is the topical landscape and how have these topics evolved over time, using an approach of scientific evolutionary pathways [15]. We also targeted a core set of articles published in three world-leading multi-disciplinary journals, namely Science, Nature, and Proceedings of the National Academy of Sciences (PNAS) of the United States of America, and identified cutting-edge issues that might either focus attention on emergent ethical and privacy issues in the current AI age or lead to novel developments in AI models to address any potential negative impacts. We anticipate that the empirical insights identified in this study will motivate the AI community to extensively and comprehensively discuss the ethical and privacy issues surrounding AI and will guide the implementation of AI in line with an ethical framework. The rest of this paper is organized as follows: Section 2 presents a review of the related work on AI ethics, privacy, and bibliometrics; Section 3 introduces the data and methodologies used in this study; Section 4 presents the results, and our key findings and Section 5 concludes the study and suggests future research directions. 2. Related work In this section, we review the current debate on the ethical and privacy issues surrounding AI and then briefly introduce the bibliometrics and topic analysis used in this study. 2.1. Ethics, ethical dilemma, and AI ethics In philosophy, ethics describes “what is good for the individual and for society”, as well as the essence of “duties that people owe themselves and one another” [16], while ethical dilemma refers to certain ethical problems can be extremely complicated and the challenges they bring cannot be easily solved. Ever-improving technologies bring along with multiple advantages to human society, but they may also “generate downside risks and challenges, including more complicated ethical dilemma2. This is true with AI technologies. With the rapid growth in AI techniques in recent decades, there has been increasing controversy over the impact of AI on the daily lives of human beings, for example, the potential for robots to replace human labor [17], the accountability and accident risk of driverless vehicles [18], the self-awareness and behavior autonomy of robotics [19], and possible fraud caused by deep-fake videos and photos [20]. Such concerns in relation to the ethics around AI has attracted attention from global federal governments and corporations, in particular, tech giants such as Google and SAP, when those corporations are willing to form national and industrial committee to formulate AI ethics guidelines [21]. An increasing number of international organizations have also started to take actions to address the ethical challenges brought by AI technology. As one of the most recent developments, the United Nations Educational, Scientific and Cultural Organization (UNESCO) has issued its first draft of Recommendation on the Ethics of 2 the United Nations Educational, Scientific and Cultural Organization, Elaboration of a Recommendation on the ethics of artificial intelligence at https://en.unesco.org/artificial-intelligence/ethics  \fArtificial Intelligence (Recommendations) 3 in September 2020, which sets up ten important Principles of the Ethics of AI, including: proportionality and do no hard, safety and security, fairness and non-discrimination, sustainability, privacy, human oversight and determination, transparency and expandability, responsibly and accountability, awareness and literacy, and multi-stakeholder and adaptive governance and collaboratio",
            {
                "entities": [
                    [
                        316,
                        396,
                        "TITLE"
                    ],
                    [
                        457,
                        537,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "bioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.Enriching Representation Learning Using 53 Million Patient Notes through Human Phenotype Ontology Embedding Maryam Danialiab, Peter D. Galerbcde, David Lewis‐Smithbcdfg, Shridhar Parthasarathybcd, Edward Kima, Dario D. Salvuccia, Jeffrey M. Millerb, Scott Haagb*, Ingo Helbigbcdh*ⵜ a Department of Computer Science, Drexel University, Philadelphia, PA, USA b Department of Biomedical and Health Informatics (DBHi), Children’s Hospital of Philadelphia, Philadelphia, PA, USA c Division of Neurology, Children’s Hospital of Philadelphia, Philadelphia, PA, USA d The Epilepsy Neuro Genetics Initiative (ENGIN), Children’s Hospital of Philadelphia, Philadelphia, PA, USA e Center for Neuroengineering and Therapeutics, University of Pennsylvania, Philadelphia PA f Translational and Clinical Research Institute, Newcastle University, Newcastle-upon-Tyne, UK g Department of Clinical Neurosciences, Royal Victoria Infirmary, Newcastle-upon-Tyne, UK h Department of Neurology, University of Pennsylvania, Perelman School of Medicine, Philadelphia, PA, USA * Equal contributions ⵜ Corresponding author, helbigi@chop.edu           \fbioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.Abstract The Human Phenotype Ontology (HPO) is a dictionary of more than 15,000 clinical phenotypic terms with defined semantic relationships, developed to standardize their representation for phenotypic analysis. Over the last decade, the HPO has been used to accelerate the implementation of precision medicine into clinical practice. In addition, recent research in representation learning, specifically in graph embedding, has led to notable progress in automated prediction via learned features. Here, we present a novel approach to phenotype representation by incorporating phenotypic frequencies based on 53 million full-text health care notes from more than 1.5 million individuals. We demonstrate the efficacy of our proposed phenotype embedding technique by comparing our work to existing phenotypic similarity-measuring methods. Using phenotype frequencies in our embedding technique, we are able to identify phenotypic similarities that surpass the current computational models. In addition, we show that our embedding technique aligns with domain experts' judgment at a level that exceeds their agreement. We show that our proposed technique efficiently represents complex and multidimensional phenotypes in HPO format, which can then be used as input for various downstream tasks that require deep phenotyping, including patient similarity analyses and disease trajectory prediction. Keywords Human phenotype ontology; Representation learning; Dimension reduction; Electronic health record; Phenotype embedding     \fbioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.1 Introduction Electronic medical records (EMRs) have been implemented in the majority of US hospitals and accumulate clinical data at a massive scale [1]. While initially created for billing purposes [2], EMRs increasingly represent a major source of data in clinical research efforts to improve patient care. However, automated interpretation of EMRs is challenging, particularly for diagnoses that incorporate dynamic and diverse sets of clinical features. Phenotypes, a set of observable characteristics and clinical traits, have an essential role in connecting clinical research and practice. Algorithms that use phenotypes to find similarities and differences between patients play a foundational role in EMR research [3]. However, phenotypic descriptions available in EMRs are often stored in the form of unstructured data and do not allow for direct comparisons. The Human Phenotype Ontology (HPO) is one approach to overcome these limitations (human-phenotype-ontology.org). The HPO is a standardized representation of more than 15,000 clinical phenotypic concepts and their relationships based on expert knowledge. We have contributed to HPO terminology since 2010 [4-7]. The HPO has been widely used for harmonization of clinical features in various studies, including, but not limited to, semantic unification of common and rare diseases [8], genetic discoveries in pediatric epilepsy [9, 10], and delineation of longitudinal phenotypes [11, 12]. In addition, the HPO is commonly used for genomic studies and allows for analyses of clinical data at a scale that is required by current and future initiatives. For example, large national and international initiatives have started to systematically link biorepositories to EMR data, including up to 80,000 cases and 500,000 controls [13-15], highlighting the possibilities for novel biological insight at scale. The HPO can be modeled as a directed acyclic graph (DAG) in a computational system, where each phenotype is presented as a node with a unique identifier and is connected to its parent phenotypes by “is a” relationships in the form of directed edges. This structure guarantees that if a disease or gene is annotated to a phenotypic term, it will also be annotated to all its ancestral terms (higher-level concepts within the larger phenotypic tree). The HPO is regularly updated to incorporate advances in phenotypic conceptualization [16]. Extraction of phenotypic concepts is a crucial step in any automated pipeline to exploit the scale of EMR data in clinical research. Natural Language Processing (NLP) pipelines such as cTAKES [17], ClinPhen [18], and MetaMap [19] are commonly used to derive phenotypic concepts from the EMR, effectively allowing  \fbioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.the transition from unstructured free text to structured representations. While these NLP pipelines share a common goal, that is, extracting phenotypes from clinical free text, they have different components and employ a variety of procedures, thus, should be evaluated from different endpoints such as precision, sensitivity, ease of use, and speed [18, 20]. Furthermore, phenotype extraction typically serves as only a starting point to more complex analyses. Therefore, studies need to perform additional analyses on the extracted phenotypes and their relationships to accomplish tasks like patient comparisons and predicting patient status [21]. Manual analysis of phenotypes is non-scalable, resource-intensive, and virtually impossible for larger cohorts. Accordingly, reliable algorithms for computational phenotype analysis are urgently needed. Comparing phenotypes to one another is a common building block for downstream clinical tasks. Methods for measuring phenotypic similarities, using measures such as the Resnik score [22], information coefficient [23], and graph information content [24], show promise for diagnoses and open doors to novel biological insights such as genetic discoveries [9, 25, 26]. However, these methods are not generally transferable to other tasks and require a significant amount of computation, even with minor changes to the data. As a result, while these methods perform relatively well on limited data with hundreds of patients, they are computationally intensive for large-scale data with millions or even thousands of patients with diverse clinical representations [18, 26]. Representation learning is a group of machine learning algorithms that discovers and learns representations of data, making it easier to extract information that can be used for various tasks such as classification and prediction [27]. Recent work in representation learning has shown success in discovering useful representations without relying on procedural techniques, especially in domains with complex and large data [27]. Embedding algorithms, discussed in Section 2, are a branch of representation learning that model discrete objects as continuous vectors. They offer a compact representation that captures similarities between the original objects and have revolutionized data processing and analysis in many domains, including text processing, by representing words in a compact space [28, 29]. Embedding algorithms have also been extended to encode other data structures such as nodes and graphs [30, 31]. A few studies have applied representation learning and embedding techniques in the health domain and presented promising results on specific phenotypes for a limited number of diseases [32, 33]. Here, we map 53 million full-text health care notes of more than 1.5 million individuals to HPO terms and perform graph embedding to assess the possibilities and limitations of representation learning  \fbioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 Int",
            {
                "entities": [
                    [
                        340,
                        447,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ArticleConnectome-based machine learning models arevulnerable to subtle data manipulationsGraphical abstractAuthorsMatthew Rosenblatt,Raimundo X. Rodriguez,Margaret L. Westwater, ...,R. Todd Constable, Stephanie Noble,Dustin ScheinostCorrespondencematthew.rosenblatt@yale.edu (M.R.),dustin.scheinost@yale.edu (D.S.)In briefImperceptible data manipulations candrastically increase or decreaseperformance in machine learning modelsthat use high-dimensional neuroimagingdata. These manipulations could achievenearly any desired predictionperformance without noticeable changesto the data or any changes in otherdownstream analyses. The feasibility ofdata manipulations highlights thesusceptibility of data sharing andscientific machine learning pipelines tofraudulent behavior.Highlightsd Enhancement attacks falsely improve the performance ofconnectome-based modelsd Adversarial attacks degrade the performance ofconnectome-based modelsd Subtle data manipulations lead to large changes inperformanceRosenblatt et al., 2023, Patterns 4, 100756July 14, 2023 ª 2023 The Author(s).https://doi.org/10.1016/j.patter.2023.100756ll\fPlease cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSArticleConnectome-based machine learning modelsare vulnerable to subtle data manipulationsMatthew Rosenblatt,1,9,* Raimundo X. Rodriguez,2 Margaret L. Westwater,3 Wei Dai,4 Corey Horien,2 Abigail S. Greene,2R. Todd Constable,1,2,3,5 Stephanie Noble,3 and Dustin Scheinost1,2,3,6,7,8,*1Department of Biomedical Engineering, Yale School of Engineering and Applied Science, New Haven, CT 06510, USA2Interdepartmental Neuroscience Program, Yale School of Medicine, New Haven, CT 06510, USA3Department of Radiology & Biomedical Imaging, Yale School of Medicine, New Haven, CT 06510, USA4Department of Biostatistics, Yale School of Public Health, New Haven, CT 06510, USA5Department of Neurosurgery, Yale School of Medicine, New Haven, CT 06510, USA6Department of Statistics & Data Science, Yale University, New Haven, CT 06510, USA7Child Study Center, Yale School of Medicine, New Haven, CT 06510, USA8Wu Tsai Institute, Yale University, New Haven, CT 06510, USA9Lead contact*Correspondence: matthew.rosenblatt@yale.edu (M.R.), dustin.scheinost@yale.edu (D.S.)https://doi.org/10.1016/j.patter.2023.100756THE BIGGER PICTURE In recent years, machine learning models using brain functional connectivity havefurthered our knowledge of brain-behavior relationships. The trustworthiness of these models has notyet been explored, and determining the extent to which data can be manipulated to change the results isa crucial step in understanding their trustworthiness. Here, we showed that only minor manipulations ofthe data could lead to drastically different performance. Although this work focuses on machine learningmodels using brain functional connectivity data, the concepts investigated here apply to any scientificresearch that uses machine learning, especially with high-dimensional data. As machine learning becomesincreasingly popular in many fields of scientific research, data manipulations may become a major obstacleto the integrity of scientific machine learning.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYNeuroimaging-based predictive models continue to improve in performance, yet a widely overlookedaspect of these models is ‘‘trustworthiness,’’ or robustness to data manipulations. High trustworthinessis imperative for researchers to have confidence in their findings and interpretations. In this work, weused functional connectomes to explore how minor data manipulations influence machine learning predic-tions. These manipulations included a method to falsely enhance prediction performance and adversarialnoise attacks designed to degrade performance. Although these data manipulations drastically changedmodel performance, the original and manipulated data were extremely similar (r = 0.99) and did not affectother downstream analysis. Essentially, connectome data could be inconspicuously modified to achieveany desired prediction performance. Overall, our enhancement attacks and evaluation of existingadversarial noise attacks in connectome-based models highlight the need for counter-measures thatimprove the trustworthiness to preserve the integrity of academic research and any potential translationalapplications.INTRODUCTIONHuman neuroimaging studies have increasingly used machinelearning approaches to identify brain-behavior associationsthat generalize to novel samples.1,2 They do so by aggregatingweak yet informative signals occurring throughout the brain.3,4Machine learning models for functional connectomes (‘‘con-nectome-based models’’)5–7 are among the most popularPatterns 4, 100756, July 14, 2023 ª 2023 The Author(s). 1This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).PATTER 100756\fPlease cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSmethods for establishing brain-behavior relationships, andthey have successfully characterized the neural correlates ofvarious clinically relevant processes,8 including general cogni-tive ability,9 psychiatric disorders,7,10 affective states,11 andabstinence in individuals with substance use disorder.12Recent work has uncovered bias, or lack of fairness acrossgroups, in connectome-based models,13–15 including predic-tion failure in individuals who defy stereotypes.15 Although im-provements in accuracy6 and fairness (i.e., race, age, orgender bias)13–15 of connectome-based models are crucialfor improving the quality of academic studies and the potentialfor clinical translation, accurate and bias-free models are notenough. Connectome-based models should also have hightrustworthiness, which we define as robustness to data ma-nipulations.In other words, the output or performance ofa trustworthy model remains similar despite minor changesto the input (i.e., X data). Without a high degree of trustworthi-ness, researchers may not be able to have confidence in theirfindings and ensuing interpretations, as even minor modifica-tions to the data could dramatically alter results.Although trustworthiness has been explored from variousperspectives in the machine learning literature, including pri-vacy16 and explainability,17 here we examine trustworthinessthrough the lens of robustness to data manipulations.18 A pop-ular form of data manipulation specific to machine learning isadversarial noise (i.e., adversarial attacks), where a pattern(or ‘‘noise’’) deliberately designed to trick a machine learningmodel is added to data to cause misclassification.19,20 Theseattacks have been investigated in various contexts, includingcybersecurity,21,22 image recognition,20,23 and medical imagingor recordings.24–26 For neuroimaging, adversarial attacks maybecome problematic in the more distant future (e.g., in clinicalapplications25,27).A more immediate concern is the potential for data manipula-tions to falsely enhance prediction performance in researchstudies. Although the majority of scientific researchers seek toperform ethical research, data manipulations are more commonthan one might expect.28–33 For example, an analysis by Bik et al.showed that about 2% of biology papers contained a figure withevidence of intentional data manipulation.31 Furthermore, 2%of scientists admitted to fabrication/falsification, and 14%admitted to seeing their colleagues fabricate/falsify in a survey.32As data manipulation can result in wasted grant money andfuture research endeavors, determining themisdirection ofextent to which the prediction performance of connectome-based models can be falsely enhanced or diminished via datamanipulations is crucial.In this work, we investigated the trustworthiness of connec-tome-based predictive models. Specifically, we introduce thefor connectome-based‘‘performance enhancement attack’’models, where data are injected with small,inconspicuouspatterns to falsely improve the prediction performance ofa specific phenotype. We also explore the effectiveness of ad-versarial noise attacks on connectome-based models. Whereasadversarial noise attacks manipulate only the test data to changea particular prediction, enhancement attacks modify the entiredataset (i.e., training and test data) to falsely improve perfor-mance. In both cases—enhancement attacks and adversarialnoise attacks—we find that subtle manipulations drastically2 Patterns 4, 100756, July 14, 2023Articlechange predictions in four large datasets. Overall, our findingsdemonstrate that currentimplementations of connectome-based models are highly susceptible to data manipulations,which points toward the need for preventive measures built into study designs and data sharing practices.RESULTSFunctional MRI data were obtained from the Adolescent BrainCognitive Development (ABCD) study,34 the Human Connec-tome Project (HCP),35 the Philadelphia NeurodevelopmentalCohort (PNC),36 and the Southwest University LongitudinalImaging Multimodal (SLIM) study.37 The first three datasets(ABCD, HCP, and PNC) were used to demonstrate enhance-ment and adversarial attacks for prediction of IQ and self-re-ported sex. SLIM was introduced to demonstrate enhance-ment with a clinically relevant measure (state anxiety). Allanalyses were conducted on resting-state data. For SLIM,we downloaded fully preprocessed functional connectomes.For ABCD and PNC, raw data were registered to commonspace as previously described.38,39 For HCP, we startedwith the minimally preprocessed data.40 Next, standard, iden-tical preprocessing steps were performed across all datasetsusing BioImage Suite41 (see experi",
            {
                "entities": [
                    [
                        1179,
                        1263,
                        "TITLE"
                    ],
                    [
                        5147,
                        5231,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Modeling of Facial Aging and Kinship: A Survey1Markos Georgopoulos1, Yannis Panagakis1, 2, and Maja Pantic11Dept. of Computing, Imperial College London, UK2Dept. of Computer Science, Middlesex University London, UK(cid:70)8102ceD1]VC.sc[2v63640.2081:viXraAbstract—Computational facial models that capture properties of facialcues related to aging and kinship increasingly attract the attention of theresearch community, enabling the development of reliable methods forage progression, age estimation, age-invariant facial characterization,and kinship verification from visual data. In this paper, we review recentadvances in modeling of facial aging and kinship. In particular, weprovide an up-to date, complete list of available annotated datasets andan in-depth analysis of geometric, hand-crafted, and learned facial rep-resentations that are used for facial aging and kinship characterization.Moreover, evaluation protocols and metrics are reviewed and notableexperimental results for each surveyed task are analyzed. This surveyallows us to identify challenges and discuss future research directionsfor the development of robust facial models in real-world conditions.1 INTRODUCTIONHumans possess explicit, cue-based, and often culturallydetermined systems for perceiving the facial appearance oftheir peers [250]. Facial appearance is a primary source of in-formation regarding the person’s identity, gender, ethnicity,affective state, head pose, age and kinship relations. Hence,the perception of facial attributes governs person percep-tion, interpersonal attraction, and consequently prosocialand social behaviour [5], [170].Human face has been thoroughly studied from different,but complementary, perspectives across several disciplinessuch as neuroscience e.g., [70], psychology e.g., [23], [157],sociology e.g., [61], anthropometry e.g, [66], medicine e.g.,[51] and computer science. From a computational point ofview, in particular, advances in computational face mod-eling enabled the development of reliable methods forautomatic detection of faces [249], recognition of identity[259], [107], [2], gender [159], and ethnicity [72]; detectionof salient facial features [223], [58], estimation of head pose[50] and analysis of facial expressions [190], [169], [251] fromvisual data. Notably, recently proposed methods match oreven achieve better accuracy than humans in several taskse.g., [97]. This progress herald a surge of novel applica-tions in communication, entertainment, cosmetology, andbiometrics, to name a few, while facilitating basic researchin social sciences and medicine e.g., [158]. A thorough listof machine learning and computer vision methods solvingthe aforementioned face modeling and analysis tasks canbe found in the comprehensive survey papers [249], [259],[107], [159], [72], [223], [50], [190], [251].Research towards the development of more detailedcomputational facial models that capture properties of fa-cial cues related to aging and kinship increasingly attractsthe attention of the community. Indeed, by capitalizing onrecent advances in machine learning, computer vision, andthe massive collections of facial data available, significantprogress has been made towards addressing the followingproblems:i Age Progression: that is, the process of transforming afacial visual input, in order to model it across differ-ent ages. The change of the age can be bidirectional,so that the facial output can appear either youngeror older than the input.ii Age Estimation: refers to the process of labelling afacial signal with an age or age group. The input sig-nal can be 2D, 3D or image sequences. The problemsthat fall into this category can be divided furtherinto two subcategories, depending on the labels ofthe training data: (a) real age or (b) apparent ageestimation, which refers to the age that is inferredby humans based on the individual’s appearance.iii Age-Invariant Facial Characterization:involves theprocess of building a signal representation that isinvariant to the facial transformations and appear-ance changes caused by aging.iv Kinship Verification: is defined as the process of de-termining whether the individuals in a pair of facialvisual inputs are blood related.Early models for facial age progression and estimationdate back to 1994-95 [118], [43], while the problem of facerecognition across ages was first investigated in 2000 [121].More recently, since 2010, methods for kinship verificationhave emerged [65]. Since then, the development of 1) ro-bust and computationally efficient models (e.g., AAMs [40],CLMs [42] etc) and descriptors (e.g., SIFT [144], HoGs [44],LPPs [99], SURF [12], DAISY[215] etc) of facial appearance,2) effective machine learning methods such as Boosting [71]and Support Vector Machines [220] and 3) manually anno-tated facial datasets e.g., MORPH2 (2006), FG-NET (2004),FERET (1998), YGA (2008), Gallagher’s Images of Groups(2009), Ni’s Web-Collected Images (2009), have facilitatedthe deployment of reliable computational models for facialaging and kinship. Models, methods, and data for facialaging modeling which have been published before 2010   \fare thoroughly surveyed in [179], [73], while an overviewof research efforts for facial kinship modeling is currentlymissing.This paper aims to provide an up-to-date literature sur-vey of the work done 1) towards the development of facialaging models, complementing previous studies in [73], [179]in several ways and 2) in the emerging topic of facial kinshipmodeling. Concretely, the aims of this survey are organizedas follows:• A complete catalogue of publicly available datasetswith manual annotations for facial age and kinshipmodeling tasks is listed in Section 3. We put partic-ular emphasis on data collected in naturalistic, real-world (in the wild) conditions by providing reviewsfor 9 recently collected datasets for age modeling andall the available (i.e., 16) collections of facial imagesfor kinship verification.• A comprehensive review of recent as well as seminalmethodologies for age progression, age estimation,facial characterization, and kinshipage-invariantverification is provided in Sections 4-7. In particular,we provide an in-depth analysis of both geometric,hand-crafted and learned facial representations forthe aforementioned tasks and discuss the type ofinformation they encode as well as their advantagesand limitations. We further elaborate on methodsthat rely on deep discriminative (e.g., CNNs [125])and generative (e.g., GANs [85]) models and appearto be highly effective. Moreover, we review evalu-ation protocols and metrics, and analyze the mostnotable experimental results for each surveyed task.• The review of data, computational methods for facialaging and kinship reveal useful practices as well aschallenges that are yet to be solved. These along withdrawn conclusions are discussed in Section 9.2a lifelike image of a person. In some cases, forensics expertsface the need to change the age of a face. Such cases includeupdating archive images of wanted criminals as well asimages of lost children. Additionally, cases such as matchingorphaned or lost children and finding the kin of a victim, toname a few, demand the verification of kin relationship oftwo people. To that end, automatic genealogical research cansignificantly aid the work of law enforcement agencies.Medicine and Cosmetology: Being able to model aging andkinship and simulating the transformations on the face isvital for modern medicine and cosmetology. Medical homesystems that are used to monitor elderly people can aidmedical diagnosis by detecting premature aging. On theother hand, automatic rejuvenation of the face can serveas a guide for cosmetic surgery. Particularly in the case ofchildren, the parents’ craniofacial aging patterns can be usedto predict the child’s head growth, so that injury-relatedcosmetic surgery can have optimal long-term results.Commercial use: The ever-growing usage of social mediaand availability of personal photos have led to the rapidintegration of facial analysis by businesses. Automaticallyestimating the customers’ age can help with efficient cus-tomer profiling and age-oriented decision making, e.g., age-oriented advertisements. Likewise, targeted ads can be moreeffective when taking kinship into considerations, as peo-ple’s preferences can be affected by their relatives.Entertainment: Visual effects that age or rejuvenate theactors are already being used in the film making industry.These effects are not limited to movies but are also widelyapplied to photo editing. The imminent integration of suchtools into popular design software will make for morerealistic retouche of photos. Make-up artists that specializein transforming the face can leverage the construction ofperson, age and kin specific morphable models. Guided bythose models, the artists will transform the face of the actorfor roles that demand sibling-like similarity between actors.To begin with, a number of modern applications of com-putational models for facial aging and kinship are discussedin the following section.3 DATASETS2 APPLICATIONSIn this section we present the most significant applicationsof modeling facial aging as well as kinship in biometrics,forensics, medicine, cosmetology, business and entertain-ment.Biometrics: The physical, physiological or behaviouralcues based on which a person is recognized, e.g.,iris,fingerprint, face are referred to as biometrics [108]. Ageand kinship comprise soft biometrics [109], [45], [164] asthey can be used to boost the effectiveness of recognition.Besides improving face recognition accuracy there is a needfor robustness towards aging and kinship. Passport checksdemand age-invariance in case of large age gap betweenthe passport image and the person in question. Similarly,kinship invariance can potentially boost automatic facerecognition, in particular towards distinguishing betweenkin that look alike.Forensics: Forensics include a set of scientific techni",
            {
                "entities": [
                    [
                        0,
                        46,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Deep Convolution Network Based Emotion Analysis towards Mental Health Care Zixiang Fei1, Erfu Yang*1, David Day-Uei Li2, Stephen Butler3, Winifred Ijomah1, Xia Li4$, Huiyu Zhou5 1 Department of Design, Manufacture and Engineering Management University of Strathclyde, Glasgow G1 1XJ, UK {zixiang.fei, erfu.yang, w.l.ijomah}@strath.ac.uk 2 Strathclyde Institute of Pharmacy & Biomedical Sciences University of Strathclyde, Glasgow G4 0RE, UK david.li@strath.ac.uk 3 School of Psychological Sciences and Health University of Strathclyde, Glasgow G1 1QE, UK stephen.butler@strath.ac.uk 4 Shanghai Jiaotong University, Shanghai lixia11111@alumni.sjtu.edu.cn 5 Department of Informatics University of Leicester, LE1 7RH, Leicester hz143@leicester.ac.uk * Correspondence author: erfu.yang@strath.ac.uk; Tel.: +44-141-574-5279 $ Correspondence author in China: lixia11111@alumni.sjtu.edu.cn; Tel.: +8621-3477-3440 Abstract: Facial expressions play an important role during communications, allowing information regarding the emotional state of an individual to be conveyed and inferred. Research suggests that automatic facial expression recognition is a promising avenue of enquiry in mental healthcare, as facial expressions can also reflect an individual’s mental state. In order to develop user-friendly, low-cost and effective facial expression analysis systems for mental health care, this paper presents a novel deep convolution network based emotion analysis framework to support mental state detection and diagnosis. The proposed system is able to process facial images and interpret the temporal evolution of emotions through a new solution in which deep features are extracted from the Fully Connected Layer 6 of the AlexNet, with a standard Linear Discriminant Analysis Classifier exploited to obtain the final classification outcome. It is tested against 5 benchmarking databases, including JAFFE, KDEF,CK+, and databases with the images obtained ‘in the wild’ such as FER2013 and AffectNet. Compared with the other state-of-the-art methods, we observe that our method has overall higher accuracy of facial expression recognition. Additionally, when compared to the state-of-the-art deep learning algorithms such as Vgg16, GoogleNet, ResNet and AlexNet, the proposed method demonstrated better efficiency and has less device requirements. The experiments presented in this paper demonstrate that the proposed method outperforms the other methods in terms of accuracy and efficiency which suggests it could act as a smart, low-cost, user-friendly cognitive aid to detect, monitor, and diagnose the mental health of a patient through automatic facial expression analysis. Keywords: Facial Expression Recognition; Deep Convolution Network; Mental Health Care; Emotion Analysis. 1. Introduction Understanding people’s emotions plays an important role in daily human communications. For advanced human-computer interaction in many emerging applications, recognizing users’ emotions is also vital. Currently, there are many approaches for automated emotional recognition, including the recognition of facial expression and analysis of   \fvoice tone [1], which exist alongside more conventional physiological measures such as measurements of blood pressure, pulse rate or skin conductivity. Automated facial expression recognition has found many practical applications such as in e-learning and health care systems [2,3]. For example, facial expressions are considered as an important feedback mechanism for teachers in terms of monitoring levels of students’ understanding [3]. Within this domain, Mau-Tsuen et al. proposed an automatic system to identify how well students were learning, by means of the analysis of videos taken from learners [3], and demonstrated that such a system could help improving teaching effectiveness and efficiency. Additionally, the user interface developed in such a system could work as a cognitive tool [4] to support and understand the users’ mental state better when and where it was appropriate without external actuation. Further applications of facial expression include its use in the fields of human-computer interaction and interface optimization.  Within these domains, Bahr et al. proposed a novel method to analyze postural and facial expressions to guide interface actuation and actions [4], whilst Pedro et al. employed electromyogram (EMG) sensors to investigate the relationship between users’ facial expressions and adverse-event occurrences [5]. Moreover, health and medical applications of automated facial recognition are also being investigated, for example in the area of diagnostics related to developmental disorders such as autism. Here, promising work has been conducted by means of a system of facial expression analysis during social interactions, where many individuals with such disorders find challenging [6]. Modern techniques in facial expression recognition systems play important roles in these practical applications. These techniques typically involve multiple components, including face localization and facial component alignment, facial feature extraction and facial feature classification. As a simple dichotomy, facial expression analysis systems can be divided into two groups: those using static images and those using continuous video frames. Static algorithms enjoy the advantage of facial expression recognition from a single image or a video frame. In this paper, we describe an algorithm which has been tested for facial expression recognition accuracy from single images mainly acquired through webcams and mobile phone cameras. We would argue that static approaches show promising, but require rigorous and various testing conditions for robust outcomes. Deepak et al, for example, proposed a convolution neural network based algorithm to recognize facial expression with high accuracy [7]. However, their algorithm was tested on only two small facial expression datasets.  Conversely, Jie et al. also proposed three novel convolution neural network (CNN) models with different architectures [8], and tested their algorithms on several datasets such as CK+ and FER2013 datasets. Within the three architectures they presented, the data suggested that the pertained CNN with 5 convolution layers had the best performance. It should be noted however that not all researchers restrict their work to 2D images. For instance, Chenlei et al. very recently proposed a new 3D facial expression modeling method based on facial landmarks [9]. On the other hand, other researchers have moved beyond the analysis of static images and proposed facial expression recognition systems using continuous video frames. For instance, Zia, Lee and other researchers worked on facial expression recognition using temporal dynamics [10,11]. Their proposed system used Fisher Independent Component Analysis as a feature extractor and Hidden Markov Models to learn the features of six different expressions. The experiment results showed that the recognition rate was about 92.85%. Despite advances in the recognition accuracy, there remain significant issues to be addressed in the field of facial expression recognition systems.  The first issue is related to the datasets employed in testing such systems.  Some facial expression recognition systems may have good performance in some image datasets, but perform poorly in the others. For instance, deep CNN approaches often need to determine large amount of weights in the training phase. Consequently it is observed that such approaches suffer from performance decrements when being trained on a small image dataset [8]. A second important issue is ecological validity, in that most of the existing systems use lab-posed facial expressions images.  This is potentially problematic, as it ignores real-world problems such as lighting conditions, image quality and background complexity. It is fatal to address such issues for real-world applications. Facial expression recognition in the wild is a challenging topic due to such issue as well as others such as variance in poses [8].  Thirdly, both traditional approaches and deep learning based approaches have inherent weaknesses. Traditional approaches such as Local Binary Pattern (LBP), Scale Invariant Feature Transform (SIFT) and Support Vector Machine (SVM) employ manually designed features, which can result in poor performance with unseen images. Deniz et al, for instance, observed that traditional approaches performed weakly when being presented with variations in pose, and instead employed a deep CNN based approach to recognize facial expressions, which they \fargued resulted in improved performance [12]. Approaches such as AlexNet, Vgg, and GoogleNet have long training and testing time as well as an enormous amount of memory resource[13] and require hardware incorporating Graphics Processing Units (GPU’s), whilst it is also essential in these approaches. Extensive literature provides  a comprehensive review of the relative advantages and weaknesses of the existing facial expression systems, we would recommend those seeking further information to consult one of the several good reviews of the area (see [14–18]). Indeed, even more recently Byoung, reviewed approaches to facial emotion recognition including conventional facial expression recognition, deep learning based facial expression recognition, well-known facial expression datasets and has also examined some common performance evaluation methods for automated facial expression recognition [19].  Whilst there are many notable developments within the field of automatic face recognition systems, there remain urgent priorities to be addressed to allow these technologies to flourish within the field of mental health care. We would argue that there is vast untapped potential for the field in this area of healthcare. Globally, there are rising numbers of people suffering from cognitive impairment, and consequently there is an urgent need to develop and",
            {
                "entities": [
                    [
                        0,
                        74,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fInformation Fusion 64 (2020) 149–187 Contents lists available at ScienceDirect Information Fusion journal homepage: www.elsevier.com/locate/inffus Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation Yu-Dong Zhang a , b , ∗ , Zhengchao Dong c , d , Shui-Hua Wang b , f , g , Xiang Yu a , Xujing Yao a , Qinghua Zhou a , Hua Hu c , e , Min Li c , h , Carmen Jiménez-Mesa i , Javier Ramirez i , Francisco J. Martinez i , Juan Manuel Gorriz i , j a School of Informatics, University of Leicester, Leicester, LE1 7RH, Leicestershire, UK b Department of Information Systems, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah 21589, Saudi Arabia c Department of Psychiatry, Columbia University, USA d New York State Psychiatric Institute, New York, NY 10032, USA e Department of Neurology, The Second Affiliated Hospital of Soochow University, China f School of Architecture Building and Civil engineering, Loughborough University, Loughborough, LE11 3TU, UK g School of Mathematics and Actuarial Science, University of Leicester, LE1 7RH, UK h School of Internet of Things, Hohai University, Changzhou, China i Department of Signal Theory, Networking and Communications, University of Granada, Granada, Spain j Department of Psychiatry, University of Cambridge, Cambridge CB21TN, UK a r t i c l e i n f o a b s t r a c t Keywords: Multimodal data fusion Neuroimaging Magnetic resonance imaging PET SPECT Fusion rules Assessment Applications Partial volume effect 1. Introduction Multimodal fusion in neuroimaging combines data from multiple imaging modalities to overcome the fundamen- tal limitations of individual modalities. Neuroimaging fusion can achieve higher temporal and spatial resolution, enhance contrast, correct imaging distortions, and bridge physiological and cognitive information. In this study, we analyzed over 450 references from PubMed, Google Scholar, IEEE, ScienceDirect, Web of Science, and var- ious sources published from 1978 to 2020. We provide a review that encompasses (1) an overview of current challenges in multimodal fusion (2) the current medical applications of fusion for specific neurological diseases, (3) strengths and limitations of available imaging modalities, (4) fundamental fusion rules, (5) fusion quality assessment methods, and (6) the applications of fusion for atlas-based segmentation and quantification. Overall, multimodal fusion shows significant benefits in clinical diagnosis and neuroscience research. Widespread edu- cation and further research amongst engineers, researchers and clinicians will benefit the field of multimodal neuroimaging. Neuroimaging has been playing pivotal roles in clinical diagnosis and basic biomedical research in the past decades. As described in the follow- ing section, the most widely used imaging modalities are magnetic res- onance imaging (MRI), computerized tomography (CT), positron emis- sion tomography (PET), and single-photon emission computed tomog- raphy (SPECT). Among them, MRI itself is a non-radioactive, non- invasive, and versatile technique that has derived many unique imaging modalities, such as diffusion-weighted imaging, diffusion tensor imag- ing, susceptibility-weighted imaging, and spectroscopic imaging. PET is also versatile, as it may use different radiotracers to target different molecules or to trace different biologic pathways of the receptors in the body. Therefore, these individual imaging modalities (the use of one imag- ing modality), with their characteristics in signal sources, energy lev- els, spatial resolutions, and temporal resolutions, provide complemen- tary information on anatomical structure, pathophysiology, metabolism, structural connectivity, functional connectivity, etc. Over the past decades, everlasting efforts have been made in developing individual modalities and improving their technical performance. Directions of im- provements include data acquisition and data processing aspects to in- crease spatial and/or temporal resolutions, improve signal-to-noise ratio and contrast to noise ratio, and reduce scan time. On application aspects, ∗ Corresponding author. E-mail addresses: yudongzhang@ieee.org (Y.-D. Zhang), zhengchao.dong@nyspi.columbia.edu (Z. Dong), shuihuawang@ieee.org (S.-H. Wang), xy144@le.ac.uk (X. Yu), xy147@le.ac.uk (X. Yao), qz105@le.ac.uk (Q. Zhou), huhua8775@suda.edu.cn (H. Hu), limin@hhu.edu.cn (M. Li), carmenj@ugr.es (C. Jiménez-Mesa), javierrp@ugr.es (J. Ramirez), fjesusmartinez@ugr.es (F.J. Martinez), gorriz@ugr.es (J.M. Gorriz). https://doi.org/10.1016/j.inffus.2020.07.006 Received 30 April 2020; Received in revised form 6 July 2020; Accepted 14 July 2020 Available online 17 July 2020 1566-2535/© 2020 Elsevier B.V. All rights reserved. \fY.-D. Zhang, Z. Dong and S.-H. Wang et al. Information Fusion 64 (2020) 149–187 the strength and limitations of the neuroimaging modalities and the cor- responding analysis methods, and in particular, the needs for improved image fusion methods and (2) we will review recent methodological development in data preprocessing and data fusion in multimodal neu- roimaging. We note that although we tried to cover all neuroimaging modalities, we inevitably paid more attention to MRI modalities. This is not only due to the most practical application and versatility of the MRI but also due to the limitations of our expertise. Fig. 2 shows the taxonomy of this review. The main contents of the paper are organized as follows. Chapter 2 will give a brief introduction to neuroimaging, and challenges of multi- modal imaging; Chapter 3 introduces the commonly used neuroimaging modalities, which include computerized tomography, positron emission tomography, single-proton emission computed tomography, and mag- netic resonance imaging, which has many modalities in its own right. For each modality, we will concisely describe its signal source, energy level, spatial resolution, temporal resolution, and major applications; Chapter 4 describe applications of neuroimaging in three major areas: the developing brains, the degenerative brains, and mental disorders. In each part, we will first briefly describe what the clinical and/or biomed- ical problems are, we then review recent papers on how neuroimaging has been used to address these problems, and we point out what the unmet needs and challenges; Chapters 5 to 9 are devoted to the multimodal neuroimaging fusion, covering some important procedures in data fusion. The topics are not necessarily complete and their order of presentation is not necessarily coherent with the pipeline of fusion processing. Chapter 5 reviews the fundamental methods, which covers types, rules, atlas-based segmenta- tion, decomposition, reconstruction, and quantification; Chapter 6 re- views subjective and objective assessment of data fusion in multimodal neuroimaging; Chapter 7 reviews the advantages of data fusion in im- proving the spatial/temporal resolution, distortion correction, and con- trast; it also reviews the benefits of these advantages in fusing structural and functional images; Chapter 8 reviews atlas-based segmentations in multimodal imaging fusion; Chapter 9 reviews the quantification in mul- timodal neuroimaging fusion. While the focus of this part is given to PET and SPECT, some of the approaches and principles discussed here, such as partial volume correction and attenuation (relaxation), can be applied to quantitative MRI modalities, such as DTI, ASL, quantitative susceptibility mapping (QSM), etc. Chapter 10 concludes the paper. 2. Multimodal imaging data fusion: challenges in neuroimaging In this part, we will review the current challenges of neuroimag- ing, including limited spatial/temporal resolution, lack of quantifica- tion, and imaging distortions. These challenges often create fundamental limitations on individual modalities of neuroimaging, while some chal- lenges also exist in current multi-modal neuroimaging. This part will mainly cover the challenges of individual neuroimaging modalities that led to the development and ongoing research of multimodal neuroimag- ing methods. 2.1. Individual modality imaging Neuroimaging can be divided into structural imaging and functional imaging according to the imaging mode. Structural imaging is used to show the structure of the brain to aid the diagnosis of some brain dis- eases, such as brain tumors or brain trauma. Functional imaging is used to show how the brain metabolizes while carrying out certain tasks, in- cluding sensory, motor, and cognitive functions. Functional imaging is mainly used in neuroscience and psychological research, but it is grad- ually becoming a new way of clinical-neurological diagnosis [10] . The amount of information obtainable through single-mode imag- ing is limited and often cannot reflect the complex specificity of organ- isms. For instance, although CT imaging is effective in identifying nor- mal structures and abnormal diseased tissues according to their density Fig. 1. Numbers of peer-reviewed papers with the keywords of “neuroimaging ”or “brain imaging ” in titles (the numbers and the bar graph were generated by PubMed in Feb 2020).",
            {
                "entities": [
                    [
                        924,
                        1019,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Medical Image Registration Using Deep Neural Networks: A Comprehensive Review Hamid Reza Boveiri a,*, Raouf Khayami a, Reza Javidan a, Ali Reza MehdiZadeh b a Department of Computer Engineering and IT, Shiraz University of Technology, Shiraz, Iran b Department of Medical Physics and Engineering, Shiraz University of Medical Science, Shiraz, Iran * Corresponding Author Email: hr.boveiri@sutech.ac.ir Abstract—Image-guided interventions are saving the lives of a large number of patients where the image registration problem should indeed be considered as the most complex and complicated issue to be tackled. On the other hand, the recently huge progress in the field of machine learning made by the possibility of implementing deep neural networks on the contemporary many-core GPUs opened up a promising window to challenge with many medical applications, where the registration is not an exception. In this paper, a comprehensive review on the state-of-the-art literature known as medical image registration using deep neural networks is presented. The review is systematic and encompasses all the related works previously published in the field. Key concepts, statistical analysis from different points of view, confiding challenges, novelties and main contributions, key-enabling techniques, future directions and prospective trends all are discussed and surveyed in details in this comprehensive review. This review allows a deep understanding and insight for the readers active in the field who are investigating the state-of-the-art and seeking to contribute the future literature. Keywords—Convolutional Neural Network (CNN); Deep Learning; Deep Reinforcement Learning; Deformable Registration; Generative Adversarial Network (GAN); Image-guided Intervention; Medical Image Registration; One-shot Registration; Staked Auto-Encoders (SAEs). ———————————————————— ♦ ———————————————————— 1. Introduction In most medical interventions, there are a number of cases in which some images need to be captured for diagnosis, prognosis, treatment and follow-up purposes. These images can be vary in terms of temporal, spatial, dimensional or modular. Image fusion causing information synergy can have a significant contribution to guide and support physicians in the process of decision making, mostly in online and real-time fasion. Lack of alignment is unavoidable for these images taken in different times and conditions; hence, can challenge the quality and accuracy of the subsequent analyses. Image registration is the process of aligning two (or more) given images based on an identical geometrical coordination system. The aim is at finding an optimum spatial transformation that registers the structures-of-interest in the best way. This problem is important in numerous ways in the field of machine vision e.g. for remote sensing, object tracing, satellite imaging and so on (Goshtasby 2017). Image registration is also fundamental to the image-guided intervention where e.g. telesurgery, image-guided radiotherapy (IGRT), and precision medicine cannot be operational without using image registration techniques (Peters & Cleary 2008). To exemplify, in IGRT, a pre-interventional image (typically high-quality 3D image), on which the treatment planning is conducted, needs to be registered on an operational image (typically low-quality and noisy 2D) so that the linear accelerator (linac) machine can be calibrated, and the radiation fragment can be conducted with maximal precision and minimal risk of radiation to the adjacent healthy organs referred to as minimally invasive procedure. In this process, the challenges like different modalities of inputted images, low-quality and noise of interventional images, deformation of abdominal cavity’s organs (because of the spontaneous contraction/inflation), movement of thorax cavity’s organs (because of the respiration and heartbeats), changing the size of organs and regions-of-interest (RoIs) due to the weight loss/gain during the treatment process can compromises the quality of solving the problem. In practice, special considerations should be taken into account, and other image processing techniques need to collaborate, which makes the issue very challenging and complicated (Hajnal 2001). Basically, conventional image registration is an iterative optimization process that requires extracting proper features, selecting a similarity measure (to evaluate the registration quality), choosing the model of transformation, and finally, a mechanism to investigate the search space (Oliveira & Tavares 2014). As illustrated in Fig. 1, a couple of images are inputted to the system, of which one is considered as fixed and the other as moving image. The optimal alignment can be - 1 -  \fachieved via iteratively sliding the moving image over the fixed image. At first, the considered similarity measure identifies the amount of correspondence between the inputted images. An optimization algorithm, using an updating mechanism, calculates the new transformation’s parameters. Appling these parameters on the moving image leads a new supposedly better-aligned image. If the termination criteria are satisfied, the algorithm is terminated, else a new iteration should be started. In each iteration, the moving image get a better correspondence with the fixed image, and the iterations continue until no further registration can be achieved, or some predefined criteria are satisfied. The system output can be either the transformation parameters or final interpolated fused image. Fixed  Image Moving Image Similarity Measure Calculation Parameter Updating Transformation Applying Termination? No Fused  Image Transform Parameters Figure 1: The workflow of conventional image registration techniques based on optimization procedures There are two main drawbacks to this strategy as follows:  This iterative manner is very slow where runtimes in the tens of minutes are norm for common deformable image registration techniques even with an efficient implementation on the contemporary GPUs (like a NVIDIA TitanX); while the practical use in clinical operations is real-time, and such a prolonged wasting time is not appreciated.  Most similarity measures have a lots of local optima around the global one, specially where dealing with images from different modalities (referred to as multimodal image registration); they lose their efficiency causing premature convergence or stagnation which are two prevalent confining dilemmas in the optimization field. Accordingly, to circumvent these two confining problems, learning-based registration approaches have gained increasing popularity in the recent years; meanwhile, deep neural networks (DNNs), as one of the most powerful techniques ever seen by the community of machine intelligence, have been applied to the various image processing applications. Of course, medical image registration is not an exception, and a number of deep learning based approaches were proposed in the literature; however, the number of works and the used techniques is very limited, and there is a promising potential for investigation (Litjens et al. 2017).   In this paper, a comprehensive systematic review on the medical image registration using deep neural networks is presented. We have gathered all the relevant state-of-the-arts, from the first one in 2013 up the last one in 2019. The works are analyses based on the different statistical perspectives and measures of interest e.g. years, publication titles, publication types, publishers, authors (total number of publications and citations), keywords, techniques, comparison metrics, datasets, organs of interest, and modalities. The approaches are categorized to the three major generations based on the breakthroughs affecting the contributions. Also, the seminal works in each category and generation are introduced and analyzed in depth, and key contributions and philosophies behind them are presented in details. Finally, there is a discussion introducing confining challenges, open problems and prospective directions. This review allows a deep - 2 -   \funderstanding and insight for the readers active in the field who are investigating the state-of-the-art and seeking to contribute the future literature. Rest of the paper is organized as follows. In the next Section, reference gathering methodology and the challenged faced are described. Section 3 and 4 are devoted to the taxonomy of medical image registration, and the problem formulation, respectively. The architectures of deep neural networks used in the literature are investigated in Section 5. Section 6 is an in-depth literature review on seminal works. Statistical analysis on the state-of-the-art is presented in Section 7. Section 8 is devoted to the discussion on the confining challenges and open problems in the field. And finally, the conclusion and future trends are presented in the last Section. 2. Reference Gathering Methodology To investigate the literature, the systematic search was conducted on two major scientific databases i.e. “SCOPUS” and “PubMed” (since the topic is multidisciplinary between engineering and medicine). “Medical Image Registration” was the main keyword searched accompanied with one of the followings as the underlying technique i.e. “Deep Learning,” “Deep Neural Network,” or “Convolutional.” Moreover, another search was conducted using the aforementioned keywords in major scientific datasets i.e. “Google Scholar” and “CrossRef” to validate the comprehensiveness of the results. The searches were restricted to the Title, Abstract and Keywords whenever it was feasible. A total number of 25 references were detected, among them, 6 cases was irrelevant (e.g. just the results were compared to the deep learning approaches) and 21 cases was completely matched with our criteria. At this point, we detected that some references were overlooked, and that these keywords are insufficient to conduct a comprehensive search; for example",
            {
                "entities": [
                    [
                        0,
                        77,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptNeurocomputing. Author manuscript; available in PMC 2016 January 05.Published in final edited form as:Neurocomputing. 2015 January 5; 147: 485–491. doi:10.1016/j.neucom.2014.06.037.Motion sequence analysis in the presence of figural cuesPawan Sinha1 and Lucia M. Vaina1,2,31Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA2Departments of Biomedical Engineering, Neuroscience and Neurology, Boston University3Department of Neurology, Harvard Medical School, Boston, MAAbstractThe perception of 3D structure in dynamic sequences is believed to be subserved primarily through the use of motion cues. However, real-world sequences contain many figural shape cues besides the dynamic ones. We hypothesize that if figural cues are perceptually significant during sequence analysis, then inconsistencies in these cues over time would lead to percepts of non-rigidity in sequences showing physically rigid objects in motion. We develop an experimental paradigm to test this hypothesis and present results with two patients with impairments in motion perception due to focal neurological damage, as well as two control subjects. Consistent with our hypothesis, the data suggest that figural cues strongly influence the perception of structure in motion sequences, even to the extent of inducing non-rigid percepts in sequences where motion information alone would yield rigid structures. Beyond helping to probe the issue of shape perception, our experimental paradigm might also serve as a possible perceptual assessment tool in a clinical setting.Keywordsmotion; form-cues; structure-from-motion; stroke patients; 3D-structure-from-motion; figural cues; motion-impaired-patientsIntroductionMotion of objects in the environment induces complex transformations in their images. The human visual system can recover the 3-D structure of the viewed objects and their motion in space by interpreting these image transformations [5, 36]. As early as 1953, Wallach and O’Connell demonstrated humans’ capacity to interpret structure from motion while studying what they termed the ‘kinetic depth effect’ [41. In their experiments, an unfamiliar object © 2014 Elsevier B.V. All rights reserved.Correspondence should be addressed to: Professor Lucia M. Vaina, Brain and Vision Research Laboratory, Department of Biomedical Engineering, Boston University, Boston, MA, 02215, USA, Ph: 617-353-2455; Fax: 617-353-6766; vaina@bu.edu. Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptSinha and VainaPage 2was rotated behind a translucent screen with its shadow being observed from the other side of the screen. In most cases, the viewers were able to describe correctly the 3-D shape of the hidden object and its motion, even when each static shadow projection of the object was unrecognizable and contained no 3-D information.Any vision system that attempts to compute 3-D structure from motion must contend with the problem that the recovery of structure is under-constrained; there are infinitely many 3-D structures consistent with a given pattern of motion in the changing 2-D image. Additional constraint is required to establish a unique interpretation. Computational studies have used the rigidity assumption to derive a unique 3-D structure and motion; they assume that if it is possible to interpret the changing 2-D image as the projection of a rigid 3-D object in motion, then such an interpretation should be chosen [3,7, 11, 14,18,22,23,24,25,26, 35,36,37,45]. The rigidity assumption was suggested by perceptual studies that described a tendency for the human visual system to choose a rigid interpretation of moving elements [9,15,16,41].The rigidity assumption has proven to be a powerful constraint, one that appears sufficient to explain how the human visual system solves the structure from motion problem in general settings. However, some interesting perceptual effects suggest that this notion of sufficiency might need to be revisited, at least insofar as modeling human performance is concerned. According to the rigidity assumption, a rigid object in motion should necessarily be perceived as rigid. But, a few studies have reported instances where displays of rigid objects in motion can give rise to the perception of distorting objects [4,6,10,42,46]. As detailed below, we suggest that these breakdowns of rigidity perception hint at a significant contribution from figural shape cues in the perceptual analysis of dynamic sequences. In this paper, we develop the hypothesis of a role for figural cues in sequence analysis, and present experiments designed to test this hypothesis.The remainder of this paper is organized as follows: we first briefly review the current state of research related to the recovery of 3-D structure from motion and static image attributes; subsequently we present a hypothesis regarding interactions between shape information derived using motion and that derived from figural cues; and in Methods we describe the psychophysical experiment and in Results we present evidence from normal observers as well as stroke patients with perceptual impairments in support of the basic thesis of this paper.Possible strategies for the analysis of dynamic sequencesIn a laboratory setting, it is possible to generate motion stimuli that cleanly dissociate between static and dynamic sources of 3-D information. For instance, the random clusters of dots often used in perceptual studies of recovering structure from motion are carefully controlled so that no single frame has any discernible static organization that may provide hints about the 3-D configuration of the dots. This has been the dominant paradigm of structure from motion research so far. Since there is no static 3-D information in such displays, the question of how statically and dynamically derived 3-D shape estimates interact with each other has been sidestepped.Neurocomputing. Author manuscript; available in PMC 2016 January 05.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptSinha and VainaPage 3In the real world, however, the static and dynamic shape cues are almost always confounded. The objects we see moving, e.g. cars, people and airplanes have non-random static configurations that may be used to derive good 3-D shape estimates. Since these estimates are available simultaneously with those from motion cues, we are faced with the question of whether they play a role in determining the eventual 3D percepts.Let us consider two extreme scenarios. In scenario 1, the visual system uses only motion trajectories of feature points to estimate structure (‘features’ are defined as points of high curvature or other punctuate discontinuities). This has been the typical approach for sequence analysis. The reason for the popularity of this approach is its ability to yield unique structure interpretations based on motion information from only a few frames and features. For instance, Ullman [36] has shown that under orthographic projection, three views of four non-coplanar points are sufficient to guarantee a unique 3-D solution. Longuet-Higgins and Prazdny [18] proved that the instantaneous velocity field and its first and second spatial derivatives at a point admit at most three different 3-D interpretations. Tsai and Huang [35] showed that two perspective views of seven points are also usually sufficient to guarantee uniqueness.While scenario 1 is mathematically elegant and powerful, we should consider whether the human visual system does in fact adopt such a strategy, or perhaps it might incorporate other, non motion-based, cues as well in its analysis of dynamic sequences. This leads us to scenario 2. Here, we treat each frame of a motion sequence as an entity to be analyzed on its own, in terms of the figural cues it contains. These cues provide 3D estimates on a frame-by-frame basis, rather than requiring the use of feature motion trajectories, as prescribed by scenario 1. Several figural cues, such as shading, or texture gradients can provide 3D structure information [7,12, 33,47]. Even in the absence of such gradients, global contour based cues provide powerful constraints for 3-D shape recovery [8,13,17,19,43,44] as demonstrated by computational schemes for the recovery of 3D structures from simple 2D line drawings [20,29,30].Is human analysis of dynamic sequences more akin to scenario 1 (predominant use of motion information), or scenario 2 (predominant use of figural information, when such information is available)? Addressing this question presents a challenge in that for most dynamic sequences, both scenarios tend to produce identical results. For instance, a moving wire-frame cube would be seen as a cube irrespective of whether one uses structure from motion algorithms on the vertex trajectories, or applies shape from contour algorithms on individual frames. In order to overcome this constraint, we need dynamic sequences where the motion based and figural content-based strategies yield different results.Here we use dynamic sequences showing rigid wire-frames in motion, where the wire-frame objects are specially constructed so that their different views suggest different 3D shapes. Conventional structure from motion algorithms would easily recover the true rigid 3D structure of these objects. However, the use of figural cues on a frame by frame basis woul",
            {
                "entities": [
                    [
                        279,
                        335,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Information Fusion 16 (2014) 3–17Contents lists available at SciVerse ScienceDirectInformation Fusionj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / i n f f u sA survey of multiple classifier systems as hybrid systemsMichał Woz´ niak a,⇑, Manuel Graña b, Emilio Corchado ca Department of Systems and Computer Networks, Wroclaw University of Technology, Wroclaw, Polandb Computational Intelligence Group, University of the Basque Country, San Sebastian, Spainc Departamento de Informática y Automática, University of Salamanca, Salamanca, Spaina r t i c l ei n f oa b s t r a c tArticle history:Available online 29 April 2013Keywords:Combined classifierMultiple classifier systemClassifier ensembleClassifier fusionHybrid classifierA current focus of intense research in pattern classification is the combination of several classifier sys-tems, which can be built following either the same or different models and/or datasets buildingapproaches. These systems perform information fusion of classification decisions at different levels over-coming limitations of traditional approaches based on single classifiers. This paper presents an up-to-date survey on multiple classifier system (MCS) from the point of view of Hybrid Intelligent Systems.The article discusses major issues, such as diversity and decision fusion methods, providing a vision ofthe spectrum of applications that are currently being developed.(cid:2) 2013 Elsevier B.V. All rights reserved.1. IntroductionHybrid Intelligent Systems offer many alternatives for unortho-dox handling of realistic increasingly complex problems, involvingambiguity, uncertainty and high-dimensionality of data. They al-low to use both a priori knowledge and raw data to compose inno-vative solutions. Therefore, there is growing attention to thismultidisciplinary research field in the computer engineering re-search community. Hybridization appears in many domains of hu-man activity. It has an immediate natural inspiration in the humanbiological systems, such as the Central Nervous System, which is ade facto hybrid composition of many diverse computational units,as discussed since the early days of computer science, e.g., byvon Neumann [1] or Newell [2]. Hybrid approaches seek to exploitthe strengths of the individual components, obtaining enhancedperformance by their combination. The famous ‘‘no free lunch’’ the-orem [3] stated by Wolpert may be extrapolated to the point ofsaying that there is no single computational view that solves allproblems. Fig. 1 is a rough representation of the computational do-mains covered by the Hybrid Intelligent System approach. Some ofthem deal with the uncertainty and ambiguity in the data by prob-abilistic or fuzzy representations and feature extraction. Othersdeal with optimization problems appearing in many facets of the⇑ Corresponding author.E-mail addresses: michal.wozniak@pwr.wroc.pl (M. Woz´ niak), ccpgrrom@g-mail.com (M. Graña), escorchado@usal.es (E. Corchado).1566-2535/$ - see front matter (cid:2) 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.inffus.2013.04.006intelligent system design and problem solving, either following anature inspired or a stochastic process approach. Finally, classifiersimplementing the intelligent decision process are also subject tohybridization by various forms of combination. In this paper, wefocus in this specific domain, which is in an extraordinary efferves-cence nowadays, under the heading of Multi-Classifier Systems(MCS). Referring to classification problems, Wolpert’s theoremhas an specific lecture: there is not a single classifier modeling ap-proach which is optimal for all pattern recognition tasks, sinceeach has its own domain of competence. For a given classificationtask, we expect the MCS to exploit the strengths of the individualclassifier models at our disposal to produce the high quality com-pound recognition system overcoming the performance of individ-ual classifiers. Summarizing:(cid:2) Hybrid Intelligent Systems (HIS) are free combinations of compu-tational intelligence techniques to solve a given problem, cover-ing al computational phases from data normalization up to finaldecision making. Specifically, they mix heterogeneous funda-mental views blending them into one effective working system.(cid:2) Information Fusion covers the ways to combine informationsources in a view providing new properties that may allow tosolve better or more efficiently the proposed problem. Informa-tion sources can be the result of additional computationalprocesses.(cid:2) Multi-Classifier Systems (MCS) focus on the combination ofclassifiers form heterogenous or homogeneous modeling back-grounds to give the final decision. MCS are therefore a subcate-gory of HIS.\f4M. Woz´niak et al. / Information Fusion 16 (2014) 3–17Fig. 1. Domains of hybrid intelligent systems.on three well known academic search sites. The growth in the num-ber of publications has an exponential trend. The last entry of theHistorical perspective. The concept of MCS was first presented byChow [4], who gave conditions for optimality of the joint decision1of independent binary classifiers with appropriately defined weights.In 1979 Dasarathy and Sheela combined a linear classifier and one k-NN classifier [6], suggesting to identify the region of the featurespace where the classifiers disagree. The k-NN classifier gives the an-swer of the MCS for the objects coming from the conflictive regionand by the linear one for the remaining objects. Such strategy signif-icantly decreases the exploitation cost of whole classifier system.This was the first work introducing a classifier selection concept,however the same idea was developed independently in 1981 byRastrigin and Erenstein [7] performing first a feature space partition-ing and, second, assigning to each partition region an individual clas-sifier that achieves the best classification accuracy over it. Otherearly relevant works formulated conclusions regarding MCS ’s classi-fication quality, such as [8] who considered a neural network ensem-ble, [9] with majority voting applied to handwriting recognition,Turner in 1996 [10] showed that averaging outputs of an infinitenumber of unbiased and independent classifiers can lead to the sameresponse as the optimal Bayes classifier, Ho [11] underlined that adecision combination function must receive useful representationof each classifier’s decision. Specifically, they considered severalmethod based on decision ranks, such as Borda count. Finally, thelandmark works devoted introducing bagging [12] and boosting[13,14] which are able to produce strong classifiers [15], in the (Prob-ably Approximately Correct) theory [16] sense, on the basis of theweak one. Nowadays MCS, are highlighted by review articles as ahot topic and promising trend in pattern recognition [17–21]. Thesereviews include the books by Kuncheva [22], Rokach [23], Seni andEdler [24], and Baruque and Corchado [25]. Even leading-edge gen-eral machine learning handbooks such as [26–28] include extensivepresentations of MCS concepts and architectures. The popularity ofthis approach is confirmed by the growing trend in the number ofpublications shown in Fig. 2. The figure reproduces the evolutionof the number of references retrieved by the application of specifickeywords related to MCS since 1990. The experiment was repeated1 We can retrace decision combination long way back in history. Perhaps the firstworthy reference is the Greek democracy (meaning government of the people) rulingthat full citizens have an equal say in any decision that affects their life. Greeksbelieved in the community wisdom, meaning that the rule of the majority willproduce the optimal joint decision. In 1785 Condorcet formulated the Jury Theoremabout the misclassification probability of a group ofindependent voters [5]],providing the first result measuring the quality of classifier committee.Fig. 2. Evolution of the number of publications per year ranges retrieved from thekeywords specified in the plot legend. Each plot corresponds to searching site: thetop to Google Scholar; the center to the Web of Knowledge, the bottom to Scopus.The first entry of the plots is for publications prior to 1990. The last entry is only forthe last 2 years.\fM. Woz´niak et al. / Information Fusion 16 (2014) 3–175plots corresponds to the last 2 years, and some of the keywords giveas many references as in the previous 5 years.Advantages. Dietterich [29] summarized the benefits of MCS: (a)allowing to filter out hypothesis that, though accurate, might beincorrect due to a small training set, (b) combining classifierstrained starting from different initial conditions could overcomethe local optima problem, and (c) the true function may be impos-sible to be modeled by any single hypothesis, but combinations ofhypotheses may expand the space of representable functions.Rephrasing it, there is widespread acknowledgment of the follow-ing advantages of MCS:(cid:2) MCS behave well in the two extreme cases of data availability:when we have very scarce data samples for learning, and whenwe have a huge amount of them at our disposal. In the scarcitycase, MCS can exploit bootstrapping methods, such as baggingor boosting. Intuitive reasoning justifies that the worst classifierwould be out of the selection by this method [30], e.g., by indi-vidual classifier output averaging [31]. In the event of availabil-ity of a huge amount of learning data samples, MCS allow totrain classifiers on dataset’s partitions and merge their decisionusing appropriate combination rule [20].(cid:2) Combined classifier can outperform the best individual classi-fier [32]. Under some conditions (e.g., majority voting by agroup of independent classifiers) this improvement has beenproven analytically [10].(cid:2) Many machine learning algorithms are de facto heuristic searchalgorithms. For example the popular decision tree inductionmethod C4.5 [33] uses a",
            {
                "entities": [
                    [
                        192,
                        249,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "7002nuJ01]na-atad.scisyhp[2v2003060/scisyhp:viXraFunctional dissipation microarrays for classificationD. Napoletani ∗, D. C. Struppa †, T. Sauer ‡andV. Morozov§, N. Vsevolodov§, C. Bailey §AbstractIn this article, we describe a new method of extracting informationfrom signals, called functional dissipation, that proves to be very effec-tive for enhancing classification of high resolution, texture-rich data.Our algorithm bypasses to some extent the need to have very special-ized feature extraction techniques, and can potentially be used as anintermediate, feature enhancement step in any classification scheme.Functional dissipation is based on signal transforms, but uses thetransforms recursively to uncover new features. We generate a varietyof masking functions and ‘extract’ features with several generalizedmatching pursuit iterations.In each iteration, the recursive processmodifies several coefficients of the transformed signal with the largestabsolute values according to the specific masking function; in this waythe greedy pursuit is turned into a slow, controlled, dissipation of thestructure of the signal that, for some masking functions, enhances sep-aration among classes.Our case study in this paper is the classification of crystallizationpatterns of amino acids solutions affected by the addition of smallquantities of proteins.Keywords: Features enhancement, matching pursuit, classification, non-linear iterative maps, wavelet transforms.∗Center for Applied Proteomics and Molecular Medicine, George Mason University,Manassas, VA 20110, email: dnapolet@gmu.edu†Department of Mathematics and Computer Science, Chapman University, Orange,CA 92866‡Department of Mathematical Sciences, George Mason University, Fairfax, VA 22030§National Center for Biodefense and Infectious Diseases, George Mason University20110, Manassas, VA1   \f1IntroductionIn this paper we introduce the functional dissipation microarray, a featureenhancement algorithm that is directly inspired by experimental microarraytechniques [1]. Our method shows how ideas from biological methodologiescan be successfully turned into functional data analysis tools.The idea behind the use of microarrays is that if a large and diverseenough data set can be collected on a phenomenon, it is often possible toanswer many questions, even when no specific interpretation for the datais known. The algorithm we describe here seems particularly suitable forhigh resolution, texture-rich data, and bypasses to some extent the needto preprocess with specialized feature extraction algorithms. Moreover, itcan potentially be used as an intermediate feature enhancement step in anyclassification scheme.Our algorithm is based on an unconventional use of matching pursuit ([2],Chapter 9; [3]). More precisely, we generate random masking functions and‘select’ features with several generalized matching pursuit iterations. In eachiteration, the recursive process modifies several of the largest coefficients ofthe transformed signal according to the masking function. In this way thematching pursuit becomes a slow, controlled dissipation of the structure ofthe signal; we call this process functional dissipation. The idea is that someunknown statistical feature of the original signal many be detected in thedissipation process at least for some of the random maskings.This process is striking in that, individually, each feature extractionwith masking becomes unintelligible because of the added randomness anddissipation, and only a string of such feature extractions can be ‘blindly’used to some effect. There is some similarity in spirit between our approachand the beautiful results on random projections reconstructions describedin [4] and [5], with the important difference that we use several distinctrandomization and dissipation processes to our benefit so that there is astrong non-linear dynamics emphasis in our work. Moreover, we bypassaltogether reconstruction issues, focusing directly on classification in theoriginal representation space. Our method can be seen also as a new instanceof ensemble classifiers (like boosting [6] and bagging [7]), in that severalfunctional dissipations are generally pulled together to achieve improvementof classification results.Other applications of matching pursuit to classification problems includekernel matching pursuit methods [8], and also boosting methods, that canbe interpreted essentially as greedy matching pursuit methods where thechoice of ‘best’ coefficient at each iteration is made with respect to more2\fsophisticated loss functions than in the standard case (see [9], [10], [11]chapter 10). We stress again that, while our approach uses the structure ofmatching pursuits, only their rich dynamical properties (highlighted for ex-ample in [3]) are used for the generation of features, since in our method thewhole iterative process of modifying coefficients becomes simply an instanceof non-linear iterative maps, disjoined from approximation purposes.The case study of this paper is the classification of crystallization pat-terns of amino acids solutions affected by addition of small quantities ofproteins; such crystallization patterns show varied and interesting textures.The goal is to recognize whether an unknown solution contains one of sev-eral proteins in a database. Crystallization patterns may be significantlyaffected by laboratory conditions, such as temperature and humidity, so thedegree of similarity of patterns belonging to a same protein is subject tosome variations, adding difficulty to the classification problem. The rich-ness of the textures and their variability gives this classification problem anappeal that goes beyond the specific application from which it arises.Our basic approach is to derive long feature vectors for each amino acidreporter by our generalized recursive greedy matching pursuit. Since thecrystallization patterns are generated by stochastic processes, there is a greatlocal variability in each droplet and any significant feature must encode astatistical information about the image to be part of a robust classifier, see[10]. We can see this case study as an instance of texture classification,and it is well established that wavelets (see [2] chapter 5, [13]) and mo-ments of wavelet coefficients (see [14], or [15] for the specific problem of irisrecognition) can be very useful for these types of problems. Therefore weimplemented our method in the context of a wavelet image transform forthis case study, even though we then summarized the information obtainedwith the functional dissipation with moments of the dissipated images inthe original representation space. Other choices of statistical quantities arepossible in principle according to the specific application, since, as we showin section 2, the method can be formulated in a quite general setting.In section 2 we introduce the features enhancement method, functionaldissipation, to explore the feature space. In sections 3 and 4 we apply themethod to the crystallization data to show how the application of a suffi-ciently large number of different functional dissipations can greatly improvethe classification error rates.3\f2 Functional dissipation for classificationIn this section we introduce a classification algorithm that is designed forcases where feature identification is complicated or difficult. We first out-line the major steps of the algorithm, and then discuss specific implemen-tations. In the following sections we apply one possible implementation ofthis method to droplet classification, and describe the results.We begin with input data divided into a training set and a test set. Wethen follow four steps as follows:A Choose a classifier and a figure of merit that quantifies the classificationquality.B Choose a basic method of generating features from the input data, andthen enhance the features by a recursive process of structure dissipa-tion, described below.C Compute the figure of merit from A for all features derived in B. For afixed integer p, search the feature space defined in B for the p featureswhich maximize the figure of merit in A on the training set.D Apply the classifier from A, using the optimal p features from C, toclassify the test set.In step A, for example, multivariate linear discrimination can be usedas a classifier. This method comes with a built-in figure of merit, the ra-tio of the between-group variance to the within-group variance. More so-phisticated classifiers often have closely associated evaluation parameters.Cross-validation or leave-one-out error rates can be used.Step B is the heart of the functional dissipation algorithm. The featuresused will depend on the problem. For two-dimensional images, orthogonalor over-complete image transforms can be used. The method of functionaldissipation is a way to leverage the extraction of general purpose featuresto generate features with increased classification power. This method usesthe transforms recursively to gradually modify the feature set.Consider a single input datum X and several invertible transforms Tk,k = 1, . . . , K, that can be applied to X (in the case study to be shownlater, X represents a 256 × 256 gray scale image and Tk represent DiscreteWavelet Transforms). At each iteration we select several coefficients from theinput datum X and we use the mask to modify the coefficients themselves.Fix positive integers K, M and set X0 = X Let A(x) be a discrete valuedfunction defined on ZZ, which we call a mask or a masking function.4\fApply the following functional dissipation steps (B1)-(B3) K times. Fork = 1, . . . , K:(B1): Compute the transform TkXk−1(B2): Choose a subset S of TkXk−1 and collect the M coefficients C(m),m = 1, ..., M in S with largest absolute value in a suitable subset.(B3): Apply the mask: Set C ′(m) = A(m)C(m), and modify the corre-k (TkXk−1)′sponding coefficients of TkXk−1 in the same fashion. Set Xk = T −1to be the inverse of the modified coefficients (TkXk−1)′.At the",
            {
                "entities": [
                    [
                        49,
                        102,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Pattern Recognition 138 (2023) 109400 Contents lists available at ScienceDirect Pattern Recognition journal homepage: www.elsevier.com/locate/patcog Learning from multiple annotators for medical image segmentation Le Zhang a , b , 1 , Ryutaro Tanno d , 1 , Moucheng Xu b , Yawen Huang f , ∗, Kevin Bronik a , Chen Jin b , Joseph Jacob b , c , Yefeng Zheng f , Ling Shao g , Olga Ciccarelli a , Frederik Barkhof a , e , Daniel C. Alexander b , ∗a Queen Square Institute of Neurology, Faculty of Brain Sciences, University College London, London, WC1B 5EH, United Kingdom b Centre for Medical Image Computing, Department of Computer Science, University College London, London, WC1E 6BT, United Kingdom c UCL Respiratory, University College London, London, WC1E 6JF, United Kingdom d Healthcare Intelligence, Microsoft Research, Cambridge, CB1 2FB, United Kingdom e Amsterdam UMC, Vrije Universiteit Amsterdam, Department of Radiology and Nuclear Medicine, Amsterdam, Netherlands f Tencent Jarvis Lab, Shenzhen, China g Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates a r t i c l e i n f o a b s t r a c t Article history: Received 10 May 2022 Revised 18 December 2022 Accepted 5 February 2023 Available online 11 February 2023 Keywords: Multi-Annotator Label fusion Segmentation Supervised machine learning methods have been widely developed for segmentation tasks in recent years. However, the quality of labels has high impact on the predictive performance of these algorithms. This issue is particularly acute in the medical image domain, where both the cost of annotation and the inter-observer variability are high. Different human experts contribute estimates of the ”actual” segmen- tation labels in a typical label acquisition process, influenced by their personal biases and competency levels. The performance of automatic segmentation algorithms is limited when these noisy labels are used as the expert consensus label. In this work, we use two coupled CNNs to jointly learn, from purely noisy observations alone, the reliability of individual annotators and the expert consensus label distribu- tions. The separation of the two is achieved by maximally describing the annotator’s “unreliable behav- ior” (we call it “maximally unreliable”) while achieving high fidelity with the noisy training data. We first create a toy segmentation dataset using MNIST and investigate the properties of the proposed algorithm. We then use three public medical imaging segmentation datasets to demonstrate our method’s efficacy, including both simulated (where necessary) and real-world annotations: 1) ISBI2015 (multiple-sclerosis lesions); 2) BraTS (brain tumors); 3) LIDC-IDRI (lung abnormalities). Finally, we create a real-world mul- tiple sclerosis lesion dataset (QSMSC at UCL: Queen Square Multiple Sclerosis Center at UCL, UK) with manual segmentations from 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label). In all datasets, our method consistently outperforms competing methods and relevant baselines, especially when the number of annotations is small and the amount of disagreement is large. The studies also reveal that the system is capable of capturing the complicated spatial characteristics of annotators’ mistakes. © 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) 1. Introduction The performance of downstream supervised machine learning models is known to be influenced by substantial inter-reader vari- ability when segmenting anatomical structures in medical images [26] . This issue is especially acute in the medical image domain, ∗ Corresponding authors. E-mail addresses: yawenhuang@tencent.com (Y. Huang), d.alexander@ucl.ac.uk (D.C. Alexander) . 1 Authors contributed equally. where labelled data is commonly scarce due to the high cost of annotations. For instance, because of the heterogeneity in lesion lo- cation, size, shape, and anatomical variability across patients [29] , accurate identification of multiple sclerosis (MS) lesions in MRIs is difficult even for experienced experts. Another example [21] shows that glioblastoma (a kind of brain tumour) segmentation had an average inter-reader variability of 74–85%. Segmentation annota- tions of structures in medical image suffer from substantial anno- tation variations, which is exacerbated by disparities in biases and level of expertise [18] . As a result, despite the current quantity of medical imaging data due to almost two decades of digitisation, the world still lacks access to data with curated labels that can be https://doi.org/10.1016/j.patcog.2023.109400 0031-3203/© 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \fL. Zhang, R. Tanno, M. Xu et al. Pattern Recognition 138 (2023) 109400 used by machine learning [14] , necessitating the use of intelligent algorithms to learn robustly from such noisy annotations. Different pre-processing techniques are often used to curate segmentation annotations by fusing labels from different experts in order to minimise inter-reader differences. The most basic and widely used approach is based on a majority vote, with the most representative expert opinion being treated as the expert consen- sus label. In the aggregation of brain tumour segmentation labels, a smarter variant [21] that accounts for class similarity has proven effective. However, one major limitation with such approaches is that all experts are presumed to be equally trustworthy [25] . proposed a label fusion approach, which is called STAPLE. This method explic- itly models individual expert reliability and uses that knowledge to ”weight” their judgments in the label aggregation step. STAPLE has been the go-to label fusion method in the construction of pub- lic medical image segmentation datasets, such as ISLES [27] , MSSeg [11] , and Gleason’19 [12] datasets, after demonstrating its superior- ity over traditional majority-vote pre-processing in various appli- cations. Asman further extended this strategy in [4] by accounting for voxel-wise consensus to solve the issue of annotators’ reliabil- ity being under-estimated. Another extension [5] was proposed to model the annotator’s reliability across different pixels in images. More recently, STAPLE has been modified in numerous ways to en- code the information of the underlying images into the label aggre- gation process in the context of multi-atlas segmentation problems [2,16] where image registration is used to warp segments from la- belled images (”atlases”) onto a new scan. STEP, which is a way to further incorporate the local morphological similarity between atlases and target images in [8] , is a notable example, and sev- eral extensions of this approach, such as [1,6] , have subsequently been examined. However, all of the previous label fusion methods have one major limitation: they don’t have a way to integrate in- formation from distinct training images. This severely restricts the scope of applications to situations in which each image has a rea- sonable number of annotations from multiple experts, which can be prohibitively expensive in practise. Moreover, to model the re- lationship between observed noisy annotations, expert consensus label and reliability of experts, relatively simplistic functions are utilized, which may fail to capture complex characteristics of hu- man annotators. In this paper, we introduce and fully evaluate an unique end- to-end segmentation approach that predicts the reliability of mul- tiple human annotators and the expert consensus label based on noisy labels alone. We use the Morpho-MNIST framework [9] to perform morphometric operations on the MNIST dataset to simu- late a variety of annotator types for evaluation. We also demon- strate the potential in several public medical imaging datasets, namely (i) MS lesion segmentation dataset (ISBI2015) from the ISBI 2015 challenge [7] , (ii) Brain tumour segmentation dataset (BraTS) [21] and (iii) Lung nodule segmentation dataset (LIDC-IDRI) [3] . Furthermore, we create a practical MS lesion segmentation dataset with 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label) to evaluate our model’s performance in real-world data. Experiments on all datasets demonstrate that our method consistently leads to bet- ter segmentation performance compared to widely adopted label- fusion methods and other relevant baselines, especially when the number of available labels for each image is low and the degree of annotator disagreement is high. The main contributions of our approach are: (1) A novel deep CNN architecture is proposed for jointly learn- ing the expert consensus label and the annotator’s label. The pro- posed architecture ( Fig. 1 ) consists of two coupled CNNs where one estimates the expert consensus label probabilities and the other 2 models the characteristics of individual annotators (e.g., tendency to over-segmentation, mix-up between different classes, etc) by es- timating the pixel-wise confusion matrices (CMs) on a per image basis. Unlike STAPLE [25] and its variants, our method models, and disentangles with deep neural networks, the complex mappings from the input images to the annotator behaviours and to the ex- pert consensus label. (2) The parameters of our CNNs are “global variables” that are optimised across different image samples; this enables the model to disentangle robustly the annotators’ mistakes and the expert consensus label based on correlations between similar image sam- ples, even when the number of available annotations is small per image (e.g., a single annotation per image). In contrast, this would not be possible with STAPLE [25] and its variants [5,8] where the annotators’ pa",
            {
                "entities": [
                    [
                        149,
                        213,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Edith Cowan University Edith Cowan University Research Online Research Online Research outputs 2022 to 2026 1-1-2022 A review of arthritis diagnosis techniques in artificial intelligence A review of arthritis diagnosis techniques in artificial intelligence era: Current trends and research challenges era: Current trends and research challenges Maleeha Imtiaz Edith Cowan University Syed Afaq Ali Shah Edith Cowan University, afaq.shah@ecu.edu.au Zia ur Rehman Follow this and additional works at: https://ro.ecu.edu.au/ecuworks2022-2026 Part of the Diseases Commons 10.1016/j.neuri.2022.100079 Imtiaz, M., Shah, S. A. A., & ur Rahman, Z. (2022). A review of arthritis diagnosis techniques in artificial intelligence era: Current trends and research challenges. Neuroscience Informatics, 2, Article 100079. https://doi.org/10.1016/j.neuri.2022.100079 This Journal Article is posted at Research Online. https://ro.ecu.edu.au/ecuworks2022-2026/1430 \fNeuroscience Informatics 2 (2022) 100079Contents lists available at ScienceDirectNeuroscience Informaticswww.elsevier.com/locate/neuriArtificial Intelligence in Brain InformaticsA review of arthritis diagnosis techniques in artificial intelligence era: Current trends and research challengesMaleeha Imtiaz a, Syed Afaq Ali Shah b,∗a Joondalup Health Campus, Australiab Centre for AI and Machine Learning, Edith Cowan University, Australiac Cairns Hospital, Cairns, QLD, Australia, Zia ur Rehman ca r t i c l e i n f oa b s t r a c tArticle history:Received 2 January 2022Received in revised form 4 May 2022Accepted 12 May 2022Keywords:Machine learning (ML)Deep learningOsteoarthritisRheumatoid arthritisDeep learning, a branch of artificial intelligence, has achieved unprecedented performance in several domains including medicine to assist with efficient diagnosis of diseases, prediction of disease progression and pre-screening step for physicians. Due to its significant breakthroughs, deep learning is now being used for the diagnosis of arthritis, which is a chronic disease affecting young to aged population. This paper provides a survey of recent and the most representative deep learning techniques (published between 2018 to 2020) for the diagnosis of osteoarthritis and rheumatoid arthritis. The paper also reviews traditional machine learning methods (published 2015 onward) and their application for the diagnosis of these diseases. The paper identifies open problems and research gaps. We believe that deep learning can assist general practitioners and consultants to predict the course of the disease, make treatment propositions and appraise their potential benefits.© 2022 The Author(s). Published by Elsevier Masson SAS. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).1. IntroductionArthritis is a term which is used for various inflammatory con-ditions that affect different parts of the body such as joints, bones, and muscles. It can be of several types such as Osteoarthritis (OA), Rheumatoid Arthritis (RA), juvenile Arthritis, psoriatic arthri-tis, and gouty Arthritis, which can result in stiffness, pain, redness and swelling in the joints [47]. According to [5], it has been re-vealed that about 3.6 million (15%) of people are affected from arthritis which includes 17.9% females and 12.1% males. Moreover, 62% of patients affected from arthritis had Osteoarthritis, 12.7% had rheumatoid arthritis, and 32.1% had suffered from an unspeci-fied form of arthritis. One in every seven Australians has Arthritis [6]. The prevalence of arthritis rises with age, primarily affecting the females (ABS, 2017). Moreover, higher mortality risk is also recorded in patients with rheumatoid arthritis (RA) as compared to the general population [22], [52].Rheumatic diseases are chronic and fluctuating in nature, in-volving complicated and unclear etiology, which further intricates the treatment of this kind of arthritis [12], [57], [52]. Regardless, even from the invention of various biological and synthetic treat-ments for rheumatoid arthritis (RA), the decrease in disease pro-* Corresponding author.E-mail address: afaq.shah@ecu.edu.au (S.A.A. Shah).gression is achieved only in a small subset of patients [23], [36]. Moreover, the clinical experiments for another rheumatic disease that is Osteoarthritis (OA) are not very fruitful due to different disease phenotypes involved in the disease. Therefore, the dis-ease diagnosis at an early stage can slow down its progression, where diagnosis involves numerous imaging modalities such as X-rays, MRI and CT. However, diagnosis techniques, such as Kellgren-Lawrence (KL) grade suffer from subjectivity, as their accuracy heavily depends on the practitioner’s experience [65]. Table 1 pro-vides details of Kellgren and Lawrence grading for completion. In order to make the diagnosis process more systematic and reli-able, computer-aided analysis and predictive modelling is required to overcome the human errors and for early disease detection in places where there are fewer experts available.In addition, to advent an appropriate treatment for arthritis, a data-intensive investigation is essential where artificial intelligence (AI) can play a significant role in disease detection. Exclusively, ma-chine learning (ML), a subfield of AI aims to design data-driven predictive models which possess the ability to learn from the ex-perience regardless of the rules explicitly specified by individuals [23]. It uses methods, algorithms and processes to expose con-cealed associations within data and to produce prescriptive, de-scriptive and predictive tools in order to exploit these associations [27]. Additionally, the advancement of Machine Learning principles and Artificial Intelligence techniques has increased the productiv-https://doi.org/10.1016/j.neuri.2022.1000792772-5286/© 2022 The Author(s). Published by Elsevier Masson SAS. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\fM. Imtiaz, S.A.A. Shah and Z. ur RehmanNeuroscience Informatics 2 (2022) 100079Table 1Kellgren and Lawrence (KL) grading.KL gradeDiagnosis01234No features of osteophytes are presentNarrowing of joint space, doubtful OACertain narrowing of joint space, minor OAMultiple osteophytes, sure joint space narrowing and some sclerotic areas, moderate OALarge osteophytes, severe joint space narrowing, severe sclerosis and bone deformity, Severe OAity and effectiveness in medical imaging research [55]. Machine learning concepts, when applied to medical data, have great po-tential to improve disease diagnosis and early detection of diseases [11], [48], [57], [8]. In clinical settings, these techniques can help medical experts to analyse the disease in a better way to predict potential future issues and treat patients more effectively.Machine learning algorithms are capable of learning useful data representations automatically [40], [50]. They can deal with a vari-ety of data inputs such as genetic information, text e.g., electronic health records, patient cohorts and medical images. Furthermore, it can also learn from the knowledge available from clinical data and generate outcomes by recognising disease patterns, and features. Further, it can also help in optimising treatment strategies. Hence, it is quite evident that ML has helped in significantly filling the gap of automatic learning from clinical experience. Furthermore, Deep learning (DL), is a subfield of ML, which utilises multi-layered neu-ral networks, intensive computational algorithms and big data [23], [46]. Over the last decade, both ML and DL have been used in the field of medicine for medical imaging, and it has been depicted that ML-based decision-making is superior to physicians’ individ-ual clinical trial decisions [23].Inspired by the recent advent of artificial intelligence in medical field, this paper presents a survey of deep learning and traditional machine learning techniques for the diagnosis of osteoarthritis and rheumatoid arthritis. The paper also aims to identify the current challenges and open research problems in this area. In contrast to current review papers [23], [55], [24], [28] which mainly focus on a specific type of arthritis e.g., OA or RA and machine learning tech-niques only, this paper reviews deep learning as well as machine learning methods for the diagnosis of both OA and RA. In addition, this paper also provides detailed information about the publicly available datasets for RA and OA research (Section 4.2). This makes our survey paper different from the existing review articles.The rest of the paper is organised as follows. Section 2 discusses most common arthritis types. Overview of the most popular ma-chine and deep learning techniques is presented in Section 3. An overview of imaging techniques and arthritis datasets is provided in Section 4. Machine learning and deep learning approaches for the diagnosis of arthritis are presented in Section 5 and 6, respec-tively. Section 7 discusses some of the open research problems and research challenges. The paper is concluded in Section 8.2. Arthritis and its typesArthritis is a degenerative disorder associated with human joints that can result in disability. There are numerous types of arthritis such as rheumatoid arthritis, Osteoarthritis, Juvenile Arthritis, Psoriatic arthritis and gout arthritis. In the following, we will briefly discuss rheumatoid arthritis, osteoarthritis and Psori-atic arthritis.2.1. Rheumatoid arthritisRheumatoid Arthritis (RA) is an autoimmune inflammatory dis-order which involves multiple organs affecting one or more joints [27]. It is a disease with unclear etiology and a combination of ge-netic and environmental factors. The complex interactions of these factors affect disease development and progression [29]. In gen-eral, RA is categorised through morning stiffness and inflammation of joints that requires skills and experience f",
            {
                "entities": [
                    [
                        187,
                        300,
                        "TITLE"
                    ],
                    [
                        647,
                        760,
                        "TITLE"
                    ],
                    [
                        1126,
                        1239,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptFuture Gener Comput Syst. Author manuscript; available in PMC 2022 February 01.Published in final edited form as:Future Gener Comput Syst. 2021 February ; 115: 610–618. doi:10.1016/j.future.2020.09.040.Estimation of laryngeal closure duration during swallowing without invasive X-raysShitong Maoa, Aliaa Sabryb, Yassin Khalifaa, James L Coyleb, Ervin Sejdica,c,*aDepartment of Electrical and Computer Engineering, Swanson School of Engineering, University of Pittsburgh, Pittsburgh, PA 15260 USAbDepartment of Communication Science and Disorders, School of Health and Rehabilitation Sciences, University of Pittsburgh, Pittsburgh, PA 15260 USAcDepartment of Bioengineering, Swanson School of Engineering Department of Biomedical Informatics, School of Medicine Intelligent Systems Program, School of Computing and Information, University of Pittsburgh, Pittsburgh, PA 15260 USAAbstractLaryngeal vestibule (LV) closure is a critical physiologic event during swallowing, since it is the first line of defense against food bolus entering the airway. Identifying the laryngeal vestibule status, including closure, reopening and closure duration, provides indispensable references for assessing the risk of dysphagia and neuromuscular function. However, commonly used radiographic examinations, known as videofluoroscopy swallowing studies, are highly constrained by their radiation exposure and cost. Here, we introduce a non-invasive sensor-based system, that acquires high-resolution cervical auscultation signals from neck and accommodates advanced deep learning techniques for the detection of LV behaviors. The deep learning algorithm, which combined convolutional and recurrent neural networks, was developed with a dataset of 588 swallows from 120 patients with suspected dysphagia and further clinically tested on 45 samples from 16 healthy participants. For classifying the LV closure and opening statuses, our method achieved 78.94% and 74.89% accuracies for these two datasets, suggesting the feasibility of implementing sensor signals for LV prediction without traditional videofluoroscopy screening methods. The sensor supported system offers a broadly applicable computational approach for *Correponding author: esejdic@pitt.edu (Ervin Sejdic).Shitong Mao: Methodology, Formal analysis, Software, Writing- Original draft preparation. Aliaa Sabry: Data curation, Resources, Investigation, Writing- Original draft preparation. Yassin Khalifa: Data Curation, Methodology, Investigation. James L Coyle: Conceptualization, Data Curation, Resources, Writing-Reviewing and Editing. Ervin Sejdic: Writing-Reviewing and Editing, Supervision, Project administration, Funding acquisition.Declaration of competing interestWe declare we have no competing interests.Declaration of interests□ The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptMao et al.Page 2clinical diagnosis and biofeedback purposes in patients with swallowing disorders without the use of radiographic examination.Graphical AbstractKeywordsLaryngeal vestibule closure; High resolution cervical auscultation (HRCA); Deep learning; Dysphagia; Health-care1. IntroductionFor humans, the respiration and digestive systems share the same entrance, therefore, protecting the airway from food bolus entering the trachea or lungs is a fundamental requirement for safe swallowing. Laryngeal vestibule (LV) closure has been considered the first line of defense against swallowed material entering the airway [1, 2, 3]. Likewise, the duration of LV closure is a predictor of airway invasion during swallowing. If the laryngeal closure is absent or its duration is too short, this can lead to aspiration/penetration[4, 5]. Aspiration has been considered as a major concern for individuals with dysphagia (swallowing disorders), especially in neurologic and neurodegenerative diseases, where aspiration-related respiratory infections are a leading cause of death[6]. Therefore, proper evaluation of LV closure and duration could provide an objective outcome measure to improve the assessment of swallowing safety, provide clinical evidence of increased risk of airway compromise during swallowing, and guide the instigation of appropriate compensatory interventions.The videofluoroscopy swallowing study (VFSS) is the only instrumental assessment technique that can visualize the event of LV closure and determine its duration during swallowing through the kinematic analysis of radiographic images[6, 7, 8]. However, practical issues will raise when the VFSS is implemented: it exposes patients to radiation, and is not feasible in all facilities without x-ray departments or qualified clinicians to perform and interpret the VFSS images.[9, 10]. Additionally, it is not suitable for the cases in which patients prefer not to undergo x-ray testing or when patients are unable to participate in the examination protocols[10, 11, 12, 13].Future Gener Comput Syst. Author manuscript; available in PMC 2022 February 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptMao et al.Page 3Furthermore, there are certain limitations of the ordinary clinical setting preventing the more frequent temporal analysis of swallowing events using VFSS images which provides quantification of LV closure at baseline and assessment of treatment efficacy. Frame-by-frame review of VFSS video is time-consuming and some clinicians may not have the ability to record VFSS images for secondary review due to lack of equipment or limited access to archived materials. Clinicians tend to comment on whether and at what phase of the swallow, the material enters the laryngeal vestibule without determining whether LV closure itself was shortened, further limiting inferences that can lead to treatment decisions[14].Because of the previously mentioned drawbacks and limitations of VFSS in the detection of LV closure and reopening, it would be practically beneficial for patients and clinicians to investigate an alternative, non-invasive tool. High-resolution cervical auscultation (HRCA) is a promising non-invasive method for dysphagia screening assessment and management[15]. It uses high-resolution accelerometers and microphones, attached to patients’ necks to record vibratory and acoustic signals during swallowing[16, 17]. The advantages of such a sensor supported approach include mobility, cost-effectiveness, non-invasiveness, and suitability for. day-to-day and even minute-to-minute monitoring[15, 18]. To investigate the relationship between the signals and the LV shifting, previous studies postulated the cardiac analogy hypothesis that explained the elusive physiologic cause of swallowing sounds[19]. This theory suggested that cervical auscultation acoustic signals are generated via vibrations caused by valve and pump systems within the vocal tract. Moreover, HRCA signal features have been found to be associated with LV closure onset and LV reopening[20]. The slapping of the epiglottis and aryepiglottic fold may provide the valve activity that generates swallowing sounds and neck vibration which can be recorded with HRCA.All these studies indicated the possibility of detecting the LV closure and reopening, and a method of determining the closure duration solely based on the HRCA signals. However, no studies attempted to quantitatively implement such an idea. The main challenge was that the explicit dependencies between the signal features and the LV behaviors were not mathematically established. In this study, we sought to investigate the ability of HRCA signals to identify LV status with an advanced deep learning model, which approximated the relationship with training examples. We hypothesized that the computer-aided algorithm with HRCA signals which were acquired from the neck was able to detect the event of LV switching, and estimate the duration of LV closure.The machine learning and deep learning methods have already become powerful tools in the health-care applications and widely employed in the computer-assisted diagnosis for swallowing, laryngeal and neck disorders or disease[21]. Based on larynx contact endoscopic video images, Esmaeili et al. attempted to apply support vector machine, k-nearest neighbor, and random forest to classify benign and malignant lesions on the superficial layers of laryngeal mucosa[22]. For early stage diagnosis of laryngeal squamous cell carcinoma, Moccia et al. implemented a support vector machine classifier with features extracted from the laryngeal endoscopic frames, and they achieved 93% sensitivity[23]. For the similar purpose, Araújo et al. applied transfer learning with pre-trained Convolutional Neural Network (CNN) models to process laryngoscopy images, and they achieved state-of-Future Gener Comput Syst. Author manuscript; available in PMC 2022 February 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptMao et al.Page 4art performance[24]. Our previous study also performed multi-scale CNN filers for hyoid bone detection on the VFSS images[25]. All those studies were conducted based on images as model input. Only several studies attempted to use signals in time series as input and build up deep learning models to serve the swallowing or laryngeal ap",
            {
                "entities": [
                    [
                        300,
                        382,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Procedia Computer ScienceVolume 53, 2015, Pages 345–3552015 INNS Conference on Big DataA classification algorithm for high-dimensional data Asim Roy1 1Department of Information Systems, Arizona State University, Tempe, Arizona, USA Asim.Roy@asu.edu Abstract With the advent of high-dimensional stored big data and streaming data, suddenly machine learning on a very large scale has become a critical need. Such machine learning should be extremely fast, should scale up easily with volume and dimension, should be able to learn from streaming data, should automatically perform dimension reduction for high-dimensional data, and should be deployable on hardware. Neural networks are well positioned to address these challenges of large scale machine learning. In this paper, we present a method that can effectively handle large scale, high-dimensional data. It is an online method that can be used for both streaming and large volumes of stored big data. It primarily uses Kohonen nets, although only a few selected neurons (nodes) from multiple Kohonen nets are actually retained in the end; we discard all Kohonen nets after training. We use Kohonen nets both for dimensionality reduction through feature selection and for building an ensemble of classifiers using single Kohonen neurons. The method is meant to exploit massive parallelism and should be easily deployable on hardware that implements Kohonen nets. Some initial computational results are presented. Keywords: Kohonen nets; classification algorithm; online learning; feature selection; high-dimensional data 1 Introduction The arrival of big and streaming data is forcing major changes to the machine learning field. In this new era, there are significantly more demands on machine learning systems - from the need to handle very large volumes of data and fast learning to the need for automation of machine learning that requires less expert assistance and for hardware deployment. Traditional artificial neural network algorithms have many properties that can meet these demands of big data and thus can certainly play a key role in the major transformations that are taking place. For example, many neural net algorithms are based on the concept of online, incremental learning that does not require simultaneous access to large volumes of data. This mode of learning not only resolves many computational issues, it also Selection and peer-review under responsibility of the Scientific Programme Committee of INNS-BigData2015c(cid:2) The Authors. Published by Elsevier B.V.345doi: 10.1016/j.procs.2015.07.311    \fA classification algorithm for high-dimensional dataAsim Royremoves the headache of correctly sampling from large volumes of data. It also makes neural net algorithms highly scalable (i.e. they can easily handle large volumes of data without running into computer memory limitations and learning (processing) time scales up linearly with data volume because of incremental learning) and provides them the capability to learn from all of the data. Neural network algorithms also have the advantage that they use simple computations that can be highly parallelized. These algorithms are already being implemented on hardware that allows parallel computations (Oh & Jung, 2004) and more powerful hardware is on the way (Monroe, 2014; Poon & Zhou, 2011; Furber et al., 2013). All these features of neural network algorithms positions the field to become the backbone of machine learning applications in the era of big and streaming data. In this paper, we present a new neural network learning method that (1) can be parallelized at different levels of granularity, (2) addresses the issue of high-dimensional data through class-based feature selection, (3) learns an ensemble of classifiers using selected Kohonen neurons (nodes) from different Kohonen nets (Kohonen, 2001), and (4) can be easily implemented on hardware. For dimensionality reduction through feature selection, we train a number of Kohonen nets in parallel with streaming data to create some representative data points. (Note that stored data can also be streamed.) Using these Kohonen nets, we perform class-based feature selection (Roy et al., 2013). The basic criteria for feature selection are to select features for each class that (1) makes the class more compact, and (2) at the same time, maximize the average distance from the other classes. Once class-based feature selection is complete, we discard these Kohonen nets. In the second phase, we construct several new Kohonen nets in parallel in different feature spaces, again from streaming data, based on the selected features. Once trained, we then extract just the active neurons from these different Kohonen nets, add class labels to them and create an ensemble of Kohonen neurons for classification. In the end, we just retain a set of dangling active Kohonen neurons from different Kohonen nets in different feature spaces and discard all Kohonen nets. The paper is organized as follows. Section 2 provides an overview of the concept of class-based feature selection and separability index of features. Section 3 has the algorithm for class-based feature selection from streaming data using Kohonen nets in parallel. Sections 4 provide details on how an ensemble classifier is constructed using neurons from different Kohonen nets. Section 5 has computational results for several high-dimensional problems and the conclusions are in Section 6. 2 Class-specific feature selection, separability index of features and dimensionality reduction A fundamental challenge for machine learning is learning from high-dimensional data. A number of new methods have been developed for both online feature selection and feature extraction for high-dimensional streaming data (Yan et al., 2006; Hoi et al., 2012; Wu et al., 2010; Law et al., 2006). However, none of them are for class-specific feature selection. Since 1997, at various conferences, Roy had proposed methods that use a subset of the original features in class-specific classifiers and Roy et al. (2013) presents one such method. However, the method in Roy et al. (2013) does not work for streaming data. In class-specific feature selection, we find separate feature sets for each class such that they are the best ones to separate that class from the rest of the classes. The concept we use is identical to the one used by LDA and Maximum Margin Criterion (MMC) methods (Li et al. 2006) that maximize the between-class scatter and minimize the within-class scatter. In other words, those methods try to maximize the distance between different class centers and at the same time make the data points in the same class as close as possible. Our method, although not a feature extraction method, is based on the same concept. 346   \fA classification algorithm for high-dimensional dataAsim RoyIn an offline mode, where a collection of data points is available, it is fairly easy to select features that maximize the average distance of data points of one class from the rest of the classes and also, at the same time, minimize the average distance of data points within that class. Roy et al. (2013) ranks and selects features on this basis and computational experiments show that it works quite well. However, that method cannot be used for streaming data where no data is stored. In the proposed method, we use the same concept for feature selection, but train multiple Kohonen nets from streaming data to do so. By training multiple Kohonen nets, we essentially create some representative data points for each class and that’s how we resolve the dilemma of not having access to a collection of data points. Once we have a collection of representative data points (represented by certain Kohonen neurons in the Kohonen nets), it is easy to use the class-based feature selection method proposed in Roy et al. (2013). We train many different Kohonen nets, of different grid sizes and for different feature subsets, and we train them in parallel on a distributed computing platform. We use Apache Spark (2015) as our distributed computing platform, but other similar platforms can be used. A Kohonen net forms clusters and the cluster centers (that is, the active Kohonen net nodes or neurons) are equivalent to representative examples of the streaming data. We then use these representative examples to select features by class.  in is the average distance between patterns within class k for feature n, and dknSuppose there are kc total classes. Our basic feature ranking principles are that (1) a good feature for class k should produce good separation between patterns in class k and those not in class k, k = 1…kc, and (2) also make the patterns in class k more compact. Roy et al. (2013) uses a measure called the separability index that is based on these concepts and can rank features for each class. out the Suppose dknaverage distance between the patterns in class k and those not in class k for feature n.  Roy et al. (2013) uses the Euclidean distance for distance measure, but other distance measures could be used. in. Roy et al. (2013) uses this separability index rkn to The separability index is given by rkn = dknrank order features of class k where a higher ratio implies a higher rank. The sense of this measure is out increases the that a feature n with a lower dknseparation of class k from the other classes. Thus, higher the ratio rkn for a feature n, greater is its ability to separate class k from the other classes and better is the feature. in makes class k more compact and with a higher dknout / dkn2.1 Why class-based feature selection? An example Gene Number Separability Indices by Class AML ALL Good Good AML Features 758 1809 4680 ALL Features 2288 760 6182 82.53 75.25 39.73 0.85 0.93 0.8 2.49 1.85 2.82 114.75 98.76 34.15 Table 2.1 – Separability indices for a few features in the AMLALL gene expression dataset 347      \fA classification algorithm for high-dimensional dataAsim RoyWe solved a number o",
            {
                "entities": [
                    [
                        87,
                        139,
                        "TITLE"
                    ],
                    [
                        2583,
                        2635,
                        "TITLE"
                    ],
                    [
                        6795,
                        6847,
                        "TITLE"
                    ],
                    [
                        9920,
                        9972,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Manuscript version: Author’s Accepted Manuscript The version presented in WRAP is the author’s accepted manuscript and may differ from the published version or Version of Record. Persistent WRAP URL: http://wrap.warwick.ac.uk/125464              How to cite: Please refer to published version for the most recent bibliographic citation information. If a published version is known of, the repository item page linked to above, will contain details on accessing it. Copyright and reuse: The Warwick Research Archive Portal (WRAP) makes this work by researchers of the University of Warwick available open access under the following conditions. © 2019 Elsevier. Licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International http://creativecommons.org/licenses/by-nc-nd/4.0/. Publisher’s statement: Please refer to the repository item page, publisher’s statement section, for further information. For more information, please contact the WRAP Team at: wrap@warwick.ac.uk. warwick.ac.uk/lib-publications          \fA Convolutional Neural Network Approach to Detect Congestive Heart Failure Mihaela Porumb1, Ernesto Iadanza2, Sebastiano Massaro3 and Leandro Pecchia1* 1 University of Warwick, School of Engineering, Coventry CV4 7AL, UK 2 University of Florence, v. S. Marta, 3, Florence, IT (E-mail: Ernesto.Iadanza@unifi.it) 3 The Organizational Neuroscience Laboratory, London WC1N 3AX, UK; University of Surrey, Guildford, GU2 7XH, UK (E-mail: sebastiano.massaro@theonelab.org) How to Cite: “Porumb, M., Iadanza, E., Massaro, S., & Pecchia, L. (2020). A convolutional neural network approach to detect congestive heart failure. Biomedical Signal Processing and Control, 55, 101597. DOI: https://doi.org/10.1016/j.bspc.2019.101597” Link: https://www.sciencedirect.com/science/article/pii/S1746809419301776 *Corresponding author: E-mail address: L.Pecchia@warwick.ac.uk, University of Warwick, School of Engineering, Coventry CV4 7AL, UK Abstract Congestive Heart Failure (CHF) is a severe pathophysiological condition associated with high prevalence, high mortality rates, and sustained healthcare costs, therefore demanding efficient methods for its detection. Despite recent research has provided methods focused on advanced signal processing and machine learning, the potential of applying Convolutional Neural Network (CNN) approaches to the automatic detection of CHF has been largely overlooked thus far. This study addresses this important gap by presenting a CNN model that accurately identifies CHF on the basis of one raw electrocardiogram (ECG) heartbeat only, also juxtaposing existing methods typically grounded on Heart Rate Variability. We trained and tested the model on publicly available ECG datasets, comprising a total of 490,505 heartbeats, to achieve 100% CHF detection accuracy. Importantly, the model also identifies those heartbeat   \fsequences and ECG’s morphological characteristics which are class-discriminative and thus prominent for CHF detection. Overall, our contribution substantially advances the current methodology for detecting CHF and caters to clinical practitioners’ needs by providing an accurate and fully transparent tool to support decisions concerning CHF detection. Keywords: Convolutional Neural Networks, Congestive Heart Failure, Machine Learning 1. INTRODUCTION Congestive Heart Failure (CHF) is a pathophysiological condition responsible for the failure of the heart in pumping blood in the body [1] which has encountered widespread research and societal attention [2]. According to the European Society of Cardiology, around 26 million people worldwide are affected by a form of heart failure [3]. CHF is a strongly degenerative condition, and its prevalence increases quickly with age [4], [5]. The mortality rate is closely associated with the degree of severity, reaching peaks of 40% in the most serious events [e.g.,  New York Heart Association (NYHA) classes III-IV] [6]. CHF is also one of the foremost reasons for hospitalization in the elderly, and it is characterized by a resilient relapse rate, with half of the outpatients readmitted within a few months from hospital discharge [7]. Moreover, just among the most industrialized countries, the healthcare expenditure for CHF consumes 2-3% of the healthcare budgets, with the cost of hospitalization being the greatest proportion of the spending [8]–[10]. Thus, with a worldwide aging population and sustained pressures on healthcare systems and resources, there is the compelling demand—among patients, healthcare providers, policymakers, and the society as a whole—to address this scenario by identifying highly accurate methods to improve detection of heart failures [11] and in turn enable early and more efficient diagnoses. Recently, research has made significant progress in these areas. In particular, given the quantity and complexity of data involved, machine learning techniques and classsifiers (e.g., SVM, MLP, k-NN, CART, Random Forest) [12]–[18] have been successfully applied to analyze, detect, and classify heart failures, as we discussed and showed in previous works [12], [13], [19]–[21]. These approaches able to distinguish between heart failure and healthy subjects are mostly based on Heart Rate Variability (HRV)—the variation over time of the period between consecutive heartbeats extracted from electrocardiographic (ECG) signals [22]—, showing that depressed HRV patterns represent accurate markers for detecting the condition. However, building accurate HRV-based models is time-consuming and prone to error steps, due to the preprocessing and the iterative process of manually selecting appropriate features. \fMoreover, the best performing HRV-based models generally require either long-term signals (i.e., 24h) or at least the combination of short-term HRV with non-standard long-term HRV features, as shown in [23]. To tackle these issues, we present a novel framework of CHF detection that does not rely on HRV features, rather it uses raw ECG signals only. This method is based on a 1-D Convolutional Neural Network (CNN). CNNs are hierarchical neural networks that mimic the human visual system and have proven to be effective in recognizing patterns and structures of input data in image classification, localization, and detection tasks, among others [24]–[26]. Moreover, they have been extensively used for time series analysis in classification tasks. Some successful examples include, among others: general time series classification [27]–[29], speech recognition tasks [30], arrhythmia detection [31], [32], and multivariate diagnostic measurements modeling [33], [34]. Inspired by this growing body of research, we aim to detect CHF through a 1-D CNN approach on the ECG signals. Adding to the significant benefit of building upon raw physiological data, this method enables visualization of the input time series subsequences that are class discriminative (i.e., CHF vs. healthy subjects). This feature considerably improves the interpretability of the CNN model and represents a crucial aspect to ensure the ‘transparency’ of the method [35], [36]. Such a transparency is fundamental to help researchers explain how conclusions are reached, and to aid professionals in better understanding correlations of pathophysiological behaviors and properties revealed by the model. As we shall explain, we used a form of class activation mapping (Grad-CAM) [37] that highlights the class discriminative regions in the input data. In other words, Grad-CAM uses the model’s last activation maps to originate a heat map that can be overlapped with the input, thus showing what regions in the input data contribute most to CHF detection. Overall, the proposed framework puts forward several developments for CHF detection, such as refraining from using hefty preprocessing and features selection steps of HRV-based models, as well as enabling visualization of the subsequences in the input time series which are used to reach certain clinically-relevant conclusions. 2.1 Data 2. METHODS We performed a retrospective analysis on two publicly available datasets. The data for the normal subjects (i.e., control group) were retrieved from the MIT-BIH Normal Sinus Rhythm Database [38] included in PhysioNet [39]. This dataset includes 18 long-term ECG recordings of normal healthy not-arrhythmic subjects (Females=13; age \frange: 20 to 50). The data for the CHF group were retrieved from the BIDMC Congestive Heart Failure Database [40] from PhysioNet [39]. This dataset includes long-term ECG recordings of 15 subjects with severe CHF (i.e., NYHA classes III-IV) (Females =4; age range: 22 to 63). Altogether, the pool of data available for this study consists of about 20 hours of ECG recordings per each subject; it contains two-channel ECG signals sampled at 250 samples per second for the BIDMC dataset, and 128 samples per second for the MIT-BIH dataset. The two datasets used in this study were published by the same laboratory, the Beth Israel Deaconess Medical Center, and were digitized using the same procedures according to the signal specification line in the header file. These datasets have been widely used in several studies concerning CHF detection, as explained in [11]. Indeed, it is an acknowledged practice to build and validate CHF classification models on these sources to ensure opportunities for reproducibility. 2.2 ECG preprocessing The ECG recordings from the BIDMC dataset were down-sampled in order to match the sampling frequency of the ECG signals from the MIT-BIH dataset (i.e., 128 Hz). Records from both databases were already made available with computed beat annotations, which we used to isolate and extract individual heartbeats. We considered a window of 235 ms before the annotated R peak (equivalent to 30 samples=128 samples/s*0.235 s), and a window of 390 ms after the R peak (equivalent to 50 samples). Only the beats that were annotated as normal (N) were retained for further analy",
            {
                "entities": [
                    [
                        1042,
                        1116,
                        "TITLE"
                    ],
                    [
                        1582,
                        1656,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Pattern Recognition 115 (2021) 107899 Contents lists available at ScienceDirect Pattern Recognition journal homepage: www.elsevier.com/locate/patcog Pruning by explaining: A novel criterion for deep neural network pruning Seul-Ki Yeom a , i , Philipp Seegerer a , h , Sebastian Lapuschkin c , Alexander Binder d , e , Simon Wiedemann c , Klaus-Robert Müller a , f , g , b , ∗, Wojciech Samek c , b , ∗a Machine Learning Group, Technische Universität Berlin, 10587 Berlin, Germany b BIFOLD – Berlin Institute for the Foundations of Learning and Data, Berlin, Germany c Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany d ISTD Pillar, Singapore University of Technology and Design, Singapore 487372, Singapore e Department of Informatics, University of Oslo, 0373 Oslo, Norway f Department of Artificial Intelligence, Korea University, Seoul 136–713, Korea g Max Planck Institut für Informatik, 66123 Saarbrücken, Germany h Aignostics GmbH, 10557 Berlin, Germany i Nota AI GmbH, 10117 Berlin, Germany a r t i c l e i n f o a b s t r a c t Article history: Received 18 December 2019 Revised 28 January 2021 Accepted 8 February 2021 Available online 22 February 2021 Keywords: Pruning Layer-wise relevance propagation (LRP) Convolutional neural network (CNN) Interpretation of models Explainable AI (XAI) The success of convolutional neural networks (CNNs) in various applications is accompanied by a sig- nificant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource- constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning. © 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) 1. Introduction Deep CNNs have become an indispensable tool for a wide range of applications [1] , such as image classification, speech recognition, natural language processing, chemistry, neuroscience, medicine and even are applied for playing games such as Go, poker or Su- per Smash Bros. They have achieved high predictive performance, ∗ Corresponding authors. E-mail addresses: yeom@tu-berlin.de (S.-K. Yeom), philipp.seegerer@tu- berlin.de (P. Seegerer), sebastian.lapuschkin@hhi.fraunhofer.de (S. Lapuschkin), alexabin@uio.no (A. Binder), simon.wiedemann@hhi.fraunhofer.de (S. Wiedemann), klaus-robert.mueller@tu-berlin.de (K.-R. Müller), wojciech.samek@hhi.fraunhofer.de (W. Samek). at times even outperforming humans. Furthermore, in specialized domains where limited training data is available, e.g., due to the cost and difficulty of data generation (medical imaging from fMRI, EEG, PET etc.), transfer learning can improve the CNN performance by extracting the knowledge from the source tasks and applying it to a target task which has limited training data. However, the high predictive performance of CNNs often comes at the expense of high storage and computational costs, which are related to the energy expenditure of the fine-tuned network. These deep architectures are composed of millions of parameters to be trained, leading to overparameterization (i.e. having more pa- rameters than training samples) of the model [2] . The run-times are typically dominated by the evaluation of convolutional layers, while dense layers are cheap but memory-heavy [3] . For instance, https://doi.org/10.1016/j.patcog.2021.107899 0031-3203/© 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \fS.-K. Yeom, P. Seegerer, S. Lapuschkin et al. Pattern Recognition 115 (2021) 107899 the VGG-16 model has approximately 138 million parameters, tak- ing up more than 500MB in storage space, and needs 15.5 bil- lion floating-point operations (FLOPs) to classify a single image. ResNet50 has approx. 23 million parameters and needs 4.1 bil- lion FLOPs. Note that overparameterization is helpful for an ef- ficient and successful training of neural networks, however, once the trained and well generalizing network structure is established, pruning can help to reduce redundancy while still maintaining good performance [4] . Reducing a model’s storage requirements and computational cost becomes critical for a broader applicability, e.g., in embedded systems, autonomous agents, mobile devices, or edge devices [5] . Neural network pruning has a decades long history with inter- est from both academia and industry [6] aiming to eliminate the subset of network units (i.e. weights or filters) which is the least important w.r.t. the network’s intended task. For network prun- ing, it is crucial to decide how to identify the “irrelevant” subset of the parameters meant for deletion. To address this issue, pre- vious researches have proposed specific criteria based on Taylor expansion, weight, gradient, and others, to reduce complexity and computation costs in the network. Related works are introduced in Section 2 . From a practical point of view, the full capacity (in terms of weights and filters) of an overparameterized model may not be re- quired, e.g., when (1) parts of the model lie dormant after training (i.e., are per- manently ”switched off”), (2) a user is not interested in the model’s full array of possible outputs, which is a common scenario in transfer learning (e.g. the user only has use for 2 out of 10 available network outputs), or (3) a user lacks data and resources for fine-tuning and running the overparameterized model. In these scenarios the redundant parts of the model will still occupy space in memory, and information will be propagated through those parts, consuming energy and increasing runtime. Thus, criteria able to stably and significantly reduce the com- putational complexity of deep neural networks across applications are relevant for practitioners. In this paper, we propose a novel pruning framework based on Layer-wise Relevance Propagation (LRP) [7] . LRP was originally de- veloped as an explanation method to assign importance scores, so called relevance , to the different input dimensions of a neural net- work that reflect the contribution of an input dimension to the model’s decision, and has been applied to different fields of com- puter vision (e.g., [8–10] ). The relevance is backpropagated from the output to the input and hereby assigned to each unit of the deep model. Since relevance scores are computed for every layer and neuron from the model output to the input, these relevance scores essentially reflect the importance of every single unit of a model and its contribution to the information flow through the network — a natural candidate to be used as pruning criterion. The LRP criterion can be motivated theoretically through the concept of Deep Taylor Decomposition (DTD) (c.f. [11–13] ). Moreover, LRP is scalable and easy to apply, and has been implemented in software frameworks such as iNNvestigate [14] . Furthermore, it has linear computational cost in terms of network inference cost, similar to backpropagation. We systematically evaluate the compression efficacy of the LRP criterion compared to common pruning criteria for two different scenarios. Scenario 1 : We prune pre-trained CNNs followed by subse- quent fine-tuning. This is the usual setting in CNN pruning and requires a sufficient amount of data and computational power. Scenario 2 : In this scenario a pretrained model needs to be transferred to a related problem as well, but the data available for the new task is too scarce for a proper fine-tuning and/or the time consumption, computational power or energy consumption is con- strained. Such transfer learning with restrictions is common in mo- bile or embedded applications. Our experimental results on various benchmark datasets and four different popular CNN architectures show that the LRP crite- rion for pruning is more scalable and efficient, and leads to bet- ter performance than existing criteria regardless of data types and model architectures if retraining is performed (Scenario 1). Especially, if retraining is prohibited due to external constraints after pruning, the LRP criterion clearly outperforms previous crite- ria on all datasets (Scenario 2). Finally, we would like to note that our proposed pruning framework is not limited to LRP and image data, but can be also used with other explanation techniques and data types. The rest of this paper is organized as follows: Section 2 sum- marizes related works for network compression and introduces the typical criteria for network pruning. Section 3 describes the frame- work and details of our approach. The experimental results are il- lustrated and discussed in Section 4 ",
            {
                "entities": [
                    [
                        149,
                        221,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptNeurocomputing. Author manuscript; available in PMC 2017 February 12.Published in final edited form as:Neurocomputing. 2016 February 12; 177: 75–88. doi:10.1016/j.neucom.2015.11.008.Dictionary Pruning with Visual Word Significance for Medical Image RetrievalFan Zhanga,b, Yang Songa, Weidong Caia, Alexander G. Hauptmannc, Sidong Liua, Sonia Pujolb, Ron Kikinisb, Michael J Fulhamd,e, David Dagan Fenga,f, and Mei Cheng,haSchool of Information Technologies, University of Sydney, AustraliabDept of Radiology, Brigham & Womens Hospital, Harvard Medical School, United StatescSchool of Computer Science, Carnegie Mellon University, United StatesdDept of PET and Nuclear Medicine, Royal Prince Alfred Hospital, AustraliaeSydney Medical School, University of Sydney, AustraliafMed-X Research Institute, Shanghai Jiaotong University, ChinagDept of Informatics, University of Albany State University of New York, United StateshRobotics Institute, Carnegie Mellon University, United StatesAbstractContent-based medical image retrieval (CBMIR) is an active research area for disease diagnosis and treatment but it can be problematic given the small visual variations between anatomical structures. We propose a retrieval method based on a bag-of-visual-words (BoVW) to identify discriminative characteristics between different medical images with Pruned Dictionary based on Latent Semantic Topic description. We refer to this as the PD-LST retrieval. Our method has two main components. First, we calculate a topic-word significance value for each visual word given a certain latent topic to evaluate how the word is connected to this latent topic. The latent topics are learnt, based on the relationship between the images and words, and are employed to bridge the gap between low-level visual features and high-level semantics. These latent topics describe the images and words semantically and can thus facilitate more meaningful comparisons between the words. Second, we compute an overall-word significance value to evaluate the significance of a visual word within the entire dictionary. We designed an iterative ranking method to measure overall-word significance by considering the relationship between all latent topics and words. The words with higher values are considered meaningful with more significant discriminative power in differentiating medical images. We evaluated our method on two public medical imaging datasets and it showed improved retrieval accuracy and efficiency.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.KeywordsMedical image retrieval; BoVW; Dictionary pruning1. IntroductionPage 2Content-based medical image retrieval (CBMIR), which retrieves a subset of images that are visually similar to the query from a large image database, is the focus of intensive research (Müller et al., 2004; Akgül et al., 2011; Kumar et al., 2013). CBMIR provides the potential of having an efficient tool for disease diagnosis, by finding related pre-diagnosed cases and it can be used for disease treatment planning and management. In the past three decades, but in particular in the last decade, medical image data have expanded rapidly due to the pivotal role of imaging in patient management and the growing range of image modalities (Duncan and Ayache, 2000; Menze et al., 2014). Traditional text-based retrieval, which manually indexes the images with alphanumerical keywords, is unable to sufficiently meet the increased demand from this growth. At the same time, advances in computer-aided content-based medical image analysis systems mean that there are methods that can automatically extract the rich visual properties/features to characterize the images efficiently (El-Naqa et al., 2004; Lehmann et al., 2004; Napel et al., 2010; Avni et al., 2011; André et al., 2012a; Xu et al., 2012; Zhang et al., 2015c).In CBMIR research, the main challenge is to design an effective image representation so that images with visually similar anatomical structures are closely correlated. A number of research groups are working in this area (Müller et al., 2004; Zhang et al., 2010; Akgül et al., 2011; Kumar et al., 2013), and there is a trend to use a bag-of-visual-words (BoVW) for medical image representation (Castellani et al., 2010; Cruz-Roa et al., 2012; Kwitt et al., 2012; Foncubierta-Rodríguez et al., 2013; Liu et al., 2013a; Depeursinge et al., 2014). The BoVW model represents an image with a visual word frequency histogram that is obtained by assigning the local visual features to the closest visual words in the dictionary. Rather than matching the visual feature descriptors directly, BoVW retrieval approaches compare the images according to the visual words that are assumed to have higher discriminative power (Foncubierta-Rodríguez et al., 2012; Tamaki et al., 2013). The BoVW model was proposed by Sivic and Zisserman (Sivic and Zisserman, 2003) and has been adopted by many researchers in non-medical domains such as computer vision (Li and Pietro, 2005; Yang et al., 2007; Bosch et al., 2008), showing the advantages of describing local patterns over using global features only. This model has recently been applied to tackle the large-scale medical image retrieval problem (Jiang et al., 2015; Zhang et al., 2015d). In this study, we focus on a new BoVW-based retrieval for better retrieval accuracy and efficiency.1.1. Related workThe aim of CBMIR is to extract visual characteristics of images to identify the level of similarity between two images. Feature extraction can be categorized into global-(GFM) and local-feature (LFM) models based on the scope of descriptors (Bannour et al., 2009). The GFM extracts a single feature vector from the whole image and the LFM partitions the image into a collection of smaller regions, namely patches, and considers that each patch has Neurocomputing. Author manuscript; available in PMC 2017 February 12.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 3its own importance in describing the whole image (Avni et al., 2011). This patch-based model is particularly useful in medical image analysis since different image regions can represent the anatomical structures that play different and essential roles in medical imaging diagnosis (Tong et al., 2014; Zhang et al., 2014).The BoVW representation builds upon the LFM. Visually similar patches from different images are assigned to the same code in a codebook. Then, the patch-code co-occurrence assignment can be used to describe the image features and to compute the similarity between images. The workflow of BoVW-based image retrieval can be generalized into three steps (Caicedo et al., 2009): feature extraction, BoVW construction and similarity calculation. Specifically, the LFM is used to extract a collection of local patch features from each image. The entire patch feature set computed from all images in the database is then grouped into clusters, with each cluster regarded as a visual word and the whole cluster collection considered as the visual dictionary. Then, all patch features in one image are assigned to visual words, generating a visual word frequency histogram to represent this image. Finally, the similarity between images is computed based on these frequency histograms for retrieval.In this workflow, an important issue is the dictionary construction. The visual word in the dictionary corresponds to a group of visually similar patches. Normally, these words are obtained within the local patch feature space using unsupervised clustering methods, e.g., k-means (André et al., 2011; Yang et al., 2012). These approaches often generate a redundant and noisy dictionary since they tend to accommodate all local patch feature patterns (Foncubierta-Rodríguez et al., 2013), thus reducing the effects of the most crucial words and increasing the computational cost. Hence, it is preferable to remove the visual words that are less essential for the BoVW representation.To ensure that only the meaningful feature patterns are included, the supervised clustering method of Bilenko et al (Bilenko et al., 2004) can be used to regulate the construction of dictionary, but the method adaptability is limited because prior knowledge is required for the learning process. Another approach is to analyze the discriminative power of visual words (Caicedo et al., 2009), but the weighting scheme also requires supervised classifiers. Some researchers have suggested that the most frequent visual words in images are ‘stop words’, which occur widely but have little influence on differentiating images, and need to be removed from the dictionary (Sivic and Zisserman, 2003). Yang et al., however, showed that ranking the visual words based on their occurrences in the different images only was not sufficient to evaluate the importance of visual words (Yang et al., 2007). Term frequency-inverse document frequency (TF-IDF) (Jones, 1972) relies on the inverse frequency weighting and has demonstrated its benefits on visual word evaluation. Nevertheless, it merely utilizes the direct co-occurrence relationship between the images and visual words. Jiang et al. (Jiang et al., 2015) proposed an unsupervised approach to refine the weights of visual words within the vocabulary tree and showed the advantages of using the correlations among the visual words. We suggest that this relationshi",
            {
                "entities": [
                    [
                        280,
                        356,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Available online at www.sciencedirect.com Available online at www.sciencedirect.com Procedia Computer Science 00 (2017) 000–000 Procedia Computer Science 00 (2017) 000–000 ScienceDirect ScienceDirect Procedia Computer Science 110 (2017) 498–503 www.elsevier.com/locate/procedia  www.elsevier.com/locate/procedia The 4th International Symposium on Emerging Inter-networks, Communication and Mobility The 4th International Symposium on Emerging Inter-networks, Communication and Mobility (EICM 2017) (EICM 2017) On the use of Networks in Biomedicine On the use of Networks in Biomedicine Eugenio Vocaturoa*, Pierangelo Veltrib Eugenio Vocaturoa*, Pierangelo Veltrib a Department of Computer Science, Modeling, Electronic and Systems Engineering (DIMES), University of Calabria, Italy a Department of Computer Science, Modeling, Electronic and Systems Engineering (DIMES), University of Calabria, Italy b Bioinformatics Laboratory Surgical and Medical Science Department University Magna Graecia of Catanzaro, Italy b Bioinformatics Laboratory Surgical and Medical Science Department University Magna Graecia of Catanzaro, Italy Abstract Abstract The concept of “neural network” emerges by electronic models inspired to the neural structure of human brain. Neural networks aim to The concept of “neural network” emerges by electronic models inspired to the neural structure of human brain. Neural networks aim to solve problems currently out of computer’s calculation capacity, trying to mimic the role of human brain. Recently, the number of biological solve problems currently out of computer’s calculation capacity, trying to mimic the role of human brain. Recently, the number of biological based applications using neural networks is growing up. Biological networks represent correlations, extracted from sets of clinical data, based applications using neural networks is growing up. Biological networks represent correlations, extracted from sets of clinical data, diseases, mutations, and patients, and many other types of clinical or biological features. Biological networks are used to model both the state diseases, mutations, and patients, and many other types of clinical or biological features. Biological networks are used to model both the state of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events. of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events. The study of biological networks, their analysis and modeling are important tasks in life sciences. Most biological networks are still far from The study of biological networks, their analysis and modeling are important tasks in life sciences. Most biological networks are still far from being complete and they are often difficult to interpret due to the complexity of relationships and the peculiarities of the data. Starting from being complete and they are often difficult to interpret due to the complexity of relationships and the peculiarities of the data. Starting from preliminary notions about neural networks, we focus on biological networks and discuss some well-known applications, like protein-protein preliminary notions about neural networks, we focus on biological networks and discuss some well-known applications, like protein-protein interaction networks, gene regulatory networks (DNA-protein interaction networks), metabolic networks, signaling networks, neuronal interaction networks, gene regulatory networks (DNA-protein interaction networks), metabolic networks, signaling networks, neuronal network, phylogenetic trees and special networks. Finally, we consider the use of biological network inside a proposed model to map health network, phylogenetic trees and special networks. Finally, we consider the use of biological network inside a proposed model to map health related data. related data. © 2017 The Authors. Published by Elsevier B.V. © 2017 The Authors. Published by Elsevier B.V.© 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs. Peer-review under responsibility of the Conference Program Chairs.Peer-review under responsibility of the Conference Program Chairs. Keywords: Biological Networks; Protein-Protein Interaction Networks (PPIn); Gene Regulatory Networks (GRN); Metabolic Networks; Signaling Networks; Keywords: Biological Networks; Protein-Protein Interaction Networks (PPIn); Gene Regulatory Networks (GRN); Metabolic Networks; Signaling Networks; Neuronal Networks; Food Webs; Phylogenetic Trees, Special Networks and Hierarchies; Health Care Model. Neuronal Networks; Food Webs; Phylogenetic Trees, Special Networks and Hierarchies; Health Care Model. 1. Introduction 1. Introduction The human brain has the capacity of processing information and making decisions instantaneously. Many researchers have The human brain has the capacity of processing information and making decisions instantaneously. Many researchers have shown that human brain performs calculations in a different way than computers, hence the aspiration to solve problems whose shown that human brain performs calculations in a different way than computers, hence the aspiration to solve problems whose complexity is beyond the current computing power, has prompted the scientific community to the neural networks. For complexity is beyond the current computing power, has prompted the scientific community to the neural networks. For biological network is meant any network applied to a biological systems. biological network is meant any network applied to a biological systems. A network, in a broad sense, identifies a system, which is characterized by interconnected sub-units. Biological networks are A network, in a broad sense, identifies a system, which is characterized by interconnected sub-units. Biological networks are types of important applicable model in various contexts; complex biological systems can be represented and analyzed by types of important applicable model in various contexts; complex biological systems can be represented and analyzed by computable networks. Like the computer networks, the high complexity degree of biological networks is generated by a simple computable networks. Like the computer networks, the high complexity degree of biological networks is generated by a simple mechanism. Bioinformatics really shifted its focus from individual genes, proteins, structures and search algorithms for large mechanism. Bioinformatics really shifted its focus from individual genes, proteins, structures and search algorithms for large networks; even more biologists are discovering the links between Internet and metabolic pathways, interactions of proteins networks; even more biologists are discovering the links between Internet and metabolic pathways, interactions of proteins through a network topology or a scale-free network. through a network topology or a scale-free network. * * * Corresponding author. Tel.: +039-0984-4799. * Corresponding author. Tel.: +039-0984-4799. E-mail address: e.vocaturo@dimes.unical.it E-mail address: e.vocaturo@dimes.unical.it 1877-0509 © 2017 The Authors. Published by Elsevier B.V. 1877-0509 © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs. Peer-review under responsibility of the Conference Program Chairs. 1877-0509 © 2017 The Authors. Published by Elsevier B.V.Peer-review under responsibility of the Conference Program Chairs.10.1016/j.procs.2017.06.13210.1016/j.procs.2017.06.1321877-0509ScienceDirectAvailable online at www.sciencedirect.com          \f2 Eugenio Vocaturo/ Procedia Computer Science 00 (2017) 000–000 Eugenio Vocaturo et al. / Procedia Computer Science 110 (2017) 498–503 499A neural network is composed of a set of parallel and distributed processing units, referred as nodes or neurons; they are arranged in layers, and are interconnected by unidirectional or bidirectional connections (see Fig. 1). Typically, a neural network has a set of N input nodes, whose generic element is related with, and each node is interconnected to others through weighted arcs. The products of input and weight are simply summed and feed through (Activation Function) to generate the output (see Fig. 2).              Fig. 1. Typical Structure of Neural Network                              Fig. 2. Activation Functions Neural network design typically consists of Topology, Transfer Function and Learning Algorithm. The neural network topologies are actually classified by the directions of interconnection in the layer; so the most referred topologies are, Feed Forward Topology and Recurrent Topology. In feed forward topology (FFT) network, the nodes are “hierarchically arranged” in layers starting with the input layers and ending with output layers. The number of hidden layers provides most of the network computational power. In literature typical application of this topology are the multilayer perception network and radial basic function network. The nodes in each layers are connected to next layer through unidirection paths starting from one layer (source) and ending at the subsequently layer (sink). The output of a given layer feeds the nodes of the following layer in a forward direction and does not allow feedback flow of information12. Unlike the FFT, in the recurring topology (RNT) the flow of information between connected nodes is bidirectional. Typical applications of RNT, for example, are Hopfield Network1 and time delay neural network (TDNN)2. A recurrent network structure has a sort of memory, which helps storing information in output nodes through dynamic states. Biological networks shapes both the state of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events14. In neural networks, the basic unit are the neurons that work like simple processors. Any neuron takes th",
            {
                "entities": [
                    [
                        510,
                        547,
                        "TITLE"
                    ],
                    [
                        548,
                        585,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Journal of Informetrics 15 (2021) 101171 Contents lists available at ScienceDirect Journal of Informetrics journal homepage: www.elsevier.com/locate/joi Gender-based homophily in research: A large-scale study of man-woman collaboration Marek Kwiek a , Wojciech Roszka b a Institute for Advanced Studies in Social Sciences and Humanities (IAS), UNESCO Chair in Institutional Research and Higher Education Policy, Adam Mickiewicz University in Poznan, Poland b Poznan University of Economics and Business, Poznan, Poland a r t i c l e i n f o a b s t r a c t Keywords: Research collaboration co-authorships gender gap sociology of science homophily scientific careers publishing patterns probabilistic record linkage sex differences 1. Introduction We examined the male-female collaboration practices of all internationally visible Polish uni- versity professors (N = 25,463) based on their Scopus-indexed publications from 2009–2018 (158,743 journal articles). We merged a national registry of 99,935 scientists (with full admin- istrative and biographical data) with the Scopus publication database, using probabilistic and deterministic record linkage. Our unique biographical, administrative, publication, and citation database ( “The Polish Science Observatory ”) included all professors with at least a doctoral de- gree employed in 85 research-involved universities. We determined what we term an “individual publication portfolio ” for every professor, and we examined the respective impacts of biological age, academic position, academic discipline, average journal prestige, and type of institution on the same-sex collaboration ratio. The gender homophily principle (publishing predominantly with scientists of the same sex) was found to apply to male scientists —but not to females. The majority of male scientists collaborate solely with males; most female scientists, in contrast, do not collab- orate with females at all. Across all age groups studied, all-female collaboration is marginal, while all-male collaboration is pervasive. Gender homophily in research-intensive institutions proved stronger for males than for females. Finally, we used a multi-dimensional fractional logit regres- sion model to estimate the impact of gender and other individual-level and institutional-level independent variables on gender homophily in research collaboration. Science is a collaborative enterprise, with (male and female) scientists collaborating internationally, nationally, and institutionally ( Wuchty, Jones, & Uzzi, 2007 ; Wagner, 2018 ). However, this is not our topic: our focus is on male–male, female–female, and male–female (or mixed-sex) research collaboration rather than collaboration across countries and institutions. The dominating view in liter- ature is that, on average, males collaborate more often with males, and females collaborate more often with females ( Jadidi, Karimi, Lietz, & Wagner, 2018 ; Lerchenmueller, Hoisl, & Schmallenbach, 2019 ; Wang, Lee, West, Bergstrom, & Erosheva, 2019 ; Holman & Morandin, 2019 ; Boschini & Sjögren, 2007 ; McDowell & Smith, 1992 ). This hypothesis is being tested using a large-scale dataset with unique variables. According to the homophily principle, “similarity breeds connection ”; consequently, personal networks are homogeneous with regard to many sociodemographic and personal characteristics (such as age, ethnic origin, class origin, wealth, education, and gender). On the positive side, homophily is reported to simplify communication ( McPherson, Smith-Lovin, & Cook, 2001 ; Kegen, 2013 ). However, on the negative side, homophily may “limit people’s social worlds in a way that has powerful implications for the information E-mail addresses: kwiekm@amu.edu.pl (M. Kwiek), wojciech.roszka@ue.poznan.pl (W. Roszka). https://doi.org/10.1016/j.joi.2021.101171 Received 6 June 2020; Received in revised form 29 April 2021; Accepted 5 May 2021 1751-1577/© 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \fM. Kwiek and W. Roszka Journal of Informetrics 15 (2021) 101171 they receive, the attitudes they form, and the interactions they experience ” ( McPherson et al., 2001 ). As science is increasingly collaborative, the homophily principle may increasingly influence academic careers. Research collaboration in science (or gender co-authorship patterns) provides fertile ground to test the homophily principle. Man–woman research collaboration patterns in science are contrasted in this paper through six lenses: biological age, academic position, academic discipline, gender-defined research collaboration type, journal prestige, and institutional research intensity. The individual scientist, rather than the individual article, is the unit of analysis. The key innovative methodological step is the determi- nation of what we term an “individual publication portfolio ” (for the decade of 2009–2018) for every internationally visible Polish scientist (N = 25,463 university professors from 85 universities, grouped into 27 disciplines, along with their 164,908 international collaborators, who together authored 158,743 Scopus-indexed publications). Co-authorships are used for the operationalization of research collaboration, following standard bibliometric practice. The individual publication portfolio reflects the distribution of gender-defined research collaboration types (same-sex collaboration and mixed-sex collaboration) for every individual scientist. Team formation in academia, understood as publishing with coauthors of varying numbers and different genders, is voluntary ( McDowell & Smith, 1992 ): researchers team up when they think that they are better off collaborating than publishing alone. The teams formed, or the articles published, are likely to reflect “individual tastes and perceptions of the returns to collaboration, as well as the costs of coordination ” ( Boschini & Sjögren, 2007 , p. 327). Some male scientists collaborate predominantly with other males, and some female scientists collaborate predominantly with other females. Still, others prefer to publish in mixed-sex collaborations (or to author individually). We examine the same-sex collaboration ratio at an individual level of every internationally visible Polish scientist (i.e., only authors with Scopus-indexed publications) and generalize the results from the individual level to the level of the national higher education system. 2. Literature review 2.1. The gender context of science The gender context of academic science has changed substantially in the past few decades ( Huang, Gates, Sinatra, & Barabàsi, 2020 ; Larivière, Ni, Gingras, Cronin, & Sugimoto, 2013 ), with more female scientists entering the higher education sector ( Elsevier, 2018 ) and occupying high academic positions ( Zippel, 2017 ; Diezmann & Grieshaber, 2019 ). Male and female scientists often pursued or were pushed onto somewhat different career tracks and were located in different academic structures, with “differential access to valuable resources ” ( Xie & Shauman, 2003 , p. 193). Females, as new entrants into a traditionally male-dominated academic profession, initially did not have equal access to professional networks ( McDowell, Singell, & Stater, 2006 ). But the academic world is changing. New bibliometric literatures applying the various gender-determination methods to authors and authorships ( Halevi, 2019 ; Elsevier, 2020 ) bring new data-driven insights to gender disparities in science, and literatures have become much less based on anecdotal and localized studies ( Larivière et al., 2013 ). Women are plugging into networks over time as the profession becomes more gender representative (as shown for academic economists by McDowell et al., 2006 , p. 154). However, somewhat paradoxically, the increased participation of women in STEM disciplines is reported to have been accompanied by an increase in gender differences regarding both productivity and impact ( Huang et al., 2020 , p. 8; Elsevier, 2018 , p. 16). As recent literature highlights, female scientists occupy more junior positions and receive lower salaries, are more often in non- tenure-track and teaching-only positions, are promoted more slowly, are less likely to be listed as either first or last author on a paper, and are allocated less research funding from national research councils. Women also tend to be less involved in international collaboration; female collaborations are more domestically oriented than are the collaborations of males from the same country; and females have less-prestigious collaborations and fewer collaborations overall (see Holman & Morandin, 2019 ; Halevi, 2019 ; Larivière et al., 2013 ; Larivière et al., 2011 ; Aksnes, Rørstad, Piro, & Sivertsen, 2011 ; Aksnes, Piro, & Rørstad, 2019 ; Huang et al., 2020 ; Maddi, Larivière, & Gingras, 2019 ; Fell & König, 2016 ; van den Besselaar & Sandström, 2016 ; Nielsen, 2016 ). In every country studied recently in Elsevier (2020) and Elsevier (2018) , the percentage of women who publish internationally is lower than the percentage of men who do so; for Poland, which is not included in the Elsevier reports, these publishing patterns are confirmed for various collaboration intensity levels and for various age groups; see Kwiek & Roszka, 2020 , on gender disparities in international collaboration). Female scientists in Poland constitute a substantial, highly productive, and highly internationalized part of the academic work- force, which is often the case in formerly communist European countries, which exhibit greater gender parity than the world and the OECD averages ( Larivière et al., 2013 , p. 212). Poland has a higher proportion of professors than any country studied in Larivière et al. (2013) or in Diezmann and Grieshaber (2019) , reaching 29.82% in 2018 ( GUS, 2019 , p. 220), even though there is a clear “the higher the fewer ” pattern acro",
            {
                "entities": [
                    [
                        153,
                        235,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Tilburg UniversityA vulnerability analysisKrupiy, TetyanaPublished in:Computer Law and Security ReviewDOI:10.1016/j.clsr.2020.105429Publication date:2020Document VersionPublisher's PDF, also known as Version of recordLink to publication in Tilburg University Research PortalCitation for published version (APA):Krupiy, T. (2020). A vulnerability analysis: Theorising the impact of artificial intelligence decision-makingprocesses on individuals, society and human diversity from a social justice perspective. Computer Law andSecurity Review, 38(September), 1-25. [105429]. https://doi.org/10.1016/j.clsr.2020.105429General rightsCopyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright ownersand it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.      • Users may download and print one copy of any publication from the public portal for the purpose of private study or research.      • You may not further distribute the material or use it for any profit-making activity or commercial gain      • You may freely distribute the URL identifying the publication in the public portalTake down policyIf you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediatelyand investigate your claim.Download date: 04. jul.. 2023  \fcomputer law & security review 38 (2020) 105429 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR A vulnerability analysis: Theorising the impact of artificial intelligence decision-making processes on individuals, society and human diversity from a social justice perspective Tetyana (Tanya) Krupiy Tilburg University, Montesquieu Building, Room 808, Prof. Cobbenhagenlaan 221, Tilburg, North Brabant, 5037 DE, the Netherlands a r t i c l e i n f o a b s t r a c t Keywords: Artificial intelligence Data science Decision-making process Social justice Human diversity Vulnerability theory Feminism Queer legal theory Critical disability theory The article examines a number of ways in which the use of artificial intelligence technolo- gies to predict the performance of individuals and to reach decisions concerning the enti- tlement of individuals to positive decisions impacts individuals and society. It analyses the effects using a social justice lens. Particular attention is paid to the experiences of individ- uals who have historically experienced disadvantage and discrimination. The article uses the university admissions process where the university utilises a fully automated decision- making process to evaluate the capability or suitability of the candidate as a case study. The article posits that the artificial intelligence decision-making process should be viewed as an institution that reconfigures the relationships between individuals, and between indi- viduals and institutions. Artificial intelligence decision-making processes have institutional elements embedded within them that result in their operation disadvantaging groups who have historically experienced discrimination. Depending on the manner in which an artifi- cial intelligence decision-making process is designed, it can produce solidarity or segrega- tion between groups in society. There is a potential for the operation of artificial intelligence decision-making processes to fail to reflect the lived experiences of individuals and as a re- sult to undermine the protection of human diversity. Some of these effects are linked to the creation of an ableist culture and to the resurrection of eugenics-type discourses. It is con- cluded that one of the contexts in which human beings should reach decisions is where the decision involves representing and evaluating the capabilities of an individual. The legisla- ture should respond accordingly by identifying contexts in which it is mandatory to employ human decision-makers and by enacting the relevant legislation. © 2020 Tetyana˜(Tanya) Krupiy. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) E-mail address: t.krupiy@uvt.nl https://doi.org/10.1016/j.clsr.2020.105429 0267-3649/© 2020 Tetyana˜(Tanya) Krupiy. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) \f2 computer law & security review 38 (2020) 105429 Erica Curtis, a former admissions evaluator at Brown Uni- versity in the United States, has noted that she evaluated each student’s application consisting of standardised test scores, the transcript, the personal statement, and multiple supple- mental essays within a twelve-minute timeframe.1 Arguably, this is a very short period of time within which an admissions officer can evaluate the applicant’s personality and academic qualities holistically.2 The time constraints create a possibility that the admissions officer may fail to detect the applicants’ capabilities or how societal barriers diminished their ability to realise their potential. Another concern with human decision- making is that the decision-maker officer may act arbitrar- ily in the course of exercising discretion 3 by putting differ- ent weight on comparable attributes that cannot be measured. What is more, an admissions officer could treat applicants on an unequal basis due to being influenced by conscious or unconscious biases.4 Advances in artificial intelligence (hereinafter AI) technology give rise to a discussion whether organisations should use AI systems to select applicants for admission to university.5 Technology companies market AI systems with a capability to predict the candidates’ perfor- mance and to follow a decision-making procedure as possess- ing the capacity to eliminate bias and to improve decision- making.6 The computer science community is now working on embedding values, such as fairness, into the AI decision- Acknowledgments: I would like to thank Professor Corien Prins for her feedback on the draft version of this article. I am grateful to Atieno Samandari, Stu Marvel, Professor Martha Albertson Fine- man, Professor Nicole Morris and Professor Paul Myers for their feedback on a presentation which formed the foundation for this article. Additionally, I wish to thank scholars who asked stimulat- ing questions during the Ethics of Data Science: Addressing the Future Use and Misuse of Our Data Conference, the BIAS in Artifi- cial Intelligence and Neuroscience Transdisciplinary Conference, and the Media & Space: The Regulation of Digital Platforms, New Media & Technologies Symposium where I presented my ongoing work. 1 Joel Butterly, ‘7 Admissions Officers Share the Things They Never Tell Applicants’ (Insider Inc., 2018) < https: //www.businessinsider.com/7- things- college- admissions- officers-wishevery-applicant-knew-2018-2?international= true&r=US&IR=T > accessed 26 June 2019 2 3 4 5 6 Ibid Mark Bovens and Stavros Zouridis, ‘From Street-Level to System-Level Bureaucracies: How Information and Communica- tion Technology is Transforming Administrative Discretion and Constitutional Control’ (2002) 62 Public Administration Review 174, 181 Josh Wood, ‘“The Wolf of Racial Bias\": the Admissions Lawsuit Rocking Harvard’ The Guardian (London 18 October < https://www.theguardian.com/education/2018/oct/18/ 2018) harvard-affirmative-action-trial-asian-american-students > accessed 10 March 2019 Moritz Hardt, How Big Data is Unfair: Understanding Unintended Sources of Unfairness in Data Driven Decision-making (Medium Cor- poration 2014) Ekta Dokania, ‘Can AI Help Humans Overcome Bias?’ The Seat- tle Globalist (Seattle 22 May 2019) < https://www.seattleglobalist. com/2019/05/22/can- ai- help- humans- overcome- bias/83957 > ac- cessed 3 March 2019 making procedure.7 Daniel Greene and colleagues view the focus on achieving fairness by incorporating values into the design of the system as short-sighted.8 The attention on how to embed fairness into the decision-making procedure of a technical system side-lines the discussion how the employ- ment of AI decision-making processes impacts on achieving social goals, such as social justice and ‘equitable human flour- ishing.’ 9 Virginia Eubank’s work underscores the importance of investigating how the use of AI decision-making processes impacts individuals and society. Her interviews with affected individuals who applied to access state benefits in the state of Indiana in the United States 10 demonstrate that the employ- ment of AI decision-making processes can lead to the deepen- ing of inequality,11 and to social division.13 The enquiry is particularly pertinent given the fact that not all sources report adverse outcomes. The British Universities and Colleges Admissions Service asserts that in its pilot project an algorithmic process selected the same pool of applicants to be admitted to universities as admissions officers; the organisa- tion did not reveal the algorithm’s design and operation pro- cedure.14 to social sorting 12 The present paper explores some of the hitherto unre- solved longstanding societal problems and new issues the employment of AI decision-making processes raises. It con- tributes to existing literature by proposing that an AI decision- making process should be understood as an institution. The AI decision-making process reconfigures relationships between individuals as well as between individuals and institutions. The paper examines some of the values and types of institu- tional arrangements the employment of AI decision-making processes embeds into society. This issue is significant. The Council of Europe Committee of Ministers stated that when data-driven technologies operate ‘at scale’ their operation prioritises certain values over others.15 The assertion of the Council of Europe Committee of Ministers that data-d",
            {
                "entities": [
                    [
                        1590,
                        1768,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Time-efficient sparse analysis of histopathological WholeSlide ImagesChao-Hui Huang, Antoine Veillard, Nicolas Lomenie, Daniel Racoceanu,Ludovic RouxTo cite this version:Chao-Hui Huang, Antoine Veillard, Nicolas Lomenie, Daniel Racoceanu, Ludovic Roux. Time-efficientsparse analysis of histopathological Whole Slide Images. Computerized Medical Imaging and Graphics,2010, pp.5. ￿hal-00553877￿HAL Id: hal-00553877https://hal.science/hal-00553877Submitted on 10 Jan 2011HAL is a multi-disciplinary open accessarchive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come fromteaching and research institutions in France orabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL, estdestinée au dépôt et à la diffusion de documentsscientifiques de niveau recherche, publiés ou non,émanant des établissements d’enseignement et derecherche français ou étrangers, des laboratoirespublics ou privés.\fTime-efficient sparse analysis of histopathological Whole Slide ImagesChao-Hui Huangb,c, Antoine Veillardb,c, Nicolas Lom´eniea,b, Daniel Racoceanua,b,c,d, Ludovic Rouxb,e,∗aCentre National de la Recherche Scientifique (CNRS), Paris, FrancebIPAL (Image & Pervasive Access Lab), International Mixed Research Unit UMI CNRS 2955 (CNRS, NUS, I2R/A*STAR, UJF),Singapore (http://ipal.i2r.a-star.edu.sg/)cNational University of Singapore, SingaporedUniversity of Franche-Comt´e, Besan¸con, FranceeUniversity Joseph Fourier, Grenoble, FranceAbstractHistopathological examination is a powerful method for the prognosis of critical diseases. But, despite significantadvances in high-speed and high-resolution scanning devices or in virtual exploration capabilities, the clinical analysisof Whole Slide Images (WSI) largely remains the work of human experts. We propose an innovative platform in whichmulti-scale computer vision algorithms perform fast analysis of a histopathological WSI. It relies on specific high andgeneric low resolution image analysis algorithms embedded in a multi-scale framework to rapidly identify the high powerfields of interest used by the pathologist to assess a global grading. GPU technologies as well speed up the globaltime-efficiency of the system.In terms ofvalidation, we are designing a computer-aided breast biopsy analysis application based on histopathology images anddesigned in collaboration with a pathology department. The current ground truth slides correspond to about 36,000high magnification (40X) high power fields. The time processing to achieve automatic WSI analysis is on a par with thepathologist’s performance (about ten minutes a WSI), which constitutes by itself a major contribution of the proposedmethodology.In a sense, sparse coding and sampling is the keystone of our approach.Keywords: Histopathology, breast cancer, Whole Slide Image, multi-scale analysis, dynamic sampling, virtualmicroscope, Graphics Processing Unit1. IntroductionHistopathology is widely accepted as a powerful goldstandard for prognosis in critical diseases such as breast,prostate, kidney and lung cancers, allowing to narrow bor-derline diagnosis issued from standard macroscopic non-invasive analysis such as mammography and ultrasonog-raphy. At the molecular/genetic scale as well challengingmethods recently emerged for clinical diagnosis purposes.However, histomorphology as operated in hospitals is andwill remain the basis for most cancer classification.The histopathological image analysis process has largelyremained the work of human experts so far. At the hos-pital level, the task consists in the daily examination ofhundreds of slides, directly impacting critical diagnosisand treatment decisions. According to pathologists’ opin-ion [1], such a tedious manual work is often inconsistentand subjective, lacking traceability and computer assistedIn addition,analysis/annotation/grading support tools.hospitals will have to manage a shortage of expert pathol-ogists keen at doing this kind of unrewarding tasks.A few image analysis algorithms and automated grad-ing systems dedicated to breast histopathology images have∗Corresponding authorEmail address: vislr@i2r.a-star.edu.sg (Ludovic Roux)already been studied. Est´evez et al.[2] and Schnorren-ber et al.[3] worked on Fine Needle Aspiration (FNA)biopsies. FNA images are relatively easier to analyze thanWSIs since such an examination has limited diagnostic op-tions and produces mostly well separated cells over a well-contrasted background. Petushi et al.[4, 5] introduceda system able to label several histological and cytologicalmicrostructures in high resolution frames, including differ-ent grades of epithelial cells, fat cells and stroma. Doyleet al. [6, 7] proposed a method based on geometrical fea-tures, to distinguish between healthy tissue, low grade andhigh grade cancer. Tutac et al. [8] initiated an innovativeknowledge guided approach relying on the prior modelingof medical knowledge using ontology designed accordingto the clinical standard called Nottingham Grading Sys-tem [9]. An extension to this work involving multi-scaleapproaches was proposed by Dalle et al. [10].In close collaboration with a histopathology depart-ment, we built up a high-speed WSI analysis platform ableto detect scale-dependent meaningful regions of interest inmicroscopic biopsy images. This platform is dedicated tothe grading of breast cancer for prognosis purposes butthe methodology we present here is quite generic. Weuse a standard optical microscope that can be found inmost of the analysis laboratories in pathology or bacteri-Preprint submitted to Computerized Medical Imaging and GraphicsMay 27, 2010\fology (in our case, an optical microscope Olympus BX51,with 4X/10X/40X/60X/100X possible magnifications, PriorH101A ProScanII motorized X/Y stage and Z focus with atravel range of 114mm×75mm and a minimum step size of0.02μm, and a 1280×1024 pixels digital camera MediaCy-bernetics “EvolutionLC color” IEEE1394 MegaPixel). Weuse a MediaCybernetics controller connected to the micro-scope to perform an acquisition of high power fields/frames(in our study at 40X magnification according to the requestof the pathologist for the high resolution analysis). Theacquired 40X high power fields are stitched together in or-der to obtain the WSI.To the best of our knowledge, most of the previousresearch works focused on the analysis of individual highresolution frames [11] and/or proposed solutions too com-putationally expensive to be applied at the WSI level [12].A few notable exceptions [13] rely on the analysis of lowerresolution images for the selection of regions of interest.Unfortunately, there is little correlation between low res-olution images and the actual levels of nuclear pleomor-phism observable at high resolution for instance. There-fore, even such methods proved to be inefficient for theparticular issue of nuclear pleomorphism assessment onfull biopsy slides. As a consequence, the time-efficiencyproblem posed by the extremely large scale of biopsy im-ages (several thousands of frames) still lacks a practicalsolution.In this work, we propose solutions to improve efficiencyof such a microscopic platform both in terms of speed andprecision, in particular with a multi-scale dynamic sam-pling approach and the use of GPU programming. Theprocessing of a WSI starts by the detection of invasiveregions of interest (ROI) at low resolution level (1.2X).This method relies on a bio-inspired visual informationparadigm related to sparse coding and Graphics Process-ing Unit (GPU) implementation to dramatically speed-up the processing line. This part will be detailed in Sec-tion 2. Once the ROIs are detected, a map of local can-cer grades is established using a new generic multi-scale,computational geometry-based dynamic sampling methodcombined with high-resolution application specific imageanalysis algorithms. Then, this map is used to analyze theWSI within an operational time frame compatible with thepathology department’s needs and on a par with the pro-cessing time of an experimented pathologist. This part willbe detailed in Section 3. Finally, Section 4 elaborates onthe results and validation issues and Section 5 is dedicatedto conclusions, future research directions and challenges.2. Low Resolution Analysis and Sparse CodingRegion of interest (ROI) detection is a fundamentalphase of breast cancer grading for histopathological im-ages. Pathologists identify ROIs to efficiently select themost important invasive areas for the grading process.Since neither pathologists nor computers are able to ex-plore every details at high magnification within a reason-able time, the effective and efficient choice of the ROI isthus a critical step.In this study, ROI detection is casted as as a classifica-tion problem. The low magnification analysis will deter-mine if a given region is an invasive area in a similar man-ner as a pathologist would do when evaluating a biopsy. Inorder to mimic this behaviour, we exploit the relationshipbetween human vision and neuroscience [14].In the visual system, a set of opponent photo-receptorsforms a Receptive Field (RF). These photoreceptors forma field which is called ganglion RF since they collect vi-sual information and send neural spikes to a ganglion cell.Eventually, the ganglion cells produce various stimulationsand send them to the primary visual cortex [15].In the primary visual cortex, there are two major kindsof cells: simple- and complex-cells. Generally speaking,these cells produce two kinds of visual features: first- andsecond-order features [16]. The first-order feature containsthe information of intensities of various color channels, andthe second-order feature includes spatial variance of visualsignal [17, 18].In this study, we simulate some mechanisms of the hu-man visual system, generate the first- and second- orderfeatures as the mechanisms in the human visual system.However, the human visu",
            {
                "entities": [
                    [
                        1003,
                        1073,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "1 Segmentation of histological images and fibrosis identification with a convolutional neural network Xiaohang Fu1 · Tong Liu2 · Zhaohan Xiong1 · Bruce H. Smaill1 · Martin K. Stiles3 · Jichao Zhao1 tissue Abstract Segmentation of histological images is one of the most crucial tasks for many biomedical analyses including quantification of certain type. However, challenges are posed by high variability and complexity of structural features in such images, in addition to imaging artifacts. Further, the conventional approach of manual thresholding is labor-intensive, and highly sensitive to inter- and intra-image intensity robust automated variations. An accurate and segmentation method is of high interest. We propose and evaluate an elegant convolutional neural network (CNN) designed for segmentation of histological images, particularly those with Masson’s trichrome stain. The network comprises of 11 successive convolutional – linear unit – batch normalization layers, and outperformed state-of-the-art CNNs on a dataset of cardiac histological images (labeling fibrosis, myocytes, and background) with a Dice similarity coefficient of 0.947. With 100 times fewer (only 300 thousand) trainable parameters, our CNN is less susceptible to overfitting, and is efficient. Additionally, it retains image resolution from input to output, captures fine-grained details, and can be trained end-to-end smoothly. To the best of our knowledge, this is the first deep CNN tailored for the problem of concern, and may be extended to solve similar segmentation tasks to facilitate investigations into pathology and clinical treatment. rectified Xiaohang Fu xfu697@aucklanduni.ac.nz Jichao Zhao j.zhao@auckland.ac.nz 1 2 3 Auckland Bioengineering Institute, The University of Auckland, Auckland 1142, New Zealand Department of Cardiology, Second Hospital of Tianjin Medical University, and Tianjin Key Laboratory of Ionic-Molecular Function of Cardiovascular Disease, Tianjin Institute of Cardiology, Tianjin 300201, P.R. China Waikato Hospital, Hamilton 3204, New Zealand Keywords Convolutional neural network learning Histology Fibrosis Image segmentation  Deep ····1 Introduction Accurate segmentation of biomedical images is fundamental for quantitative analysis. However, this task is challenging due to characteristically high inhomogeneity and complexity of features in such images, as well as inter- and intra-plane artifacts introduced as a result of the imaging procedure and methodology, such as lighting. Manual thresholding is the most prevalent method for segmenting biomedical images, for example to identify fibrosis or scarring in histology [1, 2]. Whilst straightforward in principle, this approach is labor-intensive, time-consuming, and may involve tedious re-adjustments of thresholds [3, 4]. Thresholds are also highly sensitive to subject-dependent biases, as well as inter- and intra-image intensity variations (since spatial information is not accounted for) [5, 6]. Thus, a variety of thresholds is commonly necessary for one image set. For such reasons, manual thresholding may not be feasible for large datasets, especially those containing considerable variability in intensity, contrast, or brightness. Interpretation of raw pixel intensities to image meaning or context is no trivial task for algorithms. A slight difference in image features such as illumination may be negligible to humans, but can result in a disparate algorithmic outcome. Numerous methods have been established to separate an image into groups displaying similar features, and thereby identify the class object of each pixel. Earlier segmentation techniques rely on distinguishing edges, regions, or textures [6]. However, for image data with highly irregular heterogeneous illumination, or variable coloring of similar objects, considerable pre- or postprocessing is required, thus rendering such techniques unattractive and largely unsuitable. structural features,           \f(ground to produce outputs from In recent years, machine learning for computer vision has advanced extensively, emerging as a powerful tool for a wide range of image recognition problems [7–10]. Machine learning methods can be generally classed as unsupervised or supervised. In the former, the algorithm identifies patterns in the input without learning from example data annotated with desired outputs truth). Contrastingly, supervised models are trained on labelled data, learning rules inputs. Unsupervised methods including k-means clustering [11, 12], mean-shift clustering [13], and Markov random fields [14], as well as earlier supervised approaches such as support vector machines [10] have previously been employed to segment histological images. However, these methods typically suffer the requirement of supplementary algorithms (e.g. for postprocessing [12]) to complete the segmentation objective, or additional domain expertise to define and extract suitable features from images, which are often based on strong assumptions about the data. Convolutional neural networks (CNNs) are generating great enthusiasm particularly in computer vision. Conceptualized in the 1980s [15], CNNs were biologically inspired by the visual cortex; neurons fire in response to certain features or patterns in their local receptive fields, thereby acting as spatial filters [16]. CNNs effectively map highly complex relationships between the input and desired output (such as those of shapes and colors present through interconnected stacks of nonlinear functions (most fundamentally convolutions). Contrary to alternative supervised learning approaches such as support vector machines, there is no manual hand-crafting or fine-tuning of useful features in the input. CNNs can achieve impressive performance and directly handle complex data with minimal manual effort. A CNN-based approach is fully automated and trained models are reusable after establishment. images), in Although the inception of neural networks was a few decades ago, deep networks with multiple stacked layers are a relatively recent development; brought about through progress in parallelized computation using GPUs, solutions to hindrances associated with training deep neural networks (such as rectified linear units (ReLU) for the vanishing gradient problem [17]), and the availability of very large datasets. Deep CNNs have proved to be powerful tools in a wide array of image-related applications, excelling image classification [7, 18, 19], handwriting recognition [20], object localization [21], and scene understanding [22, 23]. This technique has also successfully extended to semantic pixel-wise labeling and the biomedical in 2 domain in tasks such as image segmentation [24–26], detection [27], cell tracking [8], and computer-aided diagnosis [9]. Robust and automated segmentation methods that can overcome the inherent challenges of biomedical image segmentation are of great demand, especially for applications conventionally relying on a manual approach. An example is fibrosis identification in histology, a critical task in many key fields of clinical research including kidney failure [28], lung injury [29], hepatitis B [30], sinoatrial node [1], and atrial fibrillation [31]. Atrial fibrillation is the most common type of cardiac arrhythmia, associated with significant healthcare costs, reduced quality of life, morbidity and mortality [32]. The basic mechanisms behind its initiation and maintenance remain elusive, but accumulating recent evidence indicate that diabetes mellitus (DM) is a strong independent risk factor [31–34], and that atrial fibrosis or scarring (characterized by excessive extracellular matrix proteins including collagen) conditions under contributes considerably to arrhythmogenicity [31, 35, 36]. Quantification and comparison of atrial fibrotic remodeling under DM against controls will assist in illuminating the precise mechanisms underlying DM-induced atrial fibrillation. This requires segmentation of fibrosis from myocytes and background in a cardiac histological section, differentiated via the well-accepted Masson’s trichrome stain (Fig. 1). induced diabetic Fig. 1 Representative segmentation of fibrosis. Left typical original RGB image of left atrial tissue from a diabetes mellitus (DM) rabbit model imaged with Masson’s trichrome stain at 40× magnification (image size 0.33 mm × 0.25 mm, pixel spatial resolution      161.25 nm × 161.25 nm); red, white and blue respectively indicate healthy myocytes, fat or extracellular space, and fibrosis. Right segmented fibrotic regions via a thresholding approach shown in blue In this paper, we propose a CNN for automated segmentation of stained histology images into a required number of tissue types, with particular focus on quantifying fibrosis in cardiac sections. To the best of our knowledge, this is the first CNN designed for this application. The deep CNN displays state-of-the-art segmentation accuracy with drastically fewer parameters, and substantially greater efficiency. More   \fimportantly, our proposed CNN architecture can be extended to other similar segmentation tasks to facilitate understanding of certain diseases and to aid targeted clinical treatment. We also make our source code freely available online for the benefit of potential users. 2 Methods and experiments 2.1 Overview of CNNs for segmentation image, with During a forward pass through a CNN, characteristics specific to certain structures in an input image (such as intensity and spatial information) are discerned by trainable filters in convolutional layers. Convolutional filters (typically 3 × 3 pixels) sweep across the entire visual field of the input volume by a constant stride, thus allowing the CNN to detect features in the input regardless of their exact position. Convolutional layers compute dot products between learnable filter weights and a corresponding region of the input slice, generating activation or feature maps. Thus, activations co",
            {
                "entities": [
                    [
                        2,
                        101,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptComput Biol Med. Author manuscript; available in PMC 2023 January 31.Published in final edited form as:Comput Biol Med. 2021 May ; 132: 104353. doi:10.1016/j.compbiomed.2021.104353.In-silico development and assessment of a Kalman filter motor decoder for prosthetic hand controlMai Gamala,b, Mohamed H. Mousac, Seif Eldawlatlyb,d, Sherif M. Elbasiounyc,e,*aCenter for Informatics Science, Nile University, Giza, EgyptbComputer Science and Engineering Department, Faculty of Media Engineering and Technology, German University in Cairo, Cairo, EgyptcDepartment of Biomedical, Industrial, and Human Factors Engineering, Wright State University, Dayton, OH, USAdComputer and Systems Engineering Department, Faculty of Engineering, Ain Shams University, Cairo, EgypteDepartment of Neuroscience, Cell Biology, and Physiology, Wright State University, Dayton, OH, USAAbstractUp to 50% of amputees abandon their prostheses, partly due to rapid degradation of the control systems, which require frequent recalibration. The goal of this study was to develop a Kalman filter-based approach to decoding motoneuron activity to identify movement kinematics and thereby provide stable, long-term, accurate, real-time decoding. The Kalman filter-based decoder was examined via biologically varied datasets generated from a high-fidelity computational model of the spinal motoneuron pool. The estimated movement kinematics controlled a simulated MuJoCo prosthetic hand. This clear-box approach showed successful estimation of hand movements under eight varied physiological conditions with no retraining. The mean correlation coefficient of 0.98 and mean normalized root mean square error of 0.06 over these eight datasets provide proof of concept that this decoder would improve long-term integrity of performance while performing new, untrained movements. Additionally, the decoder operated in real-time (~0.3 ms). Further results include robust performance of the Kalman filter when re-trained to more severe post-amputation limitations in the type and number of motoneurons remaining. An additional analysis shows that the decoder achieves better accuracy when using the firing of individual motoneurons as input, compared to using aggregate pool firing. Moreover, the decoder demonstrated robustness to noise affecting both the trained decoder parameters and the decoded motoneuron activity. These results demonstrate the utility of a proof of concept Kalman filter decoder that can support prosthetics’ control systems to maintain accurate and stable real-time movement performance.*Corresponding author. 3640 Colonel Glenn Hwy, Dayton, OH, 45435, USA., sherif.elbasiouny@wright.edu (S.M. Elbasiouny). Declaration of competing interestThe authors do not have any conflict of interest.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptGamal et al.KeywordsMotoneurons; Firing rate; Decoding; Kalman filter; Prosthetic control1. IntroductionPage 2Limb loss dramatically limits the lifestyle of an amputee, and upper-limb prostheses have helped amputees to overcome this functional disability [1–3]. Muscle-based electromyographic (EMG) signals and neural-based electroneurographic (ENG) signals [4] have all been used to drive prosthesis control. The peripheral EMG and ENG signals particularly have potential to provide naturalistic control, as they contain detailed low-level information about the neural drive to the muscles [5]. This information, encoded via motoneuron action potentials or spike trains, provides a fine resolution of movement intention [6]. Thus, motor unit activity has been recorded in amputees from ENG signals through electrodes implanted in peripheral nerves [4,7–12]. Numerous spike-sorting algorithms have been applied to peripheral nerve recordings to extract individual motor unit activity [13]. Motor unit activity can also be indirectly measured by decoding from EMG activity using decomposition algorithms [4–6,14–22]. Such algorithms achieved better performance in movement estimation than did the conventional amplitude-based features of EMG activity [6,19–23]. More recently, the activity of individual motor units, indirectly measured from EMG with high-fidelity pin electrodes, has been demonstrated to provide more responsive, smooth, and proportional control compared to the conventional EMG features [23].However, several limitations and difficulties still impede amputees from attaining full, natural movement with their prostheses [1,24,25]. While state-of-the-art prostheses can perform complex movements, the control algorithms for prosthetic motion are a major reported factor in limited prosthesis functionality [1,24,25]. Thus, enhanced control systems are needed to provide more naturalistic control of prostheses [25,26]. Specific needs include improved prosthesis movement accuracy and response time [4,27]. In addition, amputees need the control system to respond accurately to new and different intended movements without retraining of the control algorithm [4,27]. Most importantly, amputees require improved longevity of prosthetic control systems. Currently, motor decoders become unreliable after a short period of real-life use. This greatly hinders their usefulness outside the laboratory [5], probably contributing to the abandonment of their upper-limb prostheses by up to 50% of amputees [25]. While the variety of decoding approaches and the limited duration of clinical testing [13] has made it difficult to quantitate the extent of this issue, some studies report neural features used by decoders degrading in just a few days [5] or that training is required on a weekly or even daily basis [9]. In consequence, amputees are burdened by either frequent recalibrations of their prostheses or a rapid decline in their limbs’ responsiveness.Such degraded performance occurs because the amputee’s physiological state changes continually, yet the limb is calibrated to one or few states and tested in one, or a few limited, conditions. For instance, one’s neuromodulatory state (the excitability level of one’s Comput Biol Med. Author manuscript; available in PMC 2023 January 31.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptGamal et al.Page 3motoneurons) fluctuates throughout the day in response to the intensity of motion required [28]. Second, decoders are also generally designed to mimic a limited range of “normal” motoneuron output, which result primarily from orderly recruitment (in which motoneurons are recruited from smallest to largest). However, reversed and mixed recruitment orders have been observed in animals and humans [29–31]. Third, on a longer timescale, amputation commonly causes neurodegeneration and ongoing shifts in motoneuron numbers and their electrical properties. These changes continue well beyond the original injury and lead to fewer and loss of certain types of motoneurons as well as higher heterogeneity in the electrical properties of remaining MNs [32]. These changes also allow procedures like bionic reconstruction and targeted muscle reinnervation (TMR) to take place leading to pool compartmentalization and changes in MN firing characteristic [33,34]. Also, hardware problems, such as electrode and wire breakage, lead to fewer cells or data channels to record from to feed the decoder with incoming information [5,35,36]. Together, these issues continue to represent critical barriers to the development of prosthesis control systems that provide long-term reliability and accuracy.Efforts to produce robustly accurate prosthetic control algorithms include pattern recognition and non-pattern recognition methods [4,27,37,38]. However, the ability of such approaches to generate naturalistic movements is limited [37,38]. Here, we propose to use the Kalman filter, which has been extensively utilized in neural decoding for prosthesis control [39,40] due to its high accuracy and fast computational speed [40]. Moreover, it is known to generate stable output from noisy input signals, which occur in motor decoding problems [5,9,41]. Importantly, the Kalman filter was also shown to be robust even without large amounts of training data [9]. Multiple studies used the Kalman filter in movement estimation by decoding data recorded either from the motor cortex [41–44], ENG signals [9,45,46], EMG signals [47–49], or by combining peripheral neural and EMG signals [50,51]. Together, these studies suggest that the Kalman filter has potential to enhance the performance of prosthetic control algorithms.Therefore, the objective of this study was to develop a motoneuron-based Kalman filter decoder to support advanced prosthetic hand control systems that maintain long-term, accurate, real-time performance, in the context of both new, untrained movements and ongoing physiological changes. To accomplish this, we employed a recently developed multi-scale, high-fidelity computational model of the motor pool by Ref. [52] to input simulated motoneuron spiking activity into our proposed decoder. The model, developed in our prior work, provides high-fidelity, 3D representations of all three types of motoneurons in the pool, and has been shown to provide a highly accurate simulation of firing behaviors [53]. The Kalman-based algorithm decodes the simulated activity to estimate the underlying synaptic input (i.e., excitation level) that drives the firing of the modeled motoneurons. The estimated synaptic input is then mapped to specific movements of a simulated prosthetic hand (by MuJoCo software). Using simulated input allows us to control, alter, and know the exact biological conditions in which the decoder is performing. Further, by using a simulated prosthetic hand, we can accurately compare hand movements resulting from decoded signals against hand movements resulting from the actual synaptic inputs, which can be directly input to the simulated prost",
            {
                "entities": [
                    [
                        279,
                        376,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Empirical Software Engineering: From Discipline to InterdisciplineDaniel M´endez Fern´andeza,∗, Jan-Hendrik PassothbaSoftware and Systems Engineering, Technical University of Munich, GermanybMunich Center for Technology in Society, Technical University of Munich, Germany8102voN71]ES.sc[3v20380.5081:viXraAbstractEmpirical software engineering has received much attention in recent years and coined the shift froma more design-science-driven engineering discipline to an insight-oriented, and theory-centric one. Yet,we still face many challenges, among which some increase the need for interdisciplinary research. This isespecially true for the investigation of social, cultural and human-centric aspects of software engineering.Although we can already observe an increased recognition of the need for more interdisciplinary research in(empirical) software engineering, such research configurations come with challenges barely discussed froma scientific point of view. In this position paper, we critically reflect upon the epistemological setting ofempirical software engineering and elaborate its configuration as an Interdiscipline. In particular, we (1)elaborate a pragmatic view on empirical research for software engineering reflecting a cyclic process forknowledge creation, (2) motivate a path towards symmetrical interdisciplinary research, and (3) adopt fiverules of thumb from other interdisciplinary collaborations in our field before concluding with new emergingchallenges. This supports to elevate empirical software engineering from a developing discipline movingtowards a paradigmatic stage of normal science to one that configures interdisciplinary teams and researchmethods symmetrically.Keywords: Empirical Software Engineering, Interdisciplinary Research, Symmetrical Collaboration,Science & Technology Studies1. IntroductionStarting as a byproduct in a hardware-dominatedworld, software has become the main driver for en-tire industries and a transformative power in manyfields of contemporary society. Software engineer-ing practice and research are likewise continuouslyevolving to cope with the emerging challenges im-posed by its ubiquitous nature: practical, institu-tional, and cultural contexts of software are dy-namic and in constant change, and as they influ-ence the shape and direction of software develop-ment, boundaries between systems and applicationdomains become fuzzy. Software engineering to-day typically takes place in settings where we needto address, inter alia, application domain-specificquestions (e.g. on domain-specific terminologies,concepts, and procedures), ethical questions (e.g.∗Corresponding authorEmail address: daniel.mendez@tum.de (Daniel M´endezFern´andez)moral assessments in context of safety-critical sit-uations), juridical questions (e.g. on data privacyor regulations of algorithms and their environmentrespectively), psychological questions (e.g. on im-provements of team communications or working en-vironments), or social and political questions (e.g.on societal impacts of software-driven technologies,the concerns of heterogenous actors, or accountabil-ity issues). Human actors – whether customers, endusers, or developers – and their interests, needs,and values, but also their cognitive capabilities,fears, experiences, and expertise render softwaredevelopment endeavours as something individualand unique rather than something standardised andstrictly formalised.In the end, software is devel-oped by human beings for human beings and whatworks in one organisational context might be com-pletely alien to the culture and needs of the next.This poses new challenges for configurations ofactors, skills, and methods in research and practice,as well as on the education of future software engi-Preprint submitted to Journal of Systems and SoftwareNovember 20, 2018   \fneers (and end users). Although we can already seemore and more calls for more interdisciplinary re-search and the integration of non-technical skills inhigher education (see, e.g., [1, 2]), the calls and pro-posals – especially those in Software Engineering –concentrate largely on educational aspects, e.g. onhow to reduce the gap between isolated disciplinaryconditions in academia and multidisciplinary real-life conditions in practice (see, e.g., [3, 4]).In-terdisciplinary research, however, comes with chal-lenges barely discussed from a scientific, epistemo-logical point of view in the software engineeringcommunity. Fields like health care and medicine,biology and neuroscience, or education have explic-itly tackled such issues in the last two decades, inparts driven by the need to reflect on conditions ofsuccess of NSF and EU funding initiatives to in-tegrate “ethical, legal and social issues / aspects”(ELSI/ELSA). Here, scholars discussed and defined“multidisciplinary”, “interdisciplinary” and “trans-disciplinary” research to classify the different chal-lenges and opportunities that arise from such con-figurations [5, 6, 7]:• Multidisciplinary projects involve researchersfrom various disciplines addressing a commonproblem in parallel (or sequentially) from theirdisciplinary-specific bases integrated by assem-bling results in a common book or workshop.• Interdisciplinary projects build on a collabo-ration of researchers working jointly, but eachstill from their disciplinary-specific basis, toaddress a common problem• Transdisciplinary projects involve researchers,practitioners and actors from governance agen-cies, NGOs or companies trying to work outa shared conceptual framework instrumental-ising concepts, approaches, and theories fromtheir parent disciplines.Being involved in inter- and transdisciplinaryprojects in stem cell research, neuroscience or urbanplanning, scholars have argued that even interdis-ciplinary collaborations involving only researchersfrom two or three disciplines create challenges veryclose to those formerly discussed only in the case oftransdisciplinary projects: without investing timeand effort (and money) into the search for com-mon problems, a provisional but common language,and institutional backup, interdisciplinary projectstend to turn into multidisciplinary ones. This holdstrue already for “close” interdisciplinary collabo-rations between fields like industrial automationand software engineering. More pressing, but also2more rewarding research challenges, as we will ar-gue, emerge when trying to integrate research onthe social, cultural and human-centric practices andcontexts of software engineering. Dealing with is-sues that arose from the proliferation of tools andframeworks and the complexity growth in softwareengineering projects, software engineering has al-ready turned itself into an evidence-driven empir-ical discipline, commonly known as empirical soft-ware engineering. However, the emerging majorchallenges need far more symmetrical forms of in-quiry and design going beyond not only the prefer-ence for rationalism and formal reasoning still dom-inant in large parts of software engineering research,but also beyond the forms of empiricism already inplace. They give rise to the need of symmetrical in-terdisciplinary research configurations at eye levelto come to valid, but still manageable solutions tothe problem of balancing different epistemic andpractical requirements and standards.Placing symmetrical interdisciplinary configura-tions at the heart of empirical software engineer-ing research – especially when involving social, cul-tural, and human-centric facets – has effects on theway that software engineering as a field can ad-dress questions central to defining it as a scientificdiscipline, such as What qualifies as (good) scien-tific practice?, What counts as theory, as method-ology, and as evidence? or How are scientific con-troversies opened up, embraced, and closed? Likein other inter- and transdisciplinary configuration,many conceptual problems and methodological is-sues in our field cannot be organised in terms ofone single and dominant epistemological frameworkif we do not want to close down necessary andfruitful exchanges. Because of the high diversityof socio-economic and technical factors that per-vade software engineering environments, it is – notonly from a practical and pragmatic perspective,but also from an epistemological point of view –not sufficient to just use methods and concepts fromvarious disciplines in our research, but we need toeffectively integrate methods and research compe-tences from the various disciplines.Contribution. In this position paper, we crit-ically reflect upon the broader epistemological set-ting of empirical software engineering and elaborateits configuration as an “Interdiscipline”. We willargue for stopping to treat empirical software engi-neering as a developing discipline moving towards aparadigmatic stage of normal science, but as a con-figuration of interdisciplinary teams and research\fmethods - an interdiscipline. To this end, we willmake the following contributions: We (1) elaboratea pragmatic view on empirical research for softwareengineering that reflects a cyclic process for knowl-edge creation, (2) motivate a path towards sym-metrical interdisciplinary research, and (3) adoptfive rules of thumb from other interdisciplinary col-laborations.The key addressees ofthis manuscript aretwofold: (1) Scholars who are new to empirical soft-ware engineering (or parts of it) in general and in-terdisciplinary research in particular, and (2) schol-ars already aware of the importance of empirical re-search methods and interdisciplinary research, andinterested in a broader epistemological view.Outline. We will first briefly elaborate onthe epistemological, methodological, and pragmaticbackground of research methods already used inempirical software engineering and highlight conse-quences for theory building, methods development,and application as well as for interdisciplinary col-laborations (Sect. 2). We use those insights to elab-orate a pragmatic view on empiri",
            {
                "entities": [
                    [
                        0,
                        66,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "A Scalable and Adaptive Method for Finding Semantically Equivalent Cue Words of Uncertainty Chaomei Chena,b, Ming Song*,b, Go Eun Heob aCollege of Computing and Informatics, Drexel University, USA bDepartment of Library and Information Science, Yonsei University, South Korea Abstract Scientific knowledge is constantly subject to a variety of changes due to new discoveries, alternative interpretations, and fresh perspectives. Understanding uncertainties associated with various stages of scientific inquiries is an integral part of scientists’ domain expertise and it serves as the core of their meta-knowledge of science. Despite the growing interest in areas such as computational linguistics, systematically characterizing and tracking the epistemic status of scientific claims and their evolution in scientific disciplines remains a challenge. We present a unifying framework for the study of uncertainties explicitly and implicitly conveyed in scientific publications. The framework aims to accommodate a wide range of uncertain types, from speculations to inconsistencies and controversies. We introduce a scalable and adaptive method to recognize semantically equivalent cues of uncertainty across different fields of research and accommodate individual analysts’ unique perspectives. We demonstrate how the new method can be used to expand a small seed list of uncertainty cue words and how the validity of the expanded candidate cue words are verified. We visualize the mixture of the original and expanded uncertainty cue words to reveal the diversity of expressions of uncertainty. These cue words offer a novel resource for the study of uncertainty in scientific assertions. Keywords Uncertainty, semantically equivalent words, scientific assertions, deep learning, resources Highlights • A generalized framework of uncertainty is presented to accommodate uncertainties due to inconsistent and contradictory information as well as those associated with hedging and other linguistically focused cues. • A scalable and adaptive method selects semantically equivalent words. • The method advances the selection of scientific assertions involving uncertainties. • The study offers new resources for studying the role of uncertainty in science. Introduction A scientific proposition is a statement such as smoking causes cancer. The epistemic status of a scientific proposition refers to the best knowledge of its truthfulness given the current scientific knowledge. Thus, the epistemic status may range from completely unknown to speculations and from hypotheses to facts. The concept of uncertainty in this context characterizes the lack of sufficient information on a given proposition. A statement concerning a proposition can be considered as a combination of two parts: the proposition proper and information relevant to the epistemic status of the proposition. In this article, we focus on uncertainties due to lack of information and, in particular, uncertainties due to lack of consensus. Scientists routinely deal with such uncertainties at various stages of their research, from formulating research questions and selecting research methods to interpreting their findings and communicating their work to others (Cordner & Brown, 2013). Light et al. (2004) estimated that 11% of sentences in MEDLINE abstracts are speculative. Sociologists have studied the formation of consensus in the scientific community concerning whether smoking indeed causes cancer and whether a consensus is   \freached on climate change (Shwed & Bearman, 2010). Scientists face intensified uncertainties when inconsistent, conflicting, or contradictory findings emerge and when competing paradigms are proposed to resolve pressing crises (Kuhn, 1970). The formation of a consensus or the establishment of a dominant paradigm may correspond to a decrease of the overall uncertainty associated with a field of research. However, as we all know, searching for answers to seemingly simple questions may quickly lead to many much more complicated questions. The ability to assess the state of the art of a field of research effectively and efficiently at various levels of granularity is crucial for scientists, science policy makers, and the public. Research in computational linguistics has made significant advances in identifying uncertainty cues and negations. Remarkably influential efforts include the development of the BioScope Corpus for uncertainty and negation in biomedical publications (Vincze et al., 2008), the CoNLL 2010 Shared Task (Farkas et al., 2010) for detecting hedges and their scope in natural language text, the enrichment of a biomedical event corpus with meta-knowledge (Thompson et al., 2011), and unifying categorizations of semantic uncertainty for cross-genre and cross domain uncertainty detection (Szarvas et al., 2012). For example, the CoNLL-2010 shared task (Farkas et al., 2010) focused on detection of uncertainty cues and its linguistic scope in natural language texts. Typical hedging cue is composed of four categories: 1) auxiliaries, 2) verbs of hedging or verbs with speculative content, 3) adjectives or adverbs, and 4) conjunctions. Uncertainty detection focused on biomedical articles and text on Wikipedia. The best uncertainty detection performance in the CoNLL-2010 shared task was achieved with sequence labeling (e.g., Conditional Random Fields) in the biomedical data and bag of words sentence classification in the Wikipedia data. For the in-sentence hedge scope detection task, they classify each token to detect specific cue scopes. Their system is different from the number of class label used target and machine learning approach. More recent studies have explored the potential of measuring the confidence of biomedical models such as pathways based on textual uncertainty (Zerva et al., 2017) and the feasibility of assessing the factuality of predications extracted by SemRep (Kilicoglu et al., 2017). In a broader context, identifying and measuring the degree of uncertainties associated with scientific knowledge embedded in the vast and fast-growing volume of scientific literature remain a bottleneck (Chen, 2016). Influential computational linguistic approaches such as hedging (Hyland, 1998), semantic uncertainty (Szarvas et al., 2012), negation (Chapman et al., 2001; Morante & Daelemans, 2009), and discourse-level uncertainty (Vincze, 2013) have been largely motivated by issues concerning uncertainties from linguistic perspectives. As demonstrated by Simmerling (2015), by using grammatical, stylistic, and rhetorical options, one can talk about scientific uncertainty without using any lexical cues of uncertainty. Furthermore, philosophical and sociological studies of science, scientific creativity, and scientific discovery have highlighted the role of identifying and resolving contradictions and inconsistencies in scientific discovery and in divergent thinking in general. In particular, the value of reconciling multiple perspectives has been long recognized and advocated (Collins, 1989; Linstone, 1981). It is critical for scientists to be able to track conflicting views on the same issue and resolve seemingly contradictory evidence at a new level (Chen, 2014, 2016). The linguistically motivated approaches to the study of scientific uncertainty may benefit from a broadened scope of perspectives. In this article, we present a conceptual framework of the study of uncertainty based on a novel conceptualization of uncertainty as an epistemic status of scientific propositions. The new conceptualization underlines the nature of uncertainty as a meta-knowledge of science and its integral role in scientific change. We introduce a scalable and adaptive method to identify uncertainty cues under the broadened conceptualization of uncertainty. The resultant uncertainty cue words are expected to provide a useful resource for further studies of scientific uncertainty. The method is adaptive in the sense that analysts may generate semantically equivalent uncertainty cues of new dimensions based on a small number of example words. The rest of the article is organized as follows. First, we introduce basic concepts concerning scientific propositions and illustrate some of the most common types of uncertainties associated semantic predications in MEDLINE and the distributions of leading uncertainty cue words in other collections of scientific publications. Next, we present a scalable and adaptive method to construct a comprehensive set \fof uncertainty cue words from scientific publications. The method begins with a set of hand-crafted uncertainty cue words as seeds based on a general-purpose thesaurus of English. Then the computational method expands the seed list to a much larger set of semantically equivalent uncertainty cue words. Two judges evaluated the expanded cue words. The accepted and rejected cue words along with the seed words are visualized as non-overlapping clusters. Sample sentences selected by these uncertainty cues are discussed. The collection of the specific uncertainty cue words, classes of these words, and corresponding statistics are provided as a community resource for researchers to build on the result of our research. Uncertainties of Scientific Knowledge Scientific knowledge is a complex adaptive system of facts, beliefs, hypotheses, speculations, opinions, and a wide variety of other types of information about what we know and how much we know. It is adaptive in that existing scientific knowledge is subject to re-examination in light of new discoveries, alternative interpretations, and scenarios that are previously thought impossible (Chen, 2014; Popper, 1961). A scientist’s domain expertise consists of not only his or her knowledge of various facts and consensus in science but also an accurate understanding of the epistemic status of a wide variety of unsettled elements of a scientific domain. The epistemic status of a scientific proposition characterizes var",
            {
                "entities": [
                    [
                        0,
                        91,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "//ISynthetic Organisms and Self-Designing Systems*“The submitted manuscript has been authored by a contractor of the U.S. Government under contract No. DE- AC05-840R21400. Accordingly, the U.S.'s Government retains a nonexclusive, royahy-free license to publish or reproduce the published form of this contribution, or aSow others to do so, for U.S. Government purposes.\"W. B. DressInstrumentation and Controls Division Oak Ridge National Laboratory Oak Ridge, Tennessee 37831-6007CONF-8905130—1 DE89 010318AbstractIntroductionThis paper examines the need for complex, adaptive solutions to certain types of complex problems typified by the Strategic Defense System and NASA's Space Station and Mars Rover. Since natural systems have evolved with capabilities of intelligent behavior in complex, dynamic situations, it is proposed that bio­logical principles be identified and abstracted for application to certain problems now facing industry, defense, and space exploration.Two classes of artificial neural networks are pre­sented—a non-adaptive network used as a genetically determined \"retina,\" and a frequency-coded network as an adaptive \"brain.\" The role of a specific envi­ronment coupled with a system of artificial neural net­works having simulated sensors and effectors is seen as an ecosystem. Evolution of synthetic organisms within this ecosystem provides a powerful optimization methodology for creating intelligent systems able to function successfully in any desired environment.A complex software system involving a simulation of an environment and a program designed to cope with that environment are presented. Reliance on adaptive systems, as found in nature, is only part of the pro­posed answer, though an essential one. The second part of the proposed method makes use of an addi­tional biological metaphor—that of natural selection— to solve the dynamic optimization problems that every intelligent system eventually faces. A third area of concern in developing an adaptive, intelligent system is that of real-time computing. It is recognized that many of the problems now being explored in this area have their parallels in biological organisms, and many of the performance issues facing artificial neural net­works may find resolution in the methodology of real­time computing.\"Research performed at Oak Ridge National Labora­tory, operated by Martin Marietta Energy Systems, Inc., for the U.S. Department of Energy under Con­tract No. DE-AC05-84OR21400.The Strategic Defense System is archetyp­ical of a certain class of complex problems that are becoming increasingly important to defense and industry as the 21st century nears. Additional examples of such prob­lems include optimized control of nuclear power plant clusters, design of new and specific molecular medicines, managing the space station, and controlling unmanned planetary exploration vehicles. The com­plexity of these and similar problems raises serious questions concerning the usual methodology of hardware and software de­sign and makes impossible demands on current methods of reliability testing and system verification. The present approach in treating complex systems is to create a simulation presumed to be representative of the actual system in its essential details. Studying a simulation is thought to be a practical alternative to reality when issues of complexity, prohibitive expense, and impos­sibility of adequate testing are concerned.The need for an accurately detailed description of the physical system compo­nents and their interactions becomes para­mount, as the behavior of the simulation is the basis for developing strategies to cope with real systems. This points to what may be a major flaw in current software simula­tion and modeling philosophy, as any change requires extensive reprogramming of major parts of the entire simulation. Thus, the predictive power of a simulation to be used for the design of a complex system is easily compromised.MASTERDISTRIBUTION OF THIS DOCUMENT IS UNLIMITED\fDISCLAIMERThis report was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.D IS C L A IM E RPortions of this document may be illegible in electronic image products. Images are produced from the best available original document.\fIncompleteness of information concerning the real, physical systems being simulated imposes another intolerable burden on the simulations and support teams. In addition to the reliance demanded from implemen­tation hardware (sensors, communications, effectors, and processors), we are demand­ing that programmers perform flawlessly under extreme stress of time constraints and imperfect knowledge. This is clearly unac­ceptable, as anyone who has ever at­tempted to write, debug, and run even the simplest program can attest.This paper results from a search for a methodology to attack these very real issues of hardware and software complexity, relia­bility, and dynamic variability; and to show how software systems might become self­designing, overcoming both the severe con­straints noted above and providing the con­fidence essential to deployment by ensuring reliable and correct functioning in a chang­ing environment. The ideas presented be­low are still in their infancy, but they have been partially tested with encouraging re­sults. The main research effort is to deter­mine which principles to abstract from na­ture, the extent of abstraction necessary, and the details required for creating intelli­gent machines; for it is only through adap­tive, intelligent systems that the limitations noted above can be overcome.The problems of modeling and simulation are discussed first, and a new principle of software engineering is proposed. The question of whether a complex system can be simulated is raised. Examination of is­sues leads to a proposal for creating syn­thetic organisms to solve certain complex problems. Two network models are pre­sented as a vehicle for implementing a self­designing, complex system. Results of the two evolutionary programs based on these networks are discussed, and a number of possible extensions to the methods are given.Simulation & ModelingFor problems of sufficient complexity, a step- by-step simulation is the most efficient means of obtaining predictions of system behavior. Wolfram1 has argued that the behavior of certain systems may be ef­fectively found only by an explicit, step-by- step simulation, and he considers such systems to be \"computationally irreducible.\" Wolfram's argument amounts to showing a contradiction between the assumptions of a universal computer for such calculations and the existence of an algorithmic shortcut for the simulation. Physics and engineering are concerned primarily with the compu­tationally reducible, while most biological systems are computationally irreducible. For example, \"the development of an organ­ism from its genetic code\" may well belong to the latter class.1 Wolfram goes on to suggest that \"the only way to find out the overall characteristics of the organism from its genetic code may be to grow it explicitly. This would make large-scale computer- aided design of biological organisms, or 'biological engineering,' effectively impossi­ble: only explicit search methods analogous to Darwinian evolution could be used.\"1Given the dynamic nature of a complex, real-time control problem, the phrase \"bio­logical organism\" may be replaced with \"software\" and \"biological engineering\" with \"software engineering,\" extending the range of applicability of the previous sentence. Wolfram's suggestion then becomes a new principle of software engineering for truly complex problems. It is this principle that we wish to explore along with neural networks and adaptive systems.Complex Systems: Where Simulations FailWhy are we concerned with biology and problems of computational irreducibility? Artificial neural networks are well-under­stood computational structures firmly rooted in the mathematics of systems of first-order\fdifferential equations, and their proponents claim that many pressing problems will yield to the new paradigm of computational neu­roscience. On the other hand, a biological system is one that lives in and has been op­timized for a certain dynamic ecosystem. Such systems are complex and not well un­derstood from the simple-system perspec­tive. Evidence is accumulating from many quarters that systems combining information management and real-time control of com­plicated hardware are likewise not simple. By the very nature of an algorithm, algorith­mic methodologies developed to cope with simple systems will most assuredly fail when applied to these complex problems. Indeed, it is already evident that expert systems (deterministic decision trees) become brittle when the application domain is slightly al­tered, as does any algorithmic structure when used outside its range of applicability. Note that the ad hoc addition of \"fuzzy rea­soning\" by adding Bayesian logic or fuzzy sets does not cure this problem: once the ranges of variation are specified, the system is still essentially deterministic. Although simulation may well be a practical way to study certain problems, it is too much to hope that th",
            {
                "entities": [
                    [
                        3,
                        49,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptPattern Recognit. Author manuscript; available in PMC 2018 March 01.Published in final edited form as:Pattern Recognit. 2017 March ; 63: 531–541. doi:10.1016/j.patcog.2016.09.019.Brain Atlas Fusion from High-Thickness Diagnostic Magnetic Resonance Images by Learning-Based Super-ResolutionJinpeng Zhang#,a, Lichi Zhang#,a,c, Lei Xianga, Yeqin Shaob, Guorong Wuc, Xiaodong Zhoud, Dinggang Shen*,c,e, and Qian Wang*,aaMed-X Research Institute, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200240, ChinabNantong University, Nantong, Jiangsu 226019, ChinacDepartment of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United StatesdShanghai United Imaging Healthcare Co., Ltd., Shanghai 201815, ChinaeDepartment of Brain and Cognitive Engineering, Korea University, Seoul 02841, Republic of KoreaAbstractIt is fundamentally important to fuse the brain atlas from magnetic resonance (MR) images for many imaging-based studies. Most existing works focus on fusing the atlases from high-quality MR images. However, for low-quality diagnostic images (i.e., with high inter-slice thickness), the problem of atlas fusion has not been addressed yet. In this paper, we intend to fuse the brain atlas from the high-thickness diagnostic MR images that are prevalent for clinical routines. The main idea of our works is to extend the conventional groupwise registration by incorporating a novel super-resolution strategy. The contribution of the proposed super-resolution framework is two-fold. First, each high-thickness subject image is reconstructed to be isotropic by the patch-based sparsity learning. Then, the reconstructed isotropic image is enhanced for better quality through the random-forest-based regression model. In this way, the images obtained by the super-resolution strategy can be fused together by applying the groupwise registration method to construct the required atlas. Our experiments have shown that the proposed framework can effectively solve the problem of atlas fusion from the low-quality brain MR images.KeywordsBrain atlas; super-resolution; image enhancement; sparsity learning; random forest regression; groupwise registrationEmail addresses: jinpengzhangsjtu@gmail.com (Jinpeng Zhang#), lichizhang@sjtu.edu.cn (Lichi Zhang#), dgshen@med.unc.edu (Dinggang Shen*), wang.qian@sjtu.edu.cn (Qian Wang*)Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.1. IntroductionPage 2Medical resonance (MR) imaging has become a pivotally important tool in many brain-related clinical applications and studies. Without introducing hazardous ionizing radiation, the technique allows researchers to observe in-vivo neural structures and functions in a non-invasive way. Large-scale studies are thus enabled for (early) brain development (Thompson et al., 2000; Casey et al., 2000; Lenroot and Giedd, 2006), maturation (Sowell et al., 1999; Paus et al., 2001), and aging (Resnick et al., 2000; Raz et al., 2005). The technique has also provided a unique perspective to investigate disease anomalies (Frisoni et al., 2010; Polman et al., 2011) and to assess the effects of pharmacological interventions (Mulnard et al., 2000; Jack et al., 2004). In general, MR imaging has played a key role in the field of neuroscience as well as translational medicine. Challenged by the studies of larger scales, a lot of efforts have been devoted toward computer-assisted automatic analysis of brain MR images.The brain atlas, which can often be fused from individual brain MR images, has attracted a lot of interest (Mazziotta et al., 1995; Joshi et al., 2004). Given a group of subjects, the atlas encodes the common morphological information within the group. To this end, researchers can compare the atlases of two individual groups (e.g., the diseased group and the normal control group) and then reveal the subtle difference that might be connected with the disease. Meanwhile, the atlas provides a common space where the inter-subject variation within a population can be measured quantitatively. For example, after being registered with the atlas, each subject owns a deformation field that is typically regarded as the pathway between the subject and the atlas. Since the deformation pathways of all images in the group are established upon the same common space defined by the atlas, comparing the estimated deformation fields of all images can capture the inter-subject variation within the group. Obviously, it is important to properly designate a high-quality atlas in advance for many similar studies.However, it is yet rare and difficult to fuse the atlas from the low-quality diagnostic MR images (i.e., with high inter-slice thickness). Currently most aforementioned studies are focusing on high-quality and (nearly) isotropic imaging data, which has identical resolutions in all dimensions. The acquisitions are often conducted on designated MR scanners and may cost high. The resources required for high-quality imaging, however, are not always available. In developing countries such as China, most diagnostic MR images are still scanned with high inter-slice thickness, partially due to concerns on costs of radiation examinations and limited medical resources per capita. For the real clinical data with high inter-slice thickness, the challenge to fuse the brain atlas is yet unresolved. The lack of the atlas fusion method apparently undermines the efforts to incorporate the low-quality diagnostic imaging data into clinical researches.In this work, we intend to apply learning-based super-resolution to real clinical low-quality brain MR images and then fuse the atlas in the groupwise manner (Joshi et al., 2004). Our super-resolution consists of two stages. First, since the subject images are heterogeneous with high inter-slice thickness, we adopt the non-local patch-based strategy and utilize sparsity learning to reconstruct the subject images in the isotropic space. Second, we turn to random forest and learn the regression model for image enhancement, such that the Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 3reconstructed isotropic image (with relatively low quality) is mapped to be of higher quality (i.e., by suppressing incorrect local anatomical patterns). With all subject images processed through super-resolution, we apply groupwise registration and then fuse the atlas. Specifically, for the iteratively updated group mean image, we can apply the aforementioned forest regression model to enhance its quality. The enhanced group mean image provides better guidance in groupwise registration and leads to the atlas with higher-quality essentially.The rest of this paper is organized as follows. In Section 2, we survey the recent development of the relevant works, especially atlas fusion and super-resolution. In Section 3, we provide details of the proposed framework, including the learning-based super-resolution technique and subsequent groupwise registration. In Section 4, we demonstrate the feasibility of our novel framework through experiments. Finally, we provide the discussions with conclusions in Section 5.2. Related Works2.1. Atlas FusionIn the literature, it is popular to select a third-party standard atlas and then conduct group-level statistical analysis. For example, MNI-152 is one of the most widely accepted atlases (Mazziotta et al., 1995). The fusion of the MNI-152 atlas clearly demonstrates two important steps when fusing the brain atlas: (1) 152 individual subjects are registered with a certain template; (2) all registered images are averaged to produce the desired atlas. The MNI-152 atlas was later adopted by International Consortium for Brain Mapping (ICBM) and became part of Statistical Parametric Mapping (SPM), in which it provides automatic parcellation of neural regions-of-interest (ROI) (Tzourio-Mazoyer et al., 2002). The parcellation facilitates numerous studies upon brain morphology and structural/functional connectivity.Though simple and convenient, it is not necessarily proper to select the atlas manually for the atlas-based analysis. The anatomical variation across human brains is typically high, implying that a single and external atlas cannot fully account for individual subjects (Toga and Thompson, 2001). The (pairwise) registration between the subject(s) and the atlas can also introduce systemic bias into subsequent statistical analysis. For example, it is known that the Alzheimer’s Disease (AD) can cause brain tissue atrophy. When examining the impact of AD upon brain morphology, a large-scale group of both patients and normal controls is usually recruited for scanning the anatomical structures. If the atlas was corresponding to normal control, then it would be relatively easier to register the normal control images with the atlas than to register the patient images. In this way, the overall quality of the registered patient data would become less reliable, resulting in the imbalanced signal-to-noise ratios of the patients and the normal controls.To attain increased signal-to-noise ratio and unbiased statistical analysis, the atlas is better to be fused from all subject images in the data-driven way. Groupwise registration has provided such an alternative solution for atlas fusion o",
            {
                "entities": [
                    [
                        277,
                        387,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fContents lists available at ScienceDirect Children and Youth Services Review journal homepage: www.elsevier.com/locate/childyouth The child abuse reporting guideline compliance in Korean newspapers Serim Lee , Jieun Lee , JongSerl Chun * Department of Social Welfare, Ewha Womans University, Republic of Korea  A R T I C L E I N F O  A B S T R A C T  Keywords: Child abuse Reporting guidelines Newspapers South Korea The rate of child abuse has sharply increased worldwide, especially during the COVID-19 pandemic. As the media’s role in addressing child abuse cases is crucial, several international and formal organizations have established child abuse reporting guidelines. This study investigated how closely journalists follow reporting guidelines in addressing child abuse cases. Five major Korean presses and 189 articles from January 1, 2018, to January 31, 2021, were selected using the keyword “child abuse.” Each article was analyzed using a guideline framework consisting of 13 items regarding the five principles of the Korean Ministry of Health and Welfare and Central Child Protection Agency reporting guidelines. This study identified a radical growth in media reporting on child abuse cases in South Korea; almost 60% of the articles analyzed came from 2020 and 2021. More than 80% of the articles analyzed did not provide abuse resources, and 70% did not provide factual information. 57.1% of the articles instigated negative stereotypes, and about 30% explicitly mentioned certain family types in the headlines. Nearly 20% of the articles provided excessive details about the method used. Approximately 16% exposed victims’ identities. Some articles (7.9%) also described victims as sharing responsibility for the abuse. This study indicates that the media reports of child abuse in South Korea did not follow the guidelines in many facets. The present study discusses the limitations of the current guidelines and suggests future directions for the news media in reporting on child abuse cases nationwide.  1. Introduction According to the Korean Ministry of Health and Welfare (2020), the number of child abuse cases in South Korea has been increasing annu-ally. Especially during the COVID-19 pandemic, the risk of child abuse has increased owing to the heightened stress and social isolation of children and their families (Rosenthal & Thompson, 2020). The number of child abuse cases has increased sharply in South Korea since the beginning of the pandemic. According to the Korean National Policy Agency (2020), the number of child abuse reports in families between February and March 2020 was 1558, up 13.8 % from 2019. Additionally, according to the Korean Ministry of Health and Welfare’s (2022) anal-ysis of child abuse cases in Korea the total number of reported child abuse cases tallied in 2021 was 53,932, a significant increase of about 27.6 % compared to the previous year. Of the 37,605 cases judged as child abuse, the age range of 13–15 years accounted for the largest portion of victims with 8693 cases (23.1 %), followed by those aged 10–12 years with 8657 cases (23.0 %), and 7–9 years old with 7219 cases (19.2 %) (Korean Ministry of Health and Welfare, 2022). In terms of family types of child abuse victims, 23,838 cases (63.4 %) occurred in families with biological parents, 4618 cases (12.3 %) in mother-and-child families, 3707 cases (9.9 %) in father-and- child families, and 1980 cases (5.3 %) in remarried families (Korean Ministry of Health and Welfare, 2022). Regarding the relationship be-tween assailants and victims, parents accounted for the highest number of cases, with 31,486 cases (83.7 %), followed by 3609 cases (9.6 %) involving surrogate caregivers, and 1517 cases (4.0 %) involving rela-tives (Korean Ministry of Health and Welfare, 2022). Among the confirmed reports of child abuse, 45.1% were reported by fathers (16,944 cases), 35.6% by mothers (13,380 cases), and 3.2% by childcare teachers (1221 cases) (Korean Ministry of Health and Welfare, 2022). Regarding types of child abuse, 16,026 cases (42.6 %) involved multiple types of abuse (Korean Ministry of Health and Welfare, 2022) This was followed by 12,351 cases of emotional abuse (32.8 %), 5780 Abbreviations: WHO, World Health Organization; UNICEF, United Nations Children’s Fund; CDC, Centers for Disease Control and Prevention; NAPAC, National Association for People Abused in Childhood; PRISMA, Preferred Reporting Items for Systematic Reviews and Meta-Analyses; CCTV, Closed-circuit television. * Corresponding author at: 52, Ewhayeodae-gil, Seodaemun-gu, Seoul 03760, Republic of Korea. E-mail address: jschun@ewha.ac.kr (J. Chun). https://doi.org/10.1016/j.childyouth.2023.107037 Received 19 September 2021; Received in revised form 29 September 2022; Accepted 25 May 2023  ChildrenandYouthServicesReview151(2023)107037Availableonline30May20230190-7409/©2023ElsevierLtd.Allrightsreserved.\fS. Lee et al.                                                                                                                    cases of physical abuse (15.4 %), 2793 cases of neglect (7.4 %), and 655 cases of sexual abuse (1.7 %) (Korean Ministry of Health and Welfare, 2022). Among the cases involving multiple forms of abuse, the highest prevalence was observed for physical abuse and emotional abuse, ac-counting for 13,538 cases (36.0 %). Additionally, emotional abuse and neglect comprised 1011 cases (2.7 %); physical abuse, emotional abuse, and neglect comprised 798 cases (2.1 %); and 16 cases (0.0 %) included all types of abuse (Korean Ministry of Health and Welfare, 2022). Thus, the issue of child abuse is severe, and it is a global concern. Worldwide, child abuse reporting has almost doubled from 8 % to 17 % since the school closures due to COVID-19 (Save the Children, 2020). Therefore, child abuse cases have received significant media attention. Naturally, the media’s role in addressing child abuse cases is growing. However, the more media reports there are about child abuse cases, the more likely they are to trigger copycat crimes (Jung & Lee, 2017). Moreover, these reports often undermine the human rights of victims (Lee & Jung, 2016), an issue observed in relation to suicide (Kim et al., 2015). According to Kim et al. (2015), newspapers have a sig-nificant influence than television in inducing copycat suicides because of their easy accessibility and reproduction through various media outlets (Stack, 2002). Therefore, newspapers reporting on child abuse must exercise greater caution. However, some studies have suggested that media coverage of child abuse increase the public awareness on the issue (Saint-Jacques et al., 2012). Moreover, the increase in reporting helps individuals recognize the importance of reporting child abuse (Saint- Jacques et al., 2012). While there is a press rule requiring respect for the human rights of suspects for the benefit of the public interest and directing the media to only report necessary facts, reporters often violate this rule when a press competition begins (Lee & Jung, 2016). Media coverage of a particular crime does not necessarily reflect its true reality as it tends to distort, artificially construct, and simulate reality when reporting social issues such as crime (Payne et al., 2008). Lee and Kim (2008) presented the characteristics of and problems with crime reporting in the Korean media, including crime-reporting trends, disclosure-oriented reporting trends, sensational trends, and violations of human rights in crime reporting. Additionally, they noted that crime reports in the Korean media tend to overemphasize the investigation stage because the police and prosecution are the primary sources of coverage (Lee & Kim, 2008). That is, problems with media coverage of child abuse cases include emphasizing the brutality or deviance of criminals or perpetrators, a lack of in-depth coverage of the causes of child abuse, and provocative and sensational reporting trends (Lee & Jung, 2016). Additionally, a significant issue arises when reporting incidents involving children, as the media often fails to respect the human rights of children and lack of comprehensive awareness of these rights (Lee & Jung, 2016). Several researchers have argued that special efforts are necessary to protect human rights in crime reports, especially when they involve minors and children (Hove et al., 2013; Mejia et al., 2012; Niner et al., 2013). Crime reports can have a tremendous emotional and cognitive impact on children, and exposure to violent crime reports increases their fear of crime (Yoo et al., 2016). Further, Lee and Jung (2016) reported that children exposed to violent crime news could overestimate the possibility of crime and suffer psychological trauma as if crime scenes are persistent. This finding shows that crime reports cause significant mental and emotional damage to children. However, the number of studies on journalism and its accuracy in reporting child abuse cases is limited. For example, a study comparing newspaper reports of child abuse and neglect and data from real cases in England and Wales demonstrated that sexual abuse cases received the most media coverage, despite neglec",
            {
                "entities": [
                    [
                        907,
                        974,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Information Processing and Management journal homepage: www.elsevier.com/locate/infoproman A little bird told me your gender: Gender inferences in social media E. Fosch-Villaronga a,*, A. Poulsen b, R.A. Søraa c, B.H.M. Custers a a eLaw Center for Law and Digital Technologies, Leiden University, the Netherlands b School of Computing and Mathematics, Charles Sturt University, Australia c Researcher at Department of Interdisciplinary Studies of Culture and Department Neuromedicine and Movement Science, Norwegian University of Science and Technology (NTNU), Norway  A R T I C L E I N F O  A B S T R A C T  Keywords: Gender Twitter Social media Inference Gender classifier Automated gender recognition system Privacy Algorithmic bias Discrimination LGBTQAI+Gender stereotyping Online Behavioral Advertising Online and social media platforms employ automated recognition methods to presume user preferences, sensitive attributes such as race, gender, sexual orientation, and opinions. These opaque methods can predict behaviors for marketing purposes and influence behavior for profit, serving attention economics but also reinforcing existing biases such as gender stereotyping. Although two international human rights treaties include explicit obligations relating to harmful and wrongful stereotyping, these stereotypes persist online and offline. By identifying how inferential analytics may reinforce gender stereotyping and affect marginalized communities, opportunities for addressing these concerns and thereby increasing privacy, diversity, and in-clusion online can be explored. This is important because misgendering reinforces gender ste-reotypes, accentuates gender binarism, undermines privacy and autonomy, and may cause feelings of rejection, impacting people’s self-esteem, confidence, and authenticity. In turn, this may increase social stigmatization. This study brings into view concerns of discrimination and exacerbation of existing biases that online platforms continue to replicate and that literature starts to highlight. The implications of misgendering on Twitter are investigated to illustrate the impact of algorithmic bias on inadvertent privacy violations and reinforcement of social preju-dices of gender through a multidisciplinary perspective, including legal, computer science, and critical feminist media-studies viewpoints. An online pilot survey was conducted to better un-derstand how accurate Twitter’s gender inferences of its users’ gender identities are. This served as a basis for exploring the implications of this social media practice.  1. Introduction Online and social media platform providers use attributes of their users, including their name, age, and gender, to improve user experience and online behavioral advertising (OBA). For instance, the social media platform Twitter infers gender from a wide variety of sources.1 By processing user attributes, companies can target or exclude certain groups more easily, tailor their services to users, and increase their attention levels (Ur, Leon, Cranor, Shay & Wang, 2012). In this way, profiling makes marketing more precise and * Corresponding author. E-mail addresses: e.fosch.villaronga@law.leidenuniv.nl (E. Fosch-Villaronga), apoulsen@csu.edu.au (A. Poulsen), roger.soraa@ntnu.no (R.A. Søraa), b.h.m.custers@law.leidenuniv.nl (B.H.M. Custers).  1 See https://help.twitter.com/en/rules-and-policies/data-processing-legal-bases. https://doi.org/10.1016/j.ipm.2021.102541 Received 29 October 2020; Received in revised form 7 January 2021; Accepted 2 February 2021  InformationProcessingandManagement58(2021)102541Availableonline18February20210306-4573/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fE. Fosch-Villaronga et al.                                                                                               effective. However, a growing concern is the increasing use of transparent inferential analytics that reveal sensitive user traits that serve attention economics,2 (Davenport & Beck, 2001) and that may reinforce existing biases that, although not explicit, can be very influential in exacerbating discrimination (Caliskan, Bryson & Narayanan, 2017; Custers, 2018). One bias is gender stereotyping, which ’refers to the practice of ascribing to an individual woman or man specific attributes, characteristics, or roles by reason only of her or his membership in the social group of women or men’ (OHCHR, 2020). However, gender stereotyping is a complex process that, although based on strong beliefs of what gender is and should be, is understood too simplistically (Kachel, Steffens & Niedlich, 2016). For instance, Sink, Mastro and Dragojevic (2018): 592) investigated how television character perceptions often judged effeminate gay men negatively. They found out that ’straight-acting’ or hyper-masculine gay men are evaluated more favorably for conforming to and even mastering heteronormative gender roles. Also, \"gay men who are perceived to be more feminine would map onto traditional female stereotypes (i.e., warm but less competent)\" (ibid). Two international human rights treaties include explicit obligations relating to harmful and wrongful stereotyping (mainly Art. 5 of the Convention on the Elimination of All Forms of Discrimination against Women and Art. 8(1)(b) of the Convention on the Rights of Persons with Disabilities).3 Although States are usually the recipients of human rights treaties, the United Nations Human Rights Council has shown growing attention to the responsibility that corporations, sectors, and industries worldwide have for respecting human rights (OHCHR, 2012). Still, these stereotypes persist online and offline (Grant, Grey & van Hell, 2020; Hentschel, Heilman & Peus, 2019), as if platforms failed to understand—or deliberately choose to ignore—that gender is not merely being a ’man’ or a ’woman,’ but a social construct (Butler, 1990). In the words of De Beauvoir (1949), \"one is not born, but rather becomes a woman\". It is now becoming clear that these practices continue to exist online, for instance, on social media platforms. As described below, Twitter often assigns you to be a ‘woman’ if you’re a gay man. This socio-technical construction of gender shows the intricate gendered way of how humans shape their own identities through the technology they use and interact with (Søraa, 2017). In this contribution, the impact of inferential analytics on inadvertent privacy violations and the reinforcement of social prejudices of gender and sexuality is investigated by means of a specific case study (i.e., Twitter) through a multidisciplinary perspective, including legal, computer science, and queer media viewpoint. Therefore, the central research question of this contribution is: What are the implications of any inaccurate gender inferences by Twitter? This research question is only relevant if Twitter actually performs gender inferences and if these inferences are sometimes inaccurate. As is shown in a pilot survey presented in this contribution, there is evidence supporting this. In a broader perspective, addressing this research question illustrates concerns of discrimination, mis-gendering, and exacerbation of existing biases that online platforms persist in replicating and that literature has started to highlight very recently (Hamidi, Scheuerman & Branham, 2018; Keyes, 2018). The rationale behind the research question is, first, that gender is a co-shaped, changing part of human-identity tied into the socio- materiality of gendered relations often treated as a binary dichotomy. The assumption that gender is physiologically-rooted harms transgender people by essentializing the body as the source of gender and also harms non-binary people, who cannot be accurately classified (Keyes, 2018). The categories ‘female’ and ‘male’ do not reflect who they are (Fergus, 2020). Second, that platform providers no longer have to learn sensitive details about a particular user or to correctly group users into categories for advertising to be effective, as advertising has a high tolerance for classification errors (Wachter, 2020). Nevertheless, not considering gender and sexuality in social media platforms can be socially harmful and expensive, as the unconscious bias of technology usage and implementation may lead to further exacerbation of existing biases, for gender and race if only (Bray, 2007; Hao, 2019a; Schiebinger, 2014). In this article, we start with providing some background information on the mechanics of inferential analytics to elucidate how companies infer specific user attributes, including gender, and how these techniques may harm users’ rights in Section 2. Also, we introduce the technical rationale of gender classifiers. In Section 3, we explain how we conducted this study, which is based on configuring an online pilot survey geared towards understanding how accurate Twitters’ gender inferences of its users’ gender identities are and, with the support of survey data, explore what implications this social media practice has. Our results, presented in Section 4, already anticipate the binary understanding of gender from Twitter, which excludes those not fitting the category ‘male’ and ‘female,’ that inferring gender is part of the Twitter’s personalization trade-off, and that in nearly 20% of the cases it misgendered its users. After presenting the data, we discuss the social implications of gender inference on social media platforms in Section 5, including the lack of diversity in social media platforms and the role designers play in accounting for inclusivity and diversity. We also argue that platforms use a form of scapegoating to get away with the inference of sensitive user traits without user awareness. Consequences for data protection and discrimination are also discussed. We conclude the article in Section 6 by su",
            {
                "entities": [
                    [
                        133,
                        201,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Delft University of TechnologyA healthy debateExploring the views of medical doctors on the ethics of artificial intelligenceMartinho, Andreia; Kroesen, Maarten; Chorus, CasparDOI10.1016/j.artmed.2021.102190Publication date2021Document VersionFinal published versionPublished inArtificial Intelligence in MedicineCitation (APA)Martinho, A., Kroesen, M., & Chorus, C. (2021). A healthy debate: Exploring the views of medical doctors onthe ethics of artificial intelligence. Artificial Intelligence in Medicine, 121, [102190].https://doi.org/10.1016/j.artmed.2021.102190Important noteTo cite this publication, please use the final published version (if applicable).Please check the document version above.CopyrightOther than for strictly personal use, it is not permitted to download, forward or distribute the text or part of it, without the consentof the author(s) and/or copyright holder(s), unless the work is under an open content license such as Creative Commons.Takedown policyPlease contact us and provide details if you believe this document breaches copyrights.We will remove access to the work immediately and investigate your claim.This work is downloaded from Delft University of Technology.For technical reasons the number of authors shown on this cover page is limited to a maximum of 10. \fContents lists available at ScienceDirect Artificial Intelligence In Medicine journal homepage: www.elsevier.com/locate/artmed A healthy debate: Exploring the views of medical doctors on the ethics of artificial intelligence Andreia Martinho *, Maarten Kroesen, Caspar Chorus Delft University of Technology, Delft, the Netherlands  A R T I C L E I N F O  A B S T R A C T  Keywords: Artificial intelligence Healthcare Medicine Ethics Q-methodology Artificial Intelligence (AI) is moving towards the health space. It is generally acknowledged that, while there is great promise in the implementation of AI technologies in healthcare, it also raises important ethical issues. In this study we surveyed medical doctors based in The Netherlands, Portugal, and the U.S. from a diverse mix of medical specializations about the ethics surrounding Health AI. Four main perspectives have emerged from the data representing different views about this matter. The first perspective (AI is a helpful tool: Let physicians do what they were trained for) highlights the efficiency associated with automation, which will allow doctors to have the time to focus on expanding their medical knowledge and skills. The second perspective (Rules & Regulations are crucial: Private companies only think about money) shows strong distrust in private tech companies and emphasizes the need for regulatory oversight. The third perspective (Ethics is enough: Private companies can be trusted) puts more trust in private tech companies and maintains that ethics is sufficient to ground these corporations. And finally the fourth perspective (Explainable AI tools: Learning is necessary and inevitable) emphasizes the importance of explainability of AI tools in order to ensure that doctors are engaged in the technological progress. Each perspective provides valuable and often contrasting insights about ethical issues that should be operationalized and accounted for in the design and development of AI Health.  1. Introduction Artificial Intelligence (AI) is moving towards the health space. Given the abundance of data generated by health systems as a result of digi-tization efforts made over the last decade, a new data-driven approach to implement AI in healthcare has emerged. In contrast with previous and somewhat failed rule-based approaches to implement AI in healthcare [1,2], this new approach relies heavily on algorithms that detect pat-terns in data from clinical practice (e.g. medical imaging and electronic health records), clinical trials, genomics studies, and insurance, phar-maceutical, and pharmacy benefits management operations [3]. There is an expectation that these state-of-the-art-data-driven AI methods and algorithms will be able to use such data to address the complex problems of health systems [4,3]. The implementation of AI in healthcare holds great promise for expanding the medical knowledge and providing optimal yet cost- effective healthcare solutions [5,6]. In the clinical domain, expected results include identification of individuals at high risk for a disease, improved diagnosis and matching of effective personalized treatment, and out-of-hospital monitoring of therapy response [4,7]. Despite the projected benefits associated with Health AI, it also raises important ethical issues [8,9]. It is well known that AI has the potential to threaten values such as Autonomy, Privacy, and Safety [10], which are core values in Medicine [11,12]. Therefore, in order for AI to promote quality of care and minimize potentially disruptive effects [13], its deployment must take ethics into account. An important step towards ethical deployment of disruptive AI technologies is to learn the views of practitioners about such technologies. This information allows a better operationalization of the ethical issues associated with AI in a particular domain, which eventually is expected to lead to more meaningful debates and robust policies. The current academic literature provides interesting and valuable information on the perspectives of practitioners about the impact of AI technologies in the medical profession [14,15,16,17]. Most of these studies are particularly suited to medical fields with a strong image processing component, which is adequate for automated analysis, such as radiology [18,19,20,21,22,23,24], pathology [25], and dermatology * Corresponding author. E-mail address: a.m.martinho@tudelft.nl (A. Martinho). https://doi.org/10.1016/j.artmed.2021.102190 Received 12 February 2021; Received in revised form 22 September 2021; Accepted 29 September 2021  ArtificialIntelligenceInMedicine121(2021)102190Availableonline12October20210933-3657/©2021DelftUniversityofTechnology.PublishedbyElsevierB.V.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fA. Martinho et al.                                                                                                                [26,27]. However, there is little knowledge on the views of medical doctors about the ethical issues associated with the implementation of AI in healthcare. The aim of this study is to gain insight into the reasoning patterns and moral opinions about Health AI from those involved in the medical practice. By surveying medical doctors in The Netherlands, Portugal, and U.S. on the ethical issues associated with the implementation of AI in healthcare, we expect to enrich existing literature on the impact of AI technologies in medicine and provide valuable knowledge for the operationalization of Health AI Ethics. We first provide a brief commentary about the ethics of AI in healthcare. Subsequently we explain the methods used in this research by outlining the basic steps of q-methodology and explaining how we established these steps in this study. Later we present the results of the study by describing the four different perspectives that have emerged from the data. These results are further analyzed and discussed. Finally we draw conclusions and present directions for further research. 2. The ethics of health AI The empirical work about AI in healthcare that has been reported in the literature focuses mainly on issues directly related to the medical practice and career, such as Future of Employment, Education about AI, and Accountability. It has been reported that medical students and practitioners under-stand the increasing importance of AI in healthcare and have positive attitudes towards the clinical use of AI [20,26,17], but mainly as a supportive system for diagnosis [18,19,26,27,25,24]. Despite the positive attitudes towards AI, it has also been reported that students and medical doctors are poorly trained on these technol-ogies [20,28,29,30]. One study indicated that, although a small cohort of UK medical students who received AI teaching felt more confident in working with AI in the future compared to students that did not receive teaching, a significant number of taught students still felt inadequately prepared [20]. In order to take full advantage of these technologies, scholars seem to agree that medical school training on AI should be expanded and improved [18,20,21,26,25]. Regarding the impact of AI on career choice and reputation, it was reported that AI has an impact in the career intentions of students with respect to radiology [20], but radiologists would still choose this spe-cialty if given that choice [21]. These specialists have, however, revealed concerns that AI might diminish their professional reputation [24]. Contrary to the perceptions of the general public that AI will completely or partially replace human doctors [31], medical students and doctors in general are not concerned about job replacement [18,26,17,32,24]. Another important issue related to medical practice and career is liability. In a study in which pathologists were surveyed, it was reported that, with respect to medico-legal responsibility for diagnostic errors made by a human/AI combination, opinions were split between those who believed that the platform vendor and pathologist should be held equally liable, and others who believed responsibility remains primarily that of the human, with only a minority reporting that the platform vendor should primarily be liable [25]. Clearly, the ethics surrounding implementation of AI in healthcare goes beyond issues related to medical practice and career. Health AI gives rise to higher level ethical issues such as Autonomy, Fairness, or Privacy [33,10] but, with the exception of fairness, these issues have received less attention in the scientific literature. Fairness concerns related to racial and gender bias in AI-powered medical applications have t",
            {
                "entities": [
                    [
                        1430,
                        1527,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Vrije Universiteit BrusselVulnerable Data SubjectsMalgieri, GianclaudioPublished in:Computer Law & Security ReviewDOI:10.1016/j.clsr.2020.105415Publication date:2020License:CC BYDocument Version:Final published versionLink to publicationCitation for published version (APA):Malgieri, G. (2020). Vulnerable Data Subjects. Computer Law & Security Review, 37, 1-22. [105415].https://doi.org/10.1016/j.clsr.2020.105415CopyrightNo part of this publication may be reproduced or transmitted in any form, without the prior written permission of the author(s) or other rightsholders to whom publication rights have been transferred, unless permitted by a license attached to the publication (a Creative Commonslicense or other), or unless exceptions to copyright law apply.Take down policyIf you believe that this document infringes your copyright or other rights, please contact openaccess@vub.be, with details of the nature of theinfringement. We will investigate the claim and if justified, we will take the appropriate steps.Download date: 04. jul. 2023 \fcomputer law & security review 37 (2020) 105415 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR Vulnerable data subjects Gianclaudio Malgieri a , 1 , J ˛edrzej Niklas b , 1 , ∗a b Vrije Universiteit Brussel, Belgium Cardiff University, United Kingdom a r t i c l e i n f o a b s t r a c t Keywords: Data protection Vulnerability Vulnerable groups Discrimination AI Research ethics Discussion about vulnerable individuals and communities spread from research ethics to consumer law and human rights. According to many theoreticians and practitioners, the framework of vulnerability allows formulating an alternative language to articulate prob- lems of inequality, power imbalances and social injustice. Building on this conceptualisa- tion, we try to understand the role and potentiality of the notion of vulnerable data subjects. The starting point for this reflection is wide-ranging development, deployment and use of data-driven technologies that may pose substantial risks to human rights, the rule of law and social justice. Implementation of such technologies can lead to discrimination system- atic marginalisation of different communities and the exploitation of people in particularly sensitive life situations. Considering those problems, we recognise the special role of per- sonal data protection and call for its vulnerability-aware interpretation. This article makes three contributions. First, we examine how the notion of vulnerability is conceptualised and used in the philosophy, human rights and European law. We then confront those findings with the presence and interpretation of vulnerability in data protection law and discourse. Second, we identify two problematic dichotomies that emerge from the theoretical and prac- tical application of this concept in data protection. Those dichotomies reflect the tensions within the definition and manifestation of vulnerability. To overcome limitations that arose from those two dichotomies we support the idea of layered vulnerability, which seems com- patible with the GDPR and the risk-based approach. Finally, we outline how the notion of vulnerability can influence the interpretation of particular provisions in the GDPR. In this process, we focus on issues of consent, Data Protection Impact Assessment, the role of Data Protection Authorities, and the participation of data subjects in the decision making about data processing. © 2020 Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) ∗ Corresponding author: J ˛edrzej Niklas, Cardiff University, United Kingdom. 1 E-mail addresses: Gianclaudio.Malgieri@vub.be (G. Malgieri), niklasj@cardiff.ac.uk (J. Niklas). Both the authors contributed equally to each paragraph. The authors would also like to thank the anonymous reviewer whose sug- gestions have greatly improved this article. The open access version of this research was funded by the EU Commission, H2020 SWAFS Programme, PANELFIT Project, research grant number 788039. https://doi.org/10.1016/j.clsr.2020.105415 0267-3649/© 2020 Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) \f2 computer law & security review 37 (2020) 105415 1. Introduction For decades, experts in research ethics have assumed that some research participants and communities are more likely to be mistreated, abused, exploited or harmed.2 Such groups seem to possess a level of vulnerability, which generates cer- tain obligations and responsibilities for researchers and over- sight entities. The principle of special treatment of “vulner- able groups” was incorporated into various declarations and guidelines that regulate especially clinical research, like the Belmont Report or the Declaration of Helsinki.3 Those docu- ments predominantly focus on the issue of consent and in- formed participation, highlighting problems of autonomy and integrity. Nevertheless, some other interpretations add more elaborated understanding of vulnerability and raise issues of power imbalance and political and economic disadvantage.4 In other words, the language of vulnerability in research ethics allows greater sensitivity and responsiveness to equity, dis- crimination and different socio-historical contexts. However, the notion of vulnerability is also discussed in other fields. From human rights to political philosophy, the concept is seen as a framework that enables the articulation of broad issues that fill into the category of social justice and uncover human exposure to harms, pain and suffering.5 As it will be argued below, human vulnerability is also (to some extent) present in the discussions about data protection, privacy and data-driven technologies. Calo, a prominent voice in this debate, argues that the rationale for privacy protec- tion is precisely addressing vulnerability of individuals.6 Put it differently, privacy and data protection regimes are man- ifestations of the idea that all individuals are vulnerable to the power imbalances created by data-driven technologies. 2 3 4 5 Carol Levine et al., “The Limitations of ‘Vulnerability’ as a Pro- tection for Human Research Participants,” The American Journal of Bioethics 4, no. 3 (August 2004): 44–49, https://doi.org/10.1080/ 15265160490497083 . Phoebe Friesen et al., “Rethinking the Belmont Report?,” The American Journal of Bioethics 17, no. 7 (July 3, 2017): 15–21, https: //doi.org/10.1080/15265161.2017.1329482 . Dearbhail Bracken-Roche et al., “The Concept of ‘Vulnerability’ in Research Ethics: An in-Depth Analysis of Policies and Guide- lines,” Health Research Policy and Systems 15, no. 1 (December 2017): 8, https://doi.org/10.1186/s12961- 016- 0164- 6 . Lourdes Peroni and Alexandra Timmer, “Vulnerable Groups: The Promise of an Emerging Concept in European Human Rights Convention Law,” International Journal of Constitutional Law 11, no. 4 (October 1, 2013): 1056–85, https://doi.org/10.1093/icon/mot042 ; Rebecca Hewer, “A Gossamer Consensus: Discourses of Vulner- ability in the Westminster Prostitution Policy Subsystem,” Social & Legal Studies 28, no. 2 (April 2019): 227–49, https://doi.org/10. 1177/0964663918758513 ; Isabelle Bartkowiak-Théron and Nicole L. Asquith, “Conceptual Divides and Practice Synergies in Law En- forcement and Public Health: Some Lessons from Policing Vulner- ability in Australia,” Policing and Society 27, no. 3 (April 3, 2017): 276–88, https://doi.org/10.1080/10439463.2016.1216553 , Martha Albert- son Fineman, “The Vulnerable Subject: Anchoring Equality in the Human Condition,” Yale Journal of Law and Feminism 20 (2008): 23; Ju- dith Butler, Precarious Life: The Powers of Mourning and Violence (Lon- don ; New York: Verso, 2004). Ryan Calo, “Privacy, Vulnerability, and Affordance,” DePaul L. 6 Rev. 66 (2017): 592–593. detect children anxiety and depression 9 Additionally, different scholars explain how data-driven tech- nologies can lead to discrimination, social marginalisation or affect human autonomy and dignity and exploit particular communities.7 Such controversial cases in the data-driven re- search concern automated systems that identify sexual orien- tation,8 or predict and prevent suicide.10 Finally, the notion of vulnerability appears in the discussion about ethics and regulation of Artificial In- telligence. Here some of the guidelines and ethical policies call for the governance frameworks that recognise the situation of vulnerable groups such as women, persons with disabilities, ethnic minorities, children, and consumers.11 It seems to us that the issue of human vulnerability should be an important topic in the data protection debate, consid- ering the new risks of individual exploitation in the algorith- mic environment. Involving vulnerability as a “heuristic tool”could emphasise existing inequalities between different data subjects and specify in a more systematic and consolidated way that the exercise of data rights is conditioned by many factors such as health, age, gender or social status. However, the scholarly discussion about vulnerable data subjects is still largely underdeveloped. Accordingly, in this article, we try to understand and conceptualise how the notion of vulnerable individuals finds its way in the data protection debate. More precisely, when human vulnerability can influence the way we are interpreting data protection regimes. We are aware that it is not possible to address this com- plex topic in one article satisfactorily. Our modest goal here is to initiate a discussion about this topic and its problem- atic aspects, suggesting some first interpretative paths, while calling for further analysis and research. To do this, we first investigate the meaning of “vulnerable individuals”, look- ing in particular at the theoretical discussion about vulner- ability ( Section 2 ). Taking ",
            {
                "entities": [
                    [
                        26,
                        50,
                        "TITLE"
                    ],
                    [
                        295,
                        319,
                        "TITLE"
                    ],
                    [
                        1187,
                        1211,
                        "TITLE"
                    ],
                    [
                        1896,
                        1920,
                        "TITLE"
                    ],
                    [
                        9217,
                        9241,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect International Journal of Information Management journal homepage: www.elsevier.com/locate/ijinfomgt Research Article Emotional reactions to robot colleagues in a role-playing experiment Nina Savela a,*, Atte Oksanen a, Max Pellert b, c,d, David Garcia b,c, d a Faculty of Social Sciences, Tampere University b Institute of Interactive Systems and Data Science, Department of Computer Science and Biomedical Engineering, Graz University of Technology c Complexity Science Hub Vienna d Center for Medical Statistics, Informatics and Intelligent Systems, Medical University of Vienna  A R T I C L E I N F O  A B S T R A C T  Keywords: Robot Work Sentiment Role-play Experiment We investigated how people react emotionally to working with robots in three scenario-based role-playing survey experiments collected in 2019 and 2020 from the United States (Study 1: N = 1003; Study 2: N = 969, Study 3: N = 1059). Participants were randomly assigned to groups and asked to write a short post about a scenario in which we manipulated the number of robot teammates or the size of the social group (work team vs. organi-zation). Emotional content of the corpora was measured using six sentiment analysis tools, and socio- demographic and other factors were assessed through survey questions and LIWC lexicons and further analyzed in Study 4. The results showed that people are less enthusiastic about working with robots than with humans. Our findings suggest these more negative reactions stem from feelings of oddity in an unusual situation and the lack of social interaction.  1. Introduction People have been using automation and working with robots in in-dustry fields such as manufacturing for many years. Researchers suggest that the exceptional situation caused by COVID-19 and social distancing guidelines will further increase the use of advanced information sys-tems, such as robots, at work (Coombs, 2020; He, Zhang, & Li, 2021). Due to the development of more interactive, collaborative, and social robots, people are more likely to be in situations in which they must work and interact with robots as coworkers or teammates (Dwivedi et al., 2021; Haidegger et al., 2013; M¨ortl et al., 2012). As a result, new-generation robots will create new social and psychological chal-lenges that could impact work life profoundly. There is a sufficient body of evidence confirming that social psy-chological processes such as attitudes and trust are essential factors in successful collaboration with robots and ultimately accepting them in everyday life (Hancock et al., 2011; Schaefer, Straub, Chen, Putney, & Evans, 2017; Sheridan, 2016; Yusif, Soar, & Hafeez-Baig, 2016). In addition to these extensively researched factors, robotization is likely to arouse both positive and negative emotional reactions in human workers. Introducing advanced technology such as social robots as co-workers in the same organization or work team presents human workers with a new situation. Adapting to this could be more challenging to some workers than others, causing negative attitudes and emotions that could have an unwanted effect on emotional well-being. In addition to examining acceptance of robots through attitudes and trust, researchers have investigated emotional attachment to companion robots (Friedman, Kahn, & Hagman, 2003); emotional reactions to ill-treatment of robots (Rosenthal-von der Pütten, Kr¨amer, Hoffmann, Sobieraj, & Eimler, 2013); and the connection between negative emo-tions, such as anxiety, and negative attitudes (Nomura, Kanda, & Suzuki, 2006). Even though working closely with robots has been argued to arouse negative attitudinal and emotional reactions in human workers (Groom & Nass, 2007), we do not currently know how people would respond emotionally to working with robots on the same work team or in the workplace community with robots. In addition to explicit methods of measuring attitudes and emotions, such as surveys, emotional and attitudinal reactions toward robot co-workers can be investigated through more implicit means such as examining textual data collected from role-playing scenarios. Computer- aided analysis methods have generated the massive new field of affec-tive computing, which offers fast and quantitative means of analyzing large amounts of text with the help of emotional lexicons (Piryani, Madhavi, & Singh, 2017). Our study was designed to fill the research gap through analysis of textual data collected from three role-playing experiments that involved * Corresponding author at: Faculty of Social Sciences, 33014 Tampere University, Tampere, Finland. E-mail address: nina.savela@tuni.fi (N. Savela). https://doi.org/10.1016/j.ijinfomgt.2021.102361 Received 5 August 2020; Received in revised form 6 May 2021; Accepted 7 May 2021  InternationalJournalofInformationManagement60(2021)102361Availableonline23May20210268-4012/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).Konstanzer Online-Publikations-System (KOPS) URL: http://nbn-resolving.de/urn:nbn:de:bsz:352-2-1cnionttcewjv7\fN. Savela et al.                                                                                                                 introduction of robots as work team members or as coworkers within a workplace. We focused on emotional reactions to the hypothetical sit-uations, as identified via sentiment analysis, in three studies and further investigated the associated factors in a fourth study. Computational social scientific analysis methods combined with an experimental design and online role-playing data collection method generated a unique multi-methodological approach that has not previously been utilized to investigate the acceptance of robots. 2. Literature review The concept of emotion has a long and complex history in philosophy and psychology, and it has traditionally been used as a metaconcept that combines different words describing feelings and attitudes (Dixon, 2012). One empirical study considered emotion as an intense mental state with hedonic content (Cabanac, 2002). There is no consensus on the definition, process, or hierarchical levels of emotion among multiple emotion theories, but most support some form of connection between emotion and cognitive appraisal (Barnard & Teasdale, 1991; Moors, 2009). Theories of attitudes often include both cognitive and emotional perspectives, and this is specifically manifested in a multicomponent model of attitude (Zanna & Rempel, 2008). In the context of technology, researchers have investigated possible connections between cognitive and emotional constructs in the framework of the technology acceptance model (TAM) and its extensions (Kulviwat, Bruner, Kumar, Nasco, & Clark, 2007; Lee, Xiong, & Hu, 2012; Saad´e & Kira, 2006; Venkatesh, 2000). For example, in a model called consumer acceptance of tech-nology, affective and cognitive attitude dimensions explain the behav-ioral attitude toward adoption, which then predicts adoption intention (Kulviwat et al., 2007). According to a literature review about the his-tory of TAM (Maranguni´c & Grani´c, 2015), further integration of emo-tions into TAM is still needed. In research focused on the advanced technology of robots specif-ically, attitudes and emotions have often overlapped, especially in research measuring and focusing on negative emotions, such as anxiety, and negative attitudes (Nomura et al., 2006). TAM and its extensions have also been used in research on human–robot interaction and user studies, but some researchers have stressed caution when applying it to interactive technology such as robots (Young, Hawkins, Sharlin, & Igarashi, 2009). For this reason and because this research area is an emerging field, the tools used to measure different social and psycho-logical constructs have varied. Because emotion is linked to attitudes and behavior (Gursoy, Chi, Lu, & Nunkoo, 2019; Kulviwat et al., 2007), and because the cognitive measures of attitude have their weaknesses (Peters & Slovic, 2007), investigating emotional responses in acceptance of emerging technologies such as robots is an important research avenue. Evidence that humans can feel empathy and get emotionally attached to artificial beings confirms that artificial entities such as ro-bots can arouse emotional reactions (Kr¨amer, Eimler, von der Pütten, & Payr, 2011; Rosenthal-von der Pütten et al., 2013). Other researchers suggested that even imagined contact with a robot can affect emotions toward robots (Wullenkord, Fraune, Eyssel, & ˇSabanovi´c, 2016). The examination of emotions toward robots is essential because they affect social processes such as identification and play an important role in human behavior (DeSteno, Dasgupta, Bartlett, & Cajdric, 2004; DeS-teno, Petty, Rucker, Wegener, & Braverman, 2004). This has conse-quences for the intended use and possible benefits gained from larger utilization of robots in work life. Emotional detection literature offers different ways to examine emotions from facial expressions, speech, and writing (Cowie & Cor-nelius, 2003; Russell, Bachorowski, & Fern´andez-Dols, 2003). For example, females and older people are more likely to express positivity in writing (Pennebaker & Stone, 2003; Thelwall, Wilkinson, & Uppal, 2010), neurotic people are likely to use negative language, and extraverted and agreeable people are more likely to use positive words (Yarkoni, 2010). However, different associations could emerge in the context of robots. The more traditional research literature on robot acceptance gives some information about the expected associations and factors to consider when studying emotional expressions in written re-actions toward robots. Some literature has suggested a difference in attitudes toward robots based on age and gender, with young individuals and males being more willing to accept robots (Flandorfer, 2012). H",
            {
                "entities": [
                    [
                        159,
                        227,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Futures journal homepage: www.elsevier.com/locate/futures Moral circle expansion: A promising strategy to impact the far future Jacy Reese Anthis a,b,*, Eze Paez c,d a The University of Chicago, USA b Sentience Institute, USA c Pompeu Fabra University, Spain d Centre de Recherche en ´Ethique, Universit´e de Montr´eal, Canada  A R T I C L E I N F O  A B S T R A C T  Keywords: Existential risk Effective altruism Artificial intelligence Animal ethics Longtermism 1. Introduction Many sentient beings suffer serious harms due to a lack of moral consideration. Importantly, such harms could also occur to a potentially astronomical number of morally considerable future beings. This paper argues that, to prevent such existential risks, we should prioritise the strategy of expanding humanity’s moral circle to include, ideally, all sentient beings. We present empirical evidence that, at micro- and macro-levels of society, increased concern for members of some outlying groups facilitates concern for others. We argue that the perspective of moral circle expansion can reveal and clarify important issues in futures studies, particularly regarding animal ethics and artificial intelligence. While the case for moral circle expansion does not hinge on specific moral criteria, we focus on sentience as the most recommendable policy when deciding, as we do, under moral uncertainty. We also address various nuances of adjusting the moral circle, such as the risk of over-expansion.  There are currently around 8 billion humans (109) on Earth. There are over 100 billion domestic animals (1011)—primarily chickens and fishes used in the food industry. There may be trillions of wild birds and mammals (1012) and over a quintillion tiny wild animals such as insects (1018) (Tomasik, 2009). These moral stakes are difficult to conceptualise, yet they pale in comparison to the potential long-term scope of human civilisation in the distant future, such as with interstellar expansion.1 Some moral philosophers and futures scholars are beginning to consider our impact on the far future as a serious and perhaps overwhelmingly important consideration that we should account for in present decisions (see, for example, Beard, Rowe, & James, 2020; Bostrom, 2014; Liu, Lauta, & Maas, 2018; Kareiva & Carranza, 2018; Moynihan, 2020; Parfit, 1992). Such work is part of a broader project of prescriptive futures studies, asking questions such as how to encourage thoughtful contemplation of the future, how to design future-oriented public policy, and how to account for the indirect effects of our actions on the future (Ahvenharju, * Corresponding author. E-mail addresses: jacy@uchicago.edu (J.R. Anthis), joseezequiel.paez@upf.edu (E. Paez).  1 For example, suppose humanity expanded to the Virgo Supercluster. Then the human population could be one hundred undecillion (1038). See Bostrom (2003). Notice, however, that the same interstellar resources could fuel many more minds of less complexity, and therefore less energy cost, than the human mind. Thus, assuming minds less complex than the human mind can be sentient, the total number of possible future sentient in-dividuals is proportionally higher. https://doi.org/10.1016/j.futures.2021.102756 Received 22 March 2021; Received in revised form 23 April 2021; Accepted 26 April 2021  Futures130(2021)102756Availableonline30April20210016-3287/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fJ.R. Anthis and E. Paez                                                                                               Minkkinen, & Lalot, 2018; Dorsser, Walker, Taneja, & Marchau, 2018; Ng, 2020). Restricting our concern to present or near-future generations of human beings may be a significant normative blindspot, neglecting the majority of individuals who will, at any time, exist. While work on ‘existential risk’ has focused on extinction risk in particular, recent work has highlighted the importance of ‘suffering risks’, which are ‘astronomical in scope and hellish in severity’ (Kovic, 2021). Human history is riddled with atrocities committed against contemporary members of our own species, and if species-based discrimination is wrong, then history is also riddled with serious harms committed against nonhuman animals. Thus, the risk that we commit atrocities on an interstellar scale is a serious danger—perhaps one of the greatest dangers that exist on the horizon of our species. In this article we present a novel approach to reducing such existential risks and bettering the future. We build on an interdisci-plinary literature including animal ethics, environmental ethics, ethical theory, history, and social psychology. We defend two claims. The first is that to minimise the danger of existential risks, particularly suffering risks, we should prioritise a strategy that we call: Moral circle expansion: a community’s moral circle has expanded with respect to some previous time t if, and only if, a number of entities which used to be given less than full moral consideration at t are now given more moral consideration. Clearly laying out this definition allows us to describe the following familiar phenomenon as a case of moral circle expansion: a community of moral agents concludes that a feature on which they previously based the moral inconsiderability of some individuals is not morally relevant. Thus, they extend moral consideration to all members of the set of entities that possess that feature. This definition is completely agnostic about what attributes—sentience, rationality, being alive, etc.—a community uses to construct its moral circle. In addition, it is not a moralised definition. Thus, according to our definition, in order to identify some shift in a com-munity’s sphere of concern as an instance of moral circle expansion, it is not necessary to judge whether they are deploying a more justified criteria of moral considerability. This has several advantages. First, we do not need to endorse the same criteria in order to agree that a community’s moral circle has expanded. Second, our definition allows us to identify instances of moral circle expansion in a society and retain the ability to criticise them (i.e., one can believe a specific instance of moral circle expansion was a mistake). Note, finally, that this is a definition of, so to speak, gross expansions of the moral circle. It could be modified to accommodate calculations of net expansion. This would consist of something like the number of new entities included with respect to t minus the number of entities excluded with respect to t, given an appropriate specification of ‘minus’ that clarifies the subtractivity of ‘sets’.2 We will argue that explicitly discussing and analysing moral circle expansion is a revealing yet underexplored perspective for understanding the nature of moral progress. While much scholarly work can fit under the umbrella of moral inclusion and exclusion (e. g., prejudice, discrimination), anything more than a passing reference to the moral circle is rare, and we argue there is much to be gained by direct exploration of moral circle expansion itself. Our second claim is that humanity’s moral circle ought to be expanded to include, ideally, all sentient beings. That is, all beings with a capacity for positive and negative experiences.3 In this paper, we will not assume that sentience is necessary for moral con-siderability. However, looking back on historical atrocities, it seems that many could have been prevented or mitigated if communities had extended their moral concern to other groups of sentient beings, such as humans of various races, ethnicities, sexualities, genders, or nationalities—or animals of various species who share this planet with humankind (Judge & Wilson, 2015; Wright, 2018). Thus, even if one is unsure what exactly the future of the moral circle should look like (e.g., the moral patienthood of artificial intelligence), pushing on the current frontiers of the moral circle (e.g., farmed animals) or otherwise engendering expansion towards other kinds of sentient beings is a compelling moral priority not only for utilitarians and others concerned with doing the most good but, more generally, for those concerned with preventing serious wrongs. Artificial intelligence researcher Paul Christiano (2013) argues that, for those trying to do the most good, ‘changing long-term social values’ should not be a priority for several reasons, particularly the changing nature of moral values upon reflection and the potentially zero-sum nature of competing moral values. In addition, there are many other pressing issues demanding our limited attention, such as the reduction of extinction risk (Bostrom, 2003), the mitigation of the effects of anthropogenic climate change, and the alleviation of global poverty.4 Thus, the claim that moral circle expansion should be prioritised, among all of those compelling projects, is in need of some defence. 2 As we mentioned, a society’s expansion of its moral circle usually occurs in terms of sets, rather than individuals. That is, increased moral consideration is usually given to members of the set of individuals who possess a particular attribute. This is consistent with prominent accounts of discrimination in terms of the exclusion of societal groups (see, for example, Lippert-Rasmussen, 2013). We decided to define gross moral circle expansion in terms of individuals, rather than in terms of set, because of the difficulties the latter generates. First, for a given situation, there may be various equally acceptable criteria for carving up sets and there seems to be no value-neutral way to decide which to choose. Employing different criteria, the same societal shift may be described either as an expansion or a contraction. Second, even if there were reasons to prefer some set constructi",
            {
                "entities": [
                    [
                        100,
                        169,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "2202nuJ32]VC.sc[2v91970.7012:viXraA survey on bias in visual datasetsSimone Fabbrizzi∗1,3, Symeon Papadopoulos1, Eirini Ntoutsi2, and IoannisKompatsiaris11CERTH-ITI, Thessaloniki, Greece2Freie Universit¨at, Berlin, Germany3Leibniz Universit¨at, Hannover, GermanyJune 24, 2022Abstract1IntroductionComputer Vision (CV) has achieved remarkableresults, outperforming humans in several tasks.Nonetheless, it may result in significant discrim-ination if not handled properly as CV systemshighly depend on the data they are fed with andcan learn and amplify biases within such data.Thus, the problems of understanding and dis-covering biases are of utmost importance. Yet,there is no comprehensive survey on bias in vi-i) de-sual datasets. Hence, this work aims to:scribe the biases that might manifest in visualdatasets;ii) review the literature on methodsfor bias discovery and quantification in visualdatasets; iii) discuss existing attempts to collectbias-aware visual datasets. A key conclusion ofour study is that the problem of bias discoveryand quantification in visual datasets is still open,and there is room for improvement in terms ofboth methods and the range of biases that canbe addressed. Moreover, there is no such thingas a bias-free dataset, so scientists and practi-tioners must become aware of the biases in theirdatasets and make them explicit. To this end,we propose a checklist to spot different types ofbias during visual dataset collection.∗Corresponding author: simone.fabbrizzi@iti.grThis work is currently under review at Computer Vi-sion and Image Understanding.In the fields of Artificial Intelligence (AI), al-gorithmic fairness, and (big) data ethics, theterm bias usually refers to the case in whichAI-powered decisions show prejudice against in-dividuals or groups of people defined based onprotected attributes like gender or race [Ntoutsiet al., 2020].Instances of this prejudice havecaused discrimination in many fields, includingrecidivism scoring [Angwin et al., 2016], onlineadvertisement [Sweeney, 2013], facial recognition[Cook et al., 2019], and credit scoring [Bartlettet al., 2019].Defining the concepts of bias and fairness inmathematical terms is not a trivial task. Vermaand Rubin [2018] provided a survey on morethan 20 different measures of algorithmic fair-ness, many of which are incompatible with eachother. This incompatibility - the so-called im-possibility theorem [Chouldechova, 2017, Klein-berg et al., 2017] - forces scientists and practi-tioners to choose the measures they use based ontheir personal beliefs or other constraints (e.g.,business models) on what has to be consideredfair for the particular problem/domain.While algorithms may also be responsible forthe amplification of pre-existing biases in thetraining data [Bolukbasi et al., 2016], the qual-ity of the data itself contributes significantly to1   \fthe development of discriminatory AI applica-tions. Ntoutsi et al. [2020] identified two ways inwhich bias is encoded in the data: correlationsand causal influences among the protected at-tributes and other features; and the lack of repre-sentation of protected groups in the data. Theyalso noted that biases manifest in ways that arespecific to the data type.In this work, we focused on how biases canbe encoded in the data (e.g., via spurious cor-relations, causal relationship among the vari-ables, and unrepresentative data samples) and,in particular,images andin visual data (i.e.,videos), which comprises one of the most popu-lar and complex data types. Visual data encap-sulates many features that require human expe-rience and context to interpret. These includethe human subjects, how they are depicted andtheir reciprocal position in the image frame, im-plicit references to culture-specific notions andbackground knowledge, etc. Even the colour-ing scheme can convey different messages. Thus,making sense of visual content remains a verycomplex task, and understanding bias in visualdata is even harder.Computer vision (CV), the primary domainthat enables computers to gain high-level under-standing from visual data, is heavily dominatednowadays by deep learning (DL) methods [Le-Cun et al., 2015, Baraniuk et al., 2020] that al-lowed for outstanding performance in tasks likeobject detection, image classification and imagesegmentation. DL methods, however, rely heav-ily on data, and the results are as good and“fair” as the data used for their training. CV hasrecently drawn attention for its ethical implica-tions when deployed in several settings, rangingfrom targeted advertising to law enforcement.There has been mounting evidence that deploy-ing CV systems without a comprehensive ethi-cal assessment may result in major discrimina-tion against protected groups. For instance, fa-cial recognition technologies [Cook et al., 2019,Robinson et al., 2020], gender classification algo-rithms [Buolamwini and Gebru, 2018], and au-tonomous driving systems [Wilson et al., 2019]have been all shown to exhibit discriminatorybehaviour.While bias in AI systems is a well-studied field,the research in biased CV is more limited despitethe abundance of visual data produced nowa-days and their widespread use in the ML com-munity. Moreover, to the best of our knowledge,there is no comprehensive survey on bias in vi-sual datasets ([Torralba and Efros, 2011] repre-sents a seminal work in the field, but it is limitedto object detection datasets). Hence, the contri-i) to explorebutions of the present work are:and discuss the different types of bias that arisefrom the collection of visual data; ii) to system-atically review the works that aim at address-ing and measuring bias in visual datasets; iii)to discuss some attempts to compile bias-awaredatasets. We believe this work to be a useful toolfor helping scientists and practitioners to bothdevelop new bias-discovery methods and collectdata in ways as less biased as possible. To thelatter end, we propose a checklist that can beused to spot the different types of bias that mightenter the data during the collection process (Ta-ble 6).The structure of the survey is as follows. First,we describe in detail the different types of biasthat might affect visual datasets (Section 2), pro-vide concrete examples of CV applications thatare affected by those biases, and a description ofhow they manifest in the life cycle of visual con-tent. Second, we systematically review the meth-ods for bias discovery in visual content proposedin the literature (Section 3). Third, in Section 4,we discuss the weaknesses and strengths of somebias-aware visual benchmark datasets. Finally,in Section 5, we conclude and outline some pos-sible future research direction.2 Manifestation of Bias inVisual DataIn this section, we describe in detail the typesof bias that pertain to the capture and collectionof visual data (Figure 1), namely: selection bias2\fFigure 1: Examples of selection, framing and label bias. On the right, a list of applications thatcan be affected by each type of bias.3\f(Section 2.1), framing bias (Section 2.2) and la-bel bias (Section 2.3). Furthermore, we describe(Section 2.4) how they manifest within the lifecycle of visual content, from capture to the de-ployment of CV algorithms. Note that a compre-hensive analysis of historical discrimination andalgorithmic bias is beyond the scope of this work.The interested reader can refer to Bandy [2021]for a survey on methods for auditing algorithmicbias both in CV and other AI-related areas.Our categorisation builds on the scheme byTorralba and Efros [2011] who organised thetypes of visual bias into four different categories:selection bias, capture bias (which we collapseinto the more general concept of framing bias),label bias, and negative set bias. The latter ariseswhen the labelling does not reflect entirely thepopulation of the negative class (say non-whitein a binary feature [white people/non-white peo-ple]). We consider negative class bias as an in-stance of selection and label bias.Even though our categorisation appears on thesurface to be similar to the one by Torralba andEfros [2011], their analysis focused on datasetsfor object detection.Instead, we contextualisebias in a more general setting and we also fo-cus on discrimination against protected groups1.Since selection, framing and label bias manifestin many different ways, we also go further by de-scribing a sub-categorisation of these three typesof bias (Table 1) including several biases com-monly encountered in Statistics, Health studies,or Psychology and adapting them to the contextof visual data. While in the following we de-scribe selection, framing, and label bias in gen-eral terms, we also provide references in Table1 for the interested reader who might want todelve further into their different manifestations.2.1 Selection BiasDefinition Selection bias is the type of biasthat “occurs when individuals or groups in a1Note that, while this is the focus of our survey, wealso take into account cases in which bias does not nec-essarily affect people, e.g., in object detection.study differ systematically from the populationof interest leading to a systematic error in anassociation or outcome”2. More generally,itrefers to any “association created as a result ofthe process by which individuals are selectedinto the analysis” ([Hern´an and Robins, 2020,Chapter 8, pg. 99]). In visual datasets, using thefirst definition would be tricky as, for instance,in the case of facial recognition, respecting theethnic composition of the population is generallynot enough to ensure good performance acrossevery subgroup, as we will see in the following.Hence, we adopt a slight modification of Hern´anand Robins [2020] definition:We call selection bias any disparities or as-sociations created as a result of the processby which subjects are included in a visualdataset.Description Torralba and Efros [2011] showedthat certain kinds of imagery are more likely tobe selected during the collection of large-scaleleading to s",
            {
                "entities": [
                    [
                        34,
                        69,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fContents lists available at ScienceDirect Computers in Human Behavior journal homepage: http://www.elsevier.com/locate/comphumbeh Full length article Machine learning techniques and older adults processing of online information and misinformation: A covid 19 study Jyoti Choudrie a,*, Snehasish Banerjee b, Ketan Kotecha c, Rahee Walambe c, Hema Karende c, Juhi Ameta c a University of Hertfordshire, Hertfordshire Business School, DeHavilland Campus, Hatfield. Herts, AL109EU, UK b York Management School, University of York, Freboys Lane, YO10 5GD, UK c Symbiosis Centre for Applied Artificial Intelligence (SCAAI), Symbiosis Institute of Technology, Pune. Symbiosis (Deemed University), Pune, Maharashtra, 412115, India  A R T I C L E I N F O  A B S T R A C T  Keywords: AI Machine learning techniques COVID-19 pandemic Older adult Interview Information-misinformation This study is informed by two research gaps. One, Artificial Intelligence’s (AI’s) Machine Learning (ML) tech-niques have the potential to help separate information and misinformation, but this capability has yet to be empirically verified in the context of COVID-19. Two, while older adults can be particularly susceptible to the virus as well as its online infodemic, their information processing behaviour amid the pandemic has not been understood. Therefore, this study explores and understands how ML techniques (Study 1), and humans, particularly older adults (Study 2), process the online infodemic regarding COVID-19 prevention and cure. Study 1 employed ML techniques to classify information and misinformation. They achieved a classification accuracy of 86.7% with the Decision Tree classifier, and 86.67% with the Convolutional Neural Network model. Study 2 then investigated older adults’ information processing behaviour during the COVID-19 infodemic period using some of the posts from Study 1. Twenty older adults were interviewed. They were found to be more willing to trust traditional media rather than new media. They were often left confused about the veracity of online content related to COVID-19 prevention and cure. Overall, the paper breaks new ground by highlighting how humans’ information processing differs from how algorithms operate. It offers fresh insights into how during a pandemic, older adults—a vulnerable demographic segment—interact with online information and misinformation. On the methodological front, the paper represents an intersection of two very disparate paradigms—ML techniques and interview data analyzed using thematic analysis and concepts drawn from grounded theory to enrich the scholarly understanding of human interaction with cutting-edge technologies.  1. Introduction When the internet was introduced to daily life, it was meant to offer immensely diverse knowledge and information (Ratchford et al., 2001). The internet has however also led to a growth of ignorance in various forms and guises that are labelled using terms such as fake news, disinformation and misinformation. This study specifically uses the term ‘misinformation’. Access to the internet is now, often, access to resources that reinforce biases, ignorance, prejudgments, and absurdity. Parallel to a right to information, some researchers believe that there is a right to ignorance (Froehlich, 2017). Meanwhile, a pandemic, COVID-19, has exposed several difficulties with the present global health care system. A societal concern for healthcare organizations and the World Health Organization (WHO) has been the spread of online misinformation that can exacerbate the impact of the pandemic (Ali, 2020). Almost 90% of Internet users seek online health information as one of the first tasks after experiencing a health concern (Chua & Banerjee, 2017). Therefore, regarding the pandemic COVID-19, where there is little a priori information and knowledge, individuals are likely to explore the online avenue. However, when searching for such information on the internet and social media, one is faced with an avalanche of information referred to as an ‘infodemic’, which includes a mixture of facts and hoaxes that are difficult to separate from one another (WHO, 2020a). If a hoax related to COVID-19 prevention and cure is mistaken as a fact, there could be serious ramifications on people’s health and well-being. Conceivably, * Corresponding author. E-mail address: j.choudrie@herts.ac.uk (J. Choudrie). https://doi.org/10.1016/j.chb.2021.106716 Received 31 July 2020; Received in revised form 10 January 2021; Accepted 22 January 2021  ComputersinHumanBehavior119(2021)106716Availableonline30January20210747-5632/©2021ElsevierLtd.Allrightsreserved.\fJ. Choudrie et al.                                                                                                                healthcare organizations and public health authorities are keen to ensure that people are not deceived by COVID-19-related misinforma-tion that has been circulating online. This is reflected in their propensity to submit misinformation-exposing posts on their social media channels (Raamkumar et al., 2020). Social media, also known as online social networks (OSN), have now emerged as contemporary ways to reach the consumer market. Artificial Intelligence (AI)—traditionally referring to an artificial creation of human-like intelligence that can learn, reason, plan, perceive, or process natural language (Russell & Norvig, 2009)—is associated with social media. It is “an area of computer science that aims to decipher data from the natural world often using cognitive technologies designed to un-derstand and complete tasks that humans have taken on in the past” (Ball, 2018, para. 4). The adoption of AI, a cutting-edge technology, has been propelled to an unprecedented level in the wake of the pandemic. With regards to health, AI-enabled mobile applications are now widely used for infection detection and contact-tracing (Fong et al., 2020). Even with regards to the infodemic, AI’s ML techniques can play a crucial role. Research has shown that ML techniques can help separate information from misinformation (Katsaros et al., 2019; Kinsora et al., 2017; Shu et al., 2017; Tacchini et al., 2017). However, despite the hype and enthusiasm around AI and social media, there is still a lack of under-standing in terms of how consumers interact and engage with these technologies (Ameen et al., 2020; Capatina et al., 2020; Rai, 2020; Wesche & Sonderegger, 2019). The extent to which algorithms can help detect misinformation amid information related to COVID-19 is there-fore worth investigating. Older adults constitute a consumer demographic group that is particularly susceptible to COVID-19 (WHO, 2020b). The pandemic causes pneumonia and symptoms such as fever, cough and shortness of breath among older adults (Adler, 2020), who usually exert maximal pressure on healthcare systems (WHO, 2020b). Moreover, ceteris par-ibus, older adults can also be susceptible to the ‘infodemic’. They are less confident than younger individuals in tackling the challenges that the online setting has to offer (Xie et al., 2021). Hence, older adults are more willing to trust the traditional media rather than what AI feeds them through social media (Media Insight Project, 2018). Still, they often end up becoming a victim of online misinformation (Guess et al., 2019; Seo et al., 2021). To protect this segment of the population from misinformation about COVID-19 prevention and cure, health care organizations would require a systematic understanding of not only the ‘infodemic’, but also of how older adults respond to it. Both are issues on which the literature has shed little light. To fill this gap, the aim of this study is: To explore and understand how AI’s ML techniques (Study 1) and older adults (Study 2) process the infodemic regarding COVID-19 prevention and cure. With this overarching research aim, the objective of this study is two- fold. First, it investigates the extent to which algorithms can distinguish between information and misinformation related to COVID-19 preven-tion and cure (Study 1). For this purpose, a supervised ML framework was developed to classify facts and hoaxes. Second, the study investigates older adults’ information processing behaviour in the face of the COVID-19 infodemic (Study 2). Informed by the results of Study 1 along with the theoretical lenses of misinforma-tion, information processing and trust, 20 older adults were interviewed to understand how they had been coping with the infodemic associated with COVID-19 prevention and cure in their daily lives. This study is important and timely for several reasons. First, The World Health Organization (WHO) declared that besides finding pre-ventions and cures for the pandemic, it was also concerned about the online infodemic. By addressing the infodemic problem from both the computational and behavioral perspectives, the study represents a timely endeavour in the aftermath of the COVID-19 outbreak. Second, the study introduces a machine learning framework to classify infor-mation and misinformation related to COVID-19 prevention and cure. As will be shown later, the classification performance was generally promising. Third",
            {
                "entities": [
                    [
                        927,
                        1041,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "BIROn - Birkbeck Institutional Research OnlineVidgen, Richard and Hindle, G. and Randolph, I. (2019) Exploring the ethicalimplications of business analytics with a business ethics canvas. EuropeanJournal of Operational Research 281 (3), pp. 491-501. ISSN 0377-2217.Downloaded from: hUsage Guidelines:Please refer to usage guidelines at hcontact lib-eprints@bbk.ac.uk.or alternativelyttps://eprints.bbk.ac.uk/id/eprint/27315/ttps://eprints.bbk.ac.uk/policies.html\fExploring the ethical implications of business analytics with a business ethics canvas Richard Vidgen* UNSW Business School, University of New South Wales, Sydney NSW 2052, Australia School of Business, Economics and Informatics, Birkbeck University, London WC1E 7HX, UK Email: r.vidgen@unsw.edu.au Giles Hindle Hull University Business School, University of Hull, Cottingham Road, Hull HU6 7RX, UK Email: giles.hindle@hull.ac.uk Ian Randolph Data Scientist Email: ian.david.randolph@gmail.com *corresponding author 1     \fExploring the ethical implications of business analytics with a business ethics canvas Abstract The ethical aspects of data science and artificial intelligence have become a major issue. Organisations that deploy data scientists and operational researchers (OR) must address the ethical implications of their use of data and algorithms. We review the OR and data science literature on ethics and find that this work is pitched at the level of guiding principles and frameworks and fails to provide a practical and grounded approach that can be used by practitioners as part of the analytics development process. Further, given the advent of the General Data Protection Regulation (GDPR) an ethical dimension is likely to become an increasingly important aspect of analytics development. Drawing on the business analytics methodology (BAM) developed by Hindle and Vidgen (2018) we tackle this challenge through action research with a pseudonymous online travel company, EuroTravel. The method that emerges uses an opportunity canvas and a business ethics canvas to explore value creation and ethical aspects jointly. The business ethics canvas draws on the Markkula Center’s five ethical principles (utility, rights, justice, common good, and virtue) to which explicit consideration of stakeholders is added. A contribution of the paper is to show how an ethical dimension can be embedded in the everyday exploration of analytics development opportunities, as distinct from a stand-alone ethical decision-making tool or as an overlay of a general set of guiding principles. We also propose that value and ethics should not be viewed as separate entities, rather they should be seen as inseparable and intertwined. Keywords: business analytics; data science; business ethics canvas; Markkula; GDPR 2   \fExploring the ethical implications of business analytics with a business ethics canvas 1 INTRODUCTION Business analytics is playing a greater and greater role in our daily lives, impacting on job applications, medical treatment, parole eligibility, and loans and financial services. There are undoubted benefits to algorithmic decision-making in general, and artificial intelligence (AI) in particular. For example, AI is being used to detect the early stages of colorectal cancer, achieving 86% accuracy (Mukherjee, 2017). Such is the interest in AI for healthcare that the UK Government is pledging millions to AI applications for the early diagnosis of cancer and other chronic diseases, using patient data and lifestyle information to highlight patients at risk (Perkins, 2018). However, algorithmic decision-making is not without its dark side. Mann and O’Neill (2017) question the use of algorithms in hiring decisions, d’Alessandro et al. (2017) raise concerns about predictive policing. The Cambridge Analytica case has thrust data analytics squarely into the public domain. It is alleged that Cambridge Analytica collected data from more than 50 million Facebook users (without permission) and used that data to build a system to target US voters with personalized political advertisements with the aim of influencing the US election outcome (Greenfield, 2018). Unsurprisingly, algorithmic decision-making is attracting the interest of researchers as well as practitioner and regulators (e.g., Newell and Mirabelli, 2015; Kitchin, 2017). The potential for harm – intended or unintended - arising from algorithmic decision-making indicates that an ethical dimension is needed. For example, Google, on discovering that its AI software was being used by the US military in its drone development programme, has pledged not to use AI for weaponry (Statt and Vincent, 2018). Google’s CEO, Sundar Pichai, published a list of ethical principles for AI development, which include: be socially beneficial, avoid creating or reinforcing unfair bias, and be accountable to people (Pichai, 2018). These principles are prefigured by the Toronto Declaration, which is calling for governments and companies to ensure that algorithms respect basic principles of equality and non-discrimination (Brandom, 2018). Rights are being further encoded in legislation such as the General Data 3  \fProtection Regulation (GDPR), which requires organizations to protect the rights and privacy of individuals with associated constraints on data usage and a responsibility to provide a right to explanation and to address any presence of discrimination and bias. We consequently argue that an ethical dimension to algorithm development and algorithmic decision-making is an essential aspect of the OR practitioner’s professional profile. Indeed, concern for the ethical aspects of OR goes right back to the early pioneers of the discipline (for example, Churchman 1968, 1970; 1971; Ackoff 1974a, 1974b) who included a concern for stakeholders and wider society within their conceptualization of OR intervention. And a degree of concern for the ethical aspects of OR as a profession is reflected in early guidelines and codes created by OR societies such as the OR Society of America (ORSA) in the USA (Caywood et al., 1971) and the Fellowship for OR in the UK (Fellowship for Operational Research, 1974). However, despite this evidence, we argue an explicit concern for ethics and the ‘goodness’ of OR practice has tended to fall outside of traditional or mainstream discussions on OR and, more latterly, data science. Such discussions have tended to focus on the efficacy of a range of quantitative modelling techniques and on how to achieve operational and process improvement in practice (Koch, 2000). The practice of OR has generally been regarded as a 'good thing' within OR communities due to the largely uncontroversial nature of process improvement within organisations and its scientific credentials and associations. Subsequently, a concern for ethics and ethical practice is not generally covered in a substantive way within OR textbooks or on courses in OR. Thus, while ethics is a long-established and on-going concern for the OR community we argue that much of this work has been at too abstract a level for it to impact meaningfully on the lived day-to-day experience of OR and data science practitioners. For example, in the context of software development, McNamara et al. (2018) found no evidence that the Association for Computing Machinery (ACM) code of ethics influences software-related ethical decision making. Given the development of data science practice and the emerging regulatory environment this abstract approach to ethics is not sufficient – practitioners need practical tools, not just frameworks. Our aim, therefore, is to address the question: how can an ethical dimension be built into the process of business analytics development? We tackle this question using action 4 \fresearch. In the next section we provide the background to ethics in OR and in section three we describe the development of a framework for ethical guidance in data science. The research approach is outlined in section 4. In sections five, six, seven, and eight we present the action research project under the headings of diagnosis and planning, action taking, evaluation, and reflection. The paper concludes with a summary. 2 OR AND ETHICS There has been an ongoing concern for the ethical aspects of OR within the community, which can be traced back to the early pioneers of the discipline (for example, Churchman 1968, 1970; 1971; Ackoff 1974a, 1974b). These researchers envisioned a broader role for OR than simply process engineering and proposed that a concern for stakeholders and wider society should be part of OR’s methodology. However, despite this genuine concern, we argue ethical support for practitioners has tended to fall outside of mainstream discussions on OR practice (Koch, 2000). This is because the practice of OR has generally been regarded as a 'good thing' within the OR community due to the largely uncontroversial nature of process improvement within organizations and OR’s scientific credentials and associations. Ethical issues in OR have been addressed by both professional societies and by academics. 2.1 The professional society perspective The website of the Institute for Operations Research and the Management Sciences (INFORMS) in the USA contains ethical guidelines (INFORMS, 2018) and the Operational Research Society (ORS) in the UK lists ethical principles (OR Society, 2018). The INFORMS guidelines are split into three sections: society, organizations and the OR profession (INFORMS, 2018). The ‘society’ section is concerned with aspiring to openness of assumptions, objectives and sponsors, to objectivity of analysis whilst being respectful of other views and values, and to undertaking work that provides positive benefits, such as progressing scientific understanding, organizational improvement and supporting the social good. The ‘organizations’ section is concerned with aspiring to be accurate, rigorous and realistic in conducting analysis, whilst being alert to the possi",
            {
                "entities": [
                    [
                        463,
                        549,
                        "TITLE"
                    ],
                    [
                        986,
                        1072,
                        "TITLE"
                    ],
                    [
                        2787,
                        2873,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Available online at www.sciencedirect.com Available online at www.sciencedirect.com ScienceDirect ScienceDirect Procedia Computer Science 00 (2022) 000–000 Procedia Computer Science 00 (2022) 000–000 Procedia Computer Science 211 (2022) 36–46www.elsevier.com/locate/procedia www.elsevier.com/locate/procedia 15th International Conference on Current Research Information Systems 15th International Conference on Current Research Information Systems Research Information Systems and Ethics relating to Open Science Research Information Systems and Ethics relating to Open Science Joachim Schöpfela*, Otmane Azeroualb, Pablo de Castroc Joachim Schöpfela*, Otmane Azeroualb, Pablo de Castroc aGERiiCO-Labor, University of Lille, 59650 Villeneuve-d’Ascq, France aGERiiCO-Labor, University of Lille, 59650 Villeneuve-d’Ascq, France bGerman Centre for Higher Education Research and Science Studies (DZHW), Schützenstraße 6a, 10117 Berlin, Germany bGerman Centre for Higher Education Research and Science Studies (DZHW), Schützenstraße 6a, 10117 Berlin, Germany cUniversity of Strathclyde, 101 St James Road, Glasgow G4 0NS, United Kingdom cUniversity of Strathclyde, 101 St James Road, Glasgow G4 0NS, United Kingdom Abstract Abstract Current research information systems (CRIS) evaluate research performance and are intended to contribute to the continuous Current research information systems (CRIS) evaluate research performance and are intended to contribute to the continuous improvement of research. Based on former research on the ethical dimensions of CRIS, our paper presents the results of a survey improvement of research. Based on former research on the ethical dimensions of CRIS, our paper presents the results of a survey with a small sample of representatives of ethics committees from different European countries on ethical aspects of CRIS. Ethics with a small sample of representatives of ethics committees from different European countries on ethical aspects of CRIS. Ethics committees and experts are rarely associated with CRIS-related projects. However, their opinion on ethical indicators and the committees and experts are rarely associated with CRIS-related projects. However, their opinion on ethical indicators and the implementation and use of a CRIS is undoubtedly essential for the future development and management of these systems. Against implementation and use of a CRIS is undoubtedly essential for the future development and management of these systems. Against this background, our purpose is to provide a deeper understanding of the ethical aspects in the field of research information this background, our purpose is to provide a deeper understanding of the ethical aspects in the field of research information management, to show how CRIS represent ethical dimensions of scientific research and to suggest some adjustment of their management, to show how CRIS represent ethical dimensions of scientific research and to suggest some adjustment of their development, implementation and use. development, implementation and use. © 2022 The Authors. Published by Elsevier B.V.© 2022 The Authors. Published by ELSEVIER B.V. © 2022 The Authors. Published by ELSEVIER B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information SystemsInformation Systems Information Systems Keywords: Current research information systems; CRIS; research information management; research ethics; research integrity; research Keywords: Current research information systems; CRIS; research information management; research ethics; research integrity; research infrastructures; ethics committees; infraethics. infrastructures; ethics committees; infraethics. 1. The challenge of ethics 1. The challenge of ethics Research ethics, as a field of applied ethics, provides concepts and recommendations of “right” and “wrong” scientific Research ethics, as a field of applied ethics, provides concepts and recommendations of “right” and “wrong” scientific practice, especially norms of conduct that distinguish between acceptable (responsible) and unacceptable scientific practice, especially norms of conduct that distinguish between acceptable (responsible) and unacceptable scientific behavior. Many different research organizations and associations have adopted specific codes, rules and policies behavior. Many different research organizations and associations have adopted specific codes, rules and policies relating to research ethics, such as the Nuremberg Code of 1947, the 2010 Singapore Statement on Research Integrity relating to research ethics, such as the Nuremberg Code of 1947, the 2010 Singapore Statement on Research Integrity * Corresponding author. E-mail address: joachim.schopfel@univ-lille.fr * Corresponding author. E-mail address: joachim.schopfel@univ-lille.fr 1877-0509 © 2022 The Authors. Published by ELSEVIER B.V. 1877-0509 © 2022 The Authors. Published by ELSEVIER B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information Systems Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information Systems 1877-0509 © 2022 The Authors. Published by Elsevier B.V.This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information Systems10.1016/j.procs.2022.10.17410.1016/j.procs.2022.10.1741877-05092 Joachim Schöpfel et al. / Procedia Computer Science 00 (2022) 000–000 (Resnik & Shamoo, 2011), the European Code of Conduct for Research Integrity (2017) or the recent National Science Foundation’s Manual of “Conflicts of Interest and Standards of Ethical Conduct”. Due to the changing research environment “with new and complex technologies, increased pressure to publish, greater competition in grant applications, increased university-industry collaborative programs, and growth in international collaborations” but also to “highly publicized cases of misconduct” (Armond et al., 2021), academic interest in research ethics and research integrity is steadily increasing, above all in medical and health sciences. Main issues are falsification and fabrication of research data, informed consent, patient safety, plagiarism and conflict of interest. Research ethics “is a matter of debate” (Corvol, 2017). Even when researchers agree that research ethics is important, they do not agree on a common meaning but rather adopt divergent meanings that reflect their priorities, which stem from their personal needs, professional demands, or roles in society. Broadly speaking, research ethics can be defined as “doing good science in a good manner” according to shared and accepted standards of excellence and in compliance with all the steps necessary to meet the rules of responsible research conduct (appropriate data storage, conflict of interest management, protection of human and animal participants, laboratory safety etc.) (DuBois & Antes, 2018). Regarding new technologies and infrastructures, a growing body of research revealed essential and recurrent themes and dimensions of ethics, such as privacy, security, autonomy, justice, human dignity, control of technology and balance of power; some of these issues have been addressed through legal adjustments and new rules and obligations while “for other ethical issues (...) such as discrimination, autonomy, human dignity and unequal balance of power, the supervision is hardly organized” (Royakkers et al., 2018). In the field of big data and artificial intelligence, ethically-aligned technology has been defined as “that which is (a) beneficial to, and respectful of, people and the environment (beneficence); (b) robust and secure (non-maleficence); (c) respectful of human values (autonomy); (d) fair (justice); and (e) explainable, accountable and understandable (explicability)” (Morley et al., 2020). Regarding artificial intelligence, Morley et al. (2021) observed “that a significant gap exists between the theory of AI ethics principles and the practical design of AI systems”. Does the same observation apply to current research information systems (CRIS)? Do we need new tools and methods designed to help CRIS developers, engineers, and designers translate ethical principles into practice? In spite of the growing body of research on ethics in the field of technology, big data, artificial intelligence and so on, so far there are but few papers on ethics in the field of research information systems. For CRIS, ethics is a double challenge: to contribute to the development of responsible research in the context of open science, and to be able to measure the practices and performances recommended by these rules of responsible conduct. In other words, CRIS must respect the regulatory framework and the good practices, principles and values of scientific communities (Diener & Crandall, 1978; Guillemin & Gillam, 2004). But, at the same time, and this is indeed the particularity of these systems,",
            {
                "entities": [
                    [
                        448,
                        512,
                        "TITLE"
                    ],
                    [
                        513,
                        577,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fEuropean Journal of Operational Research 291 (2021) 906–917 Contents lists available at ScienceDirect European Journal of Operational Research journal homepage: www.elsevier.com/locate/ejor The responsibility of social media in times of societal and political manipulation Ulrike Reisach Department of Information Management, Prof. Dr. Ulrike Reisach, Neu-Ulm University of Applied Sciences, Wiley-Street 1, D-89231 Neu-Ulm, Germany a r t i c l e i n f o a b s t r a c t Article history: Received 17 December 2019 Accepted 16 September 2020 Available online 22 September 2020 Keywords: Ethics in OR Decision-making Artificial intelligence Behavioural OR Education The way electorates were influenced to vote for the Brexit referendum, and in presidential elections both in Brazil and the USA, has accelerated a debate about whether and how machine learning techniques can influence citizens’ decisions. The access to balanced information is endangered if digital political ma- nipulation can influence voters. The techniques of profiling and targeting on social media platforms can be used for advertising as well as for propaganda: Through tracking of a person’s online behaviour, al- gorithms of social media platforms can create profiles of users. These can be used for the provision of recommendations or pieces of information to specific target groups. As a result, propaganda and dis- information can influence the opinions and (election) decisions of voters much more powerfully than previously. In order to counter disinformation and societal polarization, the paper proposes a responsibility-based approach for social media platforms in diverse political contexts. Based on the implementation require- ments of the “Ethics Guidelines for Trustworthy Artificial Intelligence” of the European Commission, the eth- ical principles will be operationalized, as far as they are directly relevant for the safeguarding of demo- cratic societies. The resulting suggestions show how the social media platform providers can minimize risks for societies through responsible action in the fields of human rights, education and transparency of algorithmic decisions. © 2020 The Author. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) 1. Introduction and research methodology 1.1. Aims and research question During the Corona-crisis in 2020, the degree of disinformation has reached a level which could endanger the proper functioning of democratic decision-making. Crises have always been a time of rising emotions and anxiety. These seem to culminate on social media platforms, where citizens and self-proclaimed experts give unsubstantiated advice dealing with Covid-19, or try to identify as- sumingly guilty parties and fabricate conspiracy theories, through amalgamating facts and false interpretations. The more exciting, the even more weird ideas that are shared, including vaccine anx- iety, and doubtful, or even potentially lethal health recipes. In the United States, these developments have fuelled the long- standing debate on whether misrepresentations and recommen- dations still fall under the freedom of speech, or should be ac- companied, e.g. with a fact check advice, or should be filtered out and deleted. The European Commission (EC) aims at combat- ing disinformation and appeals to the social media platforms to install a transparent and consistent moderation of disinformation ( EC, 2020a , 2020b ). Bell’s observation ( 2018 ) that “… techniques for fabricating, editing, and reframing news in harmful ways develop faster than they can be detected and countered …” describes the current situ- ation (p. 5). C. West Churchman asks “which end results are good in an objective sense?” ( Churchman, 1970 ). This question is ap- plies to the current issues of social media platforms: The paper asks whether and how social media platforms can (practically) and should (ethically), deal with risks of societal and political manipu- lation. The EC’s Action Plan on Disinformation ( 2019 ) defines disinfor- mation as is verifiably false or misleading information created, pre- sented and disseminated for economic gain, or to intentionally de- ceive the public. The reasons given for their action are: (a) the potential for far-reaching consequences such as public harm, (b) threats to democratic political and policy-making processes, (c) the risk of endangering the protection of EU citizens’ health, E-mail address: ulrike.reisach@hnu.de security and their environment. https://doi.org/10.1016/j.ejor.2020.09.020 0377-2217/© 2020 The Author. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) \fU. Reisach European Journal of Operational Research 291 (2021) 906–917 This research focuses on the threats to democratic decision- making processes. Based on the developments in 2019 and 2020, the EC (2020a) expresses concerns which relate to the main rea- sons for the research: “Disinformation erodes trust in institutions and in digital and traditional media and harms our democracies by hampering the ability of citizens to take informed decisions. It can polarise debates, create or deepen tensions in society and un- dermine electoral systems, and have a wider impact on European security. It impairs freedom of opinion and expression, a funda- mental right enshrined in the Charter of Fundamental Rights of the European Union.” This paper analyses how disinformation endan- gers democratic decision-making and how social media platforms could contribute to tackling those challenges. 1.2. Research methodology Reflecting on the reasons and impacts of disinformation on so- cial media, a hermeneutic process of understanding and interpret- ing the ethical and societal consequences has been chosen. What can be expected is a set of ethically grounded suggestions for ac- tions which could be discussed and implemented by the platform owners. Hermeneutics confronts “quantitative methodologies with qualitative questions” ( van Dijck, 2014 : p. 206). The tools for this process are rooted in the humanities and in social science. Critical reflectivity ( Gregory, 20 0 0 ) is one such method that could be ap- plied to a topic that raises societal and political questions. The re- sults cannot resolve the existing problems of political manipulation completely, due to societal complexity ( DeTombe, 2002 ), associated with almost ubiquitous social media. The philosopher Wilhelm Dilthey (1833–1911) established hermeneutics in the humanities as interpretative sciences in their own right. He studied the rela- tionships between personal experience, its realization in creative expression, and the reflective understanding of this experience; and, finally, the logical development from these to the understand- ing of social groups and historical processes ( Dilthey, 2013 ). Using the concept of hermeneutics and reflective understand- ing, the operation and impacts of social media with regard to the current societal trends are discussed in Part 2. Examples for ma- nipulation are given in Part 3. Concepts of digital media ethics and responsibility are presented in Part 4, and compared with the tra- ditional media’s accountability approaches. Ethics codes for AI are shown and suggestions for responsible action of social media are provided. Based on those, a reflective and strategic corporate re- sponsibility of digital media is introduced as a new concept. Part 5 gives some reflections and limitations. Finally, the conclusion in part 6 offers a global outlook on societal responsibilities. 2. The role of social media platforms and their impacts on societies To explain why societal and political manipulation is a spe- cial issue for social media, the goals, functioning and legal sta- tus of the respective corporate actors need to be clarified. Social media platforms such as Facebook, Twitter, Instagram, YouTube and TikTok facilitate an interactive one-to-few or many-to-many- communication in an international scale. “In the web 2.0 era, an infinite “crowd” of users can anonymously and with almost no cost “voice” criticism and protest, via Twitter and Facebook” ( Fengler, 2012 , p. 184). 2.1. The business model of social media platforms In their official prospectus for the computerized US stock mar- ket NASDAQ, Facebook claimed their vision was “t o make the world more open and connected ” ( Facebook, 2012 ). Nevertheless, their business model is commercial. When going public they disclaim that their goal is profit and shareholder value: “Advertisers can en- gage with users … on Facebook or subsets of our users based on in- formation they have chosen to share with us such as their age, lo- cation, gender, or interests. We offer advertisers a unique combina- tion of reach, relevance, social context, and engagement to enhance the value of their ads.” ( Facebook, 2012 ). Users do not pay but give their data to Facebook who then sells them to their customers, the advertisers. Dwyer and Martin (2017) explain that data cap- ture, data-mining and behavioural advertising are typical activi- t",
            {
                "entities": [
                    [
                        967,
                        1049,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Journal of Pragmatics 34 (2002) 227-258 www.elsevier.com/locate/pragma Pragmatics in human-computer conversations* Ayse Pinar Saygin”, Ilyas Ciceklib,* * Dept. of Cognitive Science, Univ. of California, San Diego, La Jolla, CA 92093-0515, USA h Dept. of Computer Engineering, Bilkent University, 06533 Bilkent, Ankara, Turkey Received 2 December 1999; revised version 5 May 2001 Abstract in which computers participate This paper provides a pragmatic analysis of some human-computer conversations carried out during the past six years within the context of the Loebner Prize Contest, an annual com- in Turing Tests. The Turing Test posits that to be petition granted intelligence, a computer should imitate human conversational behavior so well as to be indistinguishable from a real human being. We carried out an empirical study exploring the relationship between computers’ violations of Grice’s cooperative principle and conver- sational maxims, and their success in imitating human language use. Based on conversation analysis and a large survey, we found that different maxims have different effects when vio- lated, but more often than not, when computers violate the maxims, they reveal their identity. is at work during conversations with The results indicate that Grice’s cooperative principle computers. On the other hand, studying human-computer communication may require some in pragmatics because of certain characteristics of these modifications of existing frameworks conversational environments. Pragmatics constitutes a serious challenge to computational lin- guistics. While existing programs have other significant it may be that the biggest hurdle in developing computer programs which can successfully carry out conversa- tions will be modeling the ability to ‘cooperate’. 0 2002 Elsevier Science B.V. All rights reserved. shortcomings, Keywords: tional linguistics; Maximization principle; Natural language processing; Pragmatics Cooperative principle; Turing test; Human-computer conversation; Computa- * This work was carried out while the first author was an MS. student at Bilkent University. We would like to thank Stephen Wilson, Bilge Say, and David Davenport for reading and commenting on earlier versions of this work. We are also indebted to Hulya Saygin, Giray Uraz, and Emel Aydin for their help in conducting the surveys. * E-mail: saygin@crl.ucsd.edu; ilyas@cs.bilkent.edu.tr 0378-2166/02/$ - see front matter 0 2002 Elsevier Science B.V. All rights reserved. PII: SO378-2166(01)00035-2 \f228 A.P. Saygin, I. Cicekli I Journal of Pragmatics 34 (2002) 227-258 1. Introduction The Imitation Game (IG), better known as the Turing Test (IT), was introduced intel- in 1950 by Alan Turing as a means to detect whether a computer possesses ligence. Turing believed that a way to objectively assess machine mentality was needed, for he thought the question ‘Can machines think?’ was too ambiguous. He attempted this question into a more concrete form: the IG is played with a man to transform (C) whose gender is unimportant. The inter- (A), a woman (B), and an interrogator rogator stays in a room apart from A and B. The objective of the interrogator is to determine which of the other two is the woman while the objective of both the man that he/she is the woman and the other and the woman is to convince the interrogator is not. The players communicate thus in written nat- topics can be on any subject imaginable, from mathe- ural language. Conversation matics to poetry, from the weather to chess. through a teletype connection, According to Turing, the new question to be discussed, instead of the equivocal ‘Can machines think? ‘, can be ‘What will happen when a machine takes the part of A in this game? Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?’ ‘Can in the paper, however, Turing At a later point the question replaces machines think? ’ by the following: “Let us fix our attention to one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?” (Turing, 1950: 442, emphasis added). Notice that the woman has disappeared altogether. But the objectives of A, B and the interrogator state any change. In this version, a man and a computer program are playing the game and try- ing to convince the judge that they are women. at least Turing does not explicitly remain unaltered; As it is now generally understood, what the ‘IT tries to assess is the machine’s ability to imitate a human being, rather than its ability to simulate a woman. Most subsequent work on the TT ignores the gender issue and assumes that the game is played between a machine (A), a human (B) and an interrogator (C). In this version, C’s aim is to determine which one of the two entities he/she is conversing with is the human. Although it is not clear why Turing introduces the gender-based IG, given that he was interested in whether or not machines can think, we have argued elsewhere that Turing’s original game constitutes a controlled experimental design (Saygin, 1999; Saygin et al., 2000). It provides a fair basis for comparison: the woman (either as a participant in the game or as a concept) acts as a neutral point so that the two play- ers can be assessed in how well they imitate something which they are not. Philoso- IG (see Piccinini, 2000; Sterrett, 2000; phers also commented on the gender-based Traiger, 2000, for a recent discussion). We will return to a discussion of the original gender-based game later. Unless noted otherwise, when talking about the TT, we \fA.P. Saygin, I. Cicekli I Journal of Pragmatics 34 (2002) 227-258 229 will be referring (human vs. machine), not gender. to the game in which the decision to be made is one of ‘species’ Much has been written on the TT, with many authors discussing its implications for artificial intelligence (AI). Most works attack or defend the validity of the test as to machines. There are many computational analyses, a means to grant intelligence an abundance of philosophical comments, and occasional remarks from other disci- plines such as psychology and sociology. A detailed survey of the ‘IT can be found in Saygin et al. (2000). The TT has never been carried out exactly as Turing described it. However, there are variants of the original IT in which computer programs participate and show their skills in ‘humanness’. Since 1991, Hugh Loebner has been organizing the so- called annual Loebner Prize Competition. Participating computer programs try to convince judges that they are human. One or more human confederates also partici- pate and try to aid the judges in identifying the humans. The judges also rank the participants with respect to their ‘human-ness’. Although no program has passed the ‘IT so far, the quality of participating programs seems to be increasing every year. The year 2000 marked the fiftieth year of the IT. While many conversation sys- tems, or ‘chatterbots’, have been developed, none exhibit human-like conversational behavior to the extent that they can pass the TT. We believe it is time we analyze some recent programs within the context of pragmatics and see how, at the turn of the millennium, computers are doing as conversational partners. We will not be con- is intelligent or with cerned with whether passing the test implies related theoretical issues. Here, we take official, real, human-computer conversations and use them in a study in which subjects were asked to read and make pragmatic judgments about them. We then analyze the results both in terms of human behavior in conversations with computers and in terms of better program design. the machine We focus on one particular aspect of conversation and attempt to explore it in relation to the ‘IT. This aspect is Grice’s cooperative principle (CP) and conversa- tional maxims. Just as Turing’s TT is a milestone in AI, Grice’s theory has been very in the field of pragmatics. The powerful juxtaposition of these two con- influential cepts is thus a significant component of this study. Pragmatics, in a nutshell, is con- cerned with language in use. The TT stipulates a criterion on machine intelligence based on the way computers use language. What could be more natural than the bringing together of these two concepts in analyzing human-computer communica- tion in natural language? We believe a pragmatic approach to the TT reveals a lot of important issues that are easy to miss otherwise. Through a pragmatic analysis, we can gain valuable insights on what it means to have a human-like conversation and In this what principles, implicitly or explicitly, guide human-computer conversation. paper, we study how humans behave in relation to the CP and the conversational maxims: we analyze human-computer the relation- ships between performance conversations and we quantify in ‘ITS and judgments of maxim violations. In this paper, IT transcripts are studied as exemplars of human-computer conversa- tion. We used a selected set of conversation excerpts from Loebner contest transcripts in a pair of questionnaires. Subjects were asked to read the excerpts and to make judgments on the computers’ language use, as well as to rate their IT-performance. \f230 A.P. Saygin, I. Cicekli I Journal of Pragmatics 34 (2002) 227-258 We sought correlations between computers’ maxim violations and their performance in TTs and found some reliable relationships. Violations of maxims often cause computers to give away their identity, therefore Grice’s framework seems to be at play during conversations with computers. On the other hand, we also observe some trends that would not be straightforwardly expected based on Grice’s theory, some- thing which indicates",
            {
                "entities": [
                    [
                        80,
                        122,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "UvA-DARE (Digital Academic Repository)In defense of offense: information security research under the right to sciencevan Daalen, O.DOI10.1016/j.clsr.2022.105706Publication date2022Document VersionFinal published versionPublished inComputer Law and Security ReviewLicenseArticle 25fa Dutch Copyright Act Article 25fa Dutch Copyright Act(https://www.openaccess.nl/en/in-the-netherlands/you-share-we-take-care)Link to publicationCitation for published version (APA):van Daalen, O. (2022). In defense of offense: information security research under the right toscience. Computer Law and Security Review, 46, [105706].https://doi.org/10.1016/j.clsr.2022.105706General rightsIt is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s)and/or copyright holder(s), other than for strictly personal, individual use, unless the work is under an opencontent license (like Creative Commons).Disclaimer/Complaints regulationsIf you believe that digital publication of certain material infringes any of your rights or (privacy) interests, pleaselet the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the materialinaccessible and/or remove it from the website. Please Ask the Library: https://uba.uva.nl/en/contact, or a letterto: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. Youwill be contacted as soon as possible.Download date:04 jul. 2023UvA-DARE is a service provided by the library of the University of Amsterdam (https://dare.uva.nl)\fcomputer law & security review 46 (2022) 105706 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR In defense of offense: information security research under the right to science ✩ ∗Ot van Daalen University of Amsterdam a r t i c l e i n f o a b s t r a c t Keywords: Information security Coordinated vulnerability disclosure Right to science Communications freedom Duty to disclose Vulnerabilities Information security research Information security is something you do , not something you have . It’s a recurring process of finding weaknesses and fixing them, only for the next weakness to be discovered, and fixed, and so on. Yet, European Union rules in this field are not built around this cycle of making and breaking: doing offensive information security research is not always legal, and doubts about its legality can have a chilling effect. At the same time, the results of such research are sometimes not used to allow others to take defensive measures, but instead are used to attack. In this article, I review whether states have an obligation under the right to science and the right to communications freedom to develop governance which addresses these two issues. I first discuss the characteristics of this cycle of making and breaking. I then discuss the rules in the European Union with regard to this cycle. Then I discuss how the right to science and the right to communications freedom under the European Convention for Human Rights , the EU Charter of Fundamental Rights and the International Covenant on Economic, Social and Cultural Rights apply to this domain. I then conclude that states must recognise a right to research information security vulnerabilities, but that this right comes with a duty of researchers to disclose their findings in a way which strengthens information security. © 2022 Ot van Daalen. Published by Elsevier Ltd. All rights reserved. 1. Introduction Information security is something you do , not something you have . It’s a recurring process of finding weaknesses and fix- ing them, only for the next weakness to be discovered, and fixed, and so on. Yet, European Union rules in this field are not built around this cycle of making and breaking: doing of- fensive information security research is not always legal, and doubts about its legality can have a chilling effect. At the same time, the results of such research are sometimes not used to allow others to take defensive measures, but instead are used to attack. In this article, I review whether states have an obli- gation under the right to science and the right to communica- tions freedom to develop governance which addresses these two issues. I first discuss the characteristics of this cycle of making and breaking. I then discuss the rules in the Euro- pean Union with regard to this cycle. Then I discuss how the right to science and the right to communications freedom un- der the European Convention for Human Rights (the Conven- tion), the EU Charter of Fundamental Rights (the Charter) and the International Covenant on Economic, Social and Cultural Rights (the Covenant) apply to this domain. I then conclude that states must recognise a right to research information se- ✩ This work was supported by the Netherlands Organisation for Scientific Research (NWO/OCW), as part of the Quantum Software Con- sortium programme (project number 024.003.037/3368). It is based on a forthcoming PhD on information security, encryption, quantum computing and human rights by the author. The author would like to thank his PhD supervisors, Joris van Hoboken and Mireille van Eechoud, for their comments on earlier drafts. ∗ Corresponding author: Mr Ot van Daalen, Institute for Information Law, Netherlands E-mail address: o.l.vandaalen@uva.nl https://doi.org/10.1016/j.clsr.2022.105706 0267-3649/© 2022 Ot van Daalen. Published by Elsevier Ltd. All rights reserved. \f2 computer law & security review 46 (2022) 105706 curity vulnerabilities, but that this right comes with a duty of researchers to disclose their findings in a way which strength- ens information security. Information security as a cycle of making 2. and breaking In the literature, information security is often framed in terms of desirable security properties, such as confidentiality, in- tegrity and availability.2 And information security measures are intended to safeguard these properties against attacks. Many organisations will, at some point in their development, take these kind of information security measures. But it can be difficult to determine which measures make the most sense. Over the past decades, standard practices have emerged to help organisations make the best choices, even in the face of changing circumstances. These are generally subsumed un- der the plan-do-check-act cycle.3 In the first phase of the cycle, the plan -phase, an organisa- tion will decide which measures are necessary in view of the risks. These decisions are usually laid out in an information security policy. In the second phase, the do -phase, the organi- sation then implements these measures. That does not mean, of course, that these measures are always sufficient. That’s why information security policies need to be tested and re- viewed periodically – the check and act phases of the cycle. You periodically check whether the measures are commensurate with the risks, then adjust as needed. This approach reflects the reality of the continuous cycle of making and breaking. An important part of this cycle centres around the “vul- nerability” or weakness, in software or hardware. An attacker can exploit such a vulnerability to make a system act in a way which it is not supposed to do, or to be more precise: to violate a security policy . And while you might in theory be able to make software and hardware without vulnerabilities, in practice it’s virtually impossible. It is not doable to independently verify all components of a system.4 And even such verification only provides limited assurance that a component can actually be trusted.5 2 3 4 5 See Axel M. Arnbak, Securing Private Communications: Protecting Private Communications Security in EU Law: Fundamental Rights, Func- tional Value Chains, and Market Incentives (Kluwer Law International 2016) ch 5 for an in depth discussion of these concepts; and A. J. Menezes, Paul C. Van Oorschot and Scott A. Vanstone, Handbook of Applied Cryptography (CRC Press 1997) 4 for the definition of the first two. See for example Regulation (EU) 2016/679 of the European Par- liament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (2016 OJ L 119/1), Art. 32(1)(d). Edlyn V. Levine, “The Die Is Cast: Hardware Security Is Not As- sured” (2020) 18 ACMqueue. See Ken Thompson, “Reflections on Trusting Trust” (August) 1984 Communications of the ACM 761 for a principled argument; and Georg T. Becker and others, “Stealthy Dopant-Level Hardware Trojans: Extended Version” (2014) 4 Journal of Cryptographic Engi- neering 19 for a practical example. 2.1. Research and discovery As a result, many vulnerabilities are found in most widely used products after they are shipped, and even if they’re shipped without vulnerabilities, the deployment by users might create new weaknesses. One particular type of vulnera- bility is called the “zero day vulnerability”, or simply the “zero day”. It’s a weak spot for which no fix has been created yet, usually because the vendor doesn’t know of its existence. Of all vulnerabilities, zero days are the most coveted by attack- ers, because by definition there is not yet a direct defence for them. And that’s why zero days can also wreak the most havoc. After inception, these vulnerabilities will often lie dormant, undiscovered for months, if not years. But researchers are con- tinuously on the lookout for bugs, and they generally have the upper hand. They hunt by disassembling hardware, trawling through lines of code and remotely testing online services. This used to be done manually, and still often is. However, many tools and services have come available which enable au- tomatic testing.6 And since the information security of a sys- tem is as strong as its weakest link, attackers generally only need one vulnera",
            {
                "entities": [
                    [
                        38,
                        117,
                        "TITLE"
                    ],
                    [
                        1722,
                        1801,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Debating Darwin at the CapeLivingstone, D. (2016). Debating Darwin at the Cape. Journal of Historical Geography, 52, 1-15.https://doi.org/10.1016/j.jhg.2015.12.002Published in:Journal of Historical GeographyDocument Version:Peer reviewed versionQueen's University Belfast - Research Portal:Link to publication record in Queen's University Belfast Research PortalPublisher rights© Elsevier 2016.This is an open access article published under a Creative Commons Attribution-NonCommercial-NoDerivs License(https://creativecommons.org/licenses/by-nc-nd/4.0/), which permits distribution and reproduction for non-commercial purposes, provided theauthor and source are cited.General rightsCopyright for the publications made accessible via the Queen's University Belfast Research Portal is retained by the author(s) and / or othercopyright owners and it is a condition of accessing these publications that users recognise and abide by the legal requirements associatedwith these rights.Take down policyThe Research Portal is Queen's institutional repository that provides access to Queen's research output. Every effort has been made toensure that content in the Research Portal does not infringe any person's rights, or applicable UK laws. If you discover content in theResearch Portal that you believe breaches copyright or violates any law, please contact openaccess@qub.ac.uk.Open AccessThis research has been made openly available by Queen's academics and its Open Research team. We would love to hear how access tothis research benefits you. – Share your feedback with us: http://go.qub.ac.uk/oa-feedbackDownload date:04. jul. 2023\fDebating Darwin at the Cape On 31st May 1836, the Royal Navy’s surveying barque, HMS Beagle, dropped anchor at Simon’s Bay near Cape Town. On deck was the young Charles Darwin who, nearly four and a half years earlier, had stepped aboard the vessel as a budding geologist and table companion to Captain Robert Fitzroy who had been assigned the task of charting the coastline of South America and determining meridian distances in the southern hemisphere. The Royal Observatory outside Cape Town was a crucial port of call, and with Sir John Herschel, Britain’s highly distinguished astronomer, currently residing in Cape Colony on a four-year project to catalogue the stars, clusters and nebulae of the southern skies, the Beagle’s crew found themselves in the Cape for eighteen days – a longer stay than anywhere else on the whole voyage save for the Galápagos Islands. For all that, Darwin was remarkably silent about the Cape.1 For the fact of the matter is that Darwin did not take to the colony much at all. In his diary entry for 4th June he confessed “I saw so very little worth seeing, that I have scarcely anything to say”. The landscape he found “bleak and desolate”, its aspect “cheerless” and the Ruggensveld region devoid of interest.2 His private notes on Paarl Rock never saw the public light of day and his reflections on the Sea Point granite-slate contacts were reduced to the briefest of remarks in his 1844 Geological Observations on Volcanic Islands.3 But if Darwin more or less entirely ignored the Cape in his writings – though he did remain in touch with a number of correspondents there – the same cannot be said of the Cape’s reaction to his theories. For during the late 1860s and 1870s, when controversy surrounding the theory of evolution by natural selection was bursting into full flame, the Cape Monthly Magazine in particular carried a spate of articles subjecting Darwinism to sustained scrutiny. The Cape Monthly had come into being in 1857 under the editorship of Roderick Noble who taught at the South African College, and was designed to advance the virtues of intellectual enlightenment, social progress, and the spread of civilization in the Cape.4 As Saul Dubow remarks, the “Monthly combined the seriousness of purpose characteristic of the highbrow British quarterlies … and lay at the center of an 1 Wilhelm S. Barnard, “Darwin at the Cape”, South African Journal of Science 100 (2004): 243-48. 2 See entries for June 1836, in Charles Darwin’s Beagle Diary ed. Richard Darwin Keynes (Cambridge: Cambridge University Press, 1988). See also Barnard, “Darwin at the Cape”, p. 245. 3 On Darwin’s account of the Sea Point contacts see Sharad Master, “Darwin as a Geologist in Africa – Dispelling the Myths and Unravelling a Confused Knot”, South African Journal of Science 108 No 9/10 (2012): 1-5. 4 See the discussion in Saul Dubow, A Commonwealth of Knowledge: Science, Sensibility, and White South Africa 1820-2000 (Oxford: Oxford University Press, 2006), chapter 2. 1                           \finterlocking network of associated colonial institutions and societies such as the South African Library, Museum, the Art Gallery and the University of the Cape of Good Hope”.5 Aspiring to involve itself in the global scientific conversation, its editors kept their eyes “firmly fixed on developments in the imperial centres of London and Edinburgh”.6 Thus while much original work on the local geography and anthropology of the Cape itself graced the Monthly’s pages, its tone was, by and large, that of a liberal intelligentsia seeking a place at the international scientific table during a time when the colony was absorbed with railway construction, diamond mining, and the establishment of ‘Responsible Government’ with the appointment of its own Prime Minister in 1872. As elsewhere, the Darwinian debates in the Cape really only surfaced during the late 1860s and 1870s, and progressively intensified as the new decade wore on owing, in large measure, to the appearance of the Descent of Man in 1871 which directly applied the theory of evolution by natural selection to the human race, and to the furore surrounding John Tyndall’s infamous presidential address to the 1874 meeting of the British Association for the Advancement of Science in Belfast. Taken in the round, exchanges over Darwin’s proposals were conducted with notable civility, certainly compared with other venues, though worries over materialism were increasingly voiced in the aftermath of Tyndall’s incursion. There were, too, novel mobilisations of Darwinism for purposes of immediate cultural relevance to the colony – especially in the fields of legislation and linguistics – which had significant racial resonances. Charting something of these engagements in the cultural space marked out by the English-speaking network that congregated around the Cape Monthly Magazine, the African Library and the like, is my ambition in what follows.7 This inquiry is intended to further contribute to the growing literature on the geographies of scientific knowledge in general, and the historical geography of Darwinism more particularly, by tracing in some detail the ways in which Darwin’s theory was talked about and acted upon in the Cape during the decades around 1900. By examining the practices of science and the responses of the Cape’s intellectual elite to the latest theoretical proposals, it is intended to make a contribution to understanding something of the nature of scientific culture in a colonial setting. At the same time, by inspecting the diverse range of spheres into which evolutionary thinking was drawn – 5 Saul Dubow, “Earth History, Natural History, and Prehistory at the Cape, 1860-1785”, Comparative Studies in Society and History 46 (2004): 107-133, on p. 109. 6 Dubow, Commonwealth of Knowledge, p. 71. 7 How Afrikaner culture engaged with Darwin’s proposals in this period, so far as I am aware, remains to be explored. 2                          \fphilology, natural history, anthropology, religion, philosophy, geology, law – it demonstrates just how wide-ranging the Darwinian debates were in the Colony’s public square. What also emerges from this analysis is the complex geography of exchange between Europe and the Cape with the circulation of people, print and opinion across the imperial domain rendering local scientific cultures a compound product of both ‘here’ and ‘there’. Early Encounters Initial reactions to Darwin at the Cape were articulated in a setting already favourably disposed to scientific inquiry. The Scottish-born physical scientist, Roderick Noble, Professor at the South African College, public lecturer and editor of the Cape Monthly,8 for example, had expressed his views on the science of geology in a lecture delivered to the Mechanics Institute in 1854. Noble was deeply religious – he had studied for the ministry in Edinburgh – and was well acquainted with the tradition of Scottish Common Sense philosophy, lecturing on such figures as Dugald Stewart and Thomas Reid. Such predilections favourably disposed him to the scientific enterprise and his lecture Geology: Its Relation to Scripture was sculpted in dialogue with his theological heritage. Here no trace of literalist scriptural geology surfaced.9 Instead, calling on the authority of such figures as Thomas Chalmers, John Pye Smith, Hugh Miller, and Edward Hitchcock, not to mention Cardinal Wiseman and Archbishop Whately, he argued that they had developed a variety of hermeneutic schemes – basically harmonising strategies – showing how a lengthy earth-history was entirely compatible with enlightened readings of the Genesis narrative. Geology’s compatibility with popular religious sentiment was a different matter; but Noble assured his audience that “no such antagonism or irreconcilableness does in reality hold”.10 Later in 1868, in another public lecture, this time to the South African Public Library, an institution renowned for its rich manuscript resources, he insisted there was no inevitable conflict between Darwinian evolution and Divine 8 See William Beinart, The Rise of Conservation in South Africa: Settlers, Livestock and the Environment 1770-1950 (Oxford: Oxford University Press, 2008); W.J. De Kock (ed.) Dictionary of South African Biography (Cape Town: Tafelberg, 1968-1981) Vol. II, p. 518-519. 9 On scr",
            {
                "entities": [
                    [
                        0,
                        27,
                        "TITLE"
                    ],
                    [
                        51,
                        78,
                        "TITLE"
                    ],
                    [
                        1632,
                        1659,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Migration to the metaverse and its predictors: attachment to virtual places and metaverse-related threat Tomasz Oleksy1* Anna Wnuk1 Małgorzata Piskorska1 1 Faculty of Psychology, University of Warsaw, ul. Stawki 5/7, 00-183 Warsaw, Poland * Corresponding author E-mail addresses: tomasz.oleksy@psych.uw.edu.pl (T. Oleksy), anna.wnuk@psych.uw.edu.pl (A. Wnuk), mm.piskorska@student.uw.edu.pl (M. Piskorska)       \fAbstract The most ambitious visions of metaverse technology promise to create virtual places that offer the same possibilities as the real world. Implementation of these plans can become the next milestone on the road to the ongoing exodus from real places to digital ones. However, as any novel technology, the metaverse raises controversies and questions. Does one want to migrate to the metaverse? Does one’s willingness to move to virtual worlds depend on the bonds with existing virtual places and the sense of threat related to this technology? To address these questions, we drew on the theories of place attachment and intergroup threat. In two studies – (1) among users of open-world games (N = 366) and (2) using a sample representative of the Polish population in terms of age, gender and size of the residential place (N = 995) – we observed a low level of willingness to migrate to the metaverse. The participants displayed a high level of perceived metaverse-related threat, ranging from privacy concerns to the belief that metaverse can deprive one of access to essential human experiences. However, greater attachment to virtual, as opposed to real, places was associated with both an increased willingness to migrate to the metaverse and a low level of perceived threat. The results provide a better understanding of individuals’ perception of the metaverse and of how the bonds with virtual and real places translate into attitudes towards metaverse technology.    \fIntroduction In recent years, virtual places have played an increasingly important role in human life (Barreda-Ángeles & Hartmann, 2022). Even before the COVID-19 pandemic broke out, people were spending increasingly more time in virtual worlds (Rzeszewski & Evans, 2020; Coulson et al., 2019). The metaverse is one of the technologies of the future that is generating a particular interest in the context of the potential further exodus of people from real places to digital ones. Referred to as the ‘new internet’, the metaverse is designed to become a virtual universe where people can receive entertainment, socialize or even work (Bojic, 2022; Oh et al., 2023). The feature that distinguishes the metaverse from its predecessor, the internet, the most is its capacity to be explored in a manner analogous to how we interact with real places, which is to be achieved primarily through further development of both virtual and augmented reality technologies that can either transfer users to a virtual location or integrate virtual elements into the real world. The concept of the metaverse has been around for three decades – the term dates back to the science fiction novel Snow Crash written by Neal Stephenson in 1992 – and some of the elements of the future virtual universe are already being widely used. For example, Second Life has allowed its users to create avatars to meet other players, buy and sell virtual real estate and do business (Boellstorff, 2015). These possibilities are being further enhanced by contemporary virtual chats, such as VRChat (Barreda-Ángeles & Hartmann, 2022), or sites, such as Decentraland and Sandbox (Goanta, 2020; Jeon et al., 2022). Moreover, the past several years have been marked by a boom of augmented-reality games, such as Pokemon-Go, which enable smartphone gamers to explore the real world in search of virtual elements, and at the same time, the quality and availability of virtual reality (VR) systems have increased significantly, which affects their progressive adoption (Barreda-Ángeles & \fHartmann, 2022). However, despite the remarkable technological advancement, the public was introduced to the metaverse more recently, when major technology companies announced that they were beginning to work on creating their own versions of this technology, ranging from the enhancement of the current virtual worlds (e.g. Epic’s Fortnite and the Roblox ecosystem; Oh et al., 2023) to a more general and somewhat vague vision of the ‘next evolution of social connection’ (Zuckerberg, 2021). Although we still do not know what the most advanced implementations of the metaverse concept can look like, this technology can have the potential to significantly impact human interaction with the world. There is a lack of research on the predictors of the willingness to test this technology and the public concerns raised by it. Thus, we intend to fill this gap by focusing on users’ willingness to transfer a substantial amount of their daily activities to virtual worlds and its predictors. We hypothesise that greater acceptance of the development of the metaverse and willingness to use it is related to prior positive experiences of virtual places, primarily attachment to the currently used virtual places and the feeling that they can satisfy our needs more than real-world locations. Moreover, we expect that, as in the case of any new and evolving technology, the development of the metaverse will raise a number of concerns and questions. Building on the intergroup threat theory (Stephan & Stephan, 2017), we intend to examine whether the willingness to migrate to metaverse worlds is associated with two general types of threat: 1) realistic threat (e.g., fear of the loss of privacy, increased power of technology companies, loneliness of users or neglect of real places) and 2) symbolic threat (fears of how the technology can affect human nature and the bonds connecting humans to the real world). Considering the various existing definitions of the metaverse, this research will focus on users’ relationship to migration to the metaverse, understood as a three-dimensional, virtual world in which users can undertake most of the activities available in the real world through avatars. \fVirtual worlds as equivalent of real locations Places are natural conditions of human existence (Heidegger, 1962); however, the definition of a place remains an area of dispute among geographers, environmental psychologists and philosophers (Lewicka et al., 2019). The classic definitions of a place assume that it is a bounded entity with a unique identity and historical continuity, offering a haven and a break from the outside world (Relph, 1974). Other definitions stress that a place should instead be defined as a location with interactive potential, a meeting space rather than a separated enclave (Massey, 2004; Di Masso, 2012; Seamon, 2013). However, most researchers agree that a place, as opposed to a space, possesses a specific meaning for its inhabitants or visitors. Thus, a place is not a random location passed by but an object which people are connected to through their various life experiences. The emergence of virtual worlds has raised an important question for place theorists: What is the role of physicality in defining meaningful locations? Traditionally, places have been conceptualised by their tangibility and physical basis (Relph, 1976; Lewicka, 2011). However, current digital worlds can also provide a comfortable space for human dwelling (e.g. Rzeszewski & Evans, 2021). Currently, virtual environments can be comfortably accessed via one’s home on personal computers or smartphones, presenting an alternative world that can be perceived by the users as very real and changing the traditional ways of recreation, travel, social interactions and work (Plunkett, 2011). Therefore, physicality ceases to be a critical aspect that defines a place. Existing studies show that a lack of tangibility does not prevent virtual places from gaining meaning for their users. For example, Stoklos (2009) argues that virtual settings have several qualities of physical places (e.g. they afford social interaction and have symbolic meanings). Some scholars argue that it is crucial for the virtual world to gain meaning for it to be treated as a physical place (Plunkett, 2011; see also Tuan, 1974). However, the diminishing boundary between virtual and real places raises questions \fabout the latter’s possible de-meaning and its consequences. For example, Gifford (2011) argues that human failure to rationally divide time between physical and virtual worlds can lead to decreased interest in real, local concerns. Similarly, Lewicka (2020) considers whether current social processes, including the virtualisation of everyday life, may involve undermining human relationships with the places they inhabit (see also Seamon, 2020). This problem was further emphasised during the COVID-19 pandemic, when lockdowns and subsequent fear forced people to move many of their activities to virtual spaces (Barreda-Angeles & Hartmann, 2022). Thus, digital worlds served and continue serving as a functional alternative to temporarily unavailable options, steadily gaining popularity and new user bases (Paul et al., 2021). More individuals were able to test the possibilities offered by virtual worlds: for example, enacting different characters, exploring exciting places, shaping the space according to individual needs or simply enjoying safety from the pandemic and detachment from other everyday concerns (Paul et al., 2021). As a result, the rise of virtual worlds’ popularity during the COVID-19 pandemic forced human relationships with real places to weaken (Devine-Wright et al., 2020). Stay-at-home directives, social distancing and the feeling of coronavirus-related threats confined millions of people to their homes and displaced them from everyday locations that have meaning and serve to fulfil their material and psychological needs (e.g. Bick, Blandin, & Mertens, 2020; Honey-Roses et al., 2020). This alienation from places co",
            {
                "entities": [
                    [
                        0,
                        104,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "computer law & security review 47 (2022) 105737 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR What post-mortem privacy may teach us about privacy ✩ Uta Kohl University of Southampton, United Kingdom a b s t r a c t This paper approaches the debate about the protection of digital legacies through a medical confidentiality lens, capitalising on its outlier status in common law jurisdictions as a privacy-type duty that survives the death of the rightsholder. The discussion takes the case law in England and Wales and by the European Court of Human Rights on post-mortem medical confidentiality as a springboard for interrogating how these judgments navigate the traditional objections to post-mortem privacy. Whilst the legal duty of medical confidentiality, drawing on the professional duty of the Hippocratic Oath, acts in the first place as a trust mechanism between doctor and patient based on a reciprocity of interests, its incidental effect of protecting not just the rightsholder but also duty bearers and the industry, signals more complex operational dynamics. The post-mortem continuation of that duty in turn brings these other relationships to the surface. Indeed, the post-mortemness amplifies that confidentialities – and by extension information privacy - can rarely be located in an isolated, singular binary relationship between a duty bearer and a rightsholder but is entangled in the great messy sociality of life that involves multiple overlapping, interdependent relationships of relative trust. These may upon the death of the primary rightsholder – make an appearance as concurrent or competing claims on her legacies and incidentally also carry her post-mortem privacy. © 2022 Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) 1. Introduction The persistence of data and information in global online net- works has tested the functional and temporal boundaries of privacy protection. Such persistence can have dire personal and professional consequences for individuals if unfavourable personal information enters the online domain. Digital mem- ory lacks the ‘natural’ forgetfulness of humans as a protec- tive mechanism upon which privacy law has previously re- lied. Some relief is now provided in the EU through the right- to-be-forgotten under the General Data Protection Regulation ✩ University of Southampton. Many thanks to Remigius Nwabueze and the anonymous referees for their helpful comments on an earlier draft. E-mail address: U.Kohl@soton.ac.uk https://doi.org/10.1016/j.clsr.2022.105737 0267-3649/© 2022 Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \f2 computer law & security review 47 (2022) 105737 [GDPR] 1 which allows individuals to request data controllers such as search engines to ‘forget’ outdated or inaccurate per- sonal information in the results produced in response to name searches, and thereby gives them some control over their per- sonal narratives in the public domain.2 Yet, the problematic of informational persistence also continues after death pre- cisely because personal data and information remains unal- tered in situ, and the rightsholder is no longer there to give directions. Whilst the issue of personal legacies is not inher- ently new, digital remains exceed, in depth and breadth, the amount and sensitivity of previous analogue records. This has led to renewed discussions of the merits, or otherwise, of post- mortem privacy protection.3 Such post-mortem privacy would address itself, in the first place, to ‘digital legacies’ or ‘digital social media accounts,5 remains’ – such as email histories,4 documents and files, search histories, personal DNA or health profiles and digital footprints more generally. Post-mortem privacy has also been called upon by relatives to prevent public access to death scene images, against the threat of potentially large online circulations .6 Furthermore, personal big data has created entirely new post-mortem possibilities with privacy implications. For example, deepfakes, that is AI-generated im- personations based on existing personal digital footage, can bring the deceased ’back to life’, with both innocuous and abu- sive potentials which once more fall within the possible ambit of privacy protection.7 Copyright and other intellectual prop- 1 2 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016on the protection of natural persons with regard to the processing of personal data and on the freemove- ment of such data. Google Spain SL and Google Inc v Agencia Española de Protección de Datos (AEPD) and Mario Costeja González C-131/12 (CJEU, GC, 13 May 2014) EU:C:2014:317, interpreting the Data Protection Direc- tive 95/46/EC; now Art 17 of the GDPR. NNG de Andrade, ‘Oblivion: the right to be different … from oneself: re-proposing the right to be forgotten’ in Alessia Ghezzi, Ângela Guimarães Pereira, Lucia Vesnic-Alujevic L (eds) The Ethics of Memory in a Digital Age – In- terrogating the Right to Be Forgotten (New York: Palgrave Macmillan, 2014) 65. For some limited US authority to the same effect, see e.g. Briscoe v Reader’s Digest Association 4 Cal 3d 532 (1971) concerning a newspaper revelation of the claimant’s criminal past which had the effect of alienating his daughter and friends from him. Jason Mazzone, ‘Facebook’s Afterlife’ (2012) 90 North Carolina Law Review 1643; Lilian Edwards, Edina Harbinja, ‘Protecting Post- Mortem Privacy: Reconsidering the Privacy Interests of the De- ceased in a Digital World’ (2013) 32 (1) Cardozo Arts & Entertainment Law Journal 101; Natalie M Banta, ‘Death and Privacy in the Digi- tal Age’ (2016) 94 North Carolina Law Review 927; Floris Tomasini, Remembering and Disremembering the Dead (Palgrave, 2017) esp Ch3. Estate of Maria Cecilia Quadri v Parisi (2021) WL 3544783 (Fla Cir 4 3 5 6 Ct); see also Ajemian v Yahoo! Inc 84 NE3d 766 (Mass 2017). Digital Inheritance III ZR 183/17 (12 July 2018) BGH. In National Archives and Records Administration v Favish 541 US 157 (2004) the US Supreme Court allowed the family’s privacy claim in respect of death scene images of the deceased as an ex- emption to freedom of information requests (here made by jour- nalists): ‘The well-established cultural tradition of acknowledging a family’s control over the body and the deceased’s death images has long been recognized at common law.’ Marisa McVey, ‘Deepfakes and the Dead: The Case of An- thony Bourdain’ (10 August 2021) Modern Technologies, Privacy Law and the Dead https://thefutureofprivacylaw.wordpress.com/2021/ 08/10/deepfakes- and- the- dead- the- case- of- anthony- bourdain/ 7 erty rights which serve economic interests, outlast the death of the rightsholder, but should privacy underwritten by digni- tary concerns also remain intact post-mortem and, if so, how long and guarded by whom? In some civil law jurisdictions, such as Germany, person- ality rights do not automatically expire upon death,8 which gives those jurisdictions a solid foundation for navigating the raised stakes of personal digital remains. Meanwhile common law countries are especially ill-equipped to deal with the new phenomenon, given the absence of a developed personality rights jurisprudence and their adherence (but for some statu- tory interventions) to the long-standing maxim actio person- alis moritur cum persona , that is personal, as opposed to propri- etary, actions die with the person.9 This maxim which goes, at least, as far back as Hambly v Trott (1776) 10 is based on the idea that ‘personal rights’ are by their very design attached to the person and thus not transferrable. By implication, any post- mortem recovery would be ‘in the nature of a penalty rather than compensatory.’ 11 The maxim captures injuries to intan- gible interests of personality, of which privacy is an example par excellence : ‘[i]t is well settled that the right to privacy is purely a personal one; it cannot be asserted by anyone other than the person whose privacy has been invaded.’ 12 This paper examines the call for post-mortem privacy in respect of digital legacies by mapping one of the few con- texts where even common law jurisdictions have recognised the possibility of privacy surviving the death of the rightsh- older, that is in respect of medical confidences.13 Whilst its almost unique status may suggest an outlier case the ratio- nale of which is not transferrable to other subject-matters,14 the argument here is that the animating forces behind post- mortem medical confidentiality are instructive about post- mortem privacy generally. Thus the microcosm of medical confidentiality delivers an opportunity for interrogating why and how the law recognises post-mortem medical privacy de- 8 9 Mephisto 30/173 (24 February 1971) BVergG; Marlene Dietrich 1 ZR 49/97 (1 December 1999) BGH; Wilhelm Kaisen 1 BvR 932/94 (5 April 20012) BVerfG; see also § 189 of the German Criminal Code (offence of defiling the memory of the dead). Note, for example, Recital 27 of the GDPR which provides that ‘This Regulation does not apply to the personal data of deceased persons. Member States may provide for rules regarding the pro- cessing of personal data of deceased persons.’ In the UK data pro- tection is limited to living individuals: s.3(2) of the Data Protection Act 2018. 10 Hambly v Trott (1776) 1 Cowper 371 (where the court acknowl- edged the limited application of the maxim to torts); but the Com- mon Law Procedure Act 1833, replaced by the Law Reform (Miscel- laneous Provisions) Act 1934 allowed for the deceased’s estate to recover some losses that arose before their death. 11 Alvin E Evans, ‘Survival of Claims For and Against Executors and Administrator’ (1931) 19 Kentucky Law Journal 195, 206. 12 James v Screen Gems Inc (1959) 174 CalApp 2d 650, 653; Hendrick-",
            {
                "entities": [
                    [
                        137,
                        188,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Internet Interventions journal homepage: www.elsevier.com/locate/invent Accessibility of mental health support in China and preferences on web-based services for mood disorders: A qualitative study Yuxi Tan a,b,c, Emily G. Lattie d, Yan Qiu a, b, c, Ziwei Teng a,b, c, Chujun Wu a, b, c, Hui Tang a,b, c, Jindong Chen a, b,c, * a Department of Psychiatry, The Second Xiangya Hospital, Central South University, Changsha, Hunan, China b National Clinical Research Center for Mental Disorders, Changsha, Hunan, China c Institute of Mental Health, Central South University, Changsha, Hunan, China d Department of Medical Social Sciences, Northwestern University, Chicago, IL, United States  A R T I C L E I N F O  A B S T R A C T  Keywords: Mental health Web-based health service Mood disorder Recovery Background: The fast development of mobile technologies provides promising opportunities to fulfill the largely unmet needs of treatment and recovery for mood disorders in China. However, with limited research from China, the development of acceptable and usable web-based mental health services that are based on preference of patients from China still remains a challenge. Objective: The aims of this paper were to (1) understand the experience of patients with mood disorders on current accessibility of mental health support in China; and (2) to get insights on patients' preferences on web- based mental health services, so as to provide suggestions for the future development of web-based mental health services for mood disorders in China. Methods: Semi-structured interviews were conducted with 10 female participants diagnosed with depression and 7 with bipolar disorder (5 female and 2 male) via the audio chat function of WeChat. The interviews were 60–90 min long and were audio-recorded and transcribed verbatim. Thematic analysis was conducted using QSR NVivo 12 to identify and establish themes and sub-themes. Results: Two major sections of results with a total of 5 themes were identified. The first section was participants' treatment and recovery experience, which included three main themes: (1) professional help seeking experience; (2) establishment of self-help strategies; and (3) complex experiences from various source of social support. The second section was focused on preferences for web-based services, which were divided into two themes: (1) preferred support and features, with three sub-themes: as channels to access professionals, as databases for self- help resources, and as sources of social support; and (2) preferred modality. Conclusions: The access to mental health support for personal recovery of mood disorders in China was perceived by participants as not sufficient. Web-based mental health services that include professional, empathetic social support from real humans, and recovery-oriented, personalized self-help resources are promising to bridge the gap. The advantages of social media like WeChat were emphasized for patients in China. More user-centered research based on social, economic and cultural features are needed for the development of web-based mental health services in China.  1. Introduction Mood disorders are common in China, with a lifetime prevalence of 7.4% (Huang et al., 2019). With the episodic nature of these disorders, the risk of relapse is high (Gitlin and Miklowitz, 2017; Hardeveld et al., 2013). Management of subthreshold symptoms by providing adjunctive personalized psychological interventions appears vital to preventing disability and enhancing quality of life (Bonnín et al., 2012; Miziou et al., 2015). However, despite the high prevalence, only one fifth of patients with mood disorders have ever been in contact with mental health-care providers in their lifetime (Patel et al., 2016). The reasons for the large, unmet mental health needs in China are abundant, and are often explained by the unbalanced allocation of resources between * Corresponding author at: Department of Psychiatry, The Second Xiangya Hospital, Central South University, 139 Middle Renmin Road, Changsha, Hunan 410011, China. E-mail address: chenjindong@csu.edu.cn (J. Chen). https://doi.org/10.1016/j.invent.2021.100475 Received 19 January 2021; Received in revised form 20 October 2021; Accepted 30 October 2021  InternetInterventions26(2021)100475Availableonline4November20212214-7829/©2021TheAuthors.PublishedbyElsevierB.V.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).\fY. Tan et al.                                                                                                                   Abbreviations None.  major cities and rural areas, limited mental health workforce, especially the lack of professional social workers and psychological therapists (Liang et al., 2018). The lack of medical insurance support also contributed to the high threshold for psychological therapy (Zhang et al., 2017). In recent years, the 686 project, also called the Central Government Support for the Local Management and Treatment of Serious Mental Illness Project, has been benefiting many patients with severe mental diseases, especially schizophrenia, to receive more access to more convenient even free treatment and recovery service (Liang et al., 2018). To achieve better mental health at the population level, much more attention is needed to bridge the gap faced by patients with non-psychotic disorders. With the development of mobile technology, the Internet has become a common way for patients to search for information, seek mental health resources, or obtain social support (Rahal et al., 2018). Using mobile platforms to provide mental health services has become a feasible way to bridge the gap. Currently, there are many types of web-based mental health services, such as provision of psychological education and self- management strategies based on cognitive behavioral therapy or mindfulness (O'Connor et al., 2018; Rathbone et al., 2017), active or passive symptom monitoring and feedback using smartphone sensors or wearable devices (Simblett et al., 2018), or real-time interaction via chatbots or conversational agents (Vaidyam et al., 2019). A number of reviews have demonstrated that web-based mental health services have the advantages in preventing mental illness, alleviating symptoms and improving quality of life, especially with human support (Ebert et al., 2018; O'Connor et al., 2018). Evidence showed that patients generally have high acceptance of mental health services provided on the Internet, but are prudent about their effectiveness (Apolinario-Hagen et al., 2017; Tan et al., 2020). In addition, there are several challenges worth con-cerning for future practice and implication, including low engagement, lack of evidence support, limited implication on significant clinical symptoms (Frank et al., 2018; Torous et al., 2018). There are also numerous qualitative research studies that have demonstrated the importance of mobile technologies to facilitate the treatment and recovery of mood disorders. For instance, several studies have identified the role of mobile app use on helping with patients' social interconnectivity, skill acquisition, access and management of needs, and bridge the disconnection with health professionals (Fulford et al., 2019; Pung et al., 2018). Other studies have revealed factors that might influence the effectiveness or motivation of mobile mental health ser-vices usage, such as mental health awareness, appropriate content and medium, functionality, human support, reminders or incentives (Eccles et al., 2020; Simblett et al., 2018). Barriers can be divided into personal barriers, such as lack of time, high stress level, and barriers directly related to the program, such as complex content, functionality and privacy issues (Eccles et al., 2021; Ebert et al., 2018) However, the vast majority of research on web-based mental health services were conducted in western counties (Lal and Adair, 2014), and there is still a gap in the literature, especially qualitative researches on the needs and preferences of people living in China. How to design more acceptable and usable web-based mental health services that are truly based on patients' needs and effectively promote mental health in China still remains a challenge. Given these gaps, along with the lack of service availability in China, the use of well-designed web-based mental health intervention as auxiliary means to expand the accessibility of existing mental health resources, rather than replacing the traditional face-to- face medical services (Ebert et al., 2018), could be particularly prom-ising in China. The purposes of this study are to: (1) understand the experience of patients with mood disorders regarding the current accessibility of mental health support, and (2) identify patients' preferences on web- based mental health services, so as to provide suggestions for the future development of web-based mental health services for mood dis-orders in China. 2. Methods 2.1. Study design Single session semi-structured interviews were conducted with par-ticipants diagnosed with depression or bipolar disorder via remote audio chat function provided by WeChat. WeChat is one of the most commonly used social media platforms in China (Montag et al., 2018). Audio chat was selected over video conferencing due to the sensitivity of interview content, and belief that patients may feel more comfortable discussing personal stories and feelings in a more anonymous manner. Moreover, the study was conducted during the Coronavirus Disease 2019 (COVID- 19) pandemic, which further required the interviews to be remote. 2.2. Recruitment The criteria for recruitment are: (1) age > 18; (2) have a diagnosis of major depression disorder or bipolar disorder for at least 6 months; (3) not currently experiencing suicidal ideation or manic symptoms. Par-ticipants were drawn from a pool of indivi",
            {
                "entities": [
                    [
                        114,
                        239,
                        "TITLE"
                    ]
                ]
            }
        ],
        [
            "Fossil fuel violence and visual practices on Indigenous landCitation for published version:Spiegel, SJ 2021, 'Fossil fuel violence and visual practices on Indigenous land: Watching, witnessing andresisting settler-colonial injustices', Energy Research and Social Science, vol. 79, 102189.https://doi.org/10.1016/j.erss.2021.102189Digital Object Identifier (DOI):10.1016/j.erss.2021.102189Link:Link to publication record in Edinburgh Research ExplorerDocument Version:Publisher's PDF, also known as Version of recordPublished In:Energy Research and Social ScienceGeneral rightsCopyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s)and / or other copyright owners and it is a condition of accessing these publications that users recognise andabide by the legal requirements associated with these rights.Take down policyThe University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorercontent complies with UK legislation. If you believe that the public display of this file breaches copyright pleasecontact openaccess@ed.ac.uk providing details, and we will remove access to the work immediately andinvestigate your claim.Download date: 04. jul.. 2023   Edinburgh Research Explorer                   \fContents lists available at ScienceDirect Energy Research & Social Science journal homepage: www.elsevier.com/locate/erss Fossil fuel violence and visual practices on Indigenous land: Watching, witnessing and resisting settler-colonial injustices Samuel J. Spiegel School of Social and Political Science, University of Edinburgh, 19 George Square, Edinburgh EH8 9LD, Scotland, United Kingdom  A R T I C L E I N F O  A B S T R A C T  Keywords: Oil pipelines Decolonising energy Environmental injustice Indigenous sovereignty Visual politics Counter-watching While controversial plans for fossil fuel pipeline-building continue across Indigenous lands without consent, how are visual practices – including watching and witnessing – serving as modes of resistance? Drawing on a participant-observation ethnography over the 2018–2021 period with environmental defenders on Coast Salish land, in what is colonially called ‘British Columbia, Canada’, this article offers a lens for exploring visual realms of resistance amid expanding extractivism, police surveillance and reconfigured pipeline opposition during the COVID-19 pandemic. Grassroots photography in land-based monitoring, artistic communication in and around courtrooms and other visual practices have been serving as powerful inflection points, countering multiple facets of petro-colonialism – ecological destruction, health threats, and moral and legal transgressions by companies and state institutions. They have also been stimulating new collective actions, some led by Indigenous land protectors extending longstanding traditions of protecting human and non-human life. As ‘more-than-repre-sentational’, visual encounters can be active players in constructing knowledge, challenging structures of dispossession, genocide and ecocide, and cultivating understandings of care, sovereignty, climate justice and anti-colonial solidarity from heterogeneous vantage points. Some environmental defenders’ visual creativities invite deep reflection on ontologies rooted in reciprocity and respect that are thoroughly ignored in extractivist settler-colonial cultures. The article situates visual strategies in fraught political contexts of ramped-up police and corporate surveillance targeting Indigenous land protectors and other environmental defenders, under-scoring critical concern about superficial optical allyship and hollow gestures by state actors responding to racism and state violence on Indigenous land. It calls for attention to dialectical relationships amongst state visual tactics and counter-hegemonic visual practices in struggles to resist colonial energy regimes and to cultivate efforts towards alternative, less destructive energy futures.  1. Introduction: seeing and watching in the age of fossil fuel violence Visual encounters – including practices of seeing, watching, wit-nessing and representing – can play powerful roles in struggles over fossil fuel extraction regimes, reflecting diverse values, interests and cultural processes. Some practices of visual representation mislead, stigmatize and marginalise, reinforcing racist interactions and cultural hegemonies [1-4]. In contrast, however, others might contribute to vi-sual sovereignty and resistance in Indigenous communities amid efforts to oppose unwanted oil pipelines and colonial power, as Brígido-Cor-ach´an (2017) considered in the context of Standing Rock in the United States [5]. Practices of visual resistance constitute important yet often under-explored and under-theorized dimensions of longstanding strug-gles over fossil fuel pipelines, at times as a nexus for re-imagining ideas and discourses of identity across multiple scales [6]. From critical ‘watching back’ against land degradation and colonial oppression as police and corporate surveillance intensify, to the witnessing of envi-ronmental defenders prosecuted in courtrooms, I suggest here a lens through which to understand dynamic fossil fuel contestations and their linkages as well as the reconfigurations during the COVID pandemic. Extending literatures on the use of visual surveillance to criminalize environmental defenders and the performativity of images, this article calls for attention to how visual practices are embedded in the ever- changing dynamics of ongoing colonisation and anti-colonial strug-gles, drawing upon experiences from early 2018 to the beginning of 2021 on Indigenous lands – in what is now colonially called ‘British Columbia’ (by the Government of Canada to describe the western-most province of its federation). My overarching aim here is to explore how critical visual thinking E-mail address: sam.spiegel@ed.ac.uk. https://doi.org/10.1016/j.erss.2021.102189 Received 21 January 2021; Received in revised form 28 June 2021; Accepted 30 June 2021  EnergyResearch&SocialScience79(2021)102189Availableonline16July20212214-6296/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).\fS.J. Spiegel                                                                                                                    around energy regimes may – in some cases – be part of ‘activism that resists settling on colonial ways of knowing place’ [7]. Beyond unpacking discrete visual practices, the interrogating of multiple visual encounters as connected in systems of oppression and resistance offers a lens into dialectical relations that shape hegemonic and counter- hegemonic worlds of seeing. From state surveillance to monitor envi-ronmental defenders who oppose energy megaprojects [8–10] to the advertisement campaigns of the fossil fuel industry [6], visuals often play consequential roles amplifying agendas of corporations and prac-tices of what Finn (2013) calls ‘seeing surveillantly’ [11]. Visual prac-tices may also disorient and re-orient. Works of art may make hidden spaces visible, engendering attention to emotion and affect [12]. Acts of witnessing, watching, seeing and representing are, of course, never just acts; they are embedded in constantly moving relations with diverse possibilities for tapping into colonial and/or anti-colonial politics, and diverse theories of change, ethics and subjectivities at play. There is now growing interest in visual practices as a nexus for seeing how power asymmetries are inscribed in spaces of environmental politics [13]. Yet, while much scholarly writing has addressed fossil fuel impacts by exploring visual representations of climate change and sometimes shocking environmental degradation shown in news media [14], the interplay of multiple (radical as well as less radical) visual practices in contexts of contested fossil fuel projects has, in many milieus, received surprisingly little attention. This article particularly focuses on experiences unfolding on ílwetaʔ/Selilwitulh (Tsleil- CSkwxwú7mesh (Squamish), St´o:l¯o and SelWaututh), xʷmeθkʷey̓ em (Musqueam), Secw´epemc and Wet’suwet’en lands. Its approach focuses on three distinct periods of counter- watching, witnessing and making injustices visible, particularly in re-gard to the Trans Mountain Pipeline Expansion (TMX), a megaproject designed to triple the amount of heavy oil (bitumen) transported from the Alberta tar sands across more than 140 Indigenous Nations’ terri-tory, presenting threats to planetary wellbeing due to climate impacts as well as local health, ecosystems, economies and cultural rights [15]1. While my own field research focused on place-based struggles around TMX, the analysis is contextualized by also exploring the simultaneous efforts of Indigenous-led movements to resist the Coastal Gaslink pipe-line which is set to move fracked gas through the land of the Wet’su-wet’en people further north in BC, in direct affront to their sovereignty [16,17].2 I focus first on the burgeoning anti-pipeline resistance move-ments in BC between March 2018 and January 2020; then on a period of rapid growth of these movements in February 2020 right after mil-itarised police raids and before the COVID pandemic hit; and, finally, on transformations after the pandemic struck in March 2020 – including analysis of the (non)performativity of images and ‘optical allyship’ [20] of state actors in this period. Each of these periods was punctuated by various visual practices that reflect historically situated experiences, with various cultural politics, affects and relations at play. The 2018–2021 era constitutes a critical time span for grappling with systemic racism that propels unjust energy systems [21] and conceptualising, visually, the sometimes more distant 1 A map of the pipeline and timeline of key events in th",
            {
                "entities": [
                    [
                        1416,
                        1540,
                        "TITLE"
                    ]
                ]
            }
        ]
    ]
}