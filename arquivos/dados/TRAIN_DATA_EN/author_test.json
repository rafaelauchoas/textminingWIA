{
    "TRAINING_DATA": [
        [
            "ELSEVIER Artificial Intelligence 88 (1996) 101-142 Artificial Intelligence A statistical approach to adaptive problem solving Jonathan Gratch ‘**, Gerald DeJong b,l a Information Sciences Institute, University of Southern California, 4676 Admiral@ Way, Marina de1 Rey, CA 90292, USA h Beckman Institute, University of Illinois, 405 N. Mathews, Urbana, IL 61801, USA Received May 1994; revised February 1995 Abstract Domain independent general purpose problem solving techniques are desirable from the stand- points of software engineering and human computer interaction. They employ declarative and modular knowledge representations and present a constant homogeneous interface to the user, untainted by the peculiarities of the specific domain of interest. Unfortunately, this very insulation from domain details often precludes effective problem solving behavior. General approaches have proven successful in complex real-world situations only after a tedious cycle of manual experi- mentation and modification. Machine learning offers the prospect of automating this adaptation cycle, reducing the burden of domain specific tuning and reconciling the conflicting needs of generality and efficacy. A principal impediment to adaptive techniques is the utility problem: even if the acquired information is accurate and is helpful in isolated cases, it may degrade overall problem solving performance under difficult to predict circumstances. We develop a formal char- acterization of the utility problem and introduce COMPOSER, a statistically rigorous learning approach which avoids the utility problem. COMPOSER has been successfully applied to learning heuristics for planning and scheduling systems. This article includes theoretical results and an extensive empirical evaluation. The approach is shown to outperform significantly several other leading approaches to the utility problem. 1. Introduction There is a wide gulf between general approaches and flective solving. Practical success has come from custom [ 52,661, or other application systems specific techniques techniques * Corresponding ’ E-mail: dejong@cs.uiuc.edu. author. E-mail: gratch@isi.edu. approaches to problem like expert systems, reactive that require extensive human 0004-3702/96/$15.00 PII SOOO4-3702(96)0001 l-2 Copyright @ 1996 Elsevier Science B.V. All rights reserved. \fto complete. AI researchers have also developed domain independent algo- and constraint satisfaction algorithms. Unfortunately, show success, systems, while derived from a general it is usually only after extensive domain specific technique, bear more investment rithms such as nonlinear planning when general approaches adjustments. The resulting resemblance Adaptive problem tradeoff performance solvers to work well all about performance irrelevant problem to real-world on unseen or unlikely problems, fact, machine problem solving performance, adaptive problem solving learning to the custom approaches. solving in repetitive problem for the problems on other hypothetical is a potential means solving they actually for circumventing this generality/ situations. We want our problem solving. Pragmatically, a system’s overall performance may be enhanced. encounter problems. Worst-case behavior and we care not at is largely by sacrificing good behavior In to enhance Nonetheless, the capacity techniques have successfully demonstrated although in limited contexts [ 18,46,57,65]. in any general sense. is still far from realized The principal impediment to adaptive problem introduced to machine hypothesized adaptation actually automatically formance. Steve Minton refer to this difficulty of insuring performance discussed via search control heuristics progress on this issue understood, solving performance [ 15,40,48,53], and can fail to improve performance, in the context of improving called control under certain circumstances. the problem results solving is characterizing when an in improved problem solving per- to the term utility problem the average problem [ 531. Minton originally solving speed learning improvements rules. While there has been considerable the proposed methods are often ad hoc, poorly or worse, actually degrade problem Adaptivity can be an effective method for improving problem solving performance in through that to deduce a priori, as in the blocksworld domain where a block can never represented. On the other hand, the that real-world problems are often constrained experience. On one hand, the domain specification may implicitly embed constraints are difficult be atop itself, though distribution of tasks embeds many constraints For example, greater that can only be induced application may never contain from experience. towers of height in ways that only become obvious a particular blocksworld is not explicitly this constraint than three. Exploiting these regularities from characterizations can lead to clear performance of past problems. Most distributions improvements. Of course, look into the future to anticipate particular problems. However, the problem solver cannot exhibit it can generalize peculiarities intractable that may be exploited once detected. Conversely, even worst-case algorithms may perform well under certain distributions. For example, Goldberg suggests solved in 0( n2) time [ 221. that naturally occurring [ 5,561 or devising Recent work has focused on characterizing [3]. techniques it is available Research optimization [45] exploit construction [ 51, pp. 252-2851 the expected distribution these easy distributions information when that can exploit specific distribution satisfiability problems are frequently the fact of a problem better expected performance. solver with substantially of tasks can allow into self-organizing and dynamic that learning systems the In the next section, we provide a formal characterization decision problem theoretic solving performance terms. This introduces and casts in the notion of expected utility as a metric of through a space of the learning of the utility problem as a search \fJ. Gratch, G. DeJong/Ar@cial Intelligence 88 (1996) 101-142 103 p&~!iEEJcy Fig. I. Learning transforms an initial problem solver into a new one. To accomplish this the learner must choose one of a set of possible transformations. several techniques transformations the COMPOSER algorithm, performs a probabilistic to ensure problem solving 3 describes problem. COMPOSER and incorporates describes an extensive evaluation of the approach for a domain compares average case analysis of the algorithm’s to be polynomial confidence favorably with existing approaches independent problem in the number of transformations a probabilistic for a problem solver with high expected utility. Section the utility approach space search through the efficiency of this process. Section 4 in the context of learning control rules that COMPOSER to avoiding the transformation solver. The experiments indicate complexity. COMPOSER’s to the utility problem. Section 5 provides an run time is shown in the statistical considered and required. Finally, we discuss some limitations and present conclusions. 2. A formal characterization of the utility problem algorithm learning Before a rigorous terize what the learning formal characterization makes precise and explicit should do, and it provides a structure and precise statements. can be constructed, we must explicitly charac- to achieve. In this section, we introduce a is attempting system of a general class of learning problem solvers. This formalism intuitive and often unstated notions of what a learning system for formal analysis, enabling us to make definitive Abstractly, a learning algorithm operates on an initial problem solver, transforming relative the effect of this change criterion and a pattern of tasks associated with the intended application. solver, where is assessed it to into a different problem some evaluation This is illustrated set of potential problem transformations To characterize utility [2] as a common argued for the merits of decision systems [ 41,64,67,7 1 ] and machine in Fig. 1 where a set of hypothesized transformations defines a solvers. The learning to adopt, where “good” outcomes, we use the decision the outcome of this decision framework for characterizing system must decide which hypothesized solver. theoretic notion of expected this decision problem. Doyle has is a new problem [ 131 and it has seen increasing theory as a standard for evaluating artificial acceptance, both in artificial intelligence intelligence at large learning in particular [ 30,35,45,69]. 2.1. Expected utility Decision theory relies on the observation can, under some natural assumptions, that preferences over different outcomes be characterized by a real-valued utility function. \f104 J. Gratch. G. DeJoq/Art@cial Intelligence RR (1996) 101-142 Outcome A is preferred to outcome B iff the utility of A is greater than the utility of B. the same outcome. Even In an uncertain world, a decision may not always produce the next. The correct the best decision policy may do very well one time and poorly decision policy under uncertainty maximizes is characterized by a set of outcomes and a probability distribution over this set, and the expected utility of a decision of occurrence. is the utility of all possible outcomes weighted by their probability expected utility: a decision For learning, the characteristics solvers) problem (transformed preferences with a utility of choosing application a transformation are preferred. With decision of the intended application determine what outcomes these theory, we represent to be cast as the decision problem the learning expected utility. However, we require function, allowing that increases to obey the following two restrictions. probability as a random selection of tasks according Fixed distribution assumption. The pattern of tasks in the problem ment must be characterizable unknown) that there is some probability of occurr",
            {
                "entities": [
                    [
                        126,
                        141,
                        "AUTHOR"
                    ],
                    [
                        147,
                        160,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 696–721Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintProbabilistic planning with clear preferences on missing informationMaxim Likhachev a,∗, Anthony Stentz ba Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USAb The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 8 October 2007Received in revised form 27 October 2008Accepted 29 October 2008Available online 25 November 2008Keywords:Planning with uncertaintyPlanning with missing informationPartially Observable Markov DecisionProcessesPlanningHeuristic searchFor many real-world problems, environments at the time of planning are only partially-known. For example, robots often have to navigate partially-known terrains, planes oftenhave to be scheduled under changing weather conditions, and car route-finders oftenhave to figure out paths with only partial knowledge of traffic congestions. While generaldecision-theoretic planning that takes into account the uncertainty about the environmentis hard to scale to large problems, many such problems exhibit a special property: onecan clearly identify beforehand the best (called clearly preferred) values for the variablesthat represent the unknowns in the environment. For example, in the robot navigationproblem, it is always preferred to find out that an initially unknown location is traversablerather than not, in the plane scheduling problem, it is always preferred for the weather toremain a good flying weather, and in route-finding problem, it is always preferred for theroad of interest to be clear of traffic. It turns out that the existence of the clear preferencescan be used to construct an efficient planner, called PPCP (Probabilistic Planning with ClearPreferences), that solves these planning problems by running a series of deterministic low-dimensional A*-like searches.In this paper, we formally define the notion of clear preferences on missing information,present the PPCP algorithm together with its extensive theoretical analysis, describe severaluseful extensions and optimizations of the algorithm and demonstrate the usefulnessof PPCP on several applications in robotics. The theoretical analysis shows that onceconverged, the plan returned by PPCP is guaranteed to be optimal under certain conditions.The experimental analysis shows that running a series of fast low-dimensional searchesturns out to be much faster than solving the full problem at once since memoryrequirements are much lower and deterministic searches are orders of magnitude fasterthan probabilistic planning.© 2008 Elsevier B.V. All rights reserved.1. IntroductionA common source of uncertainty in planning problems is lack of full information about the environment. A robot maynot know the traversability of the terrain it has to traverse, an air traffic management system may not be able to forecastwith certainty future weather conditions, a car route-finder may not be able to predict well future traffic congestions oreven be sure about present traffic conditions, a shopping planner may not know whether a particular item will be on saleat one of the stores it considers. Ideally, in all of these situations, to produce a plan, a planner needs to reason over theprobability distribution over all the possible instances of the environment. Such planning is known to be hard [1,2].* Corresponding author.E-mail address: maximl@seas.upenn.edu (M. Likhachev).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.10.014\fM. Likhachev, A. Stentz / Artificial Intelligence 173 (2009) 696–721697For many of these problems, however, one can clearly name beforehand the “best” values of the variables that representthe unknowns in the environment. We call such values clearly preferred values. Thus, in the robot navigation problem, it isalways preferred to find out that an initially unknown location is traversable rather than not. In the air traffic managementproblem it is always preferred to have a good flying weather. In the problem of route planning under partially-known trafficconditions, it is always preferred to find out that there is no traffic on the road of interest. And finally, in the shoppingplanning example, it is always preferred for a store to hold a sale on the item of interest. These are just few of what webelieve to be a large class of planning problems that exhibit clear preferences on missing information. One of the reasonsfor this is that the knowledge of clear preferences on missing information is not the same as the knowledge of a best actionat a state or the value of an optimal policy. Instead, we often know at intuitive level what would be the best event for us(i.e., no traffic congestion, sale, etc.), independently of whether we choose to make use of this event or not. All the otheroutcomes, on the other hand, are of less preference to us. This intuitive information can be used in planning.In this paper we present an algorithm called PPCP (Probabilistic Planning with Clear Preferences) that is able to scaleup to very large problems by exploiting the fact that these preferences exist. PPCP constructs and refines a plan by runninga series of deterministic A*-like searches. Furthermore, by making an approximating assumption that it is not necessaryto retain information about the variables whose values were discovered to be clearly preferred values, PPCP keeps thecomplexity of each search low and independent of the amount of the missing information. Each search is extremely fast,and running a series of fast low-dimensional searches turns out to be much faster than solving the full problem at once sincethe memory requirements are much lower and deterministic searches can often be many orders of magnitude faster thanprobabilistic planning techniques. While the assumption PPCP makes does not need to hold for the algorithm to converge,the returned plan is guaranteed to be optimal if the assumption does hold.The paper is organized as follows. We first briefly go over A* search and explain how it can be used to find least-costpaths in graphs. We then explain how a planning problem changes when some of the information about the environmentis missing. In Section 4, we introduce the notion of clear preferences on missing information and briefly talk about theproblems that exhibit them. In Section 5, we explain the PPCP algorithm and how it makes use of the clear preferences. Thesame section gives an extensive theoretical analysis of PPCP that includes the correctness of the algorithm, some complexityresults as well as the conditions for the optimality of the plan returned by PPCP. In Section 6 of the paper, we describe twouseful extensions of the algorithm such as how one can interleave PPCP planning and execution. In the same section, wealso give two optimizations of the algorithm which at least for some problems can speed it up by more than a factor offour. On the experimental side, Section 7 shows how PPCP enabled us to successfully solve the path clearance problem, animportant problem in defense robotics. The experimental results in Section 8.1, on the other hand, evaluate the performanceof PPCP on the problem of robot navigation in partially-known terrains. They show that in the environments small enoughto be solved with methods guaranteed to converge to an optimal solution (such as Real-Time Dynamic Programming [3]),PPCP always returns an optimal policy while being much faster. The results also show that PPCP is able to scale up to large(costmaps of size 500 by 500 cells) environments with thousands of initially unknown locations. The experimental resultsin Section 8.2, on the other hand, show that PPCP can also solve large instances of path clearance problem and results insubstantial benefits over other alternatives. We finally conclude the paper with a short survey of related work, discussion,and conclusions.2. Backward A* search for planning with complete informationNotations. Let us first consider a planning problem that can be represented as a search for a path in a fully knowndeterministic graph G. The fact that the graph G is completely known at the time of planning means that there is nomissing information about the domain (i.e., environment). We use S to denote a state (a vertex, in the graph terminology)in the graph G. State Sstart refers to the state of the agent at the time of planning, while state Sgoal refers to the desiredstate of the agent. We use A(S) to represent a set of actions available to the agent at state S ∈ G. Each action a ∈ A(S)corresponds to a transition (i.e., an edge) in the graph G from state S to the successor state denoted by succ(S, a). Eachsuch transition is associated with the cost c(S, a, succ(S, a)). The costs need to be bounded from below by a (small) positiveconstant.Backward A* search. The goal of shortest path search algorithms such as A* search [4] is to find a path from S start toSgoal for which the cumulative cost of the transitions along the path is minimal. The PPCP algorithm we present in thispaper is based on running a series of deterministic searches. Each of these searches is a modified backward A* search—theA* search that searches from Sgoal towards Sstart by reversing all the edges in the graph. In the following, we thereforebriefly describe the operation of a backward A* search.∗(S).Suppose for every state S ∈ G we knew the cost of a least-cost path from S to S goal. Let us denote such cost by gThen a least-cost path from Sstart to Sgoal can be easily followed by starting at Sstart and always executing such action∗(succ(S, a)). Consequently, A* search tries to computea ∈ A(S) at any state S that a = arg mina∈ A(S)(c(S, a, succ(S, a)) + g∗-values. In particular, A* maintains g-values for each state it has visited so far. g(S) is always the cost of the best pathgfound so fa",
            {
                "entities": [
                    [
                        204,
                        219,
                        "AUTHOR"
                    ],
                    [
                        225,
                        239,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 300 (2021) 103563Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAbstraction for non-ground answer set programs ✩Zeynep G. Saribatur∗, Thomas Eiter, Peter SchüllerInstitute of Logic and Computation, TU Wien, Favoritenstraße 9-11, A-1040 Vienna, Austriaa r t i c l e i n f oa b s t r a c tArticle history:Received 20 December 2019Received in revised form 11 May 2021Accepted 21 July 2021Available online 28 July 2021Keywords:AbstractionAnswer set programmingDeclarative problem solvingKnowledge representation and reasoningNonmonotonic formalismsExplaining unsatisfiabilityCounterexample-guided abstraction and refinementAbstraction is an important technique utilized by humans in model building and problem solving, in order to figure out key elements and relevant details of a world of interest. This naturally has led to investigations of using abstraction in AI and Computer Science to simplify problems, especially in the design of intelligent agents and automated problem solving. By omitting details, scenarios are reduced to ones that are easier to deal with and to understand, where further details are added back only when they matter. Despite the fact that abstraction is a powerful technique, it has not been considered much in the context of nonmonotonic knowledge representation and reasoning, and specifically not in Answer Set Programming (ASP), apart from some related simplification methods. In this work, we introduce a notion for abstracting from the domain of an ASP program such that the domain size shrinks while the set of answer sets (i.e., models) of the program is over-approximated. To achieve the latter, the program is transformed into an abstract program over the abstract domain while preserving the structure of the rules. We show in elaboration how this can be also achieved for single or multiple sub-domains (sorts) of a domain, and in case of structured domains like grid environments in which structure should be preserved. Furthermore, we introduce an abstraction-&-refinement methodology that makes it possible to start with an initial abstraction and to achieve automatically an abstraction with an associated abstract answer set that matches an answer set of the original program, provided that the program is satisfiable. Experiments based on prototypical implementations reveal the potential of the approach for problem analysis, by its ability to focus on the parts of the program that cause unsatisfiability and by achieving concrete abstract answer sets that merely reflect relevant details. This makes domain abstraction an interesting topic of research whose further use in important areas like Explainable AI remains to be explored.© 2021 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).1. IntroductionAbstraction is a technique applied in human reasoning and understanding, by reasoning over the models of the world that are built mentally [30,68]. Although its meaning comes from “to draw away”, there is no precise definition that is capable of covering all meanings that abstraction has in its utilizations. There is a variety of interpretations in different Some of the results in this article were presented in preliminary form at JELIA 2019 [113] and XAI 2019 [45]. This work has been partially supported ✩by the Austrian Science Fund (FWF) grant W-1255 and by the EU’s H2020 research and innovation programme under grant agreements 825619 (AI4EU) and 952026 (HumanE-AI Net).* Corresponding author.E-mail addresses: zeynep@kr.tuwien.ac.at (Z.G. Saribatur), eiter@kr.tuwien.ac.at (T. Eiter), ps@kr.tuwien.ac.at (P. Schüller).https://doi.org/10.1016/j.artint.2021.1035630004-3702/© 2021 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fZ.G. Saribatur, T. Eiter and P. SchüllerArtificial Intelligence 300 (2021) 10356322blue14536red145green36red76 5981249(a) 3-coloring of a graph(b) SudokuFig. 1. Use of abstraction.disciplines such as Philosophy, Cognitive Science, Art, Mathematics and Artificial Intelligence, with the shared consensus of the aim to “distill the essential” [108]. Among them is the capability of abstract thinking, which is achieved by removing irrelevant details and identifying the “essence” of a problem [71]. The notion of relevance is especially important in problem solving, as a problem may become too complex to solve if every detail is taken into account. A good strategy to solve a complex problem is to start with a coarse solution and then refine it by adding back more details. When planning a trip, for instance, one may first pick the destination and determine a coarse travel plan; fleshing out the precise details of the travel, such as taking the subway to the airport, comes later. This may be done in a hierarchy of levels of abstraction, with the lowest level containing all of the details. Another view of abstraction is the generalization aspect, which is singling out the relevant features and properties shared by objects. For example, features of an airplane such as color and cargo capacity with their possible differences may be irrelevant to the travel plan; we are (mostly) only interested in the fact that there is an airplane that takes us from Vienna to New York, say. Overall, the general aim of abstraction is to simplify the problem at hand to one that is easier to understand and deal with.For solving combinatorial problems and figuring out the key elements, humans arguably employ abstraction. In Artificial Intelligence, such problems vary from planning problems like in which order to move blocks to achieve a final configuration, to solving constraint problems such as finding an admissible coloring of the nodes of a given graph. In the latter problem, for instance, isolated nodes can be viewed as a single node and colored the same without thinking about the specific details (Fig. 1a). If a given graph is non-colorable, then we may try to find some subgraph (e.g., a clique) which causes the unsolvability, and we would not care about other nodes in the graph. Similarly with the blocks: if the labels are not important, we would disregard them when figuring out the actions. If the goal configuration cannot be achieved from the initial one, we would aim to find out the particular blocks that cause this.Notably, such disregard of detail also occurs for problems with multi-dimensional structures such as grid-cells in the well-known Sudoku problem, where a partially filled 9 × 9 board must be completed by filling in numbers 1..9 into the empty cells under constraints. If an instance is unsolvable, the reason can only be meaningfully grasped by a human by focusing on the relevant sub-regions, as looking at the whole grid is too complex. For illustration, Fig. 1b shows the sub-regions of an instance that contain the reason why no solution exists: as 6 and 7 occur in the middle column, they must appear in the sub-region below in the left column, which is unfeasible as there is only one empty cell. All these examples demonstrate abstraction abilities of humans that come naturally.Due to its important role in knowledge representation and in reasoning, abstraction has been explored in AI research early on as a useful tool for problem solving: solve a problem at hand first in an abstracted space, and then use the abstract solution as a heuristic to guide the search for a solution in the original space [70,92,106]. This approach was used in planning for speeding up the solving [64] and especially for computing heuristic functions to guide the plan search in the state space. Several abstraction methods were introduced towards this direction, especially to automatically compute abstractions that give a good heuristic [38,61,116]. However, it is well known that the success in solving a problem relies on how “good” the abstraction is. For this, theoretical approaches for defining abstractions with desired properties have been investigated [59,90]. Apart from gaining efficiency (which however may not always materialize [8,63]), abstraction forms a basis to obtain high-level explanations and an understanding of a problem. For more details on these and other related works see Section 7.3.Abstraction has been studied in other areas of AI and Computer Science as well, among them model-based diagnosis [23,89], constraint satisfaction [13,52], theorem proving [102], to name a few. Particularly fruitful were applications in model checking, which is a highly successful approach to computer aided verification [27], to tackle the state explosion problem by property preserving abstractions [26,32,82]. Furthermore, the seminal counterexample guided abstraction refinement (CEGAR) method [25] allows for automatic generation of such abstractions, by starting from an initial abstraction that over-approximates the behavior of a system to verify, and then stepwise refining the abstraction as long as needed, i.e., as long as spurious (false) counterexamples exist.Abstraction for Answer Set Programming. Answer Set Programming (ASP) [18,79] is a declarative problem solving paradigm that is rooted in knowledge representation, logic programming, and nonmonotonic reasoning. A problem is represented by a non-monotonic logic program whose answer sets (also called “stable models” [57]) correspond to the solutions of the problem. Thanks to the availability of efficient solvers and the expressiveness of the formalism, ASP has been gaining 2\fZ.G. Saribatur, T. Eiter and P. SchüllerArtificial Intelligence 300 (2021) 103563popularity for applications in many areas of AI and beyond, cf. [47–49] and references therein, from combinatorial search problems (e.g. configuration, diagnosis, planning) over system modeling (e.g., behavior of dynamic systems, beliefs and actions of agents) to knowledge-in",
            {
                "entities": [
                    [
                        205,
                        217,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 184–185 (2012) 38–77Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintImportance sampling-based estimation over AND/OR search spacesfor graphical modelsVibhav Gogate a,∗, Rina Dechter a,ba Department of Computer Science, The University of Texas at Dallas, Richardson, TX 75080, USAb Donald Bren School of Information and Computer Sciences, University of California, Irvine, CA 92697, USAa r t i c l ei n f oa b s t r a c tIt is well known that the accuracy of importance sampling can be improved by reducingthe variance of its sample mean and therefore variance reduction schemes have beenthe subject of much research. In this paper, we introduce a family of variance reductionschemes that generalize the sample mean from the conventional OR search space to theAND/OR search space for graphical models. The new AND/OR sample means allow tradingtime and space with variance. At one end is the AND/OR sample tree mean which has thesame time and space complexity as the conventional OR sample tree mean but has smallervariance. At other end is the AND/OR sample graph mean which requires more time andspace to compute but has the smallest variance. Theoretically, we show that the varianceis smaller in the AND/OR space because the AND/OR sample mean is defined over a largervirtual sample size compared with the OR sample mean. Empirically, we demonstrate thatthe AND/OR sample mean is far closer to the true mean than the OR sample mean.© 2012 Elsevier B.V. All rights reserved.Article history:Received 23 March 2010Received in revised form 24 February 2012Accepted 1 March 2012Available online 3 March 2012Keywords:Probabilistic inferenceApproximate inferenceGraphical modelsImportance samplingBayesian networksConstraint networksMarkov networksModel countingVariance reduction1. IntroductionImportance sampling [1,2] is a general scheme that can be used to approximate various weighted counting tasks definedover graphical models such as computing the probability of evidence in a Bayesian network, computing the partition func-tion of a Markov network and counting the number of solutions of a constraint network. The main idea is to transform thecounting or the summation task into an expectation using a special distribution called the proposal distribution. Then, thealgorithm generates samples from the proposal distribution and approximates the expectation by a weighted average overthe samples. The weighted average is often called the sample mean. It is well known that the accuracy of the estimate isinversely proportional to the variance of the sample mean and therefore significant research has focused on reducing itsvariance [2,3]. To this effect, in this paper, we propose a family of variance reduction schemes in the context of graphicalmodels called AND/OR importance sampling.The central idea in AND/OR importance sampling is to exploit problem decomposition introduced by the conditional in-dependencies in the graphical model. Recently, graph-based problem decompositions were introduced for systematic searchin graphical models [4,5] and captured using the notion of AND/OR search spaces [6]. The usual way of performing search isto systematically go over all possible instantiations of the variables, which can be organized in an OR search tree. In AND/ORsearch, additional AND nodes are interleaved with OR nodes to capture decomposition into conditionally independent sub-problems.* Corresponding author.E-mail addresses: vgogate@hlt.utdallas.edu (V. Gogate), dechter@ics.uci.edu (R. Dechter).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2012.03.001\fV. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7739We propose to organize the generated samples as a partial cover of a full AND/OR search tree yielding an AND/OR sampletree. Likewise, the OR sample tree is the portion of a full OR search tree that is covered by the samples. The main intuitionin moving from the OR space to the AND/OR space is that at the AND nodes, we can combine samples in independentcomponents to yield a virtual increase in the sample size. For example, if X is conditionally independent of Y given Z , wecan consider N samples of X independently from those of Y given the same value of Z , thereby yielding an effective orvirtual sample size of N 2 instead of the input N. Since the variance reduces as the number of samples increases (cf. [2,3]),the sample mean computed over the AND/OR sample tree has smaller variance than the one computed over the OR sampletree.We can take this idea a step further and look at the AND/OR search graph [6] as the target for compiling the givenset of samples. Since the AND/OR search graph captures more conditional independencies than the AND/OR search tree,its partial cover corresponding to the generated samples, yields an even larger virtual sample size. As a result, the samplemean computed over the AND/OR sample graph has smaller variance than the one computed over the AND/OR sample tree.∗) time-wise and a factor ofHowever, computing the AND/OR sample graph mean is more expensive, by a factor of O (wO (N) space wise, wbeing the treewidth and N being the number of samples. Thus, the AND/OR sample tree and graphmeans allow trading time and space with accuracy.∗We provide a thorough empirical evaluation comparing the impact of exploiting varying levels of problem decom-positions via AND/OR tree and AND/OR graph on a variety of probabilistic and deterministic benchmark networks. Ourexperiments demonstrate that the AND/OR sample tree mean is slightly better than the (conventional) OR sample treemean in terms of accuracy and that the AND/OR sample graph mean is clearly superior to the AND/OR sample tree mean.The rest of the paper is organized as follows. In the next section, we present preliminaries and background. In Section 3we define the AND/OR sample tree mean and in Section 4 we prove that it has smaller variance than the OR sample treemean. The AND/OR sample graph mean is defined in Section 5. Section 6 presents empirical results and we conclude inSection 7. The research presented in this paper is based in part on Gogate and Dechter [7,8].2. Preliminaries and backgroundWe denote variables by upper case letters (e.g., X, Y , . . .) and values of variables by lower case letters (e.g., x, y, . . .).Sets of variables are denoted by bold upper case letters (e.g., X = { X1, . . . , Xn}). We denote by D( Xi) the set of possiblevalues of Xi . D( Xi) is also called the domain of Xi . Xi = xi or simply xi when the variable is clear from the context, denotesthe assignment of xi ∈ D( Xi) to Xi while X = x (or simply x) denotes a sequence of assignments to all variables in X,namely x = ( X1 = x1, X2 = x2, . . . , Xn = xn). D(X) denotes the Cartesian product of the domains of all variables in X, namelyD(X) = D( X1) × · · · × D( Xn). We denote the projection of an assignment x to a set S ⊆ X by xS. Given an assignment y andz to the partition Y and Z of X, x = (y, z) denotes the composition of assignments to the two subsets.(cid:2)(cid:2)x∈D(X)x∈D(X) asExQ [ X] of a random variable X with respect to a distribution Q is defined as: ExQ [ X] =VarQ [ X] of X is defined as: VarQ [ X] =VarQ [ X] as Var[ X], when the identity of Q is clear from the context.· · ·x∈X. The expected valuex∈ X xQ (x). The variancex∈ X (x − ExQ [ X])2 Q (x). To simplify, we will write ExQ [ X] as Ex[ X] andx∈D(X) denotes the sum over all possible configurations of variables in X, namely,(cid:2)xn∈D( Xn). For brevity, we will abuse notation and writexi ∈D( Xi ) asx2∈D( X2)x1∈D( X1)=(cid:2)(cid:2)xi ∈ Xiand(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)We denote (discrete) functions by upper case letters (e.g. F , H , C , I , etc.), and the scope (set of arguments) of a functionF by scope(F ). Given an assignment y to a superset Y of scope(F ), we will abuse notation and write F (yscope(F )) as F (y).Definition 1 (Graphical models or Markov networks). A discrete graphical model or a Markov network denoted by G is a 3-tuple (cid:5)X, D, F(cid:6) where X = { X1, . . . , Xn} is a finite set of variables, D = {D( X1), . . . , D( Xn)} is a finite set of domains whereD( Xi) is the domain of variable Xi and F = {F 1, . . . , Fm} is a finite set of discrete-valued non-negative functions (also calledpotentials). The graphical model represents a joint distribution P G over X defined as:P G(x) = 1Zm(cid:3)i=1F i(x)where Z is a normalization constant, often called the partition function. It is given by:Z =(cid:4)m(cid:3)x∈Xi=1F i(x)We will often refer to Z as weighted counts.(1)(2)The primary queries over Markov networks are computing the partition function (or the weighted counts) and computingThe marginal probability P G ( Xi = xi) is a ratio of two weighted counts. Formally, let I xi be a Dirac-delta function withthe marginal probability P G ( Xi = xi).scope Xi , defined as follows:\f40V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77(cid:5)I xi (x) =1 if x = xi0 otherwiseThen, by definition, P G (xi) is given by:(cid:2)P G(xi) =I xi (x)P G(x) =(cid:4)x∈X(cid:6)mj=1 F j(x)x∈X I xi (x)Z(3)Notice that the numerator of Eq. (3) is the weighted counts of a graphical model obtained by augmenting G with I xi . Thusalgorithms for computing the weighted counts can be used for computing P G (xi).Each graphical model is associated with a primal graph which captures the dependencies present in the model.Definition 2 (Primal graph). The primal graph of a graphical model G = (cid:5)X, D, F(cid:6) is an undirected graph G(X, E) which hasvariables of G as its vertices and an edge between two variables that appear in the scope of a function.2.1. Bayesian and constraint networksDefinition 3 (Bayesian or belief networks). A Bayesian network is a graphical model B = (cid:5)X, D, G, P(cid:6) where G = (X, E) isa directed acyclic graph over the set of variables X. Each function P",
            {
                "entities": [
                    [
                        229,
                        242,
                        "AUTHOR"
                    ],
                    [
                        248,
                        260,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 309 (2022) 103738Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA tetrachotomy of ontology-mediated queries with a covering axiomOlga Gerasimova a, Stanislav Kikot b, Agi Kurucz c,∗Michael Zakharyaschev ea HSE University, Moscow, Russiab Institute for Information Transmission Problems, Moscow, Russiac Department of Informatics, King’s College London, UKd Steklov Mathematical Institute, Moscow, Russiae Department of Computer Science and Information Systems, Birkbeck, University of London, UK, Vladimir Podolskii d,a, a r t i c l e i n f oa b s t r a c tArticle history:Received 19 July 2020Received in revised form 4 May 2022Accepted 5 May 2022Available online 13 May 2022Keywords:Ontology-mediated queryDescription logicDatalogDisjunctive datalogFirst-order rewritabilityData complexityOur concern is the problem of efficiently determining the data complexity of answering queries mediated by description logic ontologies and constructing their optimal rewritings to standard database queries. Originated in ontology-based data access and datalog optimisation, this problem is known to be computationally very complex in general, with no explicit syntactic characterisations available. In this article, aiming to understand the fundamental roots of this difficulty, we strip the problem to the bare bones and focus on Boolean conjunctive queries mediated by a simple covering axiom stating that one class is covered by the union of two other classes. We show that, on the one hand, these rudimentary ontology-mediated queries, called disjunctive sirups (or d-sirups), capture many features and difficulties of the general case. For example, answering d-sirups is (cid:2)p2 -complete for combined complexity and can be in AC0 or L-, NL-, P-, or coNP-complete for data complexity (with the problem of recognising FO-rewritability of d-sirups being 2ExpTime-hard); some d-sirups only have exponential-size resolution proofs, some only double-exponential-size positive existential FO-rewritings and single-exponential-size nonrecursive datalog rewritings. On the other hand, we prove a few partial sufficient and necessary conditions of FO- and (symmetric/linear-) datalog rewritability of d-sirups. Our main technical result is a complete and transparent syntactic AC0/NL/P/coNP tetrachotomy of d-sirups with disjoint covering classes and a path-shaped Boolean conjunctive query. To obtain this tetrachotomy, we develop new techniques for establishing P- and coNP-hardness of answering non-Horn ontology-mediated queries as well as showing that they can be answered in NL.© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).* Corresponding author.(V. Podolskii), michael@dcs.bbk.ac.uk (M. Zakharyaschev).E-mail addresses: ogerasimova@hse.ru (O. Gerasimova), staskikotx@gmail.com (S. Kikot), agi.kurucz@kcl.ac.uk (A. Kurucz), podolskii@mi.ras.ruhttps://doi.org/10.1016/j.artint.2022.1037380004-3702/© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fO. Gerasimova, S. Kikot, A. Kurucz et al.Artificial Intelligence 309 (2022) 1037381. Introduction1.1. The ultimate questionThe general research problem we are concerned with in this article can be formulated as follows: for any given ontology-mediated query (OMQ, for short) Q = (O, q) with a description logic ontology O and a conjunctive query q,(data complexity) determine the computational complexity of answering Q over any input data instance A under the open (rewritability) reduce the task of finding certain answers to Q over any input A to the task of evaluating a conventional is then called a rewriting of the OMQ with optimal data complexity directly over A (the query Q(cid:3)(cid:3)world semantics and, if possible,database query QQ ).Ontology-based data access Answering queries mediated by a description logic (DL) ontology has been known as an im-portant reasoning problem in knowledge representation since the early 1990s [1]. The proliferation of DLs and their applications [2,3], the development of the (DL-underpinned) Web Ontology Language OWL,1 and especially the paradigm of ontology-based data access (OBDA) [4–6] (proposed in the mid 2000s and recently rebranded to the virtual knowledge graph (VKG) paradigm [7]), have made theory and practice of answering ontology-mediated queries (OMQs) a hot research area lying at the crossroads of Knowledge Representation and Reasoning, Semantic Technologies and the Semantic Web, Knowledge Graphs, and Database Theory and Technologies.In a nutshell, the idea underlying OBDA is as follows. The users of an OBDA system (such as Mastro2 or Ontop3) may assume that the data they want to query is given in the form of a directed graph whose nodes are labelled with concepts (unary predicates or classes) and whose edges are labelled with roles (binary predicates or properties)—even though, in reality, the data can be physically stored in different and possibly heterogeneous data sources—hence the moniker VKG. The concept and role labels come from an ontology, designed by a domain expert, and should be familiar to the intended users who, on the other hand, do not have to know anything about the real data sources. Apart from providing a user-friendly vocabulary for queries and a high-level conceptual view of the data, an important role of the ontology is to enrich possibly incomplete data with background knowledge. To illustrate, imagine that we are interested in the life of ‘scientists’ and would like to satisfy our curiosity by querying the data available on the Web (it may come from the universities’ databases, publishing companies, personal web pages, social networks, etc.). An ontology O about scientists, provided by an OBDA system, might contain the following ‘axioms’ (given, for readability, both as DL concept inclusions and first-order sentences):BritishScientist (cid:4) ∃ affiliatedWith.UniversityInUK∀x [BritishScientist(x) → ∃ y (affiliatedWith(x, y) ∧ UniversityInUK( y))]∃ worksOnProject (cid:4) Scientist∀x [∃ y worksOnProject(x, y) → Scientist(x)]Scientist (cid:9) ∃ affiliatedWith.UniversityInUK (cid:4) BritishScientist∀x [(Scientist(x) ∧ ∃ y (affiliatedWith(x, y) ∧ UniversityInUK( y))) → BritishScientist(x)]BritishScientist (cid:4) Brexiteer (cid:10) Remainer∀x [BritishScientist(x) → (Brexiteer(x) ∨ Remainer(x))]Now, to find, for example, British scientists, we could execute a simple OMQ Q (x) = (O, q(x)) with the query(1)(2)(3)(4)q(x) = BritishScientist(x)mediated by the ontology O. The OBDA system is expected to return the members of the concept BritishScientist that are extracted from the original datasets by ‘mappings’ (database queries connecting the data with the ontology vocabulary and virtually populating its concepts and roles) and also deduced from the data and axioms in O such as (3). It is this latter reasoning task that makes OMQ answering non-trivial and potentially intractable both in practice and from the complexity-theoretic point of view.1 https://www.w3 .org /TR /owl2 -overview/.2 https://www.obdasystems .com.3 https://ontopic .biz.2\fO. Gerasimova, S. Kikot, A. Kurucz et al.Artificial Intelligence 309 (2022) 103738Uniform approach To ensure theoretical and practical tractability, the OBDA paradigm presupposes that the users’ OMQs are reformulated—or rewritten—by the OBDA system into conventional database queries over the original data sources, which have proved to be quite efficiently evaluated by the existing database management systems. Whether or not such a rewriting is possible and into which query language naturally depends on the OMQ in question. One way to uniformlyguarantee the desired rewritability is to delimit the language for OMQ ontologies and queries. Thus, the DL-Lite family of description logics [5] and the OWL 2 QL profile4 of OWL 2 were designed so as to guarantee rewritability of all OMQs with a DL-Lite ontology and a conjunctive query (CQ) into first-order (FO) queries, that is, essentially SQL queries [8]. In complexity-theoretic terms, FO-rewritability of an OMQ means that it can be answered in LogTime uniform AC0, one of the smallest complexity classes [9]. In our example above, only axioms (1) and (2) are allowed by OWL 2 QL. Various dialects of tuple-generating dependencies (tgds), aka datalogor existential rules, that admit FO-rewritability and extend OWL 2 QLhave also been identified; see, e.g., [10–13].±Any OMQ with an EL, OWL 2 EL or HornSHIQ ontology is datalog-rewritable [14–17], and so can be answered inP—polynomial time in the size of data—using various datalog engines, say GraphDB,5 LogicBlox6 or RDFox.7 Axioms (1)–(3)are admitted by the EL syntax. On the other hand, OMQs with an ALC (a notational variant of the multimodal logicKn [18]) ontology and a CQ are in general coNP-complete [1], and so often regarded as intractable and not suitable for OBDA, though they can be rewritten to disjunctive datalog [19–21] supported by systems such as DLV8 or clasp.9 For example, coNP-complete is the OMQ ({(4)}, q1) with the CQ= ∃w, x, y, z [Brexiteer(w) ∧ hasCoAuthor(w, x) ∧ Remainer(x) ∧q1hasCoAuthor(x, y) ∧ Brexiteer( y) ∧ hasCoAuthor( y, z) ∧ Remainer(z)](see also the representation of q1 as a labelled graph below). It might be of interest to note that by making the role hasCoAuthor symmetric using, for example, the role inclusion axiomhasCoAuthor (cid:4) hasCoAuthor−∀x, y [hasCoAuthor(x, y) → hasCoAuthor( y, x)](5)we obtain the OMQ ({(4), (5)}, q1), which is rewritable to a symmetric datalog query, and so can be answered by a highly parallelisable algorithm in the complexity class L (logarithmic space).For various reasons, many existing ontologies do not comply with the restrictions imposed by the standard languages for OBDA. Notable examples include the large-sca",
            {
                "entities": [
                    [
                        200,
                        215,
                        "AUTHOR"
                    ],
                    [
                        219,
                        234,
                        "AUTHOR"
                    ],
                    [
                        238,
                        248,
                        "AUTHOR"
                    ],
                    [
                        568,
                        586,
                        "AUTHOR"
                    ],
                    [
                        252,
                        273,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ELSEVI ER Artificial Intelligence 96 (1997) 351-394 Artificial Intelligence Automated model selection for simulation based on relevance reasoning Alon Y. Levy a~1, Yumi Iwasaki bp*, Richard Fikes c*2 a AT&T Bell Laboratories, 600 Mountain Ave., Room 2A-440, Murray Hill, NJ 07974, USA b Knowledge Systems Laboratory, Stanford University, Gates Bldg. 2A, Rm. 256, Stanford, CA 94305. USA ’ Knowledge Systems Laboratory, Stanford University, Gates Bldg. 2A. Rm. 246, Stanford, CA 94305, USA Received March 1996; revised August 1997 Abstract Constructing an appropriate model is a crucial step in performing the reasoning required to pieces of knowledge the behavior of a physical and Forbus successfully answer a query about modeling approach of Falkenhainer composable construction problem Model construction of dynamic behavior over a sequence of states. The latter is significantly more difficult former since one must select model fragments without knowing exactly what will happen future statles. is provided with a library of the physical world called model fragments. The model the situation. can be considered either for static analysis of a single state or for simulation than the in the selecting appropriate model In the compositional ( 1991), a system to describe fragments situation. involves about The model construction problem about relevance described by Levy reasoning about relevance of knowledge for reasoning paper, we present a model formulation procedure based on that framework fragments efficiently model must be adequate possible. We define formally in fact generates an adequate and simplest model. @ 1997 Published by Elsevier Science B.V. in general can advantageously be formulated as a problem of that is available to the system using a general framework ( 1993) and Levy and Sagiv ( 1993). In this for selecting model to be useful, the generated time, as simple as the concepts of adequacy and simplicity and show that the algorithm for the case of simulation. For such an algorithm the given query and, at the same for answering Keywords: Model formulation; Relevance reasoning; Qualitative reasoning; Simulation * Corresponding author. Email: iwasaki@ksl.stanford.edu. ’ Email: levy@research.art.com. 2 Email: filtes@ksl.stanford.edu. 0004-3702/97/$17.00 @ 1997 Published by Elsevier Science B.V. All rights reserved. PIISOOO4~-3702(97)00056-8 \f352 A. E Levy et al. /Artificial Intelligence 96 (1997) 351-394 1. Introduction study solving problem themselves for a problem the real world is too complex instead of studying endeavor, and constructing for various because are products of our intellectual Models are the conceptual objects humans is a challenging reasons. For example, Models priate model constructed constructed models of a device being designed are constructed of the device by experimenting with the model even before the device the real thing. an appro- task in itself. Models are simplified models of the real world are in its entirety, and the behavior is actually built. There are as many possible models of a given subject of study as there are reasons is no one “true” or “correct” model since any model and the goodness of a model depends on one’s goal, i.e., a model. the question detail. for for constructing models. There is necessarily the question one wishes For a model with sufficient precision Constructing answering and studying to answer too much unnecessary to comprehend in order to understand to by constructing information requires deciding what information and, therefore, should be included and accuracy without containing it must contain enough to find an answer could be relevant such a model an abstraction in the model. to be useful, the questions Thus, model formulation can be considered about and Genesereth problem of reasoning as Subramanian for reasoning of one of them, namely Levy’s, efficient procedure, purpose of simulation useful, same time, as simple as possible. of adequacy and simplicity and simplest model. about relevance of knowledge [ 301 and Levy relevance of knowledge. as a special case of a more general for a given goal. Researchers such [ 161 have proposed general frameworks In this paper, we present an application based on the general framework, for formulating of physical devices. For any model formulation to the problem of model formulation. We propose an for the to be the given query and, at the the concepts in fact generates an adequate In later sections, we will define formally and show that the procedure a model procedure for answering the generated model must be adequate 1.1. Motivations The ability to analyze a physical system using a model of its behavior skill required of engineers. Equally that is appropriate a model a good model is much formulated. Most computational rely on the user to construct a model. less understood for one’s purpose. However, important, if not more, is the ability is an important to formulate the problem of how to build it is to assist in analysis of model behavior than that of how to analyze a model once tools intended though simulation design, The ability to formulate systems greatly by making For example, in engineering involved If a system could quickly aspect of interest, perform in a readily understandable in formulating an appropriate model would enhance the utility of such it much easier to take advantage of their analysis capabilities. tool for evaluating design alternatives is a very useful a model, performing it is not currently used as freely as it could be because of the cost the results. and interpreting the particular for analyzing of the results and produce an interpretation form, a designer could much more easily analyze design a simulation, an appropriate model formulate the simulation, \fA.Z Levy et al./Art$cial Intelligence 96 (1997) 351-394 353 Fig. 1. An example circuit: SAl is a solar array and BAI is a rechargeable battery. alternatives. during the design process, quality of the final design. Such a capability would enable designers thereby improving to make better informed decisions the efficiency of the process as well as the 1.1.1. behavior modeling as modeling paradigm modeling [7] represented by an effective system of for automatically differential equations t!hat can adequately modeled a lumped-parameter sitional approach, a is provided a 3 In a physical compo- composable pieces fragment formu- model a library model fragments. or a by selecting process. The model fragments knowledge about one aspect physical world a component given physical lates model of them. composing main advantage fragments, describing compositional modeling a phenomenon, its modularity. is much easier model than composing complete model every possible and query. existing appropriate is also easier. Furthermore, context. model fragments reused fragments can in an a system systems automatically to aid given domain, in formulating in analyzing modeling model. However, must have mechanism model will order for for appropriate for promising behaviors of wide variety is approach compositional modeling such model fragments given analysis that a Also, because all the savings achieved by using an appropriate model for analysis will not be worthwhile if the cost of the selection process for the selection algorithm to have a reasonable itself is prohibitive. time complexity, to succeed, the it 11s imperative I. 1.2. Model formulation in compositional modeling is crucial when and different perspectives an appropriate model Selecting levels of abstractions various studied. #Suppose one is analyzing is rich with can be the design of an electrical power supply consisting of the problem domain at which phenomena 3 When spatial variation is not of interest, one speaks of the system as being lumped parameter [6]. \f354 A.I: Levy et al. /Artijicial Intelligence 96 (1997) 351-394 into account phenomena of the battery. Furthermore, in Fig. 1. If one is interested in the voltage a model battery and a solar array as shown that takes and discharging in the a rechargeable level supplied by the battery over the course of a day, one would variation such as solar power generation construct and charging the model would describe those phenomena with mostly electrical properties such as currents, voltages and charge in the charging capacity of the battery level. If, instead, one is interested such over several months, one would have to take into consideration as aging, whose effect becomes observable cycles. in the details of the Yet another possibility is interested In this case, an appropriate model would consist chemical processes of representations the electrodes of the battery. is that one that cause aging. of individual only after many charge-discharge chemical processes other phenomena in the variation the electrolyte in aging and involving and While the library of model fragments may contain knowledge can also be represented by several different model fragments, of physical phenomena, most of them may be irrelevant phenomenon different ways representation made, desired Therefore, to describe and the problem temporal and quantitative in the context of compositional modeling, to be solved, the same thing based on different including accuracy, and precision. about a wide variety for any given problem. Each assumptions representing about the such things as approximations the model formulation problem is: l Select an appropriate subset of the model fragment library and appropriate instan- tiations of that subset, given: l A description of a problem situation, and l An analysis goal, A more formal statement of the problem will be given in Section 4.1. The set of instantiations a model of the situation. of the selected subset comprises As discussed above, this selection process requires one to make two types of decisions, namely: l What phenomena l How to model each of the chosen phenomena? If the model to model? includes the most detailed model fragment about every phenomenon th",
            {
                "entities": [
                    [
                        164,
                        176,
                        "AUTHOR"
                    ],
                    [
                        182,
                        195,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 163 (2005) 91–135www.elsevier.com/locate/artintOn the consistency of cardinal direction constraints ✩Spiros Skiadopoulos a,∗, Manolis Koubarakis ba Knowledge and Database Systems Laboratory, School of Electrical and Computer Engineering,National Technical University of Athens, Zographou, 157 73 Athens, Greeceb Intelligent Systems Laboratory, Department of Electronic and Computer Engineering,Technical University of Crete, Chania, 731 00 Crete, GreeceReceived 3 December 2003; accepted 18 October 2004Available online 15 December 2004AbstractWe present a formal model for qualitative spatial reasoning with cardinal directions utilizing a co-ordinate system. Then, we study the problem of checking the consistency of a set of cardinal directionconstraints. We introduce the first algorithm for this problem, prove its correctness and analyze itscomputational complexity. Utilizing the above algorithm, we prove that the consistency checking of aset of basic (i.e., non-disjunctive) cardinal direction constraints can be performed in O(n5) time. Wealso show that the consistency checking of a set of unrestricted (i.e., disjunctive and non-disjunctive)cardinal direction constraints is NP-complete. Finally, we briefly discuss an extension to the basicmodel and outline an algorithm for the consistency checking problem of this extension. 2004 Elsevier B.V. All rights reserved.Keywords: Cardinal direction relations; Spatial constraints; Consistency checking; Qualitative spatial reasoning✩ This is a greatly revised and extended version of a paper which appears in Proc. of CP-02, Lecture Notes inComput. Sci., vol. 2470, Springer, Berlin, 2002, pp. 341–355.* Corresponding author.E-mail addresses: spiros@dblab.ece.ntua.gr (S. Skiadopoulos), manolis@intelligence.tuc.gr(M. Koubarakis).URLs: http://www.dblab.ece.ntua.gr/~spiros (S. Skiadopoulos), http://www.intelligence.tuc.gr/~manolis(M. Koubarakis).0004-3702/$ – see front matter  2004 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2004.10.010\f92S. Skiadopoulos, M. Koubarakis / Artificial Intelligence 163 (2005) 91–1351. IntroductionQualitative spatial reasoning has received a lot of attention in the areas of GeographicInformation Systems [13–15], Artificial Intelligence [6,8,13,29,36–38], Databases [32] andMultimedia [44]. Qualitative spatial reasoning problems have recently been posed as con-straint satisfaction problems and solved using traditional algorithms, e.g., path-consistency[38]. One of the most important problems in this area is the identification of useful andtractable classes of spatial constraints and the study of efficient algorithms for consistencychecking, minimal network computation and so on [38]. Several kinds of useful spatialconstraints have been studied so far, e.g., topological constraints [5,6,12,13,36–38], cardi-nal direction constraints [17,25,41] and qualitative distance constraints [14,48].In this paper, we concentrate on cardinal direction constraints [17,25,32]. Cardinal di-rection constraints describe how regions of space are placed relative to one another utilizinga co-ordinate system (e.g., region a is north of region b). Currently, the model of Goyaland Egenhofer [16,17] and Skiadopoulos and Koubarakis [40,42] is one of the most ex-pressive models for qualitative reasoning with cardinal directions. The model that we willpresent in this paper is closely related to the above model but there is a significant differ-ence. The model of [16,17,40,42] basically deals with extended regions that are connectedand have connected boundaries while our approach allows regions to be disconnected andhave holes. The regions that we consider are very common in Geography, Multimedia andImage Databases [4,7,44]. For example, countries are made up of separations (islands, ex-claves, external territories) and holes (enclaves) [7].We will study the problem of checking the consistency of a given set of cardinal direc-tion constraints in our model. Checking the consistency of a set of constraints in a modelof spatial information is a fundamental problem and has received a lot of attention in theliterature [25,32,38]. Algorithms for consistency checking are of immediate use in varioussituations including:• Propagating relations and detecting inconsistencies in a given set of spatial relations[25,38].• Preprocessing spatial queries so that inconsistent queries are detected or the searchspace is pruned [31].The technical contributions of this paper can be summarized as follows:1. We present a formal model for qualitative reasoning about cardinal directions. Thismodel is related to the model of [16,17,40,42] and is currently one of the most expres-sive models for qualitative reasoning with cardinal directions. The proposed modelformally defines cardinal direction relations on extended regions that can be discon-nected and have holes. The definition of a cardinal direction relation uses two types ofconstraints: order constraints (e.g., a < b) and set-union constraints (e.g., a = a1 ∪ a2).2. We use our formal framework to study the problem of checking the consistency ofa given set of cardinal direction constraints in the proposed model. We present thefirst algorithm for this problem and prove its correctness. The algorithm is interestingand has a non-trivial step where we show how to avoid using explicitly the obvious\fS. Skiadopoulos, M. Koubarakis / Artificial Intelligence 163 (2005) 91–13593but computational costly set-union constraints resulting from the definition of cardinaldirection relations.3. We present an analysis of the computational complexity of the consistency checkingproblem for cardinal direction constraints. We show that the aforementioned problemfor a given set of basic (i.e., non-disjunctive) cardinal direction constraints in n vari-ables can be solved in O(n5) time. Moreover, we prove that checking the consistencyof a set of unrestricted (i.e., disjunctive and non-disjunctive) cardinal direction con-straints is NP-complete.4. Finally, we consider the consistency checking problem of a set of cardinal directionconstraints expressed in an interesting extension of the basic model and outline analgorithm for this task. This extension considers not only extended regions but alsopoints and lines.The rest of the paper is organized as follows. In Section 2, we survey related work.Section 3 presents the cardinal direction relations and constraints of our model. In Sec-tion 4, we discuss the consistency checking of a set of basic cardinal direction constraints(expressed in the model of Section 3) and we present the first algorithm for this task. Sec-tion 5 studies the computational complexity of the consistency checking problem of basicand unrestricted sets of cardinal directions constraints. In Section 6, we outline algorithmsfor the consistency checking for an interesting extension of the basic cardinal directionmodel that we have already completed. Finally, Section 7 offers conclusions and proposesfuture directions.2. Related workQualitative spatial reasoning forms an important part of the commonsense reasoningrequired for building successful intelligent systems [10]. Most researchers in qualitativespatial reasoning have dealt with three main classes of spatial information: topological,directional and distance. Topological constraints describe how the boundaries, the interiorsand the exteriors of two regions relate [5,6,12,36–38]. For instance, if a and b are regionsthen a includes b and a externally connects with b are topological constraints. Directional(or orientation) constraints describe where regions are placed relative to one another [1,13,15,17,25,32,40,41]. For instance, a north b and a southeast b are directional constraints.Finally, distance constraints describe the relative distance of two regions [14,48]. For in-stance, a is far from b and a is close to b are distance constraints.In this paper, we concentrate on cardinal direction constraints [17,25,32,40]. Earlierqualitative models for cardinal direction relations approximate a spatial region by a repre-sentative point (most commonly the centroid) or by a representative box (most commonlythe minimum bounding box) [14,15,20,25,29,32].Depending on the particular spatial configuration these approximations may be toocrude [16,17]. Thus, expressing direction relations on these approximations can be mis-leading and contradictory (related observations are made in [28,34,45]). For instance, withrespect to the point-based approximation Spain is northeast of Portugal. Most people wouldagree that “northeast” does not describe accurately the relation between Spain and Portu-\f94S. Skiadopoulos, M. Koubarakis / Artificial Intelligence 163 (2005) 91–135Fig. 1. Problems with point and minimum bounding box approximations.gal on a map (see Fig. 1(a)). Similar examples are very common in geography. Consideralso the direction relation between Ireland and the UK (Fig. 1(b)). Summarizing, there is ademand for the formulation of a model that expresses direction relations between extendedobjects that overcomes the limitations of the point-based and box-based approximationmodels.With the above problem in mind, Goyal and Egenhofer [16,17] and Skiadopoulos andKoubarakis [40,42] presented a model in which we can express the cardinal direction re-lation of a region a with respect to a region b, by approximating b (using its minimumbounding box) while using the exact shape of a. Informally, the above model divides thespace around the reference region b, using its minimum bounding box, into nine areas andrecords the areas where the primary region a falls into (Fig. 1(c)). This gives a directionrelation between the primary and the reference region. Relations in the above model areclearly more expressive than point and box-based models. The model of [17,40] deals withconnected regions with a connected boundary. The model that we will present in Section 3,is a variation of the original model of [17,40] th",
            {
                "entities": [
                    [
                        125,
                        144,
                        "AUTHOR"
                    ],
                    [
                        150,
                        168,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 100 ( 1998) 177-224 Artificial Intelligence Model-based average reward reinforcement learning * Prasad Tadepalli ‘,*, DoKyeong Ok b*2 ’ Department of Computer Science, Oregon State University, Corvallis, OR 97331, USA ’ Korean Army Computer Center, NonSanSi DuMaMyeon BuNamRi, i?O.Box 29, ChungNam 320-919, South Korea Received 27 September 1996; revised 15 December 1997 Abstract from (RL) Reinforcement that improve their performance in many domains, than its discounted the natural criterion is the study of programs the environment. Most RL methods optimize Learning rewards and punishments by the dis- receiving is to total reward received by an agent, while, counted optimize the average reward per time step. In this paper, we introduce a model-based Average- reward Reinforcement Learning method called H-learning and show that it converges more quickly in the domain of scheduling a simulated Automatic and robustly Guided Vehicle explores to the the unexplored parts of the state space, while always choosing greedy actions with respect current value function. We show that this “Auto-exploratory H-Learning” performs better than the previously to larger state spaces, we extend it to learn action models and reward functions in the form of dynamic Bayesian networks, and approximate are effective faster in some AGV scheduling its value function using local linear regression. We show that both of these extensions it converge in significantly and making tasks. @ 1998 Published by Elsevier Science B.V. (AGV). We also introduce a version of H-learning strategies. To scale H-learning studied exploration that automatically counterpart of H-learning requirement the space reducing Keywords: Machine networks; Linear regression; AGV scheduling learning; Reinforcement learning; Average reward; Model-based; Exploration; Bayesian author. Email: * Corresponding ’ Most of the work was done when both the authors were at Oregon State University. ’ Email: okdo@unitel.co.kr. tadepall@cs.orst.edu. 0004-3702/98/$19.00 PII SOOO4-3702(98)00002-2 @ 1998 Published by Elsevier Science B.V. All rights reserved. \f178 P: 7hdepulli. D. Ok/Art$ificial Intelligence 100 (1998) 177-224 1. Introduction receives equivalent scheduling the learner is the study of programs rewards and punishments Reinforcement Learning (EU) task by receiving that improve their perfor- from the environment. RL tasks, for many and elevator scheduling optimization as money learning of good procedures including Q-learning [ 31, optimize in automatic tasks such as job-shop learning, (ARTDP) [ 181. In other words, a reward mance at some has been quite successful including some real-world [ 1 1,44,47]. Most approaches to reinforcement and Adaptive Real-Time Dynamic Programming counted reward after one time step is considered immediately. Discounted ward can be interpreted situation run will be terminated in which we would etary aspect or the probability in which we are is the average counted the average ward action sequences one. [ 461 the total dis- that is received to a fraction of the same reward received in which re- time step. Another that the at any given time for whatever reason. However, many domains the mon- in the time scales in such domains received per time step. Even so, many people have used dis- to optimize learning total re- two such the better algorithms reason is that the discounted sequence of actions and rewards. Hence, to choose to use reinforcement of immediate in. The natural in such domains, while aiming to do this criterion that can earn is when from a state can be compared by this criterion is finite even for an infinite learning do not have either at least to optimize termination, criterion is motivated by domains is a fixed probability that it is well-suited interest there [ 21,261. One reinforcement to model interested in each reward reward like with encourages the learner factor, discounting While mathematically convenient, in domains where these isn’t a natural interpretation long-term benefits reward decreases optimization when average-reward to sacrifice for the discount for short-term gains, since the impact of an action choice on long-term exponentially time. Hence, using discounted optimization be argued optimizes to 1 [ 21,261. This raises appropriate it can if that also nearly close the question whether and when discounted RL methods are that it is appropriate the average lead to suboptimal policies. Nevertheless, reward by using a discount is what is required could to optimize discounted the average reward. to use to optimize is sufficiently factor which total reward version the short-term is an undiscounted [ 31. We compare H-learning with In this paper, we describe an Average-reward RL (ARL) method called H-learning, Programming of Adaptive Real-Time Dynamic its discounted a simulated Automatic Guided Vehicle which (ARTDP) the task of scheduling dling robot used in manufacturing. Our results show that H-learning ARTDP when icy also optimizes are different, ARTDP either fails to converge converges counterpart, ARTDP, in (AGV) , a material han- is competitive with factor) optimal pol- the average reward. When short-term and long-term optimal policies policy or factor is high. Like ARTDP, and unlike Schwartz’s R-learning is model-based, [ 381, H-learning models. We show that in the AGV scheduling domain H-learning steps than R-learning [37] and Singh’s ARL algorithms in that it learns and uses explicit action and reward in fewer and is competitive with it in CPU time. This is consistent with the (discounted with a small discount to the optimal average-reward if the discount too slowly converges \fI? Tadepdli, D. Ok/Artificiul Intelligence 100 (1998) 177-224 179 previous RL [3,28]. results on the comparisons between model-based and model-free discounted Like most other RL methods, H-learning needs exploration to find an optimal policy. including (recency-based) [45]. Other meth- random actions, preferring (IE) method of Kaelbling strategies have been studied that are least recently executed occasionally in RL, to visit states that are least visited (counter-based) of reward functions used by Koenig and Simmons under uncertainty” A number of exploration executing or executing actions ods such as the Interval Estimation the idea incorporate representation the value function of states of “optimism with high values, and gradually decreasing to explore automatically, while always executing greedy actions. We introduce a version of H-learning exploring respect converges more quickly random, counter-based, to the current value function. We show that this “Auto-exploratory H-learning” to H-learning under strategies. the unexplored parts of the state space while always taking a greedy action with to a better average reward when compared and Boltzmann recency-based, in which the value function that uses optimism under uncertainty, and has the property of automatically them, these methods encourage [ 17,201. By initializing and the action-penalty ARL methods exploration the learner is stored as a table require to scale to large state spaces. Model-based methods too much space like H-learning time and training also have the additional the reward essential problem of having to explicitly functions, which is space-intensive. To scale ARL in a more compact store their action models and it is to such domains, form. the to design in such a these networks to fully specify the domain models. the to approximate its action models and value function Dynamic Bayesian networks have been successfully used in the past to represent [ 12,351. In many cases, it is possible action models way that a small number of parameters are sufficient We extended H-learning conditional probabilities for our method but also increases the action models dominates the learning time. so that it takes the network structure as input and learns in the network. This not only reduces the space requirements in domains where learning the speed of convergence learning is piecewise reinforcement of H-learning tasks, especially the performance linear. To take advantage of this, we implemented combines with learning of Bayesian network-based that can be described by small Bayesian networks, have a uniform those with action models and reward Many reward functions the optimal value func- structure over large regions of the state space. In such domains, tion of H-learning a value function approximation method based on local linear regression. Local linear regression action models and synergistically improves tasks. Combining Auto-exploratory H-learning with action model and value function approximation to even faster convergence The rest of the paper leads learning method. as follows: Section 2 introduces Markov Deci- that form the basis for RL, and motivates Average-reward RL. Section 3 it with ARTDP and R-learning sion Problems introduces H-learning and compares ing task. Section 4 introduces Auto-exploratory H-learning, previously Bayesian networks and local linear regression value function issues, and Section 7 is a summary. in an AGV schedul- it with some the use of dynamic the action models and the respectively. Section 6 is a discussion of related work and future research in some domains, producing a very effective is organized schemes. Section 5 demonstrates in many AGV scheduling studied exploration to approximate and compares \f180 P 72uiepdii. D. Ok/Artijicitrl Irltelligencr 100 (1998) 177-224 2. Background We assume that are applicable is described by a discrete that the learner’s environment is modeled by a Markov Decision Process (MDP). An MDP set S of n states, and a discrete set of i are denoted by U(i) and actions, A. The set of actions are called admissible. The Markovian assumption means that an action u in a given state i E S results in state j with some fixed probability P;,,i (u). There is a finite immediate reward r;,,;(u) a sequence of discrete steps",
            {
                "entities": [
                    [
                        129,
                        145,
                        "AUTHOR"
                    ],
                    [
                        151,
                        162,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 87 ( 1996) 145-I 86 Artificial Intelligence Planning from second principles Jana Koehler * Depariment of Computer Science, Albert Ludwigs University, Am Flughafen 17, D-791 10 Freiburg, Germany Received June 1994; revised June 1995 Abstract Planning from second principles by reusing and modifying plans is one way of improving the efficiency of planning systems. In this paper, we study it in the general framework of deductive planning and develop a logical formalization of planning from second principles, which relies on a systematic decomposition of the planning process. Deductive inference processes with clearly defined semantics formalize each of the subtasks a second principles planner has to address. Plan modification, which comprises matching and adaptation tasks, is based on a deductive approach yielding provably correct modified plans. Description logics are introduced as query languages to plan libraries, which leads to a novel and efficient solution to the indexing problem in case-based reasoning. Apart from sequential plans, this approach enables a planner to reuse and modify complex plans containing control structures like conditionals and loops. 1. Introduction actions and tries to construct to specified preconditions. A serious Planning from first principles generates plans from “scratch”. The planner its set of available with respect is the invariable nature of the planning process over time: If a planner the same planning problem, words, it is unable processes. inspects a plan the desired goal limitation of first principles planners receives exactly In other that can be drawn from previous planning the same planning operations. to benefit from experience it will repeat exactly that achieves Approaches to planning from second principles try to overcome and modifying of generated plans. From a this limitation from scratch by reusing previously planning * E-mail: koehler@informatik.uni-freiburg.de 0004-3702/96/$15.00 Copyright @ 1996 Elsevier Science B.V. All rights reserved SSDlOOO4-3702(95)00113-1 \fcomplexity case [44], easier reasonable theory point of view, WC: cannot hope to prove efficiency gains in the worst since the reuse of’ plans comprises that are not computationally subtasks than plan generation. But to reuse existing plans in many practical applications than generating a new one. The current state of the art comprises a variety of approaches it seems to be more that tackle the problems planning, from a cognitive point of view Ccf: 1341 for a summary framework of STRIPS-based In using a deductive cl‘. ( 2 I, 28.54). framework, we present a formal approach from and frame- the retrieval of candidate plans from a librar;i as well as the problem of second principles. which makes no commitments application domains. We formalize work including plan modi$cation. the whole reuse process formalisms logical to particular planning of approaches) or in the in a unique to planning Plan modification is based on deductive modified plans. It comprises planning problem task of re$ttirzg the reused plan plan modification. we discuss plans, in order to determine inference processes that yield provably correct two subtasks: First. the task of matching an old and a new the new requirements. As a new issue in in the reuse and refitting of control structures occurring and differences. Secondly, to accommodate their similarities like case analyses and loops. which introduce qualitatively new problems. library, we propose a hybrid knowledge As for the plan the planning logic with a description languages abstraction, linking as query arc introduced use leads to well-defined oretical and practical properties of interest. to develop efficient and complete [44 1, which are guaranteed problem. logic. In this approach, description to large knowledge bases or case retrieval, and update procedures In particular, description representation formalism logics libraries. Their that possess the- logics enable us for the matching problem that solve a planning approximation algorithms to retrieve all plans from the library Finally, the formal framework allows us to prove important properties ness and completeness provides developed as an integrated part of the PHI planner for the implemented of the underlying the foundation [S]. like the correct- inference procedures. Besides this, the approach plan reuse system MRL, ’ which has been The paper is organized as follows: We begin in Section 2 with a summary of the of second principles a four-phase model as the foundation that are used by MRL. The section also contains a short introduction logical formalisms as well as a short overview of the PHI planner. Section 3 into deductive planning planning. The introduces model supports a temporal view as well as a task specific view of the second principles planning the theoretical basis process. A logical for the system MRL that is described sections. Sections 4 and 6 are in the subsequent to the inference procedures working on the plan library, while in Section 5 the devoted deductive in Section 7 we review related work and propose a systematic categorization of the various principles and design decisions underlying the main properties of MRL in the light of this categorization. second principles planners. We summarize of each phase provides is presented. Finally, to plan modification formalization approach ’ MKL stands for modification and reuse 111 logic \fJ. Koehler/Ariijicial Intelligence 87 (1996) 145-186 147 2. Formal preliminaries Deductive planning is a longstanding variant of artificial origins go back to QA3 formal plan specifications specified condition, cf. [ 37, p. 141. Usually, the form [ 181. To generate plans deductively, are performed, i.e., “to construct one proves the existence of a state in which this requires us to constructively constructive intelligence planning, whose proofs of that will meet a is true”, of the condition prove plan specifications a plan v’s0 ‘Ja 32 Q[so,a,zl where SO denotes planvariable the initial state, a is an argument or input parameter, and z_ is a representing the plan term that has to be constructed [ 371. Two properties of deductive planners are particularly interesting when studying plan- from second principles: First, plans are provably correct. Once provided with a of a particular application domain and the actions which can be to can leads to a sound plan, that solves ning correct axiomatization performed work. Preserving we ensure the planning problem at hand? this property during plan modification a deductive planner generates plans reordering or adding actions is a real challenge-how that are guaranteed in this domain, that removing, Secondly, deductive planning has been closely like if-then-else and while loops. While from its origins. Plans are viewed as programs and consequently, structures generate such complex plans, many deductive planning trol structures in plans. The retrieval and modification and loops is therefore another challenge we are going to address. related to program right synthesis they contain control it is very rare that classical planners systems are able to generate con- of plans containing conditionals A deductive planning system-like complexity this means controlling a search space of enormous For a deductive planner, in such a way that plans can be constructed system to correctly “guess” appropriate with mechanisms The difficulty of this Many sacrifice soundness by ignoring complexity. inference task led to a temporary that decide which any other classical planning search and controlling instantiations the inferences automatically. This of planvariables to certain rules apply abandonment (but not all) classical planners, which have been developed in the underlying system-is faced with is a difficult problem. logic a it involves enabling and to provide logical of deductive formulae. techniques. in the meanwhile, in order to reduce search frame or ramification problems solution that allow for an efficient Recently, deductive planning logics devised, e.g., [ 8,471. On the other hand, new ways of controlling by using planning ative representation automatically giving up completeness also play an important has seen a renaissance. On one hand, various planning to the frame problem have been carefully a deductive planner in deductive the declar- so that plans can be this implies tactics tactics have been developed, is inspired by tactical theorem proving tractability. As we will demonstrate, plan modification. in order to purchase role in implementing [ 11,24,46]. Tactics support [8,53]. The use of tactics constructed when proving the plan specification. of control knowledge the inference In practice, and guide e.g., \fto use to tllustrate planning The examples we are going are to perform plan generation and plan taken from the PHI planner. PHI has been designed recognition e.g., software systems. It provides a logic-based kernel that can be used to develop intelligent help systems supporting users of PHI is the UNIX mail domain where objects of software. A prototype like messuges and mailboxes are manipulated like read, delete, and save. from second principles language environments, tasks in command application by actions logical The system uses the so-called language formalism. The logic LLP reflects the specific requirements for planning lying planning language environments. ementary like loops and conditionals complex user actions on the level of the logical formalism. For example, statements of the application [ 8 J as the under- of command the basic actions which occur in plans are the el- control structures system to describe language. Furthermore, are available 3.4 defined operators, which allows (LLP) 2.1. The logicul luttguage,ftir pluntting LLP temporal [49] with a tempo& logic with interval-based semantics, which combines LLP is a modal logic for programs tures of c/zo~~~ logic temporal formalisms logics have been proposed as ",
            {
                "entities": [
                    [
                        100,
                        112,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 951–983Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintReasoning about cardinal directions between extended objects ✩Weiming Liu a, Xiaotong Zhang b, Sanjiang Li a,b,∗, Mingsheng Ying a,ba Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology, Sydney, Australiab State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology,Department of Computer Science and Technology, Tsinghua University, Beijing 100084, Chinaa r t i c l ei n f oa b s t r a c tArticle history:Received 6 July 2009Received in revised form 18 May 2010Accepted 18 May 2010Keywords:Qualitative spatial reasoningCardinal direction calculusConnected regionsConsistency checkingMaximal canonical solution1. IntroductionDirection relations between extended spatial objects are important commonsense knowl-edge. Recently, Goyal and Egenhofer proposed a relation model, known as the cardinaldirection calculus (CDC), for representing direction relations between connected plane re-gions. The CDC is perhaps the most expressive qualitative calculus for directional infor-mation, and has attracted increasing interest from areas such as artificial intelligence,geographical information science, and image retrieval. Given a network of CDC constraints,the consistency problem is deciding if the network is realizable by connected regions in thereal plane. This paper provides a cubic algorithm for checking the consistency of completenetworks of basic CDC constraints, and proves that reasoning with the CDC is in general anNP-complete problem. For a consistent complete network of basic CDC constraints, our al-gorithm returns a ‘canonical’ solution in cubic time. This cubic algorithm is also adapted tocheck the consistency of complete networks of basic cardinal constraints between possiblydisconnected regions.© 2010 Elsevier B.V. All rights reserved.Representing and reasoning with spatial information is of particular importance in areas such as artificial intelligence(AI), geographical information systems (GISs), robotics, computer vision, image retrieval, natural language processing, etc.While the numerical quantitative approach prevails in robotics and computer vision, it is widely acknowledged in AI andGIS that the qualitative approach is more attractive (see e.g. [6]).A predominant part of spatial information is represented by relations between spatial objects. In general, spatial relationsare classified into three categories: topological, directional, and metric (e.g. size, distance, shape, etc.). The RCC8 constraintlanguage [34] is the principal topological formalism in AI, and has been extensively investigated by many researchers (seee.g. [37,35,43,7,47,46,24,25,23]). When restricted to simple plane regions, RCC8 is equivalent to the 9-Intersection Model(9IM) [9], which is a very influential relation model in GIS.Unlike for topological relations, there are several competitive models for direction relations [10,11,2]. Most of thesemodels approximate a spatial object by a point (e.g. its centroid) or a box. This is too crude in real-world applicationssuch as describing directional information between two countries, say, Portugal and Spain. Recently, Goyal and Egenhofer[16,15] proposed a relation model, known as the cardinal direction calculus (CDC), for representing direction relationsbetween connected plane regions. In the CDC the reference object is approximated by a box, while the primary object is✩This paper is an extended version of Xiaotong Zhang, Weiming Liu, Sanjiang Li, Mingsheng Ying, Reasoning with cardinal directions: An efficientalgorithm, in: AAAI 2008, pp. 387–392.* Corresponding author at: Centre for Quantum Computation and Intelligent Systems, Faculty of Engineering and Information Technology, University ofTechnology, Sydney, P.O. Box 123, Broadway, NSW 2007, Australia.E-mail address: sanjiang.li@uts.edu.au (S. Li).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.05.006\f952W. Liu et al. / Artificial Intelligence 174 (2010) 951–983not approximated. This means that the exact geometry of the primary object could be used in the representation of thedirection. This calculus has 218 basic relations, which is quite large when compared with the RCC8 and Allen’s IntervalAlgebra [1]. Due to its expressiveness, the CDC has attracted increasing interest from areas such as AI [40,41,31], GIS [17],database [39], and image retrieval [19].One basic criterion for evaluating a spatial relation model is the proper balance between its representation expressivityand reasoning complexity. While the reasoning complexity of the point-based and the box-based model of direction relationshas been investigated in depth (see [26] and [2]), there are few works discussing the complexity of reasoning with the CDC.One central reasoning problem with the CDC (and any other qualitative calculus) is the consistency (or satisfaction) prob-lem. Other reasoning problems such as deriving new knowledge from the given information, updating the given knowledge,or finding a minimal representation can be easily transformed into the consistency problem [6]. In particular, given a com-plete network of CDC constraintsN = {v iδi j v j}ni, j=1(each δi j is a CDC relation)(1)over n spatial variables v 1, . . . , vn, the consistency problem is deciding if N is realizable by a set of n connected regions inthe real plane. The consistency problem over the CDC is an open problem. Before this work, we did not know if there areefficient algorithms deciding if a set of CDC constraints are realizable. Even worse, we did not know if this is a decidableproblem. Furthermore, we did not know how to construct a realization for a satisfiable set of CDC constraints.This paper is devoted to solving these problems. We first show each consistent CDC network has a ‘canonical’ solution(Theorem 3) and then devise a cubic algorithm for checking if a complete network of basic CDC constraints is consistent.When the network is consistent, this algorithm also generates a canonical solution. We further show that deciding theconsistency of an arbitrary network of CDC constraints is an NP-Complete problem. This implies in particular that reasoningwith the CDC is decidable.Some restricted versions of the consistency problem have been discussed in the literature. Cicerone and di Felice [3]discussed the pairwise consistency problem, which decides when a pair of basic CDC relations (δ, δ(cid:3)) is consistent, i.e. when{v 1δv 2, v 2δ(cid:3)v 1} is consistent. Skiadopoulos and Koubarakis [40] investigated the weak composition problem [7,24] of theCDC, which is closely related to the consistency problem of basic CDC networks involving only three variables.The CDC algebra is defined over connected regions. A variant of the CDC was proposed in [41], where cardinal directionsbetween possibly disconnected regions are defined in the same way. This calculus, termed the CDCd in this paper, contains511 basic relations. An O (n5) algorithm1 was proposed in [41] for checking the consistency of basic constraints in the CDCd,but the consistency problem over the CDC is still open. Recently, Navarrete et al. [31] tried to adapt the approach used in[41] to cope with connected regions, but their approach turns out to be incorrect (see Remark 3 in Section 6.1 of this paper).The remainder of this paper proceeds as follows. Section 2 recalls basic notions in qualitative spatial/temporal reasoningand introduces the well-known Interval Algebra (IA) [1]. We introduce the CDC algebra in Section 3, where the connectionbetween CDC and IA relations is established in a natural way. Section 4 introduces the notion of canonical solution of aconsistent basic CDC network. Section 5 first proposes an intuitive O (n4) algorithm for consistency checking of completebasic networks and then improves it to O (n3). In Section 6, we first show local consistency is insufficient to decide theconsistency of even basic CDC networks, and then apply our main algorithm to the pairwise consistency problem and theweak composition problem. In Section 7 we adapt the main algorithm for connected regions to solve consistency checkingin two variants of the CDC. Section 8 discusses related work on the computational properties of other qualitative directioncalculi. Conclusions are given in the last section.Codes of the main algorithm are available via http://sites.google.com/site/lisanjiang/cdc, where we also provide illustra-tions for all 757 different consistent pairs of CDC basic relations and the illustration of the weak composition of S W : Wand N E : E. Interested readers may consult that webpage for detailed proofs of some minor results that are omitted in thepresent paper.Table 1 summaries notations used in this paper.2. Qualitative calculi: Basic notions and examplesSince Allen’s Interval Algebra, the study of qualitative calculi or relation models has been a central topic in qualitativespatial and temporal reasoning. This section introduces basic notions and important examples of qualitative calculi.2.1. Basic notionsLet D be a universe of temporal or spatial or spatial-temporal entities. We use small Greek symbols for representingrelations on D. For a relation α on D and two elements x, y in D, we write (x, y) ∈ α or xα y to indicate that (x, y) is aninstance of α. For two relations α, β on D, we define the complement of α, the intersection, and the union of α and β asfollows.1 We note that this algorithm applies to any (possibly incomplete) set of basic constraints.\fW. Liu et al. / Artificial Intelligence 174 (2010) 951–983953Table 1Notations.NotationsMeaningsα, β, γ , δ, θv i , v jNa, b, cI x(a), I y (a)M(a)χχ (a)a = {ai }ni=1ιx(δ), ι y (δ)ιx(δ, γ )ρ xi jρ yi jNx, N y{Ii }nmiS(a)C(a)ci jpi j , p, p(k)aribiciB(a)i",
            {
                "entities": [
                    [
                        198,
                        209,
                        "AUTHOR"
                    ],
                    [
                        3681,
                        3692,
                        "AUTHOR"
                    ],
                    [
                        213,
                        227,
                        "AUTHOR"
                    ],
                    [
                        3665,
                        3679,
                        "AUTHOR"
                    ],
                    [
                        231,
                        242,
                        "AUTHOR"
                    ],
                    [
                        3694,
                        3705,
                        "AUTHOR"
                    ],
                    [
                        250,
                        264,
                        "AUTHOR"
                    ],
                    [
                        3707,
                        3721,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial intelligence 103 (199X) 5117 Artificial Intelligence Remote Agent: to boldly go where no AI system has gone before * Nicola Muscettola ’ , P. Pandurang Nayak 2, Barney Pell *, Brian C. Williams 3 NASA Ames Research Center, MS 269-2, Mq&tt Field, CA 94035. USA Abstract toward to work and Artificial Renewed motives fleets of robotic that takes a significant exploration in space, in particular, will play a central inspired NASA have through heterogeneous for space a virtual presence technology, intelligence the Remote Agent, a specific autonomous Intelligence these explorers with a form of computational the goal of explorers. establishing role in this Information that we call remote endeavor by endowing ayems. In this paper we describe agent architecture on-board deduction and search, and goal- based on the principles of model-based programming, commanding, directed closed-loop this future. This the unique characteristics of the spacecraft domain that require highly reliable architecture addresses and autonomous operations over long periods of time with tight deadlines, concurrent activity among integrates constraint- execution, and model-based mode based temporal planning and scheduling, system as an on-board identification for a period controller the opportunity of a week in mid 1999. The development to reassess some of AI’s conventional wisdom about the challenges of implementing embedded systems, these issues, and our often contrary experiences, tightly coupled subsystems. The Remote Agent robust multi-threaded tractable reasoning, and knowledge throughout for Deep Space One, NASA’s first New Millennium mission, the paper. 0 1998 Published by Elsevier Science B.V. and reconfiguration. The demonstration of the Remote Agent also provided representation. We discuss step toward enabling resource constraints, of the integrated is scheduled Kcyrtords: Autonomous systems; Diagnosis; Recovery; Model-based reasoning agents; Architectures; Constraint-based planning; Scheduling; Execution; Reactive Authors in alphabetical order. author. RIACS. Email: pellQptolemy.arc.nasa.gov. * Corresponding ’ Recom Technologies. Email: mus@ptolemy.arc.nasa.gov. ’ RIACS. Email: nayak@ptolemy.arc.nasa.gov. ’ Email: williams@ptolemy.arc.nasa.gov. 00043702/98/$ - see front matter 0 1998 Published by Elsevier Science B.V. All rights reserved Pll: SOOO4-3702(98)00068-X \f6 N. A4uscettola et al. /Artijicinl Intelligence 103 (1998) 547 1. Introduction imagination, The melding of space exploration particularly and robotic in its vision of the future. For example, intelligence has had an amazing hold on the science the public fiction classic “2001: A Space Odyssey” offered a future in which humankind was firmly and space-stations. At the established beyond Earth, within amply populated moon-bases through the impressive same time, intelligence was firmly established beyond humankind HAL9000 computer, created in Urbana, Illinois on January 12, 1997. In fact, January I2th, 1997 has passed without a moon base or HAL9000 computer Space Station will begin its launch However, this space station is far more modest in scope. into space this year, reaching completion in sight. The International by 2002. is far from our ambitious dreams is surprising us with a different future that is particularly this reality While exploration exploration, and for the information enabling this future: technology community for humans in space, space for robotic that will play a central role in exciting in NASA is to open the Space Frontier. When people Our vision think of rocket plumes and the space shuttle. But the future of space is in information technology. We must establish a virtual presence, in space, on planets, spacecraft. think of space, they in aircraft, and - Daniel S. Goldin, NASA Administrator, Sacramento, California, May 29, 1996. Providing a virtual human presence in the universe through the actual presence of a plethora of robotic probes requires a strong motive, mechanical means, and computational that motivate space exploration the scientific questions intelligence. We briefly consider and the mechanical means for exploring this paper on our progress intelligence computational these questions, and then focus the remainder of explorers with a form of towards endowing that we call remote agents. these mechanical The development of a remote agent under tight time constraints has forced us to re- examine, and in a few places call to question, some of Al’s conventional wisdom about the challenges of implementing systems, This topic is addressed in a variety of places throughout and representation. this paper. embedded reasoning tractable 1.1. Estublishing a virtuul presence in space is evidence, found during that suggest new possibilities Renewed motives for space exploration have recently been offered. A prime example is for life in space. The best that primitive the summer of 1996, suggesting than 3.6 billion years ago. More specifically, a series of scientific discoveries known example life might have existed on Mars more the recent discovery of extremely scientists evidence suggestive of “native microfossils, mineralogical and evidence of complex organic chemistry” or overturn and is more cost effective led the Martian meteorite AlH84001 at fine resolution, where they found features characteristic of life, to confirm that has higher performance than traditional missions. Traditional planetary missions, these findings requires a new means of exploration small bacteria on Earth, called nanobacteria, [47]. Extending a virtual presence to examine such \fN. Muscettala et al. /Art$cial Intelligencr 103 (1998) 547 Fig. 1. Planned and concept missions Return missions (courtesy of NASA Johnson Space Center); (2) cryobot and hydrobot exploration airplane (courtesy of NASA Ames Research Center). (courtesy of JPL); (3) DS3 formation to extend human virtual presence flying optical interferometer in the universe. (I) Mars Sample for Europa oceanographic (courtesy of JPL); (4) Mars solar as the Galileo Jupiter mission or the Cassini Saturn mission, have price tags in excess of a billion dollars, and ground crews ranging the entire introduced a paradigm shift within life of the mission. The Mars Pathfinder NASA towards lightweight, highly focused missions, at a tenth of the cost, and operated last by small ground summer when MPF landed on Mars and enabled [48] to become the first mobile robot to land on the surface of another planet. [ 141. The viability of this concept was vividly demonstrated from 100 to 300 personnel during the Sojourner micro-rover (MPF) mission teams Pathfinder and Sojourner demonstrate a to achieving to achieve the goals for its two month life span taxing for its small ground crew. Future Mars rovers are expected to operate the need for the development of remote agents that are able to virtual presence, but currently of more challenging missions. For example, operating Sojourner was extremely for over a year, emphasizing continuously an important mechanical means intelligence necessary interact with an uncertain environment. lack the on-board and robustly Rovers are not the only means of exploring Mars. Another is a solar airplane, under study at NASA Lewis and NASA Ames. Given the thin CO2 atmosphere innovative concept \f8 N. khscettolrr et al. /Art$cial Intelligencr 103 (1998) 5-47 feet above sea level. This height is like a terrestrial plane on Mars, a plane flying a few feet above the Martian surface the reach of all but flying more than 90000 survey Mars a Martian plane a few existing planes. Developing of the Martian climate, requires the the idiosyncrasies over long durations, while surviving development of remote agents that are able to accurately model and quickly adapt to their environment. is beyond that can autonomously A second example is the discovery of the first planet around another star, which raises the intriguing question of whether or not Earth-like planets exist elsewhere. To search for [ 161, such as Earth-like planets, NASA is developing a series of interferometric telescopes the New Millennium Deep Space Three (DS3) mission. These interferometers identify and in a star, induced by its orbiting planets. They categorize planets by measuring a wobble the are so accurate that, if pointed from California to Washington DC, they could measure three optical thickness of a single piece of paper. DS3 achieves this requirement by placing flying in tight formation up to a kilometer apart. This units on three separate spacecraft, extends tightly coordinated of multiple, the computational remote agents. to the development challenge frozen surface. A final example smooth surface and chunky In February of 1998, the Galileo mission to the idea that Europa may have subsurface oceans, hidden under a thin is the question of whether or not some form of life might exist identified beneath Europa’s ice rafts, that lend features on Europa, such as a relatively icy support this subsurface ocean is an layer. One of NASA’s most intriguing ice penetrator and a submarine, that could autonomously navigate beneath Europa’s surface. This hydrobot would need to operate autonomously within an environment Taken together, flying for developing these examples of small explorers, interferometers, including micro-rovers, provide remote agents that assist in establishing airplanes, an extraordinary formation opportunity space, on land, in the air and under the sea. called a cryobot and hydrobot, that is utterly unknown. a virtual presence and hydrobots, for exploring cryobots, concepts in 1.2. Requirements for building remote agents to enable the above missions The level of on-board autonomy necessary is unprece- is the fact that NASA will need to achieve this capability to the billion that cost under 100 million dollars, in 2-3 years, and operated by a small ground team. This ambitious goal is to be low cost, technol- probe, Deep Space One (DSl), has d",
            {
                "entities": [
                    [
                        128,
                        145,
                        "AUTHOR"
                    ],
                    [
                        172,
                        183,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 42–71www.elsevier.com/locate/artintAudiences in argumentation frameworksTrevor J.M. Bench-Capon, Sylvie Doutre, Paul E. Dunne ∗Department of Computer Science, University of Liverpool, Liverpool L69 7ZF, UKReceived 11 October 2006; received in revised form 16 October 2006; accepted 17 October 2006Available online 20 November 2006AbstractAlthough reasoning about what is the case has been the historic focus of logic, reasoning about what should be done is an equallyimportant capacity for an intelligent agent. Reasoning about what to do in a given situation—termed practical reasoning in thephilosophical literature—has important differences from reasoning about what is the case. The acceptability of an argument for anaction turns not only on what is true in the situation, but also on the values and aspirations of the agent to whom the argument isdirected. There are three distinctive features of practical reasoning: first, that practical reasoning is situated in a context, directedtowards a particular agent at a particular time; second, that since agents differ in their aspirations there is no right answer for allagents, and rational disagreement is always possible; third, that since no agent can specify the relative priority of its aspirationsoutside of a particular context, such prioritisation must be a product of practical reasoning and cannot be used as an input to it.In this paper we present a framework for practical reasoning which accommodates these three distinctive features. We use thenotion of argumentation frameworks to capture the first feature. An extended form of argumentation framework in which valuesand aspirations can be represented is used to allow divergent opinions for different audiences, and complexity results relating to theextended framework are presented. We address the third feature using a formal description of a dialogue from which preferencesover values emerge. Soundness and completeness results for these dialogues are given.© 2006 Elsevier B.V. All rights reserved.Keywords: Argumentation frameworks; Practical reasoning; Dialogue1. IntroductionReasoning about what should be done in a particular situation—termed practical reasoning in the philosophicalliterature—is carried out through a process of argumentation. Argumentation is essential because no completely com-pelling answer can be given: whereas in matters of belief, we at least should be constrained by what is actually thecase, in matters of action no such constraints apply—we can choose what we will attempt to make the case. Even anorm as universal and deep seated as thou shalt not kill is acknowledged to permit of exceptions in circumstances ofself defence and war. Thus whether arguments justifying or urging a course of action are acceptable will depend onthe aspirations and values of the agent to which they are addressed: the audience for the argument. The importance ofthe audience for arguments was recognised and advocated by Perelman [28].* Corresponding author.E-mail address: ped@csc.liv.ac.uk (P.E. Dunne).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.10.013\fT.J.M. Bench-Capon et al. / Artificial Intelligence 171 (2007) 42–7143Arguments in practical reasoning provide presumptive reasons for performing an action. These presumptive argu-ments are then subject to a process of challenge, called critical questioning in [31]. These critical questions may takethe form of other arguments, which can in turn be challenged, or may be answered by further arguments, resultingin a set of arguments constructed as the debate develops. An extension of Walton’s account of practical reasoning isgiven in [2,6], which proposes an elaborated argument scheme for practical reasoning, which incorporates the valuepromoted by acceptance of the argument, and identifies all the ways in which it can be challenged. Although mostof our discussion will treat arguments at a very abstract level, where we have need of a more particular structure forarguments, we will have this account in mind.In this paper we will propose and explore a framework for the representation and evaluation of arguments in prac-tical reasoning. Any such framework must account for some important phenomena associated with such reasoning.We will review these features in this section, and will structure the development of our framework in the remainder ofthis paper around them.First, as is clear from the brief sketch of practical reasoning above, arguments cannot be considered in isolation.Whether an argument is acceptable or not depends on whether it can withstand or counter the other arguments putforward in the debate. Once the relevant arguments have been identified, whether a given argument is acceptable willdepend on its belonging to a coherent subset of the arguments put forward which is able to defend itself against allattackers. We will call such a coherent subset a position. This notion of the acceptability of an argument deriving frommembership of a defensible position has been explored in AI through the use of argumentation frameworks [9,19],and our account will be based on a framework of this sort. Dung’s framework [19] will be recapitulated in Section 2,and then extended as the paper proceeds. The reasoning involved in constructing argumentation frameworks andidentifying positions within them is naturally modelled as a dialogue between a proponent and a critic. Dialogues forthis purpose have been proposed in [8,13,22], and we will make use of the way of exploring argument frameworks.Dialogues are discussed in Section 5.A second important feature of practical reasoning is that rational disagreement is possible, the acceptability ofan argument depending in part on the audience to which it is addressed. Within Dung’s framework it is possiblefor disagreement to be represented since argumentation frameworks may contain multiple incompatible defensiblepositions. The abstract nature of arguments, however, means that there is no information that can be used to motivatethe choice of one option over another. Searle states the need to recognise that disagreement in practical reasoningcannot be eliminated as follows [29]:Assume universally valid and accepted standards of rationality, assume perfectly rational agents operating withperfect information, and you will find that rational disagreement will still occur; because, for example, the rationalagents are likely to have different and inconsistent values and interests, each of which may be rationally acceptable.What distinguishes different audiences are their values and interests, and in order to relate the positions acceptableto a given audience to the values and interests of that audience we need a way of relating arguments to such valuesand interests. Hunter [25] makes a proposal in terms of what he calls resonance, but we will build on Value BasedArgumentation Frameworks (VAFs) proposed in [9], in which every argument is explicitly associated with a valuepromoted by its acceptance, and audiences are characterised by the relative ranking they give to these values. Wewill describe VAFs in Section 3, their properties in Section 4, and discuss the relationship between our proposal andHunter’s in Section 7.The above machinery can allow us to explain disagreement in terms of differences in the rankings of values betweendifferent audiences, but it does not allow us to explain these rankings. This brings us to the third feature of practicalreasoning for which we wish to account—that we cannot assume that the parties to a debate will come with a clearranking of values: rather these rankings appear to emerge during the course of the debate. We may quote Searle again:This answer [that we can rank values in advance] while acceptable as far as it goes [as an ex post explanation],mistakenly implies that the preferences are given prior to practical reasoning, whereas, it seems to me, they aretypically the product of practical reasoning. And since ordered preferences are typically products of practicalreason, they cannot be treated as its universal presupposition. [29]\f44T.J.M. Bench-Capon et al. / Artificial Intelligence 171 (2007) 42–71The question of how value orders emerge during debate is explored in Section 6, in which we define a dialogueprocess for evaluating the status of arguments in a VAF, and in which we show how this process can be used toconstruct positions. In the course of constructing a position, the ordering of values will be determined.Although it is not reasonable to assume that participants in a debate come with a firm value order, and so we wishto account for the emergence of such an order, neither do participants usually come to an debate with a completelyopen mind. Usually there will be some actions they are predisposed to perform, and others which they are reluctantto perform, and they will have a tendency to prefer arguments which match these predispositions. For example apolitician forming a political programme may recognise that raising taxation is electorally inexpedient and so mustexclude any arguments with the conclusion that taxes should be raised from the manifesto, while ensuring that argu-ments justifying actions bringing about core objectives are present: other arguments are acceptable in so far as theyenable this. This kind of initial intuitive response to arguments will be used to drive the construction of positions andformation of a value order. A similar technique for constructing positions on the basis of Dung’s framework has beenproposed in [11]. Because this treatment does not make use of values, however, it cannot use these reasons for actionto motivate choices, and there is no relation between the arguments which can be exploited to demand that choicesare made in a consistent and coherent manner. Our extensions to include values enable us to impose this requirementof moral consistency on the reasoners.Our overall aim is ",
            {
                "entities": [
                    [
                        132,
                        145,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 805–837www.elsevier.com/locate/artintNegotiating using rewardsSarvapali D. Ramchurn a,∗, Carles Sierra b, Lluís Godo b,Nicholas R. Jennings aa IAM Group, School of Electronics and Computer Science, University of Southampton, UKb IIIA—Artificial Intelligence Research Institute, CSIC, Bellaterra, SpainReceived 8 November 2006; received in revised form 2 April 2007; accepted 2 April 2007Available online 6 May 2007AbstractNegotiation is a fundamental interaction mechanism in multi-agent systems because it allows self-interested agents to cometo mutually beneficial agreements and partition resources efficiently and effectively. Now, in many situations, the agents need tonegotiate with one another many times and so developing strategies that are effective over repeated interactions is an importantchallenge. Against this background, a growing body of work has examined the use of Persuasive Negotiation (PN), which involvesnegotiating using rhetorical arguments (such as threats, rewards, or appeals), in trying to convince an opponent to accept a givenoffer. Such mechanisms are especially suited to repeated encounters because they allow agents to influence the outcomes of futurenegotiations, while negotiating a deal in the present one, with the aim of producing results that are beneficial to both parties. Tothis end, in this paper, we develop a comprehensive PN mechanism for repeated interactions that makes use of rewards that can beasked for or given to. Our mechanism consists of two parts. First, a novel protocol that structures the interaction by capturing thecommitments that agents incur when using rewards. Second, a new reward generation algorithm that constructs promises of rewardsin future interactions as a means of permitting agents to reach better agreements, in a shorter time, in the present encounter. We thengo on to develop a specific negotiation tactic, based on this reward generation algorithm, and show that it can achieve significantlybetter outcomes than existing benchmark tactics that do not use such inducements. Specifically, we show, via empirical evaluationin a Multi-Move Prisoners’ Dilemma setting, that our tactic can lead to a 26% improvement in the utility of deals that are madeand that 21 times fewer messages need to be exchanged in order to achieve this.© 2007 Elsevier B.V. All rights reserved.Keywords: Persuasive negotiation; Repeated negotiations; Negotiation tactics; Bargaining; Bilateral negotiation1. IntroductionNegotiation is a fundamental concept in multi-agent systems (MAS) because it enables self-interested agents tofind agreements and partition resources efficiently and effectively. In most cases, such negotiation proceeds as a seriesof offers and counter-offers [20]. These offers generally indicate the preferred outcome for the proponent and theopponent may either accept them, counter-offer a more beneficial outcome, or reject them. Now, in many cases, the* Corresponding author.E-mail addresses: sdr@ecs.soton.ac.uk (S.D. Ramchurn), sierra@iiia.csic.es (C. Sierra), godo@iiia.csic.es (L. Godo), nrj@ecs.soton.ac.uk(N.R. Jennings).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.014\f806S.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837agents involved need to negotiate with one another many times. However, such repeated encounters have rarely beendealt with in the multi-agent systems literature (see Section 7 for more details). One of the main reasons for this isthat repeated encounters require additional mechanisms and structures, over and above those required for single shotencounters, to fully take into account the repeated nature of the interaction. In particular, offers that are generatedshould not only influence the present encounter, but also future ones, so that better deals can be found in the long run[9,25]. To this end, argument-based negotiation (ABN), in which arguments are used to support offers and persuadean opponent to accept them, has been advocated as an effective means to achieve this [30,36] and, therefore, this isthe approach we explore in this paper.In more detail, ABN techniques aim to enable agents to achieve better agreements faster by allowing them toexplore a larger space of possible solutions and/or to express, update, or evolve their preferences in single or multipleshot interactions [21]. They do this by providing additional explanations that justify the offer [1], identifying othergoals satisfied by the offer that the opponent might not be aware of [31], or offering additional incentives conditionalupon the acceptance of the offer [2,22,39]. While all these approaches capture, in one way or another, the notionof persuasiveness, a number of them have focused specifically on the use of rhetorical arguments such as threats,rewards, and appeals [3,28,41,44]. To be clear, here, we categorise such argument acts as persuasive elements thataim to force, entice, or convince an opponent to accept a given offer (see Section 7 for more details). In particular,we categorise such approaches under the general term of Persuasive Negotiation (PN) to denote the fact that these tryto find additional incentives (as opposed to justifying or elaborating on the goals of an offer) to move an opponent toaccept a given offer [30,36].In order to implement a PN mechanism, it is critical that the exchanges between the negotiating agents follow agiven pattern (i.e. ensuring that agents are seen to execute what they propose and that the negotiation terminates) andthat the agents are endowed with appropriate techniques to generate such exchanges (i.e. they can evaluate offers andcounter-offers during the negotiation process). These requirements can be met through the specification of a protocolthat dictates what agents are allowed to offer or commit to execute and a reasoning mechanism that allows agentsto make sense of the offers exchanged and accordingly determine their best response [30]. Given this, we present anovel protocol and reasoning mechanism for pairs of agents to engage in PN in the context of repeated games, inwhich the participating agents have to negotiate over a number of issues many times. In particular, we focus on theexchange of rewards (as opposed to threats or appeals). We do so because rewards have a clear benefit for the agentreceiving it, and entail a direct commitment by the agent giving it, to continue a long term relationship which is likelyto be beneficial to both participating agents.1 In addition to the standard use of rewards as something that is offeredas a prize or gift, our model also allows agents to ‘ask’ for rewards in an attempt to secure better outcomes in thefuture, while conceding in the current encounter and therefore closing the deal more quickly. This latter perspective iscommon in human-to-human negotiations where one of the participants may ask for a subsequent favour in return foragreeing to concede in the current round [17,33].Being more specific still, our PN mechanism constructs possible rewards in terms of constraints on issues to benegotiated in future encounters and our protocol extends Rubinstein’s [37] alternating offers protocol to allow agentsto negotiate by exchanging arguments along with their offers (in the form of promises of future rewards or requestsfor such promises in future encounters).Example. A car seller may reward a buyer who prefers red cars with a promise (or the buyer might ask for the reward)of a discount of at least 10% (i.e. a constraint on the price the seller can propose next time) on the price of her yearlycar servicing if she agrees to buy a blue one instead at the demanded price (as the buyer’s asking price for the red caris too low for the seller). Now, if the buyer accepts, it is a better outcome for both parties; the buyer benefits becauseshe is able to make savings in future that match her preference for the red car and the seller benefits in that he reduceshis stock and obtains immediate profit.1 The use of appeals and threats poses a number of problems. For example, the use of appeals usually assumes agents implement the samedeductive mechanism (an overly constraining assumption in most cases) because appeals impact directly on an agent’s beliefs or goals whichmeans that such appeals need to adopt a commonly understood belief and goal representation [1,3,22]. Threats, in turn, tend to break relationshipsdown and are not guaranteed to be enforced, which makes them harder to assess in a negotiation encounter [19].\fS.D. Ramchurn et al. / Artificial Intelligence 171 (2007) 805–837807We believe such promises are important in repeated interactions for a number of reasons. First, agents may beable to reach an agreement faster in the present game by providing some guarantees over the outcome of subsequentgames. Thus, agents may find the current offer and the reward worth more than a counter-offer (which only delaysthe agreement and future games). Second, by involving issues from future negotiations in the present game (as inthe cost of servicing in the example above), we effectively expand the negotiation space considered and, therefore,provide more possibilities for finding (better) agreements in the long run [20]. For example, agents that value futureoutcomes more (because of their lower discount factors) than their opponent are able to obtain a higher utility infuture games, while the opponent who values immediate rewards can take them more quickly. Thirdly, if the rewardguarantees the range of possible outcomes in the next game, the corresponding negotiation space is constrained bythe reward, which should reduce the number of offers exchanged to search the space and hence the time elapsedbefore an agreement is reached. Continuing the above example, the buyer starts off with an advantage next time shewants to negotiate the price to service her car and she may then not need to negotiate for long to get a rea",
            {
                "entities": [
                    [
                        124,
                        137,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 799–823Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintReasoning under inconsistency: A forgetting-based approach ✩Jérôme Lang a, Pierre Marquis b,∗a LAMSADE-CNRS / Université Paris-Dauphine, Place du Maréchal de Lattre de Tassigny, 75775 Paris Cedex 16, Franceb CRIL-CNRS / Université d’Artois, rue Jean Souvraz, S.P. 18, 62307 Lens Cedex, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 15 September 2009Received in revised form 23 April 2010Accepted 23 April 2010Available online 29 April 2010Keywords:Knowledge representationReasoning under inconsistencyForgetting1. IntroductionIn this paper, a fairly general framework for reasoning from inconsistent propositionalbases is defined. Variable forgetting is used as a basic operation for weakening pieces ofinformation so as to restore consistency. The key notion is that of recoveries, which are setsof variables whose forgetting enables restoring consistency. Several criteria for definingpreferred recoveries are proposed, depending on whether the focus is laid on the relativerelevance of the atoms or the relative entrenchment of the pieces of information (orboth). Our framework encompasses several previous approaches as specific cases, includingreasoning from preferred consistent subsets, and some forms of information merging.Interestingly, the gain in flexibility and generality offered by our framework does not implya complexity shift compared to these specific cases.© 2010 Elsevier B.V. All rights reserved.Reasoning from inconsistent pieces of information, represented as logical formulas, is an important issue in ArtificialIntelligence. Thus, there are at least two very different contexts where inconsistent sets of formulas have to be dealt with.The first one is when the formulas express beliefs about the real world, that may stem from different sources. In thiscase, inconsistency means that some of the formulas are just wrong. The second one is when the input formulas expresspreferences (or goals, desires) expressed by different agents (or by a single agent according to different criteria). In this case,inconsistency does not mean that anything is incorrect, but that some preferences will not be able to be fulfilled. Even ifthe nature of the problems is very different whether we are in one context or the other one, many notions and techniquesthat can be employed to reason from inconsistent sets of formulas are similar.Whatever the nature of the information represented, classical reasoning is inadequate to derive significant consequencesfrom inconsistent formulas, since it trivializes in this situation (ex falso quodlibet sequitur). This calls for other inference rela-tions which avoid the trivialization problem (namely, paraconsistent inference relations) but there is no general consensusabout what such relations should be. Actually, both the complexity of the problem of reasoning under inconsistency and itssignificance are reflected by the number of approaches that have been developed for decades and can be found in the liter-ature under various names, like paraconsistent logics, belief revision, argumentative inference, information merging, modelfitting, arbitration, knowledge integration, knowledge purification, etc. (see [7,5] for surveys).Corresponding to these approaches, many different mechanisms to avoid trivialization can be exploited. A first taxonomyallows for distinguishing between active approaches, where inconsistency is removed by identifying wrong pieces of beliefthrough knowledge-gathering actions (see e.g. [34,36,28]) or by the group of agents agreeing on some goals to be given✩This is an extended and revised version of a paper that appeared in the Proceedings of the 8th International Conference on Knowledge Representationand Reasoning (KR’02), 2002, pp. 239–250.* Corresponding author.E-mail addresses: lang@lamsade.dauphine.fr (J. Lang), marquis@cril.univ-artois.fr (P. Marquis).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.023\f800J. Lang, P. Marquis / Artificial Intelligence 174 (2010) 799–823up after a negotiation process, from passive approaches, where inconsistency is dealt with. In this latter case, trivializationis avoided by weakening the set of consequences that can be derived from the given base (a set of formulas). In thepropositional case, this can be achieved by two means:(1) By weakening the consequence relation of classical logic while keeping the base intact. Such an approximation by below ofclassical entailment can be achieved, which typically leads to paraconsistent logics.(2) By weakening the input base while keeping classical entailment. The pieces of information from the initial base are weakenedsuch that their conjunction becomes consistent. This technique is at work in so-called coherence-based approaches toparaconsistent inference (see e.g. [46,24,25,11,3,41,44,4] for some of the early references), where weakening the inputbase consists in inhibiting some of the pieces of information it contains (by removing them). It is also at work in beliefmerging (see e.g. [37,47,31,40] for some of the early references). Belief merging, and especially distance-based mergingconsists in weakening the pieces of information by dilating them: the piece of information φ, instead of expressing thatthe real world is for sure among the models of φ, now expresses that it is close to be a model of φ (the further a worldω from the models of φ, the less plausible it is that ω is the real world).The above dichotomy between (1) and (2) is reminiscent of the dichotomy between actual and potential contradictions, asdiscussed in [7,5]. Actual contradictions tolerate inconsistency by reasoning with a set of inconsistent statements, whereaspotential contradictions are prevented from arising by putting individually consistent yet jointly inconsistent informationtogether.In the rest of this paper we deal with potential inconsistencies and focus on the class of approaches, consisting inweakening the input base. While existing weakening-based approaches work well on some families of problems, there aretypical examples that they fail to handle in a satisfactory way (see Section 6 for a detailed discussion), the reason being thatwhile some of these approaches take account for the relative importance of pieces of information (or of the correspondingsources), they do not handle the relative importance of atoms in the problem at hand. This is problematic in many situationswhere some atoms are less central than others, especially when some atoms are meaningful only in the presence of others.For instance, it makes little sense to reason about whether John’s car is grey if there is some strong conflict about whetherJohn actually has a car. Or, in a preference merging context, suppose that a group of co-owners of a residence tries to agreeabout whether a tennis court or a swimming pool should be built: if there is no agreement about whether the swimmingpool is to be constructed, any preference concerning its colour must be (at least temporarily) ignored (this would not bethe case for its size, however, because it influences its price in a dramatic way).More generally, it is sometimes the case that ignoring a small set of propositional atoms of the formulas from an incon-sistent set renders it consistent. When reasoning from inconsistent beliefs, this allows for giving some useful informationabout the other atoms (those that are not forgotten); information about other atoms can be processed further (for instancethrough knowledge-gathering actions) if these atoms are important enough. When trying to reach a common decision froman inconsistent set of preferences, ignoring small sets of atoms allows for making a decision about all other atoms; thedecision about the few remaining atoms can then take place after a negotiation process among agents.In the following, we define a framework for reasoning from inconsistent propositional bases, using forgetting [10,39,33]as a basic operation for weakening formulas. Belief (or preference) bases are viewed as (finite) vectors of propositional for-mulas, conjunctively interpreted. Without loss of generality, each formula is assumed to be issued from a specific source ofinformation (or a specific agent). Forgetting a set X of atoms in a formula consists in replacing it by its logically strongestconsequence which is independent of X , in the sense that it is equivalent to a formula in which no atom from X occurs [33].The key notion of our approach is that of recoveries, which are sets of atoms whose forgetting enables restoring consistency.The intuition of this simple principle is that if a collection of pieces of information is jointly inconsistent, weakening it byignoring some atoms (for instance the least important ones) can help restoring consistency and derive reasonable conclu-sions. Several criteria for defining preferred recoveries are proposed, depending on whether the focus is laid on the relativerelevance of the atoms or the relative entrenchment of the pieces of information (or both).Our contributions are composed of the following models and results. We first define a general model for using variableforgetting in order to reason under inconsistency. We show that our model is general enough to encompass several classesof paraconsistent inference relations, including reasoning from preferred consistent subbases (Propositions 4.1 and 4.2) andvarious types of belief merging (Propositions 4.3, 4.4 and 4.5). Our framework does not only recover known approaches asspecific cases (which would make its interest rather limited) but it allows for new families of paraconsistent inferences,including homogeneous inferences, where propositional variables have to be forgotten in a homogeneous way from thedifferent sources, and abstraction-based inferences, where the most specific variables a",
            {
                "entities": [
                    [
                        211,
                        225,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 170 (2006) 59–113www.elsevier.com/locate/artintRobot introspection through learned hiddenMarkov modelsMaria Fox a,∗, Malik Ghallab b, Guillaume Infantes b, Derek Long aa Department of Computer and Information Sciences, University of Strathclyde, 26 Richmond Street,Glasgow, G1 1XH, UKb LAAS-CNRS, 7 Avenue du Colonel Roche, 31500 Toulouse, FranceReceived 1 December 2004; accepted 6 May 2005Available online 1 September 2005AbstractIn this paper we describe a machine learning approach for acquiring a model of a robot behaviourfrom raw sensor data. We are interested in automating the acquisition of behavioural models toprovide a robot with an introspective capability. We assume that the behaviour of a robot in achievinga task can be modelled as a finite stochastic state transition system.Beginning with data recorded by a robot in the execution of a task, we use unsupervised learningtechniques to estimate a hidden Markov model (HMM) that can be used both for predicting andexplaining the behaviour of the robot in subsequent executions of the task. We demonstrate that it isfeasible to automate the entire process of learning a high quality HMM from the data recorded bythe robot during execution of its task.The learned HMM can be used both for monitoring and controlling the behaviour of the robot.The ultimate purpose of our work is to learn models for the full set of tasks associated with a givenproblem domain, and to integrate these models with a generative task planner. We want to show thatthese models can be used successfully in controlling the execution of a plan. However, this paperdoes not develop the planning and control aspects of our work, focussing instead on the learningmethodology and the evaluation of a learned model. The essential property of the models we seekto construct is that the most probable trajectory through a model, given the observations made bythe robot, accurately diagnoses, or explains, the behaviour that the robot actually performed whenmaking these observations. In the work reported here we consider a navigation task. We explain* Corresponding author.E-mail address: maria.fox@cis.strath.ac.uk (M. Fox).0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.05.007\f60M. Fox et al. / Artificial Intelligence 170 (2006) 59–113the learning process, the experimental setup and the structure of the resulting learned behaviouralmodels. We then evaluate the extent to which explanations proposed by the learned models accordwith a human observer’s interpretation of the behaviour exhibited by the robot in its execution of thetask. 2005 Elsevier B.V. All rights reserved.Keywords: Stochastic learning; Hidden Markov models; Robot behaviour1. IntroductionThe goal of the work described in this paper is to automate the process of learning howa given robot executes a task in a particular class of dynamic environments. We want tolearn an abstract model of the behaviour of the robot when executing its task solely on thebasis of the sensed data that the robot records when performing the task. Having learnedan execution model of this task we want to use the model to reliably predict and explainthe behaviour of the robot carrying out that same task in any other environment belongingto the class. This paper describes how we have approached this goal in the context of anindoor navigation task, and how successful we have been in learning a reliable behaviouralmodel.1.1. MotivationThe work presented here illustrates that it can be advantageous to approach a complexartifact, such as an autonomous robot, not from the usual viewpoint in robotics of thedesigner, but from the observer’s point of view. Instead of the typical engineering questionof “how do I design my robot to behave according to some specifications”, here we addressthe different issue of “how do I model the observed behaviour of my robot”, ignoring, inthis process, the intricacy of its design.It may sound strange for a roboticist to engage in observing and modelling what a ro-bot is doing, since this should be inferrable from the roboticist’s own design. However,a modular design of a complex artifact develops only local models which are combinedon the basis of some composition principle of these models; it seldom provides globalbehaviour models. The design usually relies on some reasonable assumptions about theenvironment and does not model explicitly a changing, open-ended environment with hu-man interaction. Hence, a precise observation model of a robot behaviour in a varying andopen environment can be essential for understanding how the robot operates within thatenvironment.We are proposing in this paper a machine learning approach for acquiring a particu-lar class of behaviour models of a robot. The main motivation for this work is to buildmodels of robot task execution that are intermediate between the high level representationsused in deliberative reasoning, such as planning, and the low level representations usedin sensory-motor functions. A high-level action model, such as a collection of planningoperators with abstract preconditions and effects, is certainly needed in high level missionplanning. However, it is of limited use in monitoring and controlling the execution of plans.\fM. Fox et al. / Artificial Intelligence 170 (2006) 59–11361These functions need a more detailed model of how an action breaks down, depending onthe environment and the context, into low-level concurrent and sequential sensory-motorprimitives, and how these primitives are controlled. On the other hand, the representationsused for designing and modelling sensory-motor functions are necessarily too detailed.They are far too complex to be dealt with at a planning level, or even for execution moni-toring. The latter requires intermediate level models, either hand-programmed, learned, orrefined through specification and learning.Other authors have considered how intermediate level descriptions of task executionmight be used for designing a robot, i.e., how the corresponding models might be encodedand exploited within a plan execution framework. We are not concerned with programmingthe low level control of the robot but with providing the means by which a robot can intro-spect about the development of its behaviour in the execution of a task. We rely on hiddenMarkov models (HMMs) [25] as the intermediate level representation of this behaviour.Since these models are built empirically, they take into account the dynamics and uncer-tainty of the real execution environment. The resulting behavioural models provide a wayin which the controller can reason about the robot behaviour in the context of executing atask.Our focus here is not on learning topological or metric maps for robot navigation. Oth-ers have considered this problem in depth [1–4] and shown that navigation with respectto a given environment can be dynamically improved as the robot interacts with its envi-ronment. The use of stochastic learning techniques to improve robot navigation in a givenenvironment is therefore quite well-understood. We are concerned with learning abstractmodels of how a robot performs a compound task, whatever that task might be. Navigationis an example of such a compound task.1.2. ApproachOur objective is to be able to predict and explain the robot’s behaviour as it undertakes acompound task in the uncertain real world. In reality the robot passes through a number ofabstract behavioural states, some of which can be distinguished and identified by a humanobserver. For example, when picking up an object in its grippers a robot might be in thestate of positioning with respect to the object, approaching it, grasping it, knocking into it,lifting it, and so on.To illustrate the kind of model we are interested in learning, Fig. 1 shows a high levelstate transition model of a pickup task (this is an artificially simplified example that wasnot learned from real data). Time is abstracted out of the model and it is assumed that amonitoring process tracks how often the robot revisits the same state.It can be seen that, according to the model, the probability of knocking into the objectis 0.2 when the robot is positioning itself and when it is in the approaching state, havingpositioned itself ready to grasp the object. The probability of looping on the positioningstate is high, suggesting that the robot often fumbles to get into a good grasping position.The trajectories through this model that are actually followed by the robot might revisit thepositioning state multiply often and it might be that the state of knocking into the objectis entered most frequently when this is the case. Using the HMM to identify the mostprobable trajectory leading out of the current state provides a monitoring system with a\f62M. Fox et al. / Artificial Intelligence 170 (2006) 59–113Fig. 1. The compound task of picking up an object.powerful ability to determine the most likely outcome of the robot’s current behaviour. InSection 7 we discuss how the structure of the HMM can be exploited by such a monitoringsystem.The behavioural states of the model are hidden, because they cannot be sensed directlyby the robot. The robot is equipped with noisy sensors from which it can obtain only anestimate of its state. A hidden Markov model (HMM) represents the association betweenthese noisy sensor readings and the possible behavioural states of the system, as well as theprobabilities of transitioning between pairs of states. The HMM is therefore ideally suitedto our objectives. Our approach is to learn a HMM that relates the sensor readings made bythe robot to the hidden real states it traverses when executing its task, in order to equip therobot with the capacity to monitor its progress during subsequent executions of the sametask.Our work makes several innovations. First, we address the problem of learning the struc-ture as well as the parameters of the HMM, using a structural le",
            {
                "entities": [
                    [
                        126,
                        135,
                        "AUTHOR"
                    ],
                    [
                        141,
                        154,
                        "AUTHOR"
                    ],
                    [
                        158,
                        176,
                        "AUTHOR"
                    ],
                    [
                        180,
                        190,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 1540–1569Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintLearning complex action models with quantifiers and logical implicationsHankz Hankui Zhuo a,b, Qiang Yang b,∗, Derek Hao Hu b, Lei Li aa Department of Computer Science, Sun Yat-sen University, Guangzhou, China, 510275b Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clearwater Bay, Kowloon, Hong Konga r t i c l ei n f oa b s t r a c tArticle history:Received 2 January 2010Received in revised form 4 September 2010Accepted 5 September 2010Available online 29 September 2010Keywords:Action model learningMachine learningKnowledge engineeringAutomated planningAutomated planning requires action models described using languages such as the PlanningDomain Definition Language (PDDL) as input, but building action models from scratch is avery difficult and time-consuming task, even for experts. This is because it is difficult toformally describe all conditions and changes, reflected in the preconditions and effects ofaction models. In the past, there have been algorithms that can automatically learn simpleaction models from plan traces. However, there are many cases in the real world wherewe need more complicated expressions based on universal and existential quantifiers,implications in action models to precisely describe the underlyingas well as logicalmechanisms of the actions. Such complex action models cannot be learned using manyprevious algorithms. In this article, we present a novel algorithm called LAMP (LearningAction Models from Plan traces), to learn action models with quantifiers and logicalimplications from a set of observed plan traces with only partially observed intermediatestate information. The LAMP algorithm generates candidate formulas that are passed to aMarkov Logic Network (MLN) for selecting the most likely subsets of candidate formulas.The selected subset of formulas is then transformed into learned action models, which canthen be tweaked by domain experts to arrive at the final models. We evaluate our approachin four planning domains to demonstrate that LAMP is effective in learning complex actionmodels. We also analyze the human effort saved by using LAMP in helping to create actionmodels through a user study. Finally, we apply LAMP to a real-world application domain forsoftware requirement engineering to help the engineers acquire software requirements andshow that LAMP can indeed help experts a great deal in real-world knowledge-engineeringapplications.© 2010 Elsevier B.V. All rights reserved.1. IntroductionAutomated planning systems achieve goals by producing sequences of actions from the given action models that areprovided as input [14]. A typical way to describe the action models is to use action languages such as the Planning DomainDefinition Language (PDDL) [13,11,14] in which one can specify the precedence and consequence of actions. A traditionalway of building action models is to ask domain experts to analyze a task domain and manually construct a domain de-scription that includes a set of complete action models. Planning systems can then proceed to generate action sequences toachieve goals.However, it is very difficult and time-consuming to manually build action models in a given task domain, even forexperts. This is a typical problem of the knowledge-engineering bottleneck, where experts often find it difficult to articulatetheir experiences formally and completely. Because of this, researchers have started to explore ways to reduce the human* Corresponding author.E-mail addresses: zhuohank@mail.sysu.edu.cn (H.H. Zhuo), qyang@cse.ust.hk (Q. Yang), derekhh@cse.ust.hk (D.H. Hu), lnslilei@mail.sysu.edu.cn (L. Li).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.09.007\fH.H. Zhuo et al. / Artificial Intelligence 174 (2010) 1540–15691541effort of building action models by learning from observed examples or plan traces. Some researchers have developedmethods to learn action models from complete state information before and after an action in some example plan traces [4,15,31,50]. Others, such as Yang et al. [51,39] have proposed to learn action models from plan examples with only incompletestate information. Yang et al. [51,52] have developed an approach known as Action Relation Modeling System (ARMS) to learnaction models in a STRIPS (STanford Research Institute Problem Solver) [10] representation using a weighted MAXSAT-based(Maximum Satisfiability) approach. Shahaf et al. [39] have proposed an algorithm called Simultaneous Learning and Filtering(SLAF) to learn more expressive action models using consistency-based algorithms.Despite the success of these learning systems, in the real world, there are many applications where actions should beexpressed using a more expressive representation, namely, quantifiers and logical implications. For instance, consider thecase where there are different cases in a briefcase1 planning domain, such that a briefcase should not be moved to a placewhere there is another briefcase with the same color. We can model the action move in PDDL as follows.2action: move(?c1 - case ?l1 ?l2 - location)pre:(:and (at ?c1 ?l1)(forall ?c2 - case (imply (samecolor ?c2 ?c1)(not (at ?c2 ?l2)))))effect:(:and (at ?c1 ?l2) (not (at ?c1 ?l1)))That is, if we want to move the case c1 from the location l1 to l2, c1 should be at l1 first, and every other case c2 whosecolor is the same with c1 should not be at l2. After the action move, c1 will be at l2 instead of at l1. Likewise, consider apilot could not fly to a place where there are enemies. We can model the action model fly as follows.action:pre:fly(?p1 - pilot ?l1 ?l2 - location)(:and (at ?p1 ?l1)(forall ?p2 - person (imply (enemy ?p2 ?p1)(not (at ?p2 ?l2)))))effect:(:and (at ?p1 ?l2) (not (at ?p1 ?l1)))We can see that in these examples, we need universal quantifiers as well as logical implications in the precondition part ofthe action to precisely represent this action and compress the action model in a compact form.As another example, consider a driver who intends to drive a train. Before he can start, he should make sure all thepassengers have gotten on the train. After that, if there is a seat vacant, then he can start to drive the train. We representthis drive-train action model in PDDL as follows.action:pre:effect:drive-train(?d - driver ?t - train)(free ?d) (forall ?p - passenger (in ?p ?t))(:and (when (exist ?s - seat (vacant ?s)) (available ?t))(driving ?d ?t)(not (free ?d)))That is, if a driver ?d makes sure all the passengers ?p are in the train ?t and is free at that time, then he can drive thetrain ?t. Furthermore, if there is a seat ?s vacant, as a consequence of this action drive-train, the train will be set as availableto show that more passengers can take this train. Besides, the driver ?d will be in the state of driving the train, i.e., (driving?d ?t), and not free. Such an action model needs a universal quantifier in describing its preconditions and an existentialquantifier for the condition “(exist ?s - seat (vacant ?s))” of the conditional effect “(when (exist ?s - seat (vacant ?s))(available?t))”. More examples that require the use of quantifiers and logical implications can be found in many action models inrecent International Planning Competitions, such as the domains in IPC-53: trucks, openstacks, etc. These complex actionmodels can be represented by PDDL, but cannot be learned by existing algorithms proposed for action model learning.Our objective is to develop a new algorithm for learning complex action models with quantifiers (including conditionaleffects) and logical implications, from a collection of given example plan traces. The input of our algorithm includes: (1)a set of observed plan traces with partially observed intermediate state information between actions; (2) a list of actionheadings, each of which is composed of an action name and a list of parameters, but is not provided with preconditions oreffects; (3) a list of predicates along with their corresponding parameters. Our algorithm is called LAMP (Learn Action Modelsfrom Plan traces), which outputs a set of action models with quantifiers and implications. These action models ‘summarizes’the plan traces as much as possible, and can be used by domain experts, who need to spend only a small amount of time, inrevising parts of the action models that are incorrect or incomplete, before finalizing the action models for planning usage.Compared to many previous approaches, our main contributions are: (1) LAMP can learn quantifiers that conform to thePDDL definition [13,11], where the latter article shows that action models in PDDL can have quantifiers. (2) LAMP can learnaction models with implications as preconditions, which improves the expressiveness of learned action models. We require1 http://www.informatik.uni-freiburg.de/~koehler/ipp/pddl-domains.tar.gz.2 A symbol with a prefix “?” suggests that the symbol is a variable; e.g. “?c1” suggests that “c1” is a variable that can take on certain constants as values.3 http://zeus.ing.unibs.it/ipc-5/domains.html.\f1542H.H. Zhuo et al. / Artificial Intelligence 174 (2010) 1540–1569that existential quantifiers can only be used to quantify the left-hand side of a conditional effect, which is consistent withthe constraints used in PDDL1.2 and PDDL2.1 [13,11]. (3) Similar to some previous systems such as the ARMS system [51,52], LAMP can learn action models from plan traces with partially observed intermediate state information. This is importantbecause in many real world situations, what we can record between two actions in a plan trace is likely to be incomplete;e.g., when using only a small number of sensors, we can record a subset of what happens after each action is executed.In such a case, the number of sensors cannot cover all possible new information sources and ",
            {
                "entities": [
                    [
                        210,
                        227,
                        "AUTHOR"
                    ],
                    [
                        233,
                        243,
                        "AUTHOR"
                    ],
                    [
                        249,
                        261,
                        "AUTHOR"
                    ],
                    [
                        265,
                        271,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 175 (2011) 79–119Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintApproximation of action theories and its applicationto conformant planningPhan Huy Tu a, Tran Cao Son b,∗, Michael Gelfond c, A. Ricardo Morales ca Microsoft Corporation, 1 Microsoft Way, Redmond, WA 98052, USAb Computer Science Department, New Mexico State University, Las Cruces, NM 88003, USAc Computer Science Department, Texas Tech University, Lubbock, TX 79409, USAa r t i c l ei n f oa b s t r a c tArticle history:Available online 3 April 2010Keywords:Reasoning about action and changeKnowledge representationPlanningIncomplete informationAnswer set programmingThis paper describes our methodology for building conformant planners, which is basedon recent advances in the theory of action and change and answer set programming. Thedevelopment of a planner for a given dynamic domain starts with encoding the knowledgeabout fluents and actions of the domain as an action theory D of some action language.Our choice in this paper is AL – an action language with dynamic and static causal lawsand executability conditions. An action theory D of AL defines a transition diagram T (D)(cid:4)(cid:5) belongs to T (D)containing all the possible trajectories of the domain. A transition (cid:3)s, a, s(cid:4)iff the execution of the action a in the state s may move the domain to the state s.The second step in the planner development consists in finding a deterministic transitiondiagram T lp(D) such that nodes of T lp(D) are partial states of D, its arcs are labeled byactions, and a path in T lp(D) from an initial partial state δ0 to a partial state satisfying thein T (D). The transition diagramgoal δ f corresponds to a conformant plan for δ0 and δ fT lp(D) is called an ‘approximation’ of T (D). We claim that a concise description of anapproximation of T (D) can often be given by a logic program π (D) under the answersets semantics. Moreover, complex initial situations and constraints on plans can be alsoexpressed by logic programming rules and included in π (D). If this is possible then theproblem of finding a parallel or sequential conformant plan can be reduced to computinganswer sets of π (D). This can be done by general purpose answer set solvers. If plans aresequential and long then this method can be too time consuming. In this case, π (D) isused as a specification for a procedural graph searching conformant planning algorithm.The paper illustrates this methodology by building several conformant planners whichwork for domains with complex relationship between the fluents. The efficiency of theplanners is experimentally evaluated on a number of new and old benchmarks. In additionwe show that for a subclass of action theories of AL our planners are complete, i.e., if inT lp(D) we cannot get from δ0 to a state satisfying the goal δ f then there is no conformantplan for δ0 and δ fin T (D).© 2010 Elsevier B.V. All rights reserved.1. IntroductionA conformant planner is a program that generates a sequence of actions, which achieves a goal from any possible initialstate of the world, given the information about the initial state and the possible effects of actions. Such sequences arenormally referred to as conformant plans. In this paper we describe our methodology for the design and implementation of* Corresponding author.E-mail addresses: tuphan@microsoft.com (P.H. Tu), tson@cs.nmsu.edu (T.C. Son), mgelfond@cs.ttu.edu (M. Gelfond), ricardo@cs.ttu.edu (A.R. Morales).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.007\f80P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119conformant planners. The methodology is rooted in the ideas of declarative programming [42] and utilizes recent advancesin answer set programming and the theory of action and change. This allows the designer to guarantee a substantiallyhigher degree of trust in the planners’ correctness, as well as a greater degree of elaboration tolerance [43].The design of a declarative solution of a problem P normally involves the selection of a logical language capable ofrepresenting knowledge relevant to P . We base our methodology on representing such knowledge in action languages– formal models of parts of natural language used for reasoning about actions and their effects. A theory in an actionlanguage (often called an action description) is used to succinctly describe the collection of all possible trajectories of agiven dynamic domain. Usually this is done by defining the transition diagram, T (D), of an action description D. The statesof T (D) correspond to possible physical states of the domain represented by D. Arcs of T (D) are labeled by actions.A transition (cid:3)s, a, s. In someaction languages, actions are elementary (or atomic). In some others, an action a is viewed as a finite non-empty collectionof elementary actions. Intuitively, execution of an action a = {e1, . . . , en}, where the ei ’s are elementary actions, correspondsto the simultaneous execution of every ei ∈ a.(cid:4)(cid:5) ∈ T (D) if the execution of the action a in the state s may move the domain to the state s(cid:4)There are by now a large number of action languages (see for instance [8,24,25,32,41,67]) capturing different aspectsof dynamic domains. Our choice in this paper is AL [8] – an action language with dynamic causal laws describing directeffects of actions, impossibility conditions stating the conditions under which an action cannot be executed, and static causallaws (a.k.a. state constraints) describing static relations between fluents. For example the statement “putting a block A on topof block B causes A to be on top of B” can be viewed as a dynamic causal law describing the direct effect of action put( A, B).The statement “a block A cannot be put on B if there is a block located on A or on B” represents an impossibility condition.The statement “block A is above block C if A is on C or it is on B and B is above C” is an example of a (recursive) static causallaw. Note that static causal laws can cause actions to have indirect effects. Consider for instance the effects of executing theaction put( A, B) in a state in which both A and B are clear and B is located above some block C . The direct effects ofthis action (described by the dynamic causal law above) is on( A, B). An indirect effect, above( A, C), is obtained from ourstatic causal law. The problem of determining such indirect effects, known as the ramification problem, remained open fora comparatively long time. In the last decade, several solutions to this problem have been proposed, for example [5,38,28,37,41,53,54,46,64]. One of these solutions [41] is incorporated in the semantics of AL. The ability to represent causal lawsmakes AL a powerful modeling language. It was successfully used for instance to model the reactive control system of thespace shuttle [4]. The system consists of fuel and oxidizer tanks, valves and other plumbing needed to provide propellantto the maneuvering jets of the shuttle. It also includes electronic circuitry; both to control the valves in the fuel lines andto prepare the jets to receive firing commands. Overall, the system is rather complex, in that it includes 12 tanks, 44 jets,66 valves, 33 switches, and around 160 computer commands (computer-generated signals). The use of static causal laws(including recursive ones) was crucial for modeling the system and for the development of industrial size planning anddiagnostic applications.While static causal laws have been intensively studied by researchers interested in knowledge representation, they haverarely been considered by the mainstream planning community. Although the original specification of the Planning DomainDescription Language (PDDL) – a language frequently used for the specification of planning problems by the planning com-munity – includes axioms1 (which correspond to non-recursive static causal laws in our terminology) [27], most of theplanning domains investigated by this community, including those used for planning competitions [1,17,40] do not includeaxioms. This is partly due to the fact that the semantics for PDDL with axioms is not clearly specified, and partly to the(somewhat mistaken but apparently widespread) belief that static causal laws can always be replaced by dynamic causallaws. There is fortunately also an opposing view. For instance, in [63], the authors argue that the use of axioms not onlyincreases the expressiveness and elegance of the problem representation but also improves the performance of planners. Itis known that the complexity of the conformant planning problem is much higher than classical planning in deterministicdomains (Σ P2 vs. NP-complete) [6,68], and hence the question of efficiency becomes even more important.An action description D of AL describing the corresponding dynamic domain can be used for multiple purposes in-cluding classical planning and diagnostics (see for instance [3,4,7,35]). One way to attack this problem is to replace thetransition diagram T (D) by a deterministic transition diagram T lp(D) such that nodes of T lp(D) are partial states of D, itsarcs are labeled by actions, and a path in T lp(D) from an initial partial state δ0 to a partial state satisfying δ f correspondsto a conformant plan for δ0 and δ f in T (D). The transition diagram T lp(D) is called an approximation of T (D). Even thoughT lp(D) normally has many more states than T (D) does, validating whether a given sequence of actions is a conformant planusing T lp(D) is much easier. As pointed out in [6] the use of an approximation can substantially help reduce the complexityof the planning problem. Indeed, an approximation in domains with incomplete information and static causal laws has beendeveloped and applied successfully in the context of conditional and conformant planning in [66]. Of course a drawback ofthis approach is the p",
            {
                "entities": [
                    [
                        209,
                        220,
                        "AUTHOR"
                    ],
                    [
                        224,
                        236,
                        "AUTHOR"
                    ],
                    [
                        242,
                        257,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 170 (2006) 803–834www.elsevier.com/locate/artintPropagation algorithms for lexicographic ordering constraintsAlan M. Frisch a,∗, Brahim Hnich b, Zeynep Kiziltan c, Ian Miguel d, Toby Walsh ea Department of Computer Science, University of York, UKb Faculty of Computer Science, Izmir University of Economics, Turkeyc DEIS, University of Bologna, Italyd School of Computer Science, University of St Andrews, UKe National ICT Australia and Department of CS & E, UNSW, AustraliaReceived 12 April 2005; received in revised form 24 March 2006; accepted 27 March 2006AbstractFinite-domain constraint programming has been used with great success to tackle a wide variety of combinatorial problems inindustry and academia. To apply finite-domain constraint programming to a problem, it is modelled by a set of constraints on a setof decision variables. A common modelling pattern is the use of matrices of decision variables. The rows and/or columns of thesematrices are often symmetric, leading to redundancy in a systematic search for solutions. An effective method of breaking thissymmetry is to constrain the assignments of the affected rows and columns to be ordered lexicographically. This paper develops anincremental propagation algorithm, GACLexLeq, that establishes generalised arc consistency on this constraint in O(n) operations,where n is the length of the vectors. Furthermore, this paper shows that decomposing GACLexLeq into primitive constraintsavailable in current finite-domain constraint toolkits reduces the strength or increases the cost of constraint propagation. Alsopresented are extensions and modifications to the algorithm to handle strict lexicographic ordering, detection of entailment, andvectors of unequal length. Experimental results on a number of domains demonstrate the value of GACLexLeq.© 2006 Elsevier B.V. All rights reserved.Keywords: Artificial intelligence; Constraints; Constraint programming; Constraint propagation; Lexicographic ordering; Symmetry; Symmetrybreaking; Generalized arc consistency; Matrix models1. IntroductionConstraints are a natural means of knowledge representation. For instance: the maths class must be timetabledbetween 9 and 11am on Monday; the helicopter can carry up to four passengers; the sum of the variables must equal100. This generality underpins the success with which finite-domain constraint programming has been applied to awide variety of disciplines [27]. To apply finite-domain constraint programming to a given domain, a problem mustfirst be characterised or modelled by a set of constraints on a set of decision variables, which its solutions must satisfy.A common pattern arising in the modelling process is the use of matrices of decision variables, so-called matrixmodels [9]. For example, it is simple to represent many types of functions and relations in this way [15].* Corresponding author.E-mail addresses: frisch@cs.york.ac.uk (A.M. Frisch), brahim.hnich@ieu.edu.tr (B. Hnich), zkiziltan@deis.unibo.it (Z. Kiziltan),ianm@dcs.st-and.ac.uk (I. Miguel), tw@cse.unsw.edu.au (T. Walsh).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.03.002\f804A.M. Frisch et al. / Artificial Intelligence 170 (2006) 803–834Concomitant with the selection of a matrix model is the possibility that the rows and/or columns of the matrixare symmetric. Consider, for instance, a matrix model of a constraint problem that requires finding a relation R onA × B where A and B are n-element and m-element sets of interchangeable objects respectively. The matrix, M, hasn columns and m rows to represent the elements of A and B. Each decision variable Ma,b can be assigned either 1 or0 to indicate whether (cid:3)a, b(cid:4) ∈ A × B is in R. Symmetry has been introduced because the matrix, whose columns androws are indexed by A and B, distinguishes the position of the elements of the sets, whereas A and B did not. Givena (non-)solution to this problem instance, a (non-)solution can be obtained by permuting columns of assignmentsand/or permuting rows of assignments. This is known as row and column symmetry [8]. Since similar behaviour canbe found in multidimensional matrices of decision variables it is known more generally as index symmetry. As is welldocumented, symmetry can lead to a great deal of redundancy in systematic search [8].As reviewed in Section 2.5 of this paper, lexicographic ordering constraints have been shown to be an effectivemethod of breaking index symmetry. This paper describes a constraint propagation algorithm, GACLexLeq, that en-forces this constraint. Given a lexicographic ordering constraint c, the propagation algorithm removes values from thedomains of the constrained variables that cannot be part of any solution to c. This paper also shows that GACLexLeqestablishes a property called generalised arc consistency,—that is it removes all infeasible values—while only re-quiring a number of operations linear in the number of variables constrained. The GACLexLeq algorithm is alsoincremental; if the domain of a variable is reduced the algorithm can re-establish generalised arc consistency withoutworking from scratch.Although the examples and experiments in the paper employ the lexicographic ordering constraint to break indexsymmetry, we note that lexicographic ordering can be used to break any symmetry that operates on the variables ofan instance. The lex-leader method [5] breaks all symmetry by identifying a representative among the elements of theequivalence class of symmetries of an instance and adding a lexicographic ordering constraint for each other elementof the equivalence class to ensure that only the representative is allowed.The paper is organised as follows. Section 2 introduces the necessary background while Section 3 describes a num-ber of applications used to evaluate our approach. Section 4 presents a propagation algorithm for the lexicographicordering constraint. Then Section 5 discusses the complexity of the algorithm, and proves that the algorithm is soundand complete. Section 6 extends the algorithm to propagate a strict ordering constraint, to detect entailment, and tohandle vectors of different lengths. Alternative approaches to propagating the lexicographic ordering constraint arediscussed in Section 7. Section 8 demonstrates that decomposing a chain of lexicographic ordering constraints intolexicographic ordering constraints between adjacent or all pairs of vectors hinders constraint propagation. Computa-tional results are presented in Section 9. Finally, we conclude and outline some future directions in Section 10.2. BackgroundAn instance of the finite-domain constraint satisfaction problem (CSP) consists of:• a finite set of variables X ;• for each variable X ∈ X , a finite set D(X) of values (its domain); and• a finite set C of constraints on the variables, where each constraint c(X1, . . . , Xn) ∈ C is defined over the variablesX1, . . . , Xn by a subset of D(X1) × · · · × D(Xn) giving the set of allowed combinations of values. That is, c isan n-ary relation.A variable assignment maps every variable in a given instance of CSP to a member of its domain. A variableassignment A is said to satisfy a constraint c(X1, . . . , Xn) if and only if (cid:3)A(X1), . . . , A(Xn)(cid:4) is in the relation denotedby c. A solution to an instance of CSP is a variable assignment that satisfies all the constraints. An instance is said to besatisfiable if it has a solution; otherwise it is unsatisfiable. Typically, we are interested in finding one or all solutions,or an optimal solution given some objective function. In the presence of an objective function, a CSP instance is aninstance of the constraint optimisation problem.To impose total ordering constraints on variables and vectors of variables there must be an underlying total orderingon domains. If the domain of interest is not totally ordered, a total order can be imposed. And now, since domainsare always finite, every domain is isomorphic to a finite set of integers. So we shall simplify the presentation byconsidering all domains to be finite sets of integers.\fA.M. Frisch et al. / Artificial Intelligence 170 (2006) 803–834805The minimum element in the domain of variable X is min(X), and the maximum is max(X). Throughout, vars(c)is used to denote the set of variables constrained by constraint c.If a variable X has a singleton domain {v} we say that v is assigned to X, or simply that X is assigned. If two.= X(cid:6)). If v is.= X(cid:6), otherwise we write ¬(Xvariables X and X(cid:6) are assigned the same value, then we write Xassigned to X and v(cid:6) is assigned to X(cid:6) and v < v(cid:6) then we write X (cid:2) X(cid:6).A constraint c is entailed if all assignments of values to vars(c) satisfy c. If a constraint can be shown to be entailedthen running the (potentially expensive) propagation algorithm can be avoided. Similarly, a constraint c is disentailedwhen all assignments of values to vars(c) violate c. Observe that if a constraint in a CSP instance can be shown to bedisentailed then the instance has no solution.2.1. Generalised arc consistencyThis paper focuses on solving the CSP by searching for a solution in a space of assignments to subsets of thevariables. Solution methods of this type use propagation algorithms that make inferences based on the domains ofthe constrained variables and the assignments that satisfy the constraint. These inferences are typically recordedas reductions in variable domains, where the elements removed cannot form part of any assignment satisfying theconstraint, and therefore any solution. At each node in the search, constraint propagation algorithms are used toestablish a local consistency property. A common example is generalised arc consistency (see [19]).Definition 1 (Generalised arc consistency). A constraint c is generalised arc consistent (or GAC), written GAC(c), ifand only if for every X ∈ vars(c) and every v ∈ D(X)",
            {
                "entities": [
                    [
                        153,
                        165,
                        "AUTHOR"
                    ],
                    [
                        169,
                        184,
                        "AUTHOR"
                    ],
                    [
                        188,
                        198,
                        "AUTHOR"
                    ],
                    [
                        202,
                        212,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 97 (1997) 83-136 Artificial Intelligence Speeding up inferences using relevance reasoning: a formalism and algorithms Alon Y. Levy a,*, Richard E. Fikes b~l, Yehoshua Sagiv cV2 B AT&T Bell Laboratories, 180 Park Ave., Room A283, Florham Park, NJ 07932, USA b KSL, Stanford University, Stanford, CA 94305, USA c Department of Computer Science, Hebrew University, Jerusalem, Israel Received September 1995; revised May 1996 Abstract reasoning Irrelevance to a specific query. Aside from its importance refers to the process in which a system reasons about which parts of in its knowledge are relevant (or irrelevant) speeding up inferences from large knowledge bases, relevance reasoning is crucial in advanced applications such as modeling complex physical devices and information gathering in distributed heterogeneous systems. This article presents a novel framework for studying the various kinds of irrelevance that arise in inference and efficient algorithms for relevance reasoning. We present a proof-theoretic framework for analyzing definitions of irrelevance. The framework makes the necessary distinctions between different notions of irrelevance that are important when using them for speeding up inferences. We describe the query-tree algorithm which is a sound, complete and efficient algorithm for automatically deriving certain kinds of irrelevance claims for Horn-rule knowledge bases and several extensions. Finally, we describe experimental results that show that significant speedups (often orders of magnitude) are obtained by employing the query-tree in inference. @ 1997 Elsevier Science B.V. Keywords: Relevance representation reasoning; Meta-level reasoning; Static analysis; Horn rules; Constraints; Knowledge author. Email: * Corresponding ’ Email: fikes@cs.stanford.edu. 2 Email: sagiv@cs.huji.ac.il. levy@research.att.com. 0004-3702/97/$17.00 PII SOOO4-3702( @ 1997 Elsevier Science B.V. All rights reserved. 97)00049-O \fbase formance of inference knowledge irrelevant - Irrelevant engine query. Consequently, information: 84 A.L Levy et al./ArtiJicial Intelligence 97 (1997) 83-136 1. Introduction Many the inability future applications large amounts of knowledge. it is essential or task. In fact, is a major obstacle process (or base or by exploiting is a specific knowledge about in the knowledge Irrelevance the domain. irrelevant) in which the system explicit form of meta-level of Artificial In order for a system to perform efficiently Intelligence (AI) will be in domains having in such a domain to any given query is relevant that it be able to determine which knowledge of current AI systems to ignore irrelevant information in scaling up such systems. Irrelevance reasoning reasons about which parts of its knowledge to a specific query, either by automatically inspecting refers to the are relevant the knowledge reasoning irrelevance-claims given by a user. Irrelevance reasoning base, as opposed reasoning [ 37,45,54], to using in which we reason about the the knowledge base to reason is important in several contexts: a Speeding up inferences in large knowledge bases: It is well known that the per- in AI systems degrades quickly as the size of the to are due increases. Two of the major sources of inefficiency engines facts in the knowledge base: In its search for a solution, in the knowledge considers many facts base the inference it spends significant - Irrelevant distinctions in the representation: A knowledge a variety of tasks. Therefore, accommodate must be detailed enough relations, objects, etc.). As a result, the representation for any given for all those task, thereby to inefficient tasks (i.e., leading reasoning. that are irrelevant to the effort pursuing useless solution paths. to is designed of the domain refined is likely to be too complex its conceptualization it must include many base l Modeling complex physical devices: Tasks such as diagnosis, design and simulation require a model of a given physical device a model depends heavily on the task for which with the most detailed model of the system would be intractable. Therefore, important (e.g.,[ 2,32,46] system are relevant the adequacy of directly it is create a model that is suited for a given query that we determine which aspects of the ), and doing so requires to a given query. to be able to automatically it is used, and reasoning [ 11,221. However, l Large scale distributed information systems: Current communications to many enables accessing the Internet) easy access this information (e.g., At the moment, body of work in AI has the goal of designing sources of information high thereby 3 1,33,38,5 1,60,65,90]. is the ability a given query posed by a user. to automatically and providing determine which A key issue that needs technology remote sources of information. can be mostly by browsing. A growing for integrating multiple them, [4, to be addressed by these systems to architectures level querying sources are relevant facilities over information sources freeing a user from the need to know about specific information Irrelevance reasoning also plays an important 711, belief revision using applications relevance reasoning of relevance [ 351 and learning (e.g., to speed up inferences role in nonmonotonic [ 7,36, [ 30,66,74] > . The focus of this paper is on in large knowledge bases. The other reasoning reasoning are discussed briefly in Section 6. \fA.Y Levy et al. /Art$cial Intelligence 97 (1997) 83-136 8.5 In order for relevance reasoning to be a viable method for controlling inference, issues need that are irrelevant detecting parts of a knowledge base to be addressed. First, we must develop eficient for to a query. The to these algorithms may vary; it could consist of certain parts (e.g., the rules) of on facts in the knowledge base) and, possibly, some irrelevance claims supplied irrelevant knowledge reasoning two questions, we need a formal several automatically input the knowledge base, some meta-claims the ground by the user. Second, we must and the tradeoff between meta-level about understanding about relevance and base-level these of the possible meanings of irrelevance reasoning for addressing the domain. As a basis about other parts (e.g., the utility of removing integrity constraints investigate algorithms claims. in research in many contexts The notion of irrelevance has appeared in AI and related fields. However, most of the time researchers use the term informally. Formal analyses of [ 47 J >, 1950 irrelevance have been discussed by philosophers [ 161) and 1978 (Gardenfors (Catnap thrust of these analyses was notions of irrelevance by a formal definition. Most to try to capture our common-sense of the work focuses on formulating and finding properties of the notion of irrelevance the work has not been concerned that satisfy definitions with how to use irrelevance detecting for speeding up inference or how to design algorithms those properties. Consequently, as early as 1921 (Keynes [ 341). The main irrelevance. for Within AI, the notion of irrelevance was investigated [53], [21,23,70] and used there to control reasoning In the context of logical knowledge bases, Subramanian several meyer of automatically irrelevance left largely open, and consequently effective way. investigated deriving inference claims and the utility of irrelevance relevance reasoning formal definitions of irrelevance. However, in the context of probabilistic in Bayesian belief networks. [ 881, and more recently Lake- the issues reasoning were in any has not been applied This article presents a framework studied. We present efficient algorithms and we describe the results of experiments In particular, we make the following contributions. in which various definitions of irrelevance irrelevance for automatically that validate the utility of relevance deriving can be claims, reasoning. deriving framework distinctions for analyzing the necessary l We present the definitions of irrelevance, a proof-theoretic between irrelevance claims vary as we move in the space. The framework encompasses definitions for the notion. We describe how properties of yielding a space of possible definitions irrelevance and sheds new light on previous definitions of irrelevance. We show that the framework makes the problem of automatically irrelevant facts. l We consider the problem of automatically knowledge bases and several extensions, tree, for that purpose. We identify and show that the query-tree provides a sound and complete irrelevance for such claims. Strong strongly often especially useful irrelevance claims for Horn-rule and present a novel tool, called the query- claims inference procedure that removing (and it in practice. First, irrelevance claims are derived by inspecting only facts is guaranteed not to slow down an inference engine the important class of strong-irrelevance claims and the utility of removing The query-tree has two properties claims also have the property it up significantly). to address that make irrelevant to speed deriving needed \f86 A. E Levy et al. /Artijicial Intelligence 97 (1997) 83-136 irrelevance the semantics of interpreted predicates appearing in deriving facts). Second, a small part of the knowledge base (e.g., inspecting and not the ground considers (e.g., order predicates, key role in many applications, query-tree programs it provides predicates. l We describe but is distinguished sort predicates, completeness experimental that show this is an important the rules in the knowledge base claims, the query-tree in the knowledge base interpreted predicates play a feature of the query-tree. The logic from previous work in that area in that can also be viewed as a tool for partial evaluation of constraint [ 15,67,85], in the presence of recursive rules and interpreted etc.). Since speedups also results that significant (often or- in inference. The it is used to determine which facts are ir- to a query. Based on that determination,",
            {
                "entities": [
                    [
                        182,
                        196,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 246 (2017) 118–151Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintLocalising iceberg inconsistenciesGlauber De Bona∗, Anthony HunterDepartment of Computer Science, University College London, WC1E 6BT, UKa r t i c l e i n f oa b s t r a c tArticle history:Received 3 August 2016Received in revised form 16 February 2017Accepted 19 February 2017Available online 28 February 2017Keywords:Propositional logicInconsistency managementInconsistency analysisInconsistency localisationIn artificial intelligence, it is important to handle and analyse inconsistency in knowledge bases. Inconsistent pieces of information suggest questions like “where is the inconsis-tency?” and “how severe is it?”. Inconsistency measures have been proposed to tackle the latter issue, but the former seems underdeveloped and is the focus of this paper. Minimal inconsistent sets have been the main tool to localise inconsistency, but we argue that they are like the exposed part of an iceberg, failing to capture contradictions hidden under the water. Using classical propositional logic, we develop methods to characterise when a formula is contributing to the inconsistency in a knowledge base and when a set of formulas can be regarded as a primitive conflict. To achieve this, we employ an abstract consequence operation to “look beneath the water level”, generalising the minimal inconsistent set concept and the related free formula notion. We apply the framework presented to the problem of measuring inconsistency in knowledge bases, putting forward relaxed forms for two debatable postulates for inconsistency measures. Finally, we discuss the computational complexity issues related to the introduced concepts.© 2017 Elsevier B.V. All rights reserved.1. IntroductionThe occurrence of inconsistencies in data and knowledge is an important issue for the application of knowledge repre-sentation and reasoning technologies that are based on standard logics. To develop ways of dealing with an inconsistent set of formulas, it is important to understand the inconsistency, analysing its properties. Given an inconsistent knowledge base (a set of formulas), natural questions that arise are “where is the inconsistency?” and “how severe is it?”. To answer the second question in a qualitative way, inconsistent knowledge bases were classified by the severity of their inconsistency [17]. Recently, to numerically quantify the extent to which a knowledge base is inconsistent, many inconsistency measures have been proposed [29,24,25,19,28,27,20,42,43]. In contrast, the first question appears quite underdeveloped, and it is the subject of the present work.Inconsistency localisation can mean different things. One may want for instance to spot which part of the language is “contaminated” by the inconsistency, looking for the logical variables involved in contradictions (see e.g. [22,25]). Alterna-tively, one might assign numeric inconsistency values for formulas in a knowledge base, indicating the extent to which they are involved in the inconsistency, according to a given definition (e.g. [23,25]). In this paper, we focus on localising the inconsistency in a knowledge base, showing how it unfolds among the formulas.1 That is, given an inconsistent knowledge * Corresponding author.E-mail addresses: glauberbona@gmail.com (G. De Bona), anthony.hunter@ucl.ac.uk (A. Hunter).1 Note that logically closed theories are equal to the whole logical language when inconsistent, hence we focus on (possibly non-closed) knowledge bases.http://dx.doi.org/10.1016/j.artint.2017.02.0050004-3702/© 2017 Elsevier B.V. All rights reserved.\fG. De Bona, A. Hunter / Artificial Intelligence 246 (2017) 118–151119base, we are interested in discovering which subsets of formulas are contributing to the inconsistency, being its causes, and which formulas are not involved whatsoever.Fig. 1. Inconsistency as icebergs.1.1. MotivationWhen a knowledge base is inconsistent, it is not necessarily the case that its inconsistency is spread over all its formulas. For example, consider the set formed by the propositions: “Alice is a cat”, “Alice is not a cat” and “Bob is a dog”. Even though the whole set is inconsistent, intuition tends towards regarding the first two propositions as controversial and the third one as free of inconsistency somehow. To capture such intuition, minimal inconsistent sets (inconsistent sets whose all proper subsets are consistent) have been construed as the “purest form of inconsistency” [24,25]. Accordingly, a formula not contained in any minimal inconsistent set — a free formula — has been regarded as “uncontroversial”. As the first two propositions are already contradicting each other, the whole base is not a minimal inconsistent set. Furthermore, the third proposition contradicts neither the first nor the second proposition, hence “Bob is a dog” is indeed technically free, for not being in a minimal inconsistent set. Such a simple solution to the problem of localising the inconsistency probably is the reason for the lack of a systematic investigation of this issue. Nonetheless, the situation is more complex than might at first appear, since minimal inconsistent sets are alike the exposed part of the iceberg, ignoring all the inconsistency hidden under the water, as illustrated in Fig. 1.The recognition of these iceberg inconsistencies can find application in different areas where inconsistent pieces of information have to be dealt with. For instance, in software engineering, requirements extraction might reveal users’ expec-tations that cannot hold together, calling for a method for localising the conflicts. In data integration/fusion, as well as in belief merging, the proper identification of the sources of information, or the agents, that are conflicting each other allows one to narrow its attention to the focus of the problem, ignoring uncontroversial data/beliefs. In formal argumentation, in-consistency can be localised in order to show how a set of arguments is conflicting. Inconsistency localisation may also bring important clues in fraud investigation, for instance in the analysis of contradicting tax forms of a given taxpayer. In general, any decision making under inconsistent information might benefit from localising the inconsistency. For example, a physi-cian facing several different medical tests of a given patient with inconsistent results might need to choose which ones should be performed again. Example 1.1 brings a concrete situation where a decision can be influenced by inconsistency localisation.Example 1.1. The police is investigating a robbery on a jewellery shop that occurred on a weekday, during working hours. The investigators have taken testimony from all employees that were working on the day of the crime. The witnesses’ statements include the following:• salesperson: “I did not open the safe, and the criminals carried no guns!”• security chief: “Only the manager or the salesperson could have opened the safe, and the criminals carried guns.”• manager: “I did not open the safe.”As the police conceives the possibility of some of the employees having been complicit, they look for contradictions among the versions given. Inconsistent testimonies would imply some witnesses are lying, raising suspicions of complicity against them. The security chief and the salesperson are clearly contradicting each other, but is the manager involved in some contradiction? From the statements above, can one infer that it is possible that the manager is lying?To answer the questions raised in the example above, we need a tool to tell the “uncontroversial” from the “controversial” formulas in a knowledge base, since we are only interested in knowing whether the manager’s testimony is involved in the inconsistency, raising suspicion that he/she lied. This can be regarded as the relaxed form of the problem of localising inconsistency, whose solution is a partition of the inconsistent knowledge base into “controversial” and “uncontroversial” formulas. Free formulas are intended to encompass all and only “uncontroversial” formulas in a knowledge base, but we \f120G. De Bona, A. Hunter / Artificial Intelligence 246 (2017) 118–151shall argue that they are not suitable for all contexts. For instance, in the example above, the manager’s testimony is free (because it is not in any minimal inconsistent set), but it also seems to contradict the others in some way.A harder problem is identifying the atomic inconsistencies, or the primitive conflicts, in a knowledge base and can be illustrated by the following situation:Example 1.2. A university has hired a company to design a library management software to be used by all its members. In order to extract the design specifications, the company has collected requirements from the head of each department, which include:• Ecology: “The software should be open source, contributing to the whole academic community.”• Marketing: “It can’t be freely available, we need to keep our university edge in IT systems as a differential that attracts new students.”• Philosophy: “Both graduate and undergraduate students shall have the same rights in the system and it must be re-motely accessible.”• Economy: “Due to their different demands, graduate students need some privileges. If the system is to be remotely accessible, its software should not be open source, otherwise it could be vulnerable.”• Theology: “Department heads shall have no exclusive privileges.”• Arts: “I have no specific requirements.”The project manager, while reading such requirements, notes two contradictions: one between the heads of the Ecology and Marketing departments, on whether the software should be open source, and another between the heads of Philosophy and Economy departments, about the graduate and undergraduate students rights. The manager plans to arrange meetings with the department heads to discuss — and maybe relax — ",
            {
                "entities": [
                    [
                        170,
                        185,
                        "AUTHOR"
                    ],
                    [
                        188,
                        202,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 168 (2005) 119–161www.elsevier.com/locate/artintWeak nonmonotonic probabilistic logics ✩Thomas Lukasiewicz 1Dipartimento di Informatica e Sistemistica, Università di Roma “La Sapienza”,Via Salaria 113, 00198 Rome, ItalyReceived 1 October 2004; accepted 31 May 2005Available online 5 July 2005AbstractWe present an approach where probabilistic logic is combined with default reasoning from condi-tional knowledge bases in Kraus et al.’s System P , Pearl’s System Z, and Lehmann’s lexicographicentailment. The resulting probabilistic generalizations of default reasoning from conditional knowl-edge bases allow for handling in a uniform framework strict logical knowledge, default logicalknowledge, as well as purely probabilistic knowledge. Interestingly, probabilistic entailment in Sys-tem P coincides with probabilistic entailment under g-coherence from imprecise probability assess-ments. We then analyze the semantic and nonmonotonic properties of the new formalisms. It turnsout that they all are proper generalizations of their classical counterparts and have similar propertiesas them. In particular, they all satisfy the rationality postulates of System P and some Conditioningproperty. Moreover, probabilistic entailment in System Z and probabilistic lexicographic entailmentboth satisfy the property of Rational Monotonicity and some Irrelevance property, while probabilis-tic entailment in System P does not. We also analyze the relationships between the new formalisms.Here, probabilistic entailment in System P is weaker than probabilistic entailment in System Z,which in turn is weaker than probabilistic lexicographic entailment. Moreover, they all are weakerthan entailment in probabilistic logic where default sentences are interpreted as strict sentences.Under natural conditions, probabilistic entailment in System Z and lexicographic entailment evencoincide with such entailment in probabilistic logic, while probabilistic entailment in System P does✩ This paper is a significantly extended and revised version of a paper in: Proceedings of the 9th InternationalConference on Principles of Knowledge Representation and Reasoning (KR2004), Whistler, Canada, June 2004,AAAI Press, 2004, pp. 141–151.E-mail address: lukasiewicz@dis.uniroma1.it (T. Lukasiewicz).1 Alternate address: Institut für Informationssysteme, Technische Universität Wien, Favoritenstraße 9-11, 1040Vienna, Austria; e-mail: lukasiewicz@kr.tuwien.ac.at.0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.05.005\f120T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161not. Finally, we also present algorithms for reasoning under probabilistic entailment in System Z andprobabilistic lexicographic entailment, and we give a precise picture of its complexity. 2005 Elsevier B.V. All rights reserved.Keywords: Probabilistic logic; Default reasoning from conditional knowledge bases; Entailment in System P ;Entailment in System Z; Lexicographic entailment; Nonmonotonic probabilistic logics; Inconsistencyhandling; Algorithms; Computational complexity1. IntroductionDuring the recent decades, reasoning about probabilities has started to play an importantrole in AI. In particular, reasoning about interval restrictions for conditional probabilities,also called conditional constraints [49], has been a subject of extensive research efforts.Roughly, a conditional constraint is of the form (ψ|φ)[l, u], where ψ and φ are events, and[l, u] is a subinterval of the unit interval [0, 1]. It encodes that the conditional probabilityof ψ given φ lies in [l, u].An important approach for handling conditional constraints is probabilistic logic, whichhas its origin in philosophy and logic, and whose roots can be traced back to alreadyBoole in 1854 [12]. There is a wide spectrum of formal languages that have been exploredin probabilistic logic, ranging from constraints for unconditional and conditional eventsto rich languages that specify linear inequalities over events (see especially the work byNilsson [54,55], Fagin et al. [19], Dubois and Prade et al. [2,13,16,17], Frisch and Had-dawy [21], and the author [48,49,51]; see also the survey on sentential probability logic byHailperin [35]). The main decision and optimization problems in probabilistic logic are de-ciding satisfiability, deciding logical consequence, and computing tight logically entailedintervals. Recently, column generation techniques from operations research have been suc-cessfully used to solve large problem instances in probabilistic logic (see especially thework by Jaumard et al. [37] and Hansen et al. [36]).Example 1.1 (Eagles). A simple collection of conditional constraints KB may encode thestrict logical knowledge “all eagles are birds” and “all birds have feathers” as well asthe purely probabilistic knowledge “birds fly with a probability of at least 0.95” (cf. Ex-ample 2.1). This collection of conditional constraints KB is satisfiable, and some logicalconsequences in probabilistic logic from KB are “all birds have feathers”, “birds fly with aprobability of at least 0.95”, “all eagles have feathers”, and “eagles fly with a probabilitybetween 0 and 1”; in fact, these are the tightest intervals that follow from KB (cf. Exam-ple 2.2). That is, we especially cannot conclude anything from KB about the ability to flyof eagles.A closely related research area is default reasoning from conditional knowledge bases,which consist of a collection of strict statements in classical logic and a collection of defea-sible rules, also called defaults. The former must always hold, while the latter are rules ofthe kind ψ ← φ, which read as “generally, if φ then ψ”. Such rules may have exceptions,which can be handled in different ways.\fT. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161121The literature contains several different proposals for default reasoning from conditionalknowledge bases and extensive work on its desired properties. The core of these proper-ties are the rationality postulates of System P by Kraus, Lehmann, and Magidor [40],which constitute a sound and complete axiom system for several classical model-theoreticentailment relations under uncertainty measures on worlds. They characterize classicalmodel-theoretic entailment under preferential structures [40,64], infinitesimal probabili-ties [1,57], possibility measures [14], and world rankings [33,65]. As shown by Friedmanand Halpern [20], many of these uncertainty measures on worlds are expressible as plausi-bility measures. The postulates of System P also characterize an entailment relation basedon conditional objects [15]. A survey of the above relationships is given in [6,22].Mainly to solve problems with irrelevant information, the notion of rational closure asa more adventurous notion of entailment was introduced by Lehmann [45,47]. It is equiva-lent to entailment in System Z by Pearl [58], to the least specific possibility entailment byBenferhat et al. [5], and to a conditional (modal) logic-based entailment by Lamarre [44].Finally, mainly to solve problems with property inheritance from classes to exceptionalsubclasses, the maximum entropy approach to default entailment was proposed by Gold-szmidt et al. [31]; lexicographic entailment was introduced by Lehmann [46] and Benferhatet al. [4]; conditional entailment was proposed by Geffner [24,26]; and an infinitesimal be-lief function approach was suggested by Benferhat et al. [7]. The following example due toGoldszmidt and Pearl [34] illustrates default reasoning from conditional knowledge bases.Example 1.2 (Penguins). A conditional knowledge base KB may encode the strict logicalknowledge “all penguins are birds” and the default logical knowledge “generally, birds fly”,“generally, penguins do not fly”, and “generally, birds have wings”. Some desirable con-clusions from KB [34] are “generally, birds fly” and “generally, birds have wings” (whichboth belong to KB), “generally, penguins have wings” (since the set of all penguins is asubclass of the set of all birds, and thus penguins should inherit all properties of birds),“generally, penguins do not fly” (since properties of more specific classes should overrideinherited properties of less specific classes), and “generally, red birds fly” (since “red” isnot mentioned at all in KB and thus should be considered irrelevant to the ability to fly ofbirds).There are several works in the literature on probabilistic foundations for default reason-ing from conditional knowledge bases [1,11,31,57], on combinations of Reiter’s defaultlogic [63] with statistical inference [43,67], and on a rich first-order formalism for deriv-ing degrees of belief from statistical knowledge including default statements [3]. However,there has been no work so far that extends probabilistic logic by the capability of handlingdefaults as in conditional knowledge bases.In this paper, we try to fill this gap. We present extensions of probabilistic logic bydefaults as in conditional knowledge bases under Kraus et al.’s System P [40], Pearl’sSystem Z [58], and Lehmann’s lexicographic entailment [46]. The new formalisms allowfor expressing in a uniform framework strict logical knowledge and purely probabilisticknowledge from probabilistic logic, as well as default logical knowledge from default rea-soning from conditional knowledge bases. Informally, strict logical knowledge representssentences that must always hold, while purely probabilistic (resp., default logical) knowl-\f122T. Lukasiewicz / Artificial Intelligence 168 (2005) 119–161edge encodes sentences that may have exceptions, which is expressed in a quantitative(resp., qualitative) way.Example 1.3 (Ostriches). Consider the strict logical knowledge “all ostriches are birds”,the default logical knowledge “generally, birds have legs” and “generally, birds fly”, andthe purely probabilistic knowledge “ostriches fly with a probability of at most 0.05”. Ob-viously, some desired",
            {
                "entities": [
                    [
                        112,
                        130,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 1154–1193Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintFrom the textual description of an accident to its causesDaniel Kayser∗, Farid Nouioua 1Laboratoire d’Informatique de Paris-Nord, UMR 7030 du C.N.R.S. – Institut Galilée, Université Paris 13, 99 avenue Jean-Baptiste Clément, F 93430 – Villetaneuse, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 29 September 2008Received in revised form 7 April 2009Accepted 23 April 2009Available online 6 May 2009Keywords:Natural language understandingCausal reasoningNormsInference-based semanticsSemi-normal defaultsEvery human being, reading a short report concerning a road accident, gets an idea ofits causes. The work reported here attempts to enable a computer to do the same, i.e. todetermine the causes of an event from a textual description of it. It relies heavily on thenotion of norm for two reasons:• The notion of cause has often been debated but remains poorly understood: wepostulate that what people tend to take as the cause of an abnormal event, like anaccident, is the fact that a specific norm has been violated.• Natural Language Processing has given a prominent place to deduction, and for whatconcerns Semantics, to truth-based inference. However, norm-based inference is amuch more powerful technique to get the conclusions that human readers derive froma text.The paper describes a complete chain of treatments, from the text to the determinationof the cause. The focus is set on what is called “linguistic” and “semantico-pragmatic”reasoning. The former extracts so-called “semantic literals” from the result of the parse,and the latter reduces the description of the accident to a small number of “kernel literals”which are sufficient to determine its cause. Both of them use a non-monotonic reasoningsystem, viz. LPARSE and SMODELS.Several issues concerning the representation of modalities and time are discussed andillustrated by examples taken from a corpus of reports obtained from an insurancecompany.© 2009 Elsevier B.V. All rights reserved.1. Motivation1.1. Basic postulatesThe work described here is grounded on two postulates:• what is perceived as the cause of an event is:– the norm itself, if the event is perceived as normal,– and the violation of some norm, if the event is considered abnormal;• the semantics of natural language (NL) is not based on the notion of truth, but on norms.* Corresponding author.E-mail addresses: Daniel.Kayser@lipn.univ-paris13.fr (D. Kayser), Farid.Nouioua@lipn.univ-paris13.fr, Farid.nouioua@univ-cezanne.fr (F. Nouioua).1 Now at Laboratoire des Sciences de l’Information et des Systèmes, UMR 6168 du C.N.R.S. Université Paul Cézanne (Aix-Marseille 3), Avenue EscadrilleNormandie–Niemen 13397 Marseille Cedex 20, France.0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.04.002\fD. Kayser, F. Nouioua / Artificial Intelligence 173 (2009) 1154–11931155The notion of norm, which plays a central role in this paper, has both a descriptive and a prescriptive meaning.2 In thedescriptive sense, norms are just what explains the difference between what is perceived as normal or not. In the prescriptivesense, norms build a corpus on the basis of which an agent is considered, legally or otherwise, entitled or not to performan action.In Artificial Intelligence, these two meanings have given rise to two rather separate fields of study. On the one hand,non-monotonic logics have been developed in order to derive conclusions that are considered normal, in the absence ofany specific circumstance invalidating this derivation. On the other hand, deontic logics have the purpose of formalizing thereasoning of agents respecting normative prescriptions [8,17,48].In this paper, norms will generally be taken in their descriptive sense but clearly, as it is normal to follow the rules, thisacceptation of norms includes as a special case the prescriptive sense, and we will also have to deal with duties, which arenormative.Our first postulate concerns a very old and very controversial issue: when is it sensible to say that A causes B? Deter-mining the essence of the notion of cause is not in the agenda of Artificial Intelligence. However, commonsense reasoningmakes an intensive use of causation, e.g. for diagnosing, planning, predicting; and therefore AI cannot (and does not, seee.g. [33,55]) completely ignore the debate concerning this notion. What AI needs, however, does not concern the meta-physics of cause, but only how people reason causally. And, even if observation reveals that we use the word cause to meanrather different things, i.e. that this word is polysemic, in a vast majority of cases we take as causal for an abnormal eventthe fact that some agent has violated a norm. We report in the paper a psychological experiment showing which violation(s)are selectively chosen as cause(s) of the event.Consider now our second postulate: dealing with a sentence such as:The car before me braked suddenly.A truth-based approach (see e.g. [13,27,35]) will derive all the conclusions C that logically follow from the existence of atime t and a car c, such that the two following propositions are true at t: (i) c brakes suddenly and (ii) c is located in frontof the writer. Clearly, several other propositions, none of them being valid in the logical sense, come to the mind of a humanreader, e.g. (iii) at t, the writer was driving, (iv) s/he was driving in the same direction as c, (v) no vehicle was betweenc and him/her, (vi) s/he had to act quickly, in order to prevent an accident, and so on. Subsequent information may forcethe reader to retract some of these conclusions. Nonetheless, as they are likely to be present in the mind of every personwho shares our culture, there is no necessity to consider separately the propositions C derived by means of truth-preservingoperations (the only ones that are said to pertain to semantics, according to the prevailing theories), from the propositionsderived by means of norms (generally said to be the result of pragmatics).Knowing the norms of a domain is absolutely necessary to understand the texts of that domain. But there exists noexhaustive list of the norms ruling any given domain: the rules and regulations are only the visible part of the iceberg ofall we know, and keep implicit, about the domain. An indirect consequence of our study is to point out that examining howpeople ascribe causation to the events happening in a domain is a powerful means to reveal its implicit norms.1.2. Specification of the goalIn order to validate our postulates, we need to focus on a domain where the number of norms remains reasonable, whereabnormal events are frequent, where these events are reported in natural language, and where it is easy to ask people whatthey take as being the cause of the events reported.We selected the domain of road accidents, for the following reasons:• A large number of short texts exists, describing such accidents: every insurance company receives daily a number offorms filled by drivers, describing what happened to them.• Most people of our culture know enough about car crashes to give sensible answers, after having read a text, whenthey are asked what caused the accident.• Each report describes some facts, but clearly implies also a number of other facts, which are not logically entailed. Wecan therefore see whether our postulates work, i.e. check whether a reasoning based on norms captures the kind ofreasoning used by the reader.• An accident by itself is an anomaly, and the text generally goes back to another anomaly that allegedly explains why ithappened. We can thus test which anomaly, if any, is taken to be the cause of the accident, and confirm or infirm ourfirst postulate.• The number of norms involved is neither too large nor too small. They are clearly not limited to those listed in theHighway Code.• The corpus on which we perform our study has been studied from different points of views. (See for example [60,19,34].The last work presents a system that produces automatically a 3D scene of an accident from its textual description.3)2 Von Wright [62, Chap. 1] discusses in much greater details the various meanings of the word norm.3 Another study [63] concerns British records of road accidents: the authors use a description logic in order to check the consistency between a naturallanguage report and coded data which are both components of the record.\f1156D. Kayser, F. Nouioua / Artificial Intelligence 173 (2009) 1154–1193However, a drawback of this corpus is worth a mention. Most of the reports sent by a driver to an insurance company are,for obvious reasons, biased in order to lessen the author’s responsibility; the rhetorical aspect of a plea interferes with themore basic descriptive part of the report, be it truthful or not. The fact that the texts do not necessarily reflect what reallyhappened is not per se a problem: the norms of a domain are not better revealed from “true” descriptions, whatever thismay mean, than from biased ones. The unwanted consequence of the choice of this corpus is that it requires some effortto identify (and most of the time, to ignore) what has been added for pure argumentative reasons. A study is currentlyin progress that examines specifically the argumentative strategies used by the authors to highlight or to minimize someaspects of the accident, in order to convey the best possible impression of their behavior [9]. As we will see, this dimensionof the reports does not affect much the results of the present work.We obtained, by courtesy of MAIF, an insurance company, a number of reports written in French. Some of them areunclear, and only the accompanying drawings make them understandable. We discarded them and kept only those whichare self-contained, i.e. those on the basis of which we understood enough of what happened to build a hypothesis that",
            {
                "entities": [
                    [
                        195,
                        208,
                        "AUTHOR"
                    ],
                    [
                        211,
                        224,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 90 (1997) 177-223 Artificial Intelligence Semantics and complexity of abduction from default theories * Thomas Eiter a**, Georg Gottlob a~1, Nicola Leone avb*2 a Christian Doppler Laboratory for Expert Systems, Information Systems Department, TU Vienna, Paniglgasse 16, A-1040 Wien, Austria b Istituto per la Sistemistica e l’lnformatica - C.N.R., c/o DEIS - UNICAL, 87036 Rende, Italy Received December 1995; revised June 1996 Abstract Abductive reasoning (roughly speaking, find an explanation for observations out of hypothe- ses) has been recognized as an important principle of common-sense reasoning. Since logical is commonly based on nonclassical formalisms like default logic, au- knowledge representation toepistemic logic, or circumscription, it is necessary to perform abductive reasoning from theories logics. In this paper, we investigate how abduction can (i.e., knowledge bases) of nonclassical be performed from theories in default logic. In particular, we present a basic model of abduc- tion from default theories. Different modes of abduction are plausible, based on credulous and skeptical default reasoning; they appear useful for different applications such as diagnosis and planning. Moreover, we thoroughly analyze the complexity of the main abductive reasoning tasks, namely finding an explanation, deciding relevance of a hypothesis, and deciding necessity of a hypothesis. These problems are intractable even in the propositional case, and we locate them into the appropriate slots of the polynomial hierarchy. However, we also present known classes of default theories for which abduction is tractable. Moreover, we also consider first-order default theories, based on domain closure and the unique names assumption. In this setting, the abduction tasks are decidable, but have exponentially higher complexity than in the propositional case. @ 1997 Elsevier Science B.V. Keywords: Abduction; Default logic; Algorithms and complexity; Tractability *A short and preliminary version of this paper appeared International Joint Conference on Artificial Intelligence (IJCAI-95) in: C. Mellish, ed., Proceedings of the Fourteenth ( AAAI press, 1995) 870-876. author. E-mail: eiter@dbai.tuwien.ac.at. * Corresponding ’ E-mail: gottlob@dbai.tuwien.ac.at. leone@dbai.tuwien.ac.at, * E-mail: nik@si.deis.unical.it. 0004-3702/97/$17.00 PIISOOO4-3702(96)00040-9 @ 1997 Elsevier Science B.V. All rights reserved. \f178 7: Eiter et al. /Artificial Intelligence 90 (1997) 177-223 1. Introduction Abductive reasoning has been recognized as an important principle of common-sense speech recognition formalizations having applications [ 121. Various are well known. These fruitful [ 16,45,46], is represented. Roughly, by a function reasoning based diagnosis [ 341, and vision posed, among which set-covering-based [ 15,16,45,46] knowledge edge is represented are atomic entities i.e. observed tions, if X explains of X; tive explanation. On the other hand, edge potheses tions M. For a more detailed comparison see [22]. is an abductive is represented by a logical explanation, representing symptoms. The set e(X) all manifestations M, reasoning of abductive [ 301, maintenance [ 7,431 and logic-based approaches two types basically differ approach, in a number of areas such diverse as model- of database views have been pro- approaches in the way domain the domain knowl- subsets X of hypotheses, which to subsets e(X) of manifesta- power then X is an abduc- the domain knowl- language. A subset X of hy- can be seen as the explanation i.e., e(X) = M, approach in the set-covering e which maps all possible disorders, of these and other approaches by X derives the manifesta- to abduction, in the logic-based theory T in some if T augmented Until now, mainly logical abduction from theories of classical logic has been stud- ied. However, formalisms situations edge bases) as hybrid malisms [ 21. like default it is necessary of nonclassical reasoning, i.e., logical knowledge representation is commonly logic, autoepistemic logic, or circumscription. to perform logics; reasoning reasoning abductive in a sense, this on a knowledge from theories is orthogonal to what base built using different based on nonclassical Thus, in such (i.e., knowl- is known for- Since default logic [52] languages important fault motivating lead us towards a formal model of abduction that emerged to investigate how abduction In this paper, we address examples; they show that abductive logic. in the field of nonmonotonic is one of the most used logical knowledge (cf. theories [ 18,53]), (W, D) this problem. We start by first considering reasoning from can be performed representation it is in de- some logic is needed, and reasoning from default in default theories. Example 1.1. Consider edge about Bill’s skiing habits: the following set of default rules, which represent some knowl- D= ‘t : vkiing(Bil1) yskiing( Bill) weekend : -snowing skiing( Bill) ’ : -mowing ’ lsnowing ’ 1 intuitively state the following: Bill is usually out for skiing, unless The defaults weekend, Bill snowing. For the certain knowledge W = {weekend} Sunday), skiing( Bill). the default is usually not out for skiing; on the and usually, it is snowing; it is not that it is Saturday or (encoding -snowing and theory T = (W, D) has one extension which contains Suppose now that we observe (which is not consistent with the extension). Abduction means to identify a set of facts, chosen from a set of hypotheses, whose presence for this observation, that is, in the theory that Bill is not out for skiing to find an explanation \fT. Eiter et al. /Artificial Intelligence 90 (1997) 177-223 179 f C d i-i/ Fig. I. A computer network. e the observation at hand would derive the extension. We find such an explanation if we add snowing a single extension, which contains the observation lskiing(Bill), to W, we obtain for the default lskiing(BilZ), by adopting i.e., cause the hypothesis that +Gng(BiZl) snowing. is in Indeed, theory T’ = ({weekend, snowing}, D) is abduced from or that it is an abductive explanation of lskiing(Bill). lskiing( Bill). We say that snowing Observe default properties that the description of the above situation that cannot be represented properly requires in classical logic. the specification of some logic. The default about Example 1.2. Assume default setting knowledge connections established connections: between by a path following that information about a computer network theory T = (W, D) described next comprises relationships the status of a site (working between sites, and reachability of one site from another direct connections), together with is represented using in a simplified or not), (which can be about information / Vx. works(x) > path( x, x) , Vx, y. 7works( x) 3 7reaches( y, x) , conn(a,b>,conn(a,f), w= < conn(b,c),conn(b,d), conn(c,e>,conn(d,e>,conn(e,f), ‘dx. conn( x, x) , \\ Vx, y. conn( x, y) 3 conn(y, x) The first two formulas in W state that if a site works, if a site does not work, then then this site to itself, and from that from any node. The facts conn( s1, ~2) state direct connections venience, the respective Fig. 1.’ and symmetry of the connectivity axioms.3 A graphical representation reflexivity relationship of these connections between there is a trivial path it cannot be reached sites. For con- is taken care of by in is depicted ’ Negated facts omitted. If desired, : lconn(x,y)/~conn(x,y) they can be derived using in D. lconn( ~1, sz), stating that there is no direct connection the closed world assumption, between by sites $1 and ~2, are defaults introducing \f: worh( x) works(x) 180 T Eiter et al. /Artificial Intelligence 90 (1997) 177-223 The default rules are as follows. puth(x, y) : : -yuth(x, y) ’ reuche.s(x,y) ’ v-euches(x,y) ’ D= puth(x,y) Aconn(y,z) Aworks : . paw% z> The first rule states by adopting that a site works by default; the next two rules relate paths to that x reaches y if a path from x to y provably exists, and does it otherwise. The last rules states that a path can be extended by following for if the site at the other end is working; here, it is assumed reachability, not reach a direct connection, simplicity that the connections are reliable, i.e., no communication failures occur. In this representation, we implicitly adopt . V x = c,, where cl , . . . , c, are all the objects x = Cl v.. the underlying axiom of the unique names assumption /jiCj ci # cj, which expresses mentioned domain, which are those mentioned the common domain closure axiom Vx. of in T (in our case, all sites), and the that all objects (respectively individuals) In this setting, in T are different. the default and each site reaches any other site; notice path x = se, si,. andconn(sj_i,sj) We remark that the knowledge istrueforallj=l,...,n. theory T has a single extension E, in which all sites work iff there is a that reaches (x. y) is derivable . . , s, = y in the network such that worh( si) is true for all i = 0,. . . , n that transitive closure cannot be expressed between reachability sical logic. Indeed, it is well known [ l] ). Explicit (cf. vious disadvantages or its topology changes. However, default transitive closure and reachability. storage of inflexibility represented in T cannot be easily represented in clas- sites refers to the transitive closure of a graph; logic in terms of e.g. ground atoms con&( ~1, $2) has the ob- is extended cost, of logic allows for an elegant and maintenance if the network representation in classical first-order Suppose now we observe that site a works but site e is not reachable works(u) and veuches we look for an explanation we disallow an explanation; then immediately indeed, for assertion of facts puth( X, y) >, we find that-as from the formula Vx, y. -7works(x) 3 veuches(y, -veuches (a, e) , and, by the first default rule in D, works(u) from it, i.e., is not the case in the extension of T) . If in terms of sites that are down (whi",
            {
                "entities": [
                    [
                        137,
                        149,
                        "AUTHOR"
                    ],
                    [
                        155,
                        168,
                        "AUTHOR"
                    ],
                    [
                        174,
                        186,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 87 ( 1996) 295-342 Artificial Intelligence A probabilistic framework for cooperative multi-agent distributed interpretation and optimization of communication Y. Xiang” Department of Computer Science, University of Regina, Regina, Sask., Canada S4S OA2 Received September 1995 Abstract Multiply sectioned Bayesian networks for single-agent interpretation systems are extended into a framework for cooperative multi-agent as a Bayesian subnet. We show that the semantics of the joint probability distribution of such a system is well defined under reasonable conditions. systems. Each agent is represented distributed Unlike in single-agent systems where evidence is entered one subnet at a time, multiple agents may acquire evidence asynchronously in parallel. New communication operations are thus pro- posed to maintain global consistency. It may not be practical to maintain such consistency con- stantly due to the inter-agent “distance”. We show that, if the new operations are followed, between two successive communications, answers to queries from an agent are consistent with all local evidence, and are consistent with all global evidence gathered up to the last communication. During a communication operation, each agent is not available to process evidence for a period of time (called @line rime). Two criteria for the minimization of the off-line time, which may commonly be used, are considered. We derive, under each criterion, the optimal schedules when the communication is initiated from an arbitrarily selected agent. 1. Introduction Probabilistic a single-agent representation, reasoning paradigm. That updates in Bayesian networks (BNs) , as commonly applied, assumes is, a single processor accesses a single global network as over the network variables distribution the joint probability * E-mail: yxiang@cs.uregina.ca. 0004-3702/96/$15.00 Copyright @ 1996 Elsevier Science B.V. All rights reserved. SSDIOOO4-3702(95)00110-7 \f296 E Xumg/Arrificial Intellipvzce X7 (1996) 295-342 evidence becomes available and answers queries. Concurrency, marily aims at performance among multiple inference concurrent element junction and decentralization agents with multiple perspectives. The resultant is thus “ftne grained”, e.g., a node in a BN [ 151 or a clique to BNs, pri- [ 9,151, but not at modeling individual tree representation of a BN [ 91. as applied of control in the The single-agent paradigm is inadequate when uncertain are specialized to be addressed. A multi-agent (elements elements of a system between which temporal or semantic issues special each agent domain computational system’s goal cooperatively. that need is an autonomous accesses knowledge, intelligent some external reasoning differently) is performed by there is some “distance”, which may be spatial, [ I]. Such systems pose is thus required where its own partial some the subsystem. Each agent holds source and consumes information to achieve view resource. Each agent communicates with other agents Distributed artificial intelligence (DAI) addresses such “large-grained” coordinating multi-agent in DAI, e.g., blackboard systems [ 51, contract nets analyzing approaches [ 81 are essentially probabilistic approach in DAI. logic based. To our best knowledge, little has been reported the problems systems of designing and [ 2,6]. Main stream [ 31 and open systems to explore the probabilistic the problem of distributed in DAI. As defined originally system accepts evidence of objects and events reports our pilot study on applying reasoning. We address This paper erative multi-agent subclass of problems interpretation level descriptions system cation of all evidence include sensor networks, medical diagnosis by multiple complex artifacts and distributed of cooperative or self-interested paper. is needed when sensors to a centralized for collecting from some environment in the environment. A distributed evidence are distributed, interpretation and communi- site is undesirable. Examples of such systems of systems may consist in this agents image interpretation. Multi-agent agents. We consider only cooperative trouble-shooting specialists, approach interpretation, to coop- a [ 121, an and produces higher by Lesser and Erman inference for single-agent oriented and modular knowledge is based on multiply sectioned Bayesian networks Our representation which were developed and more efficient a natural extension the semantics of the joint probability distribution of a cooperative multi-agent well defined under that are used to maintain that optimize conditions. We propose new communication consistency. We derive communication the time efficiency of the communication. [ 181. We demonstrate into a multi-agent that the modularity of MSBNs allows In particular, we show that is system operations schedules representation, framework. reasonable inter-agent (MSBNs) reasoning [ 191, Section 2 briefly extension is represented semantic agent source, gathers are conditionally common distribution belief of every agent introduces BNs and single-agent MSBNs. Section 3 presents of single-agent MSBNs subnet to multi-agent MSBNs. Each cooperative that consumes its own evidence and can answer queries. When agents are cooperative, its own computational as a Bayesian re- the independent given the intersections of their subdomains and have a initial belief on their of the multi-agent intersections, system then it is shown that a joint probability is uniquely defined and is consistent with the in the system. \fY Xiang/Artijicial Intelligence 87 (1996) 295-342 291 Unlike single-agent systems where evidence is entered one subnet at a time, multiple in parallel. Section 4 discusses consistency- in the system will be globally consistent that arise from the extension. Section 5 adds new belief propagation issues to the set of single-agent MSBN operations agents may acquire evidence asynchronously related erations show that agents are performed. Inter-agent vent constant maintenance operations from an agent are consistent with all local evidence gathered tent with all global evidence gathered up to the last communication. an experimental distributed after the proposed operations cost may pre- consistency. We prove that when the proposed the answers to queries so far and are consis- Section 6 presents op- communication. We how a multi-agent MSBN may be used and the associated communication demonstration task. “distance” of inter-agent are used, between communications, two successive for inter-agent to perform a interpretation operation, each agent is not available time). Such non-availability the length of the off-line to process new evidence restriction on imposes time should be minimized. During a communication for a period of time (called @-line applications. Therefore, time-critical Section 7 defines two criteria commonly multi-agent all agents we abstract the factors the activities during that can be manipulated in the system. To facilitate be used. One is based on the total length of the off-line system. The other is based on the average for the minimization length of the off-line the study of the optimal communication of the off-line time which may time for the entire time across schedules, into a graphical model, and identify the communication in optimizing these schedules. Section 8 reduces scheduling the communication subproblems the duality of the two subproblems. the optimal schedules be derived by solving only one of the subproblems. Section 9 the communication the selected is initiated time when the communication into two independent from an arbitrarily result allows that yield schedules criterion, This for each minimization and establishes communication derives, minimum agent. off-line Section 10 discusses some general issues related to this work. Our presentation as- sumes a general terminology of graph theory. 2. Multiply sectioned Bayesian networks 2.1. Bayesian networks is a triplet A BN [9,11,13,15] (N, E, I’). N is a set of nodes. Each node is labeled with a variable associated with a space. We shall use “node” and “variable” interchange- ably. Therefore, N represents a problem domain. E is a set of arcs such that D = (N, E) is a directed acyclic graph (DAG) . We refer to D as the strucrure of the BN. The arcs signify directed dependencies the linked variables. For each node Ai E N, the strengths of the dependencies on the values of Ai’S parents. For probability any three sets X, Y and Z of variables, X and Y are said to be conditionally independent given Z under probability distribution P if P (XI YZ) = P (XI Z) whenever P (Yz) > 0. in BNs is that a variable The basic dependency between from its parent nodes 7~ are quantified by a conditional distribution p(Ail~i) of Ai conditioned is conditionally assumption embedded \f298 L Xiung/Artijicid Intelligence 87 (1996) 295-342 bcps pn apb mf7_a mmcb mcmp mupl Fig. MSBN I. Left: an example MSBN in the left. Right: a general hypertree for neural muscular diagnosis. Middle: structured MSBN. D’ @ D -:ji ND< D the hypertree organization of the independent probability of its non-descendants given its parents. This assumption allows P, the joint distribution (jpd), to be specified by the product P = nip(A;l’rri). 2.2. Single-agent oriented MSBNs To make the paper self-contained, we briefly introduce the single-agent oriented MS- BNs [ 18,191. An MSBN M consists of a set of interrelated Bayesian subnets. Each subnet represents in a large problem domain or total universe. Each subnet intersection of a subdomain dependencies shares a non-empty between each pair of subnets satisfies set of variables with at least one other subnet. The the d-sepset condition. Definition 1 (d-sepset). (N’ U N’, E’ U E2) and D’ Let D’ = (N’, E’) is a DAG. The intersection (i = I, 2) be two DAGs such that D = I = N’ n N* is a d-sepset between D’ if, for every Ai E I with its parents Z-; in D, either Z-; C: N’ or n-; C N2. It can be shown that, when",
            {
                "entities": [
                    [
                        5548,
                        5555,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 316–361Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAutomated composition of Web services via planning in asynchronousdomainsPiergiorgio Bertoli∗, Marco Pistore, Paolo TraversoFondazione Bruno Kessler, via Sommarive 18, 38100 Povo (Tn), Italya r t i c l ei n f oa b s t r a c tArticle history:Received 9 April 2008Received in revised form 13 November 2009Accepted 26 November 2009Available online 16 December 2009Keywords:PlanningWeb servicesAutomated program synthesisThe service-oriented paradigm promises a novel degree ofinteroperability betweenbusiness processes, and is leading to a major shift in way distributed applications aredesigned and realized. While novel and more powerful services can be obtained, in suchsetting, by suitably orchestrating existing ones, manually developing such orchestrationsis highly demanding,time-consuming and error-prone. Providing automated servicecomposition tools is therefore essential to reduce the time to market of services, andultimately to successfully enact the service-oriented approach.In this paper, we show that such tools can be realized based on the adoption and extensionof powerful AI planning techniques, taking the “planning via model-checking” approachas a stepping stone. In this respect, this paper summarizes and substantially extends aresearch line that started early in this decade and has continued till now. Specifically, thiswork provides three key contributions.First, we describe a novel planning framework for the automated composition of Webservices, which can handle services specified and implemented using industrial standardlike ws-bpel. Since theselanguages for business processes modeling and execution,languages describe stateful Web services that rely on asynchronous communicationprimitives, a distinctive aspect of the presented framework is its ability to model andsolve planning problems for asynchronous domains.Second, we formally spell out the theory underlying the framework, and provide algorithmsto solve service composition in such framework, proving their correctness andcompleteness. The presented algorithms significantly extend state-of-the-art techniquesfor planning under uncertainty, by allowing the combination of asynchronous domainsaccording to behavioral requirements.Third, we provide and discuss an implementation of the approach, and report extensiveexperimental results which demonstrate its ability to scale up to significant cases forwhich the manual development of ws-bpel composed services is far from trivial and timeconsuming.© 2009 Elsevier B.V. All rights reserved.1. IntroductionSince its inception, the Web has maintained a fast growth rate in terms of quantity and variety of contained information,becoming a reference information source for billions of users and business entities world-wide. In particular, in the last fewyears, the economical impact of the Web has grown substantially, due to the fact that the Web is not used anymore just topresent static information, but, more and more, to expose services with which a Web user (or a different Web-exposed ser-* Corresponding author.E-mail addresses: bertoli@fbk.eu (P. Bertoli), pistore@fbk.eu (M. Pistore), traverso@fbk.eu (P. Traverso).0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.12.002\fP. Bertoli et al. / Artificial Intelligence 174 (2010) 316–361317vice) can actively interact. This has enacted a range of Web-based solutions for commercial, learning and health-care activ-ities, and gave a substantial push to e-Commerce, e-Learning, and e-Health initiatives (see e.g. [99,51,35,55,92,80,52,33,68]).This has been the starting point for the emergence of a Service Oriented Computing paradigm, which envisages theadoption of standards for the publication and access of services over the Web, so to allow the interoperability of inde-pendently developed procedures over the Web. In such a setting, existing services can be suitably combined by means ofWeb-based “orchestration” services, so to realize novel and more complex procedures that satisfy some given user or busi-ness requirement. For instance, different services taking care of specific aspects related to the organization of a trip (e.g.flight booking, lodging, bank payment, and so on) can be suitably coordinated by an integrated “trip adviser” service, whoseadoption may save a customer considerable time and effort in setting up a trip. Indeed, being able to build new services bycomposing existing ones is crucial to the actual enactment of the service-oriented paradigm. However, the task of manuallydeveloping such orchestrations is extremely difficult, time-consuming and error-prone, even for experienced designers andprogrammers. This calls for the design of effective support techniques and automated tools capable of synthesizing serviceorchestrations starting from suitable high-level composition requirements.In this context, planning has proved to be one of the most promising techniques for the automated composition of Webservices. Several works in planning have addressed different aspects of this problem, see, e.g., [100,59,34,65,83,17,73,8,84,4,43]. In these works, automated composition is described as a planning problem: services that are available and published onthe Web, the component services, are used to construct the planning domain, composition requirements can be formalized asplanning goals, and planning algorithms can be used to generate composed services, i.e., plans that compose the componentservices. These works, which provide different technical solutions, share the conception of services as stateless entities,which enact simple query–response protocols.An even more difficult challenge for planning is the automated composition of Web services at the process level, i.e., thecomposition of component services that consist of stateful business processes, capable to establish complex multi-phaseinteractions with their partners. Indeed, in the large majority of real cases, services cannot be considered simply as atomiccomponents, which, given some inputs, return some outputs, in a single request–response step. On the contrary, in mostapplication domains, they need to be represented as stateful processes that realize interaction protocols which may involvedifferent sequential, conditional, and iterative steps. For instance, we cannot in general interact with a “flight booking” ser-vice in an atomic step. The service may require a sequence of different operations including an authentication, a submissionof a specific request for a flight, the possibility to submit iteratively different requests, acceptance (or refusal) of the offer,and finally, a payment procedure. In these cases, the process, i.e. the published interaction flow, is the key aspect to beconsidered when (automatically) composing services.The planning problem corresponding to the automated composition of services that are published as processes is farfrom trivial. First, component services cannot be simply represented as atomic actions of the planning domain. As a con-sequence, it is not obvious, like in the case of atomic component services, which is the planning domain that correspondsto the composition problem. Second, in realistic cases, component services publish nondeterministic and partially observ-able behaviors, since, in general, the outputs of a service cannot be predicted a priori and its internal status is not fullyavailable to external services. For instance, whether a payment transaction will succeed cannot be known a priori of itsexecution, and whether there are still seats available on a flight cannot be known until a specific request is submitted tothe service. Third, the plan that coordinates the component services cannot be simply a sequence of actions that call atomiccomponents; rather, it needs to interleave the (partial execution of) component services with typical programming languageconstructs such as conditionals and loops. Finally, Web service interactions are typically asynchronous: each process evolvesindependently and with unpredictable speed, and interacts with the other processes only through asynchronous messageexchanges. Message queues are used in practical implementations to guarantee that processes do not lose messages thatthey are not ready to receive.As a consequence of all these characteristics of Web services, it is far from obvious how their automated compositioncan be adequately represented as a planning problem. Moreover, their nondeterministic, partially observable, and asyn-chronous behavior poses strong requirements and introduce novel problems for the planning techniques that can be used.This has led the authors of this paper and their colleagues to investigate a research line on service oriented composition,that started with [71,95], and continued with [78,79,74,61,62], up to date [60,63,72,64,21]. While distinct for the technicalsolutions and degree of maturity, these works share two general ideas. The first consists in taking, as an algorithmic andtechnological baseline, the “planning via model checking” approach devised by the authors of this paper together with othercolleagues [16,77,32,15]. Such approach combines state-of-the-art performance with the ability to deal with general formsof nondeterminism, therefore tackling effectively one of the critical aspects implied by the nature of Web services. Thesecond idea is to face actual composition problems by pragmatically considering services expressed using de-facto standardlanguages such as ws-bpel [1,31]. While this choice renders the problem further complex, it is strongly motivated by theobjective to provide usable tools.This paper summarizes and significantly extends a large portion of the corpus of work presented in [71,95,78] and [79,74,61,62,60,63,72,64], providing for the first time both a comprehensive survey of the framework und",
            {
                "entities": [
                    [
                        209,
                        228,
                        "AUTHOR"
                    ],
                    [
                        231,
                        244,
                        "AUTHOR"
                    ],
                    [
                        246,
                        260,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 80 ( 1996) 243-308 Artificial Intelligence ,4 logic of time, chance, and action for representing plans Department of Electrical Engineering and Computer Science, University of Wisconsin-Milwaukee, Peter Haddawy * Milwaukee, WI 53201, USA Received November 1992; revised September 1994 Abstract language approaches This paper to the representation notions about time, chance, and action central logical and probabilistic a first-order integrates problems by developing explicit and precise commonsense problem. We then develop a logic, the semantics of which incorporates The logical over time points, probability values, and domain operator The can represelnt distinguishes the logic and show how the logic can be used to describe actions descriptions of planning logic of time, chance, and action. We start by making to the planning these intuitive properties. integrates both modal and probabilistic constructs and allows quantification is treated as a sentential in the language, so it can be arbitrarily nested and combined with other logical operators. It the chance the future. The model of action between action feasibility, executability, and effects. We present a proof theory for in such a way that the action to infer properties of plans via the proof theory. that facts hold and events occur at various that actions and other events affect language can represent individuals. Probability can be composed the chance times. 1. Introduction Most AI planning systems to represent planning to date [ 7,18,47,55] knowledge. This has limited as well as to deal with uncertainty have used variations of situation to represent in the planning domain. Re- their ability in AI have used temporal [ 1,20,40,45,50] logics to formalize and they have developed probability the temporal aspects of planning logics for representing [ 3,24,26]. But little work has been done toward developing lan- in the context of planning. a formal these two representational capabilities calculus temporal knowledge searchers problems uncertainty guage that integrates * E-mail: haddawy@cs.uwm.edu. 0004-3702/96/$15.00 SSD10004-3702(94)00070-O @J 1996 Elsevier Science B.V. All rights reserved \f244 P Huddawy/Art@cial Intelligence 80 (1996) 243-308 temporal and probabilistic This paper presents a logic for representing representations integrating represent aspects of planning problems that cannot be represented separately. We exploit one of the advantages of using a logical commonsense commonsense a language planning realistic domains. time, chance, and action. We show that by formalism we can if the two are treated language by building theory, Thus from the semantics of the language. Since such of for more notions of time, chance, and action directly inferences can be used follow directly to analyze planning a significant problems and prove step toward building planners algorithms, ’ into the model the correctness in a common it represents In the present work, planning is viewed as the process of formulating and choosing a set of actions which when executed will likely achieve a desirable outcome. Actions in a plan may be performed agent, to affect the state of the world, or simply for their own sake. The present work focuses on the last two types of action. To choose appropriate courses of such action, an agent in the world that his actions can must reason about influence and the extent in the world that influence his actions and the extent of that influence. to affect the state of knowledge of the performing the state of the world, conditions them, and, conversely, conditions to which he can influence Many aspects of the world are inherently for reason- in the world as well as in the effects of actions and events. For example, smoking does not deter- lung factors can influence a smoker’s chance of contracting ing about plans must be able to express chances of conditions indeterminacy ministically cancer. Uncertain cancer as can uncertainty cause lung cancer; environmental increases one’s chance of contracting so a representation it only greatly stochastic, Reasoning about plans requires to reason about time. Facts tend to be true for periods of time, and actions and events occur at particular a plan may occur sequentially not the past. Chance evolves with time: same now as it will be tonight. Ambiguities a fair coin is flipped landed heads or it certainly did not. This paper presents a first-order or concurrently. Actions and events affect the future, but the chance of rain tomorrow may not be the in the world are resolved with time: before the chance of heads is 50% but after it is flipped it either certainly logic of time, chance, and action times. Actions comprising is represented about plans. The developed Possibility relation over the world-histories. Chance and reasoning world-histories. an accessibility eralized Kripke structures By integrating resent and distinguish quantification is treated as a sentential operator combined with other logical operators. The probability so it can capture between possibility, probability, the dynamic nature of chance. both modal and probabilistic over time points, probability in terms of probability in the language, constructs, logic represents in terms Kripke structures time for representing in terms of possible [34] by defining is represented by defining gen- distributions over the world-histories. values, and domain the logical language can rep- and truth. The language allows individuals. Probability so it can be arbitrarily nested and indexed is temporally operator ’ Haddawy ( 2 I 1 uses the logic presented here to analyze and prove correct components of a construction planning system. in the effects of smoking. the ability \fI! Haddawy/Art@cial Intelligence 80 (1996) 243-308 245 Our modlel of possibility is similar to that presented by Pelavin of our logic is similar time is based on that of van Fraassen [26] probability to Halpem’s [43]. Our model of [57]. The logic & in the context of branching chance probabilistic to Bacchus’ logic of staged probabilities. component [ 31 logic of propositional probabilities, and to Haddawy and Frisch’s [ 241 1.1. Temporal logic Temporal logics represent change by specifying what is true in the world at various logic in logics which associate a time interval with each temporal the use of intervals, actions, events, and facts can have temporal extent. actions as the action. For this in this paper uses time intervals times. They can be classified as being either interval- or point-based. A point-based associates a time point with each temporal object. Most work on plan representation AI has use’d interval-based object. Through This means well as conditions reason, as well. the logic of time, chance, and action developed languages can represent plans with concurrent the execution of an action that these temporal that influence during Temporal logics logics can be further classified as being either linear or branching. Linear [ 1,2] model only that an the actual world and thus can only time event actually occurs at a given model all possible worlds and thus can represent whether or not an event can possibly occur, as well as its various possible in thought of as a degree of possibility, we will representing use a branching over the branching is modeled by defining probability distributions times of occurrence. Since we are interested chance, which can be roughly time. In contrast, branching time logic. Chance time structure. time logics [20,40,45] represent 1.2. Probalnlity school Semantic a logical relationship The logical to represent [ 58,591, and subjectivist into [ 37,46,48]. between a given hypothesis theories of probability may be classified three main schools: theory the subjective fcundation. But while subjective probability logi- cal [ 5,321, frequentist takes probabilitie,s and given evidence. The frequentist view identifies probability with some suitably defined relative the degrees of belief frequency. The subjectivist takes probability has the most solid of a rational agent. Of the three views, interpretation theory dictates how degrees of semantic for structuring knowl- belief should be represented, about the effects of actions edge about the world. Since we are interested to account for of uncertainty on the state of the world, we would like our representation theories some aspects of probabilistic of objective chance constraints on beliefs The current work builds upon van Fraassen’s model of objective chance. it does not provide us with guidance for causality by imposing causality. Subjectivists some additional have proposed that account in reasoning to represent [39,53,57]. subjective But probability plans. Probability tification provides great representational theory alone is not representationally theory does not include a notion of quantification. about First-order quan- economy by allowing us to describe properties for reasoning adequate \flifting action shared by general classes of actions, events, and facts. For example, to define a different of lifting actions, where the object being more, probability representing p. 5701. We would planning problems. This paper addresses both these limitations. rather than having the class lifted is left as a quantified variable. Further- [ 60, for describing planning problems features of for each possible object, we can describe theory provides no vocabulary like a language that facilitates the salient 1.3. Use of the logic We show in Section 4 that the logic of time, chance, and action the logic problems. Even So we do not foresee building to be useful. Rather, and analysis of planning problems axiomatizable. means of solving planning too inefficient representation us to design planning algorithms the data structures and assumptions of a planner the meaning of the knowledge we can analyze what is entailed by this knowledge without algorithm correct. is intended involving for these problems itself. We can i",
            {
                "entities": [
                    [
                        221,
                        234,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 91 ( 1997) 225-256 Artificial Intelligence The computer revolution in science: steps towards the realization of computer-supported discovery environments Hidde de Jong a**, Arie Rip b,l B Knowledge-Based Systems Group, Department of Computer Science, University of Twente, PO. Box 217, 7500 AE Enschede, Netherlands h School of Philosophy and Social Sciences, Universify of Twente, PO. Box 217, 7500 AE Enschede, Netherlands Received December 1995; revised September 1996 Abstract The tools that scientists use in their search processes together form so-called discovery environ- ments. The promise of artificial intelligence and other branches of computer science is to radically transform conventional discovery environments by equipping scientists with a range of powerful computer tools including large-scale, shared knowledge bases and discovery programs. We will describe the future computer-supported discovery environments that may result, and illustrate by means of a realistic scenario how scientists come to new discoveries in these environments. In order to make the step from the current generation of discovery tools to computer-supported discovery environments like the one presented in the scenario, developers should realize that such environments are large-scale sociotechnical systems. They should not just focus on isolated computer programs, but also pay attention to the question how these programs will be used and maintained by scientists in research practices. In order to help developers of discovery programs in achieving the integration of their tools in discovery environments, we will formulate a set of guidelines that developers could follow. @ 1997 Elsevier Science B.V. Keywords: Scientific discovery; Computer-supported technology discovery environments; Social studies of science and * Corresponding ’ E-mail: a.rip@wmw.utwente.nl. author. E-mail: hdejong@cs.utwente.nl. 0004.3702/97/$17.00 PII SOOO4-3702(97)0001 1-8 @ 1997 Elsevier Science B.V. All rights reserved. \f226 H. de Jong. A. Rip/Artificial Intelligence 91 (1997) 225-256 1. Introduction Knowledge collection of experimental hnd- explanatory models, and theories, which continuously in a scientific domain and patterns, the activities of scientists. Scientists are engaged is a heterogeneous search in which they construct new empirical phenomena, devise models accounting relating a wide range of empirical phenomena. in open-ended theories regularities through institutes, or in field work at distant ings, develops processes, for these findings and propose These search processes are local: in research a consequence processes convinced of the value of the results, edge in a domain may lead to further research and further discoveries, dynamic of science. In structuring the shared body of knowledge of the communication to relevant locations. The generality of science they take place at laboratory benches, behind desks is in local search are the claims are added to the shared body of knowl- [ 521. The extension of the body of knowledge with new discoveries thus giving rise to the characteristic audiences within a scientific community. of the discovery claims generated If these audiences systematized and rules are sometimes they work. These heuristics scientists make use of the heuristics of science have for instance described their activities, and the skills these activities exemplify, in their domain and performing disco- and rules of the research practice very activities, in which into ex- plicit methods and made available as tools to scientists grappling with similar problems, Philosophers and evaluated how experimenters reconstruct others can use in order to judge and replicate zation of scientific work and knowledge may of a practice, a specific, self-contained performing days measured by putting electrodes Within embodied. The that into procedures [ 21,471. The systemati- to the further material equipment into an apparatus its pH, is nowa- subtask. The acidity of a liquid, in the liquid and reading off the pH from a scale. are their findings lead such a piece of lab equipment for instance when heuristics a number of electrochemical and rules are embodied thus constructed, that have been regularities tools together form a discovery [ 141. Historians of discovery environments, that Boyle used for his pneumatic terial devices, their search processes descriptions instruments ern biological installations lab focusing on neuroendocrinological environment, in which scientists both methods and procedures and ma- can pursue and philosophers of science have given detailed such as the air-pump and related methods and around 1660 [ 541, a mod- [ 371, and the complex experiments research for running experiments in high-energy physics [ 201. The promise of AI, and of computer science in general, is to transform conventional and material equipment of a research practice. Systematic methods experiments environments and planning through widening discovery systematization tasks like designing of data are now being implemented atizations of bodies of knowledge, files with experimental databases existing supported discovery environments. This development and knowledge scientific discovery environments, results bases give rise (Fig. 1). The resulting the scope and increasing the depth of the for in large amounts system- in the form of handbooks or ordered in automatic discovery programs. Existing for instance and finding regularities computer in tools, to what we will call computer- to speculate about has led some integrated in office drawers, are transformed into machine-readable \fH. de Jong, A. Rip/Artijcial intelligence 91 (1997) 225-256 221 RESEARCH PRACTICE systematic scientific method scientific discovery program I. Systematization Fig. practice. and computerization of the body of knowledge and scientific activities in a research “radical, and perhaps surprising, (Allen Newell, quoted with new and promising environments, in [ 61) . How present, well-tested AI techniques, and what this means for science, techniques, can help create these computer-supported is the topic of this article. 2 transformations of the disciplinary structure of science” supplemented discovery understood then argue regulation mechanism are used by scientists the computer-supported discovery environments In Section 2, we will describe and reinforce our points, a realistic in the E. coli bacterium will be introduced. of the in their search future and the way these discovery environments scenario of the discovery processes. To illustrate In of a new genetic cannot Section 3 we will tools, but that we have to be adequately in a research practice. This widening of view them as sociotechnical tools: we should not just focus perspective has implications to the way on the task performance of isolated computer programs, but also pay attention in a discovery environment. Section 4 reviews how far we have come they are organized that toward the realization of computer-supported tools in research practices have the integration of discovery especially hardly been touched upon. In Section 5 we will therefore present a number of guidelines that computer-supported as a mere collection of computer systems embedded for the design of discovery discovery environments discovery environments issues surrounding and concludes * Amidst the enthusiasm about future computer-supported that the emergence of ideas on the large-scale techniques. however, with the availability of advanced computer age have shown much Leibniz, among others, propagated of an am inveniendi to derive new discoveries in systematizing interest from these histories. discovery systematization In fact, philosophers environments, we should not forget, of scientific practices did not coincide from the past as well as from our and activities. For example, Bacon and scientific knowledge the construction of natural and experimental histories and the development \f228 H. de Jong, A. Rip/Artificial Intelligence 91 (1997) 225-256 that developers other computer and offers some concluding could tools remarks. follow when in practices of scientists. Section 6 recapitulates they strive at embedding discovery programs and the main points 2. Search processes in computer-supported discovery environments technical and knowledge to propose a detailed In the computer-supported think of them as being built around relatively passive resources discovery environments systems will find a place alongside other tools already available architecture that are now envisaged, powerful to scientists. of discovery like like hypothesis gen- (as suggested by Fig, 1). tools will also form a part of the discovery of the future. In addition we will meet with the conventional measuring equipment discovery currently and retain ele- gradually develop discovery Although we do not intend systems, one could databases simulators, erators, process Telephone, e-mail, and other communication environments and analytic environments ments from them. computer-supported environments bases, and active discovery programs from existing discovery in laboratories; assistants revision theory found and across results obtained A computer-supported at laboratories discovery environment to a distributed database, which stores experimental is not just a loose aggregate of tools, related and adjusted, so that for instance sent in millions discovery environment but an integrated system. The tools are mutually the results of a bioassay can be processed by an analysis program and subsequently forward of bioassays performed computer-supported but is imposed by the research practice Through relationships established. These who know how to handle and combine Think, for example, of program manuals and operating and maintenance also of relationships connections. the world. The order and structure in a is not an inherent characteristic of the tools, is embedded. in the search processes of s",
            {
                "entities": [
                    [
                        187,
                        200,
                        "AUTHOR"
                    ],
                    [
                        206,
                        214,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 776–804www.elsevier.com/locate/artintAn executable specification of a formal argumentation protocolAlexander Artikis a,∗, Marek Sergot b, Jeremy Pitt ca Institute of Informatics & Telecommunications, NCSR “Demokritos”, Athens, 15310, Greeceb Department of Computing, Imperial College London, SW7 2AZ, UKc Department of Electrical & Electronic Engineering, Imperial College London, SW7 2BT, UKReceived 8 November 2006; received in revised form 3 April 2007; accepted 16 April 2007Available online 29 April 2007AbstractWe present a specification, in the action language C+, of Brewka’s reconstruction of a theory of formal disputation originallyproposed by Rescher. The focus is on the procedural aspects rather than the adequacy of this particular protocol for the conduct ofdebate and the resolution of disputes. The specification is structured in three separate levels, covering (i) the physical capabilitiesof the participant agents, (ii) the rules defining the protocol itself, specifying which actions are ‘proper’ and ‘timely’ according tothe protocol and their effects on the protocol state, and (iii) the permissions, prohibitions, and obligations of the agents, and thesanctions and enforcement strategies that deal with non-compliance. Also included is a mechanism by which an agent may objectto an action by another participant, and an optional ‘silence implies consent’ principle. Although comparatively simple, Brewka’sprotocol is thus representative of a wide range of other more complex argumentation and dispute resolution procedures that havebeen proposed. Finally, we show how the ‘Causal Calculator’ implementation of C+ can be used to animate the specification andto investigate and verify properties of the protocol.© 2007 Elsevier B.V. All rights reserved.Keywords: Argumentation; Disputation; Protocol; Norm; Multi-agent system; Specification; Action language1. IntroductionOne of the main tasks in the formal specification and analysis of (open) multi-agent systems (MAS) is the represen-tation of the protocols and procedures for agent interactions, and the norms of behaviour that govern these interactions.Examples include protocols for exchanging information, for negotiation, and for resolving disputes.It has been argued that a specification of systems of this type should satisfy at least the following two requirements:first, the interactions of the members should be governed by a formal, declarative, verifiable and meaningful seman-tics [64]; and second, to cater for the possibility that agent behaviour may deviate from what is prescribed, agentinteractions can usefully be described in terms of permissions and obligations [26].We have been developing a theoretical framework for the executable specification of open agent systems thataddresses the aforementioned requirements [3–5]. We adopt the perspective of an external observer, thus taking intoaccount only externally observable behaviours and not the internal architectures of the individual agents, and view* Corresponding author.E-mail addresses: a.artikis@acm.org (A. Artikis), mjs@doc.ic.ac.uk (M. Sergot), j.pitt@imperial.ac.uk (J. Pitt).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.008\fA. Artikis et al. / Artificial Intelligence 171 (2007) 776–804777agent systems as instances of normative systems [26] whereby constraints on agents’ behaviour (or social constraints)are specified in terms of their permissions, their institutional power to effect changes and bring about certain states ofaffairs, and their rights and obligations to one another. We employ an action formalism to specify the social constraintsgoverning the behaviour of the members and then use a computational framework to animate the specification andinvestigate its properties. For the action formalism, we have employed the Event Calculus [29], the action languageC+ [22], and an extended form of C+ specifically designed for modelling the institutional aspects of agent systems[57–59].In this paper we demonstrate how the theoretical and computational frameworks can be used with the languageC+ to specify and execute an argumentation protocol based on Brewka’s reconstruction [8], in the Situation Calculus[52], of a theory of formal disputation originally proposed by Rescher [53]. We presented a preliminary formulationin an earlier paper [4]. This present paper is a refined and much extended version.We are focusing here on the procedural aspects of the protocol rather than on the underlying logic of disputationemployed by Brewka or on the adequacy of this particular protocol for the conduct of debate and the resolution ofdisputes. The features of Brewka’s protocol are representative of a wide range of other more complex argumentationand dispute resolution procedures that have been proposed in the literature, and to which the methods of this papercan be similarly applied.The specification of the argumentation protocol is structured into three separate levels, covering:(i) the physical capabilities of the participant agents (in the present context, the messages/utterances each agent isactually capable of transmitting);(ii) the rules defining the protocol itself, specifying which actions are ‘proper’ and ‘timely’ according to the protocoland their effects on the protocol state;(iii) the permissions, prohibitions and obligations of the agents, and the sanctions and enforcement strategies that dealwith non-compliance.In any given implementation of the protocol, it may or may not be permitted for an agent to perform an action that isnot proper or timely; conversely, there may be protocol actions that are proper and timely but that are nevertheless notpermitted under certain circumstances, because, for instance, they lead to protocol runs with undesirable properties.The rules comprising level (ii) of the specification correspond to constitutive norms that define the meaning of theprotocol actions. Levels (i) and (iii), respectively, can be seen as representing the physical and normative environmentwithin which the protocol is executed. We have also been concerned with the concept of social role. Briefly, a role isassociated with a set of (role) preconditions that agents must satisfy in order to be eligible to occupy that role and aset of (role) constraints that govern the behaviour of the agents once they occupy that role. We will not discuss roleassignment in this paper. For the example in this paper, we will assume for simplicity that the participant agents arealready assigned to certain roles, and that these roles do not change during an execution of the protocol.A note on terminology. In the earlier version of this paper [4], and in the treatment of other examples, we defineda protocol by specifying the conditions under which an action was said to be ‘valid’ according to the protocol. Here,we have employed a finer structure, further classifying ‘valid’ actions as proper or timely, in line with suggestionsthat have also been made by Prakken et al. [45,49]. A ‘valid’ action in our earlier terminology is one that is bothproper and timely. Other terminology in common use employs the term ‘successful’ where we say ‘valid’: one thendistinguishes between an action, such as an utterance or the transmission of a message of a certain form, which isan ‘attempt’ to make a claim, say, and the conditions under which the attempt to claim is ‘successful’ (sometimes,‘effective’). We prefer to avoid the term ‘successful’ since even an unsuccessful ‘attempt’ can have effects on theprotocol state. We also avoid use of the term ‘legal’ for ‘valid’ or ‘successful’ since it is ambiguous as to whetherit refers to the constitutive element of the protocol (level (ii) of our specification) or the normative environment inwhich the protocol is executed (level (iii)). Also related is the concept of institutional (or ‘institutionalised’) power(sometimes, ‘competence’ or ‘capacity’). This refers to the characteristic feature of institutions—legal systems, formalorganisations, or informal groupings—whereby designated agents, often when acting in specific roles, are empoweredto create or modify facts of special significance in that institution—institutional facts in the terminology of Searle [56].(See e.g. [27,35] for further discussion and references to the literature.) Thus in the present example it is natural tosay that, under certain circumstances, an agent acting in a certain role has power (competence, capacity) to declarethe dispute resolved in favour of one or other of the protagonists; or that in certain circumstances an agent has power\f778A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804to object to an action by one of the other participants; or more generally, that the argumentation protocol definesthe conditions under which an agent has the power to perform one of the argumentation actions. We will not referexplicitly to power in the specification of the argumentation protocol presented here. The classification of actions intoproper and timely already provides a more detailed specification.In this paper we use the language C+ to formulate the specification. An advantage of C+, compared with otheraction formalisms, is that it can be given an explicit semantics in terms of transition systems. This enables us toanalyse and prove properties of the protocol. The concluding sections of the paper present some illustrative examples.The paper is structured as follows. First, we briefly describe the C+ language. Second, we present the ‘Causal Cal-culator’ software implementation, a computational framework for executing specifications formalised in C+. Third,we summarise Brewka’s reconstruction of Rescher’s theory of formal disputation. Fourth, we specify, prove propertiesof, and execute (a form of) Brewka’s argumentation protocol with the use of C+ and the Causal Calculator. Finally,we discuss related research, summarise the presented work, and",
            {
                "entities": [
                    [
                        134,
                        151,
                        "AUTHOR"
                    ],
                    [
                        157,
                        169,
                        "AUTHOR"
                    ],
                    [
                        173,
                        184,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 173 (2009) 593–618Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA heuristic search approach to planning with temporally extendedpreferencesJorge A. Baier a,b,∗, Fahiem Bacchus a, Sheila A. McIlraith aa Department of Computer Science, University of Toronto, Canadab Department of Computer Science, Pontificia Universidad Católica de Chile, Chilea r t i c l ei n f oa b s t r a c tArticle history:Received 27 October 2007Received in revised form 28 November 2008Accepted 28 November 2008Available online 6 December 2008Keywords:Planning with preferencesTemporally extended preferencesPDDL3Planning with preferences involves not only finding a plan that achieves the goal,itrequires finding a preferred plan that achieves the goal, where preferences over plansare specified as part of the planner’s input. In this paper we provide a technique foraccomplishing this objective. Our technique can deal with a rich class of preferences,including so-called temporally extended preferences (TEPs). Unlike simple preferences whichexpress desired properties of the final state achieved by a plan, TEPs can express desiredproperties of the entire sequence of states traversed by a plan, allowing the user to expressa much richer set of preferences. Our technique involves converting a planning problemwith TEPs into an equivalent planning problem containing only simple preferences. Thisconversion is accomplished by augmenting the inputed planning domain with a new set ofpredicates and actions for updating these predicates. We then provide a collection of newheuristics and a specialized search algorithm that can guide the planner towards preferredplans. Under some fairly general conditions our method is able to find a most preferredplan—i.e., an optimal plan. It can accomplish this without having to resort to admissibleheuristics, which often perform poorly in practice. Nor does our technique require anassumption of restricted plan length or make-span. We have implemented our approachin the HPlan-P planning system and used it to compete in the 5th International PlanningCompetition, where it achieved distinguished performance in the Qualitative Preferencestrack.© 2008 Elsevier B.V. All rights reserved.1. IntroductionClassical planning requires a planner to find a plan that achieves a specified goal. In practice, however, not every planthat achieves the goal is equally desirable. Preferences allow the user to provide the planner with information that it canuse to discriminate between successful plans; this information allows the planner to distinguish successful plans based onplan quality.Planning with preferences involves not just finding a plan that achieves the goal, it requires finding one that achieves thegoal while also optimizing the user’s preferences. Unfortunately, finding an optimal plan can be computationally expensive.In such cases, we would at least like the planner to direct its search towards a reasonably preferred plan.In this paper we provide a technique for accomplishing this objective. Our technique is able to deal with a rich classof preferences. Most notably this class includes temporally extended preferences (TEPs). The difference between a TEP and aso-called simple preference is that a simple preference expresses some desired property of the final state achieved by the* Corresponding author at: Department of Computer Science, University of Toronto, Canada.E-mail addresses: jabaier@cs.toronto.edu (J.A. Baier), fbacchus@cs.toronto.edu (F. Bacchus), sheila@cs.toronto.edu (S.A. McIlraith).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.11.011\f594J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618plan, while a TEP expresses a desired property of the sequence of states traversed by the plan. For example, a preferencethat a shift worker work no more than 2 overtime shifts in a week is a temporally extended preference. It expresses acondition on a sequence of daily schedules that might be constructed in a plan. Planning with TEPs has been the subject ofrecent research (e.g. [6,12,35]). It was also a theme of the 5th International Planning Competition (IPC-5).The technique we provide in this paper is able to plan with a class of preferences that includes those that can bespecified in the planning domain definition language PDDL3 [23]. PDDL3 was specifically designed for IPC-5. It extendsPDDL2.2 to include, among other things, facilities for expressing both temporally extended and simple preferences, wherethe temporally extended preferences are described by a subset of linear temporal logic (LTL). It also supports quantifying thevalue of achieving different preferences through the specification of a metric function. The metric function assigns to eachplan a value that is dependent on the specific preferences the plan satisfies. The aim in solving a PDDL3 planning instanceis to generate a plan that satisfies the hard goals and constraints while achieving the best possible metric value, optimizingthis value if possible or at least returning a high value plan if optimization is infeasible.Our technique is a two part approach. The first part exploits existing work [2] to convert planning problems with TEPsto equivalent problems containing only simple preferences defined over an extended planning domain. The second part,and main contribution of our work, is to develop a set of new heuristics, and a search algorithm that can exploit theseheuristics to guide the planner towards preferred plans. Many of our heuristics are extracted from a relaxed planninggraph a technique that has previously been used to compute heuristics in classical planning. Previous heuristics for classicalplanning, however, are not well suited to planning with preferences. The heuristics we present here are specifically designedto address the tradeoffs that arise when planning to achieve preferences.Our search algorithm is also very different from previous algorithms used in planning. As we will show, it has a numberof attractive properties, including the ability to find optimal plans without having to resort to admissible heuristics. This isimportant because admissible heuristics generally lead to unacceptable search performance. Our method is also able to findoptimal plans without requiring a restriction on plan length or make-span. This is important because such restrictions donot generally allow the planner to find a globally optimal plan. In addition, the search algorithm is incremental in that itfinds a sequence of plans each one improving on the previous. This is important because in practice it is often necessary totrade off computation time with plan quality. The first plans in this sequence of plans can often be generated fairly quicklyand provide the user with at least a working plan if they must act immediately. If more time is available the algorithmcan continue to search for a better plan. The incremental search process also employs a pruning technique to make eachincremental search more efficient. The heuristics and search algorithm presented here can easily be employed in otherplanning systems.An additional contribution of the paper is that we have brought all of these ideas together into a working planningsystem called HPlan-P. Our planner is built as an extension of the TLPlan system [1]. The basic TLPlan system uses LTLformulae to express domain control knowledge; thus, LTL formulae serve to prune the search space. However, TLPlan has nomechanism for providing heuristic guidance to the search. In contrast, our implementation extends TLPlan with a heuristicsearch mechanism that guides the planner towards plans that satisfy TEPs, while still pruning those partial plans that violatehard constraints. We also exploit TLPlan’s ability to evaluate quantified formulae to avoid having to convert the preferencestatements (many of which are quantified) into a collection of ground instances. This is important because grounding thepreferences can often yield intractably large domain descriptions. We use our implementation to evaluate the performanceof our algorithm and to analyze the relative performance of different heuristics on problems from both the IPC-5 Simple andQualitative Preferences tracks.In the rest of the paper we first provide some necessary background. This includes a brief description of the featuresof PDDL3 that our approach can handle. In Section 3 we describe the first part of our approach—a method for compilinga domain with temporally extended preferences into one that is solely in terms of simple (i.e., final state) preferences.Section 4 describes the heuristics and search algorithm we have developed. It also presents a number of formal propertiesof the algorithm, including characterizing various conditions under which the algorithm is guaranteed to return optimalplans. Section 5 presents an extensive empirical evaluation of the technique, including an analysis of the effectiveness ofvarious combinations of the heuristics presented in Section 4. Section 6 presents a discussion of the approach and Section 7summarizes our contributions and discusses related work after which we provide some final conclusions.2. BackgroundThis section reviews the background needed to understand this paper. Section 2.1 presents some basic planning defi-nitions and a brief description of the planning domain definition language PDDL. Section 2.2 describes a variation of thewell-known approach to computing domain-independent heuristics based on the computation of relaxed plans that is usedby our planner to compute heuristics. As opposed to most well-known approaches, our method is able to handle ADL do-mains directly without having to pre-compile the domain into a STRIPS domain. Section 2.3 describes the planning domaindefinition language PDDL3, a recent version of PDDL that enables the definition of hard constraints, preferences, and",
            {
                "entities": [
                    [
                        233,
                        247,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 175 (2011) 487–511Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintOnline planning for multi-agent systems with bounded communicationFeng Wu a,b,∗, Shlomo Zilberstein b, Xiaoping Chen aa School of Computer Science, University of Science and Technology of China, Jinzhai Road 96, Hefei, Anhui 230026, Chinab Department of Computer Science, University of Massachusetts at Amherst, 140 Governors Drive, Amherst, MA 01003, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 2 February 2010Received in revised form 22 September2010Accepted 26 September 2010Available online 29 September 2010Keywords:Decentralized POMDPsCooperation and collaborationPlanning under uncertaintyCommunication in multi-agent systemsWe propose an online algorithm for planning under uncertainty in multi-agent settingsmodeled as DEC-POMDPs. The algorithm helps overcome the high computationalcomplexity of solving such problems offline. The key challenges in decentralized operationare to maintain coordinated behavior with little or no communication and, whencommunication is allowed, to optimize value with minimal communication. The algorithmaddresses these challenges by generating identical conditional plans based on commonknowledge and communicating only when history inconsistency is detected, allowingcommunication to be postponed when necessary. To be suitable for online operation,the algorithm computes good local policies using a new and fast local search methodimplemented using linear programming. Moreover, it bounds the amount of memory usedat each step and can be applied to problems with arbitrary horizons. The experimentalresults confirm that the algorithm can solve problems that are too large for the bestexisting offline planning algorithms and it outperforms the best online method, producingmuch higher value with much less communication in most cases. The algorithm also provesto be effective when the communication channel is imperfect (periodically unavailable).These results contribute to the scalability of decision-theoretic planning in multi-agentsettings.© 2010 Elsevier B.V. All rights reserved.1. IntroductionA multi-agent system (MAS) consists of multiple independent agents that interact in a domain. Each agent is a decisionmaker that is situated in the environment and acts autonomously, based on its own observations and domain knowledge, toaccomplish a certain goal. A multi-agent system design can be beneficial in many AI domains, particularly when a systemis composed of multiple entities that are distributed functionally or spatially. Examples include multiple mobile robots(such as space exploration rovers) or sensor networks (such as weather tracking radars). Collaboration enables the differentagents to work more efficiently and to complete activities they are not able to accomplish individually. Even in domains inwhich agents can be centrally controlled, a MAS can improve performance, robustness and scalability by selecting actions inparallel. In principle, the agents in a MAS can have different, even conflicting, goals. We are interested in fully-cooperativeMAS, in which all the agents share a common goal.In a cooperative setting, each agent selects actions individually, but it is the resulting joint action that produces theoutcome. Coordination is therefore a key aspect in such systems. The goal of coordination is to ensure that the individualdecisions of the agents result in (near-)optimal decisions for the group as a whole. This is extremely challenging especially* Corresponding author at: Department of Computer Science, University of Massachusetts at Amherst, 140 Governors Drive, Amherst, MA 01003, USA.Tel.: +1 413 545 1985; fax: +1 413 545 1249.E-mail addresses: wufeng@mail.ustc.edu.cn (F. Wu), shlomo@cs.umass.edu (S. Zilberstein), xpchen@ustc.edu.cn (X. Chen).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.09.008\f488F. Wu et al. / Artificial Intelligence 175 (2011) 487–511when the agents operate under high-level uncertainty. For example, in the domain of robot soccer, each robot operatesautonomously, but is also part of a team and must cooperate with the other members of the team to play successfully.The sensors and actuators used in such systems introduce considerable uncertainty. What makes such problems particularlychallenging is that each agent gets a different stream of observations at runtime and has a different partial view of thesituation. And while the agents may be able to communicate with each other, sharing all their information all the time isnot possible. Besides, agents in such domains may need to perform a long sequence of actions in order to reach the goal.Different mathematical models exist to specify sequential decision-making problems. Among them, decision-theoreticmodels for planning under uncertainty have been studied extensively in artificial intelligence and operations research sincethe 1950’s. Decision-theoretic planning problems can be formalized as Markov decision processes (MDPs), in which a singleagent repeatedly interacts with a stochastically changing environment and tries to optimize a performance measure basedon rewards or costs. Partially-observable Markov decision processes (POMDPs) extend the MDP model to handle sensor un-certainty by incorporating observations and a probabilistic model of their occurrence. In a MAS, however, each individualagent may have different partial information about the other agents and about the state of the world. Over the last decade,different formal models for this problem have been proposed. We adopt decentralized partially-observable Markov deci-sion processes (DEC-POMDPs) to model a team of cooperative agents that interact within a stochastic, partially-observableenvironment.It has been proved that decentralized control of multiple agents is significantly harder than single agent control andprovably intractable. In particular, the complexity of solving a two-agent finite-horizon DEC-POMDP is NEXP-complete [12].In the last few years, several promising approximation techniques have been developed [3,11,17,19,46,47]. The vast majorityof these algorithms work offline and compute, prior to the execution, the best action to execute for all possible situations.While these offline algorithms can achieve very good performance, they often take a very long time due to the doubleexponential policy space that they explore. For example, PBIP-IPG – the state-of-the-art MBDP-based offline algorithm –takes 3.85 hours to solve a small problem such as Meeting in a 3×3 grid that involves 81 states, 5 actions and 9 observations[3]. Online algorithms, on the other hand, plan only one step at a time and they do so given all the currently availableinformation. The potential for achieving good scalability is more promising with online algorithms. But it is extremelychallenging to keep agents coordinated over a long period of time with no offline planning. Recent developments in onlinealgorithms suggest that combining online techniques with selective communication – when communication is possible –may be the most efficient way to tackle large DEC-POMDP problems. The main goal of this paper is to present, analyze,and evaluate online methods with bounded communication, and show that they present an attractive alternative to offlinetechniques for solving large DEC-POMDPs.The main contributions of this paper include: (1) a fast method for searching policies online, (2) an innovative way foragents to remain coordinated by maintaining a shared pool of histories, (3) an efficient way for bounding the number ofpossible histories agents need to consider, and (4) a new communication strategy that can cope with bounded or unreliablecommunication channels. In the presence of multiple agents, each agent must cope with limited knowledge about theenvironment and the other agents, and must reason about all the possible beliefs of the other agents and how that affectstheir decisions. Therefore, there are still many possible situations to consider even for selecting just one action given thecurrent knowledge. We present a new linear program formulation to search the space of policies very quickly. Anotherchallenge is that the number of possible histories (situations) grows very rapidly over time steps, and agents could runout of memory very quickly. We introduce a new approach to merging histories and thus bound the size of the pool ofhistories, while preserving solution quality. Finally, it is known that appropriate amounts of communication can improvethe tractability and performance of multi-agent systems. When communication is bounded, which is true in many real-world applications, it is difficult to decide how to utilize the limited communication resource efficiently. In our work,agents communicate when history inconsistency is detected. This presents a new effective way to initiate communicationdynamically at runtime.The rest of the paper is organized as follows. In Section 2, we provide the background by introducing the formal modeland discussing the offline and online algorithms as well as the communication methods in the framework of decentralizedPOMDPs. In Section 3, we present the multi-agent online planning with communication algorithms including the generalframework, policy search, history merging, communication strategy and implementation issues. In Section 4, we report theexperimental results on several common benchmark problems and a more challenging problem named grid soccer. We alsoreport the results for the cooperative box pushing domain with imperfect communication settings. In Section 5, we surveythe various existing online approaches with communications that have been applied to decentralized POMDPs, and discusstheir strengths and drawbacks. Finally, we summarize the contributions and discuss the limitations and open questions inthis work.2. BackgroundI",
            {
                "entities": [
                    [
                        202,
                        209,
                        "AUTHOR"
                    ],
                    [
                        217,
                        235,
                        "AUTHOR"
                    ],
                    [
                        239,
                        252,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 174 (2010) 726–748Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAnalysis of a probabilistic model of redundancy in unsupervisedinformation extractionDoug Downey a,∗, Oren Etzioni b, Stephen Soderland ba Northwestern University, 2133 Sheridan Road, Evanston, IL 60208, United Statesb University of Washington, Box 352350, Seattle, WA 98195, United Statesa r t i c l ei n f oa b s t r a c tArticle history:Received 7 July 2009Received in revised form 14 April 2010Accepted 26 April 2010Available online 29 April 2010Keywords:Information extractionUnsupervisedWorld Wide WebUnsupervised Information Extraction (UIE) is the task of extracting knowledge fromtext without the use of hand-labeled training examples. Because UIE systems do notrequire human intervention, they can recursively discover new relations, attributes, andinstances in a scalable manner. When applied to massive corpora such as the Web, UIEsystems present an approach to a primary challenge in artificial intelligence: the automaticaccumulation of massive bodies of knowledge.A fundamental problem for a UIE system is assessing the probability that its extractedinformation is correct. In massive corpora such as the Web, the same extraction is foundrepeatedly in different documents. How does this redundancy impact the probability ofcorrectness?We present a combinatorial “balls-and-urns” model, called Urns, that computes the impactof sample size, redundancy, and corroboration from multiple distinct extraction ruleson the probability that an extraction is correct. We describe methods for estimatingUrns’s parameters in practice and demonstrate experimentally that for UIE the model’slog likelihoods are 15 times better, on average, than those obtained by methods used inprevious work. We illustrate the generality of the redundancy model by detailing multipleapplications beyond UIE in which Urns has been effective. We also provide a theoreticalfoundation for Urns’s performance, including a theorem showing that PAC Learnability inUrns is guaranteed without hand-labeled data, under certain assumptions.© 2010 Elsevier B.V. All rights reserved.1. IntroductionAutomatically extracting knowledge from text is the task of Information Extraction (IE). When applied to the Web, IEpromises to radically improve Web search engines, allowing them to answer complicated questions by synthesizing infor-mation across multiple Web pages. Further, extraction from the Web presents a new approach to a fundamental challengein artificial intelligence: the automatic accumulation of massive bodies of knowledge.IE on the Web is particularly challenging due to the variety of different concepts expressed. The strategy employed forprevious, small-corpus IE is to hand-label examples for each target concept, and uses the examples to train an extractor [19,38,7,9,29,27]. On the Web, hand-labeling examples of each concept are intractable—the number of concepts of interestis simply far too large. IE without hand-labeled examples is referred to as Unsupervised Information Extraction (UIE). UIE* Corresponding author.E-mail addresses: ddowney@eecs.northwestern.edu (D. Downey), etzioni@cs.washington.edu (O. Etzioni), soderlan@cs.washington.edu (S. Soderland).URLs: http://www.cs.northwestern.edu/~ddowney/ (D. Downey), http://www.cs.washington.edu/homes/etzioni/ (O. Etzioni),http://www.cs.washington.edu/homes/soderlan/ (S. Soderland).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.024\fD. Downey et al. / Artificial Intelligence 174 (2010) 726–748727systems such as KnowItAll [16–18] and TextRunner [3,4] have demonstrated that at Web scale, automatically-generatedtextual patterns can perform UIE for millions of diverse facts. As a simple example, an occurrence of the phrase “C such asx” suggests that the string x is a member of the class C, as in the phrase “films such as Star Wars” [22].1However, all extraction techniques make errors, and a key problem for an IE system is determining the probability thatextracted information is correct. Specifically, given a corpus, and a set of extractions XC for a class C , we wish to estimateP (x ∈ C|corpus) for each x ∈ XC . In UIE, where hand-labeled examples are unavailable, the task is particularly challenging.How can we automatically assign probabilities of correctness to extractions for arbitrary target concepts, without hand-labeled examples?This paper presents a solution to the above question that applies across a broad spectrum of UIE systems and techniques.It relies on the KnowItAll hypothesis, which states that extractions that occur more frequently in distinct sentences in a corpusare more likely to be correct.KnowItAll hypothesis: Extractions drawn more frequently from distinctsentences in a corpus are more likely to be correct.The KnowItAll hypothesis holds on the Web. Intuitively, we would expect the KnowItAll hypothesis to hold because althoughextraction errors occur (e.g., KnowItAll erroneously extracts California as a City name from the phrase “states con-taining large cities, such as California”), errors occurring in distinct sentences tend to be different.2 Thus, typically a givenerroneous extraction is repeated only a limited number of times. Further, while the Web does contain some misinformation(for example, the statement “Elvis killed JFK” appears almost 200 times on the Web according to a major search engine),this tends to be the exception (the correct statement “Oswald killed JFK” occurs over 3000 times).At Web-scale, the KnowItAll hypothesis can identify many correct extractions due to redundancy: individual facts areoften repeated many times, and in many different ways. For example, consider the TextRunner Web information extractionsystem, which extracts relational statements between pairs of entities (e.g., from the phrase “Edison invented the light bulb,”TextRunner extracts the relational statement Invented(Edison, light bulb)). In an experiment with a set of about500 million Web pages, ignoring the extractions occurring only once (which tend to be errors), TextRunner extracted 829million total statements, of which only 218 million were unique (on average, 3.8 repetitions per statement). Well-knownfacts can be repeated many times. According to a major search engine, the Web contains over 10,000 statements thatThomas Edison invented the light bulb, and this fact is expressed in dozens of different ways (“Edison invented the lightbulb,” “The light bulb, invented by Thomas Edison,” ”Thomas Edison, after ten thousand trials, invented a workable lightbulb,” etc.).Although the KnowItAll hypothesis is simply stated, leveraging it to assess extractions is non-trivial. For example, the10,000th most frequently extracted Film is dramatically more likely to be correct than the 10,000th most frequently ex-tracted US President, due to the relative sizes of the target sets. In UIE, this distinction must be identified without anyhand-labeled data. This paper shows that a probabilistic model of the KnowItAll hypothesis, coupled with the redundancyof the Web, can power UIE for arbitrary target concepts. The primary contributions are discussed below.1.1. The urns model of redundancy in textThe KnowItAll hypothesis states that the probability that an extraction is correct increases with its repetition. But byhow much? How can we precisely quantify our confidence in an extraction given the available textual evidence?We present an answer to these questions in the form of the Urns model—an instance of the classic “balls-and-urns”model from combinatorics. In Urns, extractions are represented as draws from an urn, where each ball in the urn is labeledwith either a correct extraction, or an error—and different labels can be repeated on different numbers of balls. Giventhe frequency distribution in the urn for labels in the target set and error set, we can compute the probability that anobserved label is a target element based on how many times it is drawn. A key insight of Urns is that when the frequencydistributions have predictable structure (for example, in textual corpora the distributions tend to the Zipfian), they can beestimated without hand-labeled data.We prove that when the frequency of each label in the urn is drawn from a mixture of two Zipfian distributions (onefor the target class and another for errors), the parameters of Urns can be learned without hand-labeled data. When thedata exhibits a certain separability criterion, PAC learnability is guaranteed. We also demonstrate that Urns is effective inpractice. In experiments with UIE on the Web, the probabilities produced by the model are shown to be 15 times better, onaverage, when compared with techniques from previous work [14].1 Here, the term class may also refer to relations between multiple strings, e.g. the ordered pair (Chicago, Illinois) is a member of the Locate-dIn class.2 Two sentences are distinct when they are not comprised of exactly the same word sequence. We stipulate that sentences be distinct to avoid placingundue credence in content that is simply duplicated across many different pages, a common occurrence on the Web.\f728D. Downey et al. / Artificial Intelligence 174 (2010) 726–7481.2. Paper outlineThe paper proceeds as follows. We describe the Urns model in Section 2, experimentally demonstrate its effectivenessin UIE, and detail applications beyond UIE in which the model has been employed. The theoretical results characterizing theUrns model are presented in Section 3. We discuss future work in Section 4, and conclude.2. The URNS modelIn this section, we describe the Urns model for assigning probabilities of correctness to extractions. We begin by formallyintroducing the model, then describe our implementation and a set of experiments establishing the model’s effectivenessfor UIE.The Urns model takes the form of a classic “balls-and-urns” model from combinatorics",
            {
                "entities": [
                    [
                        221,
                        232,
                        "AUTHOR"
                    ],
                    [
                        238,
                        250,
                        "AUTHOR"
                    ],
                    [
                        254,
                        271,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 107–143www.elsevier.com/locate/artintLearning action models from plan examples usingweighted MAX-SATQiang Yang a,∗, Kangheng Wu a,b, Yunfei Jiang ba Department of Computer Science and Engineering, Hong Kong University of Science and Technology,Clearwater Bay, Kowloon, Hong Kong, Chinab Software Institute, Zhongshan University (Sun Yat-Sen University), Guangzhou, ChinaReceived 21 October 2005; received in revised form 14 November 2006; accepted 27 November 2006Available online 25 January 2007AbstractAI planning requires the definition of action models using a formal action and plan description language, such as the stan-dard Planning Domain Definition Language (PDDL), as input. However, building action models from scratch is a difficult andtime-consuming task, even for experts. In this paper, we develop an algorithm called ARMS (action-relation modelling system)for automatically discovering action models from a set of successful observed plans. Unlike the previous work in action-modellearning, we do not assume complete knowledge of states in the middle of observed plans. In fact, our approach works when noor partial intermediate states are given. These example plans are obtained by an observation agent who does not know the logicalencoding of the actions and the full state information between the actions. In a real world application, the cost is prohibitivelyhigh in labelling the training examples by manually annotating every state in a plan example from snapshots of an environment.To learn action models, ARMS gathers knowledge on the statistical distribution of frequent sets of actions in the example plans. Itthen builds a weighted propositional satisfiability (weighted MAX-SAT) problem and solves it using a MAX-SAT solver. We laythe theoretical foundations of the learning problem and evaluate the effectiveness of ARMS empirically.© 2006 Elsevier B.V. All rights reserved.Keywords: Learning action models; Automated planning; Statistical relational learning1. IntroductionAI planning systems require the definition of action models, an initial state and a goal. In the past, various actionmodelling languages have been developed. Some examples are STRIPS [13], ADL [12] and PDDL [14,17]. With theselanguages, a domain expert sits down and writes a complete set of domain action representation. These representationsare then used by planning systems as input to generate plans.However, building action models from scratch is a task that is exceedingly difficult and time-consuming even fordomain experts. Because of this difficulty, various approaches [4,5,18,34,37,42,43] have been explored to learn action* Corresponding author.E-mail address: qyang@cse.ust.hk (Q. Yang).URL: http://www.cse.ust.hk/~qyang.0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.11.005\f108Q. Yang et al. / Artificial Intelligence 171 (2007) 107–143models from examples. In knowledge acquisition for planning, many state of the art systems for acquiring actionmodels are based on a procedure in which a computer system interacts with a human expert, as illustrated by Blytheet al. [5] and McCluskey et al. [29]. A common feature of these works is that they require states just before or aftereach action to be known. Statistical and logical inferences can then be made to learn the actions’ preconditions andeffects.In this paper, we take one step towards automatically acquiring action models from observed plans in practice.The resultant algorithm is called ARMS, which stands for Action-Relation Modelling System. We assume that eachobserved plan consists of a sequence of action names together with the objects that each action uses. The intermediatestates between actions can be partially known; that is, between every adjacent pair of actions, the truth of a literal canbe totally unknown. This means that our input can be in the form of action names and associated parameter list withno state information, which is much more practical than previous systems that learn action models. Suppose that wehave several observed plans as input. From this incomplete knowledge, our ARMS system automatically guesses theapproximately correct and concise action models that can explain most of the observed plans. This action model isnot guaranteed to be completely correct, but it can serve to provide important initial guidance for human knowledgeeditors.Consider an example input and output of our algorithm in the Depot problem domain from an AI Planning com-petition [14,17]. As part of the input, we are given relations such as (clear ?x:surface) to denote that ?x is clear on topand that ?x is of type “surface”, relation (at ?x:locatable ?y:place) to denote that a locatable object ?x is located at aplace ?y. We are also given a set of plan examples consisting of action names along with their parameter list, such asdrive(?x:truck ?y:place ?z:place), and then lift(?x:hoist ?y:crate ?z:surface ?p:place). We call the pair consisting of anaction name and the associated parameter list an action signature; an example of an action signature is drive(?x:truck?y:place ?z:place). Our objective is to learn an action model for each action signature, such that the relations in thepreconditions and postconditions are fully specified.A complete description of the example is shown in Table 1, which lists the actions to be learned, and Table 2, whichdisplays the training examples. From the examples in Table 2, we wish to learn the preconditions, add and delete listsof all actions. Once an action is given with the three lists, we say that it has a complete action model. Our goal is tolearn an action model for every action in a problem domain in order to “explain” all training examples successfully.An example output from our learning algorithms for the load(?x ?y ?z ?p) action signature is:actionload(?x:hoist ?y:crate ?z:truck ?p:place)pre:del:add:(at ?x ?p), (at ?z ?p), (lifting ?x ?y)(lifting ?x ?y)(at ?y ?p), (in ?y ?z), (available ?x), (clear ?y)Table 1Input domain description for Depot planning domainDomaintypesrelationsactionsDepotplace locatable - objectdepot distributor - placetruck hoist surface - locatablepallet crate - surface(at ?x:locatable ?y:place)(on ?x:crate ?y:surface)(in ?x:crate ?y:truck)(lifting ?x:hoist ?y:crate)(available ?x:hoist)(clear ?x:surface)drive(?x:truck ?y:place ?z:place)lift(?x:hoist ?y:crate ?z:surface ?p:place)drop(?x:hoist ?y:crate ?z:surface ?p:place)load(?x:hoist ?y:crate ?z:truck ?p:place)unload(?x:hoist ?y:crate ?z:truck ?p:place)\fQ. Yang et al. / Artificial Intelligence 171 (2007) 107–143109Table 2Three plan traces as part of the training examplesInitialStep1StateStep2Step3StateStep4StateStep5Step6Step7Step8Step9GoalPlan1I1lift(h1 c0 p1 ds0),drive(t0 dp0 ds0)load(h1 c0 t0 ds0)drive(t0 ds0 dp0)(available h1)unload(h0 c0 t0 dp0)(lifting h0 c0)drop (h0 c0 p0 dp0)(on c0 p0)Plan2I2lift(h1 c1 c0 ds0)(lifting h1 c1)load(h1 c1 t0 ds0)lift(h1 c0 p1 ds0)load(h1 c0 t0 ds0)drive(t0 ds0 dp0)unload(h0 c1 t0 dp0)drop(h0 c1 p0 dp0)unload(h0 c0 t0 dp0)drop(h0 c0 c1 dp0)(on c1 p0)(on c0 c1)Plan3I3lift(h2 c1 c0 ds0)load(h2 c1 t1 ds0)lift(h2 c0 p2 ds0),drive(t1 ds0 dp1)unload(h1 c1 t1 dp1),load(h2 c0 t0 ds0)drop(h1 c1 p1 dp1),drive(t0 ds0 dp0)unload(h0 c0 t0 dp0)drop(h0 c0 p0 dp0)(on c0 p0)(on c1 p1)I1: (at p0 dp0), (clear p0), (available h0), (at h0 dp0), (at t0 dp0), (at p1 ds0), (clear c0), (on c0 p1), (available h1), (at h1 ds0).I2: (at p0 dp0), (clear p0), (available h0), (at h0 dp0), (at t0 ds0), (at p1 ds0), (clear c1), (on c1 c0), (on c0 p1), (available h1), (at h1 ds0).I3: (at p0 dp0), (clear p0), (available h0), (at h0 dp0), (at p1 dp1), (clear p1), (available h1), (at h1 dp1), (at p2 ds0), (clear c1), (on c1 c0), (on c0p2), (available h2), (at h2 ds0), (at t0 ds0), (at t1 ds0).We wish to emphasize an important feature of our problem definition, which is the relaxation of observable stateinformation requirements. The learning problem would be easier if we knew the states just before and after eachaction. However, in reality, what is observed before and after each action may just be partially known. In this paper,we address the situation where we know little about the states surrounding the actions. Thus we do not know for sureexactly what is true just before each of load(h1 c0 t0 ds0), drive(t0 ds0 dp0), unload(h0 c0 t0 dp0), drop(h0 c0 p0dp0) as well as the complete state just before the goal state in the first plan in Table 2. Part of the difficulty in learningaction models is due to the uncertainty in assigning state relations to actions. In each plan, any relation such as (on c0p0) in the goal conditions might be established by the first action, the second, or any of the rest. It is this uncertaintythat causes difficulties for many previous approaches that depend on knowing states precisely.In our methodology, the training plans can be obtained through monitoring devices such as sensors and cameras, orthrough a sequence of recorded commands through a computer system such as UNIX. These action models can thenbe revised using interactive systems such as GIPO.It is thus intriguing to ask whether we can approximate an action model in an application domain if we are given aset of recorded action signatures as well as partial or even no intermediate state information. In this paper, we take afirst step towards answering this question by presenting an algorithm known as ARMS. ARMS proceeds in two phases.In phase one of the algorithm, ARMS finds frequent action sets from plans that share a common set of parameters.In addition, ARMS finds some frequent relation-action pairs with the help of the initial state and the goal state. Theserelation-action pairs give us an initial guess on the preconditions, add lists and delete lists of actions in this subset.These action subsets and pairs are used to obtain a set of constraints that must hold in order to make the plans correct.We then transform the con",
            {
                "entities": [
                    [
                        135,
                        145,
                        "AUTHOR"
                    ],
                    [
                        151,
                        162,
                        "AUTHOR"
                    ],
                    [
                        168,
                        180,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 89 ( 1997) 317-364 Artificial Intelligence Ramification and causality Michael Thielscher * International Computer Science Institute. 1947 Center Street, Berkeley, CA 94704-1198, USA Received December 1995; revised June 1996 Abstract In formal systems laws describing dependencies the problem in action specifications the ramification problem denotes represented among components for reasoning about actions, indirect effects. These effects are not explicitly from general of handling but follow of the world state. An adequate treatment of indirect effects requires a suitably weakened version of the general law of persistence. It also requires a method to avoid unintuitive changes suggested by the aforementioned dependency laws. We propose a solution to the ramification problem that uses directed relations between two single effects, stating the circumstances under which the occurrence the second. We argue for the necessity of an approach based on causality by of the first causes principle elaborating the limitations of common paradigms employed to handle ramifications-the of categorization and the policy of minimal change. Our abstract solution is realized on the basis of a particular action calculus, namely, the fluent calculus. Keywords: Temporal reasoning; Reasoning about actions; Ramification problem; Causality; Fluent calculus 1. Introduction The ability to reason about changing environments, which effects of one’s own actions and explaining observed phenomena, basis for understanding in their habitat. Formal approaches to model This research area was initiated by McCarthy actions plays a fundamental the world to an extent sufficient this ability have a long [ 3 11, who claimed role in common involves predicting the serves humans as a to survive and to act intelligently tradition that reasoning in AI. about Drawing conclusions about dynamic is grounded on formal specifica- tions of what effects are caused by the execution of a particular obviously to provide an exhaustive description defining infeasible action. Since the result of executing it is * On leave from FG Intellektik, TH Darmstadt. E-mail: michaelt@icsi.berkeley.edu. 0004.3702/97/$17.00 Copyright @ 1997 Elsevier Science B.V. All rights reserved. PUSOOO4-3702(96)00033-l sense. environments \f318 M. Thielscher/Artijiciai lntelli~ence 89 (I 997) 317-364 in each possible that they affect-while the rest of the world is subject state of the world, action specifications i.e., is assumed in complex domains should be restricted an action to the to the part of the world to remain stable. Yet even this approach becomes law of ,~e~~istence, if one tries to put all effects into a single, complete unmanageable an action may cause only a small number of direct changes, specification. Although that can be hard to foresee. they in turn may in the first place causes For instance, nothing but a change of the switch’s position. However, the switch may be part of an electric circuit so that, say, some light bulb is turned off as a side effect, which in turn room by running against a may cause someone chair that, as a consequence, the fire alarm and so on and so forth.’ initiate a long chain of indirect effects the action of toggling a switch, which set whose implosion activates in a suddenly darkened falls into a television to hurt himself consider The task, then, is to design a framework are not assumed specifications the ramification problem. 2 A satisfactory treatment of the following two major issues. to formalize action scenarios where action to completely describe all possible effects. This is called requires a successful to this problem solution First, we need an appropriately weakened version of the aforementioned law of persis- that are unaffected by the tence that applies only to those parts of the world description the to this problem, we suggest keeping action’s direct and indirect effects. As a solution through the world description obtained law of persistence as it stands while considering result. Indirect effects are then accommodated its application merely as an intermediate by further state obtains. This method accounts perfectly both for rigorous persistence of unaffected parts and for arbitrarily complex chains of indirect effects. until an overall satisfactory successor reasoning from Second, dependencies indirect effects typically are consequences between world description components of additional, general knowledge of domain-specific (usually called not all effects suggested puenrs)-but the from are desirable standpoint of causality. As an example, consider the simple electric circuit depicted in Fig. 1, which consists of a battery, two switches and a light bulb. The obvious these components may formally be described by the logic expression connection between SM/~ A SW? E light, i.e., the light is on if and only if both switches are on. Now suppose state displayed, where both WI and light we toggle in the particular are false while sw2 is true. Then, besides true, we that the light bulb turns on. This indirect effect is inspired by the formula also expect the direct effect of SWI becoming the first switch this perspective the above may not only be described as “the fire alarm the switch”, but also as, say, “the chair the distinction between in this context concerns step and those which deserve separate state transitions indirect effects occurring during a single (also called delayed effects). state after having hits the the next state transition)“. As a reasonable, albeit informal, guidance we suggest a single agent himself, has the summarize what happens until someone, e.g., is activated in the successor (and presumably in the successor is falling state ’ A crucial question world’s state transition E.g., toggled television set during state transition possibility should to intervene by acting again (stopping the chair from falling, the reasoning for instance) * The naming was suggested cation problem to exploit aim of restricting in exactly logical consequences search space. in [ I5 1, inspired by [ IO 1. The latter, however, was not devoted the above sense, contrary (called ramifications) to what is often claimed; of goal specifications to the ramifi- rather, this thesis describes how in planning problems, with the \fM. Thielscher/Artijcial Intelligence 89 (1997) 317-364 319 Fig. I. An electric circuit consisting of a battery, both switches are closed. The respective by a unique propositional state of each of the three dynamic components constant, where negation is denoted by a bar on top of the respective symbol. involved is described two switches, and a light bulb, which is on if and only if expected the intuitively the implication SWI A SW:! > light. However, despite result, the mere knowledge of the connection between this just mentioned, which includes being the switches and the bulb is not sufficient: Notice that the above formula, SWI ASW~ = light, that instead of the light also entails is that the second one being jumps turned on, the indirect effect of toggling its position-a sw1 A light > ~2, which suggests the first switch the implication result which contradicts our intuition. of merely taking into account formalizations The reason for the inadequacy logical as pure formulas-usually pendencies formulas do not include causal information. More precisely, -. sw2 IS clearly one observes false. However, exploiting true in any state and, therefore, contains evidential information, sw1 to be true and light to be false then it is safe to conclude called domain constraints-is the implication for reasoning about causality As a solution this implication to the second problem, we propose of de- that these swl A light > that is, if that sw2 is is misleading. 3 in the form of so-called cuusuZ relationships, which formalize statements to incorporate like causality A change of FFi to SWI causes a change of light to light, provided sw2 is true. the direct effects of executing an action in a particular state of the world, indirect effects one by one, to accommodate After computing we will apply suitable causal relationships, until a satisfactory each of which only caused by a direct one and also for indirect effects the relationship effects. To illustrate result obtains. Employing the latter, consider two particular relates effects, accounts a collection of single causal relationships, for several in turn causing indirect effects indirect further A change of light provided detector-activated is true to light causes a change of light-detector to light-detector, in addition specification an automated procedure information of a formal automatic generation to the one above. Since we do not expect the designer of a formal domain to create a complete set of suitable causal relationships, we will also present to extract them from given domain constraints plus some general influence other fluents. On the basis and their specifying which fluents may possibly theory of actions in Section 2)) causal relationships (to be defined are formally introduced in Section 3. 3 See [ 33 I for a general discussion on the different natures of causal and evidential implications. \f320 M. Ti~ielscher/Arti~cial Intelligence 89 (I 997) 317-364 Yet the purpose of this article is not only to suggest incorporating but also to supply evidence causal information the principle of categorization to cope with the ramification that something like problem. To this of existing paradigms aimed at and the policy of minimal that are fluents (e.g., a change of light is considered more that some resist any fluents is the assumption of minimal approaches distinguish trying namely, to change relationships is inevitable when the limited expressiveness by means of our causal this approach end, in Section 5 we illustrate ramifications, handling speaking, categorization-based change. Roughly likely more than other fluents than a change of sw2). However, we will show likely (Section",
            {
                "entities": [
                    [
                        94,
                        112,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Artificial Intelligence 93 ( 1997) I69- 199 Artificial Intelligence Definability and commonsense reasoning Gianni Amati a**, Luigia Carlucci Aiello b,l, Fiora Pirri b-2 a Fo~dazione Ugo Bordoni, via 3alduss~~rre C~st~g~io~e 59, 00142 Roma, My h Dipurtjme~to di ~~farmat~ca e Sistemi~tjca, Universitii di Roma “La Sapienza”, via Suluria 1 J-3, 00198 Romu, Daly Received March 1996; revised September 1996 Abstract in commonsense is a central problem The definition of concepts in reasoning concern implicit and explicit definability. Implicit definability in non- nonmonotonic monotonic logic is always relative to the context-the current theory of the world. We show that fixed point equations provide a generalization of explicit definability, which correctly captures the relativized context. Theories expressed within this logical framework provide implicit definitions of concepts. Moreover, it is possible to derive these fixed points entirely within the logic. @ 1997 Elsevier Science B.V. reasoning. Many themes Kqw~rrds: Commonsense reasoning; Delinabiiity; Fixed points; Logic of provability; Default logic; Contextual reasoning; Self-reference 1. Introduction and motivations Concepts play a central role in commonsense of postulating in which the properties used as definientes are independent a definition of a concept, of the definiendum. reasoning. The classical view consists in terms of necessary and sufficient conditions, Positivists pointed out that if a concept cannot be defined via necessary and sufficient that there are concepts, do not seem to have a common core which it is not a scientific concept. noted by Wittgenstein-that It turns out, however conditions like game-as could be characterized by a set of necessary conditions (see [IS] for a discussion). * Corresponding author. E-mail: gba@fub.it. Work carried out in the framework of the agreement between the halian Ff Administration and the Fondazione Ugo Bordoni. ’ E-mail: aiello@dis.uniromal .it. *E-mail: pirri@dis.uniromal.it. 0004-3702/97/$17.00 @ 1997 Efsevier Science B.V. All rights reserved. PI1 s0004-3702(94)00049-5 \f170 G. Amati et al. /Art@cial Intelligence 93 (I 997) 169-199 natural kinds is one of the main proponents On the other hand, concepts involving necessary but not sufficient conditions. This point Eleanor Rosch view on “natural kinds”, Reiter [41] suggests resorting via some appeals patterns mirror of classical necessary conditions W and a set of “sufficient” default conditions D. like bird, lemon, etc., possess theory, where in prototype (see e.g. [ 321). In connection with this to sufficient conditions postulated “ typically”, or “assume by default”, which is relativized. And, in fact, these linguistic the role of defaults; namely, a default to which the definition linguistic pattern (lY D) accounts like “normally”, to the context is stressed for a set theory axiomatization in almost all cases reported Likewise, a circumscription definitions implicit sufficient condition tions) whereby amounts (by (e.g. [22,24,29] in the literature finding an equivalent ), solving theory) yields axiom acts like an (just as, for Reiter, the default rules act like sufficient condi- the concept. Solving for the predicates being minimized. the circumscription first-order for the predicate being minimized. The circumscription the theory is implicitly defining to finding an explicit definition logic theorem in first-order through Beth’s definability The notions of implicit and explicit definition of a predicate, with respect to a theory, [ 6]), from an implicit definition formula. in first-order are formalized which shows that the two notions are equivalent. Moreover, of a predicate an explicit definition can be found by constructing An immediate logic state we state that S t- V’x (RAVEN(X) = BLACK(X)), then talk about axioms, then by the monotonicity definition of the concept. Tarski discusses with definability if things we can for example by adding new the of Beth’s theorem of the predicate being defined. Therefore, logic, one cannot compatibly update consequence the uniqueness and those concerned with deduction in S are ravens. Whenever is that implicit definitions the connection between the notions concerned the context changes, the only black an interpolant for example, of first-order (see e.g. in (541. On the other hand, only “well-behaved” can be drawn. McCarthy’s circumscription we may lack either sufficient or necessary conditions no explicit definition strong behaviour of definability is weaker the explicit definition constraint because other words, there is no total commitment in first-order (see is achieved by minimization, than implicit definability it is relativized to define a concept; predicates are implicitly definable; in general in such cases this axiom [ 111 for a discussion). As noted above, since is no longer a strong In the circumscription [29] circumvents logic because is applied. uniqueness to the theory on which circumscription to the given definition. Consider, for example, the following theory T. ONTABLE(a)VONTABLE( b), vxRED(x) -ONTABLE( (1) The circumscription is separable (see [ 21]), that is of ONTABLE in T yields a first-order formula because ONTABLE (vxONTABLE(x) = (RED(x) i’X=U) AZ) v (vxONTABLE( x)E (FCED(x)VX= b) AZ’) (2) where Z and Z’ are formulae not containing ONTABLE (see [ 2 1, Theorem 1 ] ) \fG. Amati et al./Artijiicial fnielligence 93 (1997) 169-199 171 We note that in such a case we do not get an implicit definition as for it. Therefore we do not get of ONTABLE, to uniqueness. A first-order disjunction the lack of a classical explicit definition, a disju$z~tio~ of two de~nitio~s for the predicate ONTABLE. The shows is weaker that circumscription in Beth sense. Despite two minimal nonisomorphic models there are at least an explicit definition we have obtained example clearly and we are no longer committed is a nice generalization definition circumscription yield a very large disjunction Lin proposes a transformation state axioms. The interesting introducing the different effects of the performed action are realized. of successor that breaks contribution of definability, from weak of a theory, axiomatizing though implicit can be obtained the disjunction than Beth’s implicit definability of definitions it does not tell how a single explicit that the [23] conditions. Lin actions, may the effects of indeterminate this problem, state axioms. To overcome and yields different successor by in which is that his transfo~ation is perfo~ed shows a suitable predicate which is used, in a sense, to name the contexts As another example of how implicit and explicit definability that an approach reasoning, observe tonic negation as failure operator as a set of necessary ficient conditions shows how, in some cases, program. conditions yielding semantics to logic programming enters into nonmonotonic for the nonmono- is the Clark completion, which treats a logic program suf- and “completes” predicates. Reiter 1401 the this program by adding suitable is the result of circumscribing for the progr~ the Clark completion explicit definitions The upshot of the foregoing discussion implicit and explicit definability. of the definientes it seems clear how to weaken concern reasons for wanting explicit definitions, because by substitution However, while reasoning of how to get, in general, an explicit definition stated through a nonmonotonic one way to solve this problem. (via defauits or circumscription) is that many themes in nonmonotonic reasoning In fact there are very good computational theorem proving to be proved. then we can do efficient theorem in any for the definiendum implicit de~nability so far there is no solution from the implicit in nonmonotoni~ to the problem sufficient conditions theory. The main contribution of this paper is to show Returning to the above theory ( 1)) in order to obtain one of the disjuncts disregarding the others, we should be able to express sufficient conditions object a. Analogously contexts different context. should be the following: to a context C, the necessary and to be on the table are that either it is red or it is the is b. The role played by refer to a there is a context C’ where the object each definition in the disjunction for an object that, relative should To subsume a context of reasoning (or a context of discourse) state of affairs, we need a language that resorts a certain ability. That is, we need to formalize, about taken naturally hand. in that same language into account relies on a current is a cu~~~~nse~se in a theory axiomatizing to a kind of self-referential that can be reasoned in the language, expressions itself. A statement in which the context can be ex~~~~it~~ state~lent. And, in fact, commonsense reasoning state of affairs: the minimum resource of information at This form of relativization to the context to the belief set of an agent. So the self-referential statements is often carried out by binding commonsense ability is lifted from the \f172 G. Anzati et al. /Art+&1 Intelligence 93 (I 997) 169-l 99 language to one in which two distinct the one where the agent draws conclusions commonsense are fo~ulated: and general, in which the agent is reasoning. The following definition the conclusions structure, i.e., a computational is a metalogical the one where drawn are compared with (or even more) levels of reasoning from the initial assumptions the context, which, in object external to the language is, in this sense, paradigmatic: r E cn/, (I u (Off f -cd $8 F}) (3) f in the sense is a context, as “it is consistent the above schema the deductive closure-in theory then the nonmonotonic the above expression does not belong which says that if I is a nonmonotonic the logic n-of I are obtained by taking the formulae consistent with r, where Oa is, in fact, interpreted assume a in r”. Although belief set of an agent, When of discourse. One for the default compatible with Oa, and, f",
            {
                "entities": [
                    [
                        116,
                        128,
                        "AUTHOR"
                    ],
                    [
                        134,
                        156,
                        "AUTHOR"
                    ],
                    [
                        162,
                        173,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 242 (2017) 132–171Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintMaking friends on the fly: Cooperating with new teammates ✩Samuel Barrett a,∗,1, Avi Rosenfeld b, Sarit Kraus c,d, Peter Stone ea Cogitai, Inc., Anaheim, CA 92808, USAb Dept. of Industrial Engineering, Jerusalem College of Technology, Jerusalem, 9116001, Israelc Department of Computer Science, Bar-Ilan University, Ramat Gan, 5290002, Israeld Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USAe Dept. of Computer Science, The University of Texas at Austin, Austin, TX 78712, USAa r t i c l e i n f oa b s t r a c tArticle history:Received 28 February 2015Received in revised form 10 October 2016Accepted 17 October 2016Available online 21 October 2016Keywords:Ad hoc teamworkMultiagent systemsMultiagent cooperationReinforcement learningPursuit domainRoboCup soccerRobots are being deployed in an increasing variety of environments for longer periods of time. As the number of robots grows, they will increasingly need to interact with other robots. Additionally, the number of companies and research laboratories producing these robots is increasing, leading to the situation where these robots may not share a common communication or coordination protocol. While standards for coordination and communication may be created, we expect that robots will need to additionally reason intelligently about their teammates with limited information. This problem motivates the area of ad hoc teamwork in which an agent may potentially cooperate with a variety of teammates in order to achieve a shared goal. This article focuses on a limited version of the ad hoc teamwork problem in which an agent knows the environmental dynamics and has had past experiences with other teammates, though these experiences may not be representative of the current teammates. To tackle this problem, this article introduces a new general-purpose algorithm, PLASTIC, that reuses knowledge learned from previous teammates or provided by experts to quickly adapt to new teammates. This algorithm is instantiated in two forms: 1) PLASTIC-Model – which builds models of previous teammates’ behaviors and plans behaviors online using these models and 2) PLASTIC-Policy – which learns policies for cooperating with previous teammates and selects among these policies online. We evaluate PLASTIC on two benchmark tasks: the pursuit domain and robot soccer in the RoboCup 2D simulation domain. Recognizing that a key requirement of ad hoc teamwork is adaptability to previously unseen agents, the tests use more than 40 previously unknown teams on the first task and 7 previously unknown teams on the second. While PLASTIC assumes that there is some degree of similarity between the current and past teammates’ behaviors, no steps are taken in the experimental setup to make sure this assumption holds. The teammates were created by a variety of independent developers and were not designed to share any similarities. Nonetheless, the results show that PLASTIC was able to identify and exploit similarities between its current and past teammates’ behaviors, allowing it to quickly adapt to new teammates.© 2016 Elsevier B.V. All rights reserved.✩This article contains material from 4 prior conference papers [11–14].* Corresponding author.E-mail addresses: sam@cogitai.com (S. Barrett), rosenfa@jct.ac.il (A. Rosenfeld), sarit@cs.biu.ac.il (S. Kraus), pstone@cs.utexas.edu (P. Stone).1 This work was performed while Samuel Barrett was a graduate student at the University of Texas at Austin.http://dx.doi.org/10.1016/j.artint.2016.10.0050004-3702/© 2016 Elsevier B.V. All rights reserved.\fS. Barrett et al. / Artificial Intelligence 242 (2017) 132–1711331. IntroductionRobots are becoming cheaper and more durable and are therefore being deployed in more environments for longer periods of time. As robots continue to proliferate in this way, many of them will encounter and interact with a variety of other kinds of robots. In many cases, these interacting robots will share a set of common goals, in which case it will be desirable for them to cooperate with each other. In order to effectively perform in new environments and with changing teammates, they should observe their teammates and adapt to achieve their shared goals. For example, after a disaster, it is helpful to use robots to search the site and rescue survivors. However, the robots may come from a variety of sources and may not be designed to cooperate with each other, such as in the response to the 2011 Tohoku earthquake and tsunami [43,55,56,58]. If these robots are not pre-programmed to cooperate, they may not share information about which areas have been searched; or worse, they may unintentionally impede their teammates’ efforts to rescue survivors. Therefore, in the future, it is desirable for robots to be designed to observe their teammates and adapt to them, forming a cohesive team that quickly searches the area and rescues the survivors.This idea epitomizes the spirit of ad hoc teamwork. In ad hoc teamwork settings, agents encounter a variety of teammates and try to accomplish a shared goal. In ad hoc teamwork research, researchers focus on designing a single agent or subset of agents that can cooperate with a variety of teammates. The desire is for agents designed for ad hoc teamwork to quickly learn about these teammates and determine how they should act on this new team to achieve their shared goals. Agents that reason about ad hoc teamwork will be robust to changes in teammates in addition to changes in the environment. This article focuses on a limited version of the ad hoc teamwork problem. Specifically, this article investigates how an agent should adapt to new teammates given that it has previously interacted with other teammates and learned from these interactions. However, these past interactions may not be representative of the current teammates.In this article, the word “agent” refers to an entity that repeatedly senses its environment and takes actions that affect this environment, shown visually in Fig. 1a. As a shorthand, the terms ad hoc team agent and ad hoc agent are used in this article to refer to an agent that reasons about ad hoc teamwork. The environment includes the dynamics of the world the agent interacts with, as well as defining the observations received by the agent. We treat the other agents in the domain as teammates because they share a set of common goals; they are fully cooperative in the terminology of game theory.Previous work on teamwork has largely assumed that all agents in the domain will act as a unified team and are designed to work with their specific teammates [25,36,66,68]. Methods for coordinating multiagent teams largely rely on specifying standardized protocols for communication as well as shared algorithms for coordination. These approaches do not directly apply to ad hoc teams due to their strong assumptions about this sharing of prior knowledge, which is violated in the ad hoc teamwork scenario. This view of multiagent teams is shown in Fig. 1b.On the other hand, this article will focus on creating a single agent that cooperates with teammates coming from a variety of sources without directly altering the behavior of these teammates. However, all of the agents still share a set of common goals, so it is desirable for them to act as a team. In addition, rather than focusing on a single task, these agents may face a variety of tasks, where a task refers to both the environment other than the team’s agents as well as the team’s shared goals.The differences of this article from prior work are presented visually in Fig. 1. Another existing area of research into how agents should behave is reinforcement learning (RL). Generally, RL problems revolve around a single agent learning by interacting with its environment. In RL problems, agents receive sparse feedback about the quality of sequences of actions. Generally, RL algorithms either model other agents as part of the environment and try to learn the best policy for the single agent given this environment or they consider the case where the whole team is under a single designer’s control. In addition, RL algorithms usually learn from scratch in each new environment, ignoring information coming from previous environments. However, there is a growing body of work on applying transfer learning to RL to allow agents to reuse prior experiences on new domains [69]. Fig. 1a shows the standard RL view of an agent interacting with its environment. Fig. 1b represents a common multiagent view of a unified team interacting with the environment where the agents model their teammates as being separate from the environment. In this case, the team is designed before being deployed to cooperate with these specific agents to interact with a fixed environment. However, these agents rely on knowing their teammates and usually require an explicit communication and/or coordination protocol to be shared among the whole team [36,53,73]. On the other hand, this article will focus on ad hoc teams drawn from a set of possible teammates, where the team tackles a variety of possible environments as shown in Fig. 1c. In this case, the teammates are not programmed to cooperate with this specific ad hoc agent, and they must be treated as given and inalterable. Instead, this research focuses on enabling the ad hoc agent to cooperate with a variety of teammates in a range of possible environments.In an ad hoc team, agents need to be able to cooperate with a variety of previously unseen teammates. Rather than developing protocols for coordinating an entire team, ad hoc team research focuses on developing agents that cooperate with teammates in the absence of such explicit protocols. Therefore, we consider a single agent cooperating with teammates that may or may not adapt to its behavior. In this scenario, w",
            {
                "entities": [
                    [
                        195,
                        209,
                        "AUTHOR"
                    ],
                    [
                        3563,
                        3577,
                        "AUTHOR"
                    ],
                    [
                        217,
                        230,
                        "AUTHOR"
                    ],
                    [
                        234,
                        245,
                        "AUTHOR"
                    ],
                    [
                        251,
                        262,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 166 (2005) 81–139www.elsevier.com/locate/artintKnowledge and communication:A first-order theory ✩Ernest DavisCourant Institute, New York University, New York, NY 10012, USAReceived 1 July 2004; accepted 2 May 2005Available online 22 June 2005AbstractThis paper presents a theory of informative communications among agents that allows a speaker tocommunicate to a hearer truths about the state of the world; the occurrence of events, including othercommunicative acts; and the knowledge states of any agent—speaker, hearer, or third parties—any ofthese in the past, present, or future—and any logical combination of these, including formulas withquantifiers. We prove that this theory is consistent, and compatible with a wide range of physical the-ories. We examine how the theory avoids two potential paradoxes, and discuss how these paradoxesmay pose a danger when this theory are extended. 2005 Elsevier B.V. All rights reserved.Keywords: Communication; Knowledge; Logic; Paradox1. IntroductionIn constructing a formal theory of communications between agents, the issue of expres-sivity enters at two different levels: the scope of what can be said about the communica-tions, and the scope of what can be said in the communications. Other things being equal,it is obviously desirable to make both of these as extensive as possible. Ideally, a theoryshould allow a speaker to communicate to a hearer truths about the state of the world;the occurrence of events, including other communicative acts; the knowledge states of any✩ The research reported in this paper was supported in part by NSF grant IIS-0097537.E-mail address: davise@cs.nyu.edu (E. Davis).0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.05.002\f82E. Davis / Artificial Intelligence 166 (2005) 81–139agent—speaker, hearer, or third parties; any of these in the past, present, or future; and anylogical combination of these. This paper presents a theory that achieves pretty much that.A few examples of what can be expressed in our representation:(1) Alice tells Bob that all her children are asleep.(2) Alice tells Bob that she does not know whether he locked the door.(3) Alice tells Bob that if he finds out who was in the kitchen at midnight, then he willknow who killed Colonel Mustard.(4) Alice tells Bob that no one had ever told her she had a sister.(5) Alice tells Bob that he has never told her anything she did not already know.The above examples illustrate many of the expressive features of our representation:• Example 1 shows that the content of a communication may be a quantified formula.• Example 2 shows that the content of a communication may refer to knowledge andignorance of past actions.• Example 3 shows that the content of a communication may be a complex formulainvolving both past and future events and states of knowledge.• Examples 4 and 5 show that the content of a communication may refer to other com-munications. They also show that the language supports quantification over agents andover the content of a communication, and thus allows the content to be partially char-acterized, rather than fully specified.Moreover, our theory supports basic inference from these kinds of representations. Forexample, given that Alice tells Bob that no one has ever told her that she has a sister, andthat Alice knows that, if she did have a sister, someone would have told her, it is possibleto infer that Alice knows that she does not have a sister. The proof from our theory of thisand similar sample inferences and the representation of the five statements above is givenin Section 4.Following the research programme of [8,21,24,25], the primary purpose of this paper isto develop a representation for expressing commonsense knowledge about knowledge andcommunication, with the ultimate intention that this representation or something similar,could be used to carry out symbolic reasoning in this domain. A secondary purpose isto develop an object-level theory, expressible in the language, that will justify as broad arange as possible of commonsensically obvious inference in the domain, while entailing asfew as possible commonsensically absurd consequences. The success of the language andtheory is demonstrated in terms of their ability to capture a large number and a broad rangeof examples of commonsensically obvious inferences. We are not here concerned withspecialized applications, such as distributed systems; with subtle philosophical nuance; orwith efficiency of inference in an implemented reasoning engine. Potentially, this theorycould find practical application as a logical foundation either for planning communicationsin a multi-agent system, or for a theory of speech acts to be used in interpreting dialogueor engaging in dialogue with human speakers.Since our theory allows communications that refer to other communications, and evencommunications that refer to themselves, there is clearly a danger of running into para-\fE. Davis / Artificial Intelligence 166 (2005) 81–13983doxes of vicious self-reference. It is therefore particularly important to establish that thetheory is consistent. We prove a meta-theorem that the theory is indeed consistent; in fact,that it is consistent with a wide range of domain-specific physical theories and axiomsof knowledge acquisition. We discuss two particular apparent paradoxes—an analogue ofRussell’s paradox, and the “unexpected hanging” paradox—and we show how our theorymanages to side-step these.We should note at the outset the limitations of our theory. The theory deals only withinformative acts (and not, for example, with requests) and assumes that the following con-ditions are true and universally known: If AS communicates Q to AH, then(1) AS knows that Q is true at the time that he initiates the communication.(2) From the time that he initiates the communication, AS knows that he is carrying out acommunication; he knows that the content is Q; and he knows that the recipient is AH.(3) Similarly, when the communication is complete, AH knows that he has received a com-munication; he knows that the content was Q; and he knows that the sender was AS.(4) When the communication is complete, AS knows that the communication is completeand AH knows the time at which the communication was initiated.The paradigmatic example of a form of communication satisfying conditions (2), (3),and (4) is direct speech.1 Another example could be mail, assuming that• All messages are time-stamped with the time of sending, and signed by the sender.• There is a universally known maximal delay D between the time of sending and thetime of receiving a message. (“Receiving” here means the time when the hearer readsthe message, not the time that it arrives in his mailbox.)In this case, if we define a communication to be “complete” at the time of sending plus D,then the above conditions are met.Many aspects of the theory can be applied to communications that do not meet condi-tion (4), but I have not been able to find a plausible axiomatization of this more generalcase that I can prove to be consistent. Also, I cannot prove that the theory is consistentunless time is taken to be discrete. These are discussed further in Section 8.The paper proceeds as follows: Section 2 reviews the theories of time and of knowledge,which are not new here. Section 3 presents our language and axioms of communica-tion. Section 4 is the core of this paper; it illustrates the power of the theory by showinghow it supports the representation of the five sample statements above and three examplecommonsense inferences. Sections 5 and 6 describe two apparent paradoxes—a paradoxanalogous to Russell’s paradox and the “unexpected hanging” paradox—and explain whythese do not cause inconsistencies in the theory. Section 7 gives the statement of Theo-rems 1 and 2, which assert that the theory is internally consistent and compatible with awide range of physical theories. Sections 8 and 9 discuss related work. Section 10 dis-1 Under assumptions that are reasonable, though not universally valid: e.g. that the speaker knows what hewill say when he begins speaking, and that the speaker and hearer have common knowledge that the hearer willcorrectly understand the message.\f84E. Davis / Artificial Intelligence 166 (2005) 81–139cusses open problems and summarizes our conclusions. Appendix A gives the proofs ofTheorems 1 and 2.2. FrameworkWe use a situation-based, branching theory of time; an interval-based theory of multi-agent actions; and a possible-worlds theory of knowledge. This is all well known, so thedescription below is brief.2.1. Time and actionWe use a situation-based theory of time. Time can be either continuous2 or discrete, butit must be branching, like the situation calculus. The branching structure is described bythe partial ordering “S1 < S2”, meaning that there is a timeline containing S1 and S2 andS1 precedes S2. It is convenient to use the abbreviations “S1 (cid:1) S2” and “ordered(S1, S2).”The predicate “holds(S, Q)” means that fluent Q holds in situation S.Each agent has, in various situations, a choice about what action to perform next, andthe time structure includes a separate branch for each such choice. Thus, the statement thataction E is feasible in situation S is expressed by asserting that E occurs from S to S1 forsome S1 > S.Following McDermott [26], actions are represented as occurring over an interval; thepredicate occurs(E, S1, S2) states that action E occurs starting in S1 and ending in S2.However, the whole theory could be recast without substantial change into the situationcalculus extended to permit multiple agents, after the style of Reiter [36]. The advantageof using the “occurs” representation is the much greater ease of extensibility. The situationcalculus was developed for domains where a single agent executes a single atomic actionin each situation to bring about the next situation; and extending the situatio",
            {
                "entities": [
                    [
                        121,
                        133,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 171 (2007) 951–984www.elsevier.com/locate/artintMetatheory of actions: Beyond consistencyAndreas Herzig, Ivan Varzinczak ∗IRIT – Université Paul Sabatier, 118 route de Narbonne, 31062, Toulouse Cedex 9, FranceReceived 25 April 2006; received in revised form 15 March 2007; accepted 23 April 2007Available online 29 April 2007AbstractTraditionally, consistency is the only criterion for the quality of a theory in logic-based approaches to reasoning about actions.This work goes beyond that and contributes to the metatheory of actions by investigating what other properties a good domaindescription should have. We state some metatheoretical postulates concerning this sore spot. When all postulates are satisfied wecall the action theory modular. Besides being easier to understand and more elaboration tolerant in McCarthy’s sense, modulartheories have interesting properties. We point out the problems that arise when the postulates about modularity are violated, andpropose algorithmic checks that can help the designer of an action theory to overcome them.© 2007 Elsevier B.V. All rights reserved.Keywords: Reasoning about actions; Action theory; Modularity; Ramifications1. IntroductionIn logic-based approaches to knowledge representation, a given domain is described by a set of logical formulasT , which we call a (non-logical) theory. That is also the case for reasoning about actions, where we are interested intheories describing particular actions (or, more precisely, action types). We call such theories action theories.A priori consistency is the only criterion that formal logic provides to check the quality of such descriptions. Inthe present work we go beyond that, and argue that we should require more than the mere existence of a model for agiven theory.Our starting point is the fact that in reasoning about actions one usually distinguishes several kinds of logicalformulas. Among these are effect axioms, precondition axioms, and boolean axioms. In order to distinguish such non-logical axioms from logical axioms, we prefer to speak of effect laws, executability laws, and static laws, respectively.Moreover we single out those effect laws whose effect is ⊥, and call them inexecutability laws.Given these types of laws, suppose the language is powerful enough to state conditional effects of actions. Forexample, suppose that action a is inexecutable in contexts where ϕ1 holds, and executable in contexts where ϕ2 holds.It follows that there can be no context where ϕ1 ∧ ϕ2 holds. Now ¬(ϕ1 ∧ ϕ2) is a static law that does not mention a.It is natural to expect that ¬(ϕ1 ∧ ϕ2) follows from the static laws alone. By means of examples we show that when* Corresponding author.E-mail addresses: herzig@irit.fr (A. Herzig), ivan@irit.fr (I. Varzinczak).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.013\f952A. Herzig, I. Varzinczak / Artificial Intelligence 171 (2007) 951–984this is not the case, then unexpected conclusions might follow from the theory T , even in the case T is logicallyconsistent.This motivates postulates requiring that the different laws of an action theory should be arranged modularly, i.e.,in separated components, and in such a way that interactions between them are limited and controlled. In essence,we argue that static laws may entail new effects of actions (that cannot be inferred from the effect laws alone), whileeffect laws and executability laws should never entail new static laws that do not follow from the set of static lawsalone. We here formulate postulates that make these requirements precise. It will turn out that in all existing accountsthat allow for these four kinds of laws [1–6], consistent action theories can be written that violate these postulates.In this work we give algorithms that allow one to check whether an action theory satisfies the postulates or not. Withsuch algorithms, the task of correcting flawed action theories can be made easier.Although we here use the syntax of propositional dynamic logic (PDL) [7], all we shall say applies as well tofirst-order formalisms, in particular to the Situation Calculus [8]. All postulates we are going to present can be statedas well for other frameworks, in particular for action languages such as A, AR [9–11] and others, and for SituationCalculus based approaches. In [12] we have given a Situation Calculus version of our analysis, while in [13] wepresented a similar notion for ontologies in Description Logics [14]. The present work is the complete version of theone first appeared in [15].This text is organized as follows: after some background definitions (Section 2) we state some postulates concerningaction theories (Section 3). In Sections 4 and 5, we study the two most important of these postulates, giving algorithmicmethods to check whether an action theory satisfies them or not. We then generalize our postulates (Section 6) anddiscuss possible strengthening of them (Section 7). In Section 8 we show interesting features of modular actiontheories. Before concluding, we assess related work found in the literature on metatheory of actions (Section 9).2. Preliminaries2.1. Dynamic logicHere we establish an ontology of dynamic domains. As our base formalism we use ∗-free PDL, i.e., PDL withoutthe iteration operator ∗. For more details on PDL, see [7,16].Let Act = {a1, a2, . . .} be the set of all atomic action constants of a given domain. Our running example is in termsof the Walking Turkey Scenario [4]. There, the atomic actions are load, shoot and tease. We use a as a variable foratomic actions. To each atomic action a there is an associated modal operator [a]. Here we suppose that the underlyingmultimodal logic is independently axiomatized (i.e., the logic is a fusion and there is no interaction between the modaloperators [17,18]).Prop = {p1, p2, . . .} denotes the set of all propositional constants, also called fluents or atoms. Examples of thoseare loaded, alive and walking. We use p as a variable for propositional constants.We here suppose that both Act and Prop are nonempty and finite.We use small Greek letters ϕ, ψ, . . . to denote classical formulas, also called boolean formulas. They are recursivelydefined in the following way:ϕ ::= p | (cid:5) | ⊥ | ¬ϕ | ϕ ∧ ϕ | ϕ ∨ ϕ | ϕ → ϕ | ϕ ↔ ϕ.Fml is the set of all classical formulas.Examples of classical formulas are walking → alive and ¬(bachelor ∧ married).A classical formula is classically consistent if there is at least one valuation in classical propositional logic thatmakes it true. Given ϕ ∈ Fml, valuations(ϕ) denotes the set of all valuations of ϕ. We note |=CPL the logical conse-quence in classical propositional logic.The set of all literals is Lit = Prop ∪ {¬p: p ∈ Prop}. Examples of literals are alive and ¬walking. (cid:4) will be usedas a variable for literals. If (cid:4) = ¬p, then we identify ¬(cid:4) with p.A clause χ is a disjunction of literals. We say that a literal (cid:4) appears in a clause χ , written (cid:4) ∈ χ , if (cid:4) is a disjunctof χ .We denote complex formulas (possibly with modal operators) by capital Greek letters Φ1, Φ2, . . . They are recur-sively defined in the following way:Φ ::= ϕ | [a]Φ | (cid:11)a(cid:12)Φ | ¬Φ | Φ ∧ Φ | Φ ∨ Φ | Φ → Φ | Φ ↔ Φ\fA. Herzig, I. Varzinczak / Artificial Intelligence 171 (2007) 951–984953where Φ denotes a complex formula. (cid:11)a(cid:12) is the dual operator of [a], defined by: (cid:11)a(cid:12)Φ =def ¬[a]¬Φ. Sequentialcomposition of actions is defined by the abbreviation [a1; a2]Φ =def [a1][a2]Φ. Examples of complex formulas areloaded → [shoot]¬alive and hasGun → (cid:11)load; shoot(cid:12)(¬alive ∧ ¬loaded).If T is a set of formulas (modal or classical), atm(T ) returns the set of all atoms occurring in T . For instance,atm({¬¬¬p1, [a]p2}) = {p1, p2}.For parsimony’s sake, whenever there is no confusion we identify a set of formulas with the conjunction of itselements. The semantics is that for multimodal K [19,20].Definition 1. A PDL-model is a tuple M = (cid:11)W, R(cid:12) where W is a set of valuations (alias possible worlds), andR : Act −→ 2W ×W a function mapping action constants a to accessibility relations Ra ⊆ W × W .As an example, for Act = {a1, a2} and Prop = {p1, p2}, we have the PDL-model M = (cid:11)W, R(cid:12), whereW =(cid:2){p1, p2}, {p1, ¬p2}, {¬p1, p2}(cid:3),(cid:4)(cid:2)(cid:6)R(a1) =R(a2) =({p1, p2}, {p1, ¬p2}), ({p1, p2}, {¬p1, p2}),({¬p1, p2}, {¬p1, p2}), ({¬p1, p2}, {p1, ¬p2})(cid:7)(cid:3)(cid:6){p1, ¬p2}, {p1, ¬p2}{p1, p2}, {p1, ¬p2}.(cid:7),(cid:5),Fig. 1 gives a graphical representation of M.Given M = (cid:11)W, R(cid:12), a ∈ Act, and w, w(cid:14) ∈ W , we write Ra instead of R(a), and wRaw(cid:14) instead of w(cid:14) ∈ Ra(w).Definition 2. Given a PDL-model M = (cid:11)W, R(cid:12), the satisfaction relation is defined as the smallest relation satisfying:w p (p is true at world w of model M) if p ∈ w;• |=M[a]Φ if for every w(cid:14) such that wRaw(cid:14), |=M• |=Mw• the usual truth conditions for the other connectives.w(cid:14) Φ; andDefinition 3. A PDL-model M is a model of Φ (noted |=M Φ) if and only if for all w ∈ W , |=Ma set of formulas T (noted |=M T ) if and only if |=M Φ for every Φ ∈ T .w Φ. M is a model ofIn the model depicted in Fig. 1, we have |=M p1 → [a2]¬p2 and |=M p1 ∨ p2.Definition 4. A formula Φ is a consequence of the set of global axioms T in the class of all PDL-models (notedT |=PDL Φ) if and only if for every PDL-model M, if |=M T , then |=M Φ.1We here suppose that the logic under consideration is compact [21].Fig. 1. Example of a PDL-model for Act = {a1, a2}, and Prop = {p1, p2}.1 Instead of global consequence, in [5] local consequence is considered. For that reason, a further modal operator (cid:2) had to be introduced, givinga logic that is multimodal K plus monomodal S4 for (cid:2), and where axiom schema (cid:2)Φ → [a]Φ holds.\f954A. Herzig, I. Varzinczak / Artificial Intelligence 171 (2007) 951–98",
            {
                "entities": [
                    [
                        113,
                        127,
                        "AUTHOR"
                    ],
                    [
                        129,
                        144,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 170 (2006) 835–871www.elsevier.com/locate/artintOn the computational complexity ofcoalitional resource gamesMichael Wooldridge, Paul E. Dunne ∗Department of Computer Science, University of Liverpool, Liverpool L69 7ZF, United KingdomReceived 15 July 2005; received in revised form 22 February 2006; accepted 29 March 2006Available online 2 May 2006AbstractWe study Coalitional Resource Games (CRGs), a variation of Qualitative Coalitional Games (QCGs) in which each agent isendowed with a set of resources, and the ability of a coalition to bring about a set of goals depends on whether they are collectivelyendowed with the necessary resources. We investigate and classify the computational complexity of a number of natural decisionproblems for CRGs, over and above those previously investigated for QCGs in general. For example, we show that the complexityof determining whether conflict is inevitable between two coalitions with respect to some stated resource bound (i.e., a limit valuefor every resource) is co-NP-complete. We then investigate the relationship between CRGs and QCGs, and in particular the extent towhich it is possible to translate between the two models. We first characterise the complexity of determining equivalence betweenCRGs and QCGs. We then show that it is always possible to translate any given CRG into a succinct equivalent QCG, and that it isnot always possible to translate a QCG into an equivalent CRG; we establish some necessary and some sufficient conditions for atranslation from QCGs to CRGs to be possible, and show that even where an equivalent CRG exists, it may have size exponential inthe number of goals and agents of its source QCG.© 2006 Elsevier B.V. All rights reserved.Keywords: Coalitional games; Resources; Computational complexity; Multi-agent systems1. IntroductionThe questions of why and how self-interested agents might choose to cooperate are central to several researchareas, of which multi-agent systems is an important recent example [4,44,46]. One problem that has received particularattention is that of coalition formation [20,34–36,39,40]. The main question in coalition formation is that of whichcoalition an agent should join: the main answer to this question is that an agent should join a coalition that is stable,that is, one such that no subset of agents from the coalition would have any rational incentive to defect from it [27,p. 255].In previous work, we introduced a model of coalitional games in which agents were assumed to cooperate withone another in order that they can mutually accomplish their goals [47]. Such Qualitative Coalitional Games (QCGs)seem a useful framework for modelling goal-oriented multi-agent systems. The basic idea in QCGs is that each agent* Corresponding author.E-mail address: ped@csc.liv.ac.uk (P.E. Dunne).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.03.003\f836M. Wooldridge, P.E. Dunne / Artificial Intelligence 170 (2006) 835–871desires to achieve one of a set of goals, and every coalition has available to it a set of choices, where each choiceintuitively represents one way that the coalition could choose to cooperate. A choice is modelled as a set of goals,which would be achieved if the coalition chose to cooperate in the corresponding way. The incentive for an agent tojoin a coalition is that the individual choices available to this agent may not result in the satisfaction of its goals, butby cooperating, a coalition can achieve a set of goals to their mutual satisfaction. In [47], we presented a systematicsurvey of the complexity of decision problems associated with QCGs, and also defined an efficient representation forthem, based on propositional logic.Although QCGs seems appropriate for modelling and understanding the abstract properties of cooperation in goal-oriented multi-agent systems, they do not consider the origin of the choices available to coalitions. These choicesare simply ascribed to coalitions via a characteristic function, in much the same way as in conventional coalitionalgames [27, p. 257]. In this paper, we consider a special case of QCGs, which provides one answer to the question ofhow these choices arise. In a Coalitional Resource Game (CRG), the choices available to a coalition are dependenton the resources available to its members and the resources required to achieve goals. Thus, in CRGs, we assumethat agents have goals that they desire to achieve, exactly as in QCGs; but each agent is also assumed to have a fixedendowment of resources, while to achieve any given goal requires the expenditure of a certain profile of resources.A coalition will then form in order to pool resources to achieve a set of goals that satisfies all members of the coalition.Defined in this way, every CRG can also be understood as a QCG: given any CRG, it is possible to construct a QCG that is“equivalent”, in the sense of the choices available to coalitions. Thus, given a CRG, we can ask all the questions relatingto coalitions, goals sets, and QCGs generally that were studied in [47]. But it also becomes possible to ask questionsrelating to (for example) resource consumption (e.g., is the consumption of a given resource strictly necessary in orderto satisfy the goals of a given coalition?) and resource contention (e.g., is it the case that two given coalitions cannotachieve their goals without consuming more than some stated resource bound?). In this sense, CRGs enable us to askmore fine grained questions about cooperation in the scenarios for which they are applicable than is possible usingQCGs.Many naturally occurring scenarios in contemporary computing and AI can be understood as CRGs. One of the mosttimely and important is that of virtual organisations, (VOs), particularly within emerging software infrastructures suchas the Grid:VOs have the potential to change dramatically the way we use computers to solve problems, much as the Web haschanged how we exchange information. [. . . The] need to engage in collaborative processes is fundamental to manydiverse disciplines and activities. It is because of this broad applicability of VO concepts that Grid technology isimportant. [14]VOs are of particular interest in collaborative science projects, where a number of partners cooperate by sharingresources (e.g., particle accelerators, super-computers or Grid networks, gene sequencers) in order to accomplishindividual goals. In such situations, profit is not the motivation; the VO participants are primarily interested in accom-plishing their specific goals. Such scenarios naturally map to CRGs. When the participants in a VO are software agents,then the computational questions associated with them—particularly the complexity of these questions—naturallycome to the fore.We believe that the focus on resources is very natural, given the concerns of multi-agent systems and relateddisciplines: resource limitations, and the need to efficiently manage and share resources in a multi-agent environment,provides one of the fundamental motivations for distributed AI and multi-agent systems [4, p. 9]. Most consideration ofresources in the multi-agent systems community has been directed at the resource allocation problem, i.e., the problemof determining which agent or agents should have access to some scarce resource [4, p. 15]. Economic mechanisms(such as auctions) are currently the focus of much attention with respect to resource allocation [24,33]. In this paper,we focus not on the resource allocation problem, but rather on the properties of such allocations, and in particular,how and what coalitions may form, given a specific allocation, and what the properties of such allocations are withrespect to resources.Overall, the paper makes the following three key contributions to the computational study of games played withresources:\fM. Wooldridge, P.E. Dunne / Artificial Intelligence 170 (2006) 835–871837• First, we present ten natural decision problems associated with CRGs, and classify their computational complex-ity.• Second, we investigate the relationship between CRGs and QCGs in detail. We define the notion of “equivalence”between a CRG and QCG, and show that the problem of deciding equivalence is co-NP-complete.• Third, we investigate a number of questions associated with “translating” between CRGs and QCGs. We estab-lish that, not only is it the case that any CRG can be represented by an equivalent QCG, but that for every CRG,there exists a succinct equivalent QCG. More precisely, we show that any CRG containing t resources, m goals,and n agents can be represented by a QCG of size O(bt (n + m + b)), where b is the number of bits used to en-code endowment and resource quantity values. The proof is constructive, in that we show how to build such anequivalent QCG. With respect to translating from QCGs to CRGs, we first show that in the general case, no suchtranslation is possible. We then define some necessary and some sufficient conditions for such a translation to bepossible, and show that, even when such a translation is possible, it may result in a CRG of size exponential inn + m.The paper also makes a more general contribution to the problem of how to represent coalitional games succinctly, aproblem which has attracted a number of researchers over the past five years or so (cf. [3,8,17]); see Section 6.The remainder of the paper is structured as follows. First, in Section 2, we motivate and introduce the formalframework of CRGs. Section 3 presents our main complexity results. Section 4 considers the relationship betweenCRGs and QCGs, Section 5 discusses variants of the CRG model in which some of the underlying assumptions arerelaxed, while Section 6 presents some related work. Finally, Section 7 presents some conclusions. We begin, in thefollowing subsection, with a summary of some key notational conventions and a very brief review of some relevantconcepts from complexity theory.1.1. No",
            {
                "entities": [
                    [
                        132,
                        150,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Artificial Intelligence 186 (2012) 1–37Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintTowards fixed-parameter tractable algorithms for abstractargumentation ✩Wolfgang Dvoˇrák∗, Reinhard Pichler, Stefan WoltranInstitute of Information Systems, Vienna University of Technology, A-1040 Vienna, Austriaa r t i c l ei n f oa b s t r a c tAbstract argumentation frameworks have received a lot of interest in recent years. Mostcomputational problems in this area are intractable but several tractable fragments havebeen identified. In particular, Dunne showed that many problems can be solved in lineartime for argumentation frameworks of bounded tree-width. However, these tractabilityresults, which were obtained via Courcelle’s Theorem, do not directly lead to efficientalgorithms. The goal of this paper is to turn the theoretical tractability results into efficientalgorithms and to explore the potential of directed notions of tree-width for defining largertractable fragments. As a by-product, we will sharpen some known complexity results.© 2012 Elsevier B.V. All rights reserved.Article history:Received 16 June 2011Received in revised form 1 February 2012Accepted 11 March 2012Available online 13 March 2012Keywords:Abstract argumentationFixed-parameter tractabilityTree-widthDynamic programmingComplexity1. IntroductionArgumentation has evolved as an important field in AI with abstract argumentation frameworks (AFs, for short) asintroduced by Dung [20] being its most popular formalization. Meanwhile, a wide range of semantics for AFs has beenproposed (for an overview see [4]) and their complexity has been analyzed in depth. Most computational problems in thisarea are intractable (see e.g. [17,24,26]), but the importance of efficient algorithms for tractable fragments has been clearlyrecognized (see e.g. [18]). Such tractable fragments are, for instance, symmetric argumentation frameworks [12] or bipartiteargumentation frameworks [22].An interesting approach to dealing with intractable problems comes from parameterized complexity theory and is basedon the following observation: Many hard problems become tractable if some problem parameter is bounded by a fixedconstant. This property is referred to as fixed-parameter tractability (FPT). One important parameter of graphs is the tree-width, which measures the “tree-likeness” of a graph. Indeed, Dunne [22] showed that many problems in the area ofargumentation can be solved in linear time for argumentation frameworks of bounded tree-width. This FPT result wasshown via a seminal result by Courcelle [13]. However, as stated in [22], “rather than synthesizing methods indirectly fromCourcelle’s Theorem, one could attempt to develop practical direct methods”. The primary goal of this paper is therefore topresent new, direct algorithms for certain reasoning tasks in abstract argumentation.Clearly, the quest for FPT results in argumentation should not stop at the tree-width, and further parameters have to beanalyzed. This may of course also lead to negative results. For instance, considering as parameter the degree of an argument(i.e., the number of incoming and outgoing attacks), Dunne [22] showed that reasoning remains intractable, even if decision✩This work has been funded by the Vienna Science and Technology Fund (WWTF) through project ICT08-028 and by the Austrian Science Fund (FWF)under grant P20704-N18. A short version of this article appeared in the Proceedings of the 12th International Conference on Knowledge Representationand Reasoning (KR 2010), AAAI Press, 2010.* Corresponding author.E-mail addresses: dvorak@dbai.tuwien.ac.at (W. Dvoˇrák), pichler@dbai.tuwien.ac.at (R. Pichler), woltran@dbai.tuwien.ac.at (S. Woltran).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.03.005\f2W. Dvoˇrák et al. / Artificial Intelligence 186 (2012) 1–37problems are given over AFs with at most two incoming and two outgoing attacks. A number of further parameters ishowever, still unexplored. Hence, the second major goal of this paper is to explore the potential of further parametersfor identifying tractable fragments of argumentation. In particular, since AFs are directed graphs, it is natural to considerdirected notions of width to obtain larger classes of tractable AFs. To this end, we investigate the effect of bounded cycle-rank [28] on reasoning in AFs. We show that reasoning remains intractable even if we only consider AFs of cycle-rank 2.Actually, many further directed notions of width exist in the literature. However, it has been recently shown [6,33,31] thatproblems which are hard for bounded cycle-rank remain hard when several other directed variants of the tree-width arebounded. A notable exception is the related notion of clique-width [14] which (in contrast to tree-width) can be directlyextended to directed graphs. Moreover, meta-theorems for clique-width [15] show that Dunne’s result on tractability withrespect to bounded tree-width extend to AFs of bounded clique-width (for details, we refer to [27]).Still, the main focus of this paper is on novel algorithms for decision problems defined over the so-called preferredsemantics of AFs. Roughly speaking, the preferred extensions of an AF are maximal admissible sets of arguments, whereadmissible means that the selected arguments defend themselves against attacks. To be more precise, we present herealgorithms for the following three decision problems.• Credulous acceptance: deciding whether a given argument is contained in at least one preferred extension of a given AF.• Skeptical acceptance: deciding whether a given argument is contained in all preferred extensions of a given AF.• Ideal acceptance: deciding whether a given argument is contained in an admissible set which itself is a subset of eachpreferred extension of a given AF.The problem of ideal acceptance is better known as ideal semantics [21]. To the best of our knowledge, FPT results for idealsemantics have not been established yet, thus the algorithm that we present in the paper provides such a result as a by-product (one could alternatively use Courcelle’s meta-theorem to obtain that result). By its very nature, the running timesof our novel algorithms will heavily depend on the tree-width of the given AF, but are linear in the size of the AF. Thus forAFs of small tree-width, these algorithms are expected to be preferable over standard algorithms from the literature (seee.g. [19,38]).One reason why we have chosen the preferred semantics for our work here is that it is widely used. Moreover, admissi-bility and maximality are prototypical properties common in many other semantics, for instance complete and stable [20],stage [43], and semi-stable [10] semantics. Hence, we expect that the methods developed here can also be extended toother semantics.1.1. Summary of results• We first prove some negative results: we show that reasoning remains intractable in AFs of bounded cycle-rank [28].As has been mentioned above, this negative result carries over to many other directed notions of width. We also showthat the problem of skeptical acceptance is coNP-complete for AFs of cycle-rank 1.• We develop a dynamic programming approach to characterize admissible sets of AFs. The time complexity of our algo-rithm is linear in the size of the AFs (as expected by Courcelle’s Theorem) with a multiplicative constant that is singleexponential in the tree-width (which is in great contrast to algorithms derived via Courcelle’s Theorem). This algorithmcan be directly used to decide the problem of credulous acceptance.• This dynamic programming algorithm is then extended so as to cover also the preferred semantics, and thus to decideskeptical acceptance.• We finally show how to further adapt this algorithm to decide ideal acceptance.1.2. Structure of the paperIn Section 2, we recall some basic notions and results on AFs and discuss some width-measures for graphs. We thenshow in Section 3 some negative results for reasoning in AFs where some parameters of directed graphs are bounded. InSection 4.1, we first develop a dynamic programming approach for credulous acceptance in AFs of bounded tree-width. Thisalgorithm is then extended to cover also preferred semantics in Section 4.2 and adapted to ideal acceptance in Section 4.3.Section 5 provides some final conclusions as well as pointers to related and future work.2. BackgroundIn this section, we first introduce argumentation frameworks and then some graph measures we want to investigate forsuch frameworks.2.1. Argumentation frameworksWe start by introducing (abstract) argumentation frameworks [20], and then recall the preferred as well as the idealsemantics for such frameworks. Afterwards, we highlight some known complexity results for typical decision problemsassociated to such frameworks.\fW. Dvoˇrák et al. / Artificial Intelligence 186 (2012) 1–373Definition 1. An argumentation framework (AF) is a pair F = ( A, R) where A is a set of arguments and R ⊆ A × A is theattack relation. We sometimes use the notation a (cid:2) b instead of (a, b) ∈ R, in case no ambiguity arises. Further, for S ⊆ Aand a ∈ A, we write S (cid:2) a (resp. a (cid:2) S) iff there exists b ∈ S, such that b (cid:2) a (resp. a (cid:2) b). An argument a ∈ A is defended⊕ = {b ∈ A | S (cid:2) b}.by a set S ⊆ A iff for each b ∈ A, such that b (cid:2) a, also S (cid:2) b holds. Finally, for a set S ⊆ A we define SAn AF can naturally be represented as a directed graph.Example 1. Let F = ( A, R) with A = {a, b, c, d, e, f , g} and R = {(a, b), (c, b), (c, d), (d, c), (d, e), (e, g), ( f , e), (g, f )}. Thegraph representation of F is given as follows.We continue with a few basic concepts and the definition of preferred extensions as introduced in Dung’s seminalpaper [20] as well as the concept of ideal sets as proposed by Dung, Mancarella and Toni [21].Definition 2. Let F = ( A, R",
            {
                "entities": [
                    [
                        233,
                        249,
                        "AUTHOR"
                    ],
                    [
                        251,
                        265,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "NIH Public AccessAuthor ManuscriptNeurocomputing. Author manuscript; available in PMC 2012 June 1.Published in final edited form as:Neurocomputing. 2011 June 1; 74(12-13): 2184–2192. doi:10.1016/j.neucom.2011.02.014.Physical Activity Recognition Based on Motion in ImagesAcquired by a Wearable CameraHong Zhanga,*, Lu Lia,b, Wenyan Jiab, John D. Fernstromc, Robert J. Sclabassib, Zhi-HongMaod, and Mingui Sunb,d,*a Image Processing Center, Beihang University, Beijing 100191, Chinab Department of Neurosurgery, University of Pittsburgh, PA 15213, USAc Department of Psychiatry, University of Pittsburgh, PA 15261, USAd Department of Electrical and Computer Engineering, University of Pittsburgh, PA 15261, USAAbstractA new technique to extract and evaluate physical activity patterns from image sequences capturedby a wearable camera is presented in this paper. Unlike standard activity recognition schemes, thevideo data captured by our device do not include the wearer him/herself. The physical activity ofthe wearer, such as walking or exercising, is analyzed indirectly through the camera motionextracted from the acquired video frames. Two key tasks, pixel correspondence identification andmotion feature extraction, are studied to recognize activity patterns. We utilize a multiscaleapproach to identify pixel correspondences. When compared with the existing methods such as theGood Features detector and the Speed-up Robust Feature (SURF) detector, our technique is moreaccurate and computationally efficient. Once the pixel correspondences are determined whichdefine representative motion vectors, we build a set of activity pattern features based on motionstatistics in each frame. Finally, the physical activity of the person wearing a camera is determinedaccording to the global motion distribution in the video. Our algorithms are tested using differentmachine learning techniques such as the K-Nearest Neighbor (KNN), Naive Bayesian and SupportVector Machine (SVM). The results show that many types of physical activities can be recognizedfrom field acquired real-world video. Our results also indicate that, with a design of specificmotion features in the input vectors, different classifiers can be used successfully with similarperformances.KeywordsActivity recognition; classification; feature extraction; feature matching; motion histogram;multiscaleI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscript1. IntroductionVideo based activity recognition has been an active field of research in computer vision andmultimedia systems [1-9]. Although numerous algorithms have been developed, a© 2011 Elsevier B.V. All rights reserved.*Corresponding author. dmrzhang@buaa.edu.cn drsun@pitt.edu.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to ourcustomers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review ofthe resulting proof before it is published in its final citable form. Please note that during the production process errors may bediscovered which could affect the content, and all legal disclaimers that apply to the journal pertain.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 2fundamental requirement has been that the target object engaging in a certain activity mustappear in the video, which is not achievable in many practical cases where the object neverappears in the video because the camera can only be mounted on the target object itself.Examples of such objects include a spaceship, an aircraft, a submarine, a vehicle, a robot, ananimal, or a person. For example, if the goal is to study an individual's physical activity overan entire day in a free-living environment, it is unrealistic to track the person with videocameras. Alternatively, with today's technological advancement, a subject can comfortablywear a small camera for the entire day. Although he/she does not appear in the recordedvideo, physical activity can be recognized indirectly by observing the recorded backgroundscene. In general, a specific activity will result in a specific motion of the camera since it ismounted on the human body and the background scene will change accordingly when thecamera is moved.We have been investigating the use of a wearable video device to monitor food intakecontinuously in obese individuals [10]. However, modification of diet (energy input)represents only half of the energy balance equation of the human body. The other half isphysical activity (energy output). A worn device that unobtrusively and automaticallyrecords physical activity will provide a powerful tool for the development of individualizedobesity treatment programs that help people lose weight and keep it off.Wearable sensors that objectively measure body motion and dynamics have been developed[11-14]. One common approach is to use accelerometers attached at multiple locations of thebody to measure both acceleration and orientation [11, 12]. The accuracy of physical activityrecognition by accelerometer-based systems generally improves as the number ofaccelerometers increases. However, the obtrusiveness of such systems makes it inconvenientto wear and use in daily life. We have thus developed a new wearable device, whichcontains a video camera and other sensors, to monitor both food intake and physical activity(see Fig. 1). The device is mounted in front of the chest using a pin, a lanyard or a pair ofmagnets, allowing it to measure the trunk motion of the upper body. While the generaldesign of the device and its food intake measurement function are published elsewhere [10,15, 16], this paper describes the algorithms utilized to process the acquired video data andrecognize several common types of physical motion and activity.Numerous vision algorithms are available for activity recognition using features extractedfrom the observed target directly[2, 3]. Unfortunately, these algorithms do not apply to ourcase where the target (a person) does not appear in the video. The key problem is thereforeto find descriptions of the physical activity in the recorded video without direct observationof the target. These descriptions can be obtained if the following two assumptions aresatisfied: 1) the motion profiles of the activities to be recognized differ from each other, and2) the background scene is rich enough so that sufficient image features can be extractedreliably.We approach the indirect activity recognition problem by investigating camera motion anddeveloping an activity detection scheme based on 2D image features. Considering that thecamera in the wearable device is usually not controlled intentionally and, as a result, theacquired images are often blurry, we match correspondent points between adjacent framesusing multiscale local features. In order to reduce errors in pixel-pair matching, we imposeuniqueness and epipolar constraints which eliminate ambiguous pixel pairs. After thecorrespondence selection process, a motion histogram is defined according to motionvectors obtained from the selected pixel pairs. For each activity video, an accumulation ofmotion vectors is evaluated based on a set of motion histograms to obtain global motioncharacteristics which finally lead to physical activity recognition.Neurocomputing. Author manuscript; available in PMC 2012 June 1.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptZhang et al.Page 3This paper is organized as follows. Section 2 provides an overview of our algorithms.Detailed descriptions of these algorithms are presented in Section 3. Experimental results areprovided in Section 4. In Sections 5 and 6, we summarize our work and discuss futuredirections on physical activity recognition using the wearable camera approach.2. OutlineOur framework for recognizing physical activity is shown in Fig. 2. It consists of three dataprocessing steps: feature extraction, motion representation and activity recognition. In thefirst step, local image features are extracted from which a set of “salient key points” aredetermined. In the second step, we match these key points between neighboring frames. Thematched points define a set of motion vectors. A motion histogram is then defined accordingto these vectors. In the last step, the cumulative motion over all frame pairs is evaluated.Physical activity is recognized based on motion histograms and global motioncharacteristics.3. Methods3.1. Feature ExtractionObtaining reliable correspondences of features is essential in our activity recognition systembecause inaccurate correspondences produce ambiguous motion estimation. We use localimage features which are widely investigated [17-26]. We prefer local features to globalfeatures because the local ones can be detected and represented more easily. The keyproblem here is to find salient points in each image. Shi and Tomasi [22] described a methodcalled “Good Features”, which computes the minimum eigenvalue of the covariance matrixinstead of the cost function defined in the Harris detector [23]; Lowe [17] presented a ScaleInvariant Feature Transform (SIFT) method using scale space analysis. This method isinvariant to scale, orientation and affine distortion [18-20]. Bay et al. [24, 25] proposed aSpeed-Up Robust Features (SURF) method using the 2D Haar wavelet.Although the existing methods have been well studied for activity recognition from directlyrecorded images as the input, these methods usually require these images to have reasonablyhigh quality. In our case, however, the wearable device is uncontrolled and thus the imagesacquired are often blurred. We have found that the blur of our images resulted from manyfactors, and it is hence difficult for us to choose the most suitable model for de-blurring.Occasionally, an incorrect model even aggravates noise. Hence we use a multiscale detectorto capture motion features in an ”overlooking” scale in wh",
            {
                "entities": [
                    [
                        314,
                        320,
                        "AUTHOR"
                    ],
                    [
                        324,
                        335,
                        "AUTHOR"
                    ],
                    [
                        397,
                        408,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Adversarial dictionary learning for a robust analysis and modelling of spontaneousneuronal activityEirini Troullinoua,b, Grigorios Tsagkatakisb, Ganna Palaginac,d, Maria Papadopoulia,b, Stelios Manolis Smirnakisc,d,Panagiotis Tsakalidesa,baDepartment of Computer Science, University of Crete, Heraklion, 70013, GreecebInstitute of Computer Science, Foundation for Research and Technology Hellas, Heraklion, 70013, GreececDepartment of Neurology, Brigham and Women’s Hospital, Harvard Medical School,Boston MA 02115dBoston VA Research Institute, Jamaica Plain Veterans Administration Hospital,Harvard Medical School, Boston, United States9102ceD42]CN.oib-q[2v12710.1191:viXraAbstractThe field of neuroscience is experiencing rapid growth in the complexity and quantity of the recorded neural activityallowing us unprecedented access to its dynamics in different brain areas. The objective of this work is to discoverdirectly from the experimental data rich and comprehensible models for brain function that will be concurrently robustto noise. Considering this task from the perspective of dimensionality reduction, we develop an innovative, robust to noisedictionary learning framework based on adversarial training methods for the identification of patterns of synchronousfiring activity as well as within a time lag. We employ real-world binary datasets describing the spontaneous neuronalactivity of laboratory mice over time, and we aim to their efficient low-dimensional representation. The results on theclassification accuracy for the discrimination between the clean and the adversarial-noisy activation patterns obtainedby an SVM classifier highlight the efficacy of the proposed scheme compared to other methods, and the visualization ofthe dictionary’s distribution demonstrates the multifarious information that we obtain from it.Keywords: Dictionary Learning, Supervised Machine Learning, biological neural networks.1. IntroductionThe advances of imaging and monitoring technologies,such as in vivo 2-photon calcium imaging at the meso-scopic regime as well as the massive increases in compu-tational power and algorithmic development have enabledadvanced multivariate analyses of neural population activ-ity, recorded either sequentially or simultaneously.More specifically, high resolution optical imaging meth-ods have recently revealed the dynamic patterns of neuralactivity across the layers of the primary visual cortex (V1)leading to this important question: Neuronal groups thatfire in synchrony may be more efficient at relaying sharedinformation and are more likely to belong to networks ofneurons subserving the same function. By using 2-photonimaging, we monitored the spontaneous population burstsof activity in pyramidal cells and interneurons of mouse inL2/3 V1. We found that the sizes of spontaneous popula-tion bursts and the degree of connectivity of the neuronsin specific fields of view (FOVs) formed scale-free distri-butions, suggestive of a hierarchical small-world net ar-chitecture [1]. The existence of such groups of ”linked”units inevitably shapes the profile of spontaneous events(cid:73)Fully documented templates are available in the elsarticle pack-age on CTAN.observed in V1 networks [2, 3, 4]. Thus, the analysis of thespontaneous activity patterns provides an opportunity foridentifying groups of neurons that fire with increased lev-els of synchrony (have significant ”functional connectivity”between each other).In order to analyze these populations and to find fea-tures that are not apparent at the level of individual neu-rons, we adopt dictionary learning (DL) methods, whichprovide a parsimonious description of statistical features ofinterest via the output dictionary, discarding at the sametime some aspects of the data as noise. Moreover, dictio-naries are a natural approach for performing exploratorydata analysis as well as visualization. Given the fact thatthe dictionary is the new space of reduced dimensionality,the computational complexity of its management is muchsmaller compared to the initial raw data and thus, for allthese advantages, DL has been applied in various domains.In brain signaling specifically, the K-SVD algorithm[5], has been used for capturing the behavior of neuronalresponses into a dictionary, which was evaluated with real-world data for its generalization capacity as well as for itssensitivity with respect to noise [6]. DL has been alsosuggested for the EEG (electroencephalography) inverseproblem. Specifically, Liu et al. [7] proposed a supervisedformulation of source reconstruction and supervised sourceclassification to address the estimation of brain sourcesPreprint submitted to Journal of LATEX TemplatesDecember 25, 2019   \fand to distinguish the various sources associated with dif-ferent status of the brain. Moreover, accurate EEG signalclassification plays an important role in the performanceof BCI (Brain Computer Interface) applications. Ameri[8] adapted the projective dictionary pair learninget al.method (DPL) for EEG signal classification. They learneda synthetic as well as an analysis dictionary, which wereused in the classification step to increase the speed and[9] proposed aaccuracy of the classifier. Morioka et al.dictionary learning, sparse coding method to address theissue of the inherent variability existing in brain signalscaused by different physical and mental conditions amongmultiple subjects and sessions. Such variability compli-cates the analysis of data from multiple subjects and ses-sions in a consistent way, and degrades the performance ofneural decoding in BCI applications.In this work, we propose the Adversarial DictionaryLearning (ADL) algorithm, which captures the synchronic-ity patterns among neurons, and its extended version, theRelaxed Adversarial Dictionary Learning (RADL) for cofir-ing patterns within a time lag. Adversarial training is theprocess of explicitly training a model on adversarial ex-amples, in order to increase its robustness to noisy inputs.Thus, we create an adversarial learning environment byusing clean and adversarial-noisy activation patterns. Themain objectives are the construction of a dictionary thatwill be robust to the measurement noise (i.e. calcium fluc-tuations) as well as to the identification of firing eventsemerging by chance. Both ADL and RADL construct theoutput dictionary by selecting only those patterns of theinput data that contribute to a better reconstructed rep-resentation of the clean input signal than the adversarial-noisy one. After obtaining our trained dictionary, we quan-tify its quality and robustness by training a supervisedclassifier with the reconstructed clean and noisy signals aswell as with the raw ones, and examine when the classifierexhibits the smallest testing error. To assess whether thetrained dictionary has captured the underlying statisticsof the input data, we employ the classification accuracy(i.e. the extent to which the classifier can discriminate theclean from the noisy signal).To validate the proposed algorithms, we employed tworeal-world binary datasets that depict the neuronal activ-ity of a 9-day old and a 36-day old C57BL/6 laboratorymouse. Data was collected using 2-photon calcium imag-ing in the V1, L2/3 area of the neocortex of the animals.Fig. 1 illustrates the format of our data, where each col-umn represents an example-activation pattern that con-sists of 0s for the non-firing events, while 1s represent thefiring events.While DL has delivered impressive results in variousdomains (such as pattern recognition, and data mining),the construction of the appropriate dictionary dependingon the application still remains challenging. A commondrawback in DL algorithms is the generation of real-numbereddictionaries, which in our domain have no physical mean-ing, and thus they cannot be directly used for extracting2useful information from the data nor for visualizations.Thus, an innovative aspect of our work is shaped by therequirement of constructing binary dictionaries (given thebinary activation patterns). Additionally, while the major-ity of the algorithms require a dictionary size parameter,often there is no prior-knowledge on the number of pat-terns that should be used. To overcome these limitations,ADL constructs a dictionary, using the most representativeand robust patterns of the input data and automaticallyestimates the dictionary size, as the algorithm does thisitself during the dictionary construction. RADL offers thesame benefits and is extended to discover temporal pat-terns within a lag (i.e. temporal patterns in larger timewindows). The contributions of this work are summarizedas follows:• Adversarial DL outputs robust to noise dictionar-ies by excluding those patterns from the input data,which could be a result of noise, caused mainly fromcalcium fluctuations or other sources of imaging noise.• Acquisition of an interpretable dictionary, as the dic-tionary elements are selected from the input dataand thus, the dictionary construction is not a re-sult of a mathematical transformation, as opposedto other methods, such as K-SVD [5] or PCA [10].• In contrast to other methods that require a choice ofdimensionality K (dictionary size), here this is not aparameter that has to be determined by the user, orbe estimated (e.g. based on the choice of arbitrarycutoff values or cross-validation methods [11]).• Detection of statistically significant synchronous andwithin a lag temporal patterns of activity, whichcan be distinguished from shuffled data (adversarial-noisy examples), whose temporal correlations are de-stroyed.InThe remainder of the paper is organized as follows:Section II, we describe the proposed approaches. Evalua-tion methodology and experimental results are presentedin Section III. Related work is reported in Section IV, whileconclusions are drawn in Section V.2. Proposed Dictionary Learning FrameworkIn this section we present the proposed DL methods:• Adversarial Dictionary Learnin",
            {
                "entities": [
                    [
                        120,
                        142,
                        "AUTHOR"
                    ],
                    [
                        144,
                        159,
                        "AUTHOR"
                    ],
                    [
                        163,
                        181,
                        "AUTHOR"
                    ],
                    [
                        185,
                        211,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Understanding common human driving semanticsfor autonomous vehiclesArticleHighlightsd Reveal human auditory cortex activation during drivingd Discover the hierarchical structure of human drivingunderstandingd Propose a neural-informed semantics-driven drivingunderstanding modeld Address long-term contextual dependency of drivingbehaviorsAuthorsYingji Xia, Maosi Geng, Yong Chen, ...,Bing Zhang, Ziyou Gao,Xiqun (Michael) ChenCorrespondencechenxiqun@zju.edu.cnIn briefAutonomous vehicles will share roadswith human-driven vehicles and bringwith them problems regardingbidirectional understanding of drivingbehavior. Based on cerebral neurologicalfindings from the human process forunderstanding driving, a novel neural-inspired semantics-driven drivingunderstanding model is proposed forautonomous vehicles. The model imitatesthe way humans understand driving andcan interpret long-term driving behaviorevolutions like human drivers.Xia et al., 2023, Patterns 4, 100730June 9, 2023 ª 2023 The Author(s).https://doi.org/10.1016/j.patter.2023.100730ll\fPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSArticleUnderstanding common human drivingsemantics for autonomous vehiclesYingji Xia,1 Maosi Geng,1,2 Yong Chen,1 Sudan Sun,3 Chenlei Liao,1 Zheng Zhu,1,4,10 Zhihui Li,5Washington Yotto Ochieng,6 Panagiotis Angeloudis,6 Mireille Elhajj,6 Lei Zhang,7 Zhenyu Zeng,7 Bing Zhang,7 Ziyou Gao,8and Xiqun (Michael) Chen1,4,9,10,11,*1Institute of Intelligent Transportation Systems, College of Civil Engineering and Architecture, Zhejiang University, Hangzhou 310058, China2Polytechnic Institute & Institute of Intelligent Transportation Systems, Zhejiang University, Hangzhou 310015, China3School of Medicine, Zhejiang University, Hangzhou 310058, China4Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, Hangzhou 310027, China5School of Transportation, Jilin University, Changchun 130022, China6Department of Civil and Environmental Engineering, Imperial College London, South Kensington Campus, London SW7 2AZ, UK7Alibaba Group, Hangzhou 310052, China8School of Traffic and Transportation, Beijing Jiaotong University, Beijing 100044, China9Zhejiang University/University of Illinois Urbana-Champaign (ZJU-UIUC) Institute, Zhejiang University, Haining 314400, China10Zhejiang Provincial Engineering Research Center for Intelligent Transportation, Hangzhou 310058, China11Lead contact*Correspondence: chenxiqun@zju.edu.cnhttps://doi.org/10.1016/j.patter.2023.100730THE BIGGER PICTURE ‘‘Driving like humans’’ is the ultimate goal of autonomous driving. Hence, human-like driving understanding ability is required for autonomous vehicles to better understand the driving be-haviors of surrounding human-driven vehicles. In this study, we investigated human driving neural responseand subsequently built a biologically plausible model to interpret driving behaviors like humans. This studypioneers the design of bio-inspired, human-like autonomous vehicles and can ultimately benefit futureresearch of human-machine interactions.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYAutonomous vehicles will share roads with human-driven vehicles until the transition to fully autonomoustransport systems is complete. The critical challenge of improving mutual understanding between bothvehicle types cannot be addressed only by feeding extensive driving data into data-driven models but byenabling autonomous vehicles to understand and apply common driving behaviors analogous to humandrivers. Therefore, we designed and conducted two electroencephalography experiments for comparingthe cerebral activities of human linguistics and driving understanding. The results showed that driving acti-vates hierarchical neural functions in the auditory cortex, which is analogous to abstraction in linguistic un-derstanding. Subsequently, we proposed a neural-informed, semantics-driven framework to understandcommon human driving behavior in a brain-inspired manner. This study highlights the pathway of fusingneuroscience into complex human behavior understanding tasks and provides a computational neural modelto understand human driving behaviors, which will enable autonomous vehicles to perceive and think like hu-man drivers.INTRODUCTIONAutonomous vehicles (AVs) continue to receive significant atten-tion worldwide because they have the potential to realize a safer,faster, and more efficient mode of transportation. Every day,almost 2,700 people are killed globally in traffic crashes6; fataland non-fatal crash injuries are estimated to cost approximately1.8 trillion US dollars between 2015 and 2030.7 By shiftingvehicle control from the human driver to machines via AVs,driver-related road crashes can be eliminated and thus savePatterns 4, 100730, June 9, 2023 ª 2023 The Author(s). 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSACBDArticleEFigure 1. Four-stage development to understand the driving behavior of AVs(A) The surrounding vehicles are considered moving obstacles without self-consciousness.(B) The velocity of the surrounding vehicles is predicted using probability distribution outputs of discrete choice models.(C) The potential maneuvers of surrounding vehicles are surmised by recurrently applying short-time trajectory prediction models.(D) The intentions of the surrounding vehicles are understood from their contextual driving behaviors.(E) The mechanistic and biological requirements for AV development.lives.8 However, until the transition to fully autonomous transportis completed, AVs will inevitably share roads with human-drivenvehicles. During this transitory phase, AVs and human-driven ve-hicles need to share mutually interactive behaviors.9,10 Given thiscontext, it is impossible to expect every human driver to accom-modate certain traits and attributes of AVs, such as inconsistentor stilted driving behaviors (e.g., aggressive car following, jerk-ing, sudden braking, or unexpected mandatory lane changing).Existing studies have revealed that a lack of transparency inAV decision making creates a psychological barrier that affectshuman drivers’ trust in AVs11; human drivers expect AVs tomimic their driving behaviors to become trustworthy.12 A moreplausible approach is for AVs to acquire the ability to drive likehuman drivers, which would make it easier for other road usersto interpret their driving behaviors and react appropriately. Thiswould subsequently rebuild the driver’s trust and increase thesocial acceptability of AVs.13,14In recent years, various types of AVs were developed andtested in urban road scenarios, and they yielded promising re-sults and applications.15–17 Given that vehicular sensing andnavigation technologies are relatively mature,18 major concernswith AV adoption are related to whether AVs can interact appro-priately with the human-driven vehicles in the surrounding areas.However, research studies on understanding common drivingbehaviors and designing AVs to operate while following hu-man-like principles or brain-inspired mechanisms remain lack-ing. Therefore, we developed a method to understand AV drivingbehaviors as shown in Figure 1, where the red vehicle representsan AV and the blue vehicles represent the surrounding human-driven vehicles in a typical driving scenario.Unlike the classical vehicular trajectory prediction and routeplanning models19–21 widely accepted in the robotics field(Figure 1A) or the various discrete-choice driving models22,232 Patterns 4, 100730, June 9, 2023developed in the traffic engineering discipline (Figure 1B), recentresearch24–26 showed that subjective individual human drivingfactors are critical and cannot be neglected in the developmentof human-like AVs (Figure 1C). Unfortunately, as human drivingbehaviors evolve with an indefinite temporal dependency,state-of-the-art machine learning-based methods that partiallyimitate the nature of the human driving decision-making processmay lose the ability to adapt and generalize. For example, stand-alone data-driven intention recognizers (e.g., deep neural net-works) employed in machine learning-based methods are likelyto be trapped in the following dilemma: those with increasedtemporal inputs carry a more significant risk of overfitting localfeatures and the output confusing driving intentions, whereasthose with shorter temporal dependencies are too myopic to fullyunderstand the driving intentions of surrounding drivers, such ashuman drivers.Understanding common human driving behaviors that followthe human cerebral driving thinking mechanisms of humandrivers (Figure 1D) is necessary to address this dilemma.27–29As human driving behaviors are generated by humans ratherthan machines, AV development needs to be mechanisticallyand biologically plausible (Figure 1E).Our research is motivated by the fact that talking while drivingcan cause severe distractions because both behaviors or activ-ities share the same cerebral resources30,31 (right parietal re-sources32 and working memory in the prefrontal cortex33,34).Therefore, we attempt to fuse neuroscience and robotics in aneuroengineering manner35 in this study. To this end, we de-signed two separate electroencephalography (EEG) experi-ments to reveal the formation of cerebral driving thinking andevolution using well-studied linguistic analyses. We subse-quently present a semantics-driven method to understand com-mon driving behaviors for developing AVs in a brain-inspired\fPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous ",
            {
                "entities": [
                    [
                        357,
                        368,
                        "AUTHOR"
                    ],
                    [
                        1331,
                        1342,
                        "AUTHOR"
                    ],
                    [
                        369,
                        379,
                        "AUTHOR"
                    ],
                    [
                        1346,
                        1356,
                        "AUTHOR"
                    ],
                    [
                        1358,
                        1368,
                        "AUTHOR"
                    ],
                    [
                        1370,
                        1383,
                        "AUTHOR"
                    ],
                    [
                        1385,
                        1395,
                        "AUTHOR"
                    ],
                    [
                        1402,
                        1412,
                        "AUTHOR"
                    ],
                    [
                        1440,
                        1462,
                        "AUTHOR"
                    ],
                    [
                        1464,
                        1480,
                        "AUTHOR"
                    ],
                    [
                        1482,
                        1492,
                        "AUTHOR"
                    ],
                    [
                        1494,
                        1506,
                        "AUTHOR"
                    ],
                    [
                        1508,
                        1519,
                        "AUTHOR"
                    ],
                    [
                        396,
                        406,
                        "AUTHOR"
                    ],
                    [
                        1521,
                        1531,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "A hybrid deep-learning approach for complex biochemicalnamed entity recognitionJian Liu1,3,10, Lei Gao2,*, Sujie Guo1,4, Rui Ding1,5, Xin Huang6, LongYe7,8, Qinghua Meng8, Asef Nazari9, and Dhananjay Thiruvady91 HeFei University of Technology, Hefei, 2300092 CSIRO, Waite Campus, Urrbrae, SA 5064, Australia3 Intelligent Interconnected Systems Laboratory of Anhui Province, Hefei Universityof Technology4 Shanghai Engineering Center for Microsatellites, Shanghai 201203, China5 Xi’an Jiaotong University, Xi’an,710049, China6 College of Computer and Information Engineering, Tianjin Normal University,Tianjin 300387, China7 School of Mechatronic Engineering and Automation, Shanghai University, Shanghai200072, China8 Faculty of Business Information, Shanghai Business School, Shanghai 201400, China9 School of Information Technology, Deakin University, Geelong, Australia10 Anhui Province Key Laboratory of Industry Safety and Emergency Technology,Hefei 230601, Anhui, P.R. China*Corresponding author:Dr. Lei GaoSenior Research ScientistCSIRO Land and WaterPrivate Mail Bag 2, Waite RoadGlen Osmond SA 5064, AustraliaPh: +61-8-8273 8109Fax: +61-8-8303 8750Email: lei.gao@csiro.au1\fAbstract:Named entity recognition (NER) of chemicals and drugs is a critical domain ofinformation extraction in biochemical research. NER provides support for text miningin biochemical reactions, including entity relation extraction, attribute extraction, andmetabolic response relationship extraction. However, the existence of complex namingcharacteristics in the biomedical field, such as polysemy and special characters, makethe NER task very challenging. Here, we propose a hybrid deep learning approach toimprove the recognition accuracy of NER. Specifically, our approach applies theBidirectional Encoder Representations from Transformers (BERT) model to extract theunderlying features of the text, learns a representation of the context of the text throughBi-directional Long Short-Term Memory (BILSTM), and incorporates the multi-headattention (MHATT) mechanism to extract chapter-level features. In this approach, theMHATT mechanism aims to improve the recognition accuracy of abbreviations toefficiently deal with the problem of inconsistency in full-textlabels. Moreover,conditional random field (CRF) is used to label sequence tags because this probabilisticmethod does not need strict independence assumptions and can accommodate arbitrarycontext information. The experimental evaluation on a publicly-available dataset showsthatinthe proposed hybrid approach achieves the best recognition performance;substantially improves performance in recognizing abbreviations,particular,polysemes, and low-frequency entities, compared with the state-of-the-art approaches.For instance, compared with the recognition accuracies for low-frequency entitiesproduced by the BILSTM-CRF algorithm, those produced by the hybrid approach ontwo entity datasets (MULTIPLE and IDENTIFIER) have been increased by 80% and21.69%, respectively.itKeywords: Named entity recognition; Deep learning; Bi-directional Long Short-TermMemory (BILSTM); Conditionalrandom field (CRF); Bidirectional EncoderRepresentations from Transformers (BERT); Multi-head attention (MHATT)2\f1. Introductionartificialintelligence, particularly deep learning,In recent years, artificial intelligence has helped increase the interactions amongtheoretical chemistry, computational chemistry, and synthetic chemistry. Deep neuralnetworks have been recently used to analyze the rationality of chemical synthesis andfind a large number of reverse synthetic routes in a short amount of time [1]. Thesecomputational tools make reaction analysis faster than manual approaches and allowefficient predictions of reactions of possible reagent combinations. There is no doubtthatrevolutionizing ourunderstanding of chemistry [2, 3].Despite the advances,there are still severalimportant scientific activities and processes for the extraction of information that aredone manually, taking plenty of experts’ time. A number of these activities could beefficiently managed using Natural Language Processing (NLP) and other artificialintelligence tools [4, 5, 6]. As an example, the need for an intelligent tool that canautomatically extract materials and chemical entities from the literature in thechemistry-related fields, is vital and urgent. The necessity of such tools provides themotivation to explore the Named Entity Recognition (NER) technology [7].in the field of chemical drugs,isNER refers to the identification of entities that have a specific meaning in the text,including names, names of places, and proper nouns [8, 9]. It was first presented as aconcept and motivated as a research area at the Message Understanding Conference(MUC-6) in 1995 [10]. NLP, as an important tool in information extraction, helps theprocess of subsequent relationship extraction, event extraction, and disambiguation. Todeal with NER, several methods including methods based on rules and dictionaries,statistical methods, and hybrid approaches have been used. Methods based on rules anddictionaries usually perform better when the task of recognition is in a specific corpus,such as the Dl-cotrain algorithm using decision tables proposed by Kwak et. al. [11].However, rule-based methods are limited in their ability of effectively carrying outrecognition tasks, since they rely on many complex rules. Moreover,they arecontext-sensitive, and require expert knowledge in specific fields and substantial effortsin maintaining rules and dictionaries. To overcome these drawbacks,the HiddenMarkov Hodel (HMM),the ConditionalRandom Field (CRF), and classical machine learning methods such as the supportvector machines [12] have gradually replaced the aforementioned traditional methods.the maximum entropy model (MaxEnt),The NER approaches based on machine learning models, consist of tasks that canbe divided into pure artificial features, supervised tasks, semi-supervised tasks, andunsupervised tasks. Although traditional statistical methods based on artificial featureshave shown improvements in the field of data security, they suffer from deficienciessuch as being computationally expensive, requiring large overheads for recognition, orpoor generalizability. Supervised machine learning methods, such as the HMM namedentity classifier,is based on a word similarity smoothing technique [13], and ischaracterized by high recognition accuracy. However, training data for these models aremanually labeled, which tends to be a laborious task. Moreover, they need large trainingdata, and this can be particularly problematic when the available data are scarce.3\fSimilarly, semi-supervised and unsupervised techniques need very large amounts oftraining data. However, they have been very effective when dealing with low-frequencydata. For example, Julian Brooke presented an NER system targeted specifically atfiction uses unlabeled data to obtain results [14], however, the approach clearly takesadvantage ofrule-basedsegmentation, for instance, depends on reliable capitalization of names, which is oftennot present in social media, or in most non-European languages. What is more, thisapproach cannot be successfully applied in cases where texts are relatively short.specific properties ofliterature. The initial(English)With advances in deep learning and the large availability of computationalresources, methods based on deep learning have demonstrated obvious advantages overtraditional methods in NLP, and they have gradually become mainstream in the field ofNER. They not only address some of the shortcomings of supervised machine learningmethods (too computationally-intensive and time-consuming), but they also alleviateissues such as the lack of generalizability and large recognition workload commonlyseen in machine learning methods based on artificial features.NEC Labs America pioneered the idea of using deep learning for NLP [15].Collobert et al. utilized embedding and multi-layer one-dimensional convolutionstructures for four typical NLP problems such as part-of-speech tagging. They used acombination of Convolutional Neural Networks(CNN) and CRF to achievegroundbreaking results in the optimization of the CONLL2003 corpus in the field ofuniversal named entity recognition [16]. Subsequently, with the advent oflongshort-term memory (LSTM) and Bidirectional Long Short-Term Memory (BILSTM)based on Recurrent Neural Network (RNN), the performance for NLP has graduallyimproved. The BILSTM-CRF model designed achieved an approximate F-value of 89%in the corpus [17]. Furthermore, the F-value of the CNN-LSTM model established byChiu et al. exceeded 90% [18]. In 2016, Ma et al. proposed a method for end-to-endsequence tagging by BILSTM-CNNs-CRF, which led to the F-value of 91.21% for theCONLL2003 corpus [17].Despite deep learning methods obtaining excellent performance in a generic NER,the performance of NER technologies in some specific fields such as chemistry, biology,finance, and so forth is still far from the ideal. Therefore, NER-related tasks in somefields need significantly more attention, as extracting biochemical entity informationfrom text databases or scientific literature can assist interdisciplinary researchers in thefield of biochemistry [19, 20].There are two main difficulties in the identification of named entities in the field ofbiochemistry in comparison with generic NER. First, there has not yet been a unifiednaming methodology. There exist multiple expression methods for the same entity inaddition to complicated and irregular naming issues including differences in Englishthe number and types ofabbreviations, special characters, and so forth. Second,biochemical entities are huge, and they are growing rapidly. At the moment, the numberof new synthetic drugs is increasing exponentially, and new drugs are constantly beingproduced.To deal with the challenges of NER in the field of biochemistry, Leaman et al.developed th",
            {
                "entities": [
                    [
                        94,
                        102,
                        "AUTHOR"
                    ],
                    [
                        1005,
                        1013,
                        "AUTHOR"
                    ],
                    [
                        106,
                        116,
                        "AUTHOR"
                    ],
                    [
                        120,
                        129,
                        "AUTHOR"
                    ],
                    [
                        133,
                        143,
                        "AUTHOR"
                    ],
                    [
                        156,
                        169,
                        "AUTHOR"
                    ],
                    [
                        171,
                        183,
                        "AUTHOR"
                    ],
                    [
                        189,
                        209,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "1 Three practical field normalised alternative indicator formulae for research evaluation1 Mike Thelwall, Statistical Cybermetrics Research Group, University of Wolverhampton, UK. Although altmetrics and other web-based alternative indicators are now commonplace in publishers’ websites, they can be difficult for research evaluators to use because of the time or expense of the data, the need to benchmark in order to assess their values, the high proportion of zeros in some alternative indicators, and the time taken to calculate multiple complex indicators. These problems are addressed here by (a) a field normalisation formula, the Mean Normalised Log-transformed Citation Score (MNLCS) that allows simple confidence limits to be calculated and is similar to a proposal of Lundberg, (b) field normalisation formulae for the proportion of cited articles in a set, the Equalised Mean-based Normalised Proportion Cited (EMNPC) and the Mean-based Normalised Proportion Cited (MNPC), to deal with mostly uncited data sets, (c) a sampling strategy to minimise data collection costs, and (d) free unified software to gather the raw data, implement the sampling strategy, and calculate the indicator formulae and confidence limits. The approach is demonstrated (but not fully tested) by comparing the Scopus citations, Mendeley readers and Wikipedia mentions of research funded by Wellcome, NIH, and MRC in three large fields for 2013-2016. Within the results, statistically significant differences in both citation counts and Mendeley reader counts were found even for sets of articles that were less than six months old. Mendeley reader counts were more precise than Scopus citations for the most recent articles and all three funders could be demonstrated to have an impact in Wikipedia that was significantly above the world average. 1 Introduction Citation analysis is now a standard part of the research evaluation toolkit. Citation-based indicators are relatively straightforward to calculate and are inexpensive compared to peer review. Cost is a key issue for evaluations designed to inform policy decisions because these tend to cover large numbers of publications but may have a restricted budget. For example, reports on government research policy or national research performance can include citation indicators (e.g., Elsevier, 2013; Science-Metrix, 2015), as can programme evaluations by research funders (Dinsmore, Allen, & Dolby, 2014). Although funding programme evaluations can be conducted by aggregating end-of-project reviewer scores (Hamilton, 2011), this does not allow benchmarking against research funded by other sources in the way that citation counts do. The increasing need for such evaluations is driven by a recognition that public research funding must be accountable (Jaffe, 2002) and for charitable organisations to monitor their effectiveness (Hwang & Powell, 2009). The use of citation-based indicators has many limitations. Some well discussed issues, such as the existence of negative citations, systematic failures to cite important influences and field differences (MacRoberts & MacRoberts, 1996; Seglen, 1998; MacRoberts & MacRoberts, 2010), can be expected to average out when using appropriate indicators and comparing large enough collections of articles (van Raan, 1998). Other 1 Thelwall, M. (2017). Three practical field normalised alternative indicator formulae for research evaluation. Journal of Informetrics, 11(1), 128–151. doi: 10.1016/j.joi.2016.12.002 ©2016 This manuscript version is made available under the CC-BY-NCND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/                             \f2 problems are more difficult to deal with, such as language biases within the citation databases used for the raw data (Archambault, Vignola-Gagne, Côté, Larivière, & Gingras, 2006; Li, Qiao, Li, & Jin, 2014). More fundamentally, the ultimate purpose of research, at least from the perspective of many funders, is not to understand the world but to help shape it (Gibbons, Limoges, Nowotny, Schwartzman, Scott, & Trow, 1994). An important limitation of citations is therfore that they do not directly measure the commercial, cultural, social or health impacts of research. This has led to the creation and testing of many alternative types of indicators, such as patent citation counts (Jaffe, Trajtenberg, & Henderson, 1993; Narin, 1994), webometrics/web metrics (Thelwall, & Kousha, 2015a) and altmetrics/social media metrics (Priem, Taraborelli, Groth, & Neylon, 2010; Thelwall, & Kousha, 2015b). These indicators can exploit information created by non-scholars, such as industrial inventors’ patents, and may therefore reflect non-academic types of impacts, such as commercial value. A practical problem with many alternative indicators (i.e., those not based on citation counts) is that there is no simple cheap source for them. It can therefore be time-consuming or expensive for organisations to obtain, say, a complete list of the patent citation counts for all of their articles. This problem is exacerbated if an organisation needs to collect the same indicators for other articles so that they can benchmark their performance against the world average or against other similar organisations. Even if the cost is the same as for citation counts, alternative indicators need to be calculated in addition to, rather than instead of, citation counts (e.g., Dinsmore, Allen, & Dolby, 2014; Thelwall, Kousha, Dinsmore, & Dolby, 2016) and so their costs can outweigh their value. This can make it impractical to calculate a range of alternative indicators to reflect different types of impacts, despite this seeming to be a theoretically desirable strategy. The problem is also exacerbated by alternative indicator data usually being much sparser than citation counts (Kousha & Thelwall, 2008; Thelwall, Haustein, Larivière, & Sugimoto, 2013; Thelwall & Kousha, 2008). For example, in almost all Scopus categories, over 90% of articles have no patent citations (Kousha & Thelwall, in press-b). These low values involved make it more important to use statistical methods to detect whether differences between groups of articles are significant. Finally, the highly skewed nature of citation counts and most alternative indicator data causes problems with simple methods of averaging to create indicators, such as the arithmetic mean, and complicate the task of identifying the statistical significance of differences between groups of articles. This article addresses the above problems and introduces a relatively simple and practical strategy to calculate a set of alternative indicators for a collection of articles in an informative way. The first component of the strategy is the introduction of a new field normalisation formula, the Mean Normalised Log-transformed Citation Score (MNLCS) for benchmarking against the world average. As argued below, this is simpler and more coherent than a previous similar field normalisation approach to deal with skewed indicator data. The second component is the introduction of a second new field normalisation formula, the Equalised Mean-based Normalised Proportion Cited (EMNPC), that targets sparse data, and an alternative, the Mean-based Normalised Proportion Cited (MNPC). The third component is a simple sampling strategy to reduce the amount of data needed for effective field normalisation. The final component integrated software environment for collecting and analysing the data so that evaluators can create their own alternative indicator reports for a range of indicators with relative ease. The methods are illustrated with a comparative evaluation of the impact of the research of three large is a single, \f3 medical funders using three types of data: Scopus citation counts; Mendeley reader counts; and Wikipedia citations. 2 Mean Normalised Log-transformed Citation Score The citation count of an article must be compared to the citation counts of other articles in order to be assessed. The same is true for collections of articles and a simple solution would be to calculate the average number of citations per article for two or more collections so that the values can be compared. This is a flawed approach for the following reasons that have led to the creation of improved methods. Older articles tend to be more cited than younger articles (Wallace, Larivière, & Gingras, 2009) and so it is not fair to compare averages between sets of articles of different ages. Similarly, different fields attract citations at different rates and so comparing averages between sets of articles from different mixes of fields would also be unfair (Schubert & Braun, 1986). One solution would be to segment each collection of articles into separate sets, one for each field and year, and only compare corresponding sets between collections. Although this may give useful fine grained information, it is often impractical because each set may contain too few articles to reveal informative or statistically significant differences. The standard solution to field differences in citation counts is to use a field (and year) normalised indicator. The Mean Normalised Citation Score (MNCS), for example, adjusts each citation count by dividing it by the average for the world in its field and year. After this, the arithmetic mean of the normalised citation counts is the MNCS value (Waltman, van Eck, van Leeuwen, Visser, & van Raan, 2011ab). This can reasonably be compared between different collections of articles or against the world average, which is always exactly 1, as long as all articles are classified in a single field. If some articles are in multiple fields then weighting articles and citations with the reciprocal of the number of fields containing the article ensures that the world average is 1 (Waltman et al., 2011a). A limitation of the MNCS is that the arithmetic mean is inappropriate for citation counts and most alternative indicators because they are hi",
            {
                "entities": [
                    [
                        90,
                        104,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "6102yaM1]EC.sc[1v30300.5061:viXraA Self-Taught Artificial Agent for Multi-PhysicsComputational Model PersonalizationDominik Neumanna,c,∗, Tommaso Mansib, Lucian Itud,e,Bogdan Georgescub, Elham Kayvanpourf, Farbod Sedaghat-Hamedanif,Ali Amrf, Jan Haasf, Hugo Katusf, Benjamin Mederf, Stefan Steidlc,Joachim Horneggerc, Dorin ComaniciubaMedical Imaging Technologies, Siemens Healthcare GmbH, Erlangen, GermanybMedical Imaging Technologies, Siemens Healthcare, Princeton, USAcPattern Recognition Lab, FAU Erlangen-N¨urnberg, Erlangen, GermanydSiemens Corporate Technology, Siemens SRL, Brasov, RomaniaeTransilvania University of Brasov, Brasov, RomaniafDepartment of Internal Medicine III, University Hospital Heidelberg, GermanyAbstractPersonalization is the process of fitting a model to patient data, a criticalstep towards application of multi-physics computational models in clinicalpractice. Designing robust personalization algorithms is often a tedious,time-consuming, model- and data-specific process. We propose to use artifi-cial intelligence concepts to learn this task, inspired by how human expertsmanually perform it. The problem is reformulated in terms of reinforce-ment learning.In an off-line phase, Vito, our self-taught artificial agent,learns a representative decision process model through exploration of theit learns how the model behaves under change ofcomputational model:parameters. The agent then automatically learns an optimal strategy foron-line personalization. The algorithm is model-independent; applying itto a new model requires only adjusting few hyper-parameters of the agentand defining the observations to match. The full knowledge of the modelitself is not required. Vito was tested in a synthetic scenario, showing thatit could learn how to optimize cost functions generically. Then Vito wasapplied to the inverse problem of cardiac electrophysiology and the person-alization of a whole-body circulation model. The obtained results suggestedthat Vito could achieve equivalent, if not better goodness of fit than stan-dard methods, while being more robust (up to 11% higher success rates)and with faster (up to seven times) convergence rate. Our artificial intel-ligence approach could thus make personalization algorithms generalizableand self-adaptable to any patient and any model.∗Corresponding authorEmail address: dominik.neumann@siemens.com (Dominik Neumann)Preprint submitted to Medical Image AnalysisNovember 5, 2018   \fKeywords: Computational Modeling, Model Personalization,Reinforcement Learning, Artificial Intelligence.1. IntroductionComputational modeling attracted significant attention in cardiac re-search over the last decades (Clayton et al., 2011; Frangi et al., 2001; Hunterand Borg, 2003; Kerckhoffs et al., 2008; Krishnamurthy et al., 2013; Kui-jpers et al., 2012; Noble, 2002). It is believed that computational modelscan improve patient stratification and therapy planning. They could be-come the enabling tool for predicting disease course and therapy outcome,ultimately leading to improved clinical management of patients sufferingfrom cardiomyopathies (Kayvanpour et al., 2015). A crucial prerequisite forachieving these goals is precise model personalization: the computationalmodel under consideration needs to be fitted to each patient. However, thehigh complexity of cardiac models and the often noisy and sparse clinicaldata still hinder this task.A wide variety of manual and (semi-)automatic model parameter estima-tion approaches have been explored, including Aguado-Sierra et al. (2010,2011); Augenstein et al. (2005); Chabiniok et al. (2012); Delingette et al.(2012); Itu et al. (2014); Konukoglu et al. (2011); Le Folgoc et al. (2013);Marchesseau et al. (2013); Neumann et al. (2014a,b); Prakosa et al. (2013);Schmid et al. (2006); Seegerer et al. (2015); Sermesant et al. (2009); Wall-man et al. (2014); Wang et al. (2009); Wong et al. (2015); Xi et al. (2013);Zettinig et al. (2014). Most methods aim to iteratively reduce the misfitbetween model output and measurements using optimization algorithms,for instance variational (Delingette et al., 2012) or filtering (Marchesseauet al., 2013) approaches. Applied blindly, those techniques could easily failon unseen data, if not supervised, due to parameter ambiguity, data noiseand local minima (Konukoglu et al., 2011; Neumann et al., 2014a; Wallmanet al., 2014). Therefore, complex algorithms have been designed combiningcascades of optimizers in a very specific way to achieve high levels of robust-ness, even on larger populations, i.e. 10 or more patients (Kayvanpour et al.,2015; Neumann et al., 2014b; Seegerer et al., 2015). However, those methodsare often designed from tedious, trial-and-error-driven manual tuning, theyare model-specific rather than generic, and their generalization to varyingdata quality cannot be guaranteed. On the contrary, if the personalizationtask is assigned to an experienced human, given enough time, he almost al-ways succeeds in manually personalizing a model for any subject (althoughsolution uniqueness is not guaranteed, but this is inherent to the problem).There are several potential reasons why a human expert is often supe-rior to standard automatic methods in terms of personalization accuracyand success rates. First, an expert is likely to have an intuition of the2\fmodel’s behavior from his prior knowledge of the physiology of the modeledorgan. Second, knowledge about model design and assumptions, and modellimitations and implementation details certainly provide useful hints on the“mechanics” of the model. Third, past personalization of other datasetsallows the expert to build up experience. The combination of prior knowl-edge, intuition and experience enables to solve the personalization task moreeffectively, even on unseen data.Inspired by humans and contrary to previous works, we propose to ad-dress the personalization problem from an artificial intelligence (AI) perspec-tive. In particular, we apply reinforcement learning (RL) methods (Suttonand Barto, 1998) developed in the AI community to solve the parameterestimation task for computational physiological models. With its roots incontrol theory on the one hand, and neuroscience theories of learning on theother hand, RL encompasses a set of approaches to make an artificial agentlearn from experience generated by interacting with its environment. Con-trary to standard (supervised) machine learning (Bishop, 2006), where theobjective is to compute a direct mapping from input features to a classifica-tion label or regression output, RL aims to learn how to perform tasks. Thegoal of RL is to compute an optimal problem-solving strategy (agent behav-ior), e.g. a strategy to play the game “tic-tac-toe” successfully. In the AIfield, such a behavior is often represented as a policy, a mapping from states,describing the current “situation” the agent finds itself in (e.g. the currentlocations of all “X” and “O” on the tic-tac-toe grid), to actions, which allowthe agent to interact with the environment (e.g. place “X” on an empty cell)and thus influence that situation. The key underlying principle of RL is thatof reward (Kaelbling et al., 1996), which provides an objective means for theagent to judge the outcome of its actions. In tic-tac-toe, the agent receivesa high, positive reward if the latest action led to a horizontal, vertical ordiagonal row full of “X” marks (winning), and a negative reward (punish-ment) if the latest action would allow the opponent to win in his next move.Based on such rewards, the artificial agent learns an optimal winning policythrough trial-and-error interactions with the environment.RL was first applied to game (e.g. Tesauro, 1994) or simple controltasks. However, the past few years saw tremendous breakthroughs in RL formore complex, real-world problems (e.g. Barreto et al., 2014; Kveton andTheocharous, 2012; Nguyen-Tuong and Peters, 2011). Some noteworthy ex-amples include M¨ulling et al. (2013), where the control entity of a robotarm learned to select appropriate motor primitives to play table tennis, andMnih et al. (2015), where the authors combine RL with deep learning totrain an agent to play 49 Atari games, yielding better performance than anexpert in the majority of them.Motivated by these recent successes and building on our previous work(Neumann et al., 2015), we propose an RL-based personalization approach,henceforth called Vito, with the goal of designing a framework that can, for3\fFigure 1: Overview of Vito: a self-taught artificial agent for computational model person-alization, inspired by how human operators approach the personalization problem.the first time to our knowledge, learn by itself how to estimate model pa-rameters from clinical data while being model-independent. As illustrated inFig. 1, first, like a human expert, Vito assimilates the behavior of the physi-ological model under consideration in an off-line, one-time only, data-drivenexploration phase. From this knowledge, Vito learns the optimal strategyusing RL (Sutton and Barto, 1998). The goal of Vito during the on-linepersonalization phase is then to sequentially choose actions that maximizefuture rewards, and therefore bring Vito to the state representing the solu-tion of the personalization problem. To setup the algorithm, the user needsto define what observations need to be matched, the allowed actions, and asingle hyper-parameter related to the desired granularity of the state-space.Then everything is learned automatically. The algorithm does not dependon the underlying model.Vito was evaluated on three different tasks. First, in a synthetic ex-periment, convergence properties of the algorithm were analyzed. Then,two tasks involving real clinical data were evaluated: the inverse problemof cardiac electrophysiology and the personalization of a lumped-parametermodel of whole-body circulation. The obtained results suggested that Vitocan achieve equivalent (or better) goodness",
            {
                "entities": [
                    [
                        137,
                        151,
                        "AUTHOR"
                    ],
                    [
                        153,
                        164,
                        "AUTHOR"
                    ],
                    [
                        186,
                        203,
                        "AUTHOR"
                    ],
                    [
                        205,
                        230,
                        "AUTHOR"
                    ],
                    [
                        241,
                        250,
                        "AUTHOR"
                    ],
                    [
                        252,
                        263,
                        "AUTHOR"
                    ],
                    [
                        265,
                        280,
                        "AUTHOR"
                    ],
                    [
                        282,
                        296,
                        "AUTHOR"
                    ],
                    [
                        317,
                        333,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "NIH Public AccessAuthor ManuscriptMed Image Anal. Author manuscript; available in PMC 2008 June 1.Published in final edited form as:Med Image Anal. 2007 June ; 11(3): 207–223.Direct Cortical Mapping via Solving Partial Differential Equationson Implicit SurfacesI-NHPAAuthorManuscriptYonggang Shia, Paul M. Thompsona, Ivo Dinova, Stanley Osherb, and Arthur W. Togaa,*aLaboratory of Neuro Imaging, Department of Neurology, UCLA School of Medicine, Los Angeles, CA 90095,USAbMathematics Department, UCLA, Los Angeles, CA 90095, USAI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptAbstractIn this paper, we propose a novel approach for cortical mapping that computes a direct map betweentwo cortical surfaces while satisfying constraints on sulcal landmark curves. By computing the mapdirectly, we can avoid conventional intermediate parameterizations and help simplify the corticalmapping process. The direct map in our method is formulated as the minimizer of a flexiblevariational energy under landmark constraints. The energy can include both a harmonic term to ensuresmoothness of the map and general data terms for the matching of geometric features. Starting froma properly designed initial map, we compute the map iteratively by solving a partial differentialequation (PDE) defined on the source cortical surface. For numerical implementation, a set ofadaptive numerical schemes are developed to extend the technique of solving PDEs on implicitsurfaces such that landmark constraints are enforced. In our experiments, we show the flexibility ofthe direct mapping approach by computing smooth maps following landmark constraints from twodifferent energies. We also quantitatively compare the metric preserving property of the directmapping method with a parametric mapping method on a group of 30 subjects. Finally, wedemonstrate the direct mapping method in the brain mapping applications of atlas construction andvariability analysis.KeywordsBrain mapping; cortex; atlas; direct mapping; harmonic mapping; level-set; PDEs1 IntroductionThe cerebral cortex is a convoluted sheet of gray matter in the brain that contains many distinctareas controlling various neural functions. The size, shape, and relative locations of these areascan be affected profoundly by many normal and pathological processes. The analysis of thecorrelation between such structural changes and the correspondingly affected functions on thecortex is a fundamental problem in brain mapping (Welker, 1990). For such studies, corticalmapping is an important tool that can provide a detailed comparison of correspondingfunctional and anatomical regions on different cortices. A detailed map for a group of corticesforms the foundation for further statistical analyses of associated properties such as gray matter* Laboratory of Neuro Imaging, Department of Neurology, UCLA School of Medicine, Los Angeles, CA 90095, USA Email address:toga@loni.ucla.edu (Arthur W. Toga)..Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customerswe are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resultingproof before it is published in its final citable form. Please note that during the production process errors may be discovered which couldaffect the content, and all legal disclaimers that apply to the journal pertain.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptShi et al.Page 2thickness growth or decay at specific locations on the cortex. Mapping a group of cortices toa cortical atlas also provides a valuable platform for the visualization and analysis ofexperimental data collected from group members.Due to the convoluted nature and variability among different brains, mapping of the corticalsurfaces poses many numerical challenges for classical surface matching algorithms such asthe iterative closest point(ICP) method (Besl and McKay, 1992) and its extension in brainmapping (Wang et al., 2000). Thus, the cortical mapping problem is conventionally solvedthrough an indirect approach as illustrated in Fig. 1. A core step in this indirect approach is theparameterization of the cortical surface that assigns a 2D coordinate to each point on thesurface. Popular parameterization choices include the flat 2D plane and sphere. Considerablework has been done in this area (Schwartz and Merker, 1986; Schwartz et al., 1989; Carmanet al., 1995; Drury et al., 1996; Sereno et al., 1996; Thompson and Toga, 1996; Hurdal et al.,1999; Hurdal and Stephenson, 2004; Angenent et al., 1999; Fischl et al., 1999a; Timsari andLeahy., 2000; Grossman et al., 2002; Gu et al., 2004; Tosun et al., 2004; Tosun and Prince,2005; Ju et al., 2004; Joshi et al., 2004; Wang et al., 2005b; Van Essen, 2005). The work in(Clouchoux et al., 2005) also proposed to construct a spherical coordinate system directly onthe cortical surface. In order to map a cortical surface to a flat plane, artificial cuts have to beintroduced carefully to open the surface (Fischl et al., 1999a). Instead, the mapping of thecortical surface to a sphere maintains the original topology and can be automated completely.Anatomical features from two different cortices however may not be parameterized with thesame coordinates. To establish the final correspondences from the source cortex to the targetcortex, a warping process needs to be applied in the parameterization domain underanatomically meaningful constraints. Thanks to the parameterization step, this warping processcan be computed using algorithms developed in nonlinear image registration (Christensen etal., 1996; Davatzikos et al., 1996; Dupuis et al., 1998; Grenander and Miller, 1998; Toga,1998; Joshi and Miller, 2000; Thompson et al., 2000a,b; Toga and Thompson, 2003a,b;Thompson et al., 2004; Avants and Gee, 2004). In terms of anatomical constraints in thewarping process, one of the most popular choices is to constrain the map to match sulcal andgyral landmarks on both cortical surfaces (Van Essen et al., 1998; Glaunes et al., 2004;Thompson et al., 2000a,b, 2004). It is interesting to point out that sulcal landmarks were alsoused in many nonlinear image registration algorithms (Collins et al., 1998; Cachier et al.,2001; Hellier and Barillot, 2003). One can also apply curvature related geometric propertiesof the cortical surface to guide the mapping procedure (Fischl et al., 1999b; Tosun and Prince,2005). To establish the final cortex to cortex map, the map computed in the warping processcan be pulled back to both the source and target cortical surfaces using the parameterization.In this paper, we propose a novel and PDE-based approach to compute a direct map from thesource to the target cortical surface that follows constraints on sulcal landmark curves. Bycomputing the map directly, we can simplify the whole mapping process and potentially helpreduce numerical artifacts in the intermediate parameterization steps. Our work is built uponthe implicit harmonic mapping method proposed in (Bertalmío et al., 2001; Mémoli et al.,2004a), which computes a map between two surfaces by iteratively solving a PDE derivedfrom the Euler-Lagrange equation of the harmonic energy. A key step in this implicit mappingmethod, which is defined on the source surface, is to represent both the source and targetsurfaces as the zero level set of functions (Osher and Sethian, 1988), which enables thecalculation of intrinsic gradients on the surfaces using well understood numerical schemes onregular Cartesian grids. The work in (Mémoli et al., 2004a), however, mainly concerns withmapping between general manifolds and no landmark constraints are considered, which iscritical for our problem. The direct cortical mapping algorithm we develop here extends thework of (Mémoli et al., 2004a) in several ways. First of all, we develop a general approach toincorporate boundary conditions into implicit mapping methods. To achieve that, we constructMed Image Anal. Author manuscript; available in PMC 2008 June 1.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptShi et al.Page 3a triangular mesh representation of the boundary condition defined on landmark curves. Thismakes the information of the boundary condition easily accessible on the Cartesian grid andleads us to design a set of adaptive numerical schemes to solve the PDE derived from the Euler-Lagrange equation of the harmonic energy. This enables our algorithm to minimize theharmonic energy while respecting the boundary condition. Another important element of ouralgorithm is a novel approach to finding an initial map between two cortical surfaces using anew feature called landmark context we develop here. This provides a reasonably good startingpoint for our iterative algorithm. Besides landmark constraints, we have also extended theharmonic energy with general data terms that are valuable in matching geometric features, suchas the mean curvature, of the source and target cortical surface.Recently an important result from (Mémoli et al., 2006) also considered incorporatingboundary conditions into the direct mapping process. They formulated the boundary conditionas defined on a set of discrete points and proposed to compute the map by minimizing its globalLipschitz constant. In contrast to the implicit approach, this method is not based on solvingPDEs and finds the map using a search strategy with the aim of minimizing the Lipschitzconstant. This is, in principle, a very general method, but sulcal landmarks were not tested in(Mémoli et al., 2006), so its application in cortical mapping still needs to be further studied.In the rest of the paper, we first review the mathematical background of solving PDEs onimplicit surfaces in section 2. We then propose a general variational framework for directmapping in section 3 and extend the technique reviewed in section 2 to incorporate boundaryconditions defined on landmark cu",
            {
                "entities": [
                    [
                        316,
                        326,
                        "AUTHOR"
                    ],
                    [
                        328,
                        342,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "I-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptNIH Public AccessAuthor ManuscriptComput Med Imaging Graph. Author manuscript; available in PMC 2013 October 01.Published in final edited form as:Comput Med Imaging Graph. 2012 October ; 36(7): 542–551. doi:10.1016/j.compmedimag.2012.06.004.Improving the Correction of Eddy Current-Induced Distortion inDiffusion-Weighted Images by Excluding Signals from theCerebral Spinal FluidWei Liua,b, Xiaozheng Liua,b, Guang Yanga, Zhenyu Zhoub, Yongdi Zhouc, Gengying Lia,Marc Dubind, Ravi Bansalb, Bradley. S. Petersonb, and Dongrong Xub,*aKey Laboratory of Brain Functional Genomics, Ministry of Education & Shanghai Key Laboratoryof Brain Functional Genomics, Shanghai Key Laboratory of Magnetic Resonance, East ChinaNormal University, Shanghai, China 200062bMRI Unit, Columbia University Dept of Psychiatry, New York State Psychiatric Institute, NYSPIUnit 74, 1051 Riverside Drive, New York, NY 10032, U.S.AcDepartment of Neurosurgery, Johns Hopkins University, Baltimore, MD 21287, U.S.AdWeill Cornell Medical College, Cornell University, New York, NY 10065, U.S.AAbstractIterative Cross-Correlation (ICC) is the most popularly used schema for correcting eddy current(EC)-induced distortion in diffusion-weighted imaging data, however, it cannot process dataacquired at high b-values. We analyzed the error sources and affecting factors in parameterestimation, and propose an efficient algorithm by expanding the ICC framework with a number oftechniques: (1) Pattern recognition for excluding brain ventricles; (2) ICC with the extractedventricle for parameter initialization; (3) Gradient-based Entropy Correlation Coefficient (GECC)for optimal and finer registration. Experiments demonstrated that our method is robust with highaccuracy and error tolerance, and outperforms other ICC-family algorithms and popularapproaches currently in use.Keywordsdiffusion weighted imaging; diffusion tensor imaging; eddy current; distortion correction; iterativecross-correlation; mutual information1. IntroductionDiffusion Tensor Imaging (DTI) is a powerful imaging technique for the non-invasivecharacterization of the microstructure of normal and pathological tissues. DTI data,however, are vulnerable to geometric distortion caused by eddy current (EC). DTI datatypically are acquired using echo-planar imaging (EPI) pulse sequences. Consequently,rapid switches between the strong gradients of diffusion-sensitizing magnetic fields© 2012 Elsevier Ltd. All rights reserved.*Corresponding author: Dongrong Xu Ph.D., 1051 Riverside Drive, NYSPI Unit 74, New York, NY 10032, Tel. (212)543-5495 Fax.(212)543-0522, dx2103@columbia.edu.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to ourcustomers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review ofthe resulting proof before it is published in its final citable form. Please note that during the production process errors may bediscovered which could affect the content, and all legal disclaimers that apply to the journal pertain.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptLiu et al.Page 2inevitably create residual gradients that produce EC-induced distortion. Often the bandwidthof the phase-encoding direction is routinely much narrower than the other two (readout-encoding and frequency-encoding) directions, making the phase-encoding direction of thediffusion-weighted (DW) image more sensitive to contamination of EC-induced distortionscaused by the DW gradient. Furthermore, because the amplitude and direction of thediffusion gradients determine the magnitude of the EC-induced distortion, DW imaging(DWI) data acquired along different directions of gradient contain different distortion.Misregistration among DWI data along differing gradient directions will produce errors inthe reconstruction of the diffusion tensors in virtually every voxel of an image, andsubsequently in the derived measurements. These include the apparent diffusion coefficient(ADC)[1], fractional anisotropy (FA)[2] or ellipsoidal area ratio (EAR)[3], and theprocedure of fiber tracking. Therefore, EC-induced distortions must be corrected in all DWimages before a diffusion tensor is reconstructed. Many algorithms have been developed forcorrecting EC-induced distortions, and can be classified into three categories. The firstcategory are those algorithms that rely on improving MRI pulse sequences, including thosecompensating ECs by changing the shape of the gradient amplitude envelope [4] and thosedesigning special pulse sequences [5–7]. For example, the bipolar gradient imaging schema[6] minimized EC-induced distortion by using pairs of bipolar gradients, which requires aprolonged diffusion-weighted time and thus results in an obvious signal attenuation due totransverse relaxation. Most investigators, however, do not have easy access to theproprietary computer codes required to program pulse sequences.The second category of algorithms for correcting EC-induced distortions generally operateson DWI data in k-space [8–10]. The processing and computations involved in the k-spacecorrection algorithms, however, are often complex. For example, one representative work[10] proposed first measuring the phase evolution caused by EC fields along a set ofreference DW gradient directions that are parallel to the coordinate axes, using thesemeasurements to estimate the true phase evolution along particular spatial directions otherthan those that are parallel to the coordinate axes, and then using those estimates to removeEC-induced distortions from the k-space data. The third category refers to post-processingmethods that usually coregister the DW images to a reference image. In most cases, thereference image is one set of the baseline images that is in theory acquired without theapplication of a DW gradient and is therefore considered to be free of EC-induceddistortions by DW gradients. The methods in this third category are appealing because theyare generally flexible in permitting the correction of imaging data off-line.Among these various post-processing methods, the Iterative Cross-Correlation (ICC)algorithm [11], is one of the most popularly adopted. It estimates EC-induced distortion inDW images by cross-correlating the DWI data with the reference images along the phase-encoding direction. Unfortunately, ICC is known to correct accurately only DWI data thatare acquired at b-values lower than 300s/mm2 [12]. This is because DW signals derivingfrom the regions of cerebrospinal fluid (CSF) within the images, when measured at higher b-values, are much weaker than those measured in the reference images, thereby underminingthe accuracy of the cross-correlation of those DW images with the reference [11]. A numberof remedies have been proposed to address this limitation of the ICC algorithm [12–15],such as using a fluid attenuated inversion recovery (FLAIR) sequence to suppress signalsfrom CSF [13] or acquiring an additional set of images using gradients of opposite polarityto estimate the EC-induced distortion present in the original dataset [15]. These remedies,however, generally require either the acquisition of additional imaging data or themanipulation of specific pulse sequences, thereby significantly increasing the alreadylengthy scan time. Moreover, the need to acquire additional imaging data means that theserevised ICC methods no longer involve only post-processing procedures that can be at theconvenience of the investigator off-line.Comput Med Imaging Graph. Author manuscript; available in PMC 2013 October 01.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptLiu et al.Page 3Because the primary determinant of the errors in estimating distortion using the ICCalgorithm derives from the difference in image contrast between the reference and DWimages in the regions containing CSF, excluding the signal of the CSF when estimatingdistortion should immediately and dramatically improve the performance of an ICC-basedalgorithm for correcting EC-induced distortion in DWI data. Compared with the currentlyavailable methods for suppressing signals from CSF [13], this purely postprocessingapproach would not require extra pulse sequence or imaging data.We therefore propose to improve the ICC algorithm by excluding the CSF signal from thedistortion estimation. We first segment the brain and coarsely identify the regions of CSF(including both cortical and ventricular CSF) in a binary mask that excludes the CSFregions. This mask is then used within the conventional ICC algorithm to estimate the initialparameters of distortion. As the segmentation step may not have accurately outlined theregions of CSF, we then use Mutual Information (MI) instead of cross-correlation in the costfunction to refine the estimation of distortion for coregistering the reference with the DWimages. We use MI because it has been shown to be a robust and accurate measure ofsimilarity for registration of multimodal images in many studies [16–19]. With this measure,we use a 3-paramter affine transformation and an algorithm called Limited memory BroydenFletcher Goldfarb Shanno with bound constraint (L-BFGS-B) to find the optimal parametersfor correcting distortion (see the Method section)[20]. L-BFGS-B algorithm can performoptimization without computing the Hessian matrix, which is known to be difficult to obtainin numerical calculation [20].2. PreliminaryIterative Cross-CorrelationThe ICC model estimates distortion parameters by calculating the cross-correlation betweenthe DW and reference images along each corresponding column of their reconstructedimages at a higher resolution. The distortion is characterized by three parameters, M, T andS within the plane of an image slice (for convenience we use the notation that the image liesin the XY plane, with X, Y, and Z b",
            {
                "entities": [
                    [
                        456,
                        470,
                        "AUTHOR"
                    ],
                    [
                        474,
                        485,
                        "AUTHOR"
                    ],
                    [
                        487,
                        499,
                        "AUTHOR"
                    ],
                    [
                        501,
                        513,
                        "AUTHOR"
                    ],
                    [
                        515,
                        527,
                        "AUTHOR"
                    ],
                    [
                        541,
                        553,
                        "AUTHOR"
                    ],
                    [
                        582,
                        594,
                        "AUTHOR"
                    ],
                    [
                        2549,
                        2561,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "2202guA9]VI.ssee[1v52840.8022:viXraLongitudinal Prediction of Postnatal Brain MagneticResonance Images via a Metamorphic GenerativeAdversarial NetworkYunzhi Huanga,b, Sahar Ahmadb, Luyi Hanc, Shuai Wangd, Zhengwang Wub,Weili Linb, Gang Lib, Li Wangb, Pew-Thian Yapb,∗aSchool of Automation, Nanjing University of Information Science and Technology,Nanjing 210044, ChinabDepartment of Radiology and Biomedical Research Imaging Center (BRIC), University ofNorth Carolina, Chapel Hill, USAcDepartment of Radiology and Nuclear Medicine, Radboud University Medical Center,Geert Grooteplein 10, 6525 GA, Nijmegen, The NetherlandsdDepartment of Computer Science, Shandong University (Weihai), ChinaAbstractMissing scans are inevitable in longitudinal studies due to either subject dropoutsor failed scans. In this paper, we propose a deep learning framework to pre-dict missing scans from acquired scans, catering to longitudinal infant studies.Prediction of infant brain MRI is challenging owing to the rapid contrast andstructural changes particularly during the first year of life. We introduce atrustworthy metamorphic generative adversarial network (MGAN) for translat-ing infant brain MRI from one time-point to another. MGAN has three keyfeatures: (i) Image translation leveraging spatial and frequency information fordetail-preserving mapping; (ii) Quality-guided learning strategy that focusesattention on challenging regions. (iii) Multi-scale hybrid loss function that im-proves translation of tissue contrast and structural details. Experimental resultsindicate that MGAN outperforms existing GANs by accurately predicting bothcontrast and anatomical details.Keywords:Infant brain MRI, Longitudinal prediction, Metamorphic GAN∗Corresponding authorEmail address: ptyap@med.unc.edu (Pew-Thian Yap)Preprint submitted to Journal of Pattern RecognitionAugust 10, 2022   \f1. IntroductionBrain MRI is commonly used to investigate normative and aberrant brainevolution through infancy [1]. To precisely chart brain growth trajectories,temporally dense longitudinal datasets are often required but are difficult toacquire. Moreover, infant studies often involve incomplete longitudinal datasets,given the unique challenges associated with infant MRI acquisition. The missingdata at different time points can be due to subject dropouts or failed scans owingto excessive motion, insufficient coverage, or imaging artifacts [2].Longitudinal prediction of infant brain scans is challenging as brain MRIcontrasts change rapidly through the first year of life. The brain volume doublesto about 65% of the adult brain by the end of the first year [3]. The gray matter(GM) follows a faster growth trajectory (108% − 149% increase) compared towhite matter (WM; 11% increase) [4]. The rapid brain evolution is characterizedby both structural and contrast variations [5, 6]. As shown in Figure 1, theWM appears to be darker than the GM during the neonatal phase as the brainis going through myelination, and by sixth month, WM and GM are almostindistinguishable due to the poor tissue contrast.1.1. Related WorkThe longitudinal prediction of infant brain MRI can be formulated as animage-to-image translation task — mapping images from a source time point toa target time point [7]. Several studies in the field of computer vision [7, 8, 9, 10]have shown that generative adversarial networks (GANs) [11] yield superiorperformance in translating images from a domain to another.In the fieldof medical image analysis, [12] introduced an auto-context GAN to progres-sively refine MRI-to-CT synthesis. In their follow-up study, [13] incorporateddifficulty-aware attention mechanism to improve predictions in challenging re-gions. Similarly, [14] introduced self-attention to encourage the transformationof a foreground object while retaining the background. Medical image-to-imagetranslation network (MedGAN) [15] uses a pre-trained classification network as2\fFigure 1: Appearance and structural changes at two time points during the first year of life.Wavelet decomposition for capturing structural details.feature extractor to match textures and structural details of synthetic and targetCT images. All the aforementioned methods for cross-modality synthesis focuson appearance changes and neglect morphological changes. The longitudinalprediction of infant MR brain images, however, requires dealing with fast-pacedstructural and appearance changes.To promote structural consistency in cross-modality synthesis, several recentapproaches incorporate segmentation similarity as a learning constraint [16, 17].However, tissue segmentation of infant brain MRI is challenging due to theoverlap of GM and WM intensity distributions (Fig. 1). Several approachesattempted to ensure structural consistency without relying on tissue maps. [18]employed gradient differences in a loss function to improve the prediction ofboundaries.[19] incorporated gradient correlation differences in a structure-consistency loss to improve edge alignment in MRI-to-CT synthesis. Althoughsuccessful, the gradient-based constraint introduces noise and fail to capturesufficient boundary information in images with low contrast. [20] incorporateda patch-based self-similarity loss by comparing each patch with all its neighbors3\fin a pre-defined non-local region to ensure structural consistency. However, thesearch for corresponding non-local regions is computationally expensive.1.2. ContributionsIn this paper, we employ CycleGAN [8], a cycle consistent generation frame-work, to simultaneously learn structural and appearance changes between twotime points. Major contributions of our work are summarized below:(i) We propose a trustworthy adversarial learning metamorphosis frameworkthat accounts for both the appearance and structural changes in infantbrain MRI.(ii) We use a spatial-frequency transfer block equipped with wavelet decom-position to transform features from multiple frequency bands to learn thestructural changes.(iii) We employ a quality guidance strategy to incorporate a quality-driven lossfunction to improve predictions in challenging regions.(iv) We devise a multi-scale hybrid loss function to improve the matching ofboth the textural details and the anatomical edges between the predictedimage and the desired target image. The discriminator network is evokedat multiple resolutions via deep-supervision, thus allowing accurate pre-diction of anatomical structures through adversarial learning.The rest of the paper is organized as follows: Section 2 details the proposedmethod. Section 3.2 describes the dataset used for evaluation and presents theexperimental results. Section 4 provides additional discussion and concludes thepaper.2. MethodsIn this work, we implement a framework for prediction of metamorphicchanges using a GAN. Details of our method are described next.4\f2.1. Network ArchitectureWe propose a metamorphic GAN (MGAN) to predict the infant brain MRimage scanned at time point tb from a time point ta. Without loss of gen-erality, we assume that tb > ta. Our network architecture, shown in Fig. 2,is cycle-consistent and learns a reversible translation between the two time-points.It consists of (i) a forward path for earlier-to-later time-point imageprediction and (ii) a backward path for later-to-earlier time-point image predic-tion. The two generators Ga and Gb and their corresponding discriminators Daand Db follow an encoder-decoder architecture. Both the generators incorporatea spatial-frequency transfer (SFT) block to transform the appearance and struc-tural features via multiple branches detailed in Fig. 3. The two discriminatorsestimate voxel-level uncertainty maps, enabling the corresponding generators tofocus on challenging regions. We will describe the components of our networkin the subsequent sections.2.1.1. Metamorphic GeneratorThe metamorphic generator (Fig. 3) takes a 3D patch of size 64 × 64 × 64as input and predicts a 3D patch. The generator consists of an encoder, SFTblock, and a decoder.Encoder. The encoding path consists of two convolution blocks, each with a3 × 3 × 3 convolution layer, followed by 3D instance normalization (IN) [21] anda rectified linear unit (ReLU) [22]. For downsampling, we use convolution witha stride of 2 instead of pooling to avoid potential information loss. We keep a1-stride convolution in the first stage of the encoder to retain details, and use a2-stride convolution in the second stage. The resulting numbers of feature mapsin the two-stage encoder are 64 and 32.Spatial-frequency transfer block. Longitudinal prediction requires translatingboth contrast and structure between two time points. We propose to embeda spatial-frequency transfer block in between enocder-decoder to extract thespatial and frequency domain information of feature maps. The SFT block is5\fFigure 2: Overview of the metamorphic GAN.divided into two branches: (i) frequency transform branch, and (ii) spatial trans-form branch. The frequency transform branch is equipped with discrete wavelettransform (DWT) that takes into account the low frequency tissue contrast andhigh frequency structural details. The DWT layer decomposes the feature mapinto low frequency approximation and high frequency details along three dimen-sions, resulting in eight subvolumes: LLL, LLH, LHL, LHH, HLL, HLH, HHLand HHH. This decomposition allows more effective transfer of spatial-frequencydetails.6\fFigure 3: Network architecture of the metamorphic generator.Given the i-th channel feature map f i of size (sx × sy × sz), the decomposedj at frequency band j is obtained by convolving f i with waveletfeature map f ifilter wj:j = f i (cid:126) wj.f i(1)The wavelet filters for each frequency band are calculated by DWT decompo-sition and are preset in the convolution layer. Correspondingly, the featuremaps are reconstructed in the decode path via inverse discrete wavelet trans-form (IDWT) layer. We show the representative feature maps from the DWTand IDWT layers i",
            {
                "entities": [
                    [
                        166,
                        178,
                        "AUTHOR"
                    ],
                    [
                        180,
                        189,
                        "AUTHOR"
                    ],
                    [
                        191,
                        202,
                        "AUTHOR"
                    ],
                    [
                        204,
                        217,
                        "AUTHOR"
                    ],
                    [
                        230,
                        238,
                        "AUTHOR"
                    ],
                    [
                        240,
                        248,
                        "AUTHOR"
                    ],
                    [
                        250,
                        264,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Multimodal image fusion via deep generative models Giovanna Maria Dimitri (1,5)∗, Simeon Spasov (1)∗, Andrea Duggento (2), Luca Passamonti (3,6), Pietro Lio’(1), Nicola Toschi (2,4) 1) University of Cambridge, Cambridge, Department of Computer Science and Technology, William Gates Building, 15 J J Thomson Ave, Cambridge, CB3 0FD, UK 2) Department of Biomedicine and Prevention, University of Rome \"Tor Vergata”, Via Montpellier 1, 00133, Roma, RM, Italy 3) Department of Clinical Neurosciences, University of Cambridge, Herschel Smith Building, Forvie Site, Robinson Way, Cambridge Biomedical Campus, Cambridge, CB2 0SZ, Cambridge, UK 4) A.A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital and Harvard Medical School, Boston, USA 5) Dipartimento di Ingegneria dell’Informazione e Scienze Matematiche (DIISM), University of Siena, Italy 6) Istituto di Bioimmagini e Fisiologia Molecolare (IBFM), Consiglio Nazionale delle Ricerche (CNR) Via F.lli Cervi, 93, 20090, Segrate, Milano, Italy. *These two authors contributed equally to this work Corresponding Authors: Simeon Spasov, Giovanna Maria Dimitri            \fAbstract Recently, it is has become progressively more evident that classic diagnostic labels are unable to accurately and reliably describe the complexity and variability of several clinical phenotypes. This is particular true for a broad range of neuropsychiatric illnesses such as depression and anxiety disorders or behavioural phenotypes such as aggression and antisocial personality. Patient heterogeneity can be better described and conceptualized by grouping individuals into novel categories, which are based on an empirically-derived sections of intersecting continua that span both across and beyond traditional categorical borders. In this context, neuroimaging data carry a wealth of spatiotemporally resolved information about each patient’s brain. However, they are usually heavily collapsed a priori through procedures which are not learned as part of model training, and consequently not optimized for the downstream prediction task. This is due to the fact that every individual participant usually comes with multiple whole-brain 3D imaging modalities often accompanied by a deep genotypic and phenotypic characterization, hence posing formidable computational challenges. In this paper we design and validate a deep learning architecture based on generative models rooted in a modular approach and separable convolutional blocks (which result in a 20-fold decrease in parameter utilization) in order to a) fusing multiple 3D neuroimaging modalities on a voxel-wise level, b) efficiently convert them into informative latent embeddings through heavy dimensionality reduction, c) maintaint excellent generalizability and minimal information loss. As proof of concept, we test our architecture on the well characterized Human Connectome Project database (n=974 healthy subjects), demonstrating that our latent embeddings can be clustered into easily separable subject strata which, in turn, map to extremely different phenotypical information (including organic, neuropsychological, personality variables) which was not included in the embedding creation process. The ability to extract meaningful and separable phenotypic information from brain images alone can aid in creating multi-dimensional biomarkers able to chart spatio-temporal trajectories which may correspond to different pathophysiological mechanisms unidentifiable to traditional data anlysis approaches. In turn, this may be of aid in predicting disease evolution as well as drug response, hence supporting mechanistic disease understanding and also empowering clinical trials. Keywords • Deep Autoencoder • Phenotype Stratification • Latenet Embeddings • Precision Medicine • Separable Convolutions • Multimodal Neuroimaging    \f1. Introduction Over the last years, it is has become progressively more evident that classic diagnostic labels are unable to accurately and reliably describe the complexity and variability of several clinical phenotypes. This is particular true for a broad range of neuropsychiatric illnesses such as depression and anxiety disorders or behavioural phenotypes such as aggression and antisocial personality. In neurology and psychiatry, the problem is compounded by the clear lack of boundaries between normal and abnormal behaviour and by the presence of often overlapping diagnostic categories. The need for a trans-diagnostic psychiatry and for a brain-based categorization of the clinical entities that transcend the traditional Diagnostic Standard Manual (DSM) labels has been emphasized several times. Alzheimer’s Disease (AD) and Parkinson’s Disease (PD) are paradigmatic examples. Both diseases manifest themselves with a myriad of symptoms and signs which may or may not occur across different patients, and typically can be linked to extremely heterogeneous timescales and trajectories (i.e. evolutions over time). Accordingly, post-mortem studies have confirmed an extremely high heterogeneity in neuropathological findings not only in AD (Rabinovici et al., 2017) but also in other, classically unrelated, but most probably somehow dimensionally connected neurological syndromes (Adler et al, 2010). To overcome this issue, trans-diagnostic approaches that rely on a more nuanced and mechanistic representation of the demographic and clinical features of each individual have been proposed. More specifically, patient heterogeneity can be better described and conceptualized by grouping individuals into novel categories, which are based on an empirically-derived sections of intersecting continua that span both across and beyond traditional categorical borders. Such novel patient groupings are likely to be disjoint and possibly become independent from the currently employed disease categories. This is because a single patient condition often transcends the traditional taxonomies. In contrast, it is reasonable to visualize and describe each patient as occupying their own, unique position in a high-dimensional space that depends on specific pathophysiological mechanisms. The uniqueness of this position in such a high-dimensional space is likely to translate into new opportunities to optimize the diagnosis and treatment to the individual needs of each patient (‘personalized medicine’). However, the possibility to translate this into the clinical practice can only depend on the possibility to access the multidimensional biomarker space that defines the uniqueness of each patient’s neurocognitive and behavioural “profile”. This conceptual paradigm is often termed precision- and (in some cases) personalized medicine, which lies in stark contrast with the more commonly employed “one size fits all” approach. The latter is still pervasive in most clinical disciplines, although exceptions exist like e.g. in oncology, where some degree of personalization has been clinically successful. Such observations highlight the urgent need for more objective classification criteria and frameworks, which need to be based on measurable and reproducible biomarkers. This can be achieved by employing deep generative models that are able to meaningfully group of individuals (healthy or with clinical disorders) on the basis of key brain structural measures. Neuroimaging data carry a wealth of spatiotemporally resolved information about each patient’s brain. However, they are usually heavily collapsed a priori through procedures which are not learned as part of model training, and consequently not optimized for the downstream prediction task. Still, for a comprehensive data-driven stratification, all relevant pathophysiological mechanisms should be well-represented in the multidimensional data fed into the aggregation and pattern recognition frameworks. In this context, the recent appearance of several large multicentre and multimodal, curated large (between 1000 and 40000 individuals at the time of writing) data repositories (e.g. the Parkinson Progression Marker Initiative (PPMI) (Jennings et al., 2011), the Alzheimer Disease Neuroimaging Initiative (ADNI) (Mueller et al., 2005), the UK Biobank initiative (Sudlow et al., 2015), the Cam-CAN dataset (Taylor et al 2017) and the Human Connectome Project (HCP) (Van Essen et al., 2013) is providing novel formidable opportunities as well as challenges. On one hand, this amount of information which has never previously been accessible to neuroimaging researchers: every individual participant usually comes with multiple whole-brain imaging modalities of 105-106 voxels each, often acquired at multiple timepoints. Importantly, these data are usually accompanied by a deep geno/phenotypic characterization (e.g. genetic, biochemical, biohumoral, and neuropsychological \fmarkers). This opens up avenues to robust cross-modality data fusion, and therefore to subsequent reduction into embeddings that are fine-grained enough to 1) inform mechanistic hypotheses about disease physiology as well as 2) about the neural substrates determining currently unexplainable within-disease variability. On the other hand, the computational and conceptual challenges in designing data reduction architectures able to exploit voxel-wise multimodal 3D imaging data (most deep learning frameworks are designed to learn from 2D images) while retaining realistic computation times and, crucially, extract informative embeddings while reducing data dimensionality by at least a factor 1000, are severe. The aim of this paper is therefore to design and validate a deep learning (DL) architecture based on generative models rooted in a modular approach and separable convolutional blocks. Our goal is to efficiently extract low level informative embeddings while A) fusing multiple 3D neuroimaging modalities on a voxel-wise level B) performing heavy dimensionality reduction with minimal information loss and C) building an architecture able to efficiently reconstruct brain images. Thi",
            {
                "entities": [
                    [
                        50,
                        73,
                        "AUTHOR"
                    ],
                    [
                        1103,
                        1126,
                        "AUTHOR"
                    ],
                    [
                        81,
                        95,
                        "AUTHOR"
                    ],
                    [
                        1088,
                        1102,
                        "AUTHOR"
                    ],
                    [
                        101,
                        117,
                        "AUTHOR"
                    ],
                    [
                        122,
                        138,
                        "AUTHOR"
                    ],
                    [
                        145,
                        156,
                        "AUTHOR"
                    ],
                    [
                        161,
                        175,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "© <2021>. This manuscript version is made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/     The definitive publisher version is available online at https://doi.org/ 10.1016/j.knosys.2021.106994 \fPlease cite as: Zhang, Y., Wu, M., Tian, G. Y., Zhang, G. & Lu, J. 2021, Ethics and privacy of artificial intelligence: Understandings from bibliometrics, Knowledge-based Systems, DOI: 10.1016/j.knosys.2021.106994 Ethics and privacy of artificial intelligence: Understandings from bibliometrics Yi Zhang1, Mengjia Wu1, George Yijun Tian2, Guangquan Zhang1, Jie Lu1 1Australian Artificial Intelligence Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Australia 2Faculty of Law, University of Technology Sydney, Australia Email: yi.zhang@uts.edu.au; mengjia.wu@student.uts.edu.au; yijun.tian@uts.edu.au; guangquan.zhang@uts.edu.au; jie.lu@uts.edu.au ORCID: 0000-0002-7731-0301 (Yi Zhang); 0000-0003-3956-7808 (Mengjia Wu); 0000-0003-4472-5428 (George Yijun Tian); 0000-0003-3960-0583 (Guangquan Zhang); 0000-0003-0690-4732 (Jie Lu). Abstract Artificial intelligence (AI) and its broad applications are disruptively transforming the daily lives of human beings and a discussion of the ethical and privacy issues surrounding AI is a topic of growing interest, not only among academics but also the general public. This review identifies the key entities (i.e., leading research institutions and their affiliated countries/regions, core research journals, and communities) that contribute to the research on the ethical and privacy issues in relation to AI and their intersections using co-occurrence analysis. Topic analyses profile the topical landscape of AI ethics using a topical hierarchical tree and the changing interest of society in AI ethics over time through scientific evolutionary pathways. We also paired 15 selected AI techniques with 17 major ethical issues and identify emerging ethical issues from a core set of the most recent articles published in Nature, Science, and Proceedings of the National Science Academy of the United States. These insights, bridging the knowledge base of AI techniques and ethical issues in the literature, are of interest to the AI community and audiences in science policy, technology management, and public administration. Keywords Artificial intelligence; Ethics; Privacy; Bibliometrics; Topic analysis.  \fHighlights • Articles on AI ethics cover 199 of the 254 Web of Science Categories, indicating a broad interest from the academia. • Research communities of computer science, business and management, medical science and law are playing a leading role on studies of AI ethics. • USA, UK, and China make the major contribution to AI ethics, with a relatively high level of domestic collaborations. • Key AI techniques raise ethical concerns, such as fairness, accountability, data privacy, responsibility, liability, and crimes.  \f1. Introduction A pandora’s box of artificial intelligence (AI) has been opened and these disruptive technologies are transforming the daily lives of human beings in relation to new ways of thinking and behavioral patterns, with enhanced capabilities and efficiency. There are many examples of AI applications in use today, such as smart homes [1] smart farming [2], precision medicine [3] and healthcare surveillance systems [4].The ethical and privacy issues surrounding the use of AI have been a topic of growing interest among diverse communities. For example, the general public has expressed concern about the impact of the increased use of robots on unemployment and inequality [5], social scientists have raised deep privacy concerns related to surveillance systems [6], and limited regulation of social media has raised debate with technical giants on the abuse of private data1. Despite these concerns, the AI community stands behind the efficiency and robustness of their AI models and there is an urgent need to guide the research community to understand these ethical and privacy challenges. Bibliometrics, which is a set of approaches for analyzing scientific documents (e.g., research articles, patents, and academic proposals), has been widely used as a tool for science, technology and innovation studies [7], such as identifying technological topics [8], discovering latent relationships [9], and predicting potential future trends [10]. Recently, AI has received recognition in bibliometrics as an emerging topic for empirical investigation [11, 12]. These investigations either align with the interest in technology management (e.g., using AI as a representative case in digital transformation) or emphasize its role in examining the reliability of the proposed methods. However, from a practical perspective, a bibliometric guide which summarizes ideas, assumptions, and debate in the literature would bring significant benefits to the AI community, not only by highlighting the ethical and privacy concerns raised by the public but also by identifying the potential conflicts between AI techniques and these issues of concern. To address these concerns, this paper reports on a bibliometric study to comprehensively profile the key ethical and privacy issues discussed in the research articles and to trace how such issues have changed over the past few decades. We integrated a set of intelligent bibliometric approaches within a framework for diverse analyses. To identify the key entities, i.e., the leading research institutions and their affiliated countries and regions, and the core research journals and their behind research communities, which report the ethical and privacy issues surrounding AI, we used co-occurrence statistics with diverse bibliographical indicators (e.g., authors, affiliations, and sources). With specific foci in topic analysis, we initially retrieved terms from the combined titles and abstracts of collected articles and used a term clumping process [13] to remove noisy terms and consolidate technical synonyms. In parallel, we represented each word in the combined field with titles and abstracts as a vector using the Word2Vec model [14] and combined the word vectors into term vectors by matching the core terms refined in the term clumping process. We answered the 1 More information can be found on the website: https://www.bbc.com/news/business-49099364 \fquestions as to what is the topical landscape and how have these topics evolved over time, using an approach of scientific evolutionary pathways [15]. We also targeted a core set of articles published in three world-leading multi-disciplinary journals, namely Science, Nature, and Proceedings of the National Academy of Sciences (PNAS) of the United States of America, and identified cutting-edge issues that might either focus attention on emergent ethical and privacy issues in the current AI age or lead to novel developments in AI models to address any potential negative impacts. We anticipate that the empirical insights identified in this study will motivate the AI community to extensively and comprehensively discuss the ethical and privacy issues surrounding AI and will guide the implementation of AI in line with an ethical framework. The rest of this paper is organized as follows: Section 2 presents a review of the related work on AI ethics, privacy, and bibliometrics; Section 3 introduces the data and methodologies used in this study; Section 4 presents the results, and our key findings and Section 5 concludes the study and suggests future research directions. 2. Related work In this section, we review the current debate on the ethical and privacy issues surrounding AI and then briefly introduce the bibliometrics and topic analysis used in this study. 2.1. Ethics, ethical dilemma, and AI ethics In philosophy, ethics describes “what is good for the individual and for society”, as well as the essence of “duties that people owe themselves and one another” [16], while ethical dilemma refers to certain ethical problems can be extremely complicated and the challenges they bring cannot be easily solved. Ever-improving technologies bring along with multiple advantages to human society, but they may also “generate downside risks and challenges, including more complicated ethical dilemma2. This is true with AI technologies. With the rapid growth in AI techniques in recent decades, there has been increasing controversy over the impact of AI on the daily lives of human beings, for example, the potential for robots to replace human labor [17], the accountability and accident risk of driverless vehicles [18], the self-awareness and behavior autonomy of robotics [19], and possible fraud caused by deep-fake videos and photos [20]. Such concerns in relation to the ethics around AI has attracted attention from global federal governments and corporations, in particular, tech giants such as Google and SAP, when those corporations are willing to form national and industrial committee to formulate AI ethics guidelines [21]. An increasing number of international organizations have also started to take actions to address the ethical challenges brought by AI technology. As one of the most recent developments, the United Nations Educational, Scientific and Cultural Organization (UNESCO) has issued its first draft of Recommendation on the Ethics of 2 the United Nations Educational, Scientific and Cultural Organization, Elaboration of a Recommendation on the ethics of artificial intelligence at https://en.unesco.org/artificial-intelligence/ethics  \fArtificial Intelligence (Recommendations) 3 in September 2020, which sets up ten important Principles of the Ethics of AI, including: proportionality and do no hard, safety and security, fairness and non-discrimination, sustainability, privacy, human oversight and determination, transparency and expandability, responsibly and accountability, awareness and literacy, and multi-stakeholder and adaptive governance and collaboratio",
            {
                "entities": [
                    [
                        537,
                        546,
                        "AUTHOR"
                    ],
                    [
                        548,
                        559,
                        "AUTHOR"
                    ],
                    [
                        561,
                        579,
                        "AUTHOR"
                    ],
                    [
                        581,
                        597,
                        "AUTHOR"
                    ],
                    [
                        599,
                        606,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "2202peS82]GL.sc[3v00701.5002:viXraDistance-based Positive and Unlabeled Learning forRankingHayden S. Helm, Amitabh Basu, Avanti Athreya, Youngser Park,Joshua T. Vogelstein, Carey E. Priebe∗Johns Hopkins University100 Whitehead Hall3400 North Charles StreetBaltimore, MD 21218 USA∗corresponding author; email: cep@jhu.eduMichael Winding, Marta Zlatic, Albert CardonaUniversity of CambridgeCambridge CB2 1TN UKPatrick Bourke, Jonathan Larson, Marah Abdin, Piali Choudhury,Weiwei Yang, Christopher W. WhiteMicrosoft ResearchRedmond, WA 98052 USAAbstractLearning to rank – producing a ranked list of items specific to a query and withrespect to a set of supervisory items – is a problem of general interest. Thesetting we consider is one in which no analytic description of what constitutesa good ranking is available.Instead, we have a collection of representationsand supervisory information consisting of a (target item, interesting items set)pair. We demonstrate analytically, in simulation, and in real data examples thatlearning to rank via combining representations using an integer linear programis effective when the supervision is as light as ”these few items are similar to youritem of interest.” While this nomination task is quite general, for specificity wepresent our methodology from the perspective of vertex nomination in graphs.The methodology described herein is model agnostic.Preprint submitted to Pattern RecognitionSeptember 29, 2022   \f1. IntroductionGiven a query, a collection of items, and supervisory information, producinga ranked list relative to the query is of general interest. In particular, learningto rank [1] and algorithms from related problem settings [2] have been used toimprove popular search engines and recommender systems and, impressively,aid in the identification of human traffickers [3].When learning to rank, for each training query researchers typically have ac-cess to (feature vector, ordinal) pairs that are used to learn an ordinal regressorvia fitting a model under a set of probabilistic assumptions [4] or via deep learn-ing techniques [5] that generalize to ranking items for never-before-seen queries.A query is an element of a set of possible queries Q and the items-to-be-rankedare elements of a nomination set N .In this paper we consider the setting in which, for a given query, we knowthe dissimilarity (from multiple perspectives) between it and a set of items tobe ranked. We are also given a set of items known to be similar to the query(positive examples). We make no model assumptions. Our goal is to leveragethe knowledge of the items known to be similar to the query to produce a newdissimilarity tailored to the query. The new dissimilarity, which in this paperis exactly a convex combination of the different dissimilarities, is then usefulfor nominating items unknown to be similar to the query. This approach tolearning a ranking scheme can gainfully be seen as combining representationsto improve inference [6].In the language of graphs: with respect to a specific vertex of interest v∗, andgiven a set S of important vertices and a collection of dissimilarity measures,we solve an integer linear program that weights those dissimilarities so thatthe set of points in S are ranked minimally; we then use that learned weightedrepresentation to rank all other vertices. In short, we infer an entire rankingfrom light supervision in the form of the set S.Our setting is closely related to positive and unlabeled (PU) learning [7] inthat supervision is of the form of positive examples and that unlabeled data2\fare assumed to be neither positive nor negative. The setting herein differs fromcanonical PU learning in two major ways. First, in PU learning the infer-ence task is to label unknown-to-be-positive objects as positive. For this task,standard classification algorithms such as Naive Bayes and Support Vector Ma-chines can be modified to produce a decision function under certain samplingassumptions [8, 9]. Our task, however, is to rank the unlabeled data. Whiletransforming standard classifiers into ranking functions is possible, the typicaltransformation returns a ranking with respect to the positive conditional distri-bution (as opposed to with respect to the query) and so we do not study it here.Second, in canonical PU learning the data consists of feature vectors. Herein weare only given dissimilarities between objects and thus can only use standard PUlearning techniques naively. We think that this change in perspective is requiredto learn a ranking function with respect to the query (again, as opposed to aranking function with respect to the positive conditional distribution). Indeed,the use of PU learning in the context of recommendation systems or rankingproblems is quite new, with Zhou et al. noting “PU learning has not been exten-sively explored on recommender systems ..” [10] and Zhu et al. saying “therehas been no research on combining multiple PU learning algorithms for textranking” [11]. As far as we are aware, our “dissimilarity-based” PU learning forranking is a novel setting and is to PU learning as dissimilarity-based patternrecognition [12] is to classical pattern recognition [13].Related to our work, [14] and the corresponding literature [15] deal with amulti-media system (a system consisting of many subsystems) where the multi-media query v∗∗ is a boolean function of different v∗i a querycorresponding to subsystem i. Each subsystem is assumed to be able produce am with v∗2, ..v∗1, v∗ranked list for its corresponding query using fuzzy logic. In the context of ourdiscussion, it is fruitful to think of these ranked lists as coming from different(marginal of subsystem i) representations of v∗∗. Fagin proposes an optimal al-gorithm for combining these representations under a set of assumptions – mostnotably that the boolean function that combines the subsystem queries is knownand that the subsystems are independent. These two assumptions, in partic-3\fular, imply that supervisory information is not necessary.In our setting wemake no assumptions on the structure of the query nor on the relationship be-tween the representations and hence rely on supervisory information to combinerepresentations.Our work is also related to the set expansion literature [16, 17, 18]. Mostsimilar to the ideas discussed in this paper here is the algorithm SetExpan pro-posed by Shen and Wu et al. [16] where they combine information from a set ofranked lists from different contexts to iteratively add to their set of importantitems. There are three main differences between SetExpan and our approach.The first is that for SetExpan the resulting ranked list is relevant to the entireS ∪ {v∗} set and not just the vertex of interest v∗. Herein we do not assumesymmetry in the relationship between v∗ and the elements of S∗. The secondis that SetExpan uses the elements of S ∪ {v∗} to find a subset of the repre-sentations to use in the final ranked list. We, on the other hand, our approachuses the elements of S as supervision to a learning problem that optimizes alinear combination of the original representations. Lastly, conditioned on theselected subset of representations, SetExpan uses a simple average to combinethe representations. Our approach uses a learned convex combination instead.While our set up is quite general, we study it through the lens of vertexnomination [19].2. Problem Description: Vertex NominationIn the single graph vertex nomination problem [19] we are given a graphG = (V, E) and a single vertex of interest v∗ ∈ V , and the task is to find otherinteresting vertices. The vertex set V can be taken to be V = [n] = {1, . . . , n}(cid:1) is a subset of all possible vertex pairs {i, j} withand the edge set E ⊂ (cid:0)V2i, j ∈ [n]. The objective of vertex nomination is to return a ranked list of thecandidate vertices V \\ {v∗} such that “interesting” vertices – vertices “similar”to v∗ – are ranked high in the nomination list. Note that in vertex nominationthe query set Q is {v∗} and the nomination set N is V \\ {v∗}.4\fVertex nomination is a special case of (typically) unsupervised problemsaddressed by recommender systems [20] where it is assumed that ”[i]nformationrelevant to the task is encoded in both the structure of the graph and the attributeson the edges” [21]. There have been numerous approaches to vertex nominationproposed in recent years [22, 23, 24, 25, 3, 26, 27] with each illustrating successin sometimes adversarial settings.Notably, none of these proposed nomination schemes is universally consis-tent. Recall that a universally consistent decision rule is one where the limitingperformance of the decision rule is Bayes optimal for every possible distributionof the data.In the classification setting, for example, the famed K NearestNeighbor rule [28], with appropriate restrictions on K growing with trainingset size, is in a class of decision rules known to be universally consistent, [13,Chapters 5,6]. In their foundational paper on the theoretical framework of ver-tex nomination, Lyzinski et al. show that there does not exist a universallyconsistent vertex nomination scheme [19]. Their paper complements other the-oretical [29] and empirical [30] results on the limitations of machine learningfor popular unsupervised learning problems on graphs. The successes reportedin [22, 23, 24, 25, 3, 26, 27] were all with respect to some application-specificnotion of similarity/interestingness.In this paper, in contrast to being told what is meant by similarity, weconsider the setting in which, in addition to G and v∗, we are given a set ofvertices S ⊂ (V \\ {v∗}) explicitly known to be similar to v∗ from which we areto learn a ranking scheme specific to the task at hand. In particular, we developa nomination scheme f that takes as input (G, v∗, S) – a graph, a vertex ofinterest, and a set of vertices known to be similar to the vertex of interest –and outputs a function that maps each vertex not equal to v∗ to an ele",
            {
                "entities": [
                    [
                        106,
                        119,
                        "AUTHOR"
                    ],
                    [
                        120,
                        135,
                        "AUTHOR"
                    ],
                    [
                        136,
                        150,
                        "AUTHOR"
                    ],
                    [
                        336,
                        349,
                        "AUTHOR"
                    ],
                    [
                        350,
                        365,
                        "AUTHOR"
                    ],
                    [
                        423,
                        439,
                        "AUTHOR"
                    ],
                    [
                        440,
                        452,
                        "AUTHOR"
                    ],
                    [
                        453,
                        469,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "bioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.Enriching Representation Learning Using 53 Million Patient Notes through Human Phenotype Ontology Embedding Maryam Danialiab, Peter D. Galerbcde, David Lewis‐Smithbcdfg, Shridhar Parthasarathybcd, Edward Kima, Dario D. Salvuccia, Jeffrey M. Millerb, Scott Haagb*, Ingo Helbigbcdh*ⵜ a Department of Computer Science, Drexel University, Philadelphia, PA, USA b Department of Biomedical and Health Informatics (DBHi), Children’s Hospital of Philadelphia, Philadelphia, PA, USA c Division of Neurology, Children’s Hospital of Philadelphia, Philadelphia, PA, USA d The Epilepsy Neuro Genetics Initiative (ENGIN), Children’s Hospital of Philadelphia, Philadelphia, PA, USA e Center for Neuroengineering and Therapeutics, University of Pennsylvania, Philadelphia PA f Translational and Clinical Research Institute, Newcastle University, Newcastle-upon-Tyne, UK g Department of Clinical Neurosciences, Royal Victoria Infirmary, Newcastle-upon-Tyne, UK h Department of Neurology, University of Pennsylvania, Perelman School of Medicine, Philadelphia, PA, USA * Equal contributions ⵜ Corresponding author, helbigi@chop.edu           \fbioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.Abstract The Human Phenotype Ontology (HPO) is a dictionary of more than 15,000 clinical phenotypic terms with defined semantic relationships, developed to standardize their representation for phenotypic analysis. Over the last decade, the HPO has been used to accelerate the implementation of precision medicine into clinical practice. In addition, recent research in representation learning, specifically in graph embedding, has led to notable progress in automated prediction via learned features. Here, we present a novel approach to phenotype representation by incorporating phenotypic frequencies based on 53 million full-text health care notes from more than 1.5 million individuals. We demonstrate the efficacy of our proposed phenotype embedding technique by comparing our work to existing phenotypic similarity-measuring methods. Using phenotype frequencies in our embedding technique, we are able to identify phenotypic similarities that surpass the current computational models. In addition, we show that our embedding technique aligns with domain experts' judgment at a level that exceeds their agreement. We show that our proposed technique efficiently represents complex and multidimensional phenotypes in HPO format, which can then be used as input for various downstream tasks that require deep phenotyping, including patient similarity analyses and disease trajectory prediction. Keywords Human phenotype ontology; Representation learning; Dimension reduction; Electronic health record; Phenotype embedding     \fbioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.1 Introduction Electronic medical records (EMRs) have been implemented in the majority of US hospitals and accumulate clinical data at a massive scale [1]. While initially created for billing purposes [2], EMRs increasingly represent a major source of data in clinical research efforts to improve patient care. However, automated interpretation of EMRs is challenging, particularly for diagnoses that incorporate dynamic and diverse sets of clinical features. Phenotypes, a set of observable characteristics and clinical traits, have an essential role in connecting clinical research and practice. Algorithms that use phenotypes to find similarities and differences between patients play a foundational role in EMR research [3]. However, phenotypic descriptions available in EMRs are often stored in the form of unstructured data and do not allow for direct comparisons. The Human Phenotype Ontology (HPO) is one approach to overcome these limitations (human-phenotype-ontology.org). The HPO is a standardized representation of more than 15,000 clinical phenotypic concepts and their relationships based on expert knowledge. We have contributed to HPO terminology since 2010 [4-7]. The HPO has been widely used for harmonization of clinical features in various studies, including, but not limited to, semantic unification of common and rare diseases [8], genetic discoveries in pediatric epilepsy [9, 10], and delineation of longitudinal phenotypes [11, 12]. In addition, the HPO is commonly used for genomic studies and allows for analyses of clinical data at a scale that is required by current and future initiatives. For example, large national and international initiatives have started to systematically link biorepositories to EMR data, including up to 80,000 cases and 500,000 controls [13-15], highlighting the possibilities for novel biological insight at scale. The HPO can be modeled as a directed acyclic graph (DAG) in a computational system, where each phenotype is presented as a node with a unique identifier and is connected to its parent phenotypes by “is a” relationships in the form of directed edges. This structure guarantees that if a disease or gene is annotated to a phenotypic term, it will also be annotated to all its ancestral terms (higher-level concepts within the larger phenotypic tree). The HPO is regularly updated to incorporate advances in phenotypic conceptualization [16]. Extraction of phenotypic concepts is a crucial step in any automated pipeline to exploit the scale of EMR data in clinical research. Natural Language Processing (NLP) pipelines such as cTAKES [17], ClinPhen [18], and MetaMap [19] are commonly used to derive phenotypic concepts from the EMR, effectively allowing  \fbioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 International license.the transition from unstructured free text to structured representations. While these NLP pipelines share a common goal, that is, extracting phenotypes from clinical free text, they have different components and employ a variety of procedures, thus, should be evaluated from different endpoints such as precision, sensitivity, ease of use, and speed [18, 20]. Furthermore, phenotype extraction typically serves as only a starting point to more complex analyses. Therefore, studies need to perform additional analyses on the extracted phenotypes and their relationships to accomplish tasks like patient comparisons and predicting patient status [21]. Manual analysis of phenotypes is non-scalable, resource-intensive, and virtually impossible for larger cohorts. Accordingly, reliable algorithms for computational phenotype analysis are urgently needed. Comparing phenotypes to one another is a common building block for downstream clinical tasks. Methods for measuring phenotypic similarities, using measures such as the Resnik score [22], information coefficient [23], and graph information content [24], show promise for diagnoses and open doors to novel biological insights such as genetic discoveries [9, 25, 26]. However, these methods are not generally transferable to other tasks and require a significant amount of computation, even with minor changes to the data. As a result, while these methods perform relatively well on limited data with hundreds of patients, they are computationally intensive for large-scale data with millions or even thousands of patients with diverse clinical representations [18, 26]. Representation learning is a group of machine learning algorithms that discovers and learns representations of data, making it easier to extract information that can be used for various tasks such as classification and prediction [27]. Recent work in representation learning has shown success in discovering useful representations without relying on procedural techniques, especially in domains with complex and large data [27]. Embedding algorithms, discussed in Section 2, are a branch of representation learning that model discrete objects as continuous vectors. They offer a compact representation that captures similarities between the original objects and have revolutionized data processing and analysis in many domains, including text processing, by representing words in a compact space [28, 29]. Embedding algorithms have also been extended to encode other data structures such as nodes and graphs [30, 31]. A few studies have applied representation learning and embedding techniques in the health domain and presented promising results on specific phenotypes for a limited number of diseases [32, 33]. Here, we map 53 million full-text health care notes of more than 1.5 million individuals to HPO terms and perform graph embedding to assess the possibilities and limitations of representation learning  \fbioRxiv preprint The copyright holder forthis preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the https://doi.org/10.1101/2022.07.20.500809this version posted July 21, 2022. doi: ; preprint in perpetuity. It is made available under aCC-BY-NC-ND 4.0 Int",
            {
                "entities": [
                    [
                        447,
                        462,
                        "AUTHOR"
                    ],
                    [
                        509,
                        532,
                        "AUTHOR"
                    ],
                    [
                        536,
                        547,
                        "AUTHOR"
                    ],
                    [
                        589,
                        600,
                        "AUTHOR"
                    ],
                    [
                        603,
                        615,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ArticleConnectome-based machine learning models arevulnerable to subtle data manipulationsGraphical abstractAuthorsMatthew Rosenblatt,Raimundo X. Rodriguez,Margaret L. Westwater, ...,R. Todd Constable, Stephanie Noble,Dustin ScheinostCorrespondencematthew.rosenblatt@yale.edu (M.R.),dustin.scheinost@yale.edu (D.S.)In briefImperceptible data manipulations candrastically increase or decreaseperformance in machine learning modelsthat use high-dimensional neuroimagingdata. These manipulations could achievenearly any desired predictionperformance without noticeable changesto the data or any changes in otherdownstream analyses. The feasibility ofdata manipulations highlights thesusceptibility of data sharing andscientific machine learning pipelines tofraudulent behavior.Highlightsd Enhancement attacks falsely improve the performance ofconnectome-based modelsd Adversarial attacks degrade the performance ofconnectome-based modelsd Subtle data manipulations lead to large changes inperformanceRosenblatt et al., 2023, Patterns 4, 100756July 14, 2023 ª 2023 The Author(s).https://doi.org/10.1016/j.patter.2023.100756ll\fPlease cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSArticleConnectome-based machine learning modelsare vulnerable to subtle data manipulationsMatthew Rosenblatt,1,9,* Raimundo X. Rodriguez,2 Margaret L. Westwater,3 Wei Dai,4 Corey Horien,2 Abigail S. Greene,2R. Todd Constable,1,2,3,5 Stephanie Noble,3 and Dustin Scheinost1,2,3,6,7,8,*1Department of Biomedical Engineering, Yale School of Engineering and Applied Science, New Haven, CT 06510, USA2Interdepartmental Neuroscience Program, Yale School of Medicine, New Haven, CT 06510, USA3Department of Radiology & Biomedical Imaging, Yale School of Medicine, New Haven, CT 06510, USA4Department of Biostatistics, Yale School of Public Health, New Haven, CT 06510, USA5Department of Neurosurgery, Yale School of Medicine, New Haven, CT 06510, USA6Department of Statistics & Data Science, Yale University, New Haven, CT 06510, USA7Child Study Center, Yale School of Medicine, New Haven, CT 06510, USA8Wu Tsai Institute, Yale University, New Haven, CT 06510, USA9Lead contact*Correspondence: matthew.rosenblatt@yale.edu (M.R.), dustin.scheinost@yale.edu (D.S.)https://doi.org/10.1016/j.patter.2023.100756THE BIGGER PICTURE In recent years, machine learning models using brain functional connectivity havefurthered our knowledge of brain-behavior relationships. The trustworthiness of these models has notyet been explored, and determining the extent to which data can be manipulated to change the results isa crucial step in understanding their trustworthiness. Here, we showed that only minor manipulations ofthe data could lead to drastically different performance. Although this work focuses on machine learningmodels using brain functional connectivity data, the concepts investigated here apply to any scientificresearch that uses machine learning, especially with high-dimensional data. As machine learning becomesincreasingly popular in many fields of scientific research, data manipulations may become a major obstacleto the integrity of scientific machine learning.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYNeuroimaging-based predictive models continue to improve in performance, yet a widely overlookedaspect of these models is ‘‘trustworthiness,’’ or robustness to data manipulations. High trustworthinessis imperative for researchers to have confidence in their findings and interpretations. In this work, weused functional connectomes to explore how minor data manipulations influence machine learning predic-tions. These manipulations included a method to falsely enhance prediction performance and adversarialnoise attacks designed to degrade performance. Although these data manipulations drastically changedmodel performance, the original and manipulated data were extremely similar (r = 0.99) and did not affectother downstream analysis. Essentially, connectome data could be inconspicuously modified to achieveany desired prediction performance. Overall, our enhancement attacks and evaluation of existingadversarial noise attacks in connectome-based models highlight the need for counter-measures thatimprove the trustworthiness to preserve the integrity of academic research and any potential translationalapplications.INTRODUCTIONHuman neuroimaging studies have increasingly used machinelearning approaches to identify brain-behavior associationsthat generalize to novel samples.1,2 They do so by aggregatingweak yet informative signals occurring throughout the brain.3,4Machine learning models for functional connectomes (‘‘con-nectome-based models’’)5–7 are among the most popularPatterns 4, 100756, July 14, 2023 ª 2023 The Author(s). 1This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).PATTER 100756\fPlease cite this article in press as: Rosenblatt et al., Connectome-based machine learning models are vulnerable to subtle data manipulations, Patterns(2023), https://doi.org/10.1016/j.patter.2023.100756llOPEN ACCESSmethods for establishing brain-behavior relationships, andthey have successfully characterized the neural correlates ofvarious clinically relevant processes,8 including general cogni-tive ability,9 psychiatric disorders,7,10 affective states,11 andabstinence in individuals with substance use disorder.12Recent work has uncovered bias, or lack of fairness acrossgroups, in connectome-based models,13–15 including predic-tion failure in individuals who defy stereotypes.15 Although im-provements in accuracy6 and fairness (i.e., race, age, orgender bias)13–15 of connectome-based models are crucialfor improving the quality of academic studies and the potentialfor clinical translation, accurate and bias-free models are notenough. Connectome-based models should also have hightrustworthiness, which we define as robustness to data ma-nipulations.In other words, the output or performance ofa trustworthy model remains similar despite minor changesto the input (i.e., X data). Without a high degree of trustworthi-ness, researchers may not be able to have confidence in theirfindings and ensuing interpretations, as even minor modifica-tions to the data could dramatically alter results.Although trustworthiness has been explored from variousperspectives in the machine learning literature, including pri-vacy16 and explainability,17 here we examine trustworthinessthrough the lens of robustness to data manipulations.18 A pop-ular form of data manipulation specific to machine learning isadversarial noise (i.e., adversarial attacks), where a pattern(or ‘‘noise’’) deliberately designed to trick a machine learningmodel is added to data to cause misclassification.19,20 Theseattacks have been investigated in various contexts, includingcybersecurity,21,22 image recognition,20,23 and medical imagingor recordings.24–26 For neuroimaging, adversarial attacks maybecome problematic in the more distant future (e.g., in clinicalapplications25,27).A more immediate concern is the potential for data manipula-tions to falsely enhance prediction performance in researchstudies. Although the majority of scientific researchers seek toperform ethical research, data manipulations are more commonthan one might expect.28–33 For example, an analysis by Bik et al.showed that about 2% of biology papers contained a figure withevidence of intentional data manipulation.31 Furthermore, 2%of scientists admitted to fabrication/falsification, and 14%admitted to seeing their colleagues fabricate/falsify in a survey.32As data manipulation can result in wasted grant money andfuture research endeavors, determining themisdirection ofextent to which the prediction performance of connectome-based models can be falsely enhanced or diminished via datamanipulations is crucial.In this work, we investigated the trustworthiness of connec-tome-based predictive models. Specifically, we introduce thefor connectome-based‘‘performance enhancement attack’’models, where data are injected with small,inconspicuouspatterns to falsely improve the prediction performance ofa specific phenotype. We also explore the effectiveness of ad-versarial noise attacks on connectome-based models. Whereasadversarial noise attacks manipulate only the test data to changea particular prediction, enhancement attacks modify the entiredataset (i.e., training and test data) to falsely improve perfor-mance. In both cases—enhancement attacks and adversarialnoise attacks—we find that subtle manipulations drastically2 Patterns 4, 100756, July 14, 2023Articlechange predictions in four large datasets. Overall, our findingsdemonstrate that currentimplementations of connectome-based models are highly susceptible to data manipulations,which points toward the need for preventive measures built into study designs and data sharing practices.RESULTSFunctional MRI data were obtained from the Adolescent BrainCognitive Development (ABCD) study,34 the Human Connec-tome Project (HCP),35 the Philadelphia NeurodevelopmentalCohort (PNC),36 and the Southwest University LongitudinalImaging Multimodal (SLIM) study.37 The first three datasets(ABCD, HCP, and PNC) were used to demonstrate enhance-ment and adversarial attacks for prediction of IQ and self-re-ported sex. SLIM was introduced to demonstrate enhance-ment with a clinically relevant measure (state anxiety). Allanalyses were conducted on resting-state data. For SLIM,we downloaded fully preprocessed functional connectomes.For ABCD and PNC, raw data were registered to commonspace as previously described.38,39 For HCP, we startedwith the minimally preprocessed data.40 Next, standard, iden-tical preprocessing steps were performed across all datasetsusing BioImage Suite41 (see experi",
            {
                "entities": [
                    [
                        1500,
                        1508,
                        "AUTHOR"
                    ],
                    [
                        1510,
                        1523,
                        "AUTHOR"
                    ],
                    [
                        201,
                        217,
                        "AUTHOR"
                    ],
                    [
                        1570,
                        1586,
                        "AUTHOR"
                    ],
                    [
                        1592,
                        1609,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "1Deep Learning for Neuroimaging-based Diagnosisand Rehabilitation of Autism Spectrum Disorder:A ReviewMarjane Khodatars, Afshin Shoeibi, Delaram Sadeghi, Navid Ghassemi, Mahboobeh Jafari, Parisa Moridian,Ali Khadem, Roohallah Alizadehsani, Assef Zare, Yinan Kong, Abbas Khosravi, Saeid Nahavandi,Sadiq Hussain, U. Rajendra Acharya, Michael Berk1202voN1]GL.sc[4v58210.7002:viXraAbstract—Accurate diagnosis of Autism Spectrum Disorder(ASD) followed by effective rehabilitation is essential for the man-agement of this disorder. Artificial intelligence (AI) techniquescan aid physicians to apply automatic diagnosis and rehabili-tation procedures. AI techniques comprise traditional machinelearning (ML) approaches and deep learning (DL) techniques.Conventional ML methods employ various feature extractionand classification techniques, but in DL, the process of featureextraction and classification is accomplished intelligently andintegrally. DL methods for diagnosis of ASD have been focused onneuroimaging-based approaches. Neuroimaging techniques arenon-invasive disease markers potentially useful for ASD diagno-sis. Structural and functional neuroimaging techniques providephysicians substantial information about the structure (anatomyand structural connectivity) and function (activity and functionalconnectivity) of the brain. Due to the intricate structure andfunction of the brain, proposing optimum procedures for ASDdiagnosis with neuroimaging data without exploiting powerfulAI techniques like DL may be challenging. In this paper, studiesconducted with the aid of DL networks to distinguish ASD areinvestigated. Rehabilitation tools provided for supporting ASDpatients utilizing DL networks are also assessed. Finally, we willM. Khodatars and D. Sadeghi are with the Dept. of Medical Engineering,Mashhad Branch, Islamic Azad University, Mashhad, Iran.A. Shoeibi and N. Ghassemi are with the Faculty of Electrical En-gineering, FPGA Lab, K. N. Toosi University of Technology, Tehran,Iran, and the Computer Engineering Department, Ferdowsi University ofMashhad, Mashhad, Iran. (Corresponding author: Afshin Shoeibi, email:afshin.shoeibi@gmail.com).M. Jafari is with Electrical and Computer Engineering Faculty, SemnanUniversity, Semnan, Iran.P. Moridian is with the Faculty of Engineering, Science and ResearchBranch, Islamic Azad University, Tehran, Iran.A. Khadem is with the Faculty of Electrical Engineering, K. N. ToosiUniversity of Technology, Tehran, Iran. (Corresponding author: Ali Khadem,email: alikhadem@kntu.ac.ir).R. Alizadehsani, A. Khosravi and S. Nahavandi. are with the Institutefor Intelligent Systems Research and Innovation (IISRI), Deakin University,Victoria 3217, Australia.A. Zare is with Faculty of Electrical Engineering, Gonabad Branch, IslamicAzad University, Gonabad, Iran.Y. Kong is with the School of Engineering, Macquarie University, Sydney,2109, Australia.S. Hussain is with the Dibrugarh University, Assam, India, 786004.U. R. Acharya is with the Dept. of Electronics and Computer Engineering,Ngee Ann Polytechnic, Singapore 599489, Singapore, the Dept. of BiomedicalInformatics and Medical Engineering, Asia University, Taichung, Taiwan, andthe Dept. of Biomedical Engineering, School of Science and Technology,Singapore University of Social Sciences, Singapore.M. Berk is with the Deakin University, IMPACT - the Institute forMental and Physical Health and Clinical Translation, School of Medicine,Barwon Health, Geelong, Australia, and the Orygen, The National Centreof Excellence in Youth Mental Health, Centre for Youth Mental Health,Florey Institute for Neuroscience and Mental Health and the Department ofPsychiatry, The University of Melbourne, Melbourne, Australia.present important challenges in the automated detection andrehabilitation of ASD and propose some future works.Index Terms—Autism Spectrum Disorder, Diagnosis, Rehabil-itation, Deep Learning, Neuroimaging, Neuroscience.I. INTRODUCTIONA SD is a disorder of the nervous system that affectsthe brain and results in difficulties in speech, socialinteraction and communication deficits, repetitive behaviors,and delays in motor abilities [1]. This disease can generallybe distinguished with extant diagnostic protocols from theage of three years onwards. Autism influences many partsof the brain. This disorder also involves a genetic influencevia the gene interactions or polymorphisms [2], [3]. One in70 children worldwide is affected by autism. In 2018, theprevalence of ASD was estimated to occur in 168 out of 10,000children in the United States, one of the highest prevalencerates worldwide. Autism is significantly more common in boysthan in girls. In the United States, about 3.63 percent of boysaged 3 to 17 years have autism spectrum disorder, comparedwith approximately 1.25 percent of girls [4].Diagnosing ASD is difficult because there is no pathophys-iological marker, relying instead just on psychological criteria[5]. Psychologicaltools can identify individual behaviors,levels of social interaction, and consequently facilitate earlydiagnosis. Behavioral evaluations embrace various instrumentsand questionnaires to assist the physicians to specify the partic-ular type of delay in a child’s development, including clinicalobservations, medical history, autism diagnosis instructions,and growth and intelligence tests [6].Several investigations for the diagnosis of ASD have re-cently been conducted on neuroimaging data (structural andfunctional).Analyzing anatomy and structural connections of brain areaswith structural neuroimaging is an essential tool for study-ing structural disorders of the brain in ASD. The principaltools for structural brain imaging are magnetic resonanceimaging (MRI) techniques [7], [8], [9]. Cerebral anatomy isinvestigated by structrul MRI (sMRI) images and anatomicalconnections are assesed by diffusion tensor imaging MRI(DTI-MR) [10]. Investigating the activity and functional con-nections of brain areas using functional neuroimaging canalso be used for studying ASD. Brain functional diagnostic   \f2tools are older approaches than structural methods for studyingASD. The most basic modality of functional neuroimaging iselectroencephalography (EEG), which records the electricalactivity of the brain from the scalp with a high temporalresolution (in milliseconds order) [11]. Studies have shownthat employing EEG signals to diagnose ASD have been useful[12], [13], [14]. Functional MRI (fMRI) is one of the mostpromising imaging modalities in functional brain disorders,used as task-based (T-fMRI) or resting-state (rs-fMRI) [15],[16]. fMRI-based techniques have a high spatial resolution (inthe order of millimeters) but a low temporal resolution dueto slow response of the hemodynamic system of the brain aswell as fMRI imaging time constraints and is not ideal forrecording the fast dynamics of brain activities. In addition,these techniques have a high sensitivity to motion artifacts.It should be stressed that in consonance with studies, threeless prevalent modalities of electrocorticography (ECoG) [17],functional near-infrared spectroscopy (fNIRS) [18], and Mag-netoencephalography (MEG) [19] can also attain reasonableperformance in ASD diagnosis. An appropriate approach is toutilize machine-learning techniques alongside functional andstructural data to collaborate with physicians in the processof accurately assessing ASD. In the field of ASD, applyingmachine learning methods generally entail two categories oftraditional methods [20] and DL methods [21]. As opposedto traditional methods, much less work has been done on DLmethods to explore ASD or design rehabilitation tools.This study reviews ASD assesment methods and patients’rehabilitation with DL networks. The outline of this paper isas follows. Section 2 is search strategy. Section 3 conciselypresents the DL networks employed in the field of ASD. Insection 4, existing computer-aided diagnosis systems (CADS)that use brain functional and structural data are reviewed.In section 5, DL-based rehabilitation tools for supportingASD patients are introduced. Section 6 discusses the reviewedpapers. Section 7 reveals the challenges of ASD diagnosisand rehabilitation with DL. Finally, the paper concludes andsuggests future work in section 8.II. SEARCH STRATEGYIn this review, IEEE Xplore, ScienceDirect, SpringerLink,ACM, as well as other conferences or journals were used toacquire papers on ASD diagnosis and rehabilitation using DLmethods. Further, the keywords ”ASD”, ”Autism SpectrumDisorder” and ”Deep Learning” were used to select the papers.The papers are analyzed till June 03th, 2020 by the authors(AK, SN). Figure 1 depicts the number of considered papersusing DL methods for the automated detection and rehabilita-tion of ASD each year.III. DEEP LEARNING TECHNIQUES FOR ASD DIAGNOSISAND REHABILITATIONNowadays, DL algorithms are used in many areas ofmedicine including structural and functional neuroimaging.The application of DL in neural imaging ranges from brainMR image segmentation [22], to detection of brain lesionssuch as tumors [23], diagnosis of brain functional disordersFig. 1: Number of papers published every year for ASDdiagnosis and rehabilitation.Fig. 2: Illustration of various types of DL methods.such as ASD [24], and production of artificial structural orfunctional brain images [25]. Machine learning techniquesare categorized into three fundamental categories of learning:supervised learning [26], unsupervised learning [27], andreinforcement learning [28], and a variety of DL networks areprovided for each type. So far, most studies applied to identifyASD using DL have been based on supervised or unsupervisedapproaches. Figure 2 illustrates generally employed types ofDL networks with supervised or unsupervised learning tostudy ASD.IV. CADS-BASED DEEP LEARNING TECHNIQUES FOR ASDDIAGNOSIS BY NEUROIMAGING DATAA traditional artificial intelligence (AI)-based CADS en-compasses several stages of data acquisition, data pr",
            {
                "entities": [
                    [
                        120,
                        135,
                        "AUTHOR"
                    ],
                    [
                        2110,
                        2125,
                        "AUTHOR"
                    ],
                    [
                        136,
                        152,
                        "AUTHOR"
                    ],
                    [
                        169,
                        186,
                        "AUTHOR"
                    ],
                    [
                        187,
                        203,
                        "AUTHOR"
                    ],
                    [
                        2497,
                        2508,
                        "AUTHOR"
                    ],
                    [
                        215,
                        238,
                        "AUTHOR"
                    ],
                    [
                        239,
                        250,
                        "AUTHOR"
                    ],
                    [
                        251,
                        262,
                        "AUTHOR"
                    ],
                    [
                        263,
                        278,
                        "AUTHOR"
                    ],
                    [
                        279,
                        295,
                        "AUTHOR"
                    ],
                    [
                        331,
                        344,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "llOPEN ACCESSReviewModern views of machine learningfor precision psychiatryZhe Sage Chen,1,2,3,4,* Prathamesh (Param) Kulkarni,5 Isaac R. Galatzer-Levy,1,6 Benedetta Bigio,1 Carla Nasca,1,3and Yu Zhang7,8,*1Department of Psychiatry, New York University Grossman School of Medicine, New York, NY 10016, USA2Department of Neuroscience and Physiology, New York University Grossman School of Medicine, New York, NY 10016, USA3The Neuroscience Institute, New York University Grossman School of Medicine, New York, NY 10016, USA4Department of Biomedical Engineering, New York University Tandon School of Engineering, Brooklyn, NY 11201, USA5Headspace Health, San Francisco, CA 94102, USA6Meta Reality Lab, New York, NY, USA7Department of Bioengineering, Lehigh University, Bethlehem, PA 18015, USA8Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA 18015, USA*Correspondence: zhe.chen@nyulangone.org (Z.S.C.), yuzi20@lehigh.edu (Y.Z.)https://doi.org/10.1016/j.patter.2022.100602THE BIGGER PICTURE Mental health issues are an epidemic in the United States and the world and haveimposed a tremendous burden to the healthcare system and society. To date, there is still a lack of bio-markers and individualized treatment guidelines for mental illnesses. In recent years, machine learning(ML) and artificial intelligence (AI) have become increasingly popular in analyzing complex patterns of neuraland behavioral data for psychiatry. We provide a comprehensive review of ML methodologies and applica-tions in precision psychiatry. We argue that advances in ML-powered modern technologies will create a para-digm shift in the current practice in diagnosis, prognosis, monitoring, and treatment of mental illnesses. Wediscuss conceptual and practical challenges in precision psychiatry and highlight future research opportu-nities in ML.SUMMARYIn light of the National Institute of Mental Health (NIMH)’s Research Domain Criteria (RDoC), the advent offunctional neuroimaging, novel technologies and methods provide new opportunities to develop preciseand personalized prognosis and diagnosis of mental disorders. Machine learning (ML) and artificial intelli-gence (AI) technologies are playing an increasingly critical role in the new era of precision psychiatry.Combining ML/AI with neuromodulation technologies can potentially provide explainable solutions in clinicalpractice and effective therapeutic treatment. Advanced wearable and mobile technologies also call for thenew role of ML/AI for digital phenotyping in mobile mental health. In this review, we provide a comprehensivereview of ML methodologies and applications by combining neuroimaging, neuromodulation, and advancedmobile technologies in psychiatry practice. We further review the role of ML in molecular phenotyping andcross-species biomarker identification in precision psychiatry. We also discuss explainable AI (XAI) and neu-romodulation in a closed human-in-the-loop manner and highlight the ML potential in multi-media informa-tion extraction and multi-modal data fusion. Finally, we discuss conceptual and practical challenges in pre-cision psychiatry and highlight ML opportunities in future research.INTRODUCTIONMental health issues are an epidemic in the United States and theworld. According to the NationalInstitute of Mental Health(NIMH), nearly one in five American adults suffer from a form ofmental(www.nimh.nih.gov/health/statistics/). According to the Centers for Disease Controlillness or psychiatric disorderand Prevention (CDC), the COVID-19 pandemic has witnessed asignificant impact on our lifestyle and considerably elevatedadverse mental health conditions caused by fear, worry, and un-certainty.1 Increased suicide rates, opioid abuse, and anti-depressant usage have been observed in both adults and teen-agers. The diagnosis and treatment of mental health hasimposed a burden to the healthcare system and society. In thePatterns 3, November 11, 2022 ª 2022 The Author(s). 1This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\fllOPEN ACCESSABCFigure 1. ML research in mental health and categorization ofneuroimaging(A) The number of PubMed publications with keywords ‘‘machine learning orAI’’ and ‘‘psychiatry or mental health’’ in the title or abstract (years 2000–2021).(B) Growth of mental health tech funding in the US market (years 2017–2021;data source: https://www.cbinsights.com).(C) Human neuroimaging at various spatial and temporal resolution (copyrightIEEE; figure reproduced from Thukral et al.13 with permission).United States alone, the economic burden of depression alone isestimated to be at least $210 billion annually.2 Precision medi-cine (or personalized medicine) is an innovative approach totailoring disease prevention, diagnosis, and treatment that ac-counts for the differences in subjects’ genes, environments,and lifestyles. The goal of precision medicine is to target timelyand accurate diagnosis/prognosis/therapeutics for the individu-alized patient’s health problem and to further provide feedbackinformation to patients and surrogate decision-makers. Recentdecades have witnessed various degrees of successes in preci-sion medicine, especially in oncology3. Traditional diagnoses ofmental illnesses rely on physical exams, lab tests, and psycho-logical and behavioral evaluations. Meanwhile, precision psychi-atry has increasingly received its deserved attention.4,5 Althoughpsychiatry has not yet benefited fully from the advanced diag-nostic and therapeutic technologies that have an impact on otherclinical specialties, these technologies have the potential totransform the future psychiatric landscape.The NIMH’s Research Domain Criteria (RDoC) initiative aims toillness and provide aaddress the heterogeneity of mentalbiology-based (as opposed to symptom-based) framework forunderstanding these mental illnesses in terms of varying degrees2 Patterns 3, November 11, 2022Reviewof dysfunction in psychological or neurobiological systems; it at-tempts to bridge the power of multi-disciplinary (such as thegenetics, neuroscience, and behavioral science) research ap-proaches.6,7 The current gold standard for diagnosis and treat-ment outcome in mental disorders—the Diagnostic and Statisti-cal Manual of Mental Disorders (DSM), maintained by theAmerican Psychiatric Association (APA)—is often based on theclinician’s observations, behavioral symptoms, and patient re-porting, which are all susceptible to a high degree of variability.Therefore, it is imperative to develop quantitative neurobiologicalmarkers for mental disorders while accounting for their hetero-geneity and comorbidity.One important goal in neuropsychiatry research is to identifythe relationship between neurobiological/neurophysiologicalfindings and clinical behavioral/self-report observations. Ma-chine learning (ML) and artificial intelligence (AI) have generatedgrowing interests in psychiatry because of their strong predictivepower and generalization ability for prognosis and diagnosis ap-plications.8–10 The interest of applying ML/AI in psychiatry hasgrown steadily in the past two decades, as reflected in the num-ber of PubMed publications (Figure 1A). To improve mentalhealth outcomes with digital technologies, the so-called ‘‘digitalpsychiatry’’ focuses on developing ML/AI methods for assess-ing, diagnosing, and treating mental health issues.11 A recentglobal survey has indicated that psychiatrists were somewhatskeptical that AI could replace human empathy, but many pre-dicted that ‘‘man and machine’’ would increasingly collaboratein undertaking clinical decisions, and psychiatrists were opti-mistic that AI might improve efficiencies and access to mentalcare and reduce costs.12The past two decades have witnessed substantial growth ofML applications for psychiatry in the literature, reflected inmany applications and reviews.17–27 Although multiple reviewsof ML for psychiatry are available, the majority of reviews arerestricted to relatively narrow scopes. In this paper, we try to pro-vide a comprehensive review of ML and ML-powered technolo-gies in mental health applications. Our view is ‘‘modern’’ in thesense that the development of new technologies, consumermarket demand, and public health crises (such as COVID-19)have constantly redefined the role of ML and reshaped ourthinking in precision psychiatry. Specifically, we will coverstate-of-the-art methodological developments in ML, multi-modal neuroimaging, large-scale circuit modeling, neuromodu-lation, and human-machine interface. Due to space limitations,our reviewed literature is by no means exhaustive. To distinguishour review from others, we will focus on several issues central tothe ML applications for psychiatry: generalizability, interpret-ability, causality, and clinical and behavioral integration.Our view about this emerging field is cautiously optimistic forseveral reasons. First, with an increasing amount of data andcomputational power, there is a growing demand for psychia-trists to use ML to reevaluate clinical, behavioral, and neuroi-maging data. The interests in mental health funding from the in-dustry have also grown substantially (Figure 1B). Second, it isbecoming increasingly important to leverage the power of MLand develop explainable AI (XAI) tools for unbiased risk diag-nosis, personalized medicine recommendation, and preciseneurostimulation. The integration of ML with neuroimaging canpotentially help us identify and validate biomarkers in diagnosis\fReviewand treatment of mental illnesses. Third, there is a growing de-mand for psychiatrists in the United States, and the shortage iseven more acute in poorer countries.28 ML/AI technologiesmay change the practice of psychiatry for both clinicians and pa-tients. Finally, advanced technologies such as social media,multi-media, and mobile and wearable devices also call for thedevelopment of ML/AI tools to assist the assessment, diagno",
            {
                "entities": [
                    [
                        155,
                        171,
                        "AUTHOR"
                    ],
                    [
                        173,
                        185,
                        "AUTHOR"
                    ],
                    [
                        192,
                        201,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Modeling of Facial Aging and Kinship: A Survey1Markos Georgopoulos1, Yannis Panagakis1, 2, and Maja Pantic11Dept. of Computing, Imperial College London, UK2Dept. of Computer Science, Middlesex University London, UK(cid:70)8102ceD1]VC.sc[2v63640.2081:viXraAbstract—Computational facial models that capture properties of facialcues related to aging and kinship increasingly attract the attention of theresearch community, enabling the development of reliable methods forage progression, age estimation, age-invariant facial characterization,and kinship verification from visual data. In this paper, we review recentadvances in modeling of facial aging and kinship. In particular, weprovide an up-to date, complete list of available annotated datasets andan in-depth analysis of geometric, hand-crafted, and learned facial rep-resentations that are used for facial aging and kinship characterization.Moreover, evaluation protocols and metrics are reviewed and notableexperimental results for each surveyed task are analyzed. This surveyallows us to identify challenges and discuss future research directionsfor the development of robust facial models in real-world conditions.1 INTRODUCTIONHumans possess explicit, cue-based, and often culturallydetermined systems for perceiving the facial appearance oftheir peers [250]. Facial appearance is a primary source of in-formation regarding the person’s identity, gender, ethnicity,affective state, head pose, age and kinship relations. Hence,the perception of facial attributes governs person percep-tion, interpersonal attraction, and consequently prosocialand social behaviour [5], [170].Human face has been thoroughly studied from different,but complementary, perspectives across several disciplinessuch as neuroscience e.g., [70], psychology e.g., [23], [157],sociology e.g., [61], anthropometry e.g, [66], medicine e.g.,[51] and computer science. From a computational point ofview, in particular, advances in computational face mod-eling enabled the development of reliable methods forautomatic detection of faces [249], recognition of identity[259], [107], [2], gender [159], and ethnicity [72]; detectionof salient facial features [223], [58], estimation of head pose[50] and analysis of facial expressions [190], [169], [251] fromvisual data. Notably, recently proposed methods match oreven achieve better accuracy than humans in several taskse.g., [97]. This progress herald a surge of novel applica-tions in communication, entertainment, cosmetology, andbiometrics, to name a few, while facilitating basic researchin social sciences and medicine e.g., [158]. A thorough listof machine learning and computer vision methods solvingthe aforementioned face modeling and analysis tasks canbe found in the comprehensive survey papers [249], [259],[107], [159], [72], [223], [50], [190], [251].Research towards the development of more detailedcomputational facial models that capture properties of fa-cial cues related to aging and kinship increasingly attractsthe attention of the community. Indeed, by capitalizing onrecent advances in machine learning, computer vision, andthe massive collections of facial data available, significantprogress has been made towards addressing the followingproblems:i Age Progression: that is, the process of transforming afacial visual input, in order to model it across differ-ent ages. The change of the age can be bidirectional,so that the facial output can appear either youngeror older than the input.ii Age Estimation: refers to the process of labelling afacial signal with an age or age group. The input sig-nal can be 2D, 3D or image sequences. The problemsthat fall into this category can be divided furtherinto two subcategories, depending on the labels ofthe training data: (a) real age or (b) apparent ageestimation, which refers to the age that is inferredby humans based on the individual’s appearance.iii Age-Invariant Facial Characterization:involves theprocess of building a signal representation that isinvariant to the facial transformations and appear-ance changes caused by aging.iv Kinship Verification: is defined as the process of de-termining whether the individuals in a pair of facialvisual inputs are blood related.Early models for facial age progression and estimationdate back to 1994-95 [118], [43], while the problem of facerecognition across ages was first investigated in 2000 [121].More recently, since 2010, methods for kinship verificationhave emerged [65]. Since then, the development of 1) ro-bust and computationally efficient models (e.g., AAMs [40],CLMs [42] etc) and descriptors (e.g., SIFT [144], HoGs [44],LPPs [99], SURF [12], DAISY[215] etc) of facial appearance,2) effective machine learning methods such as Boosting [71]and Support Vector Machines [220] and 3) manually anno-tated facial datasets e.g., MORPH2 (2006), FG-NET (2004),FERET (1998), YGA (2008), Gallagher’s Images of Groups(2009), Ni’s Web-Collected Images (2009), have facilitatedthe deployment of reliable computational models for facialaging and kinship. Models, methods, and data for facialaging modeling which have been published before 2010   \fare thoroughly surveyed in [179], [73], while an overviewof research efforts for facial kinship modeling is currentlymissing.This paper aims to provide an up-to-date literature sur-vey of the work done 1) towards the development of facialaging models, complementing previous studies in [73], [179]in several ways and 2) in the emerging topic of facial kinshipmodeling. Concretely, the aims of this survey are organizedas follows:• A complete catalogue of publicly available datasetswith manual annotations for facial age and kinshipmodeling tasks is listed in Section 3. We put partic-ular emphasis on data collected in naturalistic, real-world (in the wild) conditions by providing reviewsfor 9 recently collected datasets for age modeling andall the available (i.e., 16) collections of facial imagesfor kinship verification.• A comprehensive review of recent as well as seminalmethodologies for age progression, age estimation,facial characterization, and kinshipage-invariantverification is provided in Sections 4-7. In particular,we provide an in-depth analysis of both geometric,hand-crafted and learned facial representations forthe aforementioned tasks and discuss the type ofinformation they encode as well as their advantagesand limitations. We further elaborate on methodsthat rely on deep discriminative (e.g., CNNs [125])and generative (e.g., GANs [85]) models and appearto be highly effective. Moreover, we review evalu-ation protocols and metrics, and analyze the mostnotable experimental results for each surveyed task.• The review of data, computational methods for facialaging and kinship reveal useful practices as well aschallenges that are yet to be solved. These along withdrawn conclusions are discussed in Section 9.2a lifelike image of a person. In some cases, forensics expertsface the need to change the age of a face. Such cases includeupdating archive images of wanted criminals as well asimages of lost children. Additionally, cases such as matchingorphaned or lost children and finding the kin of a victim, toname a few, demand the verification of kin relationship oftwo people. To that end, automatic genealogical research cansignificantly aid the work of law enforcement agencies.Medicine and Cosmetology: Being able to model aging andkinship and simulating the transformations on the face isvital for modern medicine and cosmetology. Medical homesystems that are used to monitor elderly people can aidmedical diagnosis by detecting premature aging. On theother hand, automatic rejuvenation of the face can serveas a guide for cosmetic surgery. Particularly in the case ofchildren, the parents’ craniofacial aging patterns can be usedto predict the child’s head growth, so that injury-relatedcosmetic surgery can have optimal long-term results.Commercial use: The ever-growing usage of social mediaand availability of personal photos have led to the rapidintegration of facial analysis by businesses. Automaticallyestimating the customers’ age can help with efficient cus-tomer profiling and age-oriented decision making, e.g., age-oriented advertisements. Likewise, targeted ads can be moreeffective when taking kinship into considerations, as peo-ple’s preferences can be affected by their relatives.Entertainment: Visual effects that age or rejuvenate theactors are already being used in the film making industry.These effects are not limited to movies but are also widelyapplied to photo editing. The imminent integration of suchtools into popular design software will make for morerealistic retouche of photos. Make-up artists that specializein transforming the face can leverage the construction ofperson, age and kin specific morphable models. Guided bythose models, the artists will transform the face of the actorfor roles that demand sibling-like similarity between actors.To begin with, a number of modern applications of com-putational models for facial aging and kinship are discussedin the following section.3 DATASETS2 APPLICATIONSIn this section we present the most significant applicationsof modeling facial aging as well as kinship in biometrics,forensics, medicine, cosmetology, business and entertain-ment.Biometrics: The physical, physiological or behaviouralcues based on which a person is recognized, e.g.,iris,fingerprint, face are referred to as biometrics [108]. Ageand kinship comprise soft biometrics [109], [45], [164] asthey can be used to boost the effectiveness of recognition.Besides improving face recognition accuracy there is a needfor robustness towards aging and kinship. Passport checksdemand age-invariance in case of large age gap betweenthe passport image and the person in question. Similarly,kinship invariance can potentially boost automatic facerecognition, in particular towards distinguishing betweenkin that look alike.Forensics: Forensics include a set of scientific techni",
            {
                "entities": [
                    [
                        68,
                        85,
                        "AUTHOR"
                    ],
                    [
                        94,
                        106,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Unsupervised visual feature learning withspike-timing-dependent plasticity: How far are we fromtraditional feature learning approaches?Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne, PierreBouletTo cite this version:Pierre Falez, Pierre Tirilly, Ioan Marius Bilasco, Philippe Devienne, Pierre Boulet. Unsupervisedvisual feature learning with spike-timing-dependent plasticity: How far are we from traditional featurelearning approaches?. Pattern Recognition, 2019, 93, pp.418-429.￿10.1016/j.patcog.2019.04.016￿.￿hal-02146284￿HAL Id: hal-02146284https://hal.science/hal-02146284Submitted on 22 Oct 2021HAL is a multi-disciplinary open accessarchive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come fromteaching and research institutions in France orabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL, estdestinée au dépôt et à la diffusion de documentsscientifiques de niveau recherche, publiés ou non,émanant des établissements d’enseignement et derecherche français ou étrangers, des laboratoirespublics ou privés.Distributed under a Creative Commons Attribution - NonCommercial| 4.0 InternationalLicense\fVersion of Record: https://www.sciencedirect.com/science/article/pii/S0031320319301621Manuscript_e9bbab9223355c7c3fec8d7737d3ac25Unsupervised Visual Feature Learning withSpike-timing-dependent Plasticity: How Far are wefrom Traditional Feature Learning Approaches?Pierre Faleza, Pierre Tirillyb,∗, Ioan Marius Bilascoa, Philippe Deviennea,Pierre BouletaaUniv. Lille, CNRS, Centrale Lille, UMR 9189 - CRIStAL - Centre de Recherche enInformatique Signal et Automatique de Lille, F-59000 Lille, FrancebUniv. Lille, CNRS, Centrale Lille, IMT Lille Douai, UMR 9189 - CRIStAL - Centre deRecherche en Informatique Signal et Automatique de Lille, F-59000 Lille, FranceAbstractSpiking neural networks (SNNs) equipped with latency coding and spike-timingdependent plasticity rules offer an alternative to solve the data and energy bot-tlenecks of standard computer vision approaches: they can learn visual featureswithout supervision and can be implemented by ultra-low power hardware ar-chitectures. However, their performance in image classification has never beenevaluated on recent image datasets. In this paper, we compare SNNs to auto-encoders on three visual recognition datasets, and extend the use of SNNs tocolor images. The analysis of the results helps us identify some bottlenecks ofSNNs: the limits of on-center/off-center coding, especially for color images, andthe ineffectiveness of current inhibition mechanisms. These issues should beaddressed to build effective SNNs for image recognition.Keywords:feature learning, unsupervised learning, spiking neural networks,spike-timing dependent plasticity, auto-encoders, image recognition.1. IntroductionMachine learning algorithms require good data representations to be effec-tive [1]. Good data representations can capture underlying correlations of the∗Corresponding authorPreprint submitted to Pattern RecognitionMarch 30, 2019© 2019 published by Elsevier. This manuscript is made available under the CC BY NC user licensehttps://creativecommons.org/licenses/by-nc/4.0/\fdata, provide invariance properties and help disentangle the data to make itlinearly separable.In computer vision, much effort has been put historicallyinto engineering the right visual features for recognizing, organizing and inter-preting visual contents [2; 3]. More recently, and especially since the rise of deeplearning, those features tend to be learned by algorithms rather than designedby human effort. Learned features have shown their superiority on a numberof tasks, such as image classification [4], image segmentation [5], and actionrecognition [6]. Although effective, feature learning has two major drawbacks:• it is data-consuming, as supervised learning algorithms – especially deeplearning ones – require large amounts of annotated data to be trained;• it is energy-consuming, as training large models, e.g., using gradient descent-based algorithms, has a high computational cost, which increases with theamount of training data. These algorithms are usually run on dedicatedhardware (typically GPU) that are power-intensive.The first issue – data consumption – can be mitigated by the use of unsu-pervised learning models. Unsupervised representation learning is recognized asone of the major challenges in machine learning [1] and is receiving a growinginterest in computer vision applications [7; 8; 9]. A number of unsupervisedmodels have been developed through the years, notably auto-encoders [10] andrestricted Boltzmann machines (RBMs) [11], and their multi-layer counterparts,stacked auto-encoders [12] and deep belief networks (DBN) [13]. Other lines ofwork include sparse coding [14] and the use of semi- or weakly supervised learn-ing algorithms [15]. Moreover, in the case of neural networks, initializing a deepneural network with features learned without supervision before training canyield better generalization capabilities than purely supervised training [12].The second issue – energy consumption – is addressed much less frequently inthe literature, but several authors acknowledge its importance [16; 17; 18] whichis bound to grow more and more as machine learning becomes overwhelminglypresent in a large range of applications: marketing, medicine, finance, education,administration, etc. Most hardware vendors have proposed dedicated machine2\flearning processor architectures (based on GPU, FPGA, etc.)recently [19].These hardware improvements help reduce the energy consumption by a smallfactor (typically one order of magnitude). Reducing further the energy con-sumption of learning algorithms requires to define new learning models andassociated ultra-low power architectures [18; 20; 21]. One promising model isspiking neural networks (SNNs). In this model, artificial neurons communicateinformation through spikes, as natural neurons do. Initially studied in neuro-science as a model of the brain, SNNs receive constant attention in the fieldsof machine learning and pattern recognition, from both the theoretical [22] andthe applicative [17; 23; 24; 25] perspectives. Dedicated hardware implementingthis model can be very energy-efficient [20]. SNNs have already shown theirability to provide near-state-of-the-art results in image classification, but onlywhen they are trained by transferring parameters from pre-trained deep neuralnetworks [21] or by variants of back-propagation [26]. In terms of energy effi-ciency, the first option is not viable as it still requires to train a standard deepneural network, which is exactly what should be avoided; the second option isnot suited either as back-propagation is a global, centralized algorithm – the er-ror must be propagated from the output to all units –, whereas the efficiency ofSNNs lies in their ability to perform highly decentralized, parallel processing onsparse spike data. The alternative is to use bio-inspired learning rules, such asHebbian rules. Among those, rules based on spike-timing dependent plasticity(STDP) [27] have shown promising results for learning visual features; how-ever, they have only been evaluated on datasets with limited challenges (rigidobjects, limited number of object instances, uncluttered backgrounds. . . ) suchas MNIST, 3D-object, ETH-80 or NORB [28; 29; 30; 31; 32], or on two-classdatasets [31; 32]. How they perform on more complex image datasets, what isthe performance gap between them and standard approaches, and what needsto be done to bridge this gap is yet to be established.Aims and scope. In this paper, we evaluate the ability of SNNs equipped withlatency coding and STDP to learn features for visual recognition on three stan-3\fdard datasets (CIFAR-10, CIFAR-100, and STL-10). Our goal is to identifysome of the factors that prevent STDP-based SNNs to reach state-of-the-artresults on actual computer vision tasks. First, we compare the performanceof SNNs on grayscale and color images (Section 5.4), then we compare themto one standard unsupervised feature learning algorithm, sparse auto-encoders(Section 5.5). The resulting models are analyzed with respect to different fac-tors (Section 6): input pre-processing, feature sparsity, feature coherence, andobjective functions.It allows us to identify some bottlenecks that should betackled to bridge the gap from SNNs to state-of-the-art models. In the conclu-sion (Section 7), we suggest some solutions to help address these bottlenecks.In this work, we consider only single-layer architectures because multi-layerSNNs with unsupervised STDP are only very recent and difficult to train, dueto the loss of spiking activity across layers [33; 31]. To our knowledge, this is thefirst work that evaluates features learned by unsupervised STDP-based SNNson recent benchmarks for object recognition and on color images, making onestep towards their use for actual vision applications.2. Unsupervised visual feature learningA visual feature extractor can be modeled as a function f : Rh×w → Rnfthat maps an image or image region of size h × w to a real vector of dimensionnf . It defines a dictionary of features of size nf . In the remaining of the paper,f will denote either the feature extractor function or the resulting dictionary,depending on the context. Early visual feature extractors were handcraftedto capture specific types of visual information (e.g., distributions of edges [3]).Recent approaches rather rely on machine learning to produce features thatbetter fit the data and that can be optimized towards a specific application.A typical learning-based feature extractor can be seen as a function fθ whoseparameters θ are optimized towards a specific goal by a learning algorithm. Thegeneral shape of fθ can be specified explicitly (e.g., a linear transform [34]), orimplicitly, based on the learning alg",
            {
                "entities": [
                    [
                        148,
                        163,
                        "AUTHOR"
                    ],
                    [
                        251,
                        266,
                        "AUTHOR"
                    ],
                    [
                        1526,
                        1541,
                        "AUTHOR"
                    ],
                    [
                        164,
                        184,
                        "AUTHOR"
                    ],
                    [
                        267,
                        287,
                        "AUTHOR"
                    ],
                    [
                        1545,
                        1565,
                        "AUTHOR"
                    ],
                    [
                        185,
                        203,
                        "AUTHOR"
                    ],
                    [
                        288,
                        306,
                        "AUTHOR"
                    ],
                    [
                        1567,
                        1585,
                        "AUTHOR"
                    ],
                    [
                        307,
                        321,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Deep Convolution Network Based Emotion Analysis towards Mental Health Care Zixiang Fei1, Erfu Yang*1, David Day-Uei Li2, Stephen Butler3, Winifred Ijomah1, Xia Li4$, Huiyu Zhou5 1 Department of Design, Manufacture and Engineering Management University of Strathclyde, Glasgow G1 1XJ, UK {zixiang.fei, erfu.yang, w.l.ijomah}@strath.ac.uk 2 Strathclyde Institute of Pharmacy & Biomedical Sciences University of Strathclyde, Glasgow G4 0RE, UK david.li@strath.ac.uk 3 School of Psychological Sciences and Health University of Strathclyde, Glasgow G1 1QE, UK stephen.butler@strath.ac.uk 4 Shanghai Jiaotong University, Shanghai lixia11111@alumni.sjtu.edu.cn 5 Department of Informatics University of Leicester, LE1 7RH, Leicester hz143@leicester.ac.uk * Correspondence author: erfu.yang@strath.ac.uk; Tel.: +44-141-574-5279 $ Correspondence author in China: lixia11111@alumni.sjtu.edu.cn; Tel.: +8621-3477-3440 Abstract: Facial expressions play an important role during communications, allowing information regarding the emotional state of an individual to be conveyed and inferred. Research suggests that automatic facial expression recognition is a promising avenue of enquiry in mental healthcare, as facial expressions can also reflect an individual’s mental state. In order to develop user-friendly, low-cost and effective facial expression analysis systems for mental health care, this paper presents a novel deep convolution network based emotion analysis framework to support mental state detection and diagnosis. The proposed system is able to process facial images and interpret the temporal evolution of emotions through a new solution in which deep features are extracted from the Fully Connected Layer 6 of the AlexNet, with a standard Linear Discriminant Analysis Classifier exploited to obtain the final classification outcome. It is tested against 5 benchmarking databases, including JAFFE, KDEF,CK+, and databases with the images obtained ‘in the wild’ such as FER2013 and AffectNet. Compared with the other state-of-the-art methods, we observe that our method has overall higher accuracy of facial expression recognition. Additionally, when compared to the state-of-the-art deep learning algorithms such as Vgg16, GoogleNet, ResNet and AlexNet, the proposed method demonstrated better efficiency and has less device requirements. The experiments presented in this paper demonstrate that the proposed method outperforms the other methods in terms of accuracy and efficiency which suggests it could act as a smart, low-cost, user-friendly cognitive aid to detect, monitor, and diagnose the mental health of a patient through automatic facial expression analysis. Keywords: Facial Expression Recognition; Deep Convolution Network; Mental Health Care; Emotion Analysis. 1. Introduction Understanding people’s emotions plays an important role in daily human communications. For advanced human-computer interaction in many emerging applications, recognizing users’ emotions is also vital. Currently, there are many approaches for automated emotional recognition, including the recognition of facial expression and analysis of   \fvoice tone [1], which exist alongside more conventional physiological measures such as measurements of blood pressure, pulse rate or skin conductivity. Automated facial expression recognition has found many practical applications such as in e-learning and health care systems [2,3]. For example, facial expressions are considered as an important feedback mechanism for teachers in terms of monitoring levels of students’ understanding [3]. Within this domain, Mau-Tsuen et al. proposed an automatic system to identify how well students were learning, by means of the analysis of videos taken from learners [3], and demonstrated that such a system could help improving teaching effectiveness and efficiency. Additionally, the user interface developed in such a system could work as a cognitive tool [4] to support and understand the users’ mental state better when and where it was appropriate without external actuation. Further applications of facial expression include its use in the fields of human-computer interaction and interface optimization.  Within these domains, Bahr et al. proposed a novel method to analyze postural and facial expressions to guide interface actuation and actions [4], whilst Pedro et al. employed electromyogram (EMG) sensors to investigate the relationship between users’ facial expressions and adverse-event occurrences [5]. Moreover, health and medical applications of automated facial recognition are also being investigated, for example in the area of diagnostics related to developmental disorders such as autism. Here, promising work has been conducted by means of a system of facial expression analysis during social interactions, where many individuals with such disorders find challenging [6]. Modern techniques in facial expression recognition systems play important roles in these practical applications. These techniques typically involve multiple components, including face localization and facial component alignment, facial feature extraction and facial feature classification. As a simple dichotomy, facial expression analysis systems can be divided into two groups: those using static images and those using continuous video frames. Static algorithms enjoy the advantage of facial expression recognition from a single image or a video frame. In this paper, we describe an algorithm which has been tested for facial expression recognition accuracy from single images mainly acquired through webcams and mobile phone cameras. We would argue that static approaches show promising, but require rigorous and various testing conditions for robust outcomes. Deepak et al, for example, proposed a convolution neural network based algorithm to recognize facial expression with high accuracy [7]. However, their algorithm was tested on only two small facial expression datasets.  Conversely, Jie et al. also proposed three novel convolution neural network (CNN) models with different architectures [8], and tested their algorithms on several datasets such as CK+ and FER2013 datasets. Within the three architectures they presented, the data suggested that the pertained CNN with 5 convolution layers had the best performance. It should be noted however that not all researchers restrict their work to 2D images. For instance, Chenlei et al. very recently proposed a new 3D facial expression modeling method based on facial landmarks [9]. On the other hand, other researchers have moved beyond the analysis of static images and proposed facial expression recognition systems using continuous video frames. For instance, Zia, Lee and other researchers worked on facial expression recognition using temporal dynamics [10,11]. Their proposed system used Fisher Independent Component Analysis as a feature extractor and Hidden Markov Models to learn the features of six different expressions. The experiment results showed that the recognition rate was about 92.85%. Despite advances in the recognition accuracy, there remain significant issues to be addressed in the field of facial expression recognition systems.  The first issue is related to the datasets employed in testing such systems.  Some facial expression recognition systems may have good performance in some image datasets, but perform poorly in the others. For instance, deep CNN approaches often need to determine large amount of weights in the training phase. Consequently it is observed that such approaches suffer from performance decrements when being trained on a small image dataset [8]. A second important issue is ecological validity, in that most of the existing systems use lab-posed facial expressions images.  This is potentially problematic, as it ignores real-world problems such as lighting conditions, image quality and background complexity. It is fatal to address such issues for real-world applications. Facial expression recognition in the wild is a challenging topic due to such issue as well as others such as variance in poses [8].  Thirdly, both traditional approaches and deep learning based approaches have inherent weaknesses. Traditional approaches such as Local Binary Pattern (LBP), Scale Invariant Feature Transform (SIFT) and Support Vector Machine (SVM) employ manually designed features, which can result in poor performance with unseen images. Deniz et al, for instance, observed that traditional approaches performed weakly when being presented with variations in pose, and instead employed a deep CNN based approach to recognize facial expressions, which they \fargued resulted in improved performance [12]. Approaches such as AlexNet, Vgg, and GoogleNet have long training and testing time as well as an enormous amount of memory resource[13] and require hardware incorporating Graphics Processing Units (GPU’s), whilst it is also essential in these approaches. Extensive literature provides  a comprehensive review of the relative advantages and weaknesses of the existing facial expression systems, we would recommend those seeking further information to consult one of the several good reviews of the area (see [14–18]). Indeed, even more recently Byoung, reviewed approaches to facial emotion recognition including conventional facial expression recognition, deep learning based facial expression recognition, well-known facial expression datasets and has also examined some common performance evaluation methods for automated facial expression recognition [19].  Whilst there are many notable developments within the field of automatic face recognition systems, there remain urgent priorities to be addressed to allow these technologies to flourish within the field of mental health care. We would argue that there is vast untapped potential for the field in this area of healthcare. Globally, there are rising numbers of people suffering from cognitive impairment, and consequently there is an urgent need to develop and",
            {
                "entities": [
                    [
                        74,
                        86,
                        "AUTHOR"
                    ],
                    [
                        88,
                        98,
                        "AUTHOR"
                    ],
                    [
                        101,
                        118,
                        "AUTHOR"
                    ],
                    [
                        120,
                        135,
                        "AUTHOR"
                    ],
                    [
                        137,
                        153,
                        "AUTHOR"
                    ],
                    [
                        155,
                        162,
                        "AUTHOR"
                    ],
                    [
                        165,
                        176,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fInformation Fusion 64 (2020) 149–187 Contents lists available at ScienceDirect Information Fusion journal homepage: www.elsevier.com/locate/inffus Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation Yu-Dong Zhang a , b , ∗ , Zhengchao Dong c , d , Shui-Hua Wang b , f , g , Xiang Yu a , Xujing Yao a , Qinghua Zhou a , Hua Hu c , e , Min Li c , h , Carmen Jiménez-Mesa i , Javier Ramirez i , Francisco J. Martinez i , Juan Manuel Gorriz i , j a School of Informatics, University of Leicester, Leicester, LE1 7RH, Leicestershire, UK b Department of Information Systems, Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah 21589, Saudi Arabia c Department of Psychiatry, Columbia University, USA d New York State Psychiatric Institute, New York, NY 10032, USA e Department of Neurology, The Second Affiliated Hospital of Soochow University, China f School of Architecture Building and Civil engineering, Loughborough University, Loughborough, LE11 3TU, UK g School of Mathematics and Actuarial Science, University of Leicester, LE1 7RH, UK h School of Internet of Things, Hohai University, Changzhou, China i Department of Signal Theory, Networking and Communications, University of Granada, Granada, Spain j Department of Psychiatry, University of Cambridge, Cambridge CB21TN, UK a r t i c l e i n f o a b s t r a c t Keywords: Multimodal data fusion Neuroimaging Magnetic resonance imaging PET SPECT Fusion rules Assessment Applications Partial volume effect 1. Introduction Multimodal fusion in neuroimaging combines data from multiple imaging modalities to overcome the fundamen- tal limitations of individual modalities. Neuroimaging fusion can achieve higher temporal and spatial resolution, enhance contrast, correct imaging distortions, and bridge physiological and cognitive information. In this study, we analyzed over 450 references from PubMed, Google Scholar, IEEE, ScienceDirect, Web of Science, and var- ious sources published from 1978 to 2020. We provide a review that encompasses (1) an overview of current challenges in multimodal fusion (2) the current medical applications of fusion for specific neurological diseases, (3) strengths and limitations of available imaging modalities, (4) fundamental fusion rules, (5) fusion quality assessment methods, and (6) the applications of fusion for atlas-based segmentation and quantification. Overall, multimodal fusion shows significant benefits in clinical diagnosis and neuroscience research. Widespread edu- cation and further research amongst engineers, researchers and clinicians will benefit the field of multimodal neuroimaging. Neuroimaging has been playing pivotal roles in clinical diagnosis and basic biomedical research in the past decades. As described in the follow- ing section, the most widely used imaging modalities are magnetic res- onance imaging (MRI), computerized tomography (CT), positron emis- sion tomography (PET), and single-photon emission computed tomog- raphy (SPECT). Among them, MRI itself is a non-radioactive, non- invasive, and versatile technique that has derived many unique imaging modalities, such as diffusion-weighted imaging, diffusion tensor imag- ing, susceptibility-weighted imaging, and spectroscopic imaging. PET is also versatile, as it may use different radiotracers to target different molecules or to trace different biologic pathways of the receptors in the body. Therefore, these individual imaging modalities (the use of one imag- ing modality), with their characteristics in signal sources, energy lev- els, spatial resolutions, and temporal resolutions, provide complemen- tary information on anatomical structure, pathophysiology, metabolism, structural connectivity, functional connectivity, etc. Over the past decades, everlasting efforts have been made in developing individual modalities and improving their technical performance. Directions of im- provements include data acquisition and data processing aspects to in- crease spatial and/or temporal resolutions, improve signal-to-noise ratio and contrast to noise ratio, and reduce scan time. On application aspects, ∗ Corresponding author. E-mail addresses: yudongzhang@ieee.org (Y.-D. Zhang), zhengchao.dong@nyspi.columbia.edu (Z. Dong), shuihuawang@ieee.org (S.-H. Wang), xy144@le.ac.uk (X. Yu), xy147@le.ac.uk (X. Yao), qz105@le.ac.uk (Q. Zhou), huhua8775@suda.edu.cn (H. Hu), limin@hhu.edu.cn (M. Li), carmenj@ugr.es (C. Jiménez-Mesa), javierrp@ugr.es (J. Ramirez), fjesusmartinez@ugr.es (F.J. Martinez), gorriz@ugr.es (J.M. Gorriz). https://doi.org/10.1016/j.inffus.2020.07.006 Received 30 April 2020; Received in revised form 6 July 2020; Accepted 14 July 2020 Available online 17 July 2020 1566-2535/© 2020 Elsevier B.V. All rights reserved. \fY.-D. Zhang, Z. Dong and S.-H. Wang et al. Information Fusion 64 (2020) 149–187 the strength and limitations of the neuroimaging modalities and the cor- responding analysis methods, and in particular, the needs for improved image fusion methods and (2) we will review recent methodological development in data preprocessing and data fusion in multimodal neu- roimaging. We note that although we tried to cover all neuroimaging modalities, we inevitably paid more attention to MRI modalities. This is not only due to the most practical application and versatility of the MRI but also due to the limitations of our expertise. Fig. 2 shows the taxonomy of this review. The main contents of the paper are organized as follows. Chapter 2 will give a brief introduction to neuroimaging, and challenges of multi- modal imaging; Chapter 3 introduces the commonly used neuroimaging modalities, which include computerized tomography, positron emission tomography, single-proton emission computed tomography, and mag- netic resonance imaging, which has many modalities in its own right. For each modality, we will concisely describe its signal source, energy level, spatial resolution, temporal resolution, and major applications; Chapter 4 describe applications of neuroimaging in three major areas: the developing brains, the degenerative brains, and mental disorders. In each part, we will first briefly describe what the clinical and/or biomed- ical problems are, we then review recent papers on how neuroimaging has been used to address these problems, and we point out what the unmet needs and challenges; Chapters 5 to 9 are devoted to the multimodal neuroimaging fusion, covering some important procedures in data fusion. The topics are not necessarily complete and their order of presentation is not necessarily coherent with the pipeline of fusion processing. Chapter 5 reviews the fundamental methods, which covers types, rules, atlas-based segmenta- tion, decomposition, reconstruction, and quantification; Chapter 6 re- views subjective and objective assessment of data fusion in multimodal neuroimaging; Chapter 7 reviews the advantages of data fusion in im- proving the spatial/temporal resolution, distortion correction, and con- trast; it also reviews the benefits of these advantages in fusing structural and functional images; Chapter 8 reviews atlas-based segmentations in multimodal imaging fusion; Chapter 9 reviews the quantification in mul- timodal neuroimaging fusion. While the focus of this part is given to PET and SPECT, some of the approaches and principles discussed here, such as partial volume correction and attenuation (relaxation), can be applied to quantitative MRI modalities, such as DTI, ASL, quantitative susceptibility mapping (QSM), etc. Chapter 10 concludes the paper. 2. Multimodal imaging data fusion: challenges in neuroimaging In this part, we will review the current challenges of neuroimag- ing, including limited spatial/temporal resolution, lack of quantifica- tion, and imaging distortions. These challenges often create fundamental limitations on individual modalities of neuroimaging, while some chal- lenges also exist in current multi-modal neuroimaging. This part will mainly cover the challenges of individual neuroimaging modalities that led to the development and ongoing research of multimodal neuroimag- ing methods. 2.1. Individual modality imaging Neuroimaging can be divided into structural imaging and functional imaging according to the imaging mode. Structural imaging is used to show the structure of the brain to aid the diagnosis of some brain dis- eases, such as brain tumors or brain trauma. Functional imaging is used to show how the brain metabolizes while carrying out certain tasks, in- cluding sensory, motor, and cognitive functions. Functional imaging is mainly used in neuroscience and psychological research, but it is grad- ually becoming a new way of clinical-neurological diagnosis [10] . The amount of information obtainable through single-mode imag- ing is limited and often cannot reflect the complex specificity of organ- isms. For instance, although CT imaging is effective in identifying nor- mal structures and abnormal diseased tissues according to their density Fig. 1. Numbers of peer-reviewed papers with the keywords of “neuroimaging ”or “brain imaging ” in titles (the numbers and the bar graph were generated by PubMed in Feb 2020).",
            {
                "entities": [
                    [
                        1019,
                        1033,
                        "AUTHOR"
                    ],
                    [
                        1045,
                        1060,
                        "AUTHOR"
                    ],
                    [
                        1068,
                        1082,
                        "AUTHOR"
                    ],
                    [
                        1094,
                        1103,
                        "AUTHOR"
                    ],
                    [
                        1107,
                        1118,
                        "AUTHOR"
                    ],
                    [
                        1122,
                        1135,
                        "AUTHOR"
                    ],
                    [
                        1139,
                        1146,
                        "AUTHOR"
                    ],
                    [
                        1154,
                        1161,
                        "AUTHOR"
                    ],
                    [
                        1193,
                        1208,
                        "AUTHOR"
                    ],
                    [
                        1238,
                        1257,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "This document is downloaded from DR‑NTU (https://dr.ntu.edu.sg)Nanyang Technological University, Singapore.Behavioral responses of nursing home residentsto a robotic pet dog with a customizableinteractive kitSheba, Jaichandar Kulandaidaasan; Salman, Adnan Ahmed; Kumar, Sampath; Phuc, LeTan; Elara, Mohan Rajesh; Martínez‑García, Edgar2018Sheba, J. K., Salman, A. A., Kumar, S., Phuc, L. T., Elara, M. R., & Martínez‑García, E. (2018).Behavioral Responses of Nursing Home Residents to a Robotic Pet Dog with a CustomizableInteractive Kit. Procedia Computer Science, 133, 409‑416. doi:10.1016/j.procs.2018.07.050https://hdl.handle.net/10356/89268https://doi.org/10.1016/j.procs.2018.07.050© 2018 The Author(s). Published by Elsevier Ltd. This is an open access article under the CCBY‑NC‑ND license(http://creativecommons.org/licenses/by‑nc‑nd/4.0/).Downloaded on 04 Jul 2023 07:44:17 SGT\fAvailable online at www.sciencedirect.comAvailable online at www.sciencedirect.comScienceDirectScienceDirectProcedia Computer Science 00 (2018) 000–000Procedia Computer Science 00 (2018) 000–000Procedia Computer Science 133 (2018) 409–416www.elsevier.com/locate/procediawww.elsevier.com/locate/procediaInternational Conference on Robotics and Smart Manufacturing (RoSMa2018)International Conference on Robotics and Smart Manufacturing (RoSMa2018)Behavioral Responses of Nursing Home Residents to a Robotic PetBehavioral Responses of Nursing Home Residents to a Robotic PetDog with a Customizable Interactive KitDog with a Customizable Interactive KitJaichandar Kulandaidaasan Shebaa*, Adnan Ahmed Salmana, Sampath Kumara, Le Tan Jaichandar Kulandaidaasan Shebaa*, Adnan Ahmed Salmana, Sampath Kumara, Le Tan Phucb, Mohan Rajesh Elarac, Edgar Martínez-GarcíadPhucb, Mohan Rajesh Elarac, Edgar Martínez-GarcíadaSchool of Electrical and Electronics Engineering, Singapore Polytechnic, Singapore aSchool of Electrical and Electronics Engineering, Singapore Polytechnic, Singapore bNanyang Technological University, Singapore bNanyang Technological University, Singapore cEngineering Product Development, Singapore University of Technology and Design cEngineering Product Development, Singapore University of Technology and Design dInstitute of Engineering and Technology, Universidad Autónoma de Ciudad Juárez dInstitute of Engineering and Technology, Universidad Autónoma de Ciudad Juárez Abstract Abstract Robot therapy for the elderly had been a novel advent for the past decade and the efficacy of such therapeutic procedures had Robot therapy for the elderly had been a novel advent for the past decade and the efficacy of such therapeutic procedures had similar benefits to pets in improving health outcomes. But there had been experiments which resulted in showing the loss of similar benefits to pets in improving health outcomes. But there had been experiments which resulted in showing the loss of interest over time due to limited levels of interaction. This paper regards that if a collection of customizable interactive games interest over time due to limited levels of interaction. This paper regards that if a collection of customizable interactive games comes along with the robotic pet dog, the long-term interest will sustain and in effect, long-term psychological benefits would be comes along with the robotic pet dog, the long-term interest will sustain and in effect, long-term psychological benefits would be rendered. Here in this study, we investigate the effect of elderly interacting with pet robot thought multimodal peripheral devices rendered. Here in this study, we investigate the effect of elderly interacting with pet robot thought multimodal peripheral devices with a different level of cognitive challenges using questionnaire, facial temperature, EMG and EEG. with a different level of cognitive challenges using questionnaire, facial temperature, EMG and EEG. © 2018 The Authors. Published by Elsevier Ltd.© 2018 The Authors. Published by Elsevier Ltd. © 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/). This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/). Peer-review under responsibility of the scientific committee of the International Conference on Robotics and Smart Manufacturing.Keywords: human-robot interaction; elderly care; robotic therapy; behavioral assessment Keywords: human-robot interaction; elderly care; robotic therapy; behavioral assessment 1. Introduction1. IntroductionExtensive research has been undertaken around the world towards the use of robot therapy to treat loneliness,Extensive research has been undertaken around the world towards the use of robot therapy to treat loneliness,depression and dementia in elderly patients. The presence of pet robots can reduce stress and improve health depression and dementia in elderly patients. The presence of pet robots can reduce stress and improve health outcomes and robotic animals can be as effective as real animals [1, 2, 3, and 4]. It has been shown that the elderly outcomes and robotic animals can be as effective as real animals [1, 2, 3, and 4]. It has been shown that the elderly with dementia are attracted to robots, raising the promise that appropriately designed robots with an interactive with dementia are attracted to robots, raising the promise that appropriately designed robots with an interactive stimulation features could play an important role in their treatment [5].stimulation features could play an important role in their treatment [5].* Corresponding author. Tel.: +65-64886303; fax: +65-67721974. * Corresponding author. Tel.: +65-64886303; fax: +65-67721974. E-mail address:jai@sp.edu.sg E-mail address:jai@sp.edu.sg 1877-0509© 2018 The Authors. Published by Elsevier Ltd. 1877-0509© 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/). This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/). 1877-0509 © 2018 The Authors. Published by Elsevier Ltd.This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)Peer-review under responsibility of the scientific committee of the International Conference on Robotics and Smart Manufacturing.10.1016/j.procs.2018.07.05010.1016/j.procs.2018.07.0501877-0509ScienceDirectAvailable online at www.sciencedirect.com\f2 410 Jaichandar Kulandaidaasan Sheba et.al / Procedia Computer Science 00 (2018) 000–000 Jaichandar Kulandaidaasan Sheba et al. / Procedia Computer Science 133 (2018) 409–416Abbreviations MAV Mean Absolute value AS SW Time taken in secs for 10 cycles using Dumbbell integrated with ActSense Time taken in secs for 10 cycles using Standard Weight without ActSense There are many research studies done to show how pet robots are helpful during therapy among elderly and how acceptance is measured [6]. Seal like therapeutic robots for instance PARO [7], are been widely used in nursing homes. Several healthcare robots like AIBO a dog like robot, NeCoRo a cat like robot, PARO a seal like robot are developed, however, not all are well accepted due to various expectations from the range of stakeholders [8,9,10,11]. In robot therapy, people feelings has to be stimulated through interactions and interest to engage with pet robots has to be sustained for long term therapies. For pet robots, shapes, feeling of touch (texture), response mimicking animals has to be carefully designed. A single perfect design which will be accepted by the elderly is unlikely. Robots used for cognitive training should engender mental effects, such as comfort, pleasure and relaxation. Actions that manifest themselves during interactions with elderly can be interpreted as if the robots had hearts and feelings [12]. After carefully accessing individual needs and preferences we are proposing a customizable therapeutic pet robot kit. The kit will allow robot’s functionality to be modified or extended based on the individual needs and thereby enable greater acceptance. By using ‘user matching strategy’ robots are matched to human expectation. The quality and quantitative study aimed to access response of elderly people to the dog robot, SNOWY, in an elderly home setting. 2. Experimental Design The study was conducted at Lions home for elderly, Singapore. Residents capable of completing the study by staff were approached and informed about the trial. 12 residents were recruited for the study. The researcher asked residents to make themselves comfortable and rest for ten minutes. The initial facial thermography reading was taken. Residents were briefed about the study and wireless EMG and EEG sensors are connected. The researcher brought SNOWY into the room and turned it on. He placed SNOWY on a table in front of them so they could cuddle it. He explained how to interact with SNOWY using ActSense (an interactive dumbbell), glove and how to play game using reminiscence and music therapy kit. More details of the interactive kit is described in Fig. 1. Fig. 1. Complete set of interactive therapeutic pet robot kit The measurement criteria for the reaction of the subjects were in the form of EEG & EMG data evaluation, face score, a holistic questionnaire and thermal activity levels before and after interaction analysis. The experiment was to be run with 12 elderlies with age ranging from 65-91. Subjects were all females. One set of SNOWY was introduced and the interaction sequence of each device is in the order of the dumbbell, the card game, the glove and the memory game. The interaction time would be 10 minutes per elderly. They were not subjected into the same room together. A different chamber was",
            {
                "entities": [
                    [
                        1615,
                        1647,
                        "AUTHOR"
                    ],
                    [
                        6692,
                        6724,
                        "AUTHOR"
                    ],
                    [
                        6776,
                        6808,
                        "AUTHOR"
                    ],
                    [
                        1571,
                        1590,
                        "AUTHOR"
                    ],
                    [
                        1650,
                        1669,
                        "AUTHOR"
                    ],
                    [
                        1592,
                        1606,
                        "AUTHOR"
                    ],
                    [
                        1671,
                        1685,
                        "AUTHOR"
                    ],
                    [
                        1687,
                        1699,
                        "AUTHOR"
                    ],
                    [
                        1701,
                        1720,
                        "AUTHOR"
                    ],
                    [
                        1751,
                        1770,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Future Generation Computer Systems 144 (2023) 291–306Contents lists available at ScienceDirectFuture Generation Computer Systemsjournal homepage: www.elsevier.com/locate/fgcsDeep learning for understanding multilabel imbalanced Chest X-raydatasetsHelena Liz a,b,∗David Camacho aa Computer Systems Engineering Department, Universidad Politécnica de Madrid, Alan Turing s/n, Madrid, 28031, Spainb Department of Computer Sciences, Universidad Rey Juan Carlos, Tulipán s/n, Móstoles, 28933, Spainc Computer Science Department, Universidad Autónoma de Madrid, Madrid, 28049, Spaind TECNALIA Basque Research & Technology Alliance (BRTA), P. Tecnologico 700, Derio, Bizkaia, 48160, Spaine University of the Basque Country (UPV/EHU), Bilbao, 48013, Spain, Javier Huertas-Tato a, Manuel Sánchez-Montañés c, Javier Del Ser d,e,a r t i c l ei n f oa b s t r a c tArticle history:Received 29 July 2022Received in revised form 28 December 2022Accepted 4 March 2023Available online 6 March 2023Keywords:Convolutional neural networksChest X-raysExplainable AIEnsemble MethodologyOver the last few years, convolutional neural networks (CNNs) have dominated the field of computervision thanks to their ability to extract features and their outstanding performance in classificationproblems, for example in the automatic analysis of X-rays. Unfortunately, these neural networks areconsidered black-box algorithms, i.e. it is impossible to understand how the algorithm has achieved thefinal result. To apply these algorithms in different fields and test how the methodology works, we needto use eXplainable AI techniques. Most of the work in the medical field focuses on binary or multiclassclassification problems. However, in many real-life situations, such as chest X-rays, radiological signsof different diseases can appear at the same time. This gives rise to what is known as \"multilabelclassification problems\". A disadvantage of these tasks is class imbalance, i.e. different labels do nothave the same number of samples. The main contribution of this paper is a Deep Learning methodologyfor imbalanced, multilabel chest X-ray datasets. It establishes a baseline for the currently underutilisedPadChest dataset and a new eXplainable AI technique based on heatmaps. This technique also includesprobabilities and inter-model matching. The results of our system are promising, especially consideringthe number of labels used. Furthermore, the heatmaps match the expected areas, i.e. they mark theareas that an expert would use to make a decision.© 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-NDlicense (http://creativecommons.org/licenses/by-nc-nd/4.0/).1. IntroductionIn recent years, the field of medicine has faced two relevantproblems that hinder patient care: staff workload and subjectiv-ity in the interpretation of tests [1,2]. These problems have noeasy solution, which is especially dangerous in medicine becauseprocedural errors can lead to serious health complications. Firstly,overwork in medicine, aggravated in recent times by the globalCOVID-19 pandemic, can lead to errors and delays in diagnosisand treatment. As mentioned above, there is also subjectivity inthe interpretation of some medical tests. The expert analysingthese tests, for example X-rays, may arrive at an erroneous di-agnosis due to, for example, the existence of signs of differentdiseases to different degrees [3]. This type of imaging test is oneof the most common in various diagnoses due to its low cost,∗Corresponding author at: Computer Systems Engineering Department,Universidad Politécnica de Madrid, Alan Turing s/n, Madrid, 28031, Spain.E-mail addresses: helena.liz@urjc.es (H. Liz), javier.huertas.tato@upm.es(J. Huertas-Tato), manuel.smontanes@uam.es (M. Sánchez-Montañés),javier.delser@tecnalia.com (J. Del Ser), david.camacho@upm.es (D. Camacho).speed of acquisition and the fact that it does not require muchpreparation [4]. Chest X-rays are useful for detecting a variety ofdiseases of the chest related to different organs such as the heart,lungs or bones. The features of X-rays make them suitable foranalysis with convolutional neural networks (CNN) [5]. The com-bination of AI algorithms and medical knowledge can improve theperformance of medical staff [6] and could also reduce patientwaiting times by speeding up the diagnostic process and reducingthe workload of doctors.CNNs have been a breakthrough in computer vision due totheir ability to extract features from images. These architecturesare composed of different layers. The first has convolutional lay-ers that are inspired by the notion of cells in visual neuroscience.The architectures are based on the visual cortex of animals. Themain reason why these architectures have stood out is their greatcapacity to extract patterns from data, improving the perfor-mance of previous systems based on Machine Learning models.This advantage has made them a benchmark in Deep Learningdue to their high performance in a wide range of tasks, such asspeech recognition, computer vision or text analysis [7].https://doi.org/10.1016/j.future.2023.03.0050167-739X/© 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\fH. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306The properties of chest X-rays make them susceptible to beanalysed by this type of algorithms. Some of the main advantagesof CNNs over traditional techniques are that it is not necessary tomanually extract image features or perform segmentation, andthat by being able to learn from large volumes of data theycan identify patterns that are difficult for the human eye to de-tect. Although in this article we focus on classification problems,other problems can be solved, such as X-ray segmentation [8],localisation, regression (such as predicting drug dosage), amongothers. CNNs are a potential tool for the analysis of chest ra-diographs. However, most of the work in this field focuses onbinary and multiclass classification problems. Actual problemsare usually more complex than the above; they tend to be mul-tilabel classification problems, i.e. the different labels are notmutually exclusive, whereas in binary and multiclass classifica-tion problems there is only one label per radiograph [9]. To solvemultilabel problems, we need to explore new strategies. Adaptingalgorithms can interpret this kind of problem by transformingthem into simpler problems that can be solved by traditionalalgorithms, i.e., transforming them into binary problems [10]. Inthe field of chest X-rays we can find samples without labels,healthy patients and samples with radiological signs of severaldiseases at the same time. On the other hand, there are a largenumber of different radiological signs in chest X-rays, so if wewant to build and validate a system that approximates realisticconditions, we have to use a dataset with a large number ofmutually non-exclusive labels. This is the case of the PadChestdatabase [11], which has 174 different radiological signs, substan-tially increasing the degree of realism and the complexity of theproblem.Many machine learning algorithms, including CNNs, work bestwhen the classes in the dataset are balanced. However, in reallife it is common to find datasets where this condition is notmet; they are imbalanced datasets, where one or more classeshave substantially more examples than the rest. As a conse-quence, with such datasets, machine learning algorithms learna bias towards the majority class, even though the minorityclass is often more relevant. Therefore, it is necessary to applydifferent methods to improve the recognition rate [12]. Thereare several options to overcome this difficulty: (a) modify thedataset, reducing the samples from the majority class or increas-ing the number of samples from the minority class; (b) modifythe algorithms to alleviate their bias towards the majority class,e.g. weighted learners [13]. The problem of unbalanced databasesis exacerbated in multilabel classification problems, where mul-tiple minority classes may appear, making this challenge moredifficult to solve. In medicine, it is widespread because eachdisease has a different incidence in the population. Heart disor-ders top the list of the deadliest diseases, followed by chronicobstructive pulmonary disease, which causes more than 6 milliondeaths a year. In contrast, other diseases such as lung cancerare the sixth leading cause of death with less than 2 milliondeaths, according to the World Health Organization.1 As a result,most radiographic datasets are imbalanced; a clear example isPadChest, the dataset used in this article, where the number ofsamples in each class approximates the incidence published bythe World Health Organization.These algorithms, like many other Deep Learning and MachineLearning methods, are considered ‘‘black box’’ algorithms becauseend users can only analyse the input and output, but the inferenceprocess is opaque, which reduces confidence in these algorithms.To alleviate this problem, explainable AI techniques have beendeveloped, such as saliency maps, which produce heatmaps thatFig. 1. Visualmethodology.representation ofthe problem and the objective ofthehighlight the pixels with the greatest influence on the final pre-diction [14]. This problem is serious in medicine, where errorscan be dangerous for patients [15]. For this reason, explainableAI techniques are essential, as they allow users to understandhow the system has arrived at the final result and use it to helpdiagnose [16]. However, the combination of medical knowledgeand AI has many advantages, such as helping to reduce medicalerrors and speeding up diagnostic processes, leading to improvedpatient care, as doctors would have more time to attend patients.The contribution of this manuscript is a methodology, seeFig. 1, for classi",
            {
                "entities": [
                    [
                        747,
                        767,
                        "AUTHOR"
                    ],
                    [
                        797,
                        812,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Medical Image Registration Using Deep Neural Networks: A Comprehensive Review Hamid Reza Boveiri a,*, Raouf Khayami a, Reza Javidan a, Ali Reza MehdiZadeh b a Department of Computer Engineering and IT, Shiraz University of Technology, Shiraz, Iran b Department of Medical Physics and Engineering, Shiraz University of Medical Science, Shiraz, Iran * Corresponding Author Email: hr.boveiri@sutech.ac.ir Abstract—Image-guided interventions are saving the lives of a large number of patients where the image registration problem should indeed be considered as the most complex and complicated issue to be tackled. On the other hand, the recently huge progress in the field of machine learning made by the possibility of implementing deep neural networks on the contemporary many-core GPUs opened up a promising window to challenge with many medical applications, where the registration is not an exception. In this paper, a comprehensive review on the state-of-the-art literature known as medical image registration using deep neural networks is presented. The review is systematic and encompasses all the related works previously published in the field. Key concepts, statistical analysis from different points of view, confiding challenges, novelties and main contributions, key-enabling techniques, future directions and prospective trends all are discussed and surveyed in details in this comprehensive review. This review allows a deep understanding and insight for the readers active in the field who are investigating the state-of-the-art and seeking to contribute the future literature. Keywords—Convolutional Neural Network (CNN); Deep Learning; Deep Reinforcement Learning; Deformable Registration; Generative Adversarial Network (GAN); Image-guided Intervention; Medical Image Registration; One-shot Registration; Staked Auto-Encoders (SAEs). ———————————————————— ♦ ———————————————————— 1. Introduction In most medical interventions, there are a number of cases in which some images need to be captured for diagnosis, prognosis, treatment and follow-up purposes. These images can be vary in terms of temporal, spatial, dimensional or modular. Image fusion causing information synergy can have a significant contribution to guide and support physicians in the process of decision making, mostly in online and real-time fasion. Lack of alignment is unavoidable for these images taken in different times and conditions; hence, can challenge the quality and accuracy of the subsequent analyses. Image registration is the process of aligning two (or more) given images based on an identical geometrical coordination system. The aim is at finding an optimum spatial transformation that registers the structures-of-interest in the best way. This problem is important in numerous ways in the field of machine vision e.g. for remote sensing, object tracing, satellite imaging and so on (Goshtasby 2017). Image registration is also fundamental to the image-guided intervention where e.g. telesurgery, image-guided radiotherapy (IGRT), and precision medicine cannot be operational without using image registration techniques (Peters & Cleary 2008). To exemplify, in IGRT, a pre-interventional image (typically high-quality 3D image), on which the treatment planning is conducted, needs to be registered on an operational image (typically low-quality and noisy 2D) so that the linear accelerator (linac) machine can be calibrated, and the radiation fragment can be conducted with maximal precision and minimal risk of radiation to the adjacent healthy organs referred to as minimally invasive procedure. In this process, the challenges like different modalities of inputted images, low-quality and noise of interventional images, deformation of abdominal cavity’s organs (because of the spontaneous contraction/inflation), movement of thorax cavity’s organs (because of the respiration and heartbeats), changing the size of organs and regions-of-interest (RoIs) due to the weight loss/gain during the treatment process can compromises the quality of solving the problem. In practice, special considerations should be taken into account, and other image processing techniques need to collaborate, which makes the issue very challenging and complicated (Hajnal 2001). Basically, conventional image registration is an iterative optimization process that requires extracting proper features, selecting a similarity measure (to evaluate the registration quality), choosing the model of transformation, and finally, a mechanism to investigate the search space (Oliveira & Tavares 2014). As illustrated in Fig. 1, a couple of images are inputted to the system, of which one is considered as fixed and the other as moving image. The optimal alignment can be - 1 -  \fachieved via iteratively sliding the moving image over the fixed image. At first, the considered similarity measure identifies the amount of correspondence between the inputted images. An optimization algorithm, using an updating mechanism, calculates the new transformation’s parameters. Appling these parameters on the moving image leads a new supposedly better-aligned image. If the termination criteria are satisfied, the algorithm is terminated, else a new iteration should be started. In each iteration, the moving image get a better correspondence with the fixed image, and the iterations continue until no further registration can be achieved, or some predefined criteria are satisfied. The system output can be either the transformation parameters or final interpolated fused image. Fixed  Image Moving Image Similarity Measure Calculation Parameter Updating Transformation Applying Termination? No Fused  Image Transform Parameters Figure 1: The workflow of conventional image registration techniques based on optimization procedures There are two main drawbacks to this strategy as follows:  This iterative manner is very slow where runtimes in the tens of minutes are norm for common deformable image registration techniques even with an efficient implementation on the contemporary GPUs (like a NVIDIA TitanX); while the practical use in clinical operations is real-time, and such a prolonged wasting time is not appreciated.  Most similarity measures have a lots of local optima around the global one, specially where dealing with images from different modalities (referred to as multimodal image registration); they lose their efficiency causing premature convergence or stagnation which are two prevalent confining dilemmas in the optimization field. Accordingly, to circumvent these two confining problems, learning-based registration approaches have gained increasing popularity in the recent years; meanwhile, deep neural networks (DNNs), as one of the most powerful techniques ever seen by the community of machine intelligence, have been applied to the various image processing applications. Of course, medical image registration is not an exception, and a number of deep learning based approaches were proposed in the literature; however, the number of works and the used techniques is very limited, and there is a promising potential for investigation (Litjens et al. 2017).   In this paper, a comprehensive systematic review on the medical image registration using deep neural networks is presented. We have gathered all the relevant state-of-the-arts, from the first one in 2013 up the last one in 2019. The works are analyses based on the different statistical perspectives and measures of interest e.g. years, publication titles, publication types, publishers, authors (total number of publications and citations), keywords, techniques, comparison metrics, datasets, organs of interest, and modalities. The approaches are categorized to the three major generations based on the breakthroughs affecting the contributions. Also, the seminal works in each category and generation are introduced and analyzed in depth, and key contributions and philosophies behind them are presented in details. Finally, there is a discussion introducing confining challenges, open problems and prospective directions. This review allows a deep - 2 -   \funderstanding and insight for the readers active in the field who are investigating the state-of-the-art and seeking to contribute the future literature. Rest of the paper is organized as follows. In the next Section, reference gathering methodology and the challenged faced are described. Section 3 and 4 are devoted to the taxonomy of medical image registration, and the problem formulation, respectively. The architectures of deep neural networks used in the literature are investigated in Section 5. Section 6 is an in-depth literature review on seminal works. Statistical analysis on the state-of-the-art is presented in Section 7. Section 8 is devoted to the discussion on the confining challenges and open problems in the field. And finally, the conclusion and future trends are presented in the last Section. 2. Reference Gathering Methodology To investigate the literature, the systematic search was conducted on two major scientific databases i.e. “SCOPUS” and “PubMed” (since the topic is multidisciplinary between engineering and medicine). “Medical Image Registration” was the main keyword searched accompanied with one of the followings as the underlying technique i.e. “Deep Learning,” “Deep Neural Network,” or “Convolutional.” Moreover, another search was conducted using the aforementioned keywords in major scientific datasets i.e. “Google Scholar” and “CrossRef” to validate the comprehensiveness of the results. The searches were restricted to the Title, Abstract and Keywords whenever it was feasible. A total number of 25 references were detected, among them, 6 cases was irrelevant (e.g. just the results were compared to the deep learning approaches) and 21 cases was completely matched with our criteria. At this point, we detected that some references were overlooked, and that these keywords are insufficient to conduct a comprehensive search; for example",
            {
                "entities": [
                    [
                        77,
                        96,
                        "AUTHOR"
                    ],
                    [
                        101,
                        115,
                        "AUTHOR"
                    ],
                    [
                        118,
                        131,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Information Fusion 16 (2014) 3–17Contents lists available at SciVerse ScienceDirectInformation Fusionj o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / i n f f u sA survey of multiple classifier systems as hybrid systemsMichał Woz´ niak a,⇑, Manuel Graña b, Emilio Corchado ca Department of Systems and Computer Networks, Wroclaw University of Technology, Wroclaw, Polandb Computational Intelligence Group, University of the Basque Country, San Sebastian, Spainc Departamento de Informática y Automática, University of Salamanca, Salamanca, Spaina r t i c l ei n f oa b s t r a c tArticle history:Available online 29 April 2013Keywords:Combined classifierMultiple classifier systemClassifier ensembleClassifier fusionHybrid classifierA current focus of intense research in pattern classification is the combination of several classifier sys-tems, which can be built following either the same or different models and/or datasets buildingapproaches. These systems perform information fusion of classification decisions at different levels over-coming limitations of traditional approaches based on single classifiers. This paper presents an up-to-date survey on multiple classifier system (MCS) from the point of view of Hybrid Intelligent Systems.The article discusses major issues, such as diversity and decision fusion methods, providing a vision ofthe spectrum of applications that are currently being developed.(cid:2) 2013 Elsevier B.V. All rights reserved.1. IntroductionHybrid Intelligent Systems offer many alternatives for unortho-dox handling of realistic increasingly complex problems, involvingambiguity, uncertainty and high-dimensionality of data. They al-low to use both a priori knowledge and raw data to compose inno-vative solutions. Therefore, there is growing attention to thismultidisciplinary research field in the computer engineering re-search community. Hybridization appears in many domains of hu-man activity. It has an immediate natural inspiration in the humanbiological systems, such as the Central Nervous System, which is ade facto hybrid composition of many diverse computational units,as discussed since the early days of computer science, e.g., byvon Neumann [1] or Newell [2]. Hybrid approaches seek to exploitthe strengths of the individual components, obtaining enhancedperformance by their combination. The famous ‘‘no free lunch’’ the-orem [3] stated by Wolpert may be extrapolated to the point ofsaying that there is no single computational view that solves allproblems. Fig. 1 is a rough representation of the computational do-mains covered by the Hybrid Intelligent System approach. Some ofthem deal with the uncertainty and ambiguity in the data by prob-abilistic or fuzzy representations and feature extraction. Othersdeal with optimization problems appearing in many facets of the⇑ Corresponding author.E-mail addresses: michal.wozniak@pwr.wroc.pl (M. Woz´ niak), ccpgrrom@g-mail.com (M. Graña), escorchado@usal.es (E. Corchado).1566-2535/$ - see front matter (cid:2) 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.inffus.2013.04.006intelligent system design and problem solving, either following anature inspired or a stochastic process approach. Finally, classifiersimplementing the intelligent decision process are also subject tohybridization by various forms of combination. In this paper, wefocus in this specific domain, which is in an extraordinary efferves-cence nowadays, under the heading of Multi-Classifier Systems(MCS). Referring to classification problems, Wolpert’s theoremhas an specific lecture: there is not a single classifier modeling ap-proach which is optimal for all pattern recognition tasks, sinceeach has its own domain of competence. For a given classificationtask, we expect the MCS to exploit the strengths of the individualclassifier models at our disposal to produce the high quality com-pound recognition system overcoming the performance of individ-ual classifiers. Summarizing:(cid:2) Hybrid Intelligent Systems (HIS) are free combinations of compu-tational intelligence techniques to solve a given problem, cover-ing al computational phases from data normalization up to finaldecision making. Specifically, they mix heterogeneous funda-mental views blending them into one effective working system.(cid:2) Information Fusion covers the ways to combine informationsources in a view providing new properties that may allow tosolve better or more efficiently the proposed problem. Informa-tion sources can be the result of additional computationalprocesses.(cid:2) Multi-Classifier Systems (MCS) focus on the combination ofclassifiers form heterogenous or homogeneous modeling back-grounds to give the final decision. MCS are therefore a subcate-gory of HIS.\f4M. Woz´niak et al. / Information Fusion 16 (2014) 3–17Fig. 1. Domains of hybrid intelligent systems.on three well known academic search sites. The growth in the num-ber of publications has an exponential trend. The last entry of theHistorical perspective. The concept of MCS was first presented byChow [4], who gave conditions for optimality of the joint decision1of independent binary classifiers with appropriately defined weights.In 1979 Dasarathy and Sheela combined a linear classifier and one k-NN classifier [6], suggesting to identify the region of the featurespace where the classifiers disagree. The k-NN classifier gives the an-swer of the MCS for the objects coming from the conflictive regionand by the linear one for the remaining objects. Such strategy signif-icantly decreases the exploitation cost of whole classifier system.This was the first work introducing a classifier selection concept,however the same idea was developed independently in 1981 byRastrigin and Erenstein [7] performing first a feature space partition-ing and, second, assigning to each partition region an individual clas-sifier that achieves the best classification accuracy over it. Otherearly relevant works formulated conclusions regarding MCS ’s classi-fication quality, such as [8] who considered a neural network ensem-ble, [9] with majority voting applied to handwriting recognition,Turner in 1996 [10] showed that averaging outputs of an infinitenumber of unbiased and independent classifiers can lead to the sameresponse as the optimal Bayes classifier, Ho [11] underlined that adecision combination function must receive useful representationof each classifier’s decision. Specifically, they considered severalmethod based on decision ranks, such as Borda count. Finally, thelandmark works devoted introducing bagging [12] and boosting[13,14] which are able to produce strong classifiers [15], in the (Prob-ably Approximately Correct) theory [16] sense, on the basis of theweak one. Nowadays MCS, are highlighted by review articles as ahot topic and promising trend in pattern recognition [17–21]. Thesereviews include the books by Kuncheva [22], Rokach [23], Seni andEdler [24], and Baruque and Corchado [25]. Even leading-edge gen-eral machine learning handbooks such as [26–28] include extensivepresentations of MCS concepts and architectures. The popularity ofthis approach is confirmed by the growing trend in the number ofpublications shown in Fig. 2. The figure reproduces the evolutionof the number of references retrieved by the application of specifickeywords related to MCS since 1990. The experiment was repeated1 We can retrace decision combination long way back in history. Perhaps the firstworthy reference is the Greek democracy (meaning government of the people) rulingthat full citizens have an equal say in any decision that affects their life. Greeksbelieved in the community wisdom, meaning that the rule of the majority willproduce the optimal joint decision. In 1785 Condorcet formulated the Jury Theoremabout the misclassification probability of a group ofindependent voters [5]],providing the first result measuring the quality of classifier committee.Fig. 2. Evolution of the number of publications per year ranges retrieved from thekeywords specified in the plot legend. Each plot corresponds to searching site: thetop to Google Scholar; the center to the Web of Knowledge, the bottom to Scopus.The first entry of the plots is for publications prior to 1990. The last entry is only forthe last 2 years.\fM. Woz´niak et al. / Information Fusion 16 (2014) 3–175plots corresponds to the last 2 years, and some of the keywords giveas many references as in the previous 5 years.Advantages. Dietterich [29] summarized the benefits of MCS: (a)allowing to filter out hypothesis that, though accurate, might beincorrect due to a small training set, (b) combining classifierstrained starting from different initial conditions could overcomethe local optima problem, and (c) the true function may be impos-sible to be modeled by any single hypothesis, but combinations ofhypotheses may expand the space of representable functions.Rephrasing it, there is widespread acknowledgment of the follow-ing advantages of MCS:(cid:2) MCS behave well in the two extreme cases of data availability:when we have very scarce data samples for learning, and whenwe have a huge amount of them at our disposal. In the scarcitycase, MCS can exploit bootstrapping methods, such as baggingor boosting. Intuitive reasoning justifies that the worst classifierwould be out of the selection by this method [30], e.g., by indi-vidual classifier output averaging [31]. In the event of availabil-ity of a huge amount of learning data samples, MCS allow totrain classifiers on dataset’s partitions and merge their decisionusing appropriate combination rule [20].(cid:2) Combined classifier can outperform the best individual classi-fier [32]. Under some conditions (e.g., majority voting by agroup of independent classifiers) this improvement has beenproven analytically [10].(cid:2) Many machine learning algorithms are de facto heuristic searchalgorithms. For example the popular decision tree inductionmethod C4.5 [33] uses a",
            {
                "entities": [
                    [
                        286,
                        302,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fPattern Recognition 123 (2022) 108403 Contents lists available at ScienceDirect Pattern Recognition journal homepage: www.elsevier.com/locate/patcog Fitbeat: COVID-19 estimation based on wristband heart rate using a contrastive convolutional auto-encoder Shuo Liu a , ∗, Jing Han a , b , Estela Laporta Puyal c , d , Spyridon Kontaxis c , d , Shaoxiong Sun e , Patrick Locatelli f , Judith Dineley a , Florian B. Pokorny a , g , Gloria Dalla Costa h , Letizia Leocani h , Ana Isabel Guerrero i , Carlos Nos i , Ana Zabalza i , Per Soelberg Sørensen j , Mathias Buron j , Melinda Magyari j , Yatharth Ranjan e , Zulqarnain Rashid e , Pauline Conde e , Callum Stewart e , Amos A Folarin e , k , Richard JB Dobson e , k , Raquel Bailón c , d , Srinivasan Vairavan l , Nicholas Cummins a , e , Vaibhav A Narayan l , Matthew Hotopf m , n , Giancarlo Comi o , Björn Schuller a , p , RADAR-CNS Consortium q a EIHW – Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Augsburg, Germany b Department of Computer Science and Technology, University of Cambridge, Cambridge, United Kingdom c BSICoS Group, Aragón Institute of Engineering Research (I3A), IIS Aragón, University of Zaragoza, Zaragoza, Spain d CIBER of Bioengineering, Biomaterials and Nanomedicine (CIBER-BNN), Madrid, Spain e The Department of Biostatistics and Health informatics, Institute of Psychiatry, Psychology and Neuroscience, King’s College London, London, UK f Department of Engineering and Applied Science, University of Bergamo, Bergamo, Italy g Division of Phoniatrics, Medical University of Graz, Graz, Austria h Università Vita Salute San Raffaele and Experimental Neurophysiology Unit, Institute of Experimental Neurology, Scientific Institute Hospital San Raffaele, Milan, Italy i Multiple Sclerosis Centre of Catalonia (Cemcat), Department of NeurologyNeuroimmunology, Hospital Universitari Vall dH ´ebron, Universitat Autónoma de Barcelona, Barcelona, Spain j Danish Multiple Sclerosis Centre, Department of Neurology, Copenhagen University Hospital Rigshospitalet, Copenhagen, Denmark k Institute of Health Informatics, University College London, London, United Kingdom l Janssen Research and Development LLC, Titusville, NJ, USA m The Department of Psychological Medicine, Institute of Psychiatry, Psychology and Neuroscience, King’s College London, London, United Kingdom n South London and Maudsley National Health Service Foundation Trust, London, United Kingdom o Università Vita Salute San Raffaele, Casa di Cura Privata del Policlinico, Milan, Italy p GLAM – Group on Language, Audio, & Music, Imperial College London, London, United Kingdom q The RADAR-CNS Consortium, London, United Kingdom a r t i c l e i n f o a b s t r a c t Article history: Received 5 April 2021 Revised 30 August 2021 Accepted 24 October 2021 Available online 26 October 2021 Keywords: COVID-19 Respiratory tract infection Anomaly detection Contrastive learning Convolutional auto-encoder This study proposes a contrastive convolutional auto-encoder (contrastive CAE), a combined architecture of an auto-encoder and contrastive loss, to identify individuals with suspected COVID-19 infection using heart-rate data from participants with multiple sclerosis (MS) in the ongoing RADAR-CNS mHealth re- search project. Heart-rate data was remotely collected using a Fitbit wristband. COVID-19 infection was either confirmed through a positive swab test, or inferred through a self-reported set of recognised symp- toms of the virus. The contrastive CAE outperforms a conventional convolutional neural network (CNN), a long short-term memory (LSTM) model, and a convolutional auto-encoder without contrastive loss (CAE). On a test set of 19 participants with MS with reported symptoms of COVID-19, each one paired with a participant with MS with no COVID-19 symptoms, the contrastive CAE achieves an unweighted average recall of 95 . 3% , a sensitivity of 100% and a specificity of 90 . 6% , an area under the receiver operating char- acteristic curve (AUC-ROC) of 0.944, indicating a maximum successful detection of symptoms in the given heart rate measurement period, whilst at the same time keeping a low false alarm rate. © 2021 Elsevier Ltd. All rights reserved. ∗ Corresponding author. E-mail address: shuo.liu@informatik.uni-augsburg.com (S. Liu). URL: http://www.radar-cns.org (R.-C. Consortium) https://doi.org/10.1016/j.patcog.2021.108403 0031-3203/© 2021 Elsevier Ltd. All rights reserved. \fS. Liu, J. Han, E.L. Puyal et al. 1. Introduction Remote passive monitoring of physiological and behavioural characteristics using smartphones and wearable devices can be used to rapidly collect a variety of data in huge volumes with min- imal effort from the wearer. Such data has the potential to improve our understanding of the interplay between a variety of health conditions at individual and population level, if rigorously collected and validated [1] . Passive data collection is typically implemented with a high temporal resolution [2] . Wearable fitness trackers, for example, estimate parameters such as heart rate up to every sec- ond and up to 24 hours a day. Monitoring individuals with a range of health states, lifestyles, and demographic variables in combina- tion with data artefacts and missing data leads to high variability, while multiple data streams, from heart rate and physical activity to GPS-based location, can be collected. Therefore, studies using wearables and smartphones in this way exhibit several vs of big data: velocity, volume, variability and variety. As such, advanced analysis methodologies such as deep learning can potentially make a significant contribution [3] , particularly in the context of infec- tious diseases, such as COVID-19, the disease caused by the novel corona virus (SARS-CoV-2). Specific applications include individual screening and population-level monitoring that minimise contact with infected individuals [4,5] . Since the outbreak of the COVID-19 pandemic in 2020, sev- eral deep learning methodologies have been applied to computed tomography (CT) scans [6] and 2D X-ray images [7] to detect COVID-19. These methods require specific clinical equipment and the patient must attend a clinical facility. Consequently, it cannot achieve early, automatic detection when COVID-19 symptoms first appear. In contrast, heart rate can be measured remotely and non- intrusively using wearable devices. Heart rate is a biomarker of particular value in such appli- cations. Patterns in heart rate fluctuations over time have been found to provide clinically relevant information about the integrity of the physiological system generating these dynamics. Previous studies have not only revealed an altered heart rate variability in a number of medical conditions [8] , but also demonstrated that the degree of short-term heart rate alteration correlates with ill- ness severity. Analysis of the autonomic regulation of heart rate has also been discussed as a promising approach for detecting infections earlier than conventional clinical methods and making prognoses [9] . Wearables such as Fitbit fitness trackers 1 provide indirect measurements of the heart rate through pulse rate estimates made using photoplethysmography (PPG). In the ongoing DETECT 2 study [5] , researchers are focusing on monitoring outbreaks of vi- ral infections including COVID-19 based on the resting heart rate collected in this way [10] . Other similar ongoing endeavours in- clude the German project Corona-Datenspende 3 , which has a co- hort of over 50 0 0 0 0 volunteers, and the TemPredict study in the US 4 . Applied to such data sets, deep learning has the potential to au- tomatically identify individuals with COVID-19 purely on the basis of data passively acquired by means of wearable devices [5,11,12] . To the best of our knowledge, the present study is the first to com- pare deep learning approaches in predicting the presence or ab- sence of COVID-19-like symptoms using Fitbit-measured heart rate data. We aim to exploit state-of-the-art methods to represent the 1 2 3 4 https://www.fitbit.com/ [as of 03 August 2021]. http://detectstudy.org/ [as of 03 August 2021]. http://corona-datenspende.de/science/en/ [as of 03 August 2021]. http://osher.ucsf.edu/research/current-research-studies/tempredict [as of 03 Au- gust 2021]. 2 Pattern Recognition 123 (2022) 108403 problem by feature maps, including convolutional neural networks (CNNs) and a convolutional auto-encoder (CAE) [13] . Considering the deficiency of class information in training a standard CAE, in some previous works, the class information was applied to latent attribute layers, leading to the supervised auto- encoder introduced in [14] . Cross-entropy losses are used to min- imise the difference between predicted labels from latent at- tributes and true labels. This approach provides a certain preser- vation of the reconstructed feature map, taking the cross-entropy loss as a regularisation method. The reconstruction error and cross entropy loss are jointly optimised. However, the optimisa- tion of the joint loss requires a proper combination factor in or- der to balance the optimisation on reconstruction",
            {
                "entities": [
                    [
                        1031,
                        1040,
                        "AUTHOR"
                    ],
                    [
                        1047,
                        1056,
                        "AUTHOR"
                    ],
                    [
                        1064,
                        1085,
                        "AUTHOR"
                    ],
                    [
                        1093,
                        1111,
                        "AUTHOR"
                    ],
                    [
                        1119,
                        1133,
                        "AUTHOR"
                    ],
                    [
                        1137,
                        1155,
                        "AUTHOR"
                    ],
                    [
                        1159,
                        1174,
                        "AUTHOR"
                    ],
                    [
                        1205,
                        1224,
                        "AUTHOR"
                    ],
                    [
                        1228,
                        1244,
                        "AUTHOR"
                    ],
                    [
                        1248,
                        1268,
                        "AUTHOR"
                    ],
                    [
                        1272,
                        1283,
                        "AUTHOR"
                    ],
                    [
                        1287,
                        1299,
                        "AUTHOR"
                    ],
                    [
                        1329,
                        1343,
                        "AUTHOR"
                    ],
                    [
                        1347,
                        1363,
                        "AUTHOR"
                    ],
                    [
                        1367,
                        1383,
                        "AUTHOR"
                    ],
                    [
                        1387,
                        1405,
                        "AUTHOR"
                    ],
                    [
                        1409,
                        1423,
                        "AUTHOR"
                    ],
                    [
                        1427,
                        1442,
                        "AUTHOR"
                    ],
                    [
                        1517,
                        1537,
                        "AUTHOR"
                    ],
                    [
                        1541,
                        1558,
                        "AUTHOR"
                    ],
                    [
                        1611,
                        1626,
                        "AUTHOR"
                    ],
                    [
                        1653,
                        1674,
                        "AUTHOR"
                    ],
                    [
                        3441,
                        3462,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ArticleDeep forecasting of translational impact in medicalresearchHighlightsd Deep learning models of biomedical paper content canaccurately predict translationd Deep content models substantially outperform traditionalcitation metricsd Models trained on patent inclusion transfer to predictingNobel Prize-preceding papersAuthorsAmy P.K. Nelson, Robert J. Gray,James K. Ruffle, ..., Bryan Williams,Geraint E. Rees, Parashkev NachevCorrespondenceamy.nelson@ucl.ac.uk (A.P.K.N.),p.nachev@ucl.ac.uk (P.N.)d Science policy is potentially better informed by deep contentmodels than by citationsIn briefAnalyzing 43.3 million biomedical papersfrom 1990–2019, we show that deeplearning models of publication title andabstract content can predict inclusion in apatent, guideline, or policy documentwith far greater fidelity than citationmetrics alone. If judgments of thetranslational potential of science are to bebased on objective metrics, then complexmodels of paper content should bepreferred over citations.Nelson et al., 2022, Patterns 3, 100483May 13, 2022 ª 2022 The Authors.https://doi.org/10.1016/j.patter.2022.100483ll\fllOPEN ACCESSArticleDeep forecasting of translationalimpact in medical researchAmy P.K. Nelson,1,7,* Robert J. Gray,1 James K. Ruffle,1 Henry C. Watkins,1 Daniel Herron,2 Nick Sorros,3 Danil Mikhailov,3M. Jorge Cardoso,4 Sebastien Ourselin,4 Nick McNally,2 Bryan Williams,2,5 Geraint E. Rees,1,6 and Parashkev Nachev1,*1High Dimensional Neurology Group, UCL Queen Square Institute of Neurology, University College London, Russell Square House,Bloomsbury, London WC1B 5EH, UK2Research & Development, NIHR University College London Hospitals Biomedical Research Centre, London WC1E 6BT, UK3Wellcome Data Labs, Wellcome Trust, London NW1 2BE, UK4School of Biomedical Engineering & Imaging Sciences, King’s College London, London WC2R 2LS, UK5UCL Institute of Cardiovascular Sciences, University College London, London WC1E 6BT, UK6Faculty of Life Sciences, University College London, Gower Street, London WC1E 6BT, UK7Lead contact*Correspondence: amy.nelson@ucl.ac.uk (A.P.K.N.), p.nachev@ucl.ac.uk (P.N.)https://doi.org/10.1016/j.patter.2022.100483THE BIGGER PICTURE The relationship of scientific activity to real-world impact is hard to describe andeven harder to quantify. Analyzing 43.3 million biomedical papers from 1990–2019, we show that deeplearning models of publication, title, and abstract content can predict inclusion of a scientific paper in a pat-ent, guideline, or policy document. We show that the best of these models, incorporating the richest infor-mation, substantially outperforms traditional metrics of paper success—citations per year—and transfersto the task of predicting Nobel Prize-preceding papers. If judgments of the translational potential of scienceare to be based on objective metrics, then complex models of paper content should be preferred over ci-tations. Our approach is naturally extensible to richer scientific content and diverse measures of impact. Itswider application could maximize the real-world benefits of scientific activity in the biomedical realm andbeyond.Development/Pre-production: Data science output has beenrolled out/validated across multiple domains/problemsSUMMARYThe value of biomedical research—a $1.7 trillion annual investment—is ultimately determined by its down-stream, real-world impact, whose predictability from simple citation metrics remains unquantified. Here wesought to determine the comparative predictability of future real-world translation—as indexed by inclusion inpatents, guidelines, or policy documents—from complex models of title/abstract-level content versus citationsand metadata alone. We quantify predictive performance out of sample, ahead of time, across major domains,using the entire corpus of biomedical research captured by Microsoft Academic Graph from 1990–2019, encom-passing 43.3 million papers. We show that citations are only moderately predictive of translational impact. Incontrast, high-dimensional models of titles, abstracts, and metadata exhibit high fidelity (area under the receiveroperating curve [AUROC] > 0.9), generalize across time and domain, and transfer to recognizing papers of Nobellaureates. We argue that content-based impact models are superior to conventional, citation-based measuresand sustain a stronger evidence-based claim to the objective measurement of translational potential.INTRODUCTIONScientometrics has existed for only a small fraction of the historyof science itself, sparked by the logical empiricists of the ViennaCircle in their philosophical quest to construct a unified languageof science.1 Developed into the familiar, citation-centered formthrough arduous manual extraction in the mid-20th century,2,3its indicators have proliferated in the Internet age. They nowdominate the research landscape, routinely informing majorfunding decisions and academic staff recruitment worldwide.4–8Patterns 3, 100483, May 13, 2022 ª 2022 The Authors. 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fllOPEN ACCESSThe importance of the original goal has become magnifiedover time: to measure scientific progress regardless of fundingor ideology, uncolored by the reputations of individuals or insti-tutions. But the fundamental focus of its current solution—thevolume and density of discussion in print—is detached fromthe ultimate, real-world objective and subject to familiar distor-tions, such as the popularity of papers notable only for beingspectacularly wrong.9–11These concerns are amplified in medical science, whose pri-mary focus is not merely knowledge but impact on patient health:necessarily a consequence rather than a constitutive character-istic of research activity, neither easily benchmarked nor directlyoptimized. And there is no doubt that optimization is needed;over the past 60 years, the number of new drug approvals perunit R&D spend has consistently halved every 9 years, whereaspublished medical research has doubled with the same period-icity,12 and only 0.004% of basic research findings ultimatelylead to clinically useful treatments.13 The critical pre-requisitefor all research—funding—shows substantial randomness in itsdistribution,14 enough for at least one major healthcare funderto award grants by lottery.15 Any decision function based onrandom chance, or a process demonstrably not much betterthan random chance, leaves room for improvement, particularlywhen commanding approximately $1.7 trillion global annual in-vestment across the United States, Japan, South Korea, andthe European Union.16Is this state of affairs partially caused by over-reliance onmisleading scientometrics, have we simply not found the rightmetrics yet, or is the relation between scientific activity andconsequent impact opaque to objective analysis? To addressthese crucial questions, we need a fully inclusive survey ofpublished medical research that relates its characteristics toan independently measured translational outcome as close toreal-world impact as can be quantified. This relationshipmust be explored with models of sufficient expressivity todetect complex relations between many candidate predictivefactors beyond paper-to-paper citations. The extant literatureis largely limited to modeling keywords or simple representa-tions of semantic content,17–21 within specific subdomains, orcomparatively restricted bibliographic databases,22–26 andwithout exploration of the impact of data dimensionality andmodel flexibility.Here we provide the first comprehensive, field-wide analysisof translational impact measured by its most widely acceptedproximalindices—patents, guidelines, or policies—based on29 years of data from the medical field encompassing 43.3million published papers. We quantify the ability to predict inclu-sion in future patents, guidelines, or policies from conventionalage-normalized citation counts and compare this with the pre-dictive fidelity of deep learning models incorporating more com-plex features extracted from metadata, titles, and abstracts. Weevaluate the performance of the best model across time and the-matic domain and in transfer to the task of recognizing papers ofNobel laureates. We derive succinct, surveyable representationsof paper title and abstract content with deep autoencoding oftransformer-based text embeddings and of publication meta-data with stochastic block models. The breadth and depth ofanalysis allow us to draw strong conclusions about the compar-ative fidelity of conventional bibliographic and novel semantic2 Patterns 3, 100483, May 13, 2022Articlepredictors of translational impact, with substantial implicationsfor research policy.RESULTSCitationsOver the period from January 1990 to March 2019, only 17.1million of the 43.3 million published papers categorized as med-ical by Microsoft Academic Graph were cited at least once. Ofthese, 964,403 were included in a patent and 16,752 in a guide-line or a policy document. Included papers were more frequentlycited, but the numbers of citations and inclusions were weaklycorrelated (Pearson’s r = 0.094 for guidelines or policies, r =0.248 for patents; Figure 1). The mean time delay from paperpublication to first patent inclusion was 4.73 years (SD 4.54;Figure S1).Predictive performanceA series of models was developed to investigate the relativecontribution of three data modalities—annual paper citations,metadata only, and the combination of metadata and abstract/ti-tle embeddings—in predicting two translational outcomes: apaper’s inclusion in a patent or policy/guideline reference list. At-tempting to predict inclusion in a guideline or policy documentfrom the traditional measure of impact—annual paper cita-tions—yielded a mean cross-validated area under the receiveroperating curve (AUROC) of 0.766 with univariable logisticregression (Citations-LogisticRegression) and 0.767 with anoptimized univariable multilayer perceptron (MLP) model (",
            {
                "entities": [
                    [
                        1276,
                        1290,
                        "AUTHOR"
                    ],
                    [
                        1292,
                        1304,
                        "AUTHOR"
                    ],
                    [
                        1306,
                        1322,
                        "AUTHOR"
                    ],
                    [
                        1342,
                        1361,
                        "AUTHOR"
                    ],
                    [
                        1363,
                        1376,
                        "AUTHOR"
                    ],
                    [
                        381,
                        396,
                        "AUTHOR"
                    ],
                    [
                        1378,
                        1393,
                        "AUTHOR"
                    ],
                    [
                        413,
                        430,
                        "AUTHOR"
                    ],
                    [
                        1421,
                        1438,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Computers in Biology and Medicine journal homepage: www.elsevier.com/locate/compbiomed A review of deep learning-based detection methods for COVID-19 Nandhini Subramanian *, Omar Elharrouss, Somaya Al-Maadeed, Muhammed Chowdhury Qatar University College of Engineering, Computer Science and Engineering, Qatar  A R T I C L E I N F O  A B S T R A C T  Keywords: COVID-19 detection DL-Based COVID-19 detection Lung image classification Coronavirus pandemic Medical image processing COVID-19 is a fast-spreading pandemic, and early detection is crucial for stopping the spread of infection. Lung images are used in the detection of coronavirus infection. Chest X-ray (CXR) and computed tomography (CT) images are available for the detection of COVID-19. Deep learning methods have been proven efficient and better performing in many computer vision and medical imaging applications. In the rise of the COVID pandemic, researchers are using deep learning methods to detect coronavirus infection in lung images. In this paper, the currently available deep learning methods that are used to detect coronavirus infection in lung images are surveyed. The available methodologies, public datasets, datasets that are used by each method and evaluation metrics are summarized in this paper to help future researchers. The evaluation metrics that are used by the methods are comprehensively compared.  1. Introduction The World Health Organization (WHO) declared the spread of the coronavirus infection a pandemic in March 2020, which is called the coronavirus pandemic or COVID-19 pandemic. The coronavirus pandemic is caused by severe acute respiratory syndrome coronavirus 2 (SARS CoV 2). The outbreak originally started in Wuhan, China, and later spread to every country in the world [1]. The coronavirus spreads through respiratory droplets of the infected person that are produced through cough or sneeze. These droplets can further contaminate the surfaces increasing the spread. Coronavirus-infected persons may suffer from mild to severe respiratory illness and may require ventilation support [2]. Older people and people with chronological disorders are easily prone to coronavirus infection. Thus, many governments have closed their borders and locked down people to break the cycle and prevent the spread of the pandemic [3]. With the sequencing of ribonucleic acid (RNA) from the coronavirus, many vaccines are being developed worldwide. The developed vaccines use both traditional and next-generation technology with six vaccine platforms, namely, live attenuated virus, inactivated virus, protein or subunit, viral vector-based, messenger RNA (mRNA), and deoxy-ribonucleic acid (DNA). Although vaccines can reduce the rapid spread and facilitate the development of immunity via the production of suit-able antibodies, the efficacy of the vaccines is still 95%. Many issues are encountered in administering the vaccine, such as supply chain logistical challenges, vaccine hesitancy, and vaccine complacency. A vaccine is a prevention measure rather than a cure [4]. Even with the availability of the vaccine, early detection of the coronavirus is important, as it can facilitate tracing of the people who were in contact directly and indi-rectly. By tracing these people, further spread of the pandemic can be avoided. COVID-19 infection manifests as lung infection, and computed tomography (CT) and chest X-ray (CXR) images are primarily used in the detection of lung infection of any type [5]. Along with doctors and clinical personnel, researchers and technol-ogists are focusing their efforts on early detection of coronavirus in-fections. According to PubMed [6], 755 academic articles were published with the search term “coronavirus” in 2019, and this number rose to 1245 in the first 80 days of 2020. Artificial intelligence and deep learning methods are the most commonly used methods by researchers for the detection of coronavirus infection from CT and CXR images. Deep learning methods have shown significant performance in many research applications, such as computer vision [7], object tracking [8], gesture recognition [9], face recognition [10], and steganography [11–13]. Deep learning methods are widely used because of their improved per-formance compared to traditional methods. In contrast to traditional methods and machine learning methods, the features need not be hand-picked. By changing the parameters and configurations of the deep learning convolutional neural network (CNN) architecture, a model can be trained to learn the best possible features for the dataset in use. Re-searchers have used deep learning methods to explore the field of * Corresponding author. E-mail addresses: ns1808900@qu.edu.qa (N. Subramanian), elharrouss.omar@gmail.com (O. Elharrouss), S_alali@qu.edu.qa (S. Al-Maadeed), mchowdhury@qu. edu.qa (M. Chowdhury). https://doi.org/10.1016/j.compbiomed.2022.105233 Received 6 June 2021; Received in revised form 10 January 2022; Accepted 10 January 2022  ComputersinBiologyandMedicine143(2022)105233Availableonline29January20220010-4825/©2022QatarUniversity.PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fN. Subramanian et al.                                                                                                             medical imaging even before the coronavirus pandemic. With the recent pandemic, the use of deep learning methods for the detection of coro-navirus infection from images has increased tremendously. A detailed survey of the available deep learning approaches for the detection of coronavirus infection from images such as CT scans or CXR images is conducted in this paper. Although other surveys are available in the literature, most of them cover a wider scope. For example, Ulhaq et al. [14] surveyed all methods that address coronaviruses, such as medical image processing, data science methods for pandemic modeling, AI and the Internet of things (IoT), AI for text mining and natural language processing (NLP), and AI in computational biology and medicine. This provides an overall view of what is happening in the research world. A survey on the application of computer vision methods for COVID-19 [15] described the segmentation of lung images. This paper aims to exclusively describe coronavirus detection methods using deep learning methods. In the hope of helping researchers develop better coronavirus detection methods, this paper summarizes all the methods that have been reported in the literature. Along with the methods, the used datasets, commonly used metrics for evaluation and comparison are discussed and future direction are elaborated in this paper. 2. Background Before discussing the details of the available methods for coronavirus infection detection, it is essential to have a working knowledge of deep convolutional neural networks and popular CNN architectures. In this section, a brief overview of CNN architectures and main points on available CNN architectures are presented. 2.1. Convolutional neural networks Convolutional neural networks, specifically artificial neural net-works, are a branch of deep learning methods that are inspired by the natural visual perception mechanism of living organisms [16]. CNNs are nothing but stacked multilayered neural networks. There are three major categories of layers, namely, convolutional layers, pooling layers and fully connected layers. The first layer of any CNN model is an input layer, where the width, height and depth of the input image are specified as the input parameters. Immediately after the input layer, convolu-tional layers are defined with the number of filters, filter window size, stride, padding and activation as the parameters. Convolutional layers are used to extract meaningful feature maps for the input location by calculating the weighted sum [17,18]. Then, each feature map is passed through an activation function, and bias is added to form the output. Usually, rectilinear unit (ReLU) activation is used as the activation function [19]. Pooling layers are used to reduce the size of the output from the convolutional layers. As the model increases in size with an increasing number of filters in the convolutional layer, the output dimensionality also increases exponentially, which makes it hard for computers to handle. Pooling layers are added to reduce the dimensions for easy computation and sometimes to suppress noise. The pooling layer can be a max pooling, average pooling, global average pooling, or spatial pooling layer. The most commonly used pooling layer is a max pooling layer [20]. The output is flattened to form a single-array feature vector, which is fed to a fully connected layer. Finally, a classification layer is defined with activation functions such as sigmoid, softmax and tanh functions [21]. The number of classes is specified in this layer, and the extracted features are aggregated into class scores. Batch normalization layers are applied after the input layer or after the activation layers to standardize the learning process and reduce the training time [22]. Another important parameter is the loss function, which summarizes the error in the predictions during training and validation. The loss is backpropagated to the CNN model after each epoch to enhance the learning process [23]. 2.2. Transfer learning and fine-tuning After designing, creating and building a deep learning model, the number of epochs is set to start training. During training, random weights are initialized, which will be refined during each epoch to make the result closer to the classification score. However, in transfer learning, instead of using random weight values, the model can be initialized with weight values from pretrained models. Transfer learning performs best when there is a limited availability of training data. When performing transfer learning, the last layer of",
            {
                "entities": [
                    [
                        191,
                        212,
                        "AUTHOR"
                    ],
                    [
                        215,
                        231,
                        "AUTHOR"
                    ],
                    [
                        232,
                        250,
                        "AUTHOR"
                    ],
                    [
                        251,
                        270,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptNeurocomputing. Author manuscript; available in PMC 2017 January 29.Published in final edited form as:Neurocomputing. 2016 January 29; 175(Pt A): 40–46. doi:10.1016/j.neucom.2015.09.103.The general critical analysis for continuous-time UPPAM recurrent neural networks*Chen Qiao†, Wen-Feng Jing‡, Jian Fang§, and Yu-Ping Wang¶Chen Qiao: qiaochen@mail.xjtu.edu.cn, cqiao@tulane.edu; Wen-Feng Jing: wfjing@mail.xjtu.edu.cn; Jian Fang: jfang3@tulane.edu; Yu-Ping Wang: wyp@tulane.edu†School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, 710049, P.R. China and with the Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA‡School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, 710049, P.R. China§School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, 710049, P.R. China and with the Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA¶Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA and the Center of Genomics and Bioinformatics, Tulane University, New Orleans, LA, 70112, USAAbstractThe uniformly pseudo-projection-anti-monotone (UPPAM) neural network model, which can be considered as the unified continuous-time neural networks (CNNs), includes almost all of the known CNNs individuals. Recently, studies on the critical dynamics behaviors of CNNs have drawn special attentions due to its importance in both theory and applications. In this paper, we will present the analysis of the UPPAM network under the general critical conditions. It is shown that the UPPAM network possesses the global convergence and asymptotical stability under the general critical conditions if the network satisfies one quasi-symmetric requirement on the connective matrices, which is easy to be verified and applied. The general critical dynamics have rarely been studied before, and this work is an attempt to gain an meaningful assurance of general critical convergence and stability of CNNs. Since UPPAM network is the unified model for CNNs, the results obtained here can generalize and extend the existing critical conclusions for CNNs individuals, let alone those non-critical cases. Moreover, the easily verified conditions for general critical convergence and stability can further promote the applications of CNNs.KeywordsContinuous-time recurrent neural network; uniformly pseudo-projection-anti-monotone network; general critical condition; dynamical analysis*This research was supported by NSFC No. 11101327, No. 11471006 and No. 11171270, and was partially supported by NIH R01 GM109068 and R01 MH104680.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.1 IntroductionPage 2The two basic elements of a recurrent neural network (RNN) are: the synaptic connections among the neurons and the nonlinear activation functions deduced from the input-output properties of the involved neurons. For applications such as associative memory, synaptic connections among the neurons are designed to encode the memories we hope to recover. The activation functions are assumed to capture the complex, nonlinear response of neurons of the brain. For different purpose of simulations and applications, both of them are preassigned before use. So understanding their properties are very important, and especially exploring the characteristics of the activation functions are quite crucial to determine the performance of the RNNs. For the commonly used RNN individuals, the activation functions are monotonically nondecreasing and saturated. To study and apply RNNs only based on such two features is far from enough. To overcome the non-thorough descriptions of activation functions, many special cases of activation functions have been brought forward, resulting in many different RNNs individuals. Furthermore, in order to obtain more useful results of RNNs, e.g., the convergence and stability of those individuals, additional strict requirements are unavoidable to impose on the networks for the lack of in-depth descriptions on the activation functions. Obviously, since those individuals are studied separately, it’s inevitable that there exist large numbers of redundancy of analysis for those individual models. In order to reduce the superabundance, establishing a harmonization methodology is a challenging work.In [16], Xu and Qiao put forward two novel concepts: uniformly anti-monotone as well as the pseudo-projection properties of the activation functions, which discover more essential characteristics other than the nondecreasing and bounded properties of the commonly used activation functions. It is shown that the proposed uniformly pseudo-projection anti-monotone (UPPAM) operator can embody most of activation operators (the precise definition of uniformly pseudo-projection-anti-monotone operator will be given in Section II), e.g., nearest-point projection, linear saturating operator, signum operator, symmetric multi-valued step operator, multi-threshold operator, winner-take-all operator, etc. Thus, the UPPAM operator can be considered as a framework of formalizing most of the activation operators of RNNs.In this paper, we use the concept of UPPAM operators to establish a unified model for continuous-time RNNs. Let’s consider the following continuous-time UPPAM RNNs mdoel:(1)where x(t) = (x1(t), x2(t), ···, xN(t))T is the neural network state, G = (g1, g2, ···, gN)T is the nonlinear activation operator deduced from all the activation functions gi, and G owns the uniformly pseudo-projection-anti-monotone property. Both A and W are the connective weight matrices, b, q are two fixed external bias vectors and τ is the state feedback coefficient. The form of model (1) includes two basic kinds of continuous-time RNNs [17], i.e., the static RNNs and the local field RNNs. Furthermore, as proved in [16], most Neurocomputing. Author manuscript; available in PMC 2017 January 29.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 3activation operators are special cases of the UPPAM operator. So, model (1) can be considered as a unified model of continuous-time RNNs and can include almost all of the existing continuous-time RNNs specials [4], e.g., Hopfield-type neural networks, Brain-State-in-a-Box neural networks, Recurrent Back-propagation neural networks, Mean-field neural networks, Bound-constraints Optimization Solvers, Convex Optimization Solvers, Recurrent Correlation Associative Memories neural networks, Cellular neural networks, etc. In addition, since model (1) owns the essential characteristics of the activation functions, i.e., the uniformly anti-monotone as well as the pseudo-projection properties, it can be expected that the analysis of model (1), especially the dynamics analysis can give more in-depth results and provide the unified and concise characterization of the continuous-time RNNs models. The main purpose of this paper will focus on discovering some essential global convergence and stability for the unified model (1), i.e., the critical convergence and stability.For RNNs, one difficult problem of dynamics analysis lies in the critical analysis. Define a discriminant matrixwhere Γ is a positive definite diagonal matrix, P is a diagonal matrix defined by the network, and W and A are the weight matrices. If there exists a positive definite diagonal matrix Γ, such that S(Γ, 2Λ − B) > 0 (i.e., S(Γ, 2Λ − B) is positive definite), where Λ and B are the anti-monotone and pseudo-projection constant matrices of the network (the definitions of them are given in Section II), then RNNs have exponential stability [4]. Many stability results have been achieved for RNNs individuals under various specifications of S(Γ, 2Λ − B) > 0 (typically, when S(Γ, 2Λ − B) > 0 is an M-matrix), and they are called as the non-critical dynamical analysis [1]. On the other hand, if there exists a positive definite diagonal matrix Γ such that S(Γ, V) is negative definite, here V = diag{r1, r2, ···, rN} with each ri > 0 being the maximum inversely Lipschitz constant of gi (i.e., for all s, t ∈ ℛN, |gi(t) − gi(s)| ≥ ri|t − s|), then RNNs are globally exponentially unstable [1, 7]. Since S(Γ, 2Λ − B) > 0 is the sufficient condition on the globally exponential stability of RNNs, and S(Γ, V) ≥ 0 is the necessary condition for RNNs to be globally stable, it is quite natural to explore the gap between S(Γ, 2Λ − B) ≤ 0 (i.e., S(Γ, 2Λ − B) is negative semi-definite) and S(Γ, V) ≥ 0 (i.e., S(Γ, V) is positive definite). Such a gap is called the general critical condition, and the dynamics analysis of RNNs under such condition is referred as the general critical dynamics analysis.For any application and practical design of RNNs, such as pattern recognition, associative memories, or as optimization solvers, the convergence and stability of RNNs are both prerequisite. For instance, when an RNN is used in associative memory or pattern recognition, any pattern we hope to store has to be an equilibrium point of the RNN. In addition, to ensure that each stored pattern can be retrieved even with noises, each equilibrium point must possess the stability. When the RNN is employed as an optimization solver, the possible optimal solutions correspond to the equilibrium of the RNN, and the Neurocomputing. Author manuscript; available in PMC 2017 Janua",
            {
                "entities": [
                    [
                        377,
                        391,
                        "AUTHOR"
                    ],
                    [
                        478,
                        492,
                        "AUTHOR"
                    ],
                    [
                        393,
                        403,
                        "AUTHOR"
                    ],
                    [
                        518,
                        528,
                        "AUTHOR"
                    ],
                    [
                        409,
                        422,
                        "AUTHOR"
                    ],
                    [
                        548,
                        561,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Nonconvulsive Epileptic Seizure Monitoring with IncrementalLearningYissel Rodr´ıguez Aldana1,2, Enrique J. Mara ˜n ´on Reyes1, Frank Sanabria Macias3, Valia Rodr´ıguezRodr´ıguez4,5,6, Lilia Morales Chac ´on7, Sabine Van Huffel2, and Borb´ala Hunyadi2,81Universidad de Oriente, Center of Neuroscience and Signals and Image Processing. Santiago de Cuba, Cuba. email:yraldana@gmail.com2KU Leuven, Department of Electrical Engineering (ESAT), Stadius Center for Dynamical Systems, Signal Processing and Data Analytics.3Escuela Polit´ecnica Superior, Universidad de Alcal´a, Alcal´a de Henares, SpainLeuven, Belgium.4Aston University. Birmingham, United Kingdom.5Cuban Neuroscience Center. Havana, Cuba.6Clinical-Surgical Hospital “Hermanos Almeijeiras”. Havana, Cuba7International Center of Neurological Restoration. Havana, Cuba.8Department of microelectronics, Delft University of Technology. Delft, NetherlandsSeptember 2, 20191 AbstractNonconvulsive epileptic seizures (NCSz) and nonconvulsive status epilepticus (NCSE) are two neurological entitiesassociated with increment in morbidity and mortality in critically ill patients. In a previous work, we introduced amethod which accurately detected NCSz in EEG data (referred here as ‘Batch method’). However, this approach wasless effective when the EEG features identified at the beginning of the recording changed over time. Such patterndrift is an issue that causes failures of automated seizure detection methods. This paper presents a support vectormachine (SVM)-based incremental learning method for NCSz detection that for the first time addresses the seizureevolution in EEG records from patients with epileptic disorders and from ICU having NCSz. To implement the in-cremental learning SVM, three methodologies are tested. These approaches differ in the way they reduce the set ofThis work has been supported by the Belgian foreign Affairs-Development Cooperation through VLIR-UOS (2013-2019) (Flemish Interuniver-sity Council-University Cooperation for Development) in the context of the Institutional University Cooperation program with Universidad deOriente.The research leading to these results has received funding from imec funds 2017 and the European Research Council under the European Union’sSeventh Framework Programme (FP7/2007-2013) / ERC Advanced Grant: BIOTENSORS (no.339804). This paper reflects only the authors’ viewsand the Union is not liable for any use that may be made of the contained information.1\fpotentially available support vectors that are used to build the decision function of the classifier. To evaluate thesuitability of the three incremental learning approaches proposed here for NCSz detection, first, a comparative studybetween the three methods is performed. Secondly, the incremental learning approach with the best performance iscompared with the Batch method and three other batch methods from the literature. From this comparison, the incre-mental learning method based on maximum relevance minimum redundancy (MRMR IL) obtained the best results.MRMR IL method proved to be an effective tool for NCSz detection in a real-time setting, achieving sensitivity andaccuracy values above 99%.Keywords Nonconvulsive epileptic seizures, Hilbert Huang Transform, Multiway Data Analysis, IncrementalLearning.2IntroductionNonconvulsive epileptic seizures (NCSz) and nonconvulsive status epilepticus (NCSE) are two related neurologicalentities that are frequently found in critically ill patients [1, 2]. Despite the nonconvulsive nature, they are associatedwith increment in morbidity and mortality in the intensive care unit (ICU). Since NCSz/NCSE present subtle or noovert clinical signs, it is not uncommon that in patients with altered mental status or coma they remain unnoticed anduntreated for long periods of time. Studies carried out in this population have reported though that NCSE lastingmore than 10 hrs are associated with permanent disabilities, while mortality is very high in NCSE lasting more than20 hrs [3].When suspected, NCSz/NCSE diagnosis is carried out using continuous EEG (cEEG) monitoring. However,several studies have reported that the likelihood to detect the first NCSz increases in patients at risk (i.e.: comatosepatients and children) when EEG is recorded for more than 24 hours [4]. Hence, seizure detection in ICU couldbe an exhausting and time-consuming process. To assist on the visual identification of changes, quantitative trendssummarizing EEG amplitude and frequency composition as well as annotating the presence of seizures have beenrecently introduced to the continuous EEG monitoring technique.Previous algorithms developed for NCSz detection combined wavelet analysis [5, 6, 7, 8], entropy [9, 10], nonlinearparameters [6, 9, 11], statistical and spectral features of the EEG [9, 10, 11, 12, 13, 14] with various machine learningtechniques [8, 9, 10, 11, 13, 15] or thresholds [6, 7, 12, 14] to detect the NCSzs. These algorithms obtained a reasonablesensitivity (over 90% in most cases) during the test process [16]. Among the methods with better results in the contextof NCSz detection in patients with epileptic etiology are those proposed by Kollialil at al (2013) [9], Sharma et al (2014)[8] and Fatma et al (2016) [14].Figures 1, 2 and 3 display the block diagrams of Kollialil’s, Sharma’s and Fatma’s methods respectively. As canbe appreciated, the cited methods iterate over the EEG channels at least until the feature extraction step. This meansthe features are extracted from a single channel without considering important characteristics of the seizure as thesynchronization and spread out/in of the seizure activity over the EEG channels [17]. These methods intended toexploit the possible cross information of the channels by combining the features computed individually into oneclassifier, as in Kollialil’s method, or imposing hard thresholds, as in Sharma’s and Fatma’s methods.Kollialil et al. proposed a patient independent training for a linear SVM. The NCSz characteristics vary enor-mously across patients. This implies that the number of patterns used to train a patient independent classifier must2\fFigure 1: Block diagram of the method proposed by Kollialil et al.[9] absence seizure detection. The proposaluses a multiclass SVM and a single feature vector computed from the fifth detail wavelet coefficients. Features asenergy, mean energy, entropy, mean cross-correlation, mean curve length, the coefficient of variation, interquartilerange (IQR) and median absolute deviation (MAD) are compared to obtain an optimal single feature. The data forthis experiment consisted of normal, epileptic and interictal EEG data from 100 subjects, from a reputed NeurologyClinic. The best performances were found for the energy, entropy, MAD, and IQR with an accuracy above 95%. Thebest feature was the IQR with an accuracy value of 99.66 %.be quite large. Given the nonconvulsive nature of NCSz the existent databases are small. With such databases it is notlikely to successfully train a classifier capable to generalize the acquired knowledge to other NCSz data, especiallyin case the data originate from ICU patients. Taking into account this characteristic of the NCSz, Sharma et al. andFatma et al. proposed patient specific methods. However, their approaches ignore the fact that the EEG patternspresent in a specific record could also differ. Having this in mind, the methods proposed by Sharma et al. and Fatmaet al. cannot guarantee to maintain their performance in longer records where these changes are more likely to occur.There are two main drawbacks in the methods proposed in the literature for NCSz detection. First, they employ apatient independent training of the classifiers and, second, thresholds for the detection are arbitrarily set. In general,the duration of the seizure and the number of channels displaying seizure activity are the most popular thresholdingcriteria [7, 15, 13, 8]. If the seizure is too short in time or affects just a few channels it is not detected. Furthermore,NCSz characteristics vary across patients. EEG patterns present in a specific record could also differ depending on thepatient disease’s etiology. Therefore, a threshold or classifier which works for one patient will not necessarily workfor another. Additionally, a more meaningful description of the seizure’s spatial localization should be considered,for instance its whole head topography instead of its distribution in a limited number of channels.In [18] we proposed a patient-specific method that mitigates the need of thresholds to detect the NCSz. Thismethod identifies the NCSz by exploiting the similarity between the first NCSz detected by the physician on theEEG and the rest of the NCSz in the recording [19].The method expands the EEG using the Hilbert Huang Transform(HHT) into a third-order tensor. This multiway representation of the data exploits the EEG high-dimensional struc-ture by analyzing its spectral, temporal and spatial properties simultaneously. This is a fundamental difference com-pared to the methods of Sharma [8] and Fatma [14] : in our approach, multichannel information is integrated at the3\fFigure 2: Block diagram of the patient specific method proposed by Sharma et al. [8] for nonconvulsive epilepticseizures detection ((cid:93) denotes “number of ”). The proposed algorithm analyzed the EEG in epochs of 1-second du-ration. Each epoch was denoised using wavelet analysis applying cubic thresholding. The extracted features arethe IQR, the MAD, and the Normalized Covariance, normalized by the median of their background EEG features.The dataset used for testing consisted of 24 seizures recorded from the EEG of 9 subjects in the All India Institute ofMedical Sciences. This method requires the seizure activity to be present in at least 50% of the channels. Otherwise,the seizures will be missed. The method reported 100% of sensitivity and 99.3% of specificity.Figure 3: Block diagram of the patien",
            {
                "entities": [
                    [
                        126,
                        148,
                        "AUTHOR"
                    ],
                    [
                        208,
                        226,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Pattern Recognition 138 (2023) 109400 Contents lists available at ScienceDirect Pattern Recognition journal homepage: www.elsevier.com/locate/patcog Learning from multiple annotators for medical image segmentation Le Zhang a , b , 1 , Ryutaro Tanno d , 1 , Moucheng Xu b , Yawen Huang f , ∗, Kevin Bronik a , Chen Jin b , Joseph Jacob b , c , Yefeng Zheng f , Ling Shao g , Olga Ciccarelli a , Frederik Barkhof a , e , Daniel C. Alexander b , ∗a Queen Square Institute of Neurology, Faculty of Brain Sciences, University College London, London, WC1B 5EH, United Kingdom b Centre for Medical Image Computing, Department of Computer Science, University College London, London, WC1E 6BT, United Kingdom c UCL Respiratory, University College London, London, WC1E 6JF, United Kingdom d Healthcare Intelligence, Microsoft Research, Cambridge, CB1 2FB, United Kingdom e Amsterdam UMC, Vrije Universiteit Amsterdam, Department of Radiology and Nuclear Medicine, Amsterdam, Netherlands f Tencent Jarvis Lab, Shenzhen, China g Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates a r t i c l e i n f o a b s t r a c t Article history: Received 10 May 2022 Revised 18 December 2022 Accepted 5 February 2023 Available online 11 February 2023 Keywords: Multi-Annotator Label fusion Segmentation Supervised machine learning methods have been widely developed for segmentation tasks in recent years. However, the quality of labels has high impact on the predictive performance of these algorithms. This issue is particularly acute in the medical image domain, where both the cost of annotation and the inter-observer variability are high. Different human experts contribute estimates of the ”actual” segmen- tation labels in a typical label acquisition process, influenced by their personal biases and competency levels. The performance of automatic segmentation algorithms is limited when these noisy labels are used as the expert consensus label. In this work, we use two coupled CNNs to jointly learn, from purely noisy observations alone, the reliability of individual annotators and the expert consensus label distribu- tions. The separation of the two is achieved by maximally describing the annotator’s “unreliable behav- ior” (we call it “maximally unreliable”) while achieving high fidelity with the noisy training data. We first create a toy segmentation dataset using MNIST and investigate the properties of the proposed algorithm. We then use three public medical imaging segmentation datasets to demonstrate our method’s efficacy, including both simulated (where necessary) and real-world annotations: 1) ISBI2015 (multiple-sclerosis lesions); 2) BraTS (brain tumors); 3) LIDC-IDRI (lung abnormalities). Finally, we create a real-world mul- tiple sclerosis lesion dataset (QSMSC at UCL: Queen Square Multiple Sclerosis Center at UCL, UK) with manual segmentations from 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label). In all datasets, our method consistently outperforms competing methods and relevant baselines, especially when the number of annotations is small and the amount of disagreement is large. The studies also reveal that the system is capable of capturing the complicated spatial characteristics of annotators’ mistakes. © 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) 1. Introduction The performance of downstream supervised machine learning models is known to be influenced by substantial inter-reader vari- ability when segmenting anatomical structures in medical images [26] . This issue is especially acute in the medical image domain, ∗ Corresponding authors. E-mail addresses: yawenhuang@tencent.com (Y. Huang), d.alexander@ucl.ac.uk (D.C. Alexander) . 1 Authors contributed equally. where labelled data is commonly scarce due to the high cost of annotations. For instance, because of the heterogeneity in lesion lo- cation, size, shape, and anatomical variability across patients [29] , accurate identification of multiple sclerosis (MS) lesions in MRIs is difficult even for experienced experts. Another example [21] shows that glioblastoma (a kind of brain tumour) segmentation had an average inter-reader variability of 74–85%. Segmentation annota- tions of structures in medical image suffer from substantial anno- tation variations, which is exacerbated by disparities in biases and level of expertise [18] . As a result, despite the current quantity of medical imaging data due to almost two decades of digitisation, the world still lacks access to data with curated labels that can be https://doi.org/10.1016/j.patcog.2023.109400 0031-3203/© 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \fL. Zhang, R. Tanno, M. Xu et al. Pattern Recognition 138 (2023) 109400 used by machine learning [14] , necessitating the use of intelligent algorithms to learn robustly from such noisy annotations. Different pre-processing techniques are often used to curate segmentation annotations by fusing labels from different experts in order to minimise inter-reader differences. The most basic and widely used approach is based on a majority vote, with the most representative expert opinion being treated as the expert consen- sus label. In the aggregation of brain tumour segmentation labels, a smarter variant [21] that accounts for class similarity has proven effective. However, one major limitation with such approaches is that all experts are presumed to be equally trustworthy [25] . proposed a label fusion approach, which is called STAPLE. This method explic- itly models individual expert reliability and uses that knowledge to ”weight” their judgments in the label aggregation step. STAPLE has been the go-to label fusion method in the construction of pub- lic medical image segmentation datasets, such as ISLES [27] , MSSeg [11] , and Gleason’19 [12] datasets, after demonstrating its superior- ity over traditional majority-vote pre-processing in various appli- cations. Asman further extended this strategy in [4] by accounting for voxel-wise consensus to solve the issue of annotators’ reliabil- ity being under-estimated. Another extension [5] was proposed to model the annotator’s reliability across different pixels in images. More recently, STAPLE has been modified in numerous ways to en- code the information of the underlying images into the label aggre- gation process in the context of multi-atlas segmentation problems [2,16] where image registration is used to warp segments from la- belled images (”atlases”) onto a new scan. STEP, which is a way to further incorporate the local morphological similarity between atlases and target images in [8] , is a notable example, and sev- eral extensions of this approach, such as [1,6] , have subsequently been examined. However, all of the previous label fusion methods have one major limitation: they don’t have a way to integrate in- formation from distinct training images. This severely restricts the scope of applications to situations in which each image has a rea- sonable number of annotations from multiple experts, which can be prohibitively expensive in practise. Moreover, to model the re- lationship between observed noisy annotations, expert consensus label and reliability of experts, relatively simplistic functions are utilized, which may fail to capture complex characteristics of hu- man annotators. In this paper, we introduce and fully evaluate an unique end- to-end segmentation approach that predicts the reliability of mul- tiple human annotators and the expert consensus label based on noisy labels alone. We use the Morpho-MNIST framework [9] to perform morphometric operations on the MNIST dataset to simu- late a variety of annotator types for evaluation. We also demon- strate the potential in several public medical imaging datasets, namely (i) MS lesion segmentation dataset (ISBI2015) from the ISBI 2015 challenge [7] , (ii) Brain tumour segmentation dataset (BraTS) [21] and (iii) Lung nodule segmentation dataset (LIDC-IDRI) [3] . Furthermore, we create a practical MS lesion segmentation dataset with 4 different annotators (3 radiologists with different level skills and 1 expert to generate the expert consensus label) to evaluate our model’s performance in real-world data. Experiments on all datasets demonstrate that our method consistently leads to bet- ter segmentation performance compared to widely adopted label- fusion methods and other relevant baselines, especially when the number of available labels for each image is low and the degree of annotator disagreement is high. The main contributions of our approach are: (1) A novel deep CNN architecture is proposed for jointly learn- ing the expert consensus label and the annotator’s label. The pro- posed architecture ( Fig. 1 ) consists of two coupled CNNs where one estimates the expert consensus label probabilities and the other 2 models the characteristics of individual annotators (e.g., tendency to over-segmentation, mix-up between different classes, etc) by es- timating the pixel-wise confusion matrices (CMs) on a per image basis. Unlike STAPLE [25] and its variants, our method models, and disentangles with deep neural networks, the complex mappings from the input images to the annotator behaviours and to the ex- pert consensus label. (2) The parameters of our CNNs are “global variables” that are optimised across different image samples; this enables the model to disentangle robustly the annotators’ mistakes and the expert consensus label based on correlations between similar image sam- ples, even when the number of available annotations is small per image (e.g., a single annotation per image). In contrast, this would not be possible with STAPLE [25] and its variants [5,8] where the annotators’ pa",
            {
                "entities": [
                    [
                        213,
                        222,
                        "AUTHOR"
                    ],
                    [
                        234,
                        248,
                        "AUTHOR"
                    ],
                    [
                        256,
                        268,
                        "AUTHOR"
                    ],
                    [
                        272,
                        284,
                        "AUTHOR"
                    ],
                    [
                        291,
                        304,
                        "AUTHOR"
                    ],
                    [
                        308,
                        317,
                        "AUTHOR"
                    ],
                    [
                        321,
                        334,
                        "AUTHOR"
                    ],
                    [
                        342,
                        355,
                        "AUTHOR"
                    ],
                    [
                        359,
                        369,
                        "AUTHOR"
                    ],
                    [
                        373,
                        389,
                        "AUTHOR"
                    ],
                    [
                        393,
                        410,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Edith Cowan University Edith Cowan University Research Online Research Online Research outputs 2022 to 2026 1-1-2022 A review of arthritis diagnosis techniques in artificial intelligence A review of arthritis diagnosis techniques in artificial intelligence era: Current trends and research challenges era: Current trends and research challenges Maleeha Imtiaz Edith Cowan University Syed Afaq Ali Shah Edith Cowan University, afaq.shah@ecu.edu.au Zia ur Rehman Follow this and additional works at: https://ro.ecu.edu.au/ecuworks2022-2026 Part of the Diseases Commons 10.1016/j.neuri.2022.100079 Imtiaz, M., Shah, S. A. A., & ur Rahman, Z. (2022). A review of arthritis diagnosis techniques in artificial intelligence era: Current trends and research challenges. Neuroscience Informatics, 2, Article 100079. https://doi.org/10.1016/j.neuri.2022.100079 This Journal Article is posted at Research Online. https://ro.ecu.edu.au/ecuworks2022-2026/1430 \fNeuroscience Informatics 2 (2022) 100079Contents lists available at ScienceDirectNeuroscience Informaticswww.elsevier.com/locate/neuriArtificial Intelligence in Brain InformaticsA review of arthritis diagnosis techniques in artificial intelligence era: Current trends and research challengesMaleeha Imtiaz a, Syed Afaq Ali Shah b,∗a Joondalup Health Campus, Australiab Centre for AI and Machine Learning, Edith Cowan University, Australiac Cairns Hospital, Cairns, QLD, Australia, Zia ur Rehman ca r t i c l e i n f oa b s t r a c tArticle history:Received 2 January 2022Received in revised form 4 May 2022Accepted 12 May 2022Keywords:Machine learning (ML)Deep learningOsteoarthritisRheumatoid arthritisDeep learning, a branch of artificial intelligence, has achieved unprecedented performance in several domains including medicine to assist with efficient diagnosis of diseases, prediction of disease progression and pre-screening step for physicians. Due to its significant breakthroughs, deep learning is now being used for the diagnosis of arthritis, which is a chronic disease affecting young to aged population. This paper provides a survey of recent and the most representative deep learning techniques (published between 2018 to 2020) for the diagnosis of osteoarthritis and rheumatoid arthritis. The paper also reviews traditional machine learning methods (published 2015 onward) and their application for the diagnosis of these diseases. The paper identifies open problems and research gaps. We believe that deep learning can assist general practitioners and consultants to predict the course of the disease, make treatment propositions and appraise their potential benefits.© 2022 The Author(s). Published by Elsevier Masson SAS. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).1. IntroductionArthritis is a term which is used for various inflammatory con-ditions that affect different parts of the body such as joints, bones, and muscles. It can be of several types such as Osteoarthritis (OA), Rheumatoid Arthritis (RA), juvenile Arthritis, psoriatic arthri-tis, and gouty Arthritis, which can result in stiffness, pain, redness and swelling in the joints [47]. According to [5], it has been re-vealed that about 3.6 million (15%) of people are affected from arthritis which includes 17.9% females and 12.1% males. Moreover, 62% of patients affected from arthritis had Osteoarthritis, 12.7% had rheumatoid arthritis, and 32.1% had suffered from an unspeci-fied form of arthritis. One in every seven Australians has Arthritis [6]. The prevalence of arthritis rises with age, primarily affecting the females (ABS, 2017). Moreover, higher mortality risk is also recorded in patients with rheumatoid arthritis (RA) as compared to the general population [22], [52].Rheumatic diseases are chronic and fluctuating in nature, in-volving complicated and unclear etiology, which further intricates the treatment of this kind of arthritis [12], [57], [52]. Regardless, even from the invention of various biological and synthetic treat-ments for rheumatoid arthritis (RA), the decrease in disease pro-* Corresponding author.E-mail address: afaq.shah@ecu.edu.au (S.A.A. Shah).gression is achieved only in a small subset of patients [23], [36]. Moreover, the clinical experiments for another rheumatic disease that is Osteoarthritis (OA) are not very fruitful due to different disease phenotypes involved in the disease. Therefore, the dis-ease diagnosis at an early stage can slow down its progression, where diagnosis involves numerous imaging modalities such as X-rays, MRI and CT. However, diagnosis techniques, such as Kellgren-Lawrence (KL) grade suffer from subjectivity, as their accuracy heavily depends on the practitioner’s experience [65]. Table 1 pro-vides details of Kellgren and Lawrence grading for completion. In order to make the diagnosis process more systematic and reli-able, computer-aided analysis and predictive modelling is required to overcome the human errors and for early disease detection in places where there are fewer experts available.In addition, to advent an appropriate treatment for arthritis, a data-intensive investigation is essential where artificial intelligence (AI) can play a significant role in disease detection. Exclusively, ma-chine learning (ML), a subfield of AI aims to design data-driven predictive models which possess the ability to learn from the ex-perience regardless of the rules explicitly specified by individuals [23]. It uses methods, algorithms and processes to expose con-cealed associations within data and to produce prescriptive, de-scriptive and predictive tools in order to exploit these associations [27]. Additionally, the advancement of Machine Learning principles and Artificial Intelligence techniques has increased the productiv-https://doi.org/10.1016/j.neuri.2022.1000792772-5286/© 2022 The Author(s). Published by Elsevier Masson SAS. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\fM. Imtiaz, S.A.A. Shah and Z. ur RehmanNeuroscience Informatics 2 (2022) 100079Table 1Kellgren and Lawrence (KL) grading.KL gradeDiagnosis01234No features of osteophytes are presentNarrowing of joint space, doubtful OACertain narrowing of joint space, minor OAMultiple osteophytes, sure joint space narrowing and some sclerotic areas, moderate OALarge osteophytes, severe joint space narrowing, severe sclerosis and bone deformity, Severe OAity and effectiveness in medical imaging research [55]. Machine learning concepts, when applied to medical data, have great po-tential to improve disease diagnosis and early detection of diseases [11], [48], [57], [8]. In clinical settings, these techniques can help medical experts to analyse the disease in a better way to predict potential future issues and treat patients more effectively.Machine learning algorithms are capable of learning useful data representations automatically [40], [50]. They can deal with a vari-ety of data inputs such as genetic information, text e.g., electronic health records, patient cohorts and medical images. Furthermore, it can also learn from the knowledge available from clinical data and generate outcomes by recognising disease patterns, and features. Further, it can also help in optimising treatment strategies. Hence, it is quite evident that ML has helped in significantly filling the gap of automatic learning from clinical experience. Furthermore, Deep learning (DL), is a subfield of ML, which utilises multi-layered neu-ral networks, intensive computational algorithms and big data [23], [46]. Over the last decade, both ML and DL have been used in the field of medicine for medical imaging, and it has been depicted that ML-based decision-making is superior to physicians’ individ-ual clinical trial decisions [23].Inspired by the recent advent of artificial intelligence in medical field, this paper presents a survey of deep learning and traditional machine learning techniques for the diagnosis of osteoarthritis and rheumatoid arthritis. The paper also aims to identify the current challenges and open research problems in this area. In contrast to current review papers [23], [55], [24], [28] which mainly focus on a specific type of arthritis e.g., OA or RA and machine learning tech-niques only, this paper reviews deep learning as well as machine learning methods for the diagnosis of both OA and RA. In addition, this paper also provides detailed information about the publicly available datasets for RA and OA research (Section 4.2). This makes our survey paper different from the existing review articles.The rest of the paper is organised as follows. Section 2 discusses most common arthritis types. Overview of the most popular ma-chine and deep learning techniques is presented in Section 3. An overview of imaging techniques and arthritis datasets is provided in Section 4. Machine learning and deep learning approaches for the diagnosis of arthritis are presented in Section 5 and 6, respec-tively. Section 7 discusses some of the open research problems and research challenges. The paper is concluded in Section 8.2. Arthritis and its typesArthritis is a degenerative disorder associated with human joints that can result in disability. There are numerous types of arthritis such as rheumatoid arthritis, Osteoarthritis, Juvenile Arthritis, Psoriatic arthritis and gout arthritis. In the following, we will briefly discuss rheumatoid arthritis, osteoarthritis and Psori-atic arthritis.2.1. Rheumatoid arthritisRheumatoid Arthritis (RA) is an autoimmune inflammatory dis-order which involves multiple organs affecting one or more joints [27]. It is a disease with unclear etiology and a combination of ge-netic and environmental factors. The complex interactions of these factors affect disease development and progression [29]. In gen-eral, RA is categorised through morning stiffness and inflammation of joints that requires skills and experience f",
            {
                "entities": [
                    [
                        344,
                        359,
                        "AUTHOR"
                    ],
                    [
                        382,
                        401,
                        "AUTHOR"
                    ],
                    [
                        1256,
                        1275,
                        "AUTHOR"
                    ],
                    [
                        446,
                        460,
                        "AUTHOR"
                    ],
                    [
                        1428,
                        1442,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "llOPEN ACCESSPerspectivePatient and public involvement to build trust in artificialintelligence: A framework, tools, and case studiesSoumya Banerjee,1,* Phil Alsop,1,2 Linda Jones,1 and Rudolf N. Cardinal11University of Cambridge, Cambridge, UK2Cambridge, Cambridgeshire, UK*Correspondence: sb2333@cam.ac.ukhttps://doi.org/10.1016/j.patter.2022.100506THE BIGGER PICTURE Hype and negative news reports about artificial intelligence (AI) abound. Involving pa-tients in healthcare AI projects may help in adoption and acceptance of these technologies. We argue that AIalgorithms should be co-designed with patients and healthcare workers.We show examples of how to involve patients in AI research and how patients can build trust in algorithms.We share some best practices, case studies, a framework, and computational tools.Avenues for future work include guidelines for patient and public involvement in AI healthcare research forfunding bodies and regulatory agencies.An understanding of what AI can and cannot do, and a realistic appraisal of risks and benefits, may help inadoption and democratize access to AI for healthcare.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYArtificial intelligence (AI) is increasingly taking on a greater role in healthcare. However, hype and negativenews reports about AI abound. Integrating patient and public involvement (PPI) in healthcare AI projectsmay help in adoption and acceptance of these technologies.We argue that AI algorithms should also be co-designed with patients and healthcare workers.We specifically suggest (1) including patients with lived experience of the disease, and (2) creating a researchadvisory group (RAG) and using these group meetings to walk patients through the process of AI model build-ing, starting with simple (e.g., linear) models.We present a framework, case studies, best practices, and tools for applying participative data science tohealthcare, enabling data scientists, clinicians, and patients to work together. The strategy of co-designingwith patients can help set more realistic expectations for all stakeholders, since conventional narratives of AIrevolve around dystopia or limitless optimism.INTRODUCTIONMachine learning is increasingly becoming pervasive in health-care. Artificial intelligence (AI) is increasingly taking on a greaterrole in healthcare, especially during the current coronavirus dis-ease 2019 (COVID-19) pandemic.1 However, hype and negativenews reports about AI abound.People do not always understand or trust AI. This overlapswith other concerns people have, such as the security of theirdata. People are not always consulted about AI that might affectthem. Part of the solution is patient and public involvement (PPI).In PPI, the general public and patients are involved in research.The level of involvement varies from project to project. Beinginvolved in a project helps build trust. There is a rich history ofPPI in healthcare. However, it has not been done very much inthe context of modern AI.As misinformation spreads around AI, integrating patient andpublic involvement in healthcare AI projects and clinical trialsmay help in adoption and acceptance of these technologies.We argue that AI software should also be co-designed with pa-tients and that patients should be involved in discussions aroundAI research applied to healthcare.We advocate a collaborative approach where patients, carers,clinicians, and data scientists work together to decide what datawill be used as inputs to computer programs and understandwhy these algorithms made a particular prediction. Recent studieshave raised awareness about designing AI algorithms in closecollaboration with healthcare workers.1 Machine learning re-searchers alone may not be able to appreciate the broader impactof their work and there is a need to involve other stakeholders.2We give examples of work we have done in this area as casestudies (in the section ‘‘case studies: examples of data-focusedPatterns 3, June 10, 2022 ª 2022 The Author(s). 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).\fllOPEN ACCESSresearch via a research advisory group’’), and make some gen-eral recommendations (in the sections ‘‘framework for buildingtrust and typical patient concerns’’ and ‘‘recommendations’’).We suggest a framework of how patients can build trust in AIand we share tools and resources that can be used to explainthe basics of AI to patients. We developed tools to demonstratekey concepts to the public (section ‘‘tools for outreach andinvolvement’’). We also review the current literature on trust inAI (section subsection ‘‘trust in AI and the role of PPI’’). Wehope that the approach of involving patients, clinicians, anddata scientists in a virtuous cycle of co-design will be used infuture AI projects in healthcare.CASE STUDIES: EXAMPLES OF DATA-FOCUSEDRESEARCH VIA A RESEARCH ADVISORY GROUPIn this section, we describe two projects as case studies. In latersections, we reflect on these projects and present our generalrecommendations.We were conducting research in this area, so we recruited pa-tients and formed a research advisory group (RAG). The RAGmet regularly and discussed data-focused research projectsrelated to severe mental illness. Additional details on the RAGare available in the section ‘‘framework for building trust andtypical patient concerns.’’Analysis of the effect of lithium medication on kidneyfunctionIn this section we describe a patient-led project. Patients withbipolar disorder are sometimes prescribed lithium. Lithium isan effective medication, but long-term use may lead to kidneydamage. A patient in the RAG had suggested looking at hos-pital data to investigate if discontinuing lithium can helprecover kidney function in patients with bipolar disorder takinglithium.The patient was involved in all stages of a researchproject. Our aim was to predict whether stopping lithiumintake, in patients with bipolar disorder (who have been onthe medication for a long time), is associated with reversalof drug-induced renal damage.We used observational from hospital electronic healthcare re-cords systems to answer these questions. We outline the variousdatasets that were used in this work:(1) EPIC prescription data. This is an electronic patient recordsystem operationalin Cambridge University Hospitals (CUH)from October 2014 until present. This system captures all CUH ac-tivity during its period of operation. This includes laboratory testsand prescriptions (these are recorded typically for inpatientsonly) and structured diagnostic codes (for a subset of patientsand a subset of diagnoses). This has features like age, gender,and ethnicity.(2) Meditech data. This is a laboratory system operational inCUH from 1995 until present. This system captures all laboratoryinvestigations data from CUH during its period of operation. Thishas laboratory results like creatinine.Patients with records in both EPIC and Meditech had their re-cords cross-matched before anonymization.We used the following linear mixed effects model (in the R pro-gramming language notation):2 Patterns 3, June 10, 2022PerspectiveeGFR = e0 + boff toff + bonton + ð1j pid + toff + tonÞ(Equation 1)where eGFR is the estimated glomerular filtration rate (eGFR)and is calculated from creatinine, age, gender, and ethnicity(data available from hospital electronic healthcare recordssystem) using the CKD-EPI formula.3 pid is the unique patientidentification numberin the electronic healthcare recordsystem. bon is the rate at which eGFR declines when a patientis on lithium. toffis the cumulative time spent off lithium, andton is the cumulative time spent on lithium. boffis the rate atwhich eGFR is declining for patients off lithium, and bon is therate at which eGFR is declining for patients on lithium. e0,bon, boff , ton, and toff are parameters that are estimated fromthe data.However, using these data on a few thousand patients, the re-sults were inconclusive. This motivated the need to go back tothe RAG and explain the need for more data. We took feedbackfrom patients as to whether we should apply for access to moredata. We also built a tool that explains how, in some cases,having more data can help in estimating parameters of statisticalmodels (see sections ‘‘tools for outreach and involvement’’ and‘‘framework for building trust and typical patient concerns’’).This process of performing research and getting inconclusiveresults also showed patients how research always takes timeand can lead to unexpected roadblocks.We also took the time to explain how to build statistical models.For example, we tried other, simpler formulations before we arrivedat the final model (see Equation 2). This showed patients how re-searchers always incrementally build more complex models.eGFR = e0 + boff toff + bonton + ð1j pidÞ(Equation 2)We explained these models using an example of a simplerlinear model:y = a,x + b(Equation 3)This is a linear model where the value of x is used to predict y(say eGFR). a and b are the parameters of the model, and thesecan be estimated. We explained that estimating means deter-mining the values of a and b from data. Once we explained theconcepts of a linear model, we progressed to more advancedconcepts like confidence intervals.We are currently validating ourresults in an additionalindependent cohort of patients (the Clinical Practice ResearchDatalink [CPRD] research database, which has general practicerecords from the United Kingdom4).At this stage, we also communicated to the patients a numberof caveats. Lithium is an effective medication for managingbipolar disorder,5 and the chances of patients developing renalcomplications is quite small.6 The benefit of discontinuing lithiumshould be carefully weighed against the risk of relapse of thepsychiatric disorder, as has been documented in case studies7and suggested in meta-analysis studies.6Our ",
            {
                "entities": [
                    [
                        152,
                        163,
                        "AUTHOR"
                    ],
                    [
                        167,
                        179,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fContents lists available at ScienceDirect Computers in Biology and Medicine journal homepage: www.elsevier.com/locate/compbiomed Computational screening of 645 antiviral peptides against the receptor-binding domain of the spike protein in SARS-CoV-2 Md Minhas Hossain Sakib a, Aktiya Anjum Nishat a, Mohammad Tarequl Islam a, Mohammad Abu Raihan Uddin a, Md Shahriar Iqbal a, Farhan Fuad Bin Hossen a, Mohammad Imran Ahmed a, Md Samiul Bashir a, Takbir Hossain a, Umma Sumia Tohura a, Saiful Islam Saif a, Nabilah Rahman Jui a, Mosharaf Alam a, Md Aminul Islam a, Md Mehadi Hasan a, Md Abu Sufian b, Md Ackas Ali a, Rajib Islam a, Mohammed Akhter Hossain c, Mohammad A. Halim d, 1,* a Division of Infectious Diseases and Division of Computer-Aided Drug Design, The Red-Green Research Centre, BICCB, 16 Tejkunipara, Tejgaon, Dhaka, 1215, Bangladesh b School of Pharmacy, Temple University, Philadelphia, PA, 19140, USA c Florey Institute of Neuroscience and Mental Health, University of Melbourne, Melbourne, Victoria 3010, Australia d Department of Physical Sciences, University of Arkansas-Fort Smith, Fort Smith, Arkansas 72913, USA  A R T I C L E I N F O  A B S T R A C T  Keywords: SARS-CoV-2 Antiviral peptide Molecular dynamics simulation Receptor-binding domain Angiotensin converting enzyme 2 The receptor-binding domain (RBD) of SARS-CoV-2 spike (S) protein plays a vital role in binding and inter-nalization through the alpha-helix (AH) of human angiotensin-converting enzyme 2 (hACE2). Thus, it is a po-tential target for designing and developing antiviral agents. Inhibition of RBD activity of the S protein may be achieved by blocking RBD interaction with hACE2. In this context, inhibitors with large contact surface area are preferable as they can form a potentially stable complex with RBD of S protein and would not allow RBD to come in contact with hACE2. Peptides represent excellent features as potential anti-RBD agents due to better efficacy, safety, and tolerability in humans compared to that of small molecules. The present study has selected 645 antiviral peptides known to inhibit various viruses and computationally screened them against the RBD of SARS- CoV-2 S protein. In primary screening, 27 out of 645 peptides exhibited higher affinity for the RBD of S protein compared to that of AH of the hACE2 receptor. Subsequently, AVP1795 appeared as the most promising candidate that could inhibit hACE2 recognition by SARS-CoV 2 as was predicted by the molecular dynamics simulation. The critical residues in RBD found for protein-peptide interactions are TYR 489, GLY 485, TYR 505, and GLU 484. Peptide-protein interactions were substantially influenced by hydrogen bonding and hydrophobic interactions. This comprehensive computational screening may provide a guideline to design the most effective peptides targeting the spike protein, which could be studied further in vitro and in vivo for assessing their anti- SARS CoV-2 activity.  1. Introduction A novel coronavirus is causing widespread respiratory tract in-fections and posing a serious threat to public life and health. Following a devastating first wave, it has emerged as even more dangerous and is causing havoc upon lives around the world. The international committee on taxonomy of viruses (ICTV) officially designated 2019 novel coronavirus (2019-nCov) as SARS-CoV-2 and the disease as COVID-19 [1,2]. This is the third time an animal to human transmission of deadly viruses has been witnessed in the past two decades [3]. The number of affected people and death toll due to COVID-19 are increasing day by day. As of July 26, 2021, 195 million people have been infected and 4.1 million are killed by this deadly virus [4]. SARS-CoV-2 is a positive sense, single-stranded, enveloped, non- * Corresponding author. E-mail address: mhalim1@kennesaw.edu (M.A. Halim).  1 Present Address: Department of Chemistry and Biochemistry, Kennesaw State University, Kennesaw, GA 30144, USA. https://doi.org/10.1016/j.compbiomed.2021.104759 Received 4 May 2021; Received in revised form 3 August 2021; Accepted 7 August 2021  ComputersinBiologyandMedicine136(2021)104759Availableonline10August20210010-4825/©2021ElsevierLtd.Allrightsreserved.\fM.M.H. Sakib et al.                                                                                                              segmented RNA virus belonging to the coronaviridae family [5]. The genome size of SARS-CoV-2 ranges from 29.8 kb to 29.9 kb [6], which encodes four main structural proteins, comprising of spike (S), envelop (E), membrane (M), nucleocapsid (N) proteins, and 16 non-structural proteins (nsp) [5,7]. S protein is a highly N-glycosylated trimeric pro-tein that covers the outer surface of SARS-CoV-2. Each monomer of S protein has a molecular weight of 180 kDa and consists of S1 and S2 subunits [8–10]. The S protein is involved in receptor recognition, membrane fusion, as well as the entry of the virus to host cells. It binds with human angiotensin-converting enzyme 2 (hACE2), which serves as the port of entry for the virus to host lung epithelial cells. hACE2 is found on the surface of many other cell types including epithelial tissues of The upper receptor-binding-domain (RBD) of S1 subunit binds with an alpha helix of the peptidase domain (PD) of the angiotensin-converting enzyme 2 (ACE2) [13,14], which subsequently triggers the fusion of viral and host cellular membrane by S2 subunit [15]. The RBD of S1 protein contains antiparallel β sheets (β1, β2, β3, β4 and β7) with short joining helices and loops forming the core. The shorter β5 and β6 strands, α4 and α5 helices, and loops are inserted between the β4 and β7 strands [16]. respiratory [11,12]. lower tracts and Drug design to counter COVID-19 can be directed towards targeting viral proteins or host-cell proteins. Designing drugs to target viral pro-teins have several benefits since they could be highly specific against the virus while maintaining minimal detrimental effects on host cells. Among the four structural proteins of SARS-CoV-2, design of inhibitors against S1 protein is an effective choice to arrest viral entry to the host cells, which is a key step in the virus infection cycle [17]. Evidently, SARS-CoV-2 exhibits a high nucleotide sequence similarity with SARS-CoV-1 (79.5%) as well as MERS (50%) [18]. Closest relatives, RaTG13-CoV and RmYN02-CoV, share 96.3% nucleotide identity in the whole genome sequence, ~97% nucleotide identity in the long 1 ab open reading frame (ORF1ab), respectively. Notably, S protein from SARS-CoV-1 and SARS-CoV-2 share critical residues within the RBD of the S1 subunit and bind to the same receptor, hACE2, for internalization [16,19,20]. Given the sequence similarity among different viruses, in-hibitors of one virus show great promise as potential therapeutics against others. Moreover, with similarities between critical residues of RBDs, inhibitors known to arrest SARS-CoV-1 entry offer high potential to halt SARS-CoV-2 entry as well. to superior The binding interface of SARS-CoV-2 and hACE2 shares a high contact surface and contains a hydrogen bonding network as well as a hydrophobic region [21]. Therefore, as potential inhibitors of RBD S1 protein, peptides are small molecules because peptide-protein interaction (PPI) has a large contact surface area [22]. As peptides have a molecular weights in between small molecules (<500 Da) and biologics (up to 150,000 Da), they exhibit some unique chemical features. Peptides are also amenable to chemical adjustment and can bind with PPIs that are therapeutically relevant [23]. This class of therapeutics have many advantages over small-molecule medications because they are highly selective, well-tolerated, have fewer side effects, and go through a shorter clinical development and FDA approval process [27,28]. Despite the challenges of short half-lives, rapid clearance, cost, and intravenous administration, several groups including ours are looking into the possibility of using an antiviral peptide to treat covid-19 [24–27]. In this regard, peptide like molecules can be an ideal solution to inhibit RBD S1 and potentially inhibit RBD-hACE2 interaction. In a previous study, the effectiveness of peptides against the S1 protein of SARS-CoV-1 was established [28]. In another study, a corresponding hexapeptide to the ACE-interacting domain of SARS-CoV-2 (AIDS) has been found to disrupt the association between RBD-ACE2 in mice [29]. Karoyan and colleagues designed peptide-mimics which has been found to inhibit S protein with inhibitory concentration (IC50) in nanomolar range [30]. In a previous work, we have computationally demonstrated that peptides known to inhibit RBD of SARS-CoV-1 S1 protein shows great promise against SARS-CoV-2 [31]. In this research, we extended our search to include 645 peptides, experimentally known to inhibit a wide variety of viruses, and compu-tationally screened them against the RBD of SARS-CoV-2 S1 protein. Only RBD of S1 was chosen as a therapeutic target instead of RBD- hACE2 complex, with the primary hypothesis that peptide inhibitors with stronger affinity for RBD than that of hACE2 would prevent the virus from host cellular ent",
            {
                "entities": [
                    [
                        1026,
                        1050,
                        "AUTHOR"
                    ],
                    [
                        1053,
                        1073,
                        "AUTHOR"
                    ],
                    [
                        1076,
                        1099,
                        "AUTHOR"
                    ],
                    [
                        1102,
                        1128,
                        "AUTHOR"
                    ],
                    [
                        1131,
                        1149,
                        "AUTHOR"
                    ],
                    [
                        1152,
                        1175,
                        "AUTHOR"
                    ],
                    [
                        1178,
                        1199,
                        "AUTHOR"
                    ],
                    [
                        1202,
                        1219,
                        "AUTHOR"
                    ],
                    [
                        1222,
                        1237,
                        "AUTHOR"
                    ],
                    [
                        1240,
                        1258,
                        "AUTHOR"
                    ],
                    [
                        1261,
                        1279,
                        "AUTHOR"
                    ],
                    [
                        1282,
                        1301,
                        "AUTHOR"
                    ],
                    [
                        1304,
                        1318,
                        "AUTHOR"
                    ],
                    [
                        1321,
                        1337,
                        "AUTHOR"
                    ],
                    [
                        1340,
                        1356,
                        "AUTHOR"
                    ],
                    [
                        1359,
                        1373,
                        "AUTHOR"
                    ],
                    [
                        1376,
                        1389,
                        "AUTHOR"
                    ],
                    [
                        1392,
                        1404,
                        "AUTHOR"
                    ],
                    [
                        1407,
                        1431,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptFuture Gener Comput Syst. Author manuscript; available in PMC 2022 February 01.Published in final edited form as:Future Gener Comput Syst. 2021 February ; 115: 610–618. doi:10.1016/j.future.2020.09.040.Estimation of laryngeal closure duration during swallowing without invasive X-raysShitong Maoa, Aliaa Sabryb, Yassin Khalifaa, James L Coyleb, Ervin Sejdica,c,*aDepartment of Electrical and Computer Engineering, Swanson School of Engineering, University of Pittsburgh, Pittsburgh, PA 15260 USAbDepartment of Communication Science and Disorders, School of Health and Rehabilitation Sciences, University of Pittsburgh, Pittsburgh, PA 15260 USAcDepartment of Bioengineering, Swanson School of Engineering Department of Biomedical Informatics, School of Medicine Intelligent Systems Program, School of Computing and Information, University of Pittsburgh, Pittsburgh, PA 15260 USAAbstractLaryngeal vestibule (LV) closure is a critical physiologic event during swallowing, since it is the first line of defense against food bolus entering the airway. Identifying the laryngeal vestibule status, including closure, reopening and closure duration, provides indispensable references for assessing the risk of dysphagia and neuromuscular function. However, commonly used radiographic examinations, known as videofluoroscopy swallowing studies, are highly constrained by their radiation exposure and cost. Here, we introduce a non-invasive sensor-based system, that acquires high-resolution cervical auscultation signals from neck and accommodates advanced deep learning techniques for the detection of LV behaviors. The deep learning algorithm, which combined convolutional and recurrent neural networks, was developed with a dataset of 588 swallows from 120 patients with suspected dysphagia and further clinically tested on 45 samples from 16 healthy participants. For classifying the LV closure and opening statuses, our method achieved 78.94% and 74.89% accuracies for these two datasets, suggesting the feasibility of implementing sensor signals for LV prediction without traditional videofluoroscopy screening methods. The sensor supported system offers a broadly applicable computational approach for *Correponding author: esejdic@pitt.edu (Ervin Sejdic).Shitong Mao: Methodology, Formal analysis, Software, Writing- Original draft preparation. Aliaa Sabry: Data curation, Resources, Investigation, Writing- Original draft preparation. Yassin Khalifa: Data Curation, Methodology, Investigation. James L Coyle: Conceptualization, Data Curation, Resources, Writing-Reviewing and Editing. Ervin Sejdic: Writing-Reviewing and Editing, Supervision, Project administration, Funding acquisition.Declaration of competing interestWe declare we have no competing interests.Declaration of interests□ The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptMao et al.Page 2clinical diagnosis and biofeedback purposes in patients with swallowing disorders without the use of radiographic examination.Graphical AbstractKeywordsLaryngeal vestibule closure; High resolution cervical auscultation (HRCA); Deep learning; Dysphagia; Health-care1. IntroductionFor humans, the respiration and digestive systems share the same entrance, therefore, protecting the airway from food bolus entering the trachea or lungs is a fundamental requirement for safe swallowing. Laryngeal vestibule (LV) closure has been considered the first line of defense against swallowed material entering the airway [1, 2, 3]. Likewise, the duration of LV closure is a predictor of airway invasion during swallowing. If the laryngeal closure is absent or its duration is too short, this can lead to aspiration/penetration[4, 5]. Aspiration has been considered as a major concern for individuals with dysphagia (swallowing disorders), especially in neurologic and neurodegenerative diseases, where aspiration-related respiratory infections are a leading cause of death[6]. Therefore, proper evaluation of LV closure and duration could provide an objective outcome measure to improve the assessment of swallowing safety, provide clinical evidence of increased risk of airway compromise during swallowing, and guide the instigation of appropriate compensatory interventions.The videofluoroscopy swallowing study (VFSS) is the only instrumental assessment technique that can visualize the event of LV closure and determine its duration during swallowing through the kinematic analysis of radiographic images[6, 7, 8]. However, practical issues will raise when the VFSS is implemented: it exposes patients to radiation, and is not feasible in all facilities without x-ray departments or qualified clinicians to perform and interpret the VFSS images.[9, 10]. Additionally, it is not suitable for the cases in which patients prefer not to undergo x-ray testing or when patients are unable to participate in the examination protocols[10, 11, 12, 13].Future Gener Comput Syst. Author manuscript; available in PMC 2022 February 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptMao et al.Page 3Furthermore, there are certain limitations of the ordinary clinical setting preventing the more frequent temporal analysis of swallowing events using VFSS images which provides quantification of LV closure at baseline and assessment of treatment efficacy. Frame-by-frame review of VFSS video is time-consuming and some clinicians may not have the ability to record VFSS images for secondary review due to lack of equipment or limited access to archived materials. Clinicians tend to comment on whether and at what phase of the swallow, the material enters the laryngeal vestibule without determining whether LV closure itself was shortened, further limiting inferences that can lead to treatment decisions[14].Because of the previously mentioned drawbacks and limitations of VFSS in the detection of LV closure and reopening, it would be practically beneficial for patients and clinicians to investigate an alternative, non-invasive tool. High-resolution cervical auscultation (HRCA) is a promising non-invasive method for dysphagia screening assessment and management[15]. It uses high-resolution accelerometers and microphones, attached to patients’ necks to record vibratory and acoustic signals during swallowing[16, 17]. The advantages of such a sensor supported approach include mobility, cost-effectiveness, non-invasiveness, and suitability for. day-to-day and even minute-to-minute monitoring[15, 18]. To investigate the relationship between the signals and the LV shifting, previous studies postulated the cardiac analogy hypothesis that explained the elusive physiologic cause of swallowing sounds[19]. This theory suggested that cervical auscultation acoustic signals are generated via vibrations caused by valve and pump systems within the vocal tract. Moreover, HRCA signal features have been found to be associated with LV closure onset and LV reopening[20]. The slapping of the epiglottis and aryepiglottic fold may provide the valve activity that generates swallowing sounds and neck vibration which can be recorded with HRCA.All these studies indicated the possibility of detecting the LV closure and reopening, and a method of determining the closure duration solely based on the HRCA signals. However, no studies attempted to quantitatively implement such an idea. The main challenge was that the explicit dependencies between the signal features and the LV behaviors were not mathematically established. In this study, we sought to investigate the ability of HRCA signals to identify LV status with an advanced deep learning model, which approximated the relationship with training examples. We hypothesized that the computer-aided algorithm with HRCA signals which were acquired from the neck was able to detect the event of LV switching, and estimate the duration of LV closure.The machine learning and deep learning methods have already become powerful tools in the health-care applications and widely employed in the computer-assisted diagnosis for swallowing, laryngeal and neck disorders or disease[21]. Based on larynx contact endoscopic video images, Esmaeili et al. attempted to apply support vector machine, k-nearest neighbor, and random forest to classify benign and malignant lesions on the superficial layers of laryngeal mucosa[22]. For early stage diagnosis of laryngeal squamous cell carcinoma, Moccia et al. implemented a support vector machine classifier with features extracted from the laryngeal endoscopic frames, and they achieved 93% sensitivity[23]. For the similar purpose, Araújo et al. applied transfer learning with pre-trained Convolutional Neural Network (CNN) models to process laryngoscopy images, and they achieved state-of-Future Gener Comput Syst. Author manuscript; available in PMC 2022 February 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptMao et al.Page 4art performance[24]. Our previous study also performed multi-scale CNN filers for hyoid bone detection on the VFSS images[25]. All those studies were conducted based on images as model input. Only several studies attempted to use signals in time series as input and build up deep learning models to serve the swallowing or laryngeal ap",
            {
                "entities": [
                    [
                        395,
                        407,
                        "AUTHOR"
                    ],
                    [
                        2441,
                        2453,
                        "AUTHOR"
                    ],
                    [
                        409,
                        424,
                        "AUTHOR"
                    ],
                    [
                        2532,
                        2547,
                        "AUTHOR"
                    ],
                    [
                        426,
                        440,
                        "AUTHOR"
                    ],
                    [
                        2591,
                        2605,
                        "AUTHOR"
                    ],
                    [
                        442,
                        455,
                        "AUTHOR"
                    ],
                    [
                        2682,
                        2695,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Procedia Computer ScienceVolume 53, 2015, Pages 345–3552015 INNS Conference on Big DataA classification algorithm for high-dimensional data Asim Roy1 1Department of Information Systems, Arizona State University, Tempe, Arizona, USA Asim.Roy@asu.edu Abstract With the advent of high-dimensional stored big data and streaming data, suddenly machine learning on a very large scale has become a critical need. Such machine learning should be extremely fast, should scale up easily with volume and dimension, should be able to learn from streaming data, should automatically perform dimension reduction for high-dimensional data, and should be deployable on hardware. Neural networks are well positioned to address these challenges of large scale machine learning. In this paper, we present a method that can effectively handle large scale, high-dimensional data. It is an online method that can be used for both streaming and large volumes of stored big data. It primarily uses Kohonen nets, although only a few selected neurons (nodes) from multiple Kohonen nets are actually retained in the end; we discard all Kohonen nets after training. We use Kohonen nets both for dimensionality reduction through feature selection and for building an ensemble of classifiers using single Kohonen neurons. The method is meant to exploit massive parallelism and should be easily deployable on hardware that implements Kohonen nets. Some initial computational results are presented. Keywords: Kohonen nets; classification algorithm; online learning; feature selection; high-dimensional data 1 Introduction The arrival of big and streaming data is forcing major changes to the machine learning field. In this new era, there are significantly more demands on machine learning systems - from the need to handle very large volumes of data and fast learning to the need for automation of machine learning that requires less expert assistance and for hardware deployment. Traditional artificial neural network algorithms have many properties that can meet these demands of big data and thus can certainly play a key role in the major transformations that are taking place. For example, many neural net algorithms are based on the concept of online, incremental learning that does not require simultaneous access to large volumes of data. This mode of learning not only resolves many computational issues, it also Selection and peer-review under responsibility of the Scientific Programme Committee of INNS-BigData2015c(cid:2) The Authors. Published by Elsevier B.V.345doi: 10.1016/j.procs.2015.07.311    \fA classification algorithm for high-dimensional dataAsim Royremoves the headache of correctly sampling from large volumes of data. It also makes neural net algorithms highly scalable (i.e. they can easily handle large volumes of data without running into computer memory limitations and learning (processing) time scales up linearly with data volume because of incremental learning) and provides them the capability to learn from all of the data. Neural network algorithms also have the advantage that they use simple computations that can be highly parallelized. These algorithms are already being implemented on hardware that allows parallel computations (Oh & Jung, 2004) and more powerful hardware is on the way (Monroe, 2014; Poon & Zhou, 2011; Furber et al., 2013). All these features of neural network algorithms positions the field to become the backbone of machine learning applications in the era of big and streaming data. In this paper, we present a new neural network learning method that (1) can be parallelized at different levels of granularity, (2) addresses the issue of high-dimensional data through class-based feature selection, (3) learns an ensemble of classifiers using selected Kohonen neurons (nodes) from different Kohonen nets (Kohonen, 2001), and (4) can be easily implemented on hardware. For dimensionality reduction through feature selection, we train a number of Kohonen nets in parallel with streaming data to create some representative data points. (Note that stored data can also be streamed.) Using these Kohonen nets, we perform class-based feature selection (Roy et al., 2013). The basic criteria for feature selection are to select features for each class that (1) makes the class more compact, and (2) at the same time, maximize the average distance from the other classes. Once class-based feature selection is complete, we discard these Kohonen nets. In the second phase, we construct several new Kohonen nets in parallel in different feature spaces, again from streaming data, based on the selected features. Once trained, we then extract just the active neurons from these different Kohonen nets, add class labels to them and create an ensemble of Kohonen neurons for classification. In the end, we just retain a set of dangling active Kohonen neurons from different Kohonen nets in different feature spaces and discard all Kohonen nets. The paper is organized as follows. Section 2 provides an overview of the concept of class-based feature selection and separability index of features. Section 3 has the algorithm for class-based feature selection from streaming data using Kohonen nets in parallel. Sections 4 provide details on how an ensemble classifier is constructed using neurons from different Kohonen nets. Section 5 has computational results for several high-dimensional problems and the conclusions are in Section 6. 2 Class-specific feature selection, separability index of features and dimensionality reduction A fundamental challenge for machine learning is learning from high-dimensional data. A number of new methods have been developed for both online feature selection and feature extraction for high-dimensional streaming data (Yan et al., 2006; Hoi et al., 2012; Wu et al., 2010; Law et al., 2006). However, none of them are for class-specific feature selection. Since 1997, at various conferences, Roy had proposed methods that use a subset of the original features in class-specific classifiers and Roy et al. (2013) presents one such method. However, the method in Roy et al. (2013) does not work for streaming data. In class-specific feature selection, we find separate feature sets for each class such that they are the best ones to separate that class from the rest of the classes. The concept we use is identical to the one used by LDA and Maximum Margin Criterion (MMC) methods (Li et al. 2006) that maximize the between-class scatter and minimize the within-class scatter. In other words, those methods try to maximize the distance between different class centers and at the same time make the data points in the same class as close as possible. Our method, although not a feature extraction method, is based on the same concept. 346   \fA classification algorithm for high-dimensional dataAsim RoyIn an offline mode, where a collection of data points is available, it is fairly easy to select features that maximize the average distance of data points of one class from the rest of the classes and also, at the same time, minimize the average distance of data points within that class. Roy et al. (2013) ranks and selects features on this basis and computational experiments show that it works quite well. However, that method cannot be used for streaming data where no data is stored. In the proposed method, we use the same concept for feature selection, but train multiple Kohonen nets from streaming data to do so. By training multiple Kohonen nets, we essentially create some representative data points for each class and that’s how we resolve the dilemma of not having access to a collection of data points. Once we have a collection of representative data points (represented by certain Kohonen neurons in the Kohonen nets), it is easy to use the class-based feature selection method proposed in Roy et al. (2013). We train many different Kohonen nets, of different grid sizes and for different feature subsets, and we train them in parallel on a distributed computing platform. We use Apache Spark (2015) as our distributed computing platform, but other similar platforms can be used. A Kohonen net forms clusters and the cluster centers (that is, the active Kohonen net nodes or neurons) are equivalent to representative examples of the streaming data. We then use these representative examples to select features by class.  in is the average distance between patterns within class k for feature n, and dknSuppose there are kc total classes. Our basic feature ranking principles are that (1) a good feature for class k should produce good separation between patterns in class k and those not in class k, k = 1…kc, and (2) also make the patterns in class k more compact. Roy et al. (2013) uses a measure called the separability index that is based on these concepts and can rank features for each class. out the Suppose dknaverage distance between the patterns in class k and those not in class k for feature n.  Roy et al. (2013) uses the Euclidean distance for distance measure, but other distance measures could be used. in. Roy et al. (2013) uses this separability index rkn to The separability index is given by rkn = dknrank order features of class k where a higher ratio implies a higher rank. The sense of this measure is out increases the that a feature n with a lower dknseparation of class k from the other classes. Thus, higher the ratio rkn for a feature n, greater is its ability to separate class k from the other classes and better is the feature. in makes class k more compact and with a higher dknout / dkn2.1 Why class-based feature selection? An example Gene Number Separability Indices by Class AML ALL Good Good AML Features 758 1809 4680 ALL Features 2288 760 6182 82.53 75.25 39.73 0.85 0.93 0.8 2.49 1.85 2.82 114.75 98.76 34.15 Table 2.1 – Separability indices for a few features in the AMLALL gene expression dataset 347      \fA classification algorithm for high-dimensional dataAsim RoyWe solved a number o",
            {
                "entities": [
                    [
                        139,
                        148,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Manuscript version: Author’s Accepted Manuscript The version presented in WRAP is the author’s accepted manuscript and may differ from the published version or Version of Record. Persistent WRAP URL: http://wrap.warwick.ac.uk/125464              How to cite: Please refer to published version for the most recent bibliographic citation information. If a published version is known of, the repository item page linked to above, will contain details on accessing it. Copyright and reuse: The Warwick Research Archive Portal (WRAP) makes this work by researchers of the University of Warwick available open access under the following conditions. © 2019 Elsevier. Licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International http://creativecommons.org/licenses/by-nc-nd/4.0/. Publisher’s statement: Please refer to the repository item page, publisher’s statement section, for further information. For more information, please contact the WRAP Team at: wrap@warwick.ac.uk. warwick.ac.uk/lib-publications          \fA Convolutional Neural Network Approach to Detect Congestive Heart Failure Mihaela Porumb1, Ernesto Iadanza2, Sebastiano Massaro3 and Leandro Pecchia1* 1 University of Warwick, School of Engineering, Coventry CV4 7AL, UK 2 University of Florence, v. S. Marta, 3, Florence, IT (E-mail: Ernesto.Iadanza@unifi.it) 3 The Organizational Neuroscience Laboratory, London WC1N 3AX, UK; University of Surrey, Guildford, GU2 7XH, UK (E-mail: sebastiano.massaro@theonelab.org) How to Cite: “Porumb, M., Iadanza, E., Massaro, S., & Pecchia, L. (2020). A convolutional neural network approach to detect congestive heart failure. Biomedical Signal Processing and Control, 55, 101597. DOI: https://doi.org/10.1016/j.bspc.2019.101597” Link: https://www.sciencedirect.com/science/article/pii/S1746809419301776 *Corresponding author: E-mail address: L.Pecchia@warwick.ac.uk, University of Warwick, School of Engineering, Coventry CV4 7AL, UK Abstract Congestive Heart Failure (CHF) is a severe pathophysiological condition associated with high prevalence, high mortality rates, and sustained healthcare costs, therefore demanding efficient methods for its detection. Despite recent research has provided methods focused on advanced signal processing and machine learning, the potential of applying Convolutional Neural Network (CNN) approaches to the automatic detection of CHF has been largely overlooked thus far. This study addresses this important gap by presenting a CNN model that accurately identifies CHF on the basis of one raw electrocardiogram (ECG) heartbeat only, also juxtaposing existing methods typically grounded on Heart Rate Variability. We trained and tested the model on publicly available ECG datasets, comprising a total of 490,505 heartbeats, to achieve 100% CHF detection accuracy. Importantly, the model also identifies those heartbeat   \fsequences and ECG’s morphological characteristics which are class-discriminative and thus prominent for CHF detection. Overall, our contribution substantially advances the current methodology for detecting CHF and caters to clinical practitioners’ needs by providing an accurate and fully transparent tool to support decisions concerning CHF detection. Keywords: Convolutional Neural Networks, Congestive Heart Failure, Machine Learning 1. INTRODUCTION Congestive Heart Failure (CHF) is a pathophysiological condition responsible for the failure of the heart in pumping blood in the body [1] which has encountered widespread research and societal attention [2]. According to the European Society of Cardiology, around 26 million people worldwide are affected by a form of heart failure [3]. CHF is a strongly degenerative condition, and its prevalence increases quickly with age [4], [5]. The mortality rate is closely associated with the degree of severity, reaching peaks of 40% in the most serious events [e.g.,  New York Heart Association (NYHA) classes III-IV] [6]. CHF is also one of the foremost reasons for hospitalization in the elderly, and it is characterized by a resilient relapse rate, with half of the outpatients readmitted within a few months from hospital discharge [7]. Moreover, just among the most industrialized countries, the healthcare expenditure for CHF consumes 2-3% of the healthcare budgets, with the cost of hospitalization being the greatest proportion of the spending [8]–[10]. Thus, with a worldwide aging population and sustained pressures on healthcare systems and resources, there is the compelling demand—among patients, healthcare providers, policymakers, and the society as a whole—to address this scenario by identifying highly accurate methods to improve detection of heart failures [11] and in turn enable early and more efficient diagnoses. Recently, research has made significant progress in these areas. In particular, given the quantity and complexity of data involved, machine learning techniques and classsifiers (e.g., SVM, MLP, k-NN, CART, Random Forest) [12]–[18] have been successfully applied to analyze, detect, and classify heart failures, as we discussed and showed in previous works [12], [13], [19]–[21]. These approaches able to distinguish between heart failure and healthy subjects are mostly based on Heart Rate Variability (HRV)—the variation over time of the period between consecutive heartbeats extracted from electrocardiographic (ECG) signals [22]—, showing that depressed HRV patterns represent accurate markers for detecting the condition. However, building accurate HRV-based models is time-consuming and prone to error steps, due to the preprocessing and the iterative process of manually selecting appropriate features. \fMoreover, the best performing HRV-based models generally require either long-term signals (i.e., 24h) or at least the combination of short-term HRV with non-standard long-term HRV features, as shown in [23]. To tackle these issues, we present a novel framework of CHF detection that does not rely on HRV features, rather it uses raw ECG signals only. This method is based on a 1-D Convolutional Neural Network (CNN). CNNs are hierarchical neural networks that mimic the human visual system and have proven to be effective in recognizing patterns and structures of input data in image classification, localization, and detection tasks, among others [24]–[26]. Moreover, they have been extensively used for time series analysis in classification tasks. Some successful examples include, among others: general time series classification [27]–[29], speech recognition tasks [30], arrhythmia detection [31], [32], and multivariate diagnostic measurements modeling [33], [34]. Inspired by this growing body of research, we aim to detect CHF through a 1-D CNN approach on the ECG signals. Adding to the significant benefit of building upon raw physiological data, this method enables visualization of the input time series subsequences that are class discriminative (i.e., CHF vs. healthy subjects). This feature considerably improves the interpretability of the CNN model and represents a crucial aspect to ensure the ‘transparency’ of the method [35], [36]. Such a transparency is fundamental to help researchers explain how conclusions are reached, and to aid professionals in better understanding correlations of pathophysiological behaviors and properties revealed by the model. As we shall explain, we used a form of class activation mapping (Grad-CAM) [37] that highlights the class discriminative regions in the input data. In other words, Grad-CAM uses the model’s last activation maps to originate a heat map that can be overlapped with the input, thus showing what regions in the input data contribute most to CHF detection. Overall, the proposed framework puts forward several developments for CHF detection, such as refraining from using hefty preprocessing and features selection steps of HRV-based models, as well as enabling visualization of the subsequences in the input time series which are used to reach certain clinically-relevant conclusions. 2.1 Data 2. METHODS We performed a retrospective analysis on two publicly available datasets. The data for the normal subjects (i.e., control group) were retrieved from the MIT-BIH Normal Sinus Rhythm Database [38] included in PhysioNet [39]. This dataset includes 18 long-term ECG recordings of normal healthy not-arrhythmic subjects (Females=13; age \frange: 20 to 50). The data for the CHF group were retrieved from the BIDMC Congestive Heart Failure Database [40] from PhysioNet [39]. This dataset includes long-term ECG recordings of 15 subjects with severe CHF (i.e., NYHA classes III-IV) (Females =4; age range: 22 to 63). Altogether, the pool of data available for this study consists of about 20 hours of ECG recordings per each subject; it contains two-channel ECG signals sampled at 250 samples per second for the BIDMC dataset, and 128 samples per second for the MIT-BIH dataset. The two datasets used in this study were published by the same laboratory, the Beth Israel Deaconess Medical Center, and were digitized using the same procedures according to the signal specification line in the header file. These datasets have been widely used in several studies concerning CHF detection, as explained in [11]. Indeed, it is an acknowledged practice to build and validate CHF classification models on these sources to ensure opportunities for reproducibility. 2.2 ECG preprocessing The ECG recordings from the BIDMC dataset were down-sampled in order to match the sampling frequency of the ECG signals from the MIT-BIH dataset (i.e., 128 Hz). Records from both databases were already made available with computed beat annotations, which we used to isolate and extract individual heartbeats. We considered a window of 235 ms before the annotated R peak (equivalent to 30 samples=128 samples/s*0.235 s), and a window of 390 ms after the R peak (equivalent to 50 samples). Only the beats that were annotated as normal (N) were retained for further analy",
            {
                "entities": [
                    [
                        1116,
                        1131,
                        "AUTHOR"
                    ],
                    [
                        1133,
                        1149,
                        "AUTHOR"
                    ],
                    [
                        1151,
                        1170,
                        "AUTHOR"
                    ],
                    [
                        1175,
                        1191,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Pattern Recognition 115 (2021) 107899 Contents lists available at ScienceDirect Pattern Recognition journal homepage: www.elsevier.com/locate/patcog Pruning by explaining: A novel criterion for deep neural network pruning Seul-Ki Yeom a , i , Philipp Seegerer a , h , Sebastian Lapuschkin c , Alexander Binder d , e , Simon Wiedemann c , Klaus-Robert Müller a , f , g , b , ∗, Wojciech Samek c , b , ∗a Machine Learning Group, Technische Universität Berlin, 10587 Berlin, Germany b BIFOLD – Berlin Institute for the Foundations of Learning and Data, Berlin, Germany c Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany d ISTD Pillar, Singapore University of Technology and Design, Singapore 487372, Singapore e Department of Informatics, University of Oslo, 0373 Oslo, Norway f Department of Artificial Intelligence, Korea University, Seoul 136–713, Korea g Max Planck Institut für Informatik, 66123 Saarbrücken, Germany h Aignostics GmbH, 10557 Berlin, Germany i Nota AI GmbH, 10117 Berlin, Germany a r t i c l e i n f o a b s t r a c t Article history: Received 18 December 2019 Revised 28 January 2021 Accepted 8 February 2021 Available online 22 February 2021 Keywords: Pruning Layer-wise relevance propagation (LRP) Convolutional neural network (CNN) Interpretation of models Explainable AI (XAI) The success of convolutional neural networks (CNNs) in various applications is accompanied by a sig- nificant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource- constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning. © 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) 1. Introduction Deep CNNs have become an indispensable tool for a wide range of applications [1] , such as image classification, speech recognition, natural language processing, chemistry, neuroscience, medicine and even are applied for playing games such as Go, poker or Su- per Smash Bros. They have achieved high predictive performance, ∗ Corresponding authors. E-mail addresses: yeom@tu-berlin.de (S.-K. Yeom), philipp.seegerer@tu- berlin.de (P. Seegerer), sebastian.lapuschkin@hhi.fraunhofer.de (S. Lapuschkin), alexabin@uio.no (A. Binder), simon.wiedemann@hhi.fraunhofer.de (S. Wiedemann), klaus-robert.mueller@tu-berlin.de (K.-R. Müller), wojciech.samek@hhi.fraunhofer.de (W. Samek). at times even outperforming humans. Furthermore, in specialized domains where limited training data is available, e.g., due to the cost and difficulty of data generation (medical imaging from fMRI, EEG, PET etc.), transfer learning can improve the CNN performance by extracting the knowledge from the source tasks and applying it to a target task which has limited training data. However, the high predictive performance of CNNs often comes at the expense of high storage and computational costs, which are related to the energy expenditure of the fine-tuned network. These deep architectures are composed of millions of parameters to be trained, leading to overparameterization (i.e. having more pa- rameters than training samples) of the model [2] . The run-times are typically dominated by the evaluation of convolutional layers, while dense layers are cheap but memory-heavy [3] . For instance, https://doi.org/10.1016/j.patcog.2021.107899 0031-3203/© 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \fS.-K. Yeom, P. Seegerer, S. Lapuschkin et al. Pattern Recognition 115 (2021) 107899 the VGG-16 model has approximately 138 million parameters, tak- ing up more than 500MB in storage space, and needs 15.5 bil- lion floating-point operations (FLOPs) to classify a single image. ResNet50 has approx. 23 million parameters and needs 4.1 bil- lion FLOPs. Note that overparameterization is helpful for an ef- ficient and successful training of neural networks, however, once the trained and well generalizing network structure is established, pruning can help to reduce redundancy while still maintaining good performance [4] . Reducing a model’s storage requirements and computational cost becomes critical for a broader applicability, e.g., in embedded systems, autonomous agents, mobile devices, or edge devices [5] . Neural network pruning has a decades long history with inter- est from both academia and industry [6] aiming to eliminate the subset of network units (i.e. weights or filters) which is the least important w.r.t. the network’s intended task. For network prun- ing, it is crucial to decide how to identify the “irrelevant” subset of the parameters meant for deletion. To address this issue, pre- vious researches have proposed specific criteria based on Taylor expansion, weight, gradient, and others, to reduce complexity and computation costs in the network. Related works are introduced in Section 2 . From a practical point of view, the full capacity (in terms of weights and filters) of an overparameterized model may not be re- quired, e.g., when (1) parts of the model lie dormant after training (i.e., are per- manently ”switched off”), (2) a user is not interested in the model’s full array of possible outputs, which is a common scenario in transfer learning (e.g. the user only has use for 2 out of 10 available network outputs), or (3) a user lacks data and resources for fine-tuning and running the overparameterized model. In these scenarios the redundant parts of the model will still occupy space in memory, and information will be propagated through those parts, consuming energy and increasing runtime. Thus, criteria able to stably and significantly reduce the com- putational complexity of deep neural networks across applications are relevant for practitioners. In this paper, we propose a novel pruning framework based on Layer-wise Relevance Propagation (LRP) [7] . LRP was originally de- veloped as an explanation method to assign importance scores, so called relevance , to the different input dimensions of a neural net- work that reflect the contribution of an input dimension to the model’s decision, and has been applied to different fields of com- puter vision (e.g., [8–10] ). The relevance is backpropagated from the output to the input and hereby assigned to each unit of the deep model. Since relevance scores are computed for every layer and neuron from the model output to the input, these relevance scores essentially reflect the importance of every single unit of a model and its contribution to the information flow through the network — a natural candidate to be used as pruning criterion. The LRP criterion can be motivated theoretically through the concept of Deep Taylor Decomposition (DTD) (c.f. [11–13] ). Moreover, LRP is scalable and easy to apply, and has been implemented in software frameworks such as iNNvestigate [14] . Furthermore, it has linear computational cost in terms of network inference cost, similar to backpropagation. We systematically evaluate the compression efficacy of the LRP criterion compared to common pruning criteria for two different scenarios. Scenario 1 : We prune pre-trained CNNs followed by subse- quent fine-tuning. This is the usual setting in CNN pruning and requires a sufficient amount of data and computational power. Scenario 2 : In this scenario a pretrained model needs to be transferred to a related problem as well, but the data available for the new task is too scarce for a proper fine-tuning and/or the time consumption, computational power or energy consumption is con- strained. Such transfer learning with restrictions is common in mo- bile or embedded applications. Our experimental results on various benchmark datasets and four different popular CNN architectures show that the LRP crite- rion for pruning is more scalable and efficient, and leads to bet- ter performance than existing criteria regardless of data types and model architectures if retraining is performed (Scenario 1). Especially, if retraining is prohibited due to external constraints after pruning, the LRP criterion clearly outperforms previous crite- ria on all datasets (Scenario 2). Finally, we would like to note that our proposed pruning framework is not limited to LRP and image data, but can be also used with other explanation techniques and data types. The rest of this paper is organized as follows: Section 2 sum- marizes related works for network compression and introduces the typical criteria for network pruning. Section 3 describes the frame- work and details of our approach. The experimental results are il- lustrated and discussed in Section 4 ",
            {
                "entities": [
                    [
                        221,
                        234,
                        "AUTHOR"
                    ],
                    [
                        242,
                        259,
                        "AUTHOR"
                    ],
                    [
                        267,
                        288,
                        "AUTHOR"
                    ],
                    [
                        292,
                        309,
                        "AUTHOR"
                    ],
                    [
                        317,
                        333,
                        "AUTHOR"
                    ],
                    [
                        376,
                        391,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptNeurocomputing. Author manuscript; available in PMC 2017 February 12.Published in final edited form as:Neurocomputing. 2016 February 12; 177: 75–88. doi:10.1016/j.neucom.2015.11.008.Dictionary Pruning with Visual Word Significance for Medical Image RetrievalFan Zhanga,b, Yang Songa, Weidong Caia, Alexander G. Hauptmannc, Sidong Liua, Sonia Pujolb, Ron Kikinisb, Michael J Fulhamd,e, David Dagan Fenga,f, and Mei Cheng,haSchool of Information Technologies, University of Sydney, AustraliabDept of Radiology, Brigham & Womens Hospital, Harvard Medical School, United StatescSchool of Computer Science, Carnegie Mellon University, United StatesdDept of PET and Nuclear Medicine, Royal Prince Alfred Hospital, AustraliaeSydney Medical School, University of Sydney, AustraliafMed-X Research Institute, Shanghai Jiaotong University, ChinagDept of Informatics, University of Albany State University of New York, United StateshRobotics Institute, Carnegie Mellon University, United StatesAbstractContent-based medical image retrieval (CBMIR) is an active research area for disease diagnosis and treatment but it can be problematic given the small visual variations between anatomical structures. We propose a retrieval method based on a bag-of-visual-words (BoVW) to identify discriminative characteristics between different medical images with Pruned Dictionary based on Latent Semantic Topic description. We refer to this as the PD-LST retrieval. Our method has two main components. First, we calculate a topic-word significance value for each visual word given a certain latent topic to evaluate how the word is connected to this latent topic. The latent topics are learnt, based on the relationship between the images and words, and are employed to bridge the gap between low-level visual features and high-level semantics. These latent topics describe the images and words semantically and can thus facilitate more meaningful comparisons between the words. Second, we compute an overall-word significance value to evaluate the significance of a visual word within the entire dictionary. We designed an iterative ranking method to measure overall-word significance by considering the relationship between all latent topics and words. The words with higher values are considered meaningful with more significant discriminative power in differentiating medical images. We evaluated our method on two public medical imaging datasets and it showed improved retrieval accuracy and efficiency.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.KeywordsMedical image retrieval; BoVW; Dictionary pruning1. IntroductionPage 2Content-based medical image retrieval (CBMIR), which retrieves a subset of images that are visually similar to the query from a large image database, is the focus of intensive research (Müller et al., 2004; Akgül et al., 2011; Kumar et al., 2013). CBMIR provides the potential of having an efficient tool for disease diagnosis, by finding related pre-diagnosed cases and it can be used for disease treatment planning and management. In the past three decades, but in particular in the last decade, medical image data have expanded rapidly due to the pivotal role of imaging in patient management and the growing range of image modalities (Duncan and Ayache, 2000; Menze et al., 2014). Traditional text-based retrieval, which manually indexes the images with alphanumerical keywords, is unable to sufficiently meet the increased demand from this growth. At the same time, advances in computer-aided content-based medical image analysis systems mean that there are methods that can automatically extract the rich visual properties/features to characterize the images efficiently (El-Naqa et al., 2004; Lehmann et al., 2004; Napel et al., 2010; Avni et al., 2011; André et al., 2012a; Xu et al., 2012; Zhang et al., 2015c).In CBMIR research, the main challenge is to design an effective image representation so that images with visually similar anatomical structures are closely correlated. A number of research groups are working in this area (Müller et al., 2004; Zhang et al., 2010; Akgül et al., 2011; Kumar et al., 2013), and there is a trend to use a bag-of-visual-words (BoVW) for medical image representation (Castellani et al., 2010; Cruz-Roa et al., 2012; Kwitt et al., 2012; Foncubierta-Rodríguez et al., 2013; Liu et al., 2013a; Depeursinge et al., 2014). The BoVW model represents an image with a visual word frequency histogram that is obtained by assigning the local visual features to the closest visual words in the dictionary. Rather than matching the visual feature descriptors directly, BoVW retrieval approaches compare the images according to the visual words that are assumed to have higher discriminative power (Foncubierta-Rodríguez et al., 2012; Tamaki et al., 2013). The BoVW model was proposed by Sivic and Zisserman (Sivic and Zisserman, 2003) and has been adopted by many researchers in non-medical domains such as computer vision (Li and Pietro, 2005; Yang et al., 2007; Bosch et al., 2008), showing the advantages of describing local patterns over using global features only. This model has recently been applied to tackle the large-scale medical image retrieval problem (Jiang et al., 2015; Zhang et al., 2015d). In this study, we focus on a new BoVW-based retrieval for better retrieval accuracy and efficiency.1.1. Related workThe aim of CBMIR is to extract visual characteristics of images to identify the level of similarity between two images. Feature extraction can be categorized into global-(GFM) and local-feature (LFM) models based on the scope of descriptors (Bannour et al., 2009). The GFM extracts a single feature vector from the whole image and the LFM partitions the image into a collection of smaller regions, namely patches, and considers that each patch has Neurocomputing. Author manuscript; available in PMC 2017 February 12.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 3its own importance in describing the whole image (Avni et al., 2011). This patch-based model is particularly useful in medical image analysis since different image regions can represent the anatomical structures that play different and essential roles in medical imaging diagnosis (Tong et al., 2014; Zhang et al., 2014).The BoVW representation builds upon the LFM. Visually similar patches from different images are assigned to the same code in a codebook. Then, the patch-code co-occurrence assignment can be used to describe the image features and to compute the similarity between images. The workflow of BoVW-based image retrieval can be generalized into three steps (Caicedo et al., 2009): feature extraction, BoVW construction and similarity calculation. Specifically, the LFM is used to extract a collection of local patch features from each image. The entire patch feature set computed from all images in the database is then grouped into clusters, with each cluster regarded as a visual word and the whole cluster collection considered as the visual dictionary. Then, all patch features in one image are assigned to visual words, generating a visual word frequency histogram to represent this image. Finally, the similarity between images is computed based on these frequency histograms for retrieval.In this workflow, an important issue is the dictionary construction. The visual word in the dictionary corresponds to a group of visually similar patches. Normally, these words are obtained within the local patch feature space using unsupervised clustering methods, e.g., k-means (André et al., 2011; Yang et al., 2012). These approaches often generate a redundant and noisy dictionary since they tend to accommodate all local patch feature patterns (Foncubierta-Rodríguez et al., 2013), thus reducing the effects of the most crucial words and increasing the computational cost. Hence, it is preferable to remove the visual words that are less essential for the BoVW representation.To ensure that only the meaningful feature patterns are included, the supervised clustering method of Bilenko et al (Bilenko et al., 2004) can be used to regulate the construction of dictionary, but the method adaptability is limited because prior knowledge is required for the learning process. Another approach is to analyze the discriminative power of visual words (Caicedo et al., 2009), but the weighting scheme also requires supervised classifiers. Some researchers have suggested that the most frequent visual words in images are ‘stop words’, which occur widely but have little influence on differentiating images, and need to be removed from the dictionary (Sivic and Zisserman, 2003). Yang et al., however, showed that ranking the visual words based on their occurrences in the different images only was not sufficient to evaluate the importance of visual words (Yang et al., 2007). Term frequency-inverse document frequency (TF-IDF) (Jones, 1972) relies on the inverse frequency weighting and has demonstrated its benefits on visual word evaluation. Nevertheless, it merely utilizes the direct co-occurrence relationship between the images and visual words. Jiang et al. (Jiang et al., 2015) proposed an unsupervised approach to refine the weights of visual words within the vocabulary tree and showed the advantages of using the correlations among the visual words. We suggest that this relationshi",
            {
                "entities": [
                    [
                        369,
                        379,
                        "AUTHOR"
                    ],
                    [
                        381,
                        393,
                        "AUTHOR"
                    ],
                    [
                        420,
                        431,
                        "AUTHOR"
                    ],
                    [
                        433,
                        445,
                        "AUTHOR"
                    ],
                    [
                        447,
                        459,
                        "AUTHOR"
                    ],
                    [
                        461,
                        478,
                        "AUTHOR"
                    ],
                    [
                        482,
                        499,
                        "AUTHOR"
                    ],
                    [
                        507,
                        516,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Automatic Segmentation of Overlapping CervicalSmear Cells based on Local Distinctive Features andGuided Shape DeformationAfaf Tareefa,∗, Yang Songa, Weidong Caia, Heng Huangb, Hang Changc,Yue Wangd, Michael Fulhame, Dagan Fenga, Mei Chenf,gaBiomedical and Multimedia Information Technology (BMIT) Research Group, School ofInformation Technologies, University of Sydney, Australia.bDepartment of Computer Science and Engineering, University of Texas, USA.cLife Sciences Division, Lawrence Berkeley National Laboratory, USA.dBradley Department of Electrical and Computer Engineering, Virginia PolytechnicInstitute and State University, USA.eDepartment of PET and Nuclear Medicine, Royal Prince Alfred Hospital, Australia, andSydney Medical School, University of Sydney, Australia.fDepartment of Informatics, University of Albany State University of New York.gRobotics Institute, Carnegie Mellon University, USA.AbstractAutomated segmentation of cells from cervical smears poses great challenge tobiomedical image analysis because of the noisy and complex background, poorcytoplasmic contrast and the presence of fuzzy and overlapping cells. In thispaper, we propose an automated segmentation method for the nucleus and cyto-plasm in a cluster of cervical cells based on distinctive local features and guidedsparse shape deformation. Our proposed approach is performed in two stages:segmentation of nuclei and cellular clusters, and segmentation of overlappingcytoplasm. In the first stage, a set of local discriminative shape and appear-ance cues of image superpixels is incorporated and classified by the SupportVector Machine (SVM) to segment the image into nuclei, cellular clusters, andbackground. In the second stage, a robust shape deformation framework is pro-posed, based on Sparse Coding (SC) theory and guided by representative shapefeatures, to construct the cytoplasmic shape of each overlapping cell. Then,the obtained shape is refined by the Distance Regularized Level Set Evolution∗Corresponding authorEmail address: atar8654@uni.sydney.edu.au (Afaf Tareef )Preprint submitted to NeurocomputingSeptember 23, 2016\f(DRLSE) model. We evaluated our approach using the ISBI 2014 challengedataset, which has 135 synthetic cell images for a total of 810 cells. Our re-sults show that our approach outperformed existing approaches in segmentingoverlapping cells and obtaining accurate nuclear boundaries.Keywords: overlapping cervical smear cells, feature extraction, sparse coding,shape deformation, distance regularized level set.1. IntroductionCervical cancer is a malignant tumor of the cervix and is the fourth mostcommon cause of cancer death in women worldwide [1]. Cervical cancer, how-ever, can be effectively treated if it is detected early during a routine Pap smear5test. In the test, a sample of cells from the cervix is smeared, or spread, onto aglass slide, and examined under a microscope for nuclear and cytoplasmic atypiato detect pre-cancerous abnormalities in cervical cells based on the shape varia-tions of the nuclei and cytoplasm. The automated segmentation of overlappingcells remains one of the most critical challenges in the analysis of microscopic10cervical images [2].Although there has been substantial progress in the segmentation of cervicalcells, unfortunately, the state-of-the-art approaches tend to underperform onimages with overlapping cells. There are currently four main categories of tech-niques to segment cervical cells and they include the segmentation of: a) isolated15or free-lying cells without any overlapping between the cells [3, 4, 5, 6, 7, 8, 9, 10];b) isolated and overlapping nuclei [11, 12, 13, 14, 15, 16, 17, 14]; c) overlappingnuclei and the whole cellular clusters consisting a number of cells [18, 19]; andd) nuclei and cytoplasm from a cluster of overlapping cells [4, 20, 21, 22, 2].In this paper, we propose a two-stage segmentation technique for the nu-20clei and cytoplasm of overlapping cells. We have incorporated discriminativeshape and appearance cues that sufficiently distinguish the nuclei and the back-ground in superpixel representation level. These superpixel-based features arethen used to train the supervised SVM to separate the nuclei and cell clusters2\ffrom the background. In the second stage, there is a cytoplasmic segmentation25using sparse shape deformation that is guided toward the target shape usingrepresentative features captured from a well-established initial shape. The ob-tained shape is refined by the distance regularized level set evolution (DRLSE)model to obtain more accurate cell segmentation.2. Literature review30The classic approach for segmenting the isolated cervical cells is the thresh-olding method [3]. This method, however, leads to unsatisfactory results dueto the complex structure of cervix cells resulted from the poor contrast andvariable staining. Marker-based and multi-scale watersheds have also been usedto segment the cytoplasm [4]. Watershed segmentation treats the image inten-35sity as a topographic relief that is filled by water from different minima, andthen assigns the same label to all regions associated with the same catchmentbasin. The main limitation of watershed-based segmentation techniques is theover-segmentation of cells. Active contour model (ACM), or ’snakes’, is anotherfamily of segmentation techniques widely used to segment cervical cells [5, 6, 7].40In ACMs, a curve or a surface is evolved under a constraint toward the objectboundary. ACMs are able to recover closed object boundaries with pixel accu-racy. These approaches, however, are insufficient for touching and overlappingcell segmentation.A number of methods to segment isolated and overlapping nuclei have been45proposed based on thresholding and morphological analysis [11], active contours[12], level set [13], adaptive active shape model [14], watershed transform [15,16], and unsupervised classification [17]. For instance, Plissiti et al.[14] usedan adaptive active shape model trained on a set of images each with a singlenucleus, and the attributes of the nuclear shapes are expressed to estimate the50shape model distribution, which is used to detect the unknown overlappingnuclei boundaries.There are also approaches to segment the overlapping nuclei and cellular3\fclusters from the background. Gentav et al. used an unsupervised approachwith automatic thresholding to separate cells from background, a multi-scale55hierarchical watershed segmentation algorithm and a binary classifier to sepa-rate nuclei from cellular cytoplasm [18]. Kale and Aksoy applied a multi-scalewatershed algorithm to initially segment the image, and then SVM to finishthe segmentation [19]. Despite the good performance of these approaches insegmenting nuclei and single cells, they are not able to delineate the boundary60of each individual overlapping cytoplasm, which is a critical deficiency as theshape of the cytoplasm is an important feature for subsequent cellular analysis.Segmentation of both nuclei and cytoplasm from overlapping cells in Papsmears is a great challenge due to the large variation in shape and obscureboundary. Recently, a limited number of techniques have been suggested to65segment overlapping cervical cells [4, 20, 21, 22, 2]. An early technique [4] used alocally constrained watershed transform to segment partially overlapping cells.However, it cannot be used for cells with a large overlapping area. Anotherautomated segmentation approach proposed by Tareef et al. [20] was based ongradient thresholding and morphological operations. This approach worked for70the overlapping cells with noticeable difference in intensities; however, it wasnot effective with large fuzzy clusters of small cells.Recently, shape constrained deformation model has been successfully usedfor medical image segmentation, where shape priors are learned from a set oftraining samples and used on optimization methods, such as Active Shape Model75(ASM) [23], sparse shape model [45, 28, 25], and level set evolution with shapepriors [24, 26, 27]. Some recent methods have tended to segment the overlappingcervical cells by incorporating an ellipsoidal shape prior [21] and a star shapeprior [22] with level set optimization for more accurate cell segmentation. Luet al.[21] proposed an overlapping cell segmentation technique incorporating80ellipsoidal shape prior with a joint level set optimization, constrained by theintensity of the overlapping region, the length and area of each cell, and theamount of cell overlap. A new improved version of this approach is presentedin [2] where a joint optimization of multiple level set functions is utilized with4\funary (intracell) constraints computed based on contour length, edge strength,85and cell shape, and pairwise (intercell) constraints computed based on the areaof the overlapping regions. Although this approach achieved good results for theoverlapping cells, there is a high degree of false negatives owing to the failure insegmenting some complicated cells.The situation for cervical smears also applies to other cytological examina-90tions.Investigators have suggested a segmentation technique for overlappingcells of breast tissue microarrays and blood smears [29]. The centers of regionscontaining densely touching and overlapping cells were detected using singlepass voting with mean-shift-based seed detection. Then a level set algorithmbased on an interactive model generated the contour of each cell using the ob-95tained centers as an initial position. Srisang et al. suggested a technique forchromosomes based on Voronoi diagrams and Delaunay triangulations [30]; withthis technique, overlapping chromosomes are cut into two chromosomes by iden-tifying all possible cut points from the contour line of overlapping chromosomesto select four target cut points to separate the overlapping chromosomes. Quel-100has et al. proposed a sliding band filter (SBF) based approach [31] to detectconvex shapes corresponding to the nucle",
            {
                "entities": [
                    [
                        136,
                        146,
                        "AUTHOR"
                    ],
                    [
                        148,
                        160,
                        "AUTHOR"
                    ],
                    [
                        162,
                        173,
                        "AUTHOR"
                    ],
                    [
                        175,
                        186,
                        "AUTHOR"
                    ],
                    [
                        198,
                        213,
                        "AUTHOR"
                    ],
                    [
                        215,
                        226,
                        "AUTHOR"
                    ],
                    [
                        228,
                        237,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Available online at www.sciencedirect.com Available online at www.sciencedirect.com Procedia Computer Science 00 (2017) 000–000 Procedia Computer Science 00 (2017) 000–000 ScienceDirect ScienceDirect Procedia Computer Science 110 (2017) 498–503 www.elsevier.com/locate/procedia  www.elsevier.com/locate/procedia The 4th International Symposium on Emerging Inter-networks, Communication and Mobility The 4th International Symposium on Emerging Inter-networks, Communication and Mobility (EICM 2017) (EICM 2017) On the use of Networks in Biomedicine On the use of Networks in Biomedicine Eugenio Vocaturoa*, Pierangelo Veltrib Eugenio Vocaturoa*, Pierangelo Veltrib a Department of Computer Science, Modeling, Electronic and Systems Engineering (DIMES), University of Calabria, Italy a Department of Computer Science, Modeling, Electronic and Systems Engineering (DIMES), University of Calabria, Italy b Bioinformatics Laboratory Surgical and Medical Science Department University Magna Graecia of Catanzaro, Italy b Bioinformatics Laboratory Surgical and Medical Science Department University Magna Graecia of Catanzaro, Italy Abstract Abstract The concept of “neural network” emerges by electronic models inspired to the neural structure of human brain. Neural networks aim to The concept of “neural network” emerges by electronic models inspired to the neural structure of human brain. Neural networks aim to solve problems currently out of computer’s calculation capacity, trying to mimic the role of human brain. Recently, the number of biological solve problems currently out of computer’s calculation capacity, trying to mimic the role of human brain. Recently, the number of biological based applications using neural networks is growing up. Biological networks represent correlations, extracted from sets of clinical data, based applications using neural networks is growing up. Biological networks represent correlations, extracted from sets of clinical data, diseases, mutations, and patients, and many other types of clinical or biological features. Biological networks are used to model both the state diseases, mutations, and patients, and many other types of clinical or biological features. Biological networks are used to model both the state of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events. of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events. The study of biological networks, their analysis and modeling are important tasks in life sciences. Most biological networks are still far from The study of biological networks, their analysis and modeling are important tasks in life sciences. Most biological networks are still far from being complete and they are often difficult to interpret due to the complexity of relationships and the peculiarities of the data. Starting from being complete and they are often difficult to interpret due to the complexity of relationships and the peculiarities of the data. Starting from preliminary notions about neural networks, we focus on biological networks and discuss some well-known applications, like protein-protein preliminary notions about neural networks, we focus on biological networks and discuss some well-known applications, like protein-protein interaction networks, gene regulatory networks (DNA-protein interaction networks), metabolic networks, signaling networks, neuronal interaction networks, gene regulatory networks (DNA-protein interaction networks), metabolic networks, signaling networks, neuronal network, phylogenetic trees and special networks. Finally, we consider the use of biological network inside a proposed model to map health network, phylogenetic trees and special networks. Finally, we consider the use of biological network inside a proposed model to map health related data. related data. © 2017 The Authors. Published by Elsevier B.V. © 2017 The Authors. Published by Elsevier B.V.© 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs. Peer-review under responsibility of the Conference Program Chairs.Peer-review under responsibility of the Conference Program Chairs. Keywords: Biological Networks; Protein-Protein Interaction Networks (PPIn); Gene Regulatory Networks (GRN); Metabolic Networks; Signaling Networks; Keywords: Biological Networks; Protein-Protein Interaction Networks (PPIn); Gene Regulatory Networks (GRN); Metabolic Networks; Signaling Networks; Neuronal Networks; Food Webs; Phylogenetic Trees, Special Networks and Hierarchies; Health Care Model. Neuronal Networks; Food Webs; Phylogenetic Trees, Special Networks and Hierarchies; Health Care Model. 1. Introduction 1. Introduction The human brain has the capacity of processing information and making decisions instantaneously. Many researchers have The human brain has the capacity of processing information and making decisions instantaneously. Many researchers have shown that human brain performs calculations in a different way than computers, hence the aspiration to solve problems whose shown that human brain performs calculations in a different way than computers, hence the aspiration to solve problems whose complexity is beyond the current computing power, has prompted the scientific community to the neural networks. For complexity is beyond the current computing power, has prompted the scientific community to the neural networks. For biological network is meant any network applied to a biological systems. biological network is meant any network applied to a biological systems. A network, in a broad sense, identifies a system, which is characterized by interconnected sub-units. Biological networks are A network, in a broad sense, identifies a system, which is characterized by interconnected sub-units. Biological networks are types of important applicable model in various contexts; complex biological systems can be represented and analyzed by types of important applicable model in various contexts; complex biological systems can be represented and analyzed by computable networks. Like the computer networks, the high complexity degree of biological networks is generated by a simple computable networks. Like the computer networks, the high complexity degree of biological networks is generated by a simple mechanism. Bioinformatics really shifted its focus from individual genes, proteins, structures and search algorithms for large mechanism. Bioinformatics really shifted its focus from individual genes, proteins, structures and search algorithms for large networks; even more biologists are discovering the links between Internet and metabolic pathways, interactions of proteins networks; even more biologists are discovering the links between Internet and metabolic pathways, interactions of proteins through a network topology or a scale-free network. through a network topology or a scale-free network. * * * Corresponding author. Tel.: +039-0984-4799. * Corresponding author. Tel.: +039-0984-4799. E-mail address: e.vocaturo@dimes.unical.it E-mail address: e.vocaturo@dimes.unical.it 1877-0509 © 2017 The Authors. Published by Elsevier B.V. 1877-0509 © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs. Peer-review under responsibility of the Conference Program Chairs. 1877-0509 © 2017 The Authors. Published by Elsevier B.V.Peer-review under responsibility of the Conference Program Chairs.10.1016/j.procs.2017.06.13210.1016/j.procs.2017.06.1321877-0509ScienceDirectAvailable online at www.sciencedirect.com          \f2 Eugenio Vocaturo/ Procedia Computer Science 00 (2017) 000–000 Eugenio Vocaturo et al. / Procedia Computer Science 110 (2017) 498–503 499A neural network is composed of a set of parallel and distributed processing units, referred as nodes or neurons; they are arranged in layers, and are interconnected by unidirectional or bidirectional connections (see Fig. 1). Typically, a neural network has a set of N input nodes, whose generic element is related with, and each node is interconnected to others through weighted arcs. The products of input and weight are simply summed and feed through (Activation Function) to generate the output (see Fig. 2).              Fig. 1. Typical Structure of Neural Network                              Fig. 2. Activation Functions Neural network design typically consists of Topology, Transfer Function and Learning Algorithm. The neural network topologies are actually classified by the directions of interconnection in the layer; so the most referred topologies are, Feed Forward Topology and Recurrent Topology. In feed forward topology (FFT) network, the nodes are “hierarchically arranged” in layers starting with the input layers and ending with output layers. The number of hidden layers provides most of the network computational power. In literature typical application of this topology are the multilayer perception network and radial basic function network. The nodes in each layers are connected to next layer through unidirection paths starting from one layer (source) and ending at the subsequently layer (sink). The output of a given layer feeds the nodes of the following layer in a forward direction and does not allow feedback flow of information12. Unlike the FFT, in the recurring topology (RNT) the flow of information between connected nodes is bidirectional. Typical applications of RNT, for example, are Hopfield Network1 and time delay neural network (TDNN)2. A recurrent network structure has a sort of memory, which helps storing information in output nodes through dynamic states. Biological networks shapes both the state of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events14. In neural networks, the basic unit are the neurons that work like simple processors. Any neuron takes th",
            {
                "entities": [
                    [
                        585,
                        602,
                        "AUTHOR"
                    ],
                    [
                        624,
                        641,
                        "AUTHOR"
                    ],
                    [
                        7687,
                        7704,
                        "AUTHOR"
                    ],
                    [
                        7749,
                        7766,
                        "AUTHOR"
                    ],
                    [
                        605,
                        623,
                        "AUTHOR"
                    ],
                    [
                        644,
                        662,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Journal of Informetrics 15 (2021) 101171 Contents lists available at ScienceDirect Journal of Informetrics journal homepage: www.elsevier.com/locate/joi Gender-based homophily in research: A large-scale study of man-woman collaboration Marek Kwiek a , Wojciech Roszka b a Institute for Advanced Studies in Social Sciences and Humanities (IAS), UNESCO Chair in Institutional Research and Higher Education Policy, Adam Mickiewicz University in Poznan, Poland b Poznan University of Economics and Business, Poznan, Poland a r t i c l e i n f o a b s t r a c t Keywords: Research collaboration co-authorships gender gap sociology of science homophily scientific careers publishing patterns probabilistic record linkage sex differences 1. Introduction We examined the male-female collaboration practices of all internationally visible Polish uni- versity professors (N = 25,463) based on their Scopus-indexed publications from 2009–2018 (158,743 journal articles). We merged a national registry of 99,935 scientists (with full admin- istrative and biographical data) with the Scopus publication database, using probabilistic and deterministic record linkage. Our unique biographical, administrative, publication, and citation database ( “The Polish Science Observatory ”) included all professors with at least a doctoral de- gree employed in 85 research-involved universities. We determined what we term an “individual publication portfolio ” for every professor, and we examined the respective impacts of biological age, academic position, academic discipline, average journal prestige, and type of institution on the same-sex collaboration ratio. The gender homophily principle (publishing predominantly with scientists of the same sex) was found to apply to male scientists —but not to females. The majority of male scientists collaborate solely with males; most female scientists, in contrast, do not collab- orate with females at all. Across all age groups studied, all-female collaboration is marginal, while all-male collaboration is pervasive. Gender homophily in research-intensive institutions proved stronger for males than for females. Finally, we used a multi-dimensional fractional logit regres- sion model to estimate the impact of gender and other individual-level and institutional-level independent variables on gender homophily in research collaboration. Science is a collaborative enterprise, with (male and female) scientists collaborating internationally, nationally, and institutionally ( Wuchty, Jones, & Uzzi, 2007 ; Wagner, 2018 ). However, this is not our topic: our focus is on male–male, female–female, and male–female (or mixed-sex) research collaboration rather than collaboration across countries and institutions. The dominating view in liter- ature is that, on average, males collaborate more often with males, and females collaborate more often with females ( Jadidi, Karimi, Lietz, & Wagner, 2018 ; Lerchenmueller, Hoisl, & Schmallenbach, 2019 ; Wang, Lee, West, Bergstrom, & Erosheva, 2019 ; Holman & Morandin, 2019 ; Boschini & Sjögren, 2007 ; McDowell & Smith, 1992 ). This hypothesis is being tested using a large-scale dataset with unique variables. According to the homophily principle, “similarity breeds connection ”; consequently, personal networks are homogeneous with regard to many sociodemographic and personal characteristics (such as age, ethnic origin, class origin, wealth, education, and gender). On the positive side, homophily is reported to simplify communication ( McPherson, Smith-Lovin, & Cook, 2001 ; Kegen, 2013 ). However, on the negative side, homophily may “limit people’s social worlds in a way that has powerful implications for the information E-mail addresses: kwiekm@amu.edu.pl (M. Kwiek), wojciech.roszka@ue.poznan.pl (W. Roszka). https://doi.org/10.1016/j.joi.2021.101171 Received 6 June 2020; Received in revised form 29 April 2021; Accepted 5 May 2021 1751-1577/© 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \fM. Kwiek and W. Roszka Journal of Informetrics 15 (2021) 101171 they receive, the attitudes they form, and the interactions they experience ” ( McPherson et al., 2001 ). As science is increasingly collaborative, the homophily principle may increasingly influence academic careers. Research collaboration in science (or gender co-authorship patterns) provides fertile ground to test the homophily principle. Man–woman research collaboration patterns in science are contrasted in this paper through six lenses: biological age, academic position, academic discipline, gender-defined research collaboration type, journal prestige, and institutional research intensity. The individual scientist, rather than the individual article, is the unit of analysis. The key innovative methodological step is the determi- nation of what we term an “individual publication portfolio ” (for the decade of 2009–2018) for every internationally visible Polish scientist (N = 25,463 university professors from 85 universities, grouped into 27 disciplines, along with their 164,908 international collaborators, who together authored 158,743 Scopus-indexed publications). Co-authorships are used for the operationalization of research collaboration, following standard bibliometric practice. The individual publication portfolio reflects the distribution of gender-defined research collaboration types (same-sex collaboration and mixed-sex collaboration) for every individual scientist. Team formation in academia, understood as publishing with coauthors of varying numbers and different genders, is voluntary ( McDowell & Smith, 1992 ): researchers team up when they think that they are better off collaborating than publishing alone. The teams formed, or the articles published, are likely to reflect “individual tastes and perceptions of the returns to collaboration, as well as the costs of coordination ” ( Boschini & Sjögren, 2007 , p. 327). Some male scientists collaborate predominantly with other males, and some female scientists collaborate predominantly with other females. Still, others prefer to publish in mixed-sex collaborations (or to author individually). We examine the same-sex collaboration ratio at an individual level of every internationally visible Polish scientist (i.e., only authors with Scopus-indexed publications) and generalize the results from the individual level to the level of the national higher education system. 2. Literature review 2.1. The gender context of science The gender context of academic science has changed substantially in the past few decades ( Huang, Gates, Sinatra, & Barabàsi, 2020 ; Larivière, Ni, Gingras, Cronin, & Sugimoto, 2013 ), with more female scientists entering the higher education sector ( Elsevier, 2018 ) and occupying high academic positions ( Zippel, 2017 ; Diezmann & Grieshaber, 2019 ). Male and female scientists often pursued or were pushed onto somewhat different career tracks and were located in different academic structures, with “differential access to valuable resources ” ( Xie & Shauman, 2003 , p. 193). Females, as new entrants into a traditionally male-dominated academic profession, initially did not have equal access to professional networks ( McDowell, Singell, & Stater, 2006 ). But the academic world is changing. New bibliometric literatures applying the various gender-determination methods to authors and authorships ( Halevi, 2019 ; Elsevier, 2020 ) bring new data-driven insights to gender disparities in science, and literatures have become much less based on anecdotal and localized studies ( Larivière et al., 2013 ). Women are plugging into networks over time as the profession becomes more gender representative (as shown for academic economists by McDowell et al., 2006 , p. 154). However, somewhat paradoxically, the increased participation of women in STEM disciplines is reported to have been accompanied by an increase in gender differences regarding both productivity and impact ( Huang et al., 2020 , p. 8; Elsevier, 2018 , p. 16). As recent literature highlights, female scientists occupy more junior positions and receive lower salaries, are more often in non- tenure-track and teaching-only positions, are promoted more slowly, are less likely to be listed as either first or last author on a paper, and are allocated less research funding from national research councils. Women also tend to be less involved in international collaboration; female collaborations are more domestically oriented than are the collaborations of males from the same country; and females have less-prestigious collaborations and fewer collaborations overall (see Holman & Morandin, 2019 ; Halevi, 2019 ; Larivière et al., 2013 ; Larivière et al., 2011 ; Aksnes, Rørstad, Piro, & Sivertsen, 2011 ; Aksnes, Piro, & Rørstad, 2019 ; Huang et al., 2020 ; Maddi, Larivière, & Gingras, 2019 ; Fell & König, 2016 ; van den Besselaar & Sandström, 2016 ; Nielsen, 2016 ). In every country studied recently in Elsevier (2020) and Elsevier (2018) , the percentage of women who publish internationally is lower than the percentage of men who do so; for Poland, which is not included in the Elsevier reports, these publishing patterns are confirmed for various collaboration intensity levels and for various age groups; see Kwiek & Roszka, 2020 , on gender disparities in international collaboration). Female scientists in Poland constitute a substantial, highly productive, and highly internationalized part of the academic work- force, which is often the case in formerly communist European countries, which exhibit greater gender parity than the world and the OECD averages ( Larivière et al., 2013 , p. 212). Poland has a higher proportion of professors than any country studied in Larivière et al. (2013) or in Diezmann and Grieshaber (2019) , reaching 29.82% in 2018 ( GUS, 2019 , p. 220), even though there is a clear “the higher the fewer ” pattern acro",
            {
                "entities": [
                    [
                        235,
                        247,
                        "AUTHOR"
                    ],
                    [
                        251,
                        267,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "NIH Public AccessAuthor ManuscriptMed Image Anal. Author manuscript; available in PMC 2011 December 1.Published in final edited form as:Med Image Anal. 2010 December ; 14(6): 770–783. doi:10.1016/j.media.2010.06.002.Detection of Neuron Membranes in Electron Microscopy Imagesusing a Serial Neural Network ArchitectureElizabeth Jurrusa,b, Antonio R. C. Paivaa, Shigeki Watanabec, James R. Andersone, BryanW. Jonese, Ross T. Whitakera,b, Erik M. Jorgensenc, Robert E. Marce, and TolgaTasdizena,daScientific Computing and Imaging InstitutebSchool of Computing, University of UtahcDepartment of Biology, University of UtahdDepartment of Electrical Engineering, University of UtaheMoran Eye Center, University of Utah School of MedicineAbstractStudy of nervous systems via the connectome, the map of connectivities of all neurons in that system,is a challenging problem in neuroscience. Towards this goal, neurobiologists are acquiring largeelectron microscopy datasets. However, the shear volume of these datasets renders manual analysisinfeasible. Hence, automated image analysis methods are required for reconstructing the connectomefrom these very large image collections. Segmentation of neurons in these images, an essential stepof the reconstruction pipeline, is challenging because of noise, anisotropic shapes and brightness,and the presence of confounding structures. The method described in this paper uses a series ofartificial neural networks (ANNs) in a framework combined with a feature vector that is composedof image intensities sampled over a stencil neighborhood. Several ANNs are applied in seriesallowing each ANN to use the classification context provided by the previous network to improvedetection accuracy. We develop the method of serial ANNs and show that the learned context doesimprove detection over traditional ANNs. We also demonstrate advantages over previous membranedetection methods. The results are a significant step towards an automated system for thereconstruction of the connectome.KeywordsMachine Learning; Membrane Detection; Auto-Context; Artificial Neural Networks; Filter Bank;Contour Completion; Neural Circuit ReconstructionI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscript1. IntroductionNeural circuit reconstruction, i.e. the connectome [1], is currently one of the grand challengesfacing neuroscientists. Similarly, the National Academy of Engineering has listed reverse-engineering the brain as one its grand challenges 1. While neural circuits are central to the study© 2010 Elsevier B.V. All rights reserved.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customerswe are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resultingproof before it is published in its final citable form. Please note that during the production process errors may be discovered which couldaffect the content, and all legal disclaimers that apply to the journal pertain.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptJurrus et al.Page 2of the nervous system, relatively little is known about differences in existing neuronal classes,patterns, and connections. Electron microscopy (EM) is an unique modality for scientistsattempting to map the anatomy of individual neurons and their connectivity because it has aresolution that is high enough to identify synaptic contacts and gap junctions. These areimportant indicators for types of neuron topology and are required for neural circuitreconstruction. Several researchers have undertaken extensive EM imaging projects in orderto create detailed maps of neuronal structure and connectivity [2, 3]. Early work in this area,by White et al. [4], includes the complete mapping of the nematode C. elegans nervous system.This is a simple organism, containing just over 300 neurons and 6000 synapses, yet it tooknearly a decade to identify all the relevant structures and reconstruct the connectivity 2. Incomparison, newer imaging techniques are producing much larger volumes of very complexorganisms, with thousands of neurons and millions of synapses [5,6]. Thus, automating thereconstruction process is of paramount importance.The ability to reconstruct neural circuitry at ultrastructural resolution is of substantial clinicalimportance. Retinal degenerative diseases, including pigmentosa and macular degeneration,result from a loss of photoreceptors. Photoreceptor cell stress and death induces subsequentchanges in the neural circuitry of the retina resulting in corruption of the surviving retinal cellclass circuitry. Ultrastructural examination of the cell identity and circuitry reveal substantialchanges to retinal circuitry with implications for vision rescue strategies [7,8,9,10,11,12,13].These findings in retinal degenerative disease mirror findings in epilepsy where neural circuitsalso undergo remodeling in presumed response to abnormal electrical activity clinicallymanifested as seizures. Scientists are interested in examining normal and pathological synapticconnectivities and how neuronal remodeling contributes to neuronal pathophysiology [14,15,16]. Examination of synaptic and dendritic spine formation during development provide insightinto the adaptivity of neural circuits [17,18]. Ultrastructural evaluation of multiple canonicalvolumes of neural tissue are critical to evaluate differences in connectivity between wild typeand mutants. The complexity and size of the these datasets, often approaching 17 terabytes,makes human segmentation of the complex textural information of electron microscopicimagery a difficult task. Moreover, population or screening studies become unfeasible sincefully manual segmentation and analysis would require multiple years of manual effort perspecimen. As a result, better image processing techniques are needed to help with automatedsegmentation of EM data including identification of neurons and the connections.1.1. Serial-section transmission electron microscopyThe modality we have chosen for reconstructing the connectome at the individual cell level isserial-section transmission electron microscopy (TEM). It provides scientists with images thatcapture the relevant structures; however, it poses some interesting challenges for imageprocessing. Most importantly, serial-section TEM offers a relatively wide field of view toidentify large sets of cells that may wander significantly as they progress through the sections.It also has an in-plane resolution that is high enough for identifying synapses. In collectingimages through TEM, sections are cut from a specimen and suspended so that an electron beamcan pass through it creating a projection. The projection can be captured on a piece of film andscanned or captured directly as a digital image. An important trade-off occurs with respect tothe section thickness. Thinner sections are preferable from an image analysis point of viewbecause structures are more easily identifiable due to less averaging. However, from anacquisition point of view, thinner sections are harder to handle and impose a limit on the areaof the section that can be cut. For instance, in the rabbit retina, scientists need to study sections1William Perry, Farouk El-Baz, Wesley Harris, Calestous Juma, Raymond Kurzweil, and Robert Langer, The unveiling of the grandchallenges for engineering, in AAAS Meeting, Feb 2008.2Emily Singer, A wiring diagram of the brain, Technology Review, Nov 2007.Med Image Anal. Author manuscript; available in PMC 2011 December 1.   \fI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptI-NHPAAuthorManuscriptJurrus et al.Page 3with areas as large as 250µm in diameter to gain a sufficient understanding of neuralconnectivity patterns. Sections of this size can be reliably cut at 50 – 90nm thickness with thecurrent serial section TEM technology. This leads to an extremely anisotropic resolution, 2 –5nm in-plane compared to 50 – 90nm out-of-plane, and poses two image processing challenges.First, the cell membranes can range from solid dark curves for neurons that run approximatelyperpendicular to the cutting-plane, to grazed grey swaths for others which run more obliquelyand suffer more from the averaging effect. Consequently, segmentations of neurons in these2-D images, are difficult given the change in membrane contrast and thickness. Second, dueto the large physical separation between sections, shapes and positions of neurons can changesignificantly between adjacent sections.There are alternative specimen preparation and EM imaging techniques that can be used forneural circuit reconstruction such as Serial-Block Face Scanning Electron Microscopy.Briggman and Denk proposed a specimen preparation which only highlights extracellularspaces removing almost all contrast from intracellular structures [5]. However, it is not possibleto identify synapses with that approach. Identification of synapses is an important part of neuralcircuit reconstruction because it determines which cells are communicating, and where in thecircuitry they connect. To highlight synapses in TEM, scientists must use a stain that alsohighlights intracellular structures, such as vesicles and mitochondria, as well as neuronmembranes. Therefore, image segmentation techniques must account for these datacharacteristics in order to identify and successfully track neurons across hundreds of sections.1.2. Neuron segmentationThere are two general approaches for neuron segmentation. One approach focuses first on thedetection of neuron membranes in each 2-D section [19,20,21]. These boundaries can be usedto identify individual neurons, which are then linked across sections to form a complete neuron.Unfortunately, accurate detection of neuron membranes in EM is a difficult problem given thepresence of intracellular structures. This makes simple thresholding, edge detection (i.e.,Canny), and region growing method",
            {
                "entities": [
                    [
                        359,
                        376,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Time-efficient sparse analysis of histopathological WholeSlide ImagesChao-Hui Huang, Antoine Veillard, Nicolas Lomenie, Daniel Racoceanu,Ludovic RouxTo cite this version:Chao-Hui Huang, Antoine Veillard, Nicolas Lomenie, Daniel Racoceanu, Ludovic Roux. Time-efficientsparse analysis of histopathological Whole Slide Images. Computerized Medical Imaging and Graphics,2010, pp.5. ￿hal-00553877￿HAL Id: hal-00553877https://hal.science/hal-00553877Submitted on 10 Jan 2011HAL is a multi-disciplinary open accessarchive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come fromteaching and research institutions in France orabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL, estdestinée au dépôt et à la diffusion de documentsscientifiques de niveau recherche, publiés ou non,émanant des établissements d’enseignement et derecherche français ou étrangers, des laboratoirespublics ou privés.\fTime-efficient sparse analysis of histopathological Whole Slide ImagesChao-Hui Huangb,c, Antoine Veillardb,c, Nicolas Lom´eniea,b, Daniel Racoceanua,b,c,d, Ludovic Rouxb,e,∗aCentre National de la Recherche Scientifique (CNRS), Paris, FrancebIPAL (Image & Pervasive Access Lab), International Mixed Research Unit UMI CNRS 2955 (CNRS, NUS, I2R/A*STAR, UJF),Singapore (http://ipal.i2r.a-star.edu.sg/)cNational University of Singapore, SingaporedUniversity of Franche-Comt´e, Besan¸con, FranceeUniversity Joseph Fourier, Grenoble, FranceAbstractHistopathological examination is a powerful method for the prognosis of critical diseases. But, despite significantadvances in high-speed and high-resolution scanning devices or in virtual exploration capabilities, the clinical analysisof Whole Slide Images (WSI) largely remains the work of human experts. We propose an innovative platform in whichmulti-scale computer vision algorithms perform fast analysis of a histopathological WSI. It relies on specific high andgeneric low resolution image analysis algorithms embedded in a multi-scale framework to rapidly identify the high powerfields of interest used by the pathologist to assess a global grading. GPU technologies as well speed up the globaltime-efficiency of the system.In terms ofvalidation, we are designing a computer-aided breast biopsy analysis application based on histopathology images anddesigned in collaboration with a pathology department. The current ground truth slides correspond to about 36,000high magnification (40X) high power fields. The time processing to achieve automatic WSI analysis is on a par with thepathologist’s performance (about ten minutes a WSI), which constitutes by itself a major contribution of the proposedmethodology.In a sense, sparse coding and sampling is the keystone of our approach.Keywords: Histopathology, breast cancer, Whole Slide Image, multi-scale analysis, dynamic sampling, virtualmicroscope, Graphics Processing Unit1. IntroductionHistopathology is widely accepted as a powerful goldstandard for prognosis in critical diseases such as breast,prostate, kidney and lung cancers, allowing to narrow bor-derline diagnosis issued from standard macroscopic non-invasive analysis such as mammography and ultrasonog-raphy. At the molecular/genetic scale as well challengingmethods recently emerged for clinical diagnosis purposes.However, histomorphology as operated in hospitals is andwill remain the basis for most cancer classification.The histopathological image analysis process has largelyremained the work of human experts so far. At the hos-pital level, the task consists in the daily examination ofhundreds of slides, directly impacting critical diagnosisand treatment decisions. According to pathologists’ opin-ion [1], such a tedious manual work is often inconsistentand subjective, lacking traceability and computer assistedIn addition,analysis/annotation/grading support tools.hospitals will have to manage a shortage of expert pathol-ogists keen at doing this kind of unrewarding tasks.A few image analysis algorithms and automated grad-ing systems dedicated to breast histopathology images have∗Corresponding authorEmail address: vislr@i2r.a-star.edu.sg (Ludovic Roux)already been studied. Est´evez et al.[2] and Schnorren-ber et al.[3] worked on Fine Needle Aspiration (FNA)biopsies. FNA images are relatively easier to analyze thanWSIs since such an examination has limited diagnostic op-tions and produces mostly well separated cells over a well-contrasted background. Petushi et al.[4, 5] introduceda system able to label several histological and cytologicalmicrostructures in high resolution frames, including differ-ent grades of epithelial cells, fat cells and stroma. Doyleet al. [6, 7] proposed a method based on geometrical fea-tures, to distinguish between healthy tissue, low grade andhigh grade cancer. Tutac et al. [8] initiated an innovativeknowledge guided approach relying on the prior modelingof medical knowledge using ontology designed accordingto the clinical standard called Nottingham Grading Sys-tem [9]. An extension to this work involving multi-scaleapproaches was proposed by Dalle et al. [10].In close collaboration with a histopathology depart-ment, we built up a high-speed WSI analysis platform ableto detect scale-dependent meaningful regions of interest inmicroscopic biopsy images. This platform is dedicated tothe grading of breast cancer for prognosis purposes butthe methodology we present here is quite generic. Weuse a standard optical microscope that can be found inmost of the analysis laboratories in pathology or bacteri-Preprint submitted to Computerized Medical Imaging and GraphicsMay 27, 2010\fology (in our case, an optical microscope Olympus BX51,with 4X/10X/40X/60X/100X possible magnifications, PriorH101A ProScanII motorized X/Y stage and Z focus with atravel range of 114mm×75mm and a minimum step size of0.02μm, and a 1280×1024 pixels digital camera MediaCy-bernetics “EvolutionLC color” IEEE1394 MegaPixel). Weuse a MediaCybernetics controller connected to the micro-scope to perform an acquisition of high power fields/frames(in our study at 40X magnification according to the requestof the pathologist for the high resolution analysis). Theacquired 40X high power fields are stitched together in or-der to obtain the WSI.To the best of our knowledge, most of the previousresearch works focused on the analysis of individual highresolution frames [11] and/or proposed solutions too com-putationally expensive to be applied at the WSI level [12].A few notable exceptions [13] rely on the analysis of lowerresolution images for the selection of regions of interest.Unfortunately, there is little correlation between low res-olution images and the actual levels of nuclear pleomor-phism observable at high resolution for instance. There-fore, even such methods proved to be inefficient for theparticular issue of nuclear pleomorphism assessment onfull biopsy slides. As a consequence, the time-efficiencyproblem posed by the extremely large scale of biopsy im-ages (several thousands of frames) still lacks a practicalsolution.In this work, we propose solutions to improve efficiencyof such a microscopic platform both in terms of speed andprecision, in particular with a multi-scale dynamic sam-pling approach and the use of GPU programming. Theprocessing of a WSI starts by the detection of invasiveregions of interest (ROI) at low resolution level (1.2X).This method relies on a bio-inspired visual informationparadigm related to sparse coding and Graphics Process-ing Unit (GPU) implementation to dramatically speed-up the processing line. This part will be detailed in Sec-tion 2. Once the ROIs are detected, a map of local can-cer grades is established using a new generic multi-scale,computational geometry-based dynamic sampling methodcombined with high-resolution application specific imageanalysis algorithms. Then, this map is used to analyze theWSI within an operational time frame compatible with thepathology department’s needs and on a par with the pro-cessing time of an experimented pathologist. This part willbe detailed in Section 3. Finally, Section 4 elaborates onthe results and validation issues and Section 5 is dedicatedto conclusions, future research directions and challenges.2. Low Resolution Analysis and Sparse CodingRegion of interest (ROI) detection is a fundamentalphase of breast cancer grading for histopathological im-ages. Pathologists identify ROIs to efficiently select themost important invasive areas for the grading process.Since neither pathologists nor computers are able to ex-plore every details at high magnification within a reason-able time, the effective and efficient choice of the ROI isthus a critical step.In this study, ROI detection is casted as as a classifica-tion problem. The low magnification analysis will deter-mine if a given region is an invasive area in a similar man-ner as a pathologist would do when evaluating a biopsy. Inorder to mimic this behaviour, we exploit the relationshipbetween human vision and neuroscience [14].In the visual system, a set of opponent photo-receptorsforms a Receptive Field (RF). These photoreceptors forma field which is called ganglion RF since they collect vi-sual information and send neural spikes to a ganglion cell.Eventually, the ganglion cells produce various stimulationsand send them to the primary visual cortex [15].In the primary visual cortex, there are two major kindsof cells: simple- and complex-cells. Generally speaking,these cells produce two kinds of visual features: first- andsecond-order features [16]. The first-order feature containsthe information of intensities of various color channels, andthe second-order feature includes spatial variance of visualsignal [17, 18].In this study, we simulate some mechanisms of the hu-man visual system, generate the first- and second- orderfeatures as the mechanisms in the human visual system.However, the human visu",
            {
                "entities": [
                    [
                        84,
                        101,
                        "AUTHOR"
                    ],
                    [
                        185,
                        202,
                        "AUTHOR"
                    ],
                    [
                        1091,
                        1108,
                        "AUTHOR"
                    ],
                    [
                        238,
                        251,
                        "AUTHOR"
                    ],
                    [
                        1158,
                        1171,
                        "AUTHOR"
                    ],
                    [
                        102,
                        118,
                        "AUTHOR"
                    ],
                    [
                        203,
                        219,
                        "AUTHOR"
                    ],
                    [
                        119,
                        136,
                        "AUTHOR"
                    ],
                    [
                        220,
                        237,
                        "AUTHOR"
                    ],
                    [
                        1133,
                        1150,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "1 Segmentation of histological images and fibrosis identification with a convolutional neural network Xiaohang Fu1 · Tong Liu2 · Zhaohan Xiong1 · Bruce H. Smaill1 · Martin K. Stiles3 · Jichao Zhao1 tissue Abstract Segmentation of histological images is one of the most crucial tasks for many biomedical analyses including quantification of certain type. However, challenges are posed by high variability and complexity of structural features in such images, in addition to imaging artifacts. Further, the conventional approach of manual thresholding is labor-intensive, and highly sensitive to inter- and intra-image intensity robust automated variations. An accurate and segmentation method is of high interest. We propose and evaluate an elegant convolutional neural network (CNN) designed for segmentation of histological images, particularly those with Masson’s trichrome stain. The network comprises of 11 successive convolutional – linear unit – batch normalization layers, and outperformed state-of-the-art CNNs on a dataset of cardiac histological images (labeling fibrosis, myocytes, and background) with a Dice similarity coefficient of 0.947. With 100 times fewer (only 300 thousand) trainable parameters, our CNN is less susceptible to overfitting, and is efficient. Additionally, it retains image resolution from input to output, captures fine-grained details, and can be trained end-to-end smoothly. To the best of our knowledge, this is the first deep CNN tailored for the problem of concern, and may be extended to solve similar segmentation tasks to facilitate investigations into pathology and clinical treatment. rectified Xiaohang Fu xfu697@aucklanduni.ac.nz Jichao Zhao j.zhao@auckland.ac.nz 1 2 3 Auckland Bioengineering Institute, The University of Auckland, Auckland 1142, New Zealand Department of Cardiology, Second Hospital of Tianjin Medical University, and Tianjin Key Laboratory of Ionic-Molecular Function of Cardiovascular Disease, Tianjin Institute of Cardiology, Tianjin 300201, P.R. China Waikato Hospital, Hamilton 3204, New Zealand Keywords Convolutional neural network learning Histology Fibrosis Image segmentation  Deep ····1 Introduction Accurate segmentation of biomedical images is fundamental for quantitative analysis. However, this task is challenging due to characteristically high inhomogeneity and complexity of features in such images, as well as inter- and intra-plane artifacts introduced as a result of the imaging procedure and methodology, such as lighting. Manual thresholding is the most prevalent method for segmenting biomedical images, for example to identify fibrosis or scarring in histology [1, 2]. Whilst straightforward in principle, this approach is labor-intensive, time-consuming, and may involve tedious re-adjustments of thresholds [3, 4]. Thresholds are also highly sensitive to subject-dependent biases, as well as inter- and intra-image intensity variations (since spatial information is not accounted for) [5, 6]. Thus, a variety of thresholds is commonly necessary for one image set. For such reasons, manual thresholding may not be feasible for large datasets, especially those containing considerable variability in intensity, contrast, or brightness. Interpretation of raw pixel intensities to image meaning or context is no trivial task for algorithms. A slight difference in image features such as illumination may be negligible to humans, but can result in a disparate algorithmic outcome. Numerous methods have been established to separate an image into groups displaying similar features, and thereby identify the class object of each pixel. Earlier segmentation techniques rely on distinguishing edges, regions, or textures [6]. However, for image data with highly irregular heterogeneous illumination, or variable coloring of similar objects, considerable pre- or postprocessing is required, thus rendering such techniques unattractive and largely unsuitable. structural features,           \f(ground to produce outputs from In recent years, machine learning for computer vision has advanced extensively, emerging as a powerful tool for a wide range of image recognition problems [7–10]. Machine learning methods can be generally classed as unsupervised or supervised. In the former, the algorithm identifies patterns in the input without learning from example data annotated with desired outputs truth). Contrastingly, supervised models are trained on labelled data, learning rules inputs. Unsupervised methods including k-means clustering [11, 12], mean-shift clustering [13], and Markov random fields [14], as well as earlier supervised approaches such as support vector machines [10] have previously been employed to segment histological images. However, these methods typically suffer the requirement of supplementary algorithms (e.g. for postprocessing [12]) to complete the segmentation objective, or additional domain expertise to define and extract suitable features from images, which are often based on strong assumptions about the data. Convolutional neural networks (CNNs) are generating great enthusiasm particularly in computer vision. Conceptualized in the 1980s [15], CNNs were biologically inspired by the visual cortex; neurons fire in response to certain features or patterns in their local receptive fields, thereby acting as spatial filters [16]. CNNs effectively map highly complex relationships between the input and desired output (such as those of shapes and colors present through interconnected stacks of nonlinear functions (most fundamentally convolutions). Contrary to alternative supervised learning approaches such as support vector machines, there is no manual hand-crafting or fine-tuning of useful features in the input. CNNs can achieve impressive performance and directly handle complex data with minimal manual effort. A CNN-based approach is fully automated and trained models are reusable after establishment. images), in Although the inception of neural networks was a few decades ago, deep networks with multiple stacked layers are a relatively recent development; brought about through progress in parallelized computation using GPUs, solutions to hindrances associated with training deep neural networks (such as rectified linear units (ReLU) for the vanishing gradient problem [17]), and the availability of very large datasets. Deep CNNs have proved to be powerful tools in a wide array of image-related applications, excelling image classification [7, 18, 19], handwriting recognition [20], object localization [21], and scene understanding [22, 23]. This technique has also successfully extended to semantic pixel-wise labeling and the biomedical in 2 domain in tasks such as image segmentation [24–26], detection [27], cell tracking [8], and computer-aided diagnosis [9]. Robust and automated segmentation methods that can overcome the inherent challenges of biomedical image segmentation are of great demand, especially for applications conventionally relying on a manual approach. An example is fibrosis identification in histology, a critical task in many key fields of clinical research including kidney failure [28], lung injury [29], hepatitis B [30], sinoatrial node [1], and atrial fibrillation [31]. Atrial fibrillation is the most common type of cardiac arrhythmia, associated with significant healthcare costs, reduced quality of life, morbidity and mortality [32]. The basic mechanisms behind its initiation and maintenance remain elusive, but accumulating recent evidence indicate that diabetes mellitus (DM) is a strong independent risk factor [31–34], and that atrial fibrosis or scarring (characterized by excessive extracellular matrix proteins including collagen) conditions under contributes considerably to arrhythmogenicity [31, 35, 36]. Quantification and comparison of atrial fibrotic remodeling under DM against controls will assist in illuminating the precise mechanisms underlying DM-induced atrial fibrillation. This requires segmentation of fibrosis from myocytes and background in a cardiac histological section, differentiated via the well-accepted Masson’s trichrome stain (Fig. 1). induced diabetic Fig. 1 Representative segmentation of fibrosis. Left typical original RGB image of left atrial tissue from a diabetes mellitus (DM) rabbit model imaged with Masson’s trichrome stain at 40× magnification (image size 0.33 mm × 0.25 mm, pixel spatial resolution      161.25 nm × 161.25 nm); red, white and blue respectively indicate healthy myocytes, fat or extracellular space, and fibrosis. Right segmented fibrotic regions via a thresholding approach shown in blue In this paper, we propose a CNN for automated segmentation of stained histology images into a required number of tissue types, with particular focus on quantifying fibrosis in cardiac sections. To the best of our knowledge, this is the first CNN designed for this application. The deep CNN displays state-of-the-art segmentation accuracy with drastically fewer parameters, and substantially greater efficiency. More   \fimportantly, our proposed CNN architecture can be extended to other similar segmentation tasks to facilitate understanding of certain diseases and to aid targeted clinical treatment. We also make our source code freely available online for the benefit of potential users. 2 Methods and experiments 2.1 Overview of CNNs for segmentation image, with During a forward pass through a CNN, characteristics specific to certain structures in an input image (such as intensity and spatial information) are discerned by trainable filters in convolutional layers. Convolutional filters (typically 3 × 3 pixels) sweep across the entire visual field of the input volume by a constant stride, thus allowing the CNN to detect features in the input regardless of their exact position. Convolutional layers compute dot products between learnable filter weights and a corresponding region of the input slice, generating activation or feature maps. Thus, activations co",
            {
                "entities": [
                    [
                        101,
                        113,
                        "AUTHOR"
                    ],
                    [
                        1641,
                        1653,
                        "AUTHOR"
                    ],
                    [
                        116,
                        125,
                        "AUTHOR"
                    ],
                    [
                        128,
                        142,
                        "AUTHOR"
                    ],
                    [
                        184,
                        196,
                        "AUTHOR"
                    ],
                    [
                        1678,
                        1690,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptComput Biol Med. Author manuscript; available in PMC 2023 January 31.Published in final edited form as:Comput Biol Med. 2021 May ; 132: 104353. doi:10.1016/j.compbiomed.2021.104353.In-silico development and assessment of a Kalman filter motor decoder for prosthetic hand controlMai Gamala,b, Mohamed H. Mousac, Seif Eldawlatlyb,d, Sherif M. Elbasiounyc,e,*aCenter for Informatics Science, Nile University, Giza, EgyptbComputer Science and Engineering Department, Faculty of Media Engineering and Technology, German University in Cairo, Cairo, EgyptcDepartment of Biomedical, Industrial, and Human Factors Engineering, Wright State University, Dayton, OH, USAdComputer and Systems Engineering Department, Faculty of Engineering, Ain Shams University, Cairo, EgypteDepartment of Neuroscience, Cell Biology, and Physiology, Wright State University, Dayton, OH, USAAbstractUp to 50% of amputees abandon their prostheses, partly due to rapid degradation of the control systems, which require frequent recalibration. The goal of this study was to develop a Kalman filter-based approach to decoding motoneuron activity to identify movement kinematics and thereby provide stable, long-term, accurate, real-time decoding. The Kalman filter-based decoder was examined via biologically varied datasets generated from a high-fidelity computational model of the spinal motoneuron pool. The estimated movement kinematics controlled a simulated MuJoCo prosthetic hand. This clear-box approach showed successful estimation of hand movements under eight varied physiological conditions with no retraining. The mean correlation coefficient of 0.98 and mean normalized root mean square error of 0.06 over these eight datasets provide proof of concept that this decoder would improve long-term integrity of performance while performing new, untrained movements. Additionally, the decoder operated in real-time (~0.3 ms). Further results include robust performance of the Kalman filter when re-trained to more severe post-amputation limitations in the type and number of motoneurons remaining. An additional analysis shows that the decoder achieves better accuracy when using the firing of individual motoneurons as input, compared to using aggregate pool firing. Moreover, the decoder demonstrated robustness to noise affecting both the trained decoder parameters and the decoded motoneuron activity. These results demonstrate the utility of a proof of concept Kalman filter decoder that can support prosthetics’ control systems to maintain accurate and stable real-time movement performance.*Corresponding author. 3640 Colonel Glenn Hwy, Dayton, OH, 45435, USA., sherif.elbasiouny@wright.edu (S.M. Elbasiouny). Declaration of competing interestThe authors do not have any conflict of interest.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptGamal et al.KeywordsMotoneurons; Firing rate; Decoding; Kalman filter; Prosthetic control1. IntroductionPage 2Limb loss dramatically limits the lifestyle of an amputee, and upper-limb prostheses have helped amputees to overcome this functional disability [1–3]. Muscle-based electromyographic (EMG) signals and neural-based electroneurographic (ENG) signals [4] have all been used to drive prosthesis control. The peripheral EMG and ENG signals particularly have potential to provide naturalistic control, as they contain detailed low-level information about the neural drive to the muscles [5]. This information, encoded via motoneuron action potentials or spike trains, provides a fine resolution of movement intention [6]. Thus, motor unit activity has been recorded in amputees from ENG signals through electrodes implanted in peripheral nerves [4,7–12]. Numerous spike-sorting algorithms have been applied to peripheral nerve recordings to extract individual motor unit activity [13]. Motor unit activity can also be indirectly measured by decoding from EMG activity using decomposition algorithms [4–6,14–22]. Such algorithms achieved better performance in movement estimation than did the conventional amplitude-based features of EMG activity [6,19–23]. More recently, the activity of individual motor units, indirectly measured from EMG with high-fidelity pin electrodes, has been demonstrated to provide more responsive, smooth, and proportional control compared to the conventional EMG features [23].However, several limitations and difficulties still impede amputees from attaining full, natural movement with their prostheses [1,24,25]. While state-of-the-art prostheses can perform complex movements, the control algorithms for prosthetic motion are a major reported factor in limited prosthesis functionality [1,24,25]. Thus, enhanced control systems are needed to provide more naturalistic control of prostheses [25,26]. Specific needs include improved prosthesis movement accuracy and response time [4,27]. In addition, amputees need the control system to respond accurately to new and different intended movements without retraining of the control algorithm [4,27]. Most importantly, amputees require improved longevity of prosthetic control systems. Currently, motor decoders become unreliable after a short period of real-life use. This greatly hinders their usefulness outside the laboratory [5], probably contributing to the abandonment of their upper-limb prostheses by up to 50% of amputees [25]. While the variety of decoding approaches and the limited duration of clinical testing [13] has made it difficult to quantitate the extent of this issue, some studies report neural features used by decoders degrading in just a few days [5] or that training is required on a weekly or even daily basis [9]. In consequence, amputees are burdened by either frequent recalibrations of their prostheses or a rapid decline in their limbs’ responsiveness.Such degraded performance occurs because the amputee’s physiological state changes continually, yet the limb is calibrated to one or few states and tested in one, or a few limited, conditions. For instance, one’s neuromodulatory state (the excitability level of one’s Comput Biol Med. Author manuscript; available in PMC 2023 January 31.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptGamal et al.Page 3motoneurons) fluctuates throughout the day in response to the intensity of motion required [28]. Second, decoders are also generally designed to mimic a limited range of “normal” motoneuron output, which result primarily from orderly recruitment (in which motoneurons are recruited from smallest to largest). However, reversed and mixed recruitment orders have been observed in animals and humans [29–31]. Third, on a longer timescale, amputation commonly causes neurodegeneration and ongoing shifts in motoneuron numbers and their electrical properties. These changes continue well beyond the original injury and lead to fewer and loss of certain types of motoneurons as well as higher heterogeneity in the electrical properties of remaining MNs [32]. These changes also allow procedures like bionic reconstruction and targeted muscle reinnervation (TMR) to take place leading to pool compartmentalization and changes in MN firing characteristic [33,34]. Also, hardware problems, such as electrode and wire breakage, lead to fewer cells or data channels to record from to feed the decoder with incoming information [5,35,36]. Together, these issues continue to represent critical barriers to the development of prosthesis control systems that provide long-term reliability and accuracy.Efforts to produce robustly accurate prosthetic control algorithms include pattern recognition and non-pattern recognition methods [4,27,37,38]. However, the ability of such approaches to generate naturalistic movements is limited [37,38]. Here, we propose to use the Kalman filter, which has been extensively utilized in neural decoding for prosthesis control [39,40] due to its high accuracy and fast computational speed [40]. Moreover, it is known to generate stable output from noisy input signals, which occur in motor decoding problems [5,9,41]. Importantly, the Kalman filter was also shown to be robust even without large amounts of training data [9]. Multiple studies used the Kalman filter in movement estimation by decoding data recorded either from the motor cortex [41–44], ENG signals [9,45,46], EMG signals [47–49], or by combining peripheral neural and EMG signals [50,51]. Together, these studies suggest that the Kalman filter has potential to enhance the performance of prosthetic control algorithms.Therefore, the objective of this study was to develop a motoneuron-based Kalman filter decoder to support advanced prosthetic hand control systems that maintain long-term, accurate, real-time performance, in the context of both new, untrained movements and ongoing physiological changes. To accomplish this, we employed a recently developed multi-scale, high-fidelity computational model of the motor pool by Ref. [52] to input simulated motoneuron spiking activity into our proposed decoder. The model, developed in our prior work, provides high-fidelity, 3D representations of all three types of motoneurons in the pool, and has been shown to provide a highly accurate simulation of firing behaviors [53]. The Kalman-based algorithm decodes the simulated activity to estimate the underlying synaptic input (i.e., excitation level) that drives the firing of the modeled motoneurons. The estimated synaptic input is then mapped to specific movements of a simulated prosthetic hand (by MuJoCo software). Using simulated input allows us to control, alter, and know the exact biological conditions in which the decoder is performing. Further, by using a simulated prosthetic hand, we can accurately compare hand movements resulting from decoded signals against hand movements resulting from the actual synaptic inputs, which can be directly input to the simulated prost",
            {
                "entities": [
                    [
                        408,
                        424,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Empirical Software Engineering: From Discipline to InterdisciplineDaniel M´endez Fern´andeza,∗, Jan-Hendrik PassothbaSoftware and Systems Engineering, Technical University of Munich, GermanybMunich Center for Technology in Society, Technical University of Munich, Germany8102voN71]ES.sc[3v20380.5081:viXraAbstractEmpirical software engineering has received much attention in recent years and coined the shift froma more design-science-driven engineering discipline to an insight-oriented, and theory-centric one. Yet,we still face many challenges, among which some increase the need for interdisciplinary research. This isespecially true for the investigation of social, cultural and human-centric aspects of software engineering.Although we can already observe an increased recognition of the need for more interdisciplinary research in(empirical) software engineering, such research configurations come with challenges barely discussed froma scientific point of view. In this position paper, we critically reflect upon the epistemological setting ofempirical software engineering and elaborate its configuration as an Interdiscipline. In particular, we (1)elaborate a pragmatic view on empirical research for software engineering reflecting a cyclic process forknowledge creation, (2) motivate a path towards symmetrical interdisciplinary research, and (3) adopt fiverules of thumb from other interdisciplinary collaborations in our field before concluding with new emergingchallenges. This supports to elevate empirical software engineering from a developing discipline movingtowards a paradigmatic stage of normal science to one that configures interdisciplinary teams and researchmethods symmetrically.Keywords: Empirical Software Engineering, Interdisciplinary Research, Symmetrical Collaboration,Science & Technology Studies1. IntroductionStarting as a byproduct in a hardware-dominatedworld, software has become the main driver for en-tire industries and a transformative power in manyfields of contemporary society. Software engineer-ing practice and research are likewise continuouslyevolving to cope with the emerging challenges im-posed by its ubiquitous nature: practical, institu-tional, and cultural contexts of software are dy-namic and in constant change, and as they influ-ence the shape and direction of software develop-ment, boundaries between systems and applicationdomains become fuzzy. Software engineering to-day typically takes place in settings where we needto address, inter alia, application domain-specificquestions (e.g. on domain-specific terminologies,concepts, and procedures), ethical questions (e.g.∗Corresponding authorEmail address: daniel.mendez@tum.de (Daniel M´endezFern´andez)moral assessments in context of safety-critical sit-uations), juridical questions (e.g. on data privacyor regulations of algorithms and their environmentrespectively), psychological questions (e.g. on im-provements of team communications or working en-vironments), or social and political questions (e.g.on societal impacts of software-driven technologies,the concerns of heterogenous actors, or accountabil-ity issues). Human actors – whether customers, endusers, or developers – and their interests, needs,and values, but also their cognitive capabilities,fears, experiences, and expertise render softwaredevelopment endeavours as something individualand unique rather than something standardised andstrictly formalised.In the end, software is devel-oped by human beings for human beings and whatworks in one organisational context might be com-pletely alien to the culture and needs of the next.This poses new challenges for configurations ofactors, skills, and methods in research and practice,as well as on the education of future software engi-Preprint submitted to Journal of Systems and SoftwareNovember 20, 2018   \fneers (and end users). Although we can already seemore and more calls for more interdisciplinary re-search and the integration of non-technical skills inhigher education (see, e.g., [1, 2]), the calls and pro-posals – especially those in Software Engineering –concentrate largely on educational aspects, e.g. onhow to reduce the gap between isolated disciplinaryconditions in academia and multidisciplinary real-life conditions in practice (see, e.g., [3, 4]).In-terdisciplinary research, however, comes with chal-lenges barely discussed from a scientific, epistemo-logical point of view in the software engineeringcommunity. Fields like health care and medicine,biology and neuroscience, or education have explic-itly tackled such issues in the last two decades, inparts driven by the need to reflect on conditions ofsuccess of NSF and EU funding initiatives to in-tegrate “ethical, legal and social issues / aspects”(ELSI/ELSA). Here, scholars discussed and defined“multidisciplinary”, “interdisciplinary” and “trans-disciplinary” research to classify the different chal-lenges and opportunities that arise from such con-figurations [5, 6, 7]:• Multidisciplinary projects involve researchersfrom various disciplines addressing a commonproblem in parallel (or sequentially) from theirdisciplinary-specific bases integrated by assem-bling results in a common book or workshop.• Interdisciplinary projects build on a collabo-ration of researchers working jointly, but eachstill from their disciplinary-specific basis, toaddress a common problem• Transdisciplinary projects involve researchers,practitioners and actors from governance agen-cies, NGOs or companies trying to work outa shared conceptual framework instrumental-ising concepts, approaches, and theories fromtheir parent disciplines.Being involved in inter- and transdisciplinaryprojects in stem cell research, neuroscience or urbanplanning, scholars have argued that even interdis-ciplinary collaborations involving only researchersfrom two or three disciplines create challenges veryclose to those formerly discussed only in the case oftransdisciplinary projects: without investing timeand effort (and money) into the search for com-mon problems, a provisional but common language,and institutional backup, interdisciplinary projectstend to turn into multidisciplinary ones. This holdstrue already for “close” interdisciplinary collabo-rations between fields like industrial automationand software engineering. More pressing, but also2more rewarding research challenges, as we will ar-gue, emerge when trying to integrate research onthe social, cultural and human-centric practices andcontexts of software engineering. Dealing with is-sues that arose from the proliferation of tools andframeworks and the complexity growth in softwareengineering projects, software engineering has al-ready turned itself into an evidence-driven empir-ical discipline, commonly known as empirical soft-ware engineering. However, the emerging majorchallenges need far more symmetrical forms of in-quiry and design going beyond not only the prefer-ence for rationalism and formal reasoning still dom-inant in large parts of software engineering research,but also beyond the forms of empiricism already inplace. They give rise to the need of symmetrical in-terdisciplinary research configurations at eye levelto come to valid, but still manageable solutions tothe problem of balancing different epistemic andpractical requirements and standards.Placing symmetrical interdisciplinary configura-tions at the heart of empirical software engineer-ing research – especially when involving social, cul-tural, and human-centric facets – has effects on theway that software engineering as a field can ad-dress questions central to defining it as a scientificdiscipline, such as What qualifies as (good) scien-tific practice?, What counts as theory, as method-ology, and as evidence? or How are scientific con-troversies opened up, embraced, and closed? Likein other inter- and transdisciplinary configuration,many conceptual problems and methodological is-sues in our field cannot be organised in terms ofone single and dominant epistemological frameworkif we do not want to close down necessary andfruitful exchanges. Because of the high diversityof socio-economic and technical factors that per-vade software engineering environments, it is – notonly from a practical and pragmatic perspective,but also from an epistemological point of view –not sufficient to just use methods and concepts fromvarious disciplines in our research, but we need toeffectively integrate methods and research compe-tences from the various disciplines.Contribution. In this position paper, we crit-ically reflect upon the broader epistemological set-ting of empirical software engineering and elaborateits configuration as an “Interdiscipline”. We willargue for stopping to treat empirical software engi-neering as a developing discipline moving towards aparadigmatic stage of normal science, but as a con-figuration of interdisciplinary teams and research\fmethods - an interdiscipline. To this end, we willmake the following contributions: We (1) elaboratea pragmatic view on empirical research for softwareengineering that reflects a cyclic process for knowl-edge creation, (2) motivate a path towards sym-metrical interdisciplinary research, and (3) adoptfive rules of thumb from other interdisciplinary col-laborations.The key addressees ofthis manuscript aretwofold: (1) Scholars who are new to empirical soft-ware engineering (or parts of it) in general and in-terdisciplinary research in particular, and (2) schol-ars already aware of the importance of empirical re-search methods and interdisciplinary research, andinterested in a broader epistemological view.Outline. We will first briefly elaborate onthe epistemological, methodological, and pragmaticbackground of research methods already used inempirical software engineering and highlight conse-quences for theory building, methods development,and application as well as for interdisciplinary col-laborations (Sect. 2). We use those insights to elab-orate a pragmatic view on empiri",
            {
                "entities": [
                    [
                        95,
                        115,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Data Augmentation using Generative Adversarial Neural Networks onBrain Structural Connectivity in Multiple SclerosisBerardino Barilea, Aldo Marzullob, Claudio Stamilec, Françoise Durand-Dubiefa,d andDominique Sappey-Mariniera,eaCREATIS (UMR 5220 CNRS & U1206 INSERM), Université Claude Bernard Lyon 1, Université de Lyon, Villeurbanne, FrancebDepartment of Mathematics and Computer Science, University of Calabria, Rende, ItalycR&D Department CGnal, Milan, ItalydHôpital Neurologique, Service de Neurologie A, Hôpital Civils de Lyon, Bron, FranceeCERMEP - Imagerie du Vivant, Université de Lyon, Bron, FranceA R T I C L E I N F OA B S T R A C TKeywords:Brain ConnectivityMultiple SclerosisData AugmentationGenerative Adversarial NetworksBackground and objective: Machine learning frameworks have demonstrated their potentials indealing with complex data structures, achieving remarkable results in many areas, including brainimaging. However, a large collection of data is needed to train these models. This is particularlychallenging in the biomedical domain since, due to acquisition accessibility, costs and pathology re-lated variability, available datasets are limited and usually imbalanced. To overcome this challenge,generative models can be used to generate new data.Methods: In this study, a framework based on generative adversarial network is proposed to createsynthetic structural brain networks in Multiple Sclerosis (MS). The dataset consists of 29 relapsing-remitting and 19 secondary-progressive MS patients. T1 and diffusion tensor imaging (DTI) acqui-sitions were used to obtain the structural brain network for each subject. Evaluation of the qualityof newly generated brain networks is performed by (i) analysing their structural properties and (ii)studying their impact on classification performance.Results: We demonstrate that advanced generative models could be directly applied to the structuralbrain networks. We quantitatively and qualitatively show that newly generated data do not presentsignificant differences compared to the real ones. In addition, augmenting the existing dataset withgenerated samples leads to an improvement of the classification performance (𝐹 1𝑠𝑐𝑜𝑟𝑒 81%) with re-spect to the baseline approach (𝐹 1𝑠𝑐𝑜𝑟𝑒 66%).Conclusions: Our approach defines a new tool for biomedical application when connectome-baseddata augmentation is needed, providing a valid alternative to usual image-based data augmentationtechniques.1. IntroductionArtificial intelligence has revolutionized many areas ofresearch, from economics and law to health-care. However,a large collection of data is essential for statistical evalua-tion and machine learning applications, particularly in thefield of deep learning (DL). Indeed, DL frameworks haveachieved remarkable results in many fields, such as patternrecognition, natural language processing, image processing,among others [1, 2]. The main advantage of using DL ap-plications lies in their great ability to recognize hidden pat-terns in the data, thanks to the multiple nonlinear transfor-mations produced by the sequential stacks of multiple lay-ers [3, 4] However, huge amount of data are required fortraining this kind of models while in the context of biomed-ical domain, and particularly in medical imaging, extensivedatasets are challenging to obtain due to systems availabil-ity, costs constraints, acquisition methodology, and pathol-ogy related variability [5, 6], resulting in small and imbal-anced dataset. Notwithstanding, when dealing with imageberardino.barile@creatis.insa-lyon.fr (B.Barile); marzullo@mat.unical.it (A. Marzullo);cstamile@cgnal.com (C. Stamile);francoise.durand-dubief@chu-lyon.fr (F. Durand-Dubief);sappey-marinier@univ-lyon1.fr (D. Sappey-Marinier)ORCID(s):data, different solutions have been proposed to overcomethese limitations [7]. A general and widely accepted solu-tion is to impose meaningless perturbations to the originaldata [8] or to apply more advanced techniques, like rota-tion, reflection, scaling among others. These approachesoffer straightforward alternatives for augmenting the train-ing set, allowing DL models to reach better performanceand/or more stable training [9]. Recently, with the rise ofDL, interesting alternatives have appeared and new gener-ative DL-based models were proposed to obtain syntheticdata with characteristics spanning the original data manifold[10]. Therefore, in this study we refer to generative modelsas a subclass of DL frameworks able to generate complexdata data structure, including the recent modeling approachused to characterize brain networks by means of graph the-ory [11, 12, 13]. Given the great capability of graphs to rep-resent complex relations among different areas of the brain,such relational data structure started to be widely employedin many contexts, including social behavioral studies. Addi-tionally, advances in brain image acquisition and computerassisted methods have begun to provide meaningful resultsin support of clinicians, leading to a steadily growing use inthe neuroscience community, particularly in brain imaging[14]. Using magnetic resonance imaging (MRI), functionalor structural brain connectivity can be obtained by analyz-Berardino Barile: Preprint submitted to ElsevierPage 1 of 12\fData Augmentation using Generative Adversarial Neural Networksing temporal correlations of gray matter (GM) activity withresting-state functional MRI (fMRI) or reconstructing whitematter (WM) fiber-bundles with diffusion tensor imaging(DTI), respectively. Such network-like structure of the hu-man connectome consists of nodes, defined by parcellisationof the brain grey matter (GM), and edges, correspondingto functional or structural links between the network nodes.These new approaches paved the way for a better character-ization of brain networks, particularly in brain diseases suchas Multiple Sclerosis (MS).MS is a demyelinating, inflammatory, chronic disease ofthe central nervous system [15]. While its etiology remainsunknown, MS is the most frequent disabling neurologicaldisease in young adults. Disease onset is characterized inabout 85% of cases [15], by a first acute episode calledclinically isolated syndrome (CIS) or a relapsing-remittingcourse (RRMS) followed by a secondary-progressive course(SPMS), while the remaining 15% of MS patients evolve di-rectly into a primary-progressive course (PPMS). The courseof the disease and the risk for developing permanent dis-ability are very different from one patient to another. Thus,the neurologist’s challenge is to predict the disease evolu-tion based on early clinical, biological and imaging mark-ers available from disease onset. However, the complexitybrought by conectome data is more cumbersome with re-spect to the grid-like pixel-by-pixel representation found inimages.In fact, due to the multiple interconnections be-tween different nodes, connectome data represent a chal-lenge for synthetic data generation for which simple opera-tions, like edge swapping, would end up changing the entirestructure of the graph network, jeopardizing the informationthey convey. [16].In this study, a generative adversarial network framework isproposed, namely Generative Adversarial Neural NetworkAutoEncoder (AAE). The framework is able to automati-cally generate synthetic structural brain connectivity data ofMS patients. To achieve this, a prior is imposed to the latentspace of the autoencoder network by means of an adversar-ial model. Moreover, a consistency loss is also introduced inorder to increase the stability of the training process. Newsamples of brain connectivity data are generated by drawingfrom the parametrized latent space. An overfitting analy-sis over generated graphs, by exploiting graph properties,is proposed for model evaluation. The synthetic generateddata can be used to augment the MS brain networks datasetto improve classification performances of classical machinelearning methods like the Random Forest Classifier.The paper is structured as follows. In Section 2, we illustratethe related literature, and in section 3, we provide a detaileddescription of our methodological approach. In Section 4,we describe our experimental results and finally, in Section5, we draw our conclusions.2. Related WorkDue to their ability to generate new data, generative mod-els have gained a lot of interest in the computer vision andmedical imaging research communities. The Generative Ad-versarial Network framework (GAN) has been previouslyused for generating realistic training images that synthet-ically augment datasets. Radford et al. [17] introduced aclass of generative model called deep convolutional gener-ative adversarial networks (DCGAN) to generate 2D brainMR images followed by an AE neural network for imagedenoising. Makhzani et al. [18] proposed a new methodfor regularizing AutoEncoders (AE) by imposing an arbi-trary prior on the latent representation. Calimeri et al. [19]proposed a GAN for the automatic generation of artificialMR images of the human brain. They demonstrated thatthe power of adversarial training could be exploited for thegeneration of brain networks data, which are more complexthan usual images.GAN frameworks have also shown to improve accuracy ofimage classification via generation of new synthetic trainingimages. Frid-Adar et al. [20], for instance, used syntheticmedical image augmentation with GAN for the classifica-tion of liver lesions. Similarly, Salehinejad et al. [21] usedthis framework to simulate pathology across five classes ofchest X-rays in order to augment the original imbalanceddataset and improve the performance of a convolutional modelin chest pathology classification. In the context of MS, Shui-Hua W. et al. [22] proposed a new transfer-learning-basedapproach to identify MS patients with higher accuracy, com-paring three different types of neural networks (DenseNet-121, DenseNet-169, and DenseNet-201), which make useof composite learning factors to different l",
            {
                "entities": [
                    [
                        134,
                        148,
                        "AUTHOR"
                    ],
                    [
                        150,
                        166,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Published in \"Medical image analysis\", 2018, vol. 43, pp. 66-84, which should be cited to refer to this work.DOI: 10.1016/j.media.2017.09.007Large-scale Retrieval for Medical Image Analytics: AComprehensive ReviewZhongyu Lia, Xiaofan Zhanga, Henning M¨ullerb, Shaoting Zhanga,∗aDepartment of Computer Science, University of North Carolina at Charlotte, USAbUniversity of Applied Sciences Western Switzerland (HES-SO), Sierre, SwitzerlandAbstractOver the past decades, medical image analytics was greatly facilitated by theexplosion of digital imaging techniques, where huge amounts of medical im-ages were produced with ever-increasing quality and diversity. However, con-ventional methods for analyzing medical images have achieved limited suc-cess, as they are not capable to tackle the huge amount of image data. In thispaper, we review state-of-the-art approaches for large-scale medical imageanalysis, which are mainly based on recent advances in computer vision, ma-chine learning and information retrieval. Specifically, we first present the gen-eral pipeline of large-scale retrieval, summarize the challenges/opportunitiesof medical image analytics on a large-scale. Then, we provide a comprehen-sive review of algorithms and techniques relevant to major processes in thepipeline, including feature representation, feature indexing, searching, etc.On the basis of existing work, we introduce the evaluation protocols andmultiple applications of large-scale medical image retrieval, with a variety ofexploratory and diagnostic scenarios. Finally, we discuss future directions oflarge-scale retrieval, which can further improve the performance of medicalimage analysis.Keywords: Medical image analysis, information retrieval, large scale,computer aided diagnosis∗Corresponding author, rutgers.shaoting@gmail.comPreprint submitted to Medical Image AnalysisSeptember 21, 2018\f123456789101112131415161718192021222324252627282930313233343536371. IntroductionMedical image analytics plays a central role in clinical diagnosis, image-guided surgery and pattern discovery. Many protocols and modalities ofdigital imaging techniques have been adopted to generate medical images,including magnetic resonance imaging (MRI) (Slichter, 2013), computed to-mography (CT) (Hsieh, 2009), photon emission tomography (PET) (Baileyet al., 2005), ultrasound (Szabo, 2004), fluorescence microscopy (Lichtmanand Conchello, 2005), X-ray (Lewis, 2004) and others. Generally, these med-ical images reflect specific aspects (anatomy, function) of tissue types/organsthat require an accurate interpretation and analysis from either domain ex-perts or computer-aided decision support. In comparison with domain ex-pert analysis that is labor intensive and time-consuming, computer-aidedapproaches are efficient and its accuracy has increased continuously withthe rapid development of computer vision, machine learning and relatedfields (Doi, 2014; Katouzian et al., 2012; May, 2010). To support computer-aided medical image analytics, one important task is content-based imageretrieval (CBIR) (Akg¨ul et al., 2011; Lehmann et al., 2004; M¨uller et al.,2004), i.e., indexing and mining images that contain a similar visual con-tent (e.g., shape, morphology, structure, etc). For a new medical image tobe analyzed, a CBIR system can first retrieve visually similar images in anexisting dataset. Then, its high-level descriptions and interpretations can beexplored based on the retrieved images.Over the past 25 years, CBIR has been one of the most vivid researchtopics in the field of computer vision. Many CBIR methods were developedfor accurate and efficient image retrieval. Especially in recent years, withthe ever-increasing number of digital images (e.g., ImageNet (Russakovskyet al., 2015), COCO (Lin et al., 2014), PASCAL VOC (Everingham et al.,2010), etc), CBIR has moved towards the era of big data. Massive amounts ofimages can provide rich information for comparison and analysis, and thusfacilitate the generation of new algorithms and techniques that can tackleIn general, large-scale image retrievalimage retrieval in large databases.can be divided into two stages, i.e., feature extraction to represent imagesand feature indexing. Deep learning (LeCun et al., 2015) is one of the mostpopular methods for feature representation that is particularly suitable forlarge image databases, where massive amounts of data can boost the retrievalperformance by training deep and complex neural networks with millions ofparameters (Babenko and Lempitsky, 2015; Wan et al., 2014). For the feature2\f38394041424344454647484950515253545556575859606162636465666768697071727374indexing at a large-scale, the key problem is computational efficiency, i.e.,similarity searching in millions of images with thousand dimensional featuresvectors. Methods such as vocabulary trees (Nister and Stewenius, 2006) andhashing (Wang et al., 2016) can efficiently tackle this problem, either throughchanging the indexing structure or compressing the original features.Despite the current large-scale methods having achieved many successesin generic image retrieval problems, how to best tackle the retrieval in large-scale medical image databases is still a very challenging topic (Zhang andMetaxas, 2016). On the one hand, the meaning of large-scale in the medicalimage field is somewhat different from large-scale in the generic image do-main. Generally, each patient can generate hundreds to thousands of imageslices using different protocols, modalities (e.g., CT, MRI, X-ray) and multi-ple dimensions (e.g., volumetric 3D, time series). These volumes are usuallystored in many single images (as slices) in the DICOM (Digitla Imaging andCommunications in Medicine) format (Kahn et al., 2007). Besides this, thesize of some medical images can be extremely large. For example, the whole-slide histopathological images can include more than 100, 000×100, 000 pixelsand thus each is usually split into millions of small patches for processing.On the other hand, medical images are usually more difficult to analyze com-pared to generic images. The complex imaging parameters (contrast agents,machine settings), anatomic difference and interactions between different dis-eases result in a more complex analysis compared with natural images, wherebroad object categories are recognized and used for similarity calculations.The relevant changes of some medical images can be very subtle, which re-quire more fine-grained and detailed analysis. Therefore, directly employingtraditional CBIR methods may not suitable for the large-scale medical imageretrieval problem. In recent years, many efforts have been made to achievelarge-scale medical image analytics, aiming to improve the efficiency andaccuracy of image retrieval.1.1. Related WorkThere have been multiple reviews focusing on content-based medical im-age retrieval. The first review in the field was (Tang et al., 1999) but thetext only contained few systems with a limited scope. Muller et al. (M¨ulleret al., 2004) presented a first complete review that concentrates on imageretrieval in the medical domain, where the techniques used in medical im-age retrieval, including visual feature extraction, image comparison, systemevaluation, etc. are summarized. Subsequently, Long et al. (Long et al.,3\f757677787980818283848586878889909192939495969798991001011021031041051061071081091101112009) introduced four medical CBIR systems, i.e., CervigramFinder (Xueet al., 2008), SPIRS (Hsu et al., 2007), IRMA (IRMA), SPIRS-IRMA (An-tani et al., 2007). The authors also discussed future directions of medicalimage retrieval. Akgul et al. (Akg¨ul et al., 2011) presented a comprehensivereview about recent techniques of content-based image retrieval in radiol-ogy until 2011, including image features/descriptors, similarity measures andstate-of-the-art systems. Additionally, they discussed challenges and futuredirections for the coming decade. Hwang et al. (Hwang et al., 2012) reviewedboth text-based and content-based medical image retrieval systems, drawinga conclusion that the image retrieval service will be more effective if CBIRand semantic systems are combined. In 2013, Kumar et al. (Kumar et al.,2013) surveyed several applications and approaches to medical CBIR thatfocus on clinical imaging data that are multidimensional or acquired usingmultiple modalities such as combined PET-CT images.Besides the abovementioned survey articles, the image retrieval task ofthe Conference and Labs of the Evaluation Forum, named ImageCLEF (Im-ageCLEF; M¨uller et al., 2010), has held several medical image retrieval tasksfrom 2004-2014. ImageCLEF provides a platform for research groups sub-mitting results and competing on the performance of their medical imageretrieval methods. After each ImageCLEF medical image retrieval task, anoverview is provided to summarize the methods and results of each compe-tition groups (de Herrera et al., 2013; Kalpathy-Cramer et al., 2015, 2011;M¨uller et al., 2012), which demonstrates the state-of-the-art in the medicalimage retrieval field. A benchmark for case-based retrieval including full vol-umetric images of more than 300 patients was run as part of the VISCERALbenchmark Jimenez-del-Toro et al. (2015).1.2. Contributions and Organization of this ArticleThis survey provides a structured and extensive overview of large-scaleretrieval for medical image analytics. Despite existing reviews having sum-marized varieties of medical retrieval systems and methods, none of themfocused on the retrieval techniques for large-scale medical data, which is cur-rently the main challenge in the field of medical analytics. This survey offersa focused overview of the retrieval approaches for the large-scale medical im-age data by expanding multidisciplinary components that involve a nexusof the idea from machine learning, computer vision, information retrieval,and bioinformatics. It explains the entire process from scratch and presentsa comprehensi",
            {
                "entities": [
                    [
                        225,
                        239,
                        "AUTHOR"
                    ],
                    [
                        259,
                        274,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Internet Interventions 18 (2019) 100265Contents lists available at ScienceDirectInternet Interventionsjournal homepage: www.elsevier.com/locate/inventA usability study of an internet-delivered behavioural intervention tailoredfor children with residual insomnia symptoms after obstructive sleep apneatreatmentMatthew Orra, Jason Isaacsa, Roger Godboutb, Manisha Witmansc, Penny Corkuma,⁎Ta Department of Psychology and Neuroscience, Dalhousie University, Canadab Department of Psychiatry, Université de Montréal, Canadac Department of Pediatrics, University of Alberta, CanadaA R T I C L E I N F OA B S T R A C TKeywords:Obstructive sleep apneaInsomniaChildInternet interventioneHealthSleep disordersBetter Nights, Better Days (BNBD) is a 5-session online intervention designed to treat insomnia in 1–10-year-oldchildren (Corkum et al. 2016). Obstructive sleep apnea (OSA) and insomnia commonly occur in children and,after surgical treatment for OSA, it is estimated that up to 50% of children may continue to suffer from insomniasymptoms. Access to insomnia interventions following OSA treatment is limited as there are few programsavailable, few trained practitioners to deliver these programs, and limited recognition that these problems exist.The current study involved the usability testing of an internet-based parent-directed session of BNBD tailoredtowards the needs of children (ages 4–10 years) who experience residual insomnia symptoms after treatment ofOSA. This new session was added to the BNBD program. Participants (n = 43) included 6 parents, 17 sleepexperts, and 20 front-line healthcare providers who completed and provided feedback on the new session.Participants completed a feedback questionnaire, with both quantitative and qualitative questions, after re-viewing the session. Quantitative responses analyzed via descriptive statistics suggested that the session wasprimarily viewed as helpful by most participants, and open-ended qualitative questions analyzed by contentanalyses generated a mix of positive and constructive feedback. The results provide insights on how to optimallytailor the BNBD program to meet the needs of the target population and suggest that testing the session on alarger scale would be beneficial.1. IntroductionInsomnia has a significant impact on children's daily functioningand development (Curcio et al., 2006; Paavonen et al., 2000) andprevious studies suggest that there are significant barriers to accessingpediatric behavioural sleep interventions (Boerner et al., 2013; Honaker& Meltzer, 2016). Better Nights, Better Days (BNBD) is an interactive 5-session, parent-directed eHealth program intended to share psychoe-ducation and behavioural strategies about insomnia in 1–10-year-oldchildren (Corkum et al., 2016). Sessions focus on the importance ofsleep and consequences of poor sleep (Session 1), healthy sleep prac-tices (Session 2), falling asleep independently (Session 3), stayingasleep through the night, reducing early morning awakenings, andensuring adequate napping for younger children (Session 4), and a re-view of progress combined with future goal setting (Session 5). Thefocus of the current study was a usability test of a new session of BNBDtailored for the specific needs of parents of children ages 4–10 yearswho had previously been surgically treated for Obstructive Sleep Apnea(OSA).OSA involves a narrowing or obstruction of the upper airway duringsleep resulting in ventilatory disturbances, caused either by adeno-tonsillar hypertrophy (Capdevila et al., 2008) or obesity (Capdevilaet al., 2008; Gaines et al., 2018; Patinkin et al., 2017), and results insleep disturbance and daytime impairment (Rosen, 2010). Snoring,gasping, and choking are the most common nighttime symptoms of OSAfor children (Dehlink and Tan, 2016; Trosman, 2013). The con-sequences of OSA are significant and can include psychosocial problemssuch as behavioural dysregulation (Blechner and Williamson, 2016;Owens, 2009), cognitive and school-related problems (Gozal, 2009;Hunter et al., 2016; Xanthopoulos et al., 2015), as well as increased riskof physical problems such as diabetes, cardiovascular disease, andobesity (Framnes and Arble, 2018).OSA is often thought of as an adult disorder; however, the disorderis common among children, with a prevalence of 1–5% (Lumeng and⁎Corresponding author at: 1355 Oxford Street, P.O. Box 15000, Halifax, NS B3H 4R2, Canada.E-mail address: penny.corkum@dal.ca (P. Corkum).https://doi.org/10.1016/j.invent.2019.100265Received 16 April 2019; Received in revised form 18 June 2019; Accepted 4 July 2019Available online 15 August 20192214-7829/ © 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/BY-NC-ND/4.0/).\fM. Orr, et al.Internet Interventions 18 (2019) 100265Table 1Overview of content for the OSA-Insomnia session and each of the original BNBD sessions.Session nameSession contentOSA-Insomnia Pre-session: navigating insomnia afterOSA treatmentSession 1: introduction to Better Nights, Better Days(BNBD)Session 2: healthy sleep practicesSession 3: settling to sleepSession 4: going back to sleepSession 5: looking back and aheadduring OSA may become habitual even after OSA treatment)(cid:129) Education about OSA etiology, symptoms, consequences, and primary treatments(cid:129) Understanding the relationship between OSA and insomnia(cid:129) Investigating the behaviourally-based connection between the two disorders (e.g., how night wakings that arose(cid:129) Education about how sleep works and the consequences of poor sleep with a focus on insomnia(cid:129) Introduction to the BNBD team(cid:129) Information about healthy sleep practices and the ways in which they can lead to better sleep(cid:129) Focus on settling independently at bedtime (i.e., self-soothing)(cid:129) Addresses what to do if a child wakes during the night or is up too early in the morning(cid:129) Review the progress made over the previous sessions(cid:129) Revisit goals and address how to maintain them in the futureChervin, 2008; Meltzer et al., 2010). While the primary risk factor foradult OSA is male gender and obesity (Gaines, Vgontzas, Fernandez-Mendoza, & Bixler, 2018; Qaseem et al., 2014), the primary contributorto pediatric OSA is enlarged adenoids or tonsils (i.e., adenotonsillarhypertrophy) (Erler and Paditz, 2004; Rosen, 2010). Various other riskfactors have been described including asthma, allergies, prematurity,ethnicity, and exposure to environmental tobacco smoke (Erler andPaditz, 2004; Redline et al., 1999; Rosen, 2010). Prevalence rates arehighest when children are 3–6 years old, a period during which thetonsils and adenoids are largest relative to the size of the upper airway(Ahn, 2010; Li et al., 2016; Sheldon et al., 2014). Due to the risks as-sociated with childhood OSA, early identification and treatment areessential (Marcus et al., 2012). Adenotonsillectomy (AT; surgical re-moval of adenoids and tonsils) is recommended as the first-line treat-ment for childhood OSA (Cielo and Gungor, 2016; Tan et al., 2016;Trosman, 2013).Previous research indicates that many children continue to havesleep disturbances after adenotonsillectomy. This can in part be un-derstood by the high comorbidity rate between OSA and insomnia (Al-Jawder and BaHammam, 2012; Lack and Sweetman, 2016). Insomniainvolves problems initiating sleep, staying asleep, or waking too early(American Academy of Sleep Medicine, 2014). Preliminary researchindicates a rate of comorbidity among children of approximately 30%(Kukwa et al., 2016; Owens et al., 1998); however, to date, little re-search has been conducted on this comorbidity in children (Byars et al.,2011). Children with comorbid OSA and insomnia, like children withjust OSA, tend to undergo adenotonsillectomy as a first-line treatmentfor their sleep-related issues. While adenotonsillectomy may success-fully treat OSA, it has been reported that > 50% of children with co-morbid insomnia and OSA may continue to have insomnia symptomsafter recovering from surgery (i.e., six months post-OSA surgery)(Chervin et al., 2014), meaning that approximately 15% of all kids withOSA may continue to have insomnia after treatment for OSA. The causeof this is likely multifaceted, but one potential cause is that apnea-re-lated episodes can result in difficulties falling asleep and contribute tonight awakenings, establishing a behavioural pattern which continueseven after adenotonsillectomy (Byars et al., 2011).Even though it is common for insomnia to persist after OSA treat-ment, research on the effectiveness of treatments for residual insomniais limited (Björnsdóttir et al., 2013; Manickam et al., 2015). Insomnia isless likely to be screened for in the context of OSA (Byars et al., 2011),and as such, insomnia may continue to present itself even after OSA hasbeen treated. Thus, treatments should be explored for children withtreated OSA who continue to suffer from insomnia after recoveringfrom adenotonsillectomy.Behaviouralinterventions are the recommended treatmentforchildren with insomnia (Vriend and Corkum, 2011; Meltzer andMindell, 2014; Morgenthaler et al., 2006; Taylor and Roane, 2010),however, in-person treatment is often difficult to access due to factorssuch as finances, transportation, or a general lack of available services(i.e., behavioural(Speth et al., 2015).eHealth interventionsinterventions delivered via the internet) have become increasinglypopular over the last decade, as they improve accessibility. eHealthinterventions designed for a wide range of children's mental and phy-sical health issues have been shown to be an effective and cost-savingalternative to providing children with in-person treatment (Cushing andSteele, 2010). While there are no published eHealth interventions forinsomnia in school-aged children, there is preliminary evidence de-monstrating that distally provided behavioural interventions can ",
            {
                "entities": [
                    [
                        322,
                        335,
                        "AUTHOR"
                    ],
                    [
                        337,
                        351,
                        "AUTHOR"
                    ],
                    [
                        353,
                        369,
                        "AUTHOR"
                    ],
                    [
                        371,
                        384,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Computers in Biology and Medicine 104 (2019) 339–351Contents lists available at ScienceDirectComputers in Biology and Medicinejournal homepage: www.elsevier.com/locate/compbiomedRethinking multiscale cardiac electrophysiology with machine learning andpredictive modellingChris D. Cantwella,b,∗Charles Houstona,c, Rasheda A. Chowdhurya,c, Fu Siong Nga,c, Anil A. Bharatha,d,Nicholas S. Petersa,ca ElectroCardioMaths Group, Imperial College Centre for Cardiac Engineering, Imperial College London, London, UKb Department of Aeronautics, Imperial College London, South Kensington Campus, London, UKc National Heart and Lung Institute, Imperial College London, South Kensington Campus, London, UKd Department of Bioengineering, Imperial College London, South Kensington Campus, London, UK, Yumnah Mohamieda,c, Konstantinos N. Tzortzisa,c, Stef Garastoa,d,TA R T I C L E I N F OA B S T R A C TKeywords:Cardiac electrophysiologyCardiac arrhythmiaElectrogramMachine learningPredictive modellingDeep learningWe review some of the latest approaches to analysing cardiac electrophysiology data using machine learning andpredictive modelling. Cardiac arrhythmias, particularly atrial fibrillation, are a major global healthcare chal-lenge. Treatment is often through catheter ablation, which involves the targeted localised destruction of regionsof the myocardium responsible for initiating or perpetuating the arrhythmia. Ablation targets are either ana-tomically defined, or identified based on their functional properties as determined through the analysis ofcontact intracardiac electrograms acquired with increasing spatial density by modern electroanatomic mappingsystems. While numerous quantitative approaches have been investigated over the past decades for identifyingthese critical curative sites, few have provided a reliable and reproducible advance in success rates. Machinelearning techniques, including recent deep-learning approaches, offer a potential route to gaining new insightfrom this wealth of highly complex spatio-temporal information that existing methods struggle to analyse.Coupled with predictive modelling, these techniques offer exciting opportunities to advance the field and pro-duce more accurate diagnoses and robust personalised treatment. We outline some of these methods and il-lustrate their use in making predictions from the contact electrogram and augmenting predictive modellingtools, both by more rapidly predicting future states of the system and by inferring the parameters of these modelsfrom experimental observations.1. IntroductionCardiac arrhythmias, particularly atrial fibrillation (AF), are a majorglobal healthcare challenge in the developed world. Arrhythmias de-scribe the abnormal and self-perpetuating propagation of action po-tentials (AP) within the myocardium. Their initiation and maintenanceare incompletely understood and this has hindered the development ofeffective and reliable therapy. Treatment for AF is often through ca-theter ablation, where the regions of myocardium determined to beresponsible for initiating or perpetuating the disturbance are targetedand made electrically inactive through the localised application ofradio-frequency energy or freezing. For paroxysmal AF, catheter abla-tion delivers relatively good outcomes, with success rates in the regionof 80–90 percent [1]. However, outcomes of catheter ablation in pa-tients with persistent AF remain disappointing, and is effective in onlyapproximately 50 percent of patients, despite all forms of adjunctiveablation strategies [2].Identifying the critical sites responsible for abnormal AF main-tenance has been a major focus of research, with a number of drivingmechanisms, including rotors [3], multiple wavelets [4] and epi-endodisassociation [5], being proposed. Recent clinical studies, such as theCONFIRM study [6], have tested the new approaches of catheter ab-lation by targeting the foci of rotational drivers, with initially promisingresults showing that 86% of 101 cases achieved AF termination orslowing. However, subsequent studies suggest more moderate outcomeswith Steinberg et al. [7] reporting only 4.7% of 47 cases achieved AFtermination, while 60% documented recurrence within 12 months. Theefficacy of this technique may be in part due to the poor spatial re-solution of the global mapping catheter used [8]. Techniques involvingthe targeting of complex fractionated atrial electrograms (CFAE) [9],∗Corresponding author. Department of Aeronautics, Imperial College London, South Kensington Campus, London, UK.E-mail address: c.cantwell@imperial.ac.uk (C.D. Cantwell).https://doi.org/10.1016/j.compbiomed.2018.10.015Received 29 June 2018; Received in revised form 4 October 2018; Accepted 14 October 20180010-4825/ © 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/BY/4.0/).\fC.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351high dominant frequency (DF) [10] and singularities identified duringphase mapping [11] have each been used as strategies for terminatingarrhythmias. However, none of these adjunctive ablation strategieshave been shown to add any value to the conventional approach ofelectrically isolating the pulmonary veins [2]. Part of the reason for thismay be that they each discard a large proportion of the informationcontent of the acquired electrogram signals or their spatio-temporalassociation during analysis. Additionally, not all identified sites may becritical to the perpetuation of the arrhythmia, leading to excessive ab-lation. The complexity of the underlying electro-architecture of myo-cardium therefore requires a more sophisticated, personalised andmulti-faceted approach to address the challenge of treating AF.The principle data modality used clinically for the treatment of AF isthe contact electrogram, which arises from the superposition of electricfields induced by charged ions moving across cell membranes in themyocardium. It is the electrical signature of action potential propaga-tion through tissue which implicitly encodes the functional and struc-tural characteristics of the local substrate. The electrogram thereforeprovides a wealth of information which is rarely fully utilised in currentclinical practice. Electrograms are normally only broadly categorised bybinary descriptors – such as simple or complex, early or late [12,13],fractionated or non-fractionated – with much of the signal content ef-fectively discarded. Despite a number of studies based on interpretingclinical electrogram data [14,15], these do not investigate how elec-trogram morphology is influenced by individual electro-architecturalfactors. Our knowledge about the direct effects of electrical remodellingon electrogram morphology is consequently poor, considering thenumber of these abnormalities related to cardiac diseases [16]. Lever-aging the electrogram to infer electroarchitectural properties of themyocardium may therefore provide new direct insight in locating cri-tical sites for ablation.Multiple concurrently recorded electrograms may be combined toevaluate the spatio-temporal propagation patterns occurring in thetissue. This activity can also be inferred from the surface of the body[17]. More recently, predictive modelling of action potential propaga-tion is emerging as a potential tool for personalised testing and opti-misation of interventions [18], but this technology is heavily dependenton the accuracy of the underlying calibration of parameters. This canonly be achieved by fully leveraging the huge wealth of informationnow available clinically. The data science revolution in the form ofsophisticated machine learning algorithms and increasing availabilityof computing power, opens up possibilities to manage this data over-load, both in terms of learning from the data, inferring model para-meters and consequently making increasingly accurate predictions.1.1. Machine learning in cardiac electrophysiologyMachine learning describes a class of algorithms which learn modelparameters from a set of training data (for which outcomes may, or maynot, be known) with the purpose of accurately predicting outcomes forpreviously unseen data. Training data that includes associated outcomelabels can be used for supervised learning in which the algorithm usesthis knowledge to directly improve its prediction. In contrast, un-supervised learning seeks patterns in the data with more limited gui-dance, of which clustering is a common example. Although there isconsiderable overlap, machine learning methods are considered todiffer from more conventional statistical modelling, such as regression,in that they are more concerned with the predictive accuracy of theresulting model rather than the ability to explain the reasoning behindits parameters and determining concrete relationships between thedata. The high accuracy of some of the more recent machine learningmethods – which are virtually impossible to analyse analytically – hasjustified this lack of transparency.All machine learning algorithms seek some form of mapping thatmodels the relationship between input data and outcome. In an abstractcontext, we suppose that we have a model f, governed by one or moreparameterslation, which maps an inputto some output, under the re-(1)The form and dimensions ofin Equation (1) are a functionandof the particular problem under consideration. For example, may be alarge one-dimensional vector (time-series) in the case of a music-clas-sification problem, or a two-dimensional image in the case of objectrecognition. For regressions, the output may be a prediction of thedependent quantity, while for classification problems,is usually alabel which assigns the corresponding input to a single class. The size ofdepends on both the problem and also on the chosen model. For ex-ample, for a linear regression between two variables would consist ofonly two values (namely the slope and intercept)",
            {
                "entities": [
                    [
                        785,
                        801,
                        "AUTHOR"
                    ],
                    [
                        834,
                        847,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "A Scalable and Adaptive Method for Finding Semantically Equivalent Cue Words of Uncertainty Chaomei Chena,b, Ming Song*,b, Go Eun Heob aCollege of Computing and Informatics, Drexel University, USA bDepartment of Library and Information Science, Yonsei University, South Korea Abstract Scientific knowledge is constantly subject to a variety of changes due to new discoveries, alternative interpretations, and fresh perspectives. Understanding uncertainties associated with various stages of scientific inquiries is an integral part of scientists’ domain expertise and it serves as the core of their meta-knowledge of science. Despite the growing interest in areas such as computational linguistics, systematically characterizing and tracking the epistemic status of scientific claims and their evolution in scientific disciplines remains a challenge. We present a unifying framework for the study of uncertainties explicitly and implicitly conveyed in scientific publications. The framework aims to accommodate a wide range of uncertain types, from speculations to inconsistencies and controversies. We introduce a scalable and adaptive method to recognize semantically equivalent cues of uncertainty across different fields of research and accommodate individual analysts’ unique perspectives. We demonstrate how the new method can be used to expand a small seed list of uncertainty cue words and how the validity of the expanded candidate cue words are verified. We visualize the mixture of the original and expanded uncertainty cue words to reveal the diversity of expressions of uncertainty. These cue words offer a novel resource for the study of uncertainty in scientific assertions. Keywords Uncertainty, semantically equivalent words, scientific assertions, deep learning, resources Highlights • A generalized framework of uncertainty is presented to accommodate uncertainties due to inconsistent and contradictory information as well as those associated with hedging and other linguistically focused cues. • A scalable and adaptive method selects semantically equivalent words. • The method advances the selection of scientific assertions involving uncertainties. • The study offers new resources for studying the role of uncertainty in science. Introduction A scientific proposition is a statement such as smoking causes cancer. The epistemic status of a scientific proposition refers to the best knowledge of its truthfulness given the current scientific knowledge. Thus, the epistemic status may range from completely unknown to speculations and from hypotheses to facts. The concept of uncertainty in this context characterizes the lack of sufficient information on a given proposition. A statement concerning a proposition can be considered as a combination of two parts: the proposition proper and information relevant to the epistemic status of the proposition. In this article, we focus on uncertainties due to lack of information and, in particular, uncertainties due to lack of consensus. Scientists routinely deal with such uncertainties at various stages of their research, from formulating research questions and selecting research methods to interpreting their findings and communicating their work to others (Cordner & Brown, 2013). Light et al. (2004) estimated that 11% of sentences in MEDLINE abstracts are speculative. Sociologists have studied the formation of consensus in the scientific community concerning whether smoking indeed causes cancer and whether a consensus is   \freached on climate change (Shwed & Bearman, 2010). Scientists face intensified uncertainties when inconsistent, conflicting, or contradictory findings emerge and when competing paradigms are proposed to resolve pressing crises (Kuhn, 1970). The formation of a consensus or the establishment of a dominant paradigm may correspond to a decrease of the overall uncertainty associated with a field of research. However, as we all know, searching for answers to seemingly simple questions may quickly lead to many much more complicated questions. The ability to assess the state of the art of a field of research effectively and efficiently at various levels of granularity is crucial for scientists, science policy makers, and the public. Research in computational linguistics has made significant advances in identifying uncertainty cues and negations. Remarkably influential efforts include the development of the BioScope Corpus for uncertainty and negation in biomedical publications (Vincze et al., 2008), the CoNLL 2010 Shared Task (Farkas et al., 2010) for detecting hedges and their scope in natural language text, the enrichment of a biomedical event corpus with meta-knowledge (Thompson et al., 2011), and unifying categorizations of semantic uncertainty for cross-genre and cross domain uncertainty detection (Szarvas et al., 2012). For example, the CoNLL-2010 shared task (Farkas et al., 2010) focused on detection of uncertainty cues and its linguistic scope in natural language texts. Typical hedging cue is composed of four categories: 1) auxiliaries, 2) verbs of hedging or verbs with speculative content, 3) adjectives or adverbs, and 4) conjunctions. Uncertainty detection focused on biomedical articles and text on Wikipedia. The best uncertainty detection performance in the CoNLL-2010 shared task was achieved with sequence labeling (e.g., Conditional Random Fields) in the biomedical data and bag of words sentence classification in the Wikipedia data. For the in-sentence hedge scope detection task, they classify each token to detect specific cue scopes. Their system is different from the number of class label used target and machine learning approach. More recent studies have explored the potential of measuring the confidence of biomedical models such as pathways based on textual uncertainty (Zerva et al., 2017) and the feasibility of assessing the factuality of predications extracted by SemRep (Kilicoglu et al., 2017). In a broader context, identifying and measuring the degree of uncertainties associated with scientific knowledge embedded in the vast and fast-growing volume of scientific literature remain a bottleneck (Chen, 2016). Influential computational linguistic approaches such as hedging (Hyland, 1998), semantic uncertainty (Szarvas et al., 2012), negation (Chapman et al., 2001; Morante & Daelemans, 2009), and discourse-level uncertainty (Vincze, 2013) have been largely motivated by issues concerning uncertainties from linguistic perspectives. As demonstrated by Simmerling (2015), by using grammatical, stylistic, and rhetorical options, one can talk about scientific uncertainty without using any lexical cues of uncertainty. Furthermore, philosophical and sociological studies of science, scientific creativity, and scientific discovery have highlighted the role of identifying and resolving contradictions and inconsistencies in scientific discovery and in divergent thinking in general. In particular, the value of reconciling multiple perspectives has been long recognized and advocated (Collins, 1989; Linstone, 1981). It is critical for scientists to be able to track conflicting views on the same issue and resolve seemingly contradictory evidence at a new level (Chen, 2014, 2016). The linguistically motivated approaches to the study of scientific uncertainty may benefit from a broadened scope of perspectives. In this article, we present a conceptual framework of the study of uncertainty based on a novel conceptualization of uncertainty as an epistemic status of scientific propositions. The new conceptualization underlines the nature of uncertainty as a meta-knowledge of science and its integral role in scientific change. We introduce a scalable and adaptive method to identify uncertainty cues under the broadened conceptualization of uncertainty. The resultant uncertainty cue words are expected to provide a useful resource for further studies of scientific uncertainty. The method is adaptive in the sense that analysts may generate semantically equivalent uncertainty cues of new dimensions based on a small number of example words. The rest of the article is organized as follows. First, we introduce basic concepts concerning scientific propositions and illustrate some of the most common types of uncertainties associated semantic predications in MEDLINE and the distributions of leading uncertainty cue words in other collections of scientific publications. Next, we present a scalable and adaptive method to construct a comprehensive set \fof uncertainty cue words from scientific publications. The method begins with a set of hand-crafted uncertainty cue words as seeds based on a general-purpose thesaurus of English. Then the computational method expands the seed list to a much larger set of semantically equivalent uncertainty cue words. Two judges evaluated the expanded cue words. The accepted and rejected cue words along with the seed words are visualized as non-overlapping clusters. Sample sentences selected by these uncertainty cues are discussed. The collection of the specific uncertainty cue words, classes of these words, and corresponding statistics are provided as a community resource for researchers to build on the result of our research. Uncertainties of Scientific Knowledge Scientific knowledge is a complex adaptive system of facts, beliefs, hypotheses, speculations, opinions, and a wide variety of other types of information about what we know and how much we know. It is adaptive in that existing scientific knowledge is subject to re-examination in light of new discoveries, alternative interpretations, and scenarios that are previously thought impossible (Chen, 2014; Popper, 1961). A scientist’s domain expertise consists of not only his or her knowledge of various facts and consensus in science but also an accurate understanding of the epistemic status of a wide variety of unsettled elements of a scientific domain. The epistemic status of a scientific proposition characterizes var",
            {
                "entities": [
                    [
                        91,
                        104,
                        "AUTHOR"
                    ],
                    [
                        122,
                        133,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptPattern Recognit. Author manuscript; available in PMC 2018 March 01.Published in final edited form as:Pattern Recognit. 2017 March ; 63: 531–541. doi:10.1016/j.patcog.2016.09.019.Brain Atlas Fusion from High-Thickness Diagnostic Magnetic Resonance Images by Learning-Based Super-ResolutionJinpeng Zhang#,a, Lichi Zhang#,a,c, Lei Xianga, Yeqin Shaob, Guorong Wuc, Xiaodong Zhoud, Dinggang Shen*,c,e, and Qian Wang*,aaMed-X Research Institute, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai 200240, ChinabNantong University, Nantong, Jiangsu 226019, ChinacDepartment of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, United StatesdShanghai United Imaging Healthcare Co., Ltd., Shanghai 201815, ChinaeDepartment of Brain and Cognitive Engineering, Korea University, Seoul 02841, Republic of KoreaAbstractIt is fundamentally important to fuse the brain atlas from magnetic resonance (MR) images for many imaging-based studies. Most existing works focus on fusing the atlases from high-quality MR images. However, for low-quality diagnostic images (i.e., with high inter-slice thickness), the problem of atlas fusion has not been addressed yet. In this paper, we intend to fuse the brain atlas from the high-thickness diagnostic MR images that are prevalent for clinical routines. The main idea of our works is to extend the conventional groupwise registration by incorporating a novel super-resolution strategy. The contribution of the proposed super-resolution framework is two-fold. First, each high-thickness subject image is reconstructed to be isotropic by the patch-based sparsity learning. Then, the reconstructed isotropic image is enhanced for better quality through the random-forest-based regression model. In this way, the images obtained by the super-resolution strategy can be fused together by applying the groupwise registration method to construct the required atlas. Our experiments have shown that the proposed framework can effectively solve the problem of atlas fusion from the low-quality brain MR images.KeywordsBrain atlas; super-resolution; image enhancement; sparsity learning; random forest regression; groupwise registrationEmail addresses: jinpengzhangsjtu@gmail.com (Jinpeng Zhang#), lichizhang@sjtu.edu.cn (Lichi Zhang#), dgshen@med.unc.edu (Dinggang Shen*), wang.qian@sjtu.edu.cn (Qian Wang*)Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.1. IntroductionPage 2Medical resonance (MR) imaging has become a pivotally important tool in many brain-related clinical applications and studies. Without introducing hazardous ionizing radiation, the technique allows researchers to observe in-vivo neural structures and functions in a non-invasive way. Large-scale studies are thus enabled for (early) brain development (Thompson et al., 2000; Casey et al., 2000; Lenroot and Giedd, 2006), maturation (Sowell et al., 1999; Paus et al., 2001), and aging (Resnick et al., 2000; Raz et al., 2005). The technique has also provided a unique perspective to investigate disease anomalies (Frisoni et al., 2010; Polman et al., 2011) and to assess the effects of pharmacological interventions (Mulnard et al., 2000; Jack et al., 2004). In general, MR imaging has played a key role in the field of neuroscience as well as translational medicine. Challenged by the studies of larger scales, a lot of efforts have been devoted toward computer-assisted automatic analysis of brain MR images.The brain atlas, which can often be fused from individual brain MR images, has attracted a lot of interest (Mazziotta et al., 1995; Joshi et al., 2004). Given a group of subjects, the atlas encodes the common morphological information within the group. To this end, researchers can compare the atlases of two individual groups (e.g., the diseased group and the normal control group) and then reveal the subtle difference that might be connected with the disease. Meanwhile, the atlas provides a common space where the inter-subject variation within a population can be measured quantitatively. For example, after being registered with the atlas, each subject owns a deformation field that is typically regarded as the pathway between the subject and the atlas. Since the deformation pathways of all images in the group are established upon the same common space defined by the atlas, comparing the estimated deformation fields of all images can capture the inter-subject variation within the group. Obviously, it is important to properly designate a high-quality atlas in advance for many similar studies.However, it is yet rare and difficult to fuse the atlas from the low-quality diagnostic MR images (i.e., with high inter-slice thickness). Currently most aforementioned studies are focusing on high-quality and (nearly) isotropic imaging data, which has identical resolutions in all dimensions. The acquisitions are often conducted on designated MR scanners and may cost high. The resources required for high-quality imaging, however, are not always available. In developing countries such as China, most diagnostic MR images are still scanned with high inter-slice thickness, partially due to concerns on costs of radiation examinations and limited medical resources per capita. For the real clinical data with high inter-slice thickness, the challenge to fuse the brain atlas is yet unresolved. The lack of the atlas fusion method apparently undermines the efforts to incorporate the low-quality diagnostic imaging data into clinical researches.In this work, we intend to apply learning-based super-resolution to real clinical low-quality brain MR images and then fuse the atlas in the groupwise manner (Joshi et al., 2004). Our super-resolution consists of two stages. First, since the subject images are heterogeneous with high inter-slice thickness, we adopt the non-local patch-based strategy and utilize sparsity learning to reconstruct the subject images in the isotropic space. Second, we turn to random forest and learn the regression model for image enhancement, such that the Pattern Recognit. Author manuscript; available in PMC 2018 March 01.  \fAuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptZhang et al.Page 3reconstructed isotropic image (with relatively low quality) is mapped to be of higher quality (i.e., by suppressing incorrect local anatomical patterns). With all subject images processed through super-resolution, we apply groupwise registration and then fuse the atlas. Specifically, for the iteratively updated group mean image, we can apply the aforementioned forest regression model to enhance its quality. The enhanced group mean image provides better guidance in groupwise registration and leads to the atlas with higher-quality essentially.The rest of this paper is organized as follows. In Section 2, we survey the recent development of the relevant works, especially atlas fusion and super-resolution. In Section 3, we provide details of the proposed framework, including the learning-based super-resolution technique and subsequent groupwise registration. In Section 4, we demonstrate the feasibility of our novel framework through experiments. Finally, we provide the discussions with conclusions in Section 5.2. Related Works2.1. Atlas FusionIn the literature, it is popular to select a third-party standard atlas and then conduct group-level statistical analysis. For example, MNI-152 is one of the most widely accepted atlases (Mazziotta et al., 1995). The fusion of the MNI-152 atlas clearly demonstrates two important steps when fusing the brain atlas: (1) 152 individual subjects are registered with a certain template; (2) all registered images are averaged to produce the desired atlas. The MNI-152 atlas was later adopted by International Consortium for Brain Mapping (ICBM) and became part of Statistical Parametric Mapping (SPM), in which it provides automatic parcellation of neural regions-of-interest (ROI) (Tzourio-Mazoyer et al., 2002). The parcellation facilitates numerous studies upon brain morphology and structural/functional connectivity.Though simple and convenient, it is not necessarily proper to select the atlas manually for the atlas-based analysis. The anatomical variation across human brains is typically high, implying that a single and external atlas cannot fully account for individual subjects (Toga and Thompson, 2001). The (pairwise) registration between the subject(s) and the atlas can also introduce systemic bias into subsequent statistical analysis. For example, it is known that the Alzheimer’s Disease (AD) can cause brain tissue atrophy. When examining the impact of AD upon brain morphology, a large-scale group of both patients and normal controls is usually recruited for scanning the anatomical structures. If the atlas was corresponding to normal control, then it would be relatively easier to register the normal control images with the atlas than to register the patient images. In this way, the overall quality of the registered patient data would become less reliable, resulting in the imbalanced signal-to-noise ratios of the patients and the normal controls.To attain increased signal-to-noise ratio and unbiased statistical analysis, the atlas is better to be fused from all subject images in the data-driven way. Groupwise registration has provided such an alternative solution for atlas fusion o",
            {
                "entities": [
                    [
                        404,
                        416,
                        "AUTHOR"
                    ],
                    [
                        422,
                        432,
                        "AUTHOR"
                    ],
                    [
                        434,
                        445,
                        "AUTHOR"
                    ],
                    [
                        447,
                        458,
                        "AUTHOR"
                    ],
                    [
                        460,
                        474,
                        "AUTHOR"
                    ],
                    [
                        476,
                        490,
                        "AUTHOR"
                    ],
                    [
                        500,
                        510,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Automated Segmentation of Retinal Layers from Optical CoherentTomography Images Using Geodesic DistanceJinming Duan∗, Christopher Tench†, Irene Gottlob‡, Frank Proudlock‡, Li Bai∗∗School of Computer Science, University of Nottingham, UK†School of Medicine, University of Nottingham, UK‡Ophthalmology Department, University of Leicester, UK6102peS7]VC.sc[1v41220.9061:viXraAbstractOptical coherence tomography (OCT) is a non-invasive imaging technique that can produce images of theeye at the microscopic level. OCT image segmentation to localise retinal layer boundaries is a fundamentalprocedure for diagnosing and monitoring the progression of retinal and optical nerve disorders. In this paper,we introduce a novel and accurate geodesic distance method (GDM) for OCT segmentation of both healthyand pathological images in either two- or three-dimensional spaces. The method uses a weighted geodesicdistance by an exponential function, taking into account both horizontal and vertical intensity variations. Theweighted geodesic distance is efficiently calculated from an Eikonal equation via the fast sweeping method.The segmentation is then realised by solving an ordinary differential equation with the geodesic distance.The results of the GDM are compared with manually segmented retinal layer boundaries/surfaces. Extensiveexperiments demonstrate that the proposed GDM is robust to complex retinal structures with large curvaturesand irregularities and it outperforms the parametric active contour algorithm as well as the graph theoreticbased approaches for delineating the retinal layers in both healthy and pathological images.Key words: optical coherence tomography segmentation; geodesic distance; Eikonal equation; partial differentialequation; ordinary differential equation; fast sweeping1 IntroductionOptical coherence tomography (OCT) is a powerful imaging modality used to image biological tissues to obtainstructural and molecular information [1]. By using the low coherence interferometry, OCT can provide high-resolution cross-sectional images from backscattering profiles of biological samples. Over the past two decades,OCT has become a well-established imaging modality and widely used by ophthalmologists for diagnosis of retinaland optical nerve diseases. One of the OCT imaging biomarkers for retinal and optical nerve disease diagnosis isthe thickness of the retinal layers. Automated OCT image segmentation is therefore necessary to delineate theretinal boundaries.Since the intensity patterns in OCT images are the result of light absorption and scattering in retinal tissues,OCT images usually contain a significant amount of speckle noise and inhomogeneity, which reduces the imagequality and poses challenges to automated segmentation to identify retinal layer boundaries and other specificretinal features. Retinal layer discontinuities due to shadows cast by the retinal blood vessels, irregular retinalstructures caused by pathologies, motion artefacts and sub-optimal imaging conditions also complicate the OCTimages and cause inaccuracy or failure of automated segmentation algorithms.Over the past two decades a number of automatic and semi-automatic OCT segmentation approaches havebeen proposed. These approaches can be roughly categorised into three families: A-scan based methods, B-scan based methods and volume based methods, as illustrated in Figure 1. A-scan based methods [2–8] detectintensity peak or valley points on the boundaries in each A-scan profile and then form a smooth and continuousboundary by connecting the detected points using model fitting techniques. These methods can be inefficiencyand lack of accuracy. Common approaches for segmenting two-dimensional (2D) B-scans include active contourmethods [9–14], shortest-path based graph search [15, 16] and statistical shape models [17–19] (i.e. active shapeand appearance models [20, 21]). B-scans methods outperform A-scan methods in general. However, they areprone to the intrinsic speckle noise in OCT images and more likely to fail in detection of pathological retinalstructures. Three-dimensional (3D) scan of the retina is now widely used in commercial OCT devices. Existingvolume based segmentation methods mainly use 3D graph based methods [22–30] and pattern recognition [31–35].Benefiting from contextual information represented in the analysis graph, graph based methods provide optimalsolutions and ideal for volumetric data processing. However, the computation can be very complex and slow.Pattern recognition methods normally require training data manually segmented by experts in order to learn afeasible model for classification. These approaches also suffer in accuracy and efficiency. Segmentation of retinallayers in OCT images thereby remains a challenging problem.1   \fFigure 1: A en-face fundus image (left) with lines overlaid representing the locations of each B-scan within avolumetric OCT data. The red line corresponds to the B-scan in the image (right top). One vertical A-scan ofthe B-scan is shown in the plot (right bottom). The fovea region is characterised by a depression in the centreof the retina surface.In this paper, we propose an algorithm for retinal layer segmentation based on a novel geodesic distanceweighted by an exponential function. As opposed to using a single horizontal gradient as in other works [15,29,30],the exponential function employed in our method integrates both horizontal and vertical gradient informationand can thus account for variations in the both directions. The function plays the role of enhancing the fovealdepression regions and highlighting weak and low contrast boundaries. As a result, the proposed geodesic distancemethod (GDM) is able to segment complex retinal structures with large curvatures and other irregularities causedby pathologies. We compute the weighted geodesic distance via an Eikonal equation using the fast sweepingmethod [36–38]. A retinal layer boundary can then be detected based on the calculated geodesic distance bysolving an ordinary differential equation via a time-dependent gradient descent equation. A local search regionis identified based on the detected boundary to delineate all the nine retinal layer boundaries and overcome thelocal minima problem of the GDM. We evaluate the proposed GDM through extensive numerical experimentsand compare it with state-of-the-art OCT segmentation approaches on both healthy and pathological images.In the following sections, we shall first review the state-of-the-art methods that are to be compared withthe proposed GDM, such as parallel double snakes [14], Chiu’s graph search [15], Dufour’s method [27], andOCTRIMA3D [29, 30]. This will be followed by the details of the proposed GDM, ground-truth validation,numerical experimental results, and comparison of the GDM with the state-of-the-art methods.2 Literature ReviewIn this section, we will provide an overview of the state-of-the-art methods (i.e. parallel double snakes [14], Chiu’smethod [15], OCTRIMA3D [29, 30], Dufour’s method [27]) that will be compared with our proposed GDM inSection 3. For a complete review on the subject, we refer the reader to [39]. Among the four methods reviewed,the first two can only segment B-scans, while the latter two are able to extract retinal surfaces from volumetricOCT data. We note that the term ‘surface’ refers to a set of voxels that fall on the interface between two adjacentretinal layer structures. The retinal layer boundaries to be delineated are shown in Figure 2.Parallel double snakes (PDS): Rossant et al. [14] detected the pathological (retinitis pigmentosa) cellularboundaries in B-scan images by minimising an energy functional that includes two parallel active parametriccontours. Their proposed PDS model consists of a centreline C(s) = (x(s), y(s)) parametrised by s and twoparallel curves C1(s) = C(s) + b(s)n(s) and C2(s) = C(s) − b(s)n(s) with b(s) being a spatially varying half-thickness and n(s) = (nx(s), ny(s)) the normal vector to the the centreline C(s). Specifically, their PDS modelis defined asE(C, C1, C2, b) = EImage(C1) + EImage(C2) + EInt(C) + R (C1, C2, b)(2.1)where the image energy EImage(C1) = − (cid:82) 10 |∇I(C1)|2ds (∇ is the image gradient operator) attracts the para-metric curve C1 towards one of retinal borders of the input B-scan I, whilst EImage(C2) handles curve C2 which(cid:82) 10 |Css (s)|2ds imposes both first and secondis parallel to C1. The internal energy EInt(C) = α2order smooth regularities on the central curve C, with α and β respectively controlling the tension and rigidity(cid:82) 10 |b(cid:48) (C)|2ds is a parallelism constraint imposed on C1 and C2. Nine retinalof this curve. R (C1, C2, b) = ϕ2borders have been delineated by the method, i.e., ILM, RNFLo, IPL-INL, INL-OPL, OPL-ONL, ONL-IS, IS-OS,OS-RPE and RPE-CH.(cid:82) 10 |Cs (s)|2ds+ β2Chiu’s method: Chiu et al. [15] modelled the boundary detection problem in OCT retinal B-scan asdetermining the shortest-path that connects two endpoints in a graph G = (V, E), where V is a set of nodes andE is a set of undirected weights assigned to each pair of two nodes in the graph. Node V corresponds to each2A-scanxzxyzFoveaFoveaB-scan\fFigure 2: An example cross-sectional B-Scan OCT image centred at the macula, showing nine target intra-retinal layer boundaries detected by the proposed method. The names of these boundaries labelled as notationsB1,B2...B9 are summarised in Table 1. Knowledge of these layer boundaries allows us to calculate the retinallayer thickness, which is imperative for detecting and monitoring ocular diseases.Table 1: Notations for nine retinal boundaries/surfaces, their corresponding names and abbreviationsNotationName of retinal boundary/surfaceAbbreviationB1B2B3B4B5B6B7B8B9internal limiting membraneouter boundary of the retinal nerve fibre layerinner plexiform layer-inner nuclear layerinner nuclear layer-outer plexiform layerouter plexiform layer-outer nuclear layerouter nuclear laye",
            {
                "entities": [
                    [
                        117,
                        135,
                        "AUTHOR"
                    ],
                    [
                        137,
                        151,
                        "AUTHOR"
                    ],
                    [
                        153,
                        169,
                        "AUTHOR"
                    ],
                    [
                        171,
                        178,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "478ANDERSON et al., Issues in Research Data ManagementResearch Paper (cid:1)Issues in Biomedical Research Data Management and Analysis:Needs and BarriersNICHOLAS R. ANDERSON, MS, E. SALLY LEE, MS, J. SCOTT BROCKENBROUGH, PHD, MARK E. MINIE, PHD,SHERRILYNNE FULLER, PHD, JAMES BRINKLEY, MD, PHD, PETER TARCZY-HORNOCH, MDA b s t r a c t Objectives: A. Identify the current state of data management needs of academic biomedicalresearchers. B. Explore their anticipated data management and analysis needs. C. Identify barriers to addressingthose needs.Design: A multimodal needs analysis was conducted using a combination of an online survey and in-depth one-on-one semi-structured interviews. Subjects were recruited via an e-mail list representing a wide range ofacademic biomedical researchers in the Pacific Northwest.Measurements: The results from 286 survey respondents were used to provide triangulation of the qualitativeanalysis of data gathered from 15 semi-structured in-depth interviews.Results: Three major themes were identified: 1) there continues to be widespread use of basic general-purposeapplications for core data management; 2) there is broad perceived need for additional support in managing andanalyzing large datasets; and 3) the barriers to acquiring currently available tools are most commonly related tofinancial burdens on small labs and unmet expectations of institutional support.Conclusion: Themes identified in this study suggest that at least some common data management needs will bestbe served by improving access to basic level tools such that researchers can solve their own problems.Additionally, institutions and informaticians should focus on three components: 1) facilitate and encourage the useof modern data exchange models and standards, enabling researchers to leverage a common layer ofinteroperability and analysis; 2) improve the ability of researchers to maintain provenance of data and models asthey evolve over time though tools and the leveraging of standards; and 3) develop and support informationmanagement service cores that could assist in these previous components while providing researchers with uniquedata analysis and information design support within a spectrum of informatics capabilities.(cid:1) J Am Med Inform Assoc. 2007;14:478 – 488. DOI 10.1197/jamia.M2114.IntroductionRapid advances in analytical technology coupled with wide-spread access to large amounts of highly detailed, heter-ogeneous and often public biomedical research data havedramatically increased the difficulties faced by biomedicalAffiliations of the authors: Division of Biomedical and HealthInformatics, Department of Medical Education and BiomedicalInformatics (NRA, ESL, MEM, SF, JB, PT-H); Department of Biolog-ical Structure (JSB, JB); Health Sciences Libraries and InformationCenter (MEM, SF); Department of Health Services, School of PublicHealth and Community Medicine (SF); Department of Pediatrics(PT-H); Department of Computer Science and Engineering (JB,PT-H), University of Washington, Seattle, WA.The authors would like to thank and acknowledge National Libraryof Medicine Training grant (Biomedical Health Informatics trainingprogram) T15LM07442, the BioMediator grant R01-HG02288, BISTIplanning grant P20-LM007714, and the Human Brain Project grantDC02310 for providing the funding to support parts of this work.Correspondences and reprints: Nicholas Anderson, University ofWashington, Department of Medical Education and BiomedicalInformatics, Boxe-mail:357240,(cid:1)nicka@u.washington.edu(cid:2).Seattle, WA 98195-7420;Received for review: 3/29/2006; accepted for publication: 3/27/2007.investigators in acquiring, archiving, annotating, and ana-lyzing data.1 Recognition of this fact is reflected in a numberof large scale initiatives by the major U.S. funding institu-tions as well as a profusion of software tools designed forbiomedical research data management and analysis.1–5 Overthe past several years we have met with many academicbiomedical researchers to discuss solutions to their datahandling problems as part of our own data integrationefforts.6 –12 Through informal discussions, we have beenstruck by the frequency with which they stated that: a) adata handling problem had become a major barrier to theprogress of their research, b) available computational solu-tions were prohibitively expensive, c) available solutionswere too complex for their needs, and/or d) computationalsolutions to their problem did not exist at all. In addition, wehave noticed that the needs of investigators can be extremelydynamic, often changing on a weekly basis. From a biomed-ical informatics standpoint, these issues raise several funda-mental questions:• How are researchers coping with managing these quicklyevolving information management problems in practice?• What obstacles are faced by researchers seeking individ-ual solutions to data management and analysis needs?lDownoadedfromhttps://iacademc.oup.com/jli////amaartice144478788143byguest/on03Juy2023l    \fJournal of the American Medical Informatics Association Volume 14 Number 4July / August 2007479• To what extent can biomedical research data handlingneeds be generalized across more than one lab (or evenmore than one project within a lab)?• What core design issues must be addressed in designingand implementing informatics solutions to aid biomedi-cal researchers in their data management and analysis?To address these questions, we have embarked on a project toidentify the data management and analysis needs of academicbiomedical researchers at the University of Washington.BackgroundInformatics journals report a steady stream of freely avail-able analytic and archiving tools with the potential tostreamline data analysis and integration tasks.6,8,10,13,14 Yetbiomedical researchers continue to struggle with increasingvolume and complexity of their own datasets. Accurate andthorough needs analysis is widely recognized as one of theearliest and most crucial events in virtually all softwaredevelopment cycles. For example, needs analyses for avariety of applications are frequently reported in softwareengineering15–20 as well in the medical fields.21–26 However,few attempts to assess the needs of biomedical research existin print27 despite recent calls for increased evaluation-basedscience to support informatics research.28 –30 We feel thatevaluation-based assessment of data management and anal-ysis needs of biomedical research is a crucial informaticsresearch area.Through our examination of existing methodologies de-scribed in the literature,21,31–37 we have concluded thatmutually supportive data resulting from a mixed methodsapproach has the greatest potential to support a comprehen-sive biomedical research needs assessment. Our approach isto use broad web-based surveys followed by personalizedin-depth interviews. The surveys were targeted toward alarge population of biomedical researchers to provide broadoverviews of generalized needs; however, surveys are lim-ited in that they don’t allow the elicitation of informationthat was not understood or imagined by the authors of thesurvey but is important to the respondents. Therefore, inaddition to the quantitative survey data, we gathered highlydetailed and context-specific qualitative data from individ-ual interviews. The semi-structured interview data not onlyprovided detailed contextual information, but helped revealideas that can be transferred to other domains.17,18,21,38 – 40Combining qualitative and quantitative methods has al-ready been successfully used in the discovery of user issuesassociated with the implementation of clinical ElectronicMedical Records (EMR) systems.36,37,41In this paper, we present the results of the survey and theinterviews in a combined analysis framework that we hopeto use in future biomedical research needs assessments.Using this multi-modal method, we describe the needs ofbiomedical investigators affiliated with the University ofWashington. The UW is an internationally recognized re-search university that was recently ranked #17 in the worldby the Economist newspaper.42 UW research supported over7,400 full-time equivalent positions in fiscal year 2005.43 Asa result, we feel that this study, though limited to one univer-sity and its local collaborator research institutes, can be appli-cable to other academic biomedical research settings.MethodsWe focused our assessment on data management and analysisneeds, including: a) current strategies for management andanalysis of experimental data, and b) obstacles to data man-agement and data sharing. A survey of 286 faculty, researchstaff, and students yielded quantifiable and moderately de-tailed data about informatics software needs. Fifteen volun-teers from this population were the subjects of semi-structuredinterviews. We conducted qualitative analysis on the interviewdata that represented in-depth views of individual needs.Human SubjectsTo ensure the safety and anonymity of the participants, allaspects of this research including participants in both thesurvey and the subsequent interviews were approved by theHuman Subjects Committee of the University of WashingtonInternal Review Board (IRB).SurveyThe survey consisted of two separate sections that togethertotaled 36 questions (See Appendix A, available as an on-linedata supplement at www.jamia.org). Twenty-three of thesequestions addressed a variety of library and informationscience issues and built on previous UW work from Yarfitzet al. involving library-based bioinformatics services.31 Thissurvey is part of a process of continuous evaluation ofbioresearch needs from both the academic research andinstitutional support perspectives. Of the 13 questions focus-ing on biomedical research information management needs,four questions related to subject demographics, with theremainder focusing on high-level overviews of generalizedneeds across biomedical research disciplines. Here we reportprimarily on data from the needs-related ",
            {
                "entities": [
                    [
                        269,
                        284,
                        "AUTHOR"
                    ],
                    [
                        294,
                        315,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Neurocomputing 222 (2017) 204–216Contents lists available at ScienceDirectNeurocomputingjournal homepage: www.elsevier.com/locate/neucomIntegration of touch attention mechanisms to improve the robotic hapticexploration of surfacesRicardo Martinsa,b,⁎, João Filipe Ferreiraa, Miguel Castelo-Brancob, Jorge Diasa,ca ISR-UC, Institute of Systems and Robotics, Department of Electrical and Computer Engineering - University of Coimbra Polo II, 3030-290 Coimbra,Portugalb IBILI-UC, Institute for Biomedical Imaging and Life Sciences, Faculty of Medicine, University of Coimbra, Coimbra, Portugalc Robotics Institute, Khalifa University, Abu Dhabi, UAEcrossmarkA R T I C L E I N F OA B S T R A C TCommunicated by Grana ManuelKeywords:Touch attentionArtificial perceptionBayesian modellingPath planningHaptic explorationProbabilistic grid mapsThis text presents the integration of touch attention mechanisms to improve the efficiency of the action-perception loop, typically involved in active haptic exploration tasks of surfaces by robotic hands. Theprogressive inference of regions of the workspace that should be probed by the robotic system uses informationrelated with haptic saliency extracted from the perceived haptic stimulus map (exploitation) and a “curiosity”-inducing prioritisation based on the reconstruction's inherent uncertainty and inhibition-of-return mechanisms(exploration), modulated by top-down influences stemming from current task objectives, updated at eachexploration iteration. This work also extends the scope of the top-down modulation of information presented ina previous work, by integrating in the decision process the influence of shape cues of the current explorationpath. The Bayesian framework proposed in this work was tested in a simulation environment. A scenario madeof three different materials was explored autonomously by a robotic system. The experimental results show thatthe system was able to perform three different haptic discontinuity following tasks with a good structuralaccuracy, demonstrating the selectivity and generalization capability of the attention mechanisms. Theseexperiments confirmed the fundamental contribution of the haptic saliency cues to the success and accuracy ofthe execution of the tasks.1. IntroductionIn an attempt to capitalise on the same advantages that havinghands benefit human beings, researchers have recently put a lot ofeffort into the development of dexterous robotic hands, due to themechanical (high number of degrees-of-freedom) and sensory (tactile,force, torque, heat) capabilities that they provide. These devices allowrobotic platforms to perform precise manipulation of objects (reaching,grasping, transportation, in-hand reorientation) [1], as well as hapticexploration of surfaces using different patterns of movements (lateralmotion, press-and-release, static contact),thereby promoting theextraction and integration of different haptic properties (contours,texture, compliance, temperature) of the materials these surfaces arecomposed of [2].The contributions presented in this work are related with therobotic haptic exploration of surfaces, following three essential as-sumptions: (1) no other type of sensors are used besides haptics (i.e.exploration is “blind”); (2) exploration paths are not predefined; (3) thesurface geometry is unknown to the robot. The objectives of theexploration tasks concern haptic discontinuity/contour following.Haptic discontinuities are defined by the transition/border regionsbetween surfaces with different haptic properties. During hapticexploration, the interaction of the robotic platform with the probedsurface provides multiple simultaneous streams of data over itsgeometry and the properties of its composing materials relayed by anensemble of haptic sensors. This data is potentially uncertain due tosensor noise and the unknown nature of the surface.To tackle these challenges, we propose a Bayesian framework toimplement autonomous haptic exploration of surfaces that implementsan action-perception loop architecture. The Bayesian formalism pro-vides a principled way of implementing the integration of the multi-modal sensory data supplied by the haptics ensemble while properlydealing with their inherent uncertainty. The proposed action-percep-tion loop architecture integrates touch attention mechanisms (i.e.stimulus-driven processes modulated by task-relevant top-down influ-ences) to optimise the exploration strategy. This in turn promotes⁎ Corresponding author at: University of Coimbra Polo II, Department of Electrical and Computer Engineering, ISR-UC, Institute of Systems and Robotics, 3030-290 Coimbra,Portugal.E-mail addresses: rmartins@isr.uc.pt (R. Martins), jfilipe@isr.uc.pt (J.F. Ferreira), mcbranco@fmed.uc.pt (M. Castelo-Branco), jorge@isr.uc.pt (J. Dias).http://dx.doi.org/10.1016/j.neucom.2016.10.027Received 21 February 2016; Received in revised form 11 July 2016; Accepted 17 October 2016Available online 26 October 20160925-2312/ © 2016 Elsevier B.V. All rights reserved.\fR. Martins et al.Neurocomputing 222 (2017) 204–216adaptive behaviour due to the exploration and exploitation traits ofsuch mechanisms.The haptic exploration of surfaces plays a fundamental role inreduced visibility scenarios (i.e.: underwater robotic manipulation,smoky and foggy disaster environments, partial or complete occlusionof elements in the scenario). Although this work only addresses theimplementation of haptic exploration strategies, the proposed Bayesianframework allows the integration of additional sensory sources such asvision (depth, color) and laser to infer the robotic exploration path. Theapproach proposed in this work can be used to complement methodsalready available to explore surfaces using exclusively non-hapticsensory inputs [4–6].The structure of the manuscript and an overview of the Bayesianmodels proposed in this work are presented in Section 1.1.1.1. Problem formulation and approach overviewIn the application scenarios used in this work, the exploration taskis performed on top of a table – a workspace defined by a planarsurface – and using a generic robotic system with manipulationcapability. The internal structure and configuration of the workspaceis unknown a priori to the robotic system. The solution to the hapticexploration task is described in two-dimensional Cartesian space,progressively unfolding a sequence of regions of the workspace to beprobed by the robotic platform during task execution.As in previous reported work, the 2D-Cartesian space is partitionedusing a planar isometric 2D grid (square cells), as represented in Fig. 1b). Each cell vk has a side of length ε and is described by a 2D Cartesianlocation (x,y) expressed in the inertial world referential {}. Thesetesselations of space have been used extensively in robotics as inferencegrids in many applications [9].The methods presented follow the principles and architecture of thehuman somatosensory processing pipeline and human cognition. Aconceptual overview of our solution is presented in Fig. 2; thecorresponding detailed diagram is given in Fig. 3, including a repre-sentation of data flow. Haptic sensory inputs are acquired during thelocal interaction of the robotic exploratory elements with the environ-ment at region vk. Haptic features such as texture, compliance,temperature are extracted from the haptic sensory inputs. Thesefeatures are integrated and used to discriminate the different classesof materials in the workspace. These processes are modelled by theBayesian model πper presented in Section 3.Next in the sensory processing pipeline, the robotic system uses theupdated perceptual representation of the workspace to infer the nextregion that should be explored. The mechanisms involved in thisprocess are implemented by the Bayesian model πtar and described inSection 5. Touch attention is modelled by integrating the following:(cid:129) Stimulus-driven processes – concurrent mechanisms that pro-Fig. 2. Conceptual representation of the action-perception loop [7] involved in thehaptic exploration of surfaces [8]. In this work, the objectives of the task andcorresponding solution is represented in two levels: symbolic and mid-level.mote both exploitation behaviour concerning perceptual represen-tations of stimuli in the form of haptic saliency and shape cues(determined by the Bayesian model πobj, Section 4), and explora-tion behaviours fuelled by spatial distribution of perceptual uncer-tainty and also inhibition-of-return mechanisms.(cid:129) Goal-directed modulation – mechanisms that influence theweights of stimulus-driven processes through top-down influencesinformed by current task objectives.The experimental setup used in this work is described in Section 6.The impact of the integration of the touch attention mechanisms in theaction-perception loop and generalization capability of the explorationstrategies inferred from the proposed Bayesian models are tested insimulation environment, Section 6. The main conclusions of this workand formulation of the main guidelines for future developments of thisapproach are presented in Section 7.1.2. Path planning of the global haptic exploration strategyThe framework conceptually represented in Fig. 2 and detailed inFig. 3 implements a haptic exploration path planning method, whichinfers a series of global via-points in the workspace that should beFig. 1. a) Results from a previous work [3], demonstrating a haptic discontinuity following task: straight line geometry. In this work, the haptic exploration tasks are more challenging:three materials and discontinuities with other geometries than straight lines. b) Illustration of a 2D isometric grid partitioning a real world workspace area. Each cell v has a dimension εand is described by position (x,y) expressed in {}.205\fR. Martins et al.Neurocomputing 222 (2017) 204–216Fig. 3. Detailed diagram of the architecture of the proposed system. The main contributions of this work are ident",
            {
                "entities": [
                    [
                        274,
                        296,
                        "AUTHOR"
                    ],
                    [
                        298,
                        309,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Pattern Recognition 00 (2011) 1–21Procedia ComputerScienceA Variational Approach to Vesicle Membrane Reconstructionfrom Fluorescence ImagingKalin Kolev1, Norbert Kirchgeßner2,5, Sebatian Houben2, Agnes Csisz´ar2,Wolfgang Rubner2, Christoph Palm4, Bj¨orn Eiben3, Rudolf Merkel2, Daniel Cremers1AbstractBiological applications like vesicle membrane analysis involve the precise segmentation of 3D structures in noisyvolumetric data, obtained by techniques like magnetic resonance imaging (MRI) or laser scanning microscopy (LSM).Dealing with such data is a challenging task and requires robust and accurate segmentation methods. In this article,we propose a novel energy model for 3D segmentation fusing various cues like regional intensity subdivision, edgealignment and orientation information. The uniqueness of the approach consists in the definition of a new anisotropicregularizer, which accounts for the unbalanced slicing of the measured volume data, and the generalization of anefficient numerical scheme for solving the arising minimization problem, based on linearization and fixed-point it-eration. We show how the proposed energy model can be optimized globally by making use of recent continuousconvex relaxation techniques. The accuracy and robustness of the presented approach are demonstrated by evaluatingit on multiple real data sets and comparing it to alternative segmentation methods based on level sets. Although theproposed model is designed with focus on the particular application at hand, it is general enough to be applied to avariety of different segmentation tasks.Keywords:3D segmentation, convex optimization, vesicle membrane analysis, fluorescence imaging.1. Introduction1.1. Model system for cell membrane deformation processesCellular membranes permanently sense their environment. They generate, stabilize or destabilize local regions ofmembrane curvature depending on changes of environmental conditions like temperature, ionic strength, and osmoticor mechanical stress [1]. To cope with environmental osmotic stress, prokaryotes have a wide repertoire of mech-anisms like ion pumping or accumulation of mechanosensitive proteins on the cell surface. For example, s-layersformed by two dimensional crystals of certain proteins on the surface of the membrane serve as structural osmopro-tectant proteins [2]. They increase the membrane rigidity under hyperosmotic conditions. Under these conditions the1Department of Computer Science, TU M¨unchen, Boltzmannstraße 3, 85748 Garching, Germany2Institute of Bio- and Nanosystems, IBN-4, Biomechanics, Forschungszentrum J¨ulich GmbH, 52425 J¨ulich, Germany3Institute of Neuroscience and Medicine (INM-1), Forschungszentrum J¨ulich GmbH, Germany4Department of Computer Science, Regensburg University of Applied Sciences, Regensburg, Germany5present address: Institute of Agricultural Sciences, ETH Zurich, LFW A4, Universittsstrasse 2, CH-8092 Zrich\fK. Kolev er al. / Pattern Recognition 00 (2011) 1–212Figure 1: a ) Streptavidin coated giant unilamellar vesicle under isoosmotic conditions and b ) under hyperosmotic conditions. In the latter case,the solute concentration outside of the vesicle was higher than in the vesicle lumen. To reduce this concentration difference, water flowed out of thevesicle, i. e. its volume decreased while its surface remained constant. Due to this excess surface after osmotic shrinkage, the vesicle underwentshape deformation and folding processes.rigid crystal layer resists higher mechanical stress and the cell membrane undergoes reversible weak surface wrin-kling inhibiting an irreversible membrane budding or fission. However, the importance of this mechanical protectionagainst osmotic stress compared to molecular signaling processes is not yet elucidated. The pressure range which canbe compensated just by mechanical osmoprotection is also unknown.Since the separation of the molecular signalling and the mechanical properties is difficult to realize in living cells,it is more promising to investigate model membrane systems. To mimic a rigid cell membrane under hyperosmoticpressures, Ratanabanangkoon et al. [3] and recently Csisz´ar et al. [4] used a model system consisting of phospholipidvesicles and a two-dimensional crystalline protein layer of streptavidin on the vesicle surfaces. Like cell membranes,the phospholipid molecules built spherically closed bilayers, while streptavidin simulated the role of surface proteinslike s-layers [2], spam [5] or COPI [6] (see Fig. 1). Under hyperosmotic conditions, the solute concentration outsideof the vesicle membrane was higher than in the vesicle lumen. To reduce this concentration difference, water streamedout of the vesicle and its volume decreased while its surface remained constant. Due to this excess of surface afterosmotic shrinkage, the vesicle underwent shape deformation and folding processes. The amount of deformationdepended on the material parameters of the vesicle shell. From the physical point of view, condensation of bendingand stretching energy in the folds of a crumpled surface structure depending on material parameters has been of highinterest [7]. Such deformation energies can be calculated from the 3-dimensional shape of the coated vesicle whenexposed to osmotic pressure [8].Among the major computational challenges regarding the understanding of these vesicle membrane deformationprocesses is therefore the precise and robust estimation of the vesicle membrane over time. To this end, one can relyon the fluorescence properties of materials as observed in volumetric image data, in our case collected by confocallaser scanning microscopy [9]. Since typical measurements are quite noisy (see Fig. 2) the reliable segmentation is amajor challenge.1.2. Variational Estimation of the Membrane ShapeInspired by recent advances in shape optimization [10, 11], we propose a novel variational approach for 3Dsegmentation which is aimed at exploiting the properties of fluorescence images obtained by confocal laser scanningmicroscopy of vesicle membranes. The model combines regional intensity subdivision, edge alignment and orientationinformation and can be globally minimized by means of convex relaxation techniques. Starting with a specified userinput in the form of roughly marked interior and exterior regions in one of the volume slices, corresponding intensity\fK. Kolev er al. / Pattern Recognition 00 (2011) 1–213slice 5slice 15slice 25slice 35slice 45Figure 2: Several slices of a typical volumetric data set, obtained by microscopic imaging of a fluorescently labeled vesicle membrane. Whileinterior and exterior regions exhibit similar intensity characteristics, the membrane surface is identified by bright areas due to its fluorescenceproperties.histograms are estimated and used to derive regional statistics. Secondly, the model encourages the extracted surfaceto pass through locations of high intensity with correspondingly high probability of the presence of the fluorescentvesicle membrane. And, thirdly, the proposed anisotropic formulation allows to impose local surface orientations toalign with the local image gradient. Due to the variety of considered image features and the robust regularizationscheme, the proposed approach enjoys remarkable resilience to noise and deviations from the primary assumptionswhich are frequently encountered in real environments. The contributions of the current work can be summarized asfollows:• We unify various established image cues like regional statistics, edge attraction and orientation information intoa continuous convex energy model.• We propose an anisotropic generalization of the classical isotropic minimal surface model, which accounts forthe sparse slicing of the measured volume data in the z-direction, e. g. inherent in confocal microscopy. Weshow that the generalized model retains convexity and thus all globality guarantees of its isotropic counterpart.• We adopt an efficient numerical scheme for solving the arising optimization problem, based on linearizationand fixed-point iteration.Although the proposed approach is designed with a focus on the particular application scenario of vesicle membraneestimation in fluorescence imaging, the proposed concepts are easily adapted to other segmentation tasks. Finally, thecurvature of the extracted surface is computed in order to facilitate further analysis of the membrane deformations.In the next section, we present and discuss related work. In Section 3 we introduce a variational model integratingregional intensity statistics, edge attraction and orientation fitting. Section 4 is devoted to the optimization of theproposed model. Section 5 briefly sketches an algorithm for estimating the curvature of the segmented surface, usedfor further analysis. In Section 6, we show experimental results on challenging real data sets, demonstrating the effectof each individual component of the energy on the quality of computed solutions and showing a clear superiority overlevel set based solutions. We conclude with a brief summary.2. Related WorkImage segmentation is one of the fundamental problems in Computer Vision and has been extensively studiedfor decades. Among the myriad of existing techniques variational methods prevail due to their accuracy, robustnessand mathematical elegance. Since the pioneering work of Kass et al. [12] considerable efforts have been made onexploring different energy models relying on various image cues. Three paradigms have proven to be particularlyuseful: regional statistics, edge terms and orientation information. In [13] the segmentation problem is posed in termsof a piecewise smooth or piecewise constant image approximation with minimal boundary length. This methodologyis generalized in [14] and [15], where regional intensity of color statistics are estimated to achieve an optimal regionassignment. The segmentation is obtained by alternatingly minimizing the underlying energy functional with respectto the con",
            {
                "entities": [
                    [
                        177,
                        193,
                        "AUTHOR"
                    ],
                    [
                        229,
                        244,
                        "AUTHOR"
                    ],
                    [
                        261,
                        275,
                        "AUTHOR"
                    ],
                    [
                        277,
                        292,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "The Evolution of Imitation and MirrorNeurons in Adaptive AgentsElhanan Borenstein a,∗ Eytan Ruppin a,baSchool of Computer Science, Tel Aviv University, Tel-Aviv 69978, IsraelbSchool of Medicine, Tel Aviv University, Tel-Aviv 69978, IsraelAbstractImitation is a highly complex cognitive process, involving vision, perception, repre-sentation, memory and motor control. The underlying mechanisms that give rise toimitative behavior have attracted a lot of attention in recent years and have been thesubject of research in various disciplines, from neuroscience to animal behavior andhuman psychology. In particular, studies in monkeys and humans have discovered aneural mirror system that demonstrates an internal correlation between the repre-sentations of perceptual and motor functionalities. In contradistinction to previousengineering-based approaches, we focus on the evolutionary origins of imitation andpresent a novel framework for studying the evolution of imitative behavior. We suc-cessfully develop evolutionary adaptive agents that demonstrate imitative learning,facilitating a comprehensive study of the emerging underlying neural mechanisms.Interestingly, these agents are found to include a neural “mirror” device analogousto those identified in biological systems. Further analysis of these agents’ networksreveals complex dynamics, combining innate perceptual-motor coupling with ac-quired context-action associations, to accomplish the required task. These findingsmay suggest a universal and fundamental link between the ability to replicate theactions of other (imitation) and the capacity to represent and match others’ actions(mirroring).Key words: Mirror-neurons, Imitation, Evolution, Agents, Context-basedimitation∗ Corresponding author.Email address: borens@post.tau.ac.il (Elhanan Borenstein).URL: http://www.cs.tau.ac.il/~borens/ (Elhanan Borenstein).Preprint submitted to Elsevier Science28 November 2004\f1 Introduction1.1 Imitation and Mirror NeuronsThe past twenty years have seen a renewed interest in imitation in variousfields of research (Prinz and Meltzoff, 2002) such as developmental psychol-ogy (Meltzoff, 1996), experimental studies of adult social cognition (Bargh,1997), and most relevant to our work, neurophysiology and neuropsychology(Rizzolatti et al., 1996, 2002). Research in this last field had led to the excit-ing discovery of mirror neurons. These neurons were originally found in theventral premotor cortex (area F5) in monkeys, an area which is characterizedby neurons that code goal-related motor acts (e.g. hand or mouth grasping).Some of the neurons in this area, which have been termed mirror neurons,discharge both when the monkey performs an action and when it observesanother individual making a similar action (Gallese et al., 1996; Rizzolattiet al., 2002). Most mirror neurons exhibit a marked similarity in their re-sponse to action observation and execution, and in some cases this similarityis extremely strict (Rizzolatti et al., 2001). An analogous mechanism, wherebycortical motor regions are activated during movement observations was alsodemonstrated in humans using TMS (Fadiga et al., 1995), MEG (Hari et al.,1998), EEG (Cochin et al., 1998) and fMRI (Iacoboni et al., 1999; Buccinoet al., 2001). Mirror neurons are thus the first identified neural mechanism thatdemonstrates a direct matching between the visual perception of an action andits execution. The ability to match the actions of self and other may have afunctional role in fundamental cognitive processes, such as understanding theactions of others, language and mind reading (Rizzolatti et al., 2001). In par-ticular, imitation of motor skills requires the capacity to match the visualperception of a demonstrator’s action to the execution of a motor command.The neural mirror system, demonstrating such an internal correlation betweenthe representations of perceptual and motor functionalities, may form one ofthe underlying mechanisms of imitative ability.1.2 Context-Based ImitationLearning by imitation, like any cognitive process, must be considered an intrin-sically embodied process, wherein the interaction between the neural system,the body and the environment cannot be ignored (Keijzer, 2002; Dautenhahnand Nehaniv, 2002a). In particular, every action, either observed or performed,occurs within a certain context. A context can represent the time or place inwhich the action is made, various properties of the environment, the state ofthe individual performing the action or the social interaction partners (see,2\ffor example, Dautenhahn, 1995). Clearly, there is no sense in learning a novelbehavior by imitating another’s actions if you do not know the context inwhich these actions are made – a certain action can be extremely beneficialin one context, but have no effect (or even be deleterious) in a different con-text. Discussing an agent-based perspective on imitation, Dautenhahn andNehaniv (2002a) consider the problem of imitating the right behavior in theappropriate context, i.e., “when to imitate”, as one of the five central ques-tions (“Big Five”) in designing experiments and research on imitation. Wehence use the term context-based imitation in the sense of being able to re-produce another’s observed action whenever the context in which the actionwas originally observed, recurs. 1 For example, an infant observing his parentsmay learn by imitation to pick up the phone (action) whenever the phone isringing (context).Context-based imitation can thus be conceived as constructing a set of associ-ations from contexts to actions, based on observations of a demonstrator per-forming different actions within various contexts. These associations shouldcomply with those that govern the demonstrator’s behavior, and should belearned (memorized) so that each context stimulates the production of theproper motor action even when the demonstrator is no longer visible. It shouldbe noted however, that “action” is an abstract notion, and in reality, an imitat-ing individual (agent) should also be capable of matching a visual perceptionof the demonstrator’s action to the corresponding motor command that acti-vates this action. 2 The key objective of this study is to gain a comprehensiveunderstanding of the mechanisms that govern such context-based imitativelearning and to examine the nature of the associations between visual percep-tion, motor control and contexts that are being formed in the process.1.3 Evolving Imitating AgentsImitation is an effective and robust way to learn new traits by utilizing theknowledge already possessed by others and it has already been applied by re-searchers in the fields of artificial intelligence and robotics. Hayes and Demiris(1994) presented a model of imitative learning to develop a robot controller.Billard and Dautenhahn (1999) studied the benefits of social interactions andimitative behavior for grounding and use of communication in autonomous1 Animal behavior and human psychology literature introduces a wide range of def-initions of imitation, focusing on what can constitute true imitation vs. other formsof social learning (Zentall, 2001; Nehaniv and Dautenhahn, 2002). Our definitionaddresses the importance of the observed action’s context for a successful behavior.2 In this study we focus on visually based imitation. However, it should be notedthat other forms of imitation, such as vocal imitation, need not involve visual modal-ity (see, for example, Nehaniv and Dautenhahn, 2002; Herman, 2002).3\frobotic agents. Borenstein and Ruppin (2003) employed learning by imita-tion to enhance the evolutionary process of autonomous agents. For an up-to-date introduction to work on imitation in both animals and artifacts seethe cross-disciplinary collection (Dautenhahn and Nehaniv, 2002b). Further-more, some researchers, motivated by the recent discovery of a neural mirrorsystem, have implemented various models for imitative learning, employingneurophysiologically inspired mechanisms. Billard (2000) presented a modelof a biologically inspired connectionist architecture for learning motor skills byimitation. The architecture was validated through a mechanical simulation oftwo humanoid avatars, learning several types of movements sequences. Demirisand Hayes (2002) and Demiris and Johnson (2003) developed a mirror-neuronbased computational architecture of imitation inspired by Meltzoff’s ActiveIntermodal Matching mechanism (Meltzoff and Moore, 1997) and combined itwith an “active” distributed imitation architecture. They have demonstratedthat this dual-route architecture is capable of imitating and acquiring a varietyof movements including unknown, partially known, and fully known sequencesof movements. Oztop and Arbib (2002), focusing on the grasp-related mirrorsystem, argued that mirror neurons first evolved to provide visual feedback onone’s own “handstate” and were later generalized to understanding the actionsof others. They have conducted a range of simulation experiments, based ona schema design implementation of that system, providing both a high-levelview of the mirror system and interesting predictions for future neurophysi-ological testing. Other researchers (Marom et al., 2002; Kozima et al., 2002)claimed that the mirror system structure can be acquired during life throughinteraction with the physical or social environment and demonstrated mod-els whereby perceptual and motor associations are built up from experienceduring a learning phase.The studies cited above, however, assume that the agents’ basic ability and mo-tivation to imitate are innate, explicitly introducing the underlying function-ality, structure or dynamics of the imitation mechanism into the experimentalsystem. In contrast to this engineering-based approach, we wish tostudy the neuronal mechanisms and processes underlying imitationfrom an evolutionary standpoint, and to demonstrate how imita-tive learning per se can evolve and prevail. Evolutionary autonomousa",
            {
                "entities": [
                    [
                        85,
                        98,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Normative challenges of identification in the Internet of Things: Privacy, profiling, discrimination, and the GDPR Sandra Wachter Oxford Internet Institute, University of Oxford and The Alan Turing Institute, British Library, London, United Kingdom ABSTRACT In the Internet of Things (IoT), identification and access control technologies provide essential infrastructure to link data between a user’s devices with unique identities, and provide seamless and linked up services. At the same time, profiling methods based on linked records can reveal unexpected details about users’ identity and private life, which can conflict with privacy rights and lead to economic, social, and other forms of discriminatory treatment. A balance must be struck between identification and access control required for the IoT to function and user rights to privacy and identity. Striking this balance is not an easy task because of weaknesses in cybersecurity and anonymisation techniques. The EU General Data Protection Regulation (GDPR), set to come into force in May 2018, may provide essential guidance to achieve a fair balance between the interests of IoT providers and users. Through a review of academic and policy literature, this paper maps the inherit tension between privacy and identifiability in the IoT. It focuses on four challenges: (1) profiling, inference, and discrimination; (2) control and context-sensitive sharing of identity; (3) consent and uncertainty; and (4) honesty, trust, and transparency. The paper will then examine the extent to which several standards defined in the GDPR will provide meaningful protection for privacy and control over identity for users of IoT. The paper concludes that in order to minimise the privacy impact of the conflicts between data protection principles and identification in the IoT, GDPR standards urgently require further specification and implementation into the design and deployment of IoT technologies. © 2018 Sandra Wachter. Published by Elsevier Ltd. All rights reserved. Keywords: Data protection, Digital Ethics, Identity, Identification, Internet of things, Privacy, Profiling, Discrimination, GDPR, Review  Dr. Sandra Wachter, Oxford Internet Institute, University of Oxford, 1 St. Giles, Oxford, OX1 3JS United Kingdom Email address: sandra.wachter@oii.ox.ac.uk Tel:+44(0)7478340679 P a g e 1 | 22                             \fIntroduction 1 Usage of the ‘Internet of Things’ (IoT)1 is rapidly growing. The European Union expects major investments in areas such as smart homes, personal wellness and wearables, smart energy, smart cities, and smart mobility.2 IoT applications are emerging across myriad sectors, for example in healthcare,3 energy consumption and utility monitoring,4 transportation and traffic control,5 logistics,6 production and supply chain management,7 agriculture,8 public space and environmental monitoring,9 social interactions,10 personalised shopping and commerce,11 domestic automation,12 and others. These IoT devices constantly collect vast amounts of personal data such as location data and health data (e.g. FitBit) in order to function properly or to optimise and customise their services. The IoT is defined by connections and linked services, tailored to the specific requirements of users. Objects and services must be connected to one another and share data about a specific user to provide networked services that are informed by more than the user’s direct interaction with a particular node. Without repeated and consistent identification of users, linked up, seamless services would not be possible. 1 Defining the ‘Internet of Things’ is not straightforward. As argued by Whitemore et al. based on a 2015 literature survey, a core concept of the IoT is that “everyday objects can be equipped with identifying, sensing, networking and processing capabilities that will allow them to communicate with one another and with other devices and services over the Internet to achieve some useful objective” (Andrew Whitmore, Anurag Agarwal and Li Da Xu, ‘The Internet of Things--A Survey of Topics and Trends’ (2015) 17 Information Systems Frontiers; New York 261, 261). While seemingly any Internet-connected object can be treated as part of the IoT, narrower definitions are also available. In logistics and supply chain management, the Internet of Things can refer simply to ‘objects’ embedded with RFID tags, allowing for unique identification and monitoring of object movement and consumption (Whitmore, Agarwal and Da Xu; Rolf H Weber, ‘Internet of Things ? New Security and Privacy Challenges’ (2010) 26 Computer Law & Security Review 23.). The term is also often used as a synonym for ubiquitous computing or ambient intelligent, referring to “smart devices, sensors, human beings, and any other object that is aware of its context and is able to communicate with other entities” (Farzad Khodadadi, Amir Vahid Dastjerdi and Rajkumar Buyya, ‘Internet of Things: An Overview’ [2017] arXiv preprint arXiv:1703.06409 <https://arxiv.org/abs/1703.06409> accessed 30 June 2017.). In other words, the IoT can refer to a network of sensing objects that monitor and record aspects of their environment and the behaviours of users within it. Alongside well-established RFID tags, wireless sensor networks and Bluetooth-enabled devices have emerged as IoT sensors. 2 European Commission, ‘Commission Staff Working Document: Advancing the Internet of Things in Europe’ (European Commission 2016) SWD(2016) 110 final 31 <http://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52016SC0110&from=EN> accessed 8 July 2017. 3 Khodadadi, Dastjerdi and Buyya (n 1); F Gonçalves and others, ‘Security Architecture for Mobile E-Health Applications in Medication Control’, 2013 21st International Conference on Software, Telecommunications and Computer Networks - (SoftCOM 2013) (2013); Cisco, ‘Securing the Internet of Things: A Proposed Framework’ (2016) <http://www.cisco.com/c/en/us/about/security-center/secure-iot-proposed-framework.html> accessed 6 July 2017. 4 S Sicari and others, ‘Security, Privacy and Trust in Internet of Things: The Road Ahead’ (2015) 76 Computer Networks 146; Khodadadi, Dastjerdi and Buyya (n 1). 5 Sicari and others (n 4); Khodadadi, Dastjerdi and Buyya (n 1). 6 Sicari and others (n 4); C Yuqiang, G Jianlan and H Xuanzi, ‘The Research of Internet of Things’ Supporting Technologies Which Face the Logistics Industry’, 2010 International Conference on Computational Intelligence and Security (2010). 7 Sicari and others (n 4); L Weiss Ferreira Chaves and C Decker, ‘A Survey on Organic Smart Labels for the Internet-of-Things’, 2010 Seventh International Conference on Networked Sensing Systems (INSS) (2010). 8 Khodadadi, Dastjerdi and Buyya (n 1). 9 Sicari and others (n 4); Khodadadi, Dastjerdi and Buyya (n 1). 10 Khodadadi, Dastjerdi and Buyya (n 1). 11 Sicari and others (n 4). 12 Khodadadi, Dastjerdi and Buyya (n 1). P a g e 2 | 22                          \fHowever, the pursuit of identification and personalisation of users poses a risk to privacy. Data controllers can draw inferences from these data.13 Users can easily perceive this insight as invasive, unexpected, and unwelcome. Discriminatory treatment can also result from inferential analytics and linkage of disparate records,14 motivating limitations on user profiling.15 The impossibility of anonymising data16 and weak cybersecurity standards (often owing to the limited computational power of identifying technologies such as WiFi or RFID)17 can exacerbate privacy risks. Together, these risks make free and well-informed consent challenging in the IoT. Privacy policies often fail to communicate clearly the risks of data processing and linkage of user records (which requires consistent user identification).18 The EUs General Data Protection Regulation (GDPR) might improve the situation. The regulation will come into force in May 2018, and accounts for many of these risks. The GDPR creates governing principles of data processing (Articles 5 and 25) and establishes new data protection standards relevant for IoT devices. New harmonised standards relating to informed consent, notification duties, privacy by design and privacy by default, data protection impact assessment, algorithmic transparency, automated decision-making, and profiling will apply across Europe. These standards will be undermined by the tendency of IoT devices and services to collect, share, and store large and varied types of personal data, to operate seamlessly and covertly, and to personalise functions based on prior behaviour. This paper analyses the inherit tension between privacy and identifiability in IoT by reviewing prior discussion in academic and policy discourse. Four topics are identified which describe the nature and effects of privacy challenges arising from identity management in the IoT: (1) profiling, inference, and discrimination; (2) control and context-sensitive sharing of identity; (3) consent and uncertainty; and (4) honesty, trust, and transparency. Key issues and potential solutions to balance privacy and identifiability are analysed in the context of new requirements and protections introduced by the GDPR. The analysis suggests that new approaches to transparency and user awareness will be crucial to balance privacy and identifiability, while accounting for potential discrimination, weaknesses in security and anonymisation, and poorly informed consent. Rather than promising that privacy can always be guaranteed in the IoT, transparency, awareness, and honesty are needed about the possible risks (e.g. via notifications, or access rights). Without open communication of the risks inherent to the IoT, informed consent and informational self-determination will be hindered. The paper is structured as follows: section 2 describes the role of identification technologies in the IoT to provide linked up, personalised services. Section 3 then examines the tension between user privac",
            {
                "entities": [
                    [
                        114,
                        129,
                        "AUTHOR"
                    ],
                    [
                        1963,
                        1978,
                        "AUTHOR"
                    ],
                    [
                        2171,
                        2186,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fContents lists available at ScienceDirect Children and Youth Services Review journal homepage: www.elsevier.com/locate/childyouth The child abuse reporting guideline compliance in Korean newspapers Serim Lee , Jieun Lee , JongSerl Chun * Department of Social Welfare, Ewha Womans University, Republic of Korea  A R T I C L E I N F O  A B S T R A C T  Keywords: Child abuse Reporting guidelines Newspapers South Korea The rate of child abuse has sharply increased worldwide, especially during the COVID-19 pandemic. As the media’s role in addressing child abuse cases is crucial, several international and formal organizations have established child abuse reporting guidelines. This study investigated how closely journalists follow reporting guidelines in addressing child abuse cases. Five major Korean presses and 189 articles from January 1, 2018, to January 31, 2021, were selected using the keyword “child abuse.” Each article was analyzed using a guideline framework consisting of 13 items regarding the five principles of the Korean Ministry of Health and Welfare and Central Child Protection Agency reporting guidelines. This study identified a radical growth in media reporting on child abuse cases in South Korea; almost 60% of the articles analyzed came from 2020 and 2021. More than 80% of the articles analyzed did not provide abuse resources, and 70% did not provide factual information. 57.1% of the articles instigated negative stereotypes, and about 30% explicitly mentioned certain family types in the headlines. Nearly 20% of the articles provided excessive details about the method used. Approximately 16% exposed victims’ identities. Some articles (7.9%) also described victims as sharing responsibility for the abuse. This study indicates that the media reports of child abuse in South Korea did not follow the guidelines in many facets. The present study discusses the limitations of the current guidelines and suggests future directions for the news media in reporting on child abuse cases nationwide.  1. Introduction According to the Korean Ministry of Health and Welfare (2020), the number of child abuse cases in South Korea has been increasing annu-ally. Especially during the COVID-19 pandemic, the risk of child abuse has increased owing to the heightened stress and social isolation of children and their families (Rosenthal & Thompson, 2020). The number of child abuse cases has increased sharply in South Korea since the beginning of the pandemic. According to the Korean National Policy Agency (2020), the number of child abuse reports in families between February and March 2020 was 1558, up 13.8 % from 2019. Additionally, according to the Korean Ministry of Health and Welfare’s (2022) anal-ysis of child abuse cases in Korea the total number of reported child abuse cases tallied in 2021 was 53,932, a significant increase of about 27.6 % compared to the previous year. Of the 37,605 cases judged as child abuse, the age range of 13–15 years accounted for the largest portion of victims with 8693 cases (23.1 %), followed by those aged 10–12 years with 8657 cases (23.0 %), and 7–9 years old with 7219 cases (19.2 %) (Korean Ministry of Health and Welfare, 2022). In terms of family types of child abuse victims, 23,838 cases (63.4 %) occurred in families with biological parents, 4618 cases (12.3 %) in mother-and-child families, 3707 cases (9.9 %) in father-and- child families, and 1980 cases (5.3 %) in remarried families (Korean Ministry of Health and Welfare, 2022). Regarding the relationship be-tween assailants and victims, parents accounted for the highest number of cases, with 31,486 cases (83.7 %), followed by 3609 cases (9.6 %) involving surrogate caregivers, and 1517 cases (4.0 %) involving rela-tives (Korean Ministry of Health and Welfare, 2022). Among the confirmed reports of child abuse, 45.1% were reported by fathers (16,944 cases), 35.6% by mothers (13,380 cases), and 3.2% by childcare teachers (1221 cases) (Korean Ministry of Health and Welfare, 2022). Regarding types of child abuse, 16,026 cases (42.6 %) involved multiple types of abuse (Korean Ministry of Health and Welfare, 2022) This was followed by 12,351 cases of emotional abuse (32.8 %), 5780 Abbreviations: WHO, World Health Organization; UNICEF, United Nations Children’s Fund; CDC, Centers for Disease Control and Prevention; NAPAC, National Association for People Abused in Childhood; PRISMA, Preferred Reporting Items for Systematic Reviews and Meta-Analyses; CCTV, Closed-circuit television. * Corresponding author at: 52, Ewhayeodae-gil, Seodaemun-gu, Seoul 03760, Republic of Korea. E-mail address: jschun@ewha.ac.kr (J. Chun). https://doi.org/10.1016/j.childyouth.2023.107037 Received 19 September 2021; Received in revised form 29 September 2022; Accepted 25 May 2023  ChildrenandYouthServicesReview151(2023)107037Availableonline30May20230190-7409/©2023ElsevierLtd.Allrightsreserved.\fS. Lee et al.                                                                                                                    cases of physical abuse (15.4 %), 2793 cases of neglect (7.4 %), and 655 cases of sexual abuse (1.7 %) (Korean Ministry of Health and Welfare, 2022). Among the cases involving multiple forms of abuse, the highest prevalence was observed for physical abuse and emotional abuse, ac-counting for 13,538 cases (36.0 %). Additionally, emotional abuse and neglect comprised 1011 cases (2.7 %); physical abuse, emotional abuse, and neglect comprised 798 cases (2.1 %); and 16 cases (0.0 %) included all types of abuse (Korean Ministry of Health and Welfare, 2022). Thus, the issue of child abuse is severe, and it is a global concern. Worldwide, child abuse reporting has almost doubled from 8 % to 17 % since the school closures due to COVID-19 (Save the Children, 2020). Therefore, child abuse cases have received significant media attention. Naturally, the media’s role in addressing child abuse cases is growing. However, the more media reports there are about child abuse cases, the more likely they are to trigger copycat crimes (Jung & Lee, 2017). Moreover, these reports often undermine the human rights of victims (Lee & Jung, 2016), an issue observed in relation to suicide (Kim et al., 2015). According to Kim et al. (2015), newspapers have a sig-nificant influence than television in inducing copycat suicides because of their easy accessibility and reproduction through various media outlets (Stack, 2002). Therefore, newspapers reporting on child abuse must exercise greater caution. However, some studies have suggested that media coverage of child abuse increase the public awareness on the issue (Saint-Jacques et al., 2012). Moreover, the increase in reporting helps individuals recognize the importance of reporting child abuse (Saint- Jacques et al., 2012). While there is a press rule requiring respect for the human rights of suspects for the benefit of the public interest and directing the media to only report necessary facts, reporters often violate this rule when a press competition begins (Lee & Jung, 2016). Media coverage of a particular crime does not necessarily reflect its true reality as it tends to distort, artificially construct, and simulate reality when reporting social issues such as crime (Payne et al., 2008). Lee and Kim (2008) presented the characteristics of and problems with crime reporting in the Korean media, including crime-reporting trends, disclosure-oriented reporting trends, sensational trends, and violations of human rights in crime reporting. Additionally, they noted that crime reports in the Korean media tend to overemphasize the investigation stage because the police and prosecution are the primary sources of coverage (Lee & Kim, 2008). That is, problems with media coverage of child abuse cases include emphasizing the brutality or deviance of criminals or perpetrators, a lack of in-depth coverage of the causes of child abuse, and provocative and sensational reporting trends (Lee & Jung, 2016). Additionally, a significant issue arises when reporting incidents involving children, as the media often fails to respect the human rights of children and lack of comprehensive awareness of these rights (Lee & Jung, 2016). Several researchers have argued that special efforts are necessary to protect human rights in crime reports, especially when they involve minors and children (Hove et al., 2013; Mejia et al., 2012; Niner et al., 2013). Crime reports can have a tremendous emotional and cognitive impact on children, and exposure to violent crime reports increases their fear of crime (Yoo et al., 2016). Further, Lee and Jung (2016) reported that children exposed to violent crime news could overestimate the possibility of crime and suffer psychological trauma as if crime scenes are persistent. This finding shows that crime reports cause significant mental and emotional damage to children. However, the number of studies on journalism and its accuracy in reporting child abuse cases is limited. For example, a study comparing newspaper reports of child abuse and neglect and data from real cases in England and Wales demonstrated that sexual abuse cases received the most media coverage, despite neglec",
            {
                "entities": [
                    [
                        974,
                        984,
                        "AUTHOR"
                    ],
                    [
                        986,
                        996,
                        "AUTHOR"
                    ],
                    [
                        998,
                        1012,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "WACHTER, MITTELSTADT, RUSSELL 03/03/2020 11:30 AM WHY FAIRNESS CANNOT BE AUTOMATED: BRIDGING THE GAP BETWEEN EU NON-DISCRIMINATION LAW AND AI Sandra Wachter,1 Brent Mittelstadt,2 & Chris Russell3 ABSTRACT In recent years a substantial literature has emerged concerning bias, discrimination, and fairness in AI and machine learning. Connecting this work to existing legal non-discrimination frameworks is essential to create tools and methods that are practically useful across divergent legal regimes. While much work has been undertaken from an American legal perspective, comparatively little has mapped the effects and requirements of EU law. This Article addresses this critical gap between legal, technical, and organisational notions of algorithmic fairness. Through analysis of EU non-discrimination law and jurisprudence of the European Court of Justice (ECJ) and national courts, we identify a critical incompatibility between European no-tions of discrimination and existing work on algorithmic and automated fairness. A clear gap exists between statistical measures of fairness as embedded in myriad fairness toolkits and governance mechanisms and the context-sensitive, often intuitive and ambiguous discrimina-tion metrics and evidential requirements used by the ECJ; we refer to this approach as “con-textual equality.” This Article makes three contributions. First, we review the evidential requirements to bring a claim under EU non-discrimination law. Due to the disparate nature of algorithmic and human discrimination, the EU’s current requirements are too contextual, reliant on intu-ition, and open to judicial interpretation to be automated. Many of the concepts fundamental to bringing a claim, such as the composition of the disadvantaged and advantaged group, the severity and type of harm suffered, and requirements for the relevance and admissibility of evidence, require normative or political choices to be made by the judiciary on a case-by-case 1 Oxford Internet Institute, University of Oxford, 1 St. Giles, Oxford, OX1 3JS, UK and The Alan Turing Institute, British Library, 96 Euston Road, London, NW1 2DB, UK. Email: sandra.wachter@oii.ox.ac.uk. 2 Oxford Internet Institute, University of Oxford, 1 St. Giles, Oxford, OX1 3JS, UK, The Alan Turing Institute, British Library, 96 Euston Road, London, NW1 2DB, UK. 3 The Alan Turing Institute, British Library, 96 Euston Road, London, NW1 2DB, UK, Department of Electrical and Electronic Engineering, University of Surrey, Guildford, GU2 7HX, UK. This work has been supported by the British Academy, Omidyar Group, Miami Foundation, and the EPSRC via the Alan Turing Institute.   \fWACHTER, MITTELSTADT, RUSSELL 03/03/2020 11:30 AM 2 [DRAFT] basis. We show that automating fairness or non-discrimination in Europe may be impossible because the law, by design, does not provide a static or homogenous framework suited to testing for discrimination in AI systems. Second, we show how the legal protection offered by non-discrimination law is challenged when AI, not humans, discriminate. Humans discriminate due to negative attitudes (e.g. ste-reotypes, prejudice) and unintentional biases (e.g. organisational practices or internalised ste-reotypes) which can act as a signal to victims that discrimination has occurred. Equivalent signalling mechanisms and agency do not exist in algorithmic systems. Compared to traditional forms of discrimination, automated discrimination is more abstract and unintuitive, subtle, intangible, and difficult to detect. The increasing use of algorithms disrupts traditional legal remedies and procedures for detection, investigation, prevention, and correction of discrimi-nation which have predominantly relied upon intuition. Consistent assessment procedures that define a common standard for statistical evidence to detect and assess prima facie automated discrimination are urgently needed to support judges, regulators, system controllers and de-velopers, and claimants. Finally, we examine how existing work on fairness in machine learning lines up with pro-cedures for assessing cases under EU non-discrimination law. A ‘gold standard’ for assessment of prima facie discrimination has been advanced by the European Court of Justice but not yet translated into standard assessment procedures for automated discrimination. We propose ‘conditional demographic disparity’ (CDD) as a standard baseline statistical measurement that aligns with the Court’s ‘gold standard’. Establishing a standard set of statistical evidence for automated discrimination cases can help ensure consistent procedures for assessment, but not judicial interpretation, of cases involving AI and automated systems. Through this proposal for procedural regularity in the identification and assessment of automated discrimination, we clarify how to build considerations of fairness into automated systems as far as possible while still respecting and enabling the contextual approach to judicial interpretation practiced under EU non-discrimination law.   \fWACHTER, MITTELSTADT, RUSSELL 03/03/2020 11:30 AM [2020] WHY FAIRNESS CANNOT BE AUTOMATED 3 TABLE OF CONTENTS OF 1. 2. UNIQUE CHALLENGE I. II. THE INTRODUCTION ...................................................................................... 4 AUTOMATED DISCRIMINATION ................................................................................. 10 III. CONTEXTUAL EQUALITY IN EU NON-DISCRIMINATION LAW .................................................................................................................... 13 A. COMPOSITION OF THE DISADVANTAGED GROUP ............................... 16 1. Multi-dimensional discrimination ........................................... 20 B. COMPOSITION OF THE COMPARATOR GROUP .................................... 21 C. REQUIREMENTS FOR A PARTICULAR DISADVANTAGE ...................... 26 D. ADMISSIBILITY AND RELEVANCE OF EVIDENCE ............................... 32 Statistical evidence.................................................................. 32 Intuitive and traditional evidence ........................................... 36 E. A COMPARATIVE ‘GOLD STANDARD’ TO ASSESS DISPARITY............. 37 REFUTING PRIMA FACIE DISCRIMINATION ......................................... 41 F. IV. CONSISTENT ASSESSMENT PROCEDURES FOR AUTOMATED DISCRIMINATION ................................................................................. 44 A. TOWARDS CONSISTENT ASSESSMENT PROCEDURES ......................... 45 STATISTICAL FAIRNESS AND THE LEGAL ‘GOLD STANDARD’ .................................................................................................................... 48 STATISTICAL FAIRNESS TESTS IN EU JURISPRUDENCE...................... 49 A. SHORTCOMINGS OF NEGATIVE DOMINANCE ..................................... 53 B. VI. CONDITIONAL DEMOGRAPHIC DISPARITY: A STATISTICAL ‘GOLD STANDARD’ FOR AUTOMATED FAIRNESS ..................... 54 A. CONDITIONAL DEMOGRAPHIC PARITY IN PRACTICE ......................... 57 SUMMARY STATISTICS TO SUPPORT JUDICIAL INTERPRETATION ...... 62 B. VII. AUTOMATED FAIRNESS: AN IMPOSSIBLE TASK? ..................... 64 APPENDIX 1 – MATHEMATICAL PROOF FOR CONDITIONAL DEMOGRAPHIC PARITY .................................................................... 71 V.  \fWACHTER, MITTELSTADT, RUSSELL 03/03/2020 11:30 AM 4 I. INTRODUCTION [DRAFT] Fairness and discrimination in algorithmic systems are globally recognised as topics of critical importance.4 To date, a majority of work has started from an American regulatory perspective defined by the notions of ‘disparate treat-ment’ and ‘disparate impact’.5 European legal notions of discrimination are not, however, equivalent. In this paper, we examine EU law and jurisprudence of the European Court of Justice concerning non-discrimination. We identify a critical incompatibility between European notions of discrimination and ex-isting work on algorithmic and automated fairness. A clear gap exists between statistical measures of fairness and the context-sensitive, often intuitive and ambiguous discrimination metrics and evidential requirements used by the Court. 4 To name only a few CATHY O’NEIL, WEAPONS OF MATH DESTRUCTION: HOW BIG DATA INCREASES INEQUALITY AND THREATENS DEMOCRACY (2017); Cass R. Sunstein, Algo-rithms, correcting biases, 86 SOC. RES. INT. Q. 499–511 (2019); JOSHUA A. KROLL ET AL., Account-able Algorithms (2016), http://papers.ssrn.com/abstract=2765268 (last visited Apr 29, 2016); VIRGINIA EUBANKS, AUTOMATING INEQUALITY: HOW HIGH-TECH TOOLS PROFILE, POLICE, AND PUNISH THE POOR (2018); Katherine J. Strandburg, Rulemaking and Inscrutable Automated Decision Tools, 119 COLUMBIA LAW REV. 1851–1886 (2019); MIREILLE HILDEBRANDT & SERGE GUTWIRTH, PROFILING THE EUROPEAN CITIZEN (2008); Mireille Hildebrandt, Profiling and the Rule of Law, 1 IDENTITY INF. SOC. IDIS (2009), https://papers.ssrn.com/ab-stract=1332076 (last visited Jul 31, 2018); Tal Zarsky, Transparent predictions (2013), https://pa-pers.ssrn.com/sol3/papers.cfm?abstract_id=2324240 (last visited Mar 4, 2017); Latanya Sweeney, Discrimination in online ad delivery, 11 QUEUE 10 (2013); Frederik Zuiderveen Bor-gesius, Algorithmic Decision-Making, Price Discrimination, and European Non-discrimination Law, EUR. BUS. LAW REV. FORTHCOM. (2019); FRANK PASQUALE, THE BLACK BOX SOCIETY: THE SECRET ALGORITHMS THAT CONTROL MONEY AND INFORMATION (2015); VIKTOR MAYER-SCHÖNBERGER, DELETE: THE VIRTUE OF FORGETTING IN THE DIGITAL AGE (2011); Jeremias Prassl & Martin Risak, Uber, taskrabbit, and co.: Platforms as employers-rethinking the legal analysis of crowdwork, 37 COMP LAB POL J 619 (2015); O’NEIL, supra note; Kate Crawford & Ryan Calo, There is a blind spot in AI research, 538 NATURE 311–313 (2016); S. C. Olhede & P. J. Wolfe, The growing ubiquity of algorithms in society: implications, impacts and innovations, 376 PHIL TRANS R SOC A 20170364 (2018); Scott R. Peppet, Regulating the internet of things:",
            {
                "entities": [
                    [
                        141,
                        156,
                        "AUTHOR"
                    ],
                    [
                        158,
                        176,
                        "AUTHOR"
                    ],
                    [
                        180,
                        194,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Delft University of TechnologyA healthy debateExploring the views of medical doctors on the ethics of artificial intelligenceMartinho, Andreia; Kroesen, Maarten; Chorus, CasparDOI10.1016/j.artmed.2021.102190Publication date2021Document VersionFinal published versionPublished inArtificial Intelligence in MedicineCitation (APA)Martinho, A., Kroesen, M., & Chorus, C. (2021). A healthy debate: Exploring the views of medical doctors onthe ethics of artificial intelligence. Artificial Intelligence in Medicine, 121, [102190].https://doi.org/10.1016/j.artmed.2021.102190Important noteTo cite this publication, please use the final published version (if applicable).Please check the document version above.CopyrightOther than for strictly personal use, it is not permitted to download, forward or distribute the text or part of it, without the consentof the author(s) and/or copyright holder(s), unless the work is under an open content license such as Creative Commons.Takedown policyPlease contact us and provide details if you believe this document breaches copyrights.We will remove access to the work immediately and investigate your claim.This work is downloaded from Delft University of Technology.For technical reasons the number of authors shown on this cover page is limited to a maximum of 10. \fContents lists available at ScienceDirect Artificial Intelligence In Medicine journal homepage: www.elsevier.com/locate/artmed A healthy debate: Exploring the views of medical doctors on the ethics of artificial intelligence Andreia Martinho *, Maarten Kroesen, Caspar Chorus Delft University of Technology, Delft, the Netherlands  A R T I C L E I N F O  A B S T R A C T  Keywords: Artificial intelligence Healthcare Medicine Ethics Q-methodology Artificial Intelligence (AI) is moving towards the health space. It is generally acknowledged that, while there is great promise in the implementation of AI technologies in healthcare, it also raises important ethical issues. In this study we surveyed medical doctors based in The Netherlands, Portugal, and the U.S. from a diverse mix of medical specializations about the ethics surrounding Health AI. Four main perspectives have emerged from the data representing different views about this matter. The first perspective (AI is a helpful tool: Let physicians do what they were trained for) highlights the efficiency associated with automation, which will allow doctors to have the time to focus on expanding their medical knowledge and skills. The second perspective (Rules & Regulations are crucial: Private companies only think about money) shows strong distrust in private tech companies and emphasizes the need for regulatory oversight. The third perspective (Ethics is enough: Private companies can be trusted) puts more trust in private tech companies and maintains that ethics is sufficient to ground these corporations. And finally the fourth perspective (Explainable AI tools: Learning is necessary and inevitable) emphasizes the importance of explainability of AI tools in order to ensure that doctors are engaged in the technological progress. Each perspective provides valuable and often contrasting insights about ethical issues that should be operationalized and accounted for in the design and development of AI Health.  1. Introduction Artificial Intelligence (AI) is moving towards the health space. Given the abundance of data generated by health systems as a result of digi-tization efforts made over the last decade, a new data-driven approach to implement AI in healthcare has emerged. In contrast with previous and somewhat failed rule-based approaches to implement AI in healthcare [1,2], this new approach relies heavily on algorithms that detect pat-terns in data from clinical practice (e.g. medical imaging and electronic health records), clinical trials, genomics studies, and insurance, phar-maceutical, and pharmacy benefits management operations [3]. There is an expectation that these state-of-the-art-data-driven AI methods and algorithms will be able to use such data to address the complex problems of health systems [4,3]. The implementation of AI in healthcare holds great promise for expanding the medical knowledge and providing optimal yet cost- effective healthcare solutions [5,6]. In the clinical domain, expected results include identification of individuals at high risk for a disease, improved diagnosis and matching of effective personalized treatment, and out-of-hospital monitoring of therapy response [4,7]. Despite the projected benefits associated with Health AI, it also raises important ethical issues [8,9]. It is well known that AI has the potential to threaten values such as Autonomy, Privacy, and Safety [10], which are core values in Medicine [11,12]. Therefore, in order for AI to promote quality of care and minimize potentially disruptive effects [13], its deployment must take ethics into account. An important step towards ethical deployment of disruptive AI technologies is to learn the views of practitioners about such technologies. This information allows a better operationalization of the ethical issues associated with AI in a particular domain, which eventually is expected to lead to more meaningful debates and robust policies. The current academic literature provides interesting and valuable information on the perspectives of practitioners about the impact of AI technologies in the medical profession [14,15,16,17]. Most of these studies are particularly suited to medical fields with a strong image processing component, which is adequate for automated analysis, such as radiology [18,19,20,21,22,23,24], pathology [25], and dermatology * Corresponding author. E-mail address: a.m.martinho@tudelft.nl (A. Martinho). https://doi.org/10.1016/j.artmed.2021.102190 Received 12 February 2021; Received in revised form 22 September 2021; Accepted 29 September 2021  ArtificialIntelligenceInMedicine121(2021)102190Availableonline12October20210933-3657/©2021DelftUniversityofTechnology.PublishedbyElsevierB.V.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fA. Martinho et al.                                                                                                                [26,27]. However, there is little knowledge on the views of medical doctors about the ethical issues associated with the implementation of AI in healthcare. The aim of this study is to gain insight into the reasoning patterns and moral opinions about Health AI from those involved in the medical practice. By surveying medical doctors in The Netherlands, Portugal, and U.S. on the ethical issues associated with the implementation of AI in healthcare, we expect to enrich existing literature on the impact of AI technologies in medicine and provide valuable knowledge for the operationalization of Health AI Ethics. We first provide a brief commentary about the ethics of AI in healthcare. Subsequently we explain the methods used in this research by outlining the basic steps of q-methodology and explaining how we established these steps in this study. Later we present the results of the study by describing the four different perspectives that have emerged from the data. These results are further analyzed and discussed. Finally we draw conclusions and present directions for further research. 2. The ethics of health AI The empirical work about AI in healthcare that has been reported in the literature focuses mainly on issues directly related to the medical practice and career, such as Future of Employment, Education about AI, and Accountability. It has been reported that medical students and practitioners under-stand the increasing importance of AI in healthcare and have positive attitudes towards the clinical use of AI [20,26,17], but mainly as a supportive system for diagnosis [18,19,26,27,25,24]. Despite the positive attitudes towards AI, it has also been reported that students and medical doctors are poorly trained on these technol-ogies [20,28,29,30]. One study indicated that, although a small cohort of UK medical students who received AI teaching felt more confident in working with AI in the future compared to students that did not receive teaching, a significant number of taught students still felt inadequately prepared [20]. In order to take full advantage of these technologies, scholars seem to agree that medical school training on AI should be expanded and improved [18,20,21,26,25]. Regarding the impact of AI on career choice and reputation, it was reported that AI has an impact in the career intentions of students with respect to radiology [20], but radiologists would still choose this spe-cialty if given that choice [21]. These specialists have, however, revealed concerns that AI might diminish their professional reputation [24]. Contrary to the perceptions of the general public that AI will completely or partially replace human doctors [31], medical students and doctors in general are not concerned about job replacement [18,26,17,32,24]. Another important issue related to medical practice and career is liability. In a study in which pathologists were surveyed, it was reported that, with respect to medico-legal responsibility for diagnostic errors made by a human/AI combination, opinions were split between those who believed that the platform vendor and pathologist should be held equally liable, and others who believed responsibility remains primarily that of the human, with only a minority reporting that the platform vendor should primarily be liable [25]. Clearly, the ethics surrounding implementation of AI in healthcare goes beyond issues related to medical practice and career. Health AI gives rise to higher level ethical issues such as Autonomy, Fairness, or Privacy [33,10] but, with the exception of fairness, these issues have received less attention in the scientific literature. Fairness concerns related to racial and gender bias in AI-powered medical applications have t",
            {
                "entities": [
                    [
                        1527,
                        1544,
                        "AUTHOR"
                    ],
                    [
                        1547,
                        1563,
                        "AUTHOR"
                    ],
                    [
                        1564,
                        1578,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Vrije Universiteit BrusselVulnerable Data SubjectsMalgieri, GianclaudioPublished in:Computer Law & Security ReviewDOI:10.1016/j.clsr.2020.105415Publication date:2020License:CC BYDocument Version:Final published versionLink to publicationCitation for published version (APA):Malgieri, G. (2020). Vulnerable Data Subjects. Computer Law & Security Review, 37, 1-22. [105415].https://doi.org/10.1016/j.clsr.2020.105415CopyrightNo part of this publication may be reproduced or transmitted in any form, without the prior written permission of the author(s) or other rightsholders to whom publication rights have been transferred, unless permitted by a license attached to the publication (a Creative Commonslicense or other), or unless exceptions to copyright law apply.Take down policyIf you believe that this document infringes your copyright or other rights, please contact openaccess@vub.be, with details of the nature of theinfringement. We will investigate the claim and if justified, we will take the appropriate steps.Download date: 04. jul. 2023 \fcomputer law & security review 37 (2020) 105415 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR Vulnerable data subjects Gianclaudio Malgieri a , 1 , J ˛edrzej Niklas b , 1 , ∗a b Vrije Universiteit Brussel, Belgium Cardiff University, United Kingdom a r t i c l e i n f o a b s t r a c t Keywords: Data protection Vulnerability Vulnerable groups Discrimination AI Research ethics Discussion about vulnerable individuals and communities spread from research ethics to consumer law and human rights. According to many theoreticians and practitioners, the framework of vulnerability allows formulating an alternative language to articulate prob- lems of inequality, power imbalances and social injustice. Building on this conceptualisa- tion, we try to understand the role and potentiality of the notion of vulnerable data subjects. The starting point for this reflection is wide-ranging development, deployment and use of data-driven technologies that may pose substantial risks to human rights, the rule of law and social justice. Implementation of such technologies can lead to discrimination system- atic marginalisation of different communities and the exploitation of people in particularly sensitive life situations. Considering those problems, we recognise the special role of per- sonal data protection and call for its vulnerability-aware interpretation. This article makes three contributions. First, we examine how the notion of vulnerability is conceptualised and used in the philosophy, human rights and European law. We then confront those findings with the presence and interpretation of vulnerability in data protection law and discourse. Second, we identify two problematic dichotomies that emerge from the theoretical and prac- tical application of this concept in data protection. Those dichotomies reflect the tensions within the definition and manifestation of vulnerability. To overcome limitations that arose from those two dichotomies we support the idea of layered vulnerability, which seems com- patible with the GDPR and the risk-based approach. Finally, we outline how the notion of vulnerability can influence the interpretation of particular provisions in the GDPR. In this process, we focus on issues of consent, Data Protection Impact Assessment, the role of Data Protection Authorities, and the participation of data subjects in the decision making about data processing. © 2020 Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) ∗ Corresponding author: J ˛edrzej Niklas, Cardiff University, United Kingdom. 1 E-mail addresses: Gianclaudio.Malgieri@vub.be (G. Malgieri), niklasj@cardiff.ac.uk (J. Niklas). Both the authors contributed equally to each paragraph. The authors would also like to thank the anonymous reviewer whose sug- gestions have greatly improved this article. The open access version of this research was funded by the EU Commission, H2020 SWAFS Programme, PANELFIT Project, research grant number 788039. https://doi.org/10.1016/j.clsr.2020.105415 0267-3649/© 2020 Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) \f2 computer law & security review 37 (2020) 105415 1. Introduction For decades, experts in research ethics have assumed that some research participants and communities are more likely to be mistreated, abused, exploited or harmed.2 Such groups seem to possess a level of vulnerability, which generates cer- tain obligations and responsibilities for researchers and over- sight entities. The principle of special treatment of “vulner- able groups” was incorporated into various declarations and guidelines that regulate especially clinical research, like the Belmont Report or the Declaration of Helsinki.3 Those docu- ments predominantly focus on the issue of consent and in- formed participation, highlighting problems of autonomy and integrity. Nevertheless, some other interpretations add more elaborated understanding of vulnerability and raise issues of power imbalance and political and economic disadvantage.4 In other words, the language of vulnerability in research ethics allows greater sensitivity and responsiveness to equity, dis- crimination and different socio-historical contexts. However, the notion of vulnerability is also discussed in other fields. From human rights to political philosophy, the concept is seen as a framework that enables the articulation of broad issues that fill into the category of social justice and uncover human exposure to harms, pain and suffering.5 As it will be argued below, human vulnerability is also (to some extent) present in the discussions about data protection, privacy and data-driven technologies. Calo, a prominent voice in this debate, argues that the rationale for privacy protec- tion is precisely addressing vulnerability of individuals.6 Put it differently, privacy and data protection regimes are man- ifestations of the idea that all individuals are vulnerable to the power imbalances created by data-driven technologies. 2 3 4 5 Carol Levine et al., “The Limitations of ‘Vulnerability’ as a Pro- tection for Human Research Participants,” The American Journal of Bioethics 4, no. 3 (August 2004): 44–49, https://doi.org/10.1080/ 15265160490497083 . Phoebe Friesen et al., “Rethinking the Belmont Report?,” The American Journal of Bioethics 17, no. 7 (July 3, 2017): 15–21, https: //doi.org/10.1080/15265161.2017.1329482 . Dearbhail Bracken-Roche et al., “The Concept of ‘Vulnerability’ in Research Ethics: An in-Depth Analysis of Policies and Guide- lines,” Health Research Policy and Systems 15, no. 1 (December 2017): 8, https://doi.org/10.1186/s12961- 016- 0164- 6 . Lourdes Peroni and Alexandra Timmer, “Vulnerable Groups: The Promise of an Emerging Concept in European Human Rights Convention Law,” International Journal of Constitutional Law 11, no. 4 (October 1, 2013): 1056–85, https://doi.org/10.1093/icon/mot042 ; Rebecca Hewer, “A Gossamer Consensus: Discourses of Vulner- ability in the Westminster Prostitution Policy Subsystem,” Social & Legal Studies 28, no. 2 (April 2019): 227–49, https://doi.org/10. 1177/0964663918758513 ; Isabelle Bartkowiak-Théron and Nicole L. Asquith, “Conceptual Divides and Practice Synergies in Law En- forcement and Public Health: Some Lessons from Policing Vulner- ability in Australia,” Policing and Society 27, no. 3 (April 3, 2017): 276–88, https://doi.org/10.1080/10439463.2016.1216553 , Martha Albert- son Fineman, “The Vulnerable Subject: Anchoring Equality in the Human Condition,” Yale Journal of Law and Feminism 20 (2008): 23; Ju- dith Butler, Precarious Life: The Powers of Mourning and Violence (Lon- don ; New York: Verso, 2004). Ryan Calo, “Privacy, Vulnerability, and Affordance,” DePaul L. 6 Rev. 66 (2017): 592–593. detect children anxiety and depression 9 Additionally, different scholars explain how data-driven tech- nologies can lead to discrimination, social marginalisation or affect human autonomy and dignity and exploit particular communities.7 Such controversial cases in the data-driven re- search concern automated systems that identify sexual orien- tation,8 or predict and prevent suicide.10 Finally, the notion of vulnerability appears in the discussion about ethics and regulation of Artificial In- telligence. Here some of the guidelines and ethical policies call for the governance frameworks that recognise the situation of vulnerable groups such as women, persons with disabilities, ethnic minorities, children, and consumers.11 It seems to us that the issue of human vulnerability should be an important topic in the data protection debate, consid- ering the new risks of individual exploitation in the algorith- mic environment. Involving vulnerability as a “heuristic tool”could emphasise existing inequalities between different data subjects and specify in a more systematic and consolidated way that the exercise of data rights is conditioned by many factors such as health, age, gender or social status. However, the scholarly discussion about vulnerable data subjects is still largely underdeveloped. Accordingly, in this article, we try to understand and conceptualise how the notion of vulnerable individuals finds its way in the data protection debate. More precisely, when human vulnerability can influence the way we are interpreting data protection regimes. We are aware that it is not possible to address this com- plex topic in one article satisfactorily. Our modest goal here is to initiate a discussion about this topic and its problem- atic aspects, suggesting some first interpretative paths, while calling for further analysis and research. To do this, we first investigate the meaning of “vulnerable individuals”, look- ing in particular at the theoretical discussion about vulner- ability ( Section 2 ). Taking ",
            {
                "entities": [
                    [
                        1211,
                        1232,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect International Journal of Information Management journal homepage: www.elsevier.com/locate/ijinfomgt Research Article Emotional reactions to robot colleagues in a role-playing experiment Nina Savela a,*, Atte Oksanen a, Max Pellert b, c,d, David Garcia b,c, d a Faculty of Social Sciences, Tampere University b Institute of Interactive Systems and Data Science, Department of Computer Science and Biomedical Engineering, Graz University of Technology c Complexity Science Hub Vienna d Center for Medical Statistics, Informatics and Intelligent Systems, Medical University of Vienna  A R T I C L E I N F O  A B S T R A C T  Keywords: Robot Work Sentiment Role-play Experiment We investigated how people react emotionally to working with robots in three scenario-based role-playing survey experiments collected in 2019 and 2020 from the United States (Study 1: N = 1003; Study 2: N = 969, Study 3: N = 1059). Participants were randomly assigned to groups and asked to write a short post about a scenario in which we manipulated the number of robot teammates or the size of the social group (work team vs. organi-zation). Emotional content of the corpora was measured using six sentiment analysis tools, and socio- demographic and other factors were assessed through survey questions and LIWC lexicons and further analyzed in Study 4. The results showed that people are less enthusiastic about working with robots than with humans. Our findings suggest these more negative reactions stem from feelings of oddity in an unusual situation and the lack of social interaction.  1. Introduction People have been using automation and working with robots in in-dustry fields such as manufacturing for many years. Researchers suggest that the exceptional situation caused by COVID-19 and social distancing guidelines will further increase the use of advanced information sys-tems, such as robots, at work (Coombs, 2020; He, Zhang, & Li, 2021). Due to the development of more interactive, collaborative, and social robots, people are more likely to be in situations in which they must work and interact with robots as coworkers or teammates (Dwivedi et al., 2021; Haidegger et al., 2013; M¨ortl et al., 2012). As a result, new-generation robots will create new social and psychological chal-lenges that could impact work life profoundly. There is a sufficient body of evidence confirming that social psy-chological processes such as attitudes and trust are essential factors in successful collaboration with robots and ultimately accepting them in everyday life (Hancock et al., 2011; Schaefer, Straub, Chen, Putney, & Evans, 2017; Sheridan, 2016; Yusif, Soar, & Hafeez-Baig, 2016). In addition to these extensively researched factors, robotization is likely to arouse both positive and negative emotional reactions in human workers. Introducing advanced technology such as social robots as co-workers in the same organization or work team presents human workers with a new situation. Adapting to this could be more challenging to some workers than others, causing negative attitudes and emotions that could have an unwanted effect on emotional well-being. In addition to examining acceptance of robots through attitudes and trust, researchers have investigated emotional attachment to companion robots (Friedman, Kahn, & Hagman, 2003); emotional reactions to ill-treatment of robots (Rosenthal-von der Pütten, Kr¨amer, Hoffmann, Sobieraj, & Eimler, 2013); and the connection between negative emo-tions, such as anxiety, and negative attitudes (Nomura, Kanda, & Suzuki, 2006). Even though working closely with robots has been argued to arouse negative attitudinal and emotional reactions in human workers (Groom & Nass, 2007), we do not currently know how people would respond emotionally to working with robots on the same work team or in the workplace community with robots. In addition to explicit methods of measuring attitudes and emotions, such as surveys, emotional and attitudinal reactions toward robot co-workers can be investigated through more implicit means such as examining textual data collected from role-playing scenarios. Computer- aided analysis methods have generated the massive new field of affec-tive computing, which offers fast and quantitative means of analyzing large amounts of text with the help of emotional lexicons (Piryani, Madhavi, & Singh, 2017). Our study was designed to fill the research gap through analysis of textual data collected from three role-playing experiments that involved * Corresponding author at: Faculty of Social Sciences, 33014 Tampere University, Tampere, Finland. E-mail address: nina.savela@tuni.fi (N. Savela). https://doi.org/10.1016/j.ijinfomgt.2021.102361 Received 5 August 2020; Received in revised form 6 May 2021; Accepted 7 May 2021  InternationalJournalofInformationManagement60(2021)102361Availableonline23May20210268-4012/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).Konstanzer Online-Publikations-System (KOPS) URL: http://nbn-resolving.de/urn:nbn:de:bsz:352-2-1cnionttcewjv7\fN. Savela et al.                                                                                                                 introduction of robots as work team members or as coworkers within a workplace. We focused on emotional reactions to the hypothetical sit-uations, as identified via sentiment analysis, in three studies and further investigated the associated factors in a fourth study. Computational social scientific analysis methods combined with an experimental design and online role-playing data collection method generated a unique multi-methodological approach that has not previously been utilized to investigate the acceptance of robots. 2. Literature review The concept of emotion has a long and complex history in philosophy and psychology, and it has traditionally been used as a metaconcept that combines different words describing feelings and attitudes (Dixon, 2012). One empirical study considered emotion as an intense mental state with hedonic content (Cabanac, 2002). There is no consensus on the definition, process, or hierarchical levels of emotion among multiple emotion theories, but most support some form of connection between emotion and cognitive appraisal (Barnard & Teasdale, 1991; Moors, 2009). Theories of attitudes often include both cognitive and emotional perspectives, and this is specifically manifested in a multicomponent model of attitude (Zanna & Rempel, 2008). In the context of technology, researchers have investigated possible connections between cognitive and emotional constructs in the framework of the technology acceptance model (TAM) and its extensions (Kulviwat, Bruner, Kumar, Nasco, & Clark, 2007; Lee, Xiong, & Hu, 2012; Saad´e & Kira, 2006; Venkatesh, 2000). For example, in a model called consumer acceptance of tech-nology, affective and cognitive attitude dimensions explain the behav-ioral attitude toward adoption, which then predicts adoption intention (Kulviwat et al., 2007). According to a literature review about the his-tory of TAM (Maranguni´c & Grani´c, 2015), further integration of emo-tions into TAM is still needed. In research focused on the advanced technology of robots specif-ically, attitudes and emotions have often overlapped, especially in research measuring and focusing on negative emotions, such as anxiety, and negative attitudes (Nomura et al., 2006). TAM and its extensions have also been used in research on human–robot interaction and user studies, but some researchers have stressed caution when applying it to interactive technology such as robots (Young, Hawkins, Sharlin, & Igarashi, 2009). For this reason and because this research area is an emerging field, the tools used to measure different social and psycho-logical constructs have varied. Because emotion is linked to attitudes and behavior (Gursoy, Chi, Lu, & Nunkoo, 2019; Kulviwat et al., 2007), and because the cognitive measures of attitude have their weaknesses (Peters & Slovic, 2007), investigating emotional responses in acceptance of emerging technologies such as robots is an important research avenue. Evidence that humans can feel empathy and get emotionally attached to artificial beings confirms that artificial entities such as ro-bots can arouse emotional reactions (Kr¨amer, Eimler, von der Pütten, & Payr, 2011; Rosenthal-von der Pütten et al., 2013). Other researchers suggested that even imagined contact with a robot can affect emotions toward robots (Wullenkord, Fraune, Eyssel, & ˇSabanovi´c, 2016). The examination of emotions toward robots is essential because they affect social processes such as identification and play an important role in human behavior (DeSteno, Dasgupta, Bartlett, & Cajdric, 2004; DeS-teno, Petty, Rucker, Wegener, & Braverman, 2004). This has conse-quences for the intended use and possible benefits gained from larger utilization of robots in work life. Emotional detection literature offers different ways to examine emotions from facial expressions, speech, and writing (Cowie & Cor-nelius, 2003; Russell, Bachorowski, & Fern´andez-Dols, 2003). For example, females and older people are more likely to express positivity in writing (Pennebaker & Stone, 2003; Thelwall, Wilkinson, & Uppal, 2010), neurotic people are likely to use negative language, and extraverted and agreeable people are more likely to use positive words (Yarkoni, 2010). However, different associations could emerge in the context of robots. The more traditional research literature on robot acceptance gives some information about the expected associations and factors to consider when studying emotional expressions in written re-actions toward robots. Some literature has suggested a difference in attitudes toward robots based on age and gender, with young individuals and males being more willing to accept robots (Flandorfer, 2012). H",
            {
                "entities": [
                    [
                        227,
                        239,
                        "AUTHOR"
                    ],
                    [
                        244,
                        257,
                        "AUTHOR"
                    ],
                    [
                        260,
                        272,
                        "AUTHOR"
                    ],
                    [
                        280,
                        293,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Futures journal homepage: www.elsevier.com/locate/futures Moral circle expansion: A promising strategy to impact the far future Jacy Reese Anthis a,b,*, Eze Paez c,d a The University of Chicago, USA b Sentience Institute, USA c Pompeu Fabra University, Spain d Centre de Recherche en ´Ethique, Universit´e de Montr´eal, Canada  A R T I C L E I N F O  A B S T R A C T  Keywords: Existential risk Effective altruism Artificial intelligence Animal ethics Longtermism 1. Introduction Many sentient beings suffer serious harms due to a lack of moral consideration. Importantly, such harms could also occur to a potentially astronomical number of morally considerable future beings. This paper argues that, to prevent such existential risks, we should prioritise the strategy of expanding humanity’s moral circle to include, ideally, all sentient beings. We present empirical evidence that, at micro- and macro-levels of society, increased concern for members of some outlying groups facilitates concern for others. We argue that the perspective of moral circle expansion can reveal and clarify important issues in futures studies, particularly regarding animal ethics and artificial intelligence. While the case for moral circle expansion does not hinge on specific moral criteria, we focus on sentience as the most recommendable policy when deciding, as we do, under moral uncertainty. We also address various nuances of adjusting the moral circle, such as the risk of over-expansion.  There are currently around 8 billion humans (109) on Earth. There are over 100 billion domestic animals (1011)—primarily chickens and fishes used in the food industry. There may be trillions of wild birds and mammals (1012) and over a quintillion tiny wild animals such as insects (1018) (Tomasik, 2009). These moral stakes are difficult to conceptualise, yet they pale in comparison to the potential long-term scope of human civilisation in the distant future, such as with interstellar expansion.1 Some moral philosophers and futures scholars are beginning to consider our impact on the far future as a serious and perhaps overwhelmingly important consideration that we should account for in present decisions (see, for example, Beard, Rowe, & James, 2020; Bostrom, 2014; Liu, Lauta, & Maas, 2018; Kareiva & Carranza, 2018; Moynihan, 2020; Parfit, 1992). Such work is part of a broader project of prescriptive futures studies, asking questions such as how to encourage thoughtful contemplation of the future, how to design future-oriented public policy, and how to account for the indirect effects of our actions on the future (Ahvenharju, * Corresponding author. E-mail addresses: jacy@uchicago.edu (J.R. Anthis), joseezequiel.paez@upf.edu (E. Paez).  1 For example, suppose humanity expanded to the Virgo Supercluster. Then the human population could be one hundred undecillion (1038). See Bostrom (2003). Notice, however, that the same interstellar resources could fuel many more minds of less complexity, and therefore less energy cost, than the human mind. Thus, assuming minds less complex than the human mind can be sentient, the total number of possible future sentient in-dividuals is proportionally higher. https://doi.org/10.1016/j.futures.2021.102756 Received 22 March 2021; Received in revised form 23 April 2021; Accepted 26 April 2021  Futures130(2021)102756Availableonline30April20210016-3287/©2021TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).\fJ.R. Anthis and E. Paez                                                                                               Minkkinen, & Lalot, 2018; Dorsser, Walker, Taneja, & Marchau, 2018; Ng, 2020). Restricting our concern to present or near-future generations of human beings may be a significant normative blindspot, neglecting the majority of individuals who will, at any time, exist. While work on ‘existential risk’ has focused on extinction risk in particular, recent work has highlighted the importance of ‘suffering risks’, which are ‘astronomical in scope and hellish in severity’ (Kovic, 2021). Human history is riddled with atrocities committed against contemporary members of our own species, and if species-based discrimination is wrong, then history is also riddled with serious harms committed against nonhuman animals. Thus, the risk that we commit atrocities on an interstellar scale is a serious danger—perhaps one of the greatest dangers that exist on the horizon of our species. In this article we present a novel approach to reducing such existential risks and bettering the future. We build on an interdisci-plinary literature including animal ethics, environmental ethics, ethical theory, history, and social psychology. We defend two claims. The first is that to minimise the danger of existential risks, particularly suffering risks, we should prioritise a strategy that we call: Moral circle expansion: a community’s moral circle has expanded with respect to some previous time t if, and only if, a number of entities which used to be given less than full moral consideration at t are now given more moral consideration. Clearly laying out this definition allows us to describe the following familiar phenomenon as a case of moral circle expansion: a community of moral agents concludes that a feature on which they previously based the moral inconsiderability of some individuals is not morally relevant. Thus, they extend moral consideration to all members of the set of entities that possess that feature. This definition is completely agnostic about what attributes—sentience, rationality, being alive, etc.—a community uses to construct its moral circle. In addition, it is not a moralised definition. Thus, according to our definition, in order to identify some shift in a com-munity’s sphere of concern as an instance of moral circle expansion, it is not necessary to judge whether they are deploying a more justified criteria of moral considerability. This has several advantages. First, we do not need to endorse the same criteria in order to agree that a community’s moral circle has expanded. Second, our definition allows us to identify instances of moral circle expansion in a society and retain the ability to criticise them (i.e., one can believe a specific instance of moral circle expansion was a mistake). Note, finally, that this is a definition of, so to speak, gross expansions of the moral circle. It could be modified to accommodate calculations of net expansion. This would consist of something like the number of new entities included with respect to t minus the number of entities excluded with respect to t, given an appropriate specification of ‘minus’ that clarifies the subtractivity of ‘sets’.2 We will argue that explicitly discussing and analysing moral circle expansion is a revealing yet underexplored perspective for understanding the nature of moral progress. While much scholarly work can fit under the umbrella of moral inclusion and exclusion (e. g., prejudice, discrimination), anything more than a passing reference to the moral circle is rare, and we argue there is much to be gained by direct exploration of moral circle expansion itself. Our second claim is that humanity’s moral circle ought to be expanded to include, ideally, all sentient beings. That is, all beings with a capacity for positive and negative experiences.3 In this paper, we will not assume that sentience is necessary for moral con-siderability. However, looking back on historical atrocities, it seems that many could have been prevented or mitigated if communities had extended their moral concern to other groups of sentient beings, such as humans of various races, ethnicities, sexualities, genders, or nationalities—or animals of various species who share this planet with humankind (Judge & Wilson, 2015; Wright, 2018). Thus, even if one is unsure what exactly the future of the moral circle should look like (e.g., the moral patienthood of artificial intelligence), pushing on the current frontiers of the moral circle (e.g., farmed animals) or otherwise engendering expansion towards other kinds of sentient beings is a compelling moral priority not only for utilitarians and others concerned with doing the most good but, more generally, for those concerned with preventing serious wrongs. Artificial intelligence researcher Paul Christiano (2013) argues that, for those trying to do the most good, ‘changing long-term social values’ should not be a priority for several reasons, particularly the changing nature of moral values upon reflection and the potentially zero-sum nature of competing moral values. In addition, there are many other pressing issues demanding our limited attention, such as the reduction of extinction risk (Bostrom, 2003), the mitigation of the effects of anthropogenic climate change, and the alleviation of global poverty.4 Thus, the claim that moral circle expansion should be prioritised, among all of those compelling projects, is in need of some defence. 2 As we mentioned, a society’s expansion of its moral circle usually occurs in terms of sets, rather than individuals. That is, increased moral consideration is usually given to members of the set of individuals who possess a particular attribute. This is consistent with prominent accounts of discrimination in terms of the exclusion of societal groups (see, for example, Lippert-Rasmussen, 2013). We decided to define gross moral circle expansion in terms of individuals, rather than in terms of set, because of the difficulties the latter generates. First, for a given situation, there may be various equally acceptable criteria for carving up sets and there seems to be no value-neutral way to decide which to choose. Employing different criteria, the same societal shift may be described either as an expansion or a contraction. Second, even if there were reasons to prefer some set constructi",
            {
                "entities": [
                    [
                        169,
                        187,
                        "AUTHOR"
                    ],
                    [
                        194,
                        203,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "2202nuJ32]VC.sc[2v91970.7012:viXraA survey on bias in visual datasetsSimone Fabbrizzi∗1,3, Symeon Papadopoulos1, Eirini Ntoutsi2, and IoannisKompatsiaris11CERTH-ITI, Thessaloniki, Greece2Freie Universit¨at, Berlin, Germany3Leibniz Universit¨at, Hannover, GermanyJune 24, 2022Abstract1IntroductionComputer Vision (CV) has achieved remarkableresults, outperforming humans in several tasks.Nonetheless, it may result in significant discrim-ination if not handled properly as CV systemshighly depend on the data they are fed with andcan learn and amplify biases within such data.Thus, the problems of understanding and dis-covering biases are of utmost importance. Yet,there is no comprehensive survey on bias in vi-i) de-sual datasets. Hence, this work aims to:scribe the biases that might manifest in visualdatasets;ii) review the literature on methodsfor bias discovery and quantification in visualdatasets; iii) discuss existing attempts to collectbias-aware visual datasets. A key conclusion ofour study is that the problem of bias discoveryand quantification in visual datasets is still open,and there is room for improvement in terms ofboth methods and the range of biases that canbe addressed. Moreover, there is no such thingas a bias-free dataset, so scientists and practi-tioners must become aware of the biases in theirdatasets and make them explicit. To this end,we propose a checklist to spot different types ofbias during visual dataset collection.∗Corresponding author: simone.fabbrizzi@iti.grThis work is currently under review at Computer Vi-sion and Image Understanding.In the fields of Artificial Intelligence (AI), al-gorithmic fairness, and (big) data ethics, theterm bias usually refers to the case in whichAI-powered decisions show prejudice against in-dividuals or groups of people defined based onprotected attributes like gender or race [Ntoutsiet al., 2020].Instances of this prejudice havecaused discrimination in many fields, includingrecidivism scoring [Angwin et al., 2016], onlineadvertisement [Sweeney, 2013], facial recognition[Cook et al., 2019], and credit scoring [Bartlettet al., 2019].Defining the concepts of bias and fairness inmathematical terms is not a trivial task. Vermaand Rubin [2018] provided a survey on morethan 20 different measures of algorithmic fair-ness, many of which are incompatible with eachother. This incompatibility - the so-called im-possibility theorem [Chouldechova, 2017, Klein-berg et al., 2017] - forces scientists and practi-tioners to choose the measures they use based ontheir personal beliefs or other constraints (e.g.,business models) on what has to be consideredfair for the particular problem/domain.While algorithms may also be responsible forthe amplification of pre-existing biases in thetraining data [Bolukbasi et al., 2016], the qual-ity of the data itself contributes significantly to1   \fthe development of discriminatory AI applica-tions. Ntoutsi et al. [2020] identified two ways inwhich bias is encoded in the data: correlationsand causal influences among the protected at-tributes and other features; and the lack of repre-sentation of protected groups in the data. Theyalso noted that biases manifest in ways that arespecific to the data type.In this work, we focused on how biases canbe encoded in the data (e.g., via spurious cor-relations, causal relationship among the vari-ables, and unrepresentative data samples) and,in particular,images andin visual data (i.e.,videos), which comprises one of the most popu-lar and complex data types. Visual data encap-sulates many features that require human expe-rience and context to interpret. These includethe human subjects, how they are depicted andtheir reciprocal position in the image frame, im-plicit references to culture-specific notions andbackground knowledge, etc. Even the colour-ing scheme can convey different messages. Thus,making sense of visual content remains a verycomplex task, and understanding bias in visualdata is even harder.Computer vision (CV), the primary domainthat enables computers to gain high-level under-standing from visual data, is heavily dominatednowadays by deep learning (DL) methods [Le-Cun et al., 2015, Baraniuk et al., 2020] that al-lowed for outstanding performance in tasks likeobject detection, image classification and imagesegmentation. DL methods, however, rely heav-ily on data, and the results are as good and“fair” as the data used for their training. CV hasrecently drawn attention for its ethical implica-tions when deployed in several settings, rangingfrom targeted advertising to law enforcement.There has been mounting evidence that deploy-ing CV systems without a comprehensive ethi-cal assessment may result in major discrimina-tion against protected groups. For instance, fa-cial recognition technologies [Cook et al., 2019,Robinson et al., 2020], gender classification algo-rithms [Buolamwini and Gebru, 2018], and au-tonomous driving systems [Wilson et al., 2019]have been all shown to exhibit discriminatorybehaviour.While bias in AI systems is a well-studied field,the research in biased CV is more limited despitethe abundance of visual data produced nowa-days and their widespread use in the ML com-munity. Moreover, to the best of our knowledge,there is no comprehensive survey on bias in vi-sual datasets ([Torralba and Efros, 2011] repre-sents a seminal work in the field, but it is limitedto object detection datasets). Hence, the contri-i) to explorebutions of the present work are:and discuss the different types of bias that arisefrom the collection of visual data; ii) to system-atically review the works that aim at address-ing and measuring bias in visual datasets; iii)to discuss some attempts to compile bias-awaredatasets. We believe this work to be a useful toolfor helping scientists and practitioners to bothdevelop new bias-discovery methods and collectdata in ways as less biased as possible. To thelatter end, we propose a checklist that can beused to spot the different types of bias that mightenter the data during the collection process (Ta-ble 6).The structure of the survey is as follows. First,we describe in detail the different types of biasthat might affect visual datasets (Section 2), pro-vide concrete examples of CV applications thatare affected by those biases, and a description ofhow they manifest in the life cycle of visual con-tent. Second, we systematically review the meth-ods for bias discovery in visual content proposedin the literature (Section 3). Third, in Section 4,we discuss the weaknesses and strengths of somebias-aware visual benchmark datasets. Finally,in Section 5, we conclude and outline some pos-sible future research direction.2 Manifestation of Bias inVisual DataIn this section, we describe in detail the typesof bias that pertain to the capture and collectionof visual data (Figure 1), namely: selection bias2\fFigure 1: Examples of selection, framing and label bias. On the right, a list of applications thatcan be affected by each type of bias.3\f(Section 2.1), framing bias (Section 2.2) and la-bel bias (Section 2.3). Furthermore, we describe(Section 2.4) how they manifest within the lifecycle of visual content, from capture to the de-ployment of CV algorithms. Note that a compre-hensive analysis of historical discrimination andalgorithmic bias is beyond the scope of this work.The interested reader can refer to Bandy [2021]for a survey on methods for auditing algorithmicbias both in CV and other AI-related areas.Our categorisation builds on the scheme byTorralba and Efros [2011] who organised thetypes of visual bias into four different categories:selection bias, capture bias (which we collapseinto the more general concept of framing bias),label bias, and negative set bias. The latter ariseswhen the labelling does not reflect entirely thepopulation of the negative class (say non-whitein a binary feature [white people/non-white peo-ple]). We consider negative class bias as an in-stance of selection and label bias.Even though our categorisation appears on thesurface to be similar to the one by Torralba andEfros [2011], their analysis focused on datasetsfor object detection.Instead, we contextualisebias in a more general setting and we also fo-cus on discrimination against protected groups1.Since selection, framing and label bias manifestin many different ways, we also go further by de-scribing a sub-categorisation of these three typesof bias (Table 1) including several biases com-monly encountered in Statistics, Health studies,or Psychology and adapting them to the contextof visual data. While in the following we de-scribe selection, framing, and label bias in gen-eral terms, we also provide references in Table1 for the interested reader who might want todelve further into their different manifestations.2.1 Selection BiasDefinition Selection bias is the type of biasthat “occurs when individuals or groups in a1Note that, while this is the focus of our survey, wealso take into account cases in which bias does not nec-essarily affect people, e.g., in object detection.study differ systematically from the populationof interest leading to a systematic error in anassociation or outcome”2. More generally,itrefers to any “association created as a result ofthe process by which individuals are selectedinto the analysis” ([Hern´an and Robins, 2020,Chapter 8, pg. 99]). In visual datasets, using thefirst definition would be tricky as, for instance,in the case of facial recognition, respecting theethnic composition of the population is generallynot enough to ensure good performance acrossevery subgroup, as we will see in the following.Hence, we adopt a slight modification of Hern´anand Robins [2020] definition:We call selection bias any disparities or as-sociations created as a result of the processby which subjects are included in a visualdataset.Description Torralba and Efros [2011] showedthat certain kinds of imagery are more likely tobe selected during the collection of large-scaleleading to s",
            {
                "entities": [
                    [
                        90,
                        110,
                        "AUTHOR"
                    ],
                    [
                        112,
                        127,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fContents lists available at ScienceDirect Computers in Human Behavior journal homepage: http://www.elsevier.com/locate/comphumbeh Full length article Machine learning techniques and older adults processing of online information and misinformation: A covid 19 study Jyoti Choudrie a,*, Snehasish Banerjee b, Ketan Kotecha c, Rahee Walambe c, Hema Karende c, Juhi Ameta c a University of Hertfordshire, Hertfordshire Business School, DeHavilland Campus, Hatfield. Herts, AL109EU, UK b York Management School, University of York, Freboys Lane, YO10 5GD, UK c Symbiosis Centre for Applied Artificial Intelligence (SCAAI), Symbiosis Institute of Technology, Pune. Symbiosis (Deemed University), Pune, Maharashtra, 412115, India  A R T I C L E I N F O  A B S T R A C T  Keywords: AI Machine learning techniques COVID-19 pandemic Older adult Interview Information-misinformation This study is informed by two research gaps. One, Artificial Intelligence’s (AI’s) Machine Learning (ML) tech-niques have the potential to help separate information and misinformation, but this capability has yet to be empirically verified in the context of COVID-19. Two, while older adults can be particularly susceptible to the virus as well as its online infodemic, their information processing behaviour amid the pandemic has not been understood. Therefore, this study explores and understands how ML techniques (Study 1), and humans, particularly older adults (Study 2), process the online infodemic regarding COVID-19 prevention and cure. Study 1 employed ML techniques to classify information and misinformation. They achieved a classification accuracy of 86.7% with the Decision Tree classifier, and 86.67% with the Convolutional Neural Network model. Study 2 then investigated older adults’ information processing behaviour during the COVID-19 infodemic period using some of the posts from Study 1. Twenty older adults were interviewed. They were found to be more willing to trust traditional media rather than new media. They were often left confused about the veracity of online content related to COVID-19 prevention and cure. Overall, the paper breaks new ground by highlighting how humans’ information processing differs from how algorithms operate. It offers fresh insights into how during a pandemic, older adults—a vulnerable demographic segment—interact with online information and misinformation. On the methodological front, the paper represents an intersection of two very disparate paradigms—ML techniques and interview data analyzed using thematic analysis and concepts drawn from grounded theory to enrich the scholarly understanding of human interaction with cutting-edge technologies.  1. Introduction When the internet was introduced to daily life, it was meant to offer immensely diverse knowledge and information (Ratchford et al., 2001). The internet has however also led to a growth of ignorance in various forms and guises that are labelled using terms such as fake news, disinformation and misinformation. This study specifically uses the term ‘misinformation’. Access to the internet is now, often, access to resources that reinforce biases, ignorance, prejudgments, and absurdity. Parallel to a right to information, some researchers believe that there is a right to ignorance (Froehlich, 2017). Meanwhile, a pandemic, COVID-19, has exposed several difficulties with the present global health care system. A societal concern for healthcare organizations and the World Health Organization (WHO) has been the spread of online misinformation that can exacerbate the impact of the pandemic (Ali, 2020). Almost 90% of Internet users seek online health information as one of the first tasks after experiencing a health concern (Chua & Banerjee, 2017). Therefore, regarding the pandemic COVID-19, where there is little a priori information and knowledge, individuals are likely to explore the online avenue. However, when searching for such information on the internet and social media, one is faced with an avalanche of information referred to as an ‘infodemic’, which includes a mixture of facts and hoaxes that are difficult to separate from one another (WHO, 2020a). If a hoax related to COVID-19 prevention and cure is mistaken as a fact, there could be serious ramifications on people’s health and well-being. Conceivably, * Corresponding author. E-mail address: j.choudrie@herts.ac.uk (J. Choudrie). https://doi.org/10.1016/j.chb.2021.106716 Received 31 July 2020; Received in revised form 10 January 2021; Accepted 22 January 2021  ComputersinHumanBehavior119(2021)106716Availableonline30January20210747-5632/©2021ElsevierLtd.Allrightsreserved.\fJ. Choudrie et al.                                                                                                                healthcare organizations and public health authorities are keen to ensure that people are not deceived by COVID-19-related misinforma-tion that has been circulating online. This is reflected in their propensity to submit misinformation-exposing posts on their social media channels (Raamkumar et al., 2020). Social media, also known as online social networks (OSN), have now emerged as contemporary ways to reach the consumer market. Artificial Intelligence (AI)—traditionally referring to an artificial creation of human-like intelligence that can learn, reason, plan, perceive, or process natural language (Russell & Norvig, 2009)—is associated with social media. It is “an area of computer science that aims to decipher data from the natural world often using cognitive technologies designed to un-derstand and complete tasks that humans have taken on in the past” (Ball, 2018, para. 4). The adoption of AI, a cutting-edge technology, has been propelled to an unprecedented level in the wake of the pandemic. With regards to health, AI-enabled mobile applications are now widely used for infection detection and contact-tracing (Fong et al., 2020). Even with regards to the infodemic, AI’s ML techniques can play a crucial role. Research has shown that ML techniques can help separate information from misinformation (Katsaros et al., 2019; Kinsora et al., 2017; Shu et al., 2017; Tacchini et al., 2017). However, despite the hype and enthusiasm around AI and social media, there is still a lack of under-standing in terms of how consumers interact and engage with these technologies (Ameen et al., 2020; Capatina et al., 2020; Rai, 2020; Wesche & Sonderegger, 2019). The extent to which algorithms can help detect misinformation amid information related to COVID-19 is there-fore worth investigating. Older adults constitute a consumer demographic group that is particularly susceptible to COVID-19 (WHO, 2020b). The pandemic causes pneumonia and symptoms such as fever, cough and shortness of breath among older adults (Adler, 2020), who usually exert maximal pressure on healthcare systems (WHO, 2020b). Moreover, ceteris par-ibus, older adults can also be susceptible to the ‘infodemic’. They are less confident than younger individuals in tackling the challenges that the online setting has to offer (Xie et al., 2021). Hence, older adults are more willing to trust the traditional media rather than what AI feeds them through social media (Media Insight Project, 2018). Still, they often end up becoming a victim of online misinformation (Guess et al., 2019; Seo et al., 2021). To protect this segment of the population from misinformation about COVID-19 prevention and cure, health care organizations would require a systematic understanding of not only the ‘infodemic’, but also of how older adults respond to it. Both are issues on which the literature has shed little light. To fill this gap, the aim of this study is: To explore and understand how AI’s ML techniques (Study 1) and older adults (Study 2) process the infodemic regarding COVID-19 prevention and cure. With this overarching research aim, the objective of this study is two- fold. First, it investigates the extent to which algorithms can distinguish between information and misinformation related to COVID-19 preven-tion and cure (Study 1). For this purpose, a supervised ML framework was developed to classify facts and hoaxes. Second, the study investigates older adults’ information processing behaviour in the face of the COVID-19 infodemic (Study 2). Informed by the results of Study 1 along with the theoretical lenses of misinforma-tion, information processing and trust, 20 older adults were interviewed to understand how they had been coping with the infodemic associated with COVID-19 prevention and cure in their daily lives. This study is important and timely for several reasons. First, The World Health Organization (WHO) declared that besides finding pre-ventions and cures for the pandemic, it was also concerned about the online infodemic. By addressing the infodemic problem from both the computational and behavioral perspectives, the study represents a timely endeavour in the aftermath of the COVID-19 outbreak. Second, the study introduces a machine learning framework to classify infor-mation and misinformation related to COVID-19 prevention and cure. As will be shown later, the classification performance was generally promising. Third",
            {
                "entities": [
                    [
                        1041,
                        1056,
                        "AUTHOR"
                    ],
                    [
                        1061,
                        1080,
                        "AUTHOR"
                    ],
                    [
                        1083,
                        1097,
                        "AUTHOR"
                    ],
                    [
                        1100,
                        1114,
                        "AUTHOR"
                    ],
                    [
                        1117,
                        1130,
                        "AUTHOR"
                    ],
                    [
                        1133,
                        1144,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "BIROn - Birkbeck Institutional Research OnlineVidgen, Richard and Hindle, G. and Randolph, I. (2019) Exploring the ethicalimplications of business analytics with a business ethics canvas. EuropeanJournal of Operational Research 281 (3), pp. 491-501. ISSN 0377-2217.Downloaded from: hUsage Guidelines:Please refer to usage guidelines at hcontact lib-eprints@bbk.ac.uk.or alternativelyttps://eprints.bbk.ac.uk/id/eprint/27315/ttps://eprints.bbk.ac.uk/policies.html\fExploring the ethical implications of business analytics with a business ethics canvas Richard Vidgen* UNSW Business School, University of New South Wales, Sydney NSW 2052, Australia School of Business, Economics and Informatics, Birkbeck University, London WC1E 7HX, UK Email: r.vidgen@unsw.edu.au Giles Hindle Hull University Business School, University of Hull, Cottingham Road, Hull HU6 7RX, UK Email: giles.hindle@hull.ac.uk Ian Randolph Data Scientist Email: ian.david.randolph@gmail.com *corresponding author 1     \fExploring the ethical implications of business analytics with a business ethics canvas Abstract The ethical aspects of data science and artificial intelligence have become a major issue. Organisations that deploy data scientists and operational researchers (OR) must address the ethical implications of their use of data and algorithms. We review the OR and data science literature on ethics and find that this work is pitched at the level of guiding principles and frameworks and fails to provide a practical and grounded approach that can be used by practitioners as part of the analytics development process. Further, given the advent of the General Data Protection Regulation (GDPR) an ethical dimension is likely to become an increasingly important aspect of analytics development. Drawing on the business analytics methodology (BAM) developed by Hindle and Vidgen (2018) we tackle this challenge through action research with a pseudonymous online travel company, EuroTravel. The method that emerges uses an opportunity canvas and a business ethics canvas to explore value creation and ethical aspects jointly. The business ethics canvas draws on the Markkula Center’s five ethical principles (utility, rights, justice, common good, and virtue) to which explicit consideration of stakeholders is added. A contribution of the paper is to show how an ethical dimension can be embedded in the everyday exploration of analytics development opportunities, as distinct from a stand-alone ethical decision-making tool or as an overlay of a general set of guiding principles. We also propose that value and ethics should not be viewed as separate entities, rather they should be seen as inseparable and intertwined. Keywords: business analytics; data science; business ethics canvas; Markkula; GDPR 2   \fExploring the ethical implications of business analytics with a business ethics canvas 1 INTRODUCTION Business analytics is playing a greater and greater role in our daily lives, impacting on job applications, medical treatment, parole eligibility, and loans and financial services. There are undoubted benefits to algorithmic decision-making in general, and artificial intelligence (AI) in particular. For example, AI is being used to detect the early stages of colorectal cancer, achieving 86% accuracy (Mukherjee, 2017). Such is the interest in AI for healthcare that the UK Government is pledging millions to AI applications for the early diagnosis of cancer and other chronic diseases, using patient data and lifestyle information to highlight patients at risk (Perkins, 2018). However, algorithmic decision-making is not without its dark side. Mann and O’Neill (2017) question the use of algorithms in hiring decisions, d’Alessandro et al. (2017) raise concerns about predictive policing. The Cambridge Analytica case has thrust data analytics squarely into the public domain. It is alleged that Cambridge Analytica collected data from more than 50 million Facebook users (without permission) and used that data to build a system to target US voters with personalized political advertisements with the aim of influencing the US election outcome (Greenfield, 2018). Unsurprisingly, algorithmic decision-making is attracting the interest of researchers as well as practitioner and regulators (e.g., Newell and Mirabelli, 2015; Kitchin, 2017). The potential for harm – intended or unintended - arising from algorithmic decision-making indicates that an ethical dimension is needed. For example, Google, on discovering that its AI software was being used by the US military in its drone development programme, has pledged not to use AI for weaponry (Statt and Vincent, 2018). Google’s CEO, Sundar Pichai, published a list of ethical principles for AI development, which include: be socially beneficial, avoid creating or reinforcing unfair bias, and be accountable to people (Pichai, 2018). These principles are prefigured by the Toronto Declaration, which is calling for governments and companies to ensure that algorithms respect basic principles of equality and non-discrimination (Brandom, 2018). Rights are being further encoded in legislation such as the General Data 3  \fProtection Regulation (GDPR), which requires organizations to protect the rights and privacy of individuals with associated constraints on data usage and a responsibility to provide a right to explanation and to address any presence of discrimination and bias. We consequently argue that an ethical dimension to algorithm development and algorithmic decision-making is an essential aspect of the OR practitioner’s professional profile. Indeed, concern for the ethical aspects of OR goes right back to the early pioneers of the discipline (for example, Churchman 1968, 1970; 1971; Ackoff 1974a, 1974b) who included a concern for stakeholders and wider society within their conceptualization of OR intervention. And a degree of concern for the ethical aspects of OR as a profession is reflected in early guidelines and codes created by OR societies such as the OR Society of America (ORSA) in the USA (Caywood et al., 1971) and the Fellowship for OR in the UK (Fellowship for Operational Research, 1974). However, despite this evidence, we argue an explicit concern for ethics and the ‘goodness’ of OR practice has tended to fall outside of traditional or mainstream discussions on OR and, more latterly, data science. Such discussions have tended to focus on the efficacy of a range of quantitative modelling techniques and on how to achieve operational and process improvement in practice (Koch, 2000). The practice of OR has generally been regarded as a 'good thing' within OR communities due to the largely uncontroversial nature of process improvement within organisations and its scientific credentials and associations. Subsequently, a concern for ethics and ethical practice is not generally covered in a substantive way within OR textbooks or on courses in OR. Thus, while ethics is a long-established and on-going concern for the OR community we argue that much of this work has been at too abstract a level for it to impact meaningfully on the lived day-to-day experience of OR and data science practitioners. For example, in the context of software development, McNamara et al. (2018) found no evidence that the Association for Computing Machinery (ACM) code of ethics influences software-related ethical decision making. Given the development of data science practice and the emerging regulatory environment this abstract approach to ethics is not sufficient – practitioners need practical tools, not just frameworks. Our aim, therefore, is to address the question: how can an ethical dimension be built into the process of business analytics development? We tackle this question using action 4 \fresearch. In the next section we provide the background to ethics in OR and in section three we describe the development of a framework for ethical guidance in data science. The research approach is outlined in section 4. In sections five, six, seven, and eight we present the action research project under the headings of diagnosis and planning, action taking, evaluation, and reflection. The paper concludes with a summary. 2 OR AND ETHICS There has been an ongoing concern for the ethical aspects of OR within the community, which can be traced back to the early pioneers of the discipline (for example, Churchman 1968, 1970; 1971; Ackoff 1974a, 1974b). These researchers envisioned a broader role for OR than simply process engineering and proposed that a concern for stakeholders and wider society should be part of OR’s methodology. However, despite this genuine concern, we argue ethical support for practitioners has tended to fall outside of mainstream discussions on OR practice (Koch, 2000). This is because the practice of OR has generally been regarded as a 'good thing' within the OR community due to the largely uncontroversial nature of process improvement within organizations and OR’s scientific credentials and associations. Ethical issues in OR have been addressed by both professional societies and by academics. 2.1 The professional society perspective The website of the Institute for Operations Research and the Management Sciences (INFORMS) in the USA contains ethical guidelines (INFORMS, 2018) and the Operational Research Society (ORS) in the UK lists ethical principles (OR Society, 2018). The INFORMS guidelines are split into three sections: society, organizations and the OR profession (INFORMS, 2018). The ‘society’ section is concerned with aspiring to openness of assumptions, objectives and sponsors, to objectivity of analysis whilst being respectful of other views and values, and to undertaking work that provides positive benefits, such as progressing scientific understanding, organizational improvement and supporting the social good. The ‘organizations’ section is concerned with aspiring to be accurate, rigorous and realistic in conducting analysis, whilst being alert to the possi",
            {
                "entities": [
                    [
                        549,
                        564,
                        "AUTHOR"
                    ],
                    [
                        761,
                        774,
                        "AUTHOR"
                    ],
                    [
                        892,
                        905,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Available online at www.sciencedirect.com Available online at www.sciencedirect.com ScienceDirect ScienceDirect Procedia Computer Science 00 (2022) 000–000 Procedia Computer Science 00 (2022) 000–000 Procedia Computer Science 211 (2022) 36–46www.elsevier.com/locate/procedia www.elsevier.com/locate/procedia 15th International Conference on Current Research Information Systems 15th International Conference on Current Research Information Systems Research Information Systems and Ethics relating to Open Science Research Information Systems and Ethics relating to Open Science Joachim Schöpfela*, Otmane Azeroualb, Pablo de Castroc Joachim Schöpfela*, Otmane Azeroualb, Pablo de Castroc aGERiiCO-Labor, University of Lille, 59650 Villeneuve-d’Ascq, France aGERiiCO-Labor, University of Lille, 59650 Villeneuve-d’Ascq, France bGerman Centre for Higher Education Research and Science Studies (DZHW), Schützenstraße 6a, 10117 Berlin, Germany bGerman Centre for Higher Education Research and Science Studies (DZHW), Schützenstraße 6a, 10117 Berlin, Germany cUniversity of Strathclyde, 101 St James Road, Glasgow G4 0NS, United Kingdom cUniversity of Strathclyde, 101 St James Road, Glasgow G4 0NS, United Kingdom Abstract Abstract Current research information systems (CRIS) evaluate research performance and are intended to contribute to the continuous Current research information systems (CRIS) evaluate research performance and are intended to contribute to the continuous improvement of research. Based on former research on the ethical dimensions of CRIS, our paper presents the results of a survey improvement of research. Based on former research on the ethical dimensions of CRIS, our paper presents the results of a survey with a small sample of representatives of ethics committees from different European countries on ethical aspects of CRIS. Ethics with a small sample of representatives of ethics committees from different European countries on ethical aspects of CRIS. Ethics committees and experts are rarely associated with CRIS-related projects. However, their opinion on ethical indicators and the committees and experts are rarely associated with CRIS-related projects. However, their opinion on ethical indicators and the implementation and use of a CRIS is undoubtedly essential for the future development and management of these systems. Against implementation and use of a CRIS is undoubtedly essential for the future development and management of these systems. Against this background, our purpose is to provide a deeper understanding of the ethical aspects in the field of research information this background, our purpose is to provide a deeper understanding of the ethical aspects in the field of research information management, to show how CRIS represent ethical dimensions of scientific research and to suggest some adjustment of their management, to show how CRIS represent ethical dimensions of scientific research and to suggest some adjustment of their development, implementation and use. development, implementation and use. © 2022 The Authors. Published by Elsevier B.V.© 2022 The Authors. Published by ELSEVIER B.V. © 2022 The Authors. Published by ELSEVIER B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information SystemsInformation Systems Information Systems Keywords: Current research information systems; CRIS; research information management; research ethics; research integrity; research Keywords: Current research information systems; CRIS; research information management; research ethics; research integrity; research infrastructures; ethics committees; infraethics. infrastructures; ethics committees; infraethics. 1. The challenge of ethics 1. The challenge of ethics Research ethics, as a field of applied ethics, provides concepts and recommendations of “right” and “wrong” scientific Research ethics, as a field of applied ethics, provides concepts and recommendations of “right” and “wrong” scientific practice, especially norms of conduct that distinguish between acceptable (responsible) and unacceptable scientific practice, especially norms of conduct that distinguish between acceptable (responsible) and unacceptable scientific behavior. Many different research organizations and associations have adopted specific codes, rules and policies behavior. Many different research organizations and associations have adopted specific codes, rules and policies relating to research ethics, such as the Nuremberg Code of 1947, the 2010 Singapore Statement on Research Integrity relating to research ethics, such as the Nuremberg Code of 1947, the 2010 Singapore Statement on Research Integrity * Corresponding author. E-mail address: joachim.schopfel@univ-lille.fr * Corresponding author. E-mail address: joachim.schopfel@univ-lille.fr 1877-0509 © 2022 The Authors. Published by ELSEVIER B.V. 1877-0509 © 2022 The Authors. Published by ELSEVIER B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information Systems Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information Systems 1877-0509 © 2022 The Authors. Published by Elsevier B.V.This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)Peer-review under responsibility of the scientific committee of the 15th International Conference on Current Research Information Systems10.1016/j.procs.2022.10.17410.1016/j.procs.2022.10.1741877-05092 Joachim Schöpfel et al. / Procedia Computer Science 00 (2022) 000–000 (Resnik & Shamoo, 2011), the European Code of Conduct for Research Integrity (2017) or the recent National Science Foundation’s Manual of “Conflicts of Interest and Standards of Ethical Conduct”. Due to the changing research environment “with new and complex technologies, increased pressure to publish, greater competition in grant applications, increased university-industry collaborative programs, and growth in international collaborations” but also to “highly publicized cases of misconduct” (Armond et al., 2021), academic interest in research ethics and research integrity is steadily increasing, above all in medical and health sciences. Main issues are falsification and fabrication of research data, informed consent, patient safety, plagiarism and conflict of interest. Research ethics “is a matter of debate” (Corvol, 2017). Even when researchers agree that research ethics is important, they do not agree on a common meaning but rather adopt divergent meanings that reflect their priorities, which stem from their personal needs, professional demands, or roles in society. Broadly speaking, research ethics can be defined as “doing good science in a good manner” according to shared and accepted standards of excellence and in compliance with all the steps necessary to meet the rules of responsible research conduct (appropriate data storage, conflict of interest management, protection of human and animal participants, laboratory safety etc.) (DuBois & Antes, 2018). Regarding new technologies and infrastructures, a growing body of research revealed essential and recurrent themes and dimensions of ethics, such as privacy, security, autonomy, justice, human dignity, control of technology and balance of power; some of these issues have been addressed through legal adjustments and new rules and obligations while “for other ethical issues (...) such as discrimination, autonomy, human dignity and unequal balance of power, the supervision is hardly organized” (Royakkers et al., 2018). In the field of big data and artificial intelligence, ethically-aligned technology has been defined as “that which is (a) beneficial to, and respectful of, people and the environment (beneficence); (b) robust and secure (non-maleficence); (c) respectful of human values (autonomy); (d) fair (justice); and (e) explainable, accountable and understandable (explicability)” (Morley et al., 2020). Regarding artificial intelligence, Morley et al. (2021) observed “that a significant gap exists between the theory of AI ethics principles and the practical design of AI systems”. Does the same observation apply to current research information systems (CRIS)? Do we need new tools and methods designed to help CRIS developers, engineers, and designers translate ethical principles into practice? In spite of the growing body of research on ethics in the field of technology, big data, artificial intelligence and so on, so far there are but few papers on ethics in the field of research information systems. For CRIS, ethics is a double challenge: to contribute to the development of responsible research in the context of open science, and to be able to measure the practices and performances recommended by these rules of responsible conduct. In other words, CRIS must respect the regulatory framework and the good practices, principles and values of scientific communities (Diener & Crandall, 1978; Guillemin & Gillam, 2004). But, at the same time, and this is indeed the particularity of these systems,",
            {
                "entities": [
                    [
                        597,
                        613,
                        "AUTHOR"
                    ],
                    [
                        652,
                        668,
                        "AUTHOR"
                    ],
                    [
                        615,
                        631,
                        "AUTHOR"
                    ],
                    [
                        670,
                        686,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Ethics by Design: Responsible Research & Innovation for AI in the Food Sector. Authors Peter J. Craigon1, Justin Sacks,2 Steve Brewer3 Jeremy Frey4, Anabel Gutierrez5, Naomi Jacobs2 Samantha Kanza4, Louise Manning3, Samuel Munday4, Alexsis Wintour6,Simon Pearson3 Corresponding Author – Peter J. Craigon – Peter.craigon4@nottingham.ac.uk Affiliations 1. Future Food Beacon of Excellence and School of Biosciences, University of Nottingham, Nottingham, UK 2. Imagination Lancaster, LICA, Lancaster University, Lancaster, UK 3. The Lincoln Institute of Agri Food Technology, University of Lincoln, Lincoln, UK 4. School of Chemistry, Faculty of Engineering & Physical Sciences, University of Southampton, Southampton, UK 5. School of Business and Management, Royal Holloway, University of London, Egham, UK 6. Lapin Limited, UK  \f1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 Ethics by Design: Responsible Research & Innovation for AI in the Food Sector Here we reflect on how a multi-disciplinary working group explored the ethical complexities of the use of new technologies for data sharing in the food supply chain. We used a three-part process of varied design methods, which included collaborative ideation and speculative scenario development, the creation of design fiction objects, and assessment using the Moral-IT deck, a card-based tool. We present, through the lens of the EPSRC’s Framework for Responsible Innovation how processes of anticipation, reflection, engagement and action built a plausible, fictional world in which a data trust uses artificial intelligence (AI) to support data sharing and decision-making across the food supply chain. This approach provides rich opportunities for considering ethical challenges to data sharing as part of a reflexive and engaged responsible innovation approach. We reflect on the value and potential of this approach as a method for engaged (co-)design and responsible innovation. Keywords: Responsible innovation, speculative design; food; ethics; card-based tools, AI 1 Introduction Predicting and influencing the future is a complex undertaking. It could however be argued that this is what Responsible (Research and) Innovation (RRI) asks innovators to do through Anticipating, Reflecting, Including and Responding about the implications of their work (Stilgoe et al 2013). This approach has been adapted by the Engineering and Physical Sciences Research Council (EPSRC) in the UK to become a framework for Responsible Innovation using the acronym for Anticipate, Reflect, Engage and Act (AREA)(EPSRC 2022) within the context of the societal desirability, ethical acceptability and sustainability of research and innovation (von Schomberg 2011) . This reflective piece describes and illustrates how RRI, regarding an autonomous food allergen tracking system was approached in a multidisciplinary way. This included researchers from disciplines including food systems, design, ethics, computer science, agriculture, chemistry and information systems. It is intended that this illustrated reflection could act as provocation and inspiration for others who find that in order to predict and influence the future of research and innovation, it helps to ‘design and make it’ first, even if it is yet to exist. The need to anticipate the future is a challenge shared by designers who have developed a series of methods for doing so in order to inform their work (Dunne & Raby 2013, Coulton et al 2017). While attempting to predict a single set future is fruitless, by exploring potential futures we can consider ramifications for the present, and inform design and innovation (Voros, 2001). The reflection set out here concerns the adoption and combination of two design methods to first ‘create’ a future of a technology through design fiction, and then engage with its potential benefits, harms, and associated amelioration strategies through the use of an ‘ethics by design’ (Dignum et al 2018, World Economic Forum 2020) card-based tool (Urquhart and Craigon 2021). This paper provides an example of ‘ethics by design fiction’ for trustworthy autonomous data sharing in the food system. 2 The Project 1        \fThis reflection examines prior work done to investigate the ethical implications of data sharing, data trusts and digital collaboration in the food system. This was undertaken by a multidisciplinary working group of experts, interested in the digitalisation of the food system, convened to develop an approach to investigating the ‘Ethical dimensions of digital collaboration in the food sector, such as the unintended consequences of AI’. (see [anonymised for review 2019, 2019a, 2021] for details). We reflect on the novel combination of design methods developed to explore and engage with these issues in a creative, open and engaging way. More detailed discussion of the working group activities can be found elsewhere (anonymized for reviews 2021a) but this current reflection will engage with the activities of the project through the lens of RRI and particularly the AREA framework to provide insight into this approach as a potential model for the engagement with the ethical complexity of the development, deployment and use of autonomous systems. The work of the project included three main activities. These are described, illustrated and the focus of the reflections throughout this paper. These activities were 1. An Initial Scoping workshop – This is where the group first met, discussed data sharing within the food sector from their differing perspectives and developed a proposed scenario for exploring it further, concerning tracking allergens through the food supply chain. 2. Worldbuilding and Design Fiction Development – The working group developed design fictions to concretise (fictional) instantiations of the allergen tracking system in action. This included worldbuilding, proposal of design fictions concepts and then development of four design fiction artefacts by the working group for ethical engagement and assessment 3. Moral IT Card Ethical Assessment – A further workshop was undertaken, with participants external to the working group, to explore the ethical implications raised by these artefacts through the use of a card-based tool in the form of the Moral-IT cards (Urquhart and Craigon 2021). This resulted in substantive rich discussion of each artefact and the digital collaboration system they instantiated as a whole. All workshops apart from the first scoping workshop were held online due to Covid-19 restrictions. The aim of this paper is to reflect on the processes of our project work and provide an example for practising engaged, reflective responsible innovation activities by design that could act as a potential methodological inspiration and provocation for other scenarios. 3. Initial Scoping Workshop 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 2    \f86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 Figure 1. A sketch created during brainstorming displays the initial allergen data trust scenario, which shows how    data about food is potentially shared across the supply chain. A shipment of food containing allergens (e.g. nuts or gluten) described in red moves from the top left corner, clockwise, being distributed amongst retailers and consumers. The arrows represent data about the shipment being communicated back and forth to the data trust (central box). The two boxes at the bottom show an app for example using the data to inform a consumer about the presence or otherwise of allergens in their food. The working group first met in February 2020 to discuss approaches to the ‘Ethical dimensions of digital collaboration in the food sector, such as the unintended consequences of AI’. Discussions from the multidisciplinary group identified the different understandings of ethical terminology across disciplines. This highlighted the need to explore and consider these multiple and different meanings in relation to AI and food, for example around transparency and traceability ([anonymised for review 2022]). Combining the expertise of the working group members facilitated deeper examination of these multiple meanings, and the establishment of a shared understanding. This was enabled by situating ethical issues via a tangible example (see below and FIG 1). Speculative design methods were thus chosen to enable development of mutually coherent artefacts ([anonymised for review]) which could also be assessed through the use of the Moral-IT cards. The concept scenario developed during this discussion was a food allergen tracking system (FIG 1). This is an example of a scenario in which proposals for data sharing in food systems raise multiple ethical issues, for example safety and privacy. – Allergen tracking involves collating data from across the distributed food system, has serious life or death ramifications, and may involve special categories of data such as health information relating to individuals with associated privacy concerns. The potential system envisaged using data from a variety of food chain stakeholders to provide information on allergen content for food products at different stages of the production and distribution chain. It was intended that this system would utilize automation and AI technology, such as machine learning algorithms to facilitate the sharing of data. 3    \f117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 4. Worldbuilding and Design Fiction Development Subsequent meetings of the working group were convened to build a world and develop design fictions in relation to this allergen tracking scenario. Design Fiction is a research methodology that aims to create space for discussion around possible futures, through ",
            {
                "entities": [
                    [
                        105,
                        118,
                        "AUTHOR"
                    ],
                    [
                        120,
                        133,
                        "AUTHOR"
                    ],
                    [
                        134,
                        146,
                        "AUTHOR"
                    ],
                    [
                        148,
                        165,
                        "AUTHOR"
                    ],
                    [
                        167,
                        180,
                        "AUTHOR"
                    ],
                    [
                        181,
                        196,
                        "AUTHOR"
                    ],
                    [
                        198,
                        213,
                        "AUTHOR"
                    ],
                    [
                        215,
                        229,
                        "AUTHOR"
                    ],
                    [
                        231,
                        247,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Journal of Strategic Information Systems 29 (2020) 101600Contents lists available at ScienceDirectJournal of Strategic Information Systemsjournal homepage: www.elsevier.com/locate/jsisThe strategic impacts of Intelligent Automation for knowledge andservice work: An interdisciplinary reviewCrispin Coombsa,⁎a School of Business and Economics, Loughborough University, Ashby Road, Loughborough LE11 3TU, UKb Business School, University of Aberdeen, Aberdeen AB24 5UA, UKc Faculty of Social Sciences, University of Nottingham, University Park, Nottingham NG7 2RD, UK, Donald Hislopb, Stanimira K. Tanevac, Sarah BarnardaTA R T I C L E I N F OA B S T R A C TKeywords:Artificial intelligenceAutomationBusiness valueComputerisationMachine learningMobile roboticsIntroductionA significant recent technological development concerns the automation of knowledge and ser-vice work as a result of advances in Artificial Intelligence (AI) and its sub-fields. We use the termIntelligent Automation to describe this phenomenon. This development presents organisationswith a new strategic opportunity to increase business value. However, academic research con-tributions that examine these developments are spread across a wide range of scholarly dis-ciplines resulting in a lack of consensus regarding key findings and implications. We conduct thefirst interdisciplinary literature review that systematically characterises the intellectual state anddevelopment of Intelligent Automation technologies in the knowledge and service sectors. Basedon this review, we provide three significant contributions. First, we conceptualise IntelligentAutomation and its associated technologies. Second, we provide a business value-based model ofIntelligent Automation for knowledge and service work and identify twelve research gaps thathinder a complete understanding of the business value realisation process. Third, we provide aresearch agenda to address these gaps.Analysts and commentators have forecast mass unemployment from the automation of a wide range of job roles that involvepredictable, repetitive work (Grace et al., 2018; Makridakis, 2017). The McKinsey Global Institute has claimed that 60% of jobs couldbecome 30% automated by the early 2020s (Chui et al., 2016), while Frey and Osborne (2017) argue that automation could eliminate47% of jobs in the United States economy by 2033. Researchers also predict that Artificial Intelligence (AI) will outperform humans inmany activities in the next ten years (Grace et al., 2018), thereby becoming a practical alternative to human labour (Makridakis,2017). These claims are based on a recent step change in the technological advance of AI. AI is the broad suite of technologies thatcan match or surpass human capabilities, particularly those involving cognition such as learning and problem solving (DeCanio,2016). Applications of AI are wide-ranging and include knowledge reasoning, machine learning, natural language processing,computer vision, and robotics. For clarity, we use the term AI to refer to all these technologies.Advances in AI and its sub-fields have enabled the development of a new form of automation that we describe as IntelligentAutomation1 (the application of AI in ways that can learn, adapt and improve over time to automate tasks that were formally⁎Corresponding author.E-mail addresses: c.r.coombs@lboro.ac.uk (C. Coombs), donald.hislop@abdn.ac.uk (D. Hislop),Stanimira.Taneva@nottingham.ac.uk (S.K. Taneva), s.h.barnard@lboro.ac.uk (S. Barnard).1 We use the term Intelligent Automation throughout this paper, rather than the abbreviation IA, to avoid confusion with AI.https://doi.org/10.1016/j.jsis.2020.101600Received 1 July 2017; Received in revised form 29 January 2020; Accepted 3 February 2020Available online 09 March 20200963-8687/ © 2020 The Authors. Published by Elsevier B.V.\fC. Coombs, et al.Journal of Strategic Information Systems 29 (2020) 101600undertaken by a human). Frey and Osborne (2017) observe that algorithms are being developed that would allow cognitive tasks tobe automated. They also state that the application of AI in mobile robotics has extended the opportunity for automation of manualtasks. Cognitive and manual tasks are commonly found in knowledge and service work (Davenport and Kirby, 2016a). Knowledgework is defined as work which is intellectual, creative, and non-routine, and which involves the utilisation and creation of knowledge(Hislop et al., 2018). Knowledge work includes work in a wide range of professional areas, such as information and communication,consulting, pharmacology, and education (Kuusisto and Meyer, 2003). Service work can be defined as the process of using one'sresources (e.g., knowledge) for someone's (self or other) benefit (Barrett et al., 2015). It includes jobs as diverse as working in retail,security, office cleaning, and more knowledge-intensive work such as consulting. Our definition of service work thus includes (white-collar) office and administrative work.Until recently, knowledge and service work tasks have been considered too difficult to automate because they require a highdegree of cognitive flexibility and physical adaptability (Lacity and Willcocks, 2016b). However, the scope and capability of AI hasrecently expanded and is likely to continue to grow (Brynjolfsson and McAfee, 2016). For example, applications of AI are predicted tosignificantly reduce the need for humans to translate languages (by 2024), drive a truck (by 2027), work in retail (by 2031), and workas surgeons (by 2053) (Grace et al., 2018). Frey and Osborne (2017) predict that most office and administrative support work, as wellas a substantial proportion of service work in the US, is likely to be automated. In which case, the advance of AI will create dramaticchanges to the supply of knowledge and service work (Loebbecke and Picot, 2015). It is this impact on knowledge and service workthat sets this change apart from previous technological revolutions, such as the industrialisation of factory work in the 19th century,or the adoption of transactional computers for administrative and service work in the late 20th century (Davenport and Kirby,2016b). This review focuses on knowledge and service work to examine the transformational effects of Intelligent Automation insectors that have previously been relatively untouched by automation compared to other industries, such as manufacturing(Brynjolfsson and McAfee, 2011).The transformation of knowledge and service work presents organisations with a new strategic opportunity to increase businessvalue. Recent advances in AI could enable organisations to create new business value opportunities through the application ofIntelligent Automation to middle-income cognitive jobs (Manyika et al., 2017). Alternatively, organisations may opt to substitute newAI capital for high-skilled labour or choose to reassign high-skilled workers to focus more exclusively on the most complex, non-routine cognitive tasks (Davenport and Kirby, 2016a). However, there is considerable disagreement regarding the possible impacts ofAI on knowledge and service work. Makridakis (2017) identified four contrasting perspectives: optimists that predict a utopian futureof AI (e.g., Kurzweil, 2005); pessimists that predict a dystopian future where AI reduces humans to a second rate status (e.g., Bostrom,2014); pragmatists that predict AI will augment human skills (e.g., Markoff, 2016); and doubters that predict that AI will never beable to replicate human intelligence (e.g., Jankel, 2015). This lack of consensus means that there is little coherent guidance regardingthe new strategies that need to be developed to realise business value from Intelligent Automation. Thus, there is a pressing need forresearch that examines the latest advances in AI and considers their impact on the application of Intelligent Automation for businessvalue.A valuable source of guidance for strategic perspectives on Intelligent Automation is current academic knowledge. Numerousstudies, many employing sound rigorous methods, consider the potential impacts of AI on work. However, these contributions aresituated in a wide range of scholarly disciplines that draw on contrasting research paradigms, theories, methods, and perspectives,resulting in a lack of consensus regarding critical findings and implications. Operating at the intersection of many scholarly dis-ciplines, considering both social and technical aspects, IS researchers are well placed to assemble a cohesive understanding of thisemerging research challenge. Thus, this paper aims to inform researchers of the current state (state of the art) of research relating tothe application of Intelligent Automation for knowledge and service work.To assist the IS research community in navigating this complex domain, this paper provides a scoping review of existing academicliterature (Paré et al., 2015). Scoping reviews focus on breadth rather than depth of coverage in the literature. They describe andsummarise the size and nature of the literature on a particular topic and allow researchers to identify research gaps in the extantliterature (see, for example, Smith et al., 2011). The advantage of this approach is to offer a comprehensive view of the researchlandscape. This review explores the potential impacts of Intelligent Automation through the classification of AI research related toknowledge and service work published between January 2011 and December 2017. We focused our review on knowledge and servicework for two reasons. First, the most significant developments associated with the work-related use of AI have been in occupationsthat have hitherto made little use of them, such as the knowledge and service industries (Brynjolfsson and McAfee, 2011; Loebbeckeand Picot, 2015). Second, the late 20th century and the start of this century has witnessed a significant growth of employment inknowledge and service work, and a decline in jobs in manufacturing sectors in",
            {
                "entities": [
                    [
                        565,
                        579,
                        "AUTHOR"
                    ],
                    [
                        603,
                        617,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect International Journal of Information Management journal homepage: www.elsevier.com/locate/ijinfomgt “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy☆ Yogesh K. Dwivedi a, b, *, Nir Kshetri c, Laurie Hughes a, Emma Louise Slade d, Anand Jeyaraj e, Arpan Kumar Kar f, g, Abdullah M. Baabdullah h, Alex Koohang i, Vishnupriya Raghavan j, Manju Ahuja k,1, Hanaa Albanna l, 1, Mousa Ahmad Albashrawi m,1, Adil S. Al-Busaidi n,o, 1, Janarthanan Balakrishnan p, 1, Yves Barlette q, 1, Sriparna Basu r, 1, Indranil Bose s, 1, Laurence Brooks t, 1, Dimitrios Buhalis u,1, Lemuria Carter v,1, Soumyadeb Chowdhury w, 1, Tom Crick x, 1, Scott W. Cunningham y, 1, Gareth H. Davies z, 1, Robert M. Davison aa, 1, Rahul D´e ab,1, Denis Dennehy a, Yanqing Duan ac,1, Rameshwar Dubey ad,ae,1, Rohita Dwivedi af,1, John S. Edwards ag,1, Carlos Flavi´an ah, 1, Robin Gauld ai, 1, Varun Grover aj, 1, Mei-Chih Hu ak,1, Marijn Janssen al, 1, Paul Jones am, 1, Iris Junglas an,1, Sangeeta Khorana ao,1, Sascha Kraus ap, 1, Kai R. Larsen aq, 1, Paul Latreille ar,1, Sven Laumer as,1, F. Tegwen Malik at,1, Abbas Mardani au,1, Marcello Mariani av,aw,1, Sunil Mithas ax,1, Emmanuel Mogaji ay, 1, Jeretta Horn Nord az,1, Siobhan O’Connor ba,1, Fevzi Okumus bb, bc,1, Margherita Pagani bd, 1, Neeraj Pandey be,1, Savvas Papagiannidis bf, 1, Ilias O. Pappas bg, bh,1, Nishith Pathak bi,1, Jan Pries-Heje bj, 1, Ramakrishnan Raman bk,1, Nripendra P. Rana bl, 1, Sven-Volker Rehm bm,1, Samuel Ribeiro-Navarrete bn, 1, Alexander Richter bo,1, Frantz Rowe bp,1, Suprateek Sarker bq, 1, Bernd Carsten Stahl br,1, Manoj Kumar Tiwari be, 1, Wil van der Aalst bs,1, Viswanath Venkatesh bt,1, Giampaolo Viglia bu,bv,1, Michael Wade bw,1, Paul Walton bx,1, Jochen Wirtz by,1, Ryan Wright bq, 1 a Digital Futures for Sustainable Business & Society Research Group, School of Management, Swansea University, Bay Campus, Fabian Bay, Swansea, Wales, UK b Department of Management, Symbiosis Institute of Business Management, Pune & Symbiosis International (Deemed University), Pune, Maharashtra, India c Bryan School of Business and Economics, University of North Carolina at Greensboro, USA d University of Bristol Business School, University of Bristol, BS8 1SD, UK e Professor of Information Systems, Raj Soin College of Business, Wright State University, 3640 Colonel Glenn Highway, Dayton, OH 45435, USA f School of Artificial Intelligence, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India g Department of Management Studies, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India h Department of Management Information Systems, Faculty of Economics and Administration, King Abdulaziz University, Jeddah, Saudi Arabia i School of Computing, Middle Georgia State University, Macon, GA, USA j Client Advisory and Transformation, Stackroute, NIIT Limited, India k Department of Information Systems, Analytics and Operations, College of Business, University of Louisville, USA l Northumbria University London, UK m IRC-FDE, KFUPM, Saudi Arabia, ISOM, KFUPM Business School, KFUPM, Saudi Arabia n Innovation and Technology Transfer Center, Sultan Qaboos University, Oman o Department of Business Communication, Sultan Qaboos University, Oman p Department of Management Studies, National Institute of Technology, Tiruchirappalli, India ☆This editorial opinion paper provides a subjective viewpoint on the potential impact of generative AI technologies such as ChatGPT in the domains of education, business, and society. Its objective is to offer initial guidance on the opportunities, challenges, and implications associated with these technologies. It is worth noting that, given its nature as an editorial opinion piece, this submission has not undergone a formal double-blind review process but has been reviewed informally by appropriate experts. * Corresponding author at: Digital Futures for Sustainable Business & Society Research Group, School of Management, Swansea University, Bay Campus, Fabian Bay, Swansea, Wales, UK. E-mail address: y.k.dwivedi@swansea.ac.uk (Y.K. Dwivedi). https://doi.org/10.1016/j.ijinfomgt.2023.102642 Received 1 March 2023; Accepted 1 March 2023  InternationalJournalofInformationManagement71(2023)102642Availableonline11March20230268-4012/©2023TheAuthor(s).PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).\fY.K. Dwivedi et al.                                                                                                               q Montpellier Business School (MBS), Montpellier, France r FORE School of Management, New Delhi, India s Indian Institute of Management Ahmedabad, Vastrapur, Ahmedabad 380015, India t Information School, University of Sheffield, UK u Bournemouth University Business School, Poole, UK v School of Information Systems and Technology Management, University of New South Wales, Sydney, Australia w Information, Operations and Management Sciences Department, TBS Business School, 1 Place Alphonse Jourdain, 31068 Toulouse, France x Department of Education & Childhood Studies, Swansea University, Swansea, United Kingdom y Faculty of Humanities and Social Sciences, University of Strathclyde, Glasgow G1 1XQ, United Kingdom z School of Management, Swansea University, Swansea, UK aa Department of Information Systems, City University of Hong Kong, Hong Kong Special Administrative Region ab Indian Institute of Management Bangalore, India ac Business and Management Research Institute, University of Bedfordshire, UK ad Montpellier Business School, Montpellier, France ae Liverpool Business School, Liverpool John Moores University, UK af Prin. L. N. Welingkar Institute of Management Development and Research, Mumbai, India ag Operations & Information Management Department, Aston Business School, UK ah Department of Marketing and Marketing Management, Faculty of Economics and Business, University of Zaragoza, Zaragoza, Spain ai Otago Business School, Co-Director, Centre for Health Systems and Technology, University of Otago, Dunedin, New Zealand aj Distinguished Professor and George & Boyce Billingsley Endowed Chair of Information Systems, IS Doctoral Program, Walton College of Business, University of Arkansas, Room 216, Fayetteville, AR 72703, USA ak Institute of Technology Management, National Tsing Hua University, Hsinchu 300, Taiwan al Faculty of Technology, Policy and Management, Delft University of Technology, the Netherlands am School of Management, Swansea University, United Kingdom an College of Charleston, School of Business, USA ao Bournemouth University Business School, Bournemouth University, UK ap Free University of Bozen-Bolzano, Italy & University of Johannesburg, South Africa aq Leeds School of Business, Boulder, University of Colorado, Boulder, USA ar Sheffield University Management School, The University of Sheffield, UK as Sch¨oller-Endowed Chair of Information Systems, Institute of Information Systems Nürnberg, School of Business, Economics and Society, Friedrich-Alexander University Erlangen-Nuremberg, Germany at School of Management, Swansea University Bay Campus, Swansea, SA1 8EN Wales, UK au Business School, Worcester Polytechnic Institute, Worcester, MA 01609-2280, USA av Henley Business School, University of Reading, Henley-on-Thames, Oxfordshire, UK aw Department of Management, University of Bologna, Bologna, Italy ax School of Information Systems and Management, University of South Florida, Tampa, FL, USA ay Greenwich Business School, University of Greenwich, London SE10 9LS, UK az Management Science and Information Systems, Spears School of Business, Oklahoma State University, Stillwater, OK 74078, USA ba Division of Nursing, Midwifery and Social Work, School of Health Sciences, The University of Manchester, Manchester, United Kingdom bb Rosen College of Hospitality Management University of Central Florida 9907 Universal Boulevard, Orlando, FL 32819, USA bc Department of Business, WSB University, Wrocław, Poland bd SKEMA Research Center for Artificial Intelligence, SKEMA Business School, 5 quai Marcel Dassault – Suresnes, France be National Institute of Industrial Engineering (NITIE), Mumbai, India bf Newcastle University Business School, Newcastle upon Tyne, United Kingdom bg Department of Information Systems, University of Agder, Norway bh Department of Computer Science, Norwegian University of Science and Technology, Norway bi Microsoft AI MVP and Microsoft Regional DirectorGlobal lead - Innovation and Architecture at DXC Technologies India bj Department of People and Technology, Roskilde University, Denmark bk Symbiosis Institute of Business Management, Pune & Symbiosis International (Deemed University), Pune, India bl Department of Management and Marketing, College of Business and Economics, Qatar University, P.O. Box 2713, Doha, Qatar bm HuManiS Research Center – Humans and Management in Society, UR 7308, Universit´e de Strasbourg – EM Strasbourg Business School, France bn ESIC University, Madrid, Spain and University of Economics and Human Sciences, Warsaw, Poland bo Wellington School of Business and Government, Rutherford House, 23 Lambton Quay, Wellington, New Zealand bp Nantes University, LEMNA, and SKEMA Business School, France bq Rolls-Royce Commonwealth Commerce, McIntire School of Commerce, University of Virginia, USA br School of Computer Science, The University of Nottingham, UK bs Process and Data Science, RWTH Aachen University, Ahornstraße 55, Aachen 52074, North Rhine-Westphalia, Germany bt Eminent Scholar and Verizon Chair, Director of Executive PhD in Business, Pamplin College of Business, Virginia Tech, Blacksburg, Virginia, USA bu University of Portsmouth, Department of Strategy, Marketing and Innovation, Richmond Building, Portsmouth, United Kingdom bv Department of Eco",
            {
                "entities": [
                    [
                        343,
                        355,
                        "AUTHOR"
                    ],
                    [
                        358,
                        372,
                        "AUTHOR"
                    ],
                    [
                        375,
                        393,
                        "AUTHOR"
                    ],
                    [
                        396,
                        410,
                        "AUTHOR"
                    ],
                    [
                        413,
                        429,
                        "AUTHOR"
                    ],
                    [
                        461,
                        474,
                        "AUTHOR"
                    ],
                    [
                        477,
                        498,
                        "AUTHOR"
                    ],
                    [
                        501,
                        513,
                        "AUTHOR"
                    ],
                    [
                        518,
                        532,
                        "AUTHOR"
                    ],
                    [
                        538,
                        561,
                        "AUTHOR"
                    ],
                    [
                        593,
                        618,
                        "AUTHOR"
                    ],
                    [
                        624,
                        638,
                        "AUTHOR"
                    ],
                    [
                        644,
                        658,
                        "AUTHOR"
                    ],
                    [
                        664,
                        678,
                        "AUTHOR"
                    ],
                    [
                        684,
                        700,
                        "AUTHOR"
                    ],
                    [
                        706,
                        724,
                        "AUTHOR"
                    ],
                    [
                        729,
                        744,
                        "AUTHOR"
                    ],
                    [
                        749,
                        769,
                        "AUTHOR"
                    ],
                    [
                        775,
                        785,
                        "AUTHOR"
                    ],
                    [
                        881,
                        895,
                        "AUTHOR"
                    ],
                    [
                        898,
                        911,
                        "AUTHOR"
                    ],
                    [
                        917,
                        933,
                        "AUTHOR"
                    ],
                    [
                        942,
                        957,
                        "AUTHOR"
                    ],
                    [
                        1008,
                        1020,
                        "AUTHOR"
                    ],
                    [
                        1027,
                        1040,
                        "AUTHOR"
                    ],
                    [
                        1047,
                        1059,
                        "AUTHOR"
                    ],
                    [
                        1065,
                        1080,
                        "AUTHOR"
                    ],
                    [
                        1087,
                        1098,
                        "AUTHOR"
                    ],
                    [
                        1105,
                        1118,
                        "AUTHOR"
                    ],
                    [
                        1124,
                        1141,
                        "AUTHOR"
                    ],
                    [
                        1147,
                        1160,
                        "AUTHOR"
                    ],
                    [
                        1188,
                        1203,
                        "AUTHOR"
                    ],
                    [
                        1209,
                        1221,
                        "AUTHOR"
                    ],
                    [
                        1249,
                        1263,
                        "AUTHOR"
                    ],
                    [
                        1269,
                        1286,
                        "AUTHOR"
                    ],
                    [
                        1295,
                        1308,
                        "AUTHOR"
                    ],
                    [
                        1314,
                        1330,
                        "AUTHOR"
                    ],
                    [
                        1337,
                        1355,
                        "AUTHOR"
                    ],
                    [
                        1384,
                        1397,
                        "AUTHOR"
                    ],
                    [
                        1407,
                        1425,
                        "AUTHOR"
                    ],
                    [
                        1432,
                        1446,
                        "AUTHOR"
                    ],
                    [
                        1452,
                        1473,
                        "AUTHOR"
                    ],
                    [
                        1506,
                        1521,
                        "AUTHOR"
                    ],
                    [
                        1527,
                        1542,
                        "AUTHOR"
                    ],
                    [
                        1549,
                        1568,
                        "AUTHOR"
                    ],
                    [
                        1599,
                        1616,
                        "AUTHOR"
                    ],
                    [
                        1622,
                        1647,
                        "AUTHOR"
                    ],
                    [
                        1654,
                        1672,
                        "AUTHOR"
                    ],
                    [
                        1678,
                        1690,
                        "AUTHOR"
                    ],
                    [
                        1696,
                        1713,
                        "AUTHOR"
                    ],
                    [
                        1720,
                        1740,
                        "AUTHOR"
                    ],
                    [
                        1746,
                        1765,
                        "AUTHOR"
                    ],
                    [
                        1772,
                        1790,
                        "AUTHOR"
                    ],
                    [
                        1796,
                        1816,
                        "AUTHOR"
                    ],
                    [
                        1822,
                        1839,
                        "AUTHOR"
                    ],
                    [
                        1848,
                        1861,
                        "AUTHOR"
                    ],
                    [
                        1867,
                        1879,
                        "AUTHOR"
                    ],
                    [
                        1885,
                        1898,
                        "AUTHOR"
                    ],
                    [
                        1904,
                        1916,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   \fEuropean Journal of Operational Research 291 (2021) 906–917 Contents lists available at ScienceDirect European Journal of Operational Research journal homepage: www.elsevier.com/locate/ejor The responsibility of social media in times of societal and political manipulation Ulrike Reisach Department of Information Management, Prof. Dr. Ulrike Reisach, Neu-Ulm University of Applied Sciences, Wiley-Street 1, D-89231 Neu-Ulm, Germany a r t i c l e i n f o a b s t r a c t Article history: Received 17 December 2019 Accepted 16 September 2020 Available online 22 September 2020 Keywords: Ethics in OR Decision-making Artificial intelligence Behavioural OR Education The way electorates were influenced to vote for the Brexit referendum, and in presidential elections both in Brazil and the USA, has accelerated a debate about whether and how machine learning techniques can influence citizens’ decisions. The access to balanced information is endangered if digital political ma- nipulation can influence voters. The techniques of profiling and targeting on social media platforms can be used for advertising as well as for propaganda: Through tracking of a person’s online behaviour, al- gorithms of social media platforms can create profiles of users. These can be used for the provision of recommendations or pieces of information to specific target groups. As a result, propaganda and dis- information can influence the opinions and (election) decisions of voters much more powerfully than previously. In order to counter disinformation and societal polarization, the paper proposes a responsibility-based approach for social media platforms in diverse political contexts. Based on the implementation require- ments of the “Ethics Guidelines for Trustworthy Artificial Intelligence” of the European Commission, the eth- ical principles will be operationalized, as far as they are directly relevant for the safeguarding of demo- cratic societies. The resulting suggestions show how the social media platform providers can minimize risks for societies through responsible action in the fields of human rights, education and transparency of algorithmic decisions. © 2020 The Author. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) 1. Introduction and research methodology 1.1. Aims and research question During the Corona-crisis in 2020, the degree of disinformation has reached a level which could endanger the proper functioning of democratic decision-making. Crises have always been a time of rising emotions and anxiety. These seem to culminate on social media platforms, where citizens and self-proclaimed experts give unsubstantiated advice dealing with Covid-19, or try to identify as- sumingly guilty parties and fabricate conspiracy theories, through amalgamating facts and false interpretations. The more exciting, the even more weird ideas that are shared, including vaccine anx- iety, and doubtful, or even potentially lethal health recipes. In the United States, these developments have fuelled the long- standing debate on whether misrepresentations and recommen- dations still fall under the freedom of speech, or should be ac- companied, e.g. with a fact check advice, or should be filtered out and deleted. The European Commission (EC) aims at combat- ing disinformation and appeals to the social media platforms to install a transparent and consistent moderation of disinformation ( EC, 2020a , 2020b ). Bell’s observation ( 2018 ) that “… techniques for fabricating, editing, and reframing news in harmful ways develop faster than they can be detected and countered …” describes the current situ- ation (p. 5). C. West Churchman asks “which end results are good in an objective sense?” ( Churchman, 1970 ). This question is ap- plies to the current issues of social media platforms: The paper asks whether and how social media platforms can (practically) and should (ethically), deal with risks of societal and political manipu- lation. The EC’s Action Plan on Disinformation ( 2019 ) defines disinfor- mation as is verifiably false or misleading information created, pre- sented and disseminated for economic gain, or to intentionally de- ceive the public. The reasons given for their action are: (a) the potential for far-reaching consequences such as public harm, (b) threats to democratic political and policy-making processes, (c) the risk of endangering the protection of EU citizens’ health, E-mail address: ulrike.reisach@hnu.de security and their environment. https://doi.org/10.1016/j.ejor.2020.09.020 0377-2217/© 2020 The Author. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) \fU. Reisach European Journal of Operational Research 291 (2021) 906–917 This research focuses on the threats to democratic decision- making processes. Based on the developments in 2019 and 2020, the EC (2020a) expresses concerns which relate to the main rea- sons for the research: “Disinformation erodes trust in institutions and in digital and traditional media and harms our democracies by hampering the ability of citizens to take informed decisions. It can polarise debates, create or deepen tensions in society and un- dermine electoral systems, and have a wider impact on European security. It impairs freedom of opinion and expression, a funda- mental right enshrined in the Charter of Fundamental Rights of the European Union.” This paper analyses how disinformation endan- gers democratic decision-making and how social media platforms could contribute to tackling those challenges. 1.2. Research methodology Reflecting on the reasons and impacts of disinformation on so- cial media, a hermeneutic process of understanding and interpret- ing the ethical and societal consequences has been chosen. What can be expected is a set of ethically grounded suggestions for ac- tions which could be discussed and implemented by the platform owners. Hermeneutics confronts “quantitative methodologies with qualitative questions” ( van Dijck, 2014 : p. 206). The tools for this process are rooted in the humanities and in social science. Critical reflectivity ( Gregory, 20 0 0 ) is one such method that could be ap- plied to a topic that raises societal and political questions. The re- sults cannot resolve the existing problems of political manipulation completely, due to societal complexity ( DeTombe, 2002 ), associated with almost ubiquitous social media. The philosopher Wilhelm Dilthey (1833–1911) established hermeneutics in the humanities as interpretative sciences in their own right. He studied the rela- tionships between personal experience, its realization in creative expression, and the reflective understanding of this experience; and, finally, the logical development from these to the understand- ing of social groups and historical processes ( Dilthey, 2013 ). Using the concept of hermeneutics and reflective understand- ing, the operation and impacts of social media with regard to the current societal trends are discussed in Part 2. Examples for ma- nipulation are given in Part 3. Concepts of digital media ethics and responsibility are presented in Part 4, and compared with the tra- ditional media’s accountability approaches. Ethics codes for AI are shown and suggestions for responsible action of social media are provided. Based on those, a reflective and strategic corporate re- sponsibility of digital media is introduced as a new concept. Part 5 gives some reflections and limitations. Finally, the conclusion in part 6 offers a global outlook on societal responsibilities. 2. The role of social media platforms and their impacts on societies To explain why societal and political manipulation is a spe- cial issue for social media, the goals, functioning and legal sta- tus of the respective corporate actors need to be clarified. Social media platforms such as Facebook, Twitter, Instagram, YouTube and TikTok facilitate an interactive one-to-few or many-to-many- communication in an international scale. “In the web 2.0 era, an infinite “crowd” of users can anonymously and with almost no cost “voice” criticism and protest, via Twitter and Facebook” ( Fengler, 2012 , p. 184). 2.1. The business model of social media platforms In their official prospectus for the computerized US stock mar- ket NASDAQ, Facebook claimed their vision was “t o make the world more open and connected ” ( Facebook, 2012 ). Nevertheless, their business model is commercial. When going public they disclaim that their goal is profit and shareholder value: “Advertisers can en- gage with users … on Facebook or subsets of our users based on in- formation they have chosen to share with us such as their age, lo- cation, gender, or interests. We offer advertisers a unique combina- tion of reach, relevance, social context, and engagement to enhance the value of their ads.” ( Facebook, 2012 ). Users do not pay but give their data to Facebook who then sells them to their customers, the advertisers. Dwyer and Martin (2017) explain that data cap- ture, data-mining and behavioural advertising are typical activi- t",
            {
                "entities": [
                    [
                        1049,
                        1064,
                        "AUTHOR"
                    ],
                    [
                        1112,
                        1127,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ELSEVIER Journal of Pragmatics 34 (2002) 227-258 www.elsevier.com/locate/pragma Pragmatics in human-computer conversations* Ayse Pinar Saygin”, Ilyas Ciceklib,* * Dept. of Cognitive Science, Univ. of California, San Diego, La Jolla, CA 92093-0515, USA h Dept. of Computer Engineering, Bilkent University, 06533 Bilkent, Ankara, Turkey Received 2 December 1999; revised version 5 May 2001 Abstract in which computers participate This paper provides a pragmatic analysis of some human-computer conversations carried out during the past six years within the context of the Loebner Prize Contest, an annual com- in Turing Tests. The Turing Test posits that to be petition granted intelligence, a computer should imitate human conversational behavior so well as to be indistinguishable from a real human being. We carried out an empirical study exploring the relationship between computers’ violations of Grice’s cooperative principle and conver- sational maxims, and their success in imitating human language use. Based on conversation analysis and a large survey, we found that different maxims have different effects when vio- lated, but more often than not, when computers violate the maxims, they reveal their identity. is at work during conversations with The results indicate that Grice’s cooperative principle computers. On the other hand, studying human-computer communication may require some in pragmatics because of certain characteristics of these modifications of existing frameworks conversational environments. Pragmatics constitutes a serious challenge to computational lin- guistics. While existing programs have other significant it may be that the biggest hurdle in developing computer programs which can successfully carry out conversa- tions will be modeling the ability to ‘cooperate’. 0 2002 Elsevier Science B.V. All rights reserved. shortcomings, Keywords: tional linguistics; Maximization principle; Natural language processing; Pragmatics Cooperative principle; Turing test; Human-computer conversation; Computa- * This work was carried out while the first author was an MS. student at Bilkent University. We would like to thank Stephen Wilson, Bilge Say, and David Davenport for reading and commenting on earlier versions of this work. We are also indebted to Hulya Saygin, Giray Uraz, and Emel Aydin for their help in conducting the surveys. * E-mail: saygin@crl.ucsd.edu; ilyas@cs.bilkent.edu.tr 0378-2166/02/$ - see front matter 0 2002 Elsevier Science B.V. All rights reserved. PII: SO378-2166(01)00035-2 \f228 A.P. Saygin, I. Cicekli I Journal of Pragmatics 34 (2002) 227-258 1. Introduction The Imitation Game (IG), better known as the Turing Test (IT), was introduced intel- in 1950 by Alan Turing as a means to detect whether a computer possesses ligence. Turing believed that a way to objectively assess machine mentality was needed, for he thought the question ‘Can machines think?’ was too ambiguous. He attempted this question into a more concrete form: the IG is played with a man to transform (C) whose gender is unimportant. The inter- (A), a woman (B), and an interrogator rogator stays in a room apart from A and B. The objective of the interrogator is to determine which of the other two is the woman while the objective of both the man that he/she is the woman and the other and the woman is to convince the interrogator is not. The players communicate thus in written nat- topics can be on any subject imaginable, from mathe- ural language. Conversation matics to poetry, from the weather to chess. through a teletype connection, According to Turing, the new question to be discussed, instead of the equivocal ‘Can machines think? ‘, can be ‘What will happen when a machine takes the part of A in this game? Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?’ ‘Can in the paper, however, Turing At a later point the question replaces machines think? ’ by the following: “Let us fix our attention to one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?” (Turing, 1950: 442, emphasis added). Notice that the woman has disappeared altogether. But the objectives of A, B and the interrogator state any change. In this version, a man and a computer program are playing the game and try- ing to convince the judge that they are women. at least Turing does not explicitly remain unaltered; As it is now generally understood, what the ‘IT tries to assess is the machine’s ability to imitate a human being, rather than its ability to simulate a woman. Most subsequent work on the TT ignores the gender issue and assumes that the game is played between a machine (A), a human (B) and an interrogator (C). In this version, C’s aim is to determine which one of the two entities he/she is conversing with is the human. Although it is not clear why Turing introduces the gender-based IG, given that he was interested in whether or not machines can think, we have argued elsewhere that Turing’s original game constitutes a controlled experimental design (Saygin, 1999; Saygin et al., 2000). It provides a fair basis for comparison: the woman (either as a participant in the game or as a concept) acts as a neutral point so that the two play- ers can be assessed in how well they imitate something which they are not. Philoso- IG (see Piccinini, 2000; Sterrett, 2000; phers also commented on the gender-based Traiger, 2000, for a recent discussion). We will return to a discussion of the original gender-based game later. Unless noted otherwise, when talking about the TT, we \fA.P. Saygin, I. Cicekli I Journal of Pragmatics 34 (2002) 227-258 229 will be referring (human vs. machine), not gender. to the game in which the decision to be made is one of ‘species’ Much has been written on the TT, with many authors discussing its implications for artificial intelligence (AI). Most works attack or defend the validity of the test as to machines. There are many computational analyses, a means to grant intelligence an abundance of philosophical comments, and occasional remarks from other disci- plines such as psychology and sociology. A detailed survey of the ‘IT can be found in Saygin et al. (2000). The TT has never been carried out exactly as Turing described it. However, there are variants of the original IT in which computer programs participate and show their skills in ‘humanness’. Since 1991, Hugh Loebner has been organizing the so- called annual Loebner Prize Competition. Participating computer programs try to convince judges that they are human. One or more human confederates also partici- pate and try to aid the judges in identifying the humans. The judges also rank the participants with respect to their ‘human-ness’. Although no program has passed the ‘IT so far, the quality of participating programs seems to be increasing every year. The year 2000 marked the fiftieth year of the IT. While many conversation sys- tems, or ‘chatterbots’, have been developed, none exhibit human-like conversational behavior to the extent that they can pass the TT. We believe it is time we analyze some recent programs within the context of pragmatics and see how, at the turn of the millennium, computers are doing as conversational partners. We will not be con- is intelligent or with cerned with whether passing the test implies related theoretical issues. Here, we take official, real, human-computer conversations and use them in a study in which subjects were asked to read and make pragmatic judgments about them. We then analyze the results both in terms of human behavior in conversations with computers and in terms of better program design. the machine We focus on one particular aspect of conversation and attempt to explore it in relation to the ‘IT. This aspect is Grice’s cooperative principle (CP) and conversa- tional maxims. Just as Turing’s TT is a milestone in AI, Grice’s theory has been very in the field of pragmatics. The powerful juxtaposition of these two con- influential cepts is thus a significant component of this study. Pragmatics, in a nutshell, is con- cerned with language in use. The TT stipulates a criterion on machine intelligence based on the way computers use language. What could be more natural than the bringing together of these two concepts in analyzing human-computer communica- tion in natural language? We believe a pragmatic approach to the TT reveals a lot of important issues that are easy to miss otherwise. Through a pragmatic analysis, we can gain valuable insights on what it means to have a human-like conversation and In this what principles, implicitly or explicitly, guide human-computer conversation. paper, we study how humans behave in relation to the CP and the conversational maxims: we analyze human-computer the relation- ships between performance conversations and we quantify in ‘ITS and judgments of maxim violations. In this paper, IT transcripts are studied as exemplars of human-computer conversa- tion. We used a selected set of conversation excerpts from Loebner contest transcripts in a pair of questionnaires. Subjects were asked to read the excerpts and to make judgments on the computers’ language use, as well as to rate their IT-performance. \f230 A.P. Saygin, I. Cicekli I Journal of Pragmatics 34 (2002) 227-258 We sought correlations between computers’ maxim violations and their performance in TTs and found some reliable relationships. Violations of maxims often cause computers to give away their identity, therefore Grice’s framework seems to be at play during conversations with computers. On the other hand, we also observe some trends that would not be straightforwardly expected based on Grice’s theory, some- thing which indicates",
            {
                "entities": [
                    [
                        123,
                        141,
                        "AUTHOR"
                    ],
                    [
                        143,
                        157,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Machine learning fairness notions: Bridging the gapwith real-world applicationsKarima Makhlouf, Sami Zhioua, Catuscia PalamidessiTo cite this version:Karima Makhlouf, Sami Zhioua, Catuscia Palamidessi. Machine learning fairness notions: Bridg-Information Processing and Management, 2021, 58 (5),ing the gap with real-world applications.￿10.1016/j.ipm.2021.102642￿. ￿hal-03624025￿HAL Id: hal-03624025https://hal.science/hal-03624025Submitted on 13 Jun 2023HAL is a multi-disciplinary open accessarchive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come fromteaching and research institutions in France orabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL, estdestinée au dépôt et à la diffusion de documentsscientifiques de niveau recherche, publiés ou non,émanant des établissements d’enseignement et derecherche français ou étrangers, des laboratoirespublics ou privés.Distributed under a Creative Commons Attribution - NonCommercial| 4.0 InternationalLicense\fVersion of Record: https://www.sciencedirect.com/science/article/pii/S0306457321001321Manuscript_fc01a0c56074f6184ab81f45e475e037Machine Learning Fairness Notions:Bridging the Gap with Real-world ApplicationsKarima Makhloufa, Sami Zhiouab, Catuscia Palamidessic,∗aUniversité du Québec à Montréal, Québec, CanadabHigher Colleges of Technology, Dubai, UAEcInria, École Polytechnique, IPP, Paris, FranceAbstractFairness emerged as an important requirement to guarantee that Machine Learning (ML) predictivesystems do not discriminate against specific individuals or entire sub-populations, in particular,minorities. Given the inherent subjectivity of viewing the concept of fairness, several notions offairness have been introduced in the literature. This paper is a survey that illustrates the subtletiesbetween fairness notions through a large number of examples and scenarios. In addition, unlikeother surveys in the literature, it addresses the question of “which notion of fairness is most suitedto a given real-world scenario and why?”. Our attempt to answer this question consists in (1)identifying the set of fairness-related characteristics of the real-world scenario at hand, (2) analyzingthe behavior of each fairness notion, and then (3) fitting these two elements to recommend the mostsuitable fairness notion in every specific setup. The results are summarized in a decision diagramthat can be used by practitioners and policy makers to navigate the relatively large catalogue of MLfairness notions.Keywords: Fairness, Machine learning, Discrimination, Survey , Systemization of Knowledge (SoK)1. IntroductionDecisions in several domains are increasingly taken by “machines”. These machines try to takethe best decisions based on relevant historical data and using Machine Learning (ML) algorithms.∗Corresponding author.Email addresses: karima.makhlouf@courrier.uqam.ca (Karima Makhlouf), szhioua@hct.ac.ae (Sami Zhioua),catuscia@lix.polytechnique.fr (Catuscia Palamidessi)Preprint submitted to Journal of Information Processing and ManagementApril 9, 2021© 2021 published by Elsevier. This manuscript is made available under the CC BY NC user licensehttps://creativecommons.org/licenses/by-nc/4.0/\fOverall, ML-based decision-making (MLDM)1 is beneficial as it allows to take into considerationorders of magnitude more factors than humans do and hence outputting decisions that are more5informed and less subjective. However, in their quest to maximize efficiency, ML algorithms cansystemize discrimination against a specific group of population, typically, minorities. As an example,consider the automated candidates selection system of St. George Hospital Medical School [1, 2].The aim of the system was to help screening for the most promising candidates for medical studies.The automated system was built using records of manual screenings from previous years. Duringthose manual screening years, applications with grammatical mistakes and misspellings were rejectedby human evaluators as they indicate a poor level of English. As non-native English speakers aremore likely to send applications with grammatical and misspelling mistakes than native Englishspeakers do, the automated screening system built on that historical data ended up correlating race,birthplace, and address with a lower likelihood of acceptance. Later, while the overall English levelof non-native speakers improved, the race and ethnicity bias persisted in the system to the extentthat an excellent candidate may be rejected simply for her birthplace or address.Given that MLDM can have a significant impact in the lives and safety of human beings, it isno surprise that social and political organization are becoming very concerned with the possibleconsequences of biased MLDM, and the related issue of lack of explanation and interpretability ofML-based decisions. The European Union has been quite active in this respect. Already in theGeneral Data Protection Regulation (GDPR) there were directives concerning Automated DecisionMaking: for instance, Article 22 states that “The data subject shall have the right not to be subjectto a decision based solely on automated processing.” Other initiatives include the European Union’sEthics Guidelines for Trustworthy AI (April 2019), and OECD’s Council Recommendation onArtificial Intelligence (May 2019).In the scientific community, the issue of fairness in machine learning has become one of the mostpopular topics in recent years. The number of publications and conferences in this field has literallyexploded, and a huge number of different notions of fairness have been proposed, leading sometimesto possible confusion. This paper, like other surveys in the literature (cf. Section 1), attempts toclassify and systematize these notions. The characteristic of our work, however, consists in our point10152025301We focus on automated decision-making system supported by ML algorithms. In the rest of the paper we refer tosuch systems as MLDM.2\fof view, which is that the very reason for having different fairness notions is how suitable each oneof them is for specific real-world scenarios. We feel that none of the existing surveys has addressedthis aspect specifically. Discussion about the suitability (and sometimes the applicability) of thefairness notions is very limited and scattered through several papers [3, 4, 5, 6, 7, 8]. In this survey35paper we show that each MLDM system can be different based on a set of criteria such as: whetherthe ground-truth exists, difference in base-rates between sub-groups, the cost of misclassification,the existence of a government regulation that needs to be enforced, etc. We then revisit exhaustivelythe list of fairness notions and discuss the suitability and applicability of each one of them based onthe list of criteria.40Another set of results from the literature which is particularly related to the applicability problemwe are addressing in this paper is the tensions that exist between some definitions of fairness. Severalpapers in the literature provide formal proofs of the impossibility to satisfy several fairness definitionssimultaneously [3, 6, 8, 9, 10]. These results are revisited and summarized as they are related to theapplicability of fairness notions.45The results of this survey are finally summarized in a decision diagram that hopefully can helpresearchers, practitioners, and policy makers to identify the subtleties of the MLDM system at handand to choose the most appropriate fairness notion to use, or at least rule out notions that can leadto wrong fairness/discrimination result.50The paper is organized as follows. Section 3 lists notable real-world MLDMs where fairness iscritical. Section 4 identifies a set of fairness-related characteristics of MLDMs that will be used in thesubsequent sections to recommend and/or discourage the use of fairness notions. Fairness notions arelisted and described in the longest section of the survey, Section 5. Section 6 discusses relaxations ofthe strict definitions of fairness notions. Section 7 describes classification and tensions that existbetween some fairness notions. The decision diagram is provided and discussed in Section 8.552. Related Work and ScopeWith the increasing fairness concerns in the field of automated decision making and machinelearning, several survey papers have been published in the literature in the few previous years. Thissection revisits these survey papers and highlights how this proposed survey deviates from them.60In 2015, Zliobaite compiled a survey about fairness notions that have been introduced previ-ously [11]. He classified fairness notions into four categories, namely, statistical tests, absolute3\fmeasures, conditional measures, and structural measures. Statistical tests indicate only the presenceor absence of discrimination. Absolute and conditional measures quantify the extent of discriminationwith the difference that conditional measures consider legitimate explanations for the discrimination.6570These three categories correspond to the group fairness notions in this survey. Structural measurescorrespond to individual fairness notions2. Most of the fairness notions listed by Zliobaite arevariants of the group fairness notions in this survey. For instance, difference of means test (Section4.1.2 in [11]) is a variant of balance for positive class (Section 5.7 in this paper). Although, hededicated one category for individual notions (structural measures), Zliobaite did not mentionimportant notions, in particular fairness through awareness. Regarding the applicability of notions,the only criterion considered was the type of variables (e.g. binary, categorical, numerical, etc.).The survey of Berk et al. [12] listed only group fairness notions that are defined using theconfusion matrix. Similar to this survey, they used simple examples based on the confusion matrix tohighlight relationshi",
            {
                "entities": [
                    [
                        95,
                        107,
                        "AUTHOR"
                    ],
                    [
                        166,
                        178,
                        "AUTHOR"
                    ],
                    [
                        1305,
                        1317,
                        "AUTHOR"
                    ],
                    [
                        108,
                        129,
                        "AUTHOR"
                    ],
                    [
                        179,
                        200,
                        "AUTHOR"
                    ],
                    [
                        1319,
                        1340,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "UvA-DARE (Digital Academic Repository)In defense of offense: information security research under the right to sciencevan Daalen, O.DOI10.1016/j.clsr.2022.105706Publication date2022Document VersionFinal published versionPublished inComputer Law and Security ReviewLicenseArticle 25fa Dutch Copyright Act Article 25fa Dutch Copyright Act(https://www.openaccess.nl/en/in-the-netherlands/you-share-we-take-care)Link to publicationCitation for published version (APA):van Daalen, O. (2022). In defense of offense: information security research under the right toscience. Computer Law and Security Review, 46, [105706].https://doi.org/10.1016/j.clsr.2022.105706General rightsIt is not permitted to download or to forward/distribute the text or part of it without the consent of the author(s)and/or copyright holder(s), other than for strictly personal, individual use, unless the work is under an opencontent license (like Creative Commons).Disclaimer/Complaints regulationsIf you believe that digital publication of certain material infringes any of your rights or (privacy) interests, pleaselet the Library know, stating your reasons. In case of a legitimate complaint, the Library will make the materialinaccessible and/or remove it from the website. Please Ask the Library: https://uba.uva.nl/en/contact, or a letterto: Library of the University of Amsterdam, Secretariat, Singel 425, 1012 WP Amsterdam, The Netherlands. Youwill be contacted as soon as possible.Download date:04 jul. 2023UvA-DARE is a service provided by the library of the University of Amsterdam (https://dare.uva.nl)\fcomputer law & security review 46 (2022) 105706 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR In defense of offense: information security research under the right to science ✩ ∗Ot van Daalen University of Amsterdam a r t i c l e i n f o a b s t r a c t Keywords: Information security Coordinated vulnerability disclosure Right to science Communications freedom Duty to disclose Vulnerabilities Information security research Information security is something you do , not something you have . It’s a recurring process of finding weaknesses and fixing them, only for the next weakness to be discovered, and fixed, and so on. Yet, European Union rules in this field are not built around this cycle of making and breaking: doing offensive information security research is not always legal, and doubts about its legality can have a chilling effect. At the same time, the results of such research are sometimes not used to allow others to take defensive measures, but instead are used to attack. In this article, I review whether states have an obligation under the right to science and the right to communications freedom to develop governance which addresses these two issues. I first discuss the characteristics of this cycle of making and breaking. I then discuss the rules in the European Union with regard to this cycle. Then I discuss how the right to science and the right to communications freedom under the European Convention for Human Rights , the EU Charter of Fundamental Rights and the International Covenant on Economic, Social and Cultural Rights apply to this domain. I then conclude that states must recognise a right to research information security vulnerabilities, but that this right comes with a duty of researchers to disclose their findings in a way which strengthens information security. © 2022 Ot van Daalen. Published by Elsevier Ltd. All rights reserved. 1. Introduction Information security is something you do , not something you have . It’s a recurring process of finding weaknesses and fix- ing them, only for the next weakness to be discovered, and fixed, and so on. Yet, European Union rules in this field are not built around this cycle of making and breaking: doing of- fensive information security research is not always legal, and doubts about its legality can have a chilling effect. At the same time, the results of such research are sometimes not used to allow others to take defensive measures, but instead are used to attack. In this article, I review whether states have an obli- gation under the right to science and the right to communica- tions freedom to develop governance which addresses these two issues. I first discuss the characteristics of this cycle of making and breaking. I then discuss the rules in the Euro- pean Union with regard to this cycle. Then I discuss how the right to science and the right to communications freedom un- der the European Convention for Human Rights (the Conven- tion), the EU Charter of Fundamental Rights (the Charter) and the International Covenant on Economic, Social and Cultural Rights (the Covenant) apply to this domain. I then conclude that states must recognise a right to research information se- ✩ This work was supported by the Netherlands Organisation for Scientific Research (NWO/OCW), as part of the Quantum Software Con- sortium programme (project number 024.003.037/3368). It is based on a forthcoming PhD on information security, encryption, quantum computing and human rights by the author. The author would like to thank his PhD supervisors, Joris van Hoboken and Mireille van Eechoud, for their comments on earlier drafts. ∗ Corresponding author: Mr Ot van Daalen, Institute for Information Law, Netherlands E-mail address: o.l.vandaalen@uva.nl https://doi.org/10.1016/j.clsr.2022.105706 0267-3649/© 2022 Ot van Daalen. Published by Elsevier Ltd. All rights reserved. \f2 computer law & security review 46 (2022) 105706 curity vulnerabilities, but that this right comes with a duty of researchers to disclose their findings in a way which strength- ens information security. Information security as a cycle of making 2. and breaking In the literature, information security is often framed in terms of desirable security properties, such as confidentiality, in- tegrity and availability.2 And information security measures are intended to safeguard these properties against attacks. Many organisations will, at some point in their development, take these kind of information security measures. But it can be difficult to determine which measures make the most sense. Over the past decades, standard practices have emerged to help organisations make the best choices, even in the face of changing circumstances. These are generally subsumed un- der the plan-do-check-act cycle.3 In the first phase of the cycle, the plan -phase, an organisa- tion will decide which measures are necessary in view of the risks. These decisions are usually laid out in an information security policy. In the second phase, the do -phase, the organi- sation then implements these measures. That does not mean, of course, that these measures are always sufficient. That’s why information security policies need to be tested and re- viewed periodically – the check and act phases of the cycle. You periodically check whether the measures are commensurate with the risks, then adjust as needed. This approach reflects the reality of the continuous cycle of making and breaking. An important part of this cycle centres around the “vul- nerability” or weakness, in software or hardware. An attacker can exploit such a vulnerability to make a system act in a way which it is not supposed to do, or to be more precise: to violate a security policy . And while you might in theory be able to make software and hardware without vulnerabilities, in practice it’s virtually impossible. It is not doable to independently verify all components of a system.4 And even such verification only provides limited assurance that a component can actually be trusted.5 2 3 4 5 See Axel M. Arnbak, Securing Private Communications: Protecting Private Communications Security in EU Law: Fundamental Rights, Func- tional Value Chains, and Market Incentives (Kluwer Law International 2016) ch 5 for an in depth discussion of these concepts; and A. J. Menezes, Paul C. Van Oorschot and Scott A. Vanstone, Handbook of Applied Cryptography (CRC Press 1997) 4 for the definition of the first two. See for example Regulation (EU) 2016/679 of the European Par- liament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (2016 OJ L 119/1), Art. 32(1)(d). Edlyn V. Levine, “The Die Is Cast: Hardware Security Is Not As- sured” (2020) 18 ACMqueue. See Ken Thompson, “Reflections on Trusting Trust” (August) 1984 Communications of the ACM 761 for a principled argument; and Georg T. Becker and others, “Stealthy Dopant-Level Hardware Trojans: Extended Version” (2014) 4 Journal of Cryptographic Engi- neering 19 for a practical example. 2.1. Research and discovery As a result, many vulnerabilities are found in most widely used products after they are shipped, and even if they’re shipped without vulnerabilities, the deployment by users might create new weaknesses. One particular type of vulnera- bility is called the “zero day vulnerability”, or simply the “zero day”. It’s a weak spot for which no fix has been created yet, usually because the vendor doesn’t know of its existence. Of all vulnerabilities, zero days are the most coveted by attack- ers, because by definition there is not yet a direct defence for them. And that’s why zero days can also wreak the most havoc. After inception, these vulnerabilities will often lie dormant, undiscovered for months, if not years. But researchers are con- tinuously on the lookout for bugs, and they generally have the upper hand. They hunt by disassembling hardware, trawling through lines of code and remotely testing online services. This used to be done manually, and still often is. However, many tools and services have come available which enable au- tomatic testing.6 And since the information security of a sys- tem is as strong as its weakest link, attackers generally only need one vulnera",
            {
                "entities": [
                    [
                        3444,
                        3458,
                        "AUTHOR"
                    ],
                    [
                        5281,
                        5295,
                        "AUTHOR"
                    ],
                    [
                        5436,
                        5450,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "0202ceD82]LC.sc[1v24110.1012:viXraAdvanced Machine Learning Techniques for Fake News (Online Disinformation)Detection: A Systematic Mapping StudyMichał Chora´sa, Konstantinos Demestichasb, Agata Giełczyka, Álvaro Herreroc, Paweł Ksieniewiczd, KonstantinaRemoundoub, Daniel Urdac, Michał Wo´zniakd,∗aUTP University of Science and Technology, PolandbNational Technical University of Athens, GreececGrupo de Inteligencia Computacional Aplicada (GICAP), Departamento de Ingeniería Informática, Escuela Politécnica Superior, Universidadde Burgos, Av. Cantabria s/n, 09006, Burgos, Spain.dWrocław University of Science and Technology, PolandAbstractFake news has now grown into a big problem for societies and also a major challenge for people fighting disinfor-mation. This phenomenon plagues democratic elections, reputations of individual persons or organizations, and hasnegatively impacted citizens, (e.g., during the COVID-19 pandemic in the US or Brazil). Hence, developing effectivetools to fight this phenomenon by employing advanced Machine Learning (ML) methods poses a significant chal-lenge. The following paper displays the present body of knowledge on the application of such intelligent tools in thefight against disinformation. It starts by showing the historical perspective and the current role of fake news in the in-formation war. Proposed solutions based solely on the work of experts are analysed and the most important directionsof the application of intelligent systems in the detection of misinformation sources are pointed out. Additionally, thepaper presents some useful resources (mainly datasets useful when assessing ML solutions for fake news detection)and provides a short overview of the most important R&D projects related to this subject. The main purpose of thiswork is to analyse the current state of knowledge in detecting fake news; on the one hand to show possible solutions,and on the other hand to identify the main challenges and methodological gaps to motivate future research.Keywords: Fake news, Machine Learning, Social media, Media content manipulation, Disinformation detection1. IntroductionLet us start with a strong statement: the fake news phenomenon is currently a big problem for societies, nationsand individual citizens. Fake news has already plagued democratic elections, reputations of individual persons ororganizations, and has negatively impacted citizens in the COVID-19 pandemic (e.g., fake news on alleged medicinesin the US or in Brazil). It is clear we need agile and reliable solutions to fight and counter the fake news problem.Therefore, this article demonstrates a critical scrutiny of the present level of knowledge in fake news detection, on onehand to show possible solutions but also to motivate the future research in this domain.Fake news is a tough challenge to overcome, however there are some efforts from the Machine Learning (ML)community to stand up to this harmful phenomenon. In this mapping study, we present such efforts, solutions andideas. As it is presented in Fig. 1, fake news detection may be performed by analysing several types of digital contentsuch as images, text and network data, as well as the author/source reputation.This survey is not the first one in the domain of fake news. Another major comprehensive work addressingthe ways to approach fake news detection (mainly text analysis-based) and mainstream fake news datasets is [1].∗Corresponding authorEmail addresses: chorasm@utp.edu.pl (Michał Chora´s), cdemest@cn.ntua.gr (Konstantinos Demestichas),agata.gielczyk@utp.edu.pl (Agata Giełczyk), ahcosio@ubu.es (Álvaro Herrero), pawel.ksieniewicz@pwr.edu.pl (PawełKsieniewicz), kremoundou@cn.ntua.gr (Konstantina Remoundou), durda@ubu.es (Daniel Urda), michal.wozniak@pwr.edu.pl (MichałWo´zniak)Preprint submitted to Applied Soft ComputingJanuary 5, 2021   \fAuthor’sreputationNetworkmetadataNEWSImage analysis- context analysis- manipulationdetectionText analysis- NLP- psycholinguistic- non-linguisticFigure 1: The types of digital content that are analysed so as to detect fake news in an automatic mannerAccording to it, the state-of-the-art approaches for this kind of analysis may be classified into five general groupswith methods relying upon: (i) linguistic features, (ii) deception modelling, (iii) clustering, (iv) predictive modellingand (v) content cues. With regard to the text characteristics, style-based and pattern-based detection methods are alsopresented in [2]. Those methods rely on the analysis of specific language attributes and the language structure. Theanalyzed attributes found by the authors of the survey include such features as: quantity of the language elements(e.g. verbs, nouns, sentences, paragraphs), statistical assessment of language complexity, uncertainty (e.g. number ofquantifiers, generalizations, question marks in the text), subjectivity, non-immediacy (such as the count of rhetoricalquestions or passive voice), sentiment, diversity, informality and specificity of the analyzed text. Paper [3] surveysseveral approaches to assessing fake news, which stem from two primary groups: linguistic cue approaches (applyingML) as well as network analysis approaches.Yet another category of solutions is network-based analysis. In [4], two distinct categories are mentioned: (i) socialnetwork behavior analysis to authenticate the news publisher’s social media identity and to verify their trustworthinessand (ii) scalable computational fact-checking methods based on knowledge networks. Beside text-based and network-based analysis, some other approaches are reviewed. For example, [5] attempts to survey identification and mitigationtechniques in combating fake news and discusses feedback-based identification approaches.Crowd-signal based methods are also reported in [6], while content propagation modelling for fake news detec-tion purposes, alongside credibility assessment methods, are discussed in [2]. Such credibility-based approaches arecategorized here into four groups: evaluation of news headlines, news source, news comments and news spreaders/re-publishers. In addition, in some surveys, content-based approaches using non-text analysis are discussed. The mostcommon ones are based on image analysis [1, 5].As complementary to the mentioned surveys, the present paper is unique by catching a very different angle offake news detection methods (focused on advanced ML approaches). Moreover, in addition to overviewing currentmethods, we propose our own analysis criterion and categorization. We also suggest expanding the context of methodsapplicable for such a task and describe the datasets, initiatives and current projects, as well as the future challenges.The remainder of the paper is structured in the following manner: in Section 1, previous surveys are overviewedand the historic evolution of fake news is presented, its current impact as well as the problem with definitions. InSection 2, we present current activities to address the fake news detection problem as well as technological andeducational actions. Section 3 constitutes the in-depth systematic mapping of ML based fake news detection methodsfocused on the analysis of text, images, network data and reputation. Section 4 describes the relevant datasets usednowadays. In the final part of the paper we present some most emerging challenges in the discussed domain and wedraw the main conclusions.2\f1.1. A historic perspectiveEven though the fake news problem has lately become increasingly important, it is not a recent phenomenon.According to different experts [7], its origins are in ancient times. The oldest recorded case of spreading lies to gainsome advantage is the disinformation campaign that took place on the eve of the Battle of Kadesh, dated around 1280B.C., where the Hittite Bedouins deliberately got arrested by the Egyptians in order to tell Pharaoh Ramses II thewrong location of the Muwatallis II army [8].Long time after that, in 1493, the Gutenberg printing press was invented. This event is widely acknowledgedas a keystone in the history of news and press media, as it revolutionized this field. As a side effect, the dis- andmisinformation campaigns had immeasurably intensified. As an example, it is worth mentioning the Great MoonHoax, dating back to 1835. This term is a reference to the collection of half a dozen papers published in The Sun, thenewspaper from New York. These articles concerned the ways life and culture had allegedly been found on the Moon.More recently, fake news and disinformation played a crucial role in World War I and II. On the one hand, Britishpropaganda during World War I was aimed at demonising German enemies, accusing them of using the remains oftheir troops to obtain bone meal and fats, and then feeding the rest to swines. As a negative consequence of that, Naziatrocities during World War II were initially doubted [9].On the other hand, fake news was also generated by Nazis for sharing propaganda. Joseph Goebbels, who co-operated closely with Hitler and was responsible for German Reich’s propaganda, performed a deciding role in thenews media of Germany. He ordered the publication of a paper The Attack, which was then used to disseminatebrainwashing information. By means of untruth and misinformation, the opinion of the public was being persuadedto be in favour of the dreadful actions of the Nazis. Furthermore, according to [10], to this day, it has been the mostdisreputable propaganda campaign ever mounted.Since the Internet and the social media that come with it became massively popularized, fake news has dissem-inated at an unprecedented scale. It is increasingly impacting on presidential elections, celebrities, climate crisis,healthcare and many other topics. This raising popularity of fake news may be easily observed in Fig.2. It presents thecount of records in the Google Scholar database appearing year by year (since 2004), related to the term \"fake news\".Figure 2: Evolution of the ",
            {
                "entities": [
                    [
                        161,
                        186,
                        "AUTHOR"
                    ],
                    [
                        265,
                        277,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "An Online Framework for Cognitive Load Assessmentin Assembly TasksMarta Lagomarsinoa,b,∗, Marta Lorenzinia, Elena De Momib and Arash AjoudaniaaHuman-Robot Interfaces and Physical Interaction Laboratory, Istituto Italiano di Tecnologia, Via San Quirico 19d, Genoa, 16163, ItalybDepartment of Electronics, Information and Bioengineering, Politecnico di Milano, Via Giuseppe Colombo, 40, Milan, 20133, Italy1202tcO91]OR.sc[2v72630.9012:viXraA R T I C L E I N F OKeywords:Cognitive ergonomics,Cognitive manufacturing,Assembly,Attention estimation,Stress detection.A B S T R A C TThe ongoing trend towards Industry 4.0 has revolutionised ordinary workplaces, profoundly changingthe role played by humans in the production chain. Research on ergonomics in industrial settingsmainly focuses on reducing the operator’s physical fatigue and discomfort to improve throughputand avoid safety hazards. However, as the production complexity increases, the cognitive resourcesdemand and mental workload could compromise the operator’s performance and the efficiency ofthe shop floor workplace. State-of-the-art methods in cognitive science work offline and/or involvebulky equipment hardly deployable in industrial settings. This paper presents a novel method for onlineassessment of cognitive load in manufacturing, primarily assembly, by detecting patterns in humanmotion directly from the input images of a stereo camera. Head pose estimation and skeleton trackingare exploited to investigate the workers’ attention and assess hyperactivity and unforeseen movements.Pilot experiments suggest that our factor assessment tool provides significant insights into workers’mental workload, even confirmed by correlations with physiological and performance measurements.According to data gathered in this study, a vision-based cognitive load assessment has the potentialto be integrated into the development of mechatronic systems for improving cognitive ergonomics inmanufacturing.1. IntroductionMental health problems at work affect hundreds ofmillions of people worldwide. About 17.6% of the globalworking population suffer from common mental disorders(CMD) [1], such as anxiety, bipolarity and acute stress. Theannual prevalence attains 38.2% in the European Union,embracing attention-deficit hyperactivity disorder (ADHD),insomnia (7.0%), and major depression (6.9%) [2]. Manyrecent surveys [3] and systematic reviews [4, 5] indicate theinadequate organisation and management of the work as aprimary cause of such disorders and outline the relationshipbetween excessive working pressures and demands and theincidence of depression, poor health functioning, anxiety,distress, fatigue, job dissatisfaction and burnout.Besides, work-related stress and psychological riskshave direct financial implications for private companies andgovernments. In Europe, the cost related to mental illnesssymptoms is around 617 billion euros annually, includingemployers’ expenses (absenteeism, presenteeism, turnoverand loss in productivity) and social welfare costs [6].On the other hand, the introduction of hybrid manufac-turing systems, where workers and autonomous machinesoperate in close proximity, has contributed to changing therole of the human in the production chain, resulting in newoccupational safety and health (OSH) challenges. The digital-isation of the actual workplace has led to work intensification,constant time pressure and adaptation to rapid and frequentchanges in customer demand and requirements (i.e. goods toproduce and services to offer). Many of these changes provide∗Corresponding authormarta.lagomarsino@iit.it (M. Lagomarsino)ORCID(s): 0000-0001-9121-1812 (M. Lagomarsino)development opportunities, nevertheless, they may perilouslyincrease cognitive demand, when inadequately handled, andresult in adverse health and safety hazards. Consequently, theelevated mental workload may compromise the operator’sperformance and the efficiency of the workplace.The study of human cognitive factors will supplementthe well-established research on physical ergonomics [7, 8],to comprehensively understand how humans interact withthe environment and facilitate a reduction of the workload.In addition, various studies have shown that psychologicalfactors at work may have a significant influence on thedevelopment of musculoskeletal disorders (MSDs) [9]. Forinstance, mental workload, fatigue, and job stress can alterbiomechanical control strategies for upper extremities (i.e.neck, shoulders, arms, and hands) and low back extension,as well as increase gait and sway variability [10]. As a finalconsequence, the phenomenon may induce muscle pain inthe worker and even occupational injuries.The global burden of work-related mental disorders isexpected to increase year on year [11] and can no longer beoverlooked. Despite cognitive load theory has aroused muchinterest in the last decade [12], the study of cognitive load inmanufacturing operations is a moderately new topic [13, 14].The field of Cognitive Manufacturing [15] (i.e. the usage ofdata across systems, equipment and processes to optimise themanufacturing performance) has only very recently aimed toattain information about human workload.To the best of our knowledge, available tools can beused almost exclusively by experts or merely provide of-fline insights about the cognitive process (e.g. subjectivequestionnaires [16]). A first attempt toward a more usabletool was made by Thorvald et al. [17], who developed ananalytic method, denoted Cognitive Load Assessment forLagomarsino et al.: Preprint submitted to ElsevierPage 1 of 14   \fAn Online Framework for Cognitive Load Assessment in Assembly TasksFigure 1: The system overview. Left: Conceptual illustration of workstation layout including: RGB-D camera, assembly, instructionsgraphical user interface (GUI), and storage area. Right: Block diagram of the proposed online framework to assess cognitive loadand provide visual feedback to the user.Manufacturing (CLAM), for assessing the cognitive burdenthat the worker is expected to employ within a particularassembly task and workstation layout. As a matter of fact,manual assembly is an essential activity in the manufacturingsector, which exposes workers to situations with varyingcognitive demands [18]. When combining the latter with hightime pressure, an increase in mental load frequently occurs[19]. The tool is intended to be used directly by workersinvolved in the manufacturing domain. Nevertheless, suchevaluation is still made offline, asking the end-users to fill aform and rate a set of factors associated with different aspectsof their daily activity.The scientific and industrial communities still need tobe provided with a validated set of models and metrics forthe cognitive workload. Particularly, gaps were identifiedin relation to the online assessment of the mental demandinflicted by manufacturing tasks.To respond to this challenge, the purpose of this paperis to develop a quantitative and online method to examinehow industrial work affects people relative to their attentiondistribution, decision-making, mental overload, frustration,stress and errors. We propose an online framework to monitorthe cognitive workload of human operators by detectingpatterns in their motion directly from the input imagesof a stereo camera. Head pose estimation and skeletontracking are exploited to investigate the workers’ attentionand assess hyperactivity and unforeseen movements (seesystem overview in Figure 1). The developed tool computesa list of indicators associated with different aspects of anassembly task and workstation layout in manufacturing. Eachfactor impacts with a weight on two defined indexes: themental effort and psychological stress level. According tothe scores interval, we determine the level of cognitive loadan individual is experiencing within the current setup. Thestudy employs assembly experiments to validate our onlineframework against state-of-the-art offline methods in the fieldof cognitive science (i.e. physiological signals, secondarytask-performance measure and subjective questionnaires).The paper is structured as follows. In section 2 wecharacterise cognitive load and provide an overview of relatedworks about the methods to measure it. Next, we present ourframework for the online assessment of mental effort andstress level. Pilot experiments are then proposed in section 5and the result are discussed and validated through statisticalanalysis. The final sections discuss the contributions andlimitations of the framework.2. Related worksThe evidence that undue cognitive demand at work canprejudice the mental health of workers and their manufactur-ing performance has increased the interest in cognitive loadtheory (CLT). CLT investigates the interaction of cognitivestructures, information and its implications [20]. In particular,the term cognitive load refers to the amount of processingthat performing a particular task imposes on the learner’scognitive system [12]. Xie and Salvendy [21] present adetailed conceptual framework of human information process-ing and distinguish between instantaneous and overall load.Instantaneous load is defined as the dynamics of cognitiveload, which constantly fluctuates over time as a response tostimuli that the present activity and environmental conditionsare imposing on the subject. Overall load results by thewhole working procedure and represents the experiencedand garnered instantaneous load in the human’s brain.A large and growing body of literature has investigatedtechniques to model human mental workload [22] andquantify the cost of performing tasks [21, 23]. Paas andVan Merriënboer [12] describe mental load, mental effort,performance, and level of stress as the measurable dimensionsof cognitive load. Generally, cognitive load measurementsbelong to three main categories: physiological measures,subjective rating scales and performance-based measures.Lagomarsino et al.: Preprint submitted to ElsevierPage 2 of 1",
            {
                "entities": [
                    [
                        89,
                        105,
                        "AUTHOR"
                    ],
                    [
                        107,
                        121,
                        "AUTHOR"
                    ],
                    [
                        126,
                        141,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Vrije Universiteit BrusselAutomated decision-making in the EU Member States: The right to explanation and other“suitable safeguards” in the national legislationsMalgieri, GianclaudioPublished in:Computer Law & Security ReviewDOI:10.1016/j.clsr.2019.05.002Publication date:2019License:CC BYDocument Version:Final published versionLink to publicationCitation for published version (APA):Malgieri, G. (2019). Automated decision-making in the EU Member States: The right to explanation and other“suitable safeguards” in the national legislations. Computer Law & Security Review, 35(5), [105327].https://doi.org/10.1016/j.clsr.2019.05.002CopyrightNo part of this publication may be reproduced or transmitted in any form, without the prior written permission of the author(s) or other rightsholders to whom publication rights have been transferred, unless permitted by a license attached to the publication (a Creative Commonslicense or other), or unless exceptions to copyright law apply.Take down policyIf you believe that this document infringes your copyright or other rights, please contact openaccess@vub.be, with details of the nature of theinfringement. We will investigate the claim and if justified, we will take the appropriate steps.Download date: 04. jul. 2023 \fcomputer law & security review 35 (2019) 105327 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR Automated decision-making in the EU Member States: The right to explanation and other “suitable safeguards” in the national legislations ∗Gianclaudio Malgieri Vrije Universiteit Brussel, Pleinlaan 2, 1020 Brussels, Belgium a r t i c l e i n f o a b s t r a c t Keywords: Right to explanation Automated decision-making AI Legibility Suitable safeguards Data Protection GDPR Article 22 Right to contest Algorithmic impact assessment The aim of this paper is to analyse the very recently approved national Member States’ laws that have implemented the GDPR in the field of automated decision-making (prohi- bition, exceptions, safeguards): all national legislations have been analysed and in partic- ular 9 Member States Law address the case of automated decision making providing spe- cific exemptions and relevant safeguards, as requested by Article 22(2)(b) of the GDPR (Bel- gium, The Netherlands, France, Germany, Hungary, Slovenia, Austria, the United Kingdom, Ireland). The approaches are very diverse: the scope of the provision can be narrow (just auto- mated decisions producing legal or similarly detrimental effects) or wide (any decision with a significant impact) and even specific safeguards proposed are very diverse. After this overview, this article will also address the following questions: are Member States free to broaden the scope of automated decision-making regulation? Are ‘positive decisions’ allowed under Article 22, GDPR, as some Member States seem to affirm? Which safeguards can better guarantee rights and freedoms of the data subject? In particular, while most Member States refers just to the three safeguards mentioned at Article 22(3) (i.e. subject’s right to express one’s point of view; right to obtain human in- tervention; right to contest the decision), three approaches seem very innovative: a) some States guarantee a right to legibility/explanation about the algorithmic decisions (France and Hungary); b) other States (Ireland and United Kingdom) regulate human intervention on algorithmic decisions through an effective accountability mechanism (e.g. notification, ex- planation of why such contestation has not been accepted, etc.); c) another State (Slovenia) require an innovative form of human rights impact assessments on automated decision- making. © 2019 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) ∗ Corresponding author: Gianclaudio Malgieri, Vrije Universiteit Brussel, Pleinlaan 2, 1020 Brussels, Belgium. E-mail address: gianclaudio.malgieri@vub.ac.be https://doi.org/10.1016/j.clsr.2019.05.002 0267-3649/© 2019 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) \f2 computer law & security review 35 (2019) 105327 1. Introduction and methodology The aim of this paper is to analyse the very recently ap- proved national Member States’ laws that have implemented the GDPR in the field of automated decision-making (prohibi- tion, exceptions, safeguards). The EU General Data Protection Regulation has tried to ad- dress the risks of the automated decision-making through different tools: a right to receive meaningful information about logics, significance and envisaged effects of automated decision-making; the right not to be subject to automated decision-making with several safeguards and restrains for the limited cases in which automated decisions are permitted. In a previous article it was suggested that the dualism be- tween right to ex post explanation vs. right to ex ante general information should be overcome: transparency and compre- hensibility should merge in the concept of “legibility”.1 One re- maining problem is the exact meaning of “suitable measures to safeguard the data subject’s rights and freedoms and legiti- mate interests” that should be taken, e.g. when the automated decision-making is authorised by Union or Member State law. Member States laws implementing the GDPR are, thus, an important reference when discussing automated decision- making and suitable safeguards to protect individuals against such decisions: Article 22(2) lett. b explicitly refers to Mem- ber States laws that should also adopt ‘suitable safeguards’ for protecting individuals. Section 2 will analyse the relevant GDPR provisions in terms of automated decision-making and “suitable safe- guards”; while Section 3 will briefly mention the debate around the right to an explanation of the algorithmic decision-making. Consequently, Section 4 will analyse pos- sible ‘suitable safeguards’ against adverse effects of auto- mated decision-making on individuals; while Section 5 will analyse the nine Member States whose data protection laws have explicitly regulated automated decision-making. Finally, Section 6 will summarize and compare some of the most rel- evant provisions of Member States Law and Section 7 will propose some preliminary conclusions, analysing advantages and disadvantages of the most innovative national regula- tions. Some preliminary remarks about methodology are also necessary. All Member States Law implementing the GDPR have been analysed here, through the official versions avail- able in different national online repositories of the approved legislation (e.g. www.gesetze- im- internet.de for German law, www.legislation.gov.uk for UK law, etc.).2 Sometimes the offi- cial language is already English (UK, Ireland, Malta), in other cases the English translation is publicly available (it is the case 1 2 Gianclaudio Malgieri and Giovanni Comandé, ‘Why a Right to Legibility of Automated Decision-Making Exists in the General Data Protection Regulation’, International Data Privacy Law 7, no. 4 (1 November 2017): 243–65, https://doi.org/10.1093/idpl/ipx019 . A useful summary of all national online repositories for different national legislations can be found here: https://iapp. org/resources/article/eu- member- state- gdpr- implementation- laws- and- drafts/ (last access, 1 December 2018). The list of links for each Member State Law will be in the following footnotes. Danish law,4 of German law,3 ). In the other cases, the author has profited from national experts who have specifically translated in English for him the relevant provi- sions regarding automated decision-making. Romanian Law 5 In addition, to improve the quality of legal comparison among different legal texts, the author has taken in due con- sideration the wording of the GDPR and the official transla- tions in all different languages of the EU: 6 it has allowed to understand whether national laws have strictly respected the GDPR wording, or, as an alternative, have proposed more orig- inal implementations. 2. The problem of automated-decision making and the GDPR (Articles 15 and 22) Profiling algorithms and automated decision-making are a growing reality in the actual data-driven society. Policy- makers, scholars and commentators are more and more con- cerned with the risks of black box society 7 in several fields: fi- nance, insurance, housing, police investigations, e-commerce, work life, etc. The GDPR has tried to provide a solution through different tools: a right to receive/access meaningful information about logics, significance and envisaged effects of the automated decision-making processes (Articles 13(2), lett. f; 14(2), lett. g; and 15(1), lett. h). In addition, Article 22(1) states that “the data subject shall have the right not to be subject to a decision based solely on au- tomated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her ”. This right shall not apply in only three cases: a. the decision “is necessary for entering into, or performance of, a contract between the data subject and a data con- troller”; 3 https://www.gesetze- im- internet.de/englisch _ bdsg/ englisch _ bdsg.html#p0310 . https://www.datatilsynet.dk/media/6894/danish-data- protection-act.pdf. https://www.privacyone.ro/files/Romanian-GDPR- implementation- law- English- translation.pdf. See the official versions here: https://eur-lex.europa.eu/ legal-content/EN/TXT/?uri=CELEX:32016R0679 . Frank Pasquale, The Black Box Society: The Secret Algorithms That Control Money and Information , Cambridge-London, 2015. Vir- ginia Eubanks, Automating Inequality – How High Tech Tools Pro- file, Police, and Punish the Poor , St. Martin Press, New York, 2018. See also, e.g., Joshua A. Kroll, Joanna Huey, Solon Barocas, Ed- ward W. Felten, Joe",
            {
                "entities": [
                    [
                        3880,
                        3901,
                        "AUTHOR"
                    ],
                    [
                        6996,
                        7017,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "ReligionISSN: 0048-721X (Print) 1096-1151 (Online) Journal homepage: https://www.tandfonline.com/loi/rrel20Daniel Dubuisson, The Western Construction ofReligionSteven Engler & Dean MillerTo cite this article: Steven Engler & Dean Miller (2006) Daniel Dubuisson, The WesternConstruction of Religion , Religion, 36:3, 119-178, DOI: 10.1016/j.religion.2006.08.001To link to this article: https://doi.org/10.1016/j.religion.2006.08.001Published online: 19 Oct 2011.Submit your article to this journal Article views: 2069View related articles Citing articles: 1 View citing articles Full Terms & Conditions of access and use can be found athttps://www.tandfonline.com/action/journalInformation?journalCode=rrel20\fReligion 36 (2006) 119e178www.elsevier.com/locate/religionReview symposiumDaniel Dubuisson, The WesternConstruction of ReligionSteven Engler a,*, Dean Miller ba Department of Humanities, Mount Royal College, Calgary, Alberta T3N 6K6, Canadab Emeritus, Department of History, University of Rochester,10848 South Hoyne Avenue, Chicago, IL 60643, USAAbstractIn The Western Construction of Religion Daniel Dubuisson argues that the concept of ‘religion’ is toohistorically and culturally contingent to serve as the basis for a comparative discipline. The concept isindigenous to Western culture and is inherently theological and phenomenological. He argues for a con-structionist view of the discipline and proposes the concept ‘cosmographic formations’ as a replacementfor ‘religion’. Religious phenomena should be taken as discursive constructions that link embodied individ-uals to the social, cultural and cosmic orders. The following reviews evaluate Dubuisson’s arguments,relating them to broader currents in the theory of religion. Daniel Dubuisson responds to each of thereviews.1(cid:2) 2006 Elsevier Ltd. All rights reserved.* Corresponding author.E-mail address: sjengler@gmail.com (S. Engler).1 The reviews by Engler, Hughes and Segal were presented in a session of the Critical Theory and Discourses onReligion Group of the American Academy of Religion meeting in San Antonio, Texas, in November 2005. Ann Taves’contribution is adapted from her response to these commentators at that AAR Session. An additional review fromthat session, by Gustavo Benavides, is not included here because portions were previously committed for publicationelsewhere. Dean Miller gathered the three additional reviews. Dubuisson’s responses were edited and translated bySteven Engler.0048-721X/$ - see front matter (cid:2) 2006 Elsevier Ltd. All rights reserved.doi:10.1016/j.religion.2006.08.001\f120S. Engler, D. Miller / Religion 36 (2006) 119e178Agency, order and time in the human science of religionSteven EnglerDepartment of Humanities, Mount Royal College, Calgary, Alberta T3N 6K6, CanadaE-mail address: sjengler@gmail.comIn The Western Construction of Religion (2003) Daniel Dubuisson criticises religious studies onthe grounds that its axiomatic category, ‘religion’, is too historically and culturally contingent toserve as the basis for a comparative discipline. He argues that ‘the West invented religion’ (a leg-acy of Christian concepts and nineteenth-century colonial scholarship) (p. 12).2 He faults scholarsof religion for propagating this deception: ‘the history of religions should not have exported thissingular notion, found nowhere else, and issuing from a history that took its own unique course,without having subjected it beforehand to a rigorous critical examination’ (p. 191).In two respects Dubuisson goes beyond similar arguments by others, such as Talad Asad, Rus-sell McCutcheon and Timothy Fitzgerald. First, he argues that ‘religion’ is not just another con-struct. Rather, the concept has filled an ‘architectonic function’it has‘supplied the nucleus about which the West has constructed its own universe of values and repre-sentations’ (pp. 117, 39). Second, he proposes as a replacement the concept ‘cosmographic forma-tions’, which he roots in a universal instinct for creating conceptions that relate cosmic, culturaland social orders.in Western culture:The value of Dubuisson’s book lies not just in its critique of religion but also in the threads thathe draws upon in beginning to suggest a way to move our discussions forward. I will argue fourrelated claims. (1) Dubuisson’s discursive link between the themes of religion and order is veryuseful. (2) An ambiguous appeal to science presents a misleading dichotomy, leaving us to choosebetween naively essentialist and radically constructionist views of the study of religion, withDubuisson throwing his weight behind radical constructionism. (3) The claim that ‘cosmographicformations’ offer a truly universal category for cross-cultural comparison needs further clarifica-tion. (4) A clearer conception of agency can reframe and supplement the argument, reclaiming thepractical dimension of religion.The key question is, What is to be gained by replacing the concept of ‘religion’? For Dubuisson,religion is too limited a construct. If ‘religion’ is a construct, then a substitution is possible. If ‘re-ligion’ is a limited concept, then a substitution is desirable.The concept of religion, according to Dubuisson, is limited in three related ways. First, it drawson metaphysical presuppositions. Dubuisson suggests an alternative, because using ‘religion’ as ananalytical tool imports insider concepts, resulting in circularity: ‘A regrettable confusion con-stantly arises between religious ideas and ideas about religion’ (p. 55). ‘How could it leave thismagic circle when the object that it is supposed to study is supplied by its own cultural traditionwhich also surreptitiously imposes on it the means and frameworks for its inquiry?’ (p. 192).Second, ‘religion’ is culturally specific: ‘Religion, that is, the word, the idea, and above all theparticular domain that they all designate represents an entirely original creation that the Westalone conceived and developed after having converted to Christianity’ (p. 190). The ‘history of2 All unattributed page references in all reviews are to Dubuisson, 2003.\fS. Engler, D. Miller / Religion 36 (2006) 119e178121religion thus reveals itself to be not only a Western discipline but a science born of the closingdecades of the nineteenth century’ (p. 155).Third, and most significant as a motivation for replacing it, the concept of religion hasplayed a foundational role in generating the ‘major paradigm’ of the West that has constrainednot just the history of religions but Western thought in general: religion plays a ‘decisive rolein the constitution of Western culture.. [R]eligion is at the heart of our ‘world’’ (p. 190).Religion is ‘the most ideological of Western creations’ (p. 147). This increases the risk of ac-cepting the concept uncritically: ‘religion’ is so foundational to the biases of Western intellectualperspectives that we can only leave behind its distorting influence by abandoning the conceptentirely.Dubuisson’s constructionism asserts that Western culture, in the wake of Christianity, is con-strained by an underlying conceptual framework:Western thought . (is) disposed of only a small number of theses and models.. a huge sys-tem of fractal shapes . dominated by the incessant activity of polemic and controversy. .This immemorial movement, inscribed in our oldest intellectual tradition, . offer(s) ourfields of knowledge the system of references and coordinates in which they inscribe them-selves. Every new idea or hypothesis immediately generates its antithesis, whose positionis a priori predictable.. We are advancing in a familiar, well-mapped universe, in whichthe same ideas never cease to be revived and recombined with one another. (pp. 132e3)The study of religion works itself out within this ‘tacit contract that a priori binds every Westernthinker to the vast complex formed by its interpretive grids’ (p. 144).Dubuisson turns to the relation between religion and science. There is a misleading ambiguityhere. Usually, he uses ‘science’ to refer to the social, or ‘human’, sciences, but occasionally he usesthe term to refer to the natural sciences. Dubuisson clearly holds that the two are different: ‘Un-like the natural sciences, the various human sciences never have to deal with raw data’ (p. 175).However, this distinction and its implications are not evoked consistently. Talk of ‘raw data’offers a misleadingly positivistic view of the natural sciences. It is all too easy to hold that thehuman sciences cannot live up to this ideal, but the portrayal is distorted, lending support bydefault to a constructionist view of the human sciences: ‘Contrary to the received opinion implicitin common parlance and embodied in a kind of scientific positivism, people do not live the THEworld, . since each human group lives only in its world (p. 204).Dubuisson distinguishes two levels at which the concepts ‘science’ and ‘religion’ work. At onelevel we find ‘the canonical opposition, religion versus science’, but ‘on another more global level,religious explanation and scientific explanation offer . undeniable affinities and amusing similar-ities’ (p. 150). Dubuisson means the human sciences here, and the affinity is constructionism. ForDubuisson, the human sciences are like religion, because both are constructs: scientific knowledgereflects the perspective of a specific culture; it is not universal and objective (see p. 203). And itmanifests inevitable progress (see p. 161).Dubuisson offers us a stark choice in conceptions of science: radical constructionism or simplis-tic realism. But this is made possible because in these passages he is referring to ‘the human sci-ences’. This excessively stark view of the natural sciences is easily discounted as a model for thesocial or human sciences (see Engler, 2004). He thus neglects the possibility of a trans-culturalmiddle ground, neither naively objective nor purely Western. I take Dubuisson’s term ‘human\f122S. Engler, D. Miller / Religion 36 (20",
            {
                "entities": [
                    [
                        208,
                        222,
                        "AUTHOR"
                    ],
                    [
                        175,
                        187,
                        "AUTHOR"
                    ],
                    [
                        224,
                        236,
                        "AUTHOR"
                    ],
                    [
                        853,
                        865,
                        "AUTHOR"
                    ],
                    [
                        2364,
                        2376,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Migration to the metaverse and its predictors: attachment to virtual places and metaverse-related threat Tomasz Oleksy1* Anna Wnuk1 Małgorzata Piskorska1 1 Faculty of Psychology, University of Warsaw, ul. Stawki 5/7, 00-183 Warsaw, Poland * Corresponding author E-mail addresses: tomasz.oleksy@psych.uw.edu.pl (T. Oleksy), anna.wnuk@psych.uw.edu.pl (A. Wnuk), mm.piskorska@student.uw.edu.pl (M. Piskorska)       \fAbstract The most ambitious visions of metaverse technology promise to create virtual places that offer the same possibilities as the real world. Implementation of these plans can become the next milestone on the road to the ongoing exodus from real places to digital ones. However, as any novel technology, the metaverse raises controversies and questions. Does one want to migrate to the metaverse? Does one’s willingness to move to virtual worlds depend on the bonds with existing virtual places and the sense of threat related to this technology? To address these questions, we drew on the theories of place attachment and intergroup threat. In two studies – (1) among users of open-world games (N = 366) and (2) using a sample representative of the Polish population in terms of age, gender and size of the residential place (N = 995) – we observed a low level of willingness to migrate to the metaverse. The participants displayed a high level of perceived metaverse-related threat, ranging from privacy concerns to the belief that metaverse can deprive one of access to essential human experiences. However, greater attachment to virtual, as opposed to real, places was associated with both an increased willingness to migrate to the metaverse and a low level of perceived threat. The results provide a better understanding of individuals’ perception of the metaverse and of how the bonds with virtual and real places translate into attitudes towards metaverse technology.    \fIntroduction In recent years, virtual places have played an increasingly important role in human life (Barreda-Ángeles & Hartmann, 2022). Even before the COVID-19 pandemic broke out, people were spending increasingly more time in virtual worlds (Rzeszewski & Evans, 2020; Coulson et al., 2019). The metaverse is one of the technologies of the future that is generating a particular interest in the context of the potential further exodus of people from real places to digital ones. Referred to as the ‘new internet’, the metaverse is designed to become a virtual universe where people can receive entertainment, socialize or even work (Bojic, 2022; Oh et al., 2023). The feature that distinguishes the metaverse from its predecessor, the internet, the most is its capacity to be explored in a manner analogous to how we interact with real places, which is to be achieved primarily through further development of both virtual and augmented reality technologies that can either transfer users to a virtual location or integrate virtual elements into the real world. The concept of the metaverse has been around for three decades – the term dates back to the science fiction novel Snow Crash written by Neal Stephenson in 1992 – and some of the elements of the future virtual universe are already being widely used. For example, Second Life has allowed its users to create avatars to meet other players, buy and sell virtual real estate and do business (Boellstorff, 2015). These possibilities are being further enhanced by contemporary virtual chats, such as VRChat (Barreda-Ángeles & Hartmann, 2022), or sites, such as Decentraland and Sandbox (Goanta, 2020; Jeon et al., 2022). Moreover, the past several years have been marked by a boom of augmented-reality games, such as Pokemon-Go, which enable smartphone gamers to explore the real world in search of virtual elements, and at the same time, the quality and availability of virtual reality (VR) systems have increased significantly, which affects their progressive adoption (Barreda-Ángeles & \fHartmann, 2022). However, despite the remarkable technological advancement, the public was introduced to the metaverse more recently, when major technology companies announced that they were beginning to work on creating their own versions of this technology, ranging from the enhancement of the current virtual worlds (e.g. Epic’s Fortnite and the Roblox ecosystem; Oh et al., 2023) to a more general and somewhat vague vision of the ‘next evolution of social connection’ (Zuckerberg, 2021). Although we still do not know what the most advanced implementations of the metaverse concept can look like, this technology can have the potential to significantly impact human interaction with the world. There is a lack of research on the predictors of the willingness to test this technology and the public concerns raised by it. Thus, we intend to fill this gap by focusing on users’ willingness to transfer a substantial amount of their daily activities to virtual worlds and its predictors. We hypothesise that greater acceptance of the development of the metaverse and willingness to use it is related to prior positive experiences of virtual places, primarily attachment to the currently used virtual places and the feeling that they can satisfy our needs more than real-world locations. Moreover, we expect that, as in the case of any new and evolving technology, the development of the metaverse will raise a number of concerns and questions. Building on the intergroup threat theory (Stephan & Stephan, 2017), we intend to examine whether the willingness to migrate to metaverse worlds is associated with two general types of threat: 1) realistic threat (e.g., fear of the loss of privacy, increased power of technology companies, loneliness of users or neglect of real places) and 2) symbolic threat (fears of how the technology can affect human nature and the bonds connecting humans to the real world). Considering the various existing definitions of the metaverse, this research will focus on users’ relationship to migration to the metaverse, understood as a three-dimensional, virtual world in which users can undertake most of the activities available in the real world through avatars. \fVirtual worlds as equivalent of real locations Places are natural conditions of human existence (Heidegger, 1962); however, the definition of a place remains an area of dispute among geographers, environmental psychologists and philosophers (Lewicka et al., 2019). The classic definitions of a place assume that it is a bounded entity with a unique identity and historical continuity, offering a haven and a break from the outside world (Relph, 1974). Other definitions stress that a place should instead be defined as a location with interactive potential, a meeting space rather than a separated enclave (Massey, 2004; Di Masso, 2012; Seamon, 2013). However, most researchers agree that a place, as opposed to a space, possesses a specific meaning for its inhabitants or visitors. Thus, a place is not a random location passed by but an object which people are connected to through their various life experiences. The emergence of virtual worlds has raised an important question for place theorists: What is the role of physicality in defining meaningful locations? Traditionally, places have been conceptualised by their tangibility and physical basis (Relph, 1976; Lewicka, 2011). However, current digital worlds can also provide a comfortable space for human dwelling (e.g. Rzeszewski & Evans, 2021). Currently, virtual environments can be comfortably accessed via one’s home on personal computers or smartphones, presenting an alternative world that can be perceived by the users as very real and changing the traditional ways of recreation, travel, social interactions and work (Plunkett, 2011). Therefore, physicality ceases to be a critical aspect that defines a place. Existing studies show that a lack of tangibility does not prevent virtual places from gaining meaning for their users. For example, Stoklos (2009) argues that virtual settings have several qualities of physical places (e.g. they afford social interaction and have symbolic meanings). Some scholars argue that it is crucial for the virtual world to gain meaning for it to be treated as a physical place (Plunkett, 2011; see also Tuan, 1974). However, the diminishing boundary between virtual and real places raises questions \fabout the latter’s possible de-meaning and its consequences. For example, Gifford (2011) argues that human failure to rationally divide time between physical and virtual worlds can lead to decreased interest in real, local concerns. Similarly, Lewicka (2020) considers whether current social processes, including the virtualisation of everyday life, may involve undermining human relationships with the places they inhabit (see also Seamon, 2020). This problem was further emphasised during the COVID-19 pandemic, when lockdowns and subsequent fear forced people to move many of their activities to virtual spaces (Barreda-Angeles & Hartmann, 2022). Thus, digital worlds served and continue serving as a functional alternative to temporarily unavailable options, steadily gaining popularity and new user bases (Paul et al., 2021). More individuals were able to test the possibilities offered by virtual worlds: for example, enacting different characters, exploring exciting places, shaping the space according to individual needs or simply enjoying safety from the pandemic and detachment from other everyday concerns (Paul et al., 2021). As a result, the rise of virtual worlds’ popularity during the COVID-19 pandemic forced human relationships with real places to weaken (Devine-Wright et al., 2020). Stay-at-home directives, social distancing and the feeling of coronavirus-related threats confined millions of people to their homes and displaced them from everyday locations that have meaning and serve to fulfil their material and psychological needs (e.g. Bick, Blandin, & Mertens, 2020; Honey-Roses et al., 2020). This alienation from places co",
            {
                "entities": [
                    [
                        104,
                        118,
                        "AUTHOR"
                    ],
                    [
                        120,
                        130,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "computer law & security review 47 (2022) 105737 Available online at www.sciencedirect.com journal homepage: www.elsevier.com/locate/CLSR What post-mortem privacy may teach us about privacy ✩ Uta Kohl University of Southampton, United Kingdom a b s t r a c t This paper approaches the debate about the protection of digital legacies through a medical confidentiality lens, capitalising on its outlier status in common law jurisdictions as a privacy-type duty that survives the death of the rightsholder. The discussion takes the case law in England and Wales and by the European Court of Human Rights on post-mortem medical confidentiality as a springboard for interrogating how these judgments navigate the traditional objections to post-mortem privacy. Whilst the legal duty of medical confidentiality, drawing on the professional duty of the Hippocratic Oath, acts in the first place as a trust mechanism between doctor and patient based on a reciprocity of interests, its incidental effect of protecting not just the rightsholder but also duty bearers and the industry, signals more complex operational dynamics. The post-mortem continuation of that duty in turn brings these other relationships to the surface. Indeed, the post-mortemness amplifies that confidentialities – and by extension information privacy - can rarely be located in an isolated, singular binary relationship between a duty bearer and a rightsholder but is entangled in the great messy sociality of life that involves multiple overlapping, interdependent relationships of relative trust. These may upon the death of the primary rightsholder – make an appearance as concurrent or competing claims on her legacies and incidentally also carry her post-mortem privacy. © 2022 Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) 1. Introduction The persistence of data and information in global online net- works has tested the functional and temporal boundaries of privacy protection. Such persistence can have dire personal and professional consequences for individuals if unfavourable personal information enters the online domain. Digital mem- ory lacks the ‘natural’ forgetfulness of humans as a protec- tive mechanism upon which privacy law has previously re- lied. Some relief is now provided in the EU through the right- to-be-forgotten under the General Data Protection Regulation ✩ University of Southampton. Many thanks to Remigius Nwabueze and the anonymous referees for their helpful comments on an earlier draft. E-mail address: U.Kohl@soton.ac.uk https://doi.org/10.1016/j.clsr.2022.105737 0267-3649/© 2022 Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ) \f2 computer law & security review 47 (2022) 105737 [GDPR] 1 which allows individuals to request data controllers such as search engines to ‘forget’ outdated or inaccurate per- sonal information in the results produced in response to name searches, and thereby gives them some control over their per- sonal narratives in the public domain.2 Yet, the problematic of informational persistence also continues after death pre- cisely because personal data and information remains unal- tered in situ, and the rightsholder is no longer there to give directions. Whilst the issue of personal legacies is not inher- ently new, digital remains exceed, in depth and breadth, the amount and sensitivity of previous analogue records. This has led to renewed discussions of the merits, or otherwise, of post- mortem privacy protection.3 Such post-mortem privacy would address itself, in the first place, to ‘digital legacies’ or ‘digital social media accounts,5 remains’ – such as email histories,4 documents and files, search histories, personal DNA or health profiles and digital footprints more generally. Post-mortem privacy has also been called upon by relatives to prevent public access to death scene images, against the threat of potentially large online circulations .6 Furthermore, personal big data has created entirely new post-mortem possibilities with privacy implications. For example, deepfakes, that is AI-generated im- personations based on existing personal digital footage, can bring the deceased ’back to life’, with both innocuous and abu- sive potentials which once more fall within the possible ambit of privacy protection.7 Copyright and other intellectual prop- 1 2 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016on the protection of natural persons with regard to the processing of personal data and on the freemove- ment of such data. Google Spain SL and Google Inc v Agencia Española de Protección de Datos (AEPD) and Mario Costeja González C-131/12 (CJEU, GC, 13 May 2014) EU:C:2014:317, interpreting the Data Protection Direc- tive 95/46/EC; now Art 17 of the GDPR. NNG de Andrade, ‘Oblivion: the right to be different … from oneself: re-proposing the right to be forgotten’ in Alessia Ghezzi, Ângela Guimarães Pereira, Lucia Vesnic-Alujevic L (eds) The Ethics of Memory in a Digital Age – In- terrogating the Right to Be Forgotten (New York: Palgrave Macmillan, 2014) 65. For some limited US authority to the same effect, see e.g. Briscoe v Reader’s Digest Association 4 Cal 3d 532 (1971) concerning a newspaper revelation of the claimant’s criminal past which had the effect of alienating his daughter and friends from him. Jason Mazzone, ‘Facebook’s Afterlife’ (2012) 90 North Carolina Law Review 1643; Lilian Edwards, Edina Harbinja, ‘Protecting Post- Mortem Privacy: Reconsidering the Privacy Interests of the De- ceased in a Digital World’ (2013) 32 (1) Cardozo Arts & Entertainment Law Journal 101; Natalie M Banta, ‘Death and Privacy in the Digi- tal Age’ (2016) 94 North Carolina Law Review 927; Floris Tomasini, Remembering and Disremembering the Dead (Palgrave, 2017) esp Ch3. Estate of Maria Cecilia Quadri v Parisi (2021) WL 3544783 (Fla Cir 4 3 5 6 Ct); see also Ajemian v Yahoo! Inc 84 NE3d 766 (Mass 2017). Digital Inheritance III ZR 183/17 (12 July 2018) BGH. In National Archives and Records Administration v Favish 541 US 157 (2004) the US Supreme Court allowed the family’s privacy claim in respect of death scene images of the deceased as an ex- emption to freedom of information requests (here made by jour- nalists): ‘The well-established cultural tradition of acknowledging a family’s control over the body and the deceased’s death images has long been recognized at common law.’ Marisa McVey, ‘Deepfakes and the Dead: The Case of An- thony Bourdain’ (10 August 2021) Modern Technologies, Privacy Law and the Dead https://thefutureofprivacylaw.wordpress.com/2021/ 08/10/deepfakes- and- the- dead- the- case- of- anthony- bourdain/ 7 erty rights which serve economic interests, outlast the death of the rightsholder, but should privacy underwritten by digni- tary concerns also remain intact post-mortem and, if so, how long and guarded by whom? In some civil law jurisdictions, such as Germany, person- ality rights do not automatically expire upon death,8 which gives those jurisdictions a solid foundation for navigating the raised stakes of personal digital remains. Meanwhile common law countries are especially ill-equipped to deal with the new phenomenon, given the absence of a developed personality rights jurisprudence and their adherence (but for some statu- tory interventions) to the long-standing maxim actio person- alis moritur cum persona , that is personal, as opposed to propri- etary, actions die with the person.9 This maxim which goes, at least, as far back as Hambly v Trott (1776) 10 is based on the idea that ‘personal rights’ are by their very design attached to the person and thus not transferrable. By implication, any post- mortem recovery would be ‘in the nature of a penalty rather than compensatory.’ 11 The maxim captures injuries to intan- gible interests of personality, of which privacy is an example par excellence : ‘[i]t is well settled that the right to privacy is purely a personal one; it cannot be asserted by anyone other than the person whose privacy has been invaded.’ 12 This paper examines the call for post-mortem privacy in respect of digital legacies by mapping one of the few con- texts where even common law jurisdictions have recognised the possibility of privacy surviving the death of the rightsh- older, that is in respect of medical confidences.13 Whilst its almost unique status may suggest an outlier case the ratio- nale of which is not transferrable to other subject-matters,14 the argument here is that the animating forces behind post- mortem medical confidentiality are instructive about post- mortem privacy generally. Thus the microcosm of medical confidentiality delivers an opportunity for interrogating why and how the law recognises post-mortem medical privacy de- 8 9 Mephisto 30/173 (24 February 1971) BVergG; Marlene Dietrich 1 ZR 49/97 (1 December 1999) BGH; Wilhelm Kaisen 1 BvR 932/94 (5 April 20012) BVerfG; see also § 189 of the German Criminal Code (offence of defiling the memory of the dead). Note, for example, Recital 27 of the GDPR which provides that ‘This Regulation does not apply to the personal data of deceased persons. Member States may provide for rules regarding the pro- cessing of personal data of deceased persons.’ In the UK data pro- tection is limited to living individuals: s.3(2) of the Data Protection Act 2018. 10 Hambly v Trott (1776) 1 Cowper 371 (where the court acknowl- edged the limited application of the maxim to torts); but the Com- mon Law Procedure Act 1833, replaced by the Law Reform (Miscel- laneous Provisions) Act 1934 allowed for the deceased’s estate to recover some losses that arose before their death. 11 Alvin E Evans, ‘Survival of Claims For and Against Executors and Administrator’ (1931) 19 Kentucky Law Journal 195, 206. 12 James v Screen Gems Inc (1959) 174 CalApp 2d 650, 653; Hendrick-",
            {
                "entities": [
                    [
                        190,
                        199,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "Contents lists available at ScienceDirect Internet Interventions journal homepage: www.elsevier.com/locate/invent Accessibility of mental health support in China and preferences on web-based services for mood disorders: A qualitative study Yuxi Tan a,b,c, Emily G. Lattie d, Yan Qiu a, b, c, Ziwei Teng a,b, c, Chujun Wu a, b, c, Hui Tang a,b, c, Jindong Chen a, b,c, * a Department of Psychiatry, The Second Xiangya Hospital, Central South University, Changsha, Hunan, China b National Clinical Research Center for Mental Disorders, Changsha, Hunan, China c Institute of Mental Health, Central South University, Changsha, Hunan, China d Department of Medical Social Sciences, Northwestern University, Chicago, IL, United States  A R T I C L E I N F O  A B S T R A C T  Keywords: Mental health Web-based health service Mood disorder Recovery Background: The fast development of mobile technologies provides promising opportunities to fulfill the largely unmet needs of treatment and recovery for mood disorders in China. However, with limited research from China, the development of acceptable and usable web-based mental health services that are based on preference of patients from China still remains a challenge. Objective: The aims of this paper were to (1) understand the experience of patients with mood disorders on current accessibility of mental health support in China; and (2) to get insights on patients' preferences on web- based mental health services, so as to provide suggestions for the future development of web-based mental health services for mood disorders in China. Methods: Semi-structured interviews were conducted with 10 female participants diagnosed with depression and 7 with bipolar disorder (5 female and 2 male) via the audio chat function of WeChat. The interviews were 60–90 min long and were audio-recorded and transcribed verbatim. Thematic analysis was conducted using QSR NVivo 12 to identify and establish themes and sub-themes. Results: Two major sections of results with a total of 5 themes were identified. The first section was participants' treatment and recovery experience, which included three main themes: (1) professional help seeking experience; (2) establishment of self-help strategies; and (3) complex experiences from various source of social support. The second section was focused on preferences for web-based services, which were divided into two themes: (1) preferred support and features, with three sub-themes: as channels to access professionals, as databases for self- help resources, and as sources of social support; and (2) preferred modality. Conclusions: The access to mental health support for personal recovery of mood disorders in China was perceived by participants as not sufficient. Web-based mental health services that include professional, empathetic social support from real humans, and recovery-oriented, personalized self-help resources are promising to bridge the gap. The advantages of social media like WeChat were emphasized for patients in China. More user-centered research based on social, economic and cultural features are needed for the development of web-based mental health services in China.  1. Introduction Mood disorders are common in China, with a lifetime prevalence of 7.4% (Huang et al., 2019). With the episodic nature of these disorders, the risk of relapse is high (Gitlin and Miklowitz, 2017; Hardeveld et al., 2013). Management of subthreshold symptoms by providing adjunctive personalized psychological interventions appears vital to preventing disability and enhancing quality of life (Bonnín et al., 2012; Miziou et al., 2015). However, despite the high prevalence, only one fifth of patients with mood disorders have ever been in contact with mental health-care providers in their lifetime (Patel et al., 2016). The reasons for the large, unmet mental health needs in China are abundant, and are often explained by the unbalanced allocation of resources between * Corresponding author at: Department of Psychiatry, The Second Xiangya Hospital, Central South University, 139 Middle Renmin Road, Changsha, Hunan 410011, China. E-mail address: chenjindong@csu.edu.cn (J. Chen). https://doi.org/10.1016/j.invent.2021.100475 Received 19 January 2021; Received in revised form 20 October 2021; Accepted 30 October 2021  InternetInterventions26(2021)100475Availableonline4November20212214-7829/©2021TheAuthors.PublishedbyElsevierB.V.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense(http://creativecommons.org/licenses/by-nc-nd/4.0/).\fY. Tan et al.                                                                                                                   Abbreviations None.  major cities and rural areas, limited mental health workforce, especially the lack of professional social workers and psychological therapists (Liang et al., 2018). The lack of medical insurance support also contributed to the high threshold for psychological therapy (Zhang et al., 2017). In recent years, the 686 project, also called the Central Government Support for the Local Management and Treatment of Serious Mental Illness Project, has been benefiting many patients with severe mental diseases, especially schizophrenia, to receive more access to more convenient even free treatment and recovery service (Liang et al., 2018). To achieve better mental health at the population level, much more attention is needed to bridge the gap faced by patients with non-psychotic disorders. With the development of mobile technology, the Internet has become a common way for patients to search for information, seek mental health resources, or obtain social support (Rahal et al., 2018). Using mobile platforms to provide mental health services has become a feasible way to bridge the gap. Currently, there are many types of web-based mental health services, such as provision of psychological education and self- management strategies based on cognitive behavioral therapy or mindfulness (O'Connor et al., 2018; Rathbone et al., 2017), active or passive symptom monitoring and feedback using smartphone sensors or wearable devices (Simblett et al., 2018), or real-time interaction via chatbots or conversational agents (Vaidyam et al., 2019). A number of reviews have demonstrated that web-based mental health services have the advantages in preventing mental illness, alleviating symptoms and improving quality of life, especially with human support (Ebert et al., 2018; O'Connor et al., 2018). Evidence showed that patients generally have high acceptance of mental health services provided on the Internet, but are prudent about their effectiveness (Apolinario-Hagen et al., 2017; Tan et al., 2020). In addition, there are several challenges worth con-cerning for future practice and implication, including low engagement, lack of evidence support, limited implication on significant clinical symptoms (Frank et al., 2018; Torous et al., 2018). There are also numerous qualitative research studies that have demonstrated the importance of mobile technologies to facilitate the treatment and recovery of mood disorders. For instance, several studies have identified the role of mobile app use on helping with patients' social interconnectivity, skill acquisition, access and management of needs, and bridge the disconnection with health professionals (Fulford et al., 2019; Pung et al., 2018). Other studies have revealed factors that might influence the effectiveness or motivation of mobile mental health ser-vices usage, such as mental health awareness, appropriate content and medium, functionality, human support, reminders or incentives (Eccles et al., 2020; Simblett et al., 2018). Barriers can be divided into personal barriers, such as lack of time, high stress level, and barriers directly related to the program, such as complex content, functionality and privacy issues (Eccles et al., 2021; Ebert et al., 2018) However, the vast majority of research on web-based mental health services were conducted in western counties (Lal and Adair, 2014), and there is still a gap in the literature, especially qualitative researches on the needs and preferences of people living in China. How to design more acceptable and usable web-based mental health services that are truly based on patients' needs and effectively promote mental health in China still remains a challenge. Given these gaps, along with the lack of service availability in China, the use of well-designed web-based mental health intervention as auxiliary means to expand the accessibility of existing mental health resources, rather than replacing the traditional face-to- face medical services (Ebert et al., 2018), could be particularly prom-ising in China. The purposes of this study are to: (1) understand the experience of patients with mood disorders regarding the current accessibility of mental health support, and (2) identify patients' preferences on web- based mental health services, so as to provide suggestions for the future development of web-based mental health services for mood dis-orders in China. 2. Methods 2.1. Study design Single session semi-structured interviews were conducted with par-ticipants diagnosed with depression or bipolar disorder via remote audio chat function provided by WeChat. WeChat is one of the most commonly used social media platforms in China (Montag et al., 2018). Audio chat was selected over video conferencing due to the sensitivity of interview content, and belief that patients may feel more comfortable discussing personal stories and feelings in a more anonymous manner. Moreover, the study was conducted during the Coronavirus Disease 2019 (COVID- 19) pandemic, which further required the interviews to be remote. 2.2. Recruitment The criteria for recruitment are: (1) age > 18; (2) have a diagnosis of major depression disorder or bipolar disorder for at least 6 months; (3) not currently experiencing suicidal ideation or manic symptoms. Par-ticipants were drawn from a pool of indivi",
            {
                "entities": [
                    [
                        239,
                        248,
                        "AUTHOR"
                    ],
                    [
                        274,
                        282,
                        "AUTHOR"
                    ],
                    [
                        291,
                        302,
                        "AUTHOR"
                    ],
                    [
                        310,
                        320,
                        "AUTHOR"
                    ],
                    [
                        329,
                        338,
                        "AUTHOR"
                    ],
                    [
                        346,
                        359,
                        "AUTHOR"
                    ]
                ]
            }
        ],
        [
            "This is a repository copy of What motivates building repair-maintenance practitioners to include or avoid energy efficiency measures? Evidence from three studies in the United Kingdom.White Rose Research Online URL for this paper:https://eprints.whiterose.ac.uk/170121/Version: Accepted VersionArticle:Murtagh, N, Owen, A orcid.org/0000-0002-1240-9319 and Simpson, K (2021) What motivates building repair-maintenance practitioners to include or avoid energy efficiency measures? Evidence from three studies in the United Kingdom. Energy Research and Social Science, 73 (1). 101943. ISSN 2214-6296 https://doi.org/10.1016/j.erss.2021.101943© 2021, Elsevier. This manuscript version is made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/.Reuse This article is distributed under the terms of the Creative Commons Attribution-NonCommercial-NoDerivs (CC BY-NC-ND) licence. This licence only allows you to download this work and share it with others as long as you credit the authors, but you can’t change the article in any way or use it commercially. More information and the full terms of the licence here: https://creativecommons.org/licenses/ Takedown If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request. eprints@whiterose.ac.ukhttps://eprints.whiterose.ac.uk/\fWhat motivates building repair-maintenance practitioners to include or avoid energy efficiency measures? Evidence from three studies in the United Kingdom Niamh Murtagh1, Alice M. Owen2, Kate Simpson3 1 The Bartlett School of Construction and Project Management, University College London (UCL), 1-19 Torrington Place, London WC1E 7HB. Email: n.murtagh@ucl.ac.uk 2 Sustainability Research Institute, School of Earth & Environment, University of Leeds, Leeds, LS2 9JT, UK 3 University Centre Scunthorpe, North Lindsey College, Scunthorpe, DN17 1AJ. Now at: School of Design Engineering, Imperial College London, Kensington, London SW7 1AL Acknowledgements Study B data collection was made possible via funding from University Centre Scunthorpe as part of the Association of Colleges Scholarship project under the HEFCE Catalyst fund, and the work of Aaron Flannagan who co-created interview scripts and data collection within a supervised internship. Study C data collection was funded by the Sainsbury’s Charitable Trusts’ Climate Collaboration and the UKRI through the UK Energy Research Centre’s flexible fund project “GLIDER”. The funders had no involvement in any aspect of the research.     \f1. Introduction Homes must become more energy efficient to meet the UK policy target of net-zero CO2 emissions by 2050 [1]. The UK residential sector alone accounts for 22% of end-use greenhouse gas emissions, a contribution which has changed little over the last 15 years [2]. The UK Industrial Strategy Construction Sector Deal [3] set an ambition to halve the emissions of new buildings by 2030, through developing innovative energy and low carbon technologies. The strategic agenda on construction is focused on new-build, with particular interest in modern technologies such as robotics, artificial intelligence, off-site and modular construction methods. However, evidence demonstrates that while UK strategy has tended to focus on new build, the primary problem lies elsewhere. An estimated 87% of existing residential building stock is expected to be in use in 2050 [4]. To meet the legally binding target of net zero CO2 emissions by 2050 [1], energy efficiency measures are required on 25 million existing homes throughout the UK [5]. This is already technologically achievable: for example, high performance insulation and window systems are readily available. But how can essential stakeholders be engaged to deliver the steep decarbonisation required? Focusing on how to engage key actors in the domain of energy-efficiency retrofit, the paper offers a novel theoretical contribution, applying an established framework from the field of behaviour change. We first briefly present an overview of the repair, maintenance and improvement sector. We consider relevant policy initiatives and review previous research in this area, before examining how energy efficiency fits within the sector. We then outline the background to the theoretical framework, COM-B (Capability, Opportunity, Motivation, Behaviour) [6] and provide a short explanation. After presenting the method and findings of the empirical study, the Discussion section considers the implications, forming the basis for recommendations in the Conclusion. 1.1 The repair-maintenance-improvement sector The construction sector in Great Britain is composed primarily of micro and small enterprise builders and tradespeople: 77.1% (1.7 million) are self-employed or work in businesses employing fewer than 50 people, and 39.9% (926,000) are sole operators [7]. Work on the existing housing stock, referred to as Repair-Maintenance-Improvement (RMI), constitutes 17% of construction output [8]. RMI work includes all forms of construction and maintenance activity on existing homes, from window replacement to adding extensions. Factors relating to energy efficiency are not typically a primary objective of RMI work [9] and the individuals already making a living within repair and maintenance may not see benefit in expanding to cover energy-related retrofit. Innovating in the technologies and techniques used in order to ensure homes and buildings become more energy efficient can be a risk for a sole tradesperson or small-to-medium enterprise (SME) who, by contrast, understand well the products they install on a regular basis [10, 11]. In sum, the core actors in the sector may not be motivated to engage with the actions necessary to deliver zero-carbon homes. Previous research on domestic energy retrofit has tended to focus on policy [12], technology [13] and performance [14]. The people involved have been less well-served in academic studies. While there has been more attention paid to householders and building occupants in recent years [15-17], the practitioners themselves have had relatively little focus. There are exceptions, such as a valuable contribution to the literature that distinguished intermediary roles from that of ‘middle actors’ [18], shifting the focus from intermediate organisations involved in policy implementation [19] towards individual practitioners. Some of the few studies which have investigated RMI or retrofit professionals include a study of the perspectives of building performance evaluation practitioners in the UK [20], an investigation of attitudes and approaches of practitioners in the sector in Ireland [21] and barriers to retrofit for the fuel poor in the UK [22]. However, the studies did not clarify if their \fresearch participants were drawn from the micro- and small-enterprises which characterise the sector nor did they draw on established theory. The role of these SMEs, and micro-enterprises in particular, has been largely neglected in policy development debates. Further, their voices are rarely heard and they are often considered to be beyond the reach of policy [23]. This invisibility has been noted too regarding heating engineers, one of the important groups of practitioners influencing domestic refurbishment and often operating as sole traders or in micro-firms [24]. Today’s RMI actors are crucial to the delivery of energy efficiency in the residential building stock but are not well-understood or represented. Nonetheless, builders are a main source of information for householders on possible retrofit options [25, 26] and finding a trusted builder has been found to be a prerequisite for many householders before undertaking retrofit activity [27]. There is evidence too for the effectiveness of shared learning between domestic heating installers and homeowners, particularly where the installers are certified and receive regular information from the certifying organisation [24]. Despite this, previous policies have failed to introduce vocational training schemes for builders, tradespeople and others to equip them with the necessary knowledge relating to energy efficiency. Although the Federation of Master Builders, a body representing the interests of small and medium-sized construction firms, has been involved in guidelines on delivering sustainability in new homes, more generally, UK government policy has been criticised for inhibiting skills development [28]. For example, in 2015, the ‘Each Home Counts’ review was launched to consider issues concerning consumer advice, protection and standards in relation to UK home energy efficiency and renewable measures. Its recommendations to government touched on certification and quality standards, consumer trust and improved training [29]. However, its remit was consumer-focused and did not address the perspectives, attitudes or knowledge needs of the actors who will deliver energy-efficient retrofit. A previous policy package intended to stimulate demand for energy-efficiency refurbishment of UK homes, the Green Deal, was launched in 2013 but closed in 2015 due largely to its failure to attract demand from householders [12]. Many installers who had registered as Green Deal accredited installers then exited the scheme, leading to the collapse of the residential energy efficiency market in the UK [18]. Energy-efficiency programmes are necessary to deliver the government commitment to zero-carbon by 2050 [30] but a lack of trust from builders’ prior experiences or lack of incentive to become involved may demotivate and result in the essential actors working against the desired outcomes. Policy has long recognised the need for substantial change in the construction industry but current industrial strategy for the sector forms part of a wider agenda to advance technological innovation, particularly for new-",
            {
                "entities": [
                    [
                        1617,
                        1631,
                        "AUTHOR"
                    ],
                    [
                        1649,
                        1662,
                        "AUTHOR"
                    ]
                ]
            }
        ]
    ]
}