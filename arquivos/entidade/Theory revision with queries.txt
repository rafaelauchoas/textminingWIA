Artificial Intelligence 156 (2004) 139–176www.elsevier.com/locate/artintTheory revision with queries:Horn, read-once, and parity formulasJudy Goldsmith a,1, Robert H. Sloan b,∗,2, Balázs Szörényi c,György Turán c,d,3a Computer Science Department, University of Kentucky, Lexington, KY 40506-0046, USAb Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607-7053, USAc Hungarian Academy of Sciences and University of Szeged, Research Group on Artificial Intelligence,Aradi vértanúk tere 1, H-6720 Szeged, Hungaryd Department of Mathematics, Statistics, and Computer Science, University of Illinois at Chicago,Chicago, IL 60607-7045, USAReceived 6 March 2003; received in revised form 11 November 2003; accepted 15 January 2004AbstractA theory, in this context, is a Boolean formula; it is used to classify instances, or truth assignments.Theories can model real-world phenomena, and can do so more or less correctly. The theoryrevision, or concept revision, problem is to correct a given, roughly correct concept. This problemis considered here in the model of learning with equivalence and membership queries. A revisionalgorithm is considered efficient if the number of queries it makes is polynomial in the revisiondistance between the initial theory and the target theory, and polylogarithmic in the number ofvariables and the size of the initial theory. The revision distance is the minimal number of syntacticrevision operations, such as the deletion or addition of literals, needed to obtain the target theory fromthe initial theory. Efficient revision algorithms are given for Horn formulas and read-once formulas,where revision operators are restricted to deletions of variables or clauses, and for parity formulas,where revision operators include both deletions and additions of variables. We also show that thequery complexity of the read-once revision algorithm is near-optimal. 2004 Published by Elsevier B.V.Keywords: Theory revision; Knowledge revision; Horn formulas; Query learning; Computational learningtheory; Boolean function learning* Corresponding author.E-mail addresses: goldsmit@cs.uky.edu (J. Goldsmith), sloan@uic.edu (R.H. Sloan), szorenyi@rgai.hu(B. Szörényi), gyt@uic.edu (G. Turán).1 Partially supported by NSF grant CCR-0100040.2 Partially supported by NSF grants CCR-9800070 and CCR-0100336.3 Partially supported by NSF grants CCR-0100336 and CCR-9800070.0004-3702/$ – see front matter  2004 Published by Elsevier B.V.doi:10.1016/j.artint.2004.01.002140J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–1761. IntroductionSometimes our model isn’t quite right. As computer scientists, we build models of real-world phenomena, based on limited data or on the opinions of sometimes-fallible experts.We verify or begin to use the models and discover that they are not quite correct. Ratherthan beginning the model-building phase again, we would prefer to quickly and simplyrevise the current model, and continue our project. If the initial model is nearly correct,this should be more efficient.The revision of an initial theory, represented by a formula, consists of applying syntacticrevision operators, such as the deletion or the addition of a literal. For instance, the CUPtheory,1 presented in Fig. 1, might be revised to become more accurate by deleting theliteral white. The revision distance of the target theory from the initial theory is definedto be the minimal number of revision operations from a specified fixed set needed toproduce a theory equivalent to the target, starting from the initial theory. As in our previouswork [30] we consider two sets of revision operators: deletions-only operators, which allowthe deletion of literals and of clauses and/or terms, and general operators, which also allowthe addition of literals. Others have also implicitly or explicitly considered both of thosemodels [32,40].If the target theory is close to the initial theory, then an efficient revision algorithmshould find it quickly. Thus, revision distance is one of the relevant parameters for definingthe efficiency of theory revision.One way of formalizing the problem of theory revision as a concept learning problemis: learn the class of concepts that are within a given revision distance of the initial theory.A novel feature of this definition is that it associates a concept class with each concept, andthus, in a sense, assigns a learning complexity to every individual concept (more precisely,to every concept representation, and every revision distance bound). This may perhapshelp formalize the intuitive, yet elusive, notion that in general, there are hard and easytarget concepts in learning theory. For instance, intuitively, there are hard and easy DNFs,but it does not make sense to talk about the difficulty of learning a particular DNF. On theother hand, it does make sense to talk about the difficulty of revising a particular DNF. Sotheory revision gives a way to quantify the learning complexity of each DNF.This article and its companion article [30] consider revision in query-based learningmodels, in particular, in the standard model of learning with membership and equivalencequeries, denoted by MQ and EQ [5]. This is a very well-studied model (e.g., [2,4–8,11,13–15]), nearly as much so as PAC-learning. In an equivalence query, the learning algorithmproposes a hypothesis, that is, a theory h, and the answer depends on whether h = c,CUP ≡ has-concavity ∧ white ∧ upward-pointing-concavity ∧ has-bottom∧ flat-bottom ∧ lightweight ∧ (has-handle ∨ (width-small ∧ styrofoam))Fig. 1. Cup theory/concept, inspired by Winston et al. [57]. Note that there may be many additional variables thatare not used in the current cup theory.1 Cups are for theory revision what elephants are for computational learning theory and perhaps for AI ingeneral, and what penguins are for nonmonotonic reasoning: the canonical toy example.J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176141where c is the target theory. If so, the answer is “correct”, and the learning algorithmhas succeeded in its goal of exact identification of the target theory. Otherwise, the answeris a counterexample: any instance x such that c(x) (cid:5)= h(x). In a membership query, thelearning algorithm gives an instance x, and the answer is either 1 or 0, depending on c(x).The query complexity of a learning algorithm is the number of queries it asks. Note thatthe query complexity is a lower bound on the running time. For running time, we do notcount the time required to answer the queries. From a formal, theoretical point of view,we assume that there are two oracles, one each to answer membership and equivalencequeries. In practice, membership queries would need to be answered by a domain expert,and equivalence queries could either be answered by a domain expert, or by using thehypothesis and waiting for evidence of an error in classification.It is typical in practical applications that one starts with an initial theory and a setof (counter)examples, for which the initial theory gives an incorrect classification. Thegoal then is to find a small modification of the initial theory that is consistent with theexamples. (In fact, many theory revision methods, including the algorithms presented here,would work even if a large number of changes were needed, but in that case it might bemore efficient to learn from scratch rather than revising.) In this setup, one can simulatean equivalence query by running through the examples. If we find a counterexample tothe current hypothesis, then we continue the simulation of the algorithm. Otherwise, weterminate the learning process with the current hypothesis serving as our final revisedtheory. In this way, an efficient equivalence and membership query algorithm can be turnedinto an efficient practical revision algorithm.Besides this motivation, there are other reasons, specific to theory revision, that justifythe use of equivalence and membership queries. In practical applications, it is often thecase that the goal of theory revision is to fix an initial theory that is provided by anexpert. It is reasonable to hope that the expert is able to answer further queries aboutthe classification of new instances. For instance, natural language applications make thispossibility apparent, as here everybody can serve as an expert, answering queries aboutthe correctness of sentences. This means that in all these cases learning algorithms may beassumed to use membership queries.Another important reason to study the query model is that it turns out to be the “right”model for many important learning problems. That is, for several basic problems, suchas learning finite automata and Horn formulas, there are nontrivial efficient learningalgorithms in this model, while in weaker models one can prove superpolynomial lowerbounds.In this paper we study two very important tractable classes of formulas: conjunctions ofHorn clauses and read-once formulas.Horn sentences are the tractable heart of several branches of computer science. Thesatisfiability of Horn sentences can be decided in polynomial—indeed linear—time(e.g., [23]). There is a combinatorial characterization of functions that can be expressedby Horn sentences [21,34,44,54].Horn sentences have many applications. For instance, Horn sentences occur as specialcases in logic, logic programming, and databases. Real-world reasoning and causality canbe described by Horn theories: If the world is like so, then these are the consequences,142J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176separately and jointly. Horn sentences model safe queries in relational database theory[43].Given the plethora of Horn sentences out there, it is imperative that we be able to mendthose that are broken. The work presented in this paper is a first step in that direction.Similarly to Horn formulas, read-once formulas form a nontrivial class that is tractablefrom several different aspects, but slight extensions are already intractable. Booleanfunctions represented by read-once formulas have a combinatorial characterization [33,35,46], and certain read restrictions make CNF satisfiability easily decidable in polynomialtime (see, e.g., [38]). It is interesting that the tractable cases for fault testing [39] and Horntheory revision [24,40] are also related to read-once formulas.The main results that we present in this paper are revision algorithms for Horn and read-once formulas in the deletions-only model of revisions, and a revision algorithm for parityfunctions in the general model of revisions. Some lower bounds are also provided.The ultimate goal of this work is to revise real expert-system style theories, such as fullHorn theories, using the types of queries we have already argued are feasible with real,human experts: membership and equivalence queries. In the course of pursuit of this asyet elusive goal, we have achieved some partial results, both for more restricted classesof theories and for more constrained revisions. Our work on parity formulas and on read-once formulas adds to a body of theoretical work on learning such formulas with queries(e.g., [8]), and showcases techniques and lower-bound proofs that we hope will be helpfulin later work. They are included here as much for their mathematical elegance as for theireventual applicability.We also include algorithms for revision constrained to deletions. We note that there is along history of studying this special case, presumably because of its greater tractability, in,and even before, the AI literature. What we call “deletions only” corresponds to the “stuck-at” faults usually studied in diagnosing faulty circuits in the 1960s and 1970s (e.g., [39])and, for instance, to the case where Koppel et al. proved the convergence of their empiricalsystem for theory revision in the 1990s [40].We note that there are scenarios where deletions-only would be quite useful for theintended application. Say, for example, that the more mature expert-system builder hasdesigned a Horn-formula-based theory and sent her apprentice out to populate the theory.The apprentice interviews experts and enthusiastically writes down almost everything thateach expert says, ignoring the experts’ self-corrections.It turns out that the model is imperfect, although the experts are sound.Thus, the expert-system builder is faced with the task of revising the theory. From areview of the apprentice’s methodology, it is clear that the revisions require only deletions.As in the full revision model, the expert-system builder has access to the experts andmay ask them the same types of queries. She could use the same algorithm to revise hertheory, but she realizes that there is a more efficient algorithm available for the deletions-only case. This is precisely the algorithm that we present in Section 5.In the next section we will discuss previous work on theory revision, and especiallycomputational learning theory approaches to theory revision. Then, in Section 3, wediscuss just what is meant by either the learning of or the revision of “propositional Hornformulas”. We formally define basic concepts in learning with queries and in Booleanformulas in Section 4. In Section 5, we give our revision algorithm for propositional HornJ. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176143formulas. We present our revision algorithm for read-once formulas in Section 6, and forparity formulas in Section 7.2. Previous workThere is an extensive discussion of related work on theory revision in both thecomputational learning theory literature and the actual AI systems literature in ourcompanion paper [30]. In this section, we briefly mention a few important, but somewhatarbitrarily selected, articles. In addition to our companion paper, we refer the reader toWrobel’s overviews of theory revision [58,59] for more detail. In the next section, we willdiscuss in more depth some papers that have each given results on something that theycalled “theory revision of propositional Horn formulas”, although different researchershave actually considered quite different problems under that name.Mooney [45] initiated the study of theory revision in computational learning theoryusing an approach based on syntactic distances. Mooney proposed considering the PAC-learnability of the class of concepts having a bounded syntactic distance from a givenconcept representation, and gave an initial positive result for sample complexity, but leftcomputational efficiency as an open problem.Numerous software systems have been built for theory revision. A few representativeexamples are EITHER [47], KBANN [52], PTR [40], and STALKER [18]. Many systems,such as STALKER, are designed for what Carbonara and Sleeman [18] have called thetweaking assumption: that the initial theory is fairly close to the correct theory. This wouldpresumably be the case, for instance, when a deployed expert system is found to make someerrors. On the other hand, KBANN can be viewed as solving essentially the usual generalconcept learning problem (using back-propagation for neural nets, and then sometimestranslating back into propositional Horn sentences if possible), but starting the learningfrom some “in the ballpark” concept, instead of from some default “null concept”. It isunclear whether KBANN’s successes actually required initial theories that were “only atweak” away from being correct.Mooney implicitly assumes the tweaking case of theory revision. Here it is appropriatefor the learning resources used (e.g., number of queries or sample size) to dependpolynomially on the revision distance, but only subpolynomially on the size of the initialtheory and the number of variables in the domain under consideration. For instance, wemight want a dependence that is O(log(initial theory size + n)), where n is the numberof variables in the domain. The reason this is desirable is that the tweaking assumptionshould mean that the revision distance e (cid:6) max(initial theory size, n), and we wish torevise using significantly fewer resources than learning from scratch.These considerations also suggest a relationship between theory revision and attribute-efficient learning (see, e.g., [12,15,19,22]). Attribute-efficient learning is concerned withlearning a concept from scratch, while using resources that depend polynomially on thenumber of variables in the target (called the number of relevant variables) and onlylogarithmically or polylogarithmically on the total number of variables in the universe.Roughly speaking, attribute-efficient learning is the special case of theory revision wherethe initial theory is the default, empty concept.144J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176Angluin et al. [7] give a query-learning algorithm for Horn sentences. Our revisionalgorithm given in Section 5 is modeled on their algorithm. The primary differencebetween learning and revising Horn formulas, or any formulas, is the more stringent querycomplexity bounds required for revision, as opposed to learning from scratch. For instance,Angluin et al.’s algorithm to learn a Horn formula of n variables must ask (cid:1)(n) queries,whereas we are limited to o(n) queries.Read-once formulas are efficiently learnable using equivalence and membershipqueries [8]. While read-twice DNF formulas are still efficiently learnable [48], for read-thrice DNF formulas there are negative results [1]. The query complexity of the learningalgorithm for read-once formulas is O(n3), where n is the number of variables, or,equivalently, the length of the formula. In contrast, our revision algorithm for read-onceformulas uses O(e log n) queries, where e is the revision distance between the initial andtarget formulas.There has been a limited amount of work on theory revision for predicate logic. Greinergives some results on theory revision in predicate logic in a paper that is primarily aboutrevising propositional Horn formulas, which we discuss in the next section [32]. In anotherpaper, Greiner [31] gives results about revision operators that change the order of therules in a logic program. These, together with some results of Argamon-Engelson andKoppel [10] and Wrobel [58], are among the very few theoretical results on theory revisionfor predicate logic.3. The dilemma of HornsIn the literature, “learning propositional Horn sentences” in fact refers to four distinctdefinitions of learning problems. Although there has been some discussion of this issue [3,20,25], we think that some more clarification is possible, both for its own sake, and becauseit will clarify our discussion of the related work, especially Greiner’s related work [31,32].1. “Monotone circuit model”. Propositional Horn sentences where only some of thevariables are observable, and the problem is to classify an instance given onlythe values of those observable variables. The classification of instances over thoseobservable variables depends on whether the sentence and the setting of the observablevariables together imply a special output variable that occurs only as the head ofclauses. This meaning is used in the EITHER theory revision system [47], and alsoin Mooney’s theoretical PAC analysis of theory revision [45]. This model is equivalentto the model that complexity theorists call monotone circuits.2. “Assignments model”. Propositional Horn sentences where the classification of theinstance depends on whether the assignment to (all) the variables agrees with orcontradicts the Horn sentence. This meaning is used by Angluin et al. [7] and in thisarticle, in our algorithm in Section 5.3. “Entailment”. Propositional Horn sentences where the instances themselves are Hornclauses, and a clause’s classification depends on whether it is entailed by the targetHorn sentence. This meaning is used by Frazier and Pitt in their work on learning byentailment [25,26], and also by Greiner [32].J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176145CUP ← UPRIGHT ∧ LIFTABLE ∧ OPEN ∧ GRASPABLE ∧ whiteUPRIGHT ← has-bottom ∧ upward-pointing-concavityLIFTABLE ← lightweight ∧ has-handleLIFTABLE ← width-small ∧ styrofoamOPEN ← upward-pointing-concavityGRASPABLE ← width-smallGRASPABLE ← has-handleFig. 2. A monotone-circuit style Horn sentence for CUP. It does not define exactly the same set as the definitionin Fig. 1.4. “Atomic entailment”. The same entailment setting as 3, but now only atoms can beinstances. Greiner also considers this case.Consider Definition 1. An example of a Definition 1 style Horn formula for CUP (theexample in Fig. 1 is of a read-once formula) is given in Fig. 2. The classification variableis CUP, and the hidden variables are UPRIGHT, LIFTABLE, OPEN, and GRASPABLE.One can describe such a sentence by a monotone circuit, with the classification variablecorresponding to the output gate, the observables to inputs, and the hidden variables tointerior gates. In fact, any monotone circuit is equivalent to such a Horn sentence.Monotone circuits are a fairly rich class, and one that has been well studied incomplexity theory. Monotone circuits are not learnable from equivalence queries alone,because the smaller class of monotone Boolean formulas is not learnable from equivalencequeries alone [37].2 To the best of our knowledge, it is an open question whether monotonecircuits are learnable from membership and equivalence queries together.Definition 2 is the one that we use for our revision result in Section 5. The cup examplein Fig. 2 follows Definition 2 if all the variables including OPEN, GRASPABLE, etc., arevisible. In general, in the Assignments model, training data for learning (or revising) fromexamples show the assignments to all the variables.In Definition 3, entailment, there are again no hidden variables, but what is being learnedis different. We might ask, in the cup example of Fig. 2, whether either of the followingtwo clauses is entailed.CUP ← LIFTABLE ∧ upward-pointing-concavityLIFTABLE ← lightweight.(Note that neither example is entailed by the Horn sentence in Fig. 2.)The main point of Model 4, entailment of atoms, is to use it to get strong negativeresults. Positive learning results would not be so useful, because Horn sentences over npropositional variables are an unreasonably large set of theories if all one wants to know iswhich of the n variables are positive and which are negative.2 One could also show the hardness of learning monotone circuits from equivalence queries by applying astandard variable substitution trick [36] to the cryptography result of Goldreich et al. that among other thingssays that the class of all polynomial-size circuits is not learnable from equivalence queries [27].146J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176Angluin [3] provides some discussion on the comparison among the last three cases, asdoes Frazier [25]. In particular, Frazier shows how to convert a query learning algorithm forthe assignment model into one for the entailment model. However, Frazier’s conversionsin general involve a multiplicative blowup in query complexity of the number of variablesin the domain (i.e., of n), so the conversions cannot automatically transfer theory revisionresults for the assignments model into the entailment model. Further comparisons of thedifferent approaches are given by De Raedt [20].Greiner [32] considers Models 3 and 4, entailment of clauses and of atoms. Looselyspeaking, Greiner shows that the non-tweaking cases of theory revision in Models 3 and 4are NP-complete in the absence of membership queries, in both the general and deletions-only model. More precisely, he considers the PAC model, and so is interested in thedecision problem of, given a sample of Horn clauses with each labeled either “entailed” or“not entailed”, deciding whether there is a Horn sentence within a stated revision distanced that would so classify the clauses. Greiner shows that even for the entailment of atomsmodel, for d = (cid:1)(|ϕ| ), where |ϕ| is the size of the initial theory ϕ, the problem is NP-complete. It is also nonapproximable, in the sense that one cannot find a Horn sentencethat, say, agrees with 90% of the classifications of the sample, given usual complexitytheory assumptions [32].√Note that Greiner’s hardness results do not contradict our results. First, we allowmembership queries in addition to sampling/equivalence queries. Some classes that haveexponential query/sample complexity when only sampling/equivalence queries are usedhave polynomial query complexity when both membership and equivalence queries areused. Read-once Boolean formulas are an example of such a class [8,37]. On the otherhand, arbitrary Boolean formulas are difficult to learn even with both membership andequivalence queries [9]. Another distinction between Greiner’s negative results and oursis that we are primarily interested in smaller values of the revision distance than d =(cid:1)(|ϕ| ).√4. PreliminariesWe use the standard model of membership and equivalence queries (with counterexam-ples), denoted by MQ and EQ [5]. In an equivalence query, the learning algorithm proposesa hypothesis, a concept h and the answer depends on whether h = c, where c is the targetconcept. If so, the answer is “correct”, and the learning algorithm has succeeded in its goalof exact identification of the target concept. Otherwise, the answer is a counterexample, anyinstance x such that c(x) (cid:5)= h(x). For read-once formulas and parity functions, our equiv-alence queries will be proper; that is, the hypothesis will always be a revision of the initialformula. For Horn sentences, our equivalence queries will be “almost proper”, meaningthat the hypothesis will always be a conjunction of Horn clauses, with each Horn clausebeing a revision of a Horn clause in the initial formula, but there may be more than onerevision of a single initial theory Horn clause in the hypothesis.3 In a membership query,3 Our lower bound for Horn sentence revision will therefore also permit almost proper equivalence queries.J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176147the learning algorithm gives an instance x and the answer is either 1 or 0, depending onc(x), that is, MQ(x) = c(x), where again c is the target concept.We also use standard notions from propositional logic such as variable, term, monotone,etc. We will assume throughout that the everywhere true and everywhere false formulashave some representation in each class of formulas that we study in this paper. The all-0vector will be denoted 0; the all-1 vector 1. We occasionally use the standard partial orderon vectors with x (cid:1) y if every component of x is less than or equal to the correspondingcomponent of y.The symbol ⊂ always denotes proper subset.A Horn clause is a disjunction with at most one unnegated variable; we will usuallythink of it as an implication and call the clause’s unnegated variable its head, and itsnegated variables its body. We write body(C) and head(C) for the body and head of C,respectively. A clause with no unnegated variables will be considered to have head F,and will sometimes be written as (body → F). A Horn sentence is a conjunction of Hornclauses.For monotone terms s and t we use s ∩t for the term that is the product of those variablesin both s and t. As an example, x1x2 ∩ x1x3 = x1. (Thus s ∩ t is different from s ∧ t, whichis the product of variables occurring in either s or t.)When convenient, we treat Horn clause bodies as either monotone terms or as vectors in{0, 1}n, and treat vectors sometimes as subsets of [n]. If for x ∈ {0, 1}n and Horn clause Cwe have body(C) ⊆ x, we say x covers C. Notice that x falsifies C if and only if x coversC and head(C) /∈ x. (By definition, F /∈ x.)Our Horn sentence revision algorithm makes frequent use of the fact that if x and y bothcover clause C, and at least one of x and y falsifies C, then x ∩ y falsifies C.A Boolean formula ϕ is a read-once formula, sometimes also called a µ-formula or aBoolean tree, if every variable has at most one occurrence in ϕ, and the operations usedare ∧, ∨, and ¬. Such a formula can be represented as a binary tree where the internalnodes are labeled with ∧, ∨, and ¬ and the leaves are labeled with distinct variables orthe constants 0 or 1. (For technical reasons, we extend the standard notion, which does notallow for constants in the leaves.) The internal nodes correspond to the subformulas. Wecall a subformula of ϕ constant if it computes a constant function. A constant subformulais maximal if it is not the subformula of any constant subformula.By the de Morgan rules, we may assume that negations are applied only to variables.As we consider read-once formulas only in the deletions-only model, and thus know thesign of each variable—we can replace the negated variables with new variables (keepingin mind that every truth assignment should be handled accordingly). Thus without loss ofgenerality we can assume each variable is unnegated (i.e., we use only ∧ and ∨ in ourread-once formulas). A Boolean function is read once if it has an equivalent read-onceformula.A substitution is a partial function σ : {x1, . . . , xn} (cid:4)→ {0, 1}. Given a substitution σ , letϕσ be the formula obtained by replacing each variable xi of ϕ that is in the domain of σ byσ (xi). Substitutions σ1 and σ2 are equivalent (with respect to ϕ) if ϕσ1 and ϕσ2 computethe same function.For the lower bound on revising read-once formulas we shall use a well known notion,the Vapnik–Chervonenkis dimension [55] for Boolean functions. Let C be a set of Boolean148J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176functions on some domain X. We say that Y ⊆ X is shattered by C if for any Z ⊆ Y thereis a cZ ∈ C such that(cid:1)cZ(x) =10if x ∈ Z,if x ∈ Y \ Z.Then VC-dim(C) := max{|Y |: Y ⊆ X and Y is shattered by C is the VC-dimension of C}.4.1. Theory revision definitionsLet ϕ be a Boolean formula using the variables x1, . . . , xn. The concept represented byϕ is the set of satisfying truth assignments for ϕ. For instance, if ϕ = (x1 ∧ x2) ∨ (x1 ∧ x3),then that concept would be {110, 101, 111}.With the exception of Section 7, our revision operator is fixing an occurrence of avariable in a formula to a constant (i.e., to either 0 or 1). For instance, if we fix x2 inϕ to 1, we obtain the revised formula (x1 ∧ 1) ∨ (x1 ∧ x3), which can be simplified tox1 ∨ (x1 ∧ x3), and is equivalent to x1. If instead we fix the second occurrence of x1 to 0,we obtain the revised formula (x1 ∧ x2) ∨ (0 ∧ x3), which can be simplified to x1 ∧ x2.Because the effect of fixing a literal to a constant for DNF and CNF formulas is to deletethat literal, a clause, or a term, we also refer to this fixing of an occurrence of a variable toa constant as a deletion. Note that for read-once formulas, this instead corresponds to the“stuck-at” faults of fault detection.For read-once formulas, where there is only one occurrence of the variable(s) beingfixed, we write a revision using substitution notation, σ = (xi → ci ), where ciis aconstant. For example, applying the substitution σ = (x2 → 1, x3 → 1) to the formulaϕ2 = (x1 ∧ x2) ∨ ¬x3 gives the revised formula ϕ2σ = (x1 ∧ 1) ∨ ¬1, which simplifiesto x1.In Section 7, we also allow the addition of a variable as a revision operator. Handlingthis operator is more difficult, and the consideration of parity formulas provides an exampleof a tractable class.We denote by Rϕ the set of formulas obtained from ϕ by fixing some occurrences ofsome variables to constants. The corresponding concept class is denoted by Cϕ.The revision distance between a formula ϕ and some concept c ∈ Cϕ is defined to bethe minimum number of applications of a specified set of revision operators to ϕ neededto obtain a formula for c. Thus, for example, we showed earlier that the revision distancebetween ϕ = (x1 ∧ x2) ∨ (x1 ∧ x3) and the concept represented by x1 is 1.A revision algorithm for a formula ϕ has access to membership and equivalence oraclesfor an unknown target concept c ∈ Cϕ and must return some representation in Rϕ ofthe target concept. Our goal is to find revision algorithms whose query complexity ispolynomial in the revision distance between ϕ and the target, but at most polylogarithmicin the size of ϕ and the size of the variable set. The total running time of all our algorithmsis always polynomial in the size of ϕ, the revision distance, and the number of attributes(since of course instances must be read and written). We do not explicitly calculate exactasymptotic running times because they are typically not drastically worse than the querycomplexity (e.g., number of attributes times query complexity) and because we expect theJ. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176149query complexity, or more generally, training data, to be the constraining factor in practicalapplications.In fact, our results provide something stronger than a revision algorithm. The algorithmswe give in this paper all revise some class of concept classes. That is, our algorithms aremeta-algorithms, as they take any formula ϕ from a specified class of formulas (e.g., read-once formulas) and then function as a revision algorithm for the concept class Cϕ. Noticethat the choice of revision operator(s) plays a double role. First, it defines the concept class:all things reachable from the specified formula with the revision operator(s). Second, itdetermines the revision distance, and that gives us a performance metric.5. Revising propositional Horn sentencesIn this section, we give an algorithm for revising Horn sentences in the deletions-onlymodel. Angluin et al. [5] gave an algorithm for learning Horn sentences with queries.Their algorithm has query complexity O(nm2), where n is the number of variables andm is the number of clauses. This complexity is unacceptable for the revision task whenthe revision distance e is much smaller than the number of variables n. We give analgorithm, REVISEHORN, displayed as Algorithm 1, that has query complexity O(em3 +m4) (independent of n).In the following subsection we give more details about the algorithm; then,inSection 5.2, we give a lengthy example of a run of the algorithm. The reader may findit helpful to switch back and forth between the two subsections. The analysis of the querycomplexity and proof of correctness is in Section 5.3. In Section 5.4, we provide a lowerbound.5.1. Overview of algorithmThe highest-level structure of Algorithm REVISEHORN is similar to the structure ofAngluin et al.’s algorithm for learning Horn sentences [5] and also to our DNF revisionalgorithm [30] (after making appropriate changes for the duality between the CNF form ofHorn sentences and DNF form). The presentation in this section is self-contained; we donot assume familiarity with either of those papers.We start with the hypothesis being the empty conjunction (i.e., everything is classifiedas true) and repeatedly, in an outer loop (lines 2–22), make equivalence queries untilthe correct Horn sentence has been found.4 Each negative counterexample is used, withthe help of membership queries made in subroutine SHRINKEXAMPLE, to make thehypothesis more restrictive; each positive counterexample is used to make the hypothesismore general.We observe the following fact about negative instances, which we make implicit use ofthroughout.4 It is somewhat surprising that we start with the empty conjunction rather than the initial theory, but we havebeen unable to find a revision algorithm with good query complexity that starts with the initial theory.150J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176if h(x) == 1 then {x is a negative counterexample}1: h = empty hypothesis (everywhere true)2: while (x = EQ(h)) (cid:5)= “Correct” do3:4:5:6:x = SHRINKEXAMPLE(x, ϕ, h)for each clause-group C ∈ h in order doif body(C) ∩ x ⊂ body(C) and then MQ(body(C) ∩ x) == 0then7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:body(C) = x ∩ body(C)if head(C) (cid:5)= F thenAdd to head(C) any variable just deleted from body(C) thatis the head of some clause of ϕend ifbreak the for loopend ifend forif x wasn’t used to shrink any clause-group in h thenh = h ∧ (x → F)end ifelse {x is a positive counterexample}for each clause C of h such that C(x) = 0 doif head(C) (cid:5)= F thenDelete C from helseChange C to clause-group with heads every head of a clause inϕ that is in x \ body(C)end ifend for23:24:25:26: end while27: return hend ifAlgorithm 1. REVISEHORN. Revises Horn sentence ϕ.Proposition 1. Every negative instance of a CNF formula falsifies some clause of that CNFformula.Each negative counterexample is first processed by a subroutine called SHRINKEX-AMPLE, Algorithm 2, which we will discuss in detail shortly. In general, that subroutinemay change certain 1’s to 0’s while still leaving the negative counterexample as a negativecounterexample to the current hypothesis.Following Angluin et al., we sometimes find it convenient to organize our hypothesisby distinct clause bodies. We call the collection of all clauses in one Horn sentencethat have the same body a clause-group. (Angluin et al. called a clause-group a meta-clause.) We will use the notation (body → x1, x4, x5) to denote the clause-group withbody body and heads x1, x4, and x5, which is shorthand for the conjunction of clauses(body → x1) ∧ (body → x4) ∧ (body → x5).Algorithm REVISEHORN attempts to use the negative counterexample x returned fromSHRINKEXAMPLE to make deletions from the body of an existing hypothesis clause-group C. This can be done when first body(C) ∩ x ⊂ body(C), so that there are somedeletions to body(C) to make. We also need that body(C) ∩ x is still a negative instance.J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176151done = truefor each clause C0 ∈ ϕ do•∩ body(C0) (cid:5)= x and then MQ(xx = xdone = false•∩ body(C0)1: repeat2:3:4:if x5:6:end if7:8:end for9: until done10: return x•∩ body(C0)) == 0 thenAlgorithm 2. SHRINKEXAMPLE(x, ϕ, h).If so, then we update body(C) to body(C) ∩ x, and, if any of the variables we are deletingfrom body(C) are possible heads, then we also add those variables as heads of C. Forinstance, if we have negative counterexample x = 11000011 and the hypothesis has clause-group (x1x2x3x4 → x7, x8) then, if MQ(x ∩ x1x2x3x4) = MQ(11000000) = 0 and if x3and x4 are both heads of some initial theory clauses, then this hypothesis clause-group isupdated to (x1x2 → x3, x4, x7, x8).If there is no hypothesis clause-group body that can be edited in that way, then we makethe hypothesis more restrictive by adding a new clause to it, specifically (x → F). Noticethat the very first counterexample will always add a new hypothesis clause.Positive counterexamples are always used to make the hypothesis more general. Wemust somehow edit every hypothesis clause that is falsified by a positive counterexample.If a positive counterexample falsifies any hypothesis clause that has a head other than F,then that clause is simply deleted. In practice, this will have the effect of deleting somebut not all the heads of a clause-group with multiple heads. (That fact follows from severallemmas that we prove in Section 5.3.)If instead a positive counterexample x falsifies a clause-group C with head F, then thismeans that x covers C. In this case, C has some head(s) added to it, making it more general.In fact, we add all possible heads. Specifically, at line 22, REVISEHORN adds as heads ofthe clause-group C all heads of clauses of ϕ that correspond to 1’s in x and are not inbody(C).5.1.1. Shrinking negative examplesThe point of Algorithm SHRINKEXAMPLE is to take a negative counterexample x to thecurrent hypothesis, and to decrease the Hamming weight of x. Ideally, x should containonly 1’s in the positions corresponding to the body of the initial theory clause C0 fromwhich the target clause C∗ that x falsifies is derived. Then if we use x to introduce a newhypothesis clause, that new hypothesis clause will not have too many extraneous variablesin it. If instead we use x to make deletions from a hypothesis clause-group body, then asmaller counterexample is helpful because it produces more deletions.We make the following observation, which we will use to help explain Algo-rithm SHRINKEXAMPLE.152J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176for each clause C in h do1: answer = x ∩ y2: repeat3:4:5:6:7:8: until answer is not changedend ifend forif x both covers and satisfies C and answer falsifies C thenChange head(C) to 1 in answerAlgorithm 3. x•∩ y with respect to Horn sentence h.Proposition 2. If target formula clause C∗ is a revision of initial theory clause C0, thenbody(C∗) ⊆ body(C0).Proof. This follows because the only revision operator that we allow is deletion. (cid:1)Now notice that if negative counterexample x falsifies target clause C∗ that is arevision of some initial theory clause C0, then x ∩ body(C0) also falsifies C∗ becauseby Proposition 2, body(C∗) ⊆ body(C0). Thus, we would like to say that for each clauseC0 of the initial theory, if MQ(x ∩ body(C0)) = 0, then set x to x ∩ body(C0). However,there is one issue to which we must pay careful attention.•We need to make sure that we do not, in the process of intersecting x with initial theoryclause bodies, change x from an example that the current hypothesis classifies as positiveto one the current hypothesis classifies as negative. This is why we use the funny notation∩ C0 instead of x ∩ C0 in lines 4 and 5 of SHRINKEXAMPLE, which we now explain.xLet h be a collection of Horn clauses. The∩ operation with respect to h (which willusually be understood to be the current constructed hypothesis) is formally defined to bethe result of the pseudocode given as Algorithm 3.•The idea is that the result of x∩ y is the same as the result of x ∩ y except when thereare one or more hypothesis clauses C such that x ∩ y covers body(C) and x has a 1 in theposition head(C), in which case that 1 stays on regardless of y, for each such hypothesisclause.•Example. Imagine that the current hypothesis is h = (x1x2 → x5) ∧ (x1x2 → x6) ∧(x5x6 → x7). (Notice, by the way, that this hypothesis contains three clauses but onlytwo clause-groups.) Now 1111111 ∩ x1x2x3 = 1110000, but 111111∩ x1x2x3 = 1110111.∩ operation will set answer to 1110110, and the second to 1110111.The first loop of the•••We make an easy observation about the∩ operation, and then next we prove that the∩ operation has the property we want in terms of making sure that its output satisfies thehypothesis.•Proposition 3. For any x and y,•x ∩ y ⊆ x∩ y ⊆ x.J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176153Lemma 4. If x satisfies hypothesis h, then for any y, the instance xalso satisfies h.•∩ y with respect to hProof. There are two different ways an instance can satisfy a Horn clause: either by notcovering the clause, or by covering the clause and having the clause’s head set to 1. We∩ y,know that x satisfies every clause ch of h. If x does not cover ch, then neither can xbecause x∩ y ⊆ x.••If x does cover ch, then x has head(ch) set to 1 because x satisfies ch. Now the procedure•∩ y will satisfy ch by having head(ch) set∩ guarantees that if x∩ y covers ch, then x••forto 1. (cid:1)The other interesting thing about Algorithm SHRINKEXAMPLE is that it repeatedlyloops through all the initial theory clauses, continuing to look for deletions from x until wemake a full pass through all the initial theory clauses without changing x at all. We needthis repeated looping to guarantee a property of the output of SHRINKEXAMPLE that isproved later in Lemma 8.5.2. An example run of REVISEHORNWe now give an example run of REVISEHORN. Suppose the variable set is {x1, x2, x3,x4, x5}, and the target formula ϕ and the target formula ψ are given byϕ ≡ (x1x2x3 → x4) ∧ (x2x4 → x1) ∧ (x2x4 → x5),ψ ≡ (x1x2x3 → x4) ∧ (x2x4 → x1) ∧ (x2 → x5).Algorithm REVISEHORN always initializes the hypothesis h to the everywhere trueempty conjunction. Say EQ(h) = 11101, a negative counterexample.•∩So now we call SHRINKEXAMPLE(11101, ϕ, h). It first determines that 11101x1x2x3 = 11100 (cid:5)= 11101, so it asks the query MQ(11100) and learns that 11100 is also∩ x2x4 = 01000 (cid:5)= 11100, soa negative instance, so x is reset to be 11100. Next 11100the query MQ(01000) = 0 is made, and x is reset to 01000. Since the third initial formula∩ x2x4 = 010000. Now SHRINKEXAMPLEclause has the same body as the second, 01000∩ x1x2x3 = 01000 andbegins the second iteration of its main loop. This time 01000•••01000∩ x2x4 = 01000, so x is not altered, and the value 01000 is returned.Accordingly, the hypothesis is updated by REVISEHORN to beh = (x2 → F).The next main loop of REVISEHORN makes the equivalence query EQ(h), and this timesay EQ(h) = 11111, a positive counterexample. Since the hypothesis clause has head F, atline 22 REVISEHORN puts in all possible heads, updating the hypothesis to:h = (x2 → x1, x4, x5).•154J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176Now suppose that EQ(h) = 11001. This positive counterexample causes the hypothesisclause (x2 → x4), which it falsifies, to be deleted. The hypothesis is updated to:h = (x2 → x1, x5).•This time say EQ(h) = 11101. Now in SHRINKEXAMPLE, 11101∩ x1x2x3 = 11101(and not 11100 because 11101 would falsify the hypothesis clause (x2 → x5)). Next∩ x2x4 = 11001, and so the membership query MQ(11001) = 1 is made. Since11101that membership query returns 1, SHRINKEXAMPLE does not modify its input at all, andreturns 11101, and the hypothesis now becomes(x2 → x1, x5) ∧ (x1x2x3x5 → F).•Now say EQ(h) = 11111, a positive counterexample. We change the heads of the secondhypothesis clause-group so the hypothesis is now:(x2 → x1, x5) ∧ (x1x2x3x5 → x4).Now say EQ(h) = 01111, another positive counterexample. REVISEHORN removes x1as a head of the first hypothesis clause-group, updating the hypothesis to(x2 → x5) ∧ (x1x2x3x5 → x4).••that 01111Say this time EQ(h) = 01111. When SHRINKEXAMPLE is called, it first determines∩ x1x2x3 = 01101 and MQ(01101) = 1, so that does not change x. Next,∩ x2x4 = 01011, and MQ(01011) = 0, so x is changed to 01011. No further changes01111to x are made in SHRINKEXAMPLE, so 01011 is returned by SHRINKEXAMPLE. Nowback in REVISEHORN, x2 ∩ 01011 = x2, so editing the first hypothesis clause-group is notconsidered. Next x1x2x3x5 ∩ 01011 = x2x5, so the membership query MQ(01001) = 1 istried, but since it returns 1, the second hypothesis clause-group is also not edited. Instead,a new clause-group is added, giving the hypothesis(x2 → x5) ∧ (x1x2x3x5 → x4) ∧ (x2x4x5 → F).Now say EQ(h) = 11011. Then REVISEHORN will use this positive counterexample tochange the third clause-group, and we will arrive at(x2 → x5) ∧ (x1x2x3x5 → x4) ∧ (x2x4x5 → x1).Finally, EQ(h) = “Correct”. Notice, by the way, that the final correct hypothesis doesnot have exactly the same form as we stated, but is equivalent to it via resolution.5.3. Horn revision algorithm correctnessOnce we have established that Algorithm REVISEHORN halts, its correctness followsfrom its form. We prove a bound on its query complexity using a series of lemmas.Several of these lemmas involve proving that some property of the hypothesis isinvariant. We point out here that there are only four places where the hypothesis is everchanged: one place where hypothesis clause-group bodies are created, one where they canbe altered, and two places where the set of heads of a clause-group can be altered. OneJ. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176155is using a positive counterexample to edit the hypothesis in lines 18–24 of REVISEHORN.The other is moving a clause-group body variable into the head of the clause-group atLine 9 of REVISEHORN.We begin with an observation about the heads of the hypothesis clauses. We thenprove several facts about SHRINKEXAMPLE, which is at the heart of making the querycomplexity independent of the number of variables n.Proposition 5. Every head of a hypothesis clause-group other than F is a head of someinitial theory clause.We record in the following lemma the fact that x remains a negative counterexample tothe current hypothesis after it is modified in SHRINKEXAMPLE.Lemma 6. If x is a negative instance satisfying Horn sentence h, then the instance returnedby SHRINKEXAMPLE(x, ϕ, h) is also a negative instance satisfying h.Proof. As the algorithm proceeds, x is modified to be xafter a membership query guarantees that xreturned instance will be negative.∩ body(C0) only immediately∩ body(C0) is a negative instance. Thus the•Lemma 4 says that if x satisfies h, then so does x•∩ y for any y. (cid:1)•Next, before proceeding to bound the query complexity of the entire REVISEHORNalgorithm, we bound the query complexity of the SHRINKEXAMPLE algorithm.Lemma 7. Algorithm SHRINKEXAMPLE makes at most O(m2) membership queries, wherem is the number of clauses in the initial theory ϕ.Proof. Each iteration of the outer repeat until loop makes at most one query for eachclause in ϕ. To prove the lemma, we will prove that there are at most 2m + 1 iterations ofthe outer repeat until loop.Each time there is an iteration of the outer loop, x must be altered. The only way that xis ever altered is by changing 1’s to 0’s. For a given initial formula clause C0 ∈ ϕ, once weset x = x∩ body(C0), we know that we have set to 0 every position of x that is not eitherin body(C0) or the head of some hypothesis clause.•Thus, x can be altered at most once for each head of a hypothesis clause, plus once foreach initial theory clause. The heads of the hypothesis clauses are a subset of the headsof the initial theory clauses, so there are at most m of them, as there are m initial theoryclauses. Thus x can be altered at most 2m times, so the outer loop can execute at most2m + 1 times, as desired. (cid:1)Now we show how the output of SHRINKEXAMPLE is connected to the notion ofrevision of the initial formula.Lemma 8. Let x be the output from SHRINKEXAMPLE. For every target clause C∗ that xfalsifies, for each initial theory clause C0 such that C∗ is a revision of C0, any position that156J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176is a 1 in x either corresponds to a variable in body(C0) or corresponds to a head of someinitial theory clause.Proof. Assume for contradiction that for some such C0 and C∗ that x contains a 1 in aposition that is neither in body(C0) nor a head of an initial theory clause. Consider thefinal iteration of the outer repeat until loop of Algorithm SHRINKEXAMPLE. Note thatx is unchanged in the final iteration of the algorithm. We will derive a contradiction byshowing that this x would be changed.∩ body(C0) (cid:5)= x. If we can show thatBy our assumption about x, we know that x•∩ body(C0) is a negative instance of the target, we have our contradiction, because thenxthis x would be modified at line 5 of SHRINKEXAMPLE, forcing another iteration of theouter repeat until loop.••Now x falsifies C∗, so x already has head(C∗) set to 0. Therefore, x∩ body(C0) also∩ body(C0) covers C∗, then we have shownhas head(C∗) set to 0. If we now show that x∩ body(C0) is a negative instance, and we are done. Because x falsifies C∗, wethat xhave that x covers C∗. Since C∗ is a revision of C0, we have body(C∗) ⊆ body(C0). ByProposition 3, x ∩ body(C0) ⊆ x∩ body(C0) covers C∗. (cid:1)∩ body(C0), so x••••Now we move on to show that every clause-group body falsifies at least one clause ofthe target Horn sentence, and that no target clause is falsified by more than one clause-group body. We first prove the first of these two facts, and then prove some lemmas wewill need to prove the second.Lemma 9. Each clause-group body in the hypothesis always falsifies some clause of thetarget concept.Proof. The body of the clause-group is always a negative instance of the target. This is truewhen the clause-group is first added at line 15 of SHRINKEXAMPLE by Lemma 6, and thisis maintained as an invariant because it is guaranteed by a membership query immediatelybefore changing a clause-group body at line 7 of REVISEHORN. (cid:1)Lemma 10. For every hypothesis clause-group C with head other than F, for every targetclause C∗ that body(C) falsifies, head(C∗) is always one of the heads of C.Proof. When created, every hypothesis clause-group has head F.When we first change the clause-group C’s head from F, we know by Lemma 9that body(C) falsifies at least one target clause. Also, we know from the existence of acounterexample that covers body(C) and is classified by the target as positive that thetarget clauses that are falsified by body(C) must have a head other than F. At this pointwe put in all possible heads. When we delete a variable from a clause-group body, if itis a possible head, we add it. We remove a head only when a positive counterexampleguarantees that it must be removed. (cid:1)J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176157From this lemma we can show that no clause-group in the hypothesis is ever altogetherdeleted from the hypothesis, although it may be revised in various ways.Corollary 11. No hypothesis clause-group, once introduced, is ever deleted.Proof. The only way that this could potentially happen would be if at line 20 ofREVISEHORN we removed the last clause (i.e., head) of a particular hypothesis clause-group C.Consider hypothesis clause-group C. If head(C) = F, then it has the one head F, andthere is no operation to remove it.If C has head(s) other than F, then by Lemma 9, body(C) falsifies some target clauseC∗. Now, by Lemma 10, one of the heads of clause-group C is head(C∗). There cannot beany positive counterexample falsifying the hypothesis clause whose body is body(C) andwhose head is head(C∗), so that head of clause-group C is never deleted. (cid:1)Lemma 12. For a given hypothesis clause-group C, no variable is ever added as headmore than once.Proof. Heads are initially added once when a clause-group head is first changed from Fat line 22 of REVISEHORN. After that happens, the clause-group will never have head Fagain. Thereafter, heads are added when they are deleted from the body. Because there areno additions made to hypothesis bodies, these heads could not previously have been headsof that hypothesis clause; they were always in the body. Once deleted from the body, theyare never restored. (cid:1)Lemma 13. No two hypothesis clause-group bodies ever falsify the same target clause.Proof. We follow the proof in Angluin et al. [5] of an analogous statement about theiralgorithm for learning Horn sentences from scratch.We first show that the following claim implies the lemma, and then prove the claim.Claim. Consider the clause-group bodies b1, b2, . . . , bh of hypothesis h in the order added.For any j , if bj falsifies target clause C∗, then no bi with i < j covers C∗.Assume that the claim is true, but nevertheless the bodies of hypothesis clause-groupsCk and C(cid:6) both falsify the target clause C∗, and WLOG, k < (cid:6). This contradicts the claim,since body(Ck) falsifies C∗.Now we prove that the claim is true by induction on the number of changes made tothe hypothesis. It is certainly vacuously true of the initial empty hypothesis. We mustshow that this property remains invariant whenever we alter the hypothesis. Positivecounterexamples do not change the set of clause-group bodies, so we need consider onlynegative counterexamples.Consider first the case of modifying a clause-group body bj at line 7 of Algorithm RE-VISEHORN by setting bj = x ∩ bj , where bj = body(Cj ). After this modification, bj can-not cover more clauses of the target formula than before, so we need worry only about158J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176clause-groups bi with i < j . Suppose for contradiction that bj now falsifies some newtarget clause C∗, and bi covers C∗, with i < j . It must be that before this change that bjcovered C∗ and so x falsified C∗. Now we have that bi ∩ x falsifies C∗, because bi coversC∗ and x falsifies C∗. Therefore x would have been used to edit bi in the for loop at lines 5–13 of REVISEHORN, as long as bi ∩ x ⊂ bi . What happens if bi ∩ x = bi ? Since bi ∩ x = bifalsifies C∗, we have that bi falsifies C∗. By Lemma 10, bi’s clause-group either has headF or has head(C∗) among its heads. Therefore x does not satisfy bi ’s clause-group, be-cause x covers bi and x falsifies C∗. Lemma 6 says that x must satisfy (every clause of)the hypothesis.Next, consider the case of adding a new clause-group with body b = x at line 15 ofAlgorithm REVISEHORN, where again x was returned by Algorithm SHRINKEXAMPLE.Suppose for contradiction that b falsifies C∗, and that hypothesis clause-group body bicovers C∗. Since b = x falsifies C∗ and bi covers C∗, we have that bi ∩ x falsifies C∗, sothe if statement at line 6 should have directed the algorithm to use x to edit bi as long asbi ∩ x ⊂ bi . If instead bi ∩ x = bi , then bi falsifies C∗, and again by Lemma 10, it must bethat b does not satisfy bi ’s clause-group, contradicting the assumption that x satisfies thehypothesis. (cid:1)Theorem 14. Algorithm REVISEHORN uses at most O(m3e + m4) queries to revise a Hornsentence containing m clauses and needing e revisions.Proof. First, remember that, by Corollary 11, once a particular clause-group is added, it isnever deleted. By Lemmas 9 and 13, the number of clause-groups is at most the number ofclauses in the target formula. Thus in the worst case one clause-group C is introduced intothe hypothesis for each target clause C∗.Let us consider how many queries that one clause-group C can generate over the lifetimeof the algorithm. Its creation required O(m2) queries for the call to SHRINKEXAMPLE forthe negative counterexample, plus O(m) in the main code of REVISEHORN.Next, consider the manipulation of heads in the clause-group. There can be at most mheads introduced to a clause (plus F). By Lemma 12, each of them can be removed ormoved exactly once. Each such edit uses O(1) queries.Finally, consider the use of negative counterexamples to edit the body of the clause-group C. By Lemma 7, each such negative counterexample may cost O(m2) queries. Wewill get our overall query bound by showing that the number of edits to the body of C is atmost O(m + e).At any point in the run of the algorithm, body(C) falsifies (at least one) target clauseC∗, by Lemma 9. By Lemma 8, the variables in body(C) fall into three categories:1. Those in body(C∗) (which should not be deleted).2. Variables that are heads of some initial theory clause.3. Variables that are in the initial theory clause C0 from which C∗ is derived, but are notin C∗. That is, the variables that need the revision.Now there are at most m heads of initial theory clauses, and there are at most e variablesthat need to be deleted.J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176159This is not quite the whole proof, however. Lemma 9 says that body(C) must alwaysfalsify the body of some target clause, but it does not say that it must always be the sametarget clause.A negative counterexample may cause body(C) to change which target clause bodyit falsifies. We now argue that this can happen only m − 1 times, because once body(C)ceases to falsify a particular target clause, it can never again in its life falsify that targetclause. This is so because the only way that the clause-group body could stop falsifyingtarget clause C∗ would be by having some variable in body(C∗) deleted from the clause-group, and once a variable is deleted from a clause-group body it is never put back in.Moreover, the m + e edits of body(C) accounted for in items 2 and 3 above are thetotal for the entire life of clause-group C, not just for the period while clause-group C isassociated with one particular target clause. This is so because m is the total number ofheads of initial theory clauses that might ever have to be deleted from C in its lifetime,and again, once one of those heads is deleted it is never replaced. Similarly, e is an upperbound on the total number of deletions to be made from all initial theory clauses, and onceone of those variables is deleted, it is never replaced.Since there are up to m hypothesis clause-groups, the total algorithm requires O(em3 +m4) queries. (cid:1)5.4. A lower bound on revising Horn sentencesIn this subsection, we give a lower bound on the query complexity of revising Hornsentences. The argument shows that in general we cannot escape some dependence on thenumber of clauses in the initial formula. We give a Horn sentence where (cid:1)(m) queries arerequired to make a single deletion revision. Note that we will not have to specify the targetfunction in advance, because, as is usual with adversary arguments, we need only makesure that all our adversary’s responses are consistent with some target function.The technical argument is very similar to our lower bound on revising DNF [30], heretransformed for the CNF form of Horn sentences.Consider the variables x1, . . . , xn, y1, . . . , yn and let ϕn = c1 ∧ · · · ∧ cn, where, fori = 1, . . . , n,ci = (x1 · · · xi−1xi+1 · · · xn ∧ yi → F).Theorem 15. The formula ϕn requires at least n − 1 membership and equivalence queriesto be revised, if each equivalence query must be a conjunction of Horn clauses, with eachHorn clause body the revision of some body of a clause in ϕn, even if it is known thatexactly one literal yi is deleted.Proof. We describe how an adversary can answer the queries of any possible revisionalgorithm in a way that forces the revision algorithm to make the claimed number ofqueries. Let ψi be the formula obtained from ϕn by deleting the single occurrence ofvariable yi . Initially any concept in Ψ = {ψ1, . . . , ψn} is a possible target concept, andthe adversary strategy that we describe will eliminate at most one concept from Ψ perquery made by the revision algorithm. This implies the claimed lower bound.160J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176Let us use ordered pairs (x, y) to denote truth assignments to the 2n variables, wherethe first component x will be the truth assignment to the xi ’s and the second componentthe truth assignment to yi ’s.A membership query (x, y) is answered as follows. If x has at most n − 2 bits that are 1,then MQ(x, y) = 1. This does not eliminate any concepts from Ψ . If x has n − 1 bits thatare 1 with position xi = 0, then MQ((x, y)) = ¯yi . If yi = 1 then this does not eliminate anyconcept from Ψ . If yi = 0 then ψi is eliminated from Ψ . If x = 1 then MQ((x, y)) = 0.This does not eliminate any concept from Ψ .Now consider an equivalence query EQ(θ ), where θ is a conjunction of Horn clauses,and for each clause C of θ , we have that body(C) is a revised version of the body of someclause of ϕn.If θ contains any clause C with at most n − 2 of the xi’s in it, then return the positivecounterexample that has a 1 for every position of body(C), and 0’s elsewhere. This doesnot eliminate any concept from Ψ .If θ has no clause with at most n − 2 of the x’s, but contains at least one clause Ci withbody(Ci ) being all the x’s except xi (and no y), then return the positive counterexamplethat has a 1 for every position of body(C), and 0’s elsewhere. This eliminates only conceptψi from Ψ .The final possibility is that every clause in θ has exactly n − 1 of the x’s in it togetherwith the corresponding y. (This case includes the case where θ = ϕn.) In this case, returnthe negative counterexample 1n0n. This does not eliminate any concept from Ψ . (cid:1)6. Revising read-once formulasIn this section we present a revision algorithm for the class of read-once formulas, andlower bounds showing that the algorithm is close to optimal. In the first subsection we givesome preliminaries for the revision algorithm. This is followed by the description of thealgorithm, its analysis and a detailed example. The final subsection gives the lower bounds.6.1. Sensitization, subformulasOur revision algorithm uses the technique of path sensitization from fault analysisin switching theory (see, e.g., Kohavi [39]). Assume that we would like to revise themonotone read-once formulaϕ = (ϕ1 ∨ ϕ2) ∧ ϕ3,and let the target formula beψ = (ψ1 ∨ ψ2) ∧ ψ3,where ψ is obtained from ϕ by replacing certain variables by constants. Consider the partialtruth assignment α that fixes all the variables in ϕ2 to 0, and all the variables in ϕ3 to 1.This fixing of the variables is called sensitizing ϕ1, and α is called the sensitizing partialtruth assignment for ϕ1. Form two vectors x0 and x1 by fixing the remaining variables to0, respectively, to 1, and ask the membership queries MQ(x0) and MQ(x1).There are three possibilities.J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–1761611. If MQ(x1) = 0, then it must be the case that either ψ1(1) = 0, in which case ψ1 isidentically 0, or ψ3(1) = 0, in which case the whole target formula is identically 0.2. If MQ(x0) = 1, then it must be the case that either ψ1(0) = 1, in which case ψ1 isidentically 1, or ψ2(0) = 1, in which case ψ2 is identically 1.3. For the revision algorithm it is important to notice that we can also gain information inthe third case, when MQ(x0) = 0 and MQ(x1) = 1. In this case we do not observe any“abnormality”, but we can conclude that for every truth assignment y to the variablesof ψ1 it holds that ψ1(y) = MQ(y, α). Thus we can simulate membership queries tothe subformula ψ1 by membership queries to the target concept, and this enables therevision algorithm to proceed by recursion. Also note that in this case it is still possiblethat ψ2(1) = 0 and/or ψ3(0) = 1.Now we give the general definition of a sensitizing partial truth assignment. Let ϕ(cid:14) bea subformula of ϕ. Consider the binary tree representing ϕ, and let P be the path leadingfrom the root of ϕ to the root of ϕ(cid:14). Then ϕ can be written asϕ = (· · · (ϕ(cid:14) ◦r ϕr ) ◦r−1 · · · ◦3 ϕ3) ◦2 ϕ2) ◦1 ϕ1,(1)where ϕ1, . . . , ϕr are the subformulas corresponding to the siblings of the nodes of P , and◦1, . . . , ◦r are either ∧ or ∨. In this representation we used the commutativity of ∧ and∨; in general ϕ(cid:14) need not be a leftmost subformula of ϕ. Let ψ be obtained from ϕ byreplacing certain variables by constants. Then, as in (1), we can write ψ as(cid:14) ◦r ψr ) ◦r−1 · · · ◦3 ψ3) ◦2 ψ2) ◦1 ψ1.ψ = (· · · (ψ(2)Definition 16. Let ϕ be a read-once formula with subformula ϕ(cid:14). Write ϕ as in Eq. (1).Let the sets of variables occurring in ϕi be Xi , and the set of variables occurring in ϕ(cid:14)be Y . Since ϕ is read-once, these sets form a partition of {x1, . . . , xn}. Now let α be thepartial truth assignment that assigns 1 (respectively, 0) to every variable in Xi if ◦i is∧ (respectively, ∨), for every i = 1, . . . , r. Then α is called the partial truth assignmentsensitizing ϕ(cid:14).Generalizing the remarks above, let α be the partial truth assignment sensitizingϕ(cid:14). Form the truth assignments x0 = (0, α) (respectively x1 = (1, α)) that extend α byassigning 0 (respectively 1) to the variables occurring in ϕ(cid:14). Now, if MQ(x1) = 0, thenit follows by the monotonicity of ψ that either ψ (cid:14) or a subformula ψi such that ◦i = ∧ isconstant 0. In this case, the whole subformula corresponding to (· · · (ψ (cid:14) ◦r ψr ) ◦r−1 · · · ◦i−1ψi−1) ◦i ψi in the target must be constant 0; thus this whole subformula can be deleted andreplaced by 0. The case is similar when MQ(x0) = 1. On the other hand, when MQ(x1) = 1and MQ(x0) = 0, we can be sure that for any partial truth assignment y of the variables inψ (cid:14), we have ψ (cid:14)(y) = MQ((y, α)). This means that ψ (cid:14) is not part of a constant subformula.These remarks are summarized in the following lemma, which is used several times lateron without mentioning it explicitly.Lemma 17. (a) Let ϕ be the initial formula, ϕ(cid:14) be a subformula of ϕ, let ψ, ψ (cid:14) be thetarget formula, respectively, its subformula corresponding to ϕ(cid:14), and let α be the partial162J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176truth assignment sensitizing ϕ(cid:14). Then ψ (cid:14) is part of a constant subformula if and only ifMQ(0, α) = 1 or MQ(1, α) = 0. Otherwise ψ (cid:14)(y) = MQ(y, α) for every truth assignmenty of the variables in ϕ(cid:14).(b) If ψ (cid:14) is a maximal constant subformula and ◦i is ∧ (respectively ∨), then ϕi(1) = 1(respectively ϕi(0) = 0) for every i = 1, . . . , r.In the rest of this subsection we formulate some useful properties of subformulas. Twosubformulas are siblings if the corresponding nodes in the tree representation are siblings.The next lemma follows directly from the definitions.Lemma 18. Two maximal constant subformulas cannot be siblings.The revision algorithm proceeds by finding maximal constant subformulas, thus it isimportant to know that identifying these is sufficient for learning.Lemma 19. Substitutions σ1 and σ2 are equivalent for formula ϕ if and only if the maximalconstant subformulas of ϕσ1 and ϕσ2 are identical.Proof. If the maximal constant subformulas are identical, then after replacing them withthe corresponding constants, one obtains the same formula. Thus the if direction of thelemma holds. For the only if direction, assume that σ1 and σ2 are equivalent for ϕ, butthe maximal constant subformulas are not identical. There are two cases. The first case iswhen there is a subformula ϕ(cid:14) of ϕ that turns into a maximal constant subformula in bothϕσ1 and ϕσ2, but ϕ(cid:14)σ1 ≡ 0 and ϕ(cid:14)σ2 ≡ 1. Let α be the partial truth assignment sensitizingϕ(cid:14). Then (ϕσ1)(1, α) = 0, while (ϕσ2)(1, α) = 1, contradicting the assumption that σ1 andσ2 are equivalent. In the second case there is a subformula which is maximal constant forone substitution, but not for the other. Let ϕ(cid:14) be a largest such subformula. We may assumew.l.o.g. that ϕ(cid:14)σ1 is a maximal constant subformula, which computes the constant 0, andϕ(cid:14)σ2 is not part of a constant subformula. Then ϕσ1(1, α) = 0 and ϕσ2(1, α) = 1, againcontradicting the assumption that σ1 and σ2 are equivalent. (cid:1)the formula ϕ that has maximal constantCorollary 20. By finding a revision ofsubformulas identical to those of the target formula, we get a formula equivalent to thetarget formula.The following lemma can be proved by a simple algorithm that uses recursion on thestructure of the formula ϕ.Lemma 21. Given a read-once formula ϕ and a constant c, one can find a substitution σsuch that ϕσ = c and σ fixes a minimal number of variables, in polynomial time.Let ϕ be a read-once formula with subformula ϕ(cid:14). We say that ϕ(cid:14) is an approximatelyhalf-size subformula of ϕ if it contains at least one-third and at most two-thirds of thevariables in ϕ. It is a standard fact that such a subformula exists (see, e.g., Wegener [56]).J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–1761631: while (x = EQ(ϕ)) (cid:5)= “correct” doσ = FINDCONSTANT(ϕ, x)2:ϕ = ϕσ3:4: end whileAlgorithm 4. Algorithm REVISEREADONCE(ϕ).For example, any minimal subformula that contains at least one-third of the variables hasthis property.If ϕ is a read-once formula with subformula ϕ(cid:14), then the ϕ(cid:14)-partition of a truthassignment x is (x1, x2), where x1 contains the values in x for all the variables in ϕ(cid:14),and x2 contains the values in x for all the variables in ϕ that are not in ϕ(cid:14).6.2. The revision algorithmNow we formulate the main result of this section, for Algorithm REVISEREADONCE(Algorithm 4), which revises read-once formulas in the deletions-only model of revisions.Theorem 22. Algorithm REVISEREADONCE uses at most O(e log n) queries to revise aread-once formula containing n variables and needing e revisions.Proof. Algorithm REVISEREADONCE consists of a loop that checks whether the targethas been found and if not calls FINDCONSTANT. In each call of FINDCONSTANT byREVISEREADONCE, we identify a maximal constant subformula of the target formula ψ,and we find a substitution that fixes this subformula to the appropriate constant value. Themaximal constant subformula is then eliminated, thus the updated formula contains fewervariables. As the membership queries always refer to truth assignments to the original setof variables, the new membership queries have to assign some values to the eliminatedvariables as well. The construction implies that these variables are irrelevant, thereforetheir values can be arbitrary.FINDCONSTANT, displayed as Algorithm 5, is a recursive procedure, which takesa formula ϕ and a counterexample x, and returns a substitution σ . The substitutionfixes a subformula to a constant c. It always holds that the subformula is a maximalconstant subformula computing the constant c in any representation of the target concept.5FINDCONSTANT works recursively, always focusing on a faulty subformula (i.e., asubformula which contains some variable(s) replaced by a constant) of the previous level’sformula. This subformula may never be a proper subformula of a constant subformula—that is, it is part of a constant subformula if and only if it itself is a maximal constantsubformula. We assume this property holds at the beginning of every recursion level, andwe maintain it as we go deeper in the recursion. This guarantees that we eventually find5 In several places in the proof we will say that a property holds for any representation of the targetconcept. Notice that this must be true, as all the information used by the algorithm comes from membershipand equivalence queries about the target, and the responses to such queries are independent of the particularrepresentation.164J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176return substitution σ fixing ϕ to the appropriate constantreturn substitution σ fixing it to constant ¬ϕ(x)1: if ϕ has one variable then2:3: end if4: if MQ(0) == 1 or MQ(1) == 0 then5:6: end if7: ϕ(cid:14) = an approximately half-size subformula of ϕ8: α = the partial truth assignment sensitizing ϕ(cid:14)9: if (MQ(0, α) == MQ(1, α) == c) thenreturn GROWFORMULA(ϕ, ϕ(cid:14), c)10:11: else12:-partition of x and x2 = (x2,1, . . . , x2,r ) corresponding(x1, x2) = the ϕ(cid:14)to subformulasif MQ(x1, α) (cid:5)= ϕ(cid:14)(x1) thenFINDCONSTANT(ϕ(cid:14), x1)i = FINDFORMULA(ϕ, ϕ(cid:14), x)FINDCONSTANT(ϕi, x2,i )else13:14:15:16:17:18:19: end ifend if// look in ϕ(cid:14)Algorithm 5. The procedure FINDCONSTANT(ϕ, x).a maximal constant subformula. Once such a subformula is found, we use Lemma 21 toreturn an appropriate substitution.As we go deeper in the recursion, we will need the ability to ask membership queriesconcerning only a subformula of the target. Therefore, when we go to a lower recursionlevel with a subformula ϕ(cid:14) of ϕ, we determine α, the partial truth assignment sensitizingϕ(cid:14). This way, whenever a need for a membership query arises on the lower level for a truthassignment y, we need only ask MQ(y, α). Recursion only occurs when MQ(0, α) = 0and MQ(1, α) = 1, thus we can be sure that MQ(y, α) is equal to the value of ψ (cid:14)(y),where ψ (cid:14) is the subformula of the target formula corresponding to ϕ(cid:14). From now on, whentalking about membership queries, we always assume that this technique is used. We writeMQ(y) instead of MQ(y, α), where α is the partial truth assignment sensitizing the currentsubformula.Now we give a detailed description of FINDCONSTANT, by explaining what it doeson one level of the recursion: how it finds an appropriate faulty subformula, and howit maintains the counterexample x so it can be carried down into the next level as acounterexample. The correctness of the algorithm follows from this discussion directly.The complexity analysis requires only one point to be considered in detail. This is done inLemma 23 at the end of the proof.Lines 1–3: We check whether the current subformula ϕ consists of a single variable.If it does (say ϕ = vi ), then—since we know that ϕ is not a proper part of any constantsubformula, but ϕ is faulty—we can be sure that ϕ is a maximal constant subformula; thusthe substitution vi → c, where c := ¬ϕ(x), will give the appropriate maximal constantsubformula.From now on we can assume that the input formula has more than one variable.J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176165Lines 4–6: FINDCONSTANT examines whether MQ(0) = 0 and MQ(1) = 1. If not,then the whole subformula is identically true or false. Since ϕ has the property that itis not properly contained in a constant subformula, ϕ itself must be a maximal constantsubformula.Lines 7–8: We now know that ϕ is not part of a constant subformula. We determine anapproximately half-size subformula ϕ(cid:14) of ϕ, and its sensitizing partial truth assignment α.Lines 9–10: We check if MQ(0, α) = MQ(1, α) = c. If that is the case, then MQ(y, α) =c for any partial truth assignment y to the variables in ϕ(cid:14). Thus ψ (cid:14) is a constant subformula,and so it is in a maximal constant subformula that is properly contained in ψ. At this pointwe do not perform any further recursive calls. The only task left is to find the node onthe path P from the root of ψ to the root of ψ (cid:14) that is the root of that maximal constantsubformula. Procedure GROWFORMULA does this. As GROWFORMULA implements asimple binary search, we give only a brief description, without displaying its pseudocode.The procedure gets as input a read-once formula ϕ, a subformula ϕ(cid:14), and a constant csuch that MQ(0, α) = MQ(1, α) = c. Using O(log n) membership queries it outputs amaximal subformula containing ϕ(cid:14) such that the corresponding subformula is identicalto the constant c in any representation of the target.We now assume that c = 1; the case c = 0 is dual. Using the notation of Definition 16,let αi for i = 0, . . . , r be the partial truth assignment that is identical to α for X1, . . . , Xi ,leaves the variables in Y unassigned, and assigns 0 to all the other variables. Then (0, 0) =(0, α0) (cid:1) (0, α1) (cid:1) (0, α2) (cid:1) · · · (cid:1) (0, αr ) = (0, α), and it holds that MQ(0, α0) = 0 andMQ(0, αr ) = 1.Asking membership queries MQ(0, αj ), we can use binary search to find an i (1 (cid:1)i (cid:1) r) such that MQ(0, αi−1) = 0 and MQ(0, αi) = 1. The only difference between thetruth assignments (0, αi−1) and (0, αi) is that the variables in Xi are off in (0, αi−1) andthey may be on in (0, αi ). In fact, they must be on, as otherwise (0, αi−1) = (0, αi). Italso follows that ◦i is ∧. Thus, on one hand, it must be the case that ψi (0) = 0 andψi (1) = 1 in any representation of the target concept. On the other hand, it must be thecase that the input to ◦i from its child on the path P is equal to 1 in both cases. As thevariables in this subformula are all set to 0, this subformula must compute the constant 1function. The inputs (0, αi−1) and (0, αi ) demonstrate that no larger subformula computesa constant function. Thus the subformula rooted at ◦i−1 is a maximal constant subformula.This completes the discussion of the procedure GROWFORMULA.Lines 11–12: If we get to line 11 then we know that ψ (cid:14) is not part of a constant subfor-mula, so we must continue the recursion to find one within ψ (cid:14). Using counterexample x,we form the ϕ(cid:14)-partition of x in line 12. In the remainder of the procedure we find a faultysubformula that has at most two-thirds of the variables in ϕ.Lines 13–14: Since α is the partial truth assignment sensitizing ϕ(cid:14), we have ϕ(x1, α) =ϕ(cid:14)(x1). Furthermore MQ(x1, α) = ψ (cid:14)(x1), because ψ (cid:14) is not part of a constant subformula.If MQ(x1, α) (cid:5)= ϕ(x1, α) then ϕ(cid:14)(x1) (cid:5)= ψ (cid:14)(x1), thus ϕ(cid:14) contains a maximal constantsubformula. Thus we can carry on finding some faulty parts that contribute to the faultyevaluation on x by the recursive call FINDCONSTANT(ϕ(cid:14), x1).Lines 15–17: The only way we could get to this point is if MQ(x1, α) = ψ (cid:14)(x1) =ϕ(cid:14)(x1) = d, and there are some faults in a subformula ϕi of ϕ for some i ∈ {1, 2, . . ., r}.Let x2 = (x2,1, x2,2, . . . , x2,r ), where x2,i is the part of x2 containing the variables in Xi .166J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176Let yi (respectively zi ) be the value computed at ◦i in ϕ (respectively ψ) on the inputvector x, for i = 1, . . . , r. Furthermore let yr+1 = d = zr+1. Thenyi = yi+1 ◦i ϕi(x2,i),and zi = zi+1 ◦i ψi(x2,i)for i = 1, . . . , r. Since x was a counterexample to EQ(ϕ), it holds that y1 = ϕ(x) (cid:5)= ψ(x) =z1.Since yr+1 = zr+1 and y1 (cid:5)= z1, there must be an i (1 (cid:1) i (cid:1) r) for which yi+1 = zi+1but yi (cid:5)= zi . Now let us assume that we know this special i (the next paragraph describesthe procedure FINDFORMULA for finding it). Then it follows that ϕi is faulty, and thatx2,i is a counterexample to the equivalence of ϕi and ψi . This means that we can carryon with the search for the faulty subformula in ϕi . This can be done by a recursive callfor FINDCONSTANT, using x2,i as the counterexample. As before, in the recursion we cansimulate any assignment y to the variables in ψi by MQ(y, ˜α), where ˜α is the partial truthassignment sensitizing ϕi in ϕ (since ϕ(cid:14) is not part of a constant subformula, neither is ϕi ,thus the answer for this query will indeed give us the value ψi (y)).The search for the appropriate index i is done by procedure FINDFORMULA usinga weighted binary search as follows. The yi values can be computed using ϕ withoutany queries. For the computation of the zi , let βi be the partial truth assignment thatassigns x2,j to the variables in Xj for j = i, . . . , r and otherwise is identical to α. Thenzi = MQ(x1, βi), since, as ϕ(cid:14) is not contained in any constant subformula, there are noconstant subformulas on the path ◦1, . . . , ◦r .j ∈I wj . In each step we have to find an index (cid:6) for whichLet nj denote the number of variables in ϕj , and define the weight of this subformulato be wj = nj −1 + nj (j = 2, . . . , r). In the binary search we use an interval I = [a, b].Initially a = 2 and b = r, as we already know y1, z1, yr+1 and zr+1. For a given I let s =(cid:2)(cid:6)j =a wj(for this we don’t need to ask any queries). We determine y(cid:6) and z(cid:6) (this can be doneusing one query). If y(cid:6) (cid:5)= z(cid:6), then let I = [(cid:6) + 1, b], otherwise let I = [a, (cid:6) − 1]. If I isnonempty, we compute s again, and continue the search. Otherwise the search is over, andif y(cid:6) (cid:5)= z(cid:6), then (cid:6) is the i index we were looking for, otherwise it is (cid:6) − 1. This completesthe description and the analysis of the revision algorithm.(cid:6)−1j =a wj < s/2 (cid:1)(cid:2)(cid:2)Since in each iteration we find a maximal constant subformula, and then we find aminimal substitution to fix the value computed by this subformula to the appropriateconstant, it follows that FINDCONSTANT is called at most e times. The claimed complexitybound then follows from the following lemma.Lemma 23. When called by LEARNREADONCE, Procedure FINDCONSTANT uses at mosta total of O(log n) membership queries.Proof. The general idea of the proof is that the more queries consumed by FINDFOR-MULA, the smaller will be the recursive call to FINDCONSTANT.Let us examine how procedure FINDFORMULA works. Let u be the number of variablesin subformula ϕ on a level of the recursion. Since ϕ(cid:14) is an approximately half-sizerj =2 2 · uj ) −subformula of ϕ,w1 − wr < 2 · u · (2/3) = u · (4/3). The value of s will reduce to less than its half in eachiteration of the search, so after k queries s will be less than 1/2k times its initial value.(cid:2)rj =1 uj (cid:1) u · (2/3); thus initially s =j ∈I wj = ((cid:2)(cid:2)J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176167Thus it will be at most u · (4/3) · (1/2k). We also know that if it is the index i that shouldbe returned, then until the last query, the weight of ϕi or the weight of ϕi−1 appears in s.But they both contain ui , thus before the last query we have s (cid:2) ui . In summary, if we getthe index i in t iterations, then we used t queries, and the number of variables in ϕi is ui (cid:1)u · (4/3) · (1/2t −1) = u/(3 · 2t −3). Thus using t (cid:1) log(u/ui) + 3 − log 3 < log(u/ui ) + 2queries we managed to restrict the location of the faulty subformula to ϕi containing uivariables. Thus on this recursion level we had to use fewer than K + 2 + log(u/ui) queries,where K is the number of queries needed in lines 4, 9 and 13.The other way to enter the next recursion level is through line 14, which does not needany additional queries above K. Furthermore at the bottom of recursion we need at mostO(log u) queries (lines 1–10).Note that on every level of the recursion the size of the subformula is at most two-thirdsof the size at the previous level. Thus, denoting the size of the formula on the ith levelof recursion by mi , we have at most q = log2/3 m0 levels of recursion, and on each level(excluding the final one) we use at most K + 2 + log(mi/mi+1) queries. Adding them up,we get that in one run of FINDCONSTANT we use(cid:4)(cid:4)(cid:3)(cid:3)+ · · · +K + 2 + log+ O(log mq ) = O(log m0)mq−1mqK + 2 + logqueries. (cid:1)m0m1The proof of Lemma 23 concludes the proof of the theorem. (cid:1)6.3. Example run of revision algorithm for read-once formulasHere is a detailed example showing how the read-once revision algorithm works. Letthe formula to be revised beϕ = ((y1 ∧ y2) ∨ (y3 ∧ y4)) ∧ ((((y5 ∧ y6) ∨ y7) ∧ y8) ∨ y9)and the substitution giving the target formula beσ = (y3 → 1, y5, y6, y8 → 0).Thus the target concept is represented by the formulaψ = ((y1 ∧ y2) ∨ (1 ∧ y4)) ∧ ((((0 ∧ 0) ∨ y7) ∧ 0) ∨ y9).(3)We start by asking the equivalence query EQ(ϕ). Let us assume that we receive the negativecounterexample x = 110011110. In Procedure FINDCONSTANT, the membership queriesMQ(0) = 0 and MQ(1) = 1 bring us to line 7. At this point we find an approximatelyhalf-size subformula, for exampleϕ(cid:14) = (y1 ∧ y2) ∨ (y3 ∧ y4).The corresponding subformula of the target is ψ (cid:14) = (y1 ∧ y2) ∨ (1 ∧ y4).Now we form the sensitizing truth assignment α for ϕ(cid:14), which in this case simply setsall variables not in ϕ(cid:14) to 1, and we ask membership queries for (0, α) and for (1, α). Theanswer is MQ(0, α) = 0 and MQ(1, α) = 1, and thus we continue on line 12. We have168J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176x1 = 1100 and x2 = 11110. By asking the membership query MQ(x1, α) we find thatψ (cid:14)(x1) = 1. Knowing ϕ, we can determine without asking any queries that ϕ(cid:14)(x1) = 1. Asψ (cid:14)(x1) = ϕ(cid:14)(x1), it follows that the x2 part of the counterexample is responsible for thedisagreement between ϕ(x) and ψ(x). In this particular case, the variables in x2 happento induce a subformula of ϕ, and so FINDFORMULA does not need to do anything. Wesubstitute 1 for ϕ(cid:14). Then x2 = 11110 is a negative counterexample for the new target,which is the subformula ψ (cid:14)(cid:14) of the target corresponding toϕ(cid:14)(cid:14) = ((((y5 ∧ y6) ∨ y7) ∧ y8) ∨ y9).It is important to note that as ψ (cid:14)(cid:14)(y) = ψ(x1, y), we can simulate membership queries tothe new target by membership queries to the original target; thus we can continue the sameprocedure recursively.As the subsequent iterations illustrate additional cases, we give further steps of thealgorithm on the example. In the next call, which is FINDCONSTANT(ϕ(cid:14)(cid:14), x2), we again getto line 7. The next half size subformula can be y5 ∧ y6. The sensitizing truth assignmentfor this subformula is 010. Now, the membership queries to (00, 010) and (11, 010) bothreturn 0, indicating that either y5 ∧ y6 or some subformula containing it is turned into theconstant 0. Thus we call GROWFORMULA, which asks the additional membership queriesMQ(11, 110) = 0 and MQ(11, 111) = 1. This shows that(((y5 ∧ y6) ∨ y7) ∧ y8)is a maximal constant 0 subformula in ϕ(cid:14)(cid:14). No further recursive calls are needed, weonly need to compute the minimal number of variables that, when turned to 0, make thesubformula identically 0. This can be achieved by the single substitution y8 → 0. Now wehave completed one call of the procedure FINDCONSTANT by the main program.The next call of FINDCONSTANT start with an equivalence query for the formulaobtained by the substitution just found, that is,(cid:14)(cid:14)(cid:14) = ((y1 ∧ y2) ∨ (y3 ∧ y4)) ∧ y9.ϕLet us assume that we receive the positive counterexample 000111111, which, restrictedto the five variables in ϕ(cid:14)(cid:14)(cid:14), is 00011. We continue with the half size subformula y1 ∧ y2,which divides the counterexample into 00 and 011. The sensitizing partial truth assignmentto the first half is 001. We find that MQ(00, 001) = 0 and MQ(11, 001) = 1, thus y1 ∧ y2is not turned into a constant subformula. (Notice that our only membership oracle needsinputs from {0, 1}9; fortunately, we may give any values to the “missing” variables.) Themembership query MQ(00, 001) = 0 tells us that the first half of the counterexamplegives the same output in y1 ∧ y2 and in the corresponding subformula of the target. Torecurse, we must find a subformula of ϕ(cid:14)(cid:14)(cid:14) that contains some constant subformula, but thethree variables y3, y4, and y9 do not induce a subformula of ϕ(cid:14)(cid:14)(cid:14). This is achieved by theprocedure FINDFORMULA.In this case we need consider only the two subformulas y3 ∧ y4 and y9, though ingeneral there could be (cid:1)(n) such subformulas, necessitating the binary search performedby FINDFORMULA. By definition, ϕ(cid:14)(cid:14)(cid:14) disagrees with the target on the counterexample,and we have just concluded that y1 ∧ y2 agrees with the counterexample. So, if subformula(y1 ∧ y2) ∨ (y3 ∧ y4) of ϕ(cid:14)(cid:14)(cid:14) disagrees with the corresponding subformula of the target,J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176169then the subformula containing a constant subformula must be y3 ∧ y4. Otherwise it isy9. To test whether the subformula (y1 ∧ y2) ∨ (y3 ∧ y4) agrees with the target on thecounterexample, we ask a membership query on an instance formed by setting y1, y2, y3,and y4 to the values they have in the counterexample, and setting the remaining variable(y9) to the value it had in the sensitizing assignment for y1 ∧ y2. That, is we make thequery MQ(00011) = 1. Since ϕ(cid:14)(cid:14)(cid:14)(00011) = 0, which disagrees with the target, there mustbe a constant subformula in y3 ∧ y4, which is the input subformula for the next call toFINDCONSTANT.That call will identify the substitution y3 → 1, and the next equivalence query to theformula((y1 ∧ y2) ∨ y4) ∧ y9will finally identify the target concept. Notice that we have actually revised fewer variablesthan given in Eq. (3). The number of variables revised is as small as possible for obtainingthe target concept.6.4. Lower bounds on revising read-once formulasWe prove a lower bound to the query complexity of revising read-once formulasby giving an example of an n-variable read-once formula, for which (cid:1)(e log(n/e))equivalence and membership queries are required to find a distance e revision. If e =O(n1−ε) for some fixed ε > 0, then this lower bound is of the same order of magnitude,as the upper bound provided by REVISEREADONCE. It is also shown that both typesof queries are needed for efficient revision. There are n-variable read-once formulas forwhich at least n/2 equivalence queries are required in order to find a single revision. Formembership queries we present an even stronger lower bound, which shows that at leastn − e membership queries may be necessary, if (instead of not using equivalence queries atall) one is allowed to use fewer than e equivalence queries. As REVISEREADONCE usesexactly e equivalence queries to find a distance e revision, this means that just by allowingone fewer equivalence query, the number of membership queries required becomes linear.Bshouty and Cleve and Bshouty et al. [16,17] give somewhat related constructions andtradeoff results for different query types.Our first two lower bounds are based on read-once formulas of the form(xi ∧ yi),using a VC-dimension, respectively an adversary argument, and the third lower bounduses an adversary argument for the n-variable disjunction.(cid:5)Theorem 24. The complexity of revising read-once formulas in the deletion-only model is(cid:1)(e log ne ), where n is the number of variables in the initial formula and e is the revisiondistance between the initial formula and the target formula.Proof. Let us assume thatn = 2me, where m = 2t .170J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176We use variables xi,j and yi,j , where 1 (cid:1) i (cid:1) e and 0 (cid:1) j (cid:1) m − 1. The initial formula isϕn =e(cid:6)m−1(cid:6)i=1j =0(xi,j ∧ yi,j ).Assume the x and y variables be arranged in respective e × m matrices called X and Y ,respectively. We look at the class of revisions of ϕn where in each row of the matrix Xexactly one variable is fixed to 1. Let the corresponding concept class be Cn.Lemma 25. VC-dim(Cn) (cid:2) e · t.Proof. For 1 (cid:1) k (cid:1) e and 1 (cid:1) (cid:6) (cid:1) t let(Xk,(cid:6), Yk,(cid:6))be a truth assignment that consists of all 0’s, with the exception of some positions in thekth row of the Y matrix: namely, those positions (k, j ), where the (cid:6)th bit of the binaryrepresentation of j is 1. Let the set of these assignments be S. We claim that S is shatteredby Cn.Consider a subset A ⊆ S. For every k (1 (cid:1) k (cid:1) e) let ak be the t-bit number describingwhich truth assignments (Xk,(cid:6), Yk,(cid:6)) belong to A. (That is, the (cid:6)th bit of ak is 1 iff(Xk,(cid:6), Yk,(cid:6)) ∈ A.) We look at the revision ϕA for which it is the akth variable which isfixed to 1 in row k of the matrix X.It remains to show that this revision classifies S in the required manner. If (Xk,(cid:6), Yk,(cid:6)) ∈A, then bit (cid:6) of ak is 1. By definition, Yk,(cid:6) has a 1 at position (k, ak). In ϕA, the variablexk,ak is fixed to 1. These observations imply thatϕA(Xk,(cid:6), Yk,(cid:6)) = 1.On the other hand, if (Xk,(cid:6), Yk,(cid:6)) /∈ A, then bit (cid:6) of ak is 0. The only 1 components of(Xk,(cid:6), Yk,(cid:6)) are in row k of the Y matrix: these are those positions (k, j ), where the (cid:6)thbit of the binary representation of j is 1. Position (k, ak) is not one of those. Thus thecorresponding x-variables are not fixed to 1 in ϕA, and as their value is 0, we getϕA(Xk,(cid:6), Yk,(cid:6)) = 0.(cid:1)By introducing dummy variables if n is not of the right form, we getVC-dim(Cn) (cid:2) elog(cid:7)(cid:8).n2eThe theorem now follows from the general result that the VC-dim(Cn) provides a lowerbound to the number of equivalence and membership queries required to learn Cn up to aconstant factor, even if the equivalence queries are not required to be proper [11,42]. (cid:1)(cid:9)(cid:10)neThe number of formulas within revision distance e of a given read-once formula is atmost 2e ·. Thus if we allow equivalence queries which are not necessarily proper, thenby using the standard halving algorithm [41] one can learn a revision using log(2e ·) =O(e log n) many equivalence queries. We now show that such a result is not possible if thequeries are required to be proper.(cid:10)ne(cid:9)J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176171Theorem 26. The complexity of revising read-once formulas in the deletion-only modelwith proper equivalence queries is at least (cid:16)n/2(cid:17), where n is the number of variables in theinitial formula, even if we assume that a single revision occurs.Proof. We use the initial formula(cid:16)n/2(cid:17)(cid:6)(x2i−1 ∧ x2i).ϕn =i=1(4)Variables in the same conjunction are called partners. The revisions considered fix exactlyone variable to 1. Let the formula obtained from ϕn by fixing xj to 1 be ϕjn , and let theclass C(cid:14)n. We describe an adversary strategy that forces everylearner to use at least (cid:16)n/2(cid:17) equivalence queries.n consist of the formulas ϕjIt may be assumed that the hypotheses are consistent with the previous counterexam-ples, otherwise one of the previous counterexamples can be returned again. Let us assumethat the learner asks an equivalence query EQ(ψ).If both a variable and its partner is fixed to 1 in ψ (i.e., ψ ≡ 1), then return 0 as anegative counterexample. This does not rule out any concept from C(cid:14)nOtherwise, if some variable xj is fixed to 0 in ψ then return the positive counterexamplewhich is all 0’s, except that xj and its partner have value 1. Again, this does not rule outany concept from C(cid:14)n.Otherwise, if a variable xj is fixed to 1 in ψ but its partner is not, then return the negativecounterexample which is all 0’s except that the partner of xj has value 1. This rules out theformula ϕjn .Finally, there remains the case when ψ is the initial formula (ψ does not have to be thefirst query). In this case the adversary looks at the set of formulas ϕjn which are not ruled outyet. If there are more formulas with j even (respectively, odd), then it returns the positivecounterexample 101010 . . . (respectively, 010101 . . .). This rules out all the formulas ϕjnwith j odd (respectively, even), but it does not rule out any with j even (respectively, odd).n, and all the other querieseliminate at most one concept. As long as there is more than one concept which is notruled out, the learning process cannot terminate, and thus the lower bound follows. (cid:1)The last query eliminates at most (cid:16)n/2(cid:17) concepts from C(cid:14)Now we present a lower bound for the case when only membership queries are allowed.Actually, we consider a more general scenario, where the learner is allowed to ask a limitednumber of equivalence queries. In particular, we assume that the learner is told in advancethat the target is at revision distance e from the initial theory, and the number of equivalencequeries allowed is at most e − 1.Theorem 27. The number of membership queries required for revising read-once formulasin the deletion-only model is at least n − e, where n is the number of variables in the initialformula, e is the revision distance between the initial formula and the target formula,assuming that the number of equivalence queries is fewer than e.172J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176Proof. We start from the initial formula x1 ∨ · · · ∨ xn, and we consider the class C(cid:14)(cid:14)n ofrevisions which fix exactly e variables to 0. The adversary maintains a partition (D, U, Q)of the variables, where D stands for deleted, U stands for undeleted and Q stands for ?.In the beginning D = U = ∅ and Q = {x1, . . . , xn}. In the course of the learning processit always holds that every concept from C(cid:14)(cid:14)n for which every variable in D is deleted, andno variable in U is deleted, is consistent with the previous answers. This implies that thelearner cannot identify the target as long as |D| < e and |D ∪ Q| > e.For a membership query MQ(x) we consider three cases. If xi = 1 for some i ∈ U ,then MQ(x) = 1 and the sets are not changed. Otherwise, if xi = 1 for some i ∈ Q, thenMQ(x) = 1, and the variable xi is moved from Q to U . Otherwise, MQ(x) = 0 and thesets are not changed.For an equivalence query EQ(ψ), we consider the following cases. If ψ is identically 1(respectively, 0) then the all 0 (respectively, all 1) vector is given as a negative (respectively,positive) counterexample, and the sets are not changed. If there is a variable xi ∈ Q in ψ,then the vector which is all 0’s except for xi is given as a negative counterexample, and xiis moved from Q to D. Otherwise, the characteristic vector of Q is returned as a positivecounterexample, and the sets are not changed.Initially |D| = 0, and |D| is increased only by an equivalence query. As there can befewer than e equivalence queries, |D| is always less than e. Thus the learning processcan only terminate by achieving |D ∪ Q| = e. But initially |D ∪ Q| = n, and its size isdecreased only by a membership query. Therefore at least n − e membership queries areneeded. (cid:1)7. Revising parity in the general revision modelSo far we considered only errors corresponding to the deletion of literals and terms. Inpractical theory revision algorithms one also has to deal with other types of errors suchas the replacement of a variable with another one, or the addition of a variable or a term.Some of these error types are hard to define in general, and one has to be careful with theirdefinition in particular cases (see, e.g., [10,40]). Replacements and additions appear to beharder to handle than deletions.Let the variables x1, . . . , xn be given. A parity function is the exclusive—or of a subsetof the variables, or the complement of such a function. Thus a parity function can bespecified by giving a (cid:20)u ∈ {0, 1}n, and an a ∈ {0, 1}, and writing the parity function ϕas ϕ(x) = (cid:20)u · x ⊕ a, where · denotes the mod 2 inner product of two vectors. Thus(cid:20)u · x =mod 2.(cid:9)(cid:2)(cid:10)ni=1 uixiGiven a parity function, we now allow the deletion of a variable, the replacement of avariable by a constant or another variable, the addition of a variable, and for parity, alsothe addition of the constant 1. Given a parity function ϕ, we denote by Rϕ the class ofparity functions that can be obtained from ϕ using the enlarged set of revision operators,and we denote by Cϕ the corresponding concept classes. Thus, Cϕ is the class of all parityfunctions over the variables x1, . . . , xn. In these cases, unlike the rest of the paper, theconcept classes do not depend on the initial formula. The only role played by the revisionJ. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176173operator is to determine the performance metric: if the target concept can be obtained witha few revisions then we have to identify it with few queries.Theorem 28. There is a revision algorithm for parity functions in the general model ofrevisions, using O(e log n) queries, where e is the revision distance between the initial andthe target concept.Proof. Let ϕ(x) = (cid:20)uϕ · x ⊕ a be the parity function to be revised, and let ψ(x) = (cid:20)uψ · x ⊕ bbe the target concept.Since ψ(0) = b, the value of b can be determined with the single equivalence queryMQ(0). If a (cid:5)= b then we change ϕ to its complement to achieve a = b, and if a = b = 1then we reverse labels. Thus it may be assumed that a = b = 0.The vectors (cid:20)uϕ and (cid:20)uψ differ in at most 2d bits.The revision algorithm starts with the equivalence query ϕ. Let x be the counterexamplereceived for this query. As a = b = 0, it holds that x (cid:5)= 0. Our goal now is to find acounterexample containing exactly one 1. Let x1 and x2, be obtained from x by switchingoff respectively the first or second half of the 1 components in x. Notice that x = x1 ⊕ x2,and soϕ(x1) ⊕ ϕ(x2) = ϕ(x1 ⊕ x2) = ϕ(x)(cid:5)= ψ(x) = ψ(x1 ⊕ x2)= ψ(x1) ⊕ ψ(x2),so exactly one of ϕ(x1) (cid:5)= ψ(x1) and ϕ(x2) (cid:5)= ψ(x2) hold. Thus exactly one of x1 and x2 isa counterexample, and one membership query will tell us which one is the counterexample.Continuing this process, a counterexample with a single 1 component can be found withO(log n) membership queries. The variable corresponding to the 1 component must be oneof the variables where ϕ and ψ differ. Hence ψ can be found by repeating this procedureO(e) times. (cid:1)8. Concluding remarksTheory revision is important because when we already know a theory close to thedesired theory, learning from scratch is wasteful, that is, needlessly expensive. Thiswhole area has received relatively little theoretical study. We have presented here efficientalgorithms for Horn and read-once formulas under the deletions-only revision model. Wehave given tight bounds on such revisions of read-once formulas. In addition, we have givenan algorithm and tight bounds for the general revision of parity formulas. These resultsprove that in at least one formal model, there are efficient theory revision algorithms.Additional results on revising various forms of DNF formulas may be found in ourcompanion paper [30]. Work on revising Valiant’s class of projective DNF functions [53]may be found in Sloan et al. [49].The work presented here by no means exhausts the area of theory revision from thelearning theory point of view. There are numerous open problems; for instance, the revisionof threshold formulas, and the revision of Horn formulas in the general model of revision.174J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176AcknowledgementsPreliminary versions of many of the results in this paper appeared in the threeconference papers [28,29,50], and in a master’s thesis of the third author [51].We thank Jignesh Doshi for careful readings of preliminary versions of this manuscript.Robert Sloan wishes to thank NSF for providing him with some time for his own researchwhile he was working there.References[1] H. Aizenstein, T. Heged˝us, L. Hellerstein, L. Pitt, Complexity theoretic results for query learning,Computational Complexity 7 (1998) 19–53.[2] H. Aizenstein, L. Hellerstein, L. Pitt, Read-thrice DNF is hard to learn with membership and equivalencequeries, in: Proc. of the 33rd Symposium on the Foundations of Computer Science, Pittsburgh, PA, IEEEComputer Society Press, Los Alamitos, CA, 1992, pp. 523–532.[3] D. Angluin, Learning propositional Horn sentences with hints, Technical Report YALEU/DCS/RR-590,Department of Computer Science, Yale University, New Haven, CT, 1987.[4] D. Angluin, Learning regular sets from queries and counterexamples, Inform. Comput. 75 (2) (1987) 87–106.[5] D. Angluin, Queries and concept learning, Machine Learning 2 (4) (1988) 319–342.[6] D. Angluin, Negative results for equivalence queries, Machine Learning 5 (1990) 121–150.[7] D. Angluin, M. Frazier, L. Pitt, Learning conjunctions of Horn clauses, Machine Learning 9 (1992) 147–164.[8] D. Angluin, L. Hellerstein, M. Karpinski, Learning read-once formulas with queries, J. ACM 40 (1) (1993)185–210.[9] D. Angluin, M. Kharitonov, When won’t membership queries help?, J. Comput. System Sci. 50 (2) (1995)336–355. Earlier version appeared 23rd STOC, 1991.[10] S. Argamon-Engelson, M. Koppel, Tractability of theory patching, J. Artificial Intelligence Res. 8 (1998)39–65.[11] P. Auer, P.M. Long, Structural results about on-line learning models with and without queries, MachineLearning 36 (3) (1999) 147–181.[12] A. Blum, L. Hellerstein, N. Littlestone, Learning in the presence of finitely or infinitely many irrelevantattributes, J. Comput. Syst. Sci. 50 (1) (1995) 32–40. Earlier version in: Proc. of the Fourth Annual Workshopon Computational Learning Theory (COLT 1991), Santa Cruz, CA, 1991, pp. 157–166.[13] A. Blum, S. Rudich, Fast learning of k-term DNF formulas with queries, J. Comput. System Sci. 51 (3)(1995) 367–373.[14] N. Bshouty, Exact learning Boolean function via the monotone theory, Inform. Comput. 123 (1995) 146–153.[15] N. Bshouty, L. Hellerstein, Attribute-efficient learning in query and mistake-bound models, J. Comput.System Sci. 56 (3) (1998) 310–319.[16] N.H. Bshouty, R. Cleve, On the exact learning of formulas in parallel, in: Proc. of the 33rd Symposium onthe Foundations of Computer Science, Pittsburgh, PA, IEEE Computer Society Press, Los Alamitos, CA,1992, pp. 513–522.[17] N.H. Bshouty, S.A. Goldman, T.R. Hancock, S. Matar, Asking questions to minimize errors, J. Comput.System Sci. 52 (2) (1996) 268–286. Earlier version in: Proc. of the Sixth Annual ACM Conference onComputational Learning Theory (COLT 1993), Santa Cruz, CA, 1993, pp. 41–50.[18] L. Carbonara, D. Sleeman, Effective and efficient knowledge base refinement, Machine Learning 37 (2)(1999) 143–181.[19] P. Damaschke, Adaptive versus nonadaptive attribute-efficient learning, Machine Learning 41 (2) (2000)197–215.[20] L. De Raedt, Logical settings for concept-learning, Artificial Intelligence 95 (1997) 187–201.[21] R. Dechter, J. Pearl, Structure identification in relational data, Artificial Intelligence 58 (1992) 237–270.J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176175[22] A. Dhagat, L. Hellerstein, PAC learning with irrelevant attributes, in: Proc. of the 35rd Annual Symposiumon Foundations of Computer Science, Santa Fe, NM, IEEE Computer Society Press, Los Alamitos, CA,1994, pp. 64–74.[23] W.F. Dowling, J.H. Gallier, Linear-time algorithms for testing the satisfiability of propositional Hornformulae, J. Logic Programming 1 (1984) 267–284.[24] R. Feldman, Probabilistic revision of logical domain theories, PhD Thesis, Cornell University, Ithaca, NY,1993.[25] M. Frazier, Matters Horn and other features in the computational learning theory landscape: The notionof membership, PhD Thesis, Dept. of Computer Science, University of Illinois at Urbana-Champaign,Technical Report UIUCDCS-R-94-1858, 1994.[26] M. Frazier, L. Pitt, Learning from entailment: An application to propositional Horn sentences, in: Proc.Tenth International Conf. Machine Learning, Amherst, MA, Morgan Kaufmann, San Mateo, CA, 1993,pp. 120–127.[27] O. Goldreich, S. Goldwasser, S. Micali, How to construct random functions, J. ACM 33 (4) (1986) 792–807.[28] J. Goldsmith, R.H. Sloan, More theory revision with queries (extended abstract), in: Proceedings of theThirty-Second Annual ACM Symposium on Theory of Computing, Portland, OR, May 21–23, 2000, ACMPress, New York, 2000, pp. 441–448.[29] J. Goldsmith, R.H. Sloan, B. Szörényi, G. Turán, Improved algorithms for theory revision with queries, in:Proc. 13th Annual Conference on Computational Learning Theory, Palo Alto, CA, Morgan Kaufmann, SanFrancisco, CA, 2000, pp. 236–247.[30] J. Goldsmith, R.H. Sloan, G. Turán, Theory revision with queries: DNF formulas, Machine Learning 47 (2/3)(2002) 257–295.[31] R. Greiner, The complexity of revising logic programs, J. Logic Programming 40 (1999) 273–298.[32] R. Greiner, The complexity of theory revision, Artificial Intelligence 107 (1999) 175–217.[33] V. Gurvich, On repetition-free Boolean functions, Uspekhi Mat. Nauk 32 (1) (1977) 183–184 (in Russian).[34] A. Horn, On sentences which are true on direct unions of algebras, J. Symbolic Logic 16 (1951) 14–21.[35] M. Karchmer, N. Linial, I. Newman, M. Saks, A. Wigderson, A combinatorial characterization of read-onceformulae, Discrete Math. 114 (1993) 275–282.[36] M. Kearns, M. Li, L. Pitt, L. Valiant, On the learnability of Boolean formulae, in: Proc. 19th Annual ACMSymp. Theory of Computing (STOC 1987), New York, ACM Press, New York, 1987, pp. 285–294.[37] M. Kearns, L. Valiant, Cryptographic limitations on learning Boolean formulae and finite automata,J. ACM 41 (1) (1994) 67–95.[38] H. Kleine Buning, T. Lettmann, Propositional Logic: Deduction and Algorithms, Cambridge UniversityPress, Cambridge, 1999.[39] Z. Kohavi, Switching and Finite Automata Theory, second edition, McGraw-Hill, New York, 1978.[40] M. Koppel, R. Feldman, A.M. Segre, Bias-driven revision of logical domain theories, J. ArtificialIntelligence Res. 1 (1994) 159–208.[41] N. Littlestone, Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm,Machine Learning 2 (4) (1988) 285–318.[42] W. Maass, G. Turán, Lower bound methods and separation results for on-line learning models, MachineLearning 9 (1992) 107–145.[43] J.A. Makowsky, Model theory and computer science: An appetizer, in: S. Abramsky, D.M. Gabbay,T.S.E. Maibaum (Eds.), Handbook of Logic in Computer Science, vol. 1 (Background: MathematicalStructures), Oxford University Press, Oxford, 1992, pp. 763–814.[44] J.C.C. McKinsey, The decision problem for some classes without quantifiers, J. Symbolic Logic 8 (1943)61–76.[45] R.J. Mooney, A preliminary PAC analysis of theory revision, in: T. Petsche (Ed.), Computational LearningTheory and Natural Learning Systems, vol. III: Selecting Good Models, MIT Press, Cambridge, MA, 1995,pp. 43–53, Chapter 3.[46] D. Mundici, Functions computed by monotone Boolean formulas with no repeated variables, Theoret.Comput. Sci. 66 (1989) 113–114.[47] D. Ourston, R.J. Mooney, Theory refinement combining analytical and empirical methods, ArtificialIntelligence 66 (1994) 273–309.176J. Goldsmith et al. / Artificial Intelligence 156 (2004) 139–176[48] K. Pillaipakkamnatt, V. Raghavan, Read-twice DNF formulas are properly learnable, in: ComputationalLearning Theory: EuroColt ’93, in: The Institute of Mathematics and its Applications Conference Series,vol. 53, Oxford University Press, Oxford, 1994, pp. 121–132.[49] R.H. Sloan, B. Szörényi, G. Turán, Projective DNF formulae and their revision, in: Learning Theory andKernel Machines, 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel2003, Washington, DC, August 24–27, 2003, Proceedings, in: Lecture Notes in Artificial Intelligence,vol. 2777, Springer, Berlin, 2003, pp. 625–639.[50] R.H. Sloan, G. Turán, On theory revision with queries, in: Proc. 12th Annual Conf. on ComputationalLearning Theory, Santa Cruz, CA, ACM Press, New York, 1999, pp. 41–52.[51] B. Szörényi, Revision algorithms in computational learning theory, Master’s Thesis, Dept. of ComputerScience, University of Szeged, 2000 (in Hungarian).[52] G.G. Towell, J.W. Shavlik, Knowledge-based artificial neural networks, Artificial Intelligence 70 (1/2)(1994) 119–165.[53] L.G. Valiant, Projection learning, Machine Learning 37 (2) (1999) 115–130.[54] M.H. Van Emden, R.A. Kowalski, The semantics of predicate logic as a programming language, J. ACM 23(1976) 733–742.[55] V.N. Vapnik, A.Y. Chervonenkis, On the uniform convergence of relative frequencies of events to theirprobabilities, Theor. Probab. Appl. 16 (2) (1971) 264–280.[56] I. Wegener, The Complexity of Boolean Functions, Wiley-Teubner, 1987.[57] P.H. Winston, T.O. Binford, B. Katz, M. Lowry, Learning physical descriptions from functional definitions,examples, and precedents, in: Proc. Natl. Conf. on Artificial Intelligence (AAAI-83), Washington, DC, 1983,pp. 433–439.[58] S. Wrobel, Concept Formation and Knowledge Revision, Kluwer, Dordrecht, 1994.[59] S. Wrobel, First order theory refinement, in: L. De Raedt (Ed.), Advances in ILP, IOS Press, Amsterdam,1995, pp. 14–33.