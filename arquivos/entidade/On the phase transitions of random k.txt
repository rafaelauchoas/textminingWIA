Artificial Intelligence 175 (2011) 914–927Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintOn the phase transitions of random k-constraint satisfaction problemsYun Fan a, Jing Shen a,b,∗a Department of Mathematics, Central China Normal University, Wuhan, 430079, Chinab School of Science, Naval University of Engineering, Wuhan, 430033, Chinaa r t i c l ei n f oa b s t r a c tArticle history:Received 29 July 2010Received in revised form 11 November 2010Accepted 11 November 2010Available online 13 November 2010Keywords:Constraint satisfaction problemPhase transitionConstraint satisfaction has received increasing attention over the years. Intense researchhas focused on solving all kinds of constraint satisfaction problems (CSPs). In this paper,first we propose a random CSP model, named k-CSP, that guarantees the existence ofphase transitions under certain circumstances. The exact location of the phase transitionis quantified and experimental results are provided to illustrate the performance ofthe proposed model. Second, we revise the model k-CSP to a random linear CSP byincorporating certain linear structure to constraint relations. We also prove the existenceof the phase transition and exhibit its exact location for this random linear CSP model.© 2010 Elsevier B.V. All rights reserved.1. IntroductionConstraint satisfaction problem, or CSP in short, is represented by a finite set of variables, each one of which is associatedwith a domain, and a finite set of constraints, each of which consists of a subset of the variables, called a constraint scope, anda constraint relation that restricts the values of the variables in the constraint scope can simultaneously take. The objectiveis to assign a value to each variable satisfying all the constraint relations. CSP is an important topic in the area of computerscience, especially in artificial intelligence, since the regularity in their formulation provides a common base to analyze andsolve the problems of many unrelated families. In recent years, the random CSP and the corresponding phase transitionshave attracted more and more attention since Cheeseman et al. proposed in [7] that many hard instances should be foundat the phase transition points.There have been various models for investigating the phase transitions of random CSP proposed by various academiccommunities, e.g. [2,10–12,17–19,27–29]. The initial standard models, named A, B, C and D [23,28], were proposed togenerate random binary CSP instances. Experiments showed that the standard models [23,28] all exhibit a “threshold-like”behavior. On the other hand, it has been proved theoretically by Achlioptas et al. in [1] that the random instances generatedby the standard models do not have an asymptotic threshold when the length of constraint scopes and the size of domainsare fixed.Improvement of the performance of standard models was addressed from various perspectives in numerous efforts[1,21–23,26,31]. Some new models incorporated special combinatorial structures on the constraints. In other words, theconstraints are subject to certain combinatorial restrictions and the restrictions ensure that the generated instances arearc consistent [23], path consistent [21], strongly 3-consistent [22] or weakly 4-consistent [22]. It has been proved that allthese revised models have non-trivial asymptotic behaviors. While the combinatorial structures provide the capability forproducing phase transitions, this achievement typically comes at the price of more restrictions on the constraint relationsof instances.Based on the model B [28] mentioned previously, Xu and Li [31] proposed a random CSP model, named model RB.Instead of fixing the size of domains associated with each instance as in the model B, the size of domains of the model RB* Corresponding author at: Department of Mathematics, Central China Normal University, Wuhan, 430079, China.E-mail addresses: yunfan02@yahoo.com.cn (Y. Fan), shendina@hotmail.com (J. Shen).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.11.004Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927915is uniform for each instance and the value of the size is variant as a power function of the cardinality of the set of variables,i.e. the number of variables. On the other hand, the length of constraint scopes and the tightness of constraint relationsfor each instance of the model RB are fixed. It has been proved theoretically in [1] that model B does not have the phasetransition point over most of the parameter space. Due to incorporating uniformly variant domain size, the revised modelRB does have phase transitions and the exact phase transition points have been quantified by Xu and Li [31]. Moreover,it has been demonstrated that the model RB has a lot of hard instances existing [32] and all the instances at the phasetransition points have exponential tree-resolution complexity [30]. Compared with revised models in [21–23], the capabilityof generating instances is dramatically enhanced for the model RB due to the fact that there is no combinatorial restrictionenforced on the constraint relations of model RB. Generally speaking, it is natural and relatively easier for the model RB togenerate asymptotically non-trivial CSP instances with relatively large domain size.Another area of active research in the field of CSP is the development of k-SAT, where k denotes the length of theconstraint scopes. It is proved by Friedgut in [17] that a phase transition exists for k-SAT if k is fixed. However, for fixed kwith k (cid:2) 3, there is still no effective method to obtain the exact location of the phase transition. For example, it is alreadyderived theoretically in [24] and [14] that the best lower bound and upper bound of the phase transition for 3-SAT are3.53 and 4.506 respectively; but the exact location is still under investigation. When compared with the results for k-SATwith fixed k, it has been demonstrated that it is possible for k-SAT to ascertain the exact location of phase transitions if theparameter k is growing moderately, as detailed in [16,20].Motivated by k-SAT with growing k in [20], in this paper first we revise the model RB in [31] to propose a new randomCSP model, named model k-CSP. When comparing with the model RB in [20], instead of fixing the parameters of constraints,including both the length of constraint scopes and the tightness of constraint relations, and varying the size of domains asa power function, for the new model k-CSP we assume that the size of domains and the tightness of constraint relations arefixed and the length of constraint scopes, which is denoted by k, is variant as a function of n, where n denotes the cardinalityof the set of variables. Although similar to k-SAT, the new proposed model k-CSP has growing length of constraint scopes,the two models are essentially different from each other, more specifically, the tightness of constraint relations is variant fork-SAT while is fixed for the model k-CSP. For the new model k-CSP we theoretically prove the existence of a phase transitionwhen the parameter k grows up to a logarithm function of n, and determine the exact location of the phase transition point.Further, we experimentally demonstrate the performance of the proposed model k-CSP. The experiments we conducted onthe k-CSP not only verify the theoretical results we established, but also illustrate that the computational complexity of thek-CSP grows exponentially with n (the number of variables) and the worse-cases happen around the phase transition point.We note that, the model k-CSP can generate instances as easily as the model RB since there is no other restrictions on theconstraint relations except the fixed tightness; on the other hand, the parameter k of the model k-CSP is growing up veryslowly as a logarithm function rather than the power function appearing in the model RB. In summary, the model k-CSPcan easily and naturally generate asymptotically non-trivial CSP instances within a reasonably small range of domain sizeand constraint scope length, thus it is very suitable for testing the capability of CSP algorithms.The algebraic CSP, which employs algebraic structures to the domains and the constraint relations of CSP model, isanother popular approach to construct a CSP model. We note that the algebraic CSP approach has received considerableattention in recent years [4]. One classical example of algebraic CSPs is the linear CSP, which domains are finite fields andconstraint relations are affine subspaces of the vector spaces over the finite fields. One of the major advantages of variouslinear CSP models [3,5,6,8,9,13,15] is that they all exhibit satisfiability thresholds. This motivates our other research inconstructing a random CSP model that combines the model k-CSP mentioned above with the linear CSP model.To combine the advantages of linear CSP and the model k-CSP, we incorporate certain algebraic structure to the domainsand constraint relations of k-CSP and then introduce another type of random linear CSP model, named k-hyper-F-linearCSP. For each instance of the new proposed model, we assume that the domain could be any finite field, which is denotedby F; the constraint relations are randomly chosen from the hyperplanes of the vector space Fk, where k is the lengthof constraint scopes. Similar to the model k-CSP, the length of constraint scopes k is uniformly variant as a function of n,where n denotes the number of variables. We exhibit theoretically the exact phase transition of the model k-hyper-F-linear CSP. When comparing with the linear CSP models from [3,5,6,8,9,13], we provide a more general formulation anda new proof based on a more general argument, which make the k-hyper-F-linear CSP model more widely applicable inpractice.This paper is organized as follows. Section 2 states some preliminary definitions, introduces the random model k-CSPand presents the main theorem on the exact phase transition of the model k-CSP. Section 3 provides the complete proof ofthe theorem stated in Section 2. Section 4 summarizes our experimental results and analyzes the performance of the modelk-CSP. Section 5 proposes the model k-hyper-F-linear CSP and quantifies the exact phase transitions of the proposed model.Conclusions are provided in Section 6.2. Random model k-CSPIn this paper, ln x = loge x denotes the natural logarithm function where e denotes the natural base, exp x = ex denotesthe natural exponential function, and H(x) = −x ln x − (1 − x) ln(1 − x) for x ∈ [0, 1] denotes the natural entropy function.By |T | for any set T we denote the cardinality of the set T .A constraint satisfaction problem, CSP in short, is described as follows:916Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927Instance. A triple I = ( X, D, C) where• X = (x1, . . . , xn) is a sequence of n variables;• D = (D1, . . . , Dn) is a sequence of finite sets, called the domains of the instance;• C = (C1, . . . , Ct ) with each Ci = ( Xi, R i), called constraints, such that∗ Xi = (xi1 , . . . , xiki∗ R i is a subset of D i1) is a subsequence of X of length ki , called the constraint scopes,× · · · × D iki, called the constraint relations.(cid:2)ni=1 D i with each f (xi) ∈ D i satisfying thatf ( Xi) :=Is there a map ffrom X to the disjoint unionQuestion.( f (xi1 ), . . . , f (xiki)) ∈ R i for all i = 1, . . . , t?If such a map f exists, then we would say that the instance I is satisfiable and f ( X) = ( f (x1), . . . , f (xn)) is a solution ofthe instance I .Let A = D1 × · · · × Dn. Any map f ( X) = ( f (x1), . . . , f (xn)) ∈ A, i.e. any element (a1, . . . , an) ∈ A, is also said to be anassignment to the variables X with values in A.It is interesting to randomize CSP and consider the asymptotic property of the random CSP.Revising the model RB from [31] and k-SAT with growing k, we introduce a random model of CSP as follows.Definition 2.1. A random CSP model is said to be k-CSP if the instances are generated as follows:• every cardinality |D i| = d for i = 1, . . . , n, where d > 1 is the size of domains;• t = t(n) is an integer function of n such that limn→∞ t(n) = ∞;• for i = 1, . . . , t the constraints are generated as follows:∗ the constraint scopes Xi = (xi1 , . . . , xik ) with length k = k(n), which is an integer function of n, are randomly selectedwith repetition allowed;∗ the constraint relations R i are randomly selected with repetition allowed from the subsets of D i1× · · · × D iksuch that the cardinality |R i| = pdk, where p represents the tightness of the constraint relations and is a real constantwith 0 < p < 1.× D i2By Pr(SAT) we denote the probability of a random instance of the model k-CSP being satisfiable. We have the followingasymptotic properties of the model k-CSP.Theorem 2.1. Keep the same notation as in Definition 2.1, and assume that t = r · n ln dfor a real ε > 0, then− ln p for a constant parameter r. If k(n) (cid:2) (2 + ε) ln nln d(cid:3)0,1,r > 1;r < 1.Pr(SAT) =limn→∞3. Proof of Theorem 2.1Given any n, let G denote the set of all the instances of the model k-CSP with X = (x1, . . . , xn) and D = (D1, . . . , Dn).Then G is a probability space with equal probability for all samples. For I ∈ G, let Sol(I) denote the set of solutions of theinstance I .For any a = (a1, . . . , an) ∈ A, let(cid:3)Sa =if a ∈ Sol(I);1,0, otherwise.Then Sa is a 0–1 random variable over the probability space G. And(cid:4)S =Saa∈ A(1)is a non-negative integer random variable over the probability space G.The random variable S is the number of solutions of the random instance I ∈ G; in particular, the probability Pr(S > 0)is just the probability Pr(SAT) for the random instance I being satisfiable.Assume that a ∈ A. Then Pr(a ∈ R i) = p as |R i| = pdk. Since R1, . . . , Rt are selected randomly independently, we havePr(Sa = 1) =t(cid:5)i=1Pr(a ∈ R i) = pt.(2)Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927So the expectation of S is given byE(S) =E(Sa) = dn pt.(cid:4)a∈ ABy the assumption of Theorem 2.1 that t = r · n ln d(cid:7)(cid:6)n(1 − r) ln d− ln p , we have= d(1−r)n.dn pt = exp(n ln d + t ln p) = expHence limn→∞ dn pt = 0 if r > 1; and by Markov’s inequalityPr(S > 0) (cid:3) E(S) = dn pt,we havePr(S > 0) = 0,if r > 1.limn→∞In the rest of this section we always assume thatt = rn ln d− ln p, 0 < r < 1,and aim at proving, with the conditions of Theorem 2.1, thatPr(S > 0) = 1.limn→∞By the convexity of the function 1(cid:4)Pr(S > 0) (cid:2)Pr(Sa = 1)E(S|Sa = 1),a∈ Ax for x > 0, it follows from the Jensen’s inequality that917(3)(4)(5)(6)(7)where E(S|Sa = 1) denotes the conditional expectation of S assuming that Sa = 1 occurs; see [25, Theorem 6.10]. By theway, we remark that the following argument based on this inequality is in fact equivalent to the so-called second momentmethod, please see Appendix A of the paper for details.Let a = (a1, . . . , an) ∈ A and b = (b1, . . . , bn) ∈ A with a (cid:6)= b. We calculate the probability of both a and b satisfying arandom instance I ∈ G, where I has constraints Xi = (xi1 , . . . , xik ) and R i ⊆ D i1× · · · × D ik for i = 1, . . . , t.Let m be the number of such indices i that ai = bi , i.e. the defect m of the Hamming distance between b and a. Thereare two cases:∗ either, a and b agree with each other on every variable of the constraint, in this case, the conditional probability ofboth a and b satisfying the constraint relation R i is(cid:9)(cid:9)(cid:10)(cid:8)(cid:8)= p;dk − 1pdk − 1dk − 2pdk − 2dkpdkdkpdk= ppdk − 1dk − 1.∗ or, the conditional probability of a and b satisfying the constraint relation R i is(cid:9)(cid:9)(cid:10)(cid:8)(cid:8)(cid:8)(cid:9)The probability that the first case occurs is σm,n = (mk )k) . Thus we obtain that(n(cid:9)(cid:8)Pr(a ∈ R i, b ∈ R i) = p · σm,n + p(1 − σm,n).pdk − 1dk − 1Since the constraint relations R1, . . . , Rt are selected independently, we get thatPr(Sa = 1, Sb = 1) =t(cid:5)Pr(a ∈ R i, b ∈ R i)i=1(cid:8)= ptσm,n + pdk − 1dk − 1(cid:9)t(1 − σm,n).According to conditional probability, we getPr(Sb = 1|Sa = 1) =(cid:8)σm,n + pdk − 1dk − 1(cid:9)t(1 − σm,n).918Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927It is clear that E(Sb|Sa = 1) = Pr(Sb = 1|Sa = 1) and E(S|Sa = 1) =b∈ A E(Sb|Sa = 1). In conclusion, we have(cid:9)tσm,n + (1 − σm,n).(8)E(S|Sa = 1) =Further(cid:8)n(cid:4)(cid:9)(d − 1)n−m(cid:8)nmm=0(cid:11)pdk − 1dk − 1pdk − 1dk − 1= pdk − p + p − 1dk − 1= p − 1 − pdk − 1consequently we obtain the following inequalityn(cid:4)(cid:9)(cid:8)(cid:6)(cid:3) p,E(S|Sa = 1) (cid:3)(d − 1)n−mσm,n + (1 − σm,n)pnmm=0(cid:7)t.Combining it with the formulas (3) and (7), we deduce thatPr(S > 0) (cid:2)(cid:11)nm=0(cid:6)nm=(cid:11)nm=0(cid:6)nm(cid:7)(cid:7)dn pt(d − 1)n−m(σm,n + (1 − σm,n)p)tdn pt(d − 1)n−m(p + (1 − p)σm,n)t.So(cid:11)nm=0(cid:7)(cid:6)nm(d − 1)n−m(p + (1 − p)σm,n)t.1Pr(S > 0)(cid:3)dn ptFor m = 0, 1, . . . , k − 1 we have σm,n = (mk )(nk)= 0, i.e.(cid:8)(cid:9)nk−1(cid:4)m=0m(cid:11)nm=0(cid:6)nmnoting that(d − 1)n−m(cid:6)p + (1 − p)σm,n(cid:7)t =(cid:7)(d − 1)n−m = dn, we have(cid:9)(d − 1)n−m pt;(cid:8)k−1(cid:4)nmm=01Pr(S > 0)(cid:3) 1 +n(cid:4)m=k(cid:6)(cid:7)nmFor m (cid:2) k, since n (cid:2) m, we have m−i1Pr(S > 0)(cid:3) 1 +n(cid:4)m=k(d − 1)n−m((p + (1 − p)σm,n)t − pt)dn ptn , hence σm,n (cid:3)n−i < m(d − 1)n−m((p + (1 − p)( mmn(cid:6)(cid:7)nm(cid:6)(cid:7)k; thusn )k)t − pt)dn pt..(9)For m with k (cid:3) m (cid:3) n let(cid:8)Rm ==(cid:9)(d − 1)n−m(cid:9)(cid:9)(cid:8)(cid:8)m1nmn(cid:8)md1 − 1d(cid:8)(cid:8)(cid:8)(cid:9)k(cid:9)t(cid:9)(cid:12)p + (1 − p)(cid:8)(cid:8)(cid:9)n−mmn1 + 1 − pp− pt(cid:9)km(cid:8)(cid:9)tndn pt(cid:9)− 1.In order to prove the equality (6), it is enough to show that, with the conditions of Theorem 2.1 and the assumption (5),we havelimn→∞n(cid:4)m=kRm = 0;i.e. for any δ > 0 there is an integer N such thatn(cid:4)m=kRm < δ, ∀n > N.Further, to prove the above, it is enough to show that for any m with k (cid:3) m (cid:3) n we havenRm < δ, ∀n > N;because this inequality implies that(cid:11)nm=k Rm <(cid:11)nm=k δ/n < δ.Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927919(cid:6)(cid:7)nmIt is known that(cid:7)(cid:6)nH(x)nRm < n exp< exp(nH(m/n)). Setting x = m· (d − 1)n(1−x)(cid:6)(cid:6)p + (1 − p)xkn hence m = nx, we have(cid:7)(cid:7)t − pt/dn pt.Define the functions fn(x) for n = 1, 2, . . . as follows(cid:6)(cid:7)nH(x)(d − 1)n(1−x)fn(x) = n exp(cid:6)(cid:6)p + (1 − p)xk(cid:7)(cid:7)t − pt/dn pt,i.e.fn(x) = n exp(cid:9)nx(cid:8)(cid:6)(cid:7)nH(x)(cid:8)1d1 − 1dn(1−x)(cid:8)(cid:8)(cid:9)1 + 1 − pp(cid:9)t(cid:9)xk− 1.(10)(11)Then, the following proposition is enough to complete a proof of Theorem 2.1.Proposition 3.1. Assume that k(n) (cid:2) (2 + ε) ln nln d for a real ε > 0 and that (5) holds. Then for any δ > 0 there is an integer N such thatfn(x) < δ, ∀n > N, ∀x ∈ (0, 1].Proof. We prove it in three steps.Step 1: we look for a positive real ζ < 1 and an integer N1 such thatfn(x) < δ, ∀n > N1, ∀x ∈ [ζ, 1].From formula (4), definition (10) and the fact that p + (1 − p)xk (cid:3) 1, we have(cid:6)fn(x) (cid:3) n exp(cid:3) n exp(cid:6)n= exp(cid:6)(cid:7)nH(x)(cid:6)(cid:7)nH(x)(cid:6)n(d − 1)n(1−x)(d − 1)n(1−x)/d(1−r)np + (1 − p)xk(cid:7)t/d(1−r)n−1 ln n + H(x) + (1 − x) ln(d − 1) − (1 − r) ln d(cid:7)(cid:7).Denote τ = (1 − r) ln d, which is a positive constant as r < 1. Since H(x) is a continuous function with non-negative valueon [0, 1] and H(1) = 0, there is a positive real ζ1 < 1 such thatH(x) < τ /4, ∀x ∈ [ζ1, 1].Obviously, we can take a positive real ζ2 < 1 such that(1 − x) ln(d − 1) < τ /4, ∀x ∈ [ζ2, 1].On the other hand, since limn→∞ n−1 ln n < τ /4, ∀n > N11.nNow, let ζ = max{ζ1, ζ2}; then−1 ln n = 0, there is an integer N11 such that−1 ln n + H(x) + (1 − x) ln(d − 1) − (1 − r) ln d < −τ /4, ∀n > N11, ∀x ∈ [ζ, 1].nTake an integer N12 > −4 ln δ/τ , thenexp(−nτ /4) < exp(ln δ) = δ, ∀n > N12.At last, take N1 = max{N11, N12}; we havefn(x) < δ, ∀n > N1, ∀x ∈ [ζ, 1].Step 2: we show that there is an η with 1 > η > 0 and an integer N2 such thatfn(x) < δ, ∀n > N2, ∀x ∈(cid:13)1d− η,1d(cid:14)+ η.For the purpose, we show an easy lemma on the entropy function.Lemma 3.1. Assume that a ∈ (0, 1). Let H(x, a) = H(x) + x ln a + (1 − x) ln(1 − a) for x ∈ (0, 1). Then H(x, a) is strictly increasing in(0, a), while H(x, a) is strictly decreasing in (a, 1); in particular, H(x, a) < 0 if x (cid:6)= a.920Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927Proof. It is derived from the following calculation of derivatives:= − ln x + ln(1 − x) + ln a − ln(1 − a);dH(x, a)dxdH(x, a)dxd2 H(x, a)dx2= 0 ⇐⇒ x = a;= − 1x− 11 − x< 0.Revising the formula (11) with the notation in Lemma 3.1, we havefn(x) = exp(cid:6)(cid:7)nH(x, 1/d)(cid:6)(cid:6)· n1 + (1 − p)xk/p(cid:7)t − 1(cid:7).By Lemma 3.1, exp(nH(x, 1/d)) (cid:3) 1; so(cid:6)(cid:6)fn(x) (cid:3) n1 + (1 − p)xk/p(cid:7)t − 1(cid:7).Let y = xk, g( y) = (1 + (1 − p) y/p)t , then the derivative(cid:8)1 + 1 − pt−1(cid:9)y;tdg( y)dy= 1 − pppand by the Mean Value Theorem, there is a θ y ∈ (0, y) such that1 + 1 − p− 1 = g( y) − g(0) = 1 − p1 + 1 − p(cid:9)txk(cid:8)(cid:8)tppp(12)(cid:9)t−1θ yy.Since θ y ∈ (0, y) = (0, xk), there is θx ∈ (0, x) such that θ kxt−1− 1 = 1 − p(cid:8)1 + 1 − p1 + 1 − p(cid:9)txk(cid:9)(cid:8)t= θ y . Thusxk (cid:3) 1 − pθ kx(cid:8)1 + 1 − ppt(cid:9)t−1xkxk.ppAnd we have that fn(x) (cid:3) n 1−pfn(x) (cid:3) 1 − pexp(cid:8)pp xk) (cid:3) t · 1−pBut, (t − 1) · ln(1 + 1−pfn(x) (cid:3) 1 − pr− ln ppSince k (cid:2) (2 + ε) ln n/ ln d, we haveexpln(cid:8)pp t(1 + 1−ppp xk)t−1xk, i.e.ln n + ln t + (t − 1) ln(cid:8)1 + 1 − p(cid:9)(cid:9)xk+ k ln x.pp xk, and t = rn ln d− ln p by (5), we obtain+ ln ln d + 2 ln n + r(1 − p) ln d−p ln p(cid:9)nxk + k ln x.n(1/d)k = nd−k (cid:3) n− ln n/ ln d(cid:6)d(cid:7)2+ε = n−(1+ε);thus there is an η1 > 0, a M > 0 and an integer N21 such thatlnr− ln p+ ln ln d + r(1 − p) ln d−p ln pnxk < M, ∀n > N21, ∀x ∈(cid:13)1d− η1,1d(cid:14)+ η1.On the other hand, by k (cid:2) (2 + ε) ln n/ ln d again, we have2 ln n + k ln(1/d) = 2 ln n − k ln d < 2 ln n − (2 + ε) ln n = −ε ln n;hence there is an η2 > 0 and an integer N22 such that2 ln n + k ln x < −M + ln(cid:6)(cid:7)pδ/(1 − p), ∀n > N22, ∀x ∈(cid:13)1d− η2,1d(cid:14)+ η2.Take η = min{η1, η2} and N2 = max{N21, N22}, then(cid:14)(cid:13)fn(x) < δ, ∀n > N2, ∀x ∈− η,+ η.1d1dStep 3: there is an integer N3 such that(cid:8)(cid:9)(cid:8)fn(x) < δ, ∀n > N3, ∀x ∈0,− η∪1d(cid:9)+ η, ζ.1dY. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927921This time we further revise the expression of fn(x) in the formula (12) as follows:(cid:8)(cid:8)(cid:9)(cid:9)(cid:6)(cid:6)1 + (1 − p)xk/p(cid:7)(cid:7)t − 1· nfn(x) = expnHx,(cid:8)(cid:8)(cid:3) expnHx,(cid:8)(cid:8)= expnHx,(cid:8)(cid:8)= expnH(cid:8)(cid:8)x,(cid:8)1d1d1d1d= expnHx,that is,(cid:8)(cid:8)(cid:8)fn(x) (cid:3) expnHx,(cid:9)(cid:9)(cid:8)· n(cid:9)txk1 + 1 − pp(cid:8)+ ln n + t ln(cid:9)(cid:9)(cid:9)p xk)pxk1 + 1 − prn ln d ln(1 + 1−p− ln pr ln d ln(1 + 1−p− ln p+p xk)(cid:9)(cid:9);+ ln n ++ ln nn+ ln nn+r ln d ln(1 + 1−p− ln pp xk)(cid:9)(cid:9).(cid:9)(cid:9)(cid:9)(cid:9)1d1dLet −σ = max{H( 1d(cid:8)(cid:9)− η, 1d ), H( 1d(cid:8)+ η, 1d )}. By Lemma 3.1, σ > 0 and(cid:9)(cid:8)(cid:9)Hx,< −σ , ∀x ∈0,− η∪+ η, ζ.1d1d1dOn the other hand, since limn→∞ ln nn= 0, we have an integer N31 such thatln nn<σ3, ∀n > N31.And, since k → ∞ when n → ∞, and x < ζ < 1, hence(cid:8)1 + 1 − pp(cid:9)xk= 0;limn→∞lnthus we have an integer N32 such thatr ln d ln(1 + 1−p− ln pp xk)<σ3(cid:8), ∀n > N32, ∀x ∈0,(cid:9)(cid:8)− η∪1d1d(cid:9)+ η, ζ.Take an integer N33 > − 3 ln δ− σ3exp(cid:9)(cid:8)nσ , we have< exp(ln δ) = δ, ∀n > N33.Now, letting N3 = max{N31, N32, N33}, we have the wanted conclusion:(cid:8)(cid:9)(cid:9)(cid:8)fn(x) < δ, ∀n > N3, ∀x ∈0,− η∪+ η, ζ.1d1dSummarizing the three steps and setting N = max{N1, N2, N3}, we reach the desired result of the proposition:fn(x) < δ, ∀n > N, ∀x ∈ (0, 1].(cid:2)4. Experimental resultsIn this section, we give some experimental results about the model k-CSP. The platform we have used for our experimen-tation is called Abscon (see http://www.cril.univ-artois.fr/~lecoutre). Note that Theorem 2.1 has guaranteed the existence ofan asymptotical phase transition and located precisely the phase transition point.Each random instance is characterized by a 5-tuple (k, n, d, r, p) of parameters, where k denotes the length of the con-n ln d a measure of the constraint density, p astraint scopes, n the number of variables, d the uniform domain size, r = −t ln pmeasure of the constraint tightness. At each setting of (k, n, d, r, p), 50 instances are generated.When the constraint density r is varied accordingly, the instances change from being soluble to insoluble. Fig. 1 depictsthe solubility phase transition for d = 4, p = 0.6, n ∈ {20, 25, 30} and k = 5, which is, by the condition of Theorem 2.1,922Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927Fig. 1. The solubility phase transition for k-CSP (5, {20, 25, 30}, 4, r, 0.6).Fig. 2. Mean search cost of solving instances in k-CSP (5, {20, 25, 30}, 4, r, 0.6).the minimal value of k corresponding to d = 4 and n ∈ {20, 25, 30}. Note that the vertical scale of Fig. 1 refers to theproportion of satisfiable instances. Fig. 1 indicates that the experimental result supports affirmatively the theoretical result.Furthermore, it is interesting that, as shown in the picture, the model k-CSP exhibits the solubility phase transition evenif the number n of variables is small, and the threshold interval becomes quickly narrow when the number n of variablesincreases.Fig. 2 depicts the hardness of solving the instances of the model k-CSP when the constraint density r is varied. We selectthe values of parameters other than r for k = 5, d = 4, p = 0.6 and n ∈ {20, 25, 30}. In Fig. 2, it clearly appears that thehard instances are found at the neighborhood of the phase transition point r = 1. The solubility phase transition and thehardness phase transition both happen around the theoretical threshold r = 1 given by Theorem 2.1.We have studied the computational complexity of solving the instances of the model k-CSP around the theoretical thresh-old r = 1 when n is varied from 10 to 50 in steps of 4. According to Theorem 2.1, k satisfies the condition k (cid:2) (2 + ε)ln n/ln dY. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927923Table 1The corresponding minimal value of k satisfying the condition against n.nk (d = 2)k (d = 5)k (d = 10)10733148431894322943261053301053341154381154421154461254501254Fig. 3. Mean search cost of solving instances in k-CSP (k, n, 2, r, 0.6).for an arbitrary positive real ε. Table 1 gives the corresponding minimal value of k satisfying the condition against n ford = 2, d = 5, d = 10. The table shows that k increases very slowly with n. So it is valuable to generate random instances inexperiments. We do experiments for d = 2, p = 0.6 and r ∈ {0.98, 1, 1.02}, and the result is shown in Fig. 3. Note that inFig. 3 the horizontal scale uses a log scale. The curves in Fig. 3 look like the straight lines, which show that the complexityof solving the hard instances grows exponentially with n.In addition, the effect of the domain size d and the constraint tightness p is investigated. We have studied the complexityof solving the hard instances at the theoretical threshold r = 1 according to the different values of d and p. Fig. 4 showsthe computational complexity grows exponentially when d increases and the other parameters are fixed. Similarly, Fig. 5indicates the computational complexity grows when p increases and the other parameters are fixed. The different values ofd and p also illustrate the wide applicability of the model k-CSP.5. A random model of linear CSPIn this section, we introduce a random model of linear CSP, which is corresponding to the model k-CSP in Section 3; andshow a phase transition of it, which is corresponding to Theorem 2.1.Let F be a finite field of order q = (cid:9)a where (cid:9) is a prime integer and a is a positive integer. For any positive integer k,denote Fk = F × · · · × F with k-multiple of F. Then Fk is a vector space of dimension k over the field F. Recall that, if U isa subspace of dimension d of the vector space Fk and v ∈ Fk is a vector, then the translation v + U := {v + u | u ∈ U } of thesubspace U is called an affine subspace of dimension d of Fk. In that sense, Fk is also said to be an affine space of dimensionk over the field F. Note that a hyperplane of the affine space Fk means an affine subspace of Fk of dimension k − 1.A CSP is said to be F-linear if every instance I = ( X, D, C) satisfies further two conditions:• the domains D = (D1, . . . , Dn) are required such that D1 = · · · = Dn = F;• for every constraint scope Xi = (xi1 , . . . , xiki), i = 1, . . . , t, the corresponding constraint relation R i is an affine subspaceof the affine space Fki = D i1× · · · × D iki.It is again interesting to randomize the linear CSP and to consider the asymptotic property of the random linear CSP.924Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927Fig. 4. Mean search cost of solving instances in k-CSP (3, 20, d, 1, 0.6).Fig. 5. Mean search cost of solving instances in k-CSP (5, 20, 4, 1, p).Definition 5.1. A random F-linear CSP is said to be k-hyper-F-linear CSP if the instances are generated as follows:• t = t(n) is an integer function of n such that limn→∞ t(n) = ∞;• for i = 1, . . . , t the constraints are generated as follows:∗ the constraint scopes Xi = (xi1 , . . . , xik ) of length k = k(n), which is an integer function of n, are randomly selectedwith repetition allowed;∗ the constraint relations R i are randomly selected with repetition allowed from the set of the hyperplanes of Fk =D i1× · · · × D ik .Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927925We denote Pr(SAT) again the probability of a random instance of the k-hyper F-linear CSP being satisfiable, and havethe following asymptotic properties.Theorem 5.1. Let notation be as in Definition 5.1, and assume that t = rn for a constant parameter r. If k(n) (cid:2) (2 + ε) ln nε > 0, thenln q for a realPr(SAT) =limn→∞(cid:3)0,1,r > 1;r < 1.Remark. It is easy to compute the parameters:• every domain D i has constant cardinality d = |D i| = |F| = q;• every R i is a hyperplane of Fk, hence |R i| = qk−1 = 1• t = rn = r n ln d• k(n) (cid:2) (2 + ε) ln n− ln p , because ln d− ln pln d , as q = d.= ln q− ln q−1= 1;q· dk = pdk, where denote p = 1q ;Thus all the conditions for the parameters in Theorem 2.1 are satisfied. Note that we cannot quote Theorem 2.1 to getTheorem 5.1 directly, because the probability space in the present case is different from that for Theorem 2.1.Proof of Theorem 5.1. For any given n, let L be the probability space consisting of all the instances of X = (x1, . . . , xn) of thek-hyper-F-linear CSP. Similarly to Section 3, we set A = Fn, and define the random variable Sa for a ∈ A and S =a∈ A Saas in the formula (1).(cid:11)Further, each constraint relation R i is a hyperplane of Fk; the cardinality of the set of the hyperplanes of Fk is (dk−1)dd−1 .It is clear thatPr(a ∈ R i) = dk − 1d − 1(cid:10)(dk − 1)dd − 1= 1d= p.We still obtain thatPr(Sa = 1) = Pr(cid:6)(cid:7)a ∈ Sol(I)= pt;and similarly to that in Section 3, the expectation E(S) = dn pt .For a ∈ A, b ∈ A, and a (cid:6)= b, we calculate, in the way similar to that in Section 3, the probability of both a and b satisfyinga random instance I ∈ L. Let m be the number of such indices i that ai = bi , there are also two cases:∗ either, a and b agree with each other on every variable of the constraint, in this case, the conditional probability ofboth a and b satisfying the constraint relation R i is(cid:10)dk − 1d − 1(dk − 1)dd − 1= 1d= p;∗ or, the conditional probability of a and b satisfying constraint relation R i is(cid:10)dk−1 − 1d − 1(cid:8)(dk − 1)dd − 1= ppdk − 1dk − 1(cid:9),where p = 1d .The probability that the first case occurs is σm,n = (mk )(nk). Thus we obtain thatPr(a ∈ R i, b ∈ R i) = p · σm,n + p(1 − σm,n).(cid:8)(cid:9)pdk − 1dk − 1Consequently we getPr(Sb = 1|Sa = 1) =(cid:8)σm,n + pdk − 1dk − 1(cid:9)t(1 − σm,n).Thus, all the arguments from the formula (7) to the end of Section 3 are still valid for completing a proof of Theo-rem 5.1. (cid:2)6. ConclusionsMotivated by k-SAT with growing k and based on the model RB, in this paper we proposed a model of random CSP. Thismodel, named k-CSP, allows to deal with random CSP which domain size is fixed, tightness of constraint relations is fixed926Y. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927and length of constraints scopes grows very slowly. We proved theoretically the existence of phase transition in the modelk-CSP and quantify the exact location of it. Experiments validate the effectiveness of the model k-CSP and illustrate that thecomputational complexity grows exponentially with the number of variables. Note that the worst-cases happen only aroundthe phase transition point. In summary, since the experiments can be designed and conducted within a reasonably smallrange of domain size and constraint scope length and there is no other restrictions on the constraint relations except thefixed tightness, the model k-CSP can easily and naturally generate asymptotically non-trivial CSP instances and very suitablefor testing the capability of CSP algorithms.Combining the advantages of the proposed model k-CSP and linear CSP, we introduced a new type of random linear CSPmodel, named k-hyper-F-linear CSP, by incorporating linear structure to the domains and constraint relations of the modelk-CSP. Similar to the arguments established for the model k-CSP, the existence and exact location of phase transition arealso demonstrated for the linear CSP model.The investigation of random CSP in this paper is by no means exhaustive. As in the model k-CSP, this is suggestive of amore general random CSP model. The core concept of general-model-building is that of a certain relation between the sizeof domains and the size of constraints, including the length of constraint scopes and the tightness of constraint relations.Given such a relation, it is very possible to construct a more generalized model that has phase transition existing and isfeasible to quantify it theoretically and more application extensively. This motivates our next step work to correlate thedomain size with the constraint size to construct a relatively general CSP model.AcknowledgementsThe authors are grateful to Professor Ke Xu from Beihang University for his encouragement and many helpful discussions.Many thanks are given to the anonymous reviewers for their valuable comments which have helped us to improve this papergreatly.This work is supported by National Science Project (973 Project) of China Grant No. 2005CB321902 and National NaturalScience Foundation of China Grant No. 60973033.Appendix ATake A to be a finite index set, and let Xa for a ∈ A be 0–1 random variables; then X =integer random variable. In Section 3 for the case that r < 1 we cite the following inequality(cid:11)a∈ A Xa is a non-negativePr(X > 0) (cid:2)(cid:4)a∈ APr(Xa = 1)E(X| Xa = 1),which can be derived from the Jensen’s inequality, see [25, Theorem 6.10]. For the model k-CSP, our proof in Section 3by using the above inequality is essentially equivalent to the so-called second moment method, this can be seen from theformulas (2), (8) in Section 3 and the following lemma.Lemma A.1. Let X =(cid:11)(cid:4)a∈ APr(Xa = 1)E(X| Xa = 1)a∈ A Xa be as above. If E( Xa) = E( Xb) for any a, b ∈ A, then(cid:2) E(X)2E(X 2);the equality holds if and only if E( X| Xa = 1) = E( X| Xb = 1) for any a, b ∈ A.Proof. Set E = E( Xa) for a ∈ A. Note that Xb Xa is still a 0–1 random variable, hence Pr( Xb = 1, Xa = 1) = Pr( Xb Xa = 1) =E( Xb Xa). We haveE(X| Xa = 1) ==(cid:4)b∈ A(cid:4)b∈ AE(Xb| Xa = 1) =(cid:4)Pr(Xb| Xa = 1)b∈ APr(Xb = 1, Xa = 1)Pr(Xa = 1)=(cid:4)b∈ AE(Xb Xa)E;soPr(Xa = 1)E(X| Xa = 1)=(cid:11)b∈ AEE( Xb Xa)E=(cid:8)(cid:4)b∈ A(cid:9)−1.E(Xb Xa)E 2By the inequality of arithmetic and harmonic means, we haveY. Fan, J. Shen / Artificial Intelligence 175 (2011) 914–927927(cid:4)a∈ APr(Xa = 1)E(X| Xa = 1)=(cid:4)(cid:8)(cid:4)(cid:9)−1E(Xb Xa)E 2a∈ Ab∈ A(cid:8)(cid:4)(cid:4)(cid:2) | A|2 ·(cid:11)b∈ Aa∈ A| A|2 E 2a,b∈ A E(Xb Xa)(cid:11)a∈ A E(Xa))2((cid:11)a,b∈ A Xa Xb)E(==(cid:9)−1E(Xb Xa)E 2(cid:11)=a,b∈ A E(Xb)E(Xa)(cid:11)a,b∈ A E(Xb Xa)= E(X)2E(X 2).The equality holds if and only if for any a, b ∈ A we have Pr( Xa=1)E( X| Xa=1)= Pr( Xb=1)E( X| Xb=1) , i.e. E( X| Xa = 1) = E( X| Xb = 1). (cid:2)References[1] D. Achlioptas, L.M. Kirousis, E. Kranakis, D. Krizanc, M.S.O. Molloy, Y.C. Stamatiou, Random constraint satisfaction: a more accurate picture, in: Proc. ofthe Third International Conference on Principles and Practice of Constraint Programming, in: LNCS, vol. 1330, 1997, pp. 107–120.[2] D. Achlioptas, A. Naor, Y. Peres, Rigorous location of phase transitions in hard optimization problems, Nature 435 (7043) (2005) 759–764.[3] G. Balakin, V. Kolchin, V. Khokhlov, Hypercycles in a random hypergraph, Diskretnaya Matematika 3 (3) (1991) 102–108.[4] A. Bulatov, P. Jeavons, A. Krokhin, Classifying the complexity of constraints using finite algebras, SIAM Journal on Computing 34 (3) (2005) 720–742.[5] N. Calkin, Dependent sets of constant weight vectors in GF(q), Random Structures and Algorithms 9 (1–2) (1996) 49–53.[6] N. Calkin, Dependent sets of constant weight binary vectors, Combinatorics, Probability and Computing 6 (3) (1997) 263–271.[7] P. Cheeseman, B. Kanefsky, W. Taylor, Where the really hard problems are, in: Proceedings of IJCAI-91, 1991, pp. 331–337.[8] N. Creignou, H. Daudé, Approximating the satisfiability threshold for random k-XOR-CNF formulas, Combinatorics, Probability and Computing 12 (2)(2001) 113–126.[9] N. Creignou, H. Daudé, Satisfiability threshold for random XOR-CNF formulas, Discrete Applied Mathematics 96–97 (1999) 41–53.[10] N. Creignou, H. Daudé, Random generalized satisfiability problems, in: Proceedings of SAT, 2002.[11] N. Creignou, H. Daudé, Generalized satisfiability problems: minimal elements and phase transitions, Theoretical Computer Science 302 (1–3) (2003)411–430.[12] N. Creignou, H. Daudé, Combinatorial sharpness criterion and phase transition classification for random CSPs, Information and Computation 190 (2)(2004) 220–238.[13] R. Darling, J. Norris, Structure of large random hypergraphs, Annals of Applied Probability (2005) 125–152.[14] O. Dubois, Y. Boufkhad, J. Mandler, Typical random 3-sat formulae and the satisfiability threshold, in: Proc. of SODA’00, 2000, pp. 126–127.[15] O. Dubois, J. Mandler, The 3-XOR-SAT threshold, in: Proceedings of the 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002,pp. 769–778.[16] A. Flaxman, A sharp threshold for a random constraint satisfaction problem, Discrete Mathematics 285 (1–3) (2004) 301–305.[17] E. Friedgut, Sharp thresholds of graph properties, and the k-SAT problem, Journal of the American Mathematical Society 12 (1999) 1017–1054, withan appendix by Jean Bourgain.[18] E. Friedgut, Hunting for sharp thresholds, Random Structures and Algorithms 26 (1–2) (2005) 37–51.[19] A. Frieze, M. Molloy, The satisfiability threshold for randomly generated binary constraint satisfaction problems, Random Structures and Algo-rithms 28 (3) (2006) 323–339.[20] A.M. Frieze, N.C. Wormald, Random k-SAT: A tight threshold for moderately growing k, in: Proceedings of the 5th International Symposium on Theoryand Applications of Satisfiability Testing, 2002, pp. 1–6.[21] Y. Gao, J. Culberson, Consistency and random constraint satisfaction models with a high constraint tightness, in: CP04, 2004, pp. 17–31.[22] Y. Gao, J. Culberson, Consistency and random constraint satisfaction problems, Journal of Artificial Intelligence Research 28 (2007) 517–557.[23] I.P. Gent, E. MacIntyre, P. Prosser, B.M. Smith, T. Walsh, Random constraint satisfaction: flaws and structure, Constraints 6 (4) (2001) 345–372.[24] A.C. Kaporis, L.M. Kirousis, E.G. Lalas, The probabilistic analysis of a greedy satisfiability algorithm, in: Proc. of the 10th Ann Eur. Symp. on Algor., 2002,pp. 574–585.[25] M. Mitzenmacher, E. Upfal, Probability and Computing: Randomized Algorithm and Probabilistic Analysis, Cambridge Univ. Press, Cambridge, 2005.[26] M. Molloy, Models and thresholds for random constraint satisfaction problems, in: Proceedings of the Thirty-Fourth Annual ACM Symposium on Theoryof Computing, 2002, pp. 209–217.[27] P. Prosser, An empirical study of phase transitions in binary constraint satisfaction problems, Artificial Intelligence 81 (1996) 81–109.[28] B.M. Smith, M.E. Dyer, Locating the phase transition in binary constraint satisfaction problems, Artificial Intelligence 81 (1996) 155–181.[29] B.M. Smith, Constructing an asymptotic phase transition in random binary constraint satisfaction problems, Theoretical Computer Science 265 (1–2)(2001) 265–283.[30] K. Xu, F. Boussemart, F. Hemery, C. Lecoutre, Random constraint satisfaction: Easy generation of hard satisfiable instances, Artificial Intelligence 171(2007) 514–534.[31] K. Xu, W. Li, Exact phase transitions in random constraint satisfaction problems, Journal of Artificial Intelligence Reseach 12 (2000) 93–103.[32] K. Xu, W. Li, Many hard examples in exact phase transition, Theoretical Computer Science 355 (2006) 291–302.