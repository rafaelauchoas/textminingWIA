Artificial Intelligence 171 (2007) 776–804www.elsevier.com/locate/artintAn executable specification of a formal argumentation protocolAlexander Artikis a,∗, Marek Sergot b, Jeremy Pitt ca Institute of Informatics & Telecommunications, NCSR “Demokritos”, Athens, 15310, Greeceb Department of Computing, Imperial College London, SW7 2AZ, UKc Department of Electrical & Electronic Engineering, Imperial College London, SW7 2BT, UKReceived 8 November 2006; received in revised form 3 April 2007; accepted 16 April 2007Available online 29 April 2007AbstractWe present a specification, in the action language C+, of Brewka’s reconstruction of a theory of formal disputation originallyproposed by Rescher. The focus is on the procedural aspects rather than the adequacy of this particular protocol for the conduct ofdebate and the resolution of disputes. The specification is structured in three separate levels, covering (i) the physical capabilitiesof the participant agents, (ii) the rules defining the protocol itself, specifying which actions are ‘proper’ and ‘timely’ according tothe protocol and their effects on the protocol state, and (iii) the permissions, prohibitions, and obligations of the agents, and thesanctions and enforcement strategies that deal with non-compliance. Also included is a mechanism by which an agent may objectto an action by another participant, and an optional ‘silence implies consent’ principle. Although comparatively simple, Brewka’sprotocol is thus representative of a wide range of other more complex argumentation and dispute resolution procedures that havebeen proposed. Finally, we show how the ‘Causal Calculator’ implementation of C+ can be used to animate the specification andto investigate and verify properties of the protocol.© 2007 Elsevier B.V. All rights reserved.Keywords: Argumentation; Disputation; Protocol; Norm; Multi-agent system; Specification; Action language1. IntroductionOne of the main tasks in the formal specification and analysis of (open) multi-agent systems (MAS) is the represen-tation of the protocols and procedures for agent interactions, and the norms of behaviour that govern these interactions.Examples include protocols for exchanging information, for negotiation, and for resolving disputes.It has been argued that a specification of systems of this type should satisfy at least the following two requirements:first, the interactions of the members should be governed by a formal, declarative, verifiable and meaningful seman-tics [64]; and second, to cater for the possibility that agent behaviour may deviate from what is prescribed, agentinteractions can usefully be described in terms of permissions and obligations [26].We have been developing a theoretical framework for the executable specification of open agent systems thataddresses the aforementioned requirements [3–5]. We adopt the perspective of an external observer, thus taking intoaccount only externally observable behaviours and not the internal architectures of the individual agents, and view* Corresponding author.E-mail addresses: a.artikis@acm.org (A. Artikis), mjs@doc.ic.ac.uk (M. Sergot), j.pitt@imperial.ac.uk (J. Pitt).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.04.008A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804777agent systems as instances of normative systems [26] whereby constraints on agents’ behaviour (or social constraints)are specified in terms of their permissions, their institutional power to effect changes and bring about certain states ofaffairs, and their rights and obligations to one another. We employ an action formalism to specify the social constraintsgoverning the behaviour of the members and then use a computational framework to animate the specification andinvestigate its properties. For the action formalism, we have employed the Event Calculus [29], the action languageC+ [22], and an extended form of C+ specifically designed for modelling the institutional aspects of agent systems[57–59].In this paper we demonstrate how the theoretical and computational frameworks can be used with the languageC+ to specify and execute an argumentation protocol based on Brewka’s reconstruction [8], in the Situation Calculus[52], of a theory of formal disputation originally proposed by Rescher [53]. We presented a preliminary formulationin an earlier paper [4]. This present paper is a refined and much extended version.We are focusing here on the procedural aspects of the protocol rather than on the underlying logic of disputationemployed by Brewka or on the adequacy of this particular protocol for the conduct of debate and the resolution ofdisputes. The features of Brewka’s protocol are representative of a wide range of other more complex argumentationand dispute resolution procedures that have been proposed in the literature, and to which the methods of this papercan be similarly applied.The specification of the argumentation protocol is structured into three separate levels, covering:(i) the physical capabilities of the participant agents (in the present context, the messages/utterances each agent isactually capable of transmitting);(ii) the rules defining the protocol itself, specifying which actions are ‘proper’ and ‘timely’ according to the protocoland their effects on the protocol state;(iii) the permissions, prohibitions and obligations of the agents, and the sanctions and enforcement strategies that dealwith non-compliance.In any given implementation of the protocol, it may or may not be permitted for an agent to perform an action that isnot proper or timely; conversely, there may be protocol actions that are proper and timely but that are nevertheless notpermitted under certain circumstances, because, for instance, they lead to protocol runs with undesirable properties.The rules comprising level (ii) of the specification correspond to constitutive norms that define the meaning of theprotocol actions. Levels (i) and (iii), respectively, can be seen as representing the physical and normative environmentwithin which the protocol is executed. We have also been concerned with the concept of social role. Briefly, a role isassociated with a set of (role) preconditions that agents must satisfy in order to be eligible to occupy that role and aset of (role) constraints that govern the behaviour of the agents once they occupy that role. We will not discuss roleassignment in this paper. For the example in this paper, we will assume for simplicity that the participant agents arealready assigned to certain roles, and that these roles do not change during an execution of the protocol.A note on terminology. In the earlier version of this paper [4], and in the treatment of other examples, we defineda protocol by specifying the conditions under which an action was said to be ‘valid’ according to the protocol. Here,we have employed a finer structure, further classifying ‘valid’ actions as proper or timely, in line with suggestionsthat have also been made by Prakken et al. [45,49]. A ‘valid’ action in our earlier terminology is one that is bothproper and timely. Other terminology in common use employs the term ‘successful’ where we say ‘valid’: one thendistinguishes between an action, such as an utterance or the transmission of a message of a certain form, which isan ‘attempt’ to make a claim, say, and the conditions under which the attempt to claim is ‘successful’ (sometimes,‘effective’). We prefer to avoid the term ‘successful’ since even an unsuccessful ‘attempt’ can have effects on theprotocol state. We also avoid use of the term ‘legal’ for ‘valid’ or ‘successful’ since it is ambiguous as to whetherit refers to the constitutive element of the protocol (level (ii) of our specification) or the normative environment inwhich the protocol is executed (level (iii)). Also related is the concept of institutional (or ‘institutionalised’) power(sometimes, ‘competence’ or ‘capacity’). This refers to the characteristic feature of institutions—legal systems, formalorganisations, or informal groupings—whereby designated agents, often when acting in specific roles, are empoweredto create or modify facts of special significance in that institution—institutional facts in the terminology of Searle [56].(See e.g. [27,35] for further discussion and references to the literature.) Thus in the present example it is natural tosay that, under certain circumstances, an agent acting in a certain role has power (competence, capacity) to declarethe dispute resolved in favour of one or other of the protagonists; or that in certain circumstances an agent has power778A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804to object to an action by one of the other participants; or more generally, that the argumentation protocol definesthe conditions under which an agent has the power to perform one of the argumentation actions. We will not referexplicitly to power in the specification of the argumentation protocol presented here. The classification of actions intoproper and timely already provides a more detailed specification.In this paper we use the language C+ to formulate the specification. An advantage of C+, compared with otheraction formalisms, is that it can be given an explicit semantics in terms of transition systems. This enables us toanalyse and prove properties of the protocol. The concluding sections of the paper present some illustrative examples.The paper is structured as follows. First, we briefly describe the C+ language. Second, we present the ‘Causal Cal-culator’ software implementation, a computational framework for executing specifications formalised in C+. Third,we summarise Brewka’s reconstruction of Rescher’s theory of formal disputation. Fourth, we specify, prove propertiesof, and execute (a form of) Brewka’s argumentation protocol with the use of C+ and the Causal Calculator. Finally,we discuss related research, summarise the presented work, and point out directions for further investigations.2. The C+ languageC+, as mentioned above, is an action language with an explicit transition systems semantics. We describe here theversion of C+ presented in [22].2.1. Basic definitionsA multi-valued propositional signature is a set σ of symbols called constants, and for each constant c ∈ σ , a non-empty finite set dom(c) of symbols, disjoint from σ , called the domain of c. For simplicity, in this presentation wewill assume that every domain contains at least two elements.An atom of signature σ is an expression of the form c = u where c ∈ σ and u ∈ dom(c). A Boolean constant is onewhose domain is the set of truth values {t, f}. When c is a Boolean constant we often write c for c = t and ¬c for c = f.A formula ϕ of signature σ is any propositional combination of atoms of σ . An interpretation I of σ is a functionthat maps every constant in σ to an element of its domain. An interpretation I satisfies an atom c = u if I (c) = u. Thesatisfaction relation is extended from atoms to formulas according to the standard truth tables for the propositionalconnectives. A model of a set X of formulas of signature σ is an interpretation of σ that satisfies all formulas in X. Ifevery model of a set X of formulas satisfies a formula ϕ then X entails ϕ, written X |= ϕ.2.2. SyntaxThe representation of an action domain in C+ consists of fluent constants and action constants.• Fluent constants are symbols characterising a state. They are divided into two categories: simple fluent constantsand statically determined fluent constants. Simple fluent constants are related to actions by dynamic laws (that is,laws describing a transition (si, εi, si+1) from a state si to its successor state si+1). Statically determined fluentconstants are characterised by static laws (that is, laws describing an individual state) relating them to other fluentconstants. Static laws can also be used to express constraints between simple fluent constants. Static and dynamiclaws are defined below.• Action constants are symbols characterising state transitions. In a transition (si, εi, si+1), the transition label εi ,also called an ‘event’, represents the occurrence of actions performed concurrently by one or more agents oroccurring in the environment. Transitions may be non-deterministic. Action constants are used to name actions,attributes of actions, or properties of transitions as a whole.An action signature (σ f, σ a) is a non-empty set σ f of fluent constants and a non-empty set σ a of action constants.An action description D in C+ is a non-empty set of causal laws that define a transition system of a particular type.A causal law can be either a static law or a dynamic law. A static law is an expressioncaused F if G(1)A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804779where F and G are formulas of fluent constants. In a static law, constants in F and G are evaluated on the same state.A dynamic law is an expressioncaused F if G after H(2)where F , G and H are formulas such that every constant occurring in F is a simple fluent constant, every constantoccurring in G is a fluent constant, and H is any combination of fluent constants and action constants. In a transitionfrom state si to state si+1, constants in F and in G are evaluated on si+1, fluent constants in H are evaluated on si andaction constants in H are evaluated on the transition itself. F is called the head of the static law (1) and the dynamiclaw (2).The full C+ language also provides action dynamic laws, which are expressions of the formcaused α if Hwhere α is a formula containing action constants only and H is a formula of action and fluent constants. We will notuse action dynamic laws in this paper and so omit the details in the interests of brevity.The C+ language provides various abbreviations for common forms of causal laws. For example, a dynamic lawof the formcaused F if (cid:4) after H ∧ αwhere α is a formula of action constants is often abbreviated asα causes F if HIn the case where H is (cid:4) the above is usually written as α causes F .When presenting the argumentation protocol specification, we will often employ the causes abbreviation to expressthe effects of the agents’ actions. We will also employ the C+ abbreviationdefault Fwhich is shorthand for the static lawcaused F if Fexpressing that F holds in the absence of information to the contrary.When it aids readability, we will writeF iff Gas a shorthand for the pair of static lawscaused F if Ganddefault ¬FTwo further abbreviations that we will employ are nonexecutable and inertial; a dynamic law of the formcaused ⊥ if (cid:4) after α ∧ Hwhere α is a formula containing only action constants and H is a formula containing only fluent constants is abbrevi-ated as:nonexecutable α if HIn the case where H is (cid:4) the above can be written as nonexecutable α.The inertia of a fluent constant c over time is represented as:inertial cThis is an abbreviation for the set of dynamic laws of the form (for all values u ∈ dom(c)):caused c = u if c = u after c = uA C+ action description is a non-empty set of causal laws. Of particular interest is the sub-class of definite actiondescriptions. A C+ action description D is definite if:780A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804• the head of every causal law of D is an atom or ⊥, and• no atom is the head of infinitely many causal laws of D.The C+ action description in this paper will be definite.2.3. SemanticsIt is not possible in the space available here to give a full account of the C+ language and its semantics. We trust thatthe C+ language, and especially its abbreviations, are sufficiently natural that readers can follow the presentation ofthe case study in later sections. Interested readers are referred to [22,23] for further technical details. For completeness,we summarise here the semantics of definite action descriptions ignoring (as we are) the presence of action dynamiclaws (and assuming that the domain of every constant contains at least two elements). We emphasise the transitionsystem semantics, as in [57].Every action description D of C+ defines a labelled transition system, as follows:• States of the transition system are interpretations of the fluent constants σ f. It is convenient to identify a state swith the set of fluent atoms satisfied by s (in other words, s |= f = v if and only if f = v ∈ s for every fluentconstant f ).Let Tstatic(s) denote the heads of all static laws in D whose conditions are satisfied by s:Tstatic(s) =def {F | static law (1) is in D, s |= G}For a definite action description D, an interpretation s of σ f is a state of the transition system defined by D (orsimply, a state of D for short) whens = Tstatic(s) ∪ Simple(s)where Simple(s) denotes the set of simple fluent atoms satisfied by s. (So s − Simple(s) is the set of staticallydetermined fluent atoms satisfied by s.)• Transition labels of the transition system defined by D (also referred to as events) are the interpretations of theaction constants σ a.A transition is a triple (s, ε, s(cid:8)) in which s is the initial state, s(cid:8) is the resulting state, and ε is the transition label(or event). Since transition labels are interpretations of σ a, it is meaningful to say that a transition label ε satisfiesa formula α of σ a: when ε |= α we sometimes say that the transition (s, ε, s(cid:8)) is of type α.• Let E(s, ε, s(cid:8)) denote the heads of all dynamic laws of D whose conditions are satisfied by the transition (s, ε, s(cid:8)):E(s, ε, s(cid:8)) =def {F | dynamic law (2) is in D, s(cid:8) |= G, s ∪ ε |= H }For a definite action description D, (s, ε, s(cid:8)) is a transition of D (or in full, a transition of the transition systemdefined by D) when s and s(cid:8) are interpretations (set of atoms) of σ f and ε is an interpretation of σ a such that:– s = Tstatic(s) ∪ Simple(s) (s is a state of D)– s(cid:8) = Tstatic(s(cid:8)) ∪ E(s, ε, s(cid:8))For any non-negative integer m, a path or history of D of length m is a sequences0 ε0 s1 . . . sm−1 εm−1 smwhere (s0, ε0, s1), . . . , (sm−1, εm−1, sm) are transitions of D.3. The causal calculatorThe Causal Calculator (CCALC) is a computational framework designed and implemented by the Action Groupof the University of Texas for representing action and change in the C+ language and performing a range of com-putational tasks on the resulting formalisations. CCALC has been applied to several ‘challenge problems’ (see, forexample, [1,33,34]).A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804781Fig. 1. Executing the specification of an open agent system: Prediction queries.A detailed account of CCALC’s operation and functionality may be found in [22]. This section describes the waywe use prediction, planning and postdiction queries to execute the specification of the social constraints (or protocolrules) of an open agent system. In each type of query, CCALC has as input a definite C+ action description Dsocexpressing the specification of social constraints. We refer to the states of the transition system defined by Dsoc associal states (or protocol states). In other words, a social state is an interpretation (with some further properties) ofthe fluent constants of Dsoc. These constants express, amongst other things, which actions are physically possible,proper, timely, permitted or obligatory, and whether an agent has been sanctioned for performing forbidden actions ornot complying with its obligations.• Prediction queries. The computation of an answer to this type of query involves: (i) an initial social state express-ing, amongst other things, which actions are initially physically possible, proper, timely, permitted or obligatory,and the initial sanctions of the agents (an initial social state may be partial or complete), and (ii) a narrative,that is, a description of temporally-sorted externally observable events of the system (a narrative is expressed asa sequence of transitions). The outcome of a prediction query (if any) is the current social state, that is, the stateresulting from the events described in the narrative, expressing, amongst other things, which actions are currentlyphysically possible, proper, timely, permitted or obligatory, and the current sanctions of the agents (see Fig. 1).Computing answers to prediction queries may be performed at run-time to inform the members of a system, atany time during the execution of the system, of their permissions, obligations, sanctions and so on.• Planning queries. Agents may issue planning queries to CCALC: (i) at design-time in order to generate plans thatwill facilitate them in avoiding run-time conflicts (say), and (ii) at run-time in order to update their plans.• Postdiction queries. New members of a system may seek to determine the past states of that system. Similarinformation may be requested by agents that have ‘crashed’ and resumed their operation. Such information canbe produced via the computation of answers to postdiction queries.The computation of answers to queries may be additionally used to prove properties of the social constraints’ specifi-cation; in Section 8 we employ CCALC to prove properties of the argumentation protocol specification.4. Rescher’s theory of formal disputationWe describe, specify, prove properties of, and animate an argumentation protocol—a procedure for the resolutionof a dispute—based on Brewka’s reconstruction [8] of Rescher’s Theory of Formal Disputation (RTFD) [53]. We havepicked this example because (i) in defining a set repertoire of possible moves for each participant, and their effects,it is typical of the kind of protocols that are encountered in the multi-agent systems (MAS) field, (ii) it provides aconcrete and comparatively simple example of a formal procedure for the resolution of disputes, and (iii) Brewka’s782A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804formalisation in the Situation Calculus provides a natural starting point and basis for comparison. This section brieflypresents RTFD. Sections 5 and 6 present Brewka’s reconstruction and our variation of RTFD respectively.According to RTFD, argumentation may be viewed as a three-player game: the proponent claims a particular thesisand the opponent may question this thesis. The determiner decides whether the proponent’s thesis was successfully de-fended or not. The main actions that the participants may perform are the following (p, q below are logical formulas):• Categorical assertions. These are assertions of the form ‘p is the case’ and are performed by the proponent.• Cautious assertions. These are assertions of the form ‘p is the case for all you have shown’ and are performed bythe opponent.• Provisoed assertions. These assertions are performed by either the proponent or the opponent and are expressedas follows: ‘p generally obtains provided that q’. A provisoed assertion of this form is accompanied with either acategorical assertion or a cautious assertion about q.The argumentation commences with a categorical assertion by the proponent regarding the topic of the argumentation,say p. The opponent may question the topic by either a challenge of the form ‘¬p is the case for all you have shown’or by a provisoed denial of the form ‘¬p generally obtains provided that q’ and ‘q is the case for all you have shown’.The argumentation continues in this manner until the topic has been ‘accepted’ by both the proponent and opponent(the precise meaning of ‘accepting’ a formula will be presented in the following section), in which case the determinerdeclares the proponent the winner, or the proponent itself does not accept the topic any more, in which case the deter-miner declares the opponent the winner. If neither of these alternatives take place and the participants cannot performany additional reasonable actions, or if a deadline occurs, then the determiner decides about the winner “based on theplausibility of the proponent’s claims that were not conceded by the opponent” [8, p. 271]. Rescher’s theory exhibitsthe silence implies consent principle. According to this principle, an agent that does not explicitly challenge a claimperformed by the other is assumed to concede to the claim.5. Brewka’s reconstruction of RTFDAn argumentation system, according to [8, Definition 4.9], includes as core components a logic of disputation andan argumentation context. In Brewka’s reconstruction, the logic of disputation is preferential default logic [9] and theargumentation context is formalised with the use of a Situation Calculus dialect [52]. The main actions of the protocolare the following: claiming, conceding to, retracting, and denying propositions and default rules, declaring the winnerof the argumentation, and objecting to actions performed by the other participants.The semantics of the protocol actions are given in terms of the premises1 held by the proponent and opponent. Thepremise(ag, q, s) fluent expresses the formulas q that ag holds explicitly. The related fluent accepts(ag, q, s) is usedto represent the formulas that ag holds implicitly: accepts(ag, q, s) expresses that q follows in the logic of disputationL from the premises explicitly held by agent ag in argumentation record s:accepts(ag, q, s) iff {p | premise(ag, p, s)} (cid:9)L q(3)An argumentation record is a situation (in the terminology of the Situation Calculus) and so includes the history ofthe protocol.The semantics of a claim action, for example, of a proposition or a default rule, are given by the following SituationCalculus effect axiom:premise(ag, q, do(claim(ag, q), s))(4)Expression (4) states that the successor situation following the performance of a claim action by ag includes a premiseabout the claimed proposition (or default). In expression (4) q represents either a proposition or a default rule of theform n :: a : b/c where n is a label associated with the default rule, a is the prerequisite, b is the justification and c isthe consequent of the rule [8].1 In work on argumentation protocols and dialogue games the term ‘commitment’ is often used where we say ‘premise’. We will not use the term‘commitment’ here partly to keep the link with Brewka’s account, but also because ‘commitment’ has another meaning related to obligation whichmight cause confusion in later sections of the paper.A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804783Brewka distinguishes between ‘possible’ and ‘legal’ actions. Possible actions are specified by means of SituationCalculus possibility axioms. Consider the following possibility axiom of the retract action:poss(retract(ag, q), s) ↔ premise(ag, q, s)(5)The above axiom states that it is ‘possible’ for an agent to retract a proposition (or a default) q if and only if that agenthas a premise about that proposition (or default). The conditions that determine whether an action is possible or notare specified in a protocol-independent manner.‘Legal’ actions, in contrast to possible actions, are specified in a protocol-dependent manner. Consider the follow-ing example of a legal action:legal(declare(det, pro), s) → accepts(pro, topic, s)(6)The above expression states that declaring the proponent ‘pro’ as the winner is legal only if the proponent accepts thetopic of the argumentation.A point of departure of Brewka’s reconstruction from Rescher’s theory is the introduction of the ‘object’ action.The participants of an argumentation protocol may perform illegal actions; the effects of an illegal action are the sameas if the action were a legal one—provided that no other participant objects to the illegal action. If some participantobjects immediately (that is, no other action takes place between the illegal action and the objection), then the effectsof the illegal action are ‘cancelled’.The ‘object’ mechanism is not a new idea in the field of argumentation protocols. Prakken points out that an objectmechanism of this type is part of Robert’s Rules of Order (RRO): “[t]he general rule is that anything goes until anymember objects, after which RRO must be strictly applied” [45, p. 10]. One can find similar mechanisms in mostprocedures for the conduct of formal debates and disputes.Enabling agents to object to other agents’ actions can lead to a more flexible argumentation protocol. In Brewka’smodification of RTFD, for example, the proponent might choose not to object to an illegal action performed by thedeterminer because it (the proponent) calculates that the illegal action will serve its benefit better than having theillegal action ruled out. However, it can be argued that Brewka’s object mechanism is too simplistic, if it is a model ofhow argumentation processes are actually conducted, and too rigid, if it is a model of how argumentation processesought to be conducted. Consider for example the case where an agent, say the determiner, repeatedly performs illegalactions. The proponent and opponent have to object to every illegal action performed by the determiner because ifthey do not object, they implicitly accept the illegal actions as legal ones. In the formalisation to be presented in thefollowing sections, we propose a way to address this issue.Note that according to Brewka’s treatment, an objection will ‘undo’ the effects of an illegal action if and only if theobjection takes place immediately after the illegal action. If for some reason an agent fails to object immediately toan illegal action (say, another action took place between the illegal action and the objection) then it will be consideredthat this agent does not object and so implicitly agrees to the treatment of the illegal action as a legal one. One way ofovercoming this limitation is by specifying that one may effectively object to Act at the latest n time-points after Act’sperformance. Such a specification, however, would raise several complications, such as the following. Assume that Acttakes place at time t and an agent objects to this action at t (cid:8) (t < t(cid:8) < t + n). The effects of the objection may include:(i) undoing the effects of all actions that took place between t and t (cid:8) (therefore, it would be necessary to keep track ofa protocol history fragment), (ii) undoing the effects of Act, and (iii) (possibly) applying the effects of all actions thattook place between t and t (cid:8). We expect that such an object mechanism would be practical only when n is small—inany setting, computerised or not. For this reason we will follow Brewka and assume that objections can only be madeto the immediately preceding action. We will allow, however, for the possibility that more than one participant objectsto the last action. Further discussion of alternative object mechanisms is beyond the scope of the paper.Finally, Brewka’s reconstruction formalises Rescher’s ‘silence implies consent’ principle as follows: an agent, saythe proponent, is assumed to have an explicit premise about a proposition, if the opponent has an explicit premiseabout this proposition, and as long as it (the proponent) does not deny or retract the proposition.6. A variation of Brewka’s reconstruction of RTFDAlthough it is our general aim in this paper to present a reconstruction of Brewka’s account of RTFD we make thefollowing adjustments to Brewka’s version. We have in mind a setting where autonomous software agents in a MASengage in the argumentation as part of a negotiation or dispute resolution process. In this setting, the protocol actions784A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804would be transmissions of messages. During the argumentation, the proponent, opponent and determiner must performtheir chosen actions by specified deadlines (timeouts). Without this feature there is no practical way of controlling theexchanges, of determining whether a participant has ‘spoken’, because otherwise one might have to wait indefinitelyfor messages to arrive over the communication channels. For similar reasons, it is also necessary to impose a limit onthe number of exchanges, or on the total elapsed time for the argumentation process. (Although Brewka states thatthe argumentation regarding a proposition may terminate due to a deadline, he provides no further details about howthis would work and how such deadlines would affect the argumentation process.) Having introduced deadlines, itis necessary to express the conditions in which an action can be said to be timely, that is, whether a participant has‘spoken’ by the specified deadline, and so on.We further refine Brewka’s distinction between ‘possible’ and ‘legal’ actions. We consider not only what kinds ofactions are proper (one possible interpretation of the Situation Calculus predicate poss—see, for example, axiom (5))but also which of these actions each agent will be practically able to perform at each stage of a given implementation.Moreover, we express the conditions in which an action can be said to be permitted (one possible interpretationof Brewka’s ‘legal’ actions) or even obligatory. We specify enforcement strategies to deal with the performance offorbidden actions and non-compliance with obligations.Finally, even though we depart from Brewka’s reconstruction of RTFD in the aforementioned points, we maintain(a form of) the object mechanism.Here then is our variant of Brewka’s reconstruction of RTFD. We will refer to it as RTFD∗. The argumentationcommences when the proponent claims the topic of the argumentation—any other action does not count as the com-mencement of the protocol. The protagonists (proponent and opponent) then take it in turn to perform actions. Eachturn lasts for a specified time period during which the protagonist may perform several actions (send several messages)up to some specified limit. After each such action the other participants are given an opportunity to object within an-other specified time period. In other words, Ag’s action Act is followed by a time period during which Ag may notperform any actions and the other participants may object to Act. The determiner may declare the winner only at theend of the argumentation, that is, when the specified period for the argumentation elapses. (Other specifications ofthe protocol’s terminating conditions are possible—for instance, the argumentation may end earlier than the specifiedtime period if a protagonist concedes to the other’s arguments.) If at the end of the argumentation both the proponentand opponent have accepted the topic of the argumentation, then the determiner may only declare the proponent thewinner. If, however, the proponent does not accept the topic then the determiner may only declare the opponent thewinner. Finally, if the proponent accepts the topic and the opponent does not, the determiner has discretion to declareeither of them the winner. It may also have an obligation to decide one way or the other, depending on which versionof the protocol we choose to adopt.7. Specifying RTFD∗We present a C+ action description DRTFD∗that expresses the specification of RTFD∗. Table 1 shows a subset ofthe action signature (σ f, σ a) of DRTFD∗. Variables are written with an upper-case first letter and constants start witha lower-case letter. The intended reading of the constants of (σ f, σ a) will be explained during the presentation of theRTFD∗ specification.There are three roles in the argumentation protocol: proponent, opponent, and determiner. Although our specifi-cation does not rely on the assumption that there is at most one agent occupying a role in any given execution ofthe protocol, in the concrete example presented we will deal with the usual case where there are three agents, one ineach role. In the protocol presented agents do not change role. We will call pro the agent that occupies the role ofthe proponent, opp the agent that occupies the role of the opponent and det the agent that occupies the role of thedeterminer. Accordingly, the example action description contains the following three static laws:2role_of (pro) = proponent if (cid:4)role_of (opp) = opponent if (cid:4)role_of (det) = determiner if (cid:4)2 For brevity, we will omit in the remaining of the paper the keyword caused which appears at the beginning of static and dynamic laws in theoriginal presentation of C+ [22].A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804785Table 1A subset of the action signature of DRTFD∗Variables:Ag, Ag(cid:8)Protag, Protag(cid:8)DetP, QActDomain:{pro, opp, det}{pro, opp}{det}a finite set of propositions/default rules{claim(Protag, Q), concede(Protag, Q), retract(Protag, Q),deny(Protag, Q), declare(Det, Protag)}Rigid Constants:role_of (Ag)topicimplies(P, Q)Domain:{proponent, opponent, determiner}a finite set of propositionsBooleanSimple Fluent Constants:turnpremise(Protag, Q)initialState, sanctioned(Ag)winnerDomain:{proponent, opponent, determiner}{t, f, u}Boolean{pro, opp, none}Statically Determined Fluent Constants:proper(Act), timely(Act), per(Act), obl(Act),objectionable(Act), accepts(Protag, Q), fairwinningDomain:Boolean{pro, opp, none}Action Constants σ act (Boolean):claim(Protag, Q), concede(Protag, Q), retract(Protag, Q),deny(Protag, Q), declare(Det, Protag)Action Constants σ aobj (Boolean):objected(Ag)The role_of (Ag) fluent constants are thus ‘rigid’, in the sense that their values are the same in all states (the Ag variableranges over the participants pro, opp and det). In other versions of the protocol, we might introduce other roles, forinstance that of an ‘observer’ who does not participate in the argumentation proper but could be allowed to object toactions made by the protagonists. We do not show that variation here.Other rigid constants are topic, whose value is a proposition expressing the topic of the argumentation, and Booleanfluent constants implies(P , Q), used to represent the underlying logic of disputation, as described in a followingsection.Each simple fluent constant of DRTFD∗is inertial, that is to say, its value persists by default from one state to thenext. The constraint that a fluent constant f is inertial is expressed in C+ by means of the causal law abbreviationinertial f(7)C+ abbreviations were presented in Section 2.The action constants σ a of DRTFD∗are partitioned into two sets, σ act and σ aobj . The set σ act includes the mainactions (claim, concede, retract, deny, declare) of the argumentation, as summarised in Table 1. For convenience, wespecify that exactly one action from σ act takes place at each state transition. This is done by means of the followingC+ laws:nonexecutable αi ∧ αj ,for all αi, αj ∈ σ act, αi (cid:12)= αj(8)together withnonexecutable ¬α1 ∧ ¬α2 ∧ · · · ∧ ¬αn(9)where α1, α2, . . . , αn are the action constants of σ act. The restriction expressed by laws (8) and (9) simplifies theRTFD∗ specification, and is also convenient when analysing executions of the protocol (see Sections 8 and 9).786A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804The action constants of σ aobj are Boolean constants of the form objected(Ag) used to represent that an objection hasbeen made by agent Ag. In the specification presented in this paper we have abstracted away details of how an objectionis transmitted within the specified deadline (recall that every action Act is followed by a time period during whichno action may take place apart from an objection to Act). Instead, every transition of the transition system defined byDRTFD∗corresponds to a claim, concede, retract, deny, or declare action by one of the participants together with anindication of whether that action was objected to by one or more of the others. For example, a transition satisfying theaction formulaclaim(pro, Q) ∧ objected(opp) ∧ objected(det)represents a transition in which a claim that Q by pro is followed by objections by both opp and det. Similarly, atransition satisfyingclaim(pro, Q) ∧ ¬objected(opp) ∧ ¬objected(det)expresses a claim that has not been objected to. We could have produced a more elaborate C+ action descriptionexpressing the objection mechanism and its associated deadlines but the details are rather fiddly and we do not presentthem here so as not to distract attention from the main points of the paper. Notice that because of laws (8) and (9),the objected(Ag) action constants do not need to specify which action has been objected to since there is exactlyone such action at each state transition. We do find it useful to specify the agent who objected, and to allow forthe possibility that more than one agent objected within the deadline. Other versions of the specification may beconstructed that do not include laws (8) and (9) but the representation of objections is then more intricate. We omitthe details.We begin by specifying the well-formed actions of the RTFD∗ protocol. A claim(Ag, Q) action, for example,is well-formed only if Ag is an agent occupying the role of the proponent or that of the opponent. There are twoways of specifying the well-formed actions. The simpler method, and the one we choose here, is to specify this inthe action signature. For instance, the action signature summarised in Table 1 contains action constants of the formclaim(Protag, Q) where Protag ranges over the two protagonists, pro and opp. The alternative method is to includeill-formed actions in the action signature and specify explicitly that they are ‘non-executable’, as in the followingexample:nonexecutable claim(Ag, Q) if role_of (Ag) = determinerAccording to the above law, there is no transition in the transition system defined by DRTFD∗corresponding to aclaim made by an agent occupying the role of determiner. This latter way of dealing with ill-formed actions resultsin a more generic and flexible specification that would require fewer changes to accommodate participants’ changingroles (assuming protocol versions in which this was possible), new agents participating in the protocol, and so on. Theformer way (which we choose here) results in a specification with simpler laws (they have fewer conditions) and isthus easier to follow.An objection is not well-formed if an agent objects to its own actions. Accordingly, we include laws of the followingformnonexecutable claim(Protag, Q) ∧ objected(Protag)(10)and similarly for the other action constants in σ act. We do not support objections to objections since they are of littlepractical or theoretical interest.We now present a number of causal laws expressing when a well-formed protocol action of RTFD∗ is physically(practically) possible, proper, timely, permitted or obligatory, and what the effects of an action are.7.1. Physical capabilityThe system events of the RTFD∗ specification are the timeouts—these are issued by a global clock. (To avoidclutter, several constants of the action signature of DRTFD∗, including those representing timeouts, are not presented inTable 1.) A type of timeout event is used to denote the turn of each participant. When RTFD∗ commences (this happenswhen the proponent claims the topic of the argumentation) a global clock starts ‘ticking’. The first timeout signals theA. Artikis et al. / Artificial Intelligence 171 (2007) 776–804787end of the proponent’s turn and the beginning of the opponent’s turn to ‘speak’, by setting turn = opponent. The nexttimeout signals the end of the opponent’s turn and the beginning of the proponent’s turn, by setting turn = proponent,and so on.The remaining actions of the RTFD∗ specification are those performed by the protocol participants. We have chosento specify that any protagonist is always capable of signalling a claim, concede, retract, deny, and object action, andthe determiner is always capable of signalling a declare and object action (except that no agent is capable of objectingto its own actions). The effects of these actions are presented next.At the initial state of the protocol the protagonists have no premises, that is, the value of every premise(Protag, Q)fluent constant is f. The variable Q ranges over the formulas (propositions and default rules) that the two protagonistsmay claim, concede to, retract, and deny (see Table 1). We assume that a finite number of such formulas can beidentified and specified at the outset. This is necessary for implementation in C+ and the CCALC system, though notnecessarily in other formalisms.The protocol commences with the proponent’s claim of the topic. The effects of a claim action are expressed asfollows:claim(Protag, Q) causes premise(Protag, Q) = t ifpremise(Protag, Q) = f ∧¬objected(pro) ∧ ¬objected(opp) ∧ ¬objected(det)The above expression is an abbreviation for the C+ fluent dynamic law:premise(Protag, Q) = t if (cid:4) afterclaim(Protag, Q) ∧ premise(Protag, Q) = f ∧¬objected(pro) ∧ ¬objected(opp) ∧ ¬objected(det)(11)Law (11) expresses that Protag’s claim of Q leads from a state in which Protag has no explicit premise that Q (that is,premise(Protag, Q) = f) to a state in which it does have an explicit premise that Q (that is, premise(Protag, Q) = t),on condition that no (other) agent, pro, opp, or det, objects to the claim. An objection is only effective in blocking theeffects of the claim action if it is well-founded (in a sense to be specified below). If the objection is not well-foundedthen it does not block the effects of the claim action (though it might have other effects, such as exposing the objectingagent to sanctions). We therefore add to law (11) the further constraint that:claim(Protag, Q) causes premise(Protag, Q) = t ifpremise(Protag, Q) = f ∧¬objectionable(claim(Protag, Q))(12)Boolean fluent constants objectionable(Act) are used to represent that an objection to Act is well-founded.When is an objection to an action Act ‘well-founded’, that is to say, when is objectionable(Act) true? In gen-eral, whenever Act is not a proper and timely action of the protocol. However, we find it adds flexibility to defineobjectionable separately. Not all improper or untimely actions need to be objectionable. As we explain in the nextsection, sometimes improper or untimely actions have no effect on the state of the argumentation, and in those cir-cumstances the protocol can be simplified by ignoring objections to them. Specification of the proper, timely, andobjectionable actions in RTFD∗ will be given in the next section.For convenience we introduce a special abbreviation, writingAct p_causes F if G(for ‘provisionally causes’) as shorthand for the pair of causal laws of the formAct causes F if G ∧ ¬objected(Ag1) ∧ · · · ∧ ¬objected(Agn)Act causes F if G ∧ ¬objectionable(Act)where Ag1, . . . , Agn are the participants in the argumentation (pro, opp, and det in the present example). When G is(cid:4) we writeAct p_causes F788A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804The laws (11) and (12) stating the effects of a claim can thus be written succinctly as follows:claim(Protag, Q) p_causes premise(Protag, Q) = t ifpremise(Protag, Q) = f(13)Suppose that protagonist Protag claims a proposition Q. Opponent Protag(cid:8) may respond to Protag’s claim by con-ceding to, or denying the claim. If Protag(cid:8) does neither then we say that Protag(cid:8) has an ‘unconfirmed’ premise thatQ, denoted by premise(Protag(cid:8), Q) = u. The value of a premise fluent constant is set to ‘unconfirmed’ as follows, forevery pair of distinct protagonists Protag and Protag(cid:8):claim(Protag, Q) p_causes premise(Protag(cid:8), Q) = u ifpremise(Protag, Q) = f ∧premise(Protag(cid:8), Q) = f(14)In other words, Protag’s claim of Q leads (subject to possible objections) to a state in which Protag(cid:8) has an un-confirmed premise that Q, provided that Protag(cid:8) does not already have a premise that Q (that is, provided thatpremise(Protag(cid:8), Q) = f). If Protag(cid:8) already has a premise that Q (that is, premise(Protag(cid:8), Q) = t) then its premisedoes not become unconfirmed, and it does not need to respond to Protag’s claim.Responding to a claim, that is, conceding to or denying a claim, can be expressed as follows:concede(Protag, Q) p_causes premise(Protag, Q) = t ifpremise(Protag, Q) = udeny(Protag, Q) p_causes premise(Protag, Q) = f ifpremise(Protag, Q) = uA claim may be retracted. The effects of a retraction are twofold:retract(Protag, Q) p_causes premise(Protag, Q) = f ifpremise(Protag, Q) = tretract(Protag, Q) p_causes premise(Protag(cid:8), Q) = f ifpremise(Protag, Q) = t ∧premise(Protag(cid:8), Q) = u(15)(16)(17)(18)for every pair of distinct protagonists Protag and Protag(cid:8). According to law (17), Protag’s retraction of its claim thatQ results in premise(Protag, Q) = f; according to law (18) this retraction also removes the unconfirmed premise heldby the other protagonist, Protag(cid:8). If Protag(cid:8) responded to Protag’s claim before Protag retracted it then the value ofpremise(Protag(cid:8), Q) would not be unconfirmed, and in that case Protag’s retraction will not affect the premise heldby Protag(cid:8).The effects of laws (13)–(18) for a single proposition Q, and assuming no objections, are summarised in Fig. 2.Notice that, because the premise fluent constants are inertial, claim, concede, deny, and retract actions have no effecton the argumentation state if the pre-conditions in laws (13)–(18) are not satisfied. Transitions corresponding to suchactions are omitted from the diagram for clarity.At the end of the argumentation the determiner may declare the winner—the effects of this action can be expressedas follows:declare(Det, Protag) p_causes winner = Protag(19)Det represents the agent occupying the role of the determiner. The fluent constant winner = Protag expresses that thewinner of the argumentation is Protag. The use of p_causes here again deals with the possibility that some agentother than Det objects to the declaration.Laws (13)–(19) express the effects of the main protocol actions. Suppose that a protagonist claims a propositionQ when it already has a premise that Q, or that it sends a retract(Protag, Q) message when it has no premise that Q.Such actions are not proper, according to the protocol. Moreover, an action may be untimely, as when, for instance, aprotagonist speaks out of turn. Some improper or untimely actions have no effects on the state of the argumentation.Others do have effects—however, their effects may be blocked by objections. In the next section we specify when aprotocol action is proper and timely, and we specify when an action is objectionable.A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804789Fig. 2. Effects of claim, concede, deny, retract actions. The proposition Q is omitted for clarity. Pr, Pr(cid:8)are shorthands for Protag, Protag(cid:8)respectively. Similarly, p(Pr), p(Pr(cid:8)) are shorthands for premise(Protag, Q) and premise(Protag(cid:8), Q). Actions that do not change the protocol stateare not shown in the diagram.Table 2Proper and timely actions in DRTFD∗Actionclaim(Protag, Q)concede(Protag, Q)retract(Protag, Q)deny(Protag, Q)declare(Det, Protag)7.2. Proper and timely actionsproperpremise(Protag, Q) = fpremise(Protag, Q) = upremise(Protag, Q) = tpremise(Protag, Q) = uwinner (cid:12)= Protag ∧( winning = Protag ∨winning = none )timely( ¬initialState ∨topic = Q ) ∧turn = role_of (Protag)¬initialState ∧turn = role_of (Protag)¬initialState ∧turn = role_of (Protag)¬initialState ∧turn = role_of (Protag)turn = role_of (Det)The second column of Table 2 shows the conditions in which the main protocol actions are said to be proper.A claim(Protag, Q) action, for example, is proper if and only if Protag does not have a premise that Q, that is,premise(Protag, Q) = f:proper(claim(Protag, Q)) iff premise(Protag, Q) = fIn other words, a claim(Protag, Q) action is improper when:(20)• premise(Protag, Q) = t; it is improper to make repeated claims, that is, claim something one has a premise about.• premise(Protag, Q) = u; this signifies that the other protagonist Protag(cid:8) has claimed Q (see law (14)). A properresponse to the other’s claim is either a concession or a denial (see Table 2).A retract(Protag, Q) action is proper if and only if premise(Protag, Q) = t; otherwise there is nothing to retract.790A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804The reader may have noticed that the conditions for proper claim, concede, deny, and retract actions coincidewith the premise(Protag, Q) pre-conditions in the laws (13)–(18). This is a feature of Brewka’s version of RTFD (orrather, our chosen formulation of some of its details) and will not necessarily be the case in other examples. In otherargumentation and dialogue protocols it is common, for example, to say that a claim by Protag is not proper if it isinconsistent, in the underlying logic of disputation, with the premises currently held by Protag. This is easily addedin our formulation but is not a feature of Brewka’s RTFD. Other formulations of proper actions will be discussed inSection 10.We now turn to declarations. A declaration is proper when the following conditions are satisfied:proper(declare(Det, Protag)) iffwinner (cid:12)= Protag ∧(winning = Protag ∨ winning = none)(21)In words: declaring Protag the winner of the argumentation is proper if and only if:• Protag has not already been declared winner, and• Protag is currently ‘winning’, that is, the conflict is resolved in favour of Protag, or no protagonist is ‘winning’,that is, the conflict is unresolved.The statically defined fluent constant winning is defined as follows:winning = Protag ifrole_of (Protag) = proponent ∧role_of (Protag(cid:8)) = opponent ∧topic = Q ∧accepts(Protag, Q) ∧accepts(Protag(cid:8), Q)winning = Protag(cid:8)ifrole_of (Protag) = proponent ∧role_of (Protag(cid:8)) = opponent ∧topic = Q ∧¬accepts(Protag, Q)default winning = none(22)(23)(24)Laws (22) and (23) express a resolved dispute. An accepts(Protag, Q) fluent constant states that Q follows, in thelogic of disputation, from Protag’s explicit premises. (A discussion of the logic of disputation is presented in Section7.5.) Law (22) deals with the case that both protagonists accept the topic Q of the dispute; law (23) deals with the casewhere the proponent no longer accepts the topic Q. Law (24) corresponds to an unresolved dispute; in the absence ofinformation to the contrary (that is, laws (22) and (23)), the dispute is unresolved.Even if proper, an action may not be in compliance with the protocol specification—for example, it may be un-timely. The third column of Table 2 shows the conditions in which the main protocol actions are said to be timely.At the initial state of the protocol, expressed by the simple fluent constant initialState, only the proponent’s claim ofthe argumentation topic is timely (at the initial protocol state turn = proponent). At the other protocol states a protag-onist’s claim, concession, retraction, or denial is timely if it is the protagonist’s turn to ‘speak’. A timely claim, forinstance, is expressed in C+ as follows:timely(claim(Protag, Q)) iff(¬initialState ∨ topic = Q) ∧turn = role_of (Protag)Declarations are defined to be timely as follows:timely(declare(Det, Protag)) iff turn = role_of (Det)(25)(26)A declaration is timely if and only if it is the determiner’s turn to ‘speak’—this happens when the argumentation timeelapses.A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804791Table 3Objectionable actions in DRTFD∗Actionclaim(Protag, Q)concede(Protag, Q)retract(Protag, Q)deny(Protag, Q)declare(Det, Protag)objectionableproper(claim(Protag, Q)) ∧ ¬timely(claim(Protag, Q))proper(concede(Protag, Q)) ∧ ¬timely(concede(Protag, Q))proper(retract(Protag, Q)) ∧ ¬timely(retract(Protag, Q))proper(deny(Protag, Q)) ∧ ¬timely(deny(Protag, Q))¬( proper(declare(Det, Protag)) ∧timely(declare(Det, Protag)) ) ∧ winner (cid:12)= ProtagIn this version we do not allow for early declarations, in the case where both protagonists accept the argumentationtopic, or the proponent does not accept it, before the argumentation time elapses; this can be easily adjusted.What about improper declarations? Or untimely actions? Some of these actions do have effects on the protocolstate—however, these effects may be blocked by a well-founded objection. Table 3 presents the conditions in whichan objection to Act is well-founded, that is, when Act is objectionable. In the present example a claim is objectionableif and only if it is untimely and proper:objectionable(claim(Protag, Q)) iffproper(claim(Protag, Q)) ∧ ¬timely(claim(Protag, Q))(27)According to the protocol specified above, improper claims do not have effects on the protocol state. It is therefore notnecessary to object to them. Similarly, a concession, retraction or denial is objectionable if and only if it is untimelyand proper.A declare(Det, Protag) action is objectionable if and only if: (i) it is not proper and timely, and (ii) winner (cid:12)= Protag(see Table 3). Notice that if winner = Protag at the time of a declare(Det, Protag) action then this action will not haveany effects on the value of the winner fluent constant. Accordingly, we choose to say that an improper or untimelydeclare(Det, Protag) action is objectionable only when winner (cid:12)= Protag at the time of the declaration.We presented the conditions in which an action is considered proper or timely, and the consequences of theperformance a proper/improper, timely/untimely action. In the following section we focus on a separate issue: the cir-cumstances in which a protocol participant is permitted or even obliged to perform a particular action. In Section 7.4we discuss potential consequences of the performance of forbidden actions and non-compliance with obligations.7.3. Permitted actionsIn Section 5 we identified a side-effect of the object mechanism: Ag’s repeated performance of objectionable actionsrequires repeated objections from some other participant Ag(cid:8) otherwise the effects of Ag’s objectionable actions arenot blocked. One way of addressing this issue is to say that the performance of repeated objectionable actions is notpermitted. Consider the following example specification of permission:per(concede(Protag, Q)) iff¬objectionable(concede(Protag, Q)) ∨objActions(Protag) < 10(28)Here, there is an upper limit (10 in the example) on the total number of objectionable actions a protagonist Protagis permitted to make. The per fluent constants express permission; the simple fluent constants objActions(Ag) recordthe number of objectionable actions issued by Ag. The formulation of C+ laws to maintain the objActions(Ag)fluent constants is very straightforward and we omit the details. Laws similar to (28) express the permission to claim,retract, deny and declare (we may impose a lower threshold for the permitted objectionable declarations). In thepresent example, an objection is never objectionable; therefore, an objection is always permitted.In another formulation, objActions(Ag) could express the number of objectionable actions issued by Ag to whichwell-founded (successful) objections have been made by other participants. In this case, Ag would be permitted toperform objectionable actions until the number of successful objections issued by the other participants to Ag’s actionshas reached the specified limit.792A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804We could formulate further permission laws to classify as not permitted other kinds of exchanges, such as repeatedclaims and retractions by the protagonists (‘yes it is’; ‘no it isn’t’; ‘yes it is’; ‘no it isn’t’; . . . ). Although such anexchange could be both proper and timely according to the protocol, it may nevertheless be undesirable—it does notprogress the resolution of the dispute.In some cases it is meaningful to say that the determiner is not only permitted, but obliged to declare a protagonistthe winner of the dispute. Such an obligation will arise when the following conditions hold:obl(declare(Det, Protag)) iffturn = role_of (Det) ∧winner (cid:12)= Protag ∧winning = Protag(29)(The obl fluent constants express obligation.) According to law (29), it is obligatory to declare Protag the winner ofthe dispute if and only if: (i) it is the determiner’s turn to ‘speak’, (ii) Protag has not already been declared the winner,and (iii) Protag is currently ‘winning’ the dispute, that is, the dispute is resolved in favour of Protag.Given expression (29), it is necessary to represent explicitly the relationship between the permission and obligationto declare the winner. For instance, the protocol should never reach a state in which both obl(declare(Det, Protag))and per(declare(Det, Protag(cid:8))) are true for Protag (cid:12)= Protag(cid:8). One way is to add the following laws, for every pair ofdistinct protagonists Protag and Protag(cid:8):per(declare(Det, Protag)) ifobl(declare(Det, Protag))per(declare(Det, Protag)) if¬obl(declare(Det, Protag(cid:8))) ∧¬objectionable(declare(Det, Protag))per(declare(Det, Protag)) if¬obl(declare(Det, Protag(cid:8))) ∧objActions(Det) < 10default ¬per(declare(Det, Protag))(30)(31)(32)(33)According to laws (30)–(33), if the determiner is obliged to declare Protag the winner then is it is also permitted todeclare Protag, but not Protag(cid:8). When there is no obligation on the determiner, at most ten objectionable declarationsare permitted (that is, the permission to declare is expressed as the permission to perform any other protocol action).It is not meaningful to associate obligations with the remaining protocol actions; therefore, we do not need toupdate the specification of permitted claims, concessions, retractions, denials or objections. Table 4 presents the con-ditions in which a main protocol action is permitted or obligatory. Clearly, different specifications are possible. OneTable 4Permission and obligation in DRTFD∗Actionclaim(Protag, Q)concede(Protag, Q)retract(Protag, Q)deny(Protag, Q)declare(Det, Protag)per¬objectionable(claim(Protag, Q)) ∨objActions(Protag) < 10¬objectionable(concede(Protag, Q)) ∨objActions(Protag) < 10¬objectionable(retract(Protag, Q)) ∨objActions(Protag) < 10¬objectionable(deny(Protag, Q)) ∨objActions(Protag) < 10obl(declare(Det, Protag) ∨( ¬obl(declare(Det, Protag(cid:8))) ∧( ¬objectionable(declare(Det, Protag))∨objActions(Det) < 10 ) )obl⊥⊥⊥⊥turn = role_of (Det) ∧winner (cid:12)= Protag ∧winning = ProtagA. Artikis et al. / Artificial Intelligence 171 (2007) 776–804793may forbid, for instance, repetitive improper claims, concessions, retractions, and denials, and ill-founded objections(objections to Act when Act is not objectionable). Although these actions have no effects on the protocol state andthus are not objectionable, they may nevertheless be forbidden (not permitted) in order to deter participants from per-forming ‘meaningless’ actions, to decrease network traffic. In [4] we presented an RTFD∗ specification in which allretractions were forbidden, whether proper/improper, timely/untimely, objectionable/non-objectionable. Furthermore,the obligation to declare the winner could arise even in the case of an unresolved dispute.Note that it is practically possible for an agent to perform forbidden actions and not comply with its obligations.We introduce enforcement strategies as a way of dealing with this type of behaviour. Such strategies are presentednext.7.4. Enforcement strategiesWe want to reduce or eliminate:• The protagonists’ performance of forbidden actions, repeated objectionable actions in this example.• The determiner’s non-compliance with the obligation to declare the winner of the dispute. (To simplify the pre-sentation, we will not address the determiner’s forbidden declarations.)We employ the simple fluent constants sanctioned to identify the aforementioned types of behaviour. Protag’s forbid-den claim, for example, results in initiating sanctioned(Protag):claim(Protag, Q) causes sanctioned(Protag) if¬per(claim(Protag, Q))(34)One way of reducing the performance of forbidden actions and non-compliance with obligations is by penalising suchbehaviour. For example: if at the close of the argumentation the dispute is unresolved, we could say that declaringProtag the winner is objectionable if Protag performed some forbidden actions but Protag(cid:8) did not. In this case, thepenalty Protag pays for its forbidden actions is that it will not win an unresolved dispute if Protag(cid:8): (i) does notperform forbidden actions, and (ii) objects to a potential declare(Det, Protag) action. (This does not necessarily implythat Protag(cid:8) will win the dispute.) The specification of this example sanction may expressed by updating the definitionof objectionable declarations as follows (for every pair of distinct protagonists Protag and Protag(cid:8)):objectionable(declare(Det, Protag)) if¬proper(declare(Det, Protag)) ∧winner (cid:12)= Protagobjectionable(declare(Det, Protag)) if¬timely(declare(Det, Protag)) ∧winner (cid:12)= Protagobjectionable(declare(Det, Protag)) ifwinning = none ∧winner (cid:12)= Protag ∧sanctioned(Protag) ∧¬sanctioned(Protag(cid:8))default ¬objectionable(declare(Det, Protag))(35)(36)(37)(38)Laws (35), (36) and (38) express the definition of objectionable declarations presented in Table 3; law (37) expressesthe protagonists’ sanctions. When turn = determiner (the argumentation has ended), winning = none (the dispute isunresolved) and winner = none (no protagonist is declared winner), declaring Protag would be proper, timely andnon-objectionable. If, however, Protag is sanctioned and Protag(cid:8) is not, declaring Protag would be proper, timely butobjectionable. (If both protagonists are sanctioned then objectionable declarations are defined only by laws (35), (36)and (38), that is, proper, timely declarations are non-objectionable.)Similarly, we could have expressed the protagonists’ sanctions in terms of objectionable claims, concessions,retractions, and so on. For example, we could have specified that the retractions of a sanctioned protagonist areobjectionable, even if proper and timely, and therefore, their effects could be blocked by objections.794A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804In addition to dealing with the protagonists’ forbidden actions, we want to reduce the possibility that the determinerwill not comply with its obligation. Assuming that the determiner is obliged to declare Protag the winner of the dispute(see law (29)), the determiner may:• declare Protag(cid:8) the winner (more precisely, the last declaration before the last protocol timeout concerns Protag(cid:8)),• make no declaration, or• declare Protag the winner.In the third case the obligation is (temporarily) discharged. New evidence, however, may arise after the declaration(and before the last protocol timeout), such as an untimely retraction by Protag, obliging the determiner to declareProtag(cid:8) the winner (by setting winning = Protag(cid:8)) and this obligation may not be discharged.Notice that the employed object mechanism is inadequate for ensuring a ‘fair’ result in the three aforementionedscenarios. (A ‘fair’ result implies that Protag is declared winner if the dispute is resolved in its favour.) In the firstcase, a well-founded objection to the action declare(Det, Protag(cid:8)) will block the effects of the declaration but will notforce winner = Protag. In the second case there is no declaration to object to, and in the third case the declaration wasnot objectionable at the time.What sanctions should be enforced in these cases? There are a number of possibilities. We may specify, for instance,that a sanctioned determiner is disqualified from acting as a determiner in future argumentations, for a specified timeperiod perhaps. If the argumentation takes place in the context of a computational system including several protocols,such as a system of negotiation or deliberative assemblies, a sanctioned determiner may have restricted permissionsor its actions may be objectionable when participating in the remaining protocols, possibly occupying other roles. Inaddition, we may specify that it is proper and timely to initiate proceedings against a sanctioned determiner (assumingthe existence of an adjudicating authority) in order to enforce compliance with the determiner’s obligation, not as a‘punishment’ to the determiner but as a way of discouraging it to avoid complying with the protocol rules and ensuring‘fairness’.Sanctions are one means by which the performance of forbidden actions and non-compliance with obligations maybe addressed. Another possible strategy is to devise physical controls that will force agents to comply with the protocolrules. For instance, repetitive objectionable actions may be physically blocked (since, in the present example, they areforbidden). The general strategy of designing mechanisms to force compliance and eliminate non-permitted behaviouris what Jones and Sergot [26] referred to as regimentation. Regimentation devices have often been employed in orderto eliminate ‘anti-social’ behaviour in computational systems (see, for instance, [28,37,54]). It has been argued [26],however, that regimentation is rarely desirable (it results in a rigid system that may discourage agents from entering it[45]), and not always practical. In any case, violations may still occur even when regimenting a computational system(consider, for instance, a faulty regimentation device). For all of these reasons, we have to allow for sanctioning andnot rely exclusively on regimentation mechanisms.7.5. Additional considerationsWe presented a formalisation of the RTFD∗ protocol rules. In order to perform computational experiments (suchas those presented in Section 9), we need to code up (fragments of) the logic of disputation. The following law, forexample, states that a protagonist accepts all (classical) logical implications of each of its premises:accepts(Protag, Q) iffpremise(Protag, P) = t ∧ implies(P, Q)(39)The implies here are simply suitably chosen rigid constants. In a similar manner, we may specify the acceptance ofpropositions as a result of the conjunctions of an agent’s premises and of premises regarding default rules.We have formalised only a small fragment of the logic of disputation, enough to conduct simple experiments.The focus of this paper lies on the protocol rules rather than the logic of disputation (that, we assume, does notnecessarily have to be prioritised default logic as in Brewka’s version). Moreover, a complete formalisation of thelogic of disputation (on top of a formalisation of the protocol rules) would substantially increase the number ofDRTFD∗laws, thus significantly increasing CCALC’s time of computing answers to queries (regarding DRTFD∗). Thesecomplications could be addressed by employing alternative action languages. We discuss this issue in Section 11.A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804795Brewka, in his reconstruction of RTFD, places emphasis on the formalisation of the ‘silence implies consent’principle: a protagonist that does not explicitly challenge a claim by the other protagonist is assumed to concede tothe claim. We may incorporate this principle in RTFD∗ by modifying the formalisation of the logic of disputation asfollows:accepts(Protag, Q) iff(premise(Protag, P) = t ∨ premise(Protag, P) = u) ∧implies(P, Q)(40)Recall that premise(Protag, P) = u expresses that Protag has an unconfirmed premise that P , that is, Protag hasnot responded to a claim that P made by the other protagonist. In a protocol adopting the ‘silence implies consent’principle, a protagonist accepts all logical implications of each of its explicit and unconfirmed premises.Note that if we do not want to incorporate the ‘silence implies consent’ principle then we disregard unconfirmedpremises in the logic of disputation, by using law (39) instead of (40).8. Proving properties of RTFD∗The explicit transition systems semantics of the C+ language enables us to prove various properties of the presentedRTFD∗ specification (which is expressed by means of the definite action description DRTFD∗). We may prove, forexample, that as long as the determiner complies with its obligations the protocol result will be ‘fair’. Suppose wespecify ‘fairness’ as follows:fair iff winning = none ∨ winner = winning(41)The statically determined fluent constant fair holds in a protocol state if and only if the dispute is unresolved(winning = none) or the dispute is resolved in favour of the declared winner (winner = winning). We want to ex-amine whether or not a protocol result is ‘fair’, that is, whether or not the fluent constant fair is true in the final stateof a protocol execution.Proposition 1. The protocol result will be ‘fair’ if and only if in the final protocol state there is no obligation on thedeterminer to declare.Proof. First we will prove that if in the final protocol state there is no obligation on the determiner then the protocolresult will be ‘fair’. Assume a final protocol state s in which there is no obligation on the determiner and the result isnot ‘fair’:s (cid:12)|= obl(declare(det, Protag)),s |= winning (cid:12)= none ∧ winner (cid:12)= winning ∧ turn = role_of (det)for any Protag(Recall that role_of (det) = determiner is ‘rigid’. Moreover, in a final protocol state turn = determiner.)Now, s |= winning (cid:12)= none iff s |= winning = Protag for some protagonist Protag, and so s |= winning = Protag ∧, it is an interpretation of σ f such that s = Tstatic(s) ∪ Simple(s), where, s |= G} and Simple(s) denotes the set of simple fluent atoms satis-winner (cid:12)= Protag. Since s is a state of DRTFD∗Tstatic(s) =def {F | static law ‘F if G’ is in DRTFD∗fied by s (see Section 2.3). From law (29) and the fact thats |= turn = role_of (det) ∧ winning = Protag ∧ winner (cid:12)= Protagwe have that obl(declare(det, Protag)) ∈ Tstatic(s).According to our initial assumption, however, there is no obligation on the determiner in s, which implies thats (cid:12)= Tstatic(s) ∪ Simple(s). Therefore, s is not a state of DRTFD∗.Second we will prove that if the protocol result is ‘fair’ then in the final state there is no obligation on the determinerto declare. Assume a final protocol state s in which there is an obligation on the determiner to declare and the resultis ‘fair’. If fair is true in s then either winning = none is in s or winner = winning is in s. Consider first the casewinning = none:s |= obl(declare(det, Protag)) ∧ winning = none ∧ turn = role_of (det)796A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804for some Protag. Given law (29), we have that obl(declare(det, Protag)) /∈ Tstatic(s). Moreover, since obl are stati-cally determined fluent constants, we also have that obl(declare(det, Protag)) /∈ Simple(s). Therefore, s (cid:12)= Tstatic(s) ∪Simple(s) and s is not a state of DRTFD∗.The proof that s is not a state of DRTFD∗for the case in which winner = winning is similar. (cid:2)The obligation to declare might never arise during a protocol execution. Proposition 1 shows that in this case theresult will be ‘fair’. If an obligation to declare does arise, however, the protocol result is also guaranteed to be ‘fair’,as long as the determiner complies with the obligation.CCALC provides an automated means for proving properties of a protocol specification. We express the C+ actiondescription DRTFD∗to prove properties of theRTFD∗ specification. (The types of query that CCALC computes were presented in Section 3. Details of CCALC’sinput language may be found in [1,32].) Consider the following example:in CCALC’s input language and then query CCALC about DRTFD∗Proposition 2. A proper, timely concede(Protag, Q) action always leads to a state in which Protag has an explicitpremise that Q.We instruct CCALC to compute all states s(cid:8) such that• (s, ε, s(cid:8)) is a transition of DRTFD∗,• s |= proper(concede(Protag, Q)) ∧ timely(concede(Protag, Q)), and• ε |= concede(Protag, Q).For every state s(cid:8) computed by CCALC we obtains(cid:8) |= premise(Protag, Q) = tThis is because, briefly, a proper and timely concession is not objectionable in the presented RTFD∗ (see Table 3).Therefore, the effects of such an action, expressed by law (15), cannot be blocked. In other words, even whenε |= concede(Protag, Q) ∧ objected(Ag)the resulting state s(cid:8) always includes premise(Protag, Q) = t.More details about the computational experiments performed with CCALC on DRTFD∗are presented in the follow-ing section.We may prove further properties of the RTFD∗ specification, in the manner shown above, such as that the effectsof any non-objectionable action cannot be blocked, an objection always blocks the effects of an objectionable action,there is always a permitted action for a participant when it is its turn to ‘speak’, performing more objectionableactions than what is specified is always forbidden and sanctioned, the determiner is never obliged and forbidden todeclare the winner, and so on (the last two properties may be viewed as ‘discouragement of disruption’ and ‘rule-consistency’ [36]).In Section 11 we discuss alternative techniques for proving properties of a protocol specification.9. Executing RTFD∗Proving properties of a specification can be seen as a design-time activity. For instance, protocol designers maywish to prove properties of a protocol specification in order to determine whether or not this specification meets theirrequirements. Additionally, agents (or their designers) may wish to prove various properties of a protocol specificationwhen deciding whether to enter (deploy their agents in) that protocol.At run-time, we may execute a protocol specification to provide, amongst other things, information about theprotocol state current at each time. Computation of such information is a special case of a prediction query (seeSection 3). A protocol state—which actions are proper, timely, objectionable, permitted, and so on—may be publicisedto (a subset of) the protocol participants, or their designers. (Such run-time services may be provided by a central serveror in various distributed configurations. Further discussion of these architectural issues is outside the scope of thispaper.) Other run-time services include the calculation of plans, by means of computing answers to planning queries,A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804797and retrieval of past protocol states, by means of computing answers to postdiction queries (as already mentionedin Section 3, plans may be additionally computed at design-time). We show in this section example prediction andplanning queries on the RTFD∗ specification, and the results obtained. To save space, details of postdiction queryexamples are omitted.To execute the presented RTFD∗ protocol, one has to choose specific values for the following parameters:• Duration of timeouts; this can be expressed as the maximum number of messages that can be exchanged in thegiven communication channel in the interval defined by any two consecutive timeout events. In our formalisationa timeout duration is expressed as the maximum number of transitions that may take place between any twoconsecutive timeout events (in this specification a transition is labelled with either a main protocol action, objectedor not, or a timeout event).• Number of turns for each protagonist.• Number of permitted objectionable actions (see Section 7.3).To conduct computational experiments, arbitrary numerical values were chosen for these parameters. For a concreteillustration we will present here experiments in which at most two main protocol actions are physically possiblebetween two consecutive timeout events (that is, at most two transitions may take place between two consecutivetimeout events), each protagonist has three turns to ‘speak’, and one objectionable action is permitted. As alreadymentioned, the specified logic of disputation is very simple (only classical implication is allowed). Moreover, for theexperiments presented ‘silence implies consent’ is incorporated in the protocol specification.We mentioned earlier that the propositions that the two protagonists may claim, concede to, retract, and deny shouldalso be specified at the outset. (As will be discussed later, this restriction may be lifted by re-compiling DRTFD∗eachtime a protagonist claims a new proposition.) In order to keep sample runs small we will present queries and thecomputed results concerning only two propositions: the topic q, and p.The first column of Table 5 shows a sample run of RTFD∗; the information displayed on the remaining columns, thatis, which actions are proper, timely, objectionable, permitted and obligatory at each state, is produced by computinganswers to a number of prediction queries. To save space, in Table 5 claim is written as cl, concede as cn, retract asrtr, deny as dn and declare as dcl. Terms containing variables Protag and Q stand for all their instances.Consider the following prediction query: at the initial protocol state the proponent pro claims the topic of theargumentation q; which actions are proper, timely, objectionable, permitted or obligatory at the resulting state? Theanswer to this query is displayed in the six rows below the claim(pro, q) action labelled cl(pro, q).At the state resulting from claim(pro, q), proponent pro has a premise that q (see law (13)). Therefore, at that state,it is improper for pro to claim q (see law (20)). It is proper for pro, however, to claim p because, at that state, pro doesnot have a premise that p. The remaining information displayed in Table 5 is computed in a similar manner.Note that at the state resulting from claim(pro, q) it is timely only for pro to perform an action. This is so becauseit is the proponent’s turn to speak (see Table 2 for the specification of timely actions).claim(opp, p), concede(opp, q) and deny(opp, q) are objectionable in the state reached by the performance ofclaim(pro, q). This is due to the fact that these actions are untimely and proper. At the same state declaring eitherpro or opp is objectionable because in either case the declaration is untimely and winner = none (see Table 3 for thespecification of objectionable actions).All actions are permitted at the resulting state of the query presented above. Recall that an action is permitted ifit is not objectionable or the specified limit of objectionable actions has not been reached; in this sample run oneobjectionable action per participant is permitted. In the same state, no action is obligatory; obligations are associatedwith declarations and arise when it is the determiner’s turn to speak. (The specifications of permissions and obligationswere presented in Table 4.)The next action in the protocol run presented in Table 5 is a timeout that sets turn = opponent. Consequently, in thefollowing state it is timely only for opp to perform an action. After the timeout opp denies q while pro objects to opp’saction. The denial is not objectionable and, therefore, pro’s objection does not block the effects of opp’s action. Thenext action in the protocol run, however, claim(pro, p), is objectionable and thus opp’s objection blocks the effects ofpro’s action.798A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804Table 5A sample run of RTFD∗actcl(pro, q)propertimelyobjtimeoutdn(opp, q),objected(pro)cl(pro, p),objected(opp)cl(Protag, p)cn(opp, q)rtr(pro, q)dn(opp, q)dcl(det, pro)cl(Protag, p)cn(opp, q)rtr(pro, q)dn(opp, q)dcl(det, pro)cl(pro, p)cl(opp, Q)rtr(pro, q)dcl(det, Protag)cl(pro, p)cl(opp, Q)rtr(pro, q)dcl(det, Protag)cl(pro, Q)cn(pro, Q)rtr(pro, Q)dn(pro, Q)cl(opp, Q)cn(opp, Q)rtr(opp, Q)dn(opp, Q)cl(opp, Q)cn(opp, Q)cn(opp, Q)dn(opp, Q)cl(opp, Q)cn(opp, Q)cn(opp, Q)dn(opp, Q)cl(opp, p)cn(opp, q)dn(opp, q)dcl(det, Protag)cl(pro, p)rtr(pro, q)dcl(det, Protag)cl(pro, p)rtr(pro, q)dcl(det, Protag)cl(pro, p)rtr(pro, q)dcl(det, Protag)oblnonenonenonenoneperallallallcl(pro, q)cl(opp, Q)cn(Protag, Q)rtr(pro, p)rtr(opp, Q)dn(Protag, Q)dcl(det, Protag)After the performance of pro’s objectionable claim, pro is no longer permitted to perform an objectionable actionbecause it reached the specified limit of permitted objectionable actions. Non-conformance with this prohibition willsanction pro.A final remark on the protocol run presented in Table 5 concerns the ‘silence implies consent’ principle. Due tothis principle, at the first two states displayed in Table 5 it is proper to declare pro, but improper to declare opp, theargumentation winner. In these states pro is ‘winning’ the dispute because premise(pro, q) = t and premise(opp, q) = uwhich imply, due to ‘silence implies consent’ (see law (40)), that both protagonists accept q, the topic of argumentation(the winning fluent constant is defined by laws (22)–(24)). If ‘silence implies consent’ were not incorporated in theprotocol then premise(opp, q) = u would not imply that opp accepts the topic (see law (39)), and thus no protagonistwould be ‘winning’. In this case, it would be proper to declare either pro or opp the argumentation winner.Table 6 presents another sample protocol run of RTFD∗ and the associated information produced by computationsof query answers. The timeout sets turn = determiner. The proponent pro does not accept the argumentation topic andthus the determiner is obliged to declare the opponent opp the winner. Clearly, the next action of the presented run,declare(det, pro), does not discharge this obligation. The obligation is discharged when declare(det, opp), althoughpro objects to this declaration; pro’s objection does not block the effects of the declaration as the latter action is notobjectionable.In addition to producing the protocol state current at each time, we may compute plans in order to facilitate agentsto achieve their goals. Consider the following planning query: we are in a state in which: (i) it is the determiner’s turnto speak, (ii) the dispute is unresolved, that is, no protagonist is ‘winning’, and (iii) there is no declared winner. Findall paths to a final protocol state, that is, a state reached after the final timeout, in which the proponent is declared thewinner.CCALC finds several solutions to this query. All solutions include the action declare(det, pro) and the final timeout.Moreover, the declaration is either not objected, or the proponent is not the only protagonist sanctioned. If the propo-A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804799Table 6A sample run of RTFD∗acttimeoutdcl(det, pro)dcl(det, opp),objected(pro)propertimelyobjperoblcl(Protag, Q)dcl(det, opp)cl(Protag, Q)dcl(det, opp)dcl(det, Protag)cl(Protag, Q)dcl(det, pro)dcl(det, Protag)cl(Protag, Q)cl(Protag, Q)dcl(det, opp)cl(Protag, Q)dcl(det, opp)dcl(det, opp)dcl(det, opp)cl(Protag, Q)dcl(det, Protag)cl(Protag, Q)dcl(det, pro)cl(Protag, Q)dcl(det, Protag)nonenent was sanctioned and the opponent was not, the declare(det, pro) action would be objectionable (see law (37)) andthus an objection would block its effects.Here is another planning query: given the initial state of the protocol, is it possible to reach a state, within the, in which an agent: (i) has exceeded the limit of permitted objectionablemaximum number of transitions of DRTFD∗actions and, (ii) is not sanctioned?CCALC finds no solution within the maximum number of transitions of DRTFD∗. Given the chosen values for thenumber of turns of each protagonist (three), and the maximum number of transitions that may take place betweenany two consecutive timeouts (two), it can be calculated that the maximum number of transitions of DRTFD∗, that is,, is twenty-one. (Intuitively, in the longest path of DRTFD∗the number of transitions of the longest path of DRTFD∗the proponent performs two actions and then a timeout takes place, signalling the opponent’s turn; the opponent thenperforms two actions, followed by a timeout signalling the proponent’s turn. The aforementioned sequence of actionsis repeated three times, that is, the number of turns of each protagonist. Finally, the determiner performs two actionsand the last timeout takes place.) Since there is no solution within the maximum number of transitions, starting fromthe initial protocol state, we will never reach a state in which an agent has exceeded the limit of permitted objectionableactions and is not sanctioned.10. Related workIn Section 6 we discussed the points of departure of our formalisation from Brewka’s account. Briefly:• We employed the C+ language to specify the argumentation protocol, instead of the Situation Calculus. Moreover,we executed the protocol specification with the use of CCALC, thus providing several design-time and run-timeservices to protocol designers, agent designers and agents themselves.• We introduced deadlines to cater for realistic multi-agent protocols.• We refined Brewka’s distinction of possible and legal actions. In our formalisation an action can be classified asphysically possible, proper, timely, objectionable, permitted or obligatory.• We maintained the object mechanism; however, we forbid multiple objectionable actions to avoid having to objectto each such action.• We introduced sanctions in order to discourage participants from performing forbidden actions and not complyingwith obligations.• We retained the ‘silence implies consent’ principle, as an optional feature of the protocol.Apart from ‘static’ argument systems, Brewka [8, Section 6] formalises ‘dynamic’ argument systems, that is,argument systems in which participants can start a meta-level debate, arguing about the protocol rules. ‘Dynamic’protocol specifications are out of the scope if this paper.Argumentation protocols have long been studied in the fields of philosophy and computer science—see [48] fora recent review. The focus of this paper was on the procedural part of argumentation, that is, we focused on thespecification of the protocol rules rather than the logic of disputation. This is in contrast to research in non-monotonicand uncertain reasoning argument systems that, as Brewka [8] mentions, have been used to define inference systems800A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804for existing non-monotonic logics, or a non-standard consequence relation for logics based on a notion of argument.(See, for example, [7,10,15,31,44,50]; Chesñevar et al. [11], and Prakken and Vreeswijk [51] provide two surveys.)Different logics of disputation are suited to different types of argument. Our protocol formalisation was not bound toa specific logic of disputation.Like [8,24,25,46,47], we adopted a ‘public protocol semantics’ [48, Section 6], that is, we made no assumptionsabout the participants’ internal architectures. This is contrast to approaches that allow for protocol rules referring to aparticipant’s internal belief base (for instance, [2,30,40–43]).A line of research that is closely related to our work is that of Bodenstaff et al. [6]. These researchers employShanahan’s [60] ‘full Event Calculus’ to formalise Prakken’s [47] dialogue system for argumentation, and Parsons,Wooldridge and Amgoud’s [43] persuasion dialogue. Bodenstaff et al. formalise the procedural aspect of argumen-tation, expressing the ‘legal’ protocol moves in terms of a ‘reply structure’, distinguishing between ‘attacking’ and‘surrendering’ replies. In this paper we presented a finer classification of protocol actions than Bodenstaff and col-leagues’ classification of ‘legal’/‘illegal’ actions. Concerning the reply structure, Prakken [48] notes that it is not astandard feature of all dialogue systems (see, for example [21]). In any case, it is possible to adjust our formalisationin order to express a reply structure (in this paper our aim was to reconstruct Brewka’s account of RTFD that did notexplicitly include such a structure). This can be done by adjusting our specification of proper actions.Apart from research on argumentation, work that has similar objectives to ours comes from the distributed artificialintelligence literature on norm-governed systems specification. A few examples are the approaches on ‘artificial socialsystems’ [20,38,39,62,63,65], ‘law-governed interaction’ [37], and ‘electronic institutions’ [16–19,55]. Close to ourwork is Yolum and Singh’s [66,67] work on ‘commitment protocols’. These researchers formalise, in Shanahan’s ‘fullEvent Calculus’, a set of operations on commitments such as create, discharge, cancel, release, and so on. Moreover,they employ an Event Calculus planner [61] to facilitate the planning of commitment protocol participants. It isimportant to note that in Yolum and Singh’s work the term ‘commitment’ refers to a form of (directed) obligationbetween agents, and is not used as an alternative term for ‘premise’. It is difficult to see how an argumentationprotocol, or many other interaction protocols for multi-agent systems (for instance, protocols for negotiation, voting,performing transactions in electronic marketplaces, and so on), can be specified simply in terms of commitments inthis sense. At the very least, a specification of a protocol’s constitutive norms is also required.11. ConclusionWe have focused in this paper on the formal representation of the procedural aspects of an argumentation protocol,using Brewka’s reconstruction of Rescher’s theory of formal disputation as a concrete example. We distinguishedin the specification between the constitutive rules defining the protocol itself—the protocol actions and their effectson the protocol state—and the physical environment within which the protocol is executed, and the normative envi-ronment which may place further constraints on what protocol actions are permitted or obligatory. Although in thesimplest cases a protocol action is permitted if and only if it is both proper and timely, there are many reasons why thisneed not always be so. We presented some simple examples by way of illustration, but it should be clear that there aremany other possibilities that we did not discuss. There is also much choice in deciding where to place the boundarybetween the constitutive and normative components. For instance, we chose in the example presented here to say thatthe claim of Q by a protagonist who already holds Q as a premise is not proper, has no effect on the protocol state,but is permitted. We could have chosen to say instead that such a claim is proper but never permitted. Or that it is notproper, does have an effect on the protocol state (in that it requires the other protagonist to concede or deny), but isnot permitted. One objective of the present work is to allow such variations to be specified and examined, and to helpevaluate the effectiveness of proposed sanctioning and enforcement mechanisms.In an earlier formalisation of Brewka’s protocol [4] we specified the constitutive elements of the protocol bydefining the conditions under which an agent has institutional power (competence, capacity) to perform a particularprotocol action; we then said that an action is ‘valid’ when it is performed by an empowered agent. In this paper wehave constructed a more structured and detailed specification by defining separately the conditions under which anaction is proper and timely, as has also been suggested by Prakken et al. [45,49]. One aim of the paper was to see howthis additional structure would be reflected in the specification of a concrete example. A more detailed specificationstill would define what exact forms of message or utterance count as expressing a claim, concede, deny, retract, declare,and object action. We have not defined that level of detail here. It would be an important component in a run-timeA. Artikis et al. / Artificial Intelligence 171 (2007) 776–804801mechanism but is not so important if we are primarily interested, as here, in investigating properties of the specifiedprotocol. We have also found that it adds flexibility to specify separately which actions are objectionable, though thatis perhaps difficult to demonstrate convincingly in a comparatively simple protocol such as Brewka’s. Finally, wehave shown how the ‘silence implies consent’ principle can be included straightforwardly as an optional componentby marking as unconfirmed those premises that have not been explicitly conceded or retracted by the opponent.In principle, the kind of specifications presented in this paper could also be expressed in other temporal reasoningformalisms. The C+ language, however, has a number of important features that have led us to choose it as the basisfor further developments. First, it is a comparatively expressive formalism with fine control for specifying defaultpersistence of fluent constants. The availability of static laws moreover means, amongst other things, that complexspecifications can be given structure. For example, most of the rules defining proper(. . .) and timely(. . .) constantsin this paper can be expressed as static laws with simple conjunctions as their conditions. Additional structure canbe provided by introducing suitably chosen intermediate concepts, such as the fluent winning used in our example,themselves defined by means of static laws. This ability is important if large specifications are to be undertaken.Second, besides its semantics through translation to the formalism of ‘non-monotonic causal theories’ [22], a C+action description has an explicit semantics in terms of transition systems. This is important because it provides a linkto a wide range of other formalisms and tools based on transition systems. We have been able to devise, for example,an extended form of C+ specifically designed for representing norms and institutions [57–59], including direct supportfor (a version of) the ‘counts as’ relation for actions [27] and a treatment of permitted/forbidden states, transitions andruns. The relationship between permitted and obligatory actions presented in this paper, for instance, which had tobe formulated explicitly in the C+ specification (see Section 7.3), is built into the semantics of the language in theextended version. The development of the extended form of C+ took place in parallel with the methods presented inthis paper, and, therefore, we leave its presentation to a separate paper. Moreover, the relationship between obligations,non-compliant behaviour and sanctions in protocol specifications (see, for instance, Section 7.4), and the notion ofpermission built in to the extended C+ language needs to be explored more fully.The language C+ (and its derivatives) also has some important limitations. Most obviously, from a representationalpoint of view, the language inherits the limitations of transition systems, in particular that the executable actions(transitions) in any given state s of the system, and their effects, can depend only on the state s and not on thepath or history by which state s was reached (unless of course we encode the entire history in every state s). TheC+ language itself (though not the underlying formalism of causal theories) can only express causes relationshipsbetween successive states; delayed effects cannot be expressed directly. This is what makes a detailed formulation ofthe object mechanism awkward and fiddly to express in C+, and why we chose to omit the details from this paper. (Butsee [14] for a modified form of C+ which can express delayed effects.) In other protocols where the moves availableto a participant might depend on the entire history of the protocol so far (a participant is restricted on the number ofrepeated claims it can make, for instance), these limitations are not so easily overcome. Extensions to C+ capable ofexpressing such constraints in a concise manner are one direction of our current research.From the point of view of implementation, CCALC provides an immediate and convenient means of implement-ing C+ action descriptions. The execution of the RTFD∗ specification, however, confirmed our previous experienceregarding CCALC’s efficiency, and in particular that it does not provide a practical means for supporting run-time ac-tivities (see Section 9). A major limitation, of course, is the need to encode the underlying logic of disputation, whichhere we have done by means of explicit instances of the implies fluent constants. This clearly works only for smallexamples. It also requires that we are able to specify at the outset the complete set of propositions and default rulesthat could be claimed during the argumentation. Whilst it is possible to improve the efficiency of the CCALC imple-mentation by pre-compiling this part of the action description (the implies constants are ‘rigid’), and even in principlere-compiling the action description every time a protagonist claims a new, unanticipated, proposition, this is clearlynot a practical way of supporting run-time activities. It is only adequate for conducting computational experimentswith the specification of the kind discussed in Section 9. (The ‘logic of disputation’ for the examples in that section istrivial but that was chosen deliberately to keep the sample runs small enough to be presented.)CCALC is not the only means by which C+ action descriptions could be executed. We have also used versions ofthe Event Calculus to specify and execute (an earlier version of) the RTFD∗ protocol [3, Sections 6.10–6.12]. Givenan instance of the specification and a narrative—a record of what actions have been performed so far—this (Prolog)implementation allows all protocol states, including what is permitted and obligatory at each state, to be queriedand computed efficiently. It is not necessary to specify all propositions in advance, and it is comparatively easy to802A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804implement the underlying logic of disputation as another Prolog module to be added to the protocol specification.The Event Calculus implementation, however, is not well suited to planning and postdiction tasks. More importantly,we also lose the explicit transition system semantics which we see as the single most important advantage of the C+formulation. A discussion comparing the use of C+ and the Event Calculus for developing executable specificationsof multi-agent protocols, including argumentation protocols, can be found in [5].Recent work by Craven [13] has investigated the relationships between Event Calculus and C+. His EC+ imple-mentation provides an efficient Event Calculus style of computation of narratives with (a restricted form of) the C+language, providing a promising means of supporting run-time activities. He has also exploited the transition systemsemantics to connect C+ to model checking software (specifically NuSMV [12]). This allows protocol properties,expressed in Liner Temporal Logic (LTL) and Computation Tree Logic (CTL), to be verified by means of standardmodel checking techniques on protocol specifications expressed in the C+ language. An area of future work is toinvestigate what types of property, especially types of property identified for argumentation protocols (see, for exam-ple, [48, Section 8] and [36]), can be expressed by means of LTL and CTL, and thus proven on protocols formalisedin C+.AcknowledgementsThis article is an updated and extended version of [4]. We are grateful to the reviewers and the participants ofICAIL’03, who gave us useful feedback. We should also like to thank Joohyung Lee and Vladimir Lifschitz for theirsuggestions regarding the C+ language and CCALC. The authors themselves, however, are solely responsible for anymisunderstanding about the use of these technologies.This work was supported partly by the EU ALFEBIITE Project (IST-1999-10298) and partly by the EPSRC project‘Theory and Technology of Norm-Governed Self-Organising Networks’ (GR S74911 01).References[1] V. Akman, S. Erdogan, J. Lee, V. Lifschitz, H. Turner, Representing the zoo world and the traffic world in the language of the Causal Calculator,Artificial Intelligence 153 (1–2) (2004) 105–140.[2] L. Amgoud, N. Maudet, S. Parsons, Modelling dialogues using argumentation, in: Proceedings of the International Conference on MultiagentSystems (ICMAS), IEEE Computer Society, 2000, pp. 31–38.[3] A. Artikis, Executable specification of open norm-governed computational systems, PhD thesis, University of London, November 2003.Retrieved March 6, 2006, from http://www.iit.demokritos.gr/~a.artikis/publications/artikis-phd.pdf, also available from the author.[4] A. Artikis, M. Sergot, J. Pitt, An executable specification of an argumentation protocol, in: Proceedings of Conference on Artificial Intelligenceand Law (ICAIL), ACM Press, 2003, pp. 1–11.[5] A. Artikis, M. Sergot, J. Pitt, Specifying norm-governed computational societies, Technical Report 2006/5, Imperial College London, Depart-ment of Computing, 2006. Retrieved March 6, 2006, from http://www.doc.ic.ac.uk/research/technicalreports/2006/DTR06-5.pdf.[6] L. Bodenstaff, H. Prakken, G. Vreeswijk, On formalising dialogue systems for argumentation in the event calculus, in: Proceedings of Work-shop on Non-Monotonic Reasoning, 2006.[7] A. Bondarenko, P.M. Dung, R. Kowalski, F. Toni, An abstract, argumentation-theoretic approach to default reasoning, Artificial Intelligence 93(1997) 63–101.[8] G. Brewka, Dynamic argument systems: a formal model of argumentation processes based on situation calculus, Journal of Logic and Com-putation 11 (2) (2001) 257–282.[9] G. Brewka, T. Eiter, Prioritizing default logic, in: Festschrift 60th Anniversary of W. Bibel, Kluwer, 1998.[10] C. Cayrol, On the relation between argumentation and coherence based entailment, in: Proceedings of International Joint Conference onArtificial Intelligence (IJCAI), 1995, pp. 1443–1448.[11] C. Chesñevar, A. Maguitman, R. Loui, Logical models of argument, ACM Computing Surveys 32 (4) (2000) 337–383.[12] A. Cimatti, E. Clarke, E. Giunchiglia, F. Giunchiglia, M. Pistore, M. Roveri, R. Sebastiani, A. Tacchella, NuSMV version 2: An opensourcetool for symbolic model checking, in: Proc. International Conference on Computer-Aided Verification (CAV 2002), Copenhagen, July 2002,in: Lecture Notes in Computer Science, vol. 2404, Springer, 2002, see http://nusmv.irst.itc.it.[13] R. Craven, Execution mechanisms for the action language C+, PhD thesis, University of London, September 2006.[14] R. Craven, M. Sergot, Distant causation in C+, Studia Logica 79 (1) (2005) 73–96.[15] P.M. Dung, On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games,Artificial Intelligence 77 (2) (1995) 321–358.[16] M. Esteva, D. de la Cruz, C. Sierra, ISLANDER: an electronic institutions editor, in: C. Castelfranchi, L. Johnson (Eds.), Proceedings ofConference on Autonomous Agents and Multi-Agent Systems (AAMAS), ACM Press, 2002, pp. 1045–1052.[17] M. Esteva, J. Padget, C. Sierra, Formalizing a language for institutions and norms, in: J.-J. Meyer, M. Tambe (Eds.), Intelligent Agents VIII:Agent Theories, Architectures, and Languages, in: Lecture Notes in Artificial Intelligence, vol. 2333, Springer, 2002, pp. 348–366.A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804803[18] M. Esteva, J. Rodriguez-Aguilar, J. Arcos, C. Sierra, P. Garcia, Institutionalising open multi-agent systems, in: E. Durfee (Ed.), Proceedingsof the International Conference on Multi-agent Systems (ICMAS), IEEE Press, 2000, pp. 381–382.[19] M. Esteva, J. Rodriguez-Aguilar, C. Sierra, P. Garcia, J. Arcos, On the formal specifications of electronic institutions, in: F. Dignum, C. Sierra(Eds.), Agent Mediated Electronic Commerce, in: Lecture Notes in Artificial Intelligence, vol. 1991, Springer, 2001, pp. 126–147.[20] D. Fitoussi, M. Tennenholtz, Choosing social laws for multi-agent systems: minimality and simplicity, Artificial Intelligence 119 (1–2) (2000)61–101.[21] J. Fulda, The logic of ‘improper cross’, Artificial Intelligence and Law 8 (4) (2001) 337–341.[22] E. Giunchiglia, J. Lee, V. Lifschitz, N. McCain, H. Turner, Nonmonotonic causal theories, Artificial Intelligence 153 (1–2) (2004) 49–104.[23] E. Giunchiglia, J. Lee, V. Lifschitz, H. Turner, Causal laws and multi-valued fluents, in: Proceedings of Workshop on Nonmonotonic Reason-ing, Action and Change (NRAC), 2001.[24] T. Gordon, The pleadings game: an exercise in computational dialectics, Artificial Intelligence and Law 2 (1994) 239–292.[25] T. Gordon, The Pleadings Game: An Artificial Intelligence Model of Procedural Justice, Kluwer Academic Publishers, 1995.[26] A. Jones, M. Sergot, On the characterisation of law and computer systems: the normative systems perspective, in: Deontic Logic in ComputerScience: Normative System Specification, J. Wiley and Sons, 1993, pp. 275–307.[27] A. Jones, M. Sergot, A formal characterisation of institutionalised power, Journal of the IGPL 4 (3) (1996) 429–445.[28] M. Klein, J. Rodriguez-Aguilar, C. Dellarocas, Using domain-independent exception handling services to enable robust open multi-agentsystems: the case of agent death, Journal of Autonomous Agents and Multi-Agent Systems 7 (1–2) (2003) 179–189.[29] R. Kowalski, M. Sergot, A logic-based calculus of events, New Generation Computing 4 (1) (1986) 67–96.[30] S. Kraus, K. Sycara, A. Evenchik, Reaching agreements through argumentation: a logical model and implementation, Artificial Intelli-gence 104 (1–2) (1998) 1–69.[31] P. Krause, S. Ambler, J. Fox, A logic of argumentation for uncertain reasoning, Computational Intelligence 11 (1) (1995) 113–131.[32] J. Lee, V. Lifschitz, H. Turner, A representation of the zoo world in the language of the Causal Calculator, in: Proceedings of Symposium onFormalizations of Commonsense Knowledge, 2001.[33] V. Lifschitz, Missionaries and cannibals in the Causal Calculator, in: A. Cohn, F. Giunchiglia, B. Selman (Eds.), Proceedings of Conferenceon Principles of Knowledge Representation and Reasoning (KR), Morgan Kaufmann, 2000, pp. 85–96.[34] V. Lifschitz, N. Mccain, E. Remolina, A. Tacchella, Getting to the airport: the oldest planning problem in AI, in: J. Minker (Ed.), Logic-BasedArtificial Intelligence, Kluwer, 2000, pp. 147–168.[35] D. Makinson, On the formal representation of rights relations, Journal of Philosophical Logic 15 (1986) 403–425.[36] P. McBurney, S. Parsons, M. Wooldridge, Desiderata for agent argumentation protocols, in: C. Castelfranchi, L. Johnson (Eds.), Proceedingsof Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), ACM Press, 2002, pp. 402–409.[37] N. Minsky, V. Ungureanu, Law-governed interaction: a coordination and control mechanism for heterogeneous distributed systems, ACMTransactions on Software Engineering and Methodology (TOSEM) 9 (3) (2000) 273–305.[38] Y. Moses, M. Tennenholtz, On computational aspects of artificial social systems, in: Proceedings of Workshop on Distributed ArtificialIntelligence (DAI), 1992, pp. 267–284.[39] Y. Moses, M. Tennenholtz, Artificial social systems, Computers and Artificial Intelligence 14 (6) (1995) 533–562.[40] S. Parsons, P. McBurney, Argumentation-based communication between agents, in: Communication in Multiagent Systems, in: Lecture Notesin Computer Science, vol. 2650, Springer, 2003, pp. 164–178.[41] S. Parsons, M. Wooldridge, L. Amgoud, An analysis of formal inter-agent dialogues, in: Proceedings of Conference on Autonomous Agentsand Multi-Agent Systems, ACM Press, 2002, pp. 394–401.[42] S. Parsons, M. Wooldridge, L. Amgoud, On the outcomes of formal inter-agent dialogues, in: Proceedings of Conference on AutonomousAgents and Multi-Agent Systems, ACM Press, 2003, pp. 616–623.[43] S. Parsons, M. Wooldridge, L. Amgoud, Properties and complexity of some formal inter-agent dialogues, Journal of Logic and Computa-tion 13 (3) (2003) 347–376.[44] J. Pollock, Oscar—a general purpose defeasible reasoner, Journal of Applied Non-Classical Logics 6 (1) (1996) 89–113.[45] H. Prakken, Formalising Robert’s rules of order, Technical Report 12, GMD—German National Research Center for Information Technology,1998.[46] H. Prakken, On dialogue systems with speech acts, arguments, and counterarguments, in: Proceedings of Workshop on Logics in ArtificialIntelligence, in: Lecture Notes in Artificial Intelligence, vol. 1919, Springer, 2000, pp. 224–238.[47] H. Prakken, Coherence and flexibility in dialogue games for argumentation, Journal of Logic and Computation 15 (2005) 1009–1040.[48] H. Prakken, Formal systems for persuasion dialogue, Knowledge Engineering Review 21 (2) (2006) 163–188.[49] H. Prakken, T. Gordon, Rules of order for electronic group decision making—a formalization methodology, in: J. Padget (Ed.), Collaborationbetween Human and Artificial Societies, in: Lecture Notes in Computer Science, vol. 1624, Springer, 1999, pp. 246–263.[50] H. Prakken, G. Sartor, Argument-based extended logic programming with defeasible priorities, Journal of Applied Non-Classical Logics 7 (1)(1997) 25–75.[51] H. Prakken, G. Vreeswijk, Logics for defeasible argumentation, in: D. Gabbay, F. Guenthner (Eds.), Handbook of Philosophical Logic, vol. 4,Kluwer Academic Publishers, 2002, pp. 218–319.[52] R. Reiter, Knowledge in Action: Logical Foundations for Describing and Implementing Dynamical Systems, The MIT Press, 2001.[53] N. Rescher, Dialectics: A Controversy-Oriented Approach to the Theory of Knowledge, State University of New York Press, 1977.[54] J. Rodriguez-Aguilar, F. Martin, P. Noriega, P. Garcia, C. Sierra, Towards a test-bed for trading agents in electronic auction markets, AICommunications 11 (1) (1998) 5–19.[55] J. Rodriguez-Aguilar, C. Sierra, Enabling open agent institutions, in: K. Dautenhahn, A. Bond, L. Canamero, B. Edmonds (Eds.), SociallyIntelligent Agents: Creating Relationships with Computers and Robots, Kluwer Academic Publishers, 2002, pp. 259–266.804A. Artikis et al. / Artificial Intelligence 171 (2007) 776–804[56] J. Searle, Speech Acts, Cambridge University Press, 1969.[57] M. Sergot, (C+)++: An action language for modelling norms and institutions, Technical Report 2004/8, Department of Computing, ImperialCollege London, 2004. Retrieved March 6, 2006, from http://www.doc.ic.ac.uk/research/technicalreports/2004/DTR04-8.pdf.[58] M. Sergot, Modelling unreliable and untrustworthy agent behaviour, in: B. Dunin-Keplicz, A. Jankowski, A. Skowron, M. Szczuka (Eds.),Proceedings of Workshop on Monitoring, Security, and Rescue Techniques in Multiagent Systems (MSRAS), in: Advances in Soft Computing,Springer, 2004, pp. 161–178.[59] M. Sergot, R. Craven, The deontic component of action language nC+, in: L. Goble, J.-J.C. Meyer (Eds.), Deontic Logic and ArtificialNormative Systems. Proc. 8th International Workshop on Deontic Logic in Computer Science (DEON’06), Utrecht, July 2006, in: LectureNotes in Artificial Intelligence, vol. 4048, Springer, 2006, pp. 222–237.[60] M. Shanahan, The event calculus explained, in: M. Wooldridge, M. Veloso (Eds.), Artificial Intelligence Today, in: Lecture Notes in ArtificialIntelligence, vol. 1600, Springer, 1999, pp. 409–430.[61] M. Shanahan, An abductive event calculus planner, Journal of Logic Programming 44 (2000) 207–239.[62] Y. Shoham, M. Tennenholtz, On the synthesis of useful social laws for artificial agent societies, in: W. Swartout (Ed.), Proceedings of Confer-ence on Artificial Intelligence (AAAI), The AAAI Press/The MIT Press, 1992, pp. 276–281.[63] Y. Shoham, M. Tennenholtz, On social laws for artificial agent societies: off-line design, Artificial Intelligence 73 (1–2) (1995) 231–252.[64] M. Singh, A social semantics for agent communication languages, in: F. Dignum, M. Greaves (Eds.), Issues in Agent Communication, in:Lecture Notes in Computer Science, vol. 1916, Springer, 2000, pp. 31–45.[65] M. Tennenholtz, On computational social laws for dynamic non-homogeneous social structures, Journal of Experimental and TheoreticalArtificial Intelligence 7 (1995) 379–390.[66] P. Yolum, M. Singh, Flexible protocol specification and execution: applying event calculus planning using commitments, in: C. Castelfranchi,L. Johnson (Eds.), Proceedings of Conference on Autonomous Agents and Multiagent Systems (AAMAS), ACM Press, 2002, pp. 527–535.[67] P. Yolum, M. Singh, Reasoning about commitments in the event calculus: An approach for specifying and executing protocols, Annals ofMathematics and Artificial Intelligence 42 (1–3) (2004) 227–253.