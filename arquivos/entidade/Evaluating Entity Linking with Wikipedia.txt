Artificial Intelligence 194 (2013) 130–150Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintEvaluating Entity Linking with WikipediaBen Hachey a,∗, Will Radford b,c, Joel Nothman b,c, Matthew Honnibal d, James R. Curran b,ca Research & Development, Thomson Reuters Corporation, St. Paul, MN 55123, USAb School of Information Technologies, University of Sydney, NSW 2006, Australiac Capital Markets CRC, 55 Harrington Street, NSW 2000, Australiad Department of Computing, Macquarie University, NSW 2109, Australiaa r t i c l ei n f oa b s t r a c tArticle history:Available online 23 April 2012Keywords:Named Entity LinkingDisambiguationInformation extractionWikipediaSemi-structured resources1. IntroductionNamed Entity Linking (nel) grounds entity mentions to their corresponding node in aKnowledge Base (kb). Recently, a number of systems have been proposed for linkingentity mentions in text to Wikipedia pages. Such systems typically search for candidateentities and then disambiguate them, returning either the best candidate or nil. However,comparison has focused on disambiguation accuracy, making it difficult to determine howsearch impacts performance. Furthermore, important approaches from the literature havenot been systematically compared on standard data sets.We reimplement three seminal nel systems and present a detailed evaluation of searchstrategies. Our experiments find that coreference and acronym handling lead to substantialimprovement, and search strategies account for much of the variation between systems.This is an interesting finding, because these aspects of the problem have often beenneglected in the literature, which has focused largely on complex candidate rankingalgorithms.© 2012 Elsevier B.V. All rights reserved.References to entities such as people, places and organisations are difficult to track in text, because entities can bereferred to by many mention strings, and the same mention string may be used to refer to multiple entities. For instance,David Murray might refer to either the jazz saxophonist or the Iron Maiden guitarist, who may be known by other aliasessuch as Mad Murray. These synonymy and ambiguity problems make it difficult for language processing systems to collectand exploit information about entities across documents without first linking the mentions to a knowledge base.Named Entity Linking (nel) is the task of resolving named entity mentions to entries in a structured Knowledge Base(kb). nel is useful wherever it is necessary to compute with direct reference to people, places and organisations, rather thanpotentially ambiguous or redundant character strings. In the finance domain, nel can be used to link textual informationabout companies to financial data, for example, news and share prices [34]. nel can also be used in search, where resultsfor named entity queries could include facts about an entity in addition to pages that talk about it [8].nel is similar to the widely-studied problem of word sense disambiguation (wsd, [36]), with Wikipedia articles playingthe role of WordNet synsets [20]. At core, both tasks address problems of synonymy and ambiguity in natural language.The tasks differ in terms of candidate search and nil detection. Search for wsd assumes that WordNet is a complete lexicalresource and consists of a lexical lookup to find the possible synsets for a given word. The same approach is taken inwikification, where arbitrary phrases including names and general terms are matched to Wikipedia pages [32,33,27,15].* Corresponding author.E-mail address: ben.hachey@gmail.com (B. Hachey).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.04.005B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150131However, this does not provide a mechanism for dealing with objects that are not present in the database. nel, on theother hand, does not assume the kb is complete, requiring entity mentions without kb entries to be marked as nil [8,31].Furthermore, named entity mentions vary more than lexical mentions in wsd. Therefore, search for nel requires a noisiercandidate generation process, often using fuzzy matching to improve recall [48,28].Until recently, wide-coverage nel was not possible since there was no general purpose, publicly available collectionof information about entities. However, Wikipedia has emerged as an important repository of semi-structured, collectiveknowledge about notable entities. Accordingly, it has been widely used for knowledge modelling [46,6,37,42]. It has beenused for nlp tasks like automatic summarisation [45,50]. And it has also been exploited for a number of information extrac-tion tasks ranging from ner learnt from Wikipedia link structure [40] to relation extraction learnt from the nearly structuredinformation encoded in Wikipedia Infoboxes [51].The most popular data sets for nel were distributed as part of the recent Knowledge Base Population tasks at the nistText Analysis Conference (tac). The thirteen participants in the 2009 task developed systems that linked a set of 3904 entitymentions in news and web text to a knowledge base extracted from Wikipedia infoboxes. The highest accuracy achievedwas 82.2% [48] with subsequent publications reporting results as high as 86% [21].The popularity of the tac shared tasks has led to a wide range of innovative entity linking systems in the literature.However, since all participants were individually striving for the highest accuracy they could achieve, the systems all differalong multiple dimensions, so it is currently unclear which aspects of the systems are necessary for good performance andwhich aspects might be improved.In this paper, we reimplement three prominent entity linking systems from the literature to obtain a better understand-ing of the named entity linking task. Our primary question concerns the relative importance of search and disambiguation:an nel system must first search for a set of candidate entities that the mention string might refer to, before selecting a sin-gle candidate given the document. These phases have not been evaluated in isolation, and the systems from the literaturetend to differ along both dimensions.We find that the search phase is far more important than previously acknowledged. System descriptions have usuallyfocused on complicated ranking methods. However, search accounts for most of the variation between systems. Furthermore,relatively unremarked search features such as query expansion based on coreference resolution and acronym detection seemto have a much larger impact on system performance than candidate ranking.2. Review of named entity disambiguation tasks and data setsSeveral research communities have addressed the named entity ambiguity problem. It has been framed in two differentways. Within computational linguistics, the problem was first conceptualised by Bagga and Baldwin [4] as an extension ofthe coreference resolution problem. Mihalcea and Csomai [32] later used Wikipedia as a word sense disambiguation dataset by attempting to reproduce the links between pages, as link text is often ambiguous. Finally, Bunescu and Pa ¸sca [8] usedWikipedia in a similar way, but include ner as a preprocessing step and require a link or (nil) for all identified mentions. Wewill follow the terminology of these papers, and refer to the three tasks respectively as cross-document coreference resolution(cdcr), wikification, and named entity linking (nel). We use the more general term named entity disambiguation when wemust avoid referring specifically to any single task.The cdcr, wikification, and nel tasks make different assumptions about the problem, and these lead to different evalu-ation measures and slightly different techniques. The cdcr task assumes that the documents are provided as a batch, andmust be clustered according to which entities they mention. Systems are evaluated using clustering evaluation measures,such as the B3 measure [3]. The wikification task assumes the existence of a knowledge base that has high coverage overthe entities of interest, and that entities not covered by the knowledge base are relatively unimportant. And nel requiresa knowledge base but does not assume that it is complete. Systems are usually evaluated on micro-accuracy (percentageof mentions linked correctly) and macro-accuracy (percentage of entities linked correctly). In this section, we review themain data sets that have been used in cdcr and nel research. Although we make some reference to approaches used, werereserve the main description of named entity disambiguation techniques for Section 3.2.1. Early cross-document coreference datasetsThe seminal work on cross-document coreference resolution (cdcr) was performed by Bagga and Baldwin [4]. Theyperformed experiments on a set of 197 documents from the New York Times whose text matched the expressionJohn.*?Smith—where .*? is a non-greedy wildcard match up to the first instance of Smith, e.g., only John DonnelSmith would be matched in John Donnell Smith bequeathed his herbarium to the Smithsonian. The documents were manuallygrouped according to which John Smith entities they mentioned. None of the articles mentioned multiple John Smiths, sothe only annotations were at the document level.The John Smith dataset approaches the problem as one name, many people: there are many entities that are referred toby an ambiguous name such as John Smith. However, there is another side to the problem: one person, many names. Anentity known as John Smith might also be known as Jack Smith, Mr. Smith, etc. In other words, there are both synonymy andambiguity issues for named entities.132B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150Most cdcr datasets are similarly collected by searching for a set of canonical names, ignoring non-canonical coreferentforms. For instance, Mann and Yarowsky [29] collected a data set of web pages returned from 32 search engine queriesfor person names sampled from US census data. This data was later included in the WePS data described in Section 2.3.While ensuring that each document contains a canonical form for an ambiguous entity, this produces an unrealistic sampledistribution.In contrast, Day et al. [11] identify coreferent entity chains between documents in the ACE 2005 corpus [38], whichalready marks in-document coreference between proper name, nominal and pronominal entity mentions. Marking in-document and cross-document coreference for all entities in a corpus addresses both synonymy and ambiguity issues.2.2. Generating data with pseudo-namesBecause manually annotating data is costly, there has been some interest in adopting the pseudo-words strategy of gen-erating artificial word sense disambiguation data first described by Gale et al. [16]. For word sense disambiguation, the datais generated by taking two words that are not sense ambiguous, and replacing all instances of them with an ambiguouskey. For instance, all instances of the words banana and door would be replaced by the ambiguous key banana–door. Theoriginal, unambiguous version is reserved as the gold standard for training and evaluation.Cross-document coreference resolved data can be generated in the same way by taking all instances of two or morenames, and conflating them under an anonymisation key such as Person X. The task is then to group the documentsaccording to their original name mentions. This strategy was first explored by Mann and Yarowsky [29], and subsequentlyby Niu et al. [39] and Gooi and Allan [17].Pseudo-data generation is problematic for both word sense and named entity disambiguation, but for different reasons.For words, most ambiguities are between related senses. For instance, the tennis and mathematical meanings of the wordset can be linked back to a common concept. Few sense ambiguities are between unrelated concepts such as banana anddoor, and it is very difficult to select word pairs that reflect the meaningful relationships between word senses.For named entity disambiguation, there is little reason to believe that two people named John Smith will share any moreproperties than one entity named Paul Simonell and another named Hugh Diamoni, so the criticism of pseudo-data that hasbeen made about word sense disambiguation does not apply. On the other hand, named entities have interesting internalstructures that a named entity disambiguation system might want to exploit. For instance, the use of a title such as Mr.or Dr. may be a critical clue. This makes named entities difficult to anonymise effectively under a key such as Person Xwithout losing important information.2.3. Web People SearchThe first large data set for cdcr was distributed by the Web People Search shared task [1]. The data set consisted of up to100 web search results for 49 personal names, for a total data set of 3489 documents manually sorted into 527 clusters. Thetask was repeated the following year, with a new evaluation set consisting of 3432 documents sorted into 559 clusters [2].The most recent task, WePS-III, provided 57,956 documents from which the new evaluation data would be drawn—the top200 search results for 300 person names. Only a subset of the documents received gold standard annotations.WePS-III also added an additional entity disambiguation task, targeted at Online Reputation Management. The organiserssearched the Twitter messaging service for posts about any of 100 companies, selected according to the ambiguity of theirnames—companies within names that were too ambiguous or too unambiguous were excluded. Mechanical Turk was usedto cheaply determine which of 100 tweets per company name actually referred to the company of interest. Participantswere supplied the tweets, the company name, and the url of the company’s homepage. This task is closer to named entitylinking than cross-document coreference resolution, but shares a common weakness of cdcr data: the data was collectedby searching for the company name, so the task does not address named entity synonymy.2.4. WikificationThe development of Wikipedia offered a new way to approach the problem of entity ambiguity. Instead of clusteringentities, as is done in cdcr, mentions could be resolved to encyclopedia pages. This was first described by Mihalcea andCsomai [32]. The task, which we refer to as wikification, is to add links from important concept mentions in text to thecorresponding Wikipedia article. The task differs from Named Entity Linking in that concepts are not necessarily named en-tities, and in that the knowledge base is assumed to complete (i.e., presence in the encyclopedia is a minimum requirementfor being identified and linked).In order to encourage further research on wikification, the inex workshops ran a Link the Wiki task between 2007 and2009 [25]. The task is designed to improve Information Retrieval and places an emphasis on Wiki creation and maintenanceas well as evaluation tools and methodologies. The 2009 task introduces a second wiki, Te Ara,1 an expert-edited encyclo-pedia about New Zealand. Te Ara does not contain inter-article links, so the first subtask is to discover them. The secondtask is to link Te Ara articles to Wikipedia articles.1 http://www.teara.govt.nz/.B. Hachey et al. / Artificial Intelligence 194 (2013) 130–1501332.5. Named Entity LinkingThe first attempts at what we term the Named Entity Linking (nel) task—the task of linking entity mentions to a knowl-edge base—predicted the target of links in Wikipedia. This resembles the pseudo-name generation task described inSection 2.2, in that it makes a large volume of data immediately available, but the data may not be entirely representa-tive. Cucerzan [9] has pointed out that the ambiguity of Wikipedia link anchor texts is much lower than named entitymentions in news data. This may be because the MediaWiki mark up requires editors to retrieve the article title in orderto make a link, and they must then actively decide to use some other mention string to anchor the text. This seems toencourage them to refer to entities using more consistent terminology than writers of other types of text.Bunescu and Pa ¸sca [8] were the first to use Wikipedia link data to train and evaluate a system for grounding text toa knowledge base. However, they did not evaluate their systems on manually linked mentions, or text from sources otherthan Wikipedia. The first to do so was Cucerzan [9], who evaluated on both Wikipedia and a manually linked set of 20news articles, described in more detail in Section 2.7.2.6. The Text Analysis Conference Knowledge Base Population challengeThe first large set of manually annotated named entity linking data was prepared by the National Institute of Standardsand Technologies (nist) as part of the Knowledge Base Population (kbp) shared task at the 2009 Text Analysis Conference(tac) [31].The 2009 tac-kbp distributed a knowledge base extracted from a 2008 dump of Wikipedia and a test set of 3904 queries.Each query consisted of an ID that identified a document within a set of Reuters news articles, a mention string thatoccurred at least once within that document, and a node ID within the knowledge base. Little training data was provided.Each knowledge base node contained the Wikipedia article title, Wikipedia article text, a predicted entity type (per, org,loc or misc), and a key-value list of information extracted from the article’s infobox. Only articles with infoboxes that werepredicted to correspond to a named entity were included in the knowledge base.The annotators did not select mentions randomly. Instead, they favoured mentions that were likely to be ambiguous, inorder to provide a more challenging evaluation. If the entity referred to did not occur in the knowledge base, it was labellednil. A high percentage of queries in the 2009 test set did not map to any nodes in the knowledge base—that is, the goldstandard answer for 2229 of the 3904 queries was nil.The 2010 challenge used the same configuration as the 2009 challenge, and kept the same knowledge base. A trainingset of 1500 queries was provided, with a test set of 2250 queries. In the 2010 training set, only 28.4% of the queries werenil, compared to 57.1% in the 2009 test data and 54.6% in the 2010 test data (details in Section 4 below). This mismatchbetween the training and test data may have harmed performance for some systems. Systems can be quite sensitive tothe number of nil queries, because it is difficult to determine whether a candidate that seems to weakly match the queryshould be discarded, in favour of guessing nil. A high percentage of nil queries thus favours conservative systems that stayclose to the nil baseline unless they are very confident of a match.The most successful participants in the 2009 challenge addressed this issue by augmenting their knowledge base witharticles from a recent Wikipedia dump. This allowed them to consider strong matches against articles that did not have anycorresponding node in the knowledge base, and return nil for these matches. This turned out to be preferable to assigninga general threshold of match strength below which nil would be returned. We use the 30th July 2010 snapshot of EnglishWikipedia as a proxy kb for nel. Since it is larger, it should provide more information to disambiguate candidate entities formentions. After disambiguation, we then check to see if the linked entity exists in the kb, returning nil for entities that wecould link, but were not in the supplied kb.2.7. Other nel evaluation dataIn addition to the data from the tac challenge, three individual researchers have made their test sets available.Cucerzan [9] manually linked all entities from 20 MSNBC news articles to a 2006 Wikipedia dump, for a total of 756links, with 127 resolving to nil. This data set is particularly interesting because mentions were linked exhaustively overarticles, unlike the tac data, where mentions were selected for annotation if the annotators regarded them as interesting.The Cucerzan dataset thus gives a better indication of how a real-world system might perform.Fader et al. [13] evaluate against 500 predicate–argument relations extracted by TextRunner from a corpus of 500 millionWeb pages, covering various topics and genres. Considering only relations where one argument was a proper noun, the au-thors manually identified the Wikipedia page corresponding to the first argument, assigning nil if there is no correspondingpage. 160 of the 500 mentions resolved to nil.Dredze et al. [12] performed manual annotation using a similar methodology to the tac challenges, in order to generateadditional training data. They linked 1496 mentions from news text to the tac knowledge base, of which 270 resolved tonil—a substantially lower percentage of nil-linked queries than the 2009 and 2010 tac data.There is also some work on integrating linking annotation with existing ner datasets, including the CoNLL-03 Englishdata [24] and ACE 2005 English data [5]. This is important since it allows evaluation of different steps of the pipeline ofnerecognition, coreference (gold-standard in the latter case) and linking.134B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150Table 1Summary of named entity disambiguation data sets.TaskcdcrcdcrcdcrcdcrcdcrwikifywikifywikifynelnelnelnelnelnelnelNameJohn SmithWePS 1Day et al.WePS 2WePS 3MihalceaKulkarniMilneCucerzantac 09Fadertac 10DredzeBentivogliHoffartYear199820072008200820092007200920102007200920092010201020102011SourceNewsWebNewsWebWebWikiWebWikiNewsNewsNewsNews, BlogsNewsNews, Web, TranscriptsNews2.8. The BioCreative challenge Gene Normalisation taskAll mentionsInstances✘✘✔✘✘✔✔✔✔✘✘✘✘✔✔19734893660343231950728617,20011,00079739045003750149616,85134,956The 2008 BioCreative workshop ran an entity linking challenge for biomedical text, which they termed Gene Normal-isation (gn, [23,35]). Participants were provided the raw text of abstracts from scientific papers, and asked to extract theEntrez Gene identifiers for all human genes and proteins mentioned in the abstract. The gn task is motivated by genomicsdatabase curation, where scientific articles are linked to the genes/proteins of interest. The gn task differs from the realcuration task in that it does not use the full text of the articles, and it annotates every human gene/protein mentioned (notjust those described with new scientific results).The version of the Entrez Gene database used for the task consists of a list of 32,975 human gene/protein identifiers,including an average of 5.5 synonyms each. Evaluation data was created by human experts trained in molecular biology andincluded 281 abstracts for training and 262 for testing. These sets have 684 and 785 total identifier annotations respectively,corresponding to averages of 2.4 and 3 per abstract. Inter-annotator agreement was reported as over 90%.2.9. Database Record LinkageRecord Linkage [49] aims to merge entries from different databases, most commonly names and addresses for the sameindividual. This is often framed as database cleaning: canonical versions of names and addresses are produced, with du-plicates sometimes removed in the process. Initial research by Fellegi and Sunter [14] presented a probabilistic descriptionof the linkage problem and subsequent work extends this to use multiple sources of information or treats it as a graph ofmentions to be partitioned into entity clusters. While similar to nel, Record Linkage tends to consider more structured data(e.g., names and addresses) cleanly separated into database fields. This does, however, allow exploration of large datasets ofperson-related data (e.g., census and medical records), motivating work on efficiency and privacy.2.10. Summary of Evaluation SetsTable 1 shows the data sets used to evaluate named entity disambiguation work. Named entity disambiguation has beenaddressed as multiple tasks, including cross-document coreference resolution (cdcr), wikification (wikify), and named entitylinking (nel).The cdcr data usually assumes that each document mentions one person of interest, usually using a canonical nameform. The task is then to cluster the documents that refer to that person. In recent years, the task has been focused on theWeb Person Search challenge datasets.Named entity disambiguation is also sometimes addressed as part of wikification tasks. In these tasks, concepts must beidentified and linked to the best Wikipedia page. Concepts are often named entities, but need not be. This is often evaluatedon Wikipedia links directly, but Kulkarni et al. [27] point out that this leads to inaccurate performance estimates due tocanonicalisation, so collected their own dataset of 17,200 terms mentions using web text from popular domains from avariety of genres.Finally, nel resembles wikification, but seeks to link all named entity mentions, requiring a mechanism for handlingmentions that do not have a corresponding node in the knowledge base. Much of the work on this problem has beendone using the tac data sets. One weakness of these datasets is that they were collected by cherry-picking ‘interesting’mentions, rather than systematically annotating all mentions within a document. One dataset that corrects this is describedby Cucerzan [9]. However, the Cucerzan data was collected by correcting the output of his system, which may bias the datatowards his approach. This may make the data unsuitable for comparison between systems.B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150135Table 2Comparative summary of seminal linkers.SystemExtractorSearcherConditionTitleRedirectLinkTruncatedBoldDABTitleFilterBunescu andPa ¸sca [8]NERNA✔✔Cucerzan [9]NER,coreferenceexpansionVarma et al.[48]NER, acronymexpansion3. ApproachesNA✔✔✘✔if acronymif expandableelseelsesearch 1if no candidates✔✔✔✔✔✔✔✔✔✔✔✔NANAin kbNAin kbNADisambiguatorsvm rank overcosineandmention contextword × categoryfeaturesScalarproductbetween candi-datecategory/term vector anddocument-levelvectorCosine betweencandidatearticle termvector andmentioncontext vectorTo date, the literature on named entity linking has largely consisted of detailed descriptions of novel complex systems.However, while nel systems are commonly described in terms of separate search and disambiguation components,2 verylittle analysis has been performed that looks at the individual effect of these components.In this section, we describe our implementations of three such complex systems from the literature [8,9,48], in order toprovide the first detailed analysis of the named entity linking task. These systems were selected for being seminal workon the task, for being highly novel, and for reporting very high performance. None of these systems have been comparedagainst each other before.3.1. A framework for Named Entity LinkingWe suggest a Named Entity Linking (nel) framework that allows replication and comparison of different approaches. Thecore task of an nel system is to link a query mention, given its document context, to a Knowledge Base (kb) entity node ornil. This can be separated into three main components: extractors, searchers and disambiguators.Extractor. Extraction is the detection and preparation of named entity mentions. Most nel datasets supply mention stringsas queries. Some additional mention detection and preparation is often desirable however, because information about otherentities in the text is useful for disambiguation. The extraction phase may also include other preprocessing such as tokeni-sation, sentence boundary detection, and in-document coreference. In-document coreference, in particular, is important asit can be used to find more specific search terms (e.g., ABC (cid:3)→ Australian Broadcasting Corporation).Searcher. Search is the process of generating a set of candidate kb entities for a mention. Titles and other Wikipedia-derivedaliases can be leveraged at this stage to capture synonyms (see Section 5 below). An ideal searcher should balance precisionand recall to capture the correct entity while maintaining a small set of candidates. This reduces the computation requiredfor disambiguation.Disambiguator.In disambiguation, the best entity is selected for a mention. We frame this as ranking problem over thecandidate set. We hold the nil-detection strategy fixed for all disambiguators. This uses a Wikipedia snapshot from 30thJuly 2010 as a larger proxy kb for linking and any entities that do not exist in the small tackb are returned as nil.Table 2 contains a summary of the extraction, search, and disambiguation components for our linker implementations,which are described in detail in the remainder of this section. Rows correspond to our implementations of seminal ap-proaches from the literature. The first column for the searcher components contains conditions that need to be met fora given search to be performed. The following columns correspond to the alias sources used (see Section 5). And the lastcolumn specifies any filters that are applied to narrow the resulting candidate set.2 McCallum et al. [30] also describe a similar decomposition, motivated by efficiency, for the related task of clustering citation references.136B. Hachey et al. / Artificial Intelligence 194 (2013) 130–1503.2. Bunescu and Pa ¸scaBunescu and Pa ¸sca [8] were the first to explore the nel task, using Support Vector Machines (svm) to rank for disam-biguation. However, its performance has not been compared against subsequent approaches.Extractor. Bunescu and Pa ¸sca use data derived from Wikipedia for an evaluation whose goal is to return the correct targetfor a given link anchor, i.e., to re-introduce link targets in Wikipedia articles given the anchor text. They did not performcoreference or any other additional preprocessing.Searcher. The search component for Bunescu and Pa ¸sca is an exact match lookup against article, redirect, and disambigua-tion title aliases. It returns all matching articles as candidates.Disambiguator. The Bunescu and Pa ¸sca disambiguator uses a Support Vector Machine (svm) ranking model, using thesvmlight toolkit [26]. Two types of features are used. The first feature type is the real-valued cosine similarity betweenthe query context and the text of the candidate entity page (see Eq. (1) below). The second feature type is generated bycreating a 2-tuple for each combination of candidate categories—Wikipedia classifications that are used to group pages onsimilar subjects—and context words. The categories are ancestors of those assigned to the candidate entity page, and thewords are those that occurred within a 55-token context window of the entity mention. Based on results from Bunescuand Pa ¸sca, our implementation uses only categories that occur 200 times or more. However, while Bunescu and Pa ¸sca fo-cused on Person by occupation pages in Wikipedia, the tac data used for experiments here includes organisation andgeopolitical entity types as well as a general person type (see Section 4 below). Thus, we explored general strategies fordisambiguating arbitrary entity types. The union of great and great-great grandparent categories performed best in prelim-inary experiments and are used in our implementation here. Bunescu and Pa ¸sca include an nil pseudo-candidate in thecandidate list, allowing the svm algorithm to learn to return nil as the top-ranked option when no good candidate exists.We do not include nil pseudo-candidates since this decreased performance in our development experiments (−0.5% accu-racy). As mentioned above, this also allows us to hold the nil-detection strategy constant for all disambiguation approaches.The learner is trained on the development data provided for the tac 2010 shared task. It is important to note that theBunescu and Pa ¸sca approach is the only one here that relies on supervised learning. The original paper derived training setsof 12,288 to 38,726 ambiguous person mentions from Wikipedia. Here, we use the tac 2010 training data, which has 1500total hand-annotated person, organisation, and geo-political entity mentions. The small size of this training set limits theperformance of the machine learning approach in the experiments here. However, this also reflects the challenges of portingsupervised approaches to different variations of the same task.3.3. CucerzanCucerzan [9] describes an nel approach that focuses on an interesting document-level disambiguation approach. He alsointroduces a preprocessing module that identifies chains of coreferring entity mentions in order to use more specific namestrings for querying. However, the effect of coreference handling on search and disambiguation is not explored.Extractor. Cucerzan report an evaluation whose goal is to link all entity mentions in a news article to their correspondingWikipedia page. Therefore, it is necessary to split the text into sentences, then detect and corefer named entity mentions.Cucerzan uses a hybrid ner tagger based on capitalisation rules, web and the CoNLL-03 ner shared task data [47] statistics.In our implementation, we first use the C&C ner tagger [10] to extract named entity mentions from the text. Next, naïvein-document coreference is performed by taking each mention and trying to match it to a longer, canonical, mention inthe document. These are expected to be longer, more specific and easier to disambiguate. Mentions are examined in turn,longest to shortest, to see if it forms the prefix or suffix of a previous mention and is no more than three tokens shorter.Uppercase mentions are considered to be acronyms and mapped to a canonical mention if the acronym letters matchthe order of the initial characters of the mention’s tokens. Our coreference implementation differs from that described byCucerzan in that we do not require a canonical mention to have the same entity type as another mention coreferred to it,since we view identity as stronger evidence than predicted type.Searcher. For candidate generation, canonical mentions are first case-normalised to comply with Wikipedia conventions.These are searched using exact-match lookup over article titles, redirect titles, apposition stripped article/redirect titles, anddisambiguation titles. In contrast to Cucerzan, we do not use link anchor texts as search aliases because we found thatthey caused a substantial drop in performance (−5.2% kb accuracy on Cucerzan news data and approximately 10× worseruntime).Disambiguator. Cucerzan disambiguated the query mention with respect to document-level vectors derived from all entitymentions. Vectors are constructed from the document and the global set of entity candidates, each candidate of each canon-ical mention. A candidate vector of indicator variables is created for each of the global candidates, based on presence of thearticle’s categories and contexts. Contexts are anchor texts from the first paragraph or those that linked to another articleB. Hachey et al. / Artificial Intelligence 194 (2013) 130–150137and back again. The extended document vector is populated to represent the union of indicator variables from all entityvectors. The category values are the number of entity vectors containing that category and the context values the count ofthat context in the document. Each candidate list for each mention is re-ranked separately with respect to the document-level vector. Specifically, candidates are ranked by the scalar product of the candidate vector and the extended documentvector, with a penalty to avoid double-counting. Following Cucerzan, we exclude categories if their name contains any ofthe following words or their plurals: article, page, date, year, birth, death, living, century, acronym, stub;or a four-digit number (i.e., a year). We also exclude the Exclude in print category, which is used to mark contentthat should not be included in printed output. We do not shrink source document context where no clear entity candidatecan be identified.Benchmarking. We compared the performance of our reimplementation on the Cucerzan evaluation data (see Section 2.7),which consists of twenty news articles from msnbc. This data includes 629 entity mentions that were automatically linkedand manually verified by Cucerzan as linkable to Wikipedia articles. We achieved an accuracy of 88.3%, while Cucerzanreports an accuracy of 91.4%. There are several possible differences in our implementation. First, we are not certain whetherwe filter lists and categories using exactly the same heuristics as Cucerzan. We may also be performing coreference resolu-tion, acronym detection or case-normalisation slightly differently. Changes in Wikipedia, especially the changes to the goldstandard, may also be a factor. We observed that the evaluation was quite sensitive to small system variations, because thesystem tended to score either very well or rather poorly on each document, due to its global disambiguation model.3.4. Varma et al.Finally, Varma et al. [48] describe a system that uses a carefully constructed backoff approach to candidate generationand a simple text similarity approach to disambiguation. Despite the fact that it eschewed the complex disambiguationapproaches of other submissions, this system achieved the best result (82.2% accuracy) at the tac 2009 shared task.Extractor. The system first determines whether a query is an acronym (e.g., ABC). This is based on a simple heuristictest that checked whether a query consists entirely of uppercase alphabetical characters. If it does, the query document issearched for an expanded form. This scans for a sequence of words starting with the letters from the acronym, ignoringstop words (e.g., Australian Broadcasting Corporation, Agricultural Bank of China). No other preprocessing of the query or querydocument was performed.Searcher. Different candidate generation strategies are followed for acronym and non-acronym queries. For acronym queries,if an expanded form of the query is found in the query document, then this is matched against kb titles. Otherwise, theoriginal query string is used in an exact-match lookup against article/redirect/disambiguation titles, and bold terms in thefirst paragraph of an article. For non-acronym queries, the query string is first matched against kb titles. If no match is found,the query string is searched against the same aliases described above. The Varma et al. system for tac 2009 also usedmetaphone search against kb titles for non-acronym queries. We omitted this feature from our implementation becauseVarma et al. reported that it degraded performance in experiments conducted after the tac data was released (personalcommunication).Disambiguator. The Varma et al. approach ranks candidates based on the textual similarity between the query contextand the text of the candidate page, using the cosine measure. Here, the query context is the full paragraph surroundingthe query mention, where paragraphs are easily identified by double-newline delimiters in the tac source documents. Thecosine score ranks candidates using the default formulation in Lucene:Cosine(q, d) =|Tq ∩ Td|maxm∈M |Tq ∩ Tm|×(cid:2)t∈Tq(cid:3)t f (t, d) ×(cid:4)1 + log(cid:5)|D|df (t)× 1√|Td|(1)where q is the text from the query context, d is the document text, Ti is the set of terms in i, M is the set of documentsthat match query q, t f (t, d) is the frequency of term t in document d, D is the full document set, and df (t) is the count ofdocuments in D that include term t.4. DataWe report results on the tac data sets. tac queries consist of a mention string (e.g., Abbot) and a source documentcontaining it (e.g., . . . Also on DVD Oct. 28: “Abbot and Costello: The Complete Universal Pictures Collection”; . . . ). The goldstandard is a reference to a tac kb node (e.g., E0064214, or Bud Abbott), or nil if there is no corresponding node in thekb. tac source documents are drawn from newswire and blog collections. We extract and store body text, discarding markupand non-visible content if they are formatted using a markup language. After tokenising, we defer any further processing tospecific extractors.138B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150Table 3Comparison of tac data sets for all queries (Q) and for unique entities (E ).|Q|kbnilperorggpeNewsWebAcronym|E|kbnilperorggpetac 2009 testtac 2010 traintac 2010 test39041675222962727105673904082756018237813636460(43%)(57%)(16%)(69%)(15%)(100%)(0%)(21%)(33%)(67%)(24%)(65%)(11%)15001074426500500500783717173−462−−−−(72%)(28%)(33%)(33%)(33%)(52%)(48%)(12%)(−)(−)(−)(−)(−)2250102012307517507491500750347871402469334332205(45%)(55%)(33%)(33%)(33%)(67%)(33%)(15%)(46%)(54%)(38%)(38%)(24%)The tac kb is derived from pages in the October 2008 Wikipedia dump3 that have infoboxes. It includes approximately200,000 per nodes, 200,000 gpe nodes, 60,000 org nodes and more than 300,000 miscellaneous/non-entity nodes. Wealso exploit a more recent English Wikipedia dump from 30th July 2010. This consumes 11.8 GB on disk with bzip2compression, including markup for 3.3 M articles. We use the mwlib4 Python package to extract article text, categories,links, disambiguation and redirect information, and store them using Tokyo Tyrant,5 a fast database server for Tokyo Cabinetkey-value stores. This provides fast access to article data structures by title as well as the ability to stream through allarticles.We use the tac 2009 test data as our main development set, so that we can benchmark against a large set of publishedresults. We use the tac 2010 training data for training the Bunescu and Pa ¸sca [8] disambiguator. And we reserve the tac2010 test data as our final held-out test set. These are summarised for all queries in the top part of Table 3. The first thingto note is the difference in the proportion of nil queries across data sets. In both the tac 2009 and tac 2010 test sets, it isapproximately 55%. However, in the tac 2010 training set, it is considerably lower at 28%. The second difference is in thedistribution of entity types. The tac 2009 test data is highly skewed towards org entities while the tac 2010 training andtest data sets are uniformly distributed across per, org and gpe entities. Finally, while tac 2009 consisted solely of newswiredocuments, tac 2010 included blogs as well. The tac 2010 training data is roughly evenly divided between news and webdocuments (blogs), while the test data is skewed towards news (67%).The bottom part of Table 3 contains the corresponding numbers (where defined) for unique entities. Note that thisanalysis is not possible for the tac 2010 training data, since its nil queries have not been clustered. The main differencebetween the data sets is in terms of the average number of queries per entity (|Q|/|E|)—7 for tac 2009 compared to 2.6 fortac 2010 test. The proportion of nil queries is the same as in the query-level analysis at approximately 55% for the tac 2009and 2010 test sets. The distribution across entity types is similarly skewed for the tac 2009 data. Where the query-levelanalysis for the tac 2010 test data showed a uniform distribution across entity types, however, the entity-level analysisshows a substantial drop in the proportion of gpe entities.4.1. Evaluation measuresWe use the following evaluation measures, defined using the notation in Table 4. The first, accuracy ( A), is the official tacmeasure for evaluation of end-to-end systems. tac also reports kb accuracy ( AC ) and nil accuracy ( A∅), which are equivalentto our candidate recall and nil recall with a maximum candidate set size of one. The remaining measures are introducedhere to analyse candidate sets generated by different search strategies.accuracy ( A): percentage of correctly linked queries.A =|{Ci,0|Ci,0 = G}|N3 http://download.wikimedia.org.4 http://code.pediapress.com/wiki/wiki/mwlib.5 http://fallabs.com/tokyotyrant/.(2)B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150139Table 4Notation for searcher analysis measures.NGGiCCiCi, jNumber of queries in data setGold standard annotations for data set (|G| = N)Gold standard for query i (kb ID or nil)Candidate sets from system output (|C| = N)Candidate set for query iCandidate at rank j for query i (where Ci (cid:9)= ∅)candidate count ((cid:10)C(cid:11)): mean cardinality of the candidate sets. Fewer candidates mean reduced disambiguation workload.(cid:10)C(cid:11) =(cid:6)|Ci|iN(3)(4)(5)(6)candidate precision ( P C ): percentage of non-empty candidate sets containing the correct entity.P C =|{Ci|Ci (cid:9)= ∅ ∧ Gi ∈ Ci}||{Ci|Ci (cid:9)= ∅}|candidate recall (RC ): percentage of non-nil queries where the candidate set includes the correct candidate.RC =|{Ci|Gi (cid:9)= nil ∧ Gi ∈ Ci}||{Gi|Gi (cid:9)= nil}|nil precision ( P ∅): percentage of empty candidate sets that are correct (i.e., correspond to nil queries).P ∅ =|{Ci|Ci = ∅ ∧ Gi = nil}||{Ci|Ci = ∅}|nil recall (R∅): percentage of nil queries for which the candidate set is empty. A high R∅ rate is valuable because it isdifficult for disambiguators to determine whether queries are nil-linked when candidates are returned.R∅ =|{Ci|Gi = nil ∧ Ci = ∅}||{Gi|Gi = nil}|5. Wikipedia alias extraction(7)We extract a set of aliases—potential mention strings that can refer to an entity—for each Wikipedia article. By queryingan index over these aliases, we are able to find candidate referents for each entity mention. We consider the followingattributes of an article as candidate aliases:Article titles (Title) The canonical title of the article. While the first character of Wikipedia titles is case-insensitive andcanonically given in the uppercase form, for articles containing the special lowercase title template (such asgzip, iPod), we extract this alias with its first character lowercased.Redirect titles (Redirect) Wikipedia provides a redirect mechanism to automatically forward a user from non-canonicaltitles—such as variant or erroneous spellings, abbreviations, foreign language titles, closely-related topics, etc.—tothe relevant article. For articles with lowercase title, if the redirect title begins with the first word of thecanonical title, its first character is also lowercased (e.g., IPods becomes iPods).Bold first paragraph terms (Bold) Common and canonical names for a topic are conventionally listed in bold in the article’sfirst paragraph.Link anchor texts (Link) Links between Wikipedia articles may use arbitrary anchor text. Link anchors offer a variety offorms used to refer to the mention in running text, but the varied reasons for authors linking makes them noisy.We therefore extract all anchor texts that have been used to link to the article at least twice.Disambiguation page titles (DABTitle) Disambiguation pages are intended to list the articles that may be referred to by anambiguous title. The title of a disambiguation page (e.g., a surname or an abbreviation) is therefore taken as analias of the pages it disambiguates.Disambiguation pages usually consist of one or more lists, with each list item linking to a candidate referent ofthe disambiguated term. However, such links are not confined exclusively to candidates; based on our observations,we only consider links that appear at the beginning of a list item, or following a single token (often a determiner).All descendants of the Disambiguation pages category are considered disambiguation pages.Disambiguation redirects and bold text (DABRedirect) One page may disambiguate multiple terms—for instance, there isone disambiguation page for both Amp and AMP. In addition to the page title, we therefore also consider boldterms in the page and the titles of redirects that point to disambiguation pages as aliases of the articles theydisambiguate.140B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150Table 5Sources of aliases, including the number of articles (excluding disambiguation pages) and aliases with each source. Support indicates the average numberof sources that support an alias.Source# Articles# AliasesSupportwithout truncationwith truncationArticle titleRedirect titleBold termsLink anchorDisamb. titleDisamb. redirectDisamb. boldDisamb. hatnoteAny3 198 2901 493 9312 984 3812 728 066933 308907 330536 43890 5643 198 290Table 6Search over individual alias fields (tac 2009).3 198 2903 960 7653 601 2965 320 4231 126 7141 312 3271 563 65096 6493 777 8184 393 7093 601 2965 320 4231 203 6481 312 3271 650 858115 52417 156 466Alias sourceTitleRedirectLinkBoldHatnoteTruncatedDABTitleDABRedirect(cid:10)C(cid:11)0.20.14.21.60.01.23.52.7∞CP83.574.655.745.142.637.834.234.0∞CR37.220.080.148.81.224.529.318.9P ∅68.162.188.671.757.762.258.757.93.41.82.82.53.73.32.32.81.6R∅96.596.259.567.299.978.665.177.3Disambiguation hatnotes (Hatnote) Even when a name or other term is highly ambiguous, one of the referents is oftenfar more frequently intended than the others. For instance, there are many notable people named John Williams,but the composer is far more famous than the others. At the top of such an article, a link known as a hatnote tem-plate points to disambiguation pages or alternative referents of the term. We extract disambiguation informationfrom many of the hatnote templates in English Wikipedia, and use the referring article’s title as an alias, or thedisambiguated redirect title specified in the template.Truncated titles (Truncated) Wikipedia conventionally appends disambiguating phrases to form a unique article title, asin John Howard (Australian actor) or Sydney, Nova Scotia. For all alias sources that are titles orredirects, we strip expressions in parenthesis or following a comma from the title, and use the truncated title asan additional alias.We store the alias sources as features of each article-alias pair, and use them to discriminate between aliases in termsof reliability. Titles and redirects are unique references to an article and are therefore considered most reliable, while linktexts may require context to be understood as a reference to a particular entity. Table 5 indicates that while aliases derivedfrom link texts are numerous, they are much less frequently supported by other alias sources than are disambiguation pagetitles.The extracted aliases are indexed using the Lucene6 search engine. Aliases are stored in Lucene keyword fields whichsupport exact match lookup. We also index the Wikipedia text. Article text is stored in Lucene text fields which are usedfor scoring matches based on terms from entity mention contexts in source documents. The entire index occupies 12 gb ofdisk space, though this includes all the fields required for our experiments. Note that all experiments reported here set theLucene query limit to return a maximum of 1000 candidates.5.1. Coverage of alias sourcesTable 6 shows the candidate count, candidate recall, candidate precision, nil recall and nil precision for the different aliassources used on our development set, tac 2009. The first thing to note is the performance of the Title alias source. Titlequeries return 0 or 1 entities, depending on whether there was an article whose title directly matched the query. Thecandidate count of 0.2 indicates that 20% of the query mentions matched Wikipedia titles. These matches return the correctentity for 37.2% of the non-nil queries. Precision over these title-matched non-nil queries was 83.5%. This means thatsystems may benefit from a simple heuristic that trusts direct title matches, and simply returns the entity if a match isfound.6 http://lucene.apache.org/.B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150141Table 7Search over multiple alias fields (tac 2009).Alias sourceTitle+Redirect+Link+Bold+Hatnote+Truncated+DABTitle+DABRedirectTable 8Backoff search over alias fields (tac 2009).Alias sourceTitle+Redirect+Link+Bold+Hatnote+Truncated+DABTitle+DABRedirect(cid:10)C(cid:11)0.20.34.24.74.75.06.97.2(cid:10)C(cid:11)0.20.32.42.42.42.42.42.4∞CP83.579.456.255.755.755.756.556.3∞CP83.579.456.255.855.855.855.855.4∞CR37.254.681.784.884.885.487.687.8∞CR37.254.676.577.177.177.177.177.1P ∅68.175.090.290.690.690.690.890.7P ∅68.175.087.688.288.288.288.288.1R∅96.592.659.455.155.154.253.352.5R∅96.592.663.862.962.962.962.962.2It is very rare for a direct title match to be returned when the answer is actually nil: this only occurred for 3.5% of thequeries. It was, however, common for title match failures to occur for non-nil queries. This can be seen in the nil precisionfigure, which is only 68.1%. A title-match system that returns an entity whose title matches the query, or nil otherwise,achieves 71.0% accuracy on the end-to-end linking task (tac 2009). This is a fairly strong baseline—half of the 35 runssubmitted to tac 2009 scored below it. Expanding this system to also consult redirect titles improves this baseline to 76.3%linking accuracy. Only 5 of the 14 tac 2009 teams achieved higher accuracy than this. The other alias sources potentiallyreturn multiple candidates, so their utility depends on the strength of the disambiguation component.Table 7 shows how the number of candidates proposed increases as extra alias sources are considered, and how muchcandidate recall improves. The addition of link anchor texts increases candidate recall to 81.7%, but also greatly increases thenumber of candidates suggested. The nil recall drops from 92.6% to 59.4%, which means that at least one candidate hasbeen proposed for over 40% of the nil-linked queries. This makes some form of nil detection necessary, either through asimilarity threshold, or a supervised model, as used by Zheng et al. [54]. Using all alias sources produces a candidate recallof 87.8%, with a mean of 7.2 candidates returned per query. The candidate recall constitutes an upper bound on linking kbaccuracy. That is, there are 12.2% of kb-linked queries which even a perfect disambiguator would not be able to answercorrectly. Many of these queries are acronyms or short forms that could be retrieved by expanding the query with anappropriate full-form from the source document (see experiments and analysis in Sections 6.2, 7, 8.2, and 9 below).One way to reduce the number of candidates proposed is to use a backoff strategy for candidate generation. Usingthis strategy, the most reliable alias sources are considered first, and the system only consults the other alias sources if 0candidates are returned. Table 8 shows the performance of the backoff strategy as each alias source is considered, orderedaccording to their candidate precision. A maximum of 2.4 candidates is returned, with a candidate recall of 77.1%. This maybe a good strategy if a simple disambiguation system is employed, such as cosine similarity.6. Analysis of searcher performanceHaving described our reimplementations of several named entity linking systems, we now examine their performancein more detail, beginning with the accuracy of their searchers—that is, how accurately the systems propose candidates frommention strings.6.1. Comparison of implemented searchersTable 9 contains analysis results for our searcher reimplementations. The first row describes the performance of ourBunescu and Pa ¸sca searcher, which uses exact match over article, redirect, and disambiguation title aliases. The second rowdescribes our Cucerzan searcher, which includes coreference and acronym handling. As described in Section 3.3, mentionsare replaced by full-forms, as determined by coreference and acronym detection heuristics. The query terms are searchedusing exact match over article, redirect, and disambiguation titles, as well as apposition-stripped article and redirect titles.Finally, the third row describes our Varma et al. searcher, which replaces acronyms with full-forms where possible andemploys a backoff search strategy that favours high-precision matching against article titles that map to the kb over alias142B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150Table 9Performance of searchers from the literature (tac 2009).SearcherBunescu and Pa ¸scaCucerzanVarma et al.(cid:10)C(cid:11)3.63.23.0∞CP56.358.659.8Table 10Effect of coreference/acronym handling on searcher performance (tac 2009).SearcherCucerzan− coreference handlingVarma et al.− acronym handling(cid:10)C(cid:11)3.24.13.03.8∞CP58.653.459.854.0∞CR77.079.381.2∞CR79.379.381.279.4P ∅86.688.890.9P ∅88.889.090.989.6R∅62.765.166.4R∅65.156.666.457.9Fig. 1. Effect of query limit on searcher candidate recall.search. Alias search includes exact match over article, redirect, and disambiguation titles, as well as bold terms in the firstparagraph of an article.The implemented Cucerzan and Varma et al. perform best. They both achieve candidate precision of close to 60% atcandidate recall near 80%. This suggests that coreference and acronym handling are important and that a preference forhigh-precision matching is also beneficial. The Varma et al. searcher is slightly better in terms of candidate precision (+1.2%)and candidate recall (+1.9%). It also returns a candidate set size that, on average, contains 0.2 fewer items. This correspondsto a reduction in ambiguity of 6.3% with respect to the Cucerzan searcher.6.2. Effect of extractors on searchTable 10 contains a subtractive analysis of coreference and acronym handling in searchers from the literature. The re-spective components result in less ambiguity (−0.9 for Cucerzan and −0.8 for Varma et al.) and a simultaneous increasein candidate precision (+5.2% and +5.8 respectively). For Varma et al., there is also an increase in candidate recall (+1.8%).This highlights the importance of using more specific mention forms where possible, as they are more likely to match thecanonical names that occur in Wikipedia.6.3. Effect of query limit on searcher candidate recallOne way to improve disambiguation efficiency is to reduce the number of candidates that must be considered. However,the correct candidate is not always the first one returned by the searcher. Fig. 1 plots the candidate recall of our searcherimplementations against the query limit—the maximum number of results returned by the lucene alias index. All threelinkers start with candidate recall under 60% and climb to their maximum at a query limit of 1000. Interestingly, thereappears to be a knee at 100 for all three searchers, which suggests the possibility of some efficiency gain. However, goingfrom a query limit of 100 down to 10 results in a substantial drop in candidate recall, especially for the Bunescu and Pa ¸scasearcher. Despite the possible efficiency gain, for the remaining experiments here we keep the query limit at 1000 so thatour implementations are as close as possible to the literature.B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150143Table 11Number of kb accuracy errors due to search (tac 2009).SystemBunescu and Pa ¸scaCucerzanVarma et al.Systems agreeSearch errorsTotal errors386384316287899847776301Table 12Distribution of searcher errors on tac 2009 queries.Error typeAmbiguousName variationAnnotationOrganisationTypographicTotalExamplesTypeTokenHealth Department, Garden CityAir Macao, Cheli, ABCMainland China, Michael KennedyNew CaledoniaBlufton–20266546111810938148287Table 13Coreference analysis over 100 queries sampled from the tac 2009 queries.CoreferrableAcronym✔✔✘✘✔✘✔✘Count12124727. Searcher errorsIn this section, we investigate the types of errors made by each of the three systems we implemented. The first questionwe asked was whether systems were making errors because their searchers were failing to find the candidates. Table 11shows the number of search errors for each system. It also shows the total number of linking kb accuracy errors (due toeither searchers or disambiguators) in the third column. The last row shows the number of queries for which all threesystems returned an incorrect result. On average, 43% of kb accuracy errors are due to search recall problems. It is alsointeresting to note that a large proportion of the searcher error queries were common to all systems.Table 12 shows the distribution of the common search errors, classified into broad categories. The Type column containserror totals over unique query mention strings, while the Token column contains error totals over individual queries. Themost common type of search error occurs when a mention is underspecified or ambiguous (e.g., Health Department). Namevariations—including nicknames (e.g., Cheli for Chris Chelios), acronyms (e.g., ABC), transliterations (e.g., Air Macao insteadof Air Macau), and inserted or deleted tokens (e.g., Ali Akbar Khamenei instead of Ali Khamenei)—are also problematic. Thereare a few cases that may indicate annotation errors. For example, several gold standard articles are disambiguation pages, orhave existed since before the dataset was prepared. Other errors are due to targeting a mention at an incorrect point in anorganisational structure. The distinction between general university sports teams and the teams for baseball, for example, issubtle and proved very difficult for the systems to draw. There are also some legitimate typographic errors: Blufton shouldbe Bluffton.We also investigated the impact of coreference on linking performance over a sample of 100 queries drawn at randomfrom the tac 2009 data. Table 13 contains the counts of these queries that can be coreferred to a more specific mention andthe count that are acronyms. Among the 24 coreferrable queries, our Cucerzan coreference module correctly resolves 5 andour Varma et al. acronym expansion module correctly resolves 6—three in common. Both systems correctly corefer someacronyms, including DCR (cid:3)→ Danish Council for Refugees, DMC (cid:3)→ DeLorean Motor Co. The Varma et al. coreference addition-ally corefers more acronym cases such as CPN-UML (cid:3)→ Communist Party of Nepal (Unified Marxist-Leninist) and TSX (cid:3)→ TokyoStock Exchange. Since the Cucerzan implementation only corefers nes, ne boundary detection error can rule out corefer-ring some acronyms, but correctly handles Cowboys (cid:3)→ Dallas Cowboys and Detroit (cid:3)→ Detroit Pistons. Note that while mostacronyms are coreferrable, only half of the coreferrable queries are acronyms, indicating that coreference is advantageousbut risks introducing complexity and potentially error.8. Analysis of disambiguator performanceNext, we examine disambiguator performance in more detail, beginning with the end-to-end accuracy of implementedlinkers.144B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150Table 14Comparison of systems from the literature (tac 2009).Systemnil baselineTitle baseline+Redirect baselineBunescu and Pa ¸scaCucerzanVarma et al. replicatedtac 09 Mediantac 09 Max (Varma)A57.171.076.377.078.380.171.182.2AC0.037.254.667.871.372.363.576.5Table 15Effect of coreference/acronym handling on end-to-end linking performance (tac 2009).SystemCucerzan− coreference handlingVarma et al.− acronym handlingA78.374.980.177.3AC71.369.472.369.7A∅100.096.592.683.883.586.078.986.4A∅83.579.086.083.08.1. Comparison of implemented linkersTable 14 summarises the performances of the different systems on the tac 2009 test data. In addition to the systemsdescribed above, we report an nil baseline that returns nil for every query. Thus the overall accuracy of 57.1% reflects thenumber of nil queries in the data set. We also report baselines based on exact matching against Wikipedia article titles, andexact matching against article titles and redirect titles (Section 5.1). The Title + Redirect baseline in particular is a strongbaseline for this task, achieving a score 5.2 points above the median and 5.9 points below the maximum score achievedby submissions to the shared task. The last two rows correspond to the median and maximum results from the tac 2009proceedings, where the maximum corresponds to the reported results from Varma et al.Of the systems we implemented, the Varma et al. approach performs best on this data, followed by Cucerzan. TheCucerzan and the Bunescu and Pa ¸sca systems perform only slightly better than the Title + Redirect baseline system, whichdoes not use any disambiguation, and simply queries for exact matches for the mention string over the title and redirectfields. However, both systems would have placed just outside the top 5 at tac 2009.While the Varma et al. system was the best system submitted to tac 2009, two recent papers have reported higher scoreson the same data. Zheng et al. [54] report an accuracy of 84.9%, the highest in the literature, using an approach based onlearnt ranking with ListNet and a separate svm classifier for nil detection over a diverse feature set. Zhang et al. [53] reportan accuracy of 83.8%, using a classifier for nil detection built over a large training set derived from Wikipedia. Nevertheless,the competitiveness of the Varma et al. approach still suggests that a good search strategy is critical to nel, while differentdisambiguators have much less impact.8.2. Effect of extractors on disambiguationTable 15 contains a subtractive analysis of coreference and acronym handling in disambiguators from the literature. InTable 10 above (effect of extractors on search), we saw that this resulted in lower ambiguity without significantly affectingprecision or recall. Here, we see that this results in substantial improvements in accuracy ( A) of approximately 3 points.For our Cucerzan implementation, the difference is mainly in terms of nil accuracy, which sees a 4.5 point increase due tothe use of more specific name variants for search. Our Varma et al. implementation sees a more balanced increase in kbaccuracy and nil accuracy of approximately 3 points each. The relatively large increase in kb accuracy for Varma et al. maybe due to its search of the entire document for acronym expansions, rather than just other entity mentions as is the case forour Cucerzan coreference handling. This makes the acronym expansion less vulnerable to Named Entity Recognition errors.We also evaluated linker performance over the 100 query sample mentioned in Section 7 above. On this sample, addingcoreference/acronym handling allowed our Cucerzan and Varma et al. implementations to correctly link one more queryeach.8.3. Effect of searchers on disambiguationTable 16 contains results for versions of our Bunescu and Pa ¸sca and Cucerzan implementations that use the describedcandidate search strategies, but replace the disambiguation approach with the simple cosine disambiguator described inSection 3.4. The results here relate directly to the search results in Table 9 (comparison of implemented searchers), withhigh accuracy achieved by the searchers that have high candidate recall and low candidate count. In Table 9, the Varma et al.B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150145Table 16Effect of searchers on cosine disambiguation (tac 2009).SearcherBunescu and Pa ¸scaCucerzanVarma et al.A77.778.880.1AC69.669.772.3Table 17Combinations of searchers on implemented disambiguators (tac 2009).SearcherBunescu and Pa ¸scaVarma et al.CucerzanVarma et al.DisambiguatorBunescu and Pa ¸scaBunescu and Pa ¸scaCucerzanCucerzanA77.078.178.379.4AC69.667.971.373.3A∅83.885.686.0A∅83.885.883.583.9Table 18Number of kb accuracy errors due to disambiguation.SystemBunescu and Pa ¸scaCucerzanVarma et al.Systems agreeDisambiguator errorsTotal errors51346346014899847776301searcher outperforms the Bunescu and Cucerzan searchers in terms of candidate recall by 1.9 and 4.2 points respectively, andin terms of candidate countby 0.2 and 0.6. Here, it also performs best in terms of accuracy at 80.1%—2.4 points better thanBunescu and 1.3 point better than Cucerzan.Note that the Bunescu and Pa ¸sca and Cucerzan disambiguators (Table 14) perform worse than the cosine disambiguatorsreported here. This may be attributed in part to differences between the training and development testing data. For example,the distributions between nil and kb queries changes as described above in Table 3. Also, the tac 2010 training data includesweb documents while the tac 2009 evaluation data used for development testing here does not. For Bunescu and Pa ¸sca,the difference may also be due in part to the fact that the training data is fairly small. The held-out evaluation data usedin Section 10 is more similar to the training data. Results on this data (Table 21 below) suggest that the Bunescu andPa ¸sca learning-to-rank disambiguator obtains higher accuracy than the corresponding cosine disambiguator (+0.7%), with a1.5 point increase in candidate recall.8.4. Effect of swapping searchersTable 17 contains a comparison of the Bunescu and Pa ¸sca and the Cucerzan disambiguators using the search strategythey describe and the search strategy from Varma et al.7 For the Cucerzan system, we use Varma et al. search for the tacquery only and Cucerzan search for the other named entity mentions in the document. The results suggest that the high-precision Varma et al. search is generally beneficial, resulting in an increase in accuracy (+1.1%) for both the Bunescu andPa ¸sca and the Cucerzan disambiguators. Both of these results suggest that selecting a good search strategy is crucial.9. Disambiguator errorsTable 18 shows the number of disambiguator errors—queries in the tac 2009 data where the correct link was notreturned because the disambiguator was unable to choose the correct candidate from the search results. It also shows thetotal number of kb accuracy errors (due to either searchers or disambiguators). The last row shows the number of queriesfor which all three systems return an incorrect result. The errors here account for the remaining errors (approximately 47%)that were not attributed to the searchers in Table 11 above. Interestingly, where search errors were largely common toall systems, few disambiguation errors are shared. Given the variation in performance and diversity of errors among thesystems compared here, it is tempting to explore voting. However, many of the approaches described here already requiresubstantial resources for large-scale applications (e.g., linking all mentions in a news archive containing decades worth ofarticles). We believe it is more important to explore efficiency improvements in future work. Therefore, we do not reportvoting experiments here.7 Note that the Varma et al. disambiguator corresponds to our cosine disambiguator. Therefore, the cosine disambiguation rows in Tables 14 and 21correspond to the Bunescu and Pa ¸sca and Cucerzan systems with Varma et al. disambiguation. Note also that we do not swap in the Bunescu and Pa ¸scasearcher since it is not competitive (as discussed in Section 6.1).146B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150Table 19Distribution of disambiguator errors on tac 2009 queries.Error typeName variationAmbiguousTotalExamplesABC, UTGarden City–Type246Token141024Table 20Characteristic errors over tac 2009 queries.SystemTypeTokenAcronymNot acronymAcronymNot acronymBunescu and Pa ¸scaCucerzanVarma et al.21301716332113881304311568Table 21Comparison of systems from the literature on the tac 2010 test data.Systemnil baselineTitle baseline+Redirect baselineBunescu and Pa ¸sca (CosDAB)Cucerzan (CosDAB)Bunescu and Pa ¸scaCucerzanVarma et al.tac 2010 Mediantac 2010 Maximum (Lehmann)A54.769.679.480.181.080.884.581.668.486.8AC0.035.060.667.171.168.478.470.5−80.6A∅100.098.495.090.989.391.189.590.7−92.0Table 19 shows a breakdown of the common errors. The types of errors are less varied than search errors, and aredominated by cases where the entities have similar names and are from similar domains. Name variation still makes up areasonable proportion of the errors at this stage, but these are exclusively acronyms (i.e., there are no nicknames, transliter-ations, or insertions/deletions as in the search errors above).Finally, Table 20 summarises the counts of queries for which each system returned an incorrect entity while the othertwo did not. The errors are categorised according to whether the mention was an acronym or not, and counts are aggregatedat type and token granularity. The relative proportion of acronym and non-acronym errors differs slightly for the threesystems, with Bunescu and Pa ¸sca making more acronym errors, while Cucerzan balances the two, and Varma et al. makesmore errors on non-acronyms. This trend reflects the level of acronym processing: Bunescu and Pa ¸sca has none whereasVarma et al. uses a finely tuned acronym search and Cucerzan uses coreference. The counts over tokens broadly follow thesame trend, although skewed by the bursty distribution of types and tokens.10. Final resultsAs a final comparison, we evaluate our implementations of seminal systems on the tac 2010 test data, which we set asideduring system development. The results are shown in Table 21. Results columns correspond to the official tac evaluationmeasures, which include accuracy ( A), kb accuracy ( AC ) and nil accuracy ( A∅). Rows correspond to systems. The nil baselineis a system that returns nil for every query. The overall accuracy of 54.7% here reflects the percentage of queries with nil asthe gold answer. The Title baseline system performs an exact match lookup on Wikipedia titles. The Title + Redirect baselineperforms an exact match on the union of article and redirect titles. The next three rows correspond to our implementationsof the Bunescu and Pa ¸sca, Cucerzan, and Varma et al. systems.Finally, the last two rows contain the median and maximum system scores from tac 2010. The maximum was obtainedby Lehmann et al. [28], whose searcher differs from those explored here in using token-based (rather than exact-match)search, coreference filtering, and Google search. The Lehmann et al. disambiguator uses features based on alias trustworthi-ness, mention-candidate name similarity, mention-candidate entity type matching, and Wikipedia citation overlap betweencandidates and unambiguous entities from the mention context. A heuristic over the features is used for candidate ranking.And a supervised binary logistic classifier is used for nil detection.The Cucerzan system is the most accurate of our systems on the evaluation data, achieving an accuracy only 2% off themaximum performance reported in the tac 2010 challenge. The strong performance of the Cucerzan system on this datais surprising, given the results on the development data. On the tac 2009 data, the Varma et al. system outperforms theCucerzan system by 2% (see Table 14). There are a number of differences between the two data sets (as detailed in Table 3).B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150147Table 22Overall accuracy by genre and entity type (tac 2010 test).Systemnil baselineTitle baseline+Redirect baselineBunescu and Pa ¸sca (CosDAB)Cucerzan (CosDAB)Bunescu and Pa ¸scaCucerzanVarma et al.Newsorg72.672.874.877.680.877.077.278.4gpe21.051.265.665.668.464.483.068.2per91.091.097.097.298.297.298.297.4Weborg33.249.680.487.686.488.483.690.0gpe56.675.176.765.560.272.371.968.7per33.172.182.986.987.689.688.087.3The 2009 data has more queries per entity, is skewed towards org queries and contains no web text. The 2010 test data ismore varied and balanced, containing more entities overall (evenly balanced between kb and nil) and an even distributionof queries by entity type. Acronyms comprise 15% of 2010 test queries versus 21% of 2009 queries and this may account forsome performance loss for the Varma et al. [48] linker, which has specialised acronym processing.10.1. Performance by genre and entity typeTable 22 contains accuracy scores broken down by genre (news or web) and entity type (org, gpe or per). Rows corre-spond to the same systems reported in Table 21 above. The best scores in each column are in bold. The first thing to noteis that no approach is consistently best across genres and entity types. This suggests that system combination by votingor entity-specific models may be worth investigating. Next, the percentage of nil queries (as reflected in the nil baselinescores) varies hugely across genre and entity types. In particular, the nil percentage in web text is much lower than in newstext for org and per entities, but much higher for gpe entities.There are two striking results about the behaviour of the Title + Redirect baseline system. First, the system performsnear perfectly on per entities in news text (97.0%). In part, this is probably attributable to the editorial standards associatedwith news, which results in per entities mentioned in news generally being referred to using canonical forms. However,since the queries for the evaluation data set are not randomly sampled, it is not possible to quantify this observation. Thesecond striking result is the fact that the Title + Redirect baseline outperforms all implemented systems on gpe entities inweb text. This suggests that candidate generation is very noisy for these entities, which results in an especially difficultdisambiguation problem. For org entities, systems with cosine disambiguators (including Varma et al.) are best in bothnews and web text. It is also interesting to note that there is very little variation in scores for per entities, especially innews text.Overall, our Cucerzan implementation is best for newswire, but does worse on web text. This holds for the cosinedisambiguators as well as for the disambiguators from the literature. This suggests that the Cucerzan search strategy istuned for more formal text. This may be attributed in part to the searcher’s reliance on coreference and acronym handling,which are more accurate on text that follows the journalistic conventions for introducing new entities into discourse fairlyunambiguously. For the Cucerzan disambiguator, the poorer performance of named entity recognition on web text is alsolikely to have the effect of introducing more noise into the document-level vector representations.11. DiscussionWikipedia is a rich source of data for natural language processing. Recently, it has been exploited for a number ofinformation extraction tasks ranging from named entity recognition to relation extraction. This article explored the problemof entity linking, which disambiguates entity mentions by linking them to their Wikipedia page. This exciting new taskmoves beyond conventional named entity recognition where the output is a list of unnormalised entity mention strings.It shifts information extraction towards actionable semantic interpretation where objects in text are grounded to a nodein an underlying knowledge base. The task opens up a range of applications from aggregation of information about agiven entity across diverse structured, semi-structured and unstructured knowledge sources, to automated reasoning overextracted information.The named entity linking task was first explored by Bunescu and Pa ¸sca [8] and Cucerzan [9] and has since been thefocus of three shared tasks organised by the US National Institute of Standards and Technology as part of the Text AnalysisConferences (tac) in 2009, 2010, and 2011. Previous approaches have largely focused on devising elaborate approaches toranking a set of candidates, with the goal of promoting the true candidate to the top of the list. These assume a searchstrategy for generating a list of candidate entities, but previous work has not investigated candidate generation in detail.A notable exception is the top-scoring entry to the tac 2009 shared task, which includes a highly tuned candidate generationstrategy, but relies on a simple cosine similarity between the query context and the candidate Wikipedia page for ranking.This suggests that it is worthwhile to consider candidate generation strategies carefully.148B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150A key theme across our results is that baseline systems are difficult to beat. Specifically, exact match lookup against page andredirect titles results in accuracy scores of 76.3% on the tac 2009 test data and 79.4% on the tac 2010 test data. This is dueto the highly curated nature of Wikipedia, where commonly searched variations of names are very likely to have redirect ordisambiguation pages. On the other hand, Wikipedia is a dynamic resource and redirect and disambiguation pages are thuslikely to reflect changes in popularity of search terms over time. This has important implications for evaluation—the versionof Wikipedia used might have a strong effect on system performance, especially the recall of candidate generation.Another theme across our results is that search strategies are extremely important. Analysis of alias sources shows thatpage titles and redirect titles have very high precision; thus the Title + Redirect baseline is able to correctly return morethan 50% of links in the tac 2009 and 2010 data sets, while maintaining an nil recall near 95%. Additionally, comparisonacross our searcher implementations highlighted the importance of coreference and acronym handling. Subtractive analysisof these components showed that they can lead to small improvements in candidate recall (+1.8 for Varma et al.). Moreimportantly, they lead to an increase of approximately 5.5% in the percentage of candidate sets that include the correctanswer (candidate precision) with a simultaneous decrease of approximately 0.8 in ambiguity as measured by the averagecandidate set size.Detailed evaluation measures for candidate generation have proved useful for predicting subsequent performance onthe end-to-end linking task. A searcher’s candidate recall, for example, sets an upper bound on disambiguator performance.That is, the maximum kb accuracy obtainable by a disambiguator is equal to the candidate recall of the searcher proceedingit. Recent work reports dramatically higher candidate recall of 96.9% [28]. This is very promising and led to improvedlinker accuracy and warrants further investigation to determine the relative effect of its novel components: using token-based rather than exact match search, coreference filtering based on character overlap, and use of Google search. However,comparison of our search and linking results suggests that improvements in candidate recall cannot come at the cost of candidateprecision, and that search ambiguity needs to be carefully managed as well. This is also supported by personal communicationwith Varma et al., in which they reported that, upon more detailed analysis, they found that the metaphone search employedin their tac 2009 system actually reduced the final accuracy of their linker.Our results highlight some interesting similarities and differences between the named entity and word sense disam-biguation tasks. Both tasks have strong baselines related to first sense heuristics: one referent of a word or entity is muchmore common than the other possibilities, even when the number of other candidates is quite large. Wikipedia editors haveadapted to this phenomenon by tuning article titles and redirects to capture the most likely intended meanings of commonqueries, which may be why the Title + Redirect baseline we present is so competitive. In both disambiguation tasks, thedocument contents are important clues for disambiguation, and simple methods based on bag-of-words models are fairlycompetitive. Early work on linking to Wikipedia [32] disambiguated arbitrary terminology, relating the task to word sensedisambiguation. However, there is an important difference between named entity linking and conventional word sense dis-ambiguation with WordNet: the candidate senses for word sense disambiguation are provided directly, but candidate generationis critical for successful named entity linking. The importance of this aspect of the problem has until now not been properlyappreciated.11.1. Recent literatureThe implementation work we present is the start of a larger effort to perform a detailed comparison of various entitylinking approaches within the same framework. A key development in the recent literature is the use learning-to-rankapproaches. In addition to the Bunescu and Pa ¸sca [8] approach explored here, Dredze et al. [12] and Zheng et al. [54] usesvmrank and ListNet respectively to incorporate a variety of features. Zheng et al. report 84.9% overall accuracy on the tac2010 test data. Another key development is the use of instance selection to generate training data from Wikipedia [53].Zhang et al. [52] leverage this in achieving the current state-of-the-art performance of 86.1% on the tac 2010 data.Wikipedia structure has continued to drive new approaches, including those that eschew supervised machine learning.Han et al. [22] propose a generative probabilistic model based on entity, mention, and context statistics, which performsat 86% accuracy over the tac 2009 data. Gottipati and Jiang [18] use language model-based information retrieval withnemention and candidate context. This is particularly competitive on the variant of the tac task in which Wikipedia text isnot allowed. It obtains 85.2%, well above the top-ranking score of 77.9% from the official tac 2010 results.Wikipedia’s link structure, in particular, has driven new approaches incorporating graph-based methods for nel. This isthe motivation behind citation overlap measures between candidates and unambiguous context entities [33,28,43,44]. Morerecent systems build a graph where vertices correspond to mentions and/or their entities and edges correspond to candidateentities for given mentions and/or entity–entity links from Wikipedia. Intuitively, highly connected regions represent the“topic” of a document and correct candidates should lie within these regions. Ploch [41] demonstrates that PageRank [7]values for candidate entities are a useful feature in their supervised ranking and nil detection systems, leading to overallaccuracy of 84.2% on the tac 2009 data. Hachey et al. [20] show that degree centrality is better than PageRank, leading toperformance of 85.5% on the tac 2010 test data. And Guo et al. [19] show that degree centrality is better than a baselinesimilar to the cosine (CosDAB) baselines reported here, leading to performance of 82.4% on the tac 2010 test data. Recentexperiments on other data sets have also explored evidence propagation [22] and community detection [24].B. Hachey et al. / Artificial Intelligence 194 (2013) 130–15014912. ConclusionEntity linking allows applications to compute with direct references to people, places and organisations, rather thanpotentially ambiguous or redundant character strings. As with other world knowledge problems, one important questionabout the task is what information a system must have access to in order to achieve satisfactory accuracy. This question isvery difficult to answer by building a single system. Instead, a range of approaches must be evaluated in a single framework,with the ability to plug together different components and analyse them in detail.We have presented the first systematic investigation of the entity linking problem, by implementing three of the canoni-cal systems in the literature. We have performed the first direct comparison of these systems, analysed their errors in detail,and come to some surprising conclusions about the nature of the entity linking task.We have found it useful to divide the entity linking task into two phases: search, and disambiguation. During the searchphase the system proposes a set of candidates for a named entity mention to be linked to, which are then ranked by thedisambiguator.To our surprise, we found that much of the variation between the systems we considered was explained by the perfor-mance of their searchers. This was surprising because the literature on named entity linking has focused almost exclusivelyon disambiguation. The disambiguation task is arguably conceptually more interesting, since it lends itself to algorithmicsolutions, and is related to the long-studied problem of word sense disambiguation. However, we have found that a simplevector space model performed surprisingly well compared to the more interesting disambiguation strategies we imple-mented.Until now, it has been impossible to compare search and disambiguation strategies for entity linking directly, since onlyfinal accuracy figures have been available. Task accuracy is less informative, because it is unclear how ambitiously thesearcher is proposing candidates for the disambiguator to rank. A conservative system with no disambiguation can performsurprisingly well, without offering any way to improve accuracy on the task in future. We have shown that state-of-the-artentity linking systems are pushing past this local maximum, but our results suggest that there is a long way to go on thedifficult problem of determining which of a given set of candidates is the most likely referent of a named entity mention.References[1] J. Artiles, J. Gonzalo, S. Sekine, The SemEval 2007 WePS evaluation: Establishing a benchmark for the Web People Search task, in: Proceedings of the4th International Workshop on Semantic Evaluations, 2007, pp. 64–69.[2] J. Artiles, J. Gonzalo, S. Sekine, WePS 2 evaluation campaign: Overview of the Web People Search clustering task, in: Proceedings of the WWW WebPeople Search Evaluation Workshop, 2009.[3] A. Bagga, B. Baldwin, Algorithms for scoring coreference chains, in: Proceedings of the LREC Linguistic Coreference Workshop, 1998, pp. 560–567.[4] A. Bagga, B. Baldwin, Entity-based cross-document coreferencing using the vector space model, in: Proceedings of the 17th International Conferenceon Computational Linguistics, 1998, pp. 79–85.[5] L. Bentivogli, P. Forner, C. Giuliano, A. Marchetti, E. Pianta, K. Tymoshenko, Extending English ACE 2005 corpus annotation with ground-truth links toWikipedia, in: Proceedings of the Coling Workshop on the People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, 2010, pp. 19–27.[6] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, S. Hellmann, DBpedia—a crystallization point for the web of data, Journal of WebSemantics 7 (2009) 154–165.[7] S. Brin, L. Page, The anatomy of a large-scale hypertextual web search engine, in: Proceedings of the 7th International World Wide Web Conference,1998, pp. 107–117.[8] R. Bunescu, M. Pa ¸sca, Using encyclopedic knowledge for named entity disambiguation, in: Proceedings of the 11th Conference of the European Chapterof the Association for Computational Linguistics, 2006, pp. 9–16.[9] S. Cucerzan, Large-scale named entity disambiguation based on Wikipedia data, in: Proceedings of the Joint Conference on Empirical Methods inNatural Language Processing and Computational Natural Language Learning, 2007, pp. 708–716.[10] J.R. Curran, S. Clark, J. Bos, Linguistically motivated large-scale NLP with C&C and Boxer, in: Proceedings of the 45th Annual Meeting of the Associationfor Computational Linguistics (demo), 2007, pp. 33–36.[11] D. Day, J. Hitzeman, M. Wick, K. Crouch, M. Poesio, A corpus for cross-document co-reference, in: Proceedings of the 6th International Conference onLanguage Resources and Evaluation, 2008, pp. 23–31.[12] M. Dredze, P. McNamee, D. Rao, A. Gerber, T. Finin, Entity disambiguation for knowledge base population, in: Proceedings of the 23rd InternationalConference on Computational Linguistics, 2010, pp. 277–285.[13] A. Fader, S. Soderland, O. Etzioni, Scaling Wikipedia-based named entity disambiguation to arbitrary web text, in: Proceedings of the IJCAI Workshopon User-Contributed Knowledge and Artificial Intelligence, 2009, pp. 21–26.[14] I.P. Fellegi, A.B. Sunter, A theory for record linkage, Journal of the American Statistical Association 64 (1969) 1183–1210.[15] P. Ferragina, U. Scaiella, TAGME: on-the-fly annotation of short text fragments (by Wikipedia entities), in: Proceedings of the 19th International Con-ference on Information and Knowledge Management, 2010, pp. 1625–1628.[16] W. Gale, K. Church, D. Yarowsky, Work on statistical methods for word sense disambiguation, in: Proceedings of the AAAI Fall Symposium on IntelligentProbabilistic Approaches to Natural Language, 1992, pp. 54–60.[17] C.H. Gooi, J. Allan, Cross-document coreference on a large-scale corpus, in: Proceedings of the 7th Annual Conference of the North American Chapterof the Association for Computational Linguistics, 2004, pp. 9–16.[18] S. Gottipati, J. Jiang, Linking entities to a knowledge base with query expansion, in: Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing, 2011, pp. 804–813.[19] Y. Guo, W. Che, T. Liu, S. Li, A graph-based method for entity linking, in: Proceedings of 5th International Joint Conference on Natural LanguageProcessing, 2011, pp. 1010–1018.[20] B. Hachey, W. Radford, J.R. Curran, Graph-based named entity linking with Wikipedia, in: Proceedings of the 12th International Conference on WebInformation System Engineering, 2011, pp. 213–226.[21] X. Han, L. Sun, A generative entity-mention model for linking entities with knowledge base, in: Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics, 2011, pp. 945–954.150B. Hachey et al. / Artificial Intelligence 194 (2013) 130–150[22] X. Han, L. Sun, J. Zhao, Collective entity linking in web text: A graph-based method, in: Proceedings of the 34th International Conference on Researchand Development in Information Retrieval, 2011, pp. 765–774.[23] L. Hirschman, M. Colosimo, A. Morgan, A. Yeh, Overview of BioCreAtIvE task 1B: Normalized gene lists, BMC Bioinformatics 6 (2005) S11.[24] J. Hoffart, M.A. Yosef, I. Bordino, H. Fürstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, G. Weikum, Robust disambiguation of named entities in text,in: Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2011, pp. 782–792.[25] W.C. Huang, S. Geva, A. Trotman, Overview of the INEX 2009 link the wiki track, in: Lecture Notes in Computer Science, vol. 6203, Springer-Verlag,Berlin/Heidelberg, 2010, pp. 312–323.[26] T. Joachims, Training linear SVMs in linear time, in: Proceedings of the 12th International Conference on Knowledge Discovery and Data Mining, 2006,pp. 217–226.[27] S. Kulkarni, A. Singh, G. Ramakrishnan, S. Chakrabarti, Collective annotation of Wikipedia entities in web text, in: Proceedings of the 15th InternationalConference on Knowledge Discovery and Data Mining, 2009, pp. 457–466.[28] J. Lehmann, S. Monahan, L. Nezda, A. Jung, Y. Shi, LCC approaches to knowledge base population at TAC 2010, in: Proceedings of the Text AnalysisConference, 2010.[29] G.S. Mann, D. Yarowsky, Unsupervised personal name disambiguation, in: Proceedings of the 7th Conference on Computational Natural LanguageLearning, 2003, pp. 33–40.[30] A. McCallum, K. Nigam, L.H. Ungar, Efficient clustering of high-dimensional data sets with application to reference matching, in: Proceedings of the 6thInternational Conference on Knowledge Discovery and Data Mining, 2000, pp. 169–178.[31] P. McNamee, H.T. Dang, H. Simpson, P. Schone, S.M. Strassel, An evaluation of technologies for knowledge base population, in: Proceedings of the 7thInternational Conference on Language Resources and Evaluation, 2010, pp. 369–372.[32] R. Mihalcea, A. Csomai, Wikify!: Linking documents to encyclopedic knowledge, in: Proceedings of the 16th Conference on Information and KnowledgeManagement, 2007, pp. 233–242.[33] D. Milne, I.H. Witten, Learning to link with Wikipedia, in: Proceedings of the 17th Conference on Information and Knowledge Management, 2008,pp. 509–518.[34] M. Milosavljevic, J.Y. Delort, B. Hachey, B. Arunasalam, W. Radford, J.R. Curran, Automating financial surveillance, in: User Centric Media, in: Lec-ture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering, vol. 40, Springer, Berlin/Heidelberg, 2010,pp. 305–311.[35] A.A. Morgan, Z. Lu, X. Wang, A.M. Cohen, J. Fluck, P. Ruch, A. Divoli, K. Fundel, R. Leaman, J. Hakenberg, C. Sun, H. Liu, R. Torres, M. Krauthammer,W.W. Lau, H. Liu, C. Hsu, M. Schuemie, K.B. Cohen, L. Hirschman, Overview of BioCreative II gene normalization, Genome Biology 9 (2008) S3.[36] R. Navigli, Word sense disambiguation: A survey, ACM Computing Surveys 41 (2009) 10:1–10:69.[37] R. Navigli, S.P. Ponzetto, Babelnet: Building a very large multilingual semantic network, in: Proceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, 2010, pp. 216–225.[38] NIST, The ACE 2005 (ACE05) evaluation plan, http://www.itl.nist.gov/iad/mig/tests/ace/2005/doc/ace05-evalplan.v3.pdf, 2005.[39] C. Niu, W. Li, R.K. Srihari, Weakly supervised learning for cross-document person name disambiguation supported by information extraction, in:Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, 2004, pp. 597–604.[40] J. Nothman, T. Murphy, J.R. Curran, Analysing Wikipedia and gold-standard corpora for NER training, in: Proceedings of the 12th Conference of theEuropean Chapter of the Association for Computational Linguistics, 2009, pp. 612–620.[41] D. Ploch, Exploring entity relations for named entity disambiguation, in: Proceedings of the ACL Student Session, 2011, pp. 18–23.[42] S.P. Ponzetto, M. Strube, Taxonomy induction based on a collaboratively built knowledge repository, Artificial Intelligence 175 (2011) 1737–1756.[43] W. Radford, B. Hachey, J. Nothman, M. Honnibal, J.R. Curran, CMCRC at TAC10: Document-level entity linking with graph-based reranking, in: Proceed-ings of the Text Analysis Conference, 2010.[44] L. Ratinov, D. Roth, D. Downey, M. Anderson, Local and global algorithms for disambiguation to Wikipedia, in: Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics, 2011, pp. 1375–1384.[45] C. Sauper, R. Barzilay, Automatically generating Wikipedia articles: A structure-aware approach, in: Proceedings of the Joint 47th Annual Meeting ofthe Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing, 2009, pp. 208–216.[46] F.M. Suchanek, G. Kasneci, G. Weikum, Yago: A large ontology from Wikipedia and WordNet, Journal of Web Semantics 6 (2008) 203–217.[47] E.F. Tjong Kim Sang, F. De Meulder, Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition, in: Proceedings ofthe 7th Conference on Natural Language Learning, 2003, pp. 142–147.[48] V. Varma, P. Bysani, K. Reddy, V. Bharat, Santosh G.S.K., K. Kumar, S. Kovelamudi, Kiran Kumar N., N. Maganti, IIIT Hyderabad at TAC 2009, in: Proceed-ings of the Text Analysis Conference, 2009.[49] W.E. Winkler, Overview of record linkage and current research directions, Technical Report, Bureau of the Census, 2006.[50] K. Woodsend, M. Lapata, Learning to simplify sentences with quasi-synchronous grammar and integer programming, in: Proceedings of the Conferenceon Empirical Methods in Natural Language Processing, 2011, pp. 409–420.[51] F. Wu, D.S. Weld, Autonomously semantifying Wikipedia, in: Proceedings of the 16th Conference on Information and Knowledge Management, 2007,pp. 41–50.[52] W. Zhang, C.S. Sim, J. Su, C.L. Tan, Entity linking with effective acronym expansion, instance selection and topic modelling, in: Proceedings of the 22ndInternational Joint Conference on Artificial Intelligence, 2011, pp. 1909–1914.[53] W. Zhang, J. Su, C.L. Tan, W.T. Wang, Entity linking leveraging automatically generated annotation, in: Proceedings of the 23rd International Conferenceon Computational Linguistics, 2010, pp. 1290–1298.[54] Z. Zheng, F. Li, M. Huang, X. Zhu, Learning to link entities with knowledge base, in: Proceedings of the 11th Annual Conference of the North AmericanChapter of the Association for Computational Linguistics, 2010, pp. 483–491.