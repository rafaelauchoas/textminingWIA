Artificial Intelligence 172 (2008) 1045–1063www.elsevier.com/locate/artintActive logic semantics for a single agent in a static worldMichael L. Anderson a,d,∗, Walid Gomaa b,e, John Grant b,c, Don Perlis a,ba Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USAb Department of Computer Science, University of Maryland, College Park, MD 20742, USAc Department of Mathematics, Towson University, Towson, MD 21252, USAd Department of Psychology, Franklin & Marshall College, Lancaster, PA 17604, USAe Department of Computer and Systems Engineering, Alexandria University, Alexandria, EgyptReceived 7 December 2006; received in revised form 14 November 2007; accepted 16 November 2007Available online 22 November 2007AbstractFor some time we have been developing, and have had significant practical success with, a time-sensitive, contradiction-tolerantlogical reasoning engine called the active logic machine (ALMA). The current paper details a semantics for a general version of theunderlying logical formalism, active logic. Central to active logic are special rules controlling the inheritance of beliefs in general(and of beliefs about the current time in particular), very tight controls on what can be derived from direct contradictions (P &¬P ),and mechanisms allowing an agent to represent and reason about its own beliefs and past reasoning. Furthermore, inspired by thenotion that until an agent notices that a set of beliefs is contradictory, that set seems consistent (and the agent therefore reasonswith it as if it were consistent), we introduce an “apperception function” that represents an agent’s limited awareness of its ownbeliefs, and serves to modify inconsistent belief sets so as to yield consistent sets. Using these ideas, we introduce a new definitionof logical consequence in the context of active logic, as well as a new definition of soundness such that, when reasoning withconsistent premises, all classically sound rules remain sound in our new sense. However, not everything that is classically soundremains sound in our sense, for by classical definitions, all rules with contradictory premises are vacuously sound, whereas in activelogic not everything follows from a contradiction.© 2007 Elsevier B.V. All rights reserved.Keywords: Logic; Active logic; Nonmonotonic logic; Paraconsistent logic; Semantics; Soundness; Brittleness; Autonomous agents; Time1. IntroductionReal agents have some important characteristics that we need to take into account when thinking about how theymight actually reason logically: (a) their reasoning takes time, meaning that agents always have only a limited, evolv-ing awareness of the consequences of their own beliefs,1 and (b) their knowledge is imperfect, meaning that someof their beliefs will need to be modified or retracted, and they will inevitably face direct contradictions and other in-* Corresponding author at: Department of Psychology, Franklin & Marshall College, P.O. Box 3003, Lancaster, PA 17604-3003, USA.E-mail addresses: michael.anderson@fandm.edu (M.L. Anderson), wgomaa@alex.edu.eg (W. Gomaa), jgrant@towson.edu (J. Grant),perlis@cs.umd.edu (D. Perlis).1 Levesque’s distinction between explicit and implicit beliefs [29] points to this same issue; however, our approach is precisely to model theevolving awareness itself, rather than trying to model the full set of (implicit) consequences of a given belief set.0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.11.0051046M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063consistencies. Indeed, real agents will not only often find their beliefs contradicted by experience, but will sometimesfind that their beliefs have been internally inconsistent for some time, although they are only now in a position tonotice this inconsistency, having derived a certain set of consequences that makes it apparent. The challenge from thestandpoint of classical logical formalisms is that, if an agent’s knowledge base can be inconsistent, then according toclassical logic, it is permissible to derive any formula from it.This fact about classical logics is commonly known by the Latin phrase ex contradictione quodlibet: from a con-tradiction everything follows. However, Graham Priest has coined the somewhat more vivid term explosive logics: alogic is explosive iff for all formulas A and B, (A&¬A) |= B. Priest defines a paraconsistent logic precisely as onewhich is not explosive [40–42]. Now, clearly real agents cannot tolerate the promiscuity of belief resulting from ex-plosive logics, and must somehow maintain control over their reasoning, watching for and dealing with contradictionsas they arise. The reasoning of real agents, that is, must be paraconsistent. But what sort of paraconsistent logic mightagents usefully employ, what methods might agents use to control inference and deal with contradictions, and howcan these logics (and methods) be modeled in terms of truth and consequence in structures?In the current paper we are primarily interested in the last of these questions. For some time we have been devel-oping, and have had significant practical success with a time-sensitive, contradiction-tolerant logical reasoning enginecalled the active logic machine (ALMA) [46]. Because ALMA was designed with the above challenges in mind,its underlying formalism, active logic [17,18,33,34], includes special rules controlling the inheritance of beliefs ingeneral (and of beliefs about the current time in particular), very tight controls on what can be derived from directcontradictions (P &¬P ), and mechanisms allowing an agent to represent and reason about its own beliefs and pastreasoning.Here we offer a semantics for a general version of active logic. We hope and expect it will be of interest as a specificmodel of formal reasoning for real-world agents that have to face both the relentlessness of time, and the inevitabilityof contradictions.In Sections 2–6 we will introduce the formal semantics for active logic, discuss a new definition of the conse-quence relation, and give examples of sound and unsound active logic inferences. This will be followed by some moreinformal discussion of the various properties of active logic (Section 7), a comparison of active logic with relatedapproaches (Section 8), and a discussion of the practical issues involved with the use of active logic in real-worldagents (Sections 9 and 10).2. A semantics for real-world reasoningIn this section we propose a semantics for a time-sensitive, contradiction-tolerant reasoning formalism, incorporat-ing the basic features of active logic.2.1. Starting assumptionsIn order to make the problem tractable for our first specification of the semantics, we will work under the followingassumptions concerning the agent, the world (i.e., everything apart from the agent), and their interactions:• There is only one agent a.• The agent starts its life at time t = 0 (t ∈ N) and runs indefinitely.• The world is stationary for t (cid:2) 0. Thus, changes occur only in the beliefs of the agent a.Given these assumptions, there is one and only one true complete theory of the world; however, given that theagent’s beliefs evolve over time, there is a different true complete theory of the agent for each time t.2.2. The language LIn order to express theories about such an agent-and-world, we define a sorted first-order language L. We defineit in two parts: the language Lw, a propositional language in which will be expressed facts about the world, and thelanguage La, a first-order language used to express facts about the agent, including the agent’s beliefs, for instance thatthe agent’s time is now t, that the agent believes P , or that the agent discovered a contradiction in its beliefs at a givenM.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–10631047time. We write SnK to mean the set of sentences of any language K. We are using the complete set of connectives{¬, →} from which other connectives, such as ∧, and ∨, can be derived. We assume that double negations are removedfrom formulas. For the sentence symbols the subscripts are used to indicate different propositional sentences and, fora fixed subscript, the superscripts are used to indicate different apperceptions (see Section 4) of the agent of the sameproposition. The superscript 0 is used for the original sentence symbol (without superscript).Definition 1. Let Lw be a propositional language consisting of the following symbols:• a set S of sentence symbols (propositional or sentential variables) S = {Sji : i, j ∈ N} (N is the set of naturalnumbers)• the propositional connectives ¬ and →• left and right parentheses ( and )SnLw is the set of sentences of Lw formed in the usual way. These represent the propositional beliefs of the agent1 might mean “John is happy”. For later use we assume there is a fixed lexicographicabout the world. For instance S0ordering for the sentences in SnLw .Definition 2. Let σ, θ ∈ SnLw . We say that {σ, θ } is a direct contradiction if one of the following holds: either θ is theformula σ preceded by a negation, or σ is the formula θ preceded by a negation, that is θ = ¬σ or σ = ¬θ .Before giving the definition of the language La, we remark the following:i. In its current version La is a restricted form of first-order logic that is essentially propositional. In future work weintend to extend it to the full power of first-order logic.ii. La contains a belief predicate that captures the fact that the agent believed a certain proposition at some time t.We allow for sentences of the form: at time s the agent believed that she believed that she . . . . To allow for thisindefinite (however finite) nesting, the definition of La has to be inductive where at stage n + 1 all sentences fromthe previous levels are captured for the belief predicate.Definition 3. The language La is a sorted restricted version of first-order logic having three sorts:• S1 is used to represent SnLw .• S2 is used to represent time.• S3 is used to represent SnL at its various stages of construction as shown below.La will be defined as the union of a sequence of languages {Ln}n∈N which are defined as follows:• n = 0: L0 is a restricted first-order sorted language that does not contain variables or quantifiers, and consists ofthe following symbols:– the propositional connective ¬– a set of constant symbols C = {i: i ∈ N} of sort S2 (this represents the time indices)– a set of constant symbols D = {σ : σ ∈ SnLw}, each is of sort S1 (here for simplicity, the constant symbols used}, each is of sort S3 (again for simplicity the sentences themselvesto represent SnLw are the sentences themselves)– a set of constant symbols E0 = {θ : θ ∈ SnLware used as constant symbols)– the unary predicate symbol Now of sort S2– the binary predicate symbol Contra of sort (S1 × S2).• n (cid:2) 1: Assume that Lm has already been defined for all 0 (cid:3) m < n. Ln is a restricted first-order sorted languagethat does not contain variables or quantifiers. In addition to the symbols of Ln−1, it contains a set of constantsymbols En = {θ : θ ∈ SnLn−1} of sort S3. Also, L1 (and hence all Li , 1 (cid:3) i) contains a binary predicate symbolBel of sort (S3 × S2).1048M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063(cid:2)(cid:2)Let E =i∈N Ei . Let the language La =All of these constant symbols are terms in the language La.Ln so SnLan∈N(cid:2)=n∈N SnLn . C, D, E are sets of constant symbols.The sort S2 stands for time. In La, Now is used to express the agent’s time, Contra is used to indicate the existenceof a direct contradiction in its beliefs, and Bel expresses the fact that the agent had a particular belief at a giventime. We use these predicate symbols because they are crucial for active logic. The semantics for these predicateswill be defined formally in Definition 8. Note that La contains only the connective ¬; hence statements such asBel(θ, t) → Bel(θ, t + 1) are not in the language.However, we do not specify one specific set of active logic rules. This means that the semantics we will specify isapplicable to a class of active logics with different rules that share a few common features.Definition 4. Let L = La ∪ Lw, in the sense that SnL = SnLa∪ SnLw .Definition 5. Let the agent’s knowledge base at time t, KBt , be a finite set of sentences from L, that is, KBt ⊆ SnL. Inthe case of KB0 the only formulas of SnLw we allow are those whose superscripts are all 0.We can imagine KB0 containing any sentences from L with which a system designer would equip an agent. KB0may or may not be consistent (no system designer is perfect!). After time 0, each KBt can be different from KB0because of inference, observation, forgetting, and the like. Also, although all the sentences about the world initiallyhave superscript 0 for the sentence symbols, as we will see later the agent may assign different superscripts to thesentence symbols thereby changing a possibly inconsistent set to one that is consistent.2.3. The semantics of LwIn the following several definitions, we define the semantics of the formalism given above, in the standard way.Definition 6. An Lw-truth assignment is a function h : S → {T , F } defined over the set S of sentence symbols in Lw.Definition 7. An Lw interpretation h (we keep the same notation for this induced interpretation) is a functionh : SnLw→ {T , F } over SnLw that extends an Lw-truth assignment h as follows:h(¬ϕ) = T ⇐⇒ h(ϕ) = Fh(ϕ → ψ) = F ⇐⇒ (h(ϕ) = T and h(ψ) = F )We also stipulate a standard definition of consistency for Lw: a set of Lw sentences is consistent iff there is someinterpretation h in which all the sentences are true. Notationally we write the usual h |= Σ, to mean all the sentencesof Σ are assigned T by h.We call Wt the set of all L-expressible facts about the external world. Thus, Wt is consistent at every time t.This means that for every t there exists an Lw-truth assignment function ht , such that ht |= Wt . We also call theinduced interpretation ht , keeping the same notation. This result does not depend upon the assumption that the worldis stationary. In a stationary world a single ht will work for all t; in a changing world there may be a different ht , butthere will still be some ht , for each t. For a brief discussion of how we intend to approach the issue of a changingworld in future work, see Section 11.This does not mean that the agent’s beliefs about the world are all true, consistent and constant (indeed, we expectthey will contain contradictions and change over time), only that there is some set of true and consistent sentencesthat describe the world for every t. We’ll detail later how to interpret and model the agent’s world-knowledge (thisbeing the crux of the issue when dealing with inconsistency). First, however, we turn to a model of the agent’s meta-knowledge.3. A model of the agent’s La beliefsFirst of all it is important to note that, even in the case where the agent’s beliefs are incomplete, incorrect, orinconsistent, there is always a complete and consistent theory of those beliefs at the meta level, and this theory can beM.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–10631049expressed using the language La. For instance, if the agent believes both that John is happy (S0happy (¬S0not happy” (Bel(¬S01 ), the two sentences “the agent believes that John is happy” (Bel(S01 )) can both be true at the same time.1 ) and that John is not1 )) and “the agent believes that John isNow we define an interpretation that models the theory about the agent at the meta level. In what follows, Σ is tobe understood formally as any set of sentences from L; typically we will assume it to be some subset of the agent’sknowledge base, combining beliefs about the world and about the agent, at some time t. Our point of view is thatat any time t the agent may deduce new beliefs from its knowledge base at time t − 1, may add new sentences, forexample from observations, or delete some sentences.The following definition consists of ten bullet points: the first identifies the domain, the next three provide theinterpretations for the three sorts; the following three provide the interpretations for all the constant symbols; the lastthree provide the interpretations for the three predicate symbols. Now keeps track of the time, and indicates the currenttime of the agent’s internal clock. Contra indicates the existence of a direct contradiction in Σ at some time s (cid:3) t. Belhas the rough meaning “believes that”, and states that a given sentence from L was in the agent’s KB at some times (cid:3) t. We define the interpretation H Σt+1 (at time t + 1 based on Σ) modeling facts about the agent as follows.Definition 8. H Σt+1 is defined as the following interpretation:t+1) = N ∪ SnL.t+1(S1) = SnLw (the set of propositions about the world).t+1(S2) = N (all non-negative integers).t+1(S3) = SnL (the set of sentences representing the agent’s knowledge base).• Domain(H Σ• H Σ• H Σ• H Σ• ∀n ∈ C: H Σ• ∀σ ∈ D: H Σ• ∀θ ∈ E: H Σ• The predicate symbol Now has the following semantics: H Σt+1t+1(n) = n.t+1(σ ) = σ .t+1(θ ) = θ .otherwise H Σt+1|= ¬Now(s).|= Now(s) ⇐⇒ s = t + 1 and Now(t) ∈ Σ;• The predicate symbol Contra has the following semantics: H Σt+1|= ¬Contra(σ, s).|= Bel(θ, s) ⇐⇒ either s < t and Bel(θ, s) ∈ Σ orContra(σ, s) ∈ Σ or s = t and ∃σ, ¬σ ∈ Σ; otherwise H Σt+1• The predicate symbol Bel has the following semantics: H Σt+1|= Contra(σ, s) ⇐⇒ either s < t ands = t and θ ∈ Σ; otherwise H Σt+1|= ¬Bel(θ, s).4. A model of the agent’s Lw beliefsNow we turn to the challenging problem of how to model, at the object level, the agent’s beliefs about the world,given that these beliefs are not just evolving from moment to moment, but that at any given time, they may beinconsistent. Our scenario is as follows. At time 0 the agent has a finite set of initial beliefs, KB0, about the world. Allthe sentence symbols have superscript 0. Then the agent starts to reason about the world using rules of active logic.This is where the agent may assign, via its apperception function, non-zero superscripts to some sentence symbols toavoid inconsistency. The agent may also obtain additional information about the world over time through other means,such as observations.We will tackle this problem initially in three steps. First, we define a weak notion of consistency allowing forinconsistency in the agent’s knowledge about the world; second, we will define a class of “apperception functions”intended to capture the intuition that an inconsistent KB will not necessarily seem inconsistent to the agent; and finally,we will show that there is some apperception function that, when applied to a given set of sentences, always producesa consistent set. Having shown this, we will proceed in the following sections to define a viable notion of activeconsequence, discuss the relation of this notion of consequence to the classical notion of logical consequence, andprove the soundness of some of the central inference rules of active logic.Recalling that Σ need not be consistent concerning facts about the world we define a weak version of consistency.Definition 9. A set of sentences Σ ⊆ SnL is said to be La consistent iff Σ ∩ SnLa is classically consistent.1050M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063Remark 1. From now on, we will assume that Σ is La consistent. We also introduce the symbol Γ to refer to thepotentially inconsistent set of Lw sentences in Σ: Γ = Σ ∩ SnLw .We next define an apperception (self-awareness) function for the agent. The notion of an apperception functionis intended to help capture, at least roughly, how the world might seem to an agent with a given belief set Σ. Fora real agent, only some logical consequences are believed at any given time, since it cannot manage to infer all thepotentially infinitely many consequences in a finite time, let alone in the present moment. Moreover, even if the agenthas contradictory beliefs, the agent still has a view of the world, and there will be limits on what the agent will andwon’t infer. This is in sharp distinction to the classical notion of a model, where (i) inconsistent beliefs are ruled outof bounds, since then there are no models, and (ii) all logical consequences of the KB are true in all models.i and ¬S0The task we are addressing, then, is that of finding a notion of semantics that avoids both (i) and (ii) above. Ouridea—via apperception functions—is to suppose that an agent’s limited resources apply also to its ability to inspectits own knowledge. Thus, if S0i are both in Σ, the agent might not realize, at first, that the two instances ofSi are in fact instances of the same sentence symbol. Thus it might seem to the agent that the world is one in which,say, S1i . This allows the agent to have inconsistent beliefs while still having a consistent worldmodel. Moreover, it allows us to see how an agent with inconsistent beliefs could avoid vacuously concluding anyproposition, and also reason in a directed way, by applying inference rules only to an appropriately apperceived subsetof its beliefs. We hope that this approach can shed some light on focused, step-wise, resource-bounded reasoning moregenerally.i is true, and so is ¬S21 ), believes that John is from the midwest (S0An example of issue (i) might be Fred, who believes that if John is from the midwest then John is unhappy→ ¬S0(S01 ). We represent the2world view of such an agent by supposing that at least one of these beliefs is taken to have a different apparentmeaning, one that is not inconsistent with the others (e.g. S01 ). This might happen because Fred hasn’t thought2carefully about all his beliefs, nor realized all of their consequences, and so never noticed that his beliefs entail both1 and ¬S0S01 , from the apperceived versionof the implication) he will recognize the contradiction at that time, and remove it (see below).1 . Note, however, that in our model, should Fred ever conclude ¬S02 ), and believes that John is happy (S01 (or ¬S1→ ¬S1An example of issue (ii), although one currently beyond what our formalism can represent, might be Andrew Wilesworking on a proof of Fermat’s Last Theorem (FLT). He did not know, until he had completed his proof, that FLT wastrue. Yet he did have among his beliefs sufficient axioms to guarantee FLT as a consequence. So how did the worldseem to him? Along the lines we are suggesting, he viewed some sentences as having possible interpretations differentfrom what he later discovered to be the case. In effect, apperception functions, collectively, allow for a blurring of theidentities, and hence meanings, of symbols.The apperception functions we define can make changes only to Γ . An apperception function does not changeΣ − Γ . We use the same notation ap when the apperception function is applied to an occurrence of a sentence symbol,a sentence, or a set of sentences. We start by defining a function that changes the superscripts of sentence symbols to0. This is used to recover the original direct contradictions that were modified by the assignment of superscripts.Definition 10. For any sentence φ ∈ SnLw , let z(φ) be the sentence φ with all superscripts reset to 0. If Σ ⊆ SnLw ,then z(Σ) = {z(φ)|φ ∈ Σ}.Definition 11. An apperception ap is a function ap : Σ → Σ (cid:14) where Σ and Σ (cid:14) are sets of L-sentences. An ap isrepresented as a finite sequence of non-negative integers: (cid:15)n1, . . . , np(cid:16). The effect of ap on Σ is as follows:1. Let Σ be a set of L-sentences and let Γ = Σ ∩ Lw. Using the lexicographic order given earlier, let the kthi . The effect of the ap = (cid:15)n1, . . . , np(cid:16) is to change Sjif 1 (cid:3) k (cid:3) p, otherwiseto Snkiisentence symbol in Γ be SjSji is unchanged.2. ap(Σ) = (Σ − Γ ) ∪ ap(Γ ) (ap does not change Σ − Γ ).Example 2. Let Σ = {Now(5), Bel(S02 , 4), ¬S1elements lexicographically yields ord(Γ ) = {S1{Now(5), Bel(S02 , ¬S32 , 4), S1→ S1652 , S21}.2 , S02 , S112 , ¬S12 , S01→ S45→ S45}. In this case Γ = {¬S1}. Writing the2 , S01}. Consider ap = (cid:15)1, 3, 2, 16, 7(cid:16). Then ap(Σ) =→ S452 , S1M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–10631051Infinitely many apperception functions are needed because a finite set of sentences in Lw may have an arbitrarilylarge (finite) number of sentence symbols. However, if Γ is known to contain p occurrences of sentence symbols,then it suffices to deal only with apperception functions that are sequences of up to p integers as the integers in thelater locations are not applied. There are only finitely many such apperception functions.The purpose of the apperception functions is to get rid of inconsistencies in Σ. Hence we are interested only inapperception functions that output consistent sets. The set of apperception functions that do this depends on Σ.Definition 12. Let AP denote the class of all apperception functions. APΣ = {ap ∈ AP|ap(Σ) is consistent}.Next we show that APΣ is never empty.Theorem 1. For all Σ, APΣ (cid:17)= ∅.Proof. Let ap assign a unique superscript to each occurrence of every sentence symbol in Γ . Then no sentence symbolappearing in ap(Γ ) is duplicated, hence each can be assigned a truth value independently. So ap(Γ ) is consistent.Since the remaining sentences in Σ are consistent by assumption, and are in La, ap ∈ APΣ . (cid:2)5. Active consequence5.1. The definition of active consequenceAt this point we are ready to define the notion of active consequence at time t—the active logic equivalent oflogical consequence. We start by defining the concept of 1-step active consequence as a relationship between sets ofsentences Σ and Θ of L, where Σ ⊆ KBt and Θ is a potential subset of KBt+1. When we define this notion we wantto make sure that Θ contains only sentences required by Σ and the definition of H Σt+1. This is the reason for the nextdefinition.Definition 13. Given Σ and ap ∈ APΣ , definedcs(Γ ) =(cid:4)(cid:3)φ ∈ Γ |∃ψ ∈ Γ such that z(φ) = ¬z(ψ) or ¬z(φ) = z(ψ),apz(Γ ) = ap(Γ ) − dcs(Γ ).The meaning of Definition 13 is that we are removing direct contradictions from ap(Γ ) while ignoring the super-scripts.Definition 14. Let Σ, Θ ⊆ SnL. Then Θ is said to be a 1-step active consequence of Σ at time t, written Σ |=1 Θ ifand only if ∃ap ∈ APΣ such thati. if σ ∈ Θ ∩ SnLw then apz(Γ ) |= σ (σ is a classical logical consequence of apz(Γ )), andii. if σ ∈ Θ ∩ SnLa then H (Σ−Γ )∪z(Γ )|= σ .t+1In this definition, for the sentences of Θ in the agent’s language (at the meta level) 1-step active consequencedepends on the interpretation Ht+1. Instead of Γ , we include z(Γ ) to capture all direct contradictions even if thesuperscripts have been changed. This also means that the Bel and Contra statements will contain sentence symbolsonly with superscript 0. For all the sentences of Θ expressing facts about the world, there must be some apperceptionfunction such that the apperception of Σ (the Lw part) minus the direct contradictions classically implies these sen-tences. In the following we define the more general case of n-active consequence for any positive integer n (similarly,as a result of this definition Θ is a potential part of KBt+n).1052M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063Definition 15.i. Let Σ, Θ ⊆ SnL. Then Θ is said to be an n-step active consequence of Σ at time t, written Σ |=n Θ, if and onlyif∃(cid:10) ⊆ SnL: Σ |=n−1 (cid:10) and (cid:10) |=1 Θ(5.1)ii. We say that Θ is an active consequence of Σ, written Σ |=a Θ, if and only if Σ |=n Θ for some positive integer n.Next we give some examples to illustrate the concept of active consequence.Example 3.1 , ¬S01i. Let Σ = {S0} and Θ = {Contra(S0→ S0ii. Let Σ = {Now(t), S01 , S01easy to see that {S0} are logical consequences of {S04 , S012Hence Σ |=1 Θ.1 , t)}. Then Σ |=1 Θ.} and Θ = {Now(t + 1), S04 , S0121 , S014 , S012→ S04 , S012}. Let ap ∈ APΣ be the identity function. It is|= Now(t + 1).}. Also by definition H Σt+1iii. Let Σ, Θ be as in the previous example with Bel(S05 /∈ Σ for any i, Ht+1 (cid:17)|= Bel(S05 , t),hence Σ (cid:17)|=1 Θ. Therefore, for any later time t + k and (cid:10) obtained by active consequence from Σ, H (cid:10)(cid:17)|=t+kBel(S5, t), so Σ (cid:17)|=a Θ.5 , t) added to Θ. Since Siiv. Let Σ = {Now(t)} and Θ = {Now(t + 5)}. Then H Σt+1(cid:17)|= Θ. However, H Σt+1|= Now(t + 1). So {Now(t)} |=1{Now(t + 1)}, and we get {Now(t)} |=5 {Now(t + 5)}, so {Now(t)} |=a {Now(t + 5)}. Hence Σ |=a Θ.1 , S0v. Let Σ = {S0→ ¬S01 , t + 1)}. We will see that Σ |=2 Θ. Let (cid:10) = {S11Σ |=1 (cid:10), through the apperception function ap(Σ) = {S1→ ¬S21the definition, regardless of the apperception function applied in this step.}. Then}. Then (cid:10) |=1 Θ by the second part of} and Θ = {Contra(S01 , ¬S212 , S022 , S221 , S2Note that in Example 3.v, it is not the case that Σ |=1 {Contra(S01 , t)} even though the conditions for the laterappearance of the relevant direct contradiction were already in place at time t. This underlines the fact that in activelogic it can take time for consequences to appear in the KB. In the case of La sentences, this temporal aspect of thelogic is regulated and enforced directly by the semantics. For Lw sentences, it is an artifact of the particular set ofrules that a given active logic agent is equipped with (see Sections 9 and 10 for more discussion of this issue).Thus, for instance, considering the types of rules in active logic, given a rule like: t: α,α→ϕ,ϕ→ψan agent couldinfer ψ in one step from the formulas given at time t; however an agent equipped only with a simple version of modusponens, such as that given in Definition 22 (see Section 6.1) would take two time steps to conclude ψ from the sameformulas. Both rules would be sound in active logic (see Definition 16), but a given agent might not be equipped withboth rules (see Section 10). Since our definition of 1-step active consequence for sentences in Lw is based on logicalimplication, it is at least as powerful any set of sound syntactical rules could be.t+1:ψ5.2. The relationship between active consequence and 1-step active consequenceBy our definition of active consequence, Σ |=1 Θ implies Σ |=a Θ. We may wonder how much bigger Θ may be1 , t), t + 1)}.in the latter case. Consider first a very simple situation: Σ = {S01Here we have Σ |=1 Θ and Θ |=1 Θ (cid:14), hence Σ |=2 Θ (cid:14). This illustrates that considering La there can be additionalsentences for each n-step active consequence for each new value of n. We show that this not does not happen forsentences of Lw.1 , t)} and Θ (cid:14) = {Bel(Bel(S0}, Θ = {Bel(S0Theorem 2. Suppose Σ, Θ ⊆ SnLw . Then Σ |=1 Θ ⇔ Σ |=a Θ.Proof. Since both Σ and Θ are sentences in Lw, it suffices to deal only with sentences in Lw. The ⇒ part followsfrom the definition of |=a.Going in the other direction assume that Σ |=a Θ. By Definition 15 there must be a positive integer n such thatΣ |=n Θ, and that means that there is a (cid:10) ⊂ SnLw such that Σ |=n−1 (cid:10) and (cid:10) |=1 Θ. We divide the proof into twocases depending on the consistency of Σ.M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–10631053Suppose that Σ is consistent. Consider what can happen in n − 1 steps, that is, Σ |=n−1 (cid:10) where φ ∈ (cid:10). Such a φmust have been obtained by n − 1 applications of classical logical implication to Σ except that we may also changesentence symbol superscripts through n − 1 apperception functions, one at each step. The key observation here isthat both the application of classical logical implication and the application of apperception functions are transitiveoperations. This means that whatever can be obtained by n − 1 applications of logical implication can already beobtained by a single application of logical implication, and the same goes for apperception functions. Hence Σ |=1 (cid:10).Doing this process again, but using (cid:10) |=1 Θ, we obtain Σ |=1 Θ.Suppose next that Σ is not consistent. Then in the first step of the implication, that is, to get Σ |=1 (cid:10), an appercep-tion function, ap, must have been applied to Σ first, making ap(Σ) consistent (and removing direct contradictions),only then is the 1-step active consequence determined. Thus Σ |=1 (cid:10) iff ap(Σ) |=1 (cid:10) for some ap ∈ APΣ , whereap(Σ) is consistent. But then we are back at the previous case where Σ was consistent (where now ap(Σ) is consis-tent) and the result follows. (cid:2)Although we proved this result only for sentences in Lw, the same proof works (restricted to sentences of Lw) evenif Σ and Θ contain sentences in La.5.3. The relationship between classical logical consequence and active consequenceHow does classical logical consequence compare to active logic consequence? For sentences in SnLa the two areincomparable. For consider Σ = {Now(t)}. Clearly, Σ |= Σ, but Σ (cid:17)|=a Σ because Now(t) will not be true at any timeafter t. Next consider Θ = {Bel(Now(t), t)}. Then Σ (cid:17)|= Θ but Σ |=a Θ.So for the comparison we restrict our attention to SnLw . In classical logic an inconsistent set of sentences logicallyimplies every sentence, but that is not the case for active consequence. The interesting question is what happens ifΣ ⊆ SnLw is consistent. It seems reasonable to expect active consequence to behave just like logical consequence.Recalling our theorem from the previous subsection, it suffices to compare only |= and |=1 because in this case |=1and |=a give the same result.Thus in the consistent case we might expect Σ |= Θ ⇔ Σ |=a Θ. The first implication, Σ |= Θ ⇒ Σ |=a Θ,holds because we can choose the apperception function to be the identity function. Intuitively the opposite implicationshould hold as well. For consider that every given set of consistent sentences has a certain definite set of conclusions(consequences)—call this the “inferential power” of the set. We would expect this same set in active logic to haveno more inferential power than it has under classical logical consequence. For consider an apperception function that}.assigns a different number to every sentence symbol in Σ = {S0Now the sentence symbol S2 can no longer be inferred for any superscript. But this also presents a problem forthe reverse implication. For Σ |=a Θ holds but Σ |= Θ does not. The equivalence holds, however, if we restrict allsentence symbols to have superscript 0.}, e.g., turns it into Θ = {S1→ S32→ S021 , S011 , S21Theorem 3. Let Σ, Θ ⊆ SnLw . If Σ is consistent, Σ = z(Σ), and Θ = z(Θ), then Σ |= Θ ⇔ Σ |=a Θ.Proof. By Theorem 2 it suffices to prove that Σ |= Θ ⇔ Σ |=1 Θ. It follows from Σ = z(Σ) and Θ = z(Θ) thatall superscripts of sentences must be 0. In the application of the definition of 1-step consequence, an apperceptionfunction must be used. Since the apperception function leaves all superscripts at 0, it must be the identity function, so1-step active consequence is identical to logical consequence, that is, Σ |= Θ ⇔ Σ |=1 Θ. (cid:2)In Section 2.2 we stated that our semantics does not presuppose any one specific set of active logic rules becauseit is applicable to many different active logic systems with different rules. This means that we cannot expect to obtainthe kind of completeness theorem for this semantics that one might get for a single specific set of rules. However, it isclear that 1-step active consequence is very powerful for consistent sets of sentences. It encompasses any set of activelogic rules for SnLw . In that sense it is the limiting case for all possible sets of such active logic rules and providesan approximation to a completeness result. In the following, we write (cid:21) for derivability in active logic, instead of thevertical notation commonly used there. See the next section for the standard vertical notation.Theorem 4. Suppose that Σ, Θ ⊆ SnLw , Σ = z(Σ), Θ = z(Θ), Σ is consistent and Σ and Θ are finite.1054M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063(a) Let (cid:21) represent the derivability relation for any active logic. If Σ (cid:21) Θ then Σ |=a Θ.(b) If Σ |=a Θ then there is an active logic with derivability relation (cid:21) such that Σ (cid:21) Θ.Proof. (a) If Σ (cid:21) Θ then every φ ∈ Θ must logically follow from Σ, hence Σ |=1 Θ, so Σ |=a Θ.(b) If Σ |=a Θ then for each φ ∈ Θ introduce a (valid) active logic rule stating that Σ (cid:21) φ. For the active logicdefined by these rules (for SnLw ), Σ (cid:21) Θ. (cid:2)6. Sound and unsound inferences in active logicAt this point we consider possible inference rules for active logic. We start with some notes about the syntax ofactive logic rules. Because active logic is a step logic, we always precede both the antecedent and the consequent(which are divided by a horizontal line) with an indication of the time, thus:t: antecedentt + 1: consequentThe antecedent can be any of the following:• a single formula, e.g. θ , or Now(t)• any set of formulas separated by commas, e.g. θ, θ → σ• any set of formulas meeting some specified conditions, and represented by a single capital letter, with a semi-colonbetween the capital letter and the conditions e.g. Σ; θ ∈ Σ• any set of formulas representing the database of an agent at a specific time. This will be represented by KBt , andmay also specify conditions using the same convention as above.The consequent can be any of the following:• a single formula, e.g. θ , or Now(t)• any set of formulas separated by commas, e.g. θ, θ → σ .Now we define the notion of a-sound inference.Definition 16. An active sound (a-sound) inference is one in which the consequent is a 1-step active consequence ofthe antecedent.Recall that (1-step) active consequence is defined between sets of sentences. However, in accordance with thesyntax defined above, we will omit the set notation symbols { and }.6.1. Some active-sound inference rulesFor all six rules given here, a-soundness follows directly from the definitions. We prove the last as an illustration.Definition 17. If Now(t) ∈ KBt then the timing inference rule is defined as follows:t: Now(t)t + 1: Now(t + 1)Definition 18. If ϕ, ¬ϕ ∈ KBt , where ϕ ∈ SnLw , then the direct contradiction inference rule is defined as follows:t: ϕ, ¬ϕt + 1: Contra(ϕ, t)Definition 19. If ϕ ∈ KBt , where ϕ ∈ SnL, then the positive introspection inference rule is defined as follows:t: ϕt + 1: Bel(ϕ, t)M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–10631055Definition 20. If ϕ /∈ KBt , where ϕ ∈ SnL, then the negative introspection inference rule is defined as follows:t: KBt ; ϕ /∈ KBtt + 1: ¬Bel(ϕ, t)Definition 21. If ϕ ∈ SnL such that ϕ ∈ KBt , ¬ϕ /∈ KBt , ϕ (cid:17)= Now(t), and ϕ is not a contradiction, then the inheritanceinference rule is defined as follows:t: ϕt + 1: ϕDefinition 22. Let Θ = {ϕ, ϕ → ψ} ⊆ (KBt ∩ SnLw ) such that Θ is consistent. Assume ¬ϕ /∈ KBt and ¬(ϕ → ψ) /∈KBt (see Section 6.3 for more on this restriction), then the active modus ponens inference rule is defined as follows:t: ϕ, ϕ → ψt + 1: ψTheorem 5. The rules given in Definitions 17–22 are a-sound.For Definitions 17–20, their a-soundness follows from the definitions. By way of illustration, consider the followingfor active modus ponens (Definition 22):Proof. Use an apperception function which is the identity on Θ and assigns a unique different superscript to any othersymbol in KBt . (cid:2)6.2. Active-unsound inference rulesWe examine a number of instances of classically unsound inference rules, and get the expected intuitive results thatthese inferences are also active-unsound. In all cases ϕ and ψ are arbitrary sentences of L.Definition 23. We call this first rule the ϕ implies ψ, or ϕ → ψ rule.t: ϕt + 1: ψTheorem 6. The ϕ → ψ inference rule is not a-sound (is a-unsound).Proof. Let ϕ = S0would mean that ψ classically follows from ϕ, and that is false. (cid:2)1 and let ψ = ¬(S01→ S01 ). Then ψ is not an active consequence of ϕ, because by Theorem 3, thisDefinition 24. We call this next rule the ϕ implies not ϕ, or ϕ-not-ϕ rule: We assume that ϕ is a consistent formula.t: ϕt + 1: ¬ϕTheorem 7. The ϕ-not-ϕ inference rule is a-unsoundProof. Let ϕ = S01 and apply Theorem 3. (cid:2)Interestingly, although the ϕ-not-ϕ inference rule is a-unsound in general (with respect to the big language L),there is one special instance in which it is sound, namely:t: Now(t)t + 1: ¬Now(t)This further underlines the special status of time and the Now() predicate in active logic; this result would obviouslynot be classically sound.1056M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063However, one rule that is classically sound, but a-unsound, is the explosive rule. This shows that active logic is aparaconsistent logic, something we consider one of its advantages over classical formalisms.Definition 25. Let Σ ⊆ SnLw be inconsistent. Let ψ ∈ SnLw . We define the explosive rule with respect to the languageLw as follows.t: Σ; Inconsistent(Σ)t + 1: ψTheorem 8. The explosive inference rule is a-unsound.Proof. Let ψ be ¬(S01Hence ap(Σ) (cid:17)|=1 ψ. By Theorem 2 the result follows. (cid:2)→ S01 ). No apperception function ap that turns Σ into a consistent set can logically derive ψ.6.3. Inconsistent KBs, apperception functions and the application of a-sound rulesWe noted above that there can be no official catalog of rules for active logic; any a-sound rule can qualify, and agiven active logic agent may be equipped with any number of these rules (see Section 9 for more on this). However,the fact that a-soundness is defined in terms active consequence, which is itself defined in terms of apperceptionfunctions, means that not every a-sound rule will be available for use in every situation. More specifically, whetheror not there are direct contradictions in Σ = KBt , the apperception function may change which rules can and cannotbe applied for that Σ.2 (We list only Γ in the examples below.) Let Γ = {S0}. Because of the directcontradiction, the active modus ponens rule would not apply (S01 , ¬S01 and ¬S01 , S01→ S021 would be removed from the KB).1 , S012 , ¬S02→ S0Next, consider a case where active modus ponens does apply, namely, let Γ = {S0derive S0using an a-sound notational variant of the classically sound rule32 by using an ap that only changes the superscript of ¬S02 . A different possible consequence is {S02}. So we can3 },→ S0¬ψψ → αand using an ap that only changes the superscript of S01 .→ S02 , S02But note that the set {S03 } is not an active consequence of Γ because there is no single apperceptionfunction that would allow this set to be derived. Thus we cannot necessarily combine a-sound rules and guarantee thatthe result is an active consequence. (The problem exists only for rules involving Lw.) This also underlines the factonce again that apperception functions can limit the inferential power of a given set of sentences. For a discussion ofthe practical effects of this limitation, see Sections 9 and 10.This concludes the presentation of the active logic semantics. In the next two sections (7 and 8) we will discusssome of the general properties of active logic that follow from its semantics, and compare active logic to other relatedwork. After that, in Sections 9 and 10, we will discuss some of the practical issues involved with using active logic inreal-world reasoning agents.7. General properties of active logicOne of the original motivations for active logic was the need to design formalisms for reasoning about an approach-ing deadline; for this use it is crucial that the reasoning take into account the ongoing passage of time as that reasoningproceeds. Thus, active logic reasons one step at a time, updating its belief about the current time at each step, usingrules like the timing rule given in Definition 17.2 In fact, it is generally true of apperception functions that they will determine which rules are applicable in a given KB at a given time; however,in a consistent KB, there will always be an eligible apperception function that makes no alterations to the KB, thus not changing which rules apply.Thus, the remarks below are limited to the case of an inconsistent KB.3 While this rule can be written so as to be a-sound, it is rather a dangerous rule in a non-monotonic logic, and it would probably not be advisableto include it among the catalog of rules with which a practical active logic agent is equipped.M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–10631057This step-wise, time-aware approach gives active logic fine control over what it does, and does not, derive andinherit at each step; for instance, Now(t) is not inherited at time step t + 1. To “inherit” P is, roughly speaking,to assert P at time t + 1 just in case it was believed at time t. However, in a temporal, non-monotonic formalism,what is justified now may not be justified later. For a simple example, consider that a certain observation at time tmay justify the conclusion that it is raining at time t, and it may be reasonable to continue to believe this at timet + 1 (i.e. to inherit the belief). However, at some point in time, t + n, neither the original observation, nor theinherited belief can be considered justification for the continued belief that it is raining. Thus, although inheriting isa reasonable default behavior, there will be conditions and limits.4 This is accomplished by special inheritance ruleslike Definition 21. Note in particular the conditions governing that rule, conditions that can be tailored for differentagents and circumstances.Such step-wise control over inference gives active logic the ability to explicitly track the individual steps of adeduction. Thus, for instance, an inference rule can refer to the results of all inferences up until now—i.e. throughtime t—as it computes the subsequent results (for time t + 1). This allows an active logic to reason, for example,about its own (past) reasoning; and in particular about what it has not yet concluded. Moreover, this can be performedquickly, since it involves little more than a lookup of the current knowledge base (see, e.g. Definition 20). Although thecomplexity of this operation is low—O(n)—it is nevertheless the case that if the KB is allowed to grow indefinitely,the operation will take increasing time. Currently beliefs older than some arbitrary threshold are removed from activememory and written to a searchable log file. However, we are investigating various more intelligent methods forselective “forgetting”.This last point is worth further elaboration and emphasis, for it is central to the active logic approach to modeling thereasoning of real-world agents. The reason that determining what one does not know—otherwise known as negativeintrospection—is simple in active logic is a direct result of the practical acknowledgment that any real agent is limitedto reasoning only with whatever formulas (wffs) it has been able to come up with so far, rather than with implicit butnot yet performed inferences. Thus, determining if a given formula P is known is not a question of seeing if P is aconsequence of one’s current beliefs, but only a question of seeing if P is actually present in the KB. This approach isespecially important to the issue of performing consistency checks before accepting new formulas into the KB. Afterall, before accepting P , one may well want to know whether P is consistent with one’s current beliefs. In general, Pis not consistent with the KB if ¬P can be derived from KB. However, it is not in general possible to know, for anygiven formula if that formula is derivable from current beliefs, without actually going through the required deductionsto prove it. That could take a great deal of time—more time than a typical agent will have before deciding to acceptP . Cutting this process down to a simple KB look-up of ¬P , then, is an important practical simplification. So, insteadof looking for arbitrary contradictions to P, we are only looking for direct contradictions (i.e. ¬P ).5But won’t this practical simplification mean that active logic KBs are more likely to become inconsistent? That iscertainly a possibility, and yet, insofar as (a) contradictions are an inevitable part of living in and reasoning about thereal world, and (b) the consistency of complex KBs is practically impossible to determine or maintain, then it seemsa better bet to focus less on maintaining consistency, and more on an ability to reason effectively in the presence ofcontradictions, taking action with respect to them only when they become revealed in the course of inference (whichitself might be directed toward finding contradictions, to be sure).This is where the other central features of active logic—its step-wise control over inference, and the built-in abilityto refer to individual steps of reasoning—come into play, making active logic a natural formalism for detecting andreasoning about contradictions and their causes. For as soon as a contradiction reveals itself—that is, as soon as Pand ¬P are both present in the KB—it is possible to “capture” it, preventing further reasoning using the contradictoryformulas as premises (and thereby preventing any explosion of wffs), while at the same time marking their presence,to allow further consideration of the cause of the contradiction. Current implementations of active logic incorporate a“conflict-recognition” inference rule like Definition 18 for this purpose.Through the use of such rules, direct contradictions can be recognized as soon as they occur, and further reasoningcan be initiated to repair the contradiction, or at least to adopt a strategy with respect to it, such as simply avoidingthe use of either of the contradictory formulas for the time being. Unlike in truth maintenance systems [15,16] where4 Inheritance and disinheritance are directly related to belief revision [23] and to the frame problem [11,31]; see [34] for further discussion.5 This discussion is not meant to imply that, if ¬P is found in the KB, that the agent will necessarily, for that reason, reject P , for there may begood reason to reject ¬P , instead.1058M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063a separate process resolves contradictions using justification information, in active logic the contradiction detectionand handling [32] occur in the same reasoning process. In fact, the Contra predicate is a meta-predicate: it is aboutthe course of reasoning itself (and yet is also part of that same evolving history).Thus, speaking somewhat more broadly, active logic is a paraconsistent logic that achieves its paraconsistency invirtue of possessing two simultaneously active (and interactive) modes of reasoning, which might be called circum-spective and literal. In literal mode, the reasoning agent is simply working with, and deriving the consequences of,its current beliefs. In circumspective mode, the reasoning agent is reasoning about its beliefs, noting, for instance,that it has derived a contradiction, and deciding what to do about that. It is important to active logic that these arenot separate, isolated modes, but interactive and part of the same overall reasoning process. Thus, for instance, the(circumspective) derivation of Contra is triggered by the (literal) derivation of P and ¬P , and reasoning with Contrahappens alongside reasoning about other matters. Likewise, reasoning about a contradiction may eventually result inthe reinstatement of one of the conclusions, P or ¬P , to be carried forward and reasoned with in literal mode. It is pre-cisely this ongoing interaction between literal and circumspective modes, between reasoning and self-monitoring, thatallows active logic to avoid the pitfalls of explosive logics, and makes it more appropriate to the needs of real-worldagents.8. Comparison with related workActive logic is primarily related to two bodies of work—work on temporal logics, and work on paraconsistentlogics. We will treat each of these subjects in turn.8.1. Temporal logicsTemporal logics—logical formalisms explicitly allowing for the representation of temporal information—wereintroduced by Prior (under the name of Tense Logic) in a series of writings between 1957 and 1969 [43–45]. Pnueliestablished the relevance of tense logic for understanding the runtime behavior of programs [38]. Such temporal logicsare modal, with operators for notions such as the future truth of a predicate. A first-order approach to reasoning abouttime was employed by Allen [1], with expressions such as Holds(A, t) to mean A is true at time t; Allen and othersmade major strides in the use of such formalisms (so-called action logics) in AI. Part of the effect of these latter effortswas to connect temporal logic to belief logics, i.e., logics for representing information about an agent that plans andacts in a dynamic world. Thus action logics typically have temporal aspects, since the passage of time is of centralimportance to the planning and carrying out of actions; see for instance [22].Another central feature of most such logics is a treatment of the frame problem. Definition 21 (the inheritance rule)might be considered a kind of frame axiom. While it does not quite assert that φ remains true despite an action havingoccurred, it has a similar effect: it says that φ will remain believed unless there is a reason not to believe it, such asmight happen if an action is known to have negated φ.Various logics of action and belief have been extensively studied for as long as AI has existed [28–31]. Typically,the formalism is designed to represent the formation of an agent’s beliefs (including its beliefs about the results ofactions) based on a starting set of information (initial beliefs, or axioms). However, since belief-formation in anyreal-world agent must occur as a process in time, it is natural to consider a logic in which not only is time represented(i.e., one is able to express things about time, as in a temporal logic) but also the passage of time is representedas an evolving process in which the “present” time moves forward during belief formation. Thus the agent has acertain set of beliefs “now”, and another set at a later “now”. But if the logic is to be used by the agent, then its ownevolving notion of what time it is must be factored into the formalism as well. This is where active logics come in: anagent/temporal logic with a twist: an evolving now and corresponding time-sensitive inference rules.Active logic is not the only formalism to consider time in this way. For instance, SNePS [50], especially as appliedto the Embodied Cassie project [49], incorporates an indexical, evolving-time variable NOW. Cassie, a natural-language-using autonomous robot, uses this variable to track the passage of time, allowing it to do such things asappropriately alter verb tenses when discussing present or past actions. Cassie’s temporal awareness also plays a rolein time-sensitive planning projects like maintaining its battery and remediating unexploded land mines (in simulation).The motivations for including such an evolving “now” in Cassie are quite similar to the motivations for includingone in active logic. Ismail and Shapiro write: “[E]mbodied cognitive agents should . . . act in and reason about aM.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–10631059changing world, using reasoning in the service of acting and acting in the service of reasoning. Second, they shouldbe able to communicate their beliefs, and report their past, ongoing, and future actions in natural language. Thisrequires a representation of time . . . ” [27]. However, there are some significant differences in the nature of the “now”incorporated into each formalism, and how it can therefore be used.Perhaps the biggest difference is that for the SNePS-based agent Cassie, NOW is a meta-logical variable, ratherthan a logical term fully integrated into the SNePS semantics. The variable NOW is implemented so that it does,indeed, change over time (and, in particular, changes whenever Cassie acts in any way, including by reasoning), butthis change is the result of actions triggering an external time-variable update. In active logic, in contrast, reasoningitself implies the passage of time. Perhaps in part because of this difference, SNePS is a monotonic logic, whereasactive logic is non-monotonic, leveraging the facts that beliefs are had at times, and beliefs can be had about beliefs,to easily represent such things as “I used to believe P , but now I believe ¬P ” using the Bel operator. SNePS is alsoable to represent beliefs about beliefs, but there is no indication that this ability is leveraged by SNePS to guide beliefupdates. Rather, all Cassie’s beliefs are about states holding over time, so that belief change is effected by allowingbeliefs to expire, rather than by formally retracting them. This is a strategy similar to that employed by the situationcalculus (which does not itself incorporate a changing Now term) [31]. Finally, although SNePS is a paraconsistentlogic, it is so in virtue of the fact that contradictions imply nothing at all, whereas in active logic contradictions implyContra, a meta-level operator that can trigger further reasoning.8.2. Paraconsistent logicsAs mentioned in the introduction, the term paraconsistent logic is applied to logics that are not explosive. Anotherway to look at this concept is to consider that classical logic is so averse to inconsistency that it cannot distinguishbetween local inconsistency, where for some formula A both A and ¬A hold, and global inconsistency, where for allformulas A both A and ¬A hold. So in a paraconsistent logic, local inconsistency does not imply global inconsis-tency. For various reasons, including philosophical issues, the intrinsic interest of investigating paraconsistency, andparticularly the increasing number of applications involving inconsistencies, there has been growing interest in thisfield, including several books, numerous papers, and three World Congresses on Paraconsistency: [5] and [12] are theProceedings of the first two (see also [13] for a historical survey).As noted in the survey paper [24], paraconsistency may be achieved in several different ways. Modifying theaxioms or rules is one technique. Another method stays within the framework of classical logic by the use of maximalconsistent subsets of formulas. Consider an inconsistent set of formulas Γ . There must always be some subsets of Γthat are consistent (for example, ∅ is consistent) hence there must be maximal consistent subsets of Γ . In this methodA is deduced from Γ if A is deduced classically from all maximal consistent subsets of Γ [48]. Some researchers useadditional criteria to find preferred consistent subsets and work with those [8].Another technique [7] extends the set of classical truth values from {True, False} to a larger set. Usually, theset of truth values is given an algebraic structure, typically a lattice. Perhaps the best-known of these is the latticeFOUR = {True, False, Both, Neither} where Both stands for an inconsistency. A fourth approach extends classicallogic by the addition of modal or metalevel operators. Modal logic has an operator for a formula to be possible (truein some world) and necessary (true in all worlds) where worlds are selected in some way. Both a formula A and itsnegation ¬A may be possible because they are true in different worlds, but that does not mean that all formulas arepossible.Consider now how active logic fits into the classification given above. In active logic the rules of inference arelimited, and are based on the passage of time. Also the language contains the meta-level operator Contra to indicatecontradictory statements. Hence active logic combines two of the methods above to achieve paraconsistency.8.3. Other related workSeveral other interesting frameworks exist that encompass many logical systems in a uniform manner. We brieflydiscuss two such frameworks here.A Labelled Deductive System (LDS) [20] is a logical reasoning system employing both formulas and annotationsfor those formulas, called labels. The labels can have various contents with effects on the deductions. For instance,if the label indicates that one formula is better supported by evidence than another, then deductions using the better1060M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063supported formula may be preferred over those using the other, especially in cases of conflict. Such an LDS wouldimplement a non-monotonic logic with preferences or prioritized defaults. A group of LDSs called restricted accesslogics [21] deal specifically with inconsistent information. We mention LDS here because there is some evidence thatthe simple version of active logic SL7, described in [17], can likewise be implemented by or described as an LDS [3].This is an interesting finding, and although it is not clear that the version of active logic described here can likewisebe expressed as an LDS, this may well turn out to be the case. Even so, active logic would be a special case of LDSwith some interesting and valuable properties, such as non-monotonicity, paraconsistency, and temporal sensitivity.Another interesting general framework is called adaptive logics. The adaptive logics that handle inconsistencies arecalled inconsistency-adaptive logics [6]. Adaptive logics are characterized by a lower limit logic, a set of abnormali-ties, and an adaptive strategy. The purpose of adaptive logics is to characterize inference relations (cid:21) for which thereis no positive test that for every Σ and ϕ will answer “yes” if Σ (cid:21) ϕ. Active logics are more well behaved and havesuch a positive test. In fact, as we will show in the next section, even the limiting use of |=a has such a test in our casewhere the logic is essentially propositional.9. The ideal active logic agentIf we imagine an active logic agent working literally as we describe in the semantics, it should be clear thatany application of an apperception function to turn an inconsistent KBt into a consistent one will itself reduce theinferential power of (number of things that can be inferred from) KBt , with the obvious extreme case being theapplication of an apperception function that uses a different unique superscript for every occurrence of every symbolin KBt . In such a case (ignoring for the moment the fact that KBt can also change through observation, and notjust through inference), the only things that could be inferred would be tautologies and simple elaborations, such asconcluding ¬S01 . Thus, it might appear that any actual active logic agent will only be able to infer a1set of active consequences from its KB that is much more limited than what is permitted by the semantics, and manywill very quickly run out of interesting things to conclude, as a result of the apperception functions they apply. Thissuggests a significant practical problem.2 from S0→ S0Nevertheless, we believe that this semantics could in fact be used as a guide to building active logic agents, througha concept called the ideal active logic agent. This is an agent that can infer in one time-step exactly the 1-step activeconsequences of any Σ. Consider how such an ideal active logic agent could be built.Obtaining the 1-step active logic consequences in SnLa is fairly easy using Definition 14. It is just a matter of gettingthe appropriate Now, Contra, and Bel statements. The difficult part is getting the 1-step active logic consequencesin SnLw .Since Σ is finite, as remarked in Section 4 only finitely many apperception functions are needed, say ap1, ap2,. . . , apn. We may imagine one subagent Ai per api , 1 (cid:3) i (cid:3) n. Observe that not all of these n apperception functionsare necessarily in APΣ because some of them may not yield a consistent set in Lw. Hence each subagent first needs∈ APΣ . In our case, since the language is basically propositional, this can be done by truth tables in ato check if apifinite amount of time.Next, continuing only with those sub-agents that passed the first test, enumerate all sentences in SnLw : ϕ1, ϕ2, . . .and have each Ai check if apzi (Σ) |= ϕj for j = 1, 2, . . . . Again, each such check can be done by truth tables in afinite amount of time. Each Ai should include the set of ϕj s that pass this test. This way, the n agents will generate(up to equivalence of apperception functions) exactly those sentences of Lw that are 1-step active consequences. Wecould even dispense with the equivalence above by using infinitely many subagents.Although it thus appears possible to build an ideal active logic agent, it is pretty clearly not a practical way toobtain active logic consequences. (In fact, if the logic were truly first-order, the consistency and implication checkscould not be done.) So we introduce another concept: the practical active logic agent. The practical active logicagent is an implementable inferencing (and observing) agent that will infer only 1-step active consequences of itsKB, but not all of them. The practical research question, then, is how such practical active logic agents might bebuilt—with what rules and what apperception functions—so that they are limited to inferring only 1-step active con-sequences of their beliefs, but do not thereby have severely limited inferential powers. We will discuss this in the nextsection.M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063106110. Practical active logic agentsIn an earlier attempt at defining a semantics for active logic [2], we discussed a version of the apperception functionquite similar to what we have laid out here, but with the key difference that after an inference had taken place (i.e.,after each time step) all the superscripts were automatically returned to 0. Let’s call this the “reset version” of theapperception function. The advantage of the reset version was that it was possible to imagine literally implementingit, such that the agent could choose a small, clearly consistent set of formulas on which to focus to do inference, andthen apply an apperception function such that the symbols in this set of formulas retained the superscript 0, and allother symbols were given unique non-zero superscripts. This guaranteed the consistency of the KB in a practicallyimplementable way, and allowed inferences similar to those described here.However, while such an agent is practically implementable, and inferentially powerful, Johan Hovold showed that itwould in fact be too powerful, capable of inferring things that are not active consequences of its KB [26]. In particular,he showed that a logic using such an apperception function would be explosive.Consider the following (inconsistent) set of sentences at time t: {S01 , S012 , ¬S02 , S02}. Applying an apperception2 , ¬S022 , ¬S0} yields a consistent set, and the following set is a possible 1-stepfunction such that this becomes {S12active consequence {S1}.6 Since in this version of the apperception function all superscripts→ S01 , S031are now returned to zero, we get, at time t + 1: {S0}. Now imagine an apperception function1 , S01}.→ S0with the following effect: {S02 , S032Since this would be true for any arbitrary sentence S3, a logic using this version of the apperception function is indeedexplosive, in that any arbitrary sentence is derivable (in two time steps) from an inconsistent set.→ S0}. A 1-step active consequence of this set at time t + 2 is {S032 , ¬S02 , ¬S1→ S031 , S011 , S012 , S02→ S0→ S0→ S0→ S0It was in response to this discovery by Hovold that we changed the apperception function used in defining thesemantics of active logic to the version detailed in Definition 11. But a practical active logic agent could still use thereset version and avoid the charge of explosivity, just as long as it lacked the necessary rule for inferring S1 → S2from ¬S1. Thus, we claim that an agent equipped with the reset version of the apperception function, along with acarefully chosen set of inference rules, could be a practical active logic agent: it would be possible to implement, andwould never conclude anything that was not an active consequence of its KB (according to Definitions 14 and 15 ofactive consequence given above).The simplest example of such an agent is one equipped with only the timing rule (Definition 17), the direct con-tradiction rule (Definition 18), the positive introspection rule (Definition 19), the inheritance rule (Definition 21), andactive modus ponens (Definition 22). Clearly, such an agent will have somewhat limited inferential abilities, yet wouldbe perfectly adequate to many practical situations. The obvious question about such practical active logic agents is:exactly which rules, in which combinations, allow for maximum inferential power, while still limiting the agent toinferring only active consequences (according to Definitions 14 and 15) of its KB? This question is something weleave to future work.11. Conclusion and future workIn this paper we have outlined a semantics for a time-sensitive, contradiction-tolerant logical reasoning formalismdesigned for on-board use by real-world agents. Central to the semantics is the notion of an apperception function,inspired by the idea that, until an agent notices that a set of beliefs is inconsistent, that set seems consistent—and thatwhen an inconsistency is noticed, that fact can be explicitly registered by the agent, and further reasoning with theinconsistent beliefs can be curtailed.To keep this initial presentation relatively simple, we made a number of assumptions that in future work we hopeto discard. For example, we assumed that the world is stationary, and thus all facts about the world are timelessly true.It should be noted that there is no problem in principle with applying active logic to the case of reasoning about achanging world—after all, the facts that beliefs are held at times, that the KB changes over time, and that inferenceis itself a temporal phenomenon, are all already explicitly modeled by the formalism. To handle a changing world,we would also have to model the additional facts that beliefs can be held not just at times, but about facts-at-times,and even about the durations of facts—e.g. that it rained yesterday, or that it rained yesterday for 1 hour between6 The added formula is a consequence of ¬S02 .1062M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–1063noon and one. Such modification is straightforward, especially given that we already have defined the changing set ofworld-facts Wt , by reference to which the truths of temporally relative beliefs would be determined.But whereas these changes are easily handled, there are some tricky aspects to modeling proper reasoning withtemporally relative beliefs in a changing world. For instance, although in active logic we do not inherit Now(t) at timet + 1, we probably do want to inherit a belief like “It is raining at t” to time t + 1, for even at time t + 1 it remains truethat it was raining at t. Further, it will generally be reasonable to conclude from “It is raining at t” that “It is raining att + 1”, for if it is raining now, it will probably still be raining at the next moment. But unlike the case of Now(t), fromwhich it will always be correct to conclude Now(t + 1), it will not always be correct to conclude “It is raining at t + 1”from “It is raining at t”. Moreover, for different facts, the likely duration of their continued truth will also differ. Fromthe fact that it is raining now, I might reasonably conclude that it is still raining five minutes later; but it would not bereasonable to conclude, just on this basis, that it is still raining twenty-four hours later. In contrast, from the fact thata mountain is in such-a-such a place it will be reasonable to continue to infer its truth for a very long time indeed. Tohandle such issues we can avail ourselves of the extensive literature on default reasoning and non-monotonic temporallogics, e.g., [4,9,10,14,19,25,35–37,39,47]. Because of these and similar complications, we thought that the issue ofreasoning about a changing world deserved special, separate treatment.Future work will also consider the extension of Lw to first-order logic; multiple agents, reasoning both about theworld and about one another’s beliefs; and extending the semantics to include other predicates.AcknowledgementsThis work was supported in part by a grant from AFOSR. A preliminary version was presented at the 7th AnnualSymposium on the Logical Formalization of Commonsense Reasoning, in May, 2005. We thank the members of theaudience for suggestions. We particularly want to thank Johan Hovold and the referees for their careful reading andmany helpful remarks on an earlier version.References[1] J. Allen, Towards a general theory of action and time, Artificial Intelligence 23 (1984) 123–154.[2] M.L. Anderson, W. Gomaa, J. Grant, D. Perlis, On the reasoning of real-word agents: Toward a semantics for active logic, in: Proceed-ings of the 7th Annual Symposium on the Logical Formalization of Commonsense Reasoning, Dresden University Technical Report (ISSN1430-211X), 2005.[3] M. Asker, J. Malec, Reasoning with limited resources: Active logics expressed as labeled deductive systems, Bulletin of the Polish Academyof Sciences (2005) 123–154.[4] F. Bacchus, A. Grove, J. Halpern, D. Koller, Statistical foundations for default reasoning, in: IJCAI, 1993.[5] D. Batens, C. Mortensen, G. Priest, J.-P. Van Bendegen, Frontiers of Paraconsistent Logic, Taylor & Francis Group, 2000.[6] D. Batens, J. Meheus, Recent results by the inconsistency-adaptive labourers, Technical report, Universiteit Gent, 2005.[7] N.D. Belnap, A useful four-valued logic, in: J.M. Dunn, G. Epstein (Eds.), Modern Uses of Multiple-Valued Logics, D. Reidel, 1977, pp. 8–37.[8] S. Benferhat, D. Dubois, H. Prade, Some syntactic approaches to the handling of inconsistent knowledge bases: A comparative study, part 1:The flat case, Studia Logica 58 (1997) 17–45.[9] P. Besnard, T. Schaub, An approach to context-based default reasoning, Fundamenta Informaticae (1995).[10] R. Brachman, I lied about the trees or, defaults and definitions in knowledge representation, AI Magazine 6 (3) (1985) 80–93.[11] F. Brown (Ed.), The Frame Problem in Artificial Intelligence, Morgan Kaufmann, 1987.[12] A. Carnielli, M.E. Coniglio, I.M.L. D’Ottaviano, Paraconsistency: The Logical Way to the Inconsistent, Marcel Dekker, Inc., 2002.[13] N.C.A. da Costa, J.-Y. Beziau, O. Bueno, Paraconsistent logic in a historical perspective, Logique & Analyse 150 (2) (1995) 111–125.[14] J.P. Delgrande, An approach to default reasoning based on first-order conditional logic: Revised report, Artificial Intelligence 36 (1) (1988)63–90.[15] J. Doyle, A truth maintenance system, Artificial Intelligence 12 (1979) 231–272.[16] J. Doyle, A model for deliberation action, and introspection, PhD thesis, Massachusetts Institute of Technology, 1980.[17] J. Elgot-Drapkin, Step-logic: Reasoning situated in time, PhD thesis, Department of Computer Science, University of Maryland, College Park,Maryland, 1988.[18] J. Elgot-Drapkin, D. Perlis, Reasoning situated in time I: Basic concepts, Journal of Experimental and Theoretical Artificial Intelligence 2 (1)(1990) 75–98.[19] D. Etherington, A semantics for default logic, in: Proceedings of the 10th Int’l Joint Conference on Artificial Intelligence, Milan, Italy, 1987,pp. 495–498.[20] D. Gabbay, Labelled Deductive Systems, Oxford University Press, 1996.[21] D.M. Gabbay, A. Hunter, Restricted access logics for inconsistent information, in: M. Clarke, R. Kruse, S. Moral (Eds.), Symbolic andQuantitative Approaches to Reasoning and Uncertainty, Springer, 1993, pp. 137–144.M.L. Anderson et al. / Artificial Intelligence 172 (2008) 1045–10631063[22] E. Giunchiglia, J. Lee, V. Lifschitz, N. McCain, H. Turner, Nonmonotonic causal theories, Artificial Intelligence 153 (2004) 49–104.[23] P. Gärdenfors, Knowledge in Flux: Modeling the Dynamics of Epistemic States, MIT Press, Cambridge, MA, 1988.[24] J. Grant, V.S. Subrahmanian, Applications of paraconsistency in data and knowledge bases, Synthese 125 (2000) 121–132.[25] S. Hanks, D. McDermott, Nonmonotonic logic and temporal projection, Artificial Intelligence 33 (1987) 379–412.[26] J. Hovold, On a semantics for active logic, MA Thesis, Department of Computer Science, Lund University, 2005.[27] H.O. Ismail, S.C. Shapiro, Two problems with reasoning and acting in time, in: Principles of Knowledge Representation and Reasoning:Proceedings of the Seventh International Conference, 2000.[28] K. Konolige, A Deduction Model of Belief, Pitman, London, 1986.[29] H. Levesque, A logic of implicit and explicit belief, in: Proceedings of the National Conference on Artificial Intelligence, Austin, TX, Ameri-can Association for Artificial Intelligence, 1984, pp. 198–202.[30] J. McCarthy, Programs with common sense, in: Proceedings of the Symposium on the Mechanization of Thought Processes, Teddington,England, National Physical Laboratory, 1958.[31] J. McCarthy, P. Hayes, Some philosophical problems from the standpoint of artificial intelligence, in: B. Meltzer, D. Michie (Eds.), MachineIntelligence, Edinburgh University Press, 1969, pp. 463–502.[32] M. Miller, A view of one’s past and other aspects of reasoned change in belief, PhD thesis, Department of Computer Science, University ofMaryland, College Park, Maryland, 1993.[33] M. Miller, D. Perlis, Presentations and this and that: logic in action, in: Proceedings of the 15th Annual Conference of the Cognitive ScienceSociety, Boulder, Colorado, 1993.[34] M. Nirkhe, S. Kraus, M. Miller, D. Perlis, How to (plan to) meet a deadline between now and then, Journal of Logic and Computation 7 (1)(1997) 109–156.[35] M. Nirkhe, D. Perlis, S. Kraus, Reasoning about change in a changing world, in: Proceedings of FLAIRS-93, 1993.[36] D. Perlis, Intentionality and defaults, Internat. J. Expert Systems 3 (1990) 345–354, Special issue on the Frame Problem. Reprinted as achapter in: K. Ford, P. Hayes (Eds.), Advances in Human and Machine Cognition, vol. 1: the Frame Problem in Artificial Intelligence, JAIPress, 1991.[37] D. Perlis, J. Elgot-Drapkin, M. Miller, Stop the world!—I want to think!, Internat. J. Intelligent Systems 6 (1991) 443–456. Special issue ontemporal reasoning.[38] A. Pnueli, The temporal logic of programs, in: Proceedings of the 18th IEEE Symposium on Foundations of Computer Science, 1977, pp. 46–67.[39] D. Poole, A logical framework for default reasoning, Artificial Intelligence 36 (1988) 27–47.[40] G. Priest, Paraconsistent logic, in: D. Gabbay, F. Guenther (Eds.), Handbook of Philosophical Logic, second ed., Kluwer Academic Publishers,2002, pp. 287–393.[41] G. Priest, R. Routley, J. Norman, Paraconsistent Logic: Essays on the Inconsistent, Philosophia Verlag, München, 1989.[42] G. Priest, K. Tanaka, Paraconsistent logic, in: E.N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy, Summer 2004.[43] A.N. Prior, Past, Present and Future, Clarendon Press, Oxford, 1967.[44] A.N. Prior, Papers on Time and Tense, Clarendon Press, Oxford, 1968.[45] A. Prior, Time and Modality, Oxford University Press, 1957.[46] K. Purang, Systems that detect and repair their own mistakes, PhD thesis, Department of Computer Science, University of Maryland, CollegePark, Maryland, 2001.[47] R. Reiter, A logic for default reasoning, Artificial Intelligence 13 (1, 2) (1980) 81–132.[48] N. Rescher, R. Manor, On inference from inconsistent premises, Theory and Decision 1 (1970) 179–219.[49] S.C. Shapiro, Embodied cassie, in: Cognitive Robotics: Papers from the 1998 AAAI Fall Symposium, AAAI Press, Menlo Park, CA, 1998,pp. 136–143.[50] S.C. Shapiro, Sneps: A logic for natural language understanding and commonsense reasoning, in: L.M. Iwanska, S.C. Shapiro (Eds.), NaturalLanguage Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language, AAAI Press/The MIT Press,2000, pp. 175–195.