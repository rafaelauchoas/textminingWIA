ELSEVIER Artificial Intelligence 87 ( 1996) l-20 Artificial Intelligence Local conditioning in Bayesian networks F.J. Diez* Departamento Inteligencia Artificial, U.N.E.D., Senda de1 Rey, 28040 Madrid, Spain Received March 1993; revised February 1994 Abstract (LC) Local conditioning is an exact algorithm for computing probability in Bayesian networks, networks. A list of developed as an extension of Kim and Pearl’s algorithm to each node guarantees that only the nodes inside a loop are conditioned variables associated on the variable which breaks it. The main advantage of this algorithm is that it computes the probability directly on the original network instead of building a cluster tree, and this can save time when debugging a model and when the sparsity of evidence allows a pruning of the network. The algorithm is also advantageous when some families in the network interact through AND/OR gates. A parallel implementation of the algorithm with a processor for each node is possible even in the case of multiply-connected networks. for singly-connected 1. Introduction A Bayesian random variable, network is an acyclic directed graph in which every node represents a together with a probability distribution such that P(x,,... 9%) =J-JWilP(xi)) i (1) is just of the where xi represents a possible value ,of variable Xi and pa( xi) is an instantiation prob- the conditional parents of Xi in the graph. For a node/variable ability property of Bayesian from Eq. (1). The basic infer- networks, ence problem the a posteriori probability can be deduced from the conditional of a variable X given a certain evidence with no parents, independence its a priori probability. The essential e 3 {“Xi = Xi”,“Xj = consists of computing called d-separation probabilities [20,22], P(x\e) * E-mail: fjdiez@dia.uned.es. WWW: http:/!www.dia.uned.es/Yjdiez 0004-3702/96/$15.00 SSDIOOO4-3702(95)00118-2 Copyright @ 1996 Elsevier Science B.V. All rights reserved El. Die?/hr(jiciul Intrlli~qwcr X7 (1996) J-20 Fig. I. Evidence propagation by message passing .}. For singly-connected x,“. gorithm algorithms the appendix-while numerical values of conditional [ 4,5]. the problem is NP-hard the complexity ( I5,21 ], hut the general case is much harder: heavily depends on the structure of the network-there networks. there exists an elegant and efficient exact al- of exact in of approximate methods depends mainly on the for both exact and approximate methods, the time complexity is an example probabilities; The best-known exact methods are clustering and conditioning. A version of the [ 14, 191, has become the standard algorithm for inference former, clique-tree propagation in Bayesian networks. Conditioning, on the other hand, up until now had never been used in real expert systems. The purpose of this paper is to offer an efficient version of conditioning for practical applications. The key idea in our approach consists of conditioning specific conditioning, inside each loop. It is called local because every node has a indicated by a list of variables. exclusively suitable the remainder of this introduction as follows: is organized [ 15,211 algorithm for singly-connected The paper Kim and Pearl’s conditioning methods. Section 2 explains how to build an associated some links and assigning derives technical details algorithm known methods and, finally, variables for evidence propagation; we will in Section 4. Section 5 compares the conclusion offers suggestions a list of conditioning for a straightforward implementation. are discussed the formulas to each node, and Section 3 try to clearly explain all the The two possible versions of the local conditioning with other for future research. summarizes three tree by removing networks and reviews I. 1. Algorithm for the polytree The goal of the algorithm probability of proposition is to tind the a posteriori probability i.e., the “The value of variable X is x” given the observed evidence e. the network, an arbitrary node X divides P(xle), In a polytree, i.e., in a singly-connected into a link XY divides e into the evidence above to its causes, es, and that connected that connected evidence Similarly, it, eir. This partition of evidence propagated in the network (see Fig. I ): justifies the following definition to its effects, e;. the link, e&,, and that below of the messages EJ. Diez/ArtQicial Intelligence 87 (1996) I-20 7r(x) 3 P(x,exf), A(x) E P(e;lx), rX(ui) E P(&,e&X), AC(X) = P(e&IX). 3 (2) (3) (4) (5) These definitions are basically taken from [ 221, although Eqs. (2) and (4) introduced by Peot and Shachter [23] to simplify the computation follow in the modification conditioning algorithms. For a singly-connected network, d-separation results in two subsidiary properties [221>: l Two children x and Yj of a node X are independent given the value of their parent: P(YilX) = P(YilXfYj>. l A parent Ui and a child Yj of a node X are independent given the value of X: P(UilX) = P(UilX,Yj). Nevertheless, two parents of a node X, which in a polytree are always a priori independent, in general become correlated by the instantiation of X: P(UijX) Z P(UilX,Uj). These independence properties lead to recursive expressions for computing the mes- sages: P(xle) = arr(x)h(x), ll, ,.... lb, i=l j=l q(x) =4x) r-pv,(XL k#j AY,(X)=~ Yl [ A(Yj> u,,...,up 3 &=I 1 P(yjlX,ol,...,u,)~?rq(L’t) c (6) (7) (8) (9) (10) where VI, . . . , V, are the causes of q other than X, and cr = [P(e) after finding constant to be computed and A(x) . r(x) ] -’ is a normalization 1.2. An overview of three conditioning methods Because of d-separation, precisely, of a loop-more the instantiation a node whose of a node which is not at the bottom its in the loop are not both two neighbors 4 (b) Fig. 2. A network and Its associated tree. in the sense that some correlations disappear in the loop as if it were part of a polytree. Therefore, the loop parents-breaks can be propagated tioning method break the loops in the resulting cutsets are {C}, {A.D} C-D-F-E-C. requires a structural process in the network) tree. A cutset for the network and {B,E}. {F} for finding a curser (a set of nodes and a numerical process which propagates in Fig. 2(a) is {A,E}: IS not valid because it does not break loop and evidence any condi- that evidence other possible RJ. Diez/Artijicial Intelligence 87 (1996) I-20 5 can be called global conditioning Pearl’s algorithm it conditions [22, pp. 204-2101 every node the network cause initializing ery evidence node to all other nodes GC is not only exponential findings. in the network on every variable the weights computes [ 281, it recursively in the network. Consequently, in the cutset size, but also proportional (GC) be- in the cutset. After from ev- transmitted the complexity of to the number of Peot and Shachter [23] introduced two important improvements in the original al- (Unfortunately, that cannot be unconnected they do not offer an algorithm gorithm. They define a knot as a portion of the network by removing one edge. knots of a graph.) Since every knot has its own cutset, conditioning with its corresponding networks consisting of more than one knot, this is a first improvement respect (see Section 1 .l ) which allows ings without having to process the worst-case multiplied the the method can be called knot (KC). For example, Fig. 2(a) consists of two knots: {A, B, C, D, E, F, G}, is the empty set. For in efficiency with in the definition of V(X) and ~x( ui) the influence of different find- for every evidence node. Therefore, cutset size the algorithm the whole network to GC. The second one is a small change is bounded by the maximum by the number of knots. the cutset for the latter of their algorithm edges, and {H}; for finding complexity to fusion the knots, applies conditioning that there tree with a list of conditioning is a specific conditioning Local conditioning (LC) , the algorithm and, instead of considering local means the term displays an associated that all the nodes conditioned depend on x. Local conditioning Shachter exponential and is much more efficient: includes in the path between introduced in this paper, goes a step further exclusively within each loop; for each node. Fig. 2(b) for every node. Observe a cutset node X and a phantom node X* are flowing along this path will by Peot and for which KC has the improvements there exist some structures introduced variables complexity while LC only requires linear time (see Section 5.1). on that variable and so every 7r- or A-message 2. Associated tree The process of building for finding cutsets, a list of conditioning and assigning algorithms both (Section 4). It consists of a depth-first detect the loops tasks at the same in the network.’ an associated tree encompasses two tasks: finding a cutset to every node. Before reviewing previous a new algorithm which performs time and can even be integrated with evidence propagation this section introduces variables search in the undirected graph as a means to 2.1. DFS, a depth-$rst search algorithm The following example shows how to transform a Bayesian network an associated tree (Fig. 2(b)). The search begins at an arbitrary pivot node-A (Fig. 2(a)) into in our ’ We assume that the network is connected, as is the case for real-world models. Otherwise, the procedure should examine every connected part separately. 6 EJ. Diez/ArtijiciuI lntellrgence 87 (1996) I-20 travels through ignoring the network them in a list called PATH. A possible the direction of the edges, marking example-and route goes through A, the nodes and including B, G, F, H, D, and C. Backtracking from a node H with no untraversed edges removes this node from PATH, such that this list always contains a path from the pivot node to node C its the node currently value is (A,B,G,F;D,C). forward from C to A, the fact that the latter is already marked denotes and a new the presence of a loop. As a result, a phantom node A* arises the loop. Every node between A edge A*C replaces and A*, i.e., every node in PATH from A to the end of the list, adds A to its own list of conditioning at the moment of examining investigated. For instance, (see Fig. 2(b) ). thus breaking When going (Fig. Z(b)) the original link AC, variables The search continues through E to F, which is marked, too. A new loop has appeared, is at the bottom of the loop. A way to make but now the node F that has been reached sure that the chosen node can break the loop is to condition on the variable at the top of loop, the value of PATH the removed edge, E in this case. When detecting in PATH from F to the end of the list must is (A, B, G, E D, C, E). Again, every node to the pivot add E to its list of conditioning node and the search is over. the algorithm backtracks variables. Then the second Clearly, the associated tree obtained by this method depends on the choice of pivot as rule, but it is to determine which neighbor must be visited first in order to achieve an efficient the neighbors of every node are visited. Selecting loops is a possible heuristic the most in which the node which breaks node and on the order the pivot difficult associated tree. 2.2. Other algorithms forjnding cutsets The problem of finding a minimal cutset is NP-hard [ 261. However, the complexity of state space the product to minimize global conditioning to the product of the number of values of the variables attempting algorithm which simply seeks a cutset with few variables. The heuristic algorithm Al proposed by Suermondt does not depend directly on the size of the cutset, but is proportional in the cutset. Hence an algorithm than an in- iteratively to obtain a small in the cutset if there is a tie the number of values of the candidate nodes it only examines them. For a network with n nodes, the algorithm visits every node up to 3n times, the cutset obtained the node that has the most neighbors, cludes cutset; among which results may include unnecessary nodes not belonging to any loop. is in principle more accurate in a worst-case complexity of O(n*). Unfortunately, and Cooper thus trying [26] [25] designed A2, a modified version of AI, which usually that for a certain network with n nodes and an two nodes, both Al and A2 yield cutsets of size a( [$nJ)-a result. on the pivot node chosen. Furthermore, Incidentally, DFS finds a cutset of only two or three nodes for that of DFS is it visits every edge only once, and this implies a saving of time with can be found to A, and A2 for sparse networks. Other heuristic the complexity algorithms For this reason, Stillman finds a smaller cutset. He also showed optimal cutset of only disastrous network, depending O(e) because respect in 1131. EJ. Diez/Artl$cial Intelligence 87 (1996) l-20 7 Recently, Bar-Yehuda it to the vertex feedback set problem. During et al. [2] have offered a new approach to the cutset problem the reduction, a variable Xi in taking on ]Xi] values gives way to a node in the undirected graph is log,( ]Xi]). The vertex feedback set of the undirected graph coincides by reducing the Bayesian network whose weight with the cutset of the Bayesian network. the pegormance ratio of a vertex feedback its total weight and the total weight of an the complexity of global ratio. The performance (for a Bayesian network), For a graph set (of a cutset) is the quotient between optimal solution. Hence the product state size and, consequently, grow exponentially with the cutset performance conditioning, ratio of an algorithm is the worst-case performance nodes. From the example above, it is clear that the performance be better The algorithms and 2d2 for general networks. Subsequently, Becker and Geiger algorithm of time complexity O(e + nlogn) are theoretical Empirically, Although its average performance these algorithms and performance impossible ratio is 1.22, an excellent to finding cutsets, that it is virtually limit themselves than R( [inj). suggesting reasons in [2] achieve performance ratio among all networks with n ratio of Al and A:! cannot ratios of 4log,, n [3] have designed an to 2; there ratio equal this result. to improve result. to either Xt or X,, and a “real” node maintaining a loop X0-X1- them in order to obtain an associated X0 breaking only neighbors, In conditioning list. Alternatively, (see Section 5.1) . The extended algorithms the resulting it is possible to extend tree suitable for local conditioning. First, every node . . .-X,--X0, must be split into a phantom node, connected the links with all its other I loops originates 1 phantom nodes. to its to assign X0 to every link XiXi+i it would be possible tree, every node between Xz and X0 will add this variable still have polynomial time complexity. in Fig. 2; a node breaking as shown Nevertheless, neither the number of variables for local conditioning. are relevant local conditioning might be more efficient desirable instead of adapting with other problems conjecture arising that it is NE-hard to design algorithms the algorithms leading in the cutset nor their product state size Even in the case of two cutsets such that Cl C C2, it would be for C2 than for Ct. Therefore, to efficient computations developed for global conditioning. in local conditioning From similarity in graph to find an optimal associated theory and Bayesian networks, we make tree for local conditioning. the 3. Propagation of evidence Local conditioning propagates the above algorithms. As an example, network whose only network, In our example, e = efc U e,, e; = e, U e,, the definitions of es, e,, e&. and e;r loop was A-B-D-C-A. evidence on an associated let Fig. 3 represent Since introduced tree generated by some of a portion of a tree for a it is in fact a singly-connected in Section 1.1 are still valid. eiB = e& U e2 U e,, etc. 3.1. Propagation from a phantom node The computation of r(a) above A. The value of A(a) is straightforward is, according to its definition, since we assumed there was no loop 8 EJ. Diez/Artijicid Intelligence 87 (I 996) I-20 Fig. 3. A portion of an associated tree. A(a) = P(eJu) = P(e,la)P(e,&Ju) = A,,(a)AB(u). (11) In the associated tree, the instantiation of A breaks the loop, such that AB(~> = P(e,lu) = C P(e, , e&, b,ulu) =CP(e,,eb,h:i6.il,u)P(be;,.i.u)P(e,,u.o)P(~.u). h.1, Because of d-separation, some factors inside the summatory are simplified: As(a) = CP(eBlh,U)P(blc,a)P(e~~ju)P(u). Let A(bja) and ~-e(u) be A(+) f P(e,jb,a). Q(U) s P(u,e&) = P(e&lu)Y(c) Then the expression for AB(u) is equal to: As(a) = c A(blu) ~P(blrl,uh h I’ CL!) 1 Hence we need A( blu). The definition of AD (b(u), A~(bla) = f’(e;Dlb,a) leads to A(bla) = A&+W+). (12) (13) (14) (15) ( 16) EJ. Diez/Ar@cial Inrelligence 87 (1996) l-20 The computation of AD( bla) is similar to that of &(a): = c P(e,Id)P(dlbyc, f)P(e~Dlc,u)P(clu)P(e~Dlf)P(f) d&f P(dlb,C,f)~D(clU)~D(f) , 1 where 2 rD(ClU> z P(C,egDlU). The value of this message is ~D(ClU>=P(e3lC)P(ec+,ClU) = &J444 where 7~(clu> = P(c,ecf(u) = c P(e&,c, wlu) = ~P(clw,ubr~(w). W (17) (18) (19) (20) (21) The removal of link AC from the original network means that it is not necessary to pass a message AC (a), because evidence through C-D-B-A. associated node. Thus, messages do not flow on the whole network but on e3 has already been propagated up to A the to each tree, where there exists exactly one path from each piece of evidence 3.2. Propagation towards a phantom node Now messages must flow in the opposite direction, transmitting above A and probability. Note that messages one, in the sense that they can be computed its effect independently. towards A* so that every node can compute i.e. collecting all the evidence its to those in the previous in this section are orthogonal For node B, message A(blu) was given by Q. (16). So we need r(b,u) E P(b,ei,u) (22) 2 We cannot define a vD(cla) normalized does depend on a and so it would disturb Shachter’s definitions of T(X) and Px(Ui) for c because the computation instead of Pearl’s the normalization (Y = [cc rrD(cla)l-’ of P( ale). This is the reason for using Peot and “constant” (see Section 1.1). 10 ffJ. IXe:/Arrijicid Intellipxce X7 (1996) l-20 in order to compute /J(Dle)=CP(b,a/e) =cuCP(h,Ll,e) (1 As we did before: 7rfI(u) = A,, (a)97(a). In the same way 77(d,a) = c P(dlb, ~.f)~n(O,cr)~n(trlLz)~iTn(f), /1X. f where rrn(6,a) s P(b,u,ei,) = A,,(D)T(O,U). Clearly, The computation of P(cle) requires a A( c; u) such that (23) (24) (25) (26) (27) (28) (29) (30) P(cle) =nC~(c;u)7r(+). L, Message partition of evidence, h(c; a) happens to be harder to define than to compute. With a suitable PCcle) = lP(e)l~‘~P((ep\e~,~~c,a)P~c,e~~u)P(u,e,’,) and comparing the last two expressions, the definition of h(c; u) turns out to be h(c;u) E P((ec\e~s)lc,u)P(u,e~B). (31) = P((ec\e,+,)Ic,u)P(a,e~,/c). Note that the intuitive definition P(e;,ulc) influence of e& on C along the instantiation decomposed evidence above link AB. of C does not modify the path A-B-D-C Message Ao(c;u) must propagate “A( c; a) = P( e;, uic)” would not be correct because the for every value a in a way such that of A. For this reason ec was = e, n e,B, and the the probability into two sets: the evidence between C and A, e,\e,‘, EJ. Diez/Art@cial Intelligence 87 (1996) I-20 11 Then, the value of A( c; a) is A(c;a) = Ae,(C)AD(GU) where (32) (33) = c P(e;,e&, (e,fD\e~,>,d,b,flc,u)P(u,e~,) d,f,b = CP(eJd)P(+,c,f)P( e~F~f)P((e~D\e~~,)lu)P(u,e~e) d,f,b (34) 3.3. Discussion The lists of conditioning presence of A in LCV(B), on a the messages conditioning this variable: variables (LCV) LCV(C) they send and receive. because LCV( F) = B; the dependency and LCV( D) prevents control the dimension of these nodes In contrast, message AD(~) messages. The from summing bears no extra on a is eliminated by summing on A(d) CP(dlb,c,f)~D(b,u)~D(clu) . b,c 1 (35) The above equations indicate that every node conditioned at “real” node A and is, roughly from every phantom node A*, conditioned fusion a message 7rn (6, a), “proportional” to P(u), speaking, proportional on A receives a message and for and a message To (clu), to P(u), (35), on a. Eqs. (26) and that originated a message instance, conditioned on a. In the implementation of the algorithm, a node need not care about which message implementations to u” and which one is “conditioned for Am, “proportional different are formally equivalent to Eq. ( 10) for the polytree: corresponding four functions to a loop are generally arrays instead of vectors. As a consequence, (or four message handlers, in object-oriented programming) Ar(x; z) and Ay(xIz). Eqs. (14), the only difference on a”. It is thus not necessary is to have (17) and (34) is that messages only are required: r(X), A(X), MY Nevertheless, and AY(W.~ the distinction between “proportional to P(u)” and “conditioned on u” is necessary methods, although describing from a mathematical point of view. Other presentations correct algorithms, fail to define of conditioning the r- and A-messages 3 Notation: T(X) can represent a vector T(X) as well as an array such as ~(x,alh). 12 properly. P(ble) EJ. Diez/Artijicial Intelligence 87 (1996) I-20 In the case of the example above, [ 231 would give the correct formula for but, instead of expressing Eq. (30) accurately, it would read P(cle) =ru~7T(c,a)A(cJN). In turn, [o] contains incorrect expressions, such as P(s,e) = C7r(XlU)h(X~U). 4. Two versions of local conditioning Local conditioning, and structural, which have been described now to the example it is possible phases, same time as it finds the associated in Fig. 2 to demonstrate to extend tree. as any other exact algorithm, consists of two processes, numerical in Sections 2 and 3 respectively. We return two independent that, instead of having the DFS algorithm so that it propagates evidence at the Starting at the same pivot node A as in Section 2. I and crossing the edges in the same requests AB (A), A(; (B) , ro ( F) , AH(F) - immediately successively order, the integrated algorithm returns vector AH( f)--71~( D), %-D(C) and %-c(A). Node A is which marked as “visited”. Therefore edge AC is removed, with the consequent update of the conditioning In the implementation, they were just a crutch for deriving lists of some nodes, and message v(C) to create phantom nodes: turns out to be n-(clu). it is not necessary The following is to compute AE(C) is also a visited node, every node in the new loop adds E to its list of conditioning variables; message AF( E) is empty because of the removal of edge EF. Subsequently, messages AE(c; e), to the pivot node.4 As G n-D(c,e]ci), goes on does not belong with 7ro(flu), to the loop broken by E, G can sum over e and propagation and AF( E). Since F propagate backwards and ~(f,ela) and As(a). r~(d,ela) Ao(bla) the formulas. step This first pass generates an associated tree. During through every edge of the tree in the reverse direction; all the information tively equivalent algorithms. to collect evidence and distribute evidence to compute necessary its probability. These the second pass, a message passes this way, every node receives two passes are respec- in other Bayesian networks The main disadvantage using depth-first search, in which there is hardly any room for applying heuristics, of using only one evaluation of a Bayesian network or because use the integrated version of the integrated version of LC is that it finds the cutset by instead in Section 2.2. However, when the network will change to the much more efficient algorithms discussed (Section 5.2)-it may compensate time in finding an efficient for a specific query instead of spending is required-because it was generated tree. 4 Note that e and (1 play different roles in these equations because evidence is traveling from a “real” node E and from a phantom node A*. 13 EJ. Diez/Artificial 87 I-20 A C (1996) Intelligence E 22zc . D B F Fig. 4. The square ladder contains N squares and 2N + 2 nodes. On the other hand, a distributed implementation of local conditioning the associated tree, so that evidence propagation has to pre- can begin at every This allows a parallel for every node, as was possible and conditioning for distributing implementation for polytrees of local conditioning [9,21]. The combina- is discussed evidence propagation viously generate end node simultaneously. with a processor tion of clustering in [ 13,241. 5. Comparison to other methods 5.1. Conditioning algorithms than KC because [ 231 knot conditioning (GC), Peot and Shachter’s Section 1.2 was an overview of three conditioning methods: Pearl’s [ 221 global condi- (KC) and local conditioning the knot. For a knot consisting of several adjacent it does not accumulate the latter is than the former. In the same way, LC is at least as efficient as KC cutset, LC never conditions on that cutset the loops, LC is much of the different tioning (LC). For a given network, KC is at least as efficient as GC; in general, much more efficient since, given a knot and its corresponding variables outside more efficient loops that form the knot. the square In KC, the whole network 2N + 2 is the number of nodes; as a consequence, that every node belongs with n. In contrast, given in the ladder, LC achieves of the number of squares ladder The same difference holds for the diamond for many other networks loops. Recently, Darwiche the complexity grows exponentially independently to at most two loops, for this structure. and loops. on the cutset, whose size is O(n), where n = linear complexity (see Fig. A. 1 in the Appendix) (Fig. 4) is a single knot containing N adjacent a new method based on relevant ladder is conditioned formed by adjacent [6] has proposed the conditioning For instance, local A(x), nr( X) and for r(x), the algorithms he proposes for finding relevant cutsets algorithm. that ruin the performance of his conditioning and results cutsets; Rs, R,, Rf XY and Riy are the relevant cutsets Ay (X) respectively. Unfortunately, produce very ineffective An example in [6] applies a cutset G actually necessary. For binary variables, message optimal cutset used in every computation ?TX( us, ui ) 25 times. Local cutsets are a partial that it is possible containing it amounts indicates to retrieve from a cache the message r(us) of TX( us, ui ) ; but again, a suboptimal local cutset results in six variables where only one was every to requesting an to this problem: and computing remedy I-1 computing request. W. Diez/Ar~ijicid lnrellipm? X7 (I 996) I-20 r( u?) several times instead of just retrieving the value obtained for the first of finding an associated The problem the tightest cutsets solved by considering (for a certain global cutset) 2.2 showed how to assign a list of conditioning can be tree with phantom nodes, instead of absorbing arcs: to each Section variables link XY. But at most, on LCV(X) f.CV( X) iJ {X}, and any message arriving at node Y is summed over the variables not in LCV( Y) U {Y). Therefore LC is implicitly using the tightest relevant cutsets included Rsy and R& searched by Darwiche: at each node, because any message originating to have a list of conditioning variables LCV(XY) in local conditioning at X is conditioned, it suffices Rjzy = R,, =LCV(XY) = (LCV(X) c! {X)) n (LCV(Y) U {Y}). (36) Clearly, With the distinction regard of n-(x-) are conditioned conditioning /\(.r) leads to5 affecting between Ri, and R,, was not necessary. that messages involved indicates to Rz, E q. (7) on U,LCV( L/ix); on the other hand, r(x) the portion of the network below X. A similar in the computation only needs reasoning the for In contrast, LCV(X) = ULCV(U,X) UULCViXY,) i. .i 1 \{X}. (37) (38) Since R,y C LCV(X)-it cutset may reduce In Fig. 3, for instance, LCV( D) = {A}, while R, = 8: thus Eqs. (26) and (29) should be replaced to differentiate variables the number of conditioning between R,f and R,--this in n-(X) and A(X). is not necessary by r(d) = c P(QJ, rr.h.1.. f = Bohr, P(rlle) c,J‘)~l,(b,n)~o(cja)?Tg(f), iw (40) which require fewer computations. Therefore, a worthy refinement of local conditioning based on Darwiche’s work consists of using R,, = LCV(XY) rithms variables in Section 2) and Rx (given by Eq. (37) ) instead of using a list of conditioning for each node. (assigned by the algo- In summary, the performance how explained _- the main problem of the algorithm. When using in [6] was that suboptimal cutsets drastically degraded (we have as local relevant cutsets information the tightest essentially the same it computes to find them), 5 Variable: X does not belong to Xi and Rx because relevant cutsets are meant to only keep track of the extra conditioning for rr(X) and h(X). EJ. Diez/Artijcial Intelligence 87 (1996) l-20 15 conditioning, The only difference variables, Darwiche’s instantiations of the relevant cutset. with relevant cutsets playing is that, instead of propagating messages algorithm requests every message several the role of lists of conditioning conditioned variables. on several times with different 5.2. Clustering methods The most widely used algorithm for multiply-connected networks [ 14,191. The structural phase, sometimes called “compilation is clique-tree prop- of the network”, agation consists of triangulating the crucial step that determines is NP-hard to find an optimal clique Shachter, Andersen and Szolovits tree [ 291. [ 241 demonstrated the graph and forming a tree of cliques. The triangulation the efficiency of the numerical phase. Unfortunately, is it in a specific cluster probability to computing triangulation. phantom alent to using a particular is formed by removing Ci containing {Xi} U pa(Xi) U LCV( Xi). An empirical rithms search in Section 2 can produce cluster [ 16,181 or simulated annealing In variable Xi, its parent variables the equivalent local conditioning, that global conditioning is equiv- tree, or put another way, equivalent tree every node Xi with a cluster variables: Ci = if the algo- trees as efficient as those obtained by heuristic [ 17,291. should determine its conditioning comparison cluster and nodes and replacing In contrast, for some triangulations there is no equivalent is that it can use the most efficient has a quite restricted choice. If the network associated tree. Therefore, triangulation, while is given once and for all, an efficient cluster tree usually pays off when propagating the main advantage of clustering local conditioning the time spent evidence. in building Nevertheless, the integrated version of local conditioning ity directly on the original network and hence can save time tions: computes in the following the probabil- situa- l When a knowledge engineer is debugging a model and wants to measure the impact of some modifications. l When there is sparce evidence. for each patient and doctors usually provide In medicine, for example, only a few tests are the expert system with only findings of those tests. Thus the pruning of unobserved performed a small part of the potential nodes reduces the size of the network and may eliminate many l When the network is generated dynamically. For instance, is the probability through AND/OR retrieval in which applications [ 111, information [ 11. loops a Bayesian network of P( (x2 V x3) A X6) ?’ by [ 22, pp. 223ff]. Nat- gates and temporal [ lo], planning usually generate a specific interacting are some of the fields can answer queries such as “What introducing new nodes ural language processing reasoning network In addition, ization directly: conditioning. case of a polytree, explanation for each situation. local conditioning the expressions Instead, clustering of reasoning. reducing is able to treat the AND/OR gates and their general- for Q, r and A in [7,9] do not change when applying algorithms have to add dummy nodes [ 121 even in the the and hindering the efficiency of evidence propagation 16 EJ. Diez/Art~ficiul Intelligence 87 (1996) I-20 6. Implementation DIAVAL [ 81 is a prototype expert system for echocardiography whose network con- tains over 200 nodes and several loops. The computation of evidence using the integrated computer. This version of local conditioning is a satisfactory in Com- mon Lisp without much concern reduced when the algorithm neglects since C++ time is significantly leaf nodes for which there is no observed evidence, in it usually eliminates is expected takes less than 5 seconds on a 486/66Mz that the current version was implemented to achieve a much higher performance. for efficiency. The computational number of loops. A future result, considering implementation a considerable 7. Conclusions and future work We have introduced local conditioning networks, discussing the algorithm is already for real-world applications the advantages as a new exact algorithm and disadvantages in for inference of its two versions. the assessment requires an empirical comparison of at least in use in a prototype expert system, Bayesian Although of its suitability three methods: ( I ) The integrated version of local conditioning, which builds the tree as it propagates evidence (Section 4). (2) Local conditioning with different heuristic algorithms for building associated trees. f 3) Clustering With regard conjecture that future research (Section 2.2), or design new algorithms above, every associated to the second method, it is NP-hard should either extend As mentioned algorithms with different triangulation techniques it would be of interest to find an optimal associated the algorithms developed [ 16- 18,291. to prove or refute tree. If it is confirmed, for global conditioning the triangulation), of a network but the opposite is not equivalent to a certain triangulation techniques using local conditioning for finding near-optimal triangulations based on heuristic algorithms (second method). for local conditioning. suitable tree is equivalent is not always to any associated to a specific cluster true. Therefore, tree (i.e. if the optimal algorithms (third method) will outperform tree, clustering the question local conditioning evidence based on depth-first search (first method) has in the original network, without any previous pro- it coincides with Kim In the presence of tree On the other hand, the virtue of propagating In a singly-connected cessing. and Pearl’s polytree algorithm loops, is whether or an efficient cluster of probability. Two determinant factors are the number and length of the loops and the a great amount of available evidence, number of loops when there are few findings. The use of AND/OR gates in the model is an additional portion of a Bayesian network, [ 15,211, the time spent in searching tree will compensate when performing an efficient associated the numerical the pruning of the network may eliminate the most efficient solution. to use local conditioning. computation reason since EJ. Diez/Artificial Intelligence 87 (1996) I-20 17 Acknowledgements of California, Los Angeles, CA, with research A first version of this paper was written at the Department of Computer Science, University facilities provided by Pro- fessor Pearl, and appeared as Technical Report R-18 1, Cognitive Systems Laboratory, in July 1992. The work was directed by Professor Jose Mira as thesis advisor UCLA, Investi- and supported by a grant gador of the Spanish Ministry of Education to thank Judea Pearl, Mark Peot, Ross Shachter and an MoisCs Goldzsmidt, David Heckerman, anonymous de Personal and Science. The author wishes for helpful discussions. the Plan National de Formation referee from Appendix A. The complexity of exact algorithms 51, no algorithm Nevertheless, much on its topology the diamond ladder A possible way Since probabilistic inference is NP-hard for multiply-connected is expected to have polynomial time complexity for a particular network, the complexity Bayesian networks [ 4, for the general case. of exact algorithms depends as as on the number of nodes. The study of a particular structure, (Fig. A.l), will illustrate to form the associated this point. in the order Do, BI, Dl, Cl, B2, etc. For the ith diamond, 1 < i < N; DFS can produce the network %-Q (Ci) are conditioned not accumulate. additions, multiplications of binary variables. Therefore, computation number of nodes is n = 3N+l of P( Do(e) Table A.1 displays on Di_1, but A (Di) tree link Di-1 Ci, for is to remove every this tree by choosing DO as the pivot node and covering ro;( Bi) and does (namely in the case is 123 for every diamond. The the the number of mathematical required the number of operations is simply a vector, so conditioning If the ladder has N diamonds, for every expression, operations takes five more operations. and a few divisions) is 123N+5 = 41n-36. For this example, clustering methods have the same order of complexity. Suermondt of the network, clique-tree and the total number of operations the compilation [27] calculated needs 330N + 111 = 110n - 119 operations that, besides although this is likely an overestimation. [ 241 needs only 84N + 5 = 28n - 33 operations, The clustering for evidence propagation in this algorithm proposed as can be deduced by examining and Cooper propagation structure, in the Mij-messages. In order the complexity to highlight how much of exact algorithms depends on the the addition of a link from DO to DN. The number of topology of the network, consider nodes remains the same and the number of edges varies only from 4N to 4N + 1, but the complexity of LC is almost double, because every node Bi and Di will bear an extra of the augmented graph may lead to the presence of conditioning the entire network on DO [ 241. DO in every clique, which or a different exact algorithm It does not seem plausible the addition of the new arc raises might produce a significantly of the algorithm employed. the intrinsic better result. Apparently, independently on DO. A triangulation of the network, that a different to conditioning is equivalent triangulation complexity In contrast, the addition of a new link barely increases the complexity of an approxi- mate method and such an increase is the same for link DoDl as for link DoDN. EJ. Diez/Art$ciul Intelligence 87 (1996) l-20 Fig. A. I, Diamond ladder structure. Table A.1 Number of mathematical operations required From D,*_, towards D;_ I # of operations required r(cil4-I 1 ~D,(4& 1 An, (h;lcl,- I ) AR. ((Ii- I ) 0 6 IO 6 From D;_l towards D,*_, # of operations required a&(4-l) r Cd 1 AD,(Ci;di-I) Probability P(bile) P ( 4 I e ) P(cile) Total 4 46 28 # of operations required 9 5 9 123 IV. Diez/Arttj?cial Intelligence 87 (1996) I-20 19 References 1 I ] M. Baker and T.E. Boult, Pruning Bayesian networks in: PP. Bonissone, M. Hemion, L.N. Kanal and J.F. Lemmer, eds., Uncertainty in Artificial Intelligence 6 (Elsevier Science Publishers, Amsterdam, for efficient computation, 1991) 225-232. [ 2 1 R. Bar-Yehuda, D. Geiger, J. Naor and R. Roth, Approximation satisfaction and Bayesian alaorithms inference, for the vertex feedback set in: Proceedings 5th Annual to constraint problem with applications ACM-SIAM Symposium On Discrete Algorithms, Arlington, VA ( 1994) 344-354. A. Becker and D. Geiger, Approximation for the loop cutset problem, Conference on Uncertainty in Artijicial Intelligence, Seattle, WA (Morgan Kaufmann, San Francisco, CA, 1994) 60-68. in: Proceedings I&h algorithms 13 complexity of probabilistic inference using Bayesian belief networks, 14 G.F. Cooper, The computational Artq: Intell. 42 ( 1990) 393-405. [ 5 1 F’. Dagum and M. Luby, Approximating probabilistic inference in Bayesian belief networks is NP-hard, Arti$ Infell. 60 (1993) 141-153. 161 A. Darwiche, Conditioning algorithms for exact and approximate inference in causal in: Proceedings 11th Conference on Uncertainty in Artificial Intelligence, Montreal, Que. Kaufmann, San Francisco, CA, 1995) 99-107. networks, (Morgan 17 1 F.J. Diez, Parameter adjustement in: Proceedings 9th Conference on Uncertainty in Arti$cial Intelligence, Washington, DC (Morgan Kaufmann, San Mateo, CA, 1993) 99-105. in Bayes networks. The generalized noisy OR-gate, para ecocardiografia, PhD thesis, Departamento lnformatica y in Bayesian networks, Cybern. Sysr. 25 (1994) 39-61. to information retrieval, Commun. ACM 38 181 F.J. Diez, Sistema expert0 Bayesiano ( 1994). Automatica, UNED, Madrid 19 I EJ. Diez and J. Mira, Distributed [ I01 R. Fung and B. del Favero, Applying Bayesian networks inference (1995) 42-48, 57. Ill II2 I R.P Goldman and E. Chamiak, Probabilistic I D. Heckerman, Causal independence text understanding, Statist. Comput. 2 ( 1992) 105-I 14. in: Proceedings 9th Conference on Uncertainty in Arfi@ial Intelligence, Washington, DC (Morgan Kaufmann, San Mateo, CA, 1993) 122-127. for knowledge acquisition inference, and 113 1 M. Hede and R. Stigaard, Distributed Hugin-an approach networks, Master’s (1994). thesis, Department of Mathematics to distributed in large Bayesian inference and Computer Science, Aalborg University [ 141 F.V. Jensen, K.G. Olesen and S.K. Andetsen, An algebra of Bayesian belief universes for knowledge- based systems, Networks 20 ( 1990) 637-660. I 151 J.H. Kim and J. Pearl, A computational model for combined causal and diagnostic reasoning in inference systems, in: Proceedings IJCAI-83, Karlsruhe ( 1983) 190-193. I I61 U. Kjsrulff, Graph University of Aalbotg triangulation-algorithms giving small total state space, Tech. Rept. R-90-09, ( 1990). [ I7 1 U. Kjaerulff, Optimal decomposition of probabilistic networks by simulated annealing, Sfatist. Compur. 2 (1992) 7-17. [ I8 1 A. Kong, Efficient methods for computing linkage likelihoods of recessive diseases in inbred pedigrees, Genetic Epidem. 8 (1991) 81-103. [ I91 S.L. Lauritzen and D.J. Spiegelhalter, Local computations with probabilities on graphical structures and their application to expert systems, J. Roy. Statist. Sot. Ser. B 50 ( 1988) 157-224. ( 201 R.E. Neapolitan, Probabilistic Reasoning in Expert Systems: Theory and Algorifhms (Wiley/lnterscience, New York, 1990). [ 21 I J. Pearl, Fusion, propagation [22] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference in belief networks, A@ Intell. 29 (1986) 241-288. and structuring (Morgan Kaufmann, San Mateo, CA, 1988; revised 2nd printing, 1991). [ 23 I M.A. Peot and R.D. Shachter, Fusion and propagation with multiple observations in belief networks, Art$ Intell. 48 (1991) 299-318. 20 FIJ. L)iez/Art&wl Intellipwcr X7 (1996) l-20 (24 1 K.D. Shachter, S.K. Andersen and P. Szolovits. Global conditioning in Artijiciul in: Proceedings 10th Conference OH Lincertuirrty networks, for probabilistic inference in belief Intelligence. Seattle. WA (Morgan Kaufmann, San Francisco, CA, 1994) Sl4-522. 125 I J. Stillman, On heuristics for finding loop cutsets in multiply connected belief networks. in: P.P. Bonissone, M. Henrion, L.N. Kanal and J.F. Lemmer. eds., Uncertclinty in Artificial fntelfigence 6 (Elsevier Science Publishers, Amsterdam, 1991) 233-243. [ 26 I H.J. Suennondt and G.F. Cooper, Probabilistic inference in multiply connected belief networks using loop cutsets, fnternat. J. Approx. Reawzing 4 ( 1990) 283-306. I27 I H.J. Suermondt and G.F. Cooper, A combination of exact algorithms for inference on Bayesian belief Internat. networks, 128 I H.J. Suermondt networks, Artif: J. Approx. Reusoning 5 ( I99 I ) 521-542. Initialization and G.F. Cooper. for Intell. 50 ( 1991) 83-94. the method of conditioning in Bayesian belief I29 1 W.X. Wen. Optimal decomposition of belief networks, in: PP. Bonissone, M. Henrion, L.N. Kanal and J.F. Lemmer, eds., Uncertcrinty UI Artificial I99 1) 209-224. fntelli,qence 6 (Elsevier Science Publishers, Amsterdam, 