Artificial Intelligence 106 (1998) 109-137 Artificial Intelligence Uncertainty measures of rough set prediction Ivo Diintsch a~b~*~2, Giinther Gediga btc* 1$2 a School of Inforrnarion and Sofbvare Engineering, University of Ulster; Newtownabbey BT 37 OQB, UK b Instirut fir Semantische Infomationsverarbeitung, Universitiit Osnabriick, 49069 Osmbriick, Germany ’ FB PsychologieMethodenlehre, 49069 Osnabriick, Germany Universitiit Osmbtick, Received 14 July 1997; received in revised form 19 March 1998 Abstract The main statistics used in rough set data analysis, the approximation quality, is of limited value when there is a choice of competing models for predicting a decision variable. In keeping within the rough set phillosophy of non-invasive data analysis, we present three model selection criteria, using information length principle. Our main in the spirit of the minimum description procedure is based on the principle of indifference combined with the maximum entropy principle, to a minimum. The applicability of the proposed method thus keeping external model assumptions is demonstrated by a comparison of its error rates with results of C4.5, using 14 published data sets. @ 1998 Elsevier Science B.V. All rights reserved. theoretic entropy Keywords: Rough set model; Minimum description length principle; Attribute prediction 1. Introduction Most of the commonly used procedures the observed phenomena, or presuppose and are subject analysis, to random influences, regression, or correlation may be applied. for data prediction require parameters outside character in order that statistical methods such as variance that the properties are of a quantitative One methiod which avoids external parameters (RSDA); it has been developed by Z. Pawlak and his co-workers since the early 1970s [15,16,20,21], and has recently received wider attention as a means of data analysis [23]. The rationale of the rough set model is the observation is rough set dutu analysis that author. Email i.duentsch@ulst.ac.uk. * Corresponding 1 Email: ggetiga@Luce.Psycho.Uni-Osnabrueck.DE. implied. 2 Equal authorship 0004-3702/98/,$ PII: SOOO4-3702(98)00091-5 - see front matter 0 1998 Elsevier Science B.V. All rights reserved. 110 I. Diintsch, G. Gediga /Artijicial Intelligence 106 (1998) 109-137 “The imprecision granularity ambiguity information about a decision from many is usually vague because of uncertainty and . Vagueness may be caused by an of the information. Granularity may sources . . . coming of representation to explanation or prescription based on vague information” introduce [24]. In other words, the original concept behind be described “roughly”: an object has a property the model is the realization that sets can only 0 CERTAINLY, l POSSIBLY, 0 CERTAINLY NOT. function internal knowledge, This looks conspicuously logical level, we can say that the algebraic semantic of a rough set logic corresponds fuzzy logic with a three-valued membership like a fuzzy membership function, and indeed, on the algebraic- (see [8,19]). to a Rough set analysis uses only and does not rely on prior model as fuzzy set methods or probabilistic models do. In other words, instead of assumptions rough set analysis utilizes solely using external numbers or other additional parameters, the granularity structure of the given data, expressed as classes of suitable equivalence relations. Of course, this does not mean that RSDA does not have any model assumptions; for example, we indicate below that the statistical model behind RSDA is the pn’nciple of indifference. However, model assumptions ignorance of what happens within the region of indiscernibility, (see Section 2.1). given by the granularity of information are such that we admit complete The results of RSDA must be seen with this background as possible in mind: the rough set model from the structural aspects of the data, of the attribute and can serve as a valuable information and other contextual to a minimum, tries to extract as much information neglecting, domains. This keeps model assumptions indicator of the direction in its pure form, numerical into which possible further analysis can go. The relationship between RSDA and statistical modeling is quite complementary (see Table l), and we have discussed it in more detail in [lo]. Knowledge representation are a tabular form of an OBJECT -+ ATTRIBUTE VALUE relationship, databases (see Section 2.2). in the rough set model is done via information systems which similar to relational If Q is a set of predictor features and d a decision attribute, then RSDA generates rules of the form xq=mq~xd=m~Vxd=m~V~~~Vxd=m~, A qEQ where X’ is the attribute value of object n with respect to attribute r. (1.1) Table 1 RSDA versus statistical modeling RSDA Statistical models Many features/attributes, few data points Few variables, many data points Describing redundancy Top down, reducing the full attribute set Reducing uncertainty Bottom up, introducing new variables I. Diintsch, G. Gediga /Artijcial Intelligence 106 (1998) 109-137 111 We see that in the rough set model that on the right hand side of (1.1) we can have a proper disjunction. If there is only on term on the right hand side, we call the rule deterministic. Whereas RSDA handles rules deterministic remains unclear. the status of the indeterministic in a straightforward manner, rules can be indeterministic in the sense rules If rules are based on a few observations only, the granularity of the system is too high, and the rule may be due to chance. In order to test the significance of rules, one can use that randomization methods the null hypothesis the conditional probability of the rule, assuming to compute “objects are randomly assigned to decision classes” two simple procedures, both based on randomization is true. In [I 11 we have developed techniques, which evaluate the validity of prediction based on the principle of indifference, which Section 2.4. statistics of RSDA; is briefly described is the underlying technique this in Although randomization methods are quite useful, they are rather expensive in resources, and are only applicable as a conditional testing scheme: a though they tell us when a rule may be due to chance, they do not provide us with a metric :for the comparison of two different rules Q + d, R-+ d, let alone for different models of uncertainty. Thus, we need a different criterion for model selection: the minimum description length (MDLP) (see [27,28]) states that the best theory to explain a given phenomenon principle d is one which minimizes the sum of l the binary l the binary predictor. length of encoding a hypothesis Q and length of encoding the decision data d using the hypothesis Q as a In the sequel, we present on three different probability distributions frame M, the attractiveness of this approach rules such as (1.1) is considered the aggregate of the three different ways of model selection within RSDA, based in the spirit of the MDLP. Within each model about the uncertainty of in a context where the selection criterion H M (Q + d) is is that information l effort of coding a hypothesis Q, expressed by an entropy function H(Q), and l uncertainty to classify a randomly chosen observation given this hypothesis, expressed as a suitable entropy in terms of the optimal number of decisions of “guessing” H”(d I Q>. the basic tools of RSDA and The paper i,s organized as follows: their main properties, as well as our usage of the entropy functions. Section 3 contains our three approaches to some well known data sets. Finally, Section 5 consists of a summary and an outlook. and Section 4 applies our main approach in Section 2 we describe to uncertainty, 112 I. Diintsch, G. Gediga /Artijicial Intelligence 106 (1998) IO%137 2. Basic tools and constructions 2. I. Approximation spaces An equivalence 8 on a set U is a transitive, reflexive, and symmetric binary relation, and we call the pair (U, 0) an approximation space. In our context, we shall sometimes call an equivalence spaces are the core mathematical of information the idea that granulation relation. relation an indiscernibility relation. Approximation can be described by classes of an indiscernibility concept of RSDA, and their usage reflects Recall that a partition P of a set U is a family of nonempty, pairwise disjoint subsets of relation 8 we associate a partition PO of U by that a, b E U are in the same class of PQ, if and only if aOb. The classes of PO U whose union is U. With each equivalence specifying have the form @a = {b E U: aeb}. By some abuse of language, we also speak of the classes of an equivalence we mean the classes of its associated partition, and call 8a the class ofa mod&o 8. relation when The interpretation only up to membership limited to the classes of 6’ and their unions. This leads to the following definition: in rough set theory is that our knowledge of the objects in U extends in the classes of 8, and our knowledge about a subset X of U is For X C U, we say that is the lower approximation or positive region of X, and XEf U{ex: x E xl is the upper approximation or possible region of X. If X C U is given by a predicate P and x E U, then (1) x E X means that x certainly has property P, (2) x E x means that x possibly has property P, (3) x E U \ x means that x definitely does not have property P. The area of uncertainty extends over X\X, and the area of certainty is XU-x. 2.2. Information systems Knowledge representation in RSDA is done via relational tables. An information system z = (UT a, vq, fq&? consists of: I. Diintsch, G. Gediga /Artificial Intelligence 106 (1998) 109-137 113 Boundary of Set X Lower approximation of X _...;:;.. .: . . . . . . . .._.. _. :.:.:/~.~..: Difference of upper and lower approximation of X Fig. 1. Rough approximation. (1) a finite set U of objects; (2) a finite set 0 of attributes; (3) for each q E i2 l a set V, of attribute values, l an information function f4 : U + V,. In the sequel we shall use Z as above as a generic P, Q, R G 0. We also will sometimes write x4 instead of &(x) x with respect to attribute q. Furthermore, we suppose which we want to predict with attribute sets Q, R G $2. information system with 1 U 1 = n and the value of to denote that d E fZ is a decision attribute Example 1. We use the small information the various concepts developed illustrate shall be predicted from two variables “Smoker” system given in Table 2 as a running example to in the sequel. An attribute “Heart Disease” (HD) (S) and “Body Mass Index” @MI). With each Q we associate an equivalence relation 8Q on U by defining and the partition induced by 8Q is denoted by P(6Q) or simply by P(Q). 114 I. Diintsch, G. Gediga /Artz$cial Intelligence IO6 (1998) 109-137 Table 2 An example of an information system No 1 2 3 4 5 6 I 8 9 S no no no no yes yes yes yes no BM HD normal obese normal obese normal normal obese obese normal no no no no yes yes no yes no The interpretation of @Q is the following: if our view of the world U is limited to the attributes given by Q, then we will not be able to distinguish objects within the equivalence Classes Of 8Q. Example 1 (Continued). The classes of 8Q and 6d are as follows: Q iSI Classes of 6 {1,2,3,4,91, (5,6,7,81 WW 11,3,5,6,9), (2,4,7,81 IS, BMIl 11.3,91. (2,413 15,61, (7,8) d IHDI 11,2,3,47,91, 15.6.81 We can now use the definition of upper, respectively, lower approximation of sets via 8Q defined in the previous section. It is not hard to see that for Y c U, u’=[xEU: 8QxnY#!d} is the upper approximation of Y with respect to Q, and IQ = {x E u: 8Q” c Y) (2.1) (2.2) is the lower approximation or 1. of Y with respect to Q. If Q is understood, we just write r The equivalence Let Q + d G P(Q) x ‘P(d) be the relation relations 8Q are used to obtain rules in the following way: (X,Y)EQ+~&$X~Y -Q . Observe that by (2.1) XcyQ ifandonlyif XOyQ#O ifandonlyif XflY#0, I. Diintsch, G. Gediga /Artificial Intelligence 106 (1998) 109-137 115 and thus, (X,Y)~Q+dexnY#0. (2.3) Observe that we can determine with the knowledge gained from Q whether X n Y = 0 and also-by (2.2)-whether X C Y A pair (X, Y) E Q + d is called a Q,d-rule (or just a rule, if Q and d are understood), it as X + Y. By some abuse of language we shall also call Q + d a rule identify singleton sets with the element usually written when there i,s no danger of confusion, and normally they contain. If (X, Y) E Q + d, then X corresponds to the left hand side of the implication (1. l), and Y corresponds to (one of) the disjuncts of the right hand side. Example 1 (Continued). The rule S -+ HD consists of the pairs (11,2,3,4,91, 11,2,3,4,7,91), ({5,6,7,g), 11,2,3,4,7,91), (15,6,7,gl, {5,6, gl), BMZ + HD has the pairs (11,3,5,6,9), {1,2,3,4,7,91), (Il, 3,5,&S), {5,6, gl), ({2,4,7,gl, 11,2,3,4,7,91), ({2,4,7,g), {5,6, g)), and for {S, 13MZ) -+ HD we obtain ({1,3,91, 11>2,3,4,7,91), (I2,41, 11,2,3,4,7,9)), ({5,61, {5,6, g)), ({7,gl, 11,2,3,4,7,91), (17,g}, (5,681). The deterministic-orfunctional-part of Q + d, written as Q 2 d, is the set {(X, I’) E Q + d: X s Y}. If (X, Y) E Q 2 d, then the class X is called d-deterministic or just deterministic, if d is understood. the values of x with respect to the attribute values of d. In this case, the values of each x E U on the attributes in Q uniquely determine Example 1 (Continued). The deterministic (5,6); deterministic the only deterministic class of (BMZ) + HD. classes of (S, BMZ) + HD are (1,3,9), class of (S) + HD is ( 1,2,3,4,9), and there {2,4), is no 116 I. Diintsch, G. Gediga /Artificial Intelligence 106 (1998) 109-137 If Q -+ d = Q 2 d, i.e., if Q --+ d is a function, then we call Q + d deterministic and write Q =+ d; in this case, we say that d is dependent on Q. It is not hard to see that Q j d if and only if 8Q G ed, so that our terminology is in line with the usual convention in RSDA. A special role will be played by the deterministic part of Q + d, and we define vQ+d %if u {x E P(Q): (x, Y) E Q 2 d}. In other words, vQ+d understood, we shall just write V instead of vQ+d. Note that the union of all d-detetministic is eQ classes. If Q -+ d is n-/VI=0 or n-IVI>2, (2.4) since every singleton class of 6Q is deterministic definable (or just dejinable, if Q is understood), for any d. A class Y of ed is called Q- if Y C V. Example 1 (Continued). The deterministic parts are easily seen to be VS-+HD = {1,2,3,4,91, VBMI-+HD = 0, VIXBMIJ+HD = {I, 2,3,4,5,6,9}. Even though RSDA is a symbolic method, it implicitly makes statistical assumptions which we briefly want to describe, and we start by looking at a single equivalence on U. The inherent metric of an approximation relation 8 system (U, 0) is the approximation quality ye(x) def l&l ;;p"l, (2.5) [22, p. 16ffl. If 0 is understood, we shall usually omit the subscripts. The value y(X) is the relative frequency of objects of U which can be correctly classified for systems [22, p. 221; we choose a different (but equivalent) definition which is quality of Q with respect with the knowledge given by 8 as being in X or not. The function y can be generalized information more suited for our purpose. As a measure of the approximation to d, we define an approximationfunction by y(Q --, d) = 1 U(X l P(Q): X is d-deterministic}] IUI (2.6) Note that v(Q+4=IUl> IVI and Q=kd ifandonlyif y(Q+d)=l. Example 1 (Continued). We see that YS+HD = $, YBkf+ HD = $, Y{S,BMI}+HD = 5. I. Diintsch, G. Gediga /Art@ial Intelligence 106 (1998) 109-137 117 It is not hard to see that the statistical principle underlying the approximation functions is the principle of indifference: l if one does not have any information to be equally assumed likely. about the occurrence of basic events, they are all Q is called a reduct ofd, if it is minimal with respect to the property Reducts are of particular importance in rough set theory as a means of feature reduction. that y (Q -+ d) = 1. 2.3. DataJiltering and discretization Even though RSDA has no inherent categorization mechanism, data satisfactorily in [ 121 which is based only on the information provided by the indiscernibility in several ways. One method which keeps close to a minimum continuous RSDA philosophy of keeping outside assumptions described relations. This technique collects values of a feature into a single value by taking a union in a class of the decision of determini;stic equivalence attribute; statistical basis of the rule may be enlarged, and the in Ithis way, the underlying significance of the rule is increased classes which are totally contained (see Section 2.4). it is possible to handle to the is the filtering procedure For example, if we have an attribute q and a rule then we can collect 2, 3,5 into a single attribute value of q . feature of this procedure The important is that the internal dependency Even the method procedure; is fixed, for example, system is kept intact, and that we do not need additional parameters. step can be regarded as a part of the operationahzation as a cheap standard algorithm if the decision attribute set engine GROBIAN [9]. though is simple, investigations of Browne et al. [4] and Browne [3] indicate. Nevertheless, scheme cannot cope effectively with complex other, more sophisticated, RSDA (which, however, use external parameters and restrictive modelling we invite the reader to consult Bazan [l] or Nguyen and Nguyen therein. interactions it sometimes works surprisingly well as the this discretization discretization methods do. For these methods applicable among continuous variable as in structure of the In other words, this it can be implemented in our rough assumptions) [ 181 and the references The claim that RSDA is not applicable to most real life problems, because it cannot handle continuous variables seems to us to be an open problem, but not a fact. The success of applications of fuzzy controlling, which also requires discretization of continuous data, shows that the distinction of “continuous data” versus “discrete data” does not necessarily “discrete imply types of data. We also refer the reader to Section 4 methods”, below, in wlhich the prediction quality of our RSDA based methods is explored also for data sets which consists of continuous variables. is a need for different “continuous methods”, these different respectively, that there to handle 118 I. Diintsch, G. Gediga /Art$cial Intelligence IO6 (1998) 109-137 2.4. Signijkance testing Suppose that we want to test the statistical significance of the rule Q + d. Let _X be the set of all permutations of U. For each cr E C, we define a new set of feature vectors Xf by a(~)~, X’, if r =d, otherwise. (2.7) the xd values according In this way, we permute constant. The resulting distribution {v( Q + a(d)): CT E E) to evaluate the strength of the prediction Q + d. The value p ( y ( Q -+ d) 1 Ho) measures quality and it is defined by rule system is denoted by Q + c(d). We now use the permutation the extremeness of the observed approximation leaving everything to r~, while else p(v(Q + 4 I Ho) := lb E x: v<Q + a(d)) 2 v<Q + 411 IUI! (2.8) If cx = p(y (Q --+ d) 1 Ho) is low, traditionally below 5%, we reject the null hypothesis, and to reject the null hypothesis call the rule signzjkant, otherwise, we call it casual. Failure does not mean that it is true, and thus, such randomization tests are a necessary condition for significance (for a discussion, see [6]). Randomization is a statistical technique which does not require a representative from a population which is a theoretical generalization sampling study, because the randomization well in accord with our stated objective. This aspect is in contrast techniques. Even technique needs some parametric one has to suppose estimators of the latent probabilities of the equivalence classes in the population. of the sample under procedure uses only information within the given sample, to most other statistical because classes are suitable that the percentages of the observed equivalence the bootstrap assumptions, Example 1 (Continued). Table 3 tells us the approximation of the sets (S}, (BMZ} and {S, BMZ) for the prediction of HD for the example system of Table 2. qualities and the significance information The best approximation quality is attained by the combination significance in terms of statistical attributes S and BMZ. However, not a significant predictor prediction {S} is smaller than that of {S, HD}, the set {S} should be preferred it is unlikely for the outcome of HD, because success is not due to chance. Therefore, although success is due to chance. that the prediction of both predicting the set (S, BMZ} is that the quality of to predict HD, because there is no evidence the approximation Table 3 Approximation quality and significance of predicting attributes Attribute set )/ Significance Interpretation 0.556 0.000 0.778 0.047 l.ocm 0.144 Not casual ((Y = 5%) Casual ((u = 5%) Casual ((Y = 5%) I. Diintsch, G. Gediga /Art$cial Intelligence 106 (1998) 109-137 119 In most applications one can observe that there are several reducts or attribute sets with an acceptable approximation about their statistical validity, but there are often several sets with comparable good statistical the foundations of which quality. Thus, we need an additional criterion for model selection, will be laid in the next section. testing gives some information quality. Significance 2.5. Partitions and information measures Let P be a partition of U with classes Xi, i < k, each having cardinality compliance with the statistical assumption elements of U are randomly distributed within of an element x being in class Xi is just ri /n. We define the entropy of P by of the rough set model we assume the classes of P, so that the probability r-i. In that the H(P)%+~log,(;). i=o I (2.9) If 8 is an equivalence instead of H(P). Furthermore, instead of H(6Q). relation on U and P its induced partition, we will also write H(8) then we usually write H(Q) if Q is a set of attributes, The entropy estimates the mean number of comparisons minimally necessary to retrieve the equivalence class information of a randomly chosen element x E U. We can also think if there is only one class, of the entropy of P as a measure of granularity of the partition: then H(P) reaches a maximum there is no information gain, since (for fixed n). In other words, with the universal there is only one class and we always guess the correct class of an element; if the partition in a specific class is hardest to predict, the inclusion of an element contains only singletons, and thus the information gain is maximized. ‘= 0, and if P corresponds tv, then H(P) to the identity relation For two partitions PI, P2 of U with associated equivalence relations 8t,&, we write Pr < P2, if 81 & 02. The following Lemma may be known: Lemma 2.1. IfPl < P2, then H(P1) 3 H(P2). Proof. Since every class of P2 is a union of classes of Pt, we can suppose without of generality associated with P2 are p1 + ~2, p3, . . . , pm. Now, loss associated with Pr are pr , . . . , pm, m > 3, and those that the probabilities = H(Pl + P2, P3,. . . , pm) + (~1 + ~2) . H Pl ___ Pl +p2’ ( P2 ___ Pl +p2 ) =H(~2)+(pl+pz)~H(--&,$-) b H(P2), see, for example, 114, p. 211. Corollary 12.2. If R C Q C 52, then H(R) < H(Q). 120 I. Diintsch, G. Gediga /Art$cial Intelligence 106 (1998) 109-137 More classes does not automatically mean higher entropy, and we need a hypothesis such as Pt < P2; for example, 1.585 = H(;, f, f) > H($ 4, $, 6) = 1.447. For later use we mention [ 14, p. 211): that the entropy function has the property of strong additivity (see Lemma 2.3. Suppose such that that (i;i: i < t} and {;li,j: j 6 ni} are sets of positive parameters CJ?i = C 6i.j = 1. i<t i<ni Then, C C Et-i I”li, j loiT2 i<t j<iai (&)=~klO&(f)+~& C fii,j10g2($) \ i<t j<n, 3. Rough set prediction The problem we want to address is a variant of the classical prediction problem: l given a decision attribute d, which is the “best” attribute set Q 2 fi to predict in Q? d-value of an object x, given the values of x under the features contained the We say “a variant”, since the RSDA rules are determined by the equivalence classes of the we are combining prediction quality partitions of U involved-see with feature reduction. (1.1) and (2.3)-and The prediction problem raises two questions: l Which subsets Q of 52 are candidates l What should a metric look like to determine and select the “best” attribute set? to be such a “best” attribute set”? the prediction the approximation quality y as defined In conventional RSDA, to describe success, which is conditional measurement by the researcher. However, approximation we use different feature sets Q and R for the prediction of d. To define an unconditional measure of prediction success, one can use the MDLP idea of combining rule in RSDA) and in (2.6) is a measure on the choice of attributes and if qualities cannot be compared, (i.e., to find a deterministic (i.e., a measure of uncertainty when applying an indeterministic l program complexity l statistical uncertainty rule) to a global measure of prediction success. In this way, dependent and independent are treated similarly. attributes In the sequel we discuss this type of uncertainty, which are based on the information-theoretic entropy functions of Section 2.5. Our model selection criterion will be an entropy value H”( Q + d) which aggregates for each set Q of attributes: three different models M to handle l the complexity of coding the hypothesis Q, measured by the entropy H(Q) of the partition of its associated equivalence relation 6Q (see (2.9)), and I. Diintsch, G. Gediga/Art$cial Intelligence 106 (1998) 109-137 121 l the conditional coding complexity H”(d ] Q) of d, given by the values of attributes in Q, so that H”((;! + d) = H(Q) + H”(d I Q). (3.1) The estimator H”(d 1 Q) measures given a class of 8Q; it is important, to the knowl’edge given by Q . the uncertainty to predict membership if we want to gauge the success of a model conditioned in a class of 6d The importance of H”( Q + d) is due to the fact that it aggregates the uncertainty i.e., the predicting elements. H”(d 1 Q) and the effort H(Q) of coding This enables, the researcher to compare different attribute sets Qi in terms of a common unit of measurement, which cannot be done by a conditional measure of prediction success like y or H”“(d ( Q). the hypothesis, Since all our entropies are defined from probability measures which arise from partitions of an n-element set, we see from the remarks after (2.9) that they have an upper bound of log+). In order to be able to compare different entropies within one model M, we define a i.e., within [0, l]-as if H(d) = log#), follows: normalized entropy measure-bounded if @d is the identity, then if8Q=&, otherwise If H (4 5 log2 (n) , s”(Q+d)‘%fl- H”(Q + d) - H(d) logz(n) - H(d) ’ (3.2) The measures SM (Q -+ d) are constructed approximation quality: in such a way that they are comparable to the l if S”( #Q + d) = 1, the entropy measure is as good as possible, whereas SM (Q -+ d) near 0 shows that the amount of coding information which indicates a poor model for predicting the attribute d. is near the theoretical maximum, Similarly, based on the bounds 0 6 H”(d ] Q) 6 log2(n), we can normalize H”(d I Q) bY S”(dI Q)=l- H”(d I Q, lo&.(n> . We shall show later that the measures S”(d even identical-to We assume the approximation that prediction requires quality y . three models presented below are distinguished their respective associated parameters. I Q) are comparable-and in a special case (3.3) the specification of a probability distribution; by the choice of such distributions the and Throughout, we suppose that the classes of 6Q are X0, . . . , Xt with ri %f [Xi 1, and that the classes of & are Yo, . . . , Y, . Furthermore, we let c < t be such that V=XuU...UX,, 122 I. Diintsch, G. Gediga /Artificial Intelligence 106 (1998) 109-137 i.e., the Xi, i < c, are exactly the deterministic classes of 8~. In accordance with our previous observations, we assume the principle of indifference, Also, we shall write y instead of y (Q -+ d), if Q and d are understood. and set r2i dzf ri /n for i < t. 3.1. Prediction I: knowing it all The first approach is based on the assumption that structure and amount of uncertainty can be estimated by the interaction of d and Q. In this case, each class X of 8Q determines probability distributions based on its intersection with the classes of &. This assumes that we know (1) the classes of &, (2) the classes of 8Q, and (3) their interaction, i.e., their intersections. It follows representative within the rough set approach in [ 111. that, in order to justify any prediction, we have to assume that the data set is a it sample. This is a general problem of data mining, and we have discussed Uncertainty in the sense of this model is not predominantly a feature of the predictor set Q (as intended by RSDA) but a local feature of the intersection of equivalence classes X E 8Q and Y E 6d. We shall show that the procedure “first code the rules and then apply them” has the same complexity as the simple procedure “guess within k)Q n odd)’ and can from this point of view; in other words, we are guided by a purely be viewed as identical statistical view. Although there is has been some effort to adopt this approach in the RSDA context [30]; we shall discuss some aspects of this work below. from the RSDA approach, this is rather different The partition induced by 19”’ dAf 6Q fled are the nonempty sets in {Xi n Yj : i < t, j < s}, and its associated parameters are defined by n Ui,j = IXi n Yjl n ’ Thus, ff(e’OC) = xx %,j log2 ($-). i<t j<s Now, we define H’““(Q + d) %f H(B’““). (3.4) (3.5) In information above to emphasize theory, H’““(Q + d) is usually written as H(Q, d); we use the notation that our view of the world consists of Q and that we want to predict d. One problem with this approach is the symmetry H’““(Q + d) = doc@Q n &) = H’oc(d + Q). We shall not discuss this problem here, but instead refer the reader to Jumarie and p. 49ffl and Li and Vitartyi [17, p. 65ffl. The proof of the following proposition is straightforward and is left to the reader. [ 14, p. 24ff I. Dtitsch, G. Gediga /Arti&ial Intelligence 106 (1998) 109-137 123 Proposition 3.1. Let d, Q C L?. Then, (1) HioC(Q + 4 2 H(d), (2) H’°C(Q -+ d) = H(Q) ifand only ifdQ C 6d. Applying 113.2)~ a normalized lot-entropy measure S’Oc( Q + d) is definable and-given H(d) -c log2 (n)-we obtain S’““(Q + d) = 1 - H’Oc( Q --f d) - H(d) log#) - H(d) ’ ForeachiGt, j<slet ;li,j d&f IXi n Yil. ri This is the estimated probability of an element of Xi being in the class Xi fl Yj. In other words, it is the conditional probability of x E Yj , given that n E Xi. Observe that CTii = Cfi;,j = 1 i<t j<s the parameters that so furthermore fri and fii,j satisfy the hypotheses of Lemma 2.3, and that Eti .rii,j = lXinYjl =cij n I . Substituting (3.6) into (3.5) and applying Lemma 2.3 we obtain H’OD(4~-*d)=H(Q)+C~iCqi,j10g2(~) j<s i<t = H(Q) + 2 i=c+l si C j<s rji,j 1% (&)> the latter sitme Gi,j = 1 for i 6 c. The conditional entropy of d given Q is now Zidoc(d 1 Q) fEf k f?i C6i.j log2 i=c+l j<s (3.6) (3.7) This is the usual statistical definition of conditional expression entropy. Its normalization leads to the S’OC(d I Q) = 1 - H’OC(d I Q> log2(n> ’ Example 1 (Continued). Table 4 shows the statistical quality within the example information system of Table 2. information analysis of prediction Although both measures S’Oc( Q + d) and S’OC(d I Q) vote for (S] as the best predicting set for the given data-and this convergence need not to be true in the general case. are in line with the results of the significance test (see Table 3), 124 I. Diintsch, G. Gediga /ArtiJicial Intelligence 106 (1998) 109-137 Table 4 Statistical information measures of predicting quality Attribute set H’Oc(Q + d) S’Oc(Q + d) f+(d I Q, S’Oc@ I Q, IS1 VW IS, BW 1.352 1.891 2.197 0.808 0.568 0.432 0.361 0.900 0.222 0.835 0.587 0.814 Y 0.556 0.000 0.778 simple examples Example 2. Some measures Hloc and the approximation shall demonstrate how the average uncertainty quality y work, and how they differ. Suppose that q1 and d take the values 0, 1, and suppose that we observe the probabilities 41 =‘I’ 41=1 d=O d=l c l/4 l/4 l/2 l/4 114 l/2 c l/2 112 1 We calculate H’Oc(ql + d) = 2, and H’OC(d I ql) = 1. Now, consider another attribute q2 with values 0, . . . , 3, and the observed probabilities 42 =o 42 = 1 92 = 2 92=3 d=O d=l c 114 0 l/4 l/16 l/16 3116 3116 114 114 l/8 l/8 l/4 c l/2 l/2 1 Whereas q2 enables us to predict 25% of the cases deterministically, namely, by the rule ifqz=O, then d = 0, whereas q1 cannot be used to predict d. Comparing the entropy measures, we observe that H’OC(q2 + d) = 2.6556 > H’Oc(ql + d) = 2, and H’OC(d 1 q2) = 0.6556 < H’OC(d 1 ql) = 1. the entropy measure H’Oc( Q + d) favors 41, the conditional in the first example the two large classes predict obviously nothing, entropy measure Whereas H’OC(d 1 Q) votes for q2 to be the better predicting attribute. The explanation of this effect the is simple: although encoding of these small number of classes can be done effectively. The prediction success in the second table is overruled by a large number of small classes with high uncertainty, causing a high coding complexity. the coding complexity of the predicting attribute, the effect of the high coding effort is eliminated, and the better prediction success of q2 results in a smaller conditional entropy measure. If we subtract I. Diintsch, G. Gediga /Art$cial Intelligence IO6 (1998) 109-137 125 A third tabsle presents an example why Hioc(d 1 Q) is not optimal for rough set prediction under certain circumstances: 43 =o 43=1 l/l6 l/16 l/16 l/2 7116 l/2 c l/2 l/2 1 d=O d=l c to a bet based on q2. Having the conditional measure H’Oc(d 1 q3) = Although q3 predicts no outcome deterministically, 0.5436 is better than H’Oc(d 1 q2) = 0.6556. The essence of the result is that a bet given q3 is preferable the knowledge q3 = 0 enables us to predict that the outcome d = 0 is much more likely than d = 1, whereas q3 = 1 predicts d = 1 the value q2 # 1 are comparably most of the time. With attribute 42, the bets given for a given observation bad. Although i, 1 < i < II, of the dataset with the knowledge anything about d, we (cannot find the value d(i) unless we search through the whole set of d-values. success of q3 is as bad as that of ql, and, consequently, In terms of RSDA, the prediction the betting situation given q3 is quite satisfactory, 93(i) = 0 and not knowing y(ql + d) q = y(q3 + d) = 0. As the examples the special layout of the (rough) prediction problem, because the lot-model optimizes guessing outcome of a dependent variable but not necessarily perfect prediction. show, the statistical entropy measures do not take into account In the next sections we will present other entropy measures, which are integrated into the rough set approach and which are more suitable for rough set prediction. The earliest paper to concern between entropy and rough set analysis was that of Wong et al. [30]. In their Theorem 2, later restated by Teghem and Benjelloun strong connection between RSDA and entropy measurement [29], Proposition 6, the following is claimed (translated itself with the connection into our terminology): Claim. Suppose thatfor each c < i < t, IXi n Yj 1 = di for all j < S. Then ft°C(d I Q) = Ir,” \Y’Ql n forall j <s. Consider Suppose that U = (0, 1, . . . , 7}, and that the partition given by d has the sets .the following counterexample: YiZ(2’i,2’i+1}, i <4, and the partition given by Q is x0 = {l, 3,5,7}, X1 = {0,2,4,6}. 126 I. Diintsch, G. Gediga /Art$cial Intelligence 106 (1998) 109-137 Now, Fe = U and Y. ‘Q = 0 for all j < 4, and thus, IF? \yi,l n = 1. Furthermore, satisfied. We now have IXi n Yj 1 = 1 for all i < 2, j < 4, so that the hypothesis of the claim is Izi = 1 2, rjij=l 4’ and it follows that iji,j log.2 L_ = + . lOgz(4) = i, > 17i.j ( C jt4 ;li,j log2 = 2. i %,j ( > Thus, Zf’OC(d ( e) = C??i ’ 2 = 2, ii2 which contradicts the claim. We can generalize to show that under the assumptions of Wong et al. [30], the value of H’OC(d ( Q) does not depend so much on y as it does on the number of classes of 6d which are not Q-definable: this example that no class of 8~ is deterministic, and that the elements of Proposition 3.2. Suppose each Xi are uniformly distributed among the classes Yj, i.e., for each i < t, j < s we have IXi II Yj I = di. Then, FP(d I Q) = logz(s + 1). Proof. By the hypothesis we have for all i < t, j G s [Xi n Yj =di, and therefore it follows from Cj qi,j = 1 that ,. qi,j = r- = - di 1 s+l‘ 1 Thus, H’Oc(d I Q> = C si C kj log2 (&) =~??i&&lOgz(S+l) i j =c s2i lOgz(S + 1) = lOgz(S + I), which proves our claim. q I. Diintsch, G. Gediga/Art$cial Intelligence 106 (1998) 109-137 127 3.2. Prediction II: playing it safe Whereas of optimal guessing the entropy measures in the previous strategies, based on to be of the measures distributions d x Q, a rough set approach should not be based on “guessing:” but on “knowing”. This means that the observations which can be predicted perfectly are assumed to be the realization of a systematic process, whereas the nature of the indeterministic section are good candidates the estimated parameters of the cross-classification to the researcher. rules is assumed to be unknown Based on these arguments, given a class Y of 19d, any observation y in the region of jJQ \ 1 Q is the result of a random process whose characteristics are unknown; uncertainty in other words, our given data is the partition obtained from Q, and we know the world only up to the equivalence classes of 6,. Given this assumption, no information within our that each such y data set will help us to classify an element y E U \ V, and we conclude requires a rule (or class) of its own. In this case, any element of U \ V may be viewed as a realization of an unknown probability distribution with its uncertainty log2(n). Note that, unlike the previous one, this approach assumes that only the classes of 6Q are observed approach requires only the sample, or-in within a representative rroe (and its estimates 20~) of the classes of 6Q. Thus, we regard probability distribution relation 6Q) as the given data, and, in accord with the Q (and its a.ssociated equivalence principles of RSDA, we only know the upper, respectively, of any class Y Of @d. the lower Q-approximation terms of parameters-&e (l/n) It follows that we may only apply the deterministic part of Q + d, and ignore whatever rules. Thus, we use only those classes of 8Q which that each y E U \ V is in its own class. In other words, the maximum entropy principle as a worst case, and look at the equivalence might be gained from the indeterministic are contained we assume relation Odet defined by in V, and assume x =ed(?t y & x = y or there exists some i 6 c such that x, y E Xi. Its associated probability distribution is given by {& : i < c + 1 U \ VI} with We now define the entropy of deterministic rough prediction (with respect to Q + d) as (3.8) ffde’( Q + d) dzf H (Odet) = c & log2 i and have Hde’( Q + d) = C jZi log2 i<c =c i<c \ r2i log2 Knowledge / Guessing 128 I. Diintsch, G. Gediga /Art$cial Intelligence IO6 (1998) 109-137 This gives us ~~~‘(d 1 Q) gf Hde’(Q + d) - H(Q) = (1 - y)log;?(n) - cl”i log, . i>C Since 0: _C 8Q fled, we note that 8Q fled has no more classes than 0,+, and therefore f$Oc( Q + d) = c iti lo& i<c a i2i log2 i<c $ I > ( = Hdet( Q -+ d), + (1 - v) log#), which implies H’OC(d 1 Q) < Hde’(d 1 Q). If we compare the Hde’( Q + d) and Hloc( Q --f d) in terms of necessary parameters, we have to assume for the computation well as the indeterministic population. between deterministic and indeterministic Indeed, rules. rules are representative within the Hloc -measures do not distinguish-up the sample of the underlying values- to quantitative of H’““(Q -+ d) that the deterministic rules as In contrast, Hde’( Q -+ d) requires a representativeness that any indeterministic rule, which rules, only for the deterministic is valid for m objects, consists of m and assumes unique (individual) rules, gathered from a random world which cannot be replicated. The proof of the following is straightforward, and is left to the reader: Proposition 3.3. Let d, Q C f2. Then, (1) Hdet(Q -+ d) 3 H(d), (2) Hdet(Q --f d) = H(Q) (3) Hdet( Q --f d) = log2(n) ifand only if6Q C t&f. ifand only if V = 0 or V is a union of singletons ofe~. is the identity relation, and everything can be explained by Q, 8Q The extremes for Hdet( Q + d) are: . l y (Q + d) = 0, and everything is guessing. In both cases we have Hdet(Q + d) = log2(n). The following gives the bounds within which Hde’(d 1 Q) varies: Proposition 3.4. (1 - y) < Hdet(d 1 Q) < (1 - v) logz(n - (VI). Proof. First, observe that by (2.4), log2 (n - I VI) > logz(2) = 1. The minimum value of Ci,C Si log, (Eti) is obtained when c = t - 1, and in this case, c i2i log2 (Ezi ) = i>c n-IV1 ---log2 n Therefore, n ___ n-IV1 ( > =(1-y)log2 & ( . > I. Diintsch, G. Gediga /Arh$cial Intelligence 106 (1998) 109-137 129 Hde’(d ( Q) = (1 - y) lo&(n) - Isi izc lo& 0 ;1- ni < (1 - y) log*(n) - (1 - Y)b2 = u- Ym&(n> -log2 & ( > = (1 - y) lo&(41 - v)) = (1 - v) log2(n - IVI). class X has at least two For the other direction, we first note that each nondeterministic if either each such class has exactly fii log2(1/&) elements, anld that Ci,c two elements, or all but one class have two elements and one class has three elements. loss Since the value of Cilc of generality is greater in the first case, we assume without that n - 1 VI is even, so that has a maximum f2i log2 (1 /?i) Therefore, Hdet@ I Q> 3 (1 - y) log,(n) - (1 - v> log,? (5) = (1 - Y) (1%2(n) - log2 (5)) = (1 - Y) log2W =1-y, which proves our claim. q We see that Hde’(d I Q) the deterministic distribution--of of the classe:s leading the lower Hde’(d I Q). We use this to show to nondeterministic is independent of the granularity-i.e., the probability classes of 8Q, and that it is dependent on the granularity rules: the higher the granularity of those classes, Propositionr 3.5. If Q g R, then Hdet(d I R) 6 Hdet(d I Q). Proof. By the remark above, we can assume that every deterministic of 6~. This implies that Be+ G e,+, and hence, class of 8Q is a class Hde’(R + d) f Hdet(Q + d). Since furthsrmore H(Q) 6 H(R) by Corollary 2.2, the conclusion follows. q A similar result does not hold for Hdet (Q + d) as the example given in Table 5 shows: there, Hdet(IqlJ + IPI) = 1.5 < 2 = Hde’(Iql, q21 --, IPI) = Hde’(tq21 + {PI). 130 I. Diintsch, G. Gediga /Artijcial Intelligence 106 (1998) 109-137 Table 5 Hdet(Q + d) u 1 2 3 4 42 1 2 3 4 41 1 1 2 2 P 1 2 2 2 As in (3.2), we define the normalized relative deterministic prediction success Sdet( Q -+ d), which we also will call normalized rough entropy (NRE): first, let & = m, so that H(d) = log2 (n). Then Otherwise, if H(d) < logz(n), we set Sdet(Q -+ d) itf 1 - Hdet(Q -+ d) - H(d) lw(n) - H(d) ’ (3.9) (3.10) success within RSDA, which can be used In this way we obtain an measure of prediction and the of coding complexity to compare different prediction uncertainty in Sde’( Q -+ d) = 1, and the worst case is at Sdet( Q + d) = 0. Sdet is an unconditional measure, because both, the complexity of the rules and the uncertainty into one measure. in terms of the combination rules in the sense that a perfect prediction of the predictions, are merged results The question arises, where the approximation function y is positioned in this model. Proposition 3.4 shows that, for fixed Q, max{ Hdet(d l R): y(R -+ 4 = Y(Q += 4) = (I- y)log# - IVI), and we denote this value by H,,,,, det (d 1 Q). The following to v(Q + d): H$$(d I Q) is strictly inversely monotone result tells us that, for fixed d, ~oposition3.6. y<Q + d) < Y(R -+ d) w H,!j$i(d 1 R) < H,‘$(d 1 Q). (“=+“) The hypothesis 7/ (Q + d) < y (R --f d) implies that I VQ+dI 5 I VR+dl. Proof. Thus, H$;(d 1 R) = (1 - Y(R -+ d))lqz(fi - IVR-td/), < (1 - y(Q + d))log,(n - IvQ-+dl>, = Hz;(d 1 Q). (“+“) First note, that for k > 1, klog2k<(k+l)log,(k+l). (3.11) 1. Diintsch, G. Gediga/Artijicial Intelligence 106 (1998) 109-137 131 We can also assume that 0 < Hz$(d 1 R), so that U \ VR-+d # 8. Now, H,$x(ti I W < H%x(d I Q> (1 - y(R -+ d))log,(n * * - iVR+do ( (1 - v<Q + d))b& - IvQedb (n - IVR+.dI)log2(n - IVR-+dl) < (a - iVQ+dl)lo&(n - IvQ+dl> =+ (n - IVR+do < (n - IVQ-dl) by (3.11) =+ IvQ-+dl ( IVR-tdI * Y(Q + 4 < Y(R + 4. This completes the proof. q We observe that l RSDA which tries to maximize y is a procedure to minimize the maximum of the conditi’onal entropy of deterministic terms of conditional In rough prediction. uncertainty, we may view y = v( Q + d) as a crude approximation of a measure of normalized prediction success, because SEX<<1 ( Q) = 1 - H,$&(d I Q) - min{H,$x(d I R): R g f2) max{H,$Lx(d I R): R g f2} - min{H$ix(d ( R): R g i2) =,_H%JdIQbo lo&(n) - 0 = y _ (1 _ )/)log2(1- v) log2 (n) =y+o 1 ~ log2h) ( . > Proposition a result similar equivalence 3.5 does not extend to the hypothesis to 3.6 does not hold, as the following relations @d, 8Q, 6~ with the following partitions: v( Q + d) < y (R + d), and thus, the shows: consider example od: {I>& 31,Pk 5,6), OQ: 11341, (2,5), {3,6), OR: 111, {2,3,4,5,6). Then, y(Q+d)=O<;=y(R+d). On the othe.r hand, Hde’(,d ) Q) = log2(6) - log2(3) = 1 < ; log2(5) = ; log2(6) - ; log2 (2) = Hde’(d 1 R). Example 1 (Continued). Table 6 presents the rough information example given in Table 2. We have skipped the presentation of the Hdet(d ( Q)-measures, because-as prediction the :success of different attribute sets. The results show that the NRE Sdet (Q + d) are identical with y for the purpose of comparing analysis for the data of the shown above-they 132 I. Diintsch, G. Gediga/A@icial Intelligence 106 (1998) 109-137 Table 6 Rough information measures of the predicting quality within the example information system Attribute set Hdet(Q --f d) Sdet(Q + d) S’“(Q + d) ISI WfIl IS, BW 1.880 3.170 2.197 0.573 0.000 0.432 0.808 0.568 0.432 Y 0.556 0.000 0.778 Significance 0.047 1.000 0.144 to evaluate is a good candidate produces the limitations of the significance see that the “defects” of S’Oc( Q + d) have been repaired. the same order of “goodness in predictability” test. Inspecting the rough prediction quality of attribute sets, because it test, without the results of {BMZ} in Table 6, one can as the significance 3.3, Prediction III: living side by side In Section 3.2 the prediction Hdet(Q + d) consists of two parts: the absolute correct deterministic part (the union of the lower bound approximations) and the random part. The prediction within the random part is done using an “element-to-class” mapping, because of that no uncertain observation can be predicted given any available source of the assumption data. If we are willing rules which are offered by RSDA, is restricted by those rules and we need another entropy estimation. to use the information provided by the indeterministic the uncertainty recognizes distribution the method can be interpreted in the sense that an indeterministic This approach to handle uncertainty in the sequel, we will spare the reader the somewhat in the prediction of &, but that the amount of uncertainty if Xi is a class of 8Q which does not lead to a deterministic that 13d induces some structure on U \ V: there are classes rule, . . . I Yi,k of Od, k > 1, such that (Xi, Yi,j) E Q + d, i.e., Xi intersects each Yi,j \ V. K,O, Uncertainty given Xi can now be measured by the uncertainty within { Yi,c \ V, . . . , Yi,k \ induced by &. The V} which also requires knowledge of the probability rule produces a certain assumption degree of imprecision is based solely on the uncertainty within d and does not interact with Q. Even though this is not “pure rough set theory”, it is certainly consistent with it: the procedure describes the upper bounds of sets defined by & in terms of a suitable probability distribution. As we shall not be using involved definitions of the resulting entropy measures H*(Q + d) and H*(d 1 Q). We shall just entropy H*(d 1 Q) mention, is not (anti-) monotone. This result is a drawback, because relationship of C and a measure of approximation within a search process we cannot use H*(d measures y, Hdet(d the H*-measure is in between deterministic approach assumes & are representative, whereas Hloc assumes that any conditional probability distribution representative, and Hdet assumes of any ClaSS of & is representative it seems that the practical value of assumption which the H*- rough entropy Hdet and the statistical entropy HI”: that the probability distributions within the upper bound of any class of is that the probability distribution within the lower bound for the pOpUhtiOn. this method in more detail in subsequent quality seems to be quite natural. As a consequence, like the other conditional that, unlike Hde’(d 1 Q) and H’Oc(d 1 Q), the conditional I R), or Hloc(d 1 I?). Therefore it takes a representativeness is rather limited, although I R) as stop criterion We shall investigate the monotone research. I. Diintsch, G. Gediga /Artificial Intelligence 106 (1998) 109-137 133 4. Data analysis and validation The approach which is closest to the non-invasive philosophy of RSDA is the entropy of rough prediction Hde’( Q + d) which combines deterministic with the maximum entropy principle because of our basic aim to use as few assumptions outside the data as possible: the principle of indifference in an RSDA context. We advocate this type of entropy “Although the principle of maximum entropy suggests largest entropy among all the possibilities. Using the appropriate definitions, be shown that there is a sense in which this /.L* incorporates information” there may be many measures Al. that are consistent with what we know, that we adopt that p* which has the it can the ‘least’ additional [ 131. To obtain an objective measurement we use the normalized where rough entropy (NRE) of (3. lo), Sdet(Q -_, d) = 1 - Hdet(Q -+ d) -H(d) log,(lUI) - H(d) . (4.1) If the NRE has a value near 1, the entropy does not use is favorable, whereas a value near 0 indicates casualness. The normalization moving standards as long as we do not change the decision attribute d. Therefore, any comparison of NRE values between different predicting attribute sets makes sense, given a fixed decision attribute. is low, and the chosen attribute combination The implemented procedure searches for attribute sets with a high NRE; since finding algorithm expensive, we use a genetic-like the NRE of each feature set is computationally to determine sets with a high NRE. We have named the method SORES, an acronym for Searching @timal Rough Entropy Sets. SORES is implemented in our rough set engine GROBIAN [9]. 3 4. I. Validation ‘to test the procedure, we have used 14 datasets available In order repository4 from which the appropriate a subset of the datasets which were used by Quinlan the UC1 references of origin can be obtained. These are [26] to test Release 8 of C4.5. from The validation by the training the into two equal sizes 100 times, assuming a balanced distribution of set method was performed by splitting full data set randomly training and testing data (IT2 method). The mean error value is our measure of prediction success. set-testing We choose only half of the set for training purposes in order to have a basis for testing the predictive power of the resulting attribute sets. Because all data sets contained continuous attributes and most of them missing values as well, a preprocessing apply the SORES algorithm to to these data sets. Missing values were replaced by the mean step was necessary 3 All material relating to SORES, e.g., datasets, a description of the algorithm, as well as GROBIAN, can be obtainedfromourwebsitehttp://www.psycho.uni-osnabrueck.de/sores/. 4http://Murw.ics.uci.edu/-mlearn/MLRepository.html. 134 I. Diintsch, G. Gediga /Artificial Intelligence IO6 (1998) IO9-137 value in case of ordinal attributes, and by the most frequent value (i.e., the mode) otherwise. The preprocessing of the continuous data was done by three different global discretization methods: in minimal granularity filtering method described Method 1 consists of the global in Section 2.3 which the NRE, but does not affect y, and thus has no influence on the dependency influences to the decision structure. This results the values of an attribute into attribute. The other two discretization methods cluster the same number of objects. The ten, respectively, the H’Oc-based methods of local discretization method can be refined by transforming discretization attributes given by Catlett [5] and Dougherty et al. [7] to the proposed Hdet- measure. This is a task which still needs to be done, but which is outside the scope of the current introductory five, classes with approximately of attributes with respect of continuous article. In Table 7 we list the basic parameters of the data sets, and compare with the C4.5 performance given by Quinlan since [26] uses IO-fold cross validation (CVlO) on data sets optimized by the SORES results [26]. This has to be taken with some care, “ . . . dividing the data into ten blocks of cases that have similar size and class distribution” [26, p. 8 1, footnote 31. Table 7 Datasets and SORES validation Dataset SORES C4.5(8) Name Cases Classes Attributes Cont. Discr. Anneal Auto Breast-W Colic Credit-A Credit-G Diabetes Glass Heart-C Heart-H Hepatitis IliS Sonar Vehicle Std. deviation 798 20.5 683 368 690 1000 768 214 303 294 155 150 208 846 6 6 2 2 2 2 2 6 2 2 2 3 2 4 9 1.5 9 10 6 7 8 9 8 8 6 4 60 18 29 10 - 12 9 13 - _ 15 15 13 _ - - No. of prcd. attr. Error Error 11 6.26 1.67 2 2 4 5 6 3 3 2 5 3 3 3 2 11.28 17.70 5.74 5.26 21.55 15.00 18.10 14.70 32.92 28.40 31.86 25.40 21.79 32.50 22.5 1 23.00 19.43 21.50 17.21 20.40 4.33 4.80 25.94 25.60 35.84 27.10 10.33 8.77 I. Diintsch, G. Gediga /Artificial Intelligence 106 (1998) 109-137 135 Because TT2 tends to result in smaller prediction success rates than CVlO, the comparison of SORBS and C4.5 is based on a conservative estimate. The SORES column “No. of pred. attr.” records the number of attributes which are feature of RSDA, and in most cases actually used for prediction; considerably this is a prominent less than the number of all attributes. indicate The results learning procedure, because that SORBS in its present version can be viewed as an effective compares well with that of the well machine established that C4.5 produces better results. However, since the standard deviation of the error percentages of SORES is higher than that of C4.5, we conclude than the current SORES. that C4.5 has a slightly better performance the odds are 7:7 (given the 14 problems) its performance (~4.5 method: 5. Sunuua~y and outlook In the fir:st part of the paper we have proposed the the context of RSDA using various entropy three approaches to estimate unconditional measures. prediction success within The statistical entropy measure is not well suited, because the assumption of a symmetric and predicted exchange of predicting is not given within attributes are discussed: given by the deterministic information RSDA frame. Two modifications information other information. The other approach, H*, additionally distributions within the indeterministic within most suitabb: measure complexity and expected prediction uncertainty. rules, and assumes an atom-like the the first one, Hde’, relies only on the structure of all about the uses the knowledge rules, but has the drawback of lacking monotony the csonditional measure H*(d 1 Q). The measure Hde’( Q + d) seems to be the to compare attribute sets Ql , . . . , Qk in terms of combined coding In the second part of the paper, we have applied the method of searching optimal rough entropy sets (SORBS) to real life data sets. The method seems to be well applicable, since we show that C4.5 performs better than SORES on only 7 of 14 problems, although C4.5 is used in a line tuned version (Release 8) and SORBS, at present, is still quite “raw”. following steps. least-the Fine tuning of the SORBS procedure will consist of-at l Both types of measures-H”(Q --f d) and H”(d 1 Q) (whatever model M is used)--are and thus, any weighted sum to some extent suitable measures for finding optimal sets for prediction, H”(Q, d, w) = wH”(Q + d) + (1 - w)H”(d 1 Q), (0 < w < 1) is a suitable measure as well. If u = 1, we weight the effort of searching for a rule as high as the effort of reducing uncertainty of the dependent attribute. If w = 0 is chosen, then the effort of coding the rules is neglected. Finally, any 0 < w < 1 estimates the relative effort of finding a rule with respect to finding an object under uncertainty. The methods in Section 3 are based on an w = 1 procedure, but it will be worthwhile these results with procedures using w -z 1. to compare l The proposed method-as a symbolic data analysis procedure-is consuming. In order to enhance the applicability of the procedure time rather to real life data sets, 136 I. Diintsch, G. Gediga /Arr$cial Intelligence IO6 (1998) 109-137 the optimization optimization must be implemented. The theory of dynamic towards such an enhancement. cannot be performed on big samples, but some kind of subsample [ 1,2] is a step reducts l The discretization of continuous attributes is another problem which has to be solved by any symbolic data analysis procedures described above work quite well in the presented numerical examples, a local discretization procedure, which optimizes &-directly, can be expected to produce an even better prediction quality. the global discretization the chosen criterion+.g., technique. Although Hdet( Q + that, except to point out like all of the procedures developed Finally, we should discretization methods, external parameters, and only the representation approaches. Thus, model assumptions (at least) serve as a preprocessing mechanism before “harder” computational methods are applied. global two numerical in Section 3 do not use any stated for each of the three can and the procedures or statistical are kept to a minimum, assumptions the for Acknowledgement We should like to thank the anonymous referees for their constructive remarks which helped to improve the clarity and the completeness of the paper. References [l] J.G. Bazan, A comparison of dynamic and non-dynamic rough set methods for extracting laws from decision tables, Institute of Mathematics, University of Rzeszdw, 1997, Preprint. [Z] J. Bazan, A. Skowron, P. Synak, Dynamic reducts as a tool for extracting laws from decision in: Proceedings Symposium on Methodologies Artificial Intelligence, Springer, Berlin, 1994, pp. 346-355. for Intelligent Systems, Charlotte, NC, Lecture Notes tables, in [3] C. Browne, Enhanced rough set data analysis of the Pima Indian diabetes data, in: Proceedings 8th Ireland Conference on Artificial Intelligence, Den-y, 1997, pp. 32-39. [4] C. Browne, I. Diintsch, G. Gediga, IRIS revisited: a comparison of discriminant and enhanced rough set data (Eds.), Rough Sets in Knowledge Discovery, Physica, Heidelberg, in: L. Polkowski, A. Skowron analysis, 1998, Vol. 2, pp. 345-368. [5] J. Catlett, On changing continuous Proceedings European Working Session on Learning-EWSL-91, attributes into ordered discrete attributes, in: Y. Kodratoff (Ed.), Springer, Berlin, 1991, pp. 164-178. [6] J. Cohen, Things I have learned (so far), American Psychologist 45 (1990) 1304-1312. [7] J. Dougherty, R. Kohavi, M. Sahami, Supervised and unsupervised discretization of continuous in: 12th International Conference on Machine Learning, Morgan Kaufmann, San Francisco, CA, features, Proceedings 1995, pp. 194-202. [8] I. Dtlntsch, A logic for rough sets, Theoret. Comput. Sci. 179 (1997) 427-436. [9] I. Diintsch, G. Gediga, The rough set engine GROBIAN, in: A. Sydow (Ed.), Proceedings 15th IMACS World Congress, Vol. 4, Wissenschaft und Tech& Berlin, 1997, pp. 613-618. [lo] I. Dtintsch, G. Gediga, ROUGHIAN-rough information analysis 15th IMACS World Congress, Vol. 4, Wissenschaft Proceedings 636.Fullpaperavailablefromhttp://www.infj.ulst.ac.uk/-cccz23/papers/roughian. html. I. Diintsch, G. Gediga, Statistical evaluation of rough set dependency Human-Computer Studies 46 (1997) 589-604. [ll] (extended abstract), in: A. Sydow (Ed.), und Technik, Berlin, 1997, pp. 631- analysis, International Journal of 1. Diintsch, G. Gediga/Art$cial Intelligence 106 (1998) 109-137 137 [12] I. Dtintsch, G. Gediga, Simple data filtering in rough set systems, Intemat. J. Approx. Reason. 18 (1998) 93-106. [13] E.T. Jaynes, Information [14] G. Jumarie, Relative Information, Springer, Berlin, 1990. [15] E. Konrad, E. Grlowska, Z. Pawlak, Knowledge theory and statistical mechanics, Physical Review 106 (1957) 62&630. representation systems-definability of informations, ICS Research Report 433, Polish Academy of Sciences, 1981. [16] E. Konrad, E. Orlowska, Z. Pawlak, On approximate concept learning, Tech. Report 81-7, Technische Universitat Berlin, 1981. [17] M. Li, P. Vitrhryi, An Introduction to Kolmogorov Complexity and Its Applications, Texts and Monographs in Computer Science, Springer, Berlin, 1993. [18] H.S. Nguyen, S.H. Nguyen, Discretization methods in data mining, in: L. Polkowski, A. Skowron (Eds.), Rough Sets in Knowledge Discovery, Physica, Heidelberg, 1998. [19] P. Paglianl, Rough Information-Rough [20] Z. Pawlak, Mathematical of Sciences, 1973. theory sets Set Analysis, Physica, Heidelberg, 1997, pp. 109-190. logic-algebraic structures, and in: E. Odowska (Ed.), Incomplete foundations of information retrieval, ICS Research Report 101, Polish Academy [21] Z. Pawlak, Rough sets, Intemat. J. Comput. Inform. Sci. 11 (1982) 341-356. [22] Z. Pawlak, Rough sets: theoretical aspects of reasoning about data, System Theory, Knowledge Engineering and Problem Solving, Vol. 9, Kluwer, Dordrecht, 1991. [23] Z. Pawlak, J.W. Grzymda-Busse, [24] Z. Pawlak, R. Slowiriski, Rough set approach Warsaw University of Technology, 1993. R. Slowiiiski, W. Ziarko, Rough sets, Comm. ACM 38 (1995) 89-95. to multi-attribute decision analysis, ICS Research Report 36, [25] L. Polkowski, A. Skowron (Eds.), Rough Sets in Knowledge Discovery, Physica, Heidelberg, 1998 (to appear). [26] R. Quinlan, Improved use of continuous attributes in C4.5, Journal of Artificial Intelligence Research 4 (1996) 77-90. [27] .I. Rissanen, Modeling by the shortest data description, Automatica 14 (1978) 465471. [28] J. Rissanen, Minimum-description-length in: S. Kotz, N.L. Johnson principle, (Eds.), Encyclopedia of Statistical ,Sciences, Wiley, New York, 1985, pp. 523-527. [29] J. Teghem, M. Benjelloun, Some experiments in: R. Siowinski Theory, System Theory, Knowledge Engineering pp. 267-284. (Ed.), Intelligent Decision Support: Handbook of Applications to compare rough sets theory and ordinal statistical methods, and Advances of Rough Set and Problem Solving, Vol. 11, Kluwer, Dordrecht. 1992, [30] S.K.M. Wong, W. Ziarko, R.L. Ye, Comparison of rough-set and statistical methods in inductive learning, Intemat. J. Mar-Mach. Stud. 24 (1986) 53-72. 