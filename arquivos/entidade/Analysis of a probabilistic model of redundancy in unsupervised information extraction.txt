Artificial Intelligence 174 (2010) 726–748Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAnalysis of a probabilistic model of redundancy in unsupervisedinformation extractionDoug Downey a,∗, Oren Etzioni b, Stephen Soderland ba Northwestern University, 2133 Sheridan Road, Evanston, IL 60208, United Statesb University of Washington, Box 352350, Seattle, WA 98195, United Statesa r t i c l ei n f oa b s t r a c tArticle history:Received 7 July 2009Received in revised form 14 April 2010Accepted 26 April 2010Available online 29 April 2010Keywords:Information extractionUnsupervisedWorld Wide WebUnsupervised Information Extraction (UIE) is the task of extracting knowledge fromtext without the use of hand-labeled training examples. Because UIE systems do notrequire human intervention, they can recursively discover new relations, attributes, andinstances in a scalable manner. When applied to massive corpora such as the Web, UIEsystems present an approach to a primary challenge in artificial intelligence: the automaticaccumulation of massive bodies of knowledge.A fundamental problem for a UIE system is assessing the probability that its extractedinformation is correct. In massive corpora such as the Web, the same extraction is foundrepeatedly in different documents. How does this redundancy impact the probability ofcorrectness?We present a combinatorial “balls-and-urns” model, called Urns, that computes the impactof sample size, redundancy, and corroboration from multiple distinct extraction ruleson the probability that an extraction is correct. We describe methods for estimatingUrns’s parameters in practice and demonstrate experimentally that for UIE the model’slog likelihoods are 15 times better, on average, than those obtained by methods used inprevious work. We illustrate the generality of the redundancy model by detailing multipleapplications beyond UIE in which Urns has been effective. We also provide a theoreticalfoundation for Urns’s performance, including a theorem showing that PAC Learnability inUrns is guaranteed without hand-labeled data, under certain assumptions.© 2010 Elsevier B.V. All rights reserved.1. IntroductionAutomatically extracting knowledge from text is the task of Information Extraction (IE). When applied to the Web, IEpromises to radically improve Web search engines, allowing them to answer complicated questions by synthesizing infor-mation across multiple Web pages. Further, extraction from the Web presents a new approach to a fundamental challengein artificial intelligence: the automatic accumulation of massive bodies of knowledge.IE on the Web is particularly challenging due to the variety of different concepts expressed. The strategy employed forprevious, small-corpus IE is to hand-label examples for each target concept, and uses the examples to train an extractor [19,38,7,9,29,27]. On the Web, hand-labeling examples of each concept are intractable—the number of concepts of interestis simply far too large. IE without hand-labeled examples is referred to as Unsupervised Information Extraction (UIE). UIE* Corresponding author.E-mail addresses: ddowney@eecs.northwestern.edu (D. Downey), etzioni@cs.washington.edu (O. Etzioni), soderlan@cs.washington.edu (S. Soderland).URLs: http://www.cs.northwestern.edu/~ddowney/ (D. Downey), http://www.cs.washington.edu/homes/etzioni/ (O. Etzioni),http://www.cs.washington.edu/homes/soderlan/ (S. Soderland).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.024D. Downey et al. / Artificial Intelligence 174 (2010) 726–748727systems such as KnowItAll [16–18] and TextRunner [3,4] have demonstrated that at Web scale, automatically-generatedtextual patterns can perform UIE for millions of diverse facts. As a simple example, an occurrence of the phrase “C such asx” suggests that the string x is a member of the class C, as in the phrase “films such as Star Wars” [22].1However, all extraction techniques make errors, and a key problem for an IE system is determining the probability thatextracted information is correct. Specifically, given a corpus, and a set of extractions XC for a class C , we wish to estimateP (x ∈ C|corpus) for each x ∈ XC . In UIE, where hand-labeled examples are unavailable, the task is particularly challenging.How can we automatically assign probabilities of correctness to extractions for arbitrary target concepts, without hand-labeled examples?This paper presents a solution to the above question that applies across a broad spectrum of UIE systems and techniques.It relies on the KnowItAll hypothesis, which states that extractions that occur more frequently in distinct sentences in a corpusare more likely to be correct.KnowItAll hypothesis: Extractions drawn more frequently from distinctsentences in a corpus are more likely to be correct.The KnowItAll hypothesis holds on the Web. Intuitively, we would expect the KnowItAll hypothesis to hold because althoughextraction errors occur (e.g., KnowItAll erroneously extracts California as a City name from the phrase “states con-taining large cities, such as California”), errors occurring in distinct sentences tend to be different.2 Thus, typically a givenerroneous extraction is repeated only a limited number of times. Further, while the Web does contain some misinformation(for example, the statement “Elvis killed JFK” appears almost 200 times on the Web according to a major search engine),this tends to be the exception (the correct statement “Oswald killed JFK” occurs over 3000 times).At Web-scale, the KnowItAll hypothesis can identify many correct extractions due to redundancy: individual facts areoften repeated many times, and in many different ways. For example, consider the TextRunner Web information extractionsystem, which extracts relational statements between pairs of entities (e.g., from the phrase “Edison invented the light bulb,”TextRunner extracts the relational statement Invented(Edison, light bulb)). In an experiment with a set of about500 million Web pages, ignoring the extractions occurring only once (which tend to be errors), TextRunner extracted 829million total statements, of which only 218 million were unique (on average, 3.8 repetitions per statement). Well-knownfacts can be repeated many times. According to a major search engine, the Web contains over 10,000 statements thatThomas Edison invented the light bulb, and this fact is expressed in dozens of different ways (“Edison invented the lightbulb,” “The light bulb, invented by Thomas Edison,” ”Thomas Edison, after ten thousand trials, invented a workable lightbulb,” etc.).Although the KnowItAll hypothesis is simply stated, leveraging it to assess extractions is non-trivial. For example, the10,000th most frequently extracted Film is dramatically more likely to be correct than the 10,000th most frequently ex-tracted US President, due to the relative sizes of the target sets. In UIE, this distinction must be identified without anyhand-labeled data. This paper shows that a probabilistic model of the KnowItAll hypothesis, coupled with the redundancyof the Web, can power UIE for arbitrary target concepts. The primary contributions are discussed below.1.1. The urns model of redundancy in textThe KnowItAll hypothesis states that the probability that an extraction is correct increases with its repetition. But byhow much? How can we precisely quantify our confidence in an extraction given the available textual evidence?We present an answer to these questions in the form of the Urns model—an instance of the classic “balls-and-urns”model from combinatorics. In Urns, extractions are represented as draws from an urn, where each ball in the urn is labeledwith either a correct extraction, or an error—and different labels can be repeated on different numbers of balls. Giventhe frequency distribution in the urn for labels in the target set and error set, we can compute the probability that anobserved label is a target element based on how many times it is drawn. A key insight of Urns is that when the frequencydistributions have predictable structure (for example, in textual corpora the distributions tend to the Zipfian), they can beestimated without hand-labeled data.We prove that when the frequency of each label in the urn is drawn from a mixture of two Zipfian distributions (onefor the target class and another for errors), the parameters of Urns can be learned without hand-labeled data. When thedata exhibits a certain separability criterion, PAC learnability is guaranteed. We also demonstrate that Urns is effective inpractice. In experiments with UIE on the Web, the probabilities produced by the model are shown to be 15 times better, onaverage, when compared with techniques from previous work [14].1 Here, the term class may also refer to relations between multiple strings, e.g. the ordered pair (Chicago, Illinois) is a member of the Locate-dIn class.2 Two sentences are distinct when they are not comprised of exactly the same word sequence. We stipulate that sentences be distinct to avoid placingundue credence in content that is simply duplicated across many different pages, a common occurrence on the Web.728D. Downey et al. / Artificial Intelligence 174 (2010) 726–7481.2. Paper outlineThe paper proceeds as follows. We describe the Urns model in Section 2, experimentally demonstrate its effectivenessin UIE, and detail applications beyond UIE in which the model has been employed. The theoretical results characterizing theUrns model are presented in Section 3. We discuss future work in Section 4, and conclude.2. The URNS modelIn this section, we describe the Urns model for assigning probabilities of correctness to extractions. We begin by formallyintroducing the model, then describe our implementation and a set of experiments establishing the model’s effectivenessfor UIE.The Urns model takes the form of a classic “balls-and-urns” model from combinatorics. We first consider the single urncase, for simplicity, and then generalize to the full multiple Urns Model used in our experiments.We think of IE abstractly as a generative process that maps text to extractions. Extractions repeat because distinctsentences may yield the same extraction. For example, the sentence containing “Scenic towns such as Yakima. . .” and thesentence containing “Washington towns such as Yakima. . .” both lead us to believe that Yakima is a correct extraction ofthe relation City(x).Each potential extraction is modeled as a labeled ball in an urn. A label represents either an instance of the targetrelation, or an error. The information extraction process is modeled as repeated draws from the urn, with replacement.Thus, in the above example, two balls are drawn from the urn, each with the label “Yakima”. The labels are instances of therelation City(x). Each label may appear on a different number of balls in the urn. Finally, there may be balls in the urnwith error labels such as “California”, representing cases where the extraction process generated a label that is not a memberof the target relation.Formally, the parameters that characterize an urn are:• C – the set of unique target labels; |C| is the number of unique target labels in the urn.• E – the set of unique error labels; |E| is the number of unique error labels in the urn.• num(b) – the function giving the number of balls labeled by b where b ∈ C ∪ E. num(B) is the multi-set giving thenumber of balls for each label b ∈ B.Of course, extraction systems do not have access to these parameters directly. The goal of an extraction system is todiscern which of the labels it extracts are in fact elements of C , based on the number of repetitions of each label. Thus, thecentral question we are investigating is: given that a particular label x was extracted k times in a set of n draws from the urn, whatis the probability that x ∈ C ?In deriving this probability formally below, we assume the system has access to multi-sets num(C) and num(E) givingthe number of times the labels in C and E appear on balls in the urn. In our experiments, we provide methods that estimatethese multi-sets in the unsupervised and supervised settings.We derive the probability that an element extracted k of n times is of the target class as follows. First, we have that:P (x appears k times in n draws|x ∈ C) =(cid:2)r∈num(C)(cid:4)k(cid:3)(cid:4)(cid:3)(cid:3)nkrs1 − rs(cid:4)n−k(cid:5)Pnum(x) = r|x ∈ C(cid:6)where s is the total number of balls in the urn, and the sum is taken over possible repetition rates r.Then we can express the desired quantity using Bayes Rule:P (x ∈ C|x appears k times in n draws) = P (x appears k times in n draws|x ∈ C)P (x ∈ C)P (x appears k times in n draws).(1)Note that these expressions include prior information about the label x—for example, P (x ∈ C) is the prior probabilitythat the string x is a target label, and P (num(x) = r|x ∈ C) represents the probability that a target label x is repeatedon r balls in the urn. In general, integrating this prior information could be valuable for extraction systems; however, inthe analysis and experiments that follow, we make the simplifying assumption of uniform priors, yielding the followingsimplified form:Proposition 1.P (x ∈ C|x appears k times in n draws) =(cid:7)(cid:7)r∈num(C)( rr(cid:5)∈num(C∪E)( r(cid:5)s )k(1 − rs )k(1 − r(cid:5)s )n−ks )n−k.D. Downey et al. / Artificial Intelligence 174 (2010) 726–748729Fig. 1. Schematic illustration of the number of distinct labels in the C and E sets with repetition rate r. The “confusion region” is shaded.2.0.1. The uniform special caseFor illustration, consider the simple case in which all labels from C are repeated on the same number of balls. That is,num(ci) = R C for all ci ∈ C , and assume also that num(ei) = R E for all ei ∈ E. While these assumptions are unrealistic (infact, we use a Zipf distribution for num(b) in our experiments), they are a reasonable approximation for the majority oflabels, which lie on the flat tail of the Zipf curve.Define p to be the precision of the extraction process; that is, the probability that a given draw comes from the targetclass. In the uniform case, we have:|C|R C|E|R E + |C|R Cp =.The probability that a particular element of C appears in a given draw is then pC = p/|C|, and similarly p E = (1 − p)/|E|.s )n−k asWe use a Poisson model to approximate the binomial from Proposition 1. That is, we approximate ( r(cid:5)−λ/(nks . Using this approximation, with algebra we have:(cid:6)k!), where λ is rns )k(1 − rλkeP USC(x ∈ C|x appears k times in n draws) ≈1)ken(pC −p E ).1 + |E||C| ( p EpC(2)In general, we expect the extraction process to be noisy but informative, such that pC > p E . Notice that when this istrue, Eq. (2) shows that the odds that x ∈ C increase exponentially with the number of times k that x is extracted, but alsodecrease exponentially with the sample size n.A few numerical examples illustrate the behavior of this equation. The examples assume that the precision p is 0.9. Let|C| = |E| = 2000. This means that R C = 9 × R E —target balls are nine times as common in the urn as error balls. Now, fork = 3 and n = 10,000 we have P (x ∈ C) = 93.0%. Thus, we see that a small number of repetitions can yield high confidencein an extracted label. However, when the sample size increases so that n = 20,000, and the other parameters are unchanged,then P (x ∈ C) drops to 19.6%. On the other hand, if C balls repeat much more frequently than E balls, say R C = 90 × R E(with |E| set to 20,000, so that p remains unchanged), then P (x ∈ C) rises to 99.9%.The above examples enable us to illustrate the advantages of Urns over the noisy-or model used in previous IE work[25,1]. The noisy-or model for IE assumes that each extraction is an independent assertion, correct a fraction p of the time,that the extracted label is correct. The noisy-or model assigns the following probability to extracted labels:P noisy-or(x ∈ C|x appears k times) = 1 − (1 − p)k.Therefore, the noisy-or model will assign the same probability—99.9%—in all three of the above examples. Yet, as explainedabove, 99.9% is only correct in the case for which n = 10,000 and R C = 90 × R E . As the other two examples show, fordifferent sample sizes or repetition rates, the noisy-or model can be highly inaccurate. This is not surprising given that thenoisy-or model ignores the sample size and the repetition rates. Section 2.2 quantifies the improvements over the noisy-orobtained by Urns in practice.2.0.2. Applicability of the Urns modelUnder what conditions does our redundancy model provide accurate probability estimates? We address this questionformally in Section 3, but informally two primary criteria must hold. First, labels from the target set C must be repeatedon more balls in the urn than labels from the E set, as in Fig. 1. The shaded region in Fig. 1 represents the “confusionregion”—if we classify labels based solely on extraction count, half of the labels in this region will be classified incorrectly,even with the ideal classifier and infinite data, because for these examples there simply isn’t enough information to decidewhether they belong to C or E. Thus, our model is effective when the confusion region is relatively small. Secondly, evenfor a small confusion region, the sample size n must be large enough to approximate the two distributions shown in Fig. 1;otherwise the probabilities output by the model will be inaccurate.730D. Downey et al. / Artificial Intelligence 174 (2010) 726–7482.0.3. Multiple urnsWe now generalize our model to encompass multiple urns. When we have multiple extraction mechanisms for the sametarget class, we could simply sum the extraction counts for each example and apply the single-urn model as described inthe previous section. However, this approach forfeits differences between the extraction mechanisms that may be informativefor classification. For example, an IE system might employ several patterns for extracting city names, e.g. “cities includingx” and “x and other towns.” It is often the case that different patterns have different modes of failure, so labels extractedby multiple patterns are generally more likely to be correct than those appearing for a single pattern. Previous work inco-training has shown that leveraging distinct uncorrelated “views” of the data is often valuable [5]. We model this situationby introducing multiple urns, where each urn represents a different pattern.3Instead of n total extractions, in the multi-urn case we have a sample size nm for each urn m ∈ M, with the label forexample x appearing km times. Let A(x, (k1, . . . , km), (n1, . . . , nm)) denote this event. Further, let Am(x, k, n) be the eventthat label x appears k times in n draws from urn m, and assuming that the draws from each urn are independent, we have:Proposition 2.(cid:5)Px ∈ C(cid:5)(cid:8)(cid:8) Ax, (k1, . . . , km), (n1, . . . , nm)(cid:6)(cid:6)(cid:7)(cid:9)=(cid:7)ci ∈Cx∈C∪Em∈M P ( Am(ci, km, nm))(cid:9)m∈M P ( Am(x, km, nm)).With multiple urns, the distributions of labels among balls in the urns are represented by multi-sets numm(C) andnumm(E). Expressing the correlation between numm(x) and numm(cid:5) (x) is an important modeling decision. Multiple urns areespecially beneficial when the repetition rates for elements of C are more strongly correlated across different urns thanthey are for elements of E—that is, when numm(x) and numm(cid:5) (x) are proportionally more similar for x ∈ C than for x ∈ E.Fortunately, this turns out to be the case in practice in IE. We describe our method for modeling multi-urn correlation inSection 2.1.1.2.1. Implementation of UrnsThis section describes how we implement Urns for both UIE and supervised IE, and identifies the assumptions made ineach case.In order to compute probabilities for extracted labels, we need a method for estimating num(C) and num(E). For thepurpose of estimating these sets from labeled or unlabeled data, we assume that num(C) and num(E) are Zipf distributed,−zC . We can then char-meaning that if ci is the ith most frequently repeated label in C , then num(ci) is proportional to iacterize the num(C) and num(E) sets with five parameters: the set sizes |C| and |E|, the shape parameters zC and zE , andthe extraction precision p.2.1.1. Multiple urnsTo model multiple urns, we consider different precisions pm for each urn, but make the simplifying assumption thatthe size and shape parameters are the same for all urns. As mentioned above, we expect repetition rate correlation acrossurns to be higher for elements of the C set than for the E set. We model this correlation as follows: first, elements ofthe C set are assumed to come from the same location on the Zipf curve for all urns, that is, their relative frequencies areperfectly correlated. Some elements of the E set are similar, and have the same relative frequency across urns—we referto these as global errors. However, the rest of the E set is made up of local errors, meaning that they appear for onlyone kind of mechanism (for example, “Eastman Kodak” is extracted as an instance of Film only in phrases involving theword “film”, and not in those involving the word “movie.”). Formally, local errors are labels that are present in some urnsand not in others. Each type of local error makes up some fraction of the E set, and these fractions are the parametersof our correlation model. Assuming this simple correlation model and identical size and shape parameters across urns istoo restrictive in general—differences between mechanisms are often more complex. However, our assumptions allow us tocompute probabilities efficiently (as described below), and don’t appear to hurt performance significantly in practice (i.e.when compared with an “ideal” model as in Section 2.2.1).With this correlation model, if a label x is an element of C or a global error, it will be present in all urns. In terms ofProposition 2, the probability that a label x appears km times in nm draws from m is:(cid:5)(cid:6)Am(x, km, nm)P=(cid:5)(cid:6)kmfm(x)(cid:5)(cid:6)nm−km1 − fm(x)(3)(cid:3)(cid:4)nmkmwhere fm(x) is the frequency of label x. That is,−zCfm(ci) = pm Q C ifm(ei) = (1 − pm)Q E ifor ci ∈ C,−zEfor ei ∈ E.3 We may lump several patterns into a single urn if they tend to behave similarly.D. Downey et al. / Artificial Intelligence 174 (2010) 726–748731In these expressions, i is the frequency rank of the label, assumed to be the same across all urns, and Q C and Q E arenormalizing constants such that(cid:2)(cid:2)−zC =Q C i−zE = 1.Q E ici ∈Cei ∈EFor a local error x which is not present in urn m, P ( Am(x, km, nm)) is 1 if km = 0 and 0 otherwise. Substituting theseexpressions for P ( Am(x, km, nm)) into Proposition 2 gives the final form of our Urns model.2.1.2. Efficient computationA feature of our implementation is that it allows for efficient computation of probabilities. In general, computing the sumin Proposition 2 over the potentially large C and E sets would require significant computation for each label. However, givena fixed number of urns, with num(C) and num(E) Zipf distributed, an integral approximation to the sum in Proposition 2(using a Poisson in place of the binomial in Eq. (3)) can be solved in closed form in terms of incomplete Gamma functions.The details of this approximation and its solution for the single-urn case are given in Section 3.4 The closed form expressioncan be evaluated quickly, and thus probabilities for labels can be obtained efficiently. This solution leverages our assumptionsthat size and shape parameters are identical across urns, and that relative frequencies are perfectly correlated. Findingefficient techniques for computing probabilities under less stringent assumptions is an item of future work.2.1.3. Supervised parameter estimationIn the event that a large sample of hand-labeled training examples is available for each target class of interest, we candirectly estimate each of the parameters of Urns. In our experiments, we use Differential Evolution to identify parametersettings that approximately maximize the conditional log likelihood of the training data [40].5 Differential Evolution isa population-based stochastic optimization technique, appropriate for optimizing the non-convex likelihood function forUrns. Once the parameters are set, the model yields a probability for each extracted label, given the number of times km itappears in each urn and the number of draws nm from each urn.2.1.4. Unsupervised parameter estimationEstimating model parameters in an unsupervised setting requires making a number of assumptions tailored to the spe-cific task. Below, we detail the assumptions employed in Urns for UIE. It is important to note that while these assumptionsare specific to UIE, they are not specific to a particular target class. As argued in [17], UIE systems cannot rely on per-class information—in the form of either assumptions or hand-labeled training examples—if they are to scale to extractinginformation on arbitrary classes that are not specified in advance.Implementing Urns for UIE requires a solution to the challenging problem of estimating num(C) and num(E) using onlyuntagged data. Let U be the multi-set consisting of the number of times each unique label was extracted in a given corpus.|U | is the number of unique labels encountered, and the sample size n =(cid:7)r∈U r.In order to learn num(C) and num(E) without hand-labeled data, we make the following assumptions:• Because the number of different possible errors is nearly unbounded, we assume that the error set is very large.6• We assume that both num(C) and num(E) are Zipf distributed where the zE parameter is set to 1.• In our experience with KnowItAll, we found that while different extraction rules have differing precision, each rule’sprecision is stable across different classes [17]. For example, the precision of the extractor “cities such as x” and “insectssuch as y” are similar. Urns takes this precision as an input. To demonstrate that Urns is not overly sensitive to thisparameter, we chose a fixed value (0.9) and used it as the precision pm for all urns in our experiments.7 Section 2.2.5provides evidence that the observed p value tends to be relatively stable across different target classes.We then use Expectation Maximization (EM) over U in order to arrive at appropriate values for |C| and zC (these twoquantities uniquely determine num(C) given our assumptions). Our EM algorithm proceeds as follows:1. Initialize |C| and zC to starting values.2. Repeat until convergence:(a) E-step Assign probabilities to each element of U using Proposition (1).(b) M-step Set |C| and zC from U using the probabilities assigned in the E-step (details below).4 For the multi-urn solution, which is obtained through a symbolic integration package and therefore complicated, we refer the reader to the Javaimplementation of the solution which is available for download—see [12], Appendix A.5 Specifically, we use the Differential Evolution routine built into Mathematica 5.0.6 In our experiments, we set |E| = 106. A sensitivity analysis showed that changing |E| by an order of magnitude, in either direction, resulted in onlysmall changes to our results.7 A sensitivity analysis showed that choosing a substantially higher (0.95) or lower (0.80) value for pm still resulted in Urns outperforming the noisy-ormodel by at least a factor of 8 and PMI by at least a factor of 10 in the experiments described in Section 2.2.1.732D. Downey et al. / Artificial Intelligence 174 (2010) 726–748We obtain |C| and zC in the M-step by first estimating the rank-frequency distribution for labels from C in the untaggeddata U . From U and the probabilities found in the E-step, we can obtain E C [k], the expected number of labels from C thatwere extracted k times for k (cid:2) 1 (the k = 0 case is detailed below). We then round these fractional expected counts into adiscrete rank-frequency distribution with a number of elements equal to the expected total number of labels from C in thek E C [k]. We obtain zC by fitting a Zipf curve to this rank-frequency distribution by linear regression on auntagged data,log–log scale.8(cid:7)Lastly, we set |C| =k E C [k]+ unseen, where we estimate the number of unseen labels of the C set (i.e. those with k = 0)using Good–Turing estimation [20]. Good–Turing estimation provides an estimate of the probability mass of the unseen labels(specifically, the estimate is equal to the expected fraction of the draws from C that extracted labels seen only once). Toconvert this probability into a number of unseen labels, we simply assume that each unseen label has probability equal tothat of the least frequent seen label. A potentially more accurate method would choose unseen such that the actual numberof unique labels observed is equal to that expected by the model (where the latter is measured e.g. by sampling). Suchmethods are an item of future work.(cid:7)This unsupervised learning strategy proved effective for target classes of different sizes; for example, Urns learned pa-rameters such that the number of elements of the Country relation with non-negligible extraction probability was abouttwo orders of magnitude smaller than that of the Film and City classes, which approximately agrees with the actualrelative sizes of these sets.2.2. Urns: Experimental resultsHow accurate is Urns at assigning probabilities of correctness to extracted labels? In this section, we answer this ques-tion by comparing the accuracy of Urns’s probabilities against other methods from previous work.This section begins by describing our experimental results for IE under two settings: unsupervised and supervised. Wefirst describe two unsupervised methods from previous work: the noisy-or model and PMI. We then compare Urns withthese methods experimentally, and lastly compare Urns with several baseline methods in a supervised setting.We evaluated our algorithms on extraction sets for the classes City(x), Film(x), Country(x), and Mayo-rOf(x,y), taken from experiments with the KnowItAll system performed in [17]. The sample size n was 64,605 forCity, 135,213 for Film, 51,390 for Country and 46,858 for MayorOf. The extraction patterns were partitioned into urnsbased on the name they employed for their target relation (e.g. “country” or “nation”) and whether they were left-handed(e.g. “countries including x”) or right-handed (e.g. “x and other countries”). We chose this partition because it results inextraction mechanisms that make relatively uncorrelated errors, as assumed in the multiple-urns model. For example, thephrase “Toronto, Canada and other cities” will mislead a right-handed pattern into extracting “Canada” as a City candi-date, whereas a left-handed pattern is far less prone to this error. Each combination of relation name and handedness wastreated as a separate urn, resulting in four urns for each of City(x), Film(x), and Country(x), and two urns forMayorOf(x, y).9,10For each relation, we tagged a random sample of 1000 extracted labels, using external knowledge bases (the TipsterGazetteer for cities and the Internet Movie Database for films) and manually tagging those instances not found in a knowl-edge base. For Country and MayorOf, we manually verified correctness for all extracted labels, using the Web. Countrieswere marked correct provided they were a correct name (including abbreviations) of a current country, and mayors weremarked correct if the person was a mayor of the city at some point in time. In the UIE experiments, we evaluate ouralgorithms on all 1000 examples, and in the supervised IE experiments we perform 10-fold cross validation.2.2.1. UIE experimentsWe compare Urns against two other methods for unsupervised information extraction. First, in the noisy-or model usedm∈M (1 − pm)km , where pmin previous work, an extracted label appearing km times in each urn is assigned probability 1 −is the extraction precision for urn m. We describe the second method below.(cid:9)Our previous work on KnowItAll used Pointwise Mutual Information (PMI) to obtain probability estimates for extractedlabels [17]. Specifically, the PMI between an extracted label and a set of automatically generated discriminator phrases (e.g.,“movies such as x”) is computed from Web search engine hit counts. These PMI scores are used as features in a Naive BayesClassifier (NBC) to produce a probability estimate for the label. The NBC is trained using a set of automatically bootstrappedseed instances. The positive seed instances are taken to be those having the highest PMI with the discriminator phrases8 To help ensure that our probability estimates are increasing with k, if zC falls below 1, we adjust zE to be less than zC .9 Draws from Urns are intended to represent independent evidence. Because the same sentence can be duplicated across multiple different Webdocuments, in these experiments we consider only each unique sentence containing an extraction to be a draw from Urns. In experiments with otherpossibilities, including counting the number of unique documents producing each label, or simply counting every extraction of each label, we found thatfor UIE, performance differences between the various approaches were small compared to the differences between Urns and other methods.10 In the unsupervised setting, we assumed that the fraction of errors in the urns that are local is 0.1, and that errors appearing for only left- or onlyright-handed patterns were equally prevalent to those appearing for only one label. The only exception was the City class, where because the target classis the union of the two class names (“city” and “town”) rather than the intersection (as with “film” and “movie”), we assumed that no local errors appearedfor only one name. Altering these settings (or indeed, simply using a single urn—see Section 2.2.4) had negligible impact on the results in Fig. 2.D. Downey et al. / Artificial Intelligence 174 (2010) 726–748733Fig. 2. Deviation of average log likelihood from the ideal for four relations (lower is better). On average, Urns outperforms noisy-or by a factor of 15, andPMI by a factor of 20.Table 1Improved efficiency due to Urns. The top row reports the number of search engine queriesmade by KnowItAll using PMI divided by the number of queries for KnowItAll using Urns.The bottom row shows that PMI’s queries increase with k—the average number of distinctlabels for each relation. Thus, speedup tends to vary inversely with the average number oftimes each label is drawn.SpeedupAverage kCity17.3×3.7Film9.5×4.0MayorOf1.9×20.7Country3.1×23.3after the bootstrapping process; the negative seeds are taken from the positive seeds of other relations, as in other work(e.g., [25]).Although PMI was shown in [17] to rank extracted labels fairly well, it has two significant shortcomings. First, obtainingthe hit counts needed to compute the PMI scores is expensive, as it requires a large number of queries to a public Websearch engine (or, alternatively, the expensive construction of a local Web-scale inverted index). Second, the seeds producedby the bootstrapping process are often noisy and not representative of the overall distribution of extractions [39]. Thiscombined with the probability polarization introduced by the NBC tends to give inaccurate probability estimates.2.2.2. Discussion of UIE resultsThe results of our unsupervised experiments are shown in Fig. 2. We plot deviation from the ideal log likelihood—definedas the maximum achievable log likelihood given our feature set. Specifically, for each class C define an ideal model P ideal(x)equal to the fraction of test set labels with the same extraction counts as x that are correct. We define the ideal loglikelihood as:ideal log likelihood =log P ideal(x) +.(4)(cid:2)x∈C(cid:2)logx∈E(cid:5)(cid:6)1 − P ideal(x)Our experimental results demonstrate that Urns overcomes the weaknesses of PMI. First, Urns’s probabilities are farmore accurate than PMI’s, achieving a log likelihood that is a factor of 20 closer to the ideal, on average (Fig. 2). Second,Urns is substantially more efficient as shown in Table 1.This efficiency gain requires some explanation. These experiments were performed using the KnowItAll system, whichrelies on queries to Web search engines to identify Web pages containing potential extractions. The number of queriesKnowItAll can issue daily is limited, and querying over the Web is, by far, KnowItAll’s most expensive operation. Thus,number of search engine queries is our efficiency metric. Let d be the number of discriminator phrases used by the PMIexplained above. The PMI method requires O (d) search engine queries to compute the PMI of each extracted label fromsearch engine hit counts. In contrast, Urns computes probabilities directly from the set of extractions—requiring no additionalqueries, which cuts KnowItAll’s queries by factors ranging from 1.9 to 17.As explained in Section 2.0.1, the noisy-or model ignores target set size and sample size, which leads it to assign prob-abilities that are far too high for the Country and MayorOf relations, where the average number of times each label isextracted is high (see bottom row of Table 1). This is further illustrated for the Country relation in Fig. 3. The noisy-ormodel assigns appropriate probabilities for low sample sizes, because in this case most extracted labels are in fact correct,as predicted by the noisy-or model. However, as sample size increases, the fraction of correct labels decreases—and thenoisy-or estimate worsens. On the other hand, Urns avoids this problem by accounting for the interaction between targetset size and sample size, adjusting its probability estimates as sample size increases. Given sufficient sample size, Urns per-forms close to the ideal log likelihood, improving slightly with more samples as the estimates obtained by the EM process734D. Downey et al. / Artificial Intelligence 174 (2010) 726–748Fig. 3. Deviation of average log likelihood from the ideal as sample size varies for the Country relation (lower is better). Urns performs close to the idealgiven sufficient sample size, whereas noisy-or becomes less accurate as sample size increases.become more accurate. Overall, Urns assigns far more accurate probabilities than the noisy-or model, and its log likelihoodis a factor of 15 closer to the ideal, on average. The very large differences between Urns and both the noisy-or model andPMI suggest that, even if the performance of Urns degrades in other domains, it is quite likely to still outperform both PMIand the noisy-or model.Our computation of log-likelihood contains a numerical detail that could potentially influence our results. To avoid thepossibility of a likelihood of zero, we restrict the probabilities generated by Urns and the other methods to lie within therange (0.00001, 0.99999). Widening this range tended to improve Urns’s performance relative to the other methods, as thisincreases the penalty for erroneously assigning extreme probabilities—a problem more prevalent for PMI and noisy-or thanfor Urns. If we narrow the range by two digits of precision, to (0.001, 0.999), Urns still outperforms PMI by a factor of15, and noisy-or by a factor of 13. Thus, we are comfortable that the differences observed are not an artifact of this designdecision.Lastly, although we focus our evaluation on the quality of each method’s probability estimates in terms of likelihood,the advantage of Urns is also reflected in other metrics such as classification accuracy. When we convert each method’sprobability estimate into a classification (positive for a label iff the probability estimate is greater than 0.5), we find thatUrns has an average accuracy of approximately 81%, compared with PMI at 63% and noisy-or at 47%. Thus, Urns decreasesclassification error over the previous methods by a factor of 1.9× to 2.8×. Urns ranks the majority of extracted labels ina manner similar to the noisy-or model (which ranks by overall frequency). Thus, Urns offers comparable performance tonoisy-or in terms of e.g. area under the precision/recall curve [6]. However, the correlations captured by multiple urns canimprove the ranking of sufficiently frequent labels, as detailed in Section 2.2.4.2.2.3. Supervised IE experimentsWe compare Urns with three supervised methods. All methods utilize the same feature set as Urns, namely the extrac-tion counts km.• noisy-or – Has one parameter per urn, making a set of M parameters (h1, . . . , hM ), and assigns probability equal to(cid:10)1 −(1 − hm)km .m∈M• logistic regression – Has M + 1 parameters (a, b1, b2, . . . , bM ), and assigns probability equal to1(cid:7)m∈M kmbm.1 + ea+• SVM – Consists of an SVM classifier with a Gaussian kernel. To transform the output of the classifier into a probability,we use the probability estimation built-in to LIBSVM [8], which is based on logistic regression of the SVM decisionvalues.Parameters maximizing the conditional likelihood of the training data were found for the noisy-or and logistic regressionmodels using Differential Evolution.11 For those models and Urns, we performed 20 iterations of Differential Evolution11 For logistic regression, different convex optimization methods are applicable; however, in our experiments the Differential Evolution routine appearedto converge to an optimum, and we do not believe the choice of optimization method impacted the logistic regression results.D. Downey et al. / Artificial Intelligence 174 (2010) 726–748735Table 2Supervised IE experiments. Deviation from the ideal log likelihood for each method and each relation (lower is better). The overall performance differencesare small, with Urns 19% closer to the ideal than noisy-or, on average, and 10% closer than logistic regression. The overall performance of SVM is close tothat of Urns.noisy-orlogistic regressionSVMUrnsCity0.04390.04660.04440.0418Film0.12560.08930.08650.0764Mayor0.08570.06550.06590.0721CountryAverage0.07950.10200.07690.08230.08370.07590.06840.0681Table 3Label-precision of the K highest-ranked extracted labels for varying valuesof K between 10 and 200. Across the five K values shown, Urns reduceserror over the single-urn model by an average of 29%.Number of highest-ranked extracted labelsSingle-urn10205010020010.98750.9250.83750.7075Urns110.9550.8450.71using 400 distinct search points. In the SVM case, we performed grid search to find the kernel parameters giving the bestlikelihood performance for each training set—this grid search was required to get acceptable performance from the SVM onour task.The results of our supervised learning experiments are shown in Table 2. Urns, because it is more expressive, is able tooutperform the noisy-or and logistic regression models. In terms of deviation from the ideal log likelihood, we find that onaverage Urns outperforms the noisy-or model by 19%, logistic regression by 10%, but SVM by only 0.4%.2.2.4. Benefit from multiple urnsThe previous results use the full multi-urn model. How much of Urns’s large performance advantage in UIE is due tomultiple urns?In terms of likelihood, as measured in Fig. 2, we found that the impact of multiple urns is negligible. This is primarilybecause the majority of extracted labels occur only a handful of times, and in these cases the multiple-urn model lacksenough data to estimate the correlation of counts across urns.Multiple urns can offer some performance benefit, however, for more commonly extracted labels. We evaluated theeffect of multiple urns for UIE across the four relations shown in Fig. 2, computing the average label-precision at K , equalto the fraction of the K highest-probability labels which are correct. The results under the single-urn and full Urns modelare shown in Table 3 for varying K . The full Urns model always performs at least as well as the single-urn model, andsometimes provides much higher precision. In fact, using multiple urns reduces the error by 29% on average for the five Kvalues shown in the table.2.2.5. Is p a “universal constant”?Our UIE experiments employed an extraction precision parameter p of 0.9. While Urns still massively outperformsprevious methods even if this value is adjusted to 0.8 or 0.95, the accuracy of Urns’s probabilities does degrade as p isaltered away from 0.9.In this section, we attempt to measure how consistent the observed p value is across varying classes. This experimentdiffers somewhat from those presented above. In order to test across a wide variety of classes, we moved beyond theKnowItAll experiments from [17] and used the TextRunner system to provide instances of classes [3]. To choose classesto investigate, we randomly selected 12 nouns from WordNet for which there were at least 100 extractions (not necessarilyunique) in TextRunner. We excluded nouns which were overly general such that nearly any extraction would be correct(e.g., the class Example) and nouns which are rarely or never used to name concrete instances (e.g., the class Purchases).The results in this section were compiled by querying TextRunner for 100 sentences containing extractions for each class.12While TextRunner provides greater coverage than KnowItAll, precision in general is lower. One of the inaccuracies of theTextRunner system is that it often fails to delimit the boundaries of extractions properly (e.g., it extracts the phrase “alkanesor cycloalkanes” as an instance of the Solvents class). We found that we could improve the precision of TextRunner byover 20% on average by post-processing all extractions, breaking on conjunctions or punctuation (i.e. the previous examplebecomes simply “alkanes”). Our results employ this heuristic.The results of the experiment are shown in Table 4. For each class, “p Observed” gives the fraction of the 100 extractionstagged correct (by manual inspection). The average p value observed across classes of 0.84 is lower than the value of 0.912 The list of excluded nouns and the labeled extractions for each selected class are available for download; see [12], Appendix A.736D. Downey et al. / Artificial Intelligence 174 (2010) 726–748Table 4Average p values for various classes, measured from 100 hand-tagged examples per class. Threeof the 12 classes have p values in bold, indicating a statistically significantly difference from themean of 0.84 (significance level of 0.01, Fisher Exact Test). However, if we adjust the estimate of pper class according to how frequently it occurs in the “such as” pattern (using the factor hclass ; seetext), none of the resulting p + hclass values are significantly different from the mean.Classsolventsdevicesthinkersrelaxantsmushroomsmechanismsresortsfliestoneswoundsmachinesculturesp ObservedHits(class such as)Hits(class)p Observed + hclass0.980.930.930.920.860.850.850.840.770.770.690.670.2010.0220.0130.0100.0010.0170.0020.00040.0010.0020.0020.0020.850.870.890.890.900.800.880.930.830.800.710.70we use in our previous experiments; this reflects the relatively lower precision of TextRunner as well as the increaseddifficulty of extracting common nouns (versus the proper noun extractions used previously). The results show that whilethere is substantial regularity in observed p values, the values are not perfectly consistent. In fact, three classes (with “pObserved” values in bold) differ significantly from the average observed p value (at significance level of 0.01, Fisher ExactTest).Given that we observe variability in p values across classes, an important question is whether the correct p value for agiven class, pclass, can be predicted. We observed empirically that the precision of extractions for a class increases with howrelatively frequently the class name is used in extraction patterns. As an example, the phrase “cultures such as x” appearsinfrequently relative to the word “cultures,” as shown in Table 4 in terms of Web hit counts obtained from a search engine.In turn, the class Cultures exhibits a relatively low p value. Intuitively, this result makes sense—class names which aremore “natural” for naming instances should both appear more frequently in extraction patterns, and provide more preciseextractions.We can exploit the above intuition by adjusting the estimate of extraction precision for each class by a factor hclass. Forillustration, based on the values in Table 4, we devised the following adjustment factor:hclass = 0.08(cid:3)−2.36 − log10(cid:4)Hits(class such as)Hits(class).(5)The adjustment factor can give us a more accurate estimate of the precision for a given class pclass = p − hclass.Obviously, the expression hclass is heuristic and could be further refined using additional experiments. Nonetheless, ad-justing by the factor does allow us to obtain better precision estimates across classes. The quantity “p Observed + hclass”has only 57% of the variance of the original “p Observed” (and the same mean, by construction). Further, none of the ob-served differences of “p Observed + hclass” are statistically significantly different from the original mean, using the samesignificance test employed previously.Lastly, we should mention that even without any adjustment factor, the variance in p value across classes is not substan-tially greater than that employed in our sensitivity analysis in Section 2.2. Thus, we expect the performance advantages ofUrns over the noisy-or and PMI models to extend to these other classes as well.2.3. Urns: Other applicationsUrns is a general model. For any classification task, if one of the features represents a count of observations following amixture of Zipf distributions as assumed by Urns, the model can be employed. In this section, we highlight three examplesof how the Urns model has been applied to tasks other than that of assigning probabilities of correctness to extractions.2.3.1. Estimating UIE precision and recallAn attractive feature of Urns is that it enables us to estimate its expected recall and precision as a function of samplesize. If the distributions in Fig. 1 cross at the dotted line shown then, given a sufficiently large sample size n, expected recallwill be the fraction of the area under the C curve lying to the right of the dotted line.For a given sample size n, define τn to be the least number of appearances k at which an extracted label is more likelyto be from the C set than the E set (given the distributions in Fig. 1, τn can be computed using Proposition 1). Then wehave:E[TruePositives] = |C| −(cid:2)τn−1(cid:2)(cid:4)(cid:3)(cid:3)n(cid:4)k(cid:3)(cid:4)n−krs1 − rsr∈num(C)k=0kD. Downey et al. / Artificial Intelligence 174 (2010) 726–748737Table 5Estimating precision and recall in UIE. Listed is the Urns model estimate for precision and recall, along with the actual measured quantities, for four classes.The major differences between the classes—that the MayorOf and Country classes have roughly two orders of magnitude lower recall than the Cityand Film classes—is qualitatively reflected by the model.CityCountryFilmMayorOfn646055139013521346858E[Recall]Actual recallE[Precision]Actual precision1290037259005814300176234001580.780.630.790.620.840.770.680.79where we define “true positives” to be the number of extracted labels ci ∈ C for which the model assigns probabilityP (ci ∈ C) > 0.5.The expected number of false positives is similarly:(cid:4)k(cid:4)(cid:3)τn−1(cid:2)(cid:2)(cid:3)nrE[FalsePositives] = |E| −r∈num(E)k=0ks(cid:3)1 − rs(cid:4)n−k.The expected precision of the system can then be approximated as:E[Precision] ≈E[TruePositives]E[FalsePositives] + E[TruePositives].To illustrate the potential benefit of the above calculations and evaluate their accuracy, we computed expected recall andprecision for the particular num(C) and num(E) learned (in the unsupervised setting) in our experiments in Section 2.2.The results appear in Table 5. The recall estimates are within 11% of the actual recall (that is, the estimated number ofcorrect examples in our set of extracted labels, based on the hand-tagged test set) for the City and Film classes. Further,the estimates reflect the important qualitative difference between the large City and Film classes as compared with thesmaller MayorOf and Country classes.Were we to increase the sample size n for the Film class and the Country class each to 1,000,000, the model predictsthat we would increase our Film recall by 81%, versus only 4% for Country. Thus, the above equations allow an informa-tion extraction system to dynamically choose how to allocate resources to match given precision and recall goals, even inthe absence of hand-labeled data.2.3.2. Estimating the functionality of relationsKnowledge of which relations in a knowledge base are functional is valuable for a variety of different tasks. Previous workhas shown that knowledge of functional relations can be used to automatically detect contradictions in text [11,34], and toautomatically identify extractor errors in IE [1]. For example, if we know that the Headquartered relation is functionaland we see one document asserting that Intel is headquartered in Santa Clara, and another asserting it is headquartered inPhoenix, we can determine that either the documents contradict each other, or we have made an error in extraction. In thissection, we illustrate how Urns can be used to automatically compute the probability that a phrase denotes a functionalrelation.The discussion in this section is based on a set of extracted tuples. An extracted tuple takes the form R(x, y) where(roughly) x is the subject of a sentence, y is the object, and R is a phrase denoting the relationship between them. If therelation denoted by R is functional, then typically the object y is a function of the subject x. Thus, our discussion focuseson this possibility, though the analysis is easily extended to the symmetric case.The main evidence that a relation R(x, y) is functional comes from the distribution of y values for a given x value. If Rdenotes a function and x is unambiguous, then we expect the extractions to be predominantly a single y value, with a fewoutliers due to noise.Example A in Fig. 4 has strong evidence for a functional relation. 66 out of 70 extractions for was_born_in (Mozart,PLACE) have the same y value. An ambiguous x argument, however, can make a functional relation appear non-functional.Example B refers to multiple real-world individuals named “John Adams” and has a distribution of y values that appearsless functional than example C, which has a non-functional relation.Logically, a relation R is functional in a variable x if it maps it to a unique variable y: ∀x, y1, y2 R(x, y1) ∧ R(x, y2) ⇒y1 = y2. Thus, given a large random sample of ground instances of R, we could detect with high confidence whether R isfunctional. In text, the situation is far more complex due to ambiguity, polysemy, synonymy, and other linguistic phenomena.To decide whether R is functional in x for all x, we first consider how to detect whether R is locally functional fora particular value of x. We later combine the local functionality probabilities to estimate the global functionality of arelation.13 Local functionality for a given x can be modeled in terms of the global functionality of R and the ambiguity of x.13 We compute global functionality as the average local scores, weighted by the probability that x is unambiguous.738D. Downey et al. / Artificial Intelligence 174 (2010) 726–748Fig. 4. Functional relations such as example A have a different distribution of y values than non-functional relations such as C. Ambiguous x argument asin B, however, can make a functional relation appear non-functional.We later outline an EM-style algorithm that alternately estimates the probability that R is functional and the probabilitythat x is ambiguous.Let θ fR be the probability that R(x, ·) is locally functional for a random x, and let Θ f be the vector of these parametersx represents the probability that x is locally unambiguous for random R, and Θ u the vectoracross all relations R. Likewise, θ ufor all x.We wish to determine the maximum a posteriori (MAP) functionality and ambiguity parameters given the observed data D,that is arg maxΘ f ,Θ u P (Θ f , Θ u|D). By Bayes Rule:(cid:6)(cid:5)(cid:6)(cid:6)(cid:5)(cid:5)PΘ f , Θ u|D∝ PD|Θ f , Θ uPΘ f , Θ u.(6)We outline a generative model for the data, P (D|Θ f , Θ u). Let R∗x indicate the event that the relation R is locally func-tional for the argument x, and that x is locally unambiguous for R. Also, let D indicate the set of observed tuples, and defineD R(x,·) as the multi-set containing the frequencies for extractions of the form R(x, ·).Let us assume that the event Rx , and further assume that given these two parameters, localambiguity and local functionality are conditionally independent. We obtain the following expression for the probability ofR∗x depends only on θ fR and θ u∗x given the parameters:= θ f|Θ f , Θ uRP(cid:6)(cid:5)∗xR θ ux .We assume each set of data D R(x,·) is generated independently of all other data and parameters, given Rthe above we have:(cid:5)D|Θ f , Θ uP(cid:6)=(cid:10)(cid:5)(cid:5)PD R(x,·)|R(cid:6)∗xθ fR θ ux+ P(cid:5)D R(x,·)|¬R∗x(cid:6)(cid:5)R,x1 − θ fR θ ux(cid:6)(cid:6).∗x . From this and(7)or not RThese independence assumptions allow us to express P (D|Θ f , Θ u) in terms of distributions over D R(x,·) given whether∗x holds. We use a single-urn model to estimate these probabilities based on binomial distributions.(cid:7)Let k = max D R(x,·), and let n =D R(x,·); we will approximate the distribution over D R(x,·) in terms of k and n. In thesingle-urn model, if R(x, ·) is locally functional and unambiguous, k has a binomial distribution with parameters n and p,where p is the precision of the extraction process. If R(x, ·) is not locally functional and unambiguous, then we expect kto typically take on smaller values. Empirically, we find that the underlying frequency of the most frequent element in the¬R∗x case tends to follow a Beta distribution.Under the model, the probability of the evidence given R∗x is:(cid:5)PD R(x,·)|R(cid:6)∗x(cid:5)k, n|R(cid:6)∗x=≈ Ppk(1 − p)n−k.(cid:4)(cid:3)nkAnd the probability of the evidence given ¬R∗x is:(cid:5)PD R(x,·)|¬R(cid:6)∗x(cid:5)k, n|¬R(cid:6)∗x=≈ P(cid:4) 1(cid:11)(cid:3)nk0(cid:5)k+α f −1(1 − p(cid:5))n+β f −1−kpB(α f , β f )(cid:5) =dp(cid:6)(cid:5)Γ (n − k + β f )Γ (α f + k)nkB(α f , β f )Γ (α f + β f + n),where n is the sum over D R(x,·), Γ is the Gamma function and B is the Beta function. α f and β f are the parameters of theBeta distribution for the ¬R∗x case (in practice, these are estimated empirically).Substituting Eq. (9) into Eq. (7) and applying an appropriate prior gives the probability of parameters Θ f and Θ u giventhe observed data D. However, Eq. (7) contains a large product of sums—with two independent vectors of coefficients, Θ fand Θ u —making it difficult to optimize analytically.If we knew which arguments were ambiguous, we would ignore them in computing the functionality of a relation. Like-wise, if we knew which relations were non-functional, we would ignore them in computing the ambiguity of an argument.Instead, we initialize the Θ f and Θ u arrays randomly, and then execute an EM-style algorithm to arrive at a high-probabilitysetting of the parameters.Note that if Θ u is fixed, we can compute the expected fraction of locally unambiguous arguments x for which R is locallyfunctional, using D R(x(cid:5),·) and Eq. (9). Likewise, for fixed Θ f , for any given x we can compute the expected fraction of locallyfunctional relations R that are locally unambiguous for x.(8)(9)D. Downey et al. / Artificial Intelligence 174 (2010) 726–748739Specifically, we repeat until convergence:1. Set θ fR2. Set θ ux= 1sR= 1sx(cid:7)(cid:7)x P (RR P (R∗x∗x|D R(x,·))θ u|D R(x,·))θ fx for all R.R for all x.(cid:7)In both steps above, the sums are taken over only those x or R for which D R(x,·) is non-empty. Also, the normalizersR =x and likewise sx =(cid:7)x θ uR θ fR .By iteratively setting the parameters to the expectations in steps 1 and 2, we arrive at a good setting of the parameters.The above algorithm is experimentally investigated in [34], showing that the technique effectively identifies functionalrelations, and can power effective contradiction detection.2.3.3. Synonym resolutionThe last application of Urns we will discuss is that of resolving which strings refer to the same objects or relations.In text, the same object is often referred to by multiple distinct names—“U.S.” and “United States” each refer to the samecountry, for example. Likewise, relationships between objects are often expressed as multiple distinct paraphrases (e.g., “xis the capital of y” and “x, capital of y”).The Resolver system performs Synonym Resolution—taking as input a set of extracted tuples (as discussed above, e.g., Is-CapitalOf(D.C., United States)) and returning a set of clusters, where each cluster contains coreferential objectstrings or relationship strings [42].Here we provide a high-level description of how Resolver employs an Urns-like model, deferring to [42] for the details.Consider the task of determining whether two strings s1 and s2 refer to the same object, based on a set of tuples eachincluding either s1 or s2 as an argument. Resolver specifies a urn-based generative process for the observed tuples; namely,the set of potential tuples for si are modeled as labels on balls in a urn, and the actual observed tuples involving si aremodeled as draws from the urn. Resolver assumes that if s1 and s2 refer to the same object, then the urn contents for s1are maximally similar to those for s2; otherwise, the two urns can differ to a greater or lesser degree. With this assumption,Resolver computes the probability that s1 and s2 co-refer based on how frequently they participate in similar tuples. Thismethod is shown to be effective for resolving synonymous strings in practice.2.4. Related workIn contrast to the bulk of previous IE work, our focus is on unsupervised IE (UIE) where Urns substantially outperformsprevious methods (Fig. 2).In addition to the noisy-or models we compare against in our experiments, the IE literature contains a variety of heuris-tics using repetition as an indication of the veracity of extracted information. For example, Riloff and Jones [33] rankextractions by the number of distinct patterns generating them, plus a factor for the reliability of the patterns. Our work isintended to formalize these heuristic techniques, and unlike the noisy-or models, we explicitly model the distribution of thetarget and error sets (our num(C) and num(E)), which is shown to be important for good performance in Section 2.2.1. Theaccuracy of the probability estimates produced by the heuristic and noisy-or methods is rarely evaluated explicitly in the IEliterature, although most systems make implicit use of such estimates. For example, bootstrap-learning systems start witha set of seed instances of a given relation, which are used to identify extraction patterns for the relation; these patternsare in turn used to extract further instances (e.g. [33,25,1,30]). As this process iterates, random extraction errors result inoverly general extraction patterns, leading the system to extract further erroneous instances. The more accurate estimatesof extraction probabilities produced by Urns would help prevent this “concept drift.”Skounakis and Craven [37] develop a probabilistic model for combining evidence from multiple extractions in a super-vised setting. Their problem formulation differs from ours, as they classify each occurrence of an extraction, and then use abinomial model along with the false positive and true positive rates of the classifier to obtain the probability that at leastone occurrence is a true positive. Similar to the above approaches, they do not explicitly account for sample size n, nor dothey model the distribution of target and error extractions.Culotta and McCallum [10] provide a model for assessing the confidence of extracted information using conditional ran-dom fields (CRFs). Their work focuses on assigning accurate confidence values to individual occurrences of an extracted fieldbased on textual features. This is complementary to our focus on combining confidence estimates from multiple occurrencesof the same extracted label. In fact, each possible feature vector processed by the CRF in [10] can be thought of as a virtualurn m in our Urns. The confidence output of Culotta and McCallum’s model could then be used to provide the precision pmfor the urn.Our UIE task is related to previous work in automatically devising logical statements from text [24,36] and unsupervisedsemantic role labeling [41,21,32]. UIE is distinct in that the target output is a knowledge base of factual relations, ratherthan an interpretation of text in terms of logic or labeled semantic roles. Because our UIE approach operates over a largecorpus, we do not attempt to identify all semantic assertions in the text corpus. Instead, we focus on only factual assertionsthat can be identified automatically at relatively high precision (using e.g. extraction patterns), and present methods forcombining this evidence at Web-scale.740D. Downey et al. / Artificial Intelligence 174 (2010) 726–748Our work is similar in spirit to BLOG, a language for specifying probability distributions over sets with unknown objects[28]. As in our work, BLOG models can express observations as draws from an unknown set of balls in an urn. WhereasBLOG is intended to be a general modeling framework for probabilistic first-order logic with varying sets of objects, ourwork is directed at modeling redundancy in IE. We also provide supervised and unsupervised learning methods for ourmodel that are effective for data sets containing many thousands of examples, along with experiments demonstrating theirefficacy in practice.One of the problems our EM-based algorithm for learning Urns parameters must solve is estimating the parameter |C|,the size of the target set. This problem has commonalities with the classic “capture–recapture” problem from ecology, inwhich the goal is to estimate the size of an animal population by capturing and marking a sample of the population, thenre-sampling at a later time [31]. There are a number of significant differences between the capture–recapture problem andestimating Urns parameters, however. First, Urns attempts to learn the parameter |C| from observations which are co-mingled with samples from a confounding error distribution. Second, Urns must also characterize how the frequencies ofthe target set vary (in terms of the Zipfian shape parameter zC ). In order to overcome these additional parameter estimationdifficulties, Urns exploits problem structures often found in textual domains, such as the fact that extraction frequenciestend to be Zipf distributed.3. URNS: Theoretical resultsThe Urns model was shown in the previous section to be effective in practice for UIE and other applications. In thissection, we analyze the Urns model theoretically. To better understand the behavior of Urns, we would like the be able tocharacterize how class probability increases with extraction count. Further, we would like a guarantee on Urns’s accuracygiven sufficient unlabeled data. How does accuracy increase with sample size? Can the parameters of the model be learnedfrom unlabeled data in general?Specifically, we investigate the following questions in the context of a single-urn model:1. In the model, at what rate does the probability that an extracted label is of the target class increase with the numberof extractions k?2. What are sufficient conditions for accurate classification, given the parameters of the model? What sample size n issufficient to achieve a given level of classification accuracy?3. Can the parameters of the model be learned from unlabeled data?4. Can the Urns model provide accurate classifications for extractions, i.e. is PAC-learnability guaranteed?We begin by considering the first two questions in the uniform special case previously introduced in Section 2.0.1.The uniform case, while not fully realistic, does provides qualitatively interpretable results useful for illustration. We thenaddress all four questions in the more realistic Zipfian model used in our experiments.In the below, for notational convenience we will utilize in place of the multi-set num(C) a multi-set F C containing, forf = 1.each element of C , the relative fraction of balls labeled with that element. We define F E similarly, such thatThen the following expression (adapted from Eq. (1)) specifies the probability that x is an element of C given the observedvalues of k and n:f ∈F C ∪F E(cid:7)P (x ∈ C|k, n) =(cid:7)(cid:7)f k(1 − f )n−kf ∈F Cf ∈F C ∪F Ef k(1 − f )n−k.We will also refer to the classifier output by Urns, which is a function from extracted labels to a binary value, indicatingthat Urns’s probability is greater than 0.5 (positive) or less than 0.5 (negative).3.1. Theoretical results: Known parametersThis section presents our theoretical results when the parameters of Urns are known. In the following, we examine Urnsunder two sets of assumptions, the Uniform Special Case (USC) and the Zipfian Case (ZC), defined below.Theorems 3 and 5 address question (1) above in each model, describing how class probability increases with the numberof times k a label is extracted. Specifically, we provide expressions for the increase in the odds ratio odds(k, n) = P (x ∈C|k, n)/(1 − P (x ∈ C|k, n)) in terms of k. Theorems 4 and 7 address question (2). Let cknown indicate the classifier output byUrns when the parameters are known; we provide upper bounds on the expected error E[error(cknown)] in terms of thesample size n and the model parameters.3.2. Analyzing the uniform special caseThe Uniform Special Case (USC) of the Urns model, first introduced in Section 2.0.1, is characterized by the followingassumptions:D. Downey et al. / Artificial Intelligence 174 (2010) 726–748741USC1 Each target label has the same probability pC of being selected in a single draw, and each error label has a corre-sponding probability p E .USC2 Each label from C is repeated on more balls in the urn than is each label from E (that is, pC > p E ).USC3 Frequency observations k are Poisson distributed (as in Eq. (2)).3.2.1. Theoretical results in the USCThe following theorem states how the odds ratio odds(k, n) increases with k in the USC.Theorem 3. In the USC(cid:3)odds(k1, n)odds(k2, n)=(cid:4)k1−k2.pCp EProof. Follows from the posterior probability in the USC (from Eq. (2)):P (x ∈ C|k, n) =1)ken(pC −p E ).(cid:2)1 + |E||C| ( p EpC(10)(11)Along with assumption USC2, Theorem 3 illustrates that in the USC the odds that an element is a member of the targetclass increase exponentially with repetition. The increase is hastened when the target and error classes are less confusable(i.e. as pC increases relative to p E ).How accurately we can classify extracted labels, given the parameters of the model and the sample size? Let cknownindicate the Urns classifier when the parameters are known. The following theorem provides an upper bound on the errorof cknown in the USC in terms of the sample size n, and the separability pC − p E between the C and E sets.Theorem 4. In the USC, the expected error E[error(cknown)] < (cid:9) when the sample size n satisfies:n (cid:2) 12pC ln 1/(cid:9)(pC − p E )2.(12)Proof. Define a model m with a threshold τ = pC +p Esuch that Pm(x ∈ C|k, n) (cid:2) 0.5 whenever k (cid:2) nτ , and Pm(x ∈ C|k, n) <0.5 otherwise. Since we can calculate the optimal threshold when the parameters are known, E[error(cknown)] is no worsethan the expected error made by model m (which utilizes a potentially sub-optimal threshold). We express the expectederror of model m over the full set C ∪ E by summing the expected contribution of each label (equal to the probability thatthe label appears a number of times resulting in misclassification).(cid:7)(cid:7)(cid:7)(cid:7)2(cid:12)(cid:13)error(cknown)E=x∈Ek(cid:2)nτ P (k|x ∈ E, n) +x∈Ck<nτ P (k|x ∈ C, n)|C ∪ E|.(13)Employing Chernoff bounds, we can bound the probability that a given label deviates from its expected frequency enoughto be misclassified. The Chernoff bounds we employ state that for a random variable X =i Xi equal to the sum ofindependent Bernoulli random variables Xi , the probability that X exceeds its expectation μ by more than a factor (1 + δ),for any δ > 0, is bounded as:(cid:7)(cid:5)(cid:6)X > (1 + δ)μP< e−μδ2/3.Likewise, the probability that X is sufficiently less than its expectation is bounded as:(cid:5)(cid:6)X < (1 − δ)μP−μδ2/2< e(14)(15)for any δ > 0.Let d = pC − p E . Then we have:(cid:2)(cid:2)P (k|x ∈ E, n) =P (k|x ∈ E, n)k(cid:2)nτk(cid:2)n(p E +d/2)(cid:5)k (cid:2) n(p E + d/2)|x ∈ E(cid:5)k > n(pC + d/2)|x ∈ E−nd2/(12pC )(cid:6)(cid:6)= P(cid:3) P< ewhere the last inequality uses the Chernoff bound in Eq. (14) with μ = npC and δ = d/(2pC ). Similarly, using the bound inEq. (15), we have:742D. Downey et al. / Artificial Intelligence 174 (2010) 726–748(cid:2)k<nτP (k|x ∈ C, n) =(cid:2)P (k|x ∈ C, n)k<n(pC −d/2)(cid:5)k < n(pC − d/2)|x ∈ C−nd2/(8pC )(cid:6)= P< e< e−nd2/(12pC ).Algebra gives the final result. (cid:2)Theorem 4 yields the following corollary, which states that under the assumptions of the USC, even a weakly indicativeextractor (one for which pC − p E is just slightly greater than zero) can provide an arbitrarily accurate classifier, givensufficiently large n. This statement is akin to similar results in boosting algorithms in machine learning [35].Corollary 1. In the USC, for any (cid:9) > 0, any extractor for which pC − p E > 0 can be used to achieve accuracy of 1 − (cid:9) given sufficientsample size n.3.3. Analyzing the Zipfian single-urn caseThe USC is a reasonable approximation for labels on the flat tail of the Zipf curve, but it is clearly an oversimplification forall labels. The following theorems are analogous to those presented for the USC above, but employ the more realistic Zipfiansingle-urn assumptions. In particular, we assume that the target and error sets are governed by known Zipf distributions,described below, with sizes |C| and |E| and shape parameters zC and zE . Further, we assume draws are generated froma mixture of these Zipf distributions, governed by a known mixing parameter p giving the probability that a single drawcomes from C :(cid:2)p =f .f ∈F C(16)As in our experiments, we will find it more mathematically convenient to work with a continuous representation of thecommonly discrete Zipfian distribution. Integrating over the continuous representation will allow us to arrive at closed-formexpressions for class probability in terms of gamma functions (Theorem 5). In the discrete Zipfian case, it is assumed thatthe ith most frequent element of C has frequency αC /izC , for αC a normalization constant. In our continuous representation,the frequency of each element of C is itself a random variable drawn by choosing a uniform x from the range [1, |C| + 1]and then mapping x to the curve f C (x) = αC /xzC to obtain a frequency. The normalization constant αC is:αC =p(cid:14) |C|+11.1xzC dx(17)The normalization constant is chosen such that if we draw |C| frequencies for the labels of the C set, the expected sum ofthe frequencies is p, as desired. The frequency of each element of E is defined analogously. We will refer to the functionsf C and f E as frequency curves.As in the USC, for a label in the ZC with underlying frequency f we assume the observed count k is Poisson distributedwith expected value nf . Thus, the likelihood of observing an example of the set S (used to denote either of the C or E sets)a total of k times in n draws is:P Z C (k|x ∈ S, n) = 1|S||S|+1(cid:11)1(nαS xenαS x−zS )k−zS k!dx.(18)The solution of this equation in terms of incomplete gamma functions is given below in Theorem 5, Eq. (19).We state the assumptions in the ZC as follows:ZC1 The distributions of labels from C and E are each Zipfian as defined above, with mixing parameter p. That is, thelikelihood of the data is governed by Eq. (18).ZC2 Confidence increases with repetition; that is, P (x ∈ C|k) increases monotonically with k.ZC3 The error label frequency curve has positive probability mass below the minimum target label frequency; that isαE /izE < αC /(|C| + 1)zE for some known i < |E| + 1.ZC4 Analogously, the target label frequency curve has positive probability mass above the maximum error label frequency;that is αC /izC > αE for some known i > 1.ZC5 Both the target and error set have non-zero probability mass in the urn; that is, p, 1 − p > M for some known lowerbound M > 0.D. Downey et al. / Artificial Intelligence 174 (2010) 726–748743Assumptions ZC3 and ZC4 encode an assumption that given a sufficient number of distinct labels in the urn, with highprobability the most frequent labels will be target labels and the least frequent will be error labels. These assumptions willallow us to establish PAC learnability from unlabeled data alone.To lend justification to the above assumptions, we note that we would expect them to hold at least approximately inUnsupervised Information Extraction applications. The Zipfian nature of extractions and monotonicity (ZC1 and ZC2) are wellknown to hold approximately in practice. Further, assumption ZC3 is certainly empirically true when one considers that, asa simple example, for any target set element there exist multiple less-frequent misspellings in the error set. AssumptionZC4 tends to be at least approximately true in practice: the most frequently extracted labels tend to be instances of thetarget class. Assumption ZC5 is nearly trivially true in practice, we would always expect the target and error sets to haveprobability mass above some non-zero minimum value.3.3.1. Theoretical results in the ZCWe start by explicitly expressing how the odds that an element is a member of the target class increases with thenumber of repetitions:Theorem 5. In the ZC, the odds ratioodds(k + 1, n)odds(k, n)=(k − 1/zC )g(k, zC , np, |C| + 1, αC ) + h(k − 1/zC ,(k − 1/zE )g(k, zE , n(1 − p), |E| + 1, αE ) + h(k − 1/zE ,np)(|C|+1)zC αCn(1−p)(|E|+1)zE αE)where(cid:5)k(cid:5)h, n(cid:6)(cid:5)= n(cid:5)(cid:5)k(cid:5)enandwithP (k|x ∈ C, n) = npαC1/zCg(k, zC , np, |C| + 1, αC )(cid:5)k(cid:5)g(cid:5)(cid:5)(cid:5), α(cid:5), s, n, z(cid:6)(cid:3)= Γ(cid:5) − 1/z(cid:5),k(cid:5)ns(cid:5)z(cid:5)α(cid:5)(cid:4)(cid:3)− Γ(cid:5) − 1/z(cid:5),k(cid:4)(cid:5)nα(cid:5)assuming that neither zC nor zE are exactly equal to 1.(19)Proof. Given that |C|, |E|, k (cid:2) 1, and zC , zE (cid:11)= 1 the above result is obtained by symbolic integration in Mathematica andalgebra.14 (cid:2)Theorem 5 does not utilize any assumptions other than the Zipfian mixture (ZC1). Eq. (19) is the closed-form likelihoodexpression used to perform efficient inference in our experiments. Of course, the odds ratio given above is complex. Anillustration of how class probability varies with k is shown in Fig. 5. In order to provide qualitative insights, the odds ratioshould be simplified into a more interpretable bound; this is an item of future work.We also wish to bound the classification error of Urns for the ZC. The following theorem provides a bound relative tothe error of the optimal classifier, which utilizes both the Urns parameters and the precise frequencies of each label (ratherthan simply the observed counts). As such, the optimal classifier exhibits the best classification performance that can beachieved using the extraction count alone.Definition 6. The optimal classifier is one which classifies each label optimally given knowledge of both the urn parameters,as well as the precise frequency in the urn of each label.Define τ such that the classification threshold of the optimal classifier for a given n is equal to nτ . From assumption ZC2,we know that a single such τ exists. Then the following theorem illustrates that as the sample size increases, the expectederror falls off nearly linearly toward that of the optimal classifier.Theorem 7. In the ZC, given any δ > 0, the expected error of urns is bounded as:(cid:12)(cid:13)error(cknown)E(cid:3) β + K C (δ) + K E (δ)(|C| + |E|)n1−δwhere K C (δ) and K E (δ) are constants (with respect to n) defined below, and β is the expected error of the optimal classifier.14 For reference, the specific Mathematica commands involved in the proof are available online, see http://www.cs.northwestern.edu/~ddowney/data/urnsIntegration.html.744D. Downey et al. / Artificial Intelligence 174 (2010) 726–748Fig. 5. Probabilities assigned in the Urns model, in the Uniform Special Case (USC), and Zipfian Case (ZC) as the Zipfian shape parameters vary. For very flatZipf curves (zC = 0.45, zE = 0.4), ZC is similar to USC, but ZC differs from USC as the shape parameters increase and diverge from each other. A zE value of1.1 implies some errors have high extraction frequency, meaning that as k increases, class probability in the ZC converges to one more slowly than in theUSC. In the above, |E| = 20,000, |C| = 500, p = 0.9, and n = 10,000.The constants K C and K E are defined as follows (for S denoting C or E):(cid:15)K S (δ) = max3/δ, 1 +(cid:16)1(αS x−zS − xτS )2dxxτ−1(cid:11)S1(20)where xτS is defined to be the unique value such that f S (xτS ) = τ , and αS is the normalization constant (see Eq. (17)).Proof. Following the proof of Theorem 4, we aggregate the probabilities that the elements are misclassified.We present the analysis for expected error on elements of the C set; the E set is analogous. When the parameters areknown, Urns makes errors the optimal classifier does not if and only if the true frequency of a target label x is greater thanthe threshold τ , but the observed count is less than nτ . We bound the probability that an element with true frequency−zC > τ appears fewer than nτ times in n draws using Chebyshev’s inequality. Chebyshev’s inequality bounds theof αC xprobability that a random variable Y with expectation μ and variance σ 2 appears sufficiently far from its expectation:(cid:5)P|Y − μ| > rσ(cid:6)(cid:3) 1r2.For a Poisson random variable with expected value ppression also bounds the probability that the deviation exceeds rin misclassification (nαC xexpected error on the C set:−zC − nxτn, so the above ex-n equal to the smallest deviation resultingC ), and integrating over the frequency curve f C , we have the following bound for then. Setting rn for 0 < p(cid:5) < 1, σ is bounded above by√√(cid:5)√(cid:12)(cid:13)errorC (cknown)ExτC(cid:11)(cid:3)(cid:3) βC +min1,11n(αC x−zC − xτC )2(cid:4)dx(21)where βC is the fraction of the expected error of the optimal classifier due to elements of C (namely, the probability massof elements of C with frequency less than τ ).Define:γn = 1n+xτ−1/n(cid:11)C11n(αC x−zC − xτC )2xτC(cid:11)(cid:3)dx (cid:2)min1,11n(αC x−zC − xτC )2(cid:4)dx.We claim γn (cid:3) K C (δ)/n1−δ , given which the theorem follows. The proof of the claim proceeds by induction. First, notethat the n = 1 case, that γ1 (cid:3) K C (δ), holds by construction of K C (δ)—the second term in the max function in Eq. (20) isequal to γ1. Then assuming γn (cid:3) K C (δ)/n1−δ , consider the n + 1 case:γn+1 = nγnn + 1+xτC−1/(n+1)(cid:11)xτC−1/n1n(αC x−zC − xτC )2dx (cid:3) K C (δ)nδn + 1+ 1n2= K C (δ)(n + 1)1−δ(cid:3)nδ(n + 1)δ+ (n + 1)1−δK C (δ)n2(cid:4).It remains to show that:(cid:3)nδ(n + 1)δ+ (n + 1)1−δK C (δ)n2D. Downey et al. / Artificial Intelligence 174 (2010) 726–748(cid:4)(cid:3) 1.745(22)With algebra, this is equivalent to the statement that K C (δ)n2((n + 1)δ − nδ) (cid:2) n + 1. From the generalized binomialtheorem, (n + 1)δ is at least as large as nδ + δnδ−1 − δ(1 − δ)nδ−2/2. With algebra, we have:(cid:5)K C (δ)n2(n + 1)δ − nδ(cid:6)(cid:2) K C (δ)δn1+δ(cid:2) 3n1+δ22(cid:2) n + 1as desired, using the fact that K C (δ) (cid:2) 3/δ. (cid:2)3.4. Theoretical results with unknown parametersIn unsupervised classification, in general we are not given the Urns parameters in advance, and must learn these fromunlabeled data. In this section, we provide theorems bounding the error in unsupervised classification even when theparameter values are unknown. The following theorem shows that with high probability the parameter values of Urns canbe estimated accurately from unlabeled data alone, as the total number of distinct labels in the urn u = |C| + |E| increases,with n fixed.Theorem 8. In the ZC, for any δ, (cid:9) > 0, given sufficiently large u = |C| + |E| for fixed n, we can obtain an estimate of the parametersof f C and f E such that with probability 1 − δ each estimate lies within (cid:9) of the true parameter value.Proof. The frequency curves f C and f E can be converted into functions gC (λ) and g E (λ) giving the probability density of aparticular frequency λ for labels in the C (resp. E) set. These functions are themselves power law distributions. For example,in the error set case:(cid:17)g E (λ) =L Eλ(1+zE )/zE0for aE (cid:3) x (cid:3) b E ,for x < aE or x > b E ,(23)for a suitable constant L E where zE indicates the exponent from the original frequency curve. The distribution of errorlabels in the model is completely characterized by four parameters: L E and zE , the minimal frequency aE , and the maximalfrequency b E .The probability that a particular label appears k times in n extractions can then be written as follows:P (k|n) =n(cid:11)(cid:5)0gC (λ) + g E (λ)(cid:6) e−λλkk!dλ.(24)Let g(x) = gC (x)+ g E (x). When written in the form of Eq. (24), the distribution over k becomes an instance of a compoundPoisson process, for which the existence of effective estimators of g(x) is well-known. In particular, Theorem 1 from [26]states that for any x < n we can obtain a sequence of estimates ˆgu(x) of g(x) such that E[ ˆgu(x) − g(x)]2 = o(1) as u → ∞.Thus, for any given δ(cid:5), (cid:9)(cid:5) > 0, we have with probability 1 − δ(cid:5)for u sufficiently large. It remains toconvert this estimator of g(x) into estimators of each of the Urns parameters. In the re-written model (Eq. (23)) we willemploy, there are eight total parameters characterizing the mixture components gC and g E . We present the constructionfor the four parameters of g E , the gC case is analogous.that | ˆgu(x) − g(x)| < (cid:9)(cid:5)Consider two estimates ˆgu(x0) and ˆgu(rx0) where x0, rx0 < αC /(|C| + 1). That is, x0 and rx0 are sufficiently small thatgC (x0) and gC (rx0) are zero by assumption ZC3. By algebra, in this region (1 + zE )/zE = (ln g(x0) − ln g(rx0))/(ln r), so zE isa continuous and bounded function of g(x0) and g(rx0) on the domain of interest. This implies we can estimate zE within(cid:9) with probability 1 − δ given our estimator ˆgu , for u suitably large. Likewise, L E is a continuous and bounded function ofg(x) and zE , so we can estimate L E effectively.(cid:5)/2. Thus, the minimal xi such that ˆgu(x) > MIt remains to obtain an estimator for the limits of support aE and b E . We begin with the minimal limit aE . We constructfrom 0 to n a uniform lattice of estimates { ˆgu(xi)} each (cid:9) apart. By assumption ZC5, g E (x) > Mfor x ∈ [aE , aE + (cid:9)) for a(cid:5)given that (cid:9) is sufficiently small. By taking u suitably large, we can ensure with probability 1 − δ thatknown constant M(cid:5)/2 and that the x j (cid:2) aE falling in the interval [aE , aE + (cid:9)) has estimate ˆgu(x j) >∀xi < aE , | ˆgu(xi) − g(xi)| = | ˆgu(xi)| < M(cid:5)/2 is with probability 1 − δ an estimate within (cid:9) of aE . Estimating theMmaximal limit of support b E is similar. The same procedure is employed, except that because gC (b E ) is non-zero, weinstead identify successive estimates ˆgu(xk) and ˆgu(xk+1) that differ by a sufficiently large margin, where xk is greater thanour estimate for aE . By taking u sufficiently large and (cid:9) sufficiently small, with probability 1 − δ the value xk is within (cid:9)of b E . (cid:2)(cid:5)746D. Downey et al. / Artificial Intelligence 174 (2010) 726–7483.4.1. PAC learnability under UrnsIn this section, we show that a sufficiently informative extractor that follows the Urns model can be used to PAC learnfrom only unlabeled data. Here, we assume we have additional features for each label beyond just the extraction counts (forexample, other features could include the co-occurrence counts of each label with textual contexts other than the extractors,as in [15]).Our result is expressed in terms of a given, fixed concept class of binary classifiers mapping the input features to {0, 1},denoted as C—as is typical in the PAC-learning setting, we assume our target function (having zero error) is in C.Our result requires that a “separability” criterion holds on the concept class C. This criterion states that no two distinctconcepts in C agree on too large a fraction of the instance space:Definition 9. A concept class C(cid:5)that c(x) = c(cid:5)(x) is less than 1 − (cid:9).is (cid:9)-separable if for any distinct concepts c, c(cid:5) ∈ C(cid:5), the fraction of examples x ∈ X suchWe also require an extractor that is sufficiently informative. We state this criteria in terms of the minimal expectedclassification error that can be achieved using the extraction counts, in the limit of u and n large. This is equivalent to thearea of the “confusion region” in Fig. 1, which we define formally as:Definition 10. The area of the confusion region of an extractor is:(cid:18) τ(cid:11)minτ0∞(cid:11)(cid:19)gC (λ) dλ +g E (λ) dλ.τ(25)Given this definition, we can state the following result, which shows that Urns is able to PAC learn from unlabeled dataalone.Proposition 11. If C is (cid:9)-separable, given an extractor that follows the ZC with confusion region of area less than 1 − (cid:9)/2, C isPAC-learnable from unlabeled data alone.Proof. By Theorem 8, with high probability we can obtain the parameters of Urns within an error of (cid:9)(cid:5), for any (cid:9)(cid:5) > 0.Because the optimal classification threshold τ is a continuous and bounded function of the Urns parameters (see Eq. (25)),Urns can achieve accuracy arbitrarily close to the confusion region size. Thus, the error of Urns is less than 1 − (cid:9)/2, given nand u sufficiently large, meaning it assigns classifications different from those of the target classifier on fewer than 1 − (cid:9)/2of the examples. By the separability criterion, the target concept is the only hypothesis that differs from the output of Urnson so few examples. Thus, an algorithm that returns the concept c ∈ C most similar to the output of Urns will always returnthe target concept. (cid:2)3.5. Related workJoachims provides theoretical results in supervised textual classification that use the Zipfian structure of text to arriveat error bounds for Support Vector Machine classifiers on textual data [23]. The strong performance of SVMs in our super-vised experiments corroborate Joachims’s claim that these classifiers are effective on textual data. However, in contrast toJoachims’s work, our theoretical results (and experiments) are focused on the unsupervised case. We show that when theZipfian structure holds, unsupervised learning is possible under certain assumptions.Our result showing that PAC Learnability is guaranteed in the Urns model (Proposition 11) extends a previous resultshowing that a single “monotonic feature” is sufficient to PAC-learn under certain assumptions (a monotonic feature isone, like the extraction counts we consider, whose value increases monotonically with class probability) [13]. The primaryadvantage of our result is that it does not require that the extraction counts be conditionally independent of the otherfeatures given the class, a strong assumption which is shown to be problematic in practice in [12]. Our result avoids thisassumption by exploiting problem structure inherent in extraction, as expressed by the Urns model.4. Future workThe techniques described in this paper leave open many potential areas of future work. One important direction isdeveloping a probabilistic model for multiple extractors that is more flexible than multiple urns. The correlation modelused for multiple urns is limited and can only handle a small, pre-defined set of distinct mechanisms. Language modelingtechniques for UIE from recent work leverage all contextual information when assessing extractions, rather than relying ona select set of extraction patterns [15,2]. However, currently these techniques only rank extracted labels, and do not outputprobabilities or classifications. A model that produces probabilities of correctness without labeled data, like Urns, yet alsoleverages all available contextual information is an important target of future work.D. Downey et al. / Artificial Intelligence 174 (2010) 726–748747When utilizing Urns for UIE in practice, the EM-based algorithm we employ to learn Urns parameters from unlabeleddata could be improved in a number of ways. The algorithm often requires a sample size of hundreds or thousands ofunlabeled observations of each class in order to be effective (as illustrated in Fig. 3). For classes where data is less plentiful,such as many of the relations extracted by the TextRunner system, the parameter learning algorithm is less effective. Weexpect that Urns could be modified to learn accurate parameters for much smaller data sets, through the use of priors ormore robust likelihood-maximization techniques.Urns also requires that a reasonable estimate of the precision of the extraction process be known. We demonstratedthat this requirement is not prohibitive when extracting instances of classes drawn from WordNet, using generic extractionpatterns; the extraction frequency can be assumed or adjusted from unlabeled text in such a way that the probabilities pro-duced by Urns still offer large improvements over previous techniques. However, for “Open IE” systems such as TextRunnerwhich discover target relations from text, the situation is more complex [3]. In TextRunner, extraction precision can varygreatly across the discovered relations; thus, the probabilities output by Urns in this case are less accurate. Automaticallyestimating extraction precision across relations in Open IE systems is an area of future work.5. ConclusionsThis paper described methods for identifying correct extractions in UIE, without the use of hand-labeled training data.The Urns model estimates the probability that an extraction is correct, based on sample size, redundancy, and corroborationfrom multiple distinct extraction rules. We described supervised and unsupervised methods for estimating the parametersof the model from data, and reported on experiments showing that Urns massively outperforms previous methods in theunsupervised case, and is slightly better than baseline methods in the supervised case. We also detailed several otherapplications in which the general Urns model of redundancy has been effective. Our theoretical results show how theaccuracy of Urns improves with sample size, that the parameters of Urns can be estimated without hand-labeled data, andthat Urns guarantees PAC-learnability from unlabeled data alone, given certain conditions.AcknowledgementsThis research was supported in part by NSF grants IIS-0535284 and IIS-0312988, DARPA contract NBCHD030010, ONRgrants N00014-02-1-0324 and N00014-08-1-0431, and gifts from Google, and carried out at the University of Washington’sTuring Center. The first author was supported by a Microsoft Research Graduate Fellowship sponsored by Microsoft LiveLabs. Google generously allowed us to issue a large number of queries to their XML API to facilitate our experiments. Wethank Pedro Domingos, Anna Karlin, Marina Meila, and Dan Weld for helpful discussions, and Jeff Bigham for comments onprevious drafts. Also, thanks to Alex Yates for suggesting we consider this problem.References[1] E. Agichtein, L. Gravano, Snowball: Extracting relations from large plain-text collections, in: Proc. of the Fifth ACM International Conference on DigitalLibraries, 2000.[2] A. Ahuja, D. Downey, Improved extraction assessment through better language models, in: Human Language Technologies: Annual Conference of theNorth American Chapter of the Association for Computational Linguistics (NAACL HLT), 2010.[3] M. Banko, M. Cafarella, S. Soderland, M. Broadhead, O. Etzioni, Open information extraction from the Web, in: Proc. of IJCAI, 2007.[4] M. Banko, O. Etzioni, The tradeoffs between traditional and open relation extraction, in: Proceedings of ACL, 2008.[5] A. Blum, T. Mitchell, Combining labeled and unlabeled data with co-training, in: COLT: Proceedings of the Workshop on Computational Learning Theory,Morgan Kaufmann Publishers, 1998, pp. 92–100.[6] M. Cafarella, D. Downey, S. Soderland, O. Etzioni, Knowitnow: Fast, scalable information extraction from the Web, in: Proc. of EMNLP, 2005.[7] M. Califf, R. Mooney, Relational learning of pattern-match rules for information extraction, in: Working Notes of AAAI Spring Symposium on ApplyingMachine Learning to Discourse Processing, AAAI Press, Menlo Park, CA, 1998, pp. 6–11.[8] C. Chang, C. Lin, LIBSVM: a library for support vector machines, 2001.[9] F. Ciravegna, Adaptive information extraction from text by rule induction and generalisation, in: Proc. of the 17th International Joint Conference onArtificial Intelligence (IJCAI 2001), Seattle, Washington, 2001, pp. 1251–1256.[10] A. Culotta, A. McCallum, Confidence estimation for information extraction, in: HLT-NAACL, 2004.[11] M.-C. de Marneffe, A. Rafferty, C.D. Manning, Finding contradictions in text, in: ACL 2008, 2008.[12] D. Downey, Redundancy in Web-scale information extraction: probabilistic model and experimental results, PhD thesis, University of Washington, 2008.[13] D. Downey, O. Etzioni, Look ma, no hands: Analyzing the monotonic feature abstraction for text classification, in: Advances in Neural InformationProcessing Systems (NIPS) 21, 2008, January 2009.[14] D. Downey, O. Etzioni, S. Soderland, A probabilistic model of redundancy in information extraction, in: Proc. of IJCAI, 2005.[15] D. Downey, S. Schoenmackers, O. Etzioni, Sparse information extraction: Unsupervised language models to the rescue, in: Proc. of ACL, 2007.[16] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, T. Shaked, S. Soderland, D. Weld, A. Yates, Web-scale information extraction in KnowItAll, in:WWW, New York City, New York, 2004, pp. 100–110.[17] O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, T. Shaked, S. Soderland, D. Weld, A. Yates, Unsupervised named-entity extraction from the Web:An experimental study, Artificial Intelligence 165 (1) (2005) 91–134.[18] O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, A. Yates, Methods for domain-independent information extractionfrom the Web: An experimental comparison, in: Proc. of the 19th National Conference on Artificial Intelligence (AAAI-04), San Jose, California, 2004,pp. 391–398.[19] D. Freitag, A. McCallum, Information extraction with HMMs and shrinkage, in: Proceedings of the AAAI-99 Workshop on Machine Learning for Infor-mation Extraction, Orlando, Florida, 1999.[20] W.A. Gale, G. Sampson, Good–Turing frequency estimation without tears, J. Quantitative Linguistics 2 (3) (1995) 217–237.748D. Downey et al. / Artificial Intelligence 174 (2010) 726–748[21] T. Grenager, C.D. Manning, Unsupervised discovery of a statistical verb lexicon, in: Conference on Empirical Methods in Natural Language Processing.[22] M. Hearst, Automatic acquisition of hyponyms from large text corpora, in: Proc. of the 14th International Conference on Computational Linguistics,Nantes, France, 1992, pp. 539–545.[23] T. Joachims, Learning to Classify Text Using Support Vector Machines: Methods, Theory and Algorithms, Kluwer Academic Publishers, Norwell, MA,USA, 2002.[24] M. Liakata, S. Pulman, From trees to predicate-argument structures, in: Proceedings of the 19th International Conference on Computational Linguistics,Association for Computational Linguistics, Morristown, NJ, USA, 2002, pp. 1–7.[25] W. Lin, R. Yangarber, R. Grishman, Bootstrapped learning of semantic classes from positive and negative examples, in: Proc. of ICML-2003 Workshopon The Continuum from Labeled to Unlabeled Data, Washington, DC, 2003, pp. 103–111.[26] W.-L. Loh, Estimating the mixing density of a mixture of power series distributions, in: S.S. Gupta, J.O. Berger (Eds.), Statist. Decision Theory andRelated Topics V, Springer, New York, 1993, pp. 87–98.[27] A. McCallum, Efficiently inducing features of conditional random fields, in: Proceedings of the Nineteenth Conference on Uncertainty in ArtificialIntelligence, Acapulco, Mexico, 2003, pp. 403–410.[28] B. Milch, B. Marthi, S. Russell, D. Sontag, D.L. Ong, A. Kolobov, Blog: Probabilistic models with unknown objects, in: L.D. Raedt, T. Dietterich, L. Getoor,S.H. Muggleton (Eds.), Probabilistic, Logical and Relational Learning – Towards a Synthesis, in: Dagstuhl Seminar Proceedings, vol. 05051, InternationalesBegegnungs- und Forschungszentrum für Informatik (IBFI), Schloss Dagstuhl, Germany, 2006, http://drops.dagstuhl.de/opus/volltexte/2006/416 [date ofcitation: 2006-01-01].[29] K. Nigam, J. Lafferty, A. McCallum, Using maximum entropy for text classification, in: Proc. of IJCAI-99 Workshop on Machine Learning for InformationFiltering, Stockholm, Sweden, 1999, pp. 61–67.[30] M. Pasca, D. Lin, J. Bigham, A. Lifchits, A. Jain, Organizing and searching the world wide web of facts – step one: The one-million fact extractionchallenge, in: AAAI 2006, AAAI Press, 2006.[31] K.H. Pollock, J.D. Nichols, C. Brownie, J.E. Hines, Statistical Inference for Capture–Recapture Experiments, Wildlife Society Monogr., vol. 107, 1990.[32] S.D. Richardson, W.B. Dolan, L. Vanderwende, Mindnet: acquiring and structuring semantic information from text, in: Proceedings of the 17th Interna-tional Conference on Computational Linguistics, Association for Computational Linguistics, Morristown, NJ, USA, 1998, pp. 1098–1102.[33] E. Riloff, R. Jones, Learning dictionaries for information extraction by multi-level bootstrapping, in: AAAI/IAAI, 1999.[34] A. Ritter, S. Soderland, D. Downey, O. Etzioni, It’s a contradiction – no, it’s not: A case study using functional relations, in: EMNLP, 2008.[35] R.E. Schapire, The strength of weak learnability, Mach. Learn. 5 (2) (1990) 197–227.[36] L. Schubert, Can we derive general world knowledge from texts?, in: Proceedings of the Second International Conference on Human Language Tech-nology Research, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2002, pp. 94–97.[37] M. Skounakis, M. Craven, Evidence combination in biomedical natural-language processing, in: BIOKDD, 2003.[38] S. Soderland, Learning information extraction rules for semi-structured and free text, Mach. Learn. 34 (1–3) (1999) 233–272.[39] S. Soderland, O. Etzioni, T. Shaked, D. Weld, The use of Web-based statistics to validate information extraction, in: AAAI-04 Workshop on Adaptive TextExtraction and Mining, 2004, pp. 21–26.[40] R. Storn, K. Price, Differential evolution – a simple and efficient heuristic for global optimization over continuous spaces, J. Global Optim. 11 (4) (1997)341–359.[41] R.S. Swier, S. Stevenson, Unsupervised semantic role labelling, in: Proceedings on EMNLP, 2004, pp. 95–102.[42] A. Yates, O. Etzioni, Unsupervised resolution of objects and relations on the Web, in: Proc. of HLT, 2007.