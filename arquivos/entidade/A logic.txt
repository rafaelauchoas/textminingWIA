Artificial Intelligence 163 (2005) 163–201www.elsevier.com/locate/artintA logic-based model of intention formation andaction for multi-agent subcontracting ✩John Grant a,∗, Sarit Kraus b,c, Donald Perlis c,da Department of Computer and Information Sciences and Department of Mathematics, Towson University,Towson, MD 21252, USAb Department of Computer Science, Bar-Ilan University, Ramat-Gan 52900, Israelc Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USAd Department of Computer Science, University of Maryland, College Park, MD 20742, USAReceived 24 March 2004; accepted 10 November 2004Available online 30 December 2004AbstractWe present a formalism for representing the formation of intentions by agents engaged in cooper-ative activity. We use a syntactic approach presenting a formal logical calculus that can be regardedas a meta-logic that describes the reasoning and activities of the agents. Our central focus is on theevolving intentions of agents over time, and the conditions under which an agent can adopt andmaintain an intention. In particular, the reasoning time and the time taken to subcontract are modeledexplicitly in the logic. We axiomatize the concept of agent interactions in the meta-language, showthat the meta-theory is consistent and describe the unique intended model of the meta-theory. In thiscontext we deal both with subcontracting between agents and the presence of multiple recipes, thatis, multiple ways of accomplishing tasks. We show that under various initial conditions and knownfacts about agent beliefs and abilities, the meta-theory representation yields good results. 2004 Elsevier B.V. All rights reserved.✩ This research was supported by NSF under grant IIS-0222914 and by the Air Force Office of ScientificResearch, and the Office of Naval Research. Preliminary version appeared in the proceedings of AAAI-2002. Wewish to thank the referees for many helpful comments and suggestions.* Corresponding author.E-mail addresses: jgrant@towson.edu (J. Grant), sarit@umiacs.umd.edu (S. Kraus), perlis@cs.umd.edu(D. Perlis).0004-3702/$ – see front matter  2004 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2004.11.003164J. Grant et al. / Artificial Intelligence 163 (2005) 163–201Keywords: Intentions; Subcontracting; Cooperative agents; Syntactic logic; Minimal model semantics1. IntroductionIn this paper we provide a formal meta-theory of the formation of intentions amongmultiple cooperating agents.1 Such a formal theory is very useful to properly understandcomputational models of such cooperative behavior, particularly as a specification lan-guage. The meta-logic can be used to analyze the behavior of a team of agents and provevarious theorems about their activities. We provide a single unified framework that cap-tures with a relatively small number of predicates and axioms an evolving notion of time,the formation of potential and actual intention of agents, subcontracting between agents,actions involving a recipe tree of subactions, parallel actions, multiple recipes for actions,and the performance or failure of agents doing these actions.For instance, agent 5 may know that in order to perform a complex action a it is suffi-cient to perform subaction b and subaction c. However, agent 5 cannot do c but agent 3 cando c. If agent 5 is asked to perform action a it can subcontract to agent 3 the performanceof action c. There is a rule here that the agent follows: when it cannot perform an action(or subaction), it finds an agent that can do the action and subcontracts the action to it. Inthis paper we specify such rules as meta-axioms relating to the mental states of agents inthe sense of the intentions that the agents adopt to perform certain actions and how theselead to the actual performance of the actions. If the agents have been designed to alwaysapply these rules in a cooperative manner, then we can prove what actions will actually beperformed.We use the approach of the mental state model of plans [42]. Pollack’s definition of theindividual plan of an individual agent to do an action α includes four constituent mentalattitudes: (1) belief that performance of certain actions βi would entail performance of α;the βi constitute “a recipe for α”; (2) belief that the agent could perform each of the βi ;(3) intentions to do each of the βi ; (4) an intention to do α by doing the βi . This definitionhas been extended to handle multi-agent systems in various ways (e.g., [19–21]). Thus, inthis approach, agents do not model explicitly the state of the world. In contrast to classicalplanning there is no attempt to search for a sequence of actions that change the world statefrom its initial state to the goal state and thus the focus is not on the problems associatedwith modeling how the state of the world is changing over time [47]. Instead, an agentattempts to perform actions and in its planning it decomposes complex actions to simpleractions using a library of recipes. The agent maintains a belief set that includes beliefs ofwhat it can and cannot do at various times which are used to decide on whether to adoptintentions. These beliefs may change over time based on observations and reasoning whichmay lead to dropping intentions and adopting new ones.Most of the previous work on the mental state model for planning lack a well establishedsemantics, and especially lack a formal treatment of time. In a previous work [18] we made1 Our focus is on how intentions are formed and action gets done in this setting. For detailed discussions ofintentions and actions see [11,35,41].J. Grant et al. / Artificial Intelligence 163 (2005) 163–201165the first step in providing semantics for this approach by presenting a framework for theway the beliefs of bounded rational agents change over time. In this paper we considerintention formation when agents can subcontract actions to one another. We focus on howthe intentions of the agents change over time.We assume all agents to be cooperative; but giving this requirement a formal character-ization involves the notion of intention. We require an agent to adopt a potential intentionto perform an action if it is asked to do so. We provide a specification for the beliefs andintentions that an agent should have in order to turn a potential intention to an intention. Wealso specify the communication needed and the change in the agents’ beliefs and intentionswhen something goes wrong.Our work differs from others in its use of a meta-theory of intentions; of an evolvingnotion of time that allows agents to reason about genuinely-approaching deadlines; and ofa reified notion of (explicit) agent beliefs so that they can be tracked over time. Thus weview the reasoning of agents as ongoing processes rather than as fixed sets of conclusions.This reasoning involves rapidly evolving sets of beliefs and intentions, and is governed inpart by formal rules of inference. We model beliefs and intentions in a formal logical agentcalculus that can be regarded as a meta-logic describing an actual on-board agent logicthat evolves in (and takes account of) that same passage of time. This is a style of formalrepresentation that departs from traditional temporal logics in that there is an internallyevolving notion of time that the logic must keep track of.For allowing this kind of reasoning, our approach utilizes a strongly sorted calculus,distinguishing the application language, time, and various syntactic sorts. We provide for-mal tools for representing agent intentions and methods for reasoning with and about suchintentions, especially in the context of cooperative actions. We focus on cases where oneagent alone cannot accomplish a complex task and must subcontract with other agents.The format of this paper is as follows. In the rest of Section 1 we describe the basicsyntactic formalism for intentions and give an example that is used throughout the paperfor illustration. Section 2 deals in more detail with the syntax and semantics of the meta-language. In Section 3 we show how our framework can be used to model the case ofcooperative multiple agents where each action has a single recipe. Multiple recipes for ac-tions are considered in Section 4. Some work related to this paper is described in Section 5.Section 6 summarizes the paper and indicates topics for future research. We also providefour tables: the first one summarizes the convention used for variables in Section 2.1; theothers at the end of the paper give the meanings of the predicates for two different sectionsand specify the locations of the axioms.1.1. Languages for reasoning about agentsIn our framework we find it useful to introduce three related languages. The first lan-guage, LS , is the language of the subject matter, that is, the subject that the agents reasonabout: e.g., web searches. The second language, LA, is the “agent” language. This in-cludes the subject matter language, but also has additional symbols allowing assertionsabout what some particular agent may believe at a given moment; this allows an agent toreason about another agent, for instance in deciding whether to ask another agent to per-166J. Grant et al. / Artificial Intelligence 163 (2005) 163–201form some subtask based on what that other agent believes.2 The third language, LM , isthe meta-language used to reason about the agents.The meta-language, LM , must be powerful enough to characterize agent behavior quitegenerally. For instance, in our meta-language we can write the statement that an agentalways applies Modus Ponens, when that is possible. This, taken as an axiom, would assertthat the agents are programmed to apply Modus Ponens; that is an aspect of their behavior.Thus, while the agent language, LA, must allow for the expression of the formulas A,A implies B, and B, still an agent does not in general have the meta-rule such as “I applyModus Ponens whenever I can”, even if that happens to be true (and expressed in the meta-language). The three languages are sorted first-order logics.In much of the existing formal work on modeling agent behavior, it is not specified pre-cisely who is doing the reasoning; it appears implicit that the reasoning is meta-reasoningby external observers about agent behavior. In particular, we use our meta-language totrack the evolving behavior (including the evolving beliefs and intentions) of the agentsover time. Hence the meta-language (as well as the agent language) must have a robustnotion of time. It is an important aspect of this approach that we explicitly represent in-dividual beliefs of an agent, i.e., we apply a syntactic approach where beliefs are objects(such as sentences) rather than a modal approach with abstract (modal) propositions. Forinstance, an agent at time t may believe A, A → B, and B → C, and yet not believe C,simply because it did not yet (have time to) apply Modus Ponens (twice); and yet it mayat a later time believe C (e.g., at time t + 2, if it applies MP once at each time step). Inorder to represent this sort of behavior we need names for beliefs; we also need namesfor other sentences, such as statements about agent intentions. For instance, agents invokea Tell process in order to inform one another of a new intention, as part of a cooperativeventure to achieve some goal. We write roughly in the form Tell(“Int(. . .)”); so Tell takesan argument that is the name of the sentence Int(. . .) that expresses an intention.The syntactical approach that we follow seems attractive to researchers in artificial intel-ligence (see Chapters 2 and 8 of [12]). For one thing, it is natural to represent the knowledgeand beliefs of a computer program by writing sentences representing facts that are knownor believed. For another, syntactical treatments can be formalized in the classical predicatecalculus, which is the lingua franca of knowledge representation [22,36,40]; since thereis good theorem-proving technology for this language (e.g., [3,4,6]), such treatments lendthemselves directly to implementations.Furthermore, in other approaches to the issue of agent reasoning, often logical omni-science is assumed. This means that the agent’s beliefs are closed under logical rules, suchas Modus Ponens, in every time period. That is, if the formulas A and A → B are believedby the agent at time t, the agent is assumed to believe B at the same time. We think thatour approach, where rules are applied over time is a more realistic way to model agent be-liefs and intentions. Some researchers avoided the use of the syntactic approach because,in general, the syntactical treatments face a serious theoretical difficulty [13,38]. RichardMontague [34] showed that under certain fairly intuitive conditions a syntactic formaliza-2 We do not assume that the agent uses logic for its reasoning. LA is the language that is used to specify theagent behavior. We do assume that each agent maintains a database with—possibly fallible—information aboutagent abilities. However, we do not make any assumptions about the specification of the database.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201167tion of modal notions such as knowledge becomes contradictory. Thomason later showed[53] that under even less restrictive conditions a syntactic formalization yields a contra-dictory set of beliefs. However, we showed in [18] that when dealing with only beliefs ourmeta-theory is always consistent. Moreover an agent theory need not be inconsistent either.Nevertheless, it is an interesting property of our approach that agent theories may indeed be(or become) inconsistent—e.g., through observations—and this is one of the advantages ofour treatment: commonsense reasoning can proceed in the presence of inconsistency. Thetime-situated character of our logic offers “protection” from inconsistency in two ways:it affords a consistent meta-theory, and it allows for an inconsistent but still useful agenttheory. We further extends these results for intentions as discussed in Section 2.2 below.Finally, we note here that we regard actions as objects. Instead of “make (arrive-on-time) true” our actions have the flavor “arrive on time”. This seeming mere terminologicdifference has the implication that actions can be treated on a par with names of beliefs,making the formal treatment more uniform.1.2. Agent beliefs and intentionsBeliefs and intentions are the two main predicates in our logic. In [18] we focus onthe way the beliefs of bounded rational agents change over time. In this paper we focuson the intention formation of cooperative bounded agents and discuss the agents’ beliefsonly when they are necessary for understanding the agents’ intention formation process.We first survey the way agents’ beliefs are changing over time. The specific axioms arepresented in [18]. In our context, these beliefs could be on the actions that an agent can do,and the actions that other agents can perform. Changes in such beliefs will lead to changesin intentions.Because of our emphasis on changing beliefs and intentions over time, agents need toknow the present time and be able to reason about time. In addition, the meta-languageshould be able to reason about time and about the change of the agents’ beliefs and inten-tions over time. In both logics, we use a discrete model of time.Also, in our framework agents have introspection capabilities [18]. By this we meanthat one of the agent’s beliefs may be that it believed or did not believe something at adifferent time. There are two types of introspection: positive and negative. An exampleof positive introspection is if an agent believed some fact (represented by a formula) attime t, it will then believe at time t + 1 that it believed that fact at time t. This way anagent can reason about its own or another agent’s beliefs over time. Suppose now that attime t the agent is considering but does not believe a particular possible fact A (that is, Ais not in its database). Then at the next time value, t + 1, it will believe (know) by negativeintrospection that it did not believe A at time t.Another important capability of an agent is the inheritance of beliefs, or as it is some-times called, persistence [37,49]. The idea is that if an agent believes some fact A anddoes not believe something contradictory to it (that is, not A), it will continue to believe A.There are exceptions to inheritance. Things that are changing, such as the location of amoving object, or the present time value, should not be inherited.We have found it useful also to add axioms concerning the cooperation of agents. Sofor example if an agent “tells” another agent a fact, the second agent will treat it as a fact.168J. Grant et al. / Artificial Intelligence 163 (2005) 163–201Fig. 1. An example of a recipe tree.This assumes that agents are trustworthy. Agents may also be helpful to other agents byanswering each other’s questions. Note that of course there can be nontrustworthy agents,and that this is a major topic of research (see a survey in [27]). But here we focus on fullycooperative (and nonforgetful) agents.The exact definition of intentions may be necessary for cooperation. For example, anagent needs to know how to interpret a belief that its partner has adopted an intention todo the given action: does this mean the action will indeed be performed by that partnerif the partner is able? Thus, it is important to formally define (a) what are the minimalrequirements that are needed for an agent to adopt an intention; (b) how an agent’s inten-tions change over time (and what are the occasions when an agent will drop an intention);(c) what are the reasoning processes that the agent will perform when it adopts an intention;and (d) what are the actions that an agent will perform when it adopts an intention.We define a notion of “potential” intention that means roughly that the agent has beengiven a task and is determining whether it can do it (perhaps with help). We reserve “in-tention” for the situation in which the agent has determined that it can do the task (perhapswith help) and in that case it will indeed attempt to carry out the task, unless in the mean-time it has dropped that intention because (a) it is told to drop it; or (b) it is given anothertask that conflicts with the first one and which is assigned higher preference; or (c) theworld changes and it finds that it can no longer do the task (perhaps because a subcontract-ing agent cannot do its part). The main difference between intentions and desires [14,28]is the strong commitment to perform the intended actions. While such a commitment isnot associated with a potential intention, the agent that has a potential intention to do anaction is committed to consider the subactions (if any) required for the action (in a recipe)and possibly communicate with other agents to get their commitments for all the requiredsubactions.In the context of collaboration, it is also important to decide whether an agent can haveintentions that another agent will perform an action. In our model, an agent cannot in-tend directly that another agent will do an action;3 however, its plan for doing a, that ismotivated by the intention to do a, may include the intentions of other agents.We divide actions into two types: basic level and complex. Basic level actions are doneby agents that can do them as primitive acts. A complex action requires a recipe [42], thatis, a specification of a list of actions, the doing of which constitutes performance of the3 Technically, intending that another agent do an action is impossible in our logic since done and intentions aresentences and the intentions are with respect to actions not sentences. Furthermore, since agents are autonomous,it is not reasonable to assume that one agent intends that another agent will do an action [33].J. Grant et al. / Artificial Intelligence 163 (2005) 163–201169complex action.4 The subsidiary actions, referred to as subactions, may be basic level orcomplex. The general situation is illustrated in the tree of Fig. 1 where we consider onlya single recipe for a complex action. Here an agent is given task a1, which has a recipeshown in the tree, i.e., to do a1, it is sufficient to do b1, b2, b3 (in that order). Similarly, b1and b2 themselves have recipes. The leaves are the basic level actions. If an agent cannotdo all the actions for a recipe, it then may subcontract some of them to other agents.1.3. ExampleWe now illustrate many of our ideas with an example that will be used in the rest of thepaper. We have chosen this example to be relatively simple and yet to have a number of themore interesting features that our work allows. We assume that there are two agents, one ofwhich has been given a request to perform an action starting at a particular time. We furtherassume for now that for each action (and subaction) there is only one available recipe forperforming that action. We use the convention that an agent must always subcontract acomplex action if it cannot do the first subaction. We may imagine that the agents are partof a network of helper agents that work in the service of a human.Let the two agents be G and H . The request to act is given to G, and the request itselfis (for G) to arrange (for the human requester) to get to the airport on time. Let us namethe action “aaat” (arrange for arriving at airport on time).There is a recipe for aaat, consisting of four subactions:(i) find flight info (number, date, airport);(ii) use this info to find time to arrive at airport;(iii) estimate time to leave for airport;(iv) arrange for a taxi.Furthermore, the recipe for action (ii) consists of two subactions: (a) find the url for theairline and (b) do a web search; while the recipe for action (iv) consists of the two subac-tions: (c) find the phone number for the taxi and (d) call for the taxi. We suppose that Gcan carry out by itself (i), (ii)(a), (iii), and (iv)(d), but it needs to subcontract (ii)(b) and(iv) to H which, not being able to do (iv)(d) will subcontract this to G.Of course, G and H have to be especially designed to be able to do (or not do) just theright things for this example to work. In a more realistic (but far more complex) example,there would be more agents, and subcontracting in general would involve an agent askingone agent after another to help with a subaction until success occurred, and even then thesuccess might later be overturned due to some later failure. Some of these complexities aretaken up in a later section of the paper, including multiple recipes.We assume that, as the various subactions in a recipe are performed, a vector of informa-tion is maintained and kept available for use during subsequent subacts. This informationincludes historical data on what has been done so far, as well as acquired information re-sulting from an action. For instance, in this example, the subaction “estimate time to leave”4 We assume that for each subaction, its relative start and maximal possible end time are specified in the recipe.170J. Grant et al. / Artificial Intelligence 163 (2005) 163–201Fig. 2. Recipe tree—the UPPER CASE actions are subcontracted to H .results in a time value that is placed into the vector and then used later when the taxi call ismade. Since these details are at the agent level, they do not directly bear on the meta-theory.The overall tree of subactions is presented in Fig. 2.2. Syntax and semanticsThis section describes the syntax and the semantics of LM , the meta-language. In thefirst subsection we write the predicates of the meta-language and explain their use. Thesecond subsection deals with the minimal model semantics that allows us to obtain, oncethe initial conditions and axioms are given, a unique minimal model of the theory showingthe mental processes and activities of the agents as they change over time.2.1. Sorted language for agent intentionIn this section we introduce the main predicates that we use in our work in the meta-language to characterize agent intentions. These will be constrained by the axioms indifferent ways. For each predicate we explain the use of the different attributes. We use asorted language for comprehensibility and convenience so that, for example, agent namesand times (for both of which we use positive integers) cannot be confused. Since the lan-guage is sorted, we use a convention for variables of different sorts, namely t, i, j , k fortime; m, n for agent names, a, b, c for actions, and r for recipes. As needed, _ is used forthe null element.We start with the two predicates for intention: PotInt and Int, as well as AskedToDo(ATD) and Refrain (Ref ). Basically, PotInt represents a potential intention for an agentasked to perform an action. Under certain conditions a PotInt will become an intention(Int).The context, that is an argument of several of the predicates, keeps track of the treestructure of the recipes used. So, when an agent has a potential intention or an intentionfor an action, the context of the action, if there is one, is the parent node in the tree. Forinstance, in the example given in Fig. 1, the context for c2 would be b1. For the root node,a1, the context is the null action _. So the context of a potential intention to do “find url”of the example of Fig. 2 is “find when to arrive” and the context of a potential intention ofJ. Grant et al. / Artificial Intelligence 163 (2005) 163–201171Table 1The convention for variables of different sortsVariablest, i, j , km, na, b, crSorttimeagent namesactionsrecipes“find when to arrive” is “aaat”. Several predicates involve two agents, such as one agentassisting another agent; we allow these agents to be the same agent.ATD(t, n, m, b, a, t (cid:3)): At time t agent n asks agent m to do action b in the context of actiona at time t (cid:3). This predicate, which requires communication, is used both by theagent’s owner who asks an agent initially to do a task, as well as by other agentsas they request one another to do various tasks for them: this is subcontracting.There are two times involved, the time of the asking and the time that the actionneeds to be done. This will also be the case for several other predicates. In theexample given above, writing aaat for “arrange for arriving at airport on time”,we could write ATD(14:30:00, _, G, aaat, _, 15:00:00) to indicate that the owner(null agent) asked agent G (in the axioms we will assume for convenience thateach agent is referred to by a number) at 2:30PM to start at 3PM the (root) actionof arranging for arrival on time at the airport.PotInt(t, m, n, b, a, t (cid:3)): At time t agent m directly assisting agent n (the null agent, incase there is no agent n) has the potential intention to do action b in the contextof action a at time t (cid:3).When G is asked to arrange for arriving at the airport on time, it adopts a potentialintention to do this task at the next time period. Assuming that a time period is onesecond, we will have PotInt(14:30:01, G, _, aaat, _, 15:00:00) indicating that Gwill have a potential intention to start doing aaat at 3PM. The task is done for theowner (hence the first _) and the arrange action is not a subaction in the contextof another action (hence the second _).Int(t, m, n, b, a, t (cid:3)): At time t agent m directly assisting agent n has the intention to doaction b in the context of action a at time t (cid:3).In our example, if G or its assistants will adopt all the needed intentions to arrangefor arriving at the airport on time in one minute, then the formula indicating thiswill be Int(14:31:00, G, _, aat, _, 15:00:00).Ref (t, m, n, b, a, t (cid:3)): At time t agent m refrains agent n from intending to do action b inthe context of action a at time t (cid:3). As we show later, the effect of this request is thatthe agent in the next time period will no longer have the potential intention to dothe action, in effect cancelling the task for the agent. We assume that Ref includesthe communication involved in an agent refraining another agent, but also allowfor the case where m = n.As noted in Section 1.2 we distinguish between basic level actions that are done byagents that can do them as primitive acts and complex actions that consist of several subac-172J. Grant et al. / Artificial Intelligence 163 (2005) 163–201tions. The various subactions for achieving a complex action are specified in a recipe. Theactions in a recipe may themselves be complex.The following three predicates do not involve time in the sense of the previous predi-cates, because they refer to facts that are considered true for every time period.BL(a, d): a is basic level, takes d units of time to complete, and has no recipe. We willuse d = 1 in the examples, but that is not necessary. In the example, we haveBL(call, 1).Rec(a, r): For action a, r is the unique recipe (multiple recipes are discussed in Section 4).Mem(r, b, i, j, k): In recipe r, b is the subaction which is the ith member of the recipestarting at relative time j and ending at relative time k (i.e., relative, or as anoffset, to the time at the beginning of the action). For example, the formulaMem(r1, get_flt_info, 1, 1, 2) states that get_flt_info is the first action for reciper1 and that it starts at time 1 and ends at time 2, where these times are relative tothe initiation of the recipe.In the next group we deal with other relevant concepts about agents: their beliefs, abili-ties to do actions, and means of communication with other agents.Bel(t, n, f ): At time t agent n believes the statement expressed by formula f (i.e., f isthe name of the statement).CanDo(t, n, a): At time t agent n can do (or at least start) action a.For example, if at 14:31, G believes that it can do at 15:00 the action aaat, then theformula indicating this will be Bel(14:31:00, G, “CanDo(15:00:00, G, aaat)”).Tell(t, n, m, f ): At time t agent n tells agent m the statement expressed by the formula f .The formulas are the formulas of the agent language, LA. These are the formulasthat are meaningful to the agents. Each such formula has a name (its quoted ver-sion) in the meta-language LM .For example, Tell(14:35:00, H, G, “Int(14:35:00, H, G, search_web, _, 15:06:00)”) means that at 14:35, agent H tells agent G that it intends to search theweb at 15:06.Two predicates deal with the agents actually doing the actions, both the initializationand completion.Ini(t, m, n, a): At time t agent m directly assisting agent n initiates action a.For example, Ini(15:00:00, G, _, aaat) indicates that at time 15:00 agent G initi-ates the action aaat.Done(t, a): At time t action a has just been done successfully.There is a predicate indicating that a subaction failed to be done stopping the perfor-mance of the action.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201173Stop(t, m, n, b, a): At time t agent m directly assisting agent n is instructed to stop actionb in the context of action a. As with Ref , Stop includes the appropriate commu-nication.Both Stop and Ref are applied to terminate an action. However, Ref is applied to ter-minate an action during the intention formation process, while Stop is applied during theexecution of the action.Finally we include two predicates used in an integrity constraint.Prefer(t, n, a, b): At time t agent n prefers to do action a over action b.Conf (t, n, a, b): At time t for agent n action a conflicts with action b.Communication is important in team activity in general, and in subcontracting in par-ticular. We apply one explicit communication predicate—Tell. In addition, as discussedabove, several other predicates require communication: ATD, Ref and Stop. The axioms inwhich these predicates appear could have been stated using the Tell predicate, but introduc-ing special predicates to these communication related activities simplified our meta-logic.Communication may also be necessary in the initialization process. We assume that anagent can observe whether an action, possibly performed by another agent, preceding itsown action has been done, and comment on the need for further communication when suchobservation is not possible.2.2. Minimal model semanticsThe axioms of the meta-theory will typically have the form ∀(cid:5)x(α1& · · · & αn → β),where (cid:5)x is the sequence of variables in the formula, each αi and β is an atom, and thetime of the relevant αi s is t and the time of β is t + 1. The axioms of the meta-theory canbe shown to be consistent by constructing a model for them. In [18, p. 361] we prove thatthe meta-theory given in that paper is consistent by showing how to construct a minimalmodel for it. The proof does not depend on the specific axioms given there, only on theform of the axioms. Hence the same process can be used to construct a model for the meta-theory presented in this paper. In general, there will be many models for such a first-ordertheory that have only a tenuous relationship to the application of our interest. We wish tointerpret the axioms in such a way that for any instantiation of the formula, where all theαi are true in the model, β is derived. This way, we will be able to use the axioms to derivein the meta-theory the beliefs, intentions, and actions of the agents at time t + 1, giveninformation about the situation at time t.In addition to axioms, we allow integrity constraints in the meta-theory as well. Integrityconstraints have the form ∀(cid:5)x¬(α1& · · · & αn). Integrity constraints are not used to deriveresults about agent beliefs, intentions, or actions over time; their purpose is to eliminatepossible models that do not make sense. Typically, integrity constraints involve time onlyin a general way. For example, one integrity constraint states that for every action theending time must come after the starting time.174J. Grant et al. / Artificial Intelligence 163 (2005) 163–201Consider now that first-order logic formulas may be written in many logically equivalentforms. For example, a formula of the form∀(cid:5)x(α1& · · · & αn → β)can be written in a logically equivalent form as∀(cid:5)x(α1& · · · & αn−1&¬β → ¬αn).For our purpose, however, it would not be useful to use this version of the axiom derivingsomething about the agent at time t from something that did not happen at time t + 1.However, these two formulas have the same models.This analysis prompts us to restrict consideration to only some of the models of themeta-theory. The first restriction is to consider only Herbrand models, i.e., models thatcontain only elements that are ground terms of LM . The second restriction is to consideronly minimal models where we minimize each predicate for one time period at a timestarting with time 0 and then proceeding by induction on time. Because of the structure ofthe axioms, there will be only one such model. We can construct this model by a processthat is similar to the construction of the unique perfect model of a locally stratified theoryin logic programming. See [18] for details. Such a theory is also called XY-stratified anddiscussed on p. 247 of [58].In order to have a sharp sense of how our agents behave, we simply define them tobe processes that behave as in this unique minimal model. This has the desired effect ofproviding certain converses of the axioms as true statements in this model. In effect weare making an assumption of complete information (akin to a closed-world assumption)about agent behavior; for example an agent comes to have a particular potential inten-tion at time t, if and only if our axioms require that agent to have that potential intentionat that time, and thus this potential intention will be true in the unique minimal model.Similarly, for example, agent G will ask agent H at time 11 to get a taxi at time 58, iffATD(11, G, H, get_taxi, aaat, 58) is true in the minimal model. This is a restriction ofsorts; after all, much of commonsense reasoning involves situations that are usually takento be too complex to fully axiomatize. But our view is to suppose that the complexitiesprimarily arise from the external world and that the agent behaviors are completely char-acterized by the axioms, once the external inputs are given.Later, by studying the minimal model under certain initial conditions, known facts aboutagent beliefs and abilities and the structure of recipes, we will prove several meta-theorems.These theorems typically state that given the beliefs and abilities of agents, and given“enough” time as defined by a formula involving information about the recipes, the agentor agents will be able to plan and execute the requested action.3. Modeling multiple agents with single-recipe actionsIn this section we model agents that have only one way, a single recipe, to performactions. We present a theory T over LM which consists of the axiom schemas that describethe desired intentions and behavior of a team of cooperative agents and how their intentionschange over time. We do not present the basic axioms of belief, time, etc. that are givenJ. Grant et al. / Artificial Intelligence 163 (2005) 163–201175in [18]. We also state and prove several general theorems about the meta-theory. Thesetheorems show that, under the appropriate conditions, the meta-theory characterizes theagent activities in a reasonable manner.3.1. Axioms of the meta-theoryWe divide the axioms into three groups: the intention formation axioms, the subcon-tracting axioms, and the performing axioms. We also have integrity constraints. Beforespecifying the axioms, we sketch the general scenario for the agent potential intentions,intentions, subcontracting and doing actions. We note that in any application of the theorythere will be additional domain specific facts (axioms) that need to be invoked.When an agent is asked to do an action a at a particular time, in the next time period itadopts a potential-intention to do it. If the agent believes that it can do the action a, then inthe basic level case, in the next time period it will adopt an intention to do it. In the case ofa complex action, it will look for a recipe and in the next time period will adopt potentialintentions for all the immediate subactions, some of which may themselves be complex, ofthe recipe.If after adopting a potential intention the agent comes to believe that it cannot do theaction a, in the next time period it will ask another agent, that it believes can do a, to doa. If at a particular time period an agent has a potential intention to do a complex action,it will adopt in the next time period an intention to do the action if for each subactionin the recipe it either adopted an intention to do it or it has found another agent with anintention to do it. Even after the initial adoption of an intention, during the waiting phase,the process of checking the needed associated intentions and beliefs and the adopting of anintention will be repeated while the agent still has the potential intention and the time to dothe action has not passed.Each request to perform an action has a time associated with it. If all the needed in-tentions have been adopted for an action in time, then at the specified time the agent willinitiate the action. This will lead to the performance of all the basic actions in the recipetree at the appropriate times, assuming that the agents can actually do the actions. Failuremay occur either in the intention formation phase, the waiting phase or during the execu-tion phase. Each such failure should be reported to the relevant agents, i.e., the assistingagents and the asking agent.In writing the axioms we use the convention that all free variables are assumed to beuniversal quantified. The axioms typically have the form α → β where the time of α is tand the time of β is t + 1. This is because the axioms characterize the way the mental stateof the agent and the state of the world change over time. This way we capture the propertythat agent reasoning and actions take time.Additional convention has been introduced to simplify the axioms. When a given propo-sition is included in the databases of all the agents, the belief predicates will be omitted,e.g., we write BL(a, d) instead of ∀m∀tBel(m, t, “BL(a, d)”). The belief predicate is ex-plicitly specified only when the meta-theory refers to the belief of a specific agent, e.g.,Bel(t, m, “CanDo(t (cid:3), m, b)”).176J. Grant et al. / Artificial Intelligence 163 (2005) 163–2013.1.1. Intention formation axiomsWe list here the axioms involved in agent intention formation. Although the subcon-tracting axioms are not included here, we write the axioms in a general way so that theyare applicable both to the case where a single agent does all the work and the case of mul-tiple agents. We explain in English the use of each axiom before writing it in logic. At theend of the subsection we write some applications of the axioms using our example.A1. Asked to do becomes potential intention. If agent n asks agent m to do an action,agent m adopts a potential intention for it. When the first agent is initially askedto do the action, n = _ and m is the agent.ATD(t, n, m, b, a, t(cid:3)) & t + 1 < t(cid:3) → PotInt(t + 1, m, n, b, a, t(cid:3))A2. Inheritance of potential intention. We distinguish between the inheritance of the po-tential intention to do an action b and the adoption (and later inheritance) of thepotential intentions of the subactions in the recipe for b.A2.1: self. If an agent has a potential intention for b and is not refrained fromdoing b, then it will inherit the potential intention.PotInt(t, m, n, b, a, t(cid:3)) & ¬∃m→ PotInt(t + 1, m, n, b, a, t(cid:3))(cid:3)Ref (t, m(cid:3), m, b, a, t(cid:3)) & t + 1 < t(cid:3)A2.2: child. If an agent has a potential intention for b and believes that it cando b, it will get a potential intention for every subaction of b for theappropriate time (based on the times of the subactions in the recipe).PotInt(t, m, n, b, a, tRec(b, r) & Mem(r, c, i, j, k) & t + 1 < t) & Bel(t, m, “CanDo(t(cid:3)(cid:3)(cid:3), m, b)”)→ PotInt(t + 1, m, m, c, b, t(cid:3) + j )A3. Potential intention becomes intention. If an agent has a potential intention for b (inthe context of a) and believes that it can do b and has the intention to do all thesubactions (if any) in the recipe for b, then it will get the intention to do b. In thenext subsection we will present a more general version of this axiom includingsubcontracting as B3.PotInt(t, m, n, b, a, t∀r, c, i, j, k(Rec(b, r) & Mem(r, c, i, j, k) → Int(t, m, m, c, b, t) & Bel(t, m, “CanDo(t, m, b)”) & t + 1 < t&(cid:3) + j ))(cid:3)(cid:3)(cid:3)→ Int(t + 1, m, n, b, a, t(cid:3))A4. Inheritance of refrain. Refrains cancel potential intentions. Thus, as in the potentialintention case, the axiom associated with the inheritance of refrains of an actionb has two parts: one has to do with the refrain of b itself, and one with respect tothe subactions in the recipe for b.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201177A4.1: self. Refrain is inherited by an action unconditionally.Ref (t, m, n, b, a, t(cid:3)) & t + 1 < t(cid:3) → Ref (t + 1, m, n, b, a, t(cid:3))A4.2: child. Refrain of an action is inherited by all the subactions in the recipe.Ref (t, m, n, b, a, t(cid:3)) & Rec(b, r) & Mem(r, c, i, j, k) & t + 1 < t(cid:3)→ Ref (t + 1, n, n, c, b, t(cid:3) + j )In the examples, for convenience, we write the time values as integers instead of as clocktimes but leave the agent names as G and H . Returning to the main example, suppose thatat time 10 agent G is asked to do aaat at time 50. In the meta-language this is representedas the statement ATD(10, _, G, aaat, 50).• Axiom A1 entails PotInt(11, G, _, aaat, _, 50).• A2.1 entails PotInt(12, G, _, aaat, _, 50) assuming that the corresponding Ref is notinvoked.Now, assumeBel(11, G, “CanDo(50, G, aaat)”),Rec(aaat, r0)and Mem(r0, estimate_time_to_leave, 3, 6, 7)are already known, where r0 is the recipe of Fig. 2 and estimate_time_to_leave is the thirdsubaction that is supposed to start at 6 time units after the beginning of aaat and ending at7 units after it.• A2.2 entails PotInt(12, G, G, estimate_time_to_leave, aaat, 56).• A3 entails Int(13, G, G, estimate_time_to_leave, aaat, 56), assumingBel(12, G, “CanDo(56, G, estimate_time_to_leave)”)and knowing BL(estimate_time_to_leave, 1).3.1.2. Subcontracting axiomsSubcontracting is the process that leads to an agent adopting an intention to do an actionmotivated by a request from another agent. It is initiated by a request of the contractingagent to the assisting agent, continues with the adoption of the potential intentions andlater an intention by the assisting agent. The subcontracting process can terminate by afailure either during the intention formation process or waiting period (e.g., refrain). Boththe adoption of the relevant intention by the assisting agent and the failures are reported tothe contracting agent.We write four axioms for subcontracting between agents. In order to keep the numberinganalogous to the numbering of the A axioms, we start the numbering with B2. Two of theaxioms involve explicit communication between agents.B2. Subcontracting an action to an agent. This axiom deals with subcontracting an actionto one of the agents that can do it. This happens when an agent has a potential178J. Grant et al. / Artificial Intelligence 163 (2005) 163–201intention to do an action, but does not believe that it can do it. Unless it willask another agent, this potential intention will not become an intention. The an-tecedents of this axiom are that the agent has a potential intention to do an actionand does not believe that it can do it and believes that agent m is the “first” agentthat can do it and it has not previously asked m to do it. The consequence is thatthe agent will ask m to do the action.PotInt(t, n, n(cid:3)(cid:3)) & ¬Bel(t, n, “CanDo(t(cid:3), n, b)”) &, b, a, t(cid:3), m, b)”) &(cid:3)Bel(t, n, “CanDo(t(cid:3)∀m(cid:3)(cid:3)∀t(Bel(t, n, “CanDo(t(cid:3)(cid:3)< t → ¬ATD(t(t→ ATD(t + 1, n, m, b, a, t(cid:3), m)(cid:3)(cid:3), n, m, b, a, t(cid:3)(cid:3), b)”) → m (cid:1) m(cid:3)) &)) & t + 1 < t(cid:3)B3. Potential intention becomes intention (subcontracting version). If an agent, assistinganother agent, has a potential intention to do an action and believes that it cando the action (possibly with help) and if each subaction in the recipe is eitherintended by the agent or an assisting agent, then the agent will have the intentionto do the action and tell that intention to the agent it is assisting.(cid:3)(cid:3)) & Bel(t, n, “CanDo(t(cid:3)PotInt(t, n, n, b, a, tRec(b, r) & t + 1 < t&∀c, i, j, k(Mem(r, c, i, j, k) → (Int(t, n, n, c, b, t(cid:3) + j )”)))(cid:3)∃mTell(t, m, n, “Int(t, m, n, c, b, t(cid:3)(cid:3)→ Int(t + 1, n, n, b, a, t(cid:3), n, b)”) &(cid:3) + j ) ∨) & Tell(t + 1, n, n, “Int(t + 1, n, n(cid:3), b, a, t(cid:3))”)B4. Inheritance of refrain by an assisting agent. If an agent is refrained from doing an ac-tion and this agent has an assisting agent, then the assisting agent will be refrainedfrom doing the action.(cid:3), n, b, a, t(cid:3)Ref (t, n∃t(cid:3)(cid:3)(cid:3)(cid:3)) & Rec(b, r) & Mem(r, c, i, j, k) &(cid:3)(cid:3)(cid:3) + j )) & t + 1 < t(cid:3) + j< t & ATD(t(t→ Ref (t + 1, n, m, c, b, t, n, m, c, b, t(cid:3) + j )B5. Communication of assisting agent about refrain. If an agent has a potential intentionto do an action for a requesting agent and is refrained from doing it, then the agentwill tell the requesting agent that it does not have the intention to do the actionand the requesting agent will believe that the assisting agent cannot do the action.PotInt(t, m, n, b, a, t∀t< t → ¬Tell(t(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)) & Ref (t, m(cid:3)(cid:3), m, n, “¬Int(t(cid:3)(cid:3)(t→ Tell(t + 1, m, n, “¬Int(t + 1, m, n, b, a, t, m, n, b, a, t(cid:3))”) &, m, b, a, t(cid:3)) &(cid:3))”))Bel(t + 2, n, “¬CanDo(t(cid:3), m, b)”)J. Grant et al. / Artificial Intelligence 163 (2005) 163–201179In both axioms B4 and B5 there are communications. In B5 the communication is statedexplicitly while in B4 it is associated with the refrain. In addition, when an agent is re-frained from doing an action there is a difference between how it influences the assistingagents of subactions and the requesting agent, as is reflected in axioms B4 and B5 respec-tively. All the assisting agents are also refrained since there is no need any more for themto carry out the subactions. The requesting agent is told about the refrain and it will askanother agent, if possible, or refrain.Continuing with our example, suppose that at time 12 G has a potential intention todo get_taxi and believes that H can do this subaction. In our meta-language this is rep-resented as PotInt(12, G, _, get_taxi, aaat, 58) and Bel(12, G, “CanDo(58, H, get_taxi)”).Also suppose that G does not believe that it can do this subaction and has not asked Hpreviously to do it. Thus the theory contains ¬Bel(12, G, “CanDo(58, G, get_taxi)”) and¬ATD(t, G, H, get_taxi, aaat, 58) for every t < 12. Then,• B2 entails ATD(13, G, H, get_taxi, aaat, 58).3.1.3. Performing axiomsNext we present four axioms involving agents doing actions. The first shows how theroot of the recipe tree is initiated. When a basic level action is initiated by an agent, it getsdone in d time units if the agent can do it. For a complex action, the agent must start byinitiating the root of the recipe tree. Each node will have to be initiated in turn. Severalagents may be involved in doing various subactions. We use the initiation of complexactions as a bookkeeping device to make sure that all the subactions get done at the propertime and in the right order.C1. Initiation of requested action. If an agent has an intention to do an action for a secondagent at the next time period, which is not a subaction of an action the agentintends to do, then the agent will initiate the action. A special case arises whenthe second agent is _; this initiates the root action.Int(t, m, n, b, _, t + 1) → Ini(t + 1, m, n, b)C2. Inheritance of initiate for first subaction. If an agent initiates an action, it will initiatethe first subaction in the recipe in the next time step.Ini(t, m, n, a) & Rec(a, r) & Mem(r, b, 1, 1, k) → Ini(t + 1, m, m, b)C3. Inheritance of initiate for later subaction. If the previous subaction is done at theright time and the agent doing action a has subcontracted this subaction to an-other agent, then this second agent will initiate the next subaction. The specialcase is where the same agent does this later subaction. Note that we assume anagent can observe the performance of the actions that are needed for the initi-ation of its own actions, and that its observations lead to beliefs. If not, furthercommunication is needed, as discussed below.Ini(t, m, n, a) & Rec(a, r) & Mem(r, b, i + 1, jMem(r, c, i, j, k) & Bel(t + j(cid:3)(cid:3) − 1, mInt(t + j, k, “Done(t + k, c)”) &(cid:3), m(cid:3)(cid:3) − 1, m(cid:3), m, b, a, t + j) → Ini(t + j, m, b)) &(cid:3)(cid:3)(cid:3)180J. Grant et al. / Artificial Intelligence 163 (2005) 163–201C4. Done for actions.C4.1: basic level. If an agent initiates a basic level action at time t and can do itthen the action will get done.BL(a, d) & Ini(t, m, n, a) & CanDo(t, m, a)→ Done(t + d, a)C4.2: complex. If an agent initiates a complex action and all its subactions inthe recipe get done, then the action will get done. (Note: b is the lastsubaction of a.)Ini(t, m, n, a) & Rec(a, r) & Mem(r, b, i, j, k) &∀r, c, i(Mem(r, c, i) → (i, k, j, j(cid:3) (cid:1) i & Done(t + k(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3), k→ Done(t + k, a)(cid:3), c)))C5. Action performance observed. If an action is done, then all agents come to believethis fact (via observations).Done(t, a) → Bel(t, n, “Done(t, a)”)Although for simplicity we assume that all agents observe all actions; in fact, only twoagents, the requesting agent and the agent intending to do the next action really need toknow. In multiagent systems in which agents cannot observe the activities of other agents,communication is needed to establish the performance of actions. There are many com-munication models that can be applied, such as broadcasting, blackboards and directedmessages. For each communication model C5 could be replaced by appropriate axioms.Again, continuing with our example from the previous subsections, suppose at time 49G has the intention to do aaat, represented as Int(49, G, _, aaat, _, 50). Then,• C1 will entail Ini(50, G, _, aaat), thereby initiating the root action.• C2 will then entail Ini(51, G, G, get_fl_info).Considering another subaction, suppose that estimate_time_to_leave is a basic levelaction taking one unit of time that G initiates and can do at time 58, written as,BL(estimate_time_to_leave, 1), Ini(58, G, G, estimate_time_to_leave), and CanDo(58, G,estimate_time_to_leave), then• C4.1 entails Done(59, estimate_time_to_leave) and C5 entails Bel(59, H, “Done(59,estimate_time_to_leave)”).• From Ini(50, G, _, aaat), Rec(aaat, r0), Mem(r0, get_taxi, 4, 10, 14), Mem(r0,estimate_time_to_leave, 3, 8, 9), Bel(59, H, “Done(59, estimate_time_to_leave)”), andInt(59, H, G, get_taxi, aaat, 60), C3 entails Ini(60, H, G, get_taxi).That is, H will initiate the get_taxi subaction at time 60 for G.J. Grant et al. / Artificial Intelligence 163 (2005) 163–2011813.1.4. Axioms for failure to performHere we present three axioms for the case when the performance of an action fails.Failure begins when an agent fails to perform a basic level action, as indicated by thenegation of the appropriate Done. This leads to a Stop action process that will release theagents waiting to perform various actions from their commitments, that is, their intentions.This is accomplished in the following way. In the recipe tree the Stop action process ispropagated upwards. At the same time the Stop action becomes a Refrain going down inthe recipe tree along the branches that come after the branch that failed.D1. Initiate stop action in case of failure. If an agent fails to do a basic level action, it tellsthe requesting agent and initiates a stop action process on the node of the recipetree above this basic level action.BL(b, d) & Ini(t, m, n, b) & ¬Done(t + d, b)& Int(t − 1, m, n, b, a, t) &(cid:3)∃t(cid:3)(cid:3)(cid:3)a(t< t − 1 & Int(tn→ Stop(t + d + 1, n, n, n, n(cid:3), a, a(cid:3)(cid:3), a, a(cid:3))(cid:3)(cid:3) + 1)), tD2. Stop action propagated up. If an agent initiates a stop action on a node, the stop actionis propagated to the parent node.Stop(t, m, n, b, a) & ∃t(cid:3)→ Stop(t + 1, n, n(cid:3)(cid:3)(t, a, a(cid:3), n, n(cid:3), a, a(cid:3)(cid:3) + 1)), t< t & Int(t(cid:3))D3. Stop triggers refrain down. A stop action triggers a refrain for those subactions thatcome after the branch that failed.Stop(t, m, n, b, a) & Rec(b, r) & Mem(r, c, i, j, k) &(cid:3)t(cid:3), m, c, b, t> t & Int(t, m(cid:3)→ Ref (t + 1, m, m, c, b, t(cid:3))(cid:3))The above actions apply both the Stop predicate and Ref . Stop is used for actions that havebeen initiated, while Ref is used for actions that one of the agents intends to perform, buttheir performance time has not yet arrived.3.1.5. Integrity constraintsIn addition to the axioms we can include integrity constraints in our theory. Integrityconstraints differ from axioms in that they eliminate certain models from consideration,models that would not make sense given the meaning of our predicates. We give only afew integrity constraint here, but in other contexts more may be needed. We write theintegrity constraints using logic programming notation.IC1. Preference among conflicting actions An agent that prefers action a to conflictingaction b cannot both potentially intend to do a and believe that it can do b at thesame time. (Agents are assumed to have preferences among conflicting actions.)182J. Grant et al. / Artificial Intelligence 163 (2005) 163–201← Conf (t, n, a, b) & Prefer(t, n, a, b) & PotInt(t, n, m, a, c, t(cid:3)) &Bel(t, n, “CanDo(t(cid:3), n, b)”)IC2. Consistency of recipe subactions. In a recipe the starting time of the i + 1st subactionmust be greater than the ending time of the ith subaction.← Rec(a, r) & Mem(r, b, i, j, k) & Mem(r, b(cid:3), i + 1, j(cid:3)(cid:3), k) & j(cid:3) (cid:1) kIC3. Consistency of recipe timing. For any member of a recipe the starting time must beless than the ending time.← Mem(r, b, i, j, k) & k (cid:1) jIC4. Uniqueness of recipe information. There cannot be more than one ith subaction in arecipe.← Rec(a, r) & Mem(r, b, i, j, k) & Mem(r, b(cid:3), i, j, k) & b (cid:9)= b(cid:3)(1)A subaction may appear only once in a recipe. This constraint is not included inthe case of repetitive actions in a recipe.← Rec(a, r) & Mem(r, b, i, j, k) & Mem(r, b, i(cid:3), j, k) & i (cid:9)= i(cid:3)A subaction must start at a unique time.← Rec(a, r) & Mem(r, b, i, j, k) & Mem(r, b, i, j(cid:3), k) & j (cid:9)= jA subaction must end at a unique time.← Rec(a, r) & Mem(r, b, i, j, k) & Mem(r, b, i, j, k(cid:3)) & k (cid:9)= k(cid:3)(2)(3)(4)3.1.6. Parallel actionsUp to now we have assumed that the actions in a recipe must be done in a serial order.So, for example, in our running example of aaat (arrange for arriving at airport on time)the four subactions must be done in the order given in Fig. 2. In particular, “find whento arrive” must be done before “estimate time to leave”. However, there are cases whereseveral actions can be done in parallel. Continuing with the travel example, we may havean action “pack suitcase” which consist of “get suitcase”, “gather objects to be taken”, and“put objects in suitcase”. The “put objects in suitcase” subaction must be done last, butthe first two subactions can be done in either order or in parallel. We now show that ourframework can handle recipes with parallel actions; only a few small changes have to bemade to the axioms.The first change is in the uniqueness of recipe information constraints. IC4 (1) mustbe changed by changing i to 1. That is, we allow several different subactions, saybi1, bi2, . . . , bik to be done in parallel after the subaction bi−1. So we consider the ithsubaction to be a set of subactions. We do not allow a subaction to appear more than oncein such a set. However, the first subaction still must be treated in a special way because theagent that believes it can do it believes that it can do the action itself (possibly with help).Hence there can be only one first subaction. We may also remove IC4 (3) and (4) in casedifferent parallel actions may start and end at different times.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201183The second change is the modification of axiom C3. In the original version of this axiomthe (i + 1)st subaction is initiated after the ith subaction is done and the assumption is thatthere is a single ith and (i + 1)st subaction. In the parallel version we need to write, seebelow, that the (i + 1)st subactions are initiated after all the ith subactions are done.Ini(t, m, n, a) & Rec(a, r) & Mem(r, b, i + 1, j, k(cid:3)(cid:3) − 1, m, “Done(t + k, c)”) &∀c, i, j, k(Mem(r, c, i, j, k) → Bel(t + jInt(t + j) &)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3) − 1, m, m, b, a, t + j(cid:3)(cid:3), m, m, b)→ Ini(t + j3.2. Consistency and meta-theoremsConsider a meta-language L and a theory T in L that contains the axioms we presentedin Section 3.1 as well as additional specific axioms related to the agents, such as the beliefsof the agents.We first prove that any such theory T is consistent.Theorem 1. T has a model and is therefore consistent.Proof. The proof is essentially the same as that of Theorem 1 of [18], so we omit some ofthe details. We note that most of the predicates have as their first argument a time value t.Those predicates that do not contain such a time value, such as BL, represent statementsthat are always true. Suppose that we are given a specific language L. The atoms of theHerbrand universe for L can be divided into groups based on the time value t; use t = 0for atoms without such a time value.The construction of the model uses the statements of T . These statements are eitheratoms or implications where the time component of the consequent is greater than or equalto the time components in the antecedent. Start with the statements whose time value (eitherexplicitly or implicitly) is 0. Proceed by induction on these time values, thus assuming thatat time t + 1 all the atoms with time t have already been constructed. Then, use the axiomsto generate the atoms at time t + 1 in those cases where by the appropriate substitutionsthe antecedent of the axiom is already true in the model. This construction yields a modelthat is also a minimal model and which we take to be the intended model. Every positivefact in this model must be there because of the applications of the axioms. (cid:1)We take this minimal model as the intended model. By studying this minimal modelunder certain initial conditions and known facts about agent beliefs and abilities and thestructure of recipes, we can prove various results. The proofs consist of tracing the steps ofthe agents over time in the model. We include three results here: two positive, one negative.The two theorems show that if there is “enough” time allowed for the intention formationprocess associated with a complex action all of whose subactions a single agent can do(Theorem 2) or subcontract some to other agents (Theorem 3), then the agent will do it.The proposition shows that if one basic level action in the recipe of a complex action cannotbe done by any agent, then no agent will get the intention to do the complex action. There184J. Grant et al. / Artificial Intelligence 163 (2005) 163–201are some models of the meta-theory where this result fails, but it is true in the intendedmodel.Theorem 2. Suppose an agent is asked at time t to do an action a starting at time t + s,and suppose that the recipe tree for a has height h, takes k units of time, and s (cid:2) 2h + 3.Further suppose the agent believes that it can do each subaction (at the needed time) itself,is not refrained at any time from doing the action or any subaction, and that it in fact cando all of them. Then the agent will complete the task at time t + s + k. Moreover, any valuefor s smaller than 2h + 3 will not be sufficient to start the action at time t + s.Proof. It suffices to show that intention formation takes at most 2h + 2 units of time be-cause then the agent can start doing the action at time t + s (cid:2) t + 2h + 3 and since it can doeach step, it will finish at time t + s + k. We do this by showing abbreviated versions of thekey formulas that become true as time changes: we omit the agent name (there is only oneagent) and the times. We also omit the inherited formulas but add some comments aboutthem at the end. We also do not show other true formulas, such as BL(z1), that are assumedto hold for all times.t ATD(a)t + 1 PotInt(a)t + 2 PotInt(b1), PotInt(b2), . . . the children node of a. . .t + h + 1 PotInt(z1), PotInt(z2), . . . the nodes at the bottom levelt + h + 2 Int(z1), Int(z2), . . .. . .t + 2h + 1 Int(b1), Int(b2), . . .t + 2h + 2 Int(a). . .t + s Ini(a)t + s + 1 Ini(b1). . . The times for the Ini and Done of the nodes depend on the tree structuret + s + k Done(a)We note that PotInt(a) will actually hold starting at time t + 1 and ending at time t + s − 1.In fact, in this case all the potential intentions and intentions will hold from their earliesttime as indicated above (e.g., t + 2 for PotInt(b1), t + h + 2 for Int(z1)) until t + s(cid:3) − 1,where the subaction is supposed to be done at time t + s(cid:3). Since s (cid:2) 2h + 3, the agent willbe able to initiate the action at time t + s. Because Ini(a) must occur after Int(a), for any sless than 2h + 2 + 1 = 2h + 3, there will not be enough time for the agent to obtain Int(a)and hence cannot initiate the action at time t + s. (cid:1)In particular, in the aaat example h = 2, hence if only one agent is doing all the subac-tions, 6 time units must be allocated for the intention formation process, so that the workcan start at 7 units after the initial request for the action.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201185The next result generalizes the previous one to the case of multiple agents with sub-contracting. We note, however, our assumption that each subcontracting between agentsis successful. This may be a reasonable assumption if each agent has knowledge of otheragents’ abilities (and their beliefs about them) and the agents are not busy doing othertasks. We will show later what happens when subcontracting is not always successful.Theorem 3. Suppose an agent is asked at time t to do an action a starting at time t + s,suppose that the recipe tree for a has height h and takes k units of time, and s (cid:2) 4h + 5.Further, suppose for each subaction the agent either believes and can do it or can success-fully subcontract it to the first agent that it believes can do it (at the needed time), and noneof the agents is refrained at any time from doing an action or any subaction, and in fact theintending agent can always do the action or subaction. Then the agent, with the assistanceof the subcontracting agents, will complete the task at time t + s + k. Moreover, any valueof s smaller than 4h + 5 will not, in general, be sufficient to start the action at time t + s.Proof. We use the proof of Theorem 2 and show where this proof differs from it. Whatmay happen at any node is that the agent subcontracts the subaction to another agent. Usingaxiom B2 takes one step and then the second agent gets a potential intention by using A1.Thus at each of the h + 1 levels at most two time steps must be added, so in the worstcase the potential intentions at the bottom level are reached at time t + 3h + 3 instead oft + h + 1. Going back up the tree takes the same amount of time as before, that is, h + 1time units. Hence the intention for a will be reached at time t + 4h + 4. The bound for s isneeded in case subcontracting takes place at each level. (cid:1)In our running example of aaat with 2 agents and s = 2, 4h + 4 = 12 units of timemust be allocated for the intention formation process, in order to initiate the action at timet + 13.Proposition 1. If there is a basic level action b in the recipe for a that no agent believes itcan do, then no agent will get an intention to do a.Proof. No agent will ever get an intention to do b and hence to do a. (cid:1)4. Modeling multiple-recipe actionsThe previous section dealt with the intention formation and subcontracting (and per-forming) of actions, where each action has a single recipe. This leads to a single recipetree for a complex action. In this section we generalize the work for intention formationand subcontracting to the case where actions (may) have multiple recipes. In this case wemay think of the recipe tree as an and-or tree with alternating levels of recipes and actionswhere the recipes for an action are joined by “or” and the actions of a recipe are joined by“and”.The key difference in reasoning between the single recipe case and the multiple recipecase is that the recipe tree becomes a complex and-or tree in the second case. When an186J. Grant et al. / Artificial Intelligence 163 (2005) 163–201agent finds that one recipe will not work, it must find and apply another one. When all therecipes for an action do not work, the agent must backtrack in the tree to the parent actionof that action and find another recipe for it. Another issue is that if a subcontracting agentis determined as unable to do a task, another agent must be subcontracted. A second back-tracking must then be introduced to represent the backtracking between the subcontractingagents in addition to the backtracking between recipes.In order to enhance the presentation we deal separately with the case of a single agentand multiple agents. We show the details for the single agent case, in particular, the han-dling of recipe backtracking, and sketch the modifications, including the backtrackingamong agents, required by multiple agents.4.1. The single agent caseAs we deal with the case of a single agent, the agent names are not needed. Weomit them everywhere in order to make the axioms easier to understand. However, somepredicates now need to include the recipe name. For convenience we start by listing thepredicates used in this section, before presenting and explaining the axioms.4.1.1. The predicatesThe following predicates are important for single agent intention formation with multi-ple recipes.ATD(t, a, t (cid:3)): At time t the agent is asked to do a at time t (cid:3).PotInt(t, b, a, r, t (cid:3)): At time t the agent has the potential intention to do subaction b ofInt(t, b, a, r, t (cid:3)): At time t the agent has the intention to do subaction b of action a usingRef (t, b, a, r, t (cid:3)): At time t the agent is refrained from intending to do subaction b ofaction a using recipe r at time t (cid:3).recipe r at time t (cid:3).action a using recipe r at time t (cid:3).We assume that the recipes for an action form a list which is represented by the predicateNextRec as follows:NextRec(a, r, r (cid:3)): In the list of recipes for a, r is followed immediately by r (cid:3).Our convention is that the first recipe r is indicated by writing NextRec(a, _, r)and the last recipe r by NextRec(a, r, _) (because _ is used as the empty recipe).When a recipe fails, the agent needs to try another recipe. For this purpose we have thefollowing predicate.FailedRec(t, a, r, t (cid:3)): At time t the recipe r for a to be done at time t (cid:3) failed.When the last recipe for a subaction fails, the agent needs to try another recipe for theaction. In order to allow the agent to work its way up the tree of recipes, we have thefollowing predicate.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201187Parent(a, r, t, a(cid:3), r (cid:3), t (cid:3)): The subaction a of action a(cid:3) is to be done using recipe r at timet, while a(cid:3) is to be done using recipe r (cid:3) at time t (cid:3).It may turn out that the agent tried all the recipes and cannot do an action it was asked todo. At this point it should report the failure of the whole process. This is done by using thefollowing:StopPlan(t, a, t (cid:3)): At time t the agent stops planning to do a at time t (cid:3).4.1.2. The axiomsAs we did in Section 3.1 we list all the axioms with explanations. Some of the ax-ioms are new versions of previous axioms; we indicate this by adding MR to the previousdesignation.A1-MR. Asked to do becomes potential intention. We do the version where the singleagent is initially asked to do the action. The difference is the addition of Parent inthe consequent.ATD(t, a, t(cid:3)Parent(a, _, t) & t + 1 < t, _, _, _)(cid:3)(cid:3) → PotInt(t + 1, a, _, _, t(cid:3)) &A2-MR. Inheritance of potential intention.A2.1-MR: self. The difference here is that the potential intention is inherited withthe same recipe.PotInt(t, b, a, r, t(cid:3)→ PotInt(t + 1, b, a, r, t) & ¬Ref (t, b, a, r, t(cid:3))(cid:3)) & t + 1 < t(cid:3)A2.2-MR: child. The difference here is substantial. The second conjunct in theantecedent indicates that this is a newly obtained potential intention.The NextRec predicate identifies r (cid:3) as the first recipe. As in A1-MR theParent predicate is set correctly in the consequent.(cid:3)PotInt(t, b, a, r, t(cid:3)Bel(t, “CanDo(t(cid:3)) & ¬PotInt(t − 1, b, a, r, t) &, b)”) & NextRec(b, _, r(cid:3)(cid:3)Mem(r, c, i, j, k) & t + 1 < t(cid:3)) &→ PotInt(t + 1, c, b, r(cid:3)(cid:3) + j ) & Parent(c, r(cid:3), t(cid:3) + j, b, r, t(cid:3)), tA3-MR. Potential intention becomes intention. The difference here is that the agent hasthe intention to do all the subactions of some recipe for b.(cid:3)(cid:3)(cid:3)(cid:3)PotInt(t, b, a, r, t∃r(∀c, i, j, k Mem(r(NextRec(b, r, r(cid:3)(cid:3)(cid:3), r) &→ Int(t + 1, b, a, r, t(cid:3))) & Bel(t, “CanDo(t(cid:3), b)”) & t + 1 < t(cid:3)&(cid:3), c, i, j, k) → Int(t, c, b, r(cid:3)(cid:3) + j )), t188J. Grant et al. / Artificial Intelligence 163 (2005) 163–201D1-MR. Chosen recipe fails. The chosen recipe has a subaction that the agent does notbelieve it can do, hence the recipe fails. The Parent predicate is needed in theantecedent for the starting time of a.PotInt(t, b, a, r, t(cid:3)) & ¬Bel(t, “CanDo(t(cid:3)(cid:3)→ FailedRec(t + 1, a, r, t)(cid:3), b)”) & Parent(b, r, t(cid:3), a, r(cid:3)(cid:3)(cid:3)(cid:3)), tD2-MR. Potential intention inherited with different recipe. If a recipe, that is not the lastrecipe of an action, fails, then the agent inherits the potential intention for theaction, but with the next recipe for the action.Parent(a(cid:3)(cid:3)(cid:3)(cid:3), t(cid:3)(cid:3)(cid:3)(cid:3)(cid:3), r, t(cid:3)(cid:3), r(cid:3), a(cid:3)) & FailedRec(t, a(cid:3)NextRec(a, r, r) & r→ PotInt(t + 1, b, a(cid:3) (cid:9)= _ & Mem(r(cid:3), t, r(cid:3)(cid:3) + j ) & Parent(b, r(cid:3)(cid:3)) &, r, t, b, i, j, k) & t + 1 < t(cid:3)(cid:3) + j, a(cid:3), t(cid:3)(cid:3)(cid:3)(cid:3)), t, rD3-MR. Recipe failure induces refrain. If a recipe fails, then a refrain is induced for eachsubaction of the recipe. This way the potential intentions will not be inherited viaA2-MR.FailedRec(t, a, r, t(cid:3)) & Mem(r, b, i, j, k) & t + 1 < t(cid:3)→ Ref (t + 1, b, a, r, t(cid:3) + j )D4-MR. Failure of the last recipe for an action.D4.1-MR: non-root node. If the last recipe for an action fails, that failure propa-gates upward in the recipe tree to the parent action. The condition a(cid:3) (cid:9)= _is needed in the antecedent because otherwise a is the root action andhas no parent.(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)PotInt(t, a, a(cid:3)(cid:3), r(cid:3), t(cid:3)Parent(a, r, t, t→ FailedRec(t + 1, a, a, r(cid:3)) & FailedRec(t, a, r, t(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3) (cid:9)= _) & a(cid:3)(cid:3)(cid:3)(cid:3)), t, r(cid:3)) & NextRec(a, r, _) &D4.2-MR: root node. If the last recipe for the root action fails, the agent stopsplanning the action entirely.(cid:3)PotInt(t, a, _, _, tNextRec(a, r, _) & t + 1 < t→ StopPlan(t + 1, a, t)(cid:3)) & FailedRec(t, a, r, t(cid:3)(cid:3))&4.1.3. ExampleIn this subsection we illustrate some of the axioms by using the example of Fig. 2 withan additional recipe. To make things simple we again assume that there is only one recipefor aaat, as given there, and one recipe for get_taxi, as given there. However, now there isa second recipe for find_when_to_arrive, as shown in Fig. 3. Thus there are two ways foran agent to find out when to arrive at the airport: first, it can find the url of the airline andJ. Grant et al. / Artificial Intelligence 163 (2005) 163–201189Fig. 3. Additional recipe tree.then do a web search, or, second, it can find the airline phone number and then call. In thisillustration we concentrate on intention formation with these two recipes.We also assume that there is only one agent, G, whose name will not be written in theformulas in accordance with the convention for single agents. Again, we start with the ATDstatement, now written as ATD(10, aaat, 50), that is, at time 10 the agent was asked to doaaat at time 50.• Axiom A1-MR entails two facts: PotInt(11, aaat, _, _, 50) and Parent(aaat, _, 50, _,_, _).• A2.1-MR entails PotInt(12, aaat, _, _, 50).Assume that the following statements are known:Bel(11, “CanDo(50, aaat)”),NextRec(aaat, _, r0),Mem(r0, find_when_to_arrive, 2, 3, 7),andthat is, the agent believes that it can do aaat, the first recipe for aaat is r0, and the secondmember of this recipe is find_when_to_arrive, starting at relative time 3 and ending atrelative time 7.• A2.2-MR entailsPotInt(12, find_when_to_arrive, aaat, r0, 53)Parent(find_when_to_arrive, r0, 53, aaat, _, 50).andRecall that we have two recipes for find_when_to_arrive. The first recipe uses the web, thesecond the phone. We write these as r1 and r2 respectively. Let us assume the followingfour statements dealing with the action find_when_to_arrive:Bel(12, “CanDo(52, find_when_to_arrive)”),NextRec(find_when_to_arrive, _, r1),Mem(r1, find_url, 1, 1, 2),and Mem(r1, searchweb, 2, 3, 4).190J. Grant et al. / Artificial Intelligence 163 (2005) 163–201• A2.2-MR entails the following four statements:PotInt(13, find_url, find_when_to_arrive, r1, 54),Parent(find_url, r1, 54, find_when_to_arrive, r0, 53),PotInt(13, searchweb, find_when_to_arrive, r1, 56),Parent(searchweb, r1, 56, find_when_to_arrive, r0, 53).Next suppose that Bel(13, “CanDo(56, searchweb)”) does not hold. This is the case wherethe chosen recipe, r1 does not work. Hence• D1-MR entails FailedRec(14, find_when_to_arrive, r1, 53).Now, using the statementsNextRec(find_when_to_arrive, r1, r2),Parent(find_when_to_arrive, r1, 53, aaat, r0, 50),Mem(r2, find_airlinephone, 1, 1, 2)andMem(r2, call_airline, 2, 3, 4).• D2-MR entailsPotInt(15, find_airlinephone, find_when_to_arrive, r2, 54)andPotInt(15, call_airline, find_when_to_arrive, r2, 56)as well as some instances of the Parent predicate, not needed for this illustration, and theprocess continues with this recipe. Note also that• D3-MR entailsRef (15, find_url, find_when_to_arrive, r1, 54)andRef (15, searchweb, find_when_to_arrive, r1, 56).So some potential intentions will not be inherited. Assuming that recipe r2 is appropriate,then,• A2.2-MR and A3-MR entail Int(17, find_when_to_arrive, aaat, r2, 55)and if intentions are obtained for all the other members of r0• Int(18, aaat, _, r0, 50) will follow.J. Grant et al. / Artificial Intelligence 163 (2005) 163–2011914.1.4. Meta-theoremTheorem 1, given for the case of a single recipe can be extended to the theory of themulti-recipe case. Thus this theory also has an intended model and is consistent. Further-more, it is possible to extend Theorem 2 to the case of multiple recipes. Before doing so,we find it useful here to think of a recipe tree in an alternative manner. At the beginningof Section 4 we indicated that in the case of multiple recipes the recipe tree becomes acomplex and-or tree. But here we think of a recipe tree as a regular tree, just as we usedthe concept in the case of single recipe actions. Then with each action we associate a setof recipe trees, where each recipe tree is obtained by choosing a specific recipe for eachcomplex action. In other words, a recipe tree is obtained from the and-or tree by choosingone subtree for each “or” branch.The generalization of Theorem 2 is as follows:Theorem 4. Suppose an agent is asked at time t to do an action a starting at time t + s,there are w recipe trees, the maximum height of all recipe trees is h, the maximum timefor any recipe is k units of time, and s (cid:2) 2hw + 3. Further, suppose there is a recipe treefor which the agent believes that it can do each subaction (at the needed time) itself, is notrefrained at any time from doing the action or any subaction, and that it in fact can dothem. Then the agent will complete the task at time t + s + k. Moreover, in the worst case,any value for s smaller than 2hw + 3 will not, in general, be sufficient to start the actionat time t + s, and hence finish at time t + s + k.Proof. As in Theorem 2 we show that intention formation takes at most 2hw + 2 unitsof time by showing abbreviated versions of the key formulas that become true as timechanges. However, we only go to the point where the second recipe tree is started. Toconsider the worst case we assume that all the w recipe trees are entirely different and haveheight h.t ATD(a)t + 1 PotInt(a, _) a is not a child node for a recipet + 2 PotInt(b1, r1), PotInt(b2, r1), . . . the children node of a in recipe r1. . .t + h + 1 PotInt(z1, r1h1), PotInt(z2, r1h2), . . . the nodes at the bottom level for the firstrecipe treet + h + 2 FailedRec(r1hk) a recipe fails at the next to bottom level. . .t + 2h + 1 FailedRec(a, r1) the first recipe tree failst + 2h + 2 PotInt(b1(cid:3), r2), PotInt(b2(cid:3), r2), . . . reasoning continues with the next recipe. . .Thus it takes 1 unit of time to get the initial potential intention for the root node and 2hunits to recognize the failure of the first recipe tree (in the worst case). Each recipe treemay take up to 2h units including the wth (last) one, where instead of getting FailedRecgoing back up the tree, Int will be entailed. For this last recipe tree one more time unit is192J. Grant et al. / Artificial Intelligence 163 (2005) 163–201needed to get Int(a). Hence, altogether, the process may take up to 1 + 2hw + 1 = 2hw + 2units. (cid:1)In the example for aaat, using h = 2 and w = 2, we obtain 2hw + 2 = 10, hence inten-tion formation takes at most 10 units of time. Note also that the formula for s = 2hw + 3reduces to 2h + 3 in case w = 1, which is the formula of Theorem 2.4.2. Multiple agentsWe only sketch the case of multiple agents and multiple recipes here. It was observed inthe previous section that in the case of multiple recipes a backtracking mechanism must beintroduced into the meta-language. We accomplished this by placing the recipes for eachaction into a list using the NextRec predicate, explicitly stating that a recipe failed by usingthe FailedRec predicate, and keeping track of where the agent’s reasoning is in the complexand-or tree of recipes by using the Parent predicate.We begin by considering the case of multiple agents with single recipes in a more gen-eral way. In our work in Section 3 we assumed that an agent, say Agent0, subcontracts anaction a to the first agent, say Agent1, that it believes can do a. We did not deal with thecase where Agent1 itself does not believe that it can do a. In this case it would be reason-able for Agent0 to try another agent, say Agent2, that Agent0 believes can do a. Setting upthis process involves backtracking in a way that is similar to the case of multiple recipes.Namely, each agent will have a list of agents; if the chosen agent does not believe that it cando the action, it will fail and the next agent will be chosen. Hence an agent backtrackingmechanism must be set up in a way that is similar to the recipe backtracking mechanism.Consider now the case of multiple agents and multiple recipes where during the rea-soning process there may be both failed recipes and failed agents. In this case a doublebacktracking must be set up. A natural way to do this is to nest one backtracking inside theother one. For example, using the recipes for the outer backtracking would mean that theagent will always start with the first recipe, but then it may have to subcontract to severalagents before finding one that believes it can do the relevant action. Each subcontractingagent, in turn, will start with the first recipe and try subcontracting to other agents, tryingthe next recipe only if none of the agents believes that it can do the subcontracting action.We now sketch a scenario for the example given in Fig. 3. Recall that there are tworecipes for find_when_to_arrive. In the previous subsection we had only one agent, butlet us go back to the case of two agents G and H from Section 1.3. Suppose also thatwhen G tries to subcontract the searchweb action to H , H does not believe that it can doit. In that case, since there are no other agents, the first recipe for find_when_to_arrivewill fail and the second recipe must be tried. Since H finds phone numbers, the ac-tion find_when_to_arrive will be subcontracted to H which may then subcontract thecall_airline action back to G.Our goal now is to generalize Theorem 2 to the case of multiple agents and multiplerecipe trees. We start with the case where there is only one recipe tree and at least twoagents, that is, w = 1 and z (cid:2) 2.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201193Theorem 5. Suppose an agent is asked at time t to do an action a starting at time t + s,there is one recipe tree, the height of the recipe tree is h, the maximum time for any recipe isk units of time, there are z agents, z (cid:2) 2, and s (cid:2) (h + 1)(3z − 1) + 1. Further, suppose theagent believes that it can do each subaction (at the needed time) itself or can successfullysubcontract it to some agent that it believes can do it (at the needed time), and in fact thereis always such an agent, and neither agent is refrained at any time from doing the actionor any subaction, and that it in fact can do it. Then the agent will complete the task at timet + s + k. Moreover, any value for s smaller than (h + 1)(3z − 1) + 1 will not, in general,be sufficient to start the action at time t + s.Proof. As in Theorem 3 we show a portion of the intention formation process in the worstcase using abbreviated versions of the key formulas. Agents are represented by the numbers1, . . . , z.t ATD(1, a) the first agent is asked to do at + 1 PotInt(1, a)t + 2 FailedAgent(1, a) the first agent does not believe that it can do at + 3 ATD(2, a) the second agent is asked to do at + 4 PotInt(2, a)t + 5 FailedAgent(2, a) the second agent does not believe that it can do a. . .t + 2 + 3(z − 2) FailedAgent(z − 1, a) the next-to-last agent does not believe that itcan do at + 2 + 3(z − 2) + 1 ATD(z, a) the last agent is asked to do at + 2 + 3(z − 2) + 2 PotInt(z, a) the last agent believes it can do at + 2 + 3(z − 2) + 3 PotInt(z, b), PotInt(z, c), . . . where b and c are the children nodesof a. . .The calculation of the time periods is as follows (in the worst case). At each level the firstagent is asked and fails: that is 2 units. The next z − 2 agents take 3(z − 2) units, and thelast agent takes 2 time units. Altogether this is 3(z − 2) + 4 = 3z − 2 units. This process isrepeated for the h + 1 levels, taking (h + 1)(3z − 2) time units to reach and succeed at thebottom level. To this must be added h + 1 units, going back up the tree with the intentions.Thus the reasoning takes (h + 1)(3z − 1) time units. (cid:1)In the example for aaat assuming a single recipe tree and two agents, that is, z = 2 andh = 2, we obtain (h + 1)(3z − 1) = 15 units that should be allocated for reasoning.We end with the most general result for success where there are multiple recipe treesand multiple agents.Theorem 6. Suppose an agent is asked at time t to do an action a starting at time t + s,there are w recipe trees, the maximum height of a recipe tree is h, the maximum time forany recipe is k units of time, there are z agents, z (cid:2) 2, and s (cid:2) (h + 1)(3z − 1)w + w.Further, suppose for some recipe tree the agent believes that it can do each subaction (at194J. Grant et al. / Artificial Intelligence 163 (2005) 163–201the needed time) itself or can successfully subcontract it to some agent that it believes cando it (at the needed time), and in fact there is always such an agent, and neither agent isrefrained at any time from doing the action or any subaction, and that it in fact can do it.Then the agent will complete the task at time t + s + k. Moreover, any value for s smallerthan (h + 1)(3z − 1)w + w will not, in general, be sufficient to start the action at time t + s.Proof. We just sketch here the calculation using the proof of Theorem 4. We start withthe same process of multiple agents for the first recipe tree. This takes (h + 1)(3z − 1)units. This process is done for all the w recipe trees, adding one time unit to get to eachnew recipe tree, finally obtaining (h + 1)(3z − 1)w + (w − 1) time units for the reasoningprocess.In the case of aaat with 2 agents and two recipe trees, the intention formation processwill take at most 3 × 5 × 2 + 1 = 31 time units. (cid:1)5. Related workIntentions in the context of SharedPlans were studied in [19,20], but no semantics weregiven. Our starting point in this paper was the axioms presented by Grosz and Kraus butour requirements for an agent having an intention are much stronger than those presentedin [19] where an agent may have an intention also when having a partial plan. For example,the agent may have only partial knowledge about a recipe, but a plan how to complete it;it may have only potential intentions toward subactions. On the other hand, we requirethat in order for the agent to have an intention, it must have a full detailed plan to do theaction and that it has adopted the appropriate intentions and beliefs. These requirementstogether with our semantics enable us to state and prove various properties and theoremsabout agent intentions and actions.Since the definition of Int in our model is much stronger than the Int.To of SharedPlans,the PotInt predicate of our model plays a more important role in the agent’s reasoning thanthe Pot.Int.To does in [19]. In [19] Pot.Int.To is used only as an intermediate operator untilInt.To is adopted. In our model the PotInt is kept for the duration of the agent’s need for theassociated intention and is used during the intention formation process and for continuousverification of the minimal requirements for having the intention.The SharedPlan model of collaborative planning uses the mental state model ofplans [42]. Bratman [5] also argues for a mental-state view of plans, emphasizing the im-portance of intentions to plans. He argues that intentions to do an action play three rolesin rational action: having an intention to do an action constrains the other intentions anagent may adopt, focuses means-ends reasoning, and guides replanning. These roles areeven more important for collaborative activity than for individual activity. In our model Intand PotInt play these roles.Most of the models of intention of a single agent do not have context parameters (e.g.,[44,51]). However, once agents are working in a group, such parameters are very impor-tant to enhance cooperation [20]. Since we apply a syntactic approach, the introductionof such parameters is not difficult compared with adding such parameters in modal logics[25]. Thus, one of the parameters for both PotInt and Int is the agent that is assisted by theJ. Grant et al. / Artificial Intelligence 163 (2005) 163–201195performance of the intended action. Another parameter is the action in whose context theintended action is performed. In [19], an intention has a general parameter of context whichincludes the intentional context in which the intended action is being performed (amongother things). Since subcontracting is the main focus of our paper, and the intentional con-text is very important for the interactions between the agents, our intentions have thesetwo explicit parameters. These parameters are different from relativized intentions suchas in Cohen and Levesque [8]. They allow an “escape clause” or “background condition”parameter. Once this clause becomes false, the agent drops the associated intention. In ourcase, explicit communication from the assisted agent may lead to dropping the intention,as discussed above.Castelfranchi [7] studies the notion of intention for describing and understanding theactivities of groups and organizations in the context of improving the exchange betweenAI and social and management approaches to cooperative work. His motivation is differentfrom our aim of developing a formal logic of beliefs and intentions.Others proposed models for team intentions (e.g., [9,32,50,52]) while we focus on theintentions of individual agents that may subcontract to other cooperative agents.There were several attempts to develop possible worlds semantics for agents’ intentions[2,8,15,25,28,29,43,55]. Some problems arise with these attempts such as that in mostof them intentions are closed under Modus Ponens or under logical equivalence and thatthe relations between the action’s recipe and the intention are not well defined. Using asyntactic approach provides more freedom in modeling the way agents’ intentions andbeliefs change over time. See [55] for an excellent survey. We discuss here a few modelsthat address some of the problems that we consider.Konolige and Pollack [25] use a special type of possible world models which they referto as cognitive structures. It is equivalent to Chellas’ minimal model semantics (that is dif-ferent from our first-order minimal models). Using the cognitive structures, Konolige andPollack eliminate the closed under inferences problem with respect to intentions. How-ever, their intentions are still closed under logical equivalence. In order to capture relationsbetween intentions, they introduce the concept of an embedding graph among intentions.This extension allows them to model a static relationship between intention and beliefs, butit is not appropriate for modeling a team of agents working together dynamically to planand perform a complex action.Georgeff and Rao [15] consider the problem of intention maintenance in the contextof changing beliefs and desires. They extend the standard possible-world semantic logicsby introducing forms for only modalities of belief, desire, and intention along the lines ofLevesque’s only belief operator [31]. Intuitively, when moving from one time to anotherthey maintain as many old intentions as possible that satisfy some constraints of whatis a consistent set of intentions. They specify semantic constraints on their models thatreflect this intuition (but do not prove completeness). Intention maintenance is done in ourmodel via the inheritance axioms. We distinguish between PotInt and Int: PotInt is alwaysinherited unless the agent is explicitly refrained from doing the action. An intention todo an action is inherited only if the associated PotInt is inherited, the agent continues tobelieve that it can do the action, and if each subaction (if exists) in the recipe of the actionis either intended by the agent or an assisting agent.196J. Grant et al. / Artificial Intelligence 163 (2005) 163–201Sonenberg and her colleagues [23,45,52] present a very comprehensive and interestingpossible-world semantics model for beliefs, intentions, and time, and propose the conceptsof a plan graph and a plan structure. We, on the other hand, propose a syntactic approachfor beliefs and intentions and time, and explicitly express them in our logic. They usethese concepts for defining joint intentions and for reasoning about team activity while wefocus on an agent’s individual intention and its capability to subcontract some actions toother agents. Although their model enables the expression of much more complex plansthan ours, the lack of explicit terms for expressing time limits their ability to reason aboutthe ways agents’ intentions change over time. We also model explicitly the process ofbacktracking in case of a failure.Singh and Asher [51] present a formal theory of intentions and beliefs based on Dis-course Representation Theory. As in our model, this theory does not assume that agents areperfect reasoners. However, they do not model the formation of intentions and its relationto planning.Cohen and Levesque [8,10] have a notion of intention based on persistence goals(P-GOAL). They assume that if an agent has a P-GOAL toward a proposition, then theagent believes that this proposition is not true now, but that it will be true at some time inthe future. The agent will drop a persistent goal p only if it comes to believe that p is trueor that p is impossible. In their logic, time does not explicitly appear in the proposition;thus, they cannot express a P-GOAL toward propositions that will be true at some specifictime in the future or consider situations where a proposition is true now, but which theagent believes will become false later and therefore has to make a P-GOAL true again afterit becomes false. Our intentions are toward actions. Since time is explicit in our logic, wecan express intentions toward performing a given action at a specific time, in addition toexpressing Cohen and Levesque’s attitude P-GOAL. This can be done by defining in ourlanguage predicates similar to those of Cohen and Levesque (e.g., BEL, GOAL, P-GOAL)and adding to our theory appropriate axioms that characterize these predicates.Sadek [46] who extended and refined Cohen and Levesque’s theory of intentions in-troduced the concept of need or potential intention. In his model, the concept of need islogically characterized from choice, belief and intention: an agent needs φ if the fact thatit does not believe φ is a sufficient condition for intending to believe φ. In our model po-tential intention is a basic concept that means roughly that the agent has been given a taskand is determining whether it can do it (perhaps with help).Goldman and Lang [17] also extended Cohen and Levesque’s work. They use Allen’stemporal logic [1] as a foundation and also use syntactic models of belief. They are ableto formalize complex intentional actions, particularly with deadlines. Intentions are mo-tivated by goals, but the process of intention foundation is not modelled, in particular,subcontracting and communications between agents are not considered.SRIs Procedural Reasoning System (PRS) [39] is a framework for constructing real-time reasoning systems. Some of our concepts are similar to features of PRS. For example,a PRS Act describes the steps of a procedure and hence is something like a simple recipein our terminology that consists of basic level actions. A PRS intention involves Acts andSubacts and is a kind of combination of our intention along with a recipe. In PRS time isnot represented explicitly and there are important differences with our approach concerningsubcontracting and other concepts.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201197Wooldridge and Jennings [56] provide a formal model of the cooperative problem solv-ing process for multi-agent systems using a logical formalism that combines aspects ofmodal, temporal, and dynamic logic. Verbrugge and Dunin-Keplicz [54] investigate thenotion of collective intention in the context of cooperative problem solving for multi-agent systems. They formalize collective intentions in a multi-modal logical frameworkand prove the completeness of the logic with respect to a class of Kripke models.An intention in our model is toward an action and not toward a proposition as in mostof the models mentioned above. This somewhat limits intention, but it allows us to clearlyrelate intentions and recipes. Our intentions can be converted to proposition by using anoperator like “Do”. The other models’ intentions can be converted to actions by using aspecial action “Make-True”.An interesting dual treatment of agents that, like ours, has both an agent language (“first-person account”) [16] and a meta-language (“third-person account”) [30], uses the Gologfamily of languages based on the situation-calculus. Those papers (unlike our own) aremore focussed on knowledge conditions than on intentions, and also do not take time-taken-to-plan into account.Wooldridge and his colleagues [48,57] are concerned with intention formation and in-tention reconsideration. They focus on the issue of designing agents that can balance theamount of time spent in reconsidering their intentions against the amount of time spentacting to achieve them. They use a formal but non-logical approach and it is an open ques-tion how the balance between intention reconsideration and acting could be modeled in ourlogic.Subcontracting was considered mainly in situations of self-interested agents where theemphasis was on finding mechanisms for motivating the subcontractor to perform its taskas required (see a survey in [26]). In this paper we assume that the agents are cooperativeand willing to perform subcontracted actions if they can.Since we followed the mental state approach for planning and focused on subcontract-ing, we adapt a rather simple approach for reasoning about actions. There is a very richliterature on reasoning about actions. For example, Konolige [24] formalizes reasoningabout the knowledge, belief, and actions of agents. He uses an object language to describeagent beliefs and a meta-language to study the object language. Actions are described inthe situation calculus. But the intentions of agents are not considered. And although itis syntactic (like ours), Konolige’s approach explicitly assumes closure under inference.While the inference rules do not have to be traditional (strictly deductive) ones, neverthe-less all the inferential conclusions (theorems) resulting from them are assumed known orbelieved by the agent—they do not come in little by little over time as the agent managesto prove them one by one—and so this is clearly not realistic. Our approach by contrastemploys an explicit time mechanism to track the evolution of an agent’s gradual process ofinference.6. Summary and future workWe presented a formal logical calculus that can be regarded as a meta-logic that de-scribes the reasoning and activities of agents. The explicit representation of evolving time198J. Grant et al. / Artificial Intelligence 163 (2005) 163–201is an important feature of this approach. We dealt with the case where agents are as-signed tasks for which a recipe is known. Recipes have a tree structure. An agent maysubcontract some of the actions/subactions to other agents. Our emphasis is on develop-ing a framework that models the beliefs, intentions, and actions of agents as they changeover time. We present a syntactic approach, propose a minimal model semantics andprove that the meta-theory is consistent and has a minimal model. The true statementsin the minimal model associated with the agents’ mental states and actions characterizethe agents that are described by the meta-logic. Using this semantics, rather than possi-ble world semantics, allows us to model agents activity more realistically and to proveseveral results to show that under the appropriate conditions the agents will act as de-sired.We plan to extend this work in several ways. At present we have results only for stronglypositive (agents always successfully subcontract actions/subactions, their beliefs abouttheir activities are correct, and communication always succeeds) and strongly negative(there is a subaction that no agent can do) cases. We will consider more complex situa-tions. Additionally we will deal with situations where agents have SharedPlans (and notonly subcontract actions).Appendix AFor summary of predicates and axioms see Tables A.1–A.3.Table A.1A summary of predicates defined in Section 2.1. In all the definitions above t is the time of the propositionNameATD(t, n, m, b, a, t (cid:3))PotInt(t, m, n, b, a, t (cid:3))Int(t, m, n, b, a, t (cid:3))Ref (t, m, n, b, a, t (cid:3))BL(a, d)Rec(a, r)Mem(r, b, i, j, k)Bel(t, n, f )CanDo(t, n, a)Tell(t, n, m, f )Ini(t, m, n, a)Done(t, a)Stop(t, m, n, b, a)Prefer(t, n, a, b)Conf (t, n, a, b)....MeaningAgent n asks agent m to do action b in the context of action a at time t (cid:3)Agent m directly assisting agent n has the potential intention to do action b in thecontext of action a at time t (cid:3)Agent m directly assisting agent n has the intention to do action b in the contextof action a at time t (cid:3)Agent m refrains agent n from intending to do action b in the context of action aat time t (cid:3)a is basic level, takes d units of time to complete.r is the unique recipe for action a.In recipe r, b is the subaction which is the ith member of the recipe starting atrelative time j and ending at relative time k.Agent n believes statement f .Agent n can do action a.Agent n tells f to agent m.Agent m directly assisting agent n initiates action a.Action a has just been done successfully.Agent m directly assisting agent n is instructed to stop action b in the context ofaction a.Agent n prefers to do action a over action b.For agent n action a conflicts with action b.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201199Table A.2A summary of predicates defined in Section 4.1.1 for intention formation with multiple recipes. In all the defini-tions above t is the time of the propositionNameATD(t, a, t (cid:3))PotInt(t, b, a, r, t (cid:3))Int(t, b, a, r, t (cid:3))Ref (t, b, a, r, t (cid:3))NextRec(a, r, r(cid:3))FailedRec(t, a, r, t (cid:3))Parent(a, r, t, a(cid:3), r(cid:3), t (cid:3)):StopPlan(t, a, t (cid:3))Table A.3A list of axiomsAxiomA1A2A3A4B2B3B4B5C1C2C3C4C5D1D2D3IC1IC2IC3IC4A1-MRA2-MRA3-MRD1-MRD2-MRD3-MRD4-MR..MeaningThe agent is asked to do a at time t (cid:3)The agent has the potential intention to do subaction b of action a using recipe rat time t (cid:3)The agent has the intention to do subaction b of action a using recipe r at time t (cid:3).The agent is refrained from intending to do subaction b of action a using recipe rat time t (cid:3)In the list of recipes for a, r is followed immediately by r(cid:3)The recipe r for a to be done at time t (cid:3)The subaction a of action a(cid:3)using recipe r(cid:3)at time t (cid:3)The agent stops planning to do a at time t (cid:3)is to be done using recipe r, while a(cid:3)is to be donefailed.....TitleSectionAsked to do becomes potential intentionInheritance of potential intentionPotential intention becomes intentionInheritance of refrainSubcontracting an action to an agentPotential intention becomes intention (subcontracting version)Inheritance of refrain by an assisting agentCommunication of assisting agent about refrainInitiation of requested actionInheritance of initiate for first subactionInheritance of initiate for later subactionDone for actionsAction performance observedInitiate stop action in case of failureStop action propagated upStop triggers refrain downPreference among conflicting actionsConsistency of recipe subactionsConsistency of recipe timingUniqueness of recipe informationAsked to do becomes potential intentionInheritance of potential intentionPotential intention becomes intentionChosen recipe failsPotential intention inherited with different recipeRecipe failure induces refrainFailure of the last recipe for an action3.1.13.1.13.1.13.1.13.1.23.1.23.1.23.1.23.1.33.1.33.1.33.1.33.1.33.1.43.1.43.1.43.1.53.1.53.1.53.1.54.1.24.1.24.1.24.1.24.1.24.1.24.1.2200J. Grant et al. / Artificial Intelligence 163 (2005) 163–201References[1] J. Allen, Towards a general theory of action and time, Artificial Intelligence 23 (2) (1984) 123–144.[2] E. Alonso, A formal framework for the representation of negotiation protocols, 1997.[3] W. Bibel, Automated Theorem Proving, Friedr Vieweg and Sohn Verlagsgesellschaft GmbH, Braunschweig,1982.[4] W.W. Bledsoe, D.W. Loveland, Automated Theorem Proving, American Mathematical Society, Providence,RI, 1984.[5] M.E. Bratman, Intention, Plans, and Practical Reason, Harvard University Press, Cambridge, MA, 1987.[6] A. Bundy, The Computer Modeling of Mathematical Reasoning, Academic Press, London, 1983.[7] C. Castelfranchi, Commitments: from individual intentions to groups and organizations, in: Proc. ICMAS95, 1995.[8] P. Cohen, H. Levesque, Intention is choice with commitment, Artificial Intelligence 42 (1990) 263–310.[9] P. Cohen, H. Levesque, Teamwork, Noûs 25 (1991) 487–512.[10] P.R. Cohen, H. Levesque, Rational interaction as the basis for communication, in: P.R. Cohen, J.L. Morgan,M.E. Pollack (Eds.), Intentions in Communication, MIT Press, Cambridge, MA, 1990, pp. 221–256.[11] P.R. Cohen, J. Morgan, M.E. Pollack (Eds.), Intentions in Communication, MIT Press, Cambridge, MA,1990.[12] E. Davis, Representation of Commonsense Knowledge, Morgan Kaufmann, San Mateo, CA, 1990.[13] J. de Rivières, H. Levesque, The consistency of syntactical treatments of knowledge (how to compile quan-tificational modal logics into classical FOL, Computational Intelligence 4 (1988) 31–41.[14] J. Doyle, Y. Shoham, M. Wellman, A logic of relative desire, in: Proc. of the 6th International Symposiumon Methodologies for Intelligent Systems, 1991.[15] M. Georgeff, A. Rao, The semantics on intention maintenance for rational agents, in: Proc. of IJCAI-95,Montreal, Quebec, 1995, pp. 704–710.[16] G. De Giacomo, Y. Lesperance, H.J. Levesque, S. Sardina, On the semantics of deliberation in IndiGolog—from theory to implementation, in: Proc. KR-02, Toulouse, France, 2002, pp. 603–614.[17] R.P. Goldman, R.R. Lang, Intentions in time, Technical Report TUTR 93-101, Tulane University, 1993.[18] J. Grant, S. Kraus, D. Perlis, A logic for characterizing multiple bounded agents, Autonomous Agents andMulti-Agent Systems J. 3 (4) (2000) 351–387.[19] B.J. Grosz, S. Kraus, Collaborative plans for complex group activities, Artificial Intelligence J. 86 (2) (1996)269–357.[20] B.J. Grosz, S. Kraus, The evolution of sharedplans, in: A. Rao, M. Wooldridge (Eds.), Foundations andTheories of Rational Agency, Kluwer Academic, Dordrecht, 1999, pp. 227–262.[21] B. Grosz, C. Sidner, A reply to Hobbs, in: P. Cohen, J. Morgan, M. Pollack (Eds.), Intentions in Communi-cation, Bradford Books/MIT Press, Cambridge, MA, 1990, pp. 461–462.[22] A. Haass, The syntactic theory of belief and knowledge, Artificial Intelligence 28 (3) (1983) 245–293.[23] D. Kinny, M. Ljungberg, A.S. Rao, E. Sonenberg, G. Tidhar, E. Werner, Planned team activity, in: C. Castel-franchi, E. Werner (Eds.), Artificial Social Systems, Amsterdam, The Netherlands, in: Lecture Notes inArtificial Intelligence, vol. 830, Springer, Berlin, 1994.[24] K. Konolige, A first-order formalisation of knowledge and action for a multi-agent planning system, MachineIntelligence 10 (1982) 41–72.[25] K. Konolige, M.E. Pollack, A representationalist theory of intention, in: Proc. of IJCAI-93, Chambéry,France, 1993, pp. 390–395.[26] S. Kraus, An overview of incentive contracting, Artificial Intelligence J. 83 (2) (1996) 297–346.[27] S. Kraus, Strategic Negotiation in Multiagent Environments, MIT Press, Cambridge, MA, 2001.[28] S. Kraus, K. Sycara, A. Evenchik, Reaching agreements through argumentation: a logical model and imple-mentation, Artificial Intelligence 104 (1–2) (1998) 1–69.[29] S. Kumar, M.J. Huber, D.R. McGee, P.R. Cohen, H.J. Levesque, Semantics of agent communication lan-guages for group interaction, in: Proc. AAAI-2000, Austin, TX, 2000, pp. 42–47.[30] Y. Lesperance, On the epistemic feasibility of plans in multiagent systems specifications, in: Proc. ATAL-01,2001.[31] H. Levesque, All I know: a study in autoepistemic logic, Artificial Intelligence 42 (1990) 263–309.J. Grant et al. / Artificial Intelligence 163 (2005) 163–201201[32] H. Levesque, P. Cohen, J. Nunes, On acting together, in: Proceedings of AAAI-90, Boston, MA, 1990,pp. 94–99.[33] K. Lochbaum, B. Grosz, C. Sidner, Models of plans to support communication: an initial report, in: Pro-ceedings of AAAI-90, Boston, MA, MIT Press, Cambridge, MA, 1990, pp. 485–490.[34] R. Montague, Syntactical treatments of modality, with corollaries on reflection principles and finite axioma-tizability, in: Modal and Many-Valued Logics, in: Acta Philosophica Fennica, vol. 16, Academic Bookstore,Helsinki, 1963. Reprinted in: R. Montague, Formal Philosophy, New Haven, 1974, pp. 286–302.[35] R. Moore, A formal theory of knowledge and action, in: J. Hobbs, R. Moore (Eds.), Formal Theories of theCommonsense World, Ablex, Norwood, NJ, 1985.[36] L. Morgenstern, Foundations of a logic of knowledge, action, and communication, PhD Thesis, New YorkUniversity, 1988.[37] L. Morgenstern, Inheritance comes of age: applying nonmonotonic techniques to problems in industry, in:Proc. IJCAI-97, Nagoya, Japan, 1997, pp. 1613–1621.[38] M. Morreau, S. Kraus, Syntactical treatments of propositional attitudes, Artificial Intelligence 106 (1998)161–177.[39] K.L. Myers, User guide for the procedural reasoning system, Technical Report, Artificial Intelligence Center,SRI International, 1997.[40] D. Perlis, Language with self references I: foundation, Artificial Intelligence 25 (1985) 301–322.[41] R. Perrault, An application of default logic to speech act theory, in: P.R. Cohen, J.L. Morgan, M.E. Pollack(Eds.), Intentions in Communication, Bradford Books/MIT Press, 1990, pp. 161–185.[42] M.E. Pollack, Plans as complex mental attitudes, in: P.N. Cohen, J.L. Morgan, M.E. Pollack (Eds.), Inten-tions in Communication, Bradford Books/MIT Press, Cambridge, MA, 1990.[43] A. Rao, M. Georgeff, Deliberation and its role in the formation of intention, in: Proceedings of the SeventhConference on Uncertainty in Artificial Intelligence, Morgan Kaufmann, San Mateo, CA, 1991.[44] A. Rao, M. Georgeff, Modeling rational agents within BDI architecture, in: Proc. of the Second InternationalConference of Knowledge Representation, Morgan Kaufmann, San Mateo, CA, 1991, pp. 473–484.[45] A. Rao, M.P. Georgeff, E. A Sonenberg, Social plans: a preliminary report, in: Decentralized ArtificialIntelligence, vol. 3, Elsevier Science, Amsterdam, 1992, pp. 57–76.[46] M.D. Sadek, A study in the logic of intention, in: Proc. KR-92, Cambridge, MA, 1992, pp. 462–473.[47] E. Sandewall, Features and Fluents, Oxford University Press, Oxford, 1994.[48] M. Schut, M. Wooldridge, S. Parsons, Reasoning about intentions in uncertain domains, in: Proceedings ofthe Sixth European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty(ECSQARU-2001), Toulouse, France, 2001.[49] M. Shanahan, Solving the Frame Problem, MIT Press, Cambridge, MA, 1997.[50] M.P. Singh, The intentions of teams: team structure, endodeixis, and exodeixis, in: Proceedings of the 13thEuropean Conference on Artificial Intelligence (ECAI), Wiley, New York, 1998, pp. 303–307.[51] M.P. Singh, N.M. Asher, A logic of intentions and beliefs, J. Philos. Logic 22 (1993) 513–544.[52] E. Sonenberg, G. Tidhar, E. Werner, D. Kinny, M. Ljungberg, A. Rao, Planned team activity, TechnicalReport 26, Australian Artificial Intelligence Institute, Australia, 1992.[53] R. Thomason, A note on syntactical treatments of modality, Synthese 44 (1980) 391–395.[54] R. Verbrugge, B. Dunin-Keplicz, Collective intentions, Fundamenta Informatica 49 (2002) 271–295.[55] M. Wooldridge, Reasoning about Rational Agents, MIT Press, Cambridge, MA, 2000.[56] M. Wooldridge, N.R. Jennings, The cooperative problem-solving process, J. Logic Comput. 9 (4) (1999)563–592.[57] M. Wooldridge, S. Parsons, Intention reconsideration reconsidered, in: J.P. Muller, M. Singh, A. Rao (Eds.),Intelligent Agents V, in: Lecture Notes in Artificial Intelligence, vol. 1365, Springer, Berlin, 1999.[58] A. Zaniolo, S. Ceri, C. Faloutsos, R.T. Snodgrass, V.S. Subrahmanian, R. Zicari, Advanced Database Sys-tems, Morgan Kaufmann, San Mateo, CA, 1997.