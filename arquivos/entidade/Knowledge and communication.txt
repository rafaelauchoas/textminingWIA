Artificial Intelligence 166 (2005) 81–139www.elsevier.com/locate/artintKnowledge and communication:A first-order theory ✩Ernest DavisCourant Institute, New York University, New York, NY 10012, USAReceived 1 July 2004; accepted 2 May 2005Available online 22 June 2005AbstractThis paper presents a theory of informative communications among agents that allows a speaker tocommunicate to a hearer truths about the state of the world; the occurrence of events, including othercommunicative acts; and the knowledge states of any agent—speaker, hearer, or third parties—any ofthese in the past, present, or future—and any logical combination of these, including formulas withquantifiers. We prove that this theory is consistent, and compatible with a wide range of physical the-ories. We examine how the theory avoids two potential paradoxes, and discuss how these paradoxesmay pose a danger when this theory are extended. 2005 Elsevier B.V. All rights reserved.Keywords: Communication; Knowledge; Logic; Paradox1. IntroductionIn constructing a formal theory of communications between agents, the issue of expres-sivity enters at two different levels: the scope of what can be said about the communica-tions, and the scope of what can be said in the communications. Other things being equal,it is obviously desirable to make both of these as extensive as possible. Ideally, a theoryshould allow a speaker to communicate to a hearer truths about the state of the world;the occurrence of events, including other communicative acts; the knowledge states of any✩ The research reported in this paper was supported in part by NSF grant IIS-0097537.E-mail address: davise@cs.nyu.edu (E. Davis).0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.05.00282E. Davis / Artificial Intelligence 166 (2005) 81–139agent—speaker, hearer, or third parties; any of these in the past, present, or future; and anylogical combination of these. This paper presents a theory that achieves pretty much that.A few examples of what can be expressed in our representation:(1) Alice tells Bob that all her children are asleep.(2) Alice tells Bob that she does not know whether he locked the door.(3) Alice tells Bob that if he finds out who was in the kitchen at midnight, then he willknow who killed Colonel Mustard.(4) Alice tells Bob that no one had ever told her she had a sister.(5) Alice tells Bob that he has never told her anything she did not already know.The above examples illustrate many of the expressive features of our representation:• Example 1 shows that the content of a communication may be a quantified formula.• Example 2 shows that the content of a communication may refer to knowledge andignorance of past actions.• Example 3 shows that the content of a communication may be a complex formulainvolving both past and future events and states of knowledge.• Examples 4 and 5 show that the content of a communication may refer to other com-munications. They also show that the language supports quantification over agents andover the content of a communication, and thus allows the content to be partially char-acterized, rather than fully specified.Moreover, our theory supports basic inference from these kinds of representations. Forexample, given that Alice tells Bob that no one has ever told her that she has a sister, andthat Alice knows that, if she did have a sister, someone would have told her, it is possibleto infer that Alice knows that she does not have a sister. The proof from our theory of thisand similar sample inferences and the representation of the five statements above is givenin Section 4.Following the research programme of [8,21,24,25], the primary purpose of this paper isto develop a representation for expressing commonsense knowledge about knowledge andcommunication, with the ultimate intention that this representation or something similar,could be used to carry out symbolic reasoning in this domain. A secondary purpose isto develop an object-level theory, expressible in the language, that will justify as broad arange as possible of commonsensically obvious inference in the domain, while entailing asfew as possible commonsensically absurd consequences. The success of the language andtheory is demonstrated in terms of their ability to capture a large number and a broad rangeof examples of commonsensically obvious inferences. We are not here concerned withspecialized applications, such as distributed systems; with subtle philosophical nuance; orwith efficiency of inference in an implemented reasoning engine. Potentially, this theorycould find practical application as a logical foundation either for planning communicationsin a multi-agent system, or for a theory of speech acts to be used in interpreting dialogueor engaging in dialogue with human speakers.Since our theory allows communications that refer to other communications, and evencommunications that refer to themselves, there is clearly a danger of running into para-E. Davis / Artificial Intelligence 166 (2005) 81–13983doxes of vicious self-reference. It is therefore particularly important to establish that thetheory is consistent. We prove a meta-theorem that the theory is indeed consistent; in fact,that it is consistent with a wide range of domain-specific physical theories and axiomsof knowledge acquisition. We discuss two particular apparent paradoxes—an analogue ofRussell’s paradox, and the “unexpected hanging” paradox—and we show how our theorymanages to side-step these.We should note at the outset the limitations of our theory. The theory deals only withinformative acts (and not, for example, with requests) and assumes that the following con-ditions are true and universally known: If AS communicates Q to AH, then(1) AS knows that Q is true at the time that he initiates the communication.(2) From the time that he initiates the communication, AS knows that he is carrying out acommunication; he knows that the content is Q; and he knows that the recipient is AH.(3) Similarly, when the communication is complete, AH knows that he has received a com-munication; he knows that the content was Q; and he knows that the sender was AS.(4) When the communication is complete, AS knows that the communication is completeand AH knows the time at which the communication was initiated.The paradigmatic example of a form of communication satisfying conditions (2), (3),and (4) is direct speech.1 Another example could be mail, assuming that• All messages are time-stamped with the time of sending, and signed by the sender.• There is a universally known maximal delay D between the time of sending and thetime of receiving a message. (“Receiving” here means the time when the hearer readsthe message, not the time that it arrives in his mailbox.)In this case, if we define a communication to be “complete” at the time of sending plus D,then the above conditions are met.Many aspects of the theory can be applied to communications that do not meet condi-tion (4), but I have not been able to find a plausible axiomatization of this more generalcase that I can prove to be consistent. Also, I cannot prove that the theory is consistentunless time is taken to be discrete. These are discussed further in Section 8.The paper proceeds as follows: Section 2 reviews the theories of time and of knowledge,which are not new here. Section 3 presents our language and axioms of communica-tion. Section 4 is the core of this paper; it illustrates the power of the theory by showinghow it supports the representation of the five sample statements above and three examplecommonsense inferences. Sections 5 and 6 describe two apparent paradoxes—a paradoxanalogous to Russell’s paradox and the “unexpected hanging” paradox—and explain whythese do not cause inconsistencies in the theory. Section 7 gives the statement of Theo-rems 1 and 2, which assert that the theory is internally consistent and compatible with awide range of physical theories. Sections 8 and 9 discuss related work. Section 10 dis-1 Under assumptions that are reasonable, though not universally valid: e.g. that the speaker knows what hewill say when he begins speaking, and that the speaker and hearer have common knowledge that the hearer willcorrectly understand the message.84E. Davis / Artificial Intelligence 166 (2005) 81–139cusses open problems and summarizes our conclusions. Appendix A gives the proofs ofTheorems 1 and 2.2. FrameworkWe use a situation-based, branching theory of time; an interval-based theory of multi-agent actions; and a possible-worlds theory of knowledge. This is all well known, so thedescription below is brief.2.1. Time and actionWe use a situation-based theory of time. Time can be either continuous2 or discrete, butit must be branching, like the situation calculus. The branching structure is described bythe partial ordering “S1 < S2”, meaning that there is a timeline containing S1 and S2 andS1 precedes S2. It is convenient to use the abbreviations “S1 (cid:1) S2” and “ordered(S1, S2).”The predicate “holds(S, Q)” means that fluent Q holds in situation S.Each agent has, in various situations, a choice about what action to perform next, andthe time structure includes a separate branch for each such choice. Thus, the statement thataction E is feasible in situation S is expressed by asserting that E occurs from S to S1 forsome S1 > S.Following McDermott [26], actions are represented as occurring over an interval; thepredicate occurs(E, S1, S2) states that action E occurs starting in S1 and ending in S2.However, the whole theory could be recast without substantial change into the situationcalculus extended to permit multiple agents, after the style of Reiter [36]. The advantageof using the “occurs” representation is the much greater ease of extensibility. The situationcalculus was developed for domains where a single agent executes a single atomic actionin each situation to bring about the next situation; and extending the situation calculus toallow multiple agents, exogenous change, real-valued time, concurrent actions, extendedactions, and alternative characterizations of actions involves a series of representationalextensions that are somewhat awkward and hard to integrate [36]. By contrast, all of thesecan be subsumed in the “occurs” representation, though finding a correct axiomatizationof a theory with these features can still be difficult.Table 1 shows the axioms of our temporal theory. Throughout this paper, we use asorted first-order logic with equality, where the sorts of variables are indicated by theirfirst letter. The sorts are clock-times (T ), situations (S), Boolean fluents (Q), actions (E),agents (A), and actionals (Z). An actional is a characterization of an action without spec-ifying the agent. For example, the term “puton(blocka, table)” denotes the actional ofsomeone putting block A on the table. The term “do(john, puton(blocka, table))” denotesthe action of John putting block A on the table. Free variables in a formula are assumed tobe universally quantified.2 As will be discussed below, I have not proven the theory consistent for continuous theories of time. However,nothing in the form of the representation inherently excludes a continuous model of time; and I conjecture thatthe theory is, actually, consistent with a continuous model of time.E. Davis / Artificial Intelligence 166 (2005) 81–13985Table 1Temporal axiomsPrimitivesT 1 < T 2—Time T 1 is earlier than T 2.S1 < S2—Situation S1 precedes S2, on the same time line. (We overload the <symbol.)time(S)—Function from a situation to its clock time.holds(S, Q)—Fluent Q holds in situation S.occurs(E, S1, S2)—Action E occurs from situation S1 to situation S2.do(A, Z)—Function. The action of agent A doing actional Z.DefinitionsTD.1 S1 (cid:1) S2 ≡ S1 < S2 ∨ S1 = S2.TD.2 ordered(S1, S2) ≡S1 < S2 ∨ S1 = S2 ∨ S2 < S1.TD.3 feasible(E, S) ⇔ ∃S2 occurs(E, S, S2).AxiomsT.1 T 1 < T 2 ∨ T 2 < T 1 ∨ T 1 = T 2.T.2 ¬[T 1 < T 2 ∧ T 2 < T 1].T.3 T 1 < T 2 ∧ T 2 < T 3 ⇒ T 1 < T 3.(Clock times are linearly ordered.)T.4 S1 < S2 ∧ S2 < S3 ⇒ S1 < S3. (Transitivity)T.5 (S1 < S ∧ S2 < S) ⇒ ordered(S1, S2).(Forward branching)T.6 S1 < S2 ⇒ time(S1) < time(S2).(The ordering on situations is consistent with the orderings of their clock times.)T.7 ∀S,T 1∃S1 ordered(S, S1) ∧ time(S1) = T 1.(Every time line contains a situation for every clock time.)T.8 occurs(E, S1, S2) ⇒ S1 < S2.(Events occur forward in time.)T.9 [occurs(E, S1, S2) ∧ S1 < SX < S2 ∧ SX < SY ] ⇒∃SZ SX < SZ ∧ ordered(SY, SZ) ∧ occurs(E, S1, SZ).(If action E starts to occur on the time line that includes SY , then it completes on that time line (Fig. 1).)Note that in our model of time, each feasible action and its consequences are representedby a branch in the time structure. Thus the time structure incorporates everything that canpossibly happen. We do not single out one particular time line or branch as the historythat will actually happen. This will be important in our discussion of the paradox of theunexpected hanging.2.2. KnowledgeAs first proposed by Moore [28,29] and widely used since, knowledge is representedby identifying temporal situations with epistemic possible worlds and positing a relationof knowledge accessibility between situations. The relation k_acc(A, S, SA) means thatsituation SA is accessible from S relative to agent A’s knowledge in S; that is, as far asA knows in S, the actual situation could be SA. The statement that A knows φ in S isrepresented by asserting that φ holds in every situation that is knowledge accessible from Sfor A. As is well known, this theory enables the expression of complex interactions of86E. Davis / Artificial Intelligence 166 (2005) 81–139Fig. 1. Axiom T.9. If the time structure has the form on the left, then it has one of the forms on the right.knowledge and time; one can represent both knowledge about change over time and changeof knowledge over time.Again following Moore [29], the state of agent A knowing what something is is ex-pressed by using a quantifier of larger scope than the universal quantification over acces-sible possible worlds. For example, the statement, “In situation s1, John knows who thePresident is” is expressed by asserting that there exists a unique individual who is the Pres-ident in all possible worlds accessible for John from s1.∃X ∀S1A k_acc(john, s1, S1A) ⇒ holds(S1A, president(X))For convenience, we posit an S5 logic of knowledge; that is, the knowledge accessibilityrelation, restricted to a single agent, is in fact an equivalence relation on situations. This isexpressed in axioms K.1, K.2, and K.3 in Table 2. Three important further axioms governthe relation of time and knowledge.K.4. Axiom of memory: if A knows φ in S, then in any later situation, he remembers thathe knew φ in S.K.5. A knows all the actions that he has begun, both those that he has completed and thosethat are ongoing. That is, he knows a standard identifier for these actions; if Bob isdialing (212) 998-3123 on the phone, he knows that he is dialing (212) 998-3123 butE. Davis / Artificial Intelligence 166 (2005) 81–13987Fig. 2. Axiom K.6.he may not know that he is calling Ernie Davis. At any time, A knows what actionsare feasible for him now.K.6. Knowledge accessibility relations do not cross in the time structure. (Fig. 2.) In adiscrete theory of time, axiom K.6 is a consequence of the axiom of memory K.4.(Knowledge accessibility relations that violate this condition have sometimes beenused in the literature for agents who do not satisfy the axiom of memory.)The theory includes a form of common knowledge, restricted to two agents. Agents A1and A2 have shared knowledge of φ if they both know φ, they both know that theyboth know φ and so on.3 We represent this by defining a further accessibility relation,“sk_acc(A1, A2, S, SA)” (SA is accessible from S relative to the shared knowledge of A1and A2). This is defined as the transitive closure of links of the form k_acc(A1, ·,·) togetherwith links of the form k_acc(A2, ·,·). (Of course, transitive closure cannot be exactly de-fined in a first-order theory; axioms K.7 and K.8 define an approximation that is adequatefor our purposes.)3. CommunicationWe now introduce the function “inform”, taking two arguments, an agent AH and afluent Q. The term “inform(AH, Q)” denotes the actional of informing AH that Q; theterm “do(AS, inform(AH, Q))” thus denotes the action of speaker AS informing AH thatQ. Our theory here treats “do(AS, inform(AH, Q))” as a primitive action; in a richertheory, it would be viewed as an illocutionary description of an underlying locutionaryact (not here represented)—the utterance or writing or broadcasting of a physical sig-nal.We also add a second actional “communicate(AH)”. This alternative characterization ofa communicative act, which specifies the hearer but not the content of the communication,enables us to separate out physical constraints on a communicative act from contentiveconstraints. Thus, we allow a purely physical theory to put constraints on the occurrence3 In [10], we need to use common knowledge by a general set of agents. The modifications to the representationand the axioms needed to support this are entirely straightforward.88E. Davis / Artificial Intelligence 166 (2005) 81–139Table 2Axioms of knowledgePrimitivesk_acc(A, SA, SB)—SB is accessible from SA relative to A’s knowledge in SA.sk_acc(A1, A2, SA, SB)—SB is accessible from SA relative to the sharedknowledge of A1 and A2 in SA.AxiomsK.1 ∀A,SA k_acc(A, SA, SA).K.2 k_acc(A, SA, SB) ⇒ k_ acc(A, SB, SA)K.3 k_acc(A, SA, SB) ∧ k_ acc(A, SB, SC) ⇒ k_acc(A, SA, SC).(K.1 through K.3 suffice to ensure that the knowledge of each agent obeys an S5 logic: what he knows istrue, if he knows φ he knows that he knows it; if he does not know φ, he knows that he does not know it.)K.4 [k_acc(A, S2A, S2B) ∧ S1A < S2A] ⇒∃S1B S1B < S2B ∧ k_acc(A, S1A, S1B).(Axiom of memory: If agent A knows φ at any time, then at any later time he knows that φ was true.)K.5 [occurs(do(A, Z), S1A, S2A) ∧ S1A (cid:1) SA ∧ordered(SA, S2A) ∧ k_acc(A, SA, SB)] ⇒∃S1B,S2B occurs(do(A, Z), S1B, S2B) ∧S1B (cid:1) SB ∧[S2A < SA ⇒ S2B < SB] ∧[S2A = SA ⇒ S2B = SB] ∧[SA < S2A ⇒ SB < S2B] ∧[S1A = SA ⇒ S1B = SB].(An agent knows which actions he has completed, which actions he has begun, and which actions are nowfeasible.)K.6 ¬∃A,S1A,S1B,S2A,S2BS1A < S2A ∧ S1B < S2B ∧ k_acc(A, S1A, S2B) ∧ k_acc(A, S2A, S1B).(Knowledge accessibility links do not cross in the time structure (Fig. 2).)K.7 sk_acc(A1, A2, SA, SB) ⇔[k_acc(A1, SA, SB) ∨ k_acc(A2, SA, SB) ∨sk_acc(A1, A2, SB, SA) ∨sk_acc(A2, A1, SA, AB) ∨∃SC sk_acc(A1, A2, SA, SC) ∧ sk_acc(A1, A2, SC, SB)].(Definition of sk_acc as a equivalence relation, symmetric in A1, A2, that includes the k_acc links for thetwo agents A1, A2.)K.8 (Induction from k_acc links to sk_acc links.) Let Φ(S) be a formula with a free situational variable S. Thenthe closure of the following formula is an axiom:[∀AS,AH[[∀SA,SBΦ(SA) ∧ k_acc(AS, SA, SB) ⇒ Φ(SB)] ∧[∀SA,SBΦ(SA) ∧ k_ acc(AH, SA, SB) ⇒ Φ(SB)]] ⇒[∀SA,SBΦ(SA) ∧ sk_ acc(AS, AH, SA, SB) ⇒ Φ(SB)].of a communication, or even to posit physical effects of a communication, but these mustbe independent of the information content of the communication.We posit five axioms of communication, summarized in Table 3. Some of these arestraightforward; others much less so. We discuss them below in increasing order of com-plexity. We also put forward a sixth axiom, a frame axiom for ignorance, but its status ismore dubious, for reasons that we will discuss.E. Davis / Artificial Intelligence 166 (2005) 81–13989Table 3Axioms of communicationI.1 Any inform act is a communication.occurs(do(AS,inform(AH, Q)), S1, S2) ⇒occurs(do(AS,communicate(AH)), S1, S2).I.2 If a speaker AS can communicate with a hearer AH, then AS can inform AH of some specific Q if and onlyif AS knows that Q holds at the time he begins speaking.feasible(do(AS, communicate(AH)), S1) ⇒[∀Q feasible(do(AS, inform(AH, Q)), S1) ⇔[∀S1A k_acc(AS, S1, S1A) ⇒ holds(S1A, Q)]]I.3 If AS informs AH of Q from S1 to S2, then in S2, AH knows that AS has informed him of Q.∀S1,S2,S2A [occurs(do(AS, inform(AH, Q)), S1, S2) ∧ k_acc(AH, S2, S2A)] ⇒∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A) ∧ k_acc(AH, S1, S1A)I.4 If AS informs AH of Q1 over [S1, S2] and the shared knowledge of AS and AH in S1 implies thatholds(S1, Q1) ⇔ holds(S1, Q2), then AS has also informed AH of Q2 over [S1, S2]. Conversely, the twoactions “do(AS,inform(AH, Q1))” and “do(AS,inform(AH, Q2))” co-occur only if Q1 and Q2 are related inthis way.occurs(do(AS, inform(AH, Q1)), S1, S2) ⇒[occurs(do(AS, inform(AH, Q2)), S1, S2) ⇔[∀S1A ¯sk_acc(AS, AH, S1, S1A) ⇒[holds(S1A, Q1) ⇔ holds(S1A, Q2)]]]I.5 Axiom of comprehension: any property of situations that can be stated in the language is a fluent.Let α(S) be a first-order formula that contains exactly one free variable S of sort “situation” and that doesnot contain Q as a free variable. (α may have other free variables of other sorts.) Then the closure of thefollowing formula is an axiom:∃Q ∀S holds(S, Q) ⇔ α(S).(The closure of a formula β is β scoped by universal quantifications of all its free variables.)I.6 Frame axiom for ignorance. See the discussion in Section 3.5 below.3.1. Relation between informing and communicationAxiom I.1. Any inform act is a communication.occurs(do(AS, inform(AH, Q)), S1, S2) ⇒occurs(do(AS, communicate(AH)), S1, S2)Axiom I.2. If a speaker AS can communicate with a hearer AH, then AS can inform AH ofsome specific Q if and only if A knows that Q holds at the time he begins speaking.[feasible(do(AS, communicate(AH)), S1)] ⇒[∀Q feasible(do(AS, inform(AH, Q)), S1) ⇔[∀S1A k_acc(AS, S1, S1A) ⇒ holds(S1A, Q)]]By virtue of these two axioms, the preconditions for AS informing AH that Q are just thatit is feasible for AS to communicate to AH and that AS knows that Q is true. The content Q90E. Davis / Artificial Intelligence 166 (2005) 81–139may not affect the feasibility in any other way. Axiom I.1 further guarantees that any otherphysical constraints over communications, such as the duration of a communication or itsphysical effects, must apply also to inform acts; that is, that the physical characteristics ofany inform act must be consistent with the physical constraints on communications. Theseaxioms do not rule out the possibility that the content could affect other physical aspects ofthe inform act—for example, that a complex content takes longer to communicate than asimple content—but I have not shown that any such constraints lead to a consistent theory.Note that axiom I.2 requires, conversely, that any fluent Q that is known to be truecan be communicated; that is, there is a branch in the time structure corresponding to thecommunication of Q.3.2. Epistemic effect of communicationSince we require the strong conditions mentioned in Section 1, we can posit the follow-ing axiom:4Axioms I.3. If AS informs AH of Q from S1 to S2, then in S2, AH knows that AS hasinformed him of Q.∀S1,S2,S2A [occurs(do(AS, inform(AH, Q)), S1, S2) ∧ k_acc(AH, S2, S2A)] ⇒∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A) ∧ k_acc(AH, S1, S1A)Lemmas 3.1 and 3.2 are important consequences of I.3 together with the precedingaxioms:Lemma 3.1. If AS informs AH of Q, then, when the communication is complete, AS andAH have shared knowledge that the communication has taken place.occurs(do(AS, inform(AH, Q)), S1, S2) ∧ sk_acc(AS, AH, S2, S2A) ⇒∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A)Proof. By K.5, AS knows when he has completed a communication.occurs(do(AS, inform(AH, Q)), S1, S2) ∧ k_acc(AS, S2, S2A) ⇒∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A)By I.3, AH knows when he has received a communication.occurs(do(AS, inform(AH, Q)), S1, S2) ∧ k_acc(AH, S2, S2A) ⇒∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A)Choosing Φ(S) to be the formula “occurs(do(AS, inform(AH, Q))”, the formula inLemma 3.1 then follows from axiom K.8. (cid:1)4 The statement of this axiom in the KR-2004 paper [9] was not correct.E. Davis / Artificial Intelligence 166 (2005) 81–13991Lemma 3.2. If AS informs AH of Q then, when the communication is complete, then ASand AH have shared knowledge that Q was true when the communication began.occurs(do(AS, inform(AH, Q)), S1, S2) ∧ sk_acc(AS, AH, S2, S2A) ⇒∃S1A occurs(do(AS, inform(AH, Q)), S1A, S2A) ∧ holds(S1A, Q)Proof. Let as, ah, q, s0, s1, s2a satisfy the left side of the above implication.By Lemma 3.1 there exists s1a such that occurs(do(as, inform(ah, q)), s1a, s2a).By K.1, k_acc(as, s1a, s1a).By I.2, holds(s1a, q). (cid:1)3.3. Axiom of comprehensionThe axiom of comprehension states that there is a fluent corresponding to any propertyof situations definable in the language. The content of this axiom therefore depends on theoverall language L. We state this as an axiom schema, i.e., an infinite set of axioms.Axiom I.5. The comprehension axiom for fluents in a language L is this: let α(S) be a first-order formula that contains exactly one free variable S of sort “situation” and that does notcontain Q as a free variable. (α may have other free variables of other sorts.) Then theclosure of the following formula is an axiom:∃Q ∀0S holds(S, Q) ⇔ α(S)(The closure of a formula β is β scoped by universal quantifications of all its free vari-ables.)What this axiom states is that, given any property α(S), there exists a fluent Q thatholds on just those situation satisfying α. Moreover, the property α may be parameterizedby free variables. The form of this axiom is modelled on the formulation of the “separation”or “subset” axiom of ZF set theory as given in, for example, [15], which states that, givenany set B and property α, there exists a subset C ⊂ B of all the elements in B satisfying α.In axiom I.5, the set of all situations corresponds to B and the fluent Q corresponds to thesubset C. Further discussion is given in Appendix A, particularly in Lemma A.21.Let us first discuss the significance of free variables in the formula α. The reason toallow free variables that are not situations is to deal with examples such as the following:We want to be able to posit that a speaker can say, for example, that some specific blockis either red or blue without requiring that the language L have a constant symbol for eachblock, or even a formula that uniquely identifies each block.55 You might well ask, “If you cannot refer to the block in L, how is the speaker talking about it?” Perhaps heis pointing. Perhaps he is using a richer language with more constant symbols. The language L does not haveto be the language that the speaker is actually using; it is a language in which we, externally, describe what thespeaker is saying. It is not a very important point, but it does make the theory more elegant and easier to use ifone assumes that a speaker can refer de re to any entity other than a situation.92E. Davis / Artificial Intelligence 166 (2005) 81–139This axiom achieves this. We choose α(S) to be the formula “holds(S, red(X)) ∨holds(S, blue(X))”. The axiom schema then state∀X ∃Q ∀S holds(S, Q) ⇔ holds(S, red(X)) ∨ holds(S, blue(X))That is, for every object X there is a fluent Q that corresponds to the situations in which Xis either red or blue.The reason to exclude formulas that have other situational free variables in addition to Sis that it does not seem to mean anything to have this kind of de re reference to situations.A situation is meaningful only in relation to the current situation; there is no other way tomeaningfully refer to a situation. It may be noted that the consistency proof for the theory(Theorem 1 below) does not depend on this restriction.The reason for the condition that Q not appear free in the formula is that, otherwise, wecould choose α(S) to be the formula ¬holds(S, Q), in which case the axiom would give us∃Q ∀S holds(S, Q) ⇔ ¬holds(S, Q), which is obviously not satisfiable.6 Note, however,that if a different variable name is chosen, there is no problem with having a free variableof sort “fluent”. For example, if we choose α(S) to be the formula ¬holds(S, Q1), thenthe schema yields the axiom ∀Q1 ∃Q ∀S holds(S, Q) ⇔ ¬holds(S, Q1) which is entirelyreasonable.The content of the comprehension axiom depends on the overall language L. In general,one supposes that the language L will contain many domain and problem specific symbolsbeyond those that are used in the axioms enumerated here. Theorem 1 shows that theseaxioms are consistent when L is a physical language augmented with the symbols from thetheory of knowledge and communication described here. In [10] we consider a languagethat includes also agent commitments and requests. In that setting, the above formulationof the axiom turns out to be too strong; we have to limit the comprehension axiom to applyonly to formulas that do not include symbols describing commitment and requests.In view of this comprehension axiom, axiom K.8 could be restated as a single axiom(rather than an axiom schema) as follows:K.8.A. ∀Q,AS,AH [[∀S,SA holds(S, Q) ∧ k_acc(AS, S, SA) ⇒ holds(SA, Q)] ∧[∀S,SA holds(S, Q) ∧ k_acc(AH, S, SA) ⇒ holds(SA, Q)]] ⇒[∀S,SA holds(S, Q) ∧ sk_acc(AS, AH, S, SA) ⇒ holds(SA, Q)].However we did not use this formulation originally because we did not want K.8 to bedependent on I.5.3.4. Independent actionsIn a temporal representation, like ours, that permits the concurrent execution of actions,it does not suffice just to describe what actions can be executed; one must also, to greater orlesser extent, describe what combinations of actions can be executed concurrently. At theminimum, if two actions are independent, it should be possible to execute the one without6 I am extremely grateful to the anonymous reviewer who pointed this out.E. Davis / Artificial Intelligence 166 (2005) 81–13993the other. In the case of “inform” acts, the natural axiom would be that, if AS knows φ,then he can choose to carry out the single act of informing AH of φ and not doing anythingelse. One might suppose that this could be expressed in the following two axioms:WRONG.1 feasible(do(AS, inform(AH, Q)), S1) ⇒∃S2 occurs(do(AS, Z), S1, S2) ⇔ Z = do(AS, inform(AH, Q)).WRONG.2 do(AS1, inform(AH1, Q1)) = do(AS2, inform(AH2, Q2)) ⇒AS1 = AS2 ∧ AH1 = AH2 ∧ Q1 = Q2.However, as my labels subtly suggest,7 this is not an acceptable formulation. In fact, aswe shall show in Section 5, these are inconsistent with the axiom of comprehension I.5.The problem, intuitively, is this: The comprehension axiom asserts that there exists afluent for every set of situations; axiom WRONG.1 asserts that there exists a separatebranch in time for every fluent. Therefore, if you try to construct a model of these axiomscombined, you first have to construct all sets of situations; then add branches for each ofthese, which gives a whole bunch more resultant situations; these in turn generate vastnumbers of new sets of situations . . . . There is no way to make this construction converge.(I’m being a little loose here, but one can make this tight. The decisive proof that thiscannot be made to work is the “misled” paradox of Section 5.)Therefore, we have to weaken axiom WRONG.1.8 The approach we take is as follows:In general, it is only necessary to distinguish an occurrence of action A1 from an occur-rence of action A2 if they have different causal consequences. For instance, in the blocksworld, if all you are interested in is the sequence of towers that are formed, then all thatmatters in discriminating actions is the ending position of the block being moved; the tra-jectory through which it moves is immaterial.Now, in the case of informative acts, the causal consequence of concern is the effect onknowledge states. Assuming axiom I.3, the main effect of AS informing AH of Q is that,when the communication is complete, AS and AH have shared knowledge that Q held atthe beginning of the communication. Therefore, if Q1 and Q2 are two informative contentssuch that the effects on the shared knowledge of AS and AH following a communicationof Q1 from AS to AH are the same as those effects following a communication of Q2,then we can treat the communication of Q1 and the communication of Q2 as the sameaction; they, so to speak, attain the same end state via different trajectories. And a suffi-cient condition to ensure this is that AS and AH have shared knowledge at the start of thecommunication that Q1 and Q2 are equivalent.For example, if Jack and Jane share the knowledge that George Bush is the Presidentand that 1600 Pennsylvania Avenue is the White House, then the action of Jack informing7 One thing I have learned in twenty years of teaching is that, if you write down a wrong formula on theblackboard for purposes of discussion, you have to label it WRONG in large letters. Otherwise, students copy itinto their notebooks . . . . Similarly, if someone is skimming through this paper looking for formal axioms, I donot want him to use these.8 Weakening axiom WRONG.2 does not work. In fact, WRONG.2 ends up being true in the model we willconstruct, but its truth won’t actually matter much once we have correctly reformulated WRONG.1.94E. Davis / Artificial Intelligence 166 (2005) 81–139Jane that Bush is at the White House is identical to the act of Jack informing Jane that thePresident is at 1600 Pennsylvania Avenue. If they do not share this knowledge, then thesetwo acts are different.This, then, is our axiom: The event of AS informing AH of Q1 and the event of ASinforming AH of Q2 co-occur over an interval [S1, S2] if and only if AS and AH haveshared knowledge in S1 that Q2 if and only if Q1,I.4. occurs(do(AS, inform(AH, Q1)), S1, S2) ⇒[occurs(do(AS, inform(AH, Q2)), S1, S2) ⇔[∀S1A sk_acc(AS, AH, S1, S1A) ⇒[holds(S1A, Q1) ⇔ holds(S1A, Q2)]]].As we shall see in Section 5, in a discrete model of time this is sufficient to avoid thecontradiction.Note: The above axiom is not sufficient to rule out models in which the informativeactions of one agent constrain the concurrent actions of another agent. The easiest way toinsure independence between agents is to posit an axiom of “anti-synchrony” that no twoagents begin two actions at the same time [36].T.10. occurs(do(A1, Z1), S1, S2) ∧ occurs(do(A2, Z2), S1, S3) ⇒ A1 = A2.However, since this axiom is part of the physical theory, and not all physical theories maywish to use it, we have not made it part of our standard set of temporal axioms.Two alternative formulations of axiom I.4 should be mentioned. We can weaken I.4 toread that communicating Q1 and Q2 co-occur just if they coincide over all situations ofthe same time as the beginning of the situation.I.4.A. occurs(do(AS, inform(AH, Q1)), S1, S2) ⇒[occurs(do(AS, inform(AH, Q2)), S1, S2) ⇔[∀S1A time(S1A) = time(S1) ⇒[holds(S1A, Q1) ⇔ holds(S1A, Q2)]]].The consistency proof in Appendix A requires only a small modification to deal with thisnew version. However, this version seems to me harder to justify than the previous version.A second alternative, which is in effect equivalent to axiom I.4.A, is to use the axiomsWRONG.1 and WRONG.2 and modify the comprehension axiom to state that there is afluent corresponding to every property of situations at some particular time T :I.5.B. Let α(S) be a first-order formula in L with exactly one free variable S of sort“situation”, in which the variable Q does not appear free. (α may have other free variablesof other sorts.) Then the closure of the following formula is an axiom:∀T ∃Q ∀S holds(S, Q) ⇔ α(S) ∧ time(S) = TE. Davis / Artificial Intelligence 166 (2005) 81–139953.5. The frame inferenceFinally, it would be desirable to carry out the frame inference over knowledge and ig-norance.The frame axiom over knowledge is just the axiom of memory, axiom K.4: if A knowsin S that φ is true, then he remembers in all later situations that φ was true. Since wehave no actions or events that cause forgetting, this simple formulation suffices. Note that“knowing φ” is represented as “all worlds in which φ is false are inaccessible.” Hencepreserving knowledge amounts to saying that if situation SB is inaccessible from SA thenany temporal descendant of SB is inaccessible from the corresponding descendant of SA.The frame axiom over ignorance is the reverse: Given that A does not know φ in S0, andgiven that nothing occurs between S0 and S1 that would cause him to learn φ, we wish toinfer that he still does not know φ in S1. Since “not knowing φ in S” is represented as “thereare possible worlds accessible from S in which φ is false,” this frame inference shouldhave the following general form: If S0A is accessible from S0, S1 > S0, S1A > S0A,and as far as A’s sources of knowledge are concerned, the interval between S0 and S1 isindistinguishable from the interval between S0A and S1A, then S1A is accessible from S1.Stating this formally is mostly a matter of collecting all the necessary sources ofknowledge. Our theory requires that agent A gains knowledge in S under the followingcircumstances(1) If A begins action E in S1, and S2 is on a branch in which E is executed, then in S2,A knows that E is executed. If E is completed at or before S2, then in S2 A knowswhen it was completed.(2) If action E is feasible for A in situation S, then A knows that E is feasible in S.(3) If A receives a communication from AS in S then A knows in S that he has received acommunication.We also assume that there are domain-specific axioms of knowledge production. Inan S5 logic, it is reasonable to assume that these are all of the following form: In allsituations S, A knows whether Φ(A, S), where Φ is a formula that can refer only to presentor past physical states or to past (but not present) knowledge states.9 Formally, we imposethe following conditions on Φ(A, S):• The only free variables in Φ(A, S) are A and S.• If S1 is a quantified variable other than S appearing in Φ, and S1 is used as either thesecond-to-last or last argument for either k_acc or sk_acc, then the quantification ofS1 imposes the restriction S1 < S.• If S1 is a quantified variable other than S appearing in Φ, and S1 is not used as an ar-gument for either k_acc or sk_acc, then the quantification of S1 imposes the restrictionS1 (cid:1) S.9 Actually, I conjecture that these restrictions are not necessary, and that it is consistent to allow Φ to be anyformula, but I have not proven it.96E. Davis / Artificial Intelligence 166 (2005) 81–139Table 4Frame action for ignoranceI.6: [k_acc(A, S0A, S0B) ∧ S0A < S1A ∧ S0B < S1B ∧ time(S1B) = time(S0B) ∧(1)[∀S2A,S3A,Z [occurs(do(A, Z), S2A, S3A) ∧ S2A (cid:1) S1A ∧ S0A < S3A ∧ordered(S1A, S3A)] ⇒∃S2B,S3B occurs(do(A, Z), S2B, S3B) ∧ time(S2B) = time(S2A) ∧[[S1A < S3A ∧ S1B < S3B] ∨[S3B (cid:1) S1B ∧ time(S3B) = time(S3A)]]] ∧(2)[∀S2B,S3B,Z [occurs(do(A, Z), S2B, S3B) ∧ S2B (cid:1) S1B ∧ S0B < S3B ∧ordered(S1B, S3B)] ⇒∃S2A,S3A occurs(do(A, Z), S2A, S3A) ∧ time(S2A) = time(S2B) ∧[[S1B < S3B ∧ S1A < S3A] ∨[S3A (cid:1) S1A ∧ time(S3A) = time(S3B)]]] ∧(3)[∀S2A,S3A,AS,Q [occurs(do(AS, inform(A, Q)), S2A, S3A) ∧ S3A (cid:1) S1A] ⇒∃S2B,S3B occurs(do(AS, inform(A, Q)), S2B, S3B) ∧ S3B (cid:1) S1B ∧time(S2B) = time(S2A) ∧ time(S3B) = time(S3A)] ∧(4)[∀S2B,S3B,AS,Q [occurs(do(AS, inform(A, Q)), S2B, S3B) ∧ S3B (cid:1) S1B] ⇒∃S2A,S3A occurs(do(AS, inform(A, Q)), S2A, S3A) ∧ S3A (cid:1) S1A ∧time(S2A) = time(S2B) ∧ time(S3A) = time(S3A)] ∧[∀S2A,S2B [S2A (cid:1) S1A ∧ S2B (cid:1) S1B ∧ time(S2A) = time(S2B)] ⇒(cid:1)i[Φi (S2A) ⇔ Φi (S2B)]](5)]⇒ k_acc(A, S1A, S1B).Thus we assume the existence of a finite collection of axioms of the form∀A,S [[∀SAk_acc(A, S, SA) ⇒ Φi(A, S)] ∨[∀SA k_acc(A, S, SA) ⇒ ¬Φi(A, S)]]For example, Scherl and Levesque [38,39] propose the use of an action “SENSEQ” whichinforms the actor whether fluent Q is true. We can achieve that in the above framework bychoosing Φ(A, S) to be the condition that A has executed SENSEQ and Q holds:Φ(A, S) ⇔ ∃S1 occurs(SENSEQ, S1, S) ∧ holds(S, Q)We now posit that every agent always knows whether Φ(A, S). Since, by axiom K.5, anagent always knows whether he has executed SENSEQ, it follows that, if an agent hasexecuted SENSEQ, then he knows whether Q is true.So now we can state the frame axiom I.6 asserting that if none of the above conditionshas been met, then a knowledge accessibility relation persists. (Table 4.)That is: suppose that S0B is knowledge accessible from S0A relative to A, S1A fol-lows S0A, S1B follows S1A, S1A and S1B have the same clock-time, and the followingconditions are met:E. Davis / Artificial Intelligence 166 (2005) 81–13997(1) If A executes actional Z, either completing it or starting it between S1A or beginningit at S2A, then he executes the same action at the corresponding times in the interval[S0B, S1B]. (If the action ends after S1A and S1B, then the clock-times of the endingsneed not be the same.)(2) The reverse of 1; if A executes an action in the “B” interval then he executes the sameaction at the corresponding time in the “A” interval.(3) If AS tells A of Q and completes this action between S0A and S1A, then the samething happens between S0B and S1B.(4) The reverse of (3): If AS tells A of Q and completes this action between S0B and S1B,then the same thing happens between S0A and S1A.(5) All of the facts Φi have the same truth value from S0A to S1A.Then nothing that A knows about has occurred to distinguish the interval [S0B, S1B] fromthe interval [S0A, S1A], and therefore S1B is knowledge accessible from S1A.Well, there it is. It is not a candidate for any “Top 10 most elegant axioms” lists.A more serious problem is that it does not give us what we want. What we want is: Giventhat in s0, Sam does not know whether Herbert Hoover invented the vacuum cleaner (P ),and given that the only thing that happens between s0 and s1 is that Jack tells Sam that teais selling for $2 a pound in Shanghai (Q), we should be able to infer that Jack still does notknow whether Herbert Hoover invented the vacuum cleaner. But that inference is not valid.The problem is that it is consistent with the givens that Sam originally knows ¬P ⇔ Q,and so, when Jack tells him Q he finds out ¬P . Alternatively, Sam may originally knowthe weaker statement, “If Jack knows Q, then P ;” again, when Jack tells him Q he caninfer that Jack knows Q and therefore P .The problem here is not with the frame axiom; the frame axiom is fine. The problem iswith the specification of the initial state. You need to add the condition that the agent doesnot know anything except the givens. Halpern [17] presents a multi-agent model in whichan agent knows only a specific collection of statements and their logical consequences,and nothing more about the world including other agents’ knowledge (more precisely, hepresents a collection of such theories corresponding to different models of knowledge);and similar studies have been done by Levesque [22]. The problem, though, is that thesetheories only work in the case where we can specify everything that the agent knows. Inmost real cases, we do not know everything that Sam knows, but we still want to make theinference. How this inference can be characterized is entirely an open question; and once itis solved (perhaps non-monotonically) it is unclear whether it would use axiom I.6 at all. Itwould be hard to find any plausible commonsense inference problems where axiom I.6 wasuseful. (In [7], I have studied a special case of this frame inference where the occurrenceof an event is physicially hidden from an agent, and it is therefore possible to infer that theagent remains ignorant of it.)4. Sample inferencesWe now illustrate the power of the above theory by showing how the sample scenariosin the introduction can be represented, and how three toy inferences can be justified.98E. Davis / Artificial Intelligence 166 (2005) 81–139Table 5Notational extensionsDefinitionsKD.1 holds(S, know(A, Q1)) ≡ [∀SA k_acc(A, S, SA) ⇒ holds(SA, Q1)].KD.2 holds(S, not(Q1)) ≡ ¬holds(S, Q1).KD.3 holds(S, know_whether(A, Q1)) ≡holds(S, know(A, Q1)) ∨ holds(S, know(A, not(Q1)))KD.4 Let β(S) be a formula with a free variable S (and possibly other free variables). Let µ be a variable of sort“fluent” that does not appear free in β and let Φ(µ) be a formula. The expression “Φ(λ(S)β(S))” shouldbe expanded to read∃µ [∀S holds(S, µ) ⇔ β(S)] ∧ Φ(µ)In an expression with multiple lambda expressions, the expressions should be expanded from left to right,from outside to inside.To help make the representations more readable and more elegant, we will begin bydefining four further notations (Table 5). First we define “know(A, Q)” as a function map-ping agent A and fluent Q to the fluent of A knowing that Q holds in S; that is, Q holds inall situations accessible from S (definition KD.1).The existence of such a fluent is guaranteed by the comprehension axiom. Let α(S)be the open formula “∀SA k_acc(A, S, SA) ⇒ holds(SA, Q1)”. Then the comprehensionschema asserts∀A,Q1 ∃Q ∀S holds(S, Q) ⇔ ∀SA k_acc(A, S, SA) ⇒ holds(SA, Q1)For any particular A and Q1, the fluent Q satisfying this property has the property weneed for know(A, Q1). Note that, in this construal “know” is a garden-variety first-orderfunction both in its syntax and its semantics.Second, we define “not(Q)” as the function mapping fluent Q to the fluent of Q notholding. Third, we define “know_whether(A, Q)” as a function mapping agent A and fluentQ to the fluent of A knowing whether or not Q is true. Again, the existence of such fluentsis guaranteed by the comprehension axiom, and these are simple first-order functions.The final notation is a macro extension to first-order syntax (“syntactic sugar”). We willuse expressions of the form λ(S)β(S) to denote the fluent that holds in situation S just ifformula β holds of S. Thus, for examples, the fluent that Joe has just completed puttingblock A on B can be denoted by the expressionλ(S)∃S0 occurs(do(joe, puton(a, b)), S0, S)The statement that Sam knows in situation s1 that Joe has just completed putting block Aonto B can thus be expressedholds(s1, know(sam, λ(S)∃S0 occurs(do(joe, puton(a, b)), S0, S)))These lambda expressions are defined within our theory as macros that expand into first-order formulas. (It should be emphasized that we are not here defining a general lambdacalculus, just lambda expressions with one situational argument and a fluent value.) Theexpansion rule is given in definition KD.4 in Table 5.For example, the formulaholds(s1, know(sam, λ(S)∃S0 occurs(do(joe, puton(a, b)), S0, S)))E. Davis / Artificial Intelligence 166 (2005) 81–13999expands to the formula∃Q [∀S holds(S, Q) ⇔ ∃S0 occurs(do(joe, puton(a, b)), S0, S)] ∧holds(s1, know(sam, Q))Using the definition of “know”, this is equivalent to∃Q [∀S holds(S, Q) ⇔ ∃S0 occurs(do(joe, puton(a, b)), S0, S)] ∧∀S1A k_acc(sam, s1, S1A) ⇒ holds(S1A, Q)Since the existence of a fluent Q satisfying this first line is guaranteed by the compre-hension axiom, this is equivalent to∀S1A k_acc(sam, s1, S1A) ⇒∃S0 occurs(do(joe, puton(a, b)), S0, S1A).In the examples that follow, we will give both the compacted representation (with“know” and lambda expressions) and the expanded versions without them.4.1. Sample representationsWe illustrate the expressive power of our representation using the examples from theintroduction.4.1.1. Sample representation 1Alice tells Bob that all her children are asleep.occurs(do(alice, inform(bob,λ(S) ∀C holds(S, child(C, alice)) ⇒ holds(S, asleep(C)))),s0, s1)In expanded form:∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧∀S holds(S, Q) ⇔[∀C holds(S, child(C, alice)) ⇒ holds(S, asleep(C))]4.1.2. Sample representation 2Alice tells Bob that she does not know whether he locked the door.occurs(do(alice, inform(bob,λ(S) holds(S, not(know_whether(alice,λ(SA) ∃S1A,S2A occurs(do(bob, lock_door), S1A, S2A) ∧ S1A < SA))))),s0, s1)100E. Davis / Artificial Intelligence 166 (2005) 81–139Expanding and rearranging gives∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧∀S holds(S, Q) ⇔[∃SA k_acc(alice, S, SA) ∧∃S1A,S2A S1A < S2A < SA ∧occurs(do(bob, lock_door), S1A, S2A)] ∧[∃SA k_acc(alice, S, SA) ∧¬∃S1A,S2A S1A < S2A < SA ∧occurs(do(bob, lock_door), S1A, S2A)]4.1.3. Sample representation 3Alice tells Bob that if he finds out who was in the kitchen at midnight, then he willknow who killed Colonel Mustard. (Note: The interpretation below assumes that exactlyone person was in the kitchen at midnight.)occurs(do(alice, inform(bob,λ(S) ∀SA [S < SA ∧∃PK holds(SA, know(bob,λ(SC) ∃S3C S3C < SC∧ time(S3C) = midnight ∧holds(S3C, in(PK, kitchen))))]⇒∃PM holds(SA, know(bob,λ(SB) ∃S2B,S3B S3B < SB ∧occurs(do(PM, kill(mustard)), S2B, S3B))))),s0, s1)Expanding and rearranging gives:∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧∀S holds(S, Q) ⇔∀S2 [S2 > S ∧∃PK ∀S2A k_acc(bob, S2, S2A) ⇒∃S3A S3A < S2A ∧ midnight(time(S3A)) ∧holds(S3A, in(PK, kitchen))] ⇒[∃PM ∀S2B k_acc(bob, S2, S2B) ⇒∃S3B,S4B S3B < S4B < S2B ∧occurs(do(PM, murder(mustard)), S3B, S4B)]E. Davis / Artificial Intelligence 166 (2005) 81–1391014.1.4. Sample representation 4Alice tells Bob that no one had ever told her she had a sister.occurs(do(alice, inform(bob,λ(S) ¬∃AP,S1,S2 S2 < S ∧occurs(do(AP, inform(alice,λ(SA) ∃P 2 holds(SA, sister(P 2, alice))))S1, S2)))s0, s1)Expanding and rearranging,∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧∀S holds(S, Q) ⇔¬∃S2,S3,Q1,P 1 S2 < S3 < S ∧occurs(do(P 1, inform(alice, Q1)), S2, S3) ∧∀SX holds(SX, Q1) ⇒ ∃P 2 holds(SX, sister(P 2, alice))4.1.5. Sample representation 5Alice tells Bob that he has never told her anything she didn’t already know.occurs(do(alice, inform(bob,λ(S) ∀S2,S3,Q S3 (cid:1) S ∧ occurs(S2, S3, do(bob, inform(alice, Q))) ⇒holds(S2, know(alice, Q))))s0, s1)Expanding and rearranging gives:∃Q occurs(do(alice, inform(bob, Q)), s0, s1) ∧∀S holds(S, Q) ⇔∀S2,S3,Q1[S2 < S3 (cid:1) S ∧occurs(do(bob, inform(alice, Q1)), S2, S3)] ⇒∀S2A k_acc(alice, S2, S2A) ⇒ holds(S2A, Q1)4.2. Sample inferencesWe next illustrate the inferential power of the above theory with three toy problems.102E. Davis / Artificial Intelligence 166 (2005) 81–1394.2.1. Sample inference 1Given:X.1: Sam knows in s0 that it will be sunny on July 4.holds(s0, know(sam, λ(S) ∀SJ S < SJ ∧ time(SJ) = july4 ⇒ holds(SJ, sunny)))Expanding gives∀S0A,SJA [k_acc(sam, s0, S0A) ∧ S0A < SJA ∧ time(SJA) = july4]⇒ holds(SJA, sunny)X.2: In any situation, if it is sunny, then Bob can play tennis.∀S holds(S, sunny) ⇒ feasible(occurs(do(bob, tennis), S))X.3: Sam can always communicate with Bob.∀S1 feasible(do(sam, communicate(bob)), S1).Infer:X.P: Sam knows that there is an action he can do (e.g., tell Bob that it will be sunny) thatwill cause Bob to know that he will be able to play tennis on July 4.holds(s0, know(sam, λ(S)∃Z feasible(do(sam, Z), S) ∧∀S2A occurs(do(sam, Z), S, S2A) ⇒holds(S2A, know(bob, λ(S2B)∀S3B S2B < S3B ∧ time(S3B) = july4 ⇒feasible(do(bob, tennis), S3B)))))Expanding givesk_acc(sam, s0, S0A) ⇒∃Z feasible(S0A, do(sam, Z)) ∧∀S2A,S2B,S3B [occurs(do(sam, Z), S0A, S2A) ∧ k_acc(bob, S2A, S2B) ∧S2B < S3B ∧ time(S3B) = july4] ⇒feasible(do(bob, tennis), S3B)Proof. By the comprehension axiom I.5 there is a fluent q1 that holds in any situation Sjust if it will be sunny on July 4 following S.P.1: ∀S holds(S, q1) ⇔ [∀S1[S < S1 ∧ time(S1) = july4] ⇒ holds(S1, sunny)].Let z1 = inform(bob, q1). By axioms I.2, X.1, and X.3, do(sam, z1) is feasible in s0.P.2: feasible(do(sam, z1), s0).E. Davis / Artificial Intelligence 166 (2005) 81–139103By axiom K.5, Sam knows in s0 that do(sam, z1) is feasible.P.3: ∀S0A k_acc(sam, s0, S0A) ⇒ feasible(do(sam, z1), S0A).Let s0a be any situation such that k_acc(sam, s0, s0a). By P.3, there exists a situation s1asuch occurs(do(sam, z1), s0a, s1a). Let s2a be any situation such that occurs(do(sam, z1),s0a, s2a). Let s2b be any situation such that k_acc(bob, s2a, s2b).By Lemma 3.2, there exists s1b such that occurs(do(sam, z1), s1b, s2b) and holds(s1b,q1). Let s3b be any situation such that s2b < s3b and time(s3b) = july4. By T.8 and T.4,s1b < s3b. By P.2, holds(s3b, sunny). By X.2, feasible(do(bob, tennis), s3b). Applyinguniversal abstraction over s0a, s2a, s2b, and s3b and existential abstraction over z1 and s1agives us formula X.P. (cid:1)4.3. Sample inference 2Given:Y.1: Bob confesses to Alice that he has cheated on her.occurs(do(bob, inform(alice,λ(S) ∃S2,S3 S3 < S ∧ occurs(do(bob, cheat), S2, S3))),s0, s1)This expands to∃Q occurs(do(bob, inform(alice, Q)), s0, s1) ∧∀S holds(S, Q) ⇔ ∃S2,S3 S3 < S ∧ occurs(do(bob, cheat), S2, S3)Y.2: Alice responds that Bob has never told her anything she did not already know.As in sample representation 5, above, in expanded form this is:∃Qoccurs(do(alice, inform(bob, Q)), s1, s2) ∧∀S holds(S, Q) ⇔∀S3,S4,Q1[S3 < S4 (cid:1) S ∧ occurs(do(bob, inform(alice, Q1)), S3, S4)] ⇒∀S3A k_acc(alice, S3, S3A) ⇒ holds(S3A, Q1)Y.P: Bob now knows that Alice had already known, before he spoke, that he had cheatedon her.holds(s2, know(bob,λ(S2A) ∃S0A,S1A,Q1 S1A < S2A ∧occurs(do(bob, inform(alice, Q1)), S0A, S1A) ∧holds(S0A, know(alice,λ(S0B) ∃S3B,S4B S4B < S0B ∧occurs(do(bob, cheat), S3B, S4B))))).104E. Davis / Artificial Intelligence 166 (2005) 81–139Expanding and rearranging gives:∀S2A k_acc(bob, s2, S2A) ⇒∃S0A,S1A,Q1 S1A < S2A ∧ occurs(do(bob, inform(alice, Q1)), S0A, S1A) ∧[∀S0B k_acc(alice, S0A, S0B) ⇒∃S3B,S4B S4B < S0B ∧ occurs(do(bob, cheat), S3B, S4B)]Proof. Let q1 be the content of Bob’s statement in Y.1, and let q2 be the content of Alice’sstatement in Y.2. By axiom I.5, both these fluents exist.By K.4, Bob knows in s2 that he has informed Alice of q1.Q.1: ∀S2A k_acc(bob, s2, S2A) ⇒∃S0A,S1A S1A < S2A ∧ occurs(do(bob, inform(alice, q1)), S0A, S1A).By Lemma 3.2, Bob knows in s2 that q2 held when Alice started to speak.Q.2: k_acc(bob, s2, S2A) ⇒∃S1A occurs(do(alice, inform(bob, q2)), S1A, S2A) ∧ holds(S1A, q2).Let s2a be any situation such that k_acc(bob, s2, s2a), and let s1a be a corresponding valueof S1A satisfying Q.2. Then holds(s1a, q2).By definition of q2, we have that in s1a, whenever Bob had previously told Alice any-thing (Q3), she had already known it.Q.3: ∀S3,S4,Q3 [S3 < S4 (cid:1) s1a ∧ occurs(do(bob, inform(alice, Q3)), S3, S4)] ⇒∀S3A k_acc(alice, S3, S3A) ⇒ holds(S3A, Q1).By K.4 and Y.3, Bob knows in s1 that he has informed Alice of q1.Q.4: ∀S1A k_acc(bob, s1, S1A) ⇒∃S0A occurs(do(bob, inform(alice, q1)), S0A, S1A).In particular, therefore, Q.4 is true of S1A = s1a.Q.5: ∃S0A occurs(do(bob, inform(alice, q1)), S0A, s1a).Let s0a be a situation satisfying Q.5. Applying Q.3, with S3 →s0z, S4 →s1a, andQ3 →q1, givesQ.6. ∀S0B k_acc(alice, s0a, S0B) ⇒ holds(S0B, q1). (cid:1)Applying the definition of q1, we get the desired result.4.4. Sample inference 3Given:Z.1: Anne does not know that she has a sister.E. Davis / Artificial Intelligence 166 (2005) 81–139105¬holds(s0, know(anne, λ(S) ∃Y holds(S, sister(Y, anne))))This expands to¬[∀S0A k_acc(anne, s0, S0A) ⇒ ∃Y holds(S0A, sister(Y, anne))]Z.2: Anne knows that, if she had a sister, someone would have told her about him.holds(s0, know(anne,λ(S) ∀Y holds(S, sister(Y, anne)) ⇒∃S1,S2,AS S2 < S ∧ occurs(do(AS, inform(anne, sister(Y, anne))), S1, S2)))Expanding and rearranging,∀S0A k_acc(anne, s0, S0A) ⇒∀Y holds(S0A, sister(Y, anne)) ⇒∃S1A,S2A,AS S2A (cid:1) S0A ∧occurs(do(AS, inform(anne, sister(Y, anne))), S1A, S2A)Z.3: Sisterhood is forever.S0 < S1 ∧ holds(S0, sister(X, Y )) ⇒ holds(S1, sister(X, Y ))Infer: Anne knows that she has no sister.holds(s0, know(anne, λ(S) ¬∃Y holds(S, sister(Y, anne))))Expands to:Z.4: ∀S0A k_acc(anne, s0, S0A) ⇒ ¬∃Y holds(S0A, sister(Y, anne)).Note: This is a monotonic variant of the “auto-epistemic” inference [30].Proof by contradiction. Suppose that Z.4 is false and Anne does not know that she doesnot has a sister—in other words, as far as she knows she might have a sister.R.1: ∃S0A,Y k_acc(anne, s0, S0A) ∧ holds(S0A, sister(Y, anne)).Let sb and yb be values satisfying R.1. Thus k_acc(anne, s0, sb) and holds(sb, sister(yb,anne)). By Z.2, in sb someone would have already told her that she had a sister.R.2: ∃S1A,S2A,AS S2A (cid:1) sb ∧ occurs(do(AS, inform(anne, sister(yb, anne))), S1A, S2A).By Lemma 3.2, Anne would know in sb that she had previously had a sister.R.3: ∀SC k_acc(anne, sb, SC) ⇒∃S1C S1C < SC ∧ holds(S1C, sister(yb, anne)).Let s0x be any situation such that k_acc(anne, s0, s0x). By K.2 and K.3 k_acc(anne, sb,s0x). By R.3 and X.3, holds(s0x, sister(yb, anne)). Applying universal abstraction to s0xwe haveR.4: ∀S0X k_acc(anne, S0, S0X) ⇒ holds(S0X, sister(yb, anne)).But this contradicts X.1. (cid:1)106E. Davis / Artificial Intelligence 166 (2005) 81–1395. ParadoxThe following Russell-like paradox seems to threaten our theory:10Paradox: Let Q be a fluent. Suppose that over interval [S0, S1], agent a1 carries out theaction of informing a2 that Q holds. Necessarily, Q must hold in S0, since agents are notallowed to lie (axiom I.2). Let us say that this communication is immediately obsolete ifQ no longer holds in S1. For example, if it is raining in s0, the event of a1 telling a2 thatit is raining occurs over [s0, s1], and it has stopped raining in s1, then this communicationis immediately obsolete. Now let us say that a1 has “misled” a2 in S if S is the end ofan immediately obsolete communication. (There is no suggestion intended here, of course,that a2 has any false beliefs.) Since “a1 having misled a2” is a property of a situation, bythe comprehension axiom it should be definable as a fluent. Symbolically,holds(S, misled(A1, A2)) ≡∃Q,A1,A2,S0 occurs(do(A1, inform(A2, Q)), S0, S) ∧ ¬holds(S, Q)Now, suppose that, as above, in s0 it is raining; from s0 to s1, a1 tells a2 that it is raining;and in s1 it is no longer raining and a1 knows that it is no longer raining. Then a1 knowsthat “misled(a1, a2)” holds in s1. Therefore, (axiom I.2) it is feasible for a1 to tell a2 that“misled(a1, a2)” holds in s1. Suppose that, from s1 to s2, a1 informs a2 that “misled(a1,a2)” holds. The question is now, does “misled(a1, a2)” hold in s2? Well, if it does, thenwhat was communicated over [s1, s2] still holds in s2, so “misled(a1, a2)” does not hold;but if it does not, then what was communicated no longer holds, so “misled(a1, a2)” doeshold in s2.The flaw in this argument is that it presupposes the independence axiom WRONG.1(p. 93) that we rejected earlier. The argument presumes that if fluent Q1 (cid:10)= Q2, and do(A1,inform(A2, Q1)) occurs from s1 to s2, then do(A1, inform(A2, Q2)) does not occur. (OurEnglish description of the argument used the phrase “what was communicated between s1and s2“, which presupposes that there was a unique content that was communicated.) Butaxiom I.4 asserts that many different fluents are communicated in the same act. Therefore,the argument collapses.In particular, as we shall show, the clock time (in the sense of “the number of situa-tions that have elapsed since the start of time”) is always common knowledge among allagents (Theorem 3, Appendix A). Now, let q1 be any fluent, and suppose that occurs(do(a1,inform(a2, q1)), s1, s2). Let t1 = time(q1) and let q2 be the fluent defined by the formula∀S holds(S, q2) ⇔ holds(S, q1) ∧ time(S) = t1By assumption, it is shared knowledge between a1 and a2 that holds(s1, q2) ⇔ holds(s1,q1). Hence, by axiom I.4, occurs(do(a1, inform(a2, q2)), s1, s2). But by construction q2does not hold in s1; hence the occurrence of do(a1, inform(as, q2)) from s1 to s2 is imme-diately obsolete. Therefore “misled(a1, a2)” holds any time a1 communicates with a2.10 The comprehension axiom in itself, without the “inform” acts, does not lead to Russell’s paradox, becausea fluent is being defined as in terms of a property of situations, so that there is no circularity. Formally, we willconstruct a set of situations, and then use the standard Zermelo–Fraenkel separation axiom to define a fluent as asubset. See Lemma A.21, p. 136.E. Davis / Artificial Intelligence 166 (2005) 81–139107Changing the definition of misled to use the universal quantifier, thus:holds(S, misled(A1, A2)) ≡∀Q,A1,A2 occurs (do(A1, inform(A, Q)), S0, S) ∧ ¬holds(S, Q)does not rescue the contradiction. One need only change the definition of q2 above to be∀S holds(S, q2) ⇔ holds(S, q1) ∨ time(S) (cid:10)= t1Clearly, the new definition of “misled(a1, a2)” never holds after any informative act.Of course, if we extend the theory to include the underlying locutionary act, then thisparadox may well return, as the locutionary act that occurs presumably is unique. However,as the content of a locutionary act is a quoted string, we can expect to have our hands fullof paradoxes in that theory; this “misled” paradox will not be our biggest problem [31].6. Unexpected hangingThe well-known paradox of the unexpected hanging (also known as the surprise exam-ination) [16,34] can be formally expressed in our theory; however, the paradox does notrender the theory inconsistent. (The analysis below is certainly not a philosophically ade-quate solution to the paradox, merely an explanation of how our particular theory managesto side-step it.)The paradox can be stated as follows:A judge announces to a prisoner, “You will be hung at noon within 30 days; however,that morning you will not know that you will be hung that day”. The prisoner reasonsto himself, “If they leave me alive until the 30th day, then I will know that morning thatthey will hang me that day. Therefore, they will have to kill me no later than the 29thday. So if I find myself alive on the morning of the 29th day, I can be sure that I will behung that day. So they will have to kill me no later than the 28th day . . . . So they cannotkill me at all!”On the 17th day, they hung him at noon. He did not know that morning that he wouldbe hung that day.Let kill_today be the fluent that the prisoner will be hung today. Then the judge’s state-ment can be represented as follows:occurs(do(judge, inform(prisoner,λ(S) ∀SX [S < SX ∧ time(SX) = time(S) + 31] ⇒∃SH S < SH < SX ∧ holds(SH, kill_today) ∧¬holds(SH, know(prisoner, kill_ today)))),s0, s1)Expanding and rearranging, this becomes108E. Davis / Artificial Intelligence 166 (2005) 81–139∃Q occurs(do(judge, inform(prisoner, Q)), s0, s1) ∧∀S holds(S, Q) ⇔∃SH,SHA S < SH < SX ∧ holds(SH, kill_today) ∧k_acc(prisoner, SH, SHA) ∧ ¬holds(SHA, kill_today)We further posit the axioms that the prisoner has never been killed before s0, and that ifan agent has never been killed, he knows that he has never been killed.11¬∃S S < s0 ∧ holds(S, kill_today).∀S2 [∀S1 S1 < S2 ⇒ ¬holds(S1, kill_today)] ⇒holds(S2, know(prisoner, λ(S) ∀S1 S1 < S ⇒ ¬holds(S1, kill_today)))Let UHlang be the judge’s statement in English and let UHlogic be the fluent Q that thejudge communicates. Let “kill(K)” be the proposition that the prisoner will be killed nolater than the Kth day. It would appear that UHlang is true; that the judge knows that in s0that it is true, and that UHlogic means the same as UHlang. By Axiom I.2, if the judge knowsthat UHlogic holds in s0, then he can inform the prisoner of it. How, then, does our theoryavoid contradiction?The first thing to note is that the prisoner cannot know UHlogic. There is simply nopossible worlds structure in which the prisoner knows UHlogic. The proof is exactly iso-morphic to the sequence of reasoning that prisoner goes through. Therefore, by Lemma 3.2above, the judge cannot inform the prisoner of UHlogic; if he did, the prisoner would knowit to be true.The critical point is that there is a subtle difference between UHlang and UHlogic. Thestatement UHlang asserts that the prisoner will not know kill_today—this means even afterthe judge finishes speaking. In our theory, however, one can only communicate propertiesof the situation at the beginning of the speech act and there is no way to refer to whatwill happens as distinguished from one could happen. So what UHlogic asserts is that theprisoner will not know kill_today whatever the judge decides to say or do in s0.In fact, it is easily shown that either [the judge does not know in s0 that UHlogic istrue], or [UHlogic is false]. It depends on what the judge knows in s0. Let us suppose thatin s0, it is inevitable that the prisoner will be killed on day 17 (the executioner has gottenirrevocable orders). There are two main cases to consider.• Case 1: All the judge knows is kill(K), for some K > 17. Then the most that the judgecan tell the prisoner is kill(K). In this case, UHlogic is in fact true in s0, but the judgedoes not know that it is true, because as far as the judge knows, it is possible that (a)he will tell the prisoner kill(K) and (b) the prisoner will be left alive until the Kth day,in which case the prisoner would know kill_today on the morning of the Kth day.11 In S5, it is a logical consequence of this axiom that if he has been killed, he knows he has been killed; but thatis beyond the scope of this paper.E. Davis / Artificial Intelligence 166 (2005) 81–139109• Case 2: The judge knows kill(17). In that case, UHlogic is not even true in s0, becausethe judge has the option of telling the prisoner kill(17), in which case the prisoner willknow kill_today on the morning of the 17th day.Again, we do not claim that this is an adequate solution to the philosophical problem,merely an explanation of how our formal theory manages to remain consistent and side-step the paradox. In fact, in the broader context the solution is not at all satisfying, forreasons that may well become serious when the theory is extended to be more powerful.There are two objections. First, the solution depends critically on the restriction that agentscannot talk about what will happen as opposed to what can happen; in talking about thefuture, they cannot take into account their own decisions or commitments about what theythemselves are planning to do. One can extend the outer theory so as to be able to representwhat will happen—in [10], we essentially do this—but then the comprehension axiom I.5must be restricted so as to exclude this from the scope of fluents that can be the content ofan “inform” act. We do not see how this limitation can be overcome.The second objection is that it depends on the possibility of the judge telling the prisonerkill(17) if he knows this. Suppose that we eliminate this possibility? Consider the follow-ing scenario: The judge knows kill(17), but he is unable to speak directly to the prisoner.Rather, he has the option of playing one of two tape recordings; one says “kill(30)” and theother says UHlogic. Now the theory is indeed inconsistent. Since the prisoner cannot knowUHlogic it follows that the judge cannot inform him of UHlogic; therefore the only thing thatthe judge can say is “kill(30)”. But in that case, the formula UHlogic is indeed true, and thejudge knows it, so he should be able to push that button.To axiomatize this situation we must change axiom I.2 to assert that the only possibleinform acts are kill(30) and UHlogic.Within the context of our theory, it seems to me that the correct answer is “So what?”Yes, you can set up a Rube Goldberg mechanism that creates this contradiction, but theproblem is not with the theory, it is with the axiom that states that only these two informacts are physically possible.(Those readers, if any, who work through the proof of Theorem 1 in Appendix A maywonder what prevents this constraint from being incorporated into the construction ofu-situations. After all, all that this amounts to is drastically restricting the class of “in-form” acts that are added on. The answer is that which of the “inform” acts are allowedto exist now depends on the interpretation of a formula in the extended language, andthat therefore the construction now involves a vicious cycle. See further the comments onLemma A.21.)In a wider context, though, this answer will not serve. After all, it is physically possi-ble to create this situation, and in a sufficiently rich theory of communication, it will beprovable that you can create this situation. However, such a theory describing the physicalreality of communication must include a theory of locutionary acts; i.e., sending signals ofquoted strings. As mentioned above such a theory will run into many paradoxes; this oneis probably not the most troublesome.110E. Davis / Artificial Intelligence 166 (2005) 81–1397. ConsistencyTwo paradoxes have come up, but the theory has side-stepped them both. How do weknow that the next paradox won’t uncover an actual inconsistency in the theory? We caneliminate all worry about paradoxes once and for all by proving that the theory is consis-tent. We do this by constructing a model satisfying the theory. More precisely, we constructa fairly broad class of models, establishing (informally) that the theory is not only consis-tent but does not necessitate any weird or highly restrictive consequences. (Just showingsoundness with respect to a model or even completeness is not sufficient for this. For in-stance, if the theory were consistent only with a model in which every agent was alwaysomniscient, and inform acts were therefore no-ops, then the theory would be consistent butnot of any interest.)As usual, establishing soundness has three steps: defining a model, defining an inter-pretation of the symbols in the model, and establishing that the axioms are true under theinterpretation.Our class of models is (apparently) more restrictive than the theory;12 that is, the theoryis not complete with respect to this class of models. The major additional restrictions inour model are:I. Time must be discrete. We believe that this restriction can be lifted with minor modi-fications to the axioms, but this is beyond the scope of this paper. We hope to addressit in future work.II. Time must have a starting point; it cannot extend infinitely far back. It would seem tobe very difficult to modify our proof to remove this constraint; at the current time, itseems to depend on the existence of highly non-standard models of set theory.III. A knowledge accessibility link always connects two situations whose time is equal,where “time” measure the number of clock ticks since the start. In other words, allagents always have common knowledge of the time. In a discrete structure, this is aconsequence of the axiom of memory. Therefore, it is not, strictly speaking, an addi-tional restriction; rather, it is a non-obvious consequence of restriction (I). If we extendthe construction to a non-discrete time line, some version of this restriction must bestated separately.There are also more minor restrictions; for example, we will define shared knowledgeto be the true transitive closure of knowledge, which is not expressible in a first-orderlanguage.Theorem 1 below states that the axioms in this theory are consistent with essentially anyphysical theory that has a model over discrete time with a starting point state and physicalactions.Definition 1. A physical language is a first-order language containing the sorts “situa-tions”, “agents”, “physical actionals”, “physical actions”, “physical fluents”, and “clock12 The only way to be sure that the theory is more general than the class of models is to prove that it is consistentwith a broader class of models.E. Davis / Artificial Intelligence 166 (2005) 81–139111times”; containing the non-logical symbols, “<”, “do”, “occurs”, “holds”, “time”, and“communicate”; and excluding the symbols, “k_acc”, “inform”, and “sk_acc”.Definition 2 (This is Definition A.6 of Appendix A). Let L be a physical language, let T bea theory over L. T is an acceptable physical theory (i.e. acceptable for use in Theorem 1below) if there exists a model M and an interpretation I of L over M such that thefollowing conditions are satisfied:(1) I maps the sort of clock times to the positive integers, and the relation T 1 < T 2 onclock times to the usual ordering on integers.(2) Axioms T.1–T.9 in Table 1 are true in M under I.(3) Theory T is true in M under I.(4) The theory is consistent with the following constraint: In any situation S, if anycommunication act is feasible, then arbitrarily many physically indistinguishable com-munication acts are feasible.(5) If α is a predicate symbol in L with more than one situational argument, thenα(X1 . . . Xk) holds only if all the situations among X1 . . . Xk are ordered with re-spect to <. (Note that this condition holds both when α is “<” and α is “occurs”.)If β(X1 . . . Xk) is a function symbol, then the above condition holds for the relationXk+1 = β(X1 . . . Xk).Condition (4) no doubt seems complex, strange, and restrictive. But in fact any physicalmodel can be easily transformed into one satisfying this condition: take the original modeland, wherever a communicative act occurs, make an infinite number of identical copies ofthe subtree following the branch where the act occurs. Moreover, most reasonable physicaltheories T will accept this transformation, or can be straightforwardly transformed intotheories that will accept this transformation. In fact, therefore, condition (4) is not a sub-stantial restriction on T . The reason it is needed is that, without this condition, the physicaltheory could include an axiom like, “In any situation S there is only one situation S1 suchthat occurs(do(AS, communicate(AH), S, S1)” whereas our theory demands that there mustexist many such situations corresponding to the different informative acts possible in S.Condition (5) is a technical one needed for the proof. We do not know of any reasonablecausal theories that contain predicates that do not satisfy this condition and that cannot bedefined in terms of simpler predicates that satisfy this condition. We do not know whether(5) is necessary for the conclusion of Theorem 1 to hold; however, we have not been ableto construct a proof without it.(The KR-2004 paper claims that condition (4) can be stated in a first-order axiomschema. This is in error. More precisely, I have not found any first-order axiom schemathat can be used to instantiate condition (4) that I can prove to be sufficient for the theorembelow.)Theorem 1. Let T be an acceptable physical theory, and let U be T together with axiomsK.1–K.8 and I.1–I.5. Then U is consistent.The proof of Theorem 1 is given in Appendix A.112E. Davis / Artificial Intelligence 166 (2005) 81–139It is possible to strengthen Theorem 1 by adding in domain-specific axioms of knowl-edge acquisition and the associated frame axiom over accessibility relation, as described inSection 3.5, plus conditions on the initial knowledge and ignorance of the agents. Specifi-cally, we have the following theorem:Theorem 2. Let T be an acceptable physical theory, and let U be the union of :A. T .B. Axioms K.1–K.7 and I.1–I.5.C. A collection of domain-specific knowledge acquisition axioms of the form specified inSection 3.5.D. The frame axiom I.6 associated with the axioms in (C).E. Any set of axioms K specifying knowledge or ignorance at time 0 as long as:i. The axioms in K do not refer to any situations of time later than 0.ii. The axioms in K are consistent with T , axioms K.1–K.3, K.5 (as regards knowingthe feasibility of actions at time 0); and the axioms in (C).Then U is consistent.In Appendix A, we sketch how the proof of Theorem 1 is modified to give a proof ofTheorem 2.8. Related workThe theory presented here was originally developed as part of a larger theory of multi-agent planning [10]. That theory includes requests as speech acts as well as informativespeech acts. However, our analysis of informative acts there was not as deep or as extensivein scope.As far as we know, this is the first attempt to characterize the content of communicationas a first-order property of possible worlds. Morgenstern [31] develops a theory in whichthe content of communication is a string of characters. A number of BDI models incorpo-rate various types of communication. The general BDI model was first proposed by Cohenand Perrault [4]; within that model, they formalized illocutionary acts such as “Request”and “Inform” and perlocutionary acts such as “Convince” using a STRIPS-like represen-tation of preconditions and effects on the mental states of the speaker and hearer. Cohenand Levesque [5] extend and generalize this work using an full modal logic of time andpropositional attitudes. Here, speech acts are defined in terms of their effects; a request, forexample, is any sequence of actions that achieves the specified effect in the mental state ofthe hearer.Update logic (e.g. [2,33]) combines dynamic logic with epistemic logic, introducing thedynamic operator [A!]φ, meaning “φ holds after A has been truthfully announced”. Theproperties of this logic have been extensively studied. Baltag et al. [1] extend this logic toallow communication to a subset of agents, and to allow “suspicious” agents. Colombetti[6] proposes a timeless modal language of communication, to deal with the interaction ofE. Davis / Artificial Intelligence 166 (2005) 81–139113intention and knowledge in communication. Parikh and Ramanujam [32] present a the-ory of messages in which the meaning of a message is interpreted relative to a proto-col.There is a large literature on the applications of modal logics of knowledge to a multi-agent systems. For example, Sadek et al. [37] present a first-order theory with two modaloperators Bi(φ) and Ii(φ) meaning “Agent i believes that φ” and “Agent i intends thatφ” respectively. An inference engine has been developed for this theory, and there is anapplication to automated telephone dialogue that uses the inference engine to choose ap-propriate responses to requests for information. However, the temporal language associatedwith this theory is both limited and awkward; it seems unlikely that the theory could beapplied to problems involving multi-step planning. (The dialogue application requires onlyan immediate response to a query.)The multi-agent communication languages KQML [13] and FIPA [14] provide rich setsof communication “performatives”. KQML was never tightly defined [40]. FIPA has a for-mal semantics defined in terms of the theory of Sadek et al. [37] discussed above. However,the content of messages is unconstrained; thus, the semantics of the representation is notinherently connected with the semantics of the content, as in our theory.Other modal theories of communication, mostly propositional rather than first-order, arediscussed in [23,35,41].9. Fagin, Halpern, Moses, and VardiThe theory of runs and messages, developed by Fagin, Halpern, Moses, and Vardi(FHMV) in their book Reasoning about Knowledge [12] and many papers, presents a con-structive model of a system of agents. Each agent is characterized as a infinite sequence.The state of agent A at time T is the prefix of the first T elements of the correspondingsequence. The global state of the system at time T is the tuple of the states of all the agentsat time T. Two global system states Q1 and Q2 are knowledge accessible relative to A ifthe state of A is the same in Q1 and Q2. A message is an event that modifies the state ofthe sender when it is sent and the state of the recipient when received. There is a protocolthat governs under what circumstances a sender may send a specified message. Messagesmay be given a semantics, and agents can be prohibited from sending messages that theyknow to be false.Thus, the FHMV theory deals with much the same issues as our theory, and arrives atmany of the same rules: axioms K.1–K.3, K.7, and K.8 are valid in all FHMV models, andFHMV have extensively studied classes of models in which axioms K.4, K.6, I.3, I.6, andthe forward implication in I.2 are valid.Nonetheless there are many major differences between FHMV and our theory. Wedivide these differences for the most part into three categories: differences in purpose,differences in the model, and differences in the representation language. These three cate-gories interact strongly.114E. Davis / Artificial Intelligence 166 (2005) 81–1399.1. Differences in purposeThe central objective of FHMV is to establish a theory for characterizing distributedsystems in terms of the “knowledge” of the components and the communications betweenthem. Such a theory can be used as the foundation for the formal analysis of such systems;e.g. proving that a given class of systems is safe, in some sense; that a specified protocolachieves a specified goal; that a given state of knowledge is inevitable or unattainable;and so on. These proofs might be carried out automatically by reasoning in terms of theformal language, but more often FHMV seem to be thinking about proofs carried out byhuman reasoners reasoning directly about the model. In most cases, the construction of themodel is the critical issue; the definition of a formal language and statement of axioms issecondary, or peripheral. Indeed, in [20] Halpern and Vardi argue strongly in favor of amodel-based as opposed to axiom-based approach to automated reasoning.By contrast, our central objective here is, primarily, to define a formal language capableof representing a wide range of statements about knowledge in commonsense domains,and, secondarily, to demonstrate the power of this language by formulating axioms suffi-cient to justify commonsense inferences. Ultimately, the language and axioms could serveas the basis for a representation and rule set in a symbolic knowledge base. The modelis secondary, as is evidenced by the fact that it is only described in the appendix; it isconstructed only in order to enable us to prove that our representation is coherent and ourtheory is consistent. (For that reason, the inelegance, not to say ugliness, of our model doesnot much matter.)This difference also underlies our different attitudes toward completeness proofs.FHMV construct models that are elegant and interesting in themselves; it is therefore aworthwhile enterprise looking for axiom sets that characterize them exactly. But our modelis constructed out of scotch tape and toothpicks to fit the axioms: what would be gainedby finding a complete axiom set, even if it were possible? After all, since the theory isfirst-order and consistent, we can be sure that there exists a class of models with respect towhich the theory is complete; namely, the class of all models satisfying the theory.Another difference is that FHMV are much more interested in the properties of com-munication itself and communication channels, and have studied in depth the properties ofsystems with unreliable channels or with unknown delays. By contrast, we have been con-tent to deal only with the case of direct speech, or, more generally, communication acrossa reliable channel of fixed delay.9.2. Differences in modelThe key difference between the two models might, at first glance, seem to be a rathertechnical one: FHMV uses a linear model of time13 whereas we use a branching modelof time. But that difference has many ramifications. In a linear model of time, one cannotspeak of an actor having options of many different possible actions. Therefore, it is not13 There are a few exceptions; [18,19,27] consider FHMV models with branching time. However, even in these,no axioms are given that require that there ever exists more than one branch in a situation; that is, these theoriespermit time to branch but do not require it.E. Davis / Artificial Intelligence 166 (2005) 81–139115possible in the FHMV model to reason about what an agent can accomplish or commu-nicate. Inferences such as sample inference 3 (that Sam can cause Bob to know that hewill be able to play tennis) and axioms such as the forward implication of I.2 (that AScan inform AH of anything that AS knows to be true) are not merely invalid in the FHMVmodel; it is essentially impossible to formulate them in that setting. Indeed, almost all thepredictive theorems in the FHMV theory are universal, asserting that a system must attainparticular conditions, or cannot attain them; there are few existential theorems, assertingthat a system can attain a particular condition. The comprehension axiom over fluents, inthis setting, can be made true, but is essentially irrelevant; since any particular system con-tains only a restricted set of messages, the only fluents that need to exist are those that arethe content of these messages.For that reason, the FHMV model cannot be applied to automated planning under theusual logical analysis. The usual logical analysis of the planning problem of states that adeterministic14 plan P correctly achieves goal G starting in situation S0, if (1) P is feasiblestarting in S0; that is, there exists an S1 > S0 such that P is executed over [S0, S1]; and(2) for all such S1, G is achieved over [S0, S1]. But in a linear model of time, (1) can neverbe true of two alternative but mutually exclusive plans.Another difference in the model, reflecting to FHMV’s interest in communication chan-nels, is that where we have a single action “do(AS, inform(AH, Q))” which involves boththe speaker and the hearer, FHMV separate this into two parts: one agent sends a message,then later another agent receives it. The FHMV model is much more general.9.3. Differences in formal languageThere are many differences between the FHMV formal language and our formal lan-guages. To some extent, this reflects the difference in the model; to some extent it reflectsthe difference in purpose; to some extent it is a matter of personal preference in represen-tation style. The representational choices all interact, which makes it difficult to separateout the different motivations behind the different choices. Among the most conspicuousdifferences are:• FHMV use modal languages of time and knowledge where we use a first-order lan-guage. The modal logic formulation has the advantage of supporting interesting theo-rems about computability in the case where the base language is propositional.• FHMV very rarely use an explicit representation of actions and events. They occasion-ally raise the possibility of using a dynamic logic, in which there is a modal operatorcorresponding to each action.• The content of a communication is not a first-order entity in FHMV. Indeed, there isno representation of a message whose form reflects the meaning of the message; themeanings of message are set by a meta-level operator σ .• Agents enter the formal language of FHMV primarily as subscripts on the modal op-erator. It is therefore not possible to quantify over agents.14 The logical analysis of non-deterministic plans such as partially ordered plans is more complex, but alsorequire non-linear models of time [3].116E. Davis / Artificial Intelligence 166 (2005) 81–139• In general, FHMV aim at a very spare formal language; since our objective is to max-imize expressivity, we tend to aim at a very rich one.9.4. Other differencesOne final difference: FHMV like to study, not a one single theory at a time, but a sheafof variant theories, whereas we have presented a single theory. This difference, I think, ismostly a stylistic difference in research method, and is not closely related to any of theother differences.Indeed, FHMV present a set of taxonomic categorizations of different theories of knowl-edge and communication [11]. In terms of that taxonomy, our theory has the followingcharacteristics:• The system is synchronous.• Knowledge is cumulative.• The environment does not determine the initial state of the agents.• The system is not required to be history independent.• Process state transitions are not independent of the environment, or of the initial envi-ronment.• The system is not deterministic.• The primitive propositions are not determined either by the current global state or bythe initial global state.• Neither the primitive propositions nor the class of agents is required to be finite.10. Conclusions and open problemsWe have developed a theory of communications which allows the content of an infor-mative act to include quantifiers and logical operators and to refer to physical states, eventsincluding other informative acts, and states of knowledge; all these in the past, present,or possible futures. We have proven that this theory is consistent, and compatible with awide range of physical theories. We have examined how the theory avoids two potentialparadoxes, and discussed how these paradoxes may pose a danger when these theories areextended. Elsewhere [10] we have shown that the theory can be integrated with a similarlyexpressive theory of multi-agent planning.The major technical problem that follows naturally on this work is to find ways to relaxthe limitations enumerated in Section 1 while preserving the consistency of the theory. Letus discuss what is involved here a little.Two related restrictions are particularly significant in terms of limiting the scope ofapplications of this theory: first, that the sender AS knows when a communication has beenreceived (a consequence of axiom K.5) and that the hearer knows when the communicationwas sent (a consequence of axiom I.3). To relax this restriction, it would be necessary, as inFHMV and similar theories, to separate the action “do(AS, inform(AH, Q))” into an actionof sending a message and an exogenous event of receiving it. The difficulty is that weE. Davis / Artificial Intelligence 166 (2005) 81–139117have not found a reasonable reformulation of axiom I.4 in a way that we can prove avoidsparadox.The restriction that the sender and recipient know each other is one that, in practice, isoften enough violated, and it would certainly be interesting to relax this. If you relax thiscondition, then a timed communication (i.e., one satisfying I.4) gives rise to anonymousshared knowledge. That is, the speaker and hearer know that they share the knowledgeof the content; they just do not know who they are sharing the knowledge with. (Or oneknows and the other does not.) This is analogous to common knowledge among non-rigidsets [12, Section 6.4] but the different setting here raises different issues.The restriction to discrete time obviously impedes the integration of this theory withphysical theories that use continuous time. The problem is that the construction of themodel in our consistency proof is inherently iterative over time, and there does not seemto be any easy way to modify this iterative structure. The proof will work if one makesstrong assumptions about the discreteness of communicative acts; e.g., one posits that itis only physically possible to begin a communication in a situation whose clock time isa non-negative integer. It is conceivable that such a theory would suffice for most ap-plications; one would have to look over examples of reasoning that integrate continuousphysical reasoning with communication, which I have not yet done. I would conjecturethat axioms K.1–K.7 and I.1–I.6 are in fact consistent with a continuous model of time,without modification, and without the need to impose strong conditions on the physics ofcommunication, but I am certainly far from a proof.Other, more far-reaching, problems include:• Our work on integrating the theory here with a theory of planning [10] involves somerather restrictive constraints on the protocols between agents. We would like to studyhow the theory can be modified to weaken these.• To my mind, the brass ring in this field would be to integrate the above theory ofillocutionary acts, which describes the content of communications, with a theory oflocutionary acts, which would describe the form of communications. Achieving a the-ory that is both general and consistent would be a major accomplishment.AcknowledgementThe work described here comes out of and builds upon a project done in collaborationwith Leora Morgenstern, stemming from a benchmark problem that she proposed. Thanksalso to the reviewers for suggestions and corrections.Appendix A. Proof of Theorem 1This appendix contains a proof of Theorem 1. Specifically, we prove that if T is aphysical theory over integer-valued time satisfying a few, not very restrictive, constraints,then T is consistent with our axioms of knowledge and of communication.118E. Davis / Artificial Intelligence 166 (2005) 81–139Outline of appendix: in Section A.1 we give a formal definition of what we mean by aphysical theory. In Section A.2, we show how a model of a physical theory can be extendedto incorporate knowledge relations and informative actions. In Section A.3, we define theinterpretation of our theory over the new model. In Section A.4, we prove that this in-terpretation over this model satisfies both the original physical theory and the axioms ofknowledge and communication.A.1. A physical theoryA physical theory is a set of constraints on actions and fluents. A communicative actionmay have physical preconditions, effects, or other constraints, but these may not depend onthe content of the communication. That is, from the physical point of view, communicativeactions are distinguished only by the identity of the speaker and hearer, not the content.Physical theories do not refer to knowledge states.Our objective here is to prove that any reasonable physical theory is consistent withour theory of knowledge and communication. To do this, we have to ensure that the twotheories “join up”, so to speak; specifically, that the physical theory does not imposeany constraints that are incompatible with the epistemic theory. There are three potentialsources of trouble.• Axioms I.1, I.2, and I.4 together imply that, if AS can communicate with AH then, ingeneral, there are a large number of different possible communicative acts that AS canperform. Specifically, in any situation S, if Q1 and Q2 are fluents such that (a) ASknows that both Q1 and Q2 hold; but (b) it is not shared knowledge between ASand AH that Q1 ⇔ Q2, then the act of AS informing AH that Q1 different from theact of AS informing AH that Q2. The physical theory could make this impossible byasserting that only a small number of different communicative acts are feasible in S.For instance, the statement that only two different communicative acts are feasible ins0 could be stated in the formula∃S1a,S1b occurs(do(as, communicate(ah)), s0, S1a) ∧occurs(do(as, communicate(ah)), s0, S1b) ∧ S1a (cid:10)= S1b ∧∀S1 occurs(do(as, communicate(ah)), s0, S1) ⇒ [S1 = S1a ∨ S1 = S1b]To block this, we impose condition (4) in Definition A.6 below: a physical theorymust be consistent with the constraint that, if any communicative action is feasible in asituation, then infinitely many physically indistinguishable actions are feasible in thatsituation.• Axiom I.5 asserts the existence of a large number of fluents. The physical theory couldassert that only a limited class of fluents exist. E.g., the following axiom asserts thatthe only fluents have the form “on(A, B)” where A and B are blocks.∀Q ∃A,B block(A) ∧ block(B) ∧ Q = on(A, B)This is not at all far-fetched; one approach to the frame problem is to assert “The onlyfluents changed by action A are Q1 . . . Qk”, which leads to the same kind of problem.E. Davis / Artificial Intelligence 166 (2005) 81–139119We get around this problem by distinguishing between physical fluents and generalfluents, and requiring that a physical theory can only refer to physical fluents.• Similarly, the theory of communication requires the existence of actionals “inform(AH,Q)” and of actions “do(AS, inform(AH, Q)).” We have to make sure that the physicaltheory does not simply prohibit these; e.g. assert that the only possible actionals havethe form “communicate(AH)” and “puton(A, B)”. To insure this, we require that thephysical theory can only refer to physical actions and actionals.Definition A.1. A physical language is a first-order language containing the sorts “situa-tions”, “agents”, “physical actionals”, “physical actions”, “physical fluents”, and “clocktimes”; containing the non-logical symbols, “<”, “do”, “occurs”, “holds”, “time”, and“communicate”; and excluding the symbols, “k_acc”, “inform”, and “sk_acc”. (The lan-guage may or may not contain any sort or non-logical symbol other than those mentionedabove.)Definition A.2. Let L be a physical language. Let M be a model and let I be an interpre-tation of L in M. Let s0 and s1 be situations in M. Situation s1 is a successor of s0 ifs0 < s1 and there is no situation sm such that s0 < sm < s1.Here, and in subsequent definitions, we implicitly use I to apply nomenclature from Lto entities in M. More formal statements of the condition “s0 < s1” above would be,“The pair (cid:11)s0, s1(cid:12) ∈ I(‘<’)” or “The open formula SA < SB is true in M under I underthe valuation SA → s0, SB → s1”. We will use the shorter form when it is clear; whennecessary, we will be more precise.Definition A.3. Let L, M, I be as above. Let s0, s1 be situations in M. We say that s1 isa communication successor of s0 if s1 is a successor of s0 and there exist agents as,ah anda situation sz such that s1 (cid:1) sz and occurs(do(as,communicate(ah)),s0,sz).Definition A.4. Let L, M, I be as above. Let τ be a function from M to itself whichis one-to-one and onto. The function τ is said to be a situational automorphism if thefollowing conditions hold:(1) If X is not a situation, then τ (X) = X.(2) Let α be a predicate symbol in L with k arguments or a function symbol with k − 1arguments. Note that, under standard Tarskian semantics, I(α) is a set of k-tuples ofelements of M. A tuple (cid:11)x1 . . . xk(cid:12) ∈ I(α) if and only if (cid:11)τ (x1) . . . τ (xk)(cid:12) ∈ I(α).Definition A.5. Two situations SA and SB are indistinguishable if the following holds: LetSSA be the part of the time structure following SA and SSB be the part of the time structurefollowing SB.SSA = {S ∈ M | SA (cid:1) S}SSB = {S ∈ M | SB (cid:1) S}120E. Davis / Artificial Intelligence 166 (2005) 81–139Then there exists a situational automorphism τ over M such that τ (SSA) = SSB, τ (SSB) =SSA, and for any situation S which is not in SSA and SSB, τ (S) = S.Definition A.6. Let L be a physical language, and let T be a theory over L. T is an ac-ceptable physical theory (i.e., acceptable for our discussion here) if there exists a model Mand an interpretation I of L over M such that the following conditions are satisfied:(1) I maps the sort of clock times to the positive integers, and the relation T 1 < T 2 onclock times to the usual ordering on integers.(2) M satisfies axioms T.1–T.9 in Table 1 under T , where T.8 and T.9 are restricted tophysical actions.(3) M satisfies theory T under I.(4) For any situations s0, s1 and agents as, ah in M, if s1 is a communication successorof s0, then there are infinitely many successors of s0 that are physically indistinguish-able from s1.(5) If α is a predicate symbol in L with more than one situational argument, thenα(X1 . . . Xk) holds only if all the situations among X1 . . . Xk are ordered with re-spect to <. (Note that this condition holds both when α is “<” and α is “occurs”.)If β(X1 . . . Xk) is a function symbol, then the above condition holds for the relationXk+1 = β(X1 . . . Xk).We can now state precisely the theorem that is the objective of this appendix.Theorem 1. Let T be an acceptable physical theory, and let A be T together with ax-ioms K.1–K.8 and I.1–I.5, and with T.8 and T.9 extended to arbitrary actions. Then A isconsistent.Sections A.2–A.4 give the proof of this theorem.A.2. Model constructionSketch of model constructionThe main sticking point of the proof is as follows: In order to satisfy the comprehensionaxiom, we must define a fluent to be any set of situations. However, if Q is a fluent, thenthe act of AS informing AH of Q in S1 generates a new situation; and if we generate aseparate “inform” act for each fluent, then we would have a unsolvable vicious circularity.We are rescued here by axiom I.4 together with the theorem, proven in Theorem 3 be-low, that, in a discrete time structure satisfying the axiom of memory (K.4), knowledgeaccessibility relations can only connect situations of the same time, and therefore the cur-rent time is always common knowledge between all agents. Let q1 be any fluent that holdsin situation s1. By axiom I.4, if AS informs AH of q1 over the interval [s1, s2] and ASand AH have shared knowledge that q1 ⇔ q2 in s1, then the same act can be characterizedas AS informing AH of q2. Let t1 = time(s1). Let q2 be the fluent such that holds(S, q2)⇔ holds(S, q1) ∧ time(S) = t1. Then AS and AH have shared knowledge in s1 that q1 isequivalent to q2. Therefore, it suffices to generate an occurrence of an inform act startingE. Davis / Artificial Intelligence 166 (2005) 81–139121Table A.1Construction of a modelConstructing a modelprocedure model_construct(in T : an acceptable physical theory;M: a model of theory T )return a structure of u-situations over which we will definea model of the extended theory.for each p-situation PS in M, construct a u-situation US.Label PHYS(US) = PS, time(US) = 0.for (each agent A), define the relation K_ACC(A, ·,·)to be some equivalence relation over the u-situations constructed above.for (K = 0 to ∞) do {for (each u-situation S of time K) do {for (each p-situation PS following PHYS(S) in M)construct a new u-situation S1 and mark PHYS(S1) = PS;for (each pair of agents AS, AH) do {if (in M there is an act starting in S of AS communicating to AH)then {SSL := the set of u-situations knowledge-accessible from Srelative to the knowledge of AS;SSU := the set of u-situations knowledge-accessible from Srelative to the shared knowledge of AS and AH;for (each set SS that is a subset of SSU and a superset of SSL) do {construct an action “do(AS,inform(AH,SS))” starting in S;construct a successor S1 of S corresponding to the execution of this action;label PHYS(S1) to be a u-situation in M following a communicate action in PHYS(S);}} } }use the axioms of knowledge to construct a valid set ofknowledge accessibility relations over the new u-situations} return (the set of u-situations plus the set of knowledge accessibility relations)in S1 only for fluents like q2 that specify the current time, and such a fluent can be iden-tified with a set of situations of the same time as S1. This limitation allow us to break thecircularity in the construction of situations and informative acts: the content of informa-tive acts starting at time K is a subset of the situations whose time is K; informative actsstarting in time K generate situations whose time is K + 1.Therefore, we can use the “algorithm” shown in Table A.1 to construct a model of thetheory A. The main difference between the model M of theory T and the model U of A isthat U contains many more situations. To avoid confusion, we will call the situations of M“p-situations” and call the situations of U “u-situations”. Each u-situation US has a corre-sponding p-situation, denoted PHYS(US), which is physically indistinguishable from US.The difference is that US may associate specific contents with some of the communicationactions that precede PHYS(US).Theorem 3. If the set of clocktimes is equal to the positive integers, then for any situationsSA, SB, if k_acc(A, SA, SB) then time(SA) = time(SB).122E. Davis / Artificial Intelligence 166 (2005) 81–139Proof. Suppose that time(SA) < time(SB) = k. By axioms T.7, T.6 and T.5, there existsituations SB0 < SB1 < · · · < SBk−1 < SB such that time(SBi) = i. By axiom K.4 thereexist SA0 . . . SAk−1 such that k_acc(A, SAi, SBi ), SAi−1 < SAi and SAk−1 < SA; but this isimpossible, since time(SA) < k. (cid:1)Formal construction of the modelThe definitions in this section essentially amount to a formalized re-statement of the“algorithm” in Table A.1.Let L be a physical language. Let T be an acceptable physical theory over L. Let Mbe a model and let I be an interpretation of L satisfying the conditions of Definition A.6.The remaining definitions in this section are relative to a fixed choice of L, T , M,and I.For convenience, for each symbol τ in T , including sorts, we use the same symbol inblock capitals to denote the image of τ under I; this is an individual, a subset, a mapping, ora relation over M. Thus, for example, AGENTS is the image under I of the sort “agents”;TIME is the image under I of the function symbol “time” and so on.We now proceed to building up the set of u-situations. This construction is recursiveover time. Naturally, the base case is at time 0.The most important and complex part of the construction is the wider class of situationsthat we will need. In general a u-situation US is a pair (cid:11)S1, MM(cid:12) where:• S1 is a p-situation. We will write S1 = PHYS(US).• MM is a set of 4-tuples (cid:11)AS, AH, USSQ, SX(cid:12). AS and AH are agents; USSQ is aset of u-situations; and SX is a p-situation such that SX < S1 and such that OC-CURS(DO(AS, COMMUNICATE(AH)), SX, SZ), for some SZ that is ordered withrespect to S1. Such a tuple asserts that an action of AS informing AH of content USSQbegan in a u-situation USX < US. We write MM = MM(US).It will be convenient to posit the existence of an atomic entity INFORM, which is notin M, and of an entity DO.Definition A.7. Let PS be a p-situation such that TIME(PS) = 0. A u-situation at time 0 isa pair of the form US = (cid:11)PS, ∅(cid:12). The function ANCESTOR(US) maps a u-situation US toa set of u-situations, the ancestors of US in the time structure.Definition A.8. A time structure of depth 0 TS is a pair:• The set of u-situations U_SITS = {(cid:11)PS, ∅(cid:12) | PS ∈ SITUATIONS, TIME(PS) = 0} withone u-situation for each p-situation at time 0.• A function K_ACC mapping any agent A ∈ AGENTS to an equivalence relation overU_SITS.Definitions A.9 through A.15 are mutually recursive over the depth k, successivelybuilding up the model forward in time.E. Davis / Artificial Intelligence 166 (2005) 81–139123Definition A.9. Let TS be a time structure of depth K. Let US be a u-situation of time Kin TS. Let S1 = PHYS(US). Let MM be a collection of 4-tuples as described above. LetS2 be a successor to S1. The simple successor to US parallel to S2 is the pair (cid:11)S2,MM(cid:12).Definition A.10. Let TS = (cid:11)U_SITS, K_ACC(cid:12), US, S1, MM be as above. Let AS andAH be agents. A possible communicative content from AS to AH is a set of u-situationsUSSQ of time K in U_SITS satisfying the following: let USSL be the set of u-situationsUSA in TS such that (cid:11)US1, USA(cid:12) ∈ K_ACC(AS). Let USSU be the set of u-situationsUSA in USSL such that there exist US0 = US, US1, US2, . . . , USN = USA, such thatfor each J, (cid:11)USJ , USJ +1(cid:12) is either in K_ ACC(AS) or in K_ACC(AH). Then USSL ⊆USSQ ⊆ USSU.The 4-tuple (cid:11)AS, AH, USSQ, S1(cid:12) is called an inform indicator starting in S1.Definition A.11. Let TS, US, S1, MM be as above. Let S2 be a successor of S1. LetI = (cid:11)AS, AH, USSQ, S1(cid:12) be an inform indicator starting in S1. I possibly leads toward S2if there exists SZ (cid:2) S2 such that OCCURS(DO(AS, COMMUNICATE(AH)), S1, SZ). Aninformative sheaf in US toward S2 is a set MMX of inform indicators in US toward S2 suchthat no two elements of MMX have the same speaker and the same hearer. An informativesuccessor to US toward S2 is a pair (cid:11)S2, MM2(cid:12) where MM2 is the union of MM with someinformative sheaf in US toward S2.Definition A.12. Let TS, US, S1, S2 be as above. A u-successor set for US toward S2 isthe union of• The simple successor to US, S2.• A set USS of informative successors to US, S2 with the following property: if M isany inform indicator in S1, then there exists an element (cid:11)S2, MM(cid:12) ∈ USS such thatM ∈ MM. That is, every inform indicator is attached to at least one successor of US.A u-successor of a u-situation at time K is a u-situation at time K + 1. If US1 is a u-successor of US then ANCESTOR(US1) = ANCESTORS(US) ∪ {US}.Definition A.13. Let TS be a time-structure of depth K. A u-situation successor spacefor TS is the union over [all u-situations US of depth K in TS] and [all successors S2 ofPHYS(US))] of some u-successor set for US, S2.Definition A.14. Let TS = (cid:11)U_SITS, K_ACC(cid:12) be a time-structure of depth K. Let USAand USB be u-situations of depth K in TS. Let US1A be a u-successor of USA and letUS1B be a u-successor of USB. Let A be an agent. Then US1B is possibly knowledgeaccessible from US1A relative to A if all the following conditions hold:• (cid:11)USA, USB(cid:12) ∈ K_ACC(A).• For any actional Z and p-situations SXA, SYA, if OCCURS(DO(A, Z), SXA, SYA)and SXA (cid:1) PHYS(USA), then124E. Davis / Artificial Intelligence 166 (2005) 81–139– If SYA < PHYS(USA), then there exist SXB, SYB such thatOCCURS(DO(A, Z), SXB, SYB) and SYB < PHYS(USB).– If SYA = PHYS(USA), then there exists SXB such thatOCCURS(DO(A, Z), SXB, PHYS(USB)).– If SXA < PHYS(USA) < SYA, then there exist SXB, SYB such thatOCCURS(DO(A, Z), SXB, SYB) and SXB < PHYS(USB) < SYB.– If SXA = PHYS(USA) < SYA, then there exists SYB such thatOCCURS(DO(A, Z), PHYS(USB), SYB).• If there exists a tuple (cid:11)AS, A, USSQ, SX(cid:12) in MM(USA) andOCCURS(DO(AS, COMMUNICATE(AH)), SX, PHYS(USA)) then there exists a p-situation SXB and a tuple (cid:11)AS, A, USSQ, SXB(cid:12) in MM(USB) andOCCURS(DO(AS, COMMUNICATE(AH)), SXB, PHYS(USB)). (That is, if AS hascompleted informing A of USSQ, then A knows that AS has completed informing himof USSQ.)Definition A.15. Let TS be a time-structure of depth K. A possible successor to TS is apair TS1 = (cid:11)U_SITS1, K_ACC1(cid:12) where• U_SITS1 is a u-situation successor space for TS;• for each agent A ∈ AGENTS, K_ACC1(A) is an equivalence relation over U_SITS1,which is a subset of the relation, “USB is possibly knowledge accessible from USA”.(Note that, since all the conditions on “possibly knowledge accessible” have the form“Some property holds on US1A iff the corresponding property holds on US1B”, therelation “possibly knowledge accessible relative to (A)” is itself always an equivalencerelation.)TS1 is said to be of depth K + 1.Finally, we let this construction go from time 0 to infinity.Definition A.16. Let TS0 = (cid:11)U_SITS0, K_ACC0(cid:12), TS1 = (cid:11)U_SITS1, K_ACC1(cid:12), . . . be asequence such that TS0 is a time structure of depth 0 and for each i, TSi+1 is a possiblesuccessor for TSi . Then the pairTS∞ = (cid:11)U_SITS∞, K_ ACC∞(cid:12) =iis a communicative model extension of M, I .A.3. Interpretation(cid:2)(cid:3)(cid:4)(cid:3)U_SITSi,K_ ACCjjLet L, M and I be as in the previous section. Let W be the language L combined withthe following additional elements:• The sorts “fluent”, “actional” and “actions”, which are super-categories of the sort“physical fluent”, “physical action”, and “physical actional”, respectively.E. Davis / Artificial Intelligence 166 (2005) 81–139125• The symbols “k_acc”, “sk_acc”, and “inform”.Let TS∞ = (cid:11)U_SITS∞, K_ ACC∞(cid:12) be a communicative model extension of M, I .In this section, we define an interpretation J of W in terms of constructions over TS∞and M. For notational convenience, we will write the image of a symbol under J bywriting it in lower-case boldface; thus, for example, sk_acc = J (“sk_acc”). We will use or-dinary Roman font where symbols are used in prefix notation and are interpreted under J .For example, if we write “occurs(E, S1, S2)” we mean the interpretation of “occurs” un-der J . Note that, if a symbol is in L, then its interpretation under I may be different thanits interpretation under J .We will first discuss the construction of J informally and then proceed to the formaldefinition.The first issue is fluents. On the one hand, axiom I.5 asserts that every property ofsituations α(S) has an associated fluent Qα such that Qα holds in just those situationssatisfying S. The usual extensionalizing trick, therefore, is to identify Qα with the set ofu-situations satisfying α; generally, to identify fluents with sets of situations. On the otherhand, to extend the theory T to the new model, we must make sure that every physicalfluent in T is still a fluent in the new theory. Moreover it is possible that T involves theexistence of two different fluents that are in fact coextensional in terms of the situationswhere they hold, but differ in terms of some other property of interest to T . Therefore, wedefine a general fluent as a pair of a label and a set of u-situations. For a physical fluentthat is, so to speak, grandfathered from T , the label is just the physical fluent; for all otherfluents, the label is immaterial. A physical fluent Q holds in u-situation S just if Q holdsin PHYS(S).The second issue is the occurrence of actions. For physical actions, as for physicalfluents, we use the “PHYS” mapping to guide us; a physical action E occurs from US1 toUS2 if E occurs from PHYS(US1) to PHYS(US2). For informative events, there are twosteps. First, axiom I.4 asserts that “do(as, inform(ah, q1))” and “do(as, inform(ah, q2))”co-occur from us1 to us2 if the intersection of q1 with the set of u-situations that are sk-accessible relative to as,ah from us1 is the same as the intersection of us2 with that set.Second, the occurrence from us1 to us2 of the act “do(as, inform(ah, q0))” where q0 is asubset of the sk-accessible situations is indicated in the second (MM) field of the u-situationus1.Finally for simplicity we assume that there are no “pointless coincidences” between Mand the constructions we will use in J . That is to say: It is conceivable that M itselfhappens to contain, as an entity, some tuple that we will want to define as an entity inthe denotation of J . Such a coincidence would cause propositions to be true and false inways that we do not intend. One could block this by modifying Definition A.20 below asfollows: Wherever the definition constructs an tuple, add an additional element that is notan element of M (e.g., M itself). That will block any such coincidences. For the sake ofreadability, I have omitted these.Otherwise, the definition is pretty much straightforward.Definition A.17. A general fluent is a pair (cid:11)LABEL, USS(cid:12) where LABEL is either a phys-ical fluent or 0, and USS is a set of u-situations.126E. Davis / Artificial Intelligence 166 (2005) 81–139Definition A.18. For any PF in PHYSICAL-FLUENTS, define PF_MAP(PF) to be the pair(cid:11)PF, {US | US ∈ U-SITUATIONS ∧ HOLDS(PHYS(US), PF)}(cid:12).Define PF_IMAGES = {PF_MAP(PF) | PF ∈ PHYSICAL-FLUENTS}.Definition A.19. We define a general mapping “U2P_MAP” from constructions over TS∞to entities in M as follows:• If U is a u-situation, then U2P_MAP(U) = PHYS(U).• If U = (cid:11)PF,USS(cid:12) ∈ PF_IMAGES then U2P_MAP(U) = PF.• If U ∈ M then U2P_MAP(U) = U.• Else U2P_MAP(U) is undefined.In reading Definition A.20 below, keep in mind that, in the standard Tarskian semanticsfor first-order logic, the denotation of a function or a predicate symbol is a set of tuples.Similarly, we take the denotation of a sort to be a set of entities.Definition A.20 (Long). Let L, M, I, W, U be as above. We define the function J over thesorts and symbols of W as follows:Sorts:J (the sort “clock time”) = the non-negative integers.J (the sort “agent”) = I(“agent”).J (the sort “situation”) = the set of u-situations in U .J (the sort “fluent”) = the set of general fluents.J (the sort “physical fluent”) = PF_IMAGES.J (the sort “physical actional”) = I(“physical actional”).J (the sort “physical action’) = I(“physical action”).Let informative_actionals ≡ {(cid:11)INFORM, AH, Q(cid:12) | AH ∈ agent ∧ Q ∈ fluent}.Let informative_actions ≡ {(cid:11)DO, A, Z(cid:12) | Z ∈ informative_ actionals}.J (the sort “actional”) = I(“physical actional”) ∪ informative_actionals.J (the sort “action”) = I(“physical action”) ∪ informative_actions.If σ is any other sort used in L, then J (σ ) = I(σ ).Non-logical symbols:J (“<”) (as a predicate on clock times) = the usual ordering on integers.J (“<”) (as a predicate on situations) = {(cid:11)S1, S2(cid:12) | S1, S2 ∈ situation and S1 isan ancestor of S2}.J (“holds”) = {(cid:11)S, Q(cid:12) | S ∈ situation, Q = (cid:11)PF, USS(cid:12) ∈ fluent and S ∈ USS}.J (“time”) = {(cid:11)S, T (cid:12) | S ∈ situation, T ∈ clock time and S is of time T }.J (“communicate”) = I(“communicate”).J (“do”) = I(“do”)∪{(cid:11)A, Z, (cid:11)DO, A, Z(cid:12)(cid:12) | A ∈ agent and Z ∈ informative_actionals}.J (“inform”) = {(cid:11)AH, Q, (cid:11)INFORM, AH, Q(cid:12)(cid:12) | AH ∈ agent and Q ∈ fluent}.J (“k_acc”) = {(cid:11)A, S1, S2(cid:12) | A ∈ agents and (cid:11)S1, S2(cid:12) ∈ K_ACC∞(A)}.E. Davis / Artificial Intelligence 166 (2005) 81–139127J (“sk_acc”) ={(cid:11)AS, AH, SA, SB(cid:12) |exists(S0 = SA, S1 . . . Sk = SB) such thatfor (i = 1 . . . k) either k_acc(AS, Si−1, Si ) or k_acc(AH, Si−1, Si )}.J (“occurs”) ={(cid:11)E,US1,US2(cid:12) |E ∈ action and US1, US2 ∈ situation and U S1 < U S2 andeither [E ∈ I(“physical action”) and OCCURS(E, PHYS(US1), PHYS(US2))] or[there exist (A,AH ∈ agent; Q1, Q2 ∈ fluent; USS1, USS2) such thatE = (cid:11)DO, AS, (cid:11)INFORM, AH, Q1(cid:12)(cid:12) andQ1 = (cid:11)PF1, USS1(cid:12), Q2 = (cid:11)PF2, USS2(cid:12);USS2 = {US ∈ USS1 | (cid:11)AS, AH, US1, US(cid:12) ∈ sk_acc},OCCURS(DO(AS, COMMUNICATE(AH)), PHYS(US1), PHYS(US2)) and(cid:11)AS, AH, USS2, PHYS(US1)(cid:12) ∈ MM(US2)]}.Let α be any symbol in L other than those enumerated above. I(α) is a set of tu-ples of entities in M. A tuple T (cid:18) is a replacement for tuple T if, for each index I ,U2P_MAP(T (cid:18)[I ]) = T [I ]. Then J (α) is the set of all replacements R for the tuplesin I(α), such that any two situations in R are ordered under J (“<”).Definition A.21. The model U is the union of clocktime, agent, situation, fluent, actional,action and M.Note that the function U2P_MAP(X) is defined for exactly those entities X which arein J (σ ) where σ is one of the sorts in the physical language (clock times, situations,agents, physical fluents, physical actionals, physical actions, and other sorts in L).A.4. SoundnessThroughout this section: Let L be a physical language. Let T be an acceptable physicaltheory over L. Let M be a model and let I be an interpretation of L in M that satisfies T .Let U and J be defined as above.We will assume that L is strongly sorted; in particular, that every variable in L is labelledwith its sort. A valuation over variables in L is required to respect the sort constraint. Thatis, if µi is a variable of sort σi , and V is a valuation of µi in M then V(µi) ∈ I(σi). If Wis a valuation of µi in U then W(µi) ∈ J (σi).Lemma A.1. For every p-situation PS in M there exists a u-situation US in U such thatPHYS(US) = PS.Proof. By induction on TIME(PS). If TIME(PS) = 0 then there exists a correspond-ing u-situation by Definition A.8. Suppose the statement is true for all PS such that128E. Davis / Artificial Intelligence 166 (2005) 81–139TIME(PS) = k. Let PS1 be a p-situation such that TIME(PS1) = k + 1. By axiom T.7,PS1 is the successor of some situation PS0 such that TIME(PS0) = k. By the inductionhypothesis, there is a situation US0 such that PHYS(US0) = PS1. By Definition A.9 thereis a simple successor US1 of US0 such that PHYS(US1) = PS1. (cid:1)Lemma A.2. For any u-situation U, TIME(PHYS(U)) = TIME(U). For any two u-situations U 1, U 2 if U 1 < U 2 then PHYS(U 1) < PHYS(US2).Proof. Immediate from the definition of J (“<”) in Definition A.20 and the definition of“ANCESTORS” in Definitions A.7 and A.12. (cid:1)Lemma A.3. Let µ1 . . . µk be variables in L. Let V be a valuation mapping each vari-able µi into I(σi). Then there exists a valuation W into U such that U2P_MAP(W(µi)) =V(µi).Proof. Immediate from Lemma A.1 together with the construction of U2P_MAP and thefact that, for each sort σ , U2P_MAP maps an element of J (σ ) to an element of I(σ ). (cid:1)Lemma A.4. Let α(µ1 . . . µk) be a predicate symbol in L, including equality, whereinto U . Define V(µi) =µ1 . . . µk have sorts in L. Let W be a valuation from µiU2P_MAP(W(µi )). Then α(µ1 . . . µk) holds in U under J , W if and only if(a)α(µ1 . . . µk) holds in M under I, V and (b) any two situations W(µi) and W(µj ) areordered under J (“<”).Proof. We must consider separately the cases where α is (A) equality over non-situations;(B) equality over situations; (C) the symbol “<” over clock times; (D) the symbol “<”over situations; (E) the symbol “occurs”; (F) the symbol “holds”; (G) any other predicatesymbol in L.(A) Equality over non-situations: from Definitions A.19 and A.20.(B) Equality over situations: following Definitions A.19 and A.20, this amounts to theclaim that US1 = US2 if and only if PHYS(US1) = PHYS(US2) and US1 and US2are ordered. The implication from left to right is trivial. For the implication from rightto left, consider that, if US1 and US2 are ordered but US1 (cid:10)= US2, then either US1 <US2 or US2 < US1. If US1 < US2, then time(US1) < time(US2) so by Lemma A.2,PHYS(US1) (cid:10)= PHYS(US2); and likewise if US2 < US1.(C) The symbol “<” over clock times: from the fact that the interpretation is the sameunder J as under I (Definition A.20).(D) The symbol “<” over situations: analogous to (B) above.(E) The symbol “occurs”. By Definition A.20, if E is a physical action then occurs(E, S1,S2) occurs under J if and only if occurs(E, PHYS(S1), PHYS(S2)) under I.(F) Let µ1, µ2 be variables of sorts “situation” and “physical fluent” respectively. LetPF = V(µ1). Since U2P_MAP(W(µ2)) = V(µ2) = PF, by Definition A.19 W(µ2) ∈PF_IMAGES, which, by Definitions A.18 and A.19, means that W(µ2) = (cid:11)PF, {US ∈E. Davis / Artificial Intelligence 166 (2005) 81–139129U-SITUATIONS | HOLDS(PHYS(US), PF)}(cid:12) By Definition A.20 it follows that(cid:11)W(µ1), W(µ2)(cid:12) ∈ J (“holds”) if and only if (cid:11)V(µ1), PF)(cid:12) ∈ I(“holds”).(G) α is any other predicate symbol in L. Immediate from Definition A.20. (cid:1)Lemma A.5. Let β(µ1 . . . µk) be a function symbol in L, where µ1 . . . µk have sortsin L. Let W be a valuation from µiinto U such that, for any two situational vari-ables µp and µq , W(µp) and W(µq ) are ordered with respect to J (“<”). DefineV(µi) = U2P_MAP(W(µi)). Then the value of β(µ1 . . . µk) in M under I, V is the imageunder U2P_MAP of the value of β(µ1 . . . µk) in U under J , W.Proof. As in the proof of Lemma A.4, we must consider separately the cases where βis (A) the function symbol “do”; (B) the function symbol “time”; (C) any other functionsymbol in L.(A) By Definitions A.19 and A.20, if A is an agent and Z is a physical actional thenU2P_MAP(J (do(A, Z))) = J (do(A, Z)) = I(do(A, Z)) = I(do(U2P_MAP(A),U2P_MAP(Z))). (Again, we are mildly abusing notation.)(B) By Definitions A.19 and A.20, if US is a u-situation then U2P_MAP(J (time(US))) =J (time(US)) = I(time(PHYS(US))) = I(time(U2P_MAP(US))).(C) Let β be any other function symbol. Let (cid:11)x1 . . . xk, y(cid:12) be any tuple where the xi andy are entities in the image under J of the sorts in L. Then by the last part of Defini-tion A.20, (cid:11)x1 . . . xk, y(cid:12) ∈ J (β) iff (cid:11)U2P_MAP(x1) . . . U2P_MAP(xk), U2P_MAP(y)(cid:12)∈ I(β).But for any terms γ1 . . . γk and any valuation W from the variables in the γ ’sto U , the denotation of β(γ1 . . . γk) under J , W is equal to y just if the tuple(cid:11)J (γ1) . . . J (γk), y(cid:12) is in J (β); and likewise for I.Unfortunately, U2P_MAP does not preserve truth-values of predicates over unorderedu-situations; it is possible that U2P_MAP(US1) = U2P_MAP(US2) even though US1 (cid:10)=US2, or that U2P_MAP(US1) < U2P_MAP(US2) even if US1 and US2 are unordered.There is, moreover, in general no way to modify U2P_MAP to preserve inequality, sincethe cardinality of the set of u-situations may be larger than the cardinality of p-situations.Therefore, in establishing below that if an open formula with inequalities or orderings issatisfiable in J then it is also satisfiable in I, it is necessary to continuously “patch” themapping U2P_MAP by mapping a u-situation US into some p-situation that is physicallyindistinguishable from U2P_MAP(US). Fortunately, we had the foresight to provide our-selves with plenty of these. Stating this exactly is a little involved; Definitions A.22–A.24and Corollary A.6 through Lemma A.9 accomplish this.Definition A.22. Let τ be a function from U to itself which is one-to-one and onto. Thefunction τ is said to be a physical automorphism over U if the following conditions hold:(1) If X is not a u-situation, then τ (X) = X.130E. Davis / Artificial Intelligence 166 (2005) 81–139(2) Let α(µ1 . . . µk) be any atomic formula in L with free variables µ1 . . . µk. Let W andY be valuations from µi to U such that Y(µi) = τ (W(µi)). Then Y satisfies α only ifW satisfies α.Note that condition (2) only applies to formulas in the physical language L, not in thebroader language.Definition A.23. Let S1, S2 be either two p-situations or two u-situations. Situation S isthe latest common ancestor (LCA) of S1 and S2, if S (cid:1) S1, S (cid:1) S2 and S is the latestsituation with that property. Since the order relation on situations is a forest of trees, anytwo situations have at most one latest common ancestor.Definition A.24. Let (cid:11)µ1 . . . µk(cid:12) be a k-tuple of variables. Let W be a valuation of the µ’sto U and let V be a valuation of the µ’s to M. V is said to be an image of W if the followingconditions hold:• If µ is not a situational variable, then V(µ) = U2P_MAP(W(µ)).• There exists a physical automorphism τ over U such that, for each pair of situationalvariables µi, µj , if S is the latest common ancestor of W(µi ), W(µj ) then PHYS(τ (S))is the LCA of V(µi ), V(µj ); and if W(µi ) and W(µj ) have no common ancestor, thenV(µi ) and V(µj ) have no common ancestor.We say that the automorphism τ establishes the correspondence between W and V.Corollary A.6. Let µ1 . . . µk, W, V, and τ be as in Definition A.24. For each i, V(µi) =U2P_MAP(τ (W(µi))).If µiis a situational variable,then applying Definition A.24 and choosingj = i, since W(µi ) is the LCA of W(µi) and itself, we have U2P_MAP(τ (W(µi))) =PHYS(τ (W(µi))) = LCA(V(µi), V(µi)) = V(µi). If µi is not a situational variable, thenthe result is immediate.Lemma A.7. Let µ1 and µ2 be situational variables in L. Let W and V be valuations ofµ1, µ2 to U and M respectively, and let V be an image of W. Then W(µ1) = W(µ2) ifand only if V(µ1) = V(µ2) and W(µ1) < W(µ2) if and only if V(µ1) < V(µ2).Proof. Let τ be an automorphism that establishes the correspondence between W and V.If W(µ1) = W(µ2) then V(µ1) = V(µ2), since V(µ) = PHYS(τ (W(µ))) and is thus afunction of W(µ). If W(µ1) < W(µ2) then by Lemma A.2, V(µ1) < V(µ2).Suppose that V(µ1) = V(µ2). Thus, LCA(V(µ1), V(µ2)) = V(µ1) = V(µ2). By Defi-nition A.24 LCA(W(µ1), W(µ2)) = W(µ1) = W(µ2).Suppose that V(µ1) < V(µ2). Thus, LCA(V(µ1), V(µ2)) = V(µ1). By Definition A.24,LCA(W(µ1), W(µ2)) = W(µ1). Therefore W(µ1) (cid:1) W(µ2). Since V(µ1) (cid:10)= V(µ2),it follows from the earlier part of this lemma that W(µ1) (cid:10)= W(µ2); hence W(µ1) <W(µ2). (cid:1)E. Davis / Artificial Intelligence 166 (2005) 81–139131Lemma A.8. Let α(µ1 . . . µk) be a predicate symbol in L. Let W be a valuation of the µ’sto U and let V be an image of W. Then α holds in U under W if and only if α holds in Munder V.Proof. Let τ be an automorphism that establishes the correspondence between W and V.Let Q(µi) = τ (W(µi)). By Definition A.22, α(µ1 . . . µk) holds under J , W if and only ifit holds under J , Q. By Lemma A.4, α(µ1 . . . µk) holds under J , Q if and only if it holdsunder I, V and for any two situational variables µa, µb, Q(µa) and Q(µb) are ordered. ByLemma A.7, Q(µa) and Q(µb) are ordered if and only if V(µa) and V(µb) are ordered;and by condition (5) of Definition A.6, α(µ1 . . . µk) holds under I, V only if V(µa) andV(µb) are ordered. Putting these together, it follows that α(µ1 . . . µk) holds under J , W ifand only if it holds under I, V. (cid:1)Lemma A.9. Let β(µ1 . . . µk) be a function symbol in L, and let µk+1 be another variable.Let W be a valuation of the µ’s to U and let V be an image of W. Then the equationµk+1 = β(µ1 . . . µk) holds in U under W if and only if it holds in M under V.Proof. Exactly analogous to the proof of Lemma A.8, substituting Lemma A.5 forLemma A.4. (cid:1)Lemma A.10. Let α(µ1 . . . µk) be a quantifier-free formula in L. Let W be a valuation ofthe µ’s to U and let V be an image of W. Then α holds in U under W if and only if α holdsin M under V.Proof. Straightforward structural induction over the form of α, using Lemmas A.8and A.9. (cid:1)Lemma A.11. Let µ1 . . . µk be variables whose sorts are in L. Let W be a valuation fromvariables µ1 . . . µk to U and let V be an image of W. (We will include here the case wherek = 0; in that case, W and V are null valuations.) Let µk+1 be a new variable of sort σk+1.(1) Let A be an entity in J (σk+1). Let W(cid:18) = W ∪ {µk+1 → A}. Then there exists B in Msuch that V(cid:18) = V ∪ {µk+1 → B} is an image of W(cid:18).(2) Let B be an entity in I(σk+1). Let V(cid:18) = V ∪ {µk+1 → B}. Then there exists A in Usuch that V(cid:18) is an image of W(cid:18) = W ∪ {µk+1 → A}.Proof. Let τ be a physical automorphism over U that establishes the correspondence of Wand V. If the sort of µk+1 is not a situation, then both (1) and (2) are trivial; one cantake A = B, leave the automorphism τ unchanged, and the result is immediate from thedefinitions. Therefore, we may assume that the sort of µk+1 is a situation, and therefore Ais a u-situation and B is a p-situation. Without loss of generality, renumber the variablesµ1 . . . µk so that µ1 . . . µm are situational variables and the rest are not situational variables.In both halves of the lemma, in order to show that W(cid:18) is an image of V(cid:18) we must exhibitan automorphism τ (cid:18) that establishes this correspondence.Let us write PT(S) = PHYS(τ (S)), and Si = W (µi) for i = 1 . . . m.132E. Davis / Artificial Intelligence 166 (2005) 81–139Part 1. There are three cases:Case A. m = 0. In this case, one can choose B = PHYS(A), and τ (cid:18) to be the identityautomorphism.Case B. Suppose that A (cid:1) Si for some i. Let τ (cid:18) = τ , and let B = PT(A). For any j , let Sbe the LCA of Sj and A. There are four cases:B.i. Sj (cid:1) A. In this case S = Sj . Since W is an image of V under τ , PT(S) =PT(Sj ) = V(µj ).B.ii. A (cid:1) Sj . In this case S = A. Since τ is an automorphism, τ (S) (cid:1) τ (Sj ). ByLemma A.2, PT(S) = PT(A) (cid:1) PT(Sj ) so PT(S) is the LCA of PT(A) andPT(Sj )B.iii. A and Sj are unordered but have LCA S. Then S is the LCA of Si and Sj , soPT(S) is the LCA of PT(Si ) and PT(Sj ). Since PT(S) < PT(A) (cid:1) PT(Si),it follows that PT(S) is the LCA of PT(A) and PT(Sj ).B.iv. A and Sj have no common ancestor. Hence Si and Sj have no com-mon ancestor. Hence PT(Si ) and PT(Sj ) have no common ancestor. HencePT(A) < PT(Si) and PT(Sj ) have no common ancestor.Case C. Suppose that A does not precede any of the Sj . Consider the set LL = {LCA(A,S1) . . . LCA(A, Sm)}. If LL is non-empty, let S be the latest situation in LL. Wehave three cases:C.i. LL is empty; that is, none of the Sj are ordered with respect to A. Then noneof the values of τ (Sj ) are ordered with respect to τ (A), so by Lemma A.4,none of the values of PT(Sj ) are ordered with respect to PT(A). Hence, wemay choose τ (cid:18) = τ and B = PT(A).C.ii. S is equal to one of the Si . Then for each Sj , LCA(A, Sj ) = LCA(Si, Sj ).Thus, again, we may choose τ (cid:18) = τ and B = PHYS(τ (A)).C.iii. S is not equal to any of the Si . Note that at least there must be one of theSj > S; call this Sx . Let Q be the successor of S such that Q (cid:1) A. Thereare two cases:C.iii.a. Q is not a communicative successor of S. Then τ (Q) is not a com-municative successor of τ (S). For any Sj , if S < Sj , let Qj bethe successor of S such that Qj (cid:1) Sj . By the construction in Def-initions A.9–A.12, it follows that PT(Q) is not equal to PT(Qj ).Therefore PT(S) is the LCA of PT(A) and PT(Sj ). If Sj is notordered with respect to S, then the LCA of Sj and A is thesame as the LCA of Sj and Sx (or neither of these LCA’s ex-ists), so again LCA(PT(A), PT(Sj )) = LCA(PT(Sx), PT(Sj )) =PHYS(LCA(τ (Sx), τ (Sj ))) = PHYS(LCA(τ (A), τ (Sj ))). There-fore we can choose τ (cid:18) = τ and B = PHYS(τ (A)).C.iii.b. Q is a communicative successor of S. Here, finally, is the casewhere τ may need to be modified. Let Q1 . . . Qp be all the suc-cessors of S that precede one of the Si . By property (4) of Defi-nition A.6, there are infinitely many successors of PT(S) that arephysically indistinguishable from PT(Q). Let C be one such that isnot equal to PT(Qi ) for any i. Let ω be the automorphism of M thatE. Davis / Artificial Intelligence 166 (2005) 81–139133interchanges the subtree of p-situations following C with the sub-tree of p-situations following PT(Q) and leaves the rest of M thesame (see Definition A.5). Let τ (cid:18) = τ ◦ ω. Let B = PHYS(τ (cid:18)(A)).Now, suppose Sj > S. Then the LCA of Sj and A = S. SincePHYS(τ (cid:18)(A)) is a descendant of C, which is a successor ofPHYS(τ (S)) and PHYS(τ (cid:18)(Sj )) is a descendant of PHYS(τ (Qj )which is a different successor of PHYS(τ (S)), it follows thatthe LCA(PHYS(τ (cid:18)(A)), PHYS(τ (cid:18)(Sj ))) = PHYS(τ (S)). Alterna-tively, if Sj is not ordered with respect to S, then we still haveLCA(PHYS(τ (A)), PHYS(τ (Sj ))) = PHYS(LCA(τ (A), τ (Sj ))),by exactly the same argument as in case C.iii.a.Part 2. The proof of Part 2 is exactly analogous to that of Part 1, but going in the oppositedirection. (cid:1)Lemma A.12. Let α be a prenex formula in L with m quantifiers and k free variablesµ1 . . . µk. Let W be a valuation from variables µ1 . . . µk to U and let V be an image of W.Then α is true under J , W if and only if it is true under I, V.Proof. By induction on m, the number of quantifiers.If m = 0, then the statement is just Lemma A.10.Suppose the statement is true for all formulas with m quantifiers. Let α be a formulawith m + 1 quantifiers. There are four cases:Case 1: α is true under J , W and α has the form “∃Xβ(X)”, where β is a formula withm quantifiers and k + 1 free variables. Since α is true, there exists an entityA ∈ U and a valuation W(cid:18) = W ∪ {X → A} such that β is true under J , W(cid:18). ByLemma A.11 there exists a valuation V(cid:18) that is an image of W(cid:18). By the inductivehypothesis, β is true under I, V(cid:18). Hence α (that is, ∃Xβ) is true under I, V.Case 2: α is true under I, V and α has the form “∃Xβ(X)”. Since α is true, there exists anentity B ∈ M and a valuation V(cid:18) = V ∪ {X → B} such that β is true under I, V(cid:18).By Lemma A.11 there exists a valuation W(cid:18) such that V(cid:18) is an image of W(cid:18). Bythe inductive hypothesis, β is true under J , W(cid:18). Hence α is true under J , W.Case 3: α is true under J , W and α has the form “∀Xβ(X)”. Let γ be the transformationinto prenex form of ¬α. Then γ is false under J , W, and γ has the form “∃Xδ”where δ is the prenex form of ¬β. By the contrapositive to case 2 above, γ is falseunder I, V; hence α is true under I, V.Case 4: α is true under I, V and α has the form “∀Xβ(X)”. Exactly analogous to case (4),but using the contrapositive to case 1. (cid:1)Corollary A.13. All the physical axioms of T , axioms T.1–T.7, and axioms T.8 and T.9restricted to physical actions are true in U under interpretation J .Proof. Immediate from Lemma A.12, taking k = 0 and using the fact that the axioms in Tand axioms T.1–T.9 are true in M (by definition of M). (cid:1)134E. Davis / Artificial Intelligence 166 (2005) 81–139Lemma A.14. If PS1 = PHYS(US1) and PS1 and PSZ are ordered, then there exists USZsuch that US1 and USZ are ordered, and PSZ = PHYS(USZ).Proof. If PS1 = PSZ then USZ = US1.If PSZ < PS1, then let USZ be the ancestor of US1 at time TIME(PSZ).If PS1 < PSZ, then let s1 = PS1, s2 . . . sk = PSZ be p-situations such that si+1 is asuccessor of si . Using Definition A.9 iteratively, let US2 be the simple successor to US1parallel to PS2, let US3 be the simple successor to US2 parallel to PS3, and so on. ThenUSk satisfies the desired conditions on USZ. (cid:1)Lemma A.15. Axioms T.8 extended to general actions and K.1–K.8 are true in U under J .(I’m just bunching together the axioms whose proof is easy.)Proof.Immediate from the definition of J (“occurs”) (Definition A.20).T.8.K.1–K.3. Immediate from Definition A.15, which requires K-ACC(A) to be an equiva-lence relation on u-situations.K.4–K.6. Immediate from Definition A.14, which restricts the “possibly accessible” onsituations that hold on the left-hand side of each of these relations to those thatsatisfy the conditions on the right-hand side of these implications; plus Defini-tion A.15, which states that the actual knowledge accessibility relation are a subsetof the possibly accessible relations.K.7,K.8. Immediate from the definition of J (“sk_acc”) in Definition A.20. (cid:1)Lemma A.16. Axiom I.1 is true in U under J .Proof. By the definition of J (“occurs”) in Definition A.20, if occurs(do(AS, inform(AH,Q)),US1, US2) then there exist QA, PF1, USS1, PFA, USSA) such that Q1 = (cid:11)PF1, USS1(cid:12),QA = (cid:11)PFA, USS2(cid:12), USSA = {US ∈ USS1 | (cid:11)AS, AH, US1, US(cid:12) ∈ k_acc,}, and (cid:11)AS, AH,USS2, S1, PHYS(US2)(cid:12) ∈ MM(US2). Let USQ be the successor of US1 such thatUSQ (cid:1) US2. By Definition A.9, (cid:11)AS, AH, USS2, S1, PHYS(US2)(cid:12) ∈ MM(USQ). ByDefinitions A.10 and A.11, OCCURS(DO(AS, COMMUNICATE(AH)), PHYS(US1),PHYS(US2)). By Definition A.20, occurs(do(AS, communicate(AH)), US1, US2). (cid:1)Lemma A.17. Axiom T.9 extended to general actions is true in U under J .Proof. Let US1, US2, USX, and USY be u-situations and E an event such that occurs(E,US1, US2), US1 < USX < US2 and USX < USY. Let S1 = PHYS(US1) and S2 =PHYS(US2). By Definition A.20, E is either a physical action or an informative action.The case where E is a physical action is covered in Corollary A.13. Suppose that E is an in-formative action; let E = (cid:11)DO, AS, (cid:11)INFORM, AH, Q1(cid:12)(cid:12). By Definition A.20 there existQA, PF1, USS1, PFA, USSA such that Q1 = (cid:11)PF1, USS1(cid:12), QA = (cid:11)PFA, USS2(cid:12), USSA ={US ∈ USS1 | (cid:11)AS, AH, US1, US(cid:12) ∈ k_acc}, and (cid:11)AS,AH,USSA,S1(cid:12) ∈ MM(US2). ByE. Davis / Artificial Intelligence 166 (2005) 81–139135Definition A.9, (cid:11)AS, AH, USSA, S1(cid:12) ∈ MM(USX). By Axiom T.9 applied to the ac-tion DO(AS, COMMUNICATE(AH)) there exists a situation SZ such that ordered(SZ,PHYS(SY)), SZ > SX, and OCCURS(DO(AS, COMMUNICATE(AH), S1, SZ). ByLemma A.14, there exists USZ such that PHYS(USZ) = SZ and USZ is ordered withrespect to USY. It follows that USZ > USX and that (cid:11)AS, AH, USS2, S1(cid:12) ∈ MM(USZ).By Definition A.20, occurs(do(AS, inform(AH, Q)), US1, USZ). (cid:1)Lemma A.18. Axiom I.2 is true in U under J .Proof. Let AS, AH be agents, let US1, US2 be u-situations, and let Q = (cid:11)PF, USSQ(cid:12)be a general fluent. Let US1ACC = {USA | (cid:11)AS, AH, US1, USA(cid:12) ∈ sk_acc}, the set ofsituations accessible from US1 in the shared knowledge of AS and AH. Let USSA =USSQ ∩ US1ACC, the set of situations satisfying Q that are knowledge accessible from S1,relative to the shared knowledge of AS and AH. Let S1 = PHYS(US1).Suppose that occurs(do(AS, inform(AH, Q)), US1, US2). By Definition A.20 (denota-tion of “occurs”), the tuple (cid:11)AS, AH, USSA, S1(cid:12) ∈ MM(US2). Let USY be the successorof US1 that is an ancestor of US2. By Definitions A.9, A.11, and A.12 it follows thatMM(USY) contains the tuple (cid:11)AS, AH, USSA, S1(cid:12). By Definition A.10, USSA is a pos-sible communicative content for S1 from AS to AH; hence, by Definition A.10, everysituation that is knowledge accessible from US1 relative to AS is an element of USSA andtherefore an element of USSQ ⊃ USSA. By Definition A.20 (“holds”) Q holds in everysituation accessible from US1.Conversely,if Q holds in every situation accessible from S1,then USSA is apossible communicative content from AS to AH. Suppose that OCCURS(DO(AS,COMMUNICATE(AH)), S1, S2). Let SY be the successor of S1 such that SY (cid:1) S2.By Definition A.12,there exists an informative successor USY of US1 such that(cid:11)AS, AH, USSA, S1(cid:12) ∈ MM(USY). By Axiom T.9 there exists a situation USZ (cid:2) USYsuch that OCCURS(DO(AS, COMMUNICATE(AH)), US1, USZ). By Definitions A.9,A.11, A.12 (cid:11)AS, AH, USSA, S1(cid:12) ∈ MM(USZ). By Definition A.20, occurs(do(AS, in-form(AH,Q)), S1, SZ). (cid:1)Lemma A.19. Axiom I.3 is true in U under J .Proof. Assume that occurs(do(AS, inform(AH, Q)), US1, US2) and that k_acc(AH, US2,US2A). We need to prove that there exists a situation US1A such that occurs(do(AS,inform(AH, Q)), US1A, US2A) and k_acc(AH, US1, US1A).Define USSA as in the proof of Lemma A.18. By Definition A.20 (denotationof “occurs”) since occurs(do(AS, inform(AH, Q)), US1, US2) it follows that the tu-ple (cid:11)AS, AH, USSA, PHYS(US1)(cid:12) ∈ MM(US2) and OCCURS(DO(AS, COMMUNI-CATE(AH)), PHYS(US1), PHYS(US2)). By Definition A.15, since k_acc(AH, US2,US2A), US2A is possibly knowledge accessible from US2 relative to AH. By Defini-tion A.14, the tuple (cid:11)AS, AH, USSA, PS1A(cid:12) ∈ MM(US2A) for some p-situation PS1A <PHYS(US2A), and OCCURS(DO(AS, COMMUNICATE(AH)), PS1A, PHYS(US2A)).By Theorem 3 and axiom K.8, any two situations that are sk_acc are at the sametime. Hence, all the situations in USSA are at the same time, and by Definition A.10136E. Davis / Artificial Intelligence 166 (2005) 81–139this time must be equal to TIME(US1) and to TIME(US1A). Hence TIME(US1) =TIME(US1A). By axiom A.4, since k_acc(AH, US2, US2A), US1 < US2, US1A < US2Aand TIME(US1) = TIME(US1A), it follows that k_acc(AH, US1, US1A). Hence, the setof situations that are accessible relative to the shared knowledge of AS and AH is the samestarting from US1 as starting from US1A. Hence the act of AS informing AH of Q startingin US1A uses the tuple (cid:11)AS, AH, USSA, PS1A(cid:12). Thus by Definition A.20, occurs(do(AS,inform(AH, Q)), US1A, US2A). (cid:1)Lemma A.20. Axiom I.4 is true in U under J .Proof. Suppose that occurs(do(AS, inform(AH, QX)), US1, US2) and that QY is a fluent.Let US3 be the successor of US1 such that US3 (cid:1) US2. LetQX = (cid:11)PFX, USSQX(cid:12)QY = (cid:11)PFY, USSQY(cid:12)QXA = USSQX ∩ {USA | sk_acc(AS, AH, US1, USA)}QYA = USSQY ∩ {USA | sk_acc(AS, AH, US1, USA)}andBy Definition A.20, (cid:11)AS, AH, QXA, PHYS(US1)(cid:12) ∈ MM(US2). By Definitions A.9,A.11, A.12, (cid:11)AS, AH, QXA, PHYS(US1)(cid:12) ∈ MM(US3).I. (Left to right in the two-way implication.)Suppose that occurs(do(AS, inform(AH, QY)), US1, US2). By the same argument asabove (cid:11)AS, AH, QYA, PHYS(US1)(cid:12) ∈ MM(US3). But by Definition A.11, US3 con-tains at most one inform indicator with starting point PHYS(US1), speaker AS, andhearer AH. Hence QXA = QYA. That is, if situation USA is accessible from US1 rel-ative to the shared knowledge of AS and AH, then QX holds in USA iff QY holds inUSA.II. (Right to left in the two-way implication.)If it is the case that∀S1A sk_acc(AS, AH, S1, S1A) ⇒ [holds(S1A, QX) ⇔ holds(S1A, QY)]then QXA = QYA, so by Definition A.20, occurs(do(AS, inform(AH, QY)), US1,US2). (cid:1)Lemma A.21. Axiom I.5 (the comprehension axiom) is true in U under J .Proof. Immediate from Definitions A.17 and A.20, using the comprehension axiom ofZermelo–Fraenkel set theory (also known as the “subset” or “separation” axiom). Sincethere exists a well-defined set U-SITS of all situations, the ZF axiom asserts that everysuch formula defines a subset of U-SITS. See, for example, [15, p. 36]. (cid:1)Considering how problematic the comprehension axiom would seem to be it may besurprising that it has a one-step proof. In fact, one might say that the whole construction wewent through in Section A.3 is precisely tailored so that the comprehension axioms shouldE. Davis / Artificial Intelligence 166 (2005) 81–139137have a one-step proof. Nonetheless the reader may well have legitimate worries about sucha powerful axiom, that are hardly assuaged by the above proof. Let me therefore discussfurther how this whole construction works.The key point is this: There is no circularity whatever in the whole structure of defi-nitions given in Section 3. The structure of u-situations is built up iteratively forward intime. The label on an “inform” action A is a set of u-situations contemporaneous with thestart of A; it gives rise to a new u-situations at the next point in time. Iterating from 1 toinfinity gives us a well-defined and fixed set U of all u-situations. Definition A.17 definesa fluent as a subset of U . Definition A.20 defines the occurrence of an inform action interms of these fluents and of the labels on the actions. More generally, Definition A.20defines the denotation of every symbol in W extensionally, in terms of structures over Uand M and the interpretation I; no aspect of J is defined in terms of J itself (except as aconvenient abbreviation). Having adopted Definition A.20, J is now fixed, and it is fixedwhich fluents satisfy which formulas under J .But is not it inherently circular to say, for example,q1 is the fluent such that∀S holds(S, q1) ⇔ ∃AS,AH,S2,Q occurs(do(AS, inform(AH, Q)), S, S2)considering that the quantification over Q contains q1 itself? Not at all, no more thansaying0 is the number such that, ∀XX + 0 = Xwhen the quantification over X includes 0 itself. The formula above is just a description ofq1, and the axioms are sufficient to guarantee that a q1 satisfying this definition exists.Theorem 1. Let T be an acceptable physical theory, and let A be T together with ax-ioms K.1–K.8 and I.1–I.5, and with T.8 and T.9 extended to arbitrary actions. Then A isconsistent.Proof. We have shown that a model and an interpretation satisfying A can be con-structed. (cid:1)Theorem 2. Let T be an acceptable physical theory, and let U be the union of :A. T .B. Axioms K.1–K.7 and I.1–I.5.C. A collection of domain-specific knowledge acquisition axioms of the form specified inSection 3.5.D. The frame axiom I.6 associated with the axioms in (C).E. Any set of axioms K specifying the presence or absence of k_acc relations amongsituations at time 0 as long as:i. The axioms in K do not refer to any situations of time later than 0.ii. The axioms in K are consistent with T , axioms K.1–K.3, K.5 (as regards knowingthe feasibility of actions at time 0), and the axioms in (C).Then U is consistent.138E. Davis / Artificial Intelligence 166 (2005) 81–139Proof (Sketch). The proof of Theorem 1 needs to be modified as follows:• In Definition A.8, initialize the K_ACC function at time 0 to satisfy the union of theaxioms in (E) with the axioms enumerated in E.ii.• In Definition A.14, add to the conditions on US1B being possibly knowledge accessi-ble from US1A:For each axiom in (C) of the form “A always knows whether Φi(A, S),” the condi-tion Φi(US1B) ⇔ Φi(US1A) must hold.• Modify the second bullet in Definition A.15 to read, “For each agent A, K_ACC1(A)is the relation over u-situations, ‘US1B is knowledge accessible from US1A relativeto A’ ”.The proof that the additional axioms enumerated in Theorem 2 are satisfied is thenstraightforward. (cid:1)References[1] A. Baltag, L. Moss, S. Solecki, The logic of public announcements: Common knowledge and private suspi-cions, Unpublished, 2002, http://www.cs.indiana.edu/ftp/techreports/TR534.html.[2] J. van Benthem, ‘One is a Lonely Number’: On the logic of communication, ILLC Tech Report 2003-07,Institute for Logic, Language and Computation, University of Amsterdam, 2003.[3] D. Chapman, Planning for conjunctive goals, Artificial Intelligence 32 (1987) 333–378.[4] P.R. Cohen, C.R. Perrault, Elements of a plan-based theory of speech acts, Cognitive Sci. 3 (3) (1979) 177–212.[5] P.R. Cohen, H. Levesque, Intention is choice with commitment, Artificial Intelligence 42 (2–3) (1990) 213–261.[6] M. Colombetti, A modal logic of intentional communication, Math. Social Sci. 38 (1999) 171–196.[7] E. Davis, Inferring ignorance from the locality of visual perception, in: Proceedings of AAAI-88, St. Paul,MN, AAAI Press/MIT Press, Cambridge, MA, 1988, pp. 786–790.[8] E. Davis, Representations of Commonsense Knowledge, Morgan Kaufmann, San Mateo, CA, 1990.[9] E. Davis, A first-order theory of communicating first-order formulas, in: D. Dubois, C. Welty, M. Williams(Eds.), Proceedings of the Ninth International Conference on Principles of Knowledge Representation andReasoning (KR2004), Whistler, BC, AAAI Press, Menlo Park, CA, 2004, pp. 235–245.[10] E. Davis, L. Morgenstern, A first-order theory of communication and multi-agent plans, J. Logic Comput.,in press.[11] R. Fagin, J. Halpern, M. Vardi, What can machines know? On the properties of knowledge in distributedsystems, J. ACM 39 (1992) 328–376.[12] R. Fagin, J. Halpern, Y. Moses, M. Vardi, Reasoning about Knowledge, MIT Press, Cambridge, MA, 1995.[13] T. Finin, J. Weber, M. Genesereth, D. McKay, J. McGuire, R. Pelavin, S. Shapiro, C. Beck, Specificationof the KQML agent communication language, DARPA Knowledge Sharing Initiative External InterfacesWorking Group, 1993.[14] FIPA, The foundation for intelligent physical agents, http://www.fipa.org/, 2001.[15] A. Fraenkel, Y. Bar-Hillel, A. Levey, A Foundations of Set Theory, second ed., Noord-Hollandsche U.M.,Amsterdam, 1973.[16] M. Gardner, The Unexpected Hanging and Other Mathematical Diversions, Chicago University Press,Chicago, IL, 1991.[17] J. Halpern, A theory of knowledge and ignorance for many agents, J. Logic Comput. 7 (1) (1997) 79–108.E. Davis / Artificial Intelligence 166 (2005) 81–139139[18] J. Halpern, M. Vardi, The complexity of reasoning about knowledge and time in asynchronous systems, in:Proc. 20th ACM Symp. on Theory of Computation, 1988, pp. 53–65.[19] J. Halpern, M. Vardi, The complexity of reasoning about knowledge time, I: lower bounds, J. Comput.Systems Sci. 38 (1) (1989) 195–237.[20] J. Halpern, M. Vardi, Model checking vs. theorem proving: A manifesto, in: J. Allen, R. Fikes, E. Sandewall(Eds.), Proceedings of the Second International Conference on Principles of Knowledge Representation andReasoning (KR1991), Morgan Kaufmann, San Mateo, CA, 1991, pp. 325–334.[21] P. Hayes, The naive physics manifesto, in: D. Michie (Ed.), Expert Systems in the Micro-Electronic Age,Edinburgh University Press, Edinburgh, 1978.[22] H. Levesque, All I know: A study in auto-epistemic logic, Artificial Intelligence 42 (1990) 263–309.[23] A. Lomuscio, M. Ryan, A spectrum of modes of knowledge sharing between agents, in: N. Jennings,Y. Lespérance (Eds.), Intelligent Agents VI: Agent Theories, Architectures, and Languages, in: LectureNotes in Artificial Intelligence, vol. 1757, Springer, Berlin, 2000, pp. 13–26.[24] J. McCarthy, Programs with common sense, in: M. Minsky (Ed.), Semantic Information Processing, MITPress, Cambridge, MA, 1968, pp. 403–418.[25] J. McCarthy, P. Hayes, Some philosophical problems from the standpoint of artificial intelligence, in:B. Meltzer, D. Michie (Eds.), Machine Intelligence, vol. 4, Edinburgh University Press, Edinburgh, 1969,pp. 463–502.[26] D. McDermott, A temporal logic for reasoning about processes and plans, Cognitive Sci. 6 (1982) 101–155.[27] R. van der Meyden, K. Wong, Complete axiomatizations for reasoning about knowledge and branching time,Studia Logica 75 (1) (2003) 93–123.[28] R. Moore, Reasoning about Knowledge and Action, Tech. Note, vol. 191, SRI International, Menlo Park,CA, 1980.[29] R. Moore, A formal theory of knowledge and action, in: J. Hobbs, R. Moore (Eds.), Formal Theories of theCommonsense World, ABLEX Publishing, Norwood, NJ, 1985, pp. 319–358.[30] R. Moore, Semantical considerations on nonmonotonic logic, Artificial Intelligence 25 (1985) 75–94.[31] L. Morgenstern, Foundations of a logic of knowledge, action, and communication, PhD thesis, NYU, 1988.[32] R. Parikh, R. Ramanujam, A knowledge-based semantics of messages, J. Logic Language Inform. 12 (4)(2003) 453–467.[33] J. Plaza, Logics of public announcements, in: Z.W. Ras (Ed.), Proc. 4th International Symposium on Method-ologies for Intelligent Systems, Charlotte, NC, North-Holland, Amsterdam, 1989.[34] W.V.O. Quine, On a so-called paradox, Mind 62 (1953) 65–67.linear[35] A.S. Rao, Decision proceduresfor propositionalin:M. Wooldridge, J. Müller, M. Tambe (Eds.), Intelligent Agents II: Agent Theories, Architectures, and Lan-guages, in: Lecture Notes in Artificial Intelligence, vol. 1037, Springer, Berlin, 1995, pp. 33–48.time belief-desire-intention logics,[36] R. Reiter, Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems,MIT Press, Cambridge, MA, 2001.[37] M.D. Sadek, P. Bretier, F. Panaget, ARTIMIS: Natural dialogue meets rational agency, in: M. Pollack (Ed.),Proceedings of IJCAI-97, Nagoya, Japan, Morgan Kaufmann, San Francisco, CA, 1997, pp. 1030–1035.[38] R. Scherl, H. Levesque, The frame problem and knowledge producing actions, in: Proceedings of AAAI-93,AAAI Press/MIT Press, Cambridge, MA, 1993, pp. 689–695.[39] R. Scherl, H. Levesque, Knowledge, action, and the frame problem, Artificial Intelligence 144 (1) (2003)1–39.[40] M. Wooldridge, An Introduction to MultiAgent Systems, Wiley, New York, 2002.[41] M. Wooldridge, A. Lomuscio, Reasoning about visibility, perception, and knowledge, in: N. Jennings,Y. Lespérance (Eds.), Intelligent Agents VI: Agent Theories, Architectures, and Languages, in: LectureNotes in Artificial Intelligence, vol. 1757, Springer, Berlin, 2000, pp. 1–12.