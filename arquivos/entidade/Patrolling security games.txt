Artificial Intelligence 184–185 (2012) 78–123Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintPatrolling security games: Definition and algorithms for solving largeinstances with single patroller and single intruderNicola Basilico, Nicola Gatti∗, Francesco AmigoniArtificial Intelligence and Robotics Laboratory, Dipartimento di Elettronica e Informazione, Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133 Milano, Italya r t i c l ei n f oa b s t r a c tArticle history:Received 29 July 2010Received in revised form 22 February 2012Accepted 8 March 2012Available online 12 March 2012Keywords:Security gamesAdversarial patrollingAlgorithmic game theorySecurity games are gaining significant interest in artificial intelligence. They are characteri-zed by two players (a defender and an attacker) and by a set of targets the defendertries to protect from the attacker’s intrusions by committing to a strategy. To reach theirgoals, players use resources such as patrollers and intruders. Security games are Stackelberggames where the appropriate solution concept is the leader–follower equilibrium. Currentalgorithms for solving these games are applicable when the underlying game is in normalform (i.e., each player has a single decision node). In this paper, we define and studysecurity games with an extensive-form infinite-horizon underlying game, where decisionnodes are potentially infinite. We introduce a novel scenario where the attacker canundertake actions during the execution of the defender’s strategy. We call this new gameclass patrolling security games (PSGs), since its most prominent application is patrollingenvironments against intruders. We show that PSGs cannot be reduced to security gamesstudied so far and we highlight their generality in tackling adversarial patrolling onarbitrary graphs. We then design algorithms to solve large instances with single patrollerand single intruder.© 2012 Elsevier B.V. All rights reserved.1. IntroductionSecurity applications for transportation, shipping, airports, ports, and other infrastructures have recently received anincreasing interest in the artificial intelligence literature [39,40,52,62]. The mainstream approach models a security problemas a two-player non-cooperative game [27] between a defender and an attacker with the aim to find effective strategies for thedefender [51]. The basic ingredients are a number of targets, each with a value (possibly different for the two players), and anumber of resources available to the defender to protect the targets and to the attacker to intrude them. In most situations ofinterest, the resources available to the defender are not enough to protect all the targets at once. This induces the defenderto randomize over the possible assignments of resources to targets to maximize her expected utility. Furthermore, whilethe defender continuously and repeatedly protects the targets, the attacker is assumed to be in the position to observe thedefender and derive a correct belief over her strategy. This last assumption pushes the defender to commit to a strategyand places security games in the more general class of leader–follower (also said Stackelberg) games where the leader is thedefender and the follower is the attacker [64].A leader–follower game is characterized by an underlying game and by the property that the leader can commit to astrategy. Von Stengel and Zamir studied this class of games in [64]. They show that, by committing to a particular strategyin a two-player normal-form underlying game, the leader cannot receive a utility worse than that she would receive when* Corresponding author. Tel.: +39 02 2399 3658; fax: +39 02 2399 3411.E-mail address: ngatti@elet.polimi.it (N. Gatti).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2012.03.003N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–12379playing a Nash equilibrium. The leader–follower equilibrium is thus proposed as the appropriate solution concept. Letchfordand Conitzer recently showed that the same result holds also when the underlying game is in extensive form [45].In the last few years, several works have addressed the field of security games. For example, one of the most influentialworks is [51], where the problem of placing checkpoints to protect some targets from intrusions is studied (the approachhas been practically applied at the Los Angeles International Airport [52]). The works proposed in this field are based onan underlying game in normal form (both players have a single decision node) and focus on the equilibrium computationproblem, i.e., on the development of efficient algorithms to compute a leader–follower equilibrium.The currently available models are not applicable when the attacker has the option to exploit the observation of theexecution of the defender’s actions to decide when to attack during the realization of the defender’s plan without beingsubject to any temporal deadline (namely, with the possibility of waiting indefinitely for attacking). In [29], the authorshows that by exploiting this option an attacker can drastically improve her expected utility.In this paper, we propose a variant of security games that accounts for such an option. To do so, we consider a securitygame with an underlying game in extensive form with infinite horizon, players having multiple (potentially infinite) decisionnodes. This contribution constitutes, to the best of our knowledge, an extension to the state of the art in security games.The main theoretical motivation behind our work is that the currently available techniques are not efficient with thisvariant of security games. Indeed, their resolution requires techniques, largely unexplored in the security games literature, toreduce the size of the game instances. The main practical motivation behind our work is that the above option is available tothe attacker in many scenarios, among which the most studied is probably adversarial patrolling, where one or more patrollers(the resources controlled by the defender, usually consisting in autonomous mobile robots) move within an environmentto protect it and one or more intruders (the resources controlled by the attacker) wait outside the environment for thebest time to attack. We focus on patrolling as reference scenario for our game models and we call them patrolling securitygames (PSGs). Formulating the adversarial patrolling problem as a PSG allows us to deal with environments represented asarbitrary graphs with targets. The drawback is that the needed computational effort is much larger than that required tosolve settings with special topologies without targets (e.g., with closed perimeters [3]).Our main original contributions, aiming at addressing the equilibrium computation problem for PSGs, follow.(i) We model a PSG as a two-player multi-stage game with infinite horizon, where the defender moves a single resourceon the vertices of an arbitrary graph environment to protect the targets while the attacker intrudes the environment byplacing, for some time, a resource on a selected target vertex. We show that the equilibrium computation problem isa multi-quadratic mathematical programming problem that does not scale to realistically large settings. To tackle theselimitations, we propose the following techniques.(ii) We study the problem to find, when it exists, an equilibrium in pure strategies (namely, deterministic patrolling strate-gies). We show that this problem is a currently unexplored variant of the travel salesman problem (TSP) and that,although NP-complete, it can be efficiently solved by a constraint satisfaction programming algorithm, that solves withhigh success rate ((cid:2) 90%) significantly large instances ((cid:2) 500 targets) in short time ((cid:3) 10 s).(iii) We develop reduction techniques to find a mixed strategy equilibrium (namely, non-deterministic patrolling strategies) inlarge game instances when no pure strategy equilibrium exists. We provide some reduction algorithms based on thecombination of removal of dominated actions and abstractions and we show that no further general reductions ex antethe actual resolution can be provided. We show that with first-order Markovian strategies (that depend only on thevertex visited last by the patroller) our algorithm optimally solves medium-size game instances (up to 75 vertices and15 targets) and sub-optimally solves large-size game instances (up to 166 vertices and 30 targets). We show that thequality of optimal and sub-optimal first-order Markovian solutions is at least 99% and 86%, respectively, of the qualityof the optimal high-order Markovian solutions.The structure of the paper follows. In Section 2, we survey the related works on security games and on robotic patrolling.In Section 3, we describe our game model and we extend the known techniques to solve it, showing their limitations. InSection 4, we discuss how a pure strategy equilibrium can be found when it exists. In Section 5, we provide techniques toreduce game instances and speed up the (mixed strategy) equilibrium computation. Our algorithm is summarized in Sec-tion 6 and experimentally evaluated in Section 7. Section 8 concludes the paper. Appendices A, B, and C report extensions,proofs, and complete experimental data, respectively.2. Related worksWe review security games and leader–follower equilibrium computation in Section 2.1. Next, we survey the main workson robotic patrolling in Section 2.2 and on other related fields in Section 2.3.2.1. Security games and leader–follower equilibrium computationThe seminal work on security games is probably the von Neumann’s hide-and-seek game [24]. It is a strategic-form zero-sum game played in grid environments where the hider chooses a location wherein to hide and the seeker chooses a set oflocations wherein to seek. Starting from this work, several significant variations have been proposed in the literature.80N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123A number of works study the problem where some pursuers search for evaders [1]. Game models in which both playersare mobile are said infiltration games [8]. When the pursuer is mobile and the evader is immobile, we have search games [28].When the situation is the reverse, we have ambush games [56]. A variation is the interdiction game [65] where the evadermoves from a source to a sink (target), while the pursuer acts to prevent the evader to reach the target. All these works aredefined on arbitrary graphs, but they do not consider targets with different importance.Several variations of the interdiction games where targets have varying importance have been recently proposed withthe goal to design randomized policies under scheduling constraints to protect targets. We call these works protection games.Significant examples are [39,40,50–52,62].The PSG model we propose in this paper considers a mobile defender and an attacker on an arbitrary graph withtargets of different importance. The attacker directly appears on a target and can be detected during the intrusion at thetarget (this model can be extended considering movements of the attacker). This is because intruding a target requiresthe attacker to spend some time on it. PSGs lay at the intersection between protection games, interdiction games, andsearch games. As the protection games, PSGs consider different targets with different importance with the aim to preventintrusions. As in interdiction games, the pursuer can capture the attacker during the approach to the target. However, theattacker can also be captured after she reached a target, during the time she spends there for the intrusion. As in searchgames, the defender is mobile and the attacker can be immobile because, while intruding a target, she stays there for sometime.A crucial point of PSGs, common with protection games, is that the appropriate solution concept is the leader–followerequilibrium. Algorithms for computing a leader–follower equilibrium constitute a recent result. The seminal work, describedin [20], shows that the computation of a leader–follower equilibrium can be formulated as a multi-linear mathematicalprogramming problem with as many linear programs as the number of the follower’s actions. This result shows that aleader–follower equilibrium can be found in polynomial time. An alternative formulation is provided in [50] where theproblem is formulated as a mixed-integer linear mathematical programming problem.2.2. Robotic patrollingA broad definition of patrolling is “the act of walking or traveling around an area, at regular intervals, in order toprotect or supervise it” [46]. Among the many scientific aspects that are involved in developing autonomous robots forpatrolling (e.g., hardware and software architectures [46]), we focus on algorithms for producing patrolling strategies. Weclassify existing algorithms for patrolling along three main dimensions.The first dimension concerns the patrolled area representation. It can be graph-based or continuous (by means of geo-metrical primitives, e.g., lines and polygons). With graph-based representations, there are four cases: open perimeter, closedperimeter, fully connected (every vertex is connected to all the others), and arbitrary. In all these cases, an environment mayhave only identical vertices or special vertices of interest, called targets.The second dimension is the patroller’s objective function. It can explicitly take into account the presence of adversaries(adversarial) or do not (non-adversarial). In the non-adversarial case, objective functions are mainly related to some form ofrepeated coverage, where the aim is to repeatedly cover the locations of a given area. Frequency-based objective functionsrelated to repeated coverage can be defined as constraint satisfaction functions (e.g., patrolling all locations with the samefrequency) or as functions that maximize some measure, e.g., the maximal average frequency of visits (also called average idle-ness), or the maximal minimum frequency of visit (also called worst idleness). When the environment has targets, frequenciesof visits are expressed relative to targets. Other cases can be encountered that refer to ad hoc objective functions. In theadversarial case, there are two kinds of objective functions: expected utility with fixed adversary, where the expected utilityof the patroller is maximized given a fixed non-rational model of the adversary, and expected utility with rational adversary,where the adversary is modeled as a rational decision maker.The third dimension is the number of adopted patrollers (i.e., available resources). It can take single agent or multiagentvalues.Table 1 shows the classification of the main works on robotic patrolling according to the above dimensions. The symbols(cid:2) and (cid:2) denote the contributions we provide in Sections 4 and 5, respectively. In the following, we review the main relatedworks on patrolling reported in the table, organizing the discussion according to the representation of the environment.The work in [23] provides efficient algorithms to find multiagent patrolling strategies for open-ended fences (i.e., open-ended polylines) that minimize different notions of idleness for realistic models of robot motion. Patrolling open perimetersis challenging because it is intrinsically inefficient, since robots must re-visit the just visited areas when they reach anendpoint and turn back. The work in [23] divides the continuous open polylines in discrete segments and determines theactions of the robots accordingly.The work presented in [3] provides an efficient (polynomial time) algorithm to solve closed perimeter multiagent pa-trolling settings in a game theoretical fashion. The perimeter is continuous but is divided into segments that can be easilymapped to the vertices of a ring-like graph. The solving algorithms are referred to this discretized environment, as opposedto the algorithms that are based on continuous environments, like that in [38], for example. For this reason, we classifythis work under graph-based environments. A possible intruder can enter any vertex and is required to spend a given time(measured in turns and called penetration time) to have success. The intruder and the patrollers have no preferences overthe vertices and the intruder will enter the vertex in which the probability to be captured is minimum. The patrollers areN. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–12381Table 1Related works on robotic patrolling. The symbols (cid:2) and (cid:2) denote the contributions of this paper.Graph–basedOpenperimeterClosedperimeterFullyconnectedArbitraryContinuous[66], (cid:2)[48][33,66][32,35]single agentmultiagentsingle agentmultiagent[23][7,18,22,46,59]single agentmultiagentsingle agentmultiagentsingle agent[47,54][58][9], (cid:2)[6][29]multiagent[2,3,4,5]desab–ycneuqerFConstraintsatisfactionMaximizationof a measureOthersExpected utility withfixed adversaryExpected utility withrational adversarylairasrevda–noNlairasrevdAsynchronous. The problem is essentially a zero-sum game and the solution (i.e., the patrolling strategy) is the patrollers’maxmin strategy. In [4] and [5] the model is extended by considering realistic uncertainty over the robots’ sensing and overadversary’s knowledge. In [2] both the presence of events and different times of detection of the intruders, which yielddifferent rewards to the patrollers, are considered. Finally, in [6] non-rational intruders are considered.In [29] the author considers a fully connected topology graph where the (single) patroller and the intruder can havedifferent preferences over the target vertices and provides an algorithm to compute a Nash equilibrium.The approach in [66] considers single and multiagent patrolling on arbitrary graphs, but the goal is to patrol edges andnot vertices. The objective function is the blanket time criterion and so the patrolling agents have to cover all the edgeswith the same frequency. The proposed ant-based algorithm is shown to converge to an Eulerian cycle in a finite numberof steps and to re-visit edges with a finite period. A similar work is reported in [33].Some works address the covering of environments represented as arbitrary graphs. The work in [46] deals with mul-tiagent patrolling of vertices of graphs whose edges have unitary lengths. Several agent architectures are experimentallycompared according to their effectiveness in minimizing the idleness. The approach is generalized in [7] to graphs in whichedges have arbitrary lengths, and analyzed from a more theoretical perspective in [18]. Moreover, another work [59] pro-poses reinforcement learning as a way to coordinate patrolling agents and to drive them around the environment.The work in [22] efficiently computes patrolling strategies for multiple agents minimizing the worst idleness in arbitrarygraphs. Patrolling strategy is calculated exploiting a minimal Hamiltonian cycle.In [47] and in [54] the authors consider multiagent patrolling settings with multi-criteria objective (e.g., idleness anddistribution probabilities over the occurrence of incidents), which is pursued exploiting MDP techniques.The work in [58] studies different adversaries: a random adversary, an adversary that always chooses to penetratethrough a recently visited node, and an adversary that predicts the chances that a node will be visited. Some patrollingstrategies for multiple agents are experimentally evaluated by simulation, showing that no strategy is optimal for all thepossible adversaries.In [9], the authors study arbitrary topology graphs providing an on-line heuristic approach to find the optimal strategiesfor a single patroller.The contribution we provide in Section 4 ((cid:2) in Table 1) studies a setting in which different targets of an arbitrarygraph must be visited by a single agent with (at least) some frequencies, specific for each target, thus satisfying a setof constraints. The main differences with [66] are that we are patrolling vertices (and not edges) and that these verticescan have different requirements in terms of frequency of visits. Furthermore, our approach is not directly comparable withthe other approaches for finding deterministic patrolling strategies, because we solve a feasibility problem (i.e., findinga patrolling strategy that satisfies some constraints), while other approaches solve an optimization problem (with somecriterion).The contribution we provide in Section 5 ((cid:2) in Table 1) generalizes both the works in [3] and [29] to arbitrary graphswith targets, but it is less computationally efficient for the settings to which both approaches are applicable. Moreover, itextends [58], capturing a rational adversary.For completeness, we cite also some works that deal with continuous environments, even if they are not directly com-parable with our graph-based approach. In [32], a system composed of multiple air vehicles that patrol a border area ispresented. The environment is represented as a continuous two-dimensional region that is divided in sub-regions. Each82N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123sub-region is assigned to an air vehicle that repeatedly patrols it with a spiral trajectory, ensuring that every point is cov-ered. Also [35] considers multirobot patrolling of continuous environments. In this case, the environment is partitioned insub-regions using a Voronoi tessellation, robots are assigned to sub-regions, and each robot patrols its sub-region in orderto have a complete coverage. The movements of a robot are determined by a neural network model that allows to deal withdynamically varying environments. Finally, in [48], unpredictable chaotic trajectories are produced to have a robot coveringa continuous environment.2.3. Other related worksA large amount of works closely related to our patrolling problem can be found in the operational research literature asvariations of the Traveling Salesman Problem (TSP). These works are close to graph-based frequency-based patrolling works,but the objective functions they adopt are not suitable for patrolling problems, as we discuss below.In the deadline-TSP [63], vertices have deadlines over their first visit and some time is spent traversing arcs. Rewards arecollected when a vertex is visited before its deadline, while penalties are assigned when a vertex is either visited after itsdeadline or not visited at all. The objective is to find a tour that visits as many vertices as possible. However, differentlyfrom what happens in patrolling, the reward/penalty is received only at the first visit.In the vehicle routing problem with time windows [41], deadlines are replaced with fixed time windows, during which visitsof vertices must occur. The time windows do not depend on the previous visits of the patroller, as it happens in patrolling.In the period vehicle routing problem [34], constraints could impose multiple visits to a same vertex in a time period.Cyclical sequences of visits are addressed in the period routing problem [19,26], where vehicle routes are constructed torun for a finite period of time in which every vertex has to be visited at least a given number of times. In the cyclic inventoryrouting problem [53] vertices represent customers with a given demand rate and storage capacity. The objective is to find atour such that a distributor can repeatedly restock customers under some constraints over visiting frequencies.Despite these works have different similarities with the patrolling problem we consider in this paper, the application ofsuch techniques to our setting is not straightforward and limited to very particular scenarios.3. Game model, solution concept, and basic algorithmIn this section we introduce our approach. In particular, in Section 3.1 we describe the model of the PSGs, in Section 3.2we discuss the appropriate solution concept, and in Section 3.3 we provide a solving algorithm inspired by the current stateof the art.3.1. Patrolling security game modelAt first we describe the patrolling setting in Section 3.1.1 and then the game model in Section 3.1.2.3.1.1. The patrolling settingThe patrolling setting is composed of an environment and of two players, an attacker a and a defender d, each with somespecific resources. We assume discrete time (developing in turns) and we model the environment by means of a directedgraph G = (V , A, T , v, d). Set V contains vertices representing the areas of the environment. Set A contains arcs connectingvertices in V , providing the topology of the environment (graph representations can be extracted from real environmentsby, e.g., [43]); we represent A by a function a : V × V → {0, 1}, where a(i, j) = 1 if (i, j) ∈ A and a(i, j) = 0 otherwise. SetT ⊆ V contains the vertices, called targets, with some values for the defender and the attacker. v is defined as a pair offunctions v = (v d, v a), where v d : T → R+assigns each target t the value for the defender of successfully protecting t ∈ Tand v a : T → R+assigns each target t ∈ T the value for the attacker of successfully intruding t. Function d : T → N \ {0}assigns each target t ∈ T the time that the attacker needs to spend on t for completing an intrusion and getting v a(t).The attacker is modeled as follows: she can wait indefinitely outside the environment observing the defender’s actionsand perfectly deriving the defender’s strategy (as in [3,51]); she can use a single resource, called intruder, to attack a targetdirectly placing it on the target; once she has attacked a target t, she cannot control the intruder for a number of turnsequal to d(t), after which she removes the intruder from the environment.The defender is modeled as follows: she can move a single resource, called patroller, along G spending one turn to coverone arc (as in the simplest motion model adopted in [3]); the patroller can sense only (and perfectly) the area correspondingto the vertex in which she is; once the patroller has sensed the intruder, the intruder is captured.The simplifying assumptions on players do not prevent to capture realistic applicative scenarios. For example, the factthat an attacker can directly pose its resource on a target can be encountered in situations in which a patroller can detectan intruder only when this last one is not moving. On the defender’s side, the simplified movement model representedby fixed weights on the graph’s arcs can model the situation in which the patroller is a software agent traveling betweennodes of a sensor network deployed in the environment and performing some data analysis on the current node. Moreover,these limitations can be partially relaxed as we show in Appendix A.3 and Appendix A.4 for the attacker and the defender,respectively.Since in our patrolling setting each player has a unique resource, in the following we will use interchangeably the terms‘patroller’ and ‘defender’, and similarly the terms ‘intruder’ and ‘attacker’.N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–12383Fig. 1. The graph representing the patrolling setting used as running example. In each vertex we report: the number of the vertex, the penetration timed(·), and, between parentheses, the value of the defender and of the attacker, respectively.Example 3.1. Fig. 1 depicts a patrolling setting where the circled numbers identify the vertices, arcs are depicted as arrows,and the set of targets is T = {06, 08, 12, 14, 18}; in each target t we report d(t) and (v d(t), v a(t)).3.1.2. The game modelPSGs are defined as two-player multi-stage games with imperfect information and infinite horizon [27]. Each stage ofthe game corresponds to a turn in the patrolling setting in which the defender and the attacker act simultaneously. Thedefender’s available actions are denoted by move( j) where j ∈ V is a vertex adjacent to the patroller’s current one. If actionmove( j) is played at turn k, then at turn k + 1 the patroller visits vertex j and checks it for the presence of the intruder. Theattacker’s available actions are denoted by wait and enter(t) with t ∈ T . Playing action wait at turn k means not to attemptany intrusion for that turn. Playing action enter(t) at turn k means to start an intrusion attempt in target t and prevents theattacker from taking any other action in the time interval {k + 1, . . . , k + d(t) − 1}. Notice that playing enter(t) will makethe game to conclude by d(t) turns. The attacker’s actions are not perfectly observable and thus the defender, when acting,does not know whether or not the intruder is currently within a target. The game has an infinite horizon, since the attackeris allowed to wait indefinitely for attacking.The possible outcomes of the game are: no-attack: when the attacker plays wait at every turn k, i.e., it never attacksany target; intruder-capture: when the attacker plays enter(t) at turn k and the patroller visits target t in the time intervalI = {k, k + 1, . . . , k + d(t) − 1} (and consequently detects the intruder); penetration-t: when the attacker plays enter(t) at turnk and the patroller does not visit target t in the time interval I defined above.Example 3.2. Fig. 2 reports a portion of the game tree of the PSG for the setting of Fig. 1, given that the initial position ofthe patroller is vertex 01. Branches represent actions and players’ information sets are depicted as dotted lines. Each turnof the game corresponds to two levels of the tree, where the defender and the attacker act simultaneously. We assume thatplayers cannot observe each other’s actions in the same turn.84N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Fig. 2. A portion of the game tree for the setting of Fig. 1 (patroller is initially in 01).Agents’ utility functions are defined as follows. The defender’s utility function ud returns the total amount of preservedtargets’ value:ud(x) =(cid:2) (cid:3)(cid:3)i∈T v d(i),i∈T \{t} v d(i),x = intruder-capture or no-attackx = penetration-tNotice that the defender gets the same utility when the intruder is captured and when the intruder never enters. Thisis because, in the case a utility surplus is given for capture, the defender could prefer a lottery between intruder-captureand penetration-t to no-attack. This behavior is not reasonable, since the defender’s primary purpose in a typical patrollingsetting is to preserve as much value as possible and not necessarily capture the intruder.The attacker’s utility function ua returns a penalty in case the intruder is captured, otherwise it returns the value of theattacked target:⎧⎨ua(x) =⎩0,v a(t),−(cid:3),x = no-attackx = penetration-tx = intruder-capturewhere (cid:3) ∈ R+is the penalty. In words, the status quo (i.e., no-attack) is better than intruder-capture for the attacker.We denote by H the space of all the possible histories h of vertices visited by the patroller (or, equivalently, actionsundertaken by the defender). For example, in Fig. 1, given that the patroller starts from vertex 01, a possible history ish = (cid:6)01, 02, 03, 07, 08(cid:7). We define the defender’s strategy (also called patrolling strategy) as σd : H → (cid:5)(V ) where (cid:5)(V ) isa probability distribution over the vertices V . Given a history h ∈ H , the strategy σd gives the probability with which thepatroller will move to vertices at the next turn. The defender’s strategy does not depend on the actions undertaken by theattacker, these being unobservable for the defender.We distinguish between deterministic and non-deterministic patrolling strategies. When σd is in pure strategies, assigninga probability of one to a single vertex for each possible history h, we say that the patrolling strategy is deterministic.Otherwise, the patrolling strategy is non-deterministic.We define the attacker’s strategy as σa : H → (cid:5)(T ∪ {wait}) where (cid:5)(T ∪ {wait}) is a probability distribution over T (or,equivalently, over the corresponding actions enter(t)) and the action wait.Example 3.3. In Fig. 1, a deterministic patrolling strategy could prescribe the cycle (cid:6)04, 05, 06, 11, 18, 17, 16, 10, 04(cid:7), whilea non-deterministic patrolling strategy when the patroller is in vertex 01 after a history h could be:⎧⎨σd(h) =⎩01 with a probability of 0.2502 with a probability of 0.2506 with a probability of 0.5An example of attacker’s strategy is: play action wait for all the histories whose last vertex is not 04 and play enter(18)otherwise.3.2. Solution conceptWe initially discuss the appropriate solution concept when the defender cannot commit to a strategy (Section 3.2.1) andthen we show that committing to a strategy is never worse for the defender (Section 3.2.2).N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123853.2.1. Solution concept in absence of any commitmentWe consider the defender’s strategy in absence of any commitment. The appropriate solution concept for a multi-stagegame with imperfect information is the sequential equilibrium [44], which refines Nash equilibrium.The presence of an infinite horizon complicates the study of the game. With an infinite horizon, classic game theorystudies a game by introducing symmetries, e.g., a player will repeat a given strategy every ¯k turns. Introducing symmetriesin our game model amounts to force the players’ strategies to be defined on histories with a maximum finite length l. As aresult, the strategies are Markovian with a memory of order l.Example 3.4. When l = 0, actions prescribed by the defender’s strategy do not depend on any previous action and the prob-ability to visit a vertex is the same for all the vertices where the patroller is. Notice that imposing l = 0 is not satisfactoryfor non-fully connected graph, where the set of actions available to the defender depends on the current vertex. Whenl = 1, the defender chooses her next action on the basis of her last action (equivalently, the next action depends only onthe current vertex of the patroller). In this case, the patrolling strategy is first-order Markovian.Obviously, when increasing the value of l, the defender’s expected utility cannot decrease, because the defender considersmore information to select her next action. Classical game theory [27] shows that games with infinite horizon admit amaximum length, say l, of the symmetries such that the expected utility does not increase anymore with l (cid:2) l (e.g., l = 0in [55]). Usually, l = 1 [27]. In our model, this means that, when the defender’s strategy is defined on the last l verticesvisited by the patroller, with l (cid:2) l the defender’s expected utility is the same she receives with l = l. Notice that the numberof possible pure strategies σd(h) and σa(h) is O (nl), where n is the number of vertices. Therefore, we expect that, whenincreasing the value of l, the computational complexity for finding a patrolling strategy exponentially increases. In practicalsettings, the selection of a value for l is a trade-off between expected utility and computational effort.3.2.2. Translation to a strategic-form game for a given lGiven a value for l, a PSG can be translated to a strategic-form game with additional constraints over the defender’sstrategies. In the strategic-form game, the defender’s actions are all the feasible probability assignments for {αh,i}, whereis the probability to execute action move(i) given history h. The attacker’s actions are enter-when(t, h), with t ∈ T ,αh,ih ∈ H , and stay-out. Action enter-when(t, h) corresponds to make wait until the patroller has followed history h and thenmake enter(t); stay-out corresponds to make wait forever. The additional constraints, formally defined in Section 3.3.1 below,take into account that the defender’s strategies in the original extensive-form game are repeated every l turns. Notice thatthe game does not depend on the initial vertex of the patroller. This is because the defender’s and attacker’s strategies donot depend on it.Example 3.5. Consider Fig. 1. With l = 1, the available defender’s strategies are all the consistent probability assignments to{αi, j} with i, j ∈ V , while the attacker’s actions are enter-when(t, j) with t ∈ T , j ∈ V and stay-out.It can be easily observed that this reduced game is strategically equivalent to the original game and therefore everyequilibrium of this game is an equilibrium of the original game. Since a Nash equilibrium of a strategic-form game is alsoa sequential equilibrium, we have that a Nash equilibrium in the reduced game is a sequential equilibrium in the originalgame.Since the attacker can wait outside the environment observing the defender’s strategy, the defender’s commitment to astrategy is credible. Therefore, the leader–follower equilibrium is the appropriate solution concept for PSGs. We state thefollowing result, whose proof is an easy application of the result discussed in [64].Proposition 3.6. Given the game described above with a fixed l, the leader never gets worse when committing to a leader–followerequilibrium strategy.3.3. Basic algorithmWe apply the algorithm presented in [20] to our setting: we discuss in Section 3.3.1 how to compute the captureprobabilities under the constraint that the patrolling strategies are repeated every l turns, and in Sections 3.3.2 and 3.3.3how a leader–follower equilibrium can be computed when the game is zero-sum and general-sum, respectively. Then, weshow in Section 3.3.4 that first-order Markovian strategies might not be optimal and we discuss the main limits of theapproach in Section 3.3.5.3.3.1. Computing the intruder capture probabilitiesWe denote by P c(t, h) the intruder capture probability related to action enter-when(t, h), defined as the probability thatthe patroller, starting from the last vertex of h, reaches target t by at most d(t) turns. P c(t, h) depends on {αh,i} in a highlynon-linear way with degree d(t). A bilinear (i.e., a special case of quadratic) formulation for the computation of P c(t, h) canbe provided by applying the sequence-form [42] and imposing constraints over the behavioral strategies. From here on we86N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123consider the formulation with l = 1 (in this case the history h reduces to a single vertex, i.e., h ∈ V ). We define γ w,tasi, jthe probability with which the patroller reaches vertex j in w turns, starting from vertex i and not sensing target t. Theconstraints are:αi, j (cid:2) 0 ∀i, j ∈ V(cid:7)αi, j = 1 ∀i ∈ Vj∈Vαi, j (cid:3) a(i, j) ∀i, j ∈ Vγ 1,ti, jγ w,ti, j= αi, j ∀t ∈ T , i, j ∈ V \ {t}γ w−1,t∀w ∈=i,xαx, j(cid:7)(cid:9)(cid:8)x∈V \{t}P c(t, h) = 1 −(cid:7)j∈V \{t}γ d(t),th, j∀t ∈ T , h ∈ V(cid:10)(cid:11)2, . . . , d(t), t ∈ T , i, j ∈ V \ {t}(1)(2)(3)(4)(5)(6)Constraints (1), (2) express that αi, j are well defined probabilities; constraints (3) express that the patroller can only movebetween two adjacent vertices; constraints (4), (5) express the first-order Markov hypothesis over the defender’s decisionpolicy; constraints (6) define P c(t, h). The bilinearity is due to constraints (5). In the worst case (with fully connectedgraphs), the number of variables αi, j is O (|V |2) and the number of variables γ w,tis O (|T | · |V |2 · maxt{d(t)}), while thei, jnumber of constraints is O (|T | · |V |2 · maxt{d(t)}). As we show in Appendix A.1, the above formulation can be extendedto the case in which l is arbitrary but, in practice, it is intractable, because the number of variables and constraints growsexponentially in l: the number of variables αi, j is O (|V |l+1) and the number of variables γ w,tis O (|T | · |V |2l · maxt{d(t)}),h1,h2while the number of constraints is O (|T | · |V |2l · maxt{d(t)}). (A more efficient formulation, (about) halving the number ofvariables and constraints, is reported in Appendix A.2.)3.3.2. Solving zero-sum patrolling security gamesWhen the defender and the attacker share the same preferences over the targets (i.e., v d(t) = v a(t) for all t ∈ T ) theresulting game is essentially zero-sum. It is not rigorously zero-sum because two outcomes (i.e., intruder-capture and no-attack) provide the defender with the same utility and the attacker with two different utilities (i.e., −(cid:3) and 0, respectively).However, we can temporarily discard the outcome no-attack, assuming that action stay-out will not be played by the attacker.We will reconsider such action in the following. Without the outcome no-attack the game is zero-sum. In this case, thedefender’s leader–follower strategy corresponds to its maxmin strategy, i.e., the strategy that maximizes the defender’sminimum expected utility. We provide a mathematical programming formulation to find it. We introduce the variable u, asthe lower bound over defender’s expected utility.Formulation 3.7. The leader–follower equilibrium of a zero-sum PSG is the solution of:max uconstraints (1), (2), (3), (4), (5), (6)(cid:8)u (cid:3) ud(intruder-capture)P c(t, h) + ud(penetration-t)(cid:9)1 − P c(t, h)∀t ∈ T , h ∈ V(7)Constraints (7) define u as a lower bound on the defender’s expected utility. By solving this problem we obtain the, i.e., the maxmin value. The values of variables {αi, j} corresponding to urepresent the optimalmaximum lower bound upatrolling strategy. The number of constraints (7) is O (|T | · |V |). The formulation is bilinear and cannot be reduced to alinear problem because constraints (5) and (6) are not convex [17].∗∗Now, we reconsider action stay-out and its corresponding outcome no-attack. Easily, from the solution of the above, as the utility of the attacker’s best∗ < 0, then the attacker will play stay-out. Otherwise, shemathematical programming problem, we compute the attacker’s expected utility, say vresponse given the capture probabilities corresponding to uwill play the optimal action prescribed by the above mathematical program.. If v∗∗3.3.3. Solving general-sum patrolling security gamesThe mathematical programming formulation for the general-sum case is an extension of the multi-linear programmingapproach described in [20] (the approach proposed in [50] cannot be adopted here because we would obtain a mixed-integer quadratic problem and, currently, no solver would be able to solve it). In our case, the programming problem is amulti-bilinear one.We define two mathematical programming problems. The first one checks whether or not there exists at least onedefender’s strategy σd such that stay-out is a best response for the attacker. If such a strategy exists, then the defender willfollow it, its utility being maximum for stay-out.N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–12387Formulation 3.8. A leader–follower equilibrium in which the attacker’s best strategy is stay-out exists when the followingmathematical programming problem is feasible:constraints (1), (2), (3), (4), (5), (6)(cid:8)ua(intruder-capture)P c(t, h) + ua(penetration-t)(cid:9)1 − P c(t, h)(cid:3) 0 ∀t ∈ T , h ∈ V(8)Constraints (8) express that stay-out is better than enter-when(t, h) for all t and h. The number of constraints (8) is O (|T | ·|V |).If the above formulation is not feasible, we need to search for the attacker’s best response such that the defender’sexpected utility is the largest. For each action enter-when(s, q) we calculate the optimal defender’s expected utility underthe constraint that such action is the attacker’s best response.Formulation 3.9. The largest defender’s expected utility when attacker’s best response is enter-when(s, q) is the solution of:(cid:8)max ud(penetration-s)(cid:9)1 − P c(s, q)+ ud(intruder-capture)P c(s, q)constraints (1), (2), (3), (4), (5), (6)(cid:8)ua(intruder-capture)(cid:9)P c(s, q) − P c(t, h)(cid:9)(cid:8)1 − P c(t, h)− ua(penetration-t)(cid:2) 0 ∀t ∈ T , h ∈ V(cid:8)+ ua(penetration-s)(cid:9)1 − P c(s, q)(9)The objective function maximizes the defender’s expected utility. Constraints (9) express that no action enter-when(t, h)gives a larger value to the attacker than action enter-when(s, q) (which is assumed to be the best response). The number ofconstraints (9) is O (|T | · |V |).We calculate the patrolling strategies {αi, j} of all the |T | · |V | above mathematical programming problems (one foreach action enter-when(s, q) assumed to be the best response). The leader–follower equilibrium is the strategy {αi, j} thatmaximizes the defender’s expected utility.Example 3.10. We report in Fig. 3 the patrolling strategy corresponding to the leader–follower equilibrium for the setting ofFig. 1. We have omitted all the vertices that are never visited by the strategy. The corresponding attacker’s best response isenter-when(08, 12).3.3.4. Non-optimality of first-order Markovian strategiesThe algorithm for solving PSGs reported in the previous sections has been formulated for l = 1. We showed in [12] that,when the graph representing the environment is fully connected, l = 0 and therefore no strategy with l > 0 is better thanthe optimal strategy with l = 0. The problem of determining l for an arbitrary graph is very complex and largely beyond thescope of this paper. However, an interesting insight on this problem is given by the following proposition, whose proof is inAppendix B.1:Proposition 3.11. There are settings in which first-order Markovian patrolling strategies provide an expected utility strictly smallerthan higher-order Markovian patrolling strategies.The above result entails that, in general, l may be larger than one. We can provide a lower bound over the efficiency ofu∗ the efficiency of a patrolling strategy σ , where u is the patroller’s expected utilityis the patroller’s expected utility of playing the optimal high-order Markovian strategy.first-order Markovian strategies. Call u∗of playing σ and uTheorem 3.12. No topology-independent lower bound over the efficiency of a first-order Markovian leader–follower equilibrium strat-egy σ tighter thani vd(i) can be provided, where u is the patroller’s expected utility of playing σ .u(cid:3)The proof is based on the fact that, given the values of a set of targets, we can always build a topology such that thedeterministic strategy exists.The following theorem shows a lower bound independent of the values of the targets (proof is reported in Appendix B.2).Theorem 3.13. The lower bound over the efficiency of a first-order Markovian leader–follower equilibrium strategy σ is 1asymptotically achieved.2 and can be88N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Fig. 3. Optimal patrolling strategy for Fig. 1 (the omitted vertices and arcs are never covered by the strategy).3.3.5. LimitsThe basic algorithm presented in the previous sections and based on the combination of results presented in [20]and [42] has two main limits.The first limit of the approach is its computational hardness for solving realistically large game instances. In general, solv-ing non-linear mathematical programming problems requires remarkable computational efforts. As we will discuss in ourexperimental evaluations, only small settings (w.r.t. the number of vertices and targets) can be solved with l = 1. Solvingsettings with l > 1 is practically intractable because, as we showed in Section 3.3.1, the number of variables and constraintsrise exponentially with 2l. This fact has two consequences. On the one hand, the limited scalability w.r.t. the settings’ sizeprevents the model from being applied to practical scenarios, even with l = 1. On the other hand, the practical impossi-bility of increasing the value of l precludes the opportunity to find more effective patrolling strategies, whose existence issuggested by Proposition 3.11.To tackle these issues, we propose two approaches. In the first one (Section 4), we limit the generality of the solutionby looking only for deterministic (pure) strategies. We show that the limit on the value of l can be overcome in the specificcase of deterministic strategies. More specifically, if a PSG admits an equilibrium deterministic strategy σd for an arbitraryvalue of l such that the associated intruder’s best response is stay-out, then σd can be efficiently found by exploiting thestructure of the problem, avoiding mathematical programming and reducing the computational burden. This is because thecomputation of equilibrium deterministic strategies is treated separately from the computation of more general equilibriumnon-deterministic strategies. The second approach (Section 5) is based on the idea of simplifying the patrolling settingby introducing a pre-processing phase that eliminates variables and constraints, while preserving the game theoreticalconsistency and the solution optimality. As a consequence, a reduced patrolling setting can be solved more efficiently fornon-deterministic patrolling strategies.The second limit is that the resulting patrolling strategies may be inconsistent. This happens when an attacker’s bestresponse enter-when(t, x) has the property that x is never visited by the patroller after an infinite number of turns. InFig. 4 we report an example of an inconsistent patrolling strategy. The intruder’s best response given the patrolling strategydepicted in figure is enter-when(12, 14), but the probability for the patroller of visiting 14 after an infinite number of turnsis zero.Essentially, a strategy inconsistency is due to the fact that a single patroller cannot patrol effectively all the targets. Formaximizing its expected utility, the defender will patrol only a subset of the (most important) targets, leaving uncoveredN. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–12389Fig. 4. Inconsistent strategy in a zero-sum PSG: some vertices cannot be covered even after an infinite number of turns. For example, vertex 14 cannot bereached from vertex 06.other (less important) targets, and the attacker may prefer to attack a patrolled target rather than a non-patrolled one. InFig. 4, the attacker prefers attacking target 12, even if the capture probability is strictly positive, rather than attacking target14 with a capture probability of zero. An inconsistent patrolling strategy is not practically effective and must be discarded.In Section 6.2, we provide an algorithm that deals with inconsistencies. The algorithms we present in the following sectionsare not influenced by the presence of inconsistencies.4. Finding deterministic patrolling strategies in large gamesIn this section, we describe a method to compute a deterministic patrolling strategy, a problem we initially addressedin [11]. In Section 4.1, we formally state the problem. We discuss its computational complexity in Section 4.2, we determinean upper bound over the solution’s length and we propose a simple but inefficient algorithm in Section 4.3, and we providea more efficient algorithm in Section 4.4.4.1. Problem formulationA deterministic patrolling strategy σd can be conveniently represented as a sequence of vertices, allowing l to be arbi-trary. Apart from degenerate cases due to strategy inconsistencies (discussed in Section 6.2), a leader–follower equilibriumof a PSG can be in pure strategy (or deterministic) if the attacker’s best response is stay-out, otherwise the defender can gainmore by randomizing. By definition, when a deterministic equilibrium strategy is adopted by the patroller, each target t isleft uncovered for a number of turns not larger than its penetration time d(t) and thus every action enter-when(t, j) wouldresult in a capture for the intruder. This kind of solution is close to those produced by the frequency-based approaches, butthese are not applicable when the visit of each specific vertex is subject to specific constraints, as it happens in our case.Without loss of generality, a deterministic strategy can be defined only on targets, assuming that the patroller willmove between two targets along the shortest path. Accordingly, we reduce graph G = (V , A, T , v, d) to a weighted graphGis the set of arcs connecting the targets defined as a function(cid:10)(i, j) = 1 if at least one(cid:10)a(cid:10)(i, j) = 0 otherwise; w is a weight functionof the shortest paths connecting i to j in G does not visit any other target, a(cid:10) = (T , A: T × T → {0, 1} and derived from set A as follows: for every pair of targets i, j ∈ T and i (cid:11)= j, a(cid:10), w, d), where targets T are the vertices of G; A(cid:10)(cid:10)90N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Fig. 5. (a) Reduced graph G(cid:10)corresponding to that of Fig. 1. (b) The same graph as in (a), but with different (relaxed) penetration times.defined as w : T × T → N \ {0} where w(i, j) is the length of the shortest path between i and j in G (w(i, j) is defined(cid:10)(i, j) = 1 and is the number of turns the patroller spends for going from target i to target j); and d is theonly when asame function defined as in G. The reduction from G to Gcan be obtained by applying Dijkstra’s algorithm to every pairof targets in G. For the sake of presentation, in the rest if this section we denote by σ a deterministic patrolling strategyover Gand we refer to vertices of G, instead of targets of G.(cid:10)(cid:10)(cid:10)isExample 4.1. Consider the graph reported in Fig. 1. The corresponding reduced graph Gcomposed of only 5 vertices. (The graph in Fig. 5(b) differs from that in (a) in the values of penetration times; we will useit in a later example.)is reported in Fig. 5(a). G(cid:10)(cid:10)Pure strategy equilibria are usually found by iterating over players’ best responses or by sampling strategy profiles.However, here the problem is different: we know the best response of the intruder, i.e., stay-out, and we need to searchefficiently for the patroller’s best strategy. The application of best response search methods would lead us to enumerateall the possible strategies and to check them one after another. This would be very inefficient, the search space being verylarge. We can provide a more convenient formulation based on constraint programming.We define a function σ : {1, 2, . . . , s} → T that represents a sequence of vertices of G, where σ ( j) is the jth elementof the sequence. The length of the sequence is s and is not known a priori. The temporal length of a sequence of visits iss−1j=1 w(σ ( j), σ ( j + 1)). The time interval between two visits ofcomputed by summing up the weights of covered arcs, i.e.,a vertex is calculated similarly, summing up the weights of the arcs covered between the two visits. A solution is a sequenceσ such that: (i) σ is cyclical, i.e., the first vertex coincides with the last one, namely, σ (1) = σ (s); (ii) every vertex in T isvisited at least once, i.e., there are no uncovered vertices; (iii) when indefinitely repeating the cycle, for any i ∈ T , the timeinterval between two successive visits of i is never larger than d(i).(cid:3)(cid:10)Let us denote by O i( j) the position in σ of the jth occurrence of vertex i and by oi the total number of i’s occurrencesin a given σ . For instance, consider Fig. 5(a): given σ = (cid:6)14, 08, 18, 08, 14(cid:7), O 08(1) = 2 and O 08(2) = 4, while o08 = 2 ando06 = 0. (Notice that, given a sequence σ , quantities O i( j) and oi can be easily calculated.) With such definitions, we canformally re-state the problem in a constraint programming fashion (the presence of highly non-linear constraints makes ithard to resort to integer linear programming formulations, extending the works discussed in Section 2.3).Formulation 4.2. A deterministic patrolling strategy σ such that the intruder’s best response is stay-out is a solution of:σ (1) = σ (s)oi (cid:2) 1 ∀i ∈ T(cid:10)(cid:9)σ ( j − 1), σ ( j)(cid:8)a= 1 ∀ j ∈ {2, 3, . . . , s}(10)(11)(12)N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123O i (k+1)−1(cid:7)j=O i (k)O i (1)−1(cid:7)wj=1(cid:8)(cid:9)σ ( j), σ ( j + 1)w(cid:3) d(i) ∀i ∈ T , ∀k ∈ {1, 2, . . . , oi − 1}(cid:8)(cid:9)σ ( j), σ ( j + 1)+s−1(cid:7)(cid:8)(cid:9)σ ( j), σ ( j + 1)w(cid:3) d(i) ∀i ∈ Tj=O i (oi )91(13)(14)(cid:10)(σ ( j − 1), σ ( j)) = 1, i.e., vertex σ ( j) can be directly reached from vertex σ ( j − 1) in GConstraint (10) states that σ is a cycle, i.e., the first and last vertices of σ coincide; constraints (11) state that every vertexis visited at least once in σ ; constraints (12) state that for every pair of consecutively visited vertices, say σ ( j − 1) andσ ( j), a; constraints (13) statethat, for every vertex i, the temporal interval between two successive visits of i in σ is not larger than d(i); similarly,constraints (14) state that for every vertex i the temporal interval between the last and first visits of i is not larger thand(i), i.e., the deadline of i must be respected also along the cycle closure.(cid:10)Example 4.3. Consider the graph of Fig. 5(a), no sequence σ of visits satisfies all the constraints listed above. Indeed, theshortest cycle covering only vertices 06 and 08, i.e., (cid:6)06, 08, 06(cid:7), has a temporal length larger than the penetration times ofboth the involved vertices, so there is no way to cover these vertices (and others) within their penetration times. As we willshow below, the graph of Fig. 5(b) admits a deterministic equilibrium strategy.4.2. NP-completenessCall DET-STRAT the problem of deciding if a deterministic patrolling strategy such that the intruder’s best response is(cid:10).stay-out, as defined in the previous section, exists in a given GTheorem 4.4. The DET-STRAT problem is NP-complete.The proof, reported in Appendix B.3, shows that the Direct Hamiltonian Circuit problem can be reduced to the DET-STRATproblem. Although the DET-STRAT is a hard problem, we will show that it is possible to design a constraint programmingbased algorithm able to efficiently compute a solution for settings composed of a large number of targets.4.3. An upper bound on the solution length and a simple algorithmThe peculiarity of the problem stated in Formulation 4.2 is that the length of the solution s and the number of oc-currences oi of vertex i are not known a priori, but they are part of the solution. The common approach adopted in theconstraint programming literature to tackle problems with an arbitrary number of variables involves two phases: initiallyanalytical bounds over the number of the variables are derived and then a set of problems, each one with the number ofvariables fixed to a value within the bounds, are solved. Although our problem resembles problems of cyclical CSP-basedscheduling (e.g., [21]), to the best of our knowledge, the situation where the number of variables is part of the solutionitself is still unaddressed. We derive a non-trivial upper bound over the temporal length of the solution.Theorem 4.5. If an instance of Formulation 4.2 is feasible, then there exists at least a solution σ with temporal length no longer thanmaxt∈T {d(t)}.We report the proof in Appendix B.4. Exploiting Theorem 4.5, upper and lower bounds for the solution length s can bederived. They are defined respectively as s = (cid:12) maxt∈T {d(t)}mini, j {w(i, j)} (cid:13) and s = |T | + 1. Once we have fixed a value for s, upper andlower bounds over the number of occurrences of each vertex t are ot = s − |T | + 1 and ot = 1 respectively. By using thesebounds we can use Algorithm 1 to solve an instance of Formulation 4.2.Algorithm 1: simple_det-strat1 for all the s in {s, s + 1, . . . , s} do2345assign σ ← C S P (s, o)if σ is not empty thenreturn σfor all the o = (o(1), . . . , o(|T |)) in {1, 2, . . . , s − |T | + 1}|T |do6 return failureThe call to CSP(s, o) solves a standard constraint programming problem where the value of s and the number of tar-gets’ occurrences are fixed. This task can be easily accomplished with commercial CP solvers [37]. Despite its simplicity92N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123and the possibility to use off-the-shelf solvers, Algorithm 1 is not efficient and requires a long time even for simplepatrolling settings because it requires the resolution of many constraint programming problems and, for most of them,CSP(s, o) explores the whole search space, which is exponential in the worst case. This pushes us to design an ad hocalgorithm.4.4. Solving algorithmWe present our basic algorithm in Section 4.4.1, we report an execution example in Section 4.4.2, and we show how toimprove it in Section 4.4.3.4.4.1. Basic algorithmWe consider each σ ( j) as a variable with domain F j ⊆ T . The constraints over the values of the variables are (10)–(14).We search for an assignment of values to all the variables such that all the constraints are satisfied. Our algorithm basicallysearches the state space with backtracking exploiting forward checking [57] in the attempt to reduce the branching of thesearch tree. Despite its simplicity, it is very efficient in practice. We report our method in Algorithms 2, 3, and 4.Algorithm 2 assigns σ (1) a vertex i ∈ T . Since the solution σ is a cycle that visits all vertices, every vertex can beassigned to σ (1) without affecting the possibility to find a feasible solution.Algorithm 2: find_solution(T , A(cid:10), w, d)1 select a vertex i in T2 assign σ (1) ← i3 call recursive_call(T , A(cid:10), w, d, σ , 2)Algorithm 3 assigns σ ( j) a vertex from domain F j ⊆ T , which contains available values for σ ( j) that are returned bythe forward checking algorithm (Algorithm 4). If F j is empty or no vertex in F j can be successfully assigned to σ ( j), thenAlgorithm 3 returns failure and a backtracking is performed.Algorithm 3: recursive_call(T , A1 if σ (1) = σ ( j − 1) and constraints (11) hold then23if constraints (14) hold thenreturn σ(cid:10), w, d, σ , j)else45return failure6 else789101112assign F j ← forward_checking(T , Afor all the i in F j doassign σ ( j) ← iassign σ (cid:10) ← recursive_call(T , Aif σ (cid:10)is not failure thenreturn σ (cid:10)(cid:10), w, d, σ , j)(cid:10), w, d, σ , j + 1)13return failureAlgorithm 4 restricts F j to the vertices that are directly reachable from the last assigned vertex σ ( j − 1) and such thattheir visits do not violate constraints (13)–(14). Notice that checking constraints (13)–(14) requires knowing the weights(temporal costs) related to the arcs between vertices that could be assigned subsequently, i.e., between the variables σ (k)with k > j. For example, consider the graph of Fig. 5(b) and suppose that the partial solution currently constructed bythe algorithm is σ = (cid:6)14(cid:7). In this situation, we cannot check the validity of constraints (13)–(14) since we have no in-formation about times to cover the arcs between the vertices that will complete the solution. Therefore, we estimate theunknown temporal costs by employing an admissible heuristic (i.e., a non-strict underestimate) based on the minimumcost between two vertices. The heuristic being admissible, no feasible solution is discarded. We denote the heuristic valueby w, e.g., w(i, σ (1)) denotes the weight of the shortest path between i and σ (1). We assume w(i, i) = 0 for any ver-tex i.Given a partial solution σ from 1 to j − 1, the forward checking algorithm considers all the vertices directly reach-able from σ ( j − 1) and keeps those that do not violate the relaxed constraints (13)–(14) computed with heuristic values.More precisely, it considers a vertex i directly reachable from σ ( j − 1) and assumes that σ ( j) = i. Step 5 of Algo-rithm 4 checks relaxed constraints (14) with respect to i, assuming that the weight along the cycle closure from σ ( j) = ito σ (1) is minimum. In the above example, with σ (1) = 14, the vertices directly reachable from σ (1) are 08 and 18.The algorithm considers σ (2) = 08. By Step 5, we have w(σ (1), 08) + w(08, σ (1)) = 4 (cid:3) d(08) = 18 and then Step 5N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–12393is satisfied. It can be easily observed that such condition holds also when σ (2) = 18. Step 8 of Algorithm 4 checks re-laxed constraints (14) with respect to all the vertices k (cid:11)= i, assuming that both the weight to reach k from σ ( j) = iand the weight along the cycle closure from k to σ (1) are minimum. Consider again the above example. It can beeasily observed that when σ (2) = 08 such conditions hold for all k. Instead when σ (2) = 18 and k = 06, we havew(σ (1), 18) + w(18, 06) + w(06, σ (1)) = 16 > d(06) = 14. The relaxed constraint is violated and vertex 18 will not beinserted in F j . Similarly, Step 6 checks relaxed constraints (13) with respect to i and Step 9 checks relaxed constraints (13)with respect to any k assuming that the weight to reach k from σ ( j) = i is minimum. In the above example, starting fromσ = (cid:6)14(cid:7), the relaxed constraints are satisfied only when i = 08 and therefore F j = {08}. Finally, we notice that Steps 5and 8 are checked only when oi = 0 and ok = 0, respectively, since it can be easily proved that when oi > 0 and ok > 0these conditions always hold.Algorithm 4: forward_checking(T , A1 assign F j ← ∅2 assign s ← j − 13 for all members i in T such that a4(cid:10)(σ (s), i) = 1 do(cid:10), w, d, σ , j)if conditions(cid:3)s−1(oi = 0 ∧l=1 w(σ (l), σ (l + 1)) + w(σ (s), i) + w(i, σ (1)) (cid:2) d(i) or(cid:3)s−1oi > 0 ∧l=O i (oi ) w(σ (l), σ (l + 1)) + w(σ (s), i) (cid:2) d(i)) and,for all k (cid:11)= i,(cid:3)(ok = 0 ∧(cid:3)ok > 0 ∧hold thens−1l=1 w(σ (l), σ (l + 1)) + w(σ (s), i) + w(i, k) + w(k, σ (1)) (cid:2) d(k) ors−1l=O k (ok ) w(σ (l), σ (l + 1)) + w(σ (s), i) + w(i, k) (cid:2) d(k))add i to F j56789101112 return F jWe state the following theorem, whose proof is in Appendix B.5.Theorem 4.6. The above algorithm is sound and complete.4.4.2. ExampleWe apply our algorithm to the example of Fig. 5(b). We perform a random selection in Step 1 of Algorithm 2 (tochoose the first visited vertex of the sequence) and in Step 7 of Algorithm 3 (to choose the elements of F j as part ofthe current candidate solution). We report part of the execution trace (Fig. 6 depicts the complete search tree). (a) Thealgorithm assigns σ (1) = 14. (b) The domain F 2 (depicted in the figure between curly brackets beside vertex σ (1) = 14) isproduced (according to the discussion of the previous sections) as follows: vertex 08 is added to F 2, since all the conditionsin Algorithm 4 with i = 08 are satisfied; vertex 18 is not added to F 2, since the condition in Step 8 of Algorithm 4 withk = 06 is not satisfied, formally, w(14, 18) + w(18, 06) + w(06, 14) > d(06); no other vertex is added to F 2, since no othervertex is directly reachable from 14. (c) The algorithm assigns σ (2) = 08. (d) The domain F 3 is produced similarly as above,yielding to F 3 = {06}. (e) The algorithm assigns σ (3) = 06 and continues.Some issues are worth noting. In the 10th node of the search tree, a sequence σ with σ (1) = σ (s) and including all thevertices was found. However, this sequence does not satisfy constraints (14). If the search is not stopped and backtrackedat the 10th node (in Step 5 of Algorithm 3), the algorithm would never terminate. Indeed, the subtrees that would followthis vertex would be the infinite repetition of part of the tree already built. Finally, in the 6th node, no possible successoris allowed by the forward checking, and therefore the algorithm backtracks.4.4.3. Improving efficiency and heuristicsOur algorithm can be improved as follows. Consider the conditions in Steps 5 and 8 of Algorithm 4. Except for the firstexecution of Algorithm 4 (i.e., when j = 2), the satisfaction of the condition at Step 5 for a given j is guaranteed if thecondition in Step 8 for j − 1 is satisfied. Therefore, we can safely limit the algorithm to check the conditions at Step 5exclusively when j = 2. The same considerations hold also for the conditions in Steps 6 and 9. Therefore, we can safelylimit the algorithm to check the conditions at Steps 6 and 9 exclusively when j = 2.(cid:3)We introduce a more sophisticated stopping criterion called LSC (Length Stopping Criterion) based on Theorem 4.5 ands−1l=1 w(σ (l), σ (l + 1)) + w(σ (s), σ (1)) > maxt∈T {d(t)}, then the search is stopped and backtracked. We alsosuch that ifintroduce an a priori check (IFC, Initial Forward Checking): before starting the search, we consider each vertex as the rootnode of the search tree and we apply the forward checking. If at least one domain is empty, the algorithm returns failure.Otherwise, the tree search is started.Finally, we propose some heuristic criteria for choosing from set F j the next vertex to expand in Step 8 of Algorithm 3:lexicographic (hl), random with uniform probability distribution (hr ), maximum and minimum number of incident arcs(hmax a and hmin a), less visited (hmin v ), and maximum and minimum penetration time (hmax d and hmin d). For all the ordering94N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Fig. 6. Search tree for the example of Fig. 5(b); bold nodes and arrows denote the obtained solution; F j s are reported besides nodes σ ( j − 1); xth denotesthe order in which the tree’s nodes are analyzed.criteria except hr , we use a criterion for breaking ties that randomly selects a vertex with a uniform probability (RTB,Random Tie Break). We can use the same heuristics also for selecting the initial node of the search tree in Step 1 ofAlgorithm 2. In Section 7, we will experimentally evaluate these heuristics.5. Finding non-deterministic patrolling strategies in large gamesIn this section, we describe techniques to reduce game instances to make the computation of non-deterministic patrollingstrategies affordable for large games. In Section 5.1 we present some algorithms to remove agents’ dominated strategies. InSection 5.2 we discuss how to compute utility lossless abstractions and in Section 5.3 how to compute abstractions withutility loss.5.1. Removing dominated strategiesAction a dominates action b when the expected utility of playing a is larger than that of playing b independently ofthe actions played by other agents and, therefore, no rational agent will play a. Removing dominated actions allows oneto obtain an equivalent (with the same equilibria) but smaller game with a consequent reduction of the computationaltime needed for its resolution. We present two techniques to remove the defender’s and attacker’s dominated actions inSection 5.1.1 and Section 5.1.2, respectively, and then we discuss the possibility to iterate the removal in Section 5.1.3.5.1.1. Removing defender’s dominated actionsA defender’s action move( j) is dominated when, if such action is removed from the set of defender’s available actionsand thus the defender cannot visit vertex j, its expected utility does not decrease. This happens when, after removingmove( j), no capture probability P c(t, i) ∀t ∈ T , i ∈ V \ { j} (i.e., for each attacker’s strategy) decreases. Practically, removingmove( j) means removing vertex j and all its incident arcs from G.Defender’s dominated actions are identified in two steps. The first one focuses on vertices and corresponding incidentarcs and is based on the following theorem, whose proof is reported in Appendix B.6.Theorem 5.1. Visiting a vertex that is not on any shortest path between any pair of targets is a dominated action.N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–12395Fig. 7. Graph Gr for the patrolling setting of Fig. 1, obtained by removing the defender’s dominated strategies.When there are multiple shortest paths connecting the same pair of targets (t1, t2), visiting each vertex of some of themcan be a dominated action. We state the following theorem, whose proof is reported in Appendix B.7.Theorem 5.2. Given targets t1 and t2 and two shortest paths P = (cid:6)t1, . . . , pi, . . . , t2(cid:7) and Q = (cid:6)t1, . . . , qi . . . , t2(cid:7) of length L betweenthem, if for all k ∈ {2, . . . , L − 1} and t ∈ T \ {t1, t2} we have dist(pk, t) (cid:2) dist(qk, t), then visiting each internal vertex of P (i.e., all piexcluding t1 and t2) is dominated.The first step identifies actions that are dominated independently of the current vertex of the patroller. If move( j) isdominated, then the patroller should not visit j from any adjacent vertex.In the second step we account for the current vertex occupied by the patroller by considering all the defender’s actionsmove( j), when the current vertex is i. We state the following theorem, whose proof is in Appendix B.8:Theorem 5.3. If the patroller is in vertex i ∈ V \ T , remaining in the same vertex i for a further turn is a dominated action.The application of Theorem 5.3 allows us to remove all the self-loops of V \ T . No more defender’s strategies can beremoved independently of the attacker’s strategy, otherwise the capture probabilities and the defender’s expected utilitycould decrease. Therefore, the above theorems allow one to remove all the defender’s dominated strategies.We call Gr = (V r, Ar, T , v, d) the reduced graph produced by removing from G all the vertices and arcs according toTheorems 5.1, 5.2, and 5.3. From here on, we work on Gr , instead of G.Example 5.4. We report in Fig. 7 the graph Gr for our running example of Fig. 1 after having removed the vertices and arcscorresponding to the defender’s dominated strategies.5.1.2. Removing attacker’s dominated actionsAttacker’s action enter-when(t, i) is dominated by action enter-when(s, j) if EUa(enter-when(t, i)) (cid:3) EUa(enter-when(s, j))for every (mixed) strategy σd, where EUa(·) is the attacker’s expected utility. Checking whether an action is dominated canbe formulated as an optimization problem.96N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Formulation 5.5. Action enter-when(t, i) is dominated by enter-when(s, j) if the result of the following mathematical pro-gram is not strictly positive:max μconstraints (1), (2), (3), (4), (5), (6)(cid:8)ua(penetration-t)(cid:9)1 − P c(t, i)(cid:8)+ ua(intruder-capture)(cid:8)− ua(penetration-s)= μ(cid:9)P c(t, i) − P c(s, j)(cid:9)1 − P c(s, j)(15)(16)Constraints (16) define μ as a lower bound on the difference between the expected utilities of actions enter-when(t, i)and enter-when(s, j). μ corresponds to the maximum achievable difference and therefore, if not positive, enter-when(t, i) isdominated by enter-when(s, j). The above problem has (asymptotically) the same number of constraints of Formulation 3.7.The non-linearity, the size of each problem, and the large number of problems to be solved (one for each pair of actions),make Formulation 5.5 computationally expensive for the removal of the attacker’s dominated actions. However, by exploitingthe problem structure, we can design a more efficient algorithm. Initially, we state the following theorem that provides twonecessary and sufficient conditions for dominance (we exploit fully mixed strategies in which every action is played withstrictly positive probability to remove even weakly dominated strategies); the proof is in Appendix B.9.Theorem 5.6. Action enter-when(t, i) is dominated by enter-when(s, j) if and only if for all fully mixed strategies σd it holds that:(i) ua(penetration-t) (cid:3) ua(penetration-s) and(ii) P c(t, i) > P c(s, j).Now we provide an efficient algorithm that removes dominated actions by using conditions (i) and (ii) of Theorem 5.6.We report it as Algorithms 5 and 6. The algorithm builds trees where each node q contains a vertex η(q). For each target t,(cid:10))such that: η(qwe build a tree of depth d(t) where the root is t and the successors of a node q are all the nodes q(cid:10)) is different both from η(q) and from the vertex contained by theis adjacent to η(q) (i.e., a(η(q), η(q(cid:10)) (cid:11)= η(father(q))). We introduce the set domination(t, v) containing all vertices i such thatfather of q (i.e., η(qenter-when(t, i) is dominated by enter-when(t, v). We build this set iteratively by initially setting domination(t, v) = V forall t ∈ T , v ∈ V and, every time a node q is explored, updating it as follows:(cid:10))) = 1) and η(q(cid:10)) (cid:11)= η(q), η(q(cid:10)domination(cid:8)(cid:9)t, η(q)= domination(cid:8)(cid:9)t|η(q)(cid:10)(cid:11)η(p), p ∈ predecessors(q)∩where predecessors(q) is the set of predecessors of q. After the construction of the tree with root t, domination(t, v) contains(cid:10)). This is because, to reach t from v by d(t) turns the patrollerall (and only) the vertices v(cid:10)) · φ <must always pass though v(cid:10)) is (weakly) dominated byP c(t, venter-when(t, v).(cid:10)(cid:10) ∈ domination(t, v) and therefore, by Markov chains with perturbation, P c(t, v) = P c(t, v(cid:10)) with φ < 1. Thus, conditions (i) and (ii) of Theorem 5.6 being satisfied, enter-when(t, vsuch that P c(t, v) < P c(t, vUsing the trees of paths we identify dominations within the scope of individual targets. However, dominations can existalso between actions involving different targets. To find them, we set:(cid:10)(cid:10)(cid:9)(cid:8)t(cid:10)tabu(t) =v ∈ V s.t. ∃t, ua(penetration-t) (cid:3) uafor all t ∈ T . tabu(t) contains all the vertices v such that there exists a pair t(cid:10), vua(penetration-t(cid:10)). We setis dominated by enter-when(tpenetration-t(cid:10) ∈ V with ua(penetration-t) (cid:3)(cid:10) (cid:11)= t, v(cid:10) ∈ T , t(cid:10)) and then, conditions (i) and (ii) of Theorem 5.6 being satisfied, enter-when(t, v)(cid:10)) and P c(t, v) > P c(t, t ∈ domination, v(cid:8)(cid:9)(cid:11)(cid:10)(cid:10), v(cid:2) (cid:12)(cid:13)nondominated(t) = V \domination(t, v) ∪ tabu(t)for all t ∈ T . All (and only) the actions enter-when(t, i) such that i ∈ nondominated(t) are not dominated. We state thefollowing theorem, whose proof is trivial due to the construction of the algorithm.v∈VAlgorithm 5: intruder_domination1 for each t ∈ T dotabu(t) = {}2for each v ∈ V do34domination(t, v) = Vexpand(t, t, {t}, 0)56 for each t ∈ T do78tabu(t) = {v ∈ V | ∀t ∃tnondominated(t) = V \ {(cid:10), t ∈ domination(t(cid:14)(cid:10), v), ua(penetration-t) (cid:2) ua(penetration-t(cid:10))}v∈V \{t} domination(t, v) ∪ tabu(t)}N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–12397Algorithm 6: expand(v, t, B, depth)1 N = { f | father(v) (cid:11)= η( f ) (cid:11)= v, a(η( f ), v) = 1}2 for each f ∈ N do3domination(t, η( f )) = domination(t, η( f )) ∩ η(B)4 if depth < d(t) then56for each f ∈ N doexpand( f , t, {B ∪ f }, depth + 1)Fig. 8. Search tree for finding dominated actions for target 06 of Fig. 7, white nodes constitute the nondominated(06) set.Theorem 5.7. Algorithms 5 and 6 are sound and complete.The worst-case computational complexity is O (|T | · bmaxt {d(t)}), where b is largest outdegree of the vertices. Although thecomplexity is exponential in maxt{d(t)}, in practice the computational time is negligible even for large patrolling settings,as we will show in Section 7.Example 5.8.when(06, 13) is dominated since every occurrence of 13 in the search tree has a node with 14 as child.In Fig. 8, black nodes denote vertices i such that actions enter-when(06, i) are dominated; e.g., enter-Finally, on the basis of the result of Algorithms 5 and 6, we can discard some targets if they appear only in dominatedactions.Corollary 5.9. Target t ∈ T such that the actions enter-when(t, i) for all i are dominated will never be entered by the intruder and thencan be discarded.5.1.3. Iterated dominanceAfter the removal of defender’s and attacker’s dominated strategies (in this order), we can only remove some otherdefender’s dominated strategies. We state the following theorem, whose proof is reported in Appendix B.10.Theorem 5.10. Assigning a positive probability to αt,t with t ∈ T is a dominated action if the attacker’s action enter-when(t, t) isdominated.98N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Fig. 9. Abstraction over vertices 01, 03.No more steps of iterated dominance are possible because, after the removal of the arcs prescribed by Theorem 5.10,the attacker’s dominated strategies do not change.1 We remark that, by resorting to the concept of never best response [60],additional attacker’s actions can be removed. However, differently from what happens for removal of dominated actions, toremove never best responses we cannot avoid using non-linear mathematical programming. As a result, removing a neverbest response has the same computational complexity of solving an instance of Formulation 3.9.5.2. Utility lossless abstractionsAlthough the removal of dominated strategies greatly reduces the computational time, the resolution of realistic-sizegames is still hard (as we show in Section 7). An effective technique that has received a lot of attention in the literature todeal with large games is strategy abstraction [30,31]. In this section we apply utility lossless abstractions to PSGs. We intro-duce the definition of abstractions in Section 5.2.1, we define a general class of utility lossless abstractions in Section 5.2.2,and we discuss how they can be applied to a PSG in Section 5.2.3.5.2.1. Abstraction definitionThe basic idea behind abstractions is to group together multiple actions into a single macro action. This allows oneto reduce the size of the game. The most interesting kind of abstractions are those without loss of utility (also calledinformation lossless), allowing one to find the optimal solution of a game by solving the abstracted one. A number of workson abstractions have been devoted to extensive-form games with imperfect information and, in particular, to poker games[30,31]. However, the application of the seminal result in [30] to a PSG produces a game that is exactly the same size asthe original one, because PSGs are general-sum and without chance moves. This pushes us to define ad hoc abstractions.Definition 5.11. An abstraction over a pair of non-adjacent vertices i, j is a pair of defender’s macro actions move-along(i, j)and move-along( j, i) with the following properties2:(a) when the defender makes macro action move-along(i, j), the patroller moves from the current vertex i to vertex j alongthe shortest path visiting turn by turn the vertices composing the path,(b) the completion of move-along(i, j) requires a number of turns equal to the length of the shortest path between i and j,(c) during the execution of a macro action the defender cannot take other actions,(d) the attacker can intrude a target during the defender’s execution of a macro action.Example 5.12. Consider Fig. 9. By applying an abstraction over vertices 01, 03 we remove the arcs labelled with α01,02,α02,01, α02,03, α03,02 (where αi, j corresponds to action move( j) from i) and we introduce the arcs labelled with α01,03,α03,01. When the patroller is in 01 and goes to 03, it will spend two turns, during which it moves from 01 to 02 (first turn)and from 02 to 03 (second turn). When the patroller is in 02, it cannot stop the execution of the current macro action andmake another action.Definition 5.13. An abstraction over G is the result of the application of some abstractions over pairs of vertices. We obtainit by removing from G some disjoint connected subgraphs G(cid:10) ⊂ G and introducing in G for each G:(cid:10)(a) a set of arcs {(i, j)}, where i, j are vertices in G \ Gcorresponds to a macro action move-along(i, j)),(cid:10)and both i and j are adjacent to vertices in G(cid:10)(each arc (i, j)(b) a function e : V × V → N assigning each arc the time needed by the patroller for traversing it.1 We notice that, after the removal of attacker’s dominated actions, we can discover that some targets will never be entered by the attacker. However, inour case, these targets are on some shortest paths connecting other targets and therefore they cannot be removed.2 For the sake of presentation, we consider a situation in which the two vertices are connected by a single shortest path. If this is not the case, we candefine a pair of macro actions for each shortest path between two vertices.N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–12399Fig. 10. An example of abstraction.Thus, an abstraction over G involves a number of abstractions over pairs of (non-adjacent) vertices.Example 5.14. We report an example of abstracted G in Fig. 10.The main problem to address is the selection of the vertices to be removed such that the obtained abstracted settingpreserves the equilibrium strategies.5.2.2. Defining utility lossless abstractionsWhen the patroller moves along an abstracted arc (i, j), the attacker can take advantage, because it knows some of thenext defender’s moves.Example 5.15. Consider Fig. 9. If the defender decides to move from 01 to 03 and the attacker observes the patroller in 02after having observed it in 01, then the intruder knows that the patroller will reach 03 at the next turn.We produce utility lossless abstractions such that the set of attacker’s dominated strategies (computed as discussed inSection 5.1.2) is an invariant, namely they are left unchanged by the application of abstractions. This is motivated by thefollowing theorem, whose proof is trivial and then omitted.Theorem 5.16. A necessary condition for an (ex ante) abstraction to be without utility loss is that the set of attacker’s dominatedstrategies is invariant.We provide some necessary conditions for a vertex to be removed without changing the set of attacker’s dominatedstrategies.Corollary 5.17. The removal of a vertex i during the application of an abstraction can be without utility loss if :(a) when i /∈ T , for all t ∈ T , actions enter-when(t, i) are dominated,(b) when i ∈ T , for all t ∈ T , actions enter-when(t, i) are dominated and, for all j ∈ V , actions enter-when(i, j) are dominated.It is not enough to assure that the set of attacker’s dominated strategies does not change, because we need to assurethat solving the abstracted game we can find a strategy no worse than the optimal strategy in the non-abstracted game.(cid:10)) is not dominated and dominates enter-when(t, i) (asWe denote by dom(i, t) the set of vertices icalculated in Section 5.1.2). We state the following theorem, whose proof is reported in Appendix B.11.such that enter-when(t, i(cid:10)Theorem 5.18. Given an abstraction over G, if, for all the abstractions over pairs of vertices i, j and for all vertices k on the shortestpath connecting i and j:(a) dist(i, dom(k, t)) (cid:2) dist(i, k) and(b) dist( j, dom(k, t)) (cid:2) dist( j, k)for all targets t ∈ T , then the set of attacker’s dominated strategies is invariant and solving the abstracted game gives a strategy as goodas the optimal strategy for the original game.100N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Notice that, after having abstracted a PSG by using our utility lossless abstractions, we can directly solve it withoutremoving other attacker’s dominated strategies, these being invariant w.r.t. the non-abstracted game. General abstractionsstronger (in terms of the number of removed vertices) than those described in the above theorem can be provided inspecific cases, but their computation is not efficient. For instance, abstractions that take the set of never best responses asan invariant can be stronger, but they require the use of non-linear mathematical programming.5.2.3. Computing utility lossless abstractionsCall C ⊂ V the set of vertices that satisfy Corollary 5.17. We introduce the binary variables xi ∈ {0, 1} with i ∈ C , wherexi = 1 means that vertex i is removed by a utility lossless abstraction and xi = 0 means that it is not. We introduce theinteger variables si,t ∈ {0, . . . , n} with i ∈ V and t ∈ T (n is the number of vertices in V ), where si,t gives the distancebetween vertex i and target t once abstractions have been applied. We call succ(i, j) the set of vertices adjacent to i in theshortest paths connecting i and j.Formulation 5.19. An abstraction is without utility loss if the following integer linear mathematical programming formula-tion associated with the abstraction is feasible:si,t = dist(i, t) ∀i /∈ C, t ∈ Tsi,t (cid:2) dist(i, t) ∀i ∈ C, t ∈ Tsi,t (cid:3) dist(i, t) + nxi ∀i ∈ C, t ∈ Tsi,t (cid:3) s j,t + 1 − n(1 − xi) ∀i ∈ C, t ∈ T , j ∈ succ(i, k), k ∈ dom(i, t)si,t (cid:2) s j,t + 1 + n(1 − xi) ∀i ∈ C, t ∈ T , j ∈ succ(i, k), k ∈ dom(i, t)si,t (cid:3) dist( j, t) ∀i ∈ C, t ∈ T , j ∈ dom(i, t)(17)(18)(19)(20)(21)(22)Constraints (17) force si,t to be equal to the distance between i and t (for all the non-removable vertices i); constraints (18)force si,t to be equal to or larger than the distance between i and t (for all the removable vertices i); constraints (19)force si,t to be equal to the distance between i and t if xi = 0 (for all the removable vertices i); constraints (20) andconstraints (21) force si,t to be equal to s j,t + 1 with j ∈ succ(i, k) where k ∈ dom(i, t) if xi = 1 (for all the removablevertices i); constraints (22) force si,t to be not larger than dist( j, t) with j ∈ dom(i, t).The above formulation allows us to check whether or not an abstraction is without loss of utility. We are now interestedin finding the strongest abstraction that produces the game that requires the minimum computational effort to be solved,namely the game with the minimum number of α variables (arcs). Call φi the outdegree of vertex i. By removing vertex ifrom the graph we remove 2φi arcs (corresponding to 2φi variables α) and we introduce φi(φi − 1) new arcs (correspondingto φi(φi − 1) new variables α).3 In practice, we can reduce the number of variables only if φi (cid:3) 3.Formulation 5.20. The strongest utility lossless abstraction is obtained as the solution of the following integer linear opti-mization mathematical programming problem:(cid:7)maxxii∈Cxi = 0 ∀i ∈ C, φi > 3constraints (17), (18), (19), (20), (21), (22)(23)(cid:10)We call Athe set of arcs of the abstracted game and we represent A: V × V → {0, 1}. An abstractedgame, presenting arbitrary (larger than one) weights, cannot be solved by using Formulation 3.7. An extension of suchformulation working with arbitrary weights is presented in Appendix A.4. Notice that the above result implicitly shows thatwe cannot discard all the non-targets vertices.with function a(cid:10)(cid:10)5.3. Utility loss abstractionsThe application of utility lossless abstractions has the potential to drastically reduce the size of PSGs making theircomputation more affordable. However, for very large games (especially those containing cycles), utility lossless abstractionsproduce abstracted games that are still hard to solve. For these games, we can relax the utility lossless constraints toproduce reduced games whose solutions are not guaranteed to be optimal for the non-abstracted game.3 Rigorously speaking by removing vertex i we remove also a number of variables γ ; however, we experimentally observed that the computational effortdepends more strongly on the number of variables α than on the number of variables γ .N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123101While utility lossless abstractions produce a game in which the set of attacker’s dominated strategies is invariant, withutility loss abstractions we produce a game in which a weaker condition holds: each target is not exposed. More precisely, wesay that a target t is exposed when there is an action enter-when(t, x) such that the related capture probability is zero. Thishappens when the vertex x can be visited by the patroller and dist(t, x) > d(t). Essentially, finding utility lossless abstractionsand utility loss abstractions is conceptually similar, the main difference being in the definition of the upper bound on si,t :when abstractions are without utility loss, we need that si,t be not larger than the maximum distance between dom(i, t)and t, instead, when abstractions are with utility loss, we need that si,t be not larger than d(t). This kind of utility lossabstractions is the strongest one. Indeed, removing further vertices would introduce a delay that makes a target exposedand therefore would make the associated capture probability equal to zero. All the candidates C that can be removed areall the vertices except the targets.Formulation 5.21. An abstraction is with utility loss if the following integer linear mathematical program associated withthe abstraction is feasible:constraints (17), (18), (19)si,t (cid:3) s j,t + 1 − n(1 − xi) ∀i ∈ C, t ∈ T , j ∈ succ(i, t)si,t (cid:2) s j,t + 1 + n(1 − xi) ∀i ∈ C, t ∈ T , j ∈ succ(i, t)si,t (cid:3) dist(i, t) + 1 − n(1 − xi) ∀i ∈ C, t ∈ T , succ(i, t) = ∅, ∃k, a(i, k) = 1, dist(k, t) = dist(i, t)si,t (cid:2) dist(i, t) + 1 + n(1 − xi) ∀i ∈ C, t ∈ T , succ(i, t) = ∅, ∃k, a(i, k) = 1, dist(k, t) = dist(i, t)si,t = dist(i, t) ∀i ∈ C, t ∈ T , succ(i, t) = ∅, ∀k, a(i, k) = 1, dist(k, t) < dist(i, t)si,t (cid:3) d(t) ∀i ∈ C, t ∈ T(24)(25)(26)(27)(28)(29)Constraints (24), (25), and (29) relax the corresponding (utility lossless) constraints (17), (18), and (22) considering directlytarget t instead of the set dom(i, t). Constraints (26), (27), and (28) are analogous to constraints (24) and (25), but they areapplied when, for a given vertex i and a target t, there is not any successor (i.e., succ(i, t) = ∅). This happens in the presenceof cycles and precisely when i is the farthest vertex from t. Constraints (26) and (27) are applied when there exists a vertexk that is as far as i from t, while constraints (28) are applied when x is the farthest vertex.As we did for utility lossless abstractions, we search for the strongest abstractions that produce the smallest game.Formulation 5.22. The strongest utility loss abstraction for a PSG is obtained as the solution of the following integer linearoptimization mathematical programming problem:(cid:7)maxxii∈C, φi (cid:2)3constraints (17), (18), (19), (24), (25), (26), (27), (28), (29)As in the previous section, we call AV → {0, 1}.(cid:10)the set of arcs of the abstracted games and we represent A(cid:10)with function a(cid:10): V ×Example 5.23. In Fig. 11 we report the setting obtained after the application of the strongest utility loss abstraction on thesetting of Fig. 1.The application of utility loss abstractions produces a reduced game whose attacker’s dominated strategies are poten-tially different from those in the original game. Furthermore, Algorithms 5 and 6 cannot be applied to the abstracted gamebecause they do not consider the possibilities that the distance between vertices is larger than one and that the intruder canenter some target when the patroller is moving from a vertex to another. We present in Appendix A.5 an extension of Al-gorithms 5 and 6 applicable to graphs generated with utility loss abstractions. Once we have removed attacker’s dominatedstrategies, the computation of the leader–follower equilibrium is based on the same mathematical programming formula-tion used for utility lossless abstractions (Section 5.2.3) except that, for each non-dominated action enter-when(t, v), we used(t) − delay(t, v) instead of delay(t, v).6. Summarizing the solving algorithm and addressing inconsistenciesWe collect the algorithmic results of previous sections in Section 6.1 and we discuss how inconsistencies can be ad-dressed in Section 6.2.102N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–1236.1. Global solving algorithmFig. 11. Utility loss abstraction for the setting of Fig. 1.Algorithm 7 summarizes the proposed approach for solving large instances of PSGs. The deterministic strategy is com-puted (Step 1) by reducing the game as described in Section 4.1 and (Step 2) by solving a CSP as described in Section 4.4. Ifno deterministic solution exists (Step 5), then a non-deterministic strategy is searched for. The game is reduced (Step 6) as:if lossless abstractions are used, agents’ dominated actions are removed as described in Sections 5.1.1, 5.1.2, and 5.1.3 andthen abstractions are found as described in Section 5.2; if, instead, lossy abstractions are used, defender’s dominated actionsare removed as described in Section 5.1.1, then abstractions are found as described in Section 5.3, and finally the intruder’sactions are removed as described in Appendix A.5. After the reduction of the game, the leader–follower equilibrium is found(Step 7) by solving the mathematical programs as described in Sections 3.3.2, 3.3.3, and 5.2.3.Algorithm 7: solving(G)(cid:10)1 reduce G to G2 search for a deterministic strategy of Gadmits a deterministic strategy then3 if Greturn it4(cid:10)(cid:10)5 else678remove agents’ dominated actions and find abstractionssolve the associated mathematical programming problemsreturn it6.2. Addressing inconsistenciesWe recall that an inconsistent strategy is characterized by an attacker’s best response enter-when(t, x) such that x isnever visited by the patroller after an infinite number of turns. The problem of checking whether or not a strategy isconsistent can be formulated as the problem of checking, for each best response enter-when(t, x) and once arcs not coveredby the strategy are removed, whether or not there is a connected portion of G including both t and x. If this connectedportion exists, then the strategy is consistent. If a strategy is inconsistent, two cases are possible: (i) when the uncoveredportion of G does not contain targets, replacing G with its covered portion is safe: it is trivial to see that by solving theobtained reduced game we obtain a non-worse equilibrium strategy for the defender; (ii) when the uncovered portion of GN. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123103contains targets, discarding the uncovered portion is not safe: intuitively, the presence of non-patrolled targets can worsenthe defender’s utility since the intruder can attack them with a guarantee of not being captured. The second case is morecomplex. An inconsistent strategy must be discarded and we need to recompute it on a sub-portion of the graph. However,simply discarding the uncovered portion of the graph (with the targets it contains) does not provide any guarantee on thesolution’s optimality. This situation raises the problem of finding the optimal subset of targets to cover. We provide analgorithm to address this problem.Call T c ⊆ T the set of targets to cover and T −c = T \ T c the set of targets that are not covered. We call G(T c) the reducedpatrolling setting that comprises only the targets T c . We obtain it by removing T −c from G’s sets of vertices V and targets Tand then taking the intersection of all the connected components of the graph in which all the remaining targets T c arepresent. Note that for some T c the setting G(T c) can be empty, for example when the removal of T −c splits the graphin two connected components each one containing at least one target. Our algorithm enumerates all the possible T c andcomputes the leader–follower equilibrium in G(T c) explicitly considering that the attacker can attack a target in T −c witha capture probability of zero. This is done by introducing in Formulations 3.7 and 3.9 additional actions available to theattacker: attacking targets t ∈ T −c with a utility equal to v a(t) independently of the defender’s strategy. Finally, we selectthe consistent strategy that maximizes the defender’s expected utility.Guarantees on solution’s optimality provided by this simple algorithm stem from the following lemma, whose proof isreported in Appendix B.12.Lemma 6.1. If solving G(T c) for some T c returns an inconsistent strategy σd, then the defender’s expected utility from σd is not largerthan the defender’s expected utility when restricting to the targets covered by σd.Therefore, there is at least a consistent patrolling strategy σ (cid:10)d is not worse than σd for thedefender. Then, we provide some insight to limit the number of possible sets T c to be enumerated. We state the followinglemma, whose proof is reported in Appendix B.13.⊆ T c such that σ (cid:10)d with T(cid:10)cLemma 6.2. For any T c and Toptimal strategy in G(T(cid:10)c with T(cid:10)c) is not better.(cid:10)c⊆ T c , if the optimal strategy on G(T c) is consistent, then the defender’s expected utility of theThe above lemma shows that, when the resolution of the game with all the targets returns a consistent strategy, solvingpatrolling settings in which the patroller is limited to patrol a subset of targets does not return a better strategy. We statethe following theorem, whose proof is in Appendix B.14.Theorem 6.3. Algorithm 8 with T c = T returns the optimal solution.Algorithm 8: removing_inconsistency(G, T c )1 solve G(T c )2 if the solution is consistent then3return the defender’s utility4 else567assign u = 0for all t ∈ T c dou = max{u, general_sum _inconsistency(G, T c \ {t})}8return u7. Experimental evaluationIn this section, we evaluate scalability and solution quality of our algorithm in patrolling settings comparable with thoseencountered in real-world scenarios [43]. Experiments have been conducted on a Linux (2.6.24 kernel) machine equippedwith an Intel XEON 2.33 GHz CPU, 4 GB RAM, and 4 MB cache. We evaluate our algorithm to find deterministic and non-deterministic patrolling strategies in Section 7.1 and Section 7.2, respectively. In Section 7.3 we evaluate the quality of thenon-deterministic solutions.7.1. Finding a deterministic equilibrium strategy(cid:10). We developed a random generator of graph instances GWithout loss of generality, we abstract away from the specific topology of the original patrolling setting and concen-with two parameters as input: the number oftrate only on Gvertices n (corresponding to targets in the original graph G) and the number of arcs m (corresponding to what we wouldobtain applying the reduction procedure of Section 4.1 to G). Given n and m, a random connected graph with n verticesis produced, m − n arcs are added and their weights are set to 1. Values d(k) are uniformly drawn from the interval(cid:10)104N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Fig. 12. Percentage of termination for the most significant algorithm configurations for finding deterministic patrolling strategies (for the sake of presenta-tion, we split the configurations on two different subfigures; n is the number of targets).Fig. 13. Computational times for the most significant algorithm configurations for finding deterministic patrolling strategies (for the sake of presentation,we split the configurations on two different subfigures; n is the number of targets).[mini, j{w(i, j) + w( j, i)}, 2n2 maxi, j w(i, j)], where w(i, j) is the length of the shortest path between vertices i and j. Thelower bound of the interval comes from the consideration that settings with d(k) < mini, j{w(i, j) + w( j, i)} are unfeasibleand our algorithm immediately detects it (by IFC). The upper bound is justified by considering that if a problem is fea-sible, then it will always admit a solution shorter than 2n2 maxi, j{w(i, j)}. The program for generating graphs and thoseimplementing our algorithms have been coded in C.Since our objective is to find a solution that satisfies all the constraints and not the optimal solution according to a givenmetric (e.g., minimizing the cycle length), we evaluate: the termination percentage (either with a solution or with a failure)of the algorithm within 10 minutes, the computational time in the case of termination, the success percentage in the case oftermination (i.e., the percentage with which a solution has been found).For each ordering criterion introduced in Section 4.4.3 (i.e., hl, hr , hmax a, hmin a, hmin v , hmax d, hmin d), with and withoutLSC and IFC, we consider n ∈ {3, 4, 5, 6, 7, 8, 100, 250, 500} and, for each n, we produce 1000 instances of Gwith m uni-formly drawn from the interval [n, (n − 1)n] (if m < n the graph is not connected, if m > (n − 1)n at least a pair of verticesis connected by more than one arc).(cid:10)The most significant experimental results are summarized in Fig. 12 (termination percentages) and Fig. 13 (average com-putational times), while an exhaustive view is reported as Table 2 in Appendix C. We do not report all the combinationsof heuristics for improving efficiency because some of them are not enough significant. All the values are averaged overthe 1000 instances. The results on the success percentages appear less significant, being strictly correlated to the termina-tion percentages. Essentially, the lower the termination percentage the higher the success percentage. This is because thealgorithm almost always terminates when a solution is found. Proving that an instance does not admit any solution usuallyrequires the algorithm to explore the entire tree and this can rarely happen within 10 minutes.N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123105For all the algorithm configurations, the average computational time keeps reasonably short (few seconds) even withlarge values of n. On the other hand, the termination percentage varies significantly in different configurations. This behaviorresembles that of many constraint programming algorithms, whose termination time is usually either very short (when asolution is found) or the deadline is exceeded. Moreover, the obtained results present outliers that can be detected byobserving the maximum computational times in Table 2. Some cases were harder than the average to solve and requiredan amount of time significantly larger (in practice, these cases both reduce the percentage of termination and increasethe computational time). These hard cases represent outliers within the population of instances obtained with the randomgraph generator. They are typically characterized by tangled topologies or oddly-distributed relative deadlines.We now comment the performance of the techniques of Section 4.4.3 for improving the efficiency of our algorithm.The best ordering criterion seems to be hmin v with RTB. The experimental results with hmax a, hmin a, hmax d, hmin d arevery similar to those obtained with hl and then omitted. The criterion hmin v with RTB leads the algorithm to terminatewith a percentage close to hr for small values of n and about 80% larger for large values of n. Instead, hl provides very badperformance, especially for large values of n, when the algorithm terminates with percentages close to 0%.The LSC improved stopping increases the termination percentage by a value between 0% and 2%, without distinguishableeffects on the computational time. This improvement depends on the configuration of the algorithm since it affects theconstruction of the search tree. Its adoption increases the percentage of termination without finding any solution.The IFC criterion increases the termination percentage by a value between 1% and 4%, reducing the computational time(since many non-feasible settings can be recognized before starting the construction of the tree). This improvement does notdepend on the configuration of the algorithm since it does not affect the search, working before it. Its adoption increasesthe percentage of termination without finding any solution.Hence, the best algorithm configuration appears to be hmin v with RTB, LSC, and IFC. The results are satisfactory: thetermination percentage is high also for large settings, even with 500 targets, and the corresponding average computationaltime, about 5.5 s, is reasonably short. The above results justify our decision to set the threshold at 10 minutes (600 s).7.2. Simplifying large patrolling security games and finding a randomized equilibrium strategyGiven the different and heterogeneous formulations of the patrolling problem proposed in the literature (and summarizedin Table 1), the definition of a data set for experimentation and comparison has not yet been addressed (at least, to thebest of our knowledge). Some partial attempts to define experimental settings for non-adversarial patrolling can be foundin [59] (used also in [47]) where the authors propose two arbitrary topology maps with about 50/60 vertices. The lack of asuitable data set for our experimental activity pushes us to develop an ad hoc data set.Our data set for adversarial patrolling is partitioned in two parts. The first one is composed of settings with perimetertopologies (in which we further distinguish between open and closed topologies), while the second part is composed ofsettings with arbitrary topologies. The settings with arbitrary topologies have been obtained both by introducing targets inthe setting presented in [59] and by producing new patrolling settings inspired by environments in RADISH (a repositorycontaining data sets of environmental acquisitions performed with real robots [36]). We characterize the patrolling settingsw.r.t. the number n of vertices and the density δ of targets, representing the percentage of targets over vertices (i.e., δ =|T |/n).We evaluated and compared multiple configurations of our algorithm that differ in the efforts devoted to the pre-processing phase. More precisely, given a PSG instance, we consider: basic algorithm (basic): we plainly compute the optimalstrategy as described in Section 3.3 on the original setting without exploiting any kind of reduction; removal of dominatedstrategies (dom): we apply the algorithms described in Section 5.1 to reduce the game, then we solve it with the basicalgorithm; utility lossless abstraction (lossless): we remove the players’ dominated strategies, we apply the strongest utilitylossless abstraction, as described in Section 5.2, and finally we solve it with the basic algorithm; utility loss abstraction(lossy): we apply the strongest utility loss abstraction as described in Section 5.3, we remove the intruder’s dominatedstrategies from the obtained game, and finally we solve it with the basic algorithm.We imposed a time deadline: one hour for the zero-sum settings and 24 hours for the general-sum ones. We coded ouralgorithm in MATLAB and we formulated all the mathematical programming problems with AMPL [25]. We used CPLEX [37]and SNOPT [61] for solving the linear and non-linear mathematical programs, respectively.7.2.1. Open perimetral settingsWe generated our settings with the following features (see Fig. 14(a) for an example): n ∈ {10, . . . , 200} and δ ∈{10%, . . . , 50%}; targets are randomly selected among the vertices with the constraint that the two extreme vertices must bet∈T v d(t) = 1 and penetration timestargets; for each target t a random value v d(t) is chosen under the global constraintd(t) are independently drawn from the interval {Dt, Dt + 1, . . . , 2Dt − 1} where Dtis the maximum distance of t froman extreme vertex (a penetration time shorter than Dt could make a target to be exposed, while with penetration timeslonger than 2Dt − 1 deterministic equilibrium strategies exist). For general-sum settings, for each target t a random valuein [0, 1] is chosen for v a(t). For each pair of values (n, δ) we generated 5 patrolling settings and we analyzed the averagevalues.(cid:3)The removal of dominated actions allows one to discard about 95% of the attacker’s actions on average and up to 99%(details can be found in Table 3, see Appendix C). The number of non-dominated attacker’s actions happens to be very small106N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Fig. 14. Example of open and closed perimetral settings.Fig. 15. Computational times for different algorithm configurations in open and closed perimetral settings.(no more than 12) even with large settings (with more than 104 vertices). This is essentially due to two reasons. First, eachattacker’s action enter-when(t, h) where h is not an extreme vertex is dominated by enter-when(t, his extreme.Second, a large number of targets in non-extreme vertices will never be attacked because there is some other target with alarger value and presenting a strictly smaller capture probability.(cid:10)) where h(cid:10)The application of lossless abstractions allows one to generate abstracted graphs discarding, after the removal of domi-nated actions, about 70% of the vertices on average and up to 93% (see Table 4 in Appendix C). This is essentially due to thevery low number of non-dominated actions. Since there are few non-dominated actions and each of them imposes a set ofconstraints over the computation of the abstractions, the associated optimization problem is weakly constrained and manyvertices can be discarded.Graphs in Fig. 15 (left-hand side) show how the computational time (of the terminated executions) varies w.r.t. n and δfor the zero-sum case. With general-sum instances the results are similar. To give a general idea, the average computationaltime for a general-sum instance is roughly the number of (non-dominated) attacker’s actions multiplied by ±10% of thecomputational time of a zero-sum instance. The detailed data for zero-sum and general-sum instances can be found inTable 5, see Appendix C.N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123107For all the configurations, the computational times rise exponentially as δn. The basic configuration (top left graph ofFig. 15) requires a large amount of computational time even with small settings and it is applicable only up to n = 40vertices and δ = 10% of targets. Going beyond this limit is impractical. The dom configuration (middle left graph of Fig. 15)solves much larger settings thanks to a significant reduction of the defender’s actions. In this case, the computational limitis for n > 100 and it is due to the excessively large number of vertices and therefore of defender’s actions. The percentageof the pre-processing time over the total computational time with dom is 4% on average (the max is 18% and the min is1%). The lossless configuration (bottom left graph of Fig. 15) solves larger instances composed up to 200 vertices and 50%of targets thanks to a reduction of the actions of the attacker and the defender. The percentage of the pre-processing timeover the total computational time with lossless is 76% on average (the max is 99% and the min is 9%). Given that realisticallylarge open perimetral settings (more than 200 vertices and 100 targets) are solvable with the lossless configuration, we didnot apply the lossy configuration.7.2.2. Closed perimetral settingsClosed perimetral settings are generated as squares with edges whose length is f vertices such that (see Fig. 14(b) for anexample): n ∈ {10, . . . , 200} and δ ∈ {10%, . . . , 50%}. Targets are randomly selected among the vertices with the constraintsthat the four corners are targets and that the graph is not reducible, by removal of patroller’s dominated actions, to anopen setting; penetration times d(t) are independently drawn from the interval {2 f , 2 f + 1, . . . , n − 1} (a penetration timeshorter than 2 f could make a target to be exposed, while with penetration times longer than n − 1 deterministic equilibriumstrategies exist). The agents’ values over targets are generated as for the open perimetral settings. For each pair of values(n, δ) we generated 5 patrolling settings and we analyzed the average values.The removal of dominated actions applied to the original settings discards about 41% of the attacker’s actions on averageand up to 50% (see Table 6 in Appendix C). The number of removed actions is much lower than with open perimeters. Thisis essentially because there are not extreme vertices.The lossless abstractions do not remove any vertex. The set of removable vertices is empty because every vertex appears(as t or h) in at least one non-dominated action enter-when(t, h). The lossy abstraction allows one to generate abstractedgraphs discarding about 50% of the vertices on average and up to 70% (see Table 7 in Appendix C). The application ofremoval of dominated actions applied to lossy abstraction graphs allows one to discard up to 50% of the actions (see againTable 6).Graphs in Fig. 15 (right-hand side) show how the computational time (of the terminated executions) varies w.r.t. n and δfor the zero-sum case. The computational time of a general-sum instance is roughly (±9%) the number of (non-dominated)attacker’s actions multiplied by the computational time of a zero-sum instance. The detailed data are in Table 8, Appendix C.Closed perimetral settings are more difficult w.r.t. open ones. The basic configuration (top right graph of Fig. 15) ranout of memory with n = 44 vertices and δ = 20% of targets and only slight improvements are obtained when enabling theremoval of dominated strategies. With the dom configuration (middle right graph of Fig. 15), the limit over the setting sizeis only improved to 44 vertices and 30% targets. The percentage of pre-processing time w.r.t. the total computational timewith dom is < 1% on average (the max is 2%, and the min is < 1%). The lossless configuration provides the same results ofdom. The lossy configuration (bottom right graph of Fig. 15) allows one to solve large instances composed up to 84 verticesand 50% of targets. The percentage of the pre-processing time over the total computational time with lossy is 53% on average(max is 99% and min is 12%).Notice that in the specific case in which all the vertices are targets and all the targets present the same value (for bothagents) and the same penetration time, settings can be solved by applying the algorithm presented in [3]. In this case, suchalgorithm outperforms ours, requiring polynomial time.7.2.3. Arbitrary settingsWe developed a software tool [14] to compose patrolling settings and we generated arbitrary settings. In additionto the settings proposed in [59], we considered a subset of the indoor environments from RADISH repository: albert-b-laser, austin_ aces3, cmu_nsh_level_a, DLR-Spatial_Cognition, fr079, kwing_wld, intel_oregon, mit-csail-3rd-floor, sdr_site_b,stanford-gates1, and ubremen_cartesium. Fig. 16 shows two examples.For every topology, we manually reproduced several bidimensional grids representing the map for different n and δwhere n ∈ {50, . . . , 166} and δ ∈ {5%, . . . , 30%}. Broadly speaking, with large values of n, each cell is associated to a small areaof the environment, while, with smaller n, each cell represents a larger part of the environment. Target cells are randomlyselected for different values of δ and penetration times d(t) are randomly chosen in the interval {Dt, Dt + 1, . . . , 2Dt − 1}where Dt is the maximum distance of t from any vertex. The target values are generated as in the open and closed perime-tral settings. For each topology and each pair of values (n, δ) we generated 5 patrolling settings and we analyzed theaverage values. In this case, we did not consider any time threshold in experiments, therefore memory was the only limitedcomputational resource.The performance of our reduction techniques is between that of the open and that of the closed perimetral settings. Theremoval of dominated actions applied to the original setting eliminates about 58% of the attacker’s actions on average andup to 83% (see Table 9 in Appendix C). The application of lossless abstractions generates abstracted graphs discarding, afterthe removal of dominated actions, about 20% of the vertices on average and up to 35% (see Table 10 in Appendix C). The108N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Fig. 16. Two examples of arbitrary patrolling settings (white cells are associated to vertices, targets are denoted with circles) with 166 vertices and 10%targets.Fig. 17. Computational times for different algorithm configurations in arbitrary settings.lossy abstraction generates abstracted graphs discarding about 55% of the vertices on average and up to 76% (Table 10). Theapplication of removal of dominated actions applied to lossy abstraction graphs discards up to 75% of the actions (Table 9).Graphs in Fig. 17 show how the computational time varies w.r.t. n and δ for the zero-sum case. Results are reported forterminated executions (those which did not exceed the memory limit) and for all the obtained strategies no inconsistencieswere observed. With general-sum instances the results are similar: the computational time of a general-sum instance isroughly (±10%) the number of (non-dominated) attacker’s actions multiplied by the computational time of a zero-suminstance. The detailed data for zero-sum and general-sum instances can be found in Table 11 in Appendix C.We solved the game instances only with the lossless and lossy configurations (results with dom are worse than those withlossless, while with basic all the instances required more than 4 GB RAM). As it can be seen from Table 11, arbitrary settingsturned out to be less hard than closed perimetral ones. By employing the lossless configuration we encountered a limit withn = 75 vertices (cells) with δ = 10% of targets. The lossy configuration allowed us to solve instances up to 166 cells with10% of targets. The computational time spent for pre-processing for both lossless and lossy configurations is similar to thatfound for closed perimetral settings.As a last experiment, we relaxed the penetration times in the above instances such that they admit a pure strategyequilibrium for which the intruder acts stay-out. We solved these instances with the lossless and lossy configurations. Theaverage computational times are close to those reported in Table 11. They are about 103 larger than those obtained withthe algorithm specific for deterministic strategies we described in Section 4. In addition, the solutions returned by the non-deterministic algorithm (being first-order Markovian) do not assure the patroller to capture the intruder with a probabilityof one. These findings justify our approach of looking for deterministic strategies with a specialized algorithm and not withthe general algorithm for non-deterministic strategies.N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123109Fig. 18. Efficiency of first-order Markovian strategies (with lossless and lossy abstractions) for different settings when there is a non-first-order Markoviandeterministic strategy. Grey boxes report the efficiency for the baseline uniform strategy.7.3. Solution quality evaluationWe evaluate the quality of the solutions produced by our algorithms in terms of efficiency as discussed in Section 3.3.4:we evaluate the efficiency of first-order Markovian strategies w.r.t. higher-order Markovian strategies and we evaluate theefficiency of solutions produced with lossy abstractions w.r.t. solutions produced with lossless abstractions.For all the previous settings (open, closed, and arbitrary topologies), the value uof the corresponding optimal (high-order Markovian) strategy is not known. In this case, as discussed in Section 3.3.4, we can calculate a lower bound on. With respect to the results presented in the previous sections, the averageefficiency consideringvalue of this lower bound on efficiency is 0.97 for the lossless configurations (max is 0.99, min is 0.95) while 0.92 for thelossy ones (max is 0.99, min is 0.81).i v d(i) instead of u(cid:3)∗∗∗In order to obtain a more accurate evaluation of solutions’ quality, we considered settings which admit a deterministicis known in advance and is equal to the sum of all the targets’ values. Inequilibrium strategy. Hence, for these settings, uthese settings, efficiencies can be computed exactly without resorting to a lower bound. The arbitrary graphs we generatedin this phase have the following features: n ∈ {40, . . . , 75} and δ ∈ {5%, . . . , 20%}; targets are randomly selected and theirvalue is chosen as in the open and closed perimetral settings. The penetration times are the smallest values such as thereexists a non-first-order Markovian deterministic strategy, but there is not any first-order Markovian deterministic strategy.This represents the worst case for the non-deterministic first-order Markovian strategies. (Obviously, relaxing the penetrationtimes, first-order Markovian non-deterministic strategies perform better.) For each pair of values (n, δ) we generated 10patrolling settings and we analyzed the average values.Graphs in Fig. 18 show the solutions’ efficiency for first-order Markovian strategies with lossless and lossy abstractions forzero-sum instances. Strategies are compared against a baseline case whose efficiency is reported in grey boxes. The baselinestrategy is a Markovian strategy that assigns a uniform probability of movement to adjacent vertices on the abstracted(lossless and lossy, respectively) graph and in settings with a small target density (δ = 5%). A remarkable improvement inefficiency is generally obtained when passing from the baseline case to a Markovian equilibrium strategy. Results obtainedfor general-sum instances are similar. The detailed data are in Table 12 in Appendix C.Lossless first-order Markovian strategies are very effective, providing at least 99% of the utility provided by the deter-ministic strategy. The expected utility strictly increases in δ, the defender preserving more targets.The employment of lossy first-order Markovian strategies is very satisfactory, providing at least 85% of the utility pro-vided by the deterministic strategy. As in the lossless case, the expected utility strictly increases in δ.We additionally compared the quality of the solutions with lossy abstractions w.r.t. lossless abstractions in all the settingsused in Sections 7.2.2 and 7.2.3. In these cases the loss in efficiency is not larger than 5%. This, being an average case, isconsistent with the worst case discussed above.Finally, a remark about inconsistencies is worth. The absolute utility loss of a consistent strategy w.r.t. the optimal (high-order Markovian) strategy is upper bounded by mint∈T v (t)P c(t, h) where enter-when(t, h) is a valid best response for theintruder. This happens because at the equilibrium (excluding some very particular topologies) the intruder is likely inducedto attack the covered target with the lowest value for the defender. This guarantees some relatively limited (w.r.t. targets’values) absolute utility loss. When inconsistencies are present, the set of covered targets can change, resulting in a larger110N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123minimum value. Thus, the upper bound on absolute utility loss can be large with a consequent arbitrary degradation of thesolution’s quality.8. Conclusions and future worksSecurity games constitute a class of games that are receiving an increasing attention in artificial intelligence and are usedin several security applications. They are characterized by two players, a defender and an attacker, and by a set of targetswith some values. The attacker wants to intrude a target, while the defender wants to protect the targets. Both attackerand defender can deploy some resources to accomplish their goals. Security games are usually modeled as leader–followergames where the defender can commit to a strategy and where the underlying games are in normal form.In this paper, we studied security games by taking into account the situation in which the attacker can observe therealization of the strategy of the defender and decide, on the basis of this observation, when to attack without being subjectto any temporal deadline. This option, if available, will be always exploited by the attacker, which could gain a larger utility.We extended the currently available models introducing an underlying game in extensive form and with infinite horizon.Since the “natural” application of this model is to mobile robot patrolling, we called patrolling security games (PSGs) this newgame class. Our model allows one to study adversarial patrolling problems where the topology of the area to be patrolledis represented with an arbitrary graph with targets. We limited our study to the case in which each agent has a singleresource: an intruder and a patroller for the attacker and the defender, respectively. The main contribution of our work hasbeen the development of algorithmic techniques for the computation of the leader–follower equilibrium in large instancesof PSGs.Differently from the state of the art in security games, in our setting the leader’s commitment is directly in behaviorstrategies and the problem to compute a leader–follower equilibrium can be formulated as a bilinear mathematical pro-gramming problem. The non-linearity is due to the presence of Markovian constraints introduced for dealing with infinitehorizon. This makes the computation of the equilibrium a hard problem to solve even for small settings. To get around thislimitation we developed specific techniques to find an equilibrium in pure strategies, when possible, and to reduce the sizeof the game instances, when we need to resort to mixed strategies.The computation of an equilibrium in pure strategies for PSGs can be formulated as a variant of the TSP. Although it isNP-complete, we showed that an efficient algorithm can be designed by resorting to constraint programming. The compu-tation of a mixed strategy equilibrium is much harder and is tractable in practice only in the case of first-order Markovianstrategies. We designed reduction techniques based on the removal of dominated strategies and on utility lossless and utilityloss abstractions (these techniques constitute an original contribution to security games). In this way, we drastically reducethe size of the game and can solve large instances. Furthermore, we showed that, in some cases, first-order Markovianstrategies produce solutions whose quality is comparable with that of optimal (high-order) solutions.Several issues are worth further investigation. The first one is the improvement of our model and techniques for thespecific patrolling problem we studied. The model could be extended along the following directions: the exploitation ofmultiple resources for the attacker and the defender (preliminary results are reported in [15]), the refinement of the move-ment models of the intruder and of the patroller, and the refinement of the patroller’s sensing capabilities. Results along thisdirection have been presented in [13], where the intruder is refined by introducing a more realistic movement model andrestricting its observation capabilities. Moreover, it would be interesting to study new algorithms to generate abstractionsthat keep into account also the defender’s expected utility [10], thus searching for the best abstraction for a given trade-offbetween computational time and expected utility. The second issue for future work is the application of the PSG model tonovel applications. We are currently exploring two applications: the patrolling by active mobile cameras (preliminary resultsare reported in [16]) and computer security. A third issue is the employment of some of the techniques proposed in thispaper to problems different from PSGs. For instance, our abstractions could be used for general security games on graphs tospeed up the generation of optimal schedules preserving the expected utility.Appendix A. Extensions and refinementsA.1. Formulation with arbitrary lWe provide a generalization of the formulation presented in Section 3.3.1 to compute intruder capture probabilitieswhen the history length l is arbitrary. Let us introduce some notation. We denote by H(l) the space of all the possiblehistories h with length l whose first and last vertices are h1 and hl, respectively. With a slight overload of notation, we−l the histories obtained from h by removing its first and last vertices respectively. We introducedenote with hthe notion of adjacent histories. Given a history h the set of all its adjacent histories is AH(h) = {hx ∈ H(l), h}.For example, given the environment of Fig. 1 and the history of length 5, h = (cid:6)06, 01, 02, 03, 07(cid:7), a history adjacent to h ishx = (cid:6)01, 02, 03, 07, 08(cid:7). The general formulation is the following:−1 and h−1 = h−lxαh, j (cid:2) 0 ∀h ∈ H(l), j ∈ V(cid:7)αh, j = 1 ∀h ∈ H(l)j∈V(30)(31)N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123(cid:9)(cid:8)hl, j= αh1,hl2(cid:7)=αh, j (cid:3) aγ 1,th1,h2γ w,th1,h2∀h ∈ H(l), j ∈ V∀t ∈ T , h2 ∈ A H(h1),(cid:8)(cid:9)γ w−1,th1,hxαhx,hl2hx∈ A H(h2)t /∈hxP c(t, h1) = 1 −(cid:7)hx∈H(l)t /∈hxγ d(t),th1,hx∀t ∈ T , h1 ∈ H(l)∀w ∈(cid:10)(cid:11)2, . . . , d(t), t ∈ T , h1, h2 ∈ H(l), hl2(cid:11)= t,111(32)(33)(34)(35)The meaning of the constraints is similar to that of the corresponding constraints of Section 3.3.1.A.2. Reducing the number of variables and constraintsConstraints (1)–(6) can be rewritten in a more efficient way for reducing the number of variables and constraints. Westart by giving the intuition with an example.Example A.1. Consider enter-when(14, 12) in Fig. 1. Focus on the event in which the patroller starting from 12 reaches 12after 2 turns without passing through 14 and call γ 2,1412,12 the probability associated with it. If this event happens, then theprobability with which the patroller reaches 14 by d(14) = 9 turns is zero, because the distance between 12 and 14 is 9and only 7 turns are left, and therefore the intruder cannot be captured. As a result, in the computation of the captureprobability related to enter-when(14, 12), we can safely discard all the events following the one associated with γ 2,1412,12.Consider action enter-when(t, h) with l = 1: when the attack to t is started, the patroller is in vertex h and, in order tocapture the intruder, it has to visit target t at least once in the following d(t) turns. In such scenario, a sufficient conditionfor a successful intrusion is the following: if after some turns, say ρ, the patroller has not yet visited t and occupies a vertexi such that ρ + dist(i, t) > d(t), then the intrusion has success, where dist(i, j) is the shortest distance between two verticesi and j. In other words, if the realization of the patroller’s path drove it in a vertex from which t cannot be reached by itspenetration time, the intruder cannot be captured at all. We can easily determine the minimum value of ρ for a target twhen the patroller occupies a generic vertex i as ρ(i, t) = d(t) − dist(i, t) + 1. We can now reduce the number of variablesof the form γ w,tthat are included in the computation of P c(t, h) by setting ρ( j, t) as the upper bound of index w. To doh, jthis, constraints (5) and (6) are replaced with the following ones:γ w,ti, j=(cid:7)(cid:8)γ w−1,ti,xαx, j(cid:9),t ∈ T , i, j ∈ V , j (cid:11)= t, ∀w ∈(cid:10)(cid:11)2, . . . , ρ( j, t)x∈V \{t}w(cid:2)ρ(x,t)(cid:15) (cid:7)P c(t, h) = 1 −γ ρ( j,t),th, j+(cid:7)(cid:7)j∈V \{t}w(cid:2)ρ( j,t)−1γ w,th, j(cid:7)x∈V \{t}w(cid:3)ρ(x,t)(cid:16)α j,x∀t ∈ T , h ∈ Vj∈V \{t}(36)(37)Constraints (36) are the same as constraints (5) with the addition of the upper bound ρ( j, t) on the w index of each γ w,ti, jvariable. The term enclosed between parentheses in constraints (37) is the success probability of an action enter-when(t, i).Its first addendum accounts for all the path realizations that start from i and end in a vertex j exactly after ρ( j, t) turns.The second addendum accounts for all the path realizations that end in a vertex x at a turn w > ρ(x, t) given that at turnw − 1 they visited a vertex j without having reached the corresponding upper bound ρ( j, t). The exact number of variablesand constraints eliminated by this refinement strongly depends on the specific instance of the PSG. From our experimentalevaluations, we observed that, on average, both the number of variables γ w,tand the number of constraints approximatelyi, jhalve.A.3. Capture probability formulation with intruder movementsIn this section we provide an extension that considers a more realistic movement model for the intruder. We assumethat, when performing an attack, the intruder follows some path starting from an access vertex and ending in a target.For the sake of simplicity, we denote with (cid:12) the length of a path and we assume that the intruder can disappear from atarget after a number of turns equal to the penetration time of that target. (Modeling situations in which the intruder, oncecompleted an intrusion, escapes from the environment following another path is an easy further extension of the resultspresented in this section.) We call P the set of all possible paths and we denote with p ∈ P a single path whose ith vertexis pi . The actions available to the intruder now are defined as enter-when(p, i), namely attack following path p as soon asthe patroller is in vertex i. Performing such action, the intruder will spend (cid:12) turns to cover the path and d(p(cid:12)) turns to112N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123complete the intrusion in the target and, therefore, it will be detectable for (cid:12) + d(p(cid:12)) turns. Assuming that p w = p(cid:12) if w (cid:2) (cid:12),the capture probabilities can be computed by replacing constraints (4)–(6) of Section 3.3.1 with the following ones:(cid:10)(cid:11)p1(cid:10)γ 1,pi, jγ w,pi, j== αi, j ∀p ∈ P , i ∈ V , j ∈ V −(cid:7)(cid:8)(cid:9)γ w−1,pi,xαx, jx∈V −{p w−1}(cid:7)P c(p, i) =γ (cid:12)+d(p(cid:12)),pi, j∀i ∈ V , p ∈ P∀w ∈2, . . . , (cid:12) + d(cid:9)(cid:11)(cid:8)p(cid:12), p ∈ P , i, j ∈ V , j (cid:11)= p w(38)(39)(40)j∈V −{p(cid:12)}A.4. Capture probability formulation for graphs with arbitrary weightsIn order to compute the intruder capture probabilities when graphs have edges with arbitrary weights, we need thefollowing constraints that capture the possibility that traversing arcs can require more than one turn:(cid:10)αi, j (cid:3) aγ e(i, j),ti, jγ w,ti, j=(i, j) ∀i, j ∈ V= αi, j ∀t ∈ T , i, j ∈ V , j (cid:11)= t(cid:9)(cid:8)(cid:7)γ w−e(x, j),ti,xαx, jx∈V \{t}w(cid:2)ρ(x,t)w(cid:3)e(i,x)+e(x, j)(cid:7)P c(t, i) = 1 −γ ρ( j,t),ti, jj∈V \{t}∀w ∈(cid:10)(cid:11)2, . . . , ρ( j, t), t ∈ T , i, j ∈ V , j (cid:11)= t(cid:7)(cid:7)−j∈V \{t}w(cid:2)ρ( j,t)−1γ w,ti, j(cid:7)x∈V \{t}w(cid:3)e(i, j)+ρ(x,t)α j,x ∀t ∈ T , i ∈ V(41)(42)(43)(44)Substituting the above constraints to constraints (3), (4), (36), and (37), respectively, in Formulations 3.7, 3.8, and 3.9 wecan calculate the equilibrium patrolling strategies.Using graphs with arbitrary weights makes the patroller movement model more expressive, allowing one to capture thetime spent in movements. In order to make this model even more expressive, we could combine arbitrary weight graphswith the model used in [3], where the patrolling robot has a heading and changing the heading requires one turn duringwhich the robot stays in the same vertex. The resulting model would lead to define the states of the patroller as a pair:a vertex and an orientation. This more realistic model has the drawback to increase the number of patroller’s states and,therefore, it would make finding a leader–follower equilibrium harder.A.5. Removing attacker’s dominated strategies with abstracted gamesThe algorithm to remove the attacker’s dominated strategies in the abstracted game is a simple variation of the algorithmpresented in Section 5.1.2. We report it as Algorithms 9 and 10.Algorithm 9: intruder_domination1 for each t ∈ T dotabu(t) = {}2for each v ∈ V do345domination(t, v) = Vdelay(t, v) = 0expand(t, t, {t}, 0)for each v ∈ V do678910for each w ∈ domination(t, v) doif dist(v, w) < delay(t, w) thendomination(t, v) = domination(t, v) \ {w}11 for each t ∈ T do1213tabu(t) = {v ∈ V | ∀t ∃tnondominated(t) = V \ {(cid:10), t ∈ domination(t(cid:14)(cid:10), v), ua(penetration-t) (cid:2) ua(penetration-t(cid:10))}v∈V \{t} domination(t, v) ∪ tabu(t)}Algorithm 9 works exactly as Algorithm 5 except for the following points. In Step 2 it defines a variable delay(t, v) = 0,in Step 3 the length of the paths is measured in terms of temporal cost, in Step 5 for each vertex η(q) we consider thelargest number of turns (i.e., the delay) the patroller must spend along the abstracted arcs to reach η(q), and in Step 6 theset domination is reduced considering also the delay delay(t, v). In particular, focusing on this last step, given a target tN. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123113Algorithm 10: expand(v, t, B, depth)1 N = { f | father(v) (cid:11)= η( f ) (cid:11)= η(v), a(η( f ), η(v)) = 1}2 for each f ∈ N do345domination(t, η( f )) = domination(t, η( f )) ∩ η(B)if dist(η(v), η( f )) > 1 thendelay(t, η(v)) = max{delay(t, η(v)), dist(η(v), η( f )) − 1}6 if depth < d(t) then78for each f ∈ N doexpand( f , t, {B ∪ f }, depth + 1)and domination(t, v) computed as described in Steps 3–5, a vertex v dominates vif delay(t, v) > dist(v, vcapture probability of enter-when(t, v(cid:10)). Indeed,(cid:10)), we have not any guarantee that the capture probability of enter-when(t, v) is smaller than the(cid:10)) with a delay of delay(t, vonly if delay(t, v) (cid:3) dist(v, v(cid:10)).(cid:10)Appendix B. ProofsB.1. Proof of Proposition 3.11Consider the setting of Fig. 1 with the following penetration times:d(06) = 14,d(08) = 18,d(12) = 23,d(14) = 22,d(18) = 18This PSG admits a leader–follower deterministic patrolling strategy where the patroller follows a cycle over the targetsmoving along the shortest paths (i.e., 14, 08, 06, 18, 12, 18, 06, 08, 14). The best intruder’s action is stay-out independentlyof the value of (cid:3), otherwise it would be captured with a probability of one. It can be easily observed that this patrollingstrategy implies l = 2. Suppose to apply Formulation 3.8 to such a game. We can show that we can always find a valueof (cid:3) such that there is no patrolling strategy with l = 1 such that stay-out is the intruder’s best response. Consider actionenter-when(06, 23), the associated capture probability is always smaller than one when l = 1. Indeed, the values α11,i withi ∈ {06, 12, 18} are strictly positive to assure that the patroller can cover all the targets. Then, by Markov chains, it followsthat the probability that the patroller reaches vertex 06 starting from vertex 23 within 9 turns is strictly smaller thanone. We can always find a strictly positive value of (cid:3) such that, when the patroller follows the strategy with l = 1, theintruder strictly prefers to attack a target rather than not to attack. Since the intruder will attack and the probability ofbeing captured is strictly lower smaller one, the utility expected by the patroller from following the strategy with l = 1 willbe strictly smaller than that expected utility from following the deterministic equilibrium strategy with l = 2.B.2. Proof of Theorem 3.13No first-order Markovian leader–follower equilibrium strategy can provide less than 0.5 ·diction that a leader–follower equilibrium strategy σ ∗v d(t) (cid:2) 0.5 ·Therefore, σ ∗i v d(i). If the defender covers only such target with a probability of one, it would gain at least 0.5 ·cannot be a leader–follower strategy and we have a contradiction.gives less than 0.5i v d(i). Assume by contra-i v d(i). Then there is a target t such thati v d(i).(cid:3)(cid:3)(cid:3)(cid:3)We now show that efficiency 12 can be arbitrarily achieved. Consider a setting with two targets t1 and t2, both withpenetration time equal to d and value, for the defender, equal to M. Assume that the topology is linear with the two targetsas extreme vertices. Assume that the number of vertices (including targets) is 2d − 2. This setting admits a deterministicequilibrium strategy and therefore the optimal high-order strategy gives the defender 2M. The first-order Markovian leader–follower equilibrium strategy is such that, as d → +∞, the capture probability of enter-when(t1, t2) and enter-when(t2, t1)(i.e., the two best responses) go to zero. Therefore, the defender’s expected utility approaches to M and the efficiency isarbitrarily close to 12 .B.3. Proof of Theorem 4.4We prove the NP-completeness by reducing the Directed Hamiltonian Circuit problem (DHC) [49] to the DET-STRATproblem. DHC is the problem of determining if a Hamiltonian path, i.e., a path that visits each vertex exactly once, exists ina given directed graph. This is a well-known NP-complete problem. Let us consider a generic instance of the DHC problemgiven by a directed graph Gh = (V h, Ah) where V h is the set of vertices and Ah is the set of arcs. In order to prove that(cid:10)DHC can be reduced to the DET-STRAT problem, we show that for every instance Gh of the DHC problem an instance Gs of(cid:10)the DET-STRAT problem can be built in polynomial time and that, by solving the DET-STRAT problem on Gs, we obtain also(cid:10)= (T s, As, w s, ds) can be easily constructed from Gh in the followinga solution for the DHC problem on Gh. An instance Gs(cid:10) ∈ T s. It is straightforward toway: T s = V h, As = Ah, for every v ∈ T s we impose d(v) = |V h| and w s(v, v(cid:10)s, if it exists, is a Hamiltonian cycle. Indeed, since the relative deadline of every target is equal to thesee that a solution of G(cid:10)) = 1 for all v, v114N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123number of targets, a deterministic equilibrium strategy should visit each target exactly once, otherwise at least one relative(cid:10)deadline would be violated (being w s(v, vs provides byconstruction a solution for Gh or, in other words, the DHC problem can be reduced to the DET-STRAT problem, proving itsNP-completeness (the proof is completed by noting that it is trivially polynomial to verify that a given sequence of verticesis a solution of the DET-STRAT problem).(cid:10) ∈ T s). Therefore, computing the solution for G(cid:10)) = 1 for all v, vB.4. Proof of Theorem 4.5In order to prove the theorem it is sufficient to prove that, if a problem is solvable, then there exists a solution σ inwhich there is at least a vertex that only appears once, excluding σ (s). Indeed, if this statement holds then the maximumtemporal length of σ is bounded by d(i) where i is the vertex that appears only one time in σ . It easily follows that, in theworst case, the maximum temporal length of σ is maxt∈T {d(t)}.We now prove that, if the problem is solvable, then there is a solution in which at least a vertex appears only once.To prove this, we consider a solution σ wherein σ (1) is the vertex with the minimum relative deadline, i.e., σ (1) =arg mint∈T {d(t)}. (Notice that this assignment does not preclude finding a solution.) We call k the minimum integer suchthat all the vertices appear in the subsequence σ (1) −σ (k). We show that, if the problem is solvable, then it is not necessarythat vertex v = σ (k) appears again after k. A visit to v after k would be observed if either it is necessary to pass through vto reach σ (1) or it is necessary to re-visit v, due to its relative deadline, before σ (1). However, since all the vertices butv = σ (k) are visited before k, all the vertices but v can be visited without necessarily visiting v. Furthermore, the deadlineof σ (1) is by hypothesis harder than σ (k)’s one and then the occurrence of v = σ (k) after k is not necessary. Therefore,vertex σ (k) occurs only one time.B.5. Proof of Theorem 4.6We initially prove the soundness of the algorithm. We need to prove that all the solutions it produces satisfy con-straints (10)–(14). Constraints (10), (11), and (14) are satisfied by Algorithm 3. If at least one of them does not hold, nosolution is produced. The satisfaction of constraints (12) is assured by Algorithm 4 in Step 3, while the satisfaction ofconstraints (13) is assured by Algorithm 4 in Steps 6 and 9.In order to prove completeness we need to show that the algorithm produces a solution whenever at least one exists.In the algorithm there are only two points in which a candidate solution is discarded. The first one is the forward checkingin Algorithm 4. Indeed, it iteratively applies constraints (13)–(14) to a partial sequence σ exploiting a heuristic over thefuture weights (i.e., the time spent to visit the successive vertices). Since the employed heuristic is admissible, no feasiblecandidate solution can be discarded. The second point is the stopping criterion in Algorithm 3: when all the vertices occurin σ (at least once) and the first and the last vertex in σ are equal, no further successor is considered and the searchis stopped. If σ satisfies all the constraints, then σ is a solution, otherwise backtracking is performed. We show that, ifa solution can be found without stopping the search at this point, then a solution can be found also by stopping thesearch and backtracking (the vice versa does not hold). This issue is of paramount importance since it assures that thealgorithm terminates (in Section 4.4.2 we provide an example in which, without this stopping criterion, the search couldnot terminate). Consider a σ such that σ (1) = σ (s) and including all the vertices in T . The search subtree following σ (s)and produced by the proposed algorithm is (non-strictly) contained in the search tree following from σ (1). This is becausethe constraints considered by the forward checking from σ (s) on are (non-strictly) harder than those considered from σ (1)to σ (s). The increased hardness is due to the activation of constraints (13) that are needed given that at least one occurrenceof each vertex is in σ . Thus, if a solution can be found by searching from σ (s), then a shorter solution can be found bystopping the search at σ (s) and backtracking. This concludes the proof of completeness.B.6. Proof of Theorem 5.1Call z a vertex that is not on any shortest path between any pair of targets. If a strategy σd prescribes that the patrollercan make action move(z) with a strictly positive probability, then it can be easily observed that, if the patroller does notmake such action, it cannot decrease its expected utility. Indeed, the intruder capture probability P c(t, x) for any t ∈ T andx ∈ V cannot decrease since visiting z would only introduce an unnecessary temporal cost.B.7. Proof of Theorem 5.2The proof is trivial. When the patroller moves along Q for going from t1 to t2, the intruder capture probabilities are notsmaller than in the case in which the patroller moves along P .B.8. Proof of Theorem 5.3The idea is that by setting α(i, i) = 0 for every i ∈ V \ T , the intruder capture probabilities do not decrease. We considera simple situation with two vertices adjacent to j, but the same argument can be applied to situations in which j has anyN. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123115Fig. 19. Example used in the proof of Theorem 5.3.number of adjacent vertices. Consider Fig. 19 where all vertices are not targets. Given α02,01, α02,02, α02,03, the probability, while the probability to reach 03 from 02 after an infiniteto reach 01 from 02 after an infinite number of turns is= 0, and α(cid:10)02,0203 from 02 do not decrease for any possible number of turns. Therefore, we obtain the thesis. Of course, it is easy to seethat the same does not hold when we set αi,i = 0 with i ∈ T ., the probabilities to reach 01 andα02,011−α02,02, α(cid:10). By setting α(cid:10)= α02,031−α02,02= α02,011−α02,02number of turns isα02,031−α02,0202,0102,03B.9. Proof of Theorem 5.6We prove the ‘if’ part. (i) and (ii) imply (1 − P c(t, i))ua(penetration-t) < (1 − P c(s, j))ua(penetration-s) for everyfully mixed strategy σd. By continuity, with non-fully mixed strategies we have (1 − P c(t, i))ua(penetration-t) (cid:3) (1 −P c(s, j))ua(penetration-s), that, since ua(intruder-capture) is non-positive, implies the definition of dominance.We prove the ‘only if’ part of (i). For all the possible patrolling settings, if ua(penetration-t) > ua(penetration-s), it ispossible to find a fully mixed strategy σd such that EUa(enter-when(t, i)) > EUa(enter-when(s, j)) in the following way. Weset all the probabilities leading to s from j equal to 1 − (cid:3) with (cid:3) > 0 arbitrarily small. If the path connecting t to i isnot strictly contained in the path connecting s to j, then we can set some probability in the path connecting t to i equalto (cid:3) and thus (1 − P c(t, i)) (cid:21) 1 and (1 − P c(s, j)) (cid:21) 0, satisfying the previous inequality. If the path connecting t to i isstrictly contained in the path connecting s to j, we have (1 − P c(t, i)) < (1 − P c(s, j)). However, we can set the probabilitiesleading to s from t equal to 1 − (cid:3)(cid:10)such that P c(s, j) (cid:2) (1 − (cid:3)(cid:10))k P c(t, i) where k is the distance between t and s. It is alwayspossible to find an (cid:3)(cid:10)such that P c(s, j) − P c(t, i) is arbitrarily small and, since the difference between ua(penetration-t) andua(penetration-s) is finite, EUa(enter-when(t, i)) > EUa(enter-when(s, j)).We prove the ‘only if’ part of (ii). If there exists a strategy σd such that P c(t, i) < P c(s, j), then the path connecting tto i is not strictly contained in the path connecting s to j. In this case, we can find a σd (as we discussed above) such that(1 − P c(t, i)) (cid:21) 1 and (1 − P c(s, j)) (cid:21) 0, and therefore action enter-when(t, i) is not dominated.B.10. Proof of Theorem 5.10The proof is trivial. The intruder’s action enter-when(t, t) being dominated, the intruder will never enter t when thepatroller is in t. Therefore, setting αt,t = 0, the probability that the intruder will be captured when it enters t will neverdecrease.B.11. Proof of Theorem 5.18The proof has two steps. In the first one, we show that, after the application of the lossless abstractions, the set ofintruder’s dominated strategies is left unchanged, and therefore we can focus only on the dominant strategies. In the secondone, we show that, for any strategy σ in the non-abstracted game, we can find, by solving the abstracted game, a strategyσ (cid:10)that gives the patroller a utility not smaller than that given by σ . With abuse of notation, we denote by P c(x, y, z) theprobability that the intruder is captured after z turns once it entered vertex x when the patroller was at vertex y.We prove the first step by showing that in the abstracted game the intruder’s probabilities to be captured when it takesa dominated action (in the non-abstracted original game) are larger than when it takes a dominant action (in the originalgame). Exactly, given an abstraction over a pair of vertices i, j and called k a vertex belonging to the shortest path betweeni and j, we need to prove that, for every target t and dom(k, t):(cid:8)(cid:9)k, t, d(t)P c(cid:2) P c(cid:8)(cid:9)dom(k, t), t, d(t)By applying our abstractions, we have:(cid:9)(cid:8)k, t, d(t)P c= max(cid:10)P c(cid:8)(cid:9)i, t, d(t) − dist(k, i), P c(cid:8)j, t, d(t) − dist(k, j)(cid:9)(cid:11)116andN. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123(cid:8)(cid:9)dom(k, t), t, d(t)P c= max(cid:17)d(t)−dist(i,t)(cid:7)(cid:8)P rdom(k, t), i, h(cid:9)(cid:8)· P ci, t, d(t) − h(cid:9),h=dist(dom(k,t),i)d(t)−dist( j,t)(cid:7)(cid:8)P rdom(k, t), j, hh=dist(dom(k,t), j)(cid:9)(cid:8)· P cj, t, d(t) − h(cid:18)(cid:9)Since dist(dom(k, t), j) (cid:2) dist(k, j) and dist(dom(k, t), i) (cid:2) dist(k, i):(cid:8)(cid:9)k, t, d(t)P c(cid:2) max(cid:17)d(t)−dist(i,t)(cid:7)P r(k, i, h) · P c(cid:8)i, t, d(t) − h(cid:9),(cid:8)h=dist(k,i)(cid:9)dom(k, t), t, d(t)(cid:2) P cP r(k, j, h) · P c(cid:8)j, t, d(t) − h(cid:18)(cid:9)d(t)−dist( j,t)(cid:7)h=dist(k, j)We prove the second step. Consider the basic situation of Fig. 9. Suppose that probabilities α01,02, α02,01, α02,03, α03,02constitute a part of a leader–follower equilibrium. We can show that we can always find values of α01,03, α03,01 such that thecapture probabilities in the abstracted game are not smaller than those in the non-abstracted game. Assign α01,03 = α01,02and α03,01 = α03,02. Assume for simplicity that, once the arc (01, ·) is traversed, the probability to come back to 01 is equalto zero. The probability to reach 03 from 01 within 2 turns in the abstracted game is α01,03. The probability to reach 03from 01 within an infinite number of turns in the original game is:α01,02 · (1 − α02,01)+∞(cid:7)l=0(α01,02 · α02,01)l = α01,02 · (1 − α02,01)1 − α01,02 · α02,011−α02,011−α01,02·α02,01Being< α01,03 and therefore the abstraction preserves the optimality ofthe solution. Given an arbitrary information lossless abstraction, we can apply iteratively the above procedure showing thatcomputing equilibrium strategies in the abstracted game allows one to find strategies as good as those in the original game.< 1 we have thatα01,02·(1−α02,01)1−α01,02·α02,01B.12. Proof of Lemma 6.1If a patrolling strategy is inconsistent, then all the best response constraints related to the attack of the targets that arenot patrolled are not active. This is because the expected utility of the attacker is larger than the value of the non-patrolledtargets. As described in Section 6.2, solving the game in which the non-patrolled targets are removed from G is equivalent tosubstitute the best response constraints related to the non-patrolled targets with new relaxed constraints. These constraintsbeing relaxed, the optimal solution is at least good as in the initial problem.B.13. Proof of Lemma 6.2(cid:10)c⊂ T c leads to better consistent solutions. Consider targets T c \ TThe proof is by contradiction. Assume that G(T c) admits a consistent solution. Assume, by contradiction, that a sub-(cid:10)c . These targets are not attacked in the solution(cid:10)c) would be worse than that of G(T c), and the associated capture probabilities are(cid:10)c), they would not be attacked neither in G(T c)partition Tof G(Tzero. However, if targets T c \ Tand therefore the defender could improve its utility by not patrolling such targets, but this leads to a contradiction.(cid:10)c were not attacked in the solution of G(T(cid:10)c), otherwise the solution of G(TB.14. Proof of Theorem 6.3The algorithm is optimal because it enumerates all the possible partitions of targets in T c and T −c except consideringsub-partitions of T c when solving G(T c) returns a consistent strategy. By Lemma 6.2, these sub-partitions cannot containbetter solutions than that associated to G(T c).Appendix C. Experimental results tablesTable 2Computing a deterministic patrolling strategy: experimental results for finding a deterministic patrolling strategy with the most significant configurationsof the algorithm (we set a time deadline of 10 minutes). Best configurations are in bold.hmin vRTBLSCnterm (%)time (s)dev (s)3100< 0.01< 0.014100< 0.01< 0.015100< 0.01< 0.01699.80.325.17799.60.101.78899.50.050.9610098.90.163.5225096.60.8714.4750090.25.5030.28N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123117Table 2 (continued)IFChrLSCIFChrIFChmin vRTBLSChrLSChrhlLSCIFChlnmax (s)min (s)suc (%)term (%)time (s)dev (s)max (s)min (s)suc (%)term (%)time (s)dev (s)max (s)min (s)suc (%)term (%)time (s)dev (s)max (s)min (s)suc (%)term (%)time (s)dev (s)max (s)min (s)suc (%)term (%)time (s)dev (s)max (s)min (s)suc (%)term (%)time (s)dev (s)max (s)min (s)suc (%)term (%)time (s)dev (s)max (s)min (s)suc (%)3< 0.01< 0.0160100< 0.01< 0.01< 0.01< 0.0160100< 0.01< 0.01< 0.01< 0.0160100< 0.01< 0.01< 0.01< 0.0160100< 0.01< 0.01< 0.01< 0.0160100< 0.01< 0.01< 0.01< 0.0160100< 0.01< 0.01< 0.01< 0.0160100< 0.01< 0.01< 0.01< 0.01604< 0.01< 0.0158100< 0.01< 0.01< 0.01< 0.01581000.448.68173.55< 0.0158100< 0.01< 0.01< 0.01< 0.0158100< 0.01< 0.01< 0.01< 0.01581007.4555.45556.92< 0.015899.27.4555.45548.41< 0.015899.27.4255.23548.41< 0.01585< 0.01< 0.01621000.111.6432.00< 0.016299.03.6538.89594.10< 0.01621000.346.29125.03< 0.01621000.7913.58270.03< 0.016298.72.4528.61506.72< 0.016291.02.4528.61505.74< 0.016588.02.6128.66505.74< 0.0167698.00< 0.015998.50.091.7033.00< 0.015997.20.142.2443.03< 0.015996.72.9833.77519.75< 0.016095.43.0424.32303.72< 0.016194.24.7842.13496.84< 0.016181.14.7842.13497.46< 0.01< 0.0178.05.1242.65497.46< 0.0171735.00< 0.016197.50.161.7324.00< 0.016296.70.262.3631.86< 0.016396.00.162.2442.22< 0.016393.90.302.7741.53< 0.016393.01.389.96140.31< 0.016375.31.389.96140.11< 0.01< 0.0171.71.6110.57140.11< 0.0174819.00< 0.016096.50.020.182.00< 0.016295.50.010.162.09< 0.016395.50.010.112.41< 0.016392.50.030.212.83< 0.016491.80.141.0312.86< 0.016469.00.141.0312.01< 0.01< 0.0165.00.201.2912.01< 0.018610078.26< 0.016195.11.346.1993.36< 0.016294.01.3539.32561.95< 0.016395.00.306.50145.22< 0.016391.21.3639.53566.040.026490.31.376.2893.260.02653.90.10< 0.01< 0.01< 0.01< 0.010.0–––––250316.90.016355.12.5216.75513.660.017853.03.4118.02501.720.018093.31.0015.32366.420.016352.43.4818.46531.640.018251.03.7418.45516.720.01832.30.01< 0.010.010.010.010.0–––––500413.940.07649.84.6651.62590.870.07928.95.9455.14582.770.079386.26.1935.77498.040.07677.75.8355.65596.420.07947.16.1856.80576.520.07951.50.07< 0.010.070.070.070.0–––––Table 3Computing a non-deterministic patrolling strategy in open perimetral settings: number of candidate best responses for the basic case (without any reduc-tion) and the dom case (once dominated strategies have been removed).Number ofvertices(n)102030406080100130160200Percentage of targets/vertices (δ)10%basic1040901603606401000169025604000dom223.42.8565.611.86.66.220%basic208018032072012802000338051208000dom23.24.45.476.25.87.28.48.830%basic301202704801080192030005070768012,000dom3.8445.278.69.47.87.47.440%basic40160360640144025604000676010,24016,000dom44.85.2877.88.28.6911.21050%basic50200450800180032005000845012,80020,000dom4.665.89.27.689.29.2911.8118N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Table 4Computing a non-deterministic patrolling strategy in open perimetral settings: number of remaining vertices after the application of lossless abstractions.Number ofvertices (n)102030406080100130160200Percentage of targets/vertices (δ)10%566.67.28.69.810.210.411.21120%56.47.28.29.89.29.410.812.812.7530%5.26.67.88.810.21112.411.411.811.540%5.26.489.610.611.811.812.214.214.450%5.67.28.410.410.21112.212.61315.4Table 5Computing a non-deterministic patrolling strategy in open perimetral settings: computational times and standard deviations (in parentheses) with zero-sum(top line) and general-sum (bottom line) settings.Number ofvertices (n)102030406080100130basicdomlosslessbasicdomlosslessbasicdomlosslessbasicdomlosslessbasicdomlosslessbasicdomdomlosslessbasicdomlosslessbasicdomlosslessPercentage of targets/vertices (δ)10%0.36 (0.1)3.44 (1.5)0.05 (0.0)0.12 (0.0)0.08 (0.0)0.11 (0.0)586.34 (1.2)23,018 (1502)0.29 (0.1)0.61 (0.2)0.12 (0.0)0.22 (0.0)122.24 (24)11,731 (1331)1.69 (1.5)5.82 (0.7)0.19 (0.1)0.71 (0.2)1614.14 (315)4.94 (1.1)13.92 (2.4)0.36 (0.1)1.01 (0.2)–30.66 (18)153.30 (26.1)0.67 (0.1)3.35 (0.5)–47.43 (21)250.82 (30.1)1.63 (0.3)9.72 (1.4)–100.25 (36)568.12 (69.4)7.32 (0.6)40.99 (8.7)––4.32 (0.5)50.88 (8.2)20%0.39 (0.2)7.51 (1.2)0.05 (0.0)0.16 (0.0)0.08 (0.0)0.15 (0.0)23.55 (5.9)1899 (393)0.59 (0.2)1.98 (0.6)0.15 (0.0)0.51 (0.0)506.68 (259)–1.47 (1.7)7.01 (1.3)0.23 (0.1)1.04 (0.3)–5.13 (1.9)28.18 (3.9)0.44 (0.1)2.37 (0.5)–25.23 (18)176.61 (30.2)1.22 (0.3)8.61 (1.0)–54.68 (24)341.63 (53.1)2.64 (0.7)16.87 (2.6)–120.08 (41)701.42 (100.7)4.34 (0.7)25.17 (6.6)––11.79 (3.0)90.47 (14.5)30%0.48 (0.2)15.01 (2.4)0.07 (0.0)0.29 (0.0)0.09 (0.0)0.36 (0.1)46.45 (20)5782 (963)0.39 (0.3)1.65 (0.7)0.15 (0.0)0.59 (0.1)1304 (516)–4.31 (3.6)16.64 (4.2)0.32 (0.1)1.52 (0.7)–6.10 (2.1)31.98 (4.1)0.80 (0.2)4.19 (0.9)–43.24 (20)302.68 (45.2)1.77 (0.3)13.00 (2.4)–83.51 (29)731.42 (85.2)4.57 (0.8)39.52 (5.7)––10.23 (1.9)96.16 (11.5)––24.09 (2.5)190.31 (26.6)40%0.57 (0.2)22.87 (3.0)0.08 (0.0)0.30 (0.1)0.10 (0.0)0.43 (0.1)77.38 (24)12,531 (1283)0.63 (0.3)2.89 (0.9)0.18 (0.0)0.82 (0.1)1941 (638)–2.59 (4.4)14.61 (5.3)0.36 (0.1)1.84 (0.8)–5.85 (2.5)40.51 (4.7)1.02 (0.2)7.21 (1.0)–22.83 (25)178.07 (54.6)3.10 (0.9)24.21 (4.2)–103.74 (70)867.42 (93.1)7.66 (1.9)66.42 (9.2)––14.89 (2.5)128.62 (16.3)––39.25 (3.7)366.12 (40.1)50%0.77 (0.2)39.01 (4.6)0.08 (0.0)0.37 (0.1)0.11 (0.0)0.57 (0.2)150.99 (84)33,912 (4142)0.51 (0.4)3.13 (0.9)0.21 (0.1)1.43 (0.3)2336 (679)–2.69 (5.7)15.91 (5.5)0.42 (0.1)2.32 (0.9)–8.88 (2.8)79.14 (11.5)1.61 (0.3)14.90 (2.7)–37.57 (29)285.53 (60.4)4.48 (1.3)33.84 (6.3)––10.55 (2.5)89.09 (13.6)––24.03 (2.9)230.63 (42.7)––59.25 (3.6)569.72 (237.6)160basic–––––N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123119Table 5 (continued)Number ofvertices (n)200domlosslessbasicdomlosslessPercentage of targets/vertices (δ)10%–9.15 (2.1)60.52 (8.5)––22.11 (5)142.12 (21.2)20%30%–28.78 (7.3)213.50 (32.5)––67.24 (9)561.25 (70.2)–51.25 (7.6)380.09 (56.2)––117.20 (10)856.29 (99.1)40%–95.47 (11)1067 (156)––220.73 (21)22,018 (583)50%–129.95 (15)1201 (137)––367.06 (51)4118 (973)Table 6Computing a non-deterministic patrolling strategy in closed perimetral settings: number of candidate best responses for the basic case (without any reduc-tion) and the dom case (once dominated strategies have been removed) when no abstractions or lossy abstractions are used.Number ofvertices(n)1620242832446484Percentage of targets/vertices (δ)10%basic321440169615112271281819444–66–112dom2210251832175522781811738497620%basic6421803611644157562057238896–208–391dom40173820622799401184521266126192nolossynolossynolossynolossynolossynolossynolossynolossy30%basic77441204917370236117308126580221–418–725dom4626833411448154701908430012323843240%basic1036316080231117314156410204–357–800––dom553710252121691959420411320930950%basic17880200120178160392224512288–528–1088––dom78491217615392206118296165273556Table 7Computing a non-deterministic patrolling strategy in closed perimetral settings: number of remaining vertices after the application of lossy abstractions.Vertices(n)1620242832446484Percentage of targets/vertices (δ)10%77.869611.611.41420%7.298.810.811.812162330%8.88.210.412.814.217.422.429.840%10.610.413.213.217.621.230.8–50%1012.414.416.218.22434.2–Table 8Computing a non-deterministic patrolling strategy in closed perimetral settings: computational times and standard deviations (between parentheses) withzero-sum (top line) and general-sum (bottom line) settings.Number ofvertices (n)16Percentage of targets/vertices (δ)10%20%30%40%50%basicdom5.87 (1.5)198.53 (25.37)3.32 (1.3)73.04 (12.29)5.83 (2.5)386.75 (42.73)3.79 (2.2)155.24 (20.64)7.92 (3.6)609.47 (120.34)3.82 (1.4)178.04 (35.72)12.85 (4.6)1324 (300.08)6.40 (2.7)358.00 (80.11)15.04 (5.1)1929 (546.99)7.37 (3.0)574.86 (130.73)(continued on next page)120N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123Table 8 (continued)Number ofvertices (n)20242832446484Percentage of targets/vertices (δ)10%1.58 (0.4)16.52 (3.75)14.89 (4)1190 (167.63)8.45 (3.0)380.12 (98.81)0.52 (0.25)9.42 (2.43)65.43 (35)6280 (1263)39.71 (21)2496 (762)0.56 (0.2)9.57 (2.09)85.46 (0.1)9562 (3712)150.67 (41)8421 (2536)0.37 (0.1)8.66 (2.33)246.38 (92)31,488 (10,535)130.46 (84)10,140 (3261)0.25 (0.1)4.52 (1.31)2390 (68)–794.68 (536)–4.74 (2.1)180.23 (65.11)––12.09 (3.9)572.62 (200.87)––69.22 (22)4862 (1.555)20%0.25 (0.1)4.25 (2.17)20.07 (9)1725 (238.41)5.96 (3.0)207.54 (50.24)0.25 (0.1)4.77 (1.00)82.88 (43)9421 (3172)33.03 (16)1875 (540)0.38 (0.2)11.58 (3.72)325.44 (2.0)51,938 (27,261)188.01 (101)18,225 (5116)2.27 (1.3)91.72 (19.85)706.17 (278)–346.72 (130)47,420 (17,822)2.57 (1.7)115.31 (33.71)3306 (964)–3076 (1029)–6.35 (4.1)387.62 (128.51)––208.26 (100)2654 (686.99)––1381 (546)–lossybasicdomlossybasicdomlossybasicdomlossybasicdomlossybasicdomlossybasicdomlossybasicdomlossy30%0.37 (0.2)9.62 (5.63)39.81 (10)4521 (550.98)37.41 (17)3007 (784.21)0.64 (0.3)21.53 (4.09)214.00 (51)37,009 (9730)137.96 (30)14,422 (5927)1.94 (1.1)93.11 (25.82)637.50 (12)–330.47 (176)48,722 (17,428)11.69 (3.6)788.72 (204.57)1774 (392)–555.53 (270)–15.52 (5.9)1352 (273.67)–2691 (1122)–120.51 (65)–––862.59 (166)–––3261 (853)–40%1.95 (0.7)72.15 (10.80)70.07 (25)11,315 (2452)40.55 (20)3952 (996.55)2.07 (1.6)100.57 (33.72)332.17 (111)73,962 (23,662)111.86 (35)13,526 (4227)6.28 (4.1)433.73 (102.37)1064.62 (14)–468.69 (152)78,625 (22,945)16.99 (21)1567 (308.63)1885 (413)–673.56 (339)–61.39 (56)6725 (2.887)––515.58 (143)–––2319 (411)––––50%1.44 (0.8)70.56 (11.67)93.61 (26)18,362 (4873)54.87 (21)6426 (1652)5.51 (2.9)419.00 (167.52)406.66 (101)–219.53 (53)30,624 (10,826)11.17 (3.5)1042 (296)1178 (22)–494.90 (282)–37.39 (41)4412 (1037)2319 (415)–1474 (460)–158.57 (65)26,735 (8.113)––1494 (981)–––3276 (534)––––Table 9Computing a non-deterministic patrolling strategy in arbitrary settings: number of candidate best responses for the basic case (without any reduction) andthe dom case (once dominated strategies have been removed) when no abstractions or lossy abstractions are used.Number ofvertices(n)5075100133166Percentage of targets/vertices (δ)5%basic15075300136–143–232–413dom228431849657910%basic250110600272–332–468–663dom8126915310216019520%basic500202–420–560–702––dom2378815824730330%basic750345–638–870––––nolossynolossynolossynolossynolossydom321163262271N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123121Table 10Computing a non-deterministic patrolling strategy in arbitrary settings: number of remaining vertices after the application of lossless and lossy abstractions.Number ofvertices (n)5075100133166losslesslossylosslesslossylosslesslossylosslesslossylosslesslossyPercentage of targets/vertices (δ)5%36.2525.2551.7534.33–28.67–34.33–57.6610%40.2521.7553.534–33.25–35.5–3920%4020.25–28.25–28–27––30%4223.25–29–29––––Table 11Computing a non-deterministic patrolling strategy in arbitrary settings: computational times and standard deviations (between parentheses) with zero-sum(top line) and general-sum (bottom line) settings.Number ofvertices (n)5075100133166losslesslossylosslesslossylosslesslossylosslesslossylosslesslossyPercentage of targets/vertices (δ)5%10.34 (6.8)269.13 (35.12)0.32 (0.14)2.56 (0.53)311.14 (285.54)13,624 (4852)0.53 (0.3)9.58 (1.28)–6.79 (3.47)332.71 (49.31)–115.62 (29.72)7435 (994)–625.82 (296.56)49,055 (11,462)10%210.97 (184.57)15,084 (2367)1.39 (1.32)33.61 (4.59)4615 (6421)–53.77 (47.04)2849 (771)–1017 (882.57)––3710 (938)––11,442 (1318)–20%2923 (1479.8)–110.05 (137.67)9415 (1089)–1219 (695.3)––5704 (2741)––4375 (1399)–––30%6972 (4068)–1073 (1310)––6351 (2898)––4045 (1644)––––––Table 12Defender’s expected utility by using first-order Markovian strategies (with lossless and lossy abstractions) for different zero-sum and (between parentheses)general-sum settings when there is a non-first-order Markovian deterministic strategy with a value of 1.Vertices(n)40506075ReferenceslosslesslossylosslesslossylosslesslossylosslesslossyPercentage of targets/vertices (δ)5%0.997 (0.998)0.888 (0.902)0.997 (0.994)0.848 (0.899)0.997 (0.998)0.940 (0.952)0.991 (0.989)0.924 (0.953)10%0.998 (0.998)0.920 (0.937)0.998 (0.999)0.951 (0.976)0.998 (0.994)0.961 (0.983)0.999 (0.999)0.977 (0.982)20%0.999 (0.999)0.970 (0.965)0.999 (0.999)0.990 (0.992)0.999 (0.999)0.981 (0.996)0.999 (0.998)0.994 (0.997)[1] M. Adler, H. Räcke, N. Sivadasan, C. Sohler, B. Vöcking, Randomized pursuit–evasion in graphs, Combinatorics, Probability and Computing 12 (2003)225–244.[2] N. Agmon, On events in multi-robot patrol in adversarial environments, in: Proceedings of International Joint Conference on Autonomous Agents andMulti-Agent Systems (AAMAS), 2010, pp. 591–598.122N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123[3] N. Agmon, S. Kraus, G. Kaminka, Multi-robot perimeter patrol in adversarial settings, in: Proceedings of the IEEE International Conference on Roboticsand Automation (ICRA), 2008, pp. 2339–2345.[4] N. Agmon, S. Kraus, G. Kaminka, Uncertainties in adversarial patrol, in: Proceedings of the International Joint Conference on Autonomous Agents andMulti-Agent Systems (AAMAS), 2009, pp. 1267–1268.[5] N. Agmon, S. Kraus, G. Kaminka, V. Sadov, Adversarial uncertainty in multi-robot patrol, in: Proceedings of the International Joint Conference onArtificial Intelligence (IJCAI), 2009, pp. 1811–1817.[6] N. Agmon, V. Sadov, G. Kaminka, S. Kraus, The impact of adversarial knowledge on adversarial planning in perimeter patrol, in: Proceedings of theInternational Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2008, pp. 55–62.[7] A. Almeida, G. Ramalho, H. Santana, P. Tedesco, T. Menezes, V. Corruble, Y. Chevaleyre, Recent advances on multi-agent patrolling, in: Proceedings ofthe Brazilian Symposium on Artificial Intelligence (SBIA), 2004, pp. 126–138.[8] S. Alpen, Infiltration games on arbitrary graphs, Journal of Mathematical Analysis and Applications 163 (1) (1992) 286–288.[9] F. Amigoni, N. Gatti, A. Ippedico, A game-theoretic approach to determining efficient patrolling strategies for mobile robots, in: Proceedings of theIEEE/WIC/ACM International Conference on Agent Intelligent Technology (IAT), Sydney, Australia, 2008, pp. 500–503.[10] N. Basilico, N. Gatti, Automated abstractions for patrolling security games, in: Proceedings of the National Conference on Artificial Intelligence (AAAI),2011, pp. 1096–1099.[11] N. Basilico, N. Gatti, F. Amigoni, Developing a deterministic patrolling strategy for security agents, in: Proceedings of the IEEE/WIC/ACM InternationalConference on Intelligent Agent Technology (IAT), 2009, pp. 565–572.[12] N. Basilico, N. Gatti, F. Amigoni, Leader–follower strategies for robotic patrolling in environments with arbitrary topologies, in: Proceedings of theInternational Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2009, pp. 57–64.[13] N. Basilico, N. Gatti, T. Rossi, S. Ceppi, F. Amigoni, Extending algorithms for mobile robot patrolling in the presence of adversaries to more realisticsettings, in: Proceedings of the IEEE/WIC/ACM International Conference on Intelligent Agent Technology (IAT), 2009, pp. 557–564.[14] N. Basilico, N. Gatti, P. Testa, Talos: A tool for designing security applications with mobile patrolling robots, in: Proceedings of the International JointConference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2011, pp. 1317–1318.[15] N. Basilico, N. Gatti, F. Villa, Asynchronous multi-robot patrolling against intrusion in arbitrary topologies, in: Proceedings of the National Conferenceon Artificial Intelligence (AAAI), 2010, pp. 1224–1229.[16] N. Basilico, D. Rossignoli, N. Gatti, F. Amigoni, A game-theoretical model applied to an active patrolling camera, in: Proceedings of the InternationalConference on Emerging Security Technologies (EST), 2010, pp. 130–135.[17] M. Bazaraa, H. Sherali, C. Shetty, Nonlinear Programming: Theory and Algorithms, Wiley, 2006.[18] Y. Chevaleyre, Theoretical analysis of the multi-agent patrolling problem, in: Proceedings of the IEEE/WIC/ACM International Conference on AgentIntelligent Technology (IAT), Beijing, China, 2004, pp. 302–308.[19] N. Christofides, J. Beasley, The period routing problem, Networks 14 (2) (1984) 237–256.[20] V. Conitzer, T. Sandholm, Computing the optimal strategy to commit to, in: Proceedings of the ACM Conference on Electronic Commerce (EC), 2006,pp. 82–90.[21] D. Draper, A.K. Jónsson, D.P. Clements, D. Joslin, Cyclic scheduling, in: Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),1999, pp. 1016–1021.[22] Y. Elmaliach, N. Agmon, G. Kaminka, Multi-robot area patrol under frequency constraints, in: Proceedings of the IEEE International Conference onRobotics and Automation (ICRA), 2007, pp. 385–390.[23] Y. Elmaliach, A. Shiloni, G. Kaminka, A realistic model of frequency-based multi-robot polyline patrolling, in: Proceedings of the International JointConference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2008, pp. 63–70.[24] M.M. Flood, The hide and seek game of von Neumann, Management Science 18 (5) (1972) 107–109.[25] R. Fourer, D.M. Gay, B.W. Kernighan, A modeling language for mathematical programming, Management Science 36 (5) (1990) 519–554.[26] P. Francis, K. Smilowitz, M. Tzur, The period vehicle routing problem with service choice, Transportation Science 40 (4) (2006) 439–454.[27] D. Fudenberg, J. Tirole, Game Theory, The MIT Press, 1991.[28] S. Gal, Search Games, Academic Press, 1980.[29] N. Gatti, Game theoretical insights in strategic patrolling: Model and algorithm in normal form, in: Proceedings of the European Conference on ArtificialIntelligence (ECAI), 2008, pp. 403–407.[30] A. Gilpin, T. Sandholm, Lossless abstraction of imperfect information games, Journal of the ACM 54 (5) (2007).[31] A. Gilpin, T. Sandholm, T.B. Sørensen, A heads-up no-limit Texas hold’em poker player: Discretized betting models and automatically generatedequilibrium-finding programs, in: Proceedings of the International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2008,pp. 911–918.[32] A. Girard, A. Howell, J.K. Hedrick, Border patrol and surveillance missions using multiple unmanned air vehicles, in: Proceedings of the IEEE Conferenceon Decision and Control (CDC), 2004, pp. 620–625.[33] A. Glad, O. Simonin, O. Buffet, F. Charpillet, Theoretical study of ant-based algorithms for multi-agent patrolling, in: Proceedings of European Conferenceon Artificial Intelligence (ECAI), 2008, pp. 626–630.[34] D. Gulczynski, B. Golden, E. Wasil, The period vehicle routing problem: New heuristics and real-world variants, Transportation Research Part E: Logisticsand Transportation Review 47 (5) (2011) 648–668.[35] Y. Guo, L. Parker, R. Madhavan, Collaborative robots for infrastructure security applications, in: Mobile Robots: The Evolutionary Approach, in: BookSeries on Intelligent Systems Engineering, vol. 50, 2007, pp. 185–200.[36] A. Howard, N. Roy, The robotics data set repository (radish), 2003.[37] ILOG CP, http://www.ilog.com/products/cp/.[38] V. Isler, S. Kannan, S. Khanna, Randomized pursuit–evasion in a polygonal environment, IEEE Transactions on Robotics 5 (21) (2005) 864–875.[39] M. Jain, J. Pita, M. Tambe, F. Ordóñez, P. Paruchuri, S. Kraus, Bayesian Stackelberg games and their application for security at Los Angeles InternationalAirport, SIGecom Exchanges 7 (2) (2008).[40] C. Kiekintveld, M. Jain, J. Tsai, J. Pita, F. Ordóñez, M. Tambe, Computing optimal randomized resource allocations for massive security games, in:Proceedings of the International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2009, pp. 689–696.[41] A. Kolen, A. Kan, H. Trienekens, Vehicle routing with time windows, Operations Research 35 (2) (1987) 266–273.[42] D. Koller, N. Megiddo, B. von Stengel, Efficient computation of equilibria for extensive two-person games, Games and Economic Behavior 14 (2) (1996)220–246.[43] A. Kolling, S. Carpin, Extracting surveillance graphs from robot maps, in: Proceedings of the IEEE International Conference on Intelligent Robots andSystems (IROS), 2008, pp. 2323–2328.[44] D. Kreps, R. Wilson, Sequential equilibria, Econometrica 50 (4) (1982) 863–894.[45] J. Letchford, V. Conitzer, Computing optimal strategies to commit to in extensive-form games, in: Proceedings of the ACM Conference on ElectronicCommerce (EC), 2010, pp. 83–92.[46] A. Machado, G. Ramalho, J.-D. Zucker, A. Drogoul, Multi-agent patrolling: An empirical analysis of alternative architectures, in: Proceedings of the ThirdInternational Workshop on Multi-Agent-Based Simulation (MABS2002), vol. 2581, 2003, pp. 155–170.N. Basilico et al. / Artificial Intelligence 184–185 (2012) 78–123123[47] J.S. Marier, C. Besse, B. Chaib-draa, Solving the continuous time multiagent patrol problem, in: Proceedings of IEEE International Conference on Roboticsand Automation (ICRA), 2010, pp. 941–946.[48] L. Martins-Filho, E. Macau, Patrol mobile robots and chaotic trajectories, in: Mathematical Problems in Engineering, Hindawi, 2007.[49] C. Papadimitriou, Computational Complexity, Addison–Wesley, 1993.[50] P. Paruchuri, J. Pearce, J. Marecki, M. Tambe, F. Ordonez, S. Kraus, Playing games for security: An efficient exact algorithm for solving Bayesian Stackel-berg games, in: Proceedings of the International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2008, pp. 895–902.[51] P. Paruchuri, J. Pearce, M. Tambe, F. Ordonez, S. Kraus, An efficient heuristic approach for security against multiple adversaries, in: Proceedings of theInternational Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2007, pp. 311–318.[52] J. Pita, M. Jain, J. Marecki, F. Ordonez, C. Portway, M. Tambe, C. Western, P. Paruchuri, S. Kraus, Deployed ARMOR protection: The application of a gametheoretic model for security at the Los Angeles International Airport, in: Proceedings of the International Joint Conference on Autonomous Agents andMulti-Agent Systems (AAMAS), 2010, pp. 125–132.[53] B. Raa, E. Aghezzaf, A practical solution approach for the cyclic inventory routing problem, European Journal of Operational Research 192 (2) (2009)429–441.[54] S. Ruan, C. Meirina, F. Yu, K. Pattipati, R. Popp, Patrolling in a stochastic environment, in: Proceeding of the International Command and ControlResearch and Technology Symposium (CCRTS), 2005.[55] A. Rubinstein, Perfect equilibrium in a bargaining model, Econometrica 50 (1) (1982) 97–109.[56] W. Ruckle, R. Fennel, P.T. Holmes, C. Fennemore, Ambushing random walk. I: Finite models, Operations Research 24 (1976) 314–324.[57] S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, third ed., Prentice Hall, 2010.[58] T. Sak, J. Wainer, S.K. Goldenstein, Probabilistic multiagent patrolling, in: Proceedings of the Brazilian Symposium on Artificial Intelligence (SBIA), 2008,pp. 124–133.[59] H. Santana, G. Ramalho, V. Corruble, B. Ratitch, Multi-agent patrolling with reinforcement learning, in: Proceedings of the Third International JointConference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2004, pp. 1120–1127.[60] Y. Shoham, K. Leyton-Brown, Multiagent Systems: Algorithmic, Game Theoretic and Logical Foundations, Cambridge University Press, 2008.[61] Stanford Business Software Inc., http://www.sbsi-sol-optimize.com/.[62] J. Tsai, Z. Yin, J.-Y. Kwak, D. Kempe, C. Kiekintveld, M. Tambe, How to protect a city: Strategic security placement in graph-based domains, in: Proceed-ings of the International Joint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2010, pp. 1453–1454.[63] J. Tsitsiklis, Special cases of traveling salesman and repairman problems with time windows, Networks 22 (3) (1992) 263–282.[64] B. von Stengel, S. Zamir, Leadership with commitment to mixed strategies, CDAM Research Report LSE-CDAM-2004-01, London School of Economics,2004.[65] A. Wahsburn, K. Wood, Two-person zero-sum games for network interdiction, Operations Research 43 (2) (1995) 243–251.[66] V. Yanovski, I. Wagner, A. Bruckstein, A distributed ant algorithm for efficiently patrolling a network, Algorithmica 37 (2003) 165–186.