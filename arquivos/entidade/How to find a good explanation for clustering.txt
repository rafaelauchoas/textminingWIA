Artificial Intelligence 322 (2023) 103948Contents lists available at ScienceDirectArtificial Intelligencejournal homepage: www.elsevier.com/locate/artintHow to find a good explanation for clustering? ✩Sayan Bandyapadhyay a, Fedor V. Fomin b, Petr A. Golovach b,∗Nidhi Purohit b, Kirill Simonov da Department of Computer Science, Portland State University, United States of Americab Department of Informatics, University of Bergen, Norwayc LIRMM, Université Montpellier, CNRS, Montpellier, Franced Hasso Plattner Institute, University of Potsdam, Germanya r t i c l e i n f oa b s t r a c t, William Lochet c, Article history:Received 2 November 2022Received in revised form 3 March 2023Accepted 22 May 2023Available online 1 June 2023Keywords:Explainable clusteringClustering with outliersMultivariate analysisk-means and k-median clustering are powerful unsupervised machine learning techniques. However, due to complicated dependencies on all the features, it is challenging to interpret the resulting cluster assignments. Moshkovitz, Dasgupta, Rashtchian, and Frost proposed an elegant model of explainable k-means and k-median clustering in ICML 2020. In this model, a decision tree with k leaves provides a straightforward characterization of the data set into clusters.We study two natural algorithmic questions about explainable clustering. (1) For a given clustering, how to find the “best explanation” by using a decision tree with k leaves? (2) For a given set of points, how to find a decision tree with k leaves minimizing the k-means/median objective of the resulting explainable clustering? To address the first question, we introduce a new model of explainable clustering. Our model, inspired by the notion of outliers in robust statistics, is the following. We are seeking a small number of points (outliers) whose removal makes the existing clustering well-explainable. For addressing the second question, we initiate the study of the model of Moshkovitz et al. from the perspective of multivariate complexity. Our rigorous algorithmic analysis sheds some light on the influence of parameters like the input size, dimension of the data, the number of outliers, the number of clusters, and the approximation ratio, on the computational complexity of explainable clustering.© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).1. IntroductionInterpretation or explanation of decisions produced by learning models, including clustering, is a significant direction in machine learning (ML) and artificial intelligence (AI), and has given rise to the subfield of Explainable AI. Explainable AI has attracted a lot of attention from researchers in recent years (see the surveys by Carvalho et al. [6] and Marcinkeviˇcs and Vogt [39]). All these works can be divided into two main categories: pre-modeling [48,47,26,16,34] and post-modelingA preliminary version of this paper appeared as an extended abstract in the proceedings of AAAI 2022. The research leading to these results has been✩supported by the Research Council of Norway via the project BWCA (grant no. 314528), European Research Council (ERC) via grant LOPPRE, reference 819416, and DFG Research Group ADYN via grant DFG 411362735.* Corresponding author at: University of Bergen, PB 7803, N-5020, Bergen, Norway.(W. Lochet), kirillsimonov@gmail.com (K. Simonov).E-mail addresses: sayanb@pdx.edu (S. Bandyapadhyay), fedor.fomin@uib.no (F.V. Fomin), petr.golovach@uib.no (P.A. Golovach), william.lochet@gmail.comhttps://doi.org/10.1016/j.artint.2023.1039480004-3702/© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Fig. 1. (a) An example of an optimal solution to 5-means. (b) An explainable 5-means clustering and (c) the corresponding threshold tree. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)[43,45,4,46,35] explainability. While post-modeling explainability focuses on giving the reasoning behind decisions made by black box models, pre-modeling explainability deals with ML systems that are inherently understandable or perceivable by humans. One of the canonical approaches to pre-modeling explainability builds on decision trees [40,42]. In fact, a significant amount of work on explainable clustering is based on unsupervised decision trees [3,19,23,24,33,41]. In each node of the decision tree, the data is partitioned according to some feature’s threshold value. While such a threshold tree provides a clear interpretation of the resulting clustering, its cost measured by the standard k-means/median objective can be significantly worse than the cost of the optimal clustering. Thus, on the one hand, the efficient algorithms developed for k-means/median clustering [1] are often challenging to explain. On the other hand, the easily explainable models could output very costly clusterings. Subsequently, Moshkovitz et al. [41], in a fundamental work, posed the natural algorithmic question of whether it is possible to kill two birds with one stone. To be precise, is it possible to design an efficient procedure for clustering that• Is explainable by a small decision tree; and• Does not cost significantly more than the cost of an optimal k-means/median clustering?To address this question, Moshkovitz et al. [41] introduced explainable k-means/median clustering. In this scheme, a clustering is represented by a binary (threshold) tree whose leaves correspond to clusters, and each internal node corre-sponds to partitioning a collection of points by a threshold on a fixed coordinate. Thus, the number of leaves in such a tree is k, the number of clusters sought. Also, any cluster assignment can be explained by the thresholds along the corresponding root-leaf path. For example, consider Fig. 1: Fig. 1a shows an optimal 5-means clustering of a 2D data set; Fig. 1b shows an explainable 5-means clustering of the same data set; The threshold tree inducing the explainable clustering is shown in Fig. 1c. The tree has five leaves, corresponding to 5 clusters. Note that in this model of explainability, any clustering has a clear geometric interpretation, where each cluster is formed by a set of axis-aligned cuts defined by the tree. As Moshkovitz et al. argue, the classical k-means clustering algorithm leads to more complicated clusters while the threshold tree leads to an easy explanation. The advantage of the explainable approach becomes even more evident in higher dimensions when many feature values in k-means contribute to the formation of the clusters.Moshkovitz et al. [41] define the quality of any explainable clustering as the “cost of explainability”, that is the ratio of the cost of the explainable clustering to the cost of an optimal clustering. Subsequently, they obtain efficient algorithms for computing explainable clusterings whose “cost of explainability” is O(k) for k-median and O(k2) for k-means.In this work, we propose a new model for explaining a clustering, called Clustering Explanation. Our Our contributionsapproach to explainability is inspired by the research on robustness in statistics and machine learning, especially the vast field of outlier detection and removal in the context of clustering [10,20,17,9,7,25,30]. In this model, we are given a k-means/median clustering and we would like to explain the clustering by a threshold tree after removing a subset of points. To be precise, we are interested in finding a subset of points S (which are to be removed) and a threshold tree T such that the explainable clustering induced by the leaves of T is exactly the same as the given clustering after removing the points in S. For the given clustering, we define an optimal (or best) explainable clustering to be the one that minimizes the size of S, i.e. for which the given clustering can be explained by removing the minimum number of points. Thus in Clustering Explanation we measure the “explainability” as the number of outlying points whose removal turns the given clustering into an explainable clustering. The reasoning behind the new measure of cluster explainability is the following. In certain situations, we would be satisfied with a small decision tree explaining a clustering of all but a few outlying data points. We note that for a given clustering that is already an explainable clustering, i.e. can be explained by a threshold tree, the size of S is 0.In Fig. 2, we provide an example of an optimal 5-means clustering of exactly the same data set as in Fig. 1. However, the new explainable clustering is obtained in a different way. If we remove a small number of points (in Fig. 2b these are the 9 red larger points), then the explainable clustering is the same as the optimal clustering after removing those 9 points.2S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Fig. 2. (a) An optimal 5-clustering and (b) an explainable clustering that fits this clustering after removing the larger (red) points.We note that Clustering Explanation corresponds to the classical machine learning setting of interpreting a black-box model, i.e. it lies within the scope of post-modeling explainability. Surprisingly, this area is widely unexplored when it comes to the rigorous algorithmic analysis of clustering explanation. Consequently, we study Clustering Explanation from the perspective of computational complexity. Our new model naturally raises the following algorithmic questions: (i) Given a clustering, how efficiently can one decide whether the clustering can be explained by a threshold tree (without removing any points)?and (ii) Given a clustering and an integer s, how efficiently can one decide whether the clustering can be explained by removing spoints?In our work, we design a polynomial time algorithm that resolves the first question. Regarding the second question, we give an algorithm that in time 22 min{s,k} · n2d · (dn)O(1) decides whether a given clustering of n points in Rd could be O(1) time (k − 1)-approximation algorithm for Clustering Explanation. explained by removing s points. We also give an nThat is, we give a polynomial time algorithm that returns a solution set of at most s(k − 1) points that are to be removed, whereas any best explainable clustering removes s points. Moreover, we provide an efficient data reduction procedure that reduces an instance of Clustering Explanation to an equivalent instance with at most r = 2(s + 1)dk points in Rdwith integer coordinates within the range {1, . . . , r}. The procedure can be used to speed up any algorithm for Clustering Explanation, as long as n > 2(s +1)dk. We complement our algorithms by showing a hardness lower bound. In particular, we show that Clustering Explanation cannot be approximated within a factor of F (s) in time f (s)(nd)o(s), for any computable functions F and f , unless the Exponential Time Hypothesis (ETH) [28] fails. All these results appear in Section 3.We also provide new insight into the computational complexity of the model of Moshkovitz et al. [41]. While the vanilla k-median and k-means problems are NP-hard for k = 2 [2,14,12] or d = 2 [36], this is not the case for explainable clustering! We design two simple algorithms computing optimal (best) explainable clustering with k-means/median objective that run in time (4nd)k+O(1) and n2d · nO(1), respectively. Hence for constant k or constant d, an optimal explainable clustering can be computed in polynomial time. The research on approximation algorithms on the “cost of explainability” in [41,8,15,22,32,37]implicitly assumes that solving the problem exactly is NP-hard. However, we did not find a proof of this fact in the literature. To fill this gap, we obtain the following hardness lower bound: An optimal explainable clustering cannot be found in f (k) · no(k) time for any computable function f (·), unless the Exponential Time Hypothesis (ETH) fails. This lower bound demonstrates that asymptotically the running times of our simple algorithms are unlikely to be improved. Our reduction also yields that the problem is NP-hard. These results are described in Section 4.Finally, we combine the above two explainability models to obtain the Approximate Explainable Clustering model: For a collection of n points in Rd and a positive real constant ε < 1, we seek whether we can identify at most εn outliers, such that the cost of explainable k-means/median of the remaining points does not exceed the optimal cost of an explainable k-means/median clustering of the original data set. Thus, if we are allowed to remove a small number of points, can we do as good as any original optimal solution? While our hardness result of Section 4 holds for explaining the whole dataset, by “sacrificing” a small fraction of points it might be possible to solve the problem more efficiently. And indeed, for this model, we obtain an algorithm whose running time ( 4d(k+(cid:3))O(1) has a significantly better dependence on d and k. For )k · nexample, compare this with the above time bounds of (4nd)k+O(1) and n2d · (dn)O(1). This algorithm appears in Section 5. See Table 1 for a summary of all our results.(cid:3)Related work A recent series of work [8,15,22,32,37,38] have further investigated the “cost of explainability”, that is the ratio of the cost of the explainable clustering to the cost of an optimal clustering. The work of these papers has significantly improved the bound of O(k) for k-median and O(k2) for k-means, which were shown by Moshkovitz et al. [41]. The state-of-the-art bound for k-median is O (poly log k) and for k-means O (k · poly log k). We note that all of these works are based on the random sampling of orthogonal cuts and are different from the techniques we use. In another related work, Izza et al. [29] studied the redundancy in explanations provided by decision trees and gave efficient algorithms to eliminate such 3S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Table 1A summary of our results.ModelClustering ExplanationExplainable ClusteringApproximate Explainable ClusteringO(1)Algorithms/Upper bounds22 min{s,k}n2dn(k − 1)-approximation Reduction to O(sdk) points(4nd)k+O(1) n2d · n( 4d(k+(cid:3)))k · n(cid:3)O(1)O(1)Hardness/Lower boundsNo F (s)-approx. in f (s)(nd)o(s)f (k) · no(k)redundancy. We note that investigation of such redundancy is beyond the scope of our work. Finally, we mention the recent survey of Cañete-Sifuentes, Monroy, and Medina-Pérez [5] about experimental results for multivariate decision trees.2. Preliminariesk-means/median Given a collection X = {x1, . . . , xn} of n points in Rd and a positive integer k, the task of k-clustering is to partition X into k parts C1, . . . , Ck, called clusters, such that the cost of clustering is minimized. We follow the convention in the previous work [41] for defining the cost. In particular, for k-means, we consider the Euclidean distance, and for (cid:3)k-median, the Manhattan distance. For a collection of points Xof Rd, we define(cid:5)c − x(cid:5)22,(1)(cid:3)cost2(X) = minc∈Rd(cid:2)x∈X(cid:3)and call the point c(or simply means) cost is cost2(C1, . . . , Ck) =(cid:3)cost1(Xwe call the k-median (or simply median) cost of the clustering.(cid:3)∗ ∈ Rd minimizing the sum in (1) the mean of X. For a clustering {C1, . . . , Ck} of X ⊆ Rd, its k-means(cid:3)ki=1 cost2(Ci). With respect to the Manhattan distance, we define analogously (cid:3)ki=1 cost1(Ci), which (cid:3)x∈X(cid:3) (cid:5)c − x(cid:5)1, which is minimized at the median of X, and cost1(C1, . . . , Ck) =(cid:3)) = minc∈RdExplainable clustering For a vector x ∈ Rd, we use x[i] to denote the i-th element (coordinate) of the vector for i ∈ {1, . . . , d}. Let X be a collection of points of Rd. For i ∈ {1, . . . , d} and θ ∈ R, we define Cuti,θ (X) = (X1, X2), where {X1, X2} is a partition of X withX1 = {x ∈ X | x[i] ≤ θ} and X2 = {x ∈ X | x[i] > θ}.Then, given a collection X ⊆ Rd and a positive integer k, we cluster X as follows. If k = 1, then X is the unique cluster. If k = 2, then we choose i ∈ {1, . . . , d} and θ ∈ R and construct two clusters C1 and C2, where (C1, C2) = Cuti,θ (X). For k > 2, we select i ∈ {1, . . . , d} and θ ∈ R, and construct a partition (X1, X2) = Cuti,θ (X) of X. Then clustering of X is defined recursively as the union of a k1-clustering of X1 and a k2-clustering of X2 for some integers k1 and k2 such that k1 + k2 = k. We say that a clustering {C1, . . . , Ck} is an explainable k-clustering of a collection of points X ⊆ Rd if C1, . . . , Ck can be constructed by the described procedure.Threshold treeIt is useful to represent an explainable k-clustering as a triple (T , k, ϕ), called a threshold tree, where Tis a rooted binary tree with k leaves, where each nonleaf node has two children called left and right, respectively, and ϕ : U → {1, . . . , d} × R, where U is the set of nonleaf nodes of T . For each node v of T , we define a collection of points Xv ⊆ X. For the root r, Xr = X. Let v be a nonleaf node of T and let u and w be its left and right children, respectively, and assume that Xv is constructed. We set (Xu, Xw ) = Cutϕ(v)(X). If v is a leaf, then Xv is a cluster. A clustering {C1, . . . , Ck} is an explainable k-clustering of a collection of points X ⊆ Rd if there is a threshold tree (T , k, ϕ) such that C1, . . . , Ck are the clusters corresponding to the leaves of T . Note that T is a full binary tree with k leaves and the total number of such trees is the (k − 1)-th Catalan number, which is upper bounded by 4k . Note also that a full binary tree with k leaves has exactly k − 1 nonleaf nodes.For a collection X = {x1, . . . , xn} of n points and i ∈ {1, . . . , d}, we denote by coordi(X) the set of distinct values of i-th coordinates x j[i] for j ∈ {1, . . . , n}. It is easy to observe that in the construction of a threshold tree for a set of points X ⊆ Rd, it is sufficient to consider cuts Cuti,θ with θ ∈ coordi(X); we call such values of θ and cuts canonical. We say that a threshold tree (T , k, ϕ) for a collection of points X ⊆ Rd is canonical, if for every nonleaf node u ∈ V (T ), ϕ(u) = (i, θ)where θ ∈ coordi(X). Throughout the paper, we consider only canonical threshold trees.Parameterized complexity and ETH A parameterized problem (cid:6) is a subset of (cid:7)∗ × N, where (cid:7) is a finite alphabet. Thus, an instance of (cid:6) is a pair (I, k), where I ⊆ (cid:7)∗and k is a nonnegative integer called a parameter. A parameterized problem (cid:6)is fixed-parameter tractable (FPT) if it can be solved in f (k) · |I|O(1) time for some computable function f (·). Parameterized complexity theory also provides tools to refute the existence of an FPT algorithm for a parameterized problem. The standard way is to show that the considered problem is hard in the parameterized complexity classes W[1] or W[2]. We refer to the book [11] for the formal definitions of the parameterized complexity classes. The basic complexity assumption of the theory 4S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Fig. 3. (a) An example 3-means clustering. (b) The first (vertical) cut. (c) The second (horizontal) cut.is that for the class FPT, formed by all parameterized fixed-parameter tractable problems, FPT ⊂ W[1] ⊂ W[2]. The hardness is proved by demonstrating a parameterized reduction from a problem known to be hard in the considered complexity class. A parameterized reduction is a many-one reduction that takes an input (I, k) of the first problem, and in f (k)|I|O(1) time (cid:3) ≤ g(k), where f (·) and g(·) are computable functions. outputs an equivalent instance (IAnother way to obtain lower bounds is to use the Exponential Time Hypothesis (ETH) formulated by Impagliazzo, Paturi and Zane [27,28]. For an integer k ≥ 3, let δk be the infimum of the real numbers c such that the k-Satisfiability problem can be solved in time O(2cn), where n is the number of variables. The Exponential Time Hypothesis states that δ3 > 0. In particular, ETH implies that k-Satisfiability cannot be solved in time 2o(n)n(cid:3)) of the second problem with kO(1).(cid:3), kA kernelization (or kernel) for a parametrized problem (cid:6) is a polynomial time algorithm that, given an instance (I, k) of (cid:3) ≤ f (k) for a computable (cid:6), outputs an instance (Ifunction f (·). The function f (·) is called the kernel size; a kernel is polynomial if f (·) is a polynomial. It can be shown that every decidable FPT problem admits a kernel. However, it is unlikely that all FPT problems have polynomial kernels. We refer to [11] and the recent book on kernelization of Fomin et al. [18] for details.(cid:3)) of (cid:6) such that (i) (I, k) ∈ (cid:6) if and only if (I(cid:3)) ∈ (cid:6) and (ii) |I(cid:3)| + k(cid:3), k(cid:3), k3. Clustering explanationClustering explanation In the Clustering Explanation problem, the input contains a k-clustering {C1, . . . , Ck} of X ⊆ Rd and a nonnegative integer s, and the task is to decide whether there is a collection of points W ⊆ X with |W | ≤ s such that {C1 \ W , . . . , Ck \ W } is an explainable k-clustering. Note that some Ci \ W may be empty here.3.1. A polynomial-time (k − 1)-approximationIn the optimization version of Clustering Explanation, we are given a k-clustering C = {C1, . . . , Ck} of X in Rd, and the goal is to find a minimum-sized subset W ⊆ X such that {C1 \ W , . . . , Ck \ W } is an explainable clustering. In the following, we design an approximation algorithm for this problem based on a greedy scheme.For any subset W ⊆ X, let C − W = {C1 \ W , . . . , Ck \ W }. Also, for any subset Y ⊆ X, define the clustering induced by Y as C(Y ) = {C1 ∩ Y , . . . , Ck ∩ Y }. Denote by OPT(Y ) the size of the minimum-sized subset W such that the clustering C(Y ) − Wis explainable. First, we have the following simple observation which follows trivially from the definition of OPT(.).Observation 1. For any subset Y ⊆ X, OPT(Y ) ≤ OPT(X).For any cut (i, θ) where i ∈ {1, . . . , d} and θ ∈ coordi(X), let L(i, θ) = {x ∈ Rd | x[i] ≤ θ} and R(i, θ) = {x ∈ Rd | x[i] > θ}.Lemma 1. Consider any subset Y ⊆ X such that C(Y ) contains at least two non-empty clusters. It is possible to select a cut (i, θ) for i ∈ {1, . . . , d} and θ ∈ coordi(Y ), and a subset W ⊆ Y , in polynomial time, such that (i) each cluster in C(Y ) − W is fully contained in either L(i, θ) or in R(i, θ), (ii) at least one cluster in C(Y ) − W is in L(i, θ), (iii) at least one cluster in C(Y ) − W is in R(i, θ) and (iv) size of W is at most OPT(Y ).Before we prove this lemma, we show how to use it to design the desired approximation algorithm.The algorithm We start with the set of all points X. We apply the algorithm in Lemma 1 with Y = X to find a cut (i, θ) and a subset W 1 ⊆ X such that each cluster in C(X) − W 1 is fully contained in either L(i, θ) or in R(i, θ). Let X1 = (X \ W 1) ∩ L(i, θ)and X2 = (X \ W 1) ∩ R(i, θ). We recursively apply the above step on both X1 and X2 separately. If at some level the point set is a subset of a single cluster, we simply return.An illustration of the algorithm on an example with three clusters is shown in Fig. 3. Here any horizontal or vertical cut separates at least one point of a cluster. We select the first cut to be the vertical one with the coordinate value 4. W 1 contains only the square (red) point in Fig. 3(b). After this cut, X1 contains only one cluster, so we just return in that recursive branch. X2 still contains two clusters. So, we further divide the point set by choosing a horizontal cut with 5S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948coordinate value 4. This cut separates a point from the top cluster as shown in Fig. 3(c). Thus, in this case, W 1 is this square (red) point. After this cut, each part contains a single cluster, and thus we return on both branches. Hence, we obtain an explainable clustering by removing only 2 points.The correctness of the above algorithm trivially follows from Lemma 1. In particular, the recursion tree of the algorithm gives rise to the desired threshold tree. Also, the algorithm runs in polynomial time, as each successful cut (i, θ) can be found in polynomial time and the algorithm finds only k − 1 such cuts that separate the clusters. The last claim follows due to the properties (ii) and (iii) in Lemma 1.Consider the threshold tree generated by the algorithm. For each internal node u, let Xu be the corresponding points and W u be the points removed from Xu for finding an explainable clustering of the points in Xu \ W u . Note that we have at most k − 1 such nodes. The total number of points removed from X for finding the explainable clustering is |W u|. By Lemma 1,(cid:3)u|W u| ≤ OPT(Xu).Now, as Xu ⊆ X, by Observation 1, OPT( Xu) ≤ OPT(X). It follows that(cid:2)|W u| ≤ (k − 1) · OPT(X).uTheorem 1. There is a polynomial-time (k − 1)-approximation algorithm for the optimization version of Clustering Explanation.By noting that OPT(X) = 0 if C is an explainable clustering, we obtain the following corollary.Corollary 1. Explainability of any given k-clustering in Rd can be tested in polynomial time.Proof of Lemma 1. We probe all possible choices for cuts (i, θ) with i ∈ {1, . . . , d} and θ ∈ coordi(Y ), and select one which incurs the minimum cost. We also select a subset W of points to be removed w.r.t. each cut. The cost of such a cut is exactly the size of W .Fix a cut (i, θ). We have the following three cases. In the first case, for all clusters in C(Y ), strictly more than half of the points are contained in L(i, θ). In this case select a cluster C which has the minimum intersection with L(i, θ). Put all the (cid:3) ∩ R(i, θ) into W . The second case is points in C ∩ L(i, θ) into W . Also, for any other cluster Csymmetric to the first one – for all clusters in C(Y ), strictly more than half of the points are contained in R(i, θ). In this case we again select a cluster C which has the minimum intersection with R(i, θ). Put all the points in C ∩ R(i, θ) into W . (cid:3) ∩ L(i, θ) into W . In both of the above cases, the first three desired Also, for any other cluster Cproperties are satisfied for C(Y ) − W . In the third case, for each cluster C ∈ C(Y ), add the smaller part among C ∩ L(i, θ) and C ∩ R(i, θ) to W . In case |C ∩ L(i, θ)| = |C ∩ R(i, θ)|, we break the tie in a way so that properties (ii) and (iii) are satisfied. As C(Y ) contains at least two clusters this can always be done. Moreover, property (i) is trivially satisfied.(cid:3) ∈ C(Y ), put the points in C(cid:3) ∈ C(Y ), put the points in C(cid:3)(cid:3)∗∗In the above, we showed that for all the choices of the cuts, it is possible to select W so that the first three properties are satisfied. Let wm be the minimum size of the set W over all cuts. As we select a cut for which the size of W is minimized, it is sufficient to show that wm ≤ OPT(Y ).for Y such that C(Y ) − Wclusters C ∈ C(Y ). In other words, C(Y ) − Wbe the number of clusters in C(Y ). Consider any optimal set W∗, θ ∗). In the first of the above-mentioned three cases, suppose WLet kis explainable. Let ∗, θ ∗) be the canonical cut corresponding to the root of the threshold tree corresponding to the explainable clustering (i. Such a cut exists, as C(Y ) contains at least two clusters. Let (cid:4)W be the set selected in our algorithm correspond-C(Y ) − W∗, θ ∗)ing to the cut (i∗, θ ∗). But, fully for any of the k∗, θ ∗), which then even after choosing the root cut (icuts and hence we arrive contains points from all the k∗ ∈ C(Y ). In this case, our algorithm adds at a contradiction. Hence, C∗, θ ∗)| is minimized over all C ∈ C(Y ) and for any other cluster the points in C ∩ L(i∗| = OPT(Y ). The proof for the second case is the (cid:3) ∈ C(Y ), we put the points in CCsame as the one for the first case. We discuss the proof for the third case. Consider the clusters C ∈ C(Y ) such that both ∗, θ ∗) are non-empty. Note that these are the only clusters whose points are put into (cid:4)W . But, then C ∩ L(i∗, θ ∗). For each such cluster C, we add ∗Wthe smaller part among C ∩ L(i, θ) and C ∩ R(i, θ) to (cid:4)W . Hence, in this case also |(cid:4)W | ≤ |W∗| = OPT(Y ). The lemma follows by noting that wm ≤ |(cid:4)W |. (cid:2)must contain all the points from at least one of the parts C ∩ L(iclusters. However, by definition, the threshold tree must use only k∗, θ ∗) to (cid:4)W such that the size |C ∩ L(icontains points from each such part C ∩ L(imore cuts to separate the points in (Y \ W∗, θ ∗) must be fully contained in W∗, θ ∗) into (cid:4)W . Thus, |(cid:4)W | ≤ |Wdoes not contain the part C ∩ L(i∗, θ ∗) we still need k∗, θ ∗) and C ∩ R(i∗, θ ∗) and C ∩ R(i(cid:3)∗ ∩ L(ifor some C∗) ∩ L(i(cid:3) ∩ R(i∗∗∗∗(cid:3)(cid:3)3.2. Exact algorithmOur 22 min{s,k} · n2d · (dn)O(1) time algorithm is based on a novel dynamic programming scheme. Here, we briefly describe the algorithm. Our first observation is that each subproblem can be defined w.r.t. a bounding box in Rd , as each cut used to split a point set in any threshold tree is an axis-parallel hyperplane. The number of such distinct bounding boxes is at most 6S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Fig. 4. The collection X is in the interval (a, b], Z is outside (a, b], and (a, b] splits Y. Subfamilies {X, Y} and {X} are all (a, b]-proper subfamilies of {X, Y, Z}. {X, Y, Z} is 1-feasible with respect to (a, b] since only Y is split by the interval.n2d, as in each dimension a box is specified by two bounding values. This explains the n2d factor in the running time. Now, consider a fixed bounding box corresponding to a subproblem containing a number of given clusters, maybe partially. If a new canonical cut splits a cluster, then one of the two resulting parts has to be removed, and this choice has to be passed on along the dynamic programming. As we remove at most s points and the number of clusters is at most k, the number of such distinct choices can be bounded by 22 min{s,k}. This roughly gives us the following theorem.Theorem 2. Clustering Explanation can be solved in 22 min{s,k} · n2d · (dn)O(1) time.Before we move to the formal proof of the theorem, let us introduce some specific notations (illustrated by Fig. 4). Let a, b ∈ Rd be such that a < b. We denote (a, b] = {x ∈ Rd | a < x ≤ b} and call (a, b] an interval. For a collection of points X ⊆ Rd, we say that X is in (a, b] if X ⊆ (a, b], X is outside (a, b] if X ∩ (a, b] = ∅, and we say that (a, b] splits X if X ∩ (a, b] (cid:13)= ∅ and X \ (a, b] (cid:13)= ∅.Let X be a family of disjoint collections of points of Rd . A subfamily Y ⊆ X is said to be (a, b]-proper if (i) every X ∈ Xthat is in (a, b] is in Y , and (ii) every X ∈ X that is outside (a, b] is not included in Y . Note that X ⊆ X that are split by (a, b] may be either in Y or not in Y . The truncation of X with respect to (a, b], is the familytr(a,b](X ) = { X ∩ (a, b] | X ∈ X s.t. X ∩ (a, b] (cid:13)= ∅}.For an integer s ≥ 0, X is s-feasible with respect to (a, b] if (a, b] splits at most s collections in X .(cid:5)kProof of Theorem 2. Let (C, s) be an instance of Clustering Explanation, where C = {C1, . . . , Ck} for disjoint collections of points Ci of Rd. Let X =i=1 Ci . We say that a vector z ∈ (R ∪ {±∞})d is canonical if z[i] ∈ coordi(X) ∪ {±∞} for every i ∈ {1, . . . , d}. Recall that it suffices to consider only canonical cuts for the construction of a decision tree; canonical vectors represent sequences of canonical cuts along various dimensions.\ W, . . . , S\ W}, where {SFor every pair of canonical vectors (a, b) such that a < b and C is s-feasible with respect to (a, b], and every (a, b]-proper S = {S1, . . . , S(cid:9)} ⊆ C, we denote by ω(a, b, S) the minimum size of a collection of points W ⊆ X ∩ (a, b] such that } = tr(a,b](S), is an explainable (cid:9)-clustering. We assume that ω(a, b, S) = 0 if S is {Sempty. Intuitively, the tuple (a, b, S) defines a subproblem in the dynamic programming, bounded by the box (a, b] in which the clusters of S have to be explained. That is why S is (a, b]-proper, as it necessarily contains all clusters in (a, b], and some clusters that are split by (a, b]; those split clusters that are part of S are assumed to be contained in (a, b] after removing the extra points. Clearly, S should be s-feasible with respect to (a, b], as otherwise simply accounting for clusters split by (a, b] already exceeds the budget of s.(cid:3)1, . . . , S(cid:3)1(cid:3)(cid:9)(cid:3)(cid:9)Now, instead of ω(a, b, S), we compute the following slightly different value(cid:2)(cid:2)w(a, b, S) = ω(a, b, S) +|Ci \ (a, b]| +|Ci ∩ (a, b]|.(2)Ci ∈SCi ∈C\SObserve that w(a, b, S) additionally accounts for the cost of removing extra points from clusters that are split by (a, b], while ω(a, b, S) only computes the cost of explanation for the truncation of S in (a, b]. Since we are interested only in clustering that can be obtained by deleting at most s points, we assume that w(a, b, S) = +∞ if this value is bigger than s. This slightly informal definition simplifies arguments. In particular, observe the two sums in (2) give a value that is bigger than s if S is not s-feasible with respect to (a, b]. In fact, this is the reason why these sums are included in (2).Notice that (C, s) is a yes-instance of Clustering Explanation if and only if w(a∗, b∗, C) ≤ s, where a∗[i] = −∞ and ∗[i] = +∞ for i ∈ {1, . . . , d}.bWe now proceed to define the computation. The values w(a, b, S) are computed in different ways depending on (cid:9) = |S|. |C j ∩ (a, b]|. If (cid:9) = 1, then ω(a, b, S) = 0 by definition. Then If (cid:9) = 0, that is, S = ∅, then ω(a, b, S) = 0 and w(a, b, S) =(cid:3)S = {Ci} for some i ∈ {1, . . . , k} such that Ci ∩ (a, b] (cid:13)= ∅ andC j ∈Cw(a, b, S) = |Ci \ (a, b]| +(cid:2)C j ∈C\{Ci }|C j ∩ (a, b]|.7S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Assume that (cid:9) ≥ 2, and the values of ω(aFor i ∈ {1, . . . , d} and θ ∈ coordi(X) such that a[i] < θ < b[i], we define the vectors ai,θ and bi,θ by setting(cid:3), S(cid:3)) are computed for |S(cid:3)| < (cid:9).(cid:3), b(cid:6)ai,θ [ j] =θa[ j]if j = i,if j (cid:13)= i,and bi,θ [ j] =(cid:6)θb[ j]if j = i,if j (cid:13)= i.That is, (a, bi,θ ] and (ai,θ , b] are the two intervals formed by splitting (a, b] along the cit (i, θ). We also say that (i, θ) is s-feasible if C is s-feasible with respect to (a, bi,θ ] and (ai,θ , b]. For an s-feasible (i, θ), a partition (S1, S2) of S is (i, θ)-properif S1 and S2 are (a, bi,θ ] and (ai,θ , b]-proper, respectively. We defineδi,θ (S) =|Ci ∩ (a, b]|,(cid:2)Ci ∈S : Ci ∩(a,bi,θ ](cid:13)=∅and Ci ∩(ai,θ ,b](cid:13)=∅i.e., the total size of the clusters of C split by the cut (i, θ) in (a, b].We compute ω(a, b, S) by the following recurrence.w(a, b, S) = min{(∗∗), (∗ ∗ ∗)},(3)where the right part is denoted by (∗), and(∗∗) = min{|Ci \ (a, b]| +(cid:2)|C j| +(cid:2)|C j ∩ (a, b]| | Ci ∈ S},C j ∈S\{Ci }C j ∈C\S(∗ ∗ ∗) = min{w(a, bi,θ , S1) + w(ai,θ , b, S2) − δi,θ (S) for 1 ≤ i ≤ d, θ ∈ coordi(X),(S1, S2) is partition of S s.t. a[i] < θ < b[i], (i, θ) is s-feasible, (S1, S2) is (i, θ)-proper}.We assume that (∗ ∗ ∗) = +∞ if there is no triple (i, θ, (S1, S2)) satisfying the conditions in the definition of the set. We also assume that (∗) = +∞ if its value proves to be bigger than s. Intuitively, the term (∗ ∗ ∗) corresponds to trying all feasible cuts at the top level of the subproblem and reducing to the two respective smaller subproblems. The term (∗∗)then additionally covers the case where all except one of the remaining clusters are empty in the solution, which is not covered by (∗ ∗ ∗) since the following cuts do not play a role in this case.We now prove the correctness of (3) by showing the inequalities between the left and the rights parts in both directions.First, we show that w(a, b, S) ≥ (∗). This is trivial if w(a, b, S) = +∞. Assume that this is not the case. Then by our Ci ∈C\S |Ci ∩ (a, b]|. Let r = ω(a, b, S)} = tr(a,b](S), is an assumption, w(a, b, S) ≤ s. Recall that w(a, b, S) = ω(a, b, S) +and let W ⊆ X ∩ (a, b] be a collection of r points such that S (cid:3) = {Sexplainable (cid:9)-clustering. Assume that S = {Ci1 , . . . , Ci(cid:9)= Ci j(cid:3)(cid:3)(cid:9)1∩ (a, b]. Let Wi = W ∩ SCi ∈S |Ci \ (a, b]| +\ W, . . . , S\ W}, where {SNotice that it may happen that W j = S(cid:3)j(cid:3)j for some j ∈ {1, . . . , (cid:9)}. Then Ci j(a, b] = W j for at most (cid:9) − 1 values of j. Suppose that there is h ∈ {1, . . . , (cid:9)} such that Cihfor every h ∈ {1, . . . , (cid:9)} such that j (cid:13)= h. In this case, we obtain that} and S(cid:3)1, . . . , S(cid:3)i for i ∈ {1, . . . , (cid:9)}.∩ (a, b] = W j . Observe, however, that Ci j∩ (a, b] (cid:13)= Wh and Ci j∩∩ (a, b] = W j(cid:3)(cid:3)(cid:3)(cid:9)ω(a, b, S) =and(cid:2)C j ∈S\{Cih}|C j ∩ (a, b]|w(a, b, S) = |Cih\ (a, b]| +(cid:2)(cid:2)|C j| +C j ∈S\CihC j ∈C\S|C j ∩ (a, b]|.Then w(a, b, S) ≥ (∗∗) ≥ (∗). Assume from now that this is not the case and S j \ W j (cid:13)= ∅ for at least two distinct indices j ∈ {1, . . . , (cid:9)}. Then we show that w(a, b, S) ≥ (∗ ∗ ∗).(cid:3)i(cid:3)jBecause we separate at least two nonempty collections of points, the definition of explainable clustering implies that there are i ∈ {1, . . . , d} and θ ∈ coordi(X), such that a[i] < θ < b[i] and there is a partition (I1, I2) of {1, . . . , (cid:9)} with the property that (i) ˆS1 = {S\ W j | j ∈ I1} is an explainable (cid:9)1 = |I1|-clustering with the clusters in (a, bi,θ ], and (ii) ˆS2 =\ W j | j ∈ I2} is an explainable (cid:9)2 = |I2|-clustering with the clusters in (ai,θ , b], where both ˆS1 and ˆS2 contain nonempty {Scollections of points. Moreover, we assume that if S j \ W j = ∅ for some j ∈ {1, . . . , (cid:9)}, then S j \ W j is placed in ˆS1 if S j has a point in (a, bi,θ ] and, otherwise, i.e. if S j has only points in (ai,θ , b], it is placed in ˆS2.(cid:5)We define S1 = {Ci jj∈I1= (ai,θ , b] ∩ W j . We set W1 =. Observe that (W1, R1, W2, R2) is a partition of W where some sets may be empty. Denote by w 1 = |W1| and w 2 = |W2|, and let r1 = |R1|and r2 = |R2|. Clearly, w 1 + w 2 + r1 + r2 = r.= (a, bi,θ ] ∩ W j , and let W2| j ∈ I2}. For j ∈ I1, let W1(cid:7) (cid:5)jjand R2 = (a, bi,θ ] ∩W j| j ∈ I1} and S2 = {Ci jj∈I2j . Let also R1 = (ai,θ , b] ∩j and W2 =(cid:7) (cid:5)W2W1j∈I2j∈I1W j(cid:5)(cid:8)(cid:8)8S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948= {SNotice that S(cid:3)1\ R1 | j ∈ I1} = tr(a,bi,θ ](S1) and S(cid:3)and (ai,θ , b]-proper, respectively. Furthermore, for h ∈ {1, 2}, ˆSh is obtained from S(cid:3)clusters. Also we have that\ R2 | j ∈ I2} = tr(ai,θ ,b](S2). Also, S1 and S2 are (a, bi,θ ]h by deleting the points of W1 from the = {S(cid:3)j(cid:3)j2(4)(5)(6)(7)(8)|C j \ (a, bi,θ ]| =|C j \ (ai,θ , b]| =|C j \ (a, b]| + |R1|,|C j \ (a, b]| + |R2|,(cid:2)C j ∈S1(cid:2)C j ∈S2(cid:2)C j ∈S1(cid:2)C j ∈S2(cid:2)andC j ∈C\S1Note also that|C j ∩ (a, bi,θ ]| +(cid:2)C j ∈C\S2|C j ∩ (ai,θ , b]| =(cid:2)C j ∈C\S|C j ∩ (a, b]| + |R1| + |R2|.δi,θ (S) = |R1| + |R2|.Then by (4)–(7),w(a, b, S) = (w1 + w 2 + r1 + r2) += (w 1 + w 2 + r1 + r2) +(cid:2)C j ∈S(cid:2)|C j \ (a, b]| +(cid:2)|C j ∩ (a, b]|C j ∈C\S(cid:2)|C j \ (a, b]| +|C j \ (a, b]|(cid:2)+C j ∈C\S1(cid:2)= w 1 +C j ∈S1|C j ∩ (a, bi,θ ]| +(cid:2)C j ∈C\S2(cid:2)|C j \ (a, bi,θ ]| +C j ∈S2|C j ∩ (ai,θ , b]| − 2r1 − 2r2|C j ∩ (a, bi,θ ]|C j ∈S1(cid:2)+ w 2 +C j ∈C\S1(cid:2)|C j \ (ai,θ , b]| +|C j ∩ (ai,θ , b]| − δi,θ (S).C j ∈S2C j ∈C\S2Recall that w(a, b, S) ≤ s. Note that(cid:2)(cid:2)|C j ∩ (a, bi,θ ]| ≤C j ∈C\S1C j ∈C\S|C j ∩ (a, b]| + |R2|.Using (4), we obtain that(cid:2)C j ∈S1|C j \ (a, bi,θ ]| +(cid:2)C j ∈C\S1|C j ∩ (a, bi,θ ]| ≤ w(a, b, S),which is at most s. This means that S1 is (a, bi,θ ]-proper. Similarly, we have that S2 is (ai,θ , b]-proper. Therefore,w 1 +w 2 +and(cid:2)C j ∈S1(cid:2)C j ∈S2|C j \ (a, bi,θ ]| +|C j \ (ai,θ , b]| +(cid:2)C j ∈C\S1(cid:2)C j ∈C\S2|C j ∩ (a, bi,θ ]| ≥ w(a, bi,θ , S1)|C j ∩ (ai,θ , b]| ≥ w(ai,θ , b, S2).This allows us to extend (8) and conclude thatw(a, b, S) ≥ w(a, bi,θ , S1) + w(ai,θ , b, S2) ≥ (∗ ∗ ∗).This shows that w(a, b, S) ≥ (∗ ∗ ∗) ≥ (∗) and concludes the proof of the first inequality.Now we show that w(a, b, S) ≤ (∗). The inequality is trivial if (∗) = +∞. Suppose that this is not the case. Then, by our assumption about assigning the value to (∗), (∗) ≤ s.Suppose that the minimum in (∗) is achieved in the first part, that is, (∗∗) ≤ (∗ ∗ ∗). Then(∗) = |Ci \ (a, b]| +|C j| +(cid:2)(cid:2)C j ∈S\{Ci }C j ∈C\S|C j ∩ (a, b]|9S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948(cid:5)C j ∈S\{C}for some Ci ∈ S. We define W =|C j ∩ (a, b]|. We have that the family obtained from tr(a,b](S) by the deletion of the points of W is an explainable (cid:9)-clustering because we deleted the points in (a, b] of every C j ∈ S excepts Ci . This implies that ω(a, b, S) ≤ |W| and we obtain thatw(a, b, S) ≤ |W| +|Ci \ (a, b]| +|Ci ∩ (a, b]| = |Ci \ (a, b]| +|C j| +|C j ∩ (a, b]|,(cid:2)(cid:2)(cid:2)(cid:2)Ci ∈SCi ∈C\SC j ∈S\{Ci }C j ∈C\Swhich is exactly (∗∗), so ω(a, b, S) ≤ (∗).Assume from now that the minimum in (∗) is achieved for the second part, that is, (∗) = (∗ ∗ ∗) < (∗∗). Suppose that i ∈ {1, . . . , d}, θ ∈ coordi(X) where a[i] ≤ θ ≤ b[i] and (i, θ) is s-feasible, and a partition (S1, S2) of S that is (i, θ)-proper are chosen in such a way that (∗∗) achieves the minimum value for them, that is, (∗ ∗ ∗) = w(a, bi,θ , S1) + w(ai,θ , b, S2) −δi,θ (S).(cid:7) (cid:5)(cid:7) (cid:5)C j ∈S2C j). Then we obtain thatLet R1 = (ai,θ , b] ∩C j ∈S1C j) and R2 = (a, bi,θ ] ∩(cid:2)|C j \ (a, b]| + |R1|,C j ∈S1(cid:2)|C j \ (a, b]| + |R2|,|C j \ (a, bi,θ ]| =|C j \ (ai,θ , b]| =(cid:2)C j ∈S1(cid:2)C j ∈S2(cid:2)C j ∈S2(cid:2)|C j ∩ (a, bi,θ ]| +C j ∈C\S1C j ∈C\S2andδi,θ (S) = |R1| + |R2|.|C j ∩ (ai,θ , b]| =(cid:2)C j ∈C\S|C j ∩ (a, b]| + |R1| + |R2|,(9)(10)(11)(12)Let w 1 = ω(a, bi,θ , S1) and w 2 = ω(ai,θ , b, S2). Then there is a collection W1 ⊆ (a, bi,θ ] ∩ X such that the collection of sets obtained from the collections of tr(a,bi,θ ](S1) by the deletions of the points of W1 is an explainable |S1|-clustering. Sim-ilarly, there is a collection W2 ⊆ (ai,θ , b] ∩ X such that the family of collections obtained from the collections of tr(ai,θ ,b](S2)by the deletions of the points of W2 is an explainable |S2|-clustering. Consider W = W1 ∪ W2 ∪ R1 ∪ R2. The crucial obser-vation is that the family obtained from the collections of tr(a,b](S) by the deletions of the points of W is an explainable (cid:9)-clustering, where the first cut is Cuti,θ . Using (9)–(12), we obtain that(∗ ∗ ∗) = w(a, bi,θ , S1) + w(ai,θ , b, S2) − δi,θ (S) = |W| +≥ ω(a, b, S) +(cid:2)Ci ∈S(cid:2)Ci ∈S|Ci \ (a, b]| +|Ci \ (a, b]| +(cid:2)Ci ∈C\S(cid:2)Ci ∈C\S|Ci ∩ (a, b]||Ci ∩ (a, b]| = w(a, b, S).Since (∗) = (∗ ∗ ∗), this completes the correctness proof of the recurrence (3).In the final stage of the proof, we evaluate the running time. We construct the table of values of w(a, b, S) for pairs (a, b) of canonical vectors such that a < b. The total number of such pairs is at most (n + 2)2d and they can be constructed in n2d · (dn)O(1) time. We are interested only in a and b such that C is s-feasible with respect to (a, b]. Clearly, for given a and b, the s-feasibility can be checked in (dn)O(1) time. If C is s-feasible with respect to (a, b], then all (a, b]-proper subfamilies S of C can be listed by brute force as follows. Observe that X = {Ci ∈ C | Ci is in (a, b]} that can be constructed in polynomial time is a subfamily of every (a, b]-proper S. Let Y = {Ci ∈ C | Ci is split by (a, b]}. Since C is s-feasible, |Y| ≤ min{s, k}. We can construct Y in polynomial time and then generate at most 2min{s,k}subfamilies Z of Y in total 2min{s,k} · (dn)O(1) time. Then the (a, b]-proper subfamilies S are exactly the families of the form Z ∪ X . We obtain that there are at most 2min{s,k} (a, b]-proper subfamilies that can be generated in time 2min{s,k} · (dn)O(1) time. Then we conclude that the dynamic programming algorithm computes at most 2min{s,k} · n2d values of w(a, b, S).The value of w(a, b, S) for |S| is constructed in (dn)O(1) time if (cid:9) ≤ 1. If (cid:9) ≥ 2, we are using the recurrence (3). Computing (∗∗) can be done in polynomial time. To compute (∗ ∗ ∗), we go through i ∈ {1, . . . , d}, θ ∈ coordi(X), and partitions (S1, S) of S, where (S1, S2) is required to be (i, θ)-proper. This implies that (∗ ∗ ∗) can be computed in 2min{s,k} ·(dn)O(1) time. Summarizing, we have that the total running time of the dynamic programming algorithm is 22 min{s,k} · n2d ·(dn)O(1). This concludes the proof. (cid:2)3.3. Data reductionIn this section, we prove that Clustering Explanation admits a polynomial kernel when parameterized by k + d + s. Thus, we show that an instance of Clustering Explanation can be effectively compressed to an equivalent one of size polynomial in k, d, and s, irrespective of the original number of points in the instance.10S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Theorem 3. Clustering Explanation parameterized by k, d and s admits a kernel with r = 2(s + 1)dk points of {1, . . . , r}d.Proof. Let (C, s) be an instance of Clustering Explanation, where C = {C1, . . . , Ck} for disjoint collections of points Ci of Rd. Let X =(cid:5)ki=1 Ci .Our first aim is to reduce the number of points. For this, we use a procedure that marks essential points.For every i ∈ {1, . . . , k} and every j ∈ {1, . . . , d}, do the following:• Order the points of Ci in increasing order of their j-th coordinate; the ties are broken arbitrarily.• Mark the first min{s + 1, |Ci|} points and the last min{s + 1, |Ci|} points in the ordering.The procedure marks at most 2(s + 1)dk points. Then we delete the remaining unmarked points. Formally, we denote by Y the collection of marked points and set Si = Ci ∩ Y for all i ∈ {1, . . . , k}. Then we consider the instance (S, s) of Clustering Explanation, where S = {S1, . . . , Sk}. We show the following claim.Claim 3.1. (C, s) is a yes-instance of Clustering Explanation if and only if (S, s) is a yes-instance.Proof of Claim 3.1. Trivially, if (C, s) is a yes-instance, then (S, s) is a yes-instance because we just deleted some points to construct (S, s). We show that if (S, s) is a yes-instance, then (C, s) is a yes-instance.Because (S, s) is a yes-instance, there is a collection of at most s points W ⊆ Y such that {S1 \ W, . . . , Sk \ W} is an explainable k-clustering. In other words, there is an explainable clustering of Y \ W with a canonical threshold tree (T , k, ϕ)such that the clusters S1 \ W, . . . , Sk \ W correspond to the leaves of the threshold tree. We claim that if we use the same threshold tree for X \ W, then C1 \ W, . . . , Ck \ W correspond to the leaves.The proof is by contradiction. Assume that at least one collection of points corresponding to a leaf is distinct from every C1 \ W, . . . , Ck \ W. Then there is a node v ∈ V (T ) such that for some j ∈ {1, . . . , k}, C j \ W is split by the cut Cuti,θ for (i, θ) = ϕ(v), that is, for (A, B) = Cuti,θ (X), A ∩ (C j \ W) (cid:13)= ∅ and B ∩ (C j \ W) (cid:13)= ∅. Observe that either A ∩ (S j \ W) = ∅ or B ∩ (S j \ W) = ∅. We assume without loss of generality that A ∩ (S j \ W) = ∅ (the other case is symmetric). This means that there is an unmarked point x ∈ C j \ W in A and all the marked points of C j \ W are in B. Because C j has an unmarked point, |C j| ≥ 2(s + 1) + 1. Following the marking procedure, we order the points of C j by the increase of the i-th coordinate breaking ties exactly as in the marking procedure. Let L be the collection of the first s + 1 points that are marked. Since |W| ≤ s, there is y ∈ L \ W. Because L \ W ⊆ S j \ W ⊆ B, we have that y[i] > θ . Then x[i] ≥ y[i] > θ and x ∈ B; a contradiction.We conclude that if we use (T , k, ϕ) to cluster X \ W, then C1 \ W, . . . , Ck \ W correspond to the leaves. This proves that (C, s) is a yes-instance of Clustering Explanation. (cid:2)(cid:5)kWe obtained the instance (S, s), where Y =i=1 Si has (cid:9) ≤ 2(s + 1)dk points, that is equivalent to the original instance. Now we modify the points to ensure that they are in {1, . . . , (cid:9)}d. For this, we observe that for each i ∈ {1, . . . , d}, the values of the i-th coordinates can be changed if we maintain their order. Formally, we do the following. For every i ∈ {1, . . . , d}, = y[i], let coordi(Y) = {θ ifor each i ∈ {1, . . . , d}. Then for Si containing y, we replace y by z. Denote by Z the constructed collection of points, and let R = {R1, . . . , Rk} be the family of the collections of points constructed from S1, . . . , Sk.. For every y ∈ Y, we construct a point z, by setting z[i] = j, where θ ijWe have that (R, s) is a yes-instance of Clustering Explanation if and only if (S, s) is a yes-instance, and Z ⊆ {1, . . . , (cid:9)}d. Then the data reduction algorithm returns (R, s). To complete the proof, it remains to observe that the marking procedure is polynomial, and the coordinates replacement also can be done in polynomial time. (cid:2)1 < · · · < θ iri}, where θ i1, . . . , θ iri3.4. Hardness of approximationIn Theorem 1, we proved that the optimization Clustering Explanation problem admits a polynomial (k − 1)-approximation. Here we show that it is hard to approximate the minimum number of deleted points s within a factor that depends on s. Specifically, we provide a parameter-preserving reduction from Hitting Set to Clustering Explanationthat transfers known results about the hardness of parameterized approximation for the Hitting Set problem to Clustering Explanation. Recall that in the decision version of the Hitting Set problem, the input is a family of sets A over a universe U together with a parameter (cid:9), and the goal is to decide whether there is a set H ⊆ U of size at most (cid:9) such that H has non-empty intersection with each set in A. In the parameterized approximation variant, where (cid:9) is a parameter, the goal is to find a hitting set of size at most F ((cid:9))(cid:9) whenever G has a hitting set of size at most (cid:9).The starting point of our reduction is the following result for Hitting Set by Karthik, Laekhanukit, and Manurangsi [44].Theorem 4 (Theorem 1.4 in [44]). Assuming ETH, no f ((cid:9))(|U ||A|)o((cid:9))-time algorithm can approximate Hitting Set within a factor of F ((cid:9)), for any computable functions f and F of (cid:9).Note that Theorem 1.4 in [44] is stated for the Dominating Set problem, however by the standard parameter-preserving reduction from Dominating Set to Hitting Set (see e.g. Theorem 13.28 in [11]), the statement above immediately follows.11S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948(cid:3)mj=1|S j |Now, intuitively, given an instance (U , A, (cid:9)) of Hitting Set, where A = {S1, . . . , Sm}, our reduction constructs clusters C0, . The clusters C1, . . . , Cm represent the sets S1, . . . , Sm in the family A, and C0 is a special cluster that . . . , Cm in Rneeds to be separated from each of C1, . . . , Cm so that the clustering is explainable. The separation can only be performed by removing special points from C0 each of which corresponds to an element of the universe U . Removing such a point allows for separation between C0 and each C j such that the corresponding set S j contains the corresponding universe element. The two clusters can be separated along a special coordinate where only that special point “blocks” the separation. This is the crux of the reduction, which results in the following theorem.Theorem 5. For any computable functions f and F , there is no F (s)-approximation algorithm for the optimization version of Clus-tering Explanation with running time f (s)(nd)o(s), unless ETH fails.Proof. We show a reduction from Hitting Set. Consider an instance of Hitting Set over a universe U with a family of sets A and the size of the target hitting set (cid:9). We construct the following instance of Clustering Explanation, denote |A| = m. The target dimension d is equal to the sum of set sizes in the family A, d =S∈A |S|. The number of clusters in the constructed instance is m + 1, and for clarity, we denote them by C0, . . . , Cm. The target parameter s, that is, the number of points to remove from the cluster, is set exactly to (cid:9).(cid:3)Now we describe how the clusters are composed. Intuitively, the clusters C1, . . . , Cm represent the sets in the family A, and C0 is a special cluster that needs to be separated from each of C1, . . . , Cm so that the clustering is explainable. The separation can only be performed by removing special points from C0 that correspond to an element of the universe U . Removing such a point allows for separation between C0 and each C j such that the corresponding set S j contains the corresponding universe element. The two clusters can be separated along a special coordinate where only that special point “blocks” the separation. This is the crux of the reduction.(cid:9)(cid:3)Formally, we define point sets C0, . . . , Cm in terms of coordinates in Rd. The constructed instance is binary, that is, only values zero and one are used in the vectors. Order arbitrarily the sets in the family A, i.e. A = {S 1, . . . , Sm}, for each j ∈ {1, . . . , m}, the cluster C j corresponds to the set S j . The d coordinates are partitioned between the m sets in ⊆ {1, . . . , d}. That is, the first range of the following way. For each j ∈ {1, . . . , m}, denote I j =j(cid:3)< jcoordinates I1 is the first |S1| coordinates, I2 are the following |S2| coordinates, and so on. Now, for j ∈ {1, . . . , m}, the set C j consists of F ((cid:9)) · (cid:9) + 1 identical points w j , where w j[i] = 1 for j ∈ I j and w j[i] = 0 for j /∈ I j . The set C0 consists of two parts, C0 = O ∪ V. First, there are F ((cid:9)) · (cid:9) + 1 identical zero vectors in C0, we denote this set by O. Second, for each element u in the universe U , there is a point vu in V. The coordinates of vu are set so that for every j ∈ {1, . . . , m} such that u ∈ S j , there is exactly one coordinate in I j where vu is set to one, and this coordinate is unique for each u ∈ S j . If for j ∈ {1, . . . , m}, u /∈ S j , then vu[i] = 0 for all i ∈ I j . More specifically, for each S j ∈ A, order arbitrarily the elements of S j , S j = {u1, . . . , u|S j |}. For i ∈ [|S j|], vui|S j| + i] = 1. After performing the above for each j ∈ {1, . . . , m}, set the remaining coordinates of each vector vu to zero. This concludes the construction.|S j(cid:3) | + 1,|S j(cid:3) |j(cid:3)< jj(cid:3)≤ j(cid:3)(cid:3)(cid:10)[Now we show that an F (s)-approximate solution to the constructed Clustering Explanation instance would imply an F ((cid:9))-approximate solution to the original Hitting Set instance. Specifically, we prove the following.Claim 3.2. Whenever there exists a set W ⊆ X of size at most F (s) · s such that {C0 \ W, . . . , Cm \ W} is an explainable clustering, there also exists a set H ⊆ U that is a hitting set of A and |H| ≤ |W|. On the other hand, for any hitting set H ⊆ U there exists a solution Wto the Clustering Explanation instance such that |W| = |H|.Proof. In the forward direction, consider such a set W ⊆ X. We may assume that W ∩ C j = ∅ for each j ∈ {1, . . . , m} and W ∩ O = ∅, since these sets consist of F (s) · s + 1 identical points, and replacing W by a smaller set not intersecting the sets above is still a solution. Thus, W ⊆ V. Recall that each of the points in V corresponds to an element of the universe U in the Hitting Set instance. Denote by H the subset of U corresponding to W, that is, H = {u ∈ U : vu ∈ W}. Clearly |H| ≤ |W|, we claim that H is a solution to the Hitting Set instance.Consider a threshold tree that provides an explanation for {C0 \ W, . . . , Cm \ W}. By the above, all these clusters are (cid:3)(cid:3)non-empty. For j ∈ {0, . . . , m}, denote Cj for all j ∈ {1, . . . , m}. We now show that = C j \ W. Recall that we assume C j = Cjfor any j ∈ {1, . . . , m}, the set H has a non-empty intersection with the set S j . Since the tree represents the clustering, for (cid:3)(cid:3)(cid:3)(cid:3)(cid:3)) = (Xeach j ∈ {1, . . . , m}, there exists a cut in the tree that separates Cj and C0. That is, there exists a cut Cuti,θ (X1, X2)(cid:3)(cid:3)(cid:3)(cid:3) ⊆ X such that C⊆ X∪ Cin the tree for X2 (w.l.o.g). The dimension of the cut i necessarily belongs 0j(cid:3)j is indistinguishable from O. For i ∈ I j , there exists a point vu ∈ V such that vu[i] = 1to I j , since in all other coordinates C(cid:3)and u ∈ S j . Since the points in Cj are set to one in this coordinate and the points in O to zero, vu has to be in W. Thus, by construction, u ∈ H , and the set S j is hit by H .In the other direction, assume that there exists a hitting set H for A. We set W to be the corresponding to H subset of V, |W| = |H|. For every j ∈ {1, . . . , m}, there exists an element u ∈ H such that u ∈ S j . Consider the corresponding element vu of W, and the coordinate i ∈ I j where vu[i] = 1. By construction, vu is the only element of C0 that has one in the i-th (cid:3) (cid:13)= j, the elements of C j(cid:3) have zero in these coordinate. Thus, cutting over the dimension i with coordinate, and for any j(cid:3)(cid:3)⊆ X1, C0(cid:3), and Cj(cid:3)⊆ X12S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948the threshold θ set to zero separates C j from all the other clusters. Finally, the threshold tree for {C0 \ W, . . . , Cm \ W} is constructed by separating out each C j one by one via the corresponding cut. (cid:2)The theorem follows easily from Claim 3.2. Namely, assume there exists an F (s)-approximate algorithm for Clustering Explanation with running time f (s)(nd)o(s), that is, an algorithm that correctly decides either that the input instance has a solution of size at most F (s) · s, or that it has no solution of size at most s. We construct an F ((cid:9))-approximate algorithm for Hitting Set as follows. First, construct a Clustering Explanation instance via the reduction above. Second, run theClustering Explanation algorithm on that instance and output its answer. If it returns that there is a solution W of size at most F (s) · s, by Claim 3.2 there is a solution H to the original Hitting Set instance of size at most F (s) · s = F ((cid:9)) · (cid:9). If there is no solution to the Clustering Explanation instance of size at most s, by Claim 3.2 there also cannot be a solution to theHitting Set instance of size at most s = (cid:9). This shows that the constructed algorithm provides indeed an F ((cid:9))-approximation for the Hitting Set problem. Finally, its running time can be bounded as f (s)(nd)o(s) = f ((cid:9))(F ((cid:9))|U |m)o((cid:9)), which contradicts Theorem 4 unless ETH fails. (cid:2)4. Explainable clusteringExplainable k-means/median clustering We consider the Explainable k-means (resp. Explainable k-median) problem where given a collection X ⊆ Rd of n ≥ k points, the task is to find an explainable k-clustering {C1, . . . , Ck} of X of minimum k-means (resp. k-median) cost.4.1. Exact algorithmsOur (nd)k+O(1) time algorithm is indeed very simple and based on a branching technique. At each non-leaf node of a threshold tree, we would like to find an optimal cut. As we focus on canonical threshold trees, the number of distinct choices for branching is at most nd. Also as the number of non-leaf nodes in a threshold binary tree is k − 1, we have the following theorem.Theorem 6. Explainable k-means and Explainable k-median can be solved in (nd)k+O(1) time.Our n2d · (dn)O(1) time algorithm is based on dynamic programming, which we describe in the following. For two vectors x, y ∈ Rd, we write x ≤ y (x < y, respectively) to denote that x[i] ≤ y[i] (x[i] < y[i], respectively) for every i ∈ {1, . . . , d}. We highlight that when we write x < y, we require the strict inequality for every coordinate.Theorem 7. Explainable k-means and Explainable k-median can be solved in n2d · (dn)O(1) time.Proof. The algorithms for both problems are almost the same. Hence, we demonstrate it for Explainable k-means. For simplicity, we only show how to find the minimum cost of clustering but the algorithm can be easily modified to produce an optimal clustering as well by standard arguments.Let (X, k) be an instance of the problem with X = {x1, . . . , xn} and X ⊆ Rd. Following the proof of Theorem 2, we say that a vector z ∈ (R ∪ {±∞})d is canonical if z[i] ∈ coordi(X) ∪ {±∞} for every i ∈ {1, . . . , d}. For every pair of canonical vectors (a, b) such that a ≤ b and every positive integer s ≤ k, we compute the minimum means cost of an explainable s-clustering of Xa,b = {xi ∈ X | a < xi ≤ b} and denote this value ω(a, b, s). We assume that ω(a, b, s) = +∞ if Xa,b does not admit an explainable s-clustering. It is also convenient to assume that ω(a, b, s) = +∞ if Xa,b = ∅ because we are not ∗, k), where interested in empty clusters. Notice that the minimum means cost of an explainable k-clustering of X is ω(a∗[i] = +∞ for i ∈ {1, . . . , d}. We compute the table of values of ω(a, b, s) consecutively for s = 1, 2, . . . , k.∗[i] = −∞ and ba∗, bIf s = 1, then by definition,(cid:6)ω(a, b, s) =cost2(Xa,b)+∞if Xa,b (cid:13)= ∅,if Xa,b = ∅,and this value can be computed in polynomial time. Let s ≥ 2 and assume that the tables are already constructed for the lesser values of s. Consider a pair (a, b) of canonical vectors of (R ∪ {±∞})d such that a ≤ b. For i ∈ {1, . . . , d} and θ ∈ coordi(X) such that a[i] < θ < b[i], we define the vectors ai,θ and bi,θ by setting(cid:6)ai,θ [ j] =θa[ j]if j = i,if j (cid:13)= i,and bi,θ [ j] =(cid:6)θb[ j]if j = i,if j (cid:13)= i.Then we compute ω(a, b, s) using the following recurrence13S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948ω(a, b, s) = min{ω(a,bi,θ , s1) + ω(ai,θ , b, s2)for 1 ≤ i ≤ d, θ ∈ coordi(X),a[i] < θ < b[i],s1, s2 ≥ 1, and s1 + s2 = s}.(13)The correctness of (13) follows from the definition of the explainable clustering. It is sufficient to observe that to compute the optimum means cost of an explainable s-clustering of Xa,b, we have to take the minimum over the sums of optimum costs of explainable s1-clusterings and s2-clusterings of X1 and X2, respectively, where ( X1, X2) = Cuti,θ (Xa,b) for some i ∈ {1, . . . , d}, θ ∈ coordi(Xa,b) and s1 + s2 = s, and this is exactly what is done in (13).To evaluate the running time, observe that to compute ω(a, b, s) using (13), we consider d values of i, at most n values of θ and at most k ≤ n values of s1 and s2, that is, we go over at most dn2 choices. Thus, computing ω(a, b, s) for s ≥ 2 and fixed a and b can be done in O(dn2) time. Since there are at most (n + 2)2d pairs of canonical vectors a and b, we obtain that the time to compute the table of values of Xa,b for all pairs of vectors is n2d · (dn)O(1). Since the table for s = 1 can be constructed in n2d · (dn)O(1) and we iterate using (13) k − 1 ≤ n times, the total running time is n2d · (dn)O(1). (cid:2)4.2. HardnessIn this section, we show our hardness results for Explainable k-means and Explainable k-median: the problems are NP-complete, W[2]-hard when parameterized by k, and cannot be solved in f (k) · no(k) time for a computable function f (·)unless ETH fails. Moreover, the hardness holds even if the input points are binary. More precisely, we prove the following theorem.Theorem 8. Given a collection of n points X ⊆ {0, 1}d, a positive integer k ≤ n, and a nonnengative integer B, it is W[2]-hard to decide whether X admits an explainable k-clustering of mean (median, respectively) cost at most B, when the problem is parameterized by k. Moreover, the problems are NP-complete and cannot be solved in f (k) · no(k) time for a computable function f (·) unless ETH fails.Proof. We show the theorem for the means costs and then briefly explain how the proof is modified for medians. The case of median cost is easier due to the fact that for a collection of binary points, its median is also a binary vector.We reduce from the Hitting Set problem. The task of the problem is, given a family of sets W = {W 1, . . . , W m} over universe U = {u1, . . . , un}, and a positive integer k, decide whether there is S ⊆ U of size at most k that is a hitting set for W , i.e. such that S ∩ W i (cid:13)= ∅ for every i ∈ {1, . . . , m}. This problem is well-known to be W[2]-complete when parameterized by k even if the input is restricted to families of sets of the same size (see [13]).Let W = {W 1, . . . , W m} be a family of sets over U = {u1, . . . , un} with |W 1| = · · · = |W m| = r, and let k be a positive integer. We construct the following points.• Let s = 8nm2. For each i ∈ {1, . . . , n}, we construct a vector ui ∈ {0, 1}n by setting(cid:6)ui[ j] =1 if j = i,0 otherwise.We also define Ui to be the collection of s points identical to ui .• For each i ∈ {1, . . . , m}, let wi be the characteristic vector of W i . That is,(cid:6)wi[ j] =1 if u j ∈ W i,0 otherwise.We define W = {w1, . . . , wm}.• For t = 16ns2 = 1024n3m3 we construct a collection Z of t zero points.Finally, we define(cid:8)(cid:7) (cid:5)ni=1 Ui(cid:3) = k + 1, and• X =• k• B = m(r − 1) + s(n − k) = m(r − 1) + 8nm2(n − k).∪ W ∪ Z,Clearly, X, kand B can be constructed from the considered instance of Hitting Set in polynomial time. We claim that W has a hitting set S of size at most k if and only if X has an explainable k-clustering of means cost at most B.(cid:3)(cid:3)For the forward direction, assume that S is a hitting set of size at most k. Without loss of generality, we assume that -clustering {C1, . . . , Ck+1} as follows. We set C1 = {x ∈ X | x[i1] = 1} and for }. We define the k(cid:3)|S| = k. Let S = {ui1 , . . . , uikj = 2, . . . , k + 1,14S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948C j = {x ∈ X | x[i j] = 1} \j−1(cid:11)h=1Ch.j−1h=1 Ch. Then ( X j, C j) =Observe that the constructed clustering is explainable. To see this, let X0 = X and set X j = X \Cuti j ,0( X j−1) for j ∈ {1, . . . , k + 1}. Notice that Ui j⊆ C j for every j ∈ {1, . . . , k}, and we have that Z ⊆ Ck+1 and Uh ⊆ Ck+1for all h ∈ {1, . . . , m} \ {i1, . . . , ik}. Moreover, because S is a hitting set, each w j ∈ W is included in some cluster Ch for some (cid:5)h ∈ {1, . . . , k}, that is, Ck+1 = Z ∪ R, where R =For every j ∈ {1, . . . , k}, we define c j = ui j , and we set ck+1 be the zero vector. Clearly, for every j ∈ {1, . . . , k + 1}, 2. Let W j = W ∩ C j for j ∈ {1, . . . , k}. Note that C j = W j ∪ Ui j for j ∈ {1, . . . , k}. Because x[i j] = 1= r − 1 for every x ∈ W j for cost2(C j) ≤for every x ∈ C j for j ∈ {1, . . . , k} and x has exactly r nonzero elements, we have that (cid:5)x − c j(cid:5)22j ∈ {1, . . . , k}. If x ∈ R, then (cid:5)x − ck+1(cid:5)22= 1. We obtain that(cid:5)x − c j||2h∈{1,...,m}\{i1,...,ik} Uh.x∈C j(cid:3)(cid:5)cost2(C1, . . . , Ck+1) ≤k+1(cid:2)(cid:2)j=1x∈C j(cid:5)x − c j(cid:5)22=k(cid:2)(cid:2)j=1x∈W j(cid:5)x − c j(cid:5)22+(cid:2)x∈R(cid:5)x − ck+1(cid:5)22= m(r − 1) + s(n − k) = B.Thus, {C1, . . . , Ck+1} is an explainable k(cid:3)-clustering of means cost at most B.For the opposite direction, let {C1, . . . , Ck+1} be an explainable k-clustering with cost2(C1, . . . , Ck+1) ≤ B. Let also (cid:3), ϕ) be a canonical threshold tree for this clustering. Notice that because every point of X is binary, for every nonleaf (cid:3)(T , kv ∈ V (T ), ϕ(v) = (i, 0) for some i ∈ {1, . . . , d}.We show the following property of (T , k(cid:3), ϕ): for every nonleaf node v ∈ V (T ), its right child is a leaf. Suppose that this is not the case. Denote by x the root of T and let P = x1 · · · xp be the root-leaf path, where x1 = x and xi is the left child of xi−1 for i ∈ {2, . . . , p}, that is, P is constructed starting from the root and following the left children until we achieve the (cid:3) − 1 = k. Denote by Cqleftmost leaf. Because T has kthe cluster corresponding to the leaf xp of T . Notice that Z ⊆ Cq and n − (p − 1) collections Ui are included in Cq for some i ∈ {1, . . . , n}. Let i1, . . . , in−p+1 ∈ {1, . . . , n} be the distinct indices such that Ui j⊆ Cq. Denote by c the mean of Cq. Observe that for each j ∈ {1, . . . , n}, the multiset of the j-th coordinates of the points of Cq contains at most s + m ones and at least t zeros. Then for every j ∈ {1, . . . , n},leaves and at least one right child is not a leaf, we have that p ≤ k(cid:3)c[ j] ≤ m + s|Cq|and we obtain that≤ m + st≤ 2st= 18ns≤ 12s,cost2(Cq) ≥n−p+1(cid:2)(cid:2)j=1x∈Ui j(cid:5)x − c(cid:5)22= sn−p+1(cid:2)j=1(cid:5)u ji− c(cid:5)22(cid:8)(cid:7)(cid:7)1 − 12 > s(n − k + 1)≥s(n − k + 1)2s≥s(n − k) + s − n ≥ s(n − k) + 2nm − n>m(n − 1) + s(n − k) ≥ B,(cid:8)1 − 1scontradicting that cost2(C1, . . . , Ck+1) ≤ B.Because for every nonleaf node v ∈ V (T ), its right child is a leaf, we have that the clustering is obtained by consecutive cutting each cluster from the set of points. Then we can assume without loss of generality that there are k-tuple of distinct indices (i1, . . . , ik) from {1, . . . , n} such that C1 = {x ∈ X | x[i1] = 1} andC j = {x ∈ X | x[i j] = 1} \j−1(cid:11)h=1Chfor j = 2, . . . , k + 1. We claim that S = {ui1 , . . . , uik} is a hitting set for W .The proof is by contradiction. Suppose that S is not a hitting set. For every i ∈ {1, . . . , k + 1}, let Wi = Ci ∩ W. Notice that because S is not a hitting set, Wk+1 (cid:13)= ∅. We analyze the structure of clusters and upper bound their means costs. For this, denote by c1, . . . , ck+1 the means of C1, . . . , Ck+1.Let j ∈ {1, . . . , k} and consider C j with the mean c j . We have that C j = Ui j∪ W j . Then c j[i j] = 1. Let h ∈ {1, . . . , n} be distinct from i j . If x ∈ Ui j , then x[h] = 0. Then the multiset of the h-th coordinates of the points of C j contains at most |W j| ≤ m ones and at least s zeros, and we have thatc j[h] ≤ m|C j|≤ ms≤ 18mn.15S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Recall that x ∈ W has exactly r elements that are equal to one. This implies, that for every x ∈ W j ,(cid:7)2 ≥ (r − 1)(cid:8)(cid:8)(cid:5)x − c j(cid:5)22(cid:7)1 − 1≥ (r − 1)8mn1 − 14mnand, therefore,(cid:7)cost2(C j) ≥ (r − 1)(cid:8)1 − 14mn|W j| ≥ (r − 1)|W j| − 14m|W j|,(14)because r ≤ n.(cid:5)Now we consider Ck+1 and the corresponding mean ck+1. Notice that Ck+1 = Z ∪ Wk+1 ∪ R, where R =h∈{1,...,m}\{i1,...,ik} Uh. Let h ∈ {1, . . . , n}. We have that x[h] = 0 for x ∈ Z. Hence, the multiset of h-th coordinates of the points of Ck+1 contains at most |Wk+1| + s ≤ m + s ones and at least t zeros. Therefore,ck+1[h] ≤ m + s|Ck+1|≤ 2st≤ 18sn.Since x ∈ W has exactly r ≤ n elements that are equal to one, for every x ∈ Wk+1,1 − 18mn≥ r − 14m1 − 14mn1 − 18sn(cid:5)x − ck+1(cid:5)222 ≥ r2 ≥ r≥ r(cid:7)(cid:7)(cid:8)(cid:7)(cid:8)(cid:8).For every x ∈ R, x contains a unique nonzero element and, therefore,(cid:5)x − ck+1(cid:5)22≥(cid:8)(cid:7)1 − 18sn2 ≥ 1 − 14sn.Note that R contains exactly s(n − k) points. Then(cid:2)(cid:2)cost2(Ck+1) ≥(cid:5)x − ck+1(cid:5)22+(cid:5)x − ck+1(cid:5)22x∈Wk+1x∈R(cid:7)r − 14m≥(cid:8)(cid:7)|Wk+1| + s(n − k)(cid:8)1 − 14sn≥ r|Wk+1| + s(n − k) − 14m|Wk+1| − 14.(15)Recall that |W1| + · · · + |Wk+1| = m. Then combining (14) and (15), we obtain thatcost2(C1, . . . , Ck+1) =k+1(cid:2)j=1cost2(C j) ≥ m(r − 1) + s(n − k) + |Wk+1| − 12≥ B + 12because Wk+1 (cid:13)= ∅. However, this contradicts that the means cost of {C1, . . . , Ck(cid:3) } is at most B. This means that S is a hitting set for W .This concludes the hardness proof for Explainable k-means. For the median cost, we use exactly the same reduction fromHitting Set. Then we show that W has a hitting set S of size at most k if and only if X has an explainable k-clustering of median cost at most B. The proof for the forward direction is identical to the proof for the means up to the replacement of 2 by (cid:5) · (cid:5)1. For the opposite direction, the proof follows the same lines as the above proof for means cost2 by cost1 and of (cid:5) · (cid:5)2but gets simplified because we can assume that medians are binary. In particular, if the multiset of h-th coordinates of the points of a cluster C j contains more zeros than ones, then c j[h] = 0 for the median c j . Notice that the crucial part of the proof for the means is obtaining upper bounds for the values c j[h]. Now we can immediately assume that c j[h] = 0 in all considered cases whenever we upper bound c j[h], and the lower bounds for the costs in the proof become straightforward.To see the second part of the theorem, note that our reduction from Hitting Set is polynomial. This immediately implies NP-hardness of the considered problems for both measures. For the lower bound up to ETH, we use the well-known fact (see, e.g. [11, Chapter 14]) that Hitting Set does not admit an algorithm with running time f (k) · (n + m)o(k) for any computable function f (·) assuming ETH. Because our reduction is polynomial and, moreover, the value of the parameter in (cid:3) = k + 1, we obtain that an algorithm with running time f (k) · (n + m)o(k) for our problems the constructed instance is kwould contradict ETH. (cid:2)(cid:3)5. Approximate explainable clusteringApproximate explainable k-means/median clustering In Approximate Explainable k-means, we are given a collection of npoints X ⊆ Rd, a positive integer k ≤ n, and a positive real constant ε < 1. Then the task is to find a collection of points Y ⊆ X with |Y| ≥ (1 − ε)|X| and an explainable k-clustering of Y whose k-median cost does not exceed the optimum k-median cost of an explainable k-clustering for the original collection of points X. Note that we ask about the construction of Y and the corresponding clustering as the decision variant is trivial. Observe also that the optimum cost is unknown a priori. Approximate Explainable k-median differs only by the clustering measure.16S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Fig. 5. An example of the definition of a set Zu for a node u ∈ T . The points X are partitioned into stripes containing n(cid:17) (6 in the figure) points. The set Zu is the stripe containing the optimal cut, represented in red. Any cut inside that stripe gives the same partition of X \ Zu as the optimal one. This means that by removing npoints, the algorithm essentially reduces the numbers of different cuts from n to O( k(cid:3) = (cid:16) (cid:3)nk(cid:3)(cid:3) ).Theorem 9. Approximate Explainable k-means and Approximate Explainable k-median are solvable in ( 4d(k+(cid:3))(cid:3))k · nO(1) time.(cid:8)(cid:7)2(k−1)k−1Formally, let nProof. As the proofs for both problems are identical, we describe only the algorithm for Approximate Explainable k-means.Let X ⊆ Rd be an instance of Approximate Explainable k-means with |X| = n and let (T , k, ϕ) be the optimal canoni-cal threshold tree for explainable k-means clustering and (C1, . . . , Ck) the clustering induced by (T , k, ϕ). The goal of the algorithm will be to guess an approximation of (T , k, ϕ). Recall that the total number of full binary trees with k leaves is the (k − 1)-th Catalan number Ck−1 = 1≤ 4k. Since T is a full binary tree with k leaves, guessing T only requires at kmost 4k tries. Guessing ϕ is more complicated however, as there is d · (n − 1) choices at each node u of T because we have d possibilities to choose a coordinate i and |coordi(X)| − 1 possibilities to select the threshold value. This gives potentially (d(n − 1))k−1 possibilities for k − 1 nonleaf nodes of T . The idea here will be to guess for every nonleaf node u of T the second element of ϕ(u) up to a precision of O( (cid:3)nk ), which gives only O(d · k(cid:17) and note first that if n(cid:3) = 0, then the algorithm trying all the possible values of T and ϕ, and computing the value of the obtained clustering, runs in time 4k( dk(cid:3) )k · nLet U denote the set of nonleaf nodes of T . Let ϕ(cid:3) : U → {1, . . . , d} × R be the function obtained from ϕ by the fol-lowing rounding. Consider u ∈ U and assume that ϕ(u) = ( j, θ). Denote by x1, . . . , xn the points of X assuming that they are sorted by their j-th coordinate, that is, x1[ j] ≤ · · · ≤ xn[ j]. Recall that because (T , k, ϕ) is canonical, θ ∈ coord j(X) ={x1[ j], . . . , xn[ j]}. We define ϕ(cid:3)(u) = ( j, θ (cid:3)), where θ (cid:3) = xin(cid:3)+1[ j] for the largest nonnegative integer i such that xin(cid:3)+1[ j] ≤ θ .Consider now the clustering obtained from the threshold tree (T , k, ϕ(cid:3)). At each node u ∈ U such that ϕ(u) = ( j, θ)and ϕ(cid:3)(u) = ( j, θ (cid:3)) for θ (cid:3) = xin(cid:3)+1, the points x of X that can be misplaced by Cutϕ(cid:3)(u)(X) are exactly the points such that θ (cid:3) < x[ j] ≤ θ . By the choice of i, we have that θ ≤ x(i+1)n(cid:3) . This means that, if Zu denotes the set of all the points xh such that i ·nu∈U Zu , then (T , k, ϕ(cid:3)) and (T , k, ϕ) induce the exact same clustering on X \ Z. Because |Zu| ≤ n, then the partitions Cutϕ(cid:3)(u)(X \ Zu) and Cutϕ(u)(X \ Zu) are identical (see Fig. 5). Therefore, if Z =(cid:3) ) choices at each nonleaf node.(cid:3) . This means that if nO(1), which ends the proof. From now on, let us assume that nk < 1 and thus n ≤ k(cid:3) ≥ 1., we have that |Z| ≤ kn(cid:3) < h ≤ (i + 1)n(cid:3) = 0, then (cid:3)n(cid:3) = (cid:16) (cid:3)nk(cid:3) ≤ (cid:3)n.(cid:5)(cid:3)}.(cid:3) < h ≤ (i + 1)nOur algorithm is based on these observations. The algorithm tries all possible choices for T . Given T with the set of nonleaf nodes U , it tries all possible choices of ϕ(cid:3)(u) = ( j, θ (cid:3)) for u ∈ U as follows. For each u, we first pick a coordinate j ∈ {1, . . . , d}. Then we sort X by the j-th coordinate of the points and assume that X = {x1, . . . , xn} and x1[ j] ≤ · · · ≤ xn[ j]. We consider all possible values of θ (cid:3) = xin(cid:3)+1 for 0 ≤ i ≤ k(cid:3) . Furthermore, the algorithm constructs the set of points Zu = {xh | in(cid:5)For each choice T and ϕ, the algorithm constructs Z =u∈U Zu , removes Z and computes the value of the clustering induced by (T , k, ϕ(cid:3)) on Y = X \ Z. Finally, it outputs the set Y as well as the threshold tree (T , k, ϕ(cid:3)) which minimizes the value of the clustering.We have that the value of the clustering for (T , k, ϕ(cid:3)) on Y does not exceed the value of the clustering for an optimum threshold tree on Y. Therefore, we obtain an explainable k-clustering of Y whose cost is upper bounded by the minimum cost of an explainable k-clustering of X. Because for every set of choices of T and ϕ(cid:3)(cid:3) ≤ (cid:3)n, the algorithm outputs the set Y such that |Y| ≥ (1 −(cid:3))|X|. This concludes the description of the algorithm and its correctness proof., the set Z has size at most k · nSince there are at most 4k possible choices of T and d · ( k(cid:3)tree, we conclude that the total number of possible choices of T and ϕ(cid:3)that the running time of the algorithm is ( 4d(k+(cid:3))(cid:7)d( k(cid:3)O(1). This concludes the proof. (cid:2)+ 1) possible choices of ϕ(cid:3)(u) for every nonleaf node of the (cid:8)k−1 ≤+ 1). This implies is at most 4k4d(k+(cid:3))(cid:3))k · n(cid:8)k(cid:7)(cid:3)(cid:3)(cid:3)6. ConclusionIn this paper, we initiated the study of the computational complexity of several variants of explainable clustering. Con-cluding, we discuss some limitations of our approach, outline further research directions, and state a number of open problems.17S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948Explainable k-means and Explainable k-median Our first question concerns FPT-approximability of Explainable k-meansand Explainable k-median. Recall that by Theorem 8, both problems are W[2]-hard being parameterized by k. The reduction also rules out algorithms with running time f (k) · (nd)o(k) for both of these problems up to a widely believed assumption in the complexity theory. Theorem 6 indicates that this lower bound is tight since the naive brute force algorithm solves each of the problems in time (nd)k+O(1). However, our hardness reduction does not exclude the possibility of the existence of efficient FPT approximation algorithms. Such algorithms exist for vanilla clustering. Particularly, for k-means, an (1 + ε)-approximation in 2(k/ε)O(1) · nd time was demonstrated by Kumar, Sabharwal, and Sen [31]. The same techniques can be used for k-median [31]. Our question is: Is it possible to find a (1 + ε)-approximation for Explainable k-means or Explainable k-median in time 2 f (k/ε) · (nd)O(1)?In Theorem 7, we give an algorithm solving Explainable k-means and Explainable k-median in time n2d · (nd)O(1). The exponential dependence on d makes an algorithm with such a running time unsuitable for instances with large dimension d(number of features). Due to our lower bound, the exponential dependence on d is unavoidable. However, it is plausible that the problem is FPT parameterized by d. In other words, the question is whether it is possible to obtain an f (d) · (dn)O(1)time algorithm solving Explainable k-means or Explainable k-median for some function f (·)? The question about FPT-approximation is also open. In particular, the existence of an (1 + ε)-approximation for Explainable k-means or Explainable k-median in time 2 f (d/ε) · (nk)O(1) remains open. Observe that the standard dimension reduction tools seem to be inappli-cable for explainable clusterings because a specific dimension could be crucial for separating clusters even when it does not contribute significantly to the distance, and it is very unclear how to distinguish such dimensions.The same question is valid for approximation under the “stronger” parameterization by k + d, that is, (1 + ε)-O(1). Our Theorem 9 shows that approximation for Explainable k-means or Explainable k-median in time 2 f ((d+k)/ε) · nO(1) time, if we are allowed to sacrifice εn of the input points in order to con-an approximation can be done in ( 8dkstruct an explainable clustering (in fact, we obtain an explainable k-clustering without increasing the cost). Simultaneously, this theorem indicates the main technical difficulty for the approximation which is the fact that a small fraction of points can drastically increase the clustering cost (see also the bounds in the paper Moshkovitz et al. [41] for the cost of the explanation of an optimal k-means (medians) clustering).(cid:3) )k · nIn Theorem 1, we show that Clustering Explanation admits a polynomial-time approximation with a factor of (k −1). We do not know whether there exist polynomial time algorithms providing a better ratio. The concrete question we leave open for further research is whether the approximation ratio log k for Clustering Explanation is achievable by some polynomial time algorithm.Clustering explanation Further in the paper, we obtained a number of results about the Clustering Explanation problem, where the aim is to explain a given k-clustering by the cost of deletion of a bounded number of points. Theorem 5 shows that it is unlikely that the minimum number s of deleted points may be approximated in f (s) · no(s) time within F (s) factor for any computable functions f (·) and F (·). However, Theorem 1 shows that Clustering Explanation admits a polynomial-time approximation with a factor of (k − 1). The concrete question we leave open for further research is whether the approximation ratio log k for Clustering Explanation is achievable by some polynomial time algorithm.The complexity lower bound from Theorem 5 leads to the questions about the parameterized complexity of Clustering Explanation for various parameterizations by k, d, and s and their combinations. In particular, is Clustering ExplanationFPT when parameterized by k or d, or for the combined parameterizations by k + d, s + d, or s + k? Note also that inClustering Explanation, we do not make any assumptions about the input clustering {C1, . . . , Ck} except that it is a partition of the input points. However, in a more practical setting, it may be assumed that C1, . . . , Ck are “real” clusters. For example, {C1, . . . , Ck} can be an optimum k-means or k-medians clustering. Would such an assumption make Clustering Explanationmore tractable?Non-unary conditions and other generalizations From a high-level perspective, the idea of Moshkovitz et al. [41], which we adapt in our work, is to design an efficient procedure for clustering that is explainable by a small decision tree and whose quality is close to an optimal k-clustering. In this model, the conditions in the decision tree are unary (in the form x ≤ a and y < b) and a natural research direction here is to investigate more general linear conditions (like x + y < a). An interesting question here is what is the right balance here between the complexity of the conditions of the decision tree and the human ability to grasp such explanations? But in certain situations, it is plausible to assume that a decision tree with non-unary conditions could have a reasonable interpretation for the purposes of clustering. From mathematical and algorithmic perspectives, such trees are also interesting objects. It is easy to construct examples when decision trees with a linear combination of two coordinates provide a much better “price of explainability” than trees with unary conditions. However, providing quantitative bounds as well as efficient algorithms for computing such trees seems to be way more challenging.Another interesting research direction is to study decision trees that allow more than k leaves. In this case, one cluster may be assigned to several different leaves. How helpful is it to introduce more leaves? Frost, Moshkovitz, and (cid:3) ≥ k of leaves and Rashtchian [21] proposed a heuristic algorithm that constructs a decision tree with a given number kanalyzed its performance empirically. Makarychev and Shan [38] took a theoretical approach to this problem and designed for a parameter δ ∈ (0, 1) a polynomial-time randomized algorithm constructing a threshold decision tree with (1 + δ)kleaves. The constructed tree induces an explainable clustering whose cost is within 1/δ · polylog(k) the cost of an optimal 18S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948k-clustering. However, to the best of our knowledge, the parameterized complexity of constructing such types of decision trees remains completely unexplored.Declaration of competing interestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.Data availabilityNo data was used for the research described in the article.AcknowledgementsWe thank the anonymous referees for their helpful comments and suggestions.References[1] Charu C. Aggarwal, Chandan K. Reddy (Eds.), Data Clustering: Algorithms and Applications, CRC Press, 2013.[2] Daniel Aloise, Amit Deshpande, Pierre Hansen, Preyas Popat, NP-hardness of Euclidean sum-of-squares clustering, Mach. Learn. 75 (2) (2009) 245–248, [3] Dimitris Bertsimas, Agni Orfanoudaki, Holly M. Wiberg, Interpretable clustering: an optimization approach, Mach. Learn. 110 (1) (2021) 89–138, https://https://doi .org /10 .1007 /s10994 -009 -5103 -0.doi .org /10 .1007 /s10994 -020 -05896 -2.[4] Leo Breiman, Random forests, Mach. Learn. 45 (1) (2001) 5–32.[5] Leonardo Cañete-Sifuentes, Raúl Monroy, Miguel Angel Medina-Pérez, A review and experimental comparison of multivariate decision trees, IEEE Access 9 (2021) 110451–110479, https://doi .org /10 .1109 /ACCESS .2021.3102239.[6] Diogo V. Carvalho, Eduardo M. Pereira, Jaime S. Cardoso, Machine learning interpretability: a survey on methods and metrics, Electronics 8 (8) (2019) 832.2013.[7] Deeparnab Chakrabarty, Prachi Goyal, Ravishankar Krishnaswamy, The non-uniform k-center problem, in: 43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016, July 11-15, 2016, Rome, Italy, 2016, 67.[8] Moses Charikar, Lunjia Hu, Near-optimal explainable k-means for all dimensions, in: Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA 2022, Virtual Conference / Alexandria, VA, USA, January 9 - 12, 2022, SIAM, 2022, pp. 2580–2606.[9] Moses Charikar, Samir Khuller, David M. Mount, Giri Narasimhan, Algorithms for facility location problems with outliers, in: Proceedings of the Twelfth Annual ACM-SIAM Symposium on Discrete Algorithms, Society for Industrial and Applied Mathematics, 2001, pp. 642–651.[10] Chen Ke, A constant factor approximation algorithm for k-median clustering with outliers, in: Proceedings of the Nineteenth Annual ACM-SIAM Sym-posium on Discrete Algorithms, 2008, pp. 826–835.Algorithms, Springer, ISBN 978-3-319-21274-6, 2015.[11] Marek Cygan, Fedor V. Fomin, Lukasz Kowalik, Daniel Lokshtanov, Dániel Marx, Marcin Pilipczuk, Michal Pilipczuk, Saket Saurabh, Parameterized [12] Sanjoy Dasgupta, The Hardness of k-Means Clustering, Department of Computer Science and Engineering, University of California, 2008.[13] Rodney G. Downey, Michael R. Fellows, Fundamentals of Parameterized Complexity, Texts in Computer Science, Springer, ISBN 978-1-4471-5558-4, [14] Petros Drineas, Alan M. Frieze, Ravi Kannan, Santosh S. Vempala, V. Vinay, Clustering large graphs via the singular value decomposition, Mach. Learn. 56 (1–3) (2004) 9–33, https://doi .org /10 .1023 /B :MACH .0000033113 .59016 .96.[15] Hossein Esfandiari, Vahab S. Mirrokni, Shyam Narayanan, Almost tight approximation algorithms for explainable clustering, in: Joseph (Seffi) Naor, Niv Buchbinder (Eds.), Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA 2022, Virtual Conference / Alexandria, VA, USA, January 9 - 12, 2022, SIAM, 2022, pp. 2641–2663.[16] Jean Feng, Noah Simon, Sparse-input neural networks for high-dimensional nonparametric regression and classification, arXiv preprint, arXiv:1711.07592, 2017.2019.1–12.(2019) 1–25.[17] Qilong Feng, Zhen Zhang, Ziyun Huang, Jinhui Xu, Jianxin Wang, Improved algorithms for clustering with outliers, in: 30th International Symposium on Algorithms and Computation (ISAAC 2019), Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2019.[18] Fedor V. Fomin, Daniel Lokshtanov, Saket Saurabh, Meirav Zehavi, Kernelization: Theory of Parameterized Preprocessing, Cambridge University Press, [19] Ricardo Fraiman, Badih Ghattas, Marcela Svarc, Interpretable clustering using unsupervised binary trees, Adv. Data Anal. Classif. 7 (2) (2013) 125–145.[20] Zachary Friggstad, Kamyar Khodamoradi, Mohsen Rezapour, Mohammad R. Salavatipour, Approximation schemes for clustering with outliers, ACM [21] Nave Frost, Michal Moshkovitz, Cyrus Rashtchian, ExKMC: expanding explainable k-means clustering, CoRR, arXiv:2006 .02399 [abs], 2020, https://Trans. Algorithms 15 (2) (February 2019).arxiv.org /abs /2006 .02399.[22] Buddhima Gamlath, Xinrui Jia, Adam Polak, Ola Svensson, Nearly-tight and oblivious algorithms for explainable clustering, in: Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, Virtual, 2021, pp. 28929–28939, https://proceedings .neurips .cc /paper /2021 /hash /f24ad6f72d6cc4cb51464f2b29ab69d3 -Abstract .html.[23] Pierre Geurts, Nizar Touleimat, Marie Dutreix, Florence d’AlchéBuc, Inferring biological networks with output kernel trees, BMC Bioinform. 8 (2) (2007) [24] Badih Ghattas, Pierre Michel, Laurent Boyer, Clustering nominal data using unsupervised binary decision trees: comparisons with the state of the art methods, Pattern Recognit. 67 (2017) 177–185.[25] David G. Harris, Thomas Pensyl, Aravind Srinivasan, Khoa Trinh, A lottery model for center-type problems with outliers, ACM Trans. Algorithms 15 (3) [26] Trevor Hastie, Robert Tibshirani, Generalized additive models, Stat. Sci. 1 (3) (1986) 297–318.[27] Russell Impagliazzo, Ramamohan Paturi, Complexity of k-SAT, in: Proceedings of the 14th Annual IEEE Conference on Computational Complexity, Atlanta, Georgia, USA, May 4–6, 1999, IEEE Computer Society, 1999, pp. 237–240.[28] Russell Impagliazzo, Ramamohan Paturi, Francis Zane, Which problems have strongly exponential complexity, J. Comput. Syst. Sci. 63 (4) (2001) 512–530.19S. Bandyapadhyay, F.V. Fomin, P.A. Golovach et al.Artificial Intelligence 322 (2023) 103948[29] Yacine Izza, Alexey Ignatiev, Joao Marques-Silva, On tackling explanation redundancy in decision trees, J. Artif. Intell. Res. 75 (2022).[30] Ravishankar Krishnaswamy, Shi Li, Sai Sandeep, Constant approximation for k-median and k-means with outliers via iterative rounding, in: Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, 2018, pp. 646–659.[31] Amit Kumar, Yogish Sabharwal, Sandeep Sen, Linear-time approximation schemes for clustering problems in any dimensions, J. ACM 57 (2) (2010) 5, https://doi .org /10 .1145 /1667053 .1667054.[32] Eduardo Sany Laber, Lucas Murtinho, On the price of explainability for some clustering problems, in: Proceedings of the 38th International Conference on Machine Learning (ICML), 2021, pp. 5915–5925, http://proceedings .mlr.press /v139/.[33] Zachary C. Lipton, The mythos of model interpretability: in machine learning, the concept of interpretability is both important and slippery, Queue 16 (3) (2018) 31–57.doi .org /10 .1016 /j .tcs .2010 .05 .034.[34] Yang Young Lu, Yingying Fan, Jinchi Lv, William Stafford Noble, DeepPINK: reproducible feature selection in deep neural networks, in: NeurIPS, 2018.[35] Scott M. Lundberg, Su-In Lee, A unified approach to interpreting model predictions, Adv. Neural Inf. Process. Syst. 30 (2017) 4765–4774.[36] Meena Mahajan, Prajakta Nimbhorkar, Kasturi R. Varadarajan, The planar k-means problem is NP-hard, Theor. Comput. Sci. 442 (2012) 13–21, https://[37] Konstantin Makarychev, Liren Shan, Near-optimal algorithms for explainable k-medians and k-means, in: Proceedings of the 38th International Con-ference on Machine Learning (ICML), vol. 139, PMLR, 2021, pp. 7358–7367, http://proceedings .mlr.press /v139/.[38] Konstantin Makarychev, Liren Shan, Explainable k-means: don’t be greedy, plant bigger trees!, in: Stefano Leonardi, Anupam Gupta (Eds.), STOC ’22: 54th Annual ACM SIGACT Symposium on Theory of Computing, Rome, Italy, June 20 - 24, 2022, ACM, 2022, pp. 1629–1642.[39] Riˇcards Marcinkeviˇcs, Julia E. Vogt, Interpretability and explainability: a machine learning zoo mini-tour, arXiv preprint, arXiv:2012 .01805, 2020.[40] Christoph Molnar, Interpretable machine learning, Lulu .com, 2020.[41] Michal Moshkovitz, Sanjoy Dasgupta, Cyrus Rashtchian, Nave Frost, Explainable k-means and k-medians clustering, in: Proceedings of the 37th Inter-national Conference on Machine Learning (ICML), vol. 119, PMLR, 2020, pp. 7055–7065, http://proceedings .mlr.press /v119 /moshkovitz20a .html.[42] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, Bin Yu, Interpretable machine learning: definitions, methods, and applications, arXiv preprint, arXiv:1901.04592, 2019.[43] Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin, “Why should I trust you?” Explaining the predictions of any classifier, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.[44] C.S. Karthik, Bundit Laekhanukit, Pasin Manurangsi, On the parameterized complexity of approximating dominating set, J. ACM (ISSN 0004-5411) 66 (5) (August 2019), https://doi .org /10 .1145 /3325116.ence on Machine Learning, PMLR, 2017, pp. 3145–3153.[45] Avanti Shrikumar, Peyton Greenside, Anshul Kundaje, Learning important features through propagating activation differences, in: International Confer-[46] Mukund Sundararajan, Ankur Taly, Qiqi Yan, Axiomatic attribution for deep networks, in: International Conference on Machine Learning, PMLR, 2017, pp. 3319–3328.[47] Berk Ustun, Cynthia Rudin, Supersparse linear integer models for optimized medical scoring systems, Mach. Learn. 102 (3) (2016) 349–391.[48] Fulton Wang, Cynthia Rudin, Falling rule lists, in: Artificial Intelligence and Statistics, PMLR, 2015, pp. 1013–1022.20