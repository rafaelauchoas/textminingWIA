Artiﬁcial Intelligence 173 (2009) 413–436

Contents lists available at ScienceDirect

Artiﬁcial Intelligence

www.elsevier.com/locate/artint

Using arguments for making and explaining decisions ✩

Leila Amgoud

∗

, Henri Prade

Institut de Recherche en Informatique de Toulouse, CNRS – University of Toulouse III, 118 route de Narbonne, 31062 Toulouse, Cedex 09, France

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 8 November 2006
Received in revised form 13 November 2008
Accepted 17 November 2008
Available online 24 November 2008

Keywords:
Decision making
Argumentation

Arguments play two different roles in day life decisions, as well as in the discussion of
more crucial issues. Namely, they help to select one or several alternatives, or to explain
and justify an already adopted choice.
This paper proposes the ﬁrst general and abstract argument-based framework for decision
making. This framework follows two main steps. At the ﬁrst step, arguments for beliefs and
arguments for options are built and evaluated using classical acceptability semantics. At the
second step, pairs of options are compared using decision principles. Decision principles
are based on the accepted arguments supporting the options. Three classes of decision
principles are distinguished: unipolar, bipolar or non-polar principles depending on whether
i) only arguments pros or only arguments cons, or ii) both types, or iii) an aggregation
of them into a meta-argument are used. The abstract model
is then instantiated by
expressing formally the mental states (beliefs and preferences) of a decision maker. In
the proposed framework, information is given in the form of a stratiﬁed set of beliefs.
The bipolar nature of preferences is emphasized by making an explicit distinction between
prioritized goals to be pursued, and prioritized rejections that are stumbling blocks to
be avoided. A typology that identiﬁes four types of argument is proposed. Indeed, each
decision is supported by arguments emphasizing its positive consequences in terms of
goals certainly satisﬁed and rejections certainly avoided. A decision can also be attacked
by arguments emphasizing its negative consequences in terms of certainly missed goals,
or rejections certainly led to by that decision. Finally, this paper articulates the optimistic
and pessimistic decision criteria deﬁned in qualitative decision making under uncertainty,
in terms of an argumentation process. Similarly, different decision principles identiﬁed in
multiple criteria decision making are restated in our argumentation-based framework.

© 2008 Elsevier B.V. All rights reserved.

1. Introduction

Decision making, often viewed as a form of reasoning toward action, has raised the interest of many scholars including
philosophers, economists, psychologists, and computer scientists for a long time. Any decision problem amounts to selecting

✩

The present paper uniﬁes and develops the content of several conference papers [L. Amgoud, J.-F. Bonnefon, H. Prade, An argumentation-based approach
to multiple criteria decision, in: Proceedings of the 8th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty
(ECSQARU’05), 2005, pp. 269–280; L. Amgoud, H. Prade, A bipolar argumentation-based decision framework, in: Proceedings of the 11th International
Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU’06), 2006, pp. 323–330; L. Amgoud, H. Prade,
Comparing decisions in an argumentation-based setting, in: Proceedings of the 11th International Workshop on Non-Monotonic Reasoning (NMR’06), 2006;
L. Amgoud, H. Prade, Explaining qualitative decision under uncertainty by argumentation, in: Proceedings of the 21st National Conference on Artiﬁcial
Intelligence (AAAI’06), 2006, pp. 219–224 [2,7–9]].
* Corresponding author.

E-mail address: amgoud@irit.fr (L. Amgoud).
URL: http://www.irit.fr/~Leila.Amgoud/ (L. Amgoud).

0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.
doi:10.1016/j.artint.2008.11.006

414

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

the “best” or suﬃciently “good” action(s) that are feasible among different alternatives, given some available information
about the current state of the world and the consequences of potential actions. Note that available information may be
incomplete or pervaded with uncertainty. Besides, the goodness of an action is judged by estimating, maybe by means of
several criteria, how much its possible consequences ﬁt the preferences or the intentions of the decision maker. This agent
is assumed to behave in a rational way [41,42,49], at least in the sense that his decisions should be as much as possible
consistent with his preferences. However, we may have a more requiring view of rationality, such as demanding for the
conformity of decision maker’s behavior with postulates describing how a rational agent should behave [45].

Decision problems have been considered from different points of view. We may distinguish two main trends, which are
currently inﬂuencing research in artiﬁcial intelligence (AI): classical decision theory on the one hand, and cognitively-oriented
approaches such as practical reasoning or beliefs-desires-intentions (BDI) settings on the other hand.

1.1. Classical decision making vs. practical reasoning

Classical decision theory, as developed mainly by economists, has focused on making clear what is a rational decision
maker. Thus, they have looked for principles for comparing different alternatives. A particular decision principle, such as
the classical expected utility [45], has been justiﬁed on the basis of a set of rationality postulates to which the preference
relation between actions should obey. This means that in this approach, rationality is captured through a set of postulates
that describe what is a rational decision behavior. Moreover, a minimal set of postulates is identiﬁed in such a way that it
corresponds to a unique decision principle. The inputs of this approach are a set of candidate actions, and a function that
assesses the value of their consequences when the actions are performed in a given state, together with complete or partial
information about the current state of the world. In other words, such an approach distinguishes between knowledge and
preferences, which are respectively encoded in practice by a distribution function assessing the plausibility of the different
states of the world, and by a utility function encoding preferences by estimating how good a consequence is. The output is a
preference relation between actions encoded by the associated principle. Note that such an approach aims at rank-ordering
a group of candidate actions rather than focusing on a candidate action individually. Moreover, the candidate actions are
supposed to be feasible. Roughly speaking, we may distinguish two groups of works in AI dealing with decision that follow
the above type of approach. The ﬁrst group is represented by researches using Bayesian networks [40], on planning under
uncertainty (e.g. [21]). Besides, some AI works have aimed at developing more qualitative frameworks for decision, but still
along the same line of thoughts (e.g. [22,27,47]).

Other researchers working on practical reasoning, starting with the generic question “what is the right thing to do for
an agent in a given situation” [41,43], have proposed a two steps process to answer this question. The ﬁrst step, often
called deliberation [49], consists of identifying the goals of the agent. In the second step, they look for ways of achieving
those goals, i.e. for plans, and thus for intermediary goals and sub-plans. Such an approach raises issues such as: how
are goals generated ? are actions feasible ? do actions have undesirable consequences ? are sub-plans compatible ? are
there alternative plans for achieving a given goal, . . . . In [16], it has been argued that this can be done by representing
the cognitive states, namely agent’s beliefs, desires and intentions (thus the so-called BDI architecture). This requires a
rich knowledge/preference representation setting, which contrasts with the classical decision setting that directly uses an
uncertainty distribution (a probability distribution in the case of expected utility), and a utility (value) function. Besides,
the deliberation step is merely an inference problem since it amounts to ﬁnding a set of desires that are justiﬁed on the
basis of the current state of the world and of conditional desires. Checking if a plan is feasible and does not lead to bad
consequences is still a matter of inference. A decision problem only occurs when several plans or sub-plans are possible,
and one of them has to be chosen. This latter issue may be viewed as a classical decision problem. What is worth noticing
in most works on practical reasoning is the use of argument schemes for providing reasons for choosing or discarding an
action (e.g. [30,35]). For instance, an action may be considered as potentially useful on the basis of the so-called practical
syllogism [48]:

• G is a goal for agent X
• Doing action A is suﬃcient for agent X to carry out goal G
• Then, agent X ought to do action A

The above syllogism is in essence already an argument in favor of doing action A. However, this does not mean that the
action is warranted, since other arguments (called counter-arguments) may be built or provided against the action. Those
counter-arguments refer to critical questions identiﬁed in [48] for the above syllogism. In particular, relevant questions
are “Are there alternative ways of realizing G?”, “Is doing A feasible?”, “Has agent X other goals than G?”, “Are there
other consequences of doing A which should be taken into account?”. Recently in [10,11], the above syllogism has been
extended to explicitly take into account the reference to ethical values in arguments. Anyway, the idea of using arguments
for justifying or discarding candidate decisions is certainty very old, and its account in the literature at least dates back to
Aristotle. See also Benjamin Franklin [33] for an early precise account on the way of balancing arguments pros and cons a
choice, more than two hundred years ago.

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

415

1.2. Argumentation and decision making

Generally speaking, argumentation is a reasoning model based on the construction and the evaluation of interact-
ing arguments. Those arguments are intended to support/explain/attack statements that can be decisions, opinions, . . . .
Argumentation has been used for different purposes [44], such as non-monotonic reasoning (e.g. [28]). Indeed, several
frameworks have been developed for handling inconsistency in knowledge bases (e.g. [1,3,13]). Moreover, it has been shown
that such an approach is general enough to capture different existing approaches for non-monotonic reasoning [28]. Argu-
mentation has also been extensively used for modeling different kinds of dialogues, in particular persuasion (e.g. [5]), and
negotiation (e.g. [37]). Indeed, an argumentation-based approach for negotiation has the advantage of exchanging in addition
to offers, reasons that support these offers. These reasons may lead their receivers to change their preferences. Consequently,
an agreement may be more easily reached with such approaches, when in other approaches (where agent’s preferences are
ﬁxed) negotiation may fail. Adopting such an approach in a decision problem would have some obvious beneﬁts. Indeed,
not only would the user be provided with a “good” choice, but also with the reasons underlying this recommendation, in a
format that is easy to grasp. Note that each potential choice usually has pros and cons of various strengths. Argumentation-
based decision making is expected to be more akin with the way humans deliberate and ﬁnally make or also understand a
choice. This has been pointed out for a long time (see e.g. [33]).

1.3. Contribution of the paper

In this paper we deal with an argumentative view of decision making, thus focusing on the issue of justifying the
best decision to make in a given situation, and leaving aside the other related aspects of practical reasoning such as goal
generation, feasibility, and planning. It is why we remain close to the classical view of decision, but now discussed in terms
of arguments. The idea of articulating decisions on the basis of arguments is relevant for different decision problems or
approaches such as decision making under uncertainty, multiple criteria decisions, or rule-based decisions. These problems
are usually handled separately, and until recently without a close reference to argumentation. In practical applications, for
instance in medical domain, the decision to be made has to be chosen under incomplete or uncertain information, the
potential results of candidate decisions are evaluated from different criteria. Moreover, there may exist some expertise in
the form of decision rules that associate possible decisions to given contexts. This makes the different decision problems
somewhat related, and consequently a uniﬁed argumentation-based model is needed. This paper proposes such a model.

This paper proposes the ﬁrst general, and abstract argument-based framework for decision making. This framework fol-
lows two main steps. At the ﬁrst step, arguments for beliefs and arguments for options are built and evaluated using classical
acceptability semantics. At the second step, pairs of options are compared using decision principles. Decision principles are
based on the accepted arguments supporting the options. Three classes of decision principles are distinguished: unipolar,
bipolar or non-polar principles depending on whether i) only arguments pros or only arguments cons, or ii) both types, or
iii) an aggregation of them into a meta-argument are used. The abstract model is then instantiated by expressing formally
the mental states (beliefs and preferences) of a decision maker. In the proposed framework, information is given in the form
of a stratiﬁed set of beliefs. The bipolar nature of preferences is emphasized by making an explicit distinction between prior-
itized goals to be pursued, and prioritized rejections that are stumbling blocks to be avoided. A typology that identiﬁes four
types of argument is also proposed. Indeed, each decision is supported by arguments emphasizing its positive consequences
in terms of goals certainly satisﬁed and rejections certainly avoided. A decision can also be attacked by arguments empha-
sizing its negative consequences in terms of certainly missed goals, or rejections certainly led to by that decision. Another
contribution of the paper consists of applying the general framework to decision making under uncertainty and to multiple
criteria decision. Proper choices of decision principles are shown to be equivalent to known qualitative decision approaches.
The paper is organized as follows: Section 2 presents an abstract framework for decision making. Section 3 discusses
a typology of arguments supporting or attacking candidate decisions. Section 4 applies the abstract framework to multiple
criteria decision making, and Section 5 applies the framework to decision making under uncertainty. Section 6 compares our
approach to existing works on argumentation-based decision making, and Section 7 is devoted to some concluding remarks
and perspectives.

2. A general framework for argumentative decision making

Solving a decision problem amounts to deﬁning a pre-ordering, usually a complete one, on a set D of possible options
(or candidate decisions), on the basis of the different consequences of each decision. Let us illustrate this problem through
a simple example borrowed from [31].

Example 1 (Having or not a surgery). The example is about having a surgery (sg) or not (¬sg), knowing that the patient has
colonic polyps. The knowledge base contains the following information:

• having a surgery has side-effects,
• not having surgery avoids having side-effects,
• when having a cancer, having a surgery avoids loss of life,

416

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

• if a patient has cancer and has no surgery, the patient would lose his life,
• the patient has colonic polyps,
• having colonic polyps may lead to cancer.

In addition to the above knowledge, the patient has also some goals like: “no side effects” and “to not lose his life”.
Obviously it is more important for him to not lose his life than to not have side effects.

In what follows, L will denote a logical language. From L, a ﬁnite set D = {d1, . . . , dn} of n options is identiﬁed. Note
that an option di may be a conjunction of other options in D. Let us, for instance, assume that an agent wants a drink
and has to choose between tea, milk or both. Thus, there are three options: d1 : tea, d2 : milk and d3 : tea and milk. In
Example 1, the set D contains only two options: d1 : sg and d2 : ¬sg.

Argumentation is used in this paper for ordering the set D. An argumentation-based decision process can be decomposed

into the following steps:

(1) Constructing arguments in favor/against statements (pertaining to beliefs or decisions)
(2) Evaluating the strength of each argument
(3) Determining the different conﬂicts among arguments
(4) Evaluating the acceptability of arguments
(5) Comparing decisions on the basis of relevant “accepted” arguments

Note that the ﬁrst four steps globally correspond to an “inference problem” in which one looks for accepted arguments, and
consequently warranted beliefs. At this step, one only knows what is the quality of arguments in favor/against candidate
decisions, but the “best” candidate decision is not determined yet. The last step answers this question once a decision
principle is chosen.

2.1. Types of arguments

As shown in Example 1, decisions are made on the basis of available knowledge and the preferences of the decision
maker. Thus, two categories of arguments are distinguished: i) epistemic arguments justifying beliefs and are themselves
based only on beliefs, and ii) practical arguments justifying options and are built from both beliefs and preferences/goals.
Note that a practical argument may highlight either a positive feature of a candidate decision, supporting thus that decision,
or a negative one, attacking thus the decision.

Example 2 (Example 1 cont.). In this example, α = [“the patient has colonic polyps”, and “having colonic polyps may lead
to cancer”] is considered as an argument for believing that the patient may have cancer. This epistemic argument involves
only beliefs. While δ1 = [“the patient may have a cancer”, “when having a cancer, having a surgery avoids loss of life”] is
an argument for having a surgery. This is a practical argument since it supports the option “having a surgery”. Note that
such argument involves both beliefs and preferences. Similarly, δ2 = [“not having surgery avoids having side-effects”] is a
practical argument in favor of “not having a surgery”. However, the two practical arguments δ3 = [“having a surgery has
side-effects”] and δ4 = [“the patient has colonic polyps”, and “having colonic polyps may lead to cancer”, “if a patient has
cancer and has no surgery, the patient would lose his life”] are respectively against surgery and no surgery since they point
out negative consequences of the two options.

In what follows, Ae denotes a set of epistemic arguments, and Ap denotes a set of practical arguments such that
Ae ∩ Ap = ∅. Let A = Ae ∪ Ap (i.e. A will contain all those arguments). The structure and origin of the arguments are
assumed to be unknown. Epistemic arguments will be denoted by variables α1, α2, . . . , while practical arguments will be
referred to by variables δ1, δ2, . . . . When no distinction is necessary between arguments, we will use the variables a, b, c, . . . .

Example 3 (Example 1 cont.). Ae = {α} while Ap = {δ1, δ2, δ3, δ4}.

Let us now deﬁne two functions that relate each option to the arguments supporting it and to the arguments against it.

• Fp : D → 2
the option.
• Fc : D → 2
option.

Ap is a function that returns the arguments in favor of a candidate decision. Such arguments are said pro

Ap is a function that returns the arguments against a candidate decision. Such arguments are said con the

The two functions satisfy the following requirements:

• ∀d ∈ D, (cid:2)δ ∈ Ap s.t. δ ∈ Fp(d) and δ ∈ Fc(d). This means that an argument is either in favor of an option or against

that option. It cannot be both.

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

417

• If δ ∈ Fp(d) and δ ∈ Fp(d

one option.

• Let D = {d1, . . . , dn}. Ap = (
concern options of the set D.

(cid:9)) (resp. if δ ∈ Fc(d) and δ ∈ Fc(d
(cid:2)

(cid:2)

(cid:9))), then d = d

(cid:9)

. This means that an argument refers only to

Fp(di)) ∪ (

Fc(di)), with i = 1, . . . , n. This means that the available practical arguments

When δ ∈ Fx(d) with x ∈ {p, c}, we say that d is the conclusion of δ, and we write Conc(δ) = d.

Example 4 (Example 1 cont.). The two options of the set D = {sg, ¬sg} are supported/attacked by the following arguments:
Fp(sg) = {δ1}, Fc(sg) = {δ3}, Fp(¬sg) = {δ2}, and Fc(¬sg) = {δ4}.

2.2. Comparing arguments

As pointed out by several researchers (e.g. [19,29]), arguments may have forces of various strengths. These forces play
two key roles: i) they may be used in order to reﬁne the notion of acceptability of epistemic or practical arguments, ii)
they allow the comparison of practical arguments in order to rank-order candidate decisions. Generally, the strength of an
epistemic argument reﬂects the quality, such as the certainty level, of the pieces of information involved in it. Whereas the
strength of a practical argument reﬂects both the quality of knowledge used in the argument, as well as how important it
is to fulﬁll the preferences to which the argument refers.

In our particular application, three preference relations between arguments are deﬁned. The ﬁrst one, denoted by (cid:2)e , is a
(partial or total) preorder1 on the set Ae . The second relation, denoted by (cid:2)p , is a (partial or total) preorder on the set Ap .
Finally, a third relation, denoted by (cid:2)m (m stands for mixed relation), captures the idea that any epistemic argument is
stronger than any practical argument. The role of epistemic arguments in a decision problem is to validate or to undermine
the beliefs on which practical arguments are built. Indeed, decisions should be made under “certain” information. Thus,
∀α ∈ Ae , ∀δ ∈ Ap , (α, δ) ∈ (cid:2)m and (δ, α) /∈ (cid:2)m.

Note that (a, b) ∈ (cid:2)x, with x ∈ {e, p, m}, means that a is at least as good as b. At some places, we will also write a (cid:2)x b.
In what follows, >x denotes the strict relation associated with (cid:2)x. It is deﬁned as follows: (a, b) ∈ >x iff (a, b) ∈ (cid:2)x and
(b, a) /∈ (cid:2)x. When (a, b) ∈ (cid:2)x and (b, a) ∈ (cid:2)x, we say that a and b are indifferent, and we write a ≈x b. When (a, b) /∈ (cid:2)x and
(b, a) /∈ (cid:2)x, the two arguments are said incomparable.

Example 5 (Example 1 cont.). (cid:2)e = {(α, α)} and (cid:2)m = {(α, δ1), (α, δ2)}. Now, regarding (cid:2)p , one may, for instance, assume
that δ1 is stronger than δ2 since the goal satisﬁed by δ1 (namely, not loss of life) is more important than the one satisﬁed
by δ2 (not having side effects). Thus, (cid:2)p= {(δ1, δ1), (δ2, δ2), (δ1, δ2)}. This example will be detailed in a next section.

2.3. Attacks among arguments

Since knowledge may be inconsistent, the arguments may be conﬂicting too. Indeed, epistemic arguments may attack
each others. Such conﬂicts are captured by the binary relation Re ⊆ Ae × Ae . This relation is assumed abstract and its
origin is not speciﬁed.

Epistemic arguments may also attack practical arguments when they challenge their knowledge part. The idea is that an
epistemic argument may undermine the beliefs part of a practical argument. However, practical arguments are not allowed
to attack epistemic ones. This avoids wishful thinking. This relation, denoted by Rm, contains pairs (α, δ) where α ∈ Ae
and δ ∈ Ap .

We assume that practical arguments do not conﬂict. The idea is that each practical argument points out some advantage
or some weakness of a candidate decision, and it is crucial in a decision problem to list all those arguments for each
candidate decision, provided that they are accepted w.r.t. the current epistemic state, i.e. built from warranted beliefs.
According to the attitude of the decision maker in face of uncertain or inconsistent knowledge, these lists associated with
the candidate decisions may be taken into account in different manners, thus leading to different orderings of the decisions.
This is why all accepted arguments should be kept, whatever their strengths, for preserving all relevant information in the
decision process. Otherwise, getting rid of some of those accepted arguments (w.r.t. knowledge), for instance because they
would be weaker than others, may prevent us to have a complete view of the decision problem and then may even lead us
to recommend decisions that would be wrong w.r.t. some decision principles (agreeing with the presumed decision maker’s
attitude). This point will be made more concrete in a next section. Thus, the relation Rp ⊆ Ap × Ap is equal to the empty
set (Rp = ∅).

Each preference relation (cid:2)x (with x ∈ {e, p, m}) is combined with the conﬂict relation Rx into a unique relation between

arguments, denoted by Defx and called defeat relation, in the same way as in ([4], Deﬁnition 3.3, page 204).

1 A preorder is a binary relation that is reﬂexive and transitive.

418

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

Deﬁnition 1 (Defeat relation). Let A be a set of arguments, and a, b ∈ A. (a, b) ∈ Defx iff:

• (a, b) ∈ Rx, and
• (b, a) /∈ >x

Let Defe , Defp and Defm denote the three defeat relations corresponding to the three attack relations. In case of Defm,
the second bullet of Deﬁnition 1 is always true since epistemic arguments are strictly preferred (in the sense of (cid:2)m) to any
practical arguments. Thus, Defm = Rm (i.e. the defeat relation is exactly the attack relation Rm). The relation Defp is the
same as Rp , thus it is empty. However, the relation Defe coincides with its corresponding attack relation Re in case all
the arguments of the set Ae are incomparable.

2.4. Extensions of arguments

Now that the sets of arguments and the defeat relations are identiﬁed, we can deﬁne the decision system.

Deﬁnition 2 (Decision system). Let D be a set of options. A decision system for ordering D is a triple AF = (D, A, Def) where
A = Ae ∪ Ap

2 and Def = Defe ∪ Defp ∪ Defm.3

Note that a Dung style argumentation system is associated to a decision system AF = (D, A, Def), namely the system
(A, Def). This latter can be seen as the union of two distinct argumentation systems: AFe = (Ae, Defe), called epistemic
system, and AFp = (Ap, Defp), called practical system. The two systems are related to each other by the defeat relation
Defm.

Due to Dung’s acceptability semantics deﬁned in [28], it is possible to identify among all the conﬂicting arguments,
which ones will be kept for ordering the options. An acceptability semantics amounts to deﬁne sets of arguments that
satisfy a consistency requirement and must defend all their elements.

Deﬁnition 3 (Conﬂict-free, Defence). Let AF = (D, A, Def) be a decision system, B ⊆ A, and a ∈ A.

• B is conﬂict-free iff (cid:2)a, b ∈ B s.t. (a, b) ∈ Def.
• B defends a iff ∀b ∈ A, if (b, a) ∈ Def, then ∃c ∈ B s.t. (c, b) ∈ Def.

The main semantics introduced by Dung are recalled in the following deﬁnition. Note that other semantics have been

deﬁned in the literature (e.g. [12]). However, these will not be discussed in this paper.

Deﬁnition 4 (Acceptability semantics). Let AF = (D, A, Def) be a decision system, and B be a conﬂict-free set of arguments.

• B is admissible extension iff it defends any element in B.
• B is a preferred extension iff B is a maximal (w.r.t. set ⊆) admissible set.
• B is a stable extension iff it is a preferred extension that defeats any argument in A\B.

Using these acceptability semantics, a status is assigned to each argument of AF as follows.

Deﬁnition 5 (Argument status). Let AF = (D, A, Def) be a decision system, and E1, . . . , Ex its extensions under a given
semantics. Let a ∈ A.

• a is skeptically accepted iff a ∈ Ei , ∀Ei with i = 1, . . . , x.
• a is credulously accepted iff ∃Ei such that a ∈ Ei .
• a is rejected iff (cid:2)Ei such that a ∈ Ei .

A direct consequence of the above deﬁnition is that an argument is skeptically accepted iff it belongs to the intersection

of all extensions, and that it is rejected iff it does not belong to the union of all extensions. Formally:

Property 1. Let AF = (D, A, Def) be a decision system, and E1, . . . , Ex its extensions under a given semantics. Let a ∈ A.

• a is skeptically accepted iff a ∈
• a is rejected iff a /∈

(cid:2)

Ei .

x
i=1

(cid:3)

x
i=1

Ei ;

2 Recall that options are related to their supporting and attacking arguments by the functions Fp and Fc respectively.
3 Since the relation Defp is empty, then Def = Defe ∪ Defm .

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

419

Let Acc(x, y) be a function that returns the skeptically accepted arguments of decision system x under semantics y
( y ∈ {ad, st, pr} with ad (resp. st and pr) stands for admissible (resp. stable and preferred) semantics). This set may contain
both epistemic and practical arguments. Such arguments are very important in argumentation process since they support
the conclusions to be inferred from a knowledge base or the options that will be chosen. Indeed, for ordering the different
candidate decisions, only skeptically accepted practical arguments are used. The following property shows the links between
the sets of accepted arguments under different semantics.

Property 2. Let AF = (D, A, Def) be a decision system.

• Acc(AF, ad) = ∅.
• If AF has no stable extensions, then Acc(AF, st) = ∅ and Acc(AF, st) ⊆ Acc(AF, pr).
• If AF has stable extensions, then Acc(AF, pr) ⊆ Acc(AF, st).

Proof. Let AF = (D, A, Def) be a decision system.

• In [28],

(cid:3)

it has been shown that the empty set is an admissible extension of any argumentation system. Thus,

Ei=1,...,x = ∅ where E1, . . . , Ex are the admissible extensions of AF. Consequently, Acc(AF, ad) = ∅.

• Let us assume that the system AF has no stable extensions. Thus, according to Deﬁnition 5, all arguments of A are

rejected. Thus, Acc(AF, st) = ∅.

• Let us now assume that the system AF has stable extensions, say E1, . . . , En. Dung has shown in [28] that any stable
extension is a preferred one, but the converse is not true. Thus, E1, . . . , En are also preferred extensions. Let us now
assume that the system has other extensions that are preferred but not stable, say En+1, . . . , Ex with x (cid:2) n + 1. From
set theory, it is clear that

Ei . According to Property 1, it follows that Acc(AF, pr) ⊆ Acc(AF, st). (cid:2)

Ei ⊆

(cid:3)

(cid:3)
n
i=1

x
i=1

From the above property, one concludes that in a decision problem, it is not interesting to use admissible semantics. The
reason is that no argument is accepted. Consequently, argumentation will not help at all for ordering the different candidate
decisions. Let us illustrate this issue through the following simple example.

Example 6. Let us consider the decision system AF = (D, Ae ∪ Ap, Def) where D = {d1, d2}, Ae = {α1, α2, α3}, Ap = {δ}
and Def is depicted in ﬁgure below. We assume that Fp(d1) = δ whereas Fp(d2) = Fc(d2) = ∅.

The admissible extensions of this system are: E1 = {}, E2 = {α1}, E3 = {α2}, E4 = {α1, δ} and E5 = {α2, δ}. Under admissible
semantics, the practical argument δ is not skeptically accepted. Thus, the two options d1 and d2 may be equally preferred
since the ﬁrst one has an argument but not an accepted one, and the second has no argument at all. However, the same
decision system has two preferred extensions: E4 and E5. Under preferred semantics, the set Acc(AF, pr) contains the
argument δ (i.e. Acc(AF, pr) = {δ}). Thus, it is natural to prefer the option d1 to d2.

Consequently, in the following, we will use stable semantics if the system has stable extensions, otherwise preferred

semantics will be considered for computing the set Acc(AF, y).

Since the defeat relation Defp is empty, it is trivial that the practical system AFp has exactly one preferred/stable

extension which is the set Ap itself.

Property 3. The practical system AFp = (Ap, Defp) has a unique preferred/stable extension, which is the set Ap .

Proof. This follows directly from the fact that the set Ap is conﬂict-free since Defp = ∅. (cid:2)

It is important to notice that the epistemic system AFe in its side is very general and does not necessarily present

particular properties like for instance the existence of stable/preferred extensions.

In what follows, we will show that the result of the decision system depends broadly on the outcome of its epistemic
system. The ﬁrst result states that the epistemic arguments of each admissible extension of AF constitute an admissible
extension of the epistemic system AFe .

Theorem 1. Let AF = (D, Ae ∪ Ap, Defe ∪ Defp ∪ Defm) be a decision system, E1, . . . , En its admissible extensions, and AFe =
(Ae, Defe) its associated epistemic system.

420

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

• ∀Ei , the set Ei ∩ Ae is an admissible extension of AFe .
• ∀E (cid:9)

is an admissible extension of AFe , ∃Ei such that E (cid:9) ⊆ Ei ∩ Ae .

such that E (cid:9)

Proof.

• Let Ei be an admissible extension of AF. Let E = Ei ∩ Ae . Let us assume that E is not an admissible extension of AFe .

There are two cases:

Case 1:

Case 2:

E is not conﬂict-free. This means that ∃α1, α2 ∈ E such that (α1, α2) ∈ Defe . Thus, ∃α1, α2 ∈ Ei such that
(α1, α2) ∈ Def. This is impossible since Ei is an admissible extension, thus conﬂict-free.
E does not defend its elements. This means that ∃α ∈ E , such that ∃α(cid:9) ∈ Ae , (α(cid:9), α) ∈ Defe and (cid:2)α(cid:9)(cid:9) ∈ E such
that (α(cid:9)(cid:9), α(cid:9)) ∈ Defe . Since (α(cid:9), α) ∈ Defe , this means that (α(cid:9), α) ∈ Def with α ∈ Ei . However, Ei is admissi-
ble, then ∃a ∈ Ei such that (a, α(cid:9)) ∈ Def. Assume that a ∈ Ap . This is impossible since practical arguments are
not allowed to defeat epistemic ones. Thus, a ∈ Ae . Hence, a ∈ E . Contradiction.

• Let E (cid:9)

does not defend all its elements in the system AF. This means that ∃a ∈ E (cid:9)

be an admissible extension of AFe . Let us prove that E (cid:9)
an admissible extension of AF. There are two possibilities: i) E (cid:9)
is an admissible extension of AFe , thus conﬂict-free.
such that E (cid:9)
ii) E (cid:9)
does not defend a.
This means also that ∃b /∈ E (cid:9)
such that (c, b) ∈ Def. There are two cases: either
b ∈ Ae or b ∈ Ap . b cannot be in Ae since E (cid:9)
is an admissible extension thus defends its arguments against any attack,
consequently it defends also a against b. Assume now that b ∈ Ap , this is also impossible since practical arguments are
not allowed to attack epistemic ones. Thus, E (cid:9)

is not
is not conﬂict-free in AF. This is not possible since E (cid:9)

is an admissible extension of AF. Assume that E (cid:9)

is an admissible extension of the system AF. (cid:2)

such that (b, a) ∈ Def and (cid:2)c ∈ E (cid:9)

Note that the above theorem holds as well for stable and preferred extensions since each stable (resp. preferred) exten-

sion is an admissible one.

It is easy to show that when Defm is empty, i.e. no epistemic argument defeats a practical one, then the extensions of
AF (under a given semantics) are exactly the different extensions of AFe (under the same semantics) augmented by the
set AFp .

Theorem 2. Let AF = (D, Ae ∪ Ap, Defe ∪ Defp ∪ Defm) be a decision system. Let E1, . . . , En be the extensions of AFe under a
given semantics. If Defm = ∅ then ∀Ei with i = 1, . . . , n, then the set Ei ∪ Ap is an extension of AF.

Proof. Let E be an admissible extension of AFe . Let us assume that E ∪ Ap is not an admissible extension of AF. There are
two cases:

Case 1:

Case 2:

E ∪ Ap is not conﬂict-free. Since E and Ap are conﬂict-free, then ∃α ∈ E and ∃δ ∈ Ap such that (α, δ) ∈ Def.
Contradiction with the fact that Defm = ∅.
E ∪ Ap does not defend its elements. This means that: i) ∃α ∈ E such that ∃α(cid:9) ∈ Ae , (α(cid:9), α) ∈ Defe and E ∪ Ap
does not defend it. Impossible since E is an admissible extension then it defends its arguments. ii) ∃δ ∈ Ap such
that ∃a ∈ A, and (a, δ) ∈ Def and δ is not defended by E ∪ Ap . Since Defm = ∅ then a ∈ Ap . This is impossible
since Rp = ∅. Contradiction. (cid:2)

Finally, it can be shown that if the empty set is the only admissible extension of the decision system AF, then the empty
set is also the only admissible extension of the corresponding epistemic system AFe . Moreover, each practical argument is
attacked by at least one epistemic argument.

Theorem 3. Let AF = (D, Ae ∪ Ap, Defe ∪ Defp ∪ Defm) be a decision system. The only admissible extension of AF is the empty
set iff :

(1) The only admissible extension of AFe is the empty set, and
(2) ∀δ ∈ Ap , ∃α ∈ Ae such that (α, δ) ∈ Defm.

Proof. Let AF = (D, Ae ∪ Ap, Defe ∪ Defp ∪ Defm) be a decision system.

Case 1: Assume that the empty set is the only admissible extension of AF. Assume also that the epistemic system AFe has
a non-empty admissible extension, say E. This means that E is not an admissible extension of AF. There are two
cases:
a) E is not conﬂict-free. This is impossible since E is an admissible extension of AFe .
b) E does not defend its elements. This means that ∃a ∈ Ae ∪ Ap such that ∃b ∈ E and (a, b) ∈ Def and (cid:2)c ∈ E

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

421

such that (c, a) ∈ Def. There are two possibilities: i) a ∈ Ap . This is impossible since practical arguments are not
allowed to attack epistemic arguments. ii) a ∈ Ae . Since E is an admissible extension of AFe , then ∃c ∈ E such that
(c, a) ∈ Defe . Thus, (c, a) ∈ Def. Contradiction.
Let us now assume that the empty set is the only admissible extension of AFe and that ∀δ ∈ Ap , ∃α ∈ Ae such
that (α, δ) ∈ Defm. Assume also that ∃E (cid:13)= ∅ such that E is an admissible extension of the decision system AF.

From Theorem 1, E ∩ Ae is an admissible extension of AFe . Since the only admissible extension of AFe is the

Case 2:

empty set, then E ∩ Ae = ∅. Thus, E ⊆ Ap .

Let δ ∈ E. By assumption, ∃α ∈ Ae such that (α, δ) ∈ Defm. Since E is an admissible extension, thus it defends
all its elements. Consequently, ∃δ(cid:9) ∈ E such that (δ(cid:9), α) ∈ Def. Since E ⊆ Ap , then δ(cid:9) ∈ Ap . It is impossible to have
(δ(cid:9), α) ∈ Def since practical arguments are not allowed to attack epistemic ones. (cid:2)

At this step, we have only deﬁned the accepted arguments among all the existing ones. However, nothing is yet said
about which option to prefer. In the next section, we will study different ways of comparing pairs of options on the basis
of skeptically accepted practical arguments.

2.5. Ordering options

Comparing candidate decisions, i.e. deﬁning a preference relation (cid:14) on the set D of options, is a key step in a decision
process. In an argumentation-based approach, the deﬁnition of this relation is based on the sets of “accepted” arguments
pros or cons associated with candidate decisions. Thus, the input of this relation is no longer Ap , but the set Acc(AF, y) ∩
Ap , where Acc(AF, y) is the set of skeptically accepted arguments of the decision system (D, A, Def) under stable or
preferred semantics. In what follows, we will use the notation Acc(AF) for short.

Note that in a decision system, when the defeat relation Defm is empty, the epistemic arguments become useless for

the decision problem, i.e. for ordering options. Thus, only the practical system AFp is needed.

Depending on what sets are considered and how they are handled, one can roughly distinguish between three categories

of principles:

Unipolar principles: are those that only refer to either the arguments pros or the arguments cons.
Bipolar principles: are those that take into account both types of arguments at the same time.
Non-polar principles: are those where arguments pro and arguments con a given choice are aggregated into a unique

meta-argument. It results that the negative and positive polarities disappear in the aggregation.

Whatever the category is, a relation (cid:14) should suitably satisfy the following minimal requirements:

(1) Transitivity: The relation should be transitive (as usually required in decision theory).
(2) Completeness: Since one looks for the “best” candidate decision, it should then be possible to compare any pair of

choices. Thus, the relation should be complete.

2.5.1. Unipolar principles

In this section we present basic principles for comparing decisions on the basis of only arguments pro. Similar ideas
apply to arguments cons. We start by presenting those principles that do not involve the strength of arguments, then their
respective reﬁnements when strength is taken into account.

A ﬁrst natural criterion consists of preferring the decision that has more arguments pros.

Deﬁnition 6 (Counting arguments pros). Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let
d1, d2 ∈ D.

d1 (cid:14) d2

iff

(cid:4)
(cid:4)
(cid:4) (cid:2)
(cid:4)Fp(d1) ∩ Acc(AF)

(cid:4)
(cid:4)
(cid:4).
(cid:4)Fp(d2) ∩ Acc(AF)

Property 4. This relation is a complete preorder.

Note that when the decision system has no accepted arguments (i.e. Acc(AF) = ∅), all the options in D are equally
preferred w.r.t. the relation (cid:14). It can be checked that if a practical argument is deﬁned as done later in Deﬁnition 18, then
(cid:9)
with such a principle, one may prefer a decision d, which has three arguments pointing all to the same goal, to decision d
,
which is supported by two arguments pointing to different goals.

When the strength of arguments is taken into account in the decision process, one may think of preferring a choice that
has a dominant argument, i.e. an argument pros that is preferred w.r.t. the relation (cid:2)p⊆ Ap × Ap to any argument pro the
other choices. This principle is called promotion focus principle in [2].

Deﬁnition 7. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.

d1 (cid:14) d2

iff ∃δ ∈ Fp(d1) ∩ Acc(AF) such that ∀δ

(cid:9) ∈ Fp(d2) ∩ Acc(AF), δ (cid:2)p δ

(cid:9)

.

422

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

With this criterion, if the decision system has no accepted arguments, then all the options in D are equally preferred. The
above deﬁnition relies heavily on the relation (cid:2)p that compares practical arguments. Thus, the properties of this criterion
depend on those of (cid:2)p . Namely, it can be checked that the above criterion works properly if (cid:2)p is a complete preorder.

Property 5. If the relation (cid:2)p is a complete preorder, then (cid:14) is also a complete preorder.

Note that the above relation may be found to be too restrictive, since when the strongest arguments in favor of d1 and
d2 have equivalent strengths (i.e. are indifferent), d1 and d2 are also seen as equivalent. However, we can reﬁne the above
deﬁnition by ignoring the strongest arguments with equal strengths, by means of the following strict preorder.

Deﬁnition 8. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D, and (cid:2)p be
a complete preorder. Let (δ1, . . . , δr ), (δ(cid:9)
∈ Fp(d2) ∩
Acc(AF).

s) such that ∀δi=1,...,r , δi ∈ Fp(d1) ∩ Acc(AF), and ∀δ(cid:9)

j=1,...,s, δ(cid:9)

1, . . . , δ(cid:9)

j

Each of these vectors is assumed to be decreasingly ordered w.r.t. (cid:2)p (e.g. δ1 (cid:2)p · · · (cid:2)p δr ). Let v = min(r, s).

d1 (cid:14) d2 iff:

1, or

• δ1 >p δ(cid:9)
k and ∀ j < k, δ j ≈p δ(cid:9)
• ∃k (cid:3) v such that δk >p δ(cid:9)
• r > v and ∀ j (cid:3) v, δ j ≈p δ(cid:9)
j .

j , or

Till now, we have only discussed decision principles based on arguments pros. However, the counterpart principles when
arguments cons are considered can also be deﬁned. Thus, the counterpart principle of the one deﬁned in Deﬁnition 6 is the
following complete preorder:

Deﬁnition 9 (Counting arguments cons). Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let
d1, d2 ∈ D.

d1 (cid:14) d2

iff

(cid:4)
(cid:4)
(cid:4) (cid:3)
(cid:4)Fc(d1) ∩ Acc(AF)

(cid:4)
(cid:4)
(cid:4).
(cid:4)Fc(d2) ∩ Acc(AF)

The principles that take into account the strengths of arguments have also their counterparts when handling arguments
cons. The prevention focus principle prefers a decision when all its cons are weaker than at least one argument against the
other decision. Formally:

Deﬁnition 10. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.

d1 (cid:14) d2

iff ∃δ ∈ Fc(d2) ∩ Acc(AF) such that ∀δ

(cid:9) ∈ Fc(d1) ∩ Acc(AF), δ (cid:2)p δ

(cid:9)

.

As in the case of arguments pros, when the relation (cid:2)p is a complete preorder, the above relation is also a complete

preorder, and can be reﬁned into the following strict one.

Deﬁnition 11. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.
Let (δ1, . . . , δr ), (δ(cid:9)
s) such that ∀δi=1,...,r , δi ∈ Fc(d1) ∩ Acc(AF), and ∀δ(cid:9)
∈ Fc(d2) ∩ Acc(AF).
Each of these vectors is assumed to be decreasingly ordered w.r.t. (cid:2)p (e.g. δ1 (cid:2)p · · · (cid:2)p δr ). Let v = min(r, s).
d1 (cid:15) d2 iff:

j=1,...,s, δ(cid:9)

1, . . . , δ(cid:9)

j

1 >p δ1, or

• δ(cid:9)
• ∃k (cid:3) v such that δ(cid:9)
• v < s and ∀ j (cid:3) v, δ j ≈p δ(cid:9)
j .

k >p δk and ∀ j < k, δ j ≈p δ(cid:9)

j , or

2.5.2. Bipolar principles

Let’s now deﬁne some principles where both types of arguments (pros and cons) are taken into account when com-
paring decisions. Generally speaking, we can conjunctively combine the principles dealing with arguments pros with their
counterpart handling arguments cons. For instance, the principles given in Deﬁnition 6 and Deﬁnition 9 can be combined
as follows:

Deﬁnition 12. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D. d1 (cid:14) d2 iff

• |Fp(d1) ∩ Acc(AF)| (cid:2) |Fp(d2) ∩ Acc(AF)|, and
• |Fc(d1) ∩ Acc(AF)| (cid:3) |Fc(d2) ∩ Acc(AF)|.

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

423

However, note that unfortunately this is no longer a complete preorder. Similarly, the principles given respectively in

Deﬁnition 7 and Deﬁnition 10 can be combined into the following one:

Deﬁnition 13. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D. d1 (cid:14) d2 iff:

• ∃δ ∈ Fp(d1) ∩ Acc(AF) such that ∀δ(cid:9) ∈ Fp(d2) ∩ Acc(AF), δ (cid:2)p δ(cid:9)
• (cid:2)δ ∈ Fc(d1) ∩ Acc(AF) such that ∀δ(cid:9) ∈ Fc(d2) ∩ Acc(AF), δ (cid:2)p δ(cid:9)
.

, and

This means that one prefers a decision that has at least one supporting argument which is better than any supporting
argument of the other decision, and also has not a very strong argument against it. Note that the above deﬁnition can be
also reﬁned in the same spirit as Deﬁnitions 8 and 11.

Another family of bipolar decision principles applies the Franklin principle which is a natural extension to the bipolar case
of the idea underlying Deﬁnition 8. This principle consists, when comparing pros and cons a decision, of ignoring pairs of
arguments pros and cons which have the same strength. After such a simpliﬁcation, one can apply any of the above bipolar
principles. In what follows, we will deﬁne formally the Franklin simpliﬁcation.

Deﬁnition 14 (Franklin simpliﬁcation). Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let
d ∈ D.

Let P = (δ1, . . . , δr), C = (δ(cid:9)
Each of these vectors is assumed to be decreasingly ordered w.r.t. (cid:2)p (e.g. δ1 (cid:2)p · · · (cid:2)p δr ). The result of the simpliﬁcation
(cid:9) = (δ j+1, . . . , δr), C

m) such that ∀δi, δi ∈ Fp(d) ∩ Acc(AF) and ∀δ(cid:9)

∈ Fc(d) ∩ Acc(AF).

1, . . . , δ(cid:9)

(cid:9) = (δ(cid:9)

j, δ(cid:9)

is P

j

j+1, . . . , δ(cid:9)

m) s.t.

• ∀1 (cid:3) i (cid:3) j, δi ≈p δ(cid:9)
• If j = r (resp. j = m), then P

i and (δ j+1 >p δ(cid:9)

j+1 or δ(cid:9)
(cid:9) = ∅ (resp. C

j+1 >p δ j+1).
(cid:9) = ∅).

2.5.3. Non-polar principles

In some applications, the arguments in favor of and against a decision are aggregated into a unique meta-argument
having a unique strength. Thus, comparing two decisions amounts to compare the resulting meta-arguments. Such a view is
well in agreement with current practice in multiple criteria decision making, where each decision is evaluated according to
different criteria using the same scale (with a positive and a negative part), and an aggregation function is used to obtain a
global evaluation of each decision.

Deﬁnition 15 (Aggregation criterion). Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let
d1, d2 ∈ D. Let (δ1, . . . , δn)4 and (δ(cid:9)
k )7) the vectors of the arguments pro and
con the decision d1 (resp. d2).
d1 (cid:14) d2 iff h(δ1, . . . , δn, δ(cid:9)

1, . . . , δ(cid:9)
m) (cid:2)p h(γ1, . . . , γl, γ (cid:9)

m)5 (resp. (γ1, . . . , γl)6 and (γ (cid:9)

k), where h is an aggregation function.

1, . . . , γ (cid:9)

1, . . . , γ (cid:9)

1, . . . , δ(cid:9)

A simple example of this aggregation attitude is computing the difference of the number of arguments pros and cons.

Deﬁnition 16. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.
d1 (cid:14) d2 iff |Fp(d1) ∩ Acc(AF)| − |Fc(d1) ∩ Acc(AF)| (cid:2) |Fp(d2) ∩ Acc(AF)| − |Fc(d2) ∩ Acc(AF)|.

This has the advantage to be again a complete preorder, while taking into account both pros and cons arguments.

3. A typology of formal practical arguments

This section aims at presenting a systematic study of practical arguments. Epistemic arguments will not be discussed here
because they have been much studied in the literature (e.g. [3,13,46]), and their handling does not make new problems in
the general setting of Section 2, even in the decision process perspective of this paper. Moreover, they only play a role when
the knowledge base is inconsistent. Before presenting the different types of practical arguments, we start ﬁrst by introducing
the logical language as well as the different bases needed in a decision making problem.

4 Each δi ∈ Fp(d1) ∩ Acc(AF).
5 Each δ(cid:9)
∈ Fc(d1) ∩ Acc(AF).
i
6 Each γi ∈ Fp(d2) ∩ Acc(AF).
7 Each γ (cid:9)
∈ Fc(d2) ∩ Acc(AF).
i

424

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

3.1. Logical representation of knowledge and preference

This section introduces the representation setting of knowledge and preference which are here distinct, as it is in classical
decision theory. Moreover, preferences are supposed to be handled in a bipolar way, which means that what the decision
maker is really looking for may be more restrictive than what it is just willing to avoid.

In what follows, a vocabulary P of propositional variables contains two kinds of variables: decision variables, denoted
by v 1, . . . , vn, and state variables. Decision variable are controllable, that is their value can be ﬁxed by the decision maker.
Making a decision then amounts to ﬁxing the truth value of every decision variable. On the contrary, state variables are
ﬁxed by nature, and their value is a matter of knowledge by the decision maker. He has no control on them (although he
may express preferences about their values).

(1) D is a set of formulas built from the decision variables. Elements of D represent the different alternatives, or candidate
decisions. Let us consider the following example of an agent who wants to know whether she should take her umbrella,
her raincoat or both. In this case, there are two decision variables: umb (for umbrella) and rac (for raincoat). Assume
that this agent hesitates between the three following options: i) d1 : umb (i.e. to take her umbrella), ii) d2 : rac (i.e.
to take her raincoat), or iii) d3 : umb ∧ rac (i.e. to take both). Thus, D = {d1, d2, d3}. Note that elements of D are not
necessarily mutually exclusive. In the example, if the agent chooses the option d3 then the two other options are
satisﬁed.

(2) G is a set of propositional formulas built from state variables. It gathers the goals of an agent (the decision maker).
A goal represents what the agent wants to achieve, and has thus a positive ﬂavor. This means that if g ∈ G, the decision
maker wants that the chosen decision leads to a state of affairs where g is true. This base may be inconsistent. In this
case it would be for sure impossible to satisfy all the goals, which would induce the simultaneous existence of practical
arguments pros and cons. In general G contains several goals. Clearly, an agent should try to satisfy all goals in its
goal base G if possible. This means that G may be thought as a conjunction. However, the two goal bases G = {g1, g2}
and G(cid:9) = {g1 ∧ g2} although they are logically equivalent, will not be handled in the same way in an argumentative
perspective, since in the second case there is no way to consider intermediary objectives such as here satisfying g1,
or satisfying g2 only, in case it turns out that it is impossible to satisfy g1 ∧ g2. This means that our approach is
syntax-dependent.

(3) The set R is a set of propositional formulas built from state variables. It gathers the rejections of an agent. A rejection
represents what the agent wants to avoid. Clearly rejections express negative preferences. The set {¬r | r ∈ R} describing
what is acceptable for the agent is assumed to be consistent, since acceptable alternatives should satisfy ¬r due to the
rejection of r, and at least there should remain some possible worlds that are not rejected. There are at least two
reasons for separately considering a set of goals and a set of rejections. First, since agents naturally express themselves
in terms of what they are looking for (i.e. their goals), and in terms of what they want to avoid (i.e. their rejections), it
is better to consider goals and rejections separately in order to articulate arguments referring to them in a way easily
understandable for the agents. Moreover, recent cognitive psychology studies [17] have conﬁrmed the cognitive validity
of this distinction between goals and rejections. Second, if r is a rejection, this does not necessarily mean that ¬r is a
goal, and thus rejections cannot be equivalently restated as goals. For instance, in case of choosing a medical drug, one
may have as a goal the immediate availability of the drug, and as a rejection its availability only after at least two days.
In such a case, if the candidate decision guarantees the availability only after one day, this decision will for sure avoid
the rejection without satisfying the goal. Another simple example is the case of an agent who wants to get a cup of
either coffee or tea, and wants to avoid getting no drink. If the agent obtains a glass of water, again he would avoid its
rejection, without being completely satisﬁed.
We can imagine different forms of consistency between the goals and the rejections. A minimal requirement is to have
G ∩ R = ∅, otherwise it will mean that an agent both wants to have p true and to avoid it.

(4) The set K represents the background knowledge that is not necessarily assumed to be consistent. The argumentation
framework for inference presented in Section 2 will handle such inconsistency, namely with the epistemic system.
Elements of K are propositional formulas built from the alphabet P , and assumed to be put in a clausal form. The
base K contains basically two kinds of clauses: i) those not involving any element from D, which encode pieces of
knowledge or factual information (possibly involving goals) about how the world is; ii) those involving one negation of
a formula d of the set D, and which states what follows when decision d is applied.

Thus, the decision problem we consider will always be encoded with the four above sets of formulas (with the restrictions
stated above). Moreover, we suppose that each of the three bases K, G, and R are stratiﬁed. Having K stratiﬁed would
mean that we consider that some pieces of knowledge are fully certain, while others are less certain (maybe distinguishing
between several levels of partial certainty such as “almost certain”, “rather certain”, . . . ). Clearly, formulas that are not
certain at all cannot be in K. Similarly, having G (resp. R) stratiﬁed means that some goals (resp. rejections) are imperative,
while some others are less important (one may have more than two levels of importance). Completely unimportant goals
(resp. rejections) do not appear in any stratum of G (resp. R).

It is worth pointing out that we assume that candidate decisions are all considered as a priori equally potentially suitable,

and thus there is no need to have D stratiﬁed.

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

425

For encoding the stratiﬁcations, we use the set {0, 1, . . . , n} of integers as a linearly ordered scale, where n stands for
the highest level of certainty if dealing with K (resp. level of importance if dealing with G or R) and ‘0’ corresponds to the
complete lack of certainty (resp. importance). Other encodings (e.g. using levels inside the unit interval or using the integer
scale in a reversed way) would be equivalent.

Deﬁnition 17 (Decision theory). A decision theory (or a theory for short) is a tuple T = (cid:17)D, K, G, R(cid:18).

• The base K is partitioned and stratiﬁed into K1, . . . , Kn (K = K1 ∪ · · · ∪ Kn) such that formulas in Ki have the same
certainty level and are more certain than formulas in K j where j < i. Moreover, K0 is not considered since it gathers
formulas that are completely uncertain.

• The base G is partitioned and stratiﬁed into G1, . . . , Gn (G = G1 ∪ · · · ∪ Gn) such that goals in Gi have the same impor-
tance and are more important than goals in G j where j < i. Moreover, G0 is not considered since it gathers goals that
are completely unimportant.

• The base R is partitioned and stratiﬁed into R1, . . . , Rn (R = R1 ∪ · · · ∪ Rn) such that rejections in Ri have the same
importance and are more important than rejections in R j where j < i. Moreover, R0 is not considered since it gathers
rejections that are completely unimportant.

3.2. A typology of formal practical arguments

Each candidate decision may have arguments in its favor (called pros), and arguments against it (called cons). In the

following, an argument is associated with an alternative, and always either refers to a goal or to a rejection.

Arguments pros point out the “existence of good consequences” or the “absence of bad consequences” for a candidate
decision. A good consequence means that applying decision d will lead to the satisfaction of a goal, or to the avoidance of a
rejection. Similarly, a bad consequence means that the application of d leads for sure to miss a goal, or to reach a rejected
situation.

We can distinguish between practical arguments referring to a goal, and those arguments referring to rejections. When
focusing on the base G, an argument pros corresponds to the guaranteed satisfaction of a goal when there exists a consistent
subset S of K such that S ∪ {d} (cid:19) g.

Deﬁnition 18 (Positive arguments pros). Let T be a theory. A positively expressed argument in favor of an option d is a tuple
δ = (cid:17)S, d, g(cid:18) s.t.:

(1) S ⊆ K, d ∈ D, g ∈ G, S ∪ {d} is consistent
(2) S ∪ {d} (cid:19) g, and S is minimal for set inclusion among subsets of K satisfying the above criteria (arguments of Type PP).

S is called the support of the argument, and d is its conclusion. Let APP be the set of all arguments of type PP that can be
built from a decision theory T .

In what follows, Supp denotes a function that returns the support S of an argument, Conc denotes a function that
returns the conclusion d of the argument, and Result denotes a function that returns the consequence of the decision.
The consequence may be either a goal as in the previous deﬁnition, or a rejection as we can see in the next deﬁnitions of
argument types.

The above deﬁnition deserves several comments.

• The consistency of S ∪ {d} means that d is applicable in the context S, in other words that we cannot prove from S
that d is impossible. This means that impossible alternatives w.r.t. K have been already taken out when deﬁning the
set D. In the particular case where the base K would be consistent, then condition 1, namely S ∪ {d} is consistent, is
equivalent to K ∪ {d} is consistent. But, in the case where K is inconsistent, independently from the existence of a PP
(cid:9) (cid:19) ¬d. This would mean that there is some doubt
argument, it may happen that for another consistent subset S
about the feasibility of d, and then constitute an epistemic argument against d. In the general framework proposed in
Section 2, such an argument will overrule decision d since epistemic arguments take precedence over any practical
argument (provided that this epistemic argument is not itself killed by another epistemic argument).

of K, S

• Note that argument of type PP are reminiscent of the practical syllogism recalled in the introduction. Indeed, it em-
phasizes that a candidate decision might be chosen if it leads to the satisfaction of a goal. However, this is only a clue
for choosing the decision since this last may have arguments against, which would weaken it, or there may exist other
candidate decisions with stronger arguments. Moreover, due to the nature of the practical syllogism, it is worth noticing
that practical arguments have an abductive form, contrarily to epistemic arguments that are deﬁned in a deductive way,
as revealed by their formal respective deﬁnitions.

(cid:9)

Another type of arguments pros refers to rejections. It amounts to avoid a rejection for sure, i.e. S ∪ {d} (cid:19) ¬r (where S is

a consistent subset of K).

426

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

Deﬁnition 19 (Negative arguments pros). Let T be a theory. A negatively expressed argument in favor of an option is a tuple
δ = (cid:17)S, d, r(cid:18) s.t.:

(1) S ⊆ K, d ∈ D, r ∈ R, S ∪ {d} is consistent
(2) S ∪ {d} (cid:19) ¬r and S is minimal for set inclusion among subsets of K satisfying the above criteria (arguments of Type NP).

Let ANP be the set of all arguments of type NP that can be built from a decision theory T .

Arguments cons highlight the existence of bad consequences for a given candidate decision. Negatively expressed argu-

ments cons are deﬁned by exhibiting a rejection that is necessarily satisﬁed. Formally:

Deﬁnition 20 (Negative arguments cons). Let T be a theory. A negatively expressed argument against an option d is a tuple
δ = (cid:17)S, d, r(cid:18) s.t.:

(1) S ⊆ K, d ∈ D, r ∈ R, S ∪ {d} is consistent,
(2) S ∪ {d} (cid:19) r and S is minimal for set inclusion among subsets of K satisfying the above criteria (arguments of Type NC).

Let ANC be the set of all arguments of type NC that can be built from a decision theory T .

Lastly, the absence of positive consequences can also be seen as an argument against (cons) an alternative.

Deﬁnition 21 (Positive arguments cons). Let T be a theory. A positively expressed argument against an option d is a tuple
δ = (cid:17)S, d, g(cid:18) s.t.:

(1) S ⊆ K, d ∈ D, g ∈ G, S ∪ {d} is consistent,
(2) S ∪ {d} (cid:19) ¬g and S is minimal for set inclusion among subsets of K satisfying the above criteria (arguments of Type PC).

Let APC be the set of all arguments of type PC that can be built from a decision theory T .

Let us illustrate the previous deﬁnitions on an example.

Example 7. Two decisions are possible, organizing a show (d), or not (¬d). Thus D = {d, ¬d}. The knowledge base K contains
the following pieces of knowledge: if a show is organized and it rains then small money loss (¬d ∨ ¬r ∨ sml); if a show
is organized and it does not rain then beneﬁt (¬d ∨ r ∨ b); small money loss entails money loss (¬sml ∨ ml); if beneﬁt
there is no money loss (¬b ∨ ¬ml); small money loss is not large money loss (¬sml ∨ ¬lml); large money loss is money
loss (¬lml ∨ ml); there are clouds (c); if there are clouds then it may rain (¬c ∨ r). All these pieces of knowledge are in the
stratum of level n, except the last one which is in a stratum with a lower level due to uncertainty. Consider now the cases
of two organizers (O 1 and O 2) having different preferences. O 1 does not want any loss R = {ml}, and would like beneﬁt
G = {b}. O 2 does not want large money loss R = {lml}, and would like beneﬁt G = {b}. In such case, it is expected that
O 1 prefers ¬d to d, since there is a NC argument against d and no argument for ¬d. For O 2, there is no longer any NC
argument against d. He might even prefer d to ¬d, if he is optimistic and he considers that there is a possibility that it does
not rain (leading to a potential PP argument under the hypothesis to have ¬r in K).

Due to the asymmetry in human mind between what is rejected and what is desired, the former being usually consid-
ered as stronger than the latter, one may assume that NC arguments are stronger than PC arguments, and conversely PP
arguments are stronger than NP arguments.

In classical decision frameworks, bipolarity is not considered. Indeed, we are in the particular case where rejections
mirror goals in the sense that g is a goal iff ¬g is a rejection. Consequently, in our argumentation setting the two types NC
and PC coincide. Similarly, the two types PP and NP are the same.

4. Application to multiple criteria decision making

4.1. Introduction to multiple criteria decision making

In multiple criteria decision making, each candidate decision d in D is evaluated from a set C of m different points of
view (i = 1, m), called criteria. The evaluation can be done in an absolute manner or in a relative way. This means that
for each i, d can be either evaluated by an absolute estimate ci(d) belonging to the evaluation scale used for i, or there
(cid:9)) of elements
exists a possibly valued preference relation R i(d, d
of D. Then one can distinguish between two families of approaches: i) the ones based on a global aggregation of value
criteria-based functions where the obtained global absolute evaluations are of the form g( f 1(C1(d), . . . , fm(Cm(d)))) where

(cid:9)) associated with each i that is applicable to any pair (d, d

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

427

the mappings f i map the original evaluations on a unique scale, which assumes commensurability, and ii) the ones that
(cid:9)) from which a ranking of the elements in D can
aggregate the preference indices R i(d, d
be obtained. In the following, only the ﬁrst type of approach is considered.

(cid:9)) into a global preference R(d, d

4.2. Arguments in multiple criteria decision making

The decision maker uses a set C of different criteria. For each criterion ci , one assumes that we have a bipolar univariate
ordered scale T i which enables us to distinguish between positive and negative values. Such a scale has a neutral point,
or more generally a neutral area that separates positive and negative values. The lower bound of the scale stands for total
dissatisfaction and the upper bound for total satisfaction, while neutral value(s) stand for indifference. The closer to the upper
bound the value of criterion ci for choice d, denoted ci(d) is, the more satisfactory choice d is w.r.t. ci ; the closer to the
lower bound the value of criterion ci for choice d is, the more dis-satisfactory choice d is w.r.t. ci . As in multiple criteria
aggregation, we assume that the different scales T i can be mapped on a unique bipolar scale T , i.e. for any i, f i(ci(d)) ∈ T .
Moreover, we assume here that T is discrete and will be denoted T = {−k, . . . , −1, 0, +1, . . . , +k} with the classical ordering
convention of relative integers.

Example 8 (Choosing an apartment). Imagine we have a set C of three criteria for choosing an apartment: Price (c1),
Size (c2), and Location w.r.t. downtown (c3). The criteria are valued on the same bipolar univariate scale {−2, −1, 0, +1, +2}
(this means that all the f i mappings are the identity). Prices of apartments may be judged ‘very expensive’,
‘expensive’,
‘reasonably priced’,
‘very large’. Distance
may be ‘very far’,
‘very close’. In each case, the ﬁve linguistic expressions would be valued by
−2, −1, 0, +1, +2 respectively. Thus an apartment d that is expensive, medium-sized, and very close to downtown will
be evaluated as c1(d) = −1, c2(d) = 0, and c3(d) = +2. It is clear that this scale implicitly encodes that the best apartments
are those that are very cheap, very large, and very close to downtown.

‘very cheap’. Size may be ‘very small’,

‘cheap’,
‘far’,

‘normal sized’,

‘medium’,

‘small’,

‘close’,

‘large’,

From this setting, it is possible to express goals and rejections in terms of criteria values. A bipolar-valued criterion can
be straightforwardly translated into a set of stratiﬁed goals, and a stratiﬁed set of rejections. The idea is the following. The
criteria may be satisﬁed either in a positive way (if the satisfaction degree is higher than the neutral point 0 of T ) or in a
negative way (if the satisfaction degree is lower than the neutral point of T ). Formally speaking, the two bases G and R are
deﬁned as follows: having the condition f i(ci(d)) (cid:2) + j satisﬁed, where + j belongs to the positive part of T , is a goal g j
for the agent that uses ci as a criterion. This goal is all the more important as j is small (but positive), since as suggested
by the above example, the less restrictive conditions are the most imperative ones. The importance of g j can be taken as
equal to k − j + 1 for j (cid:2) 1 (using the standard order-reversing map on {1, . . . , k}). Indeed the most important condition
f i(ci(d)) (cid:2) +1 will have the maximal value in T , while the condition f i(ci(d)) (cid:2) +k will have the minimal positive level
in T , i.e. +1. We can proceed similarly with rejections. The rejection r j corresponding to the condition f i(ci(d)) (cid:3) − j will
have importance j (importance uses only the positive part of the scale). This corresponds to the view of a fuzzy set as a
nested family of level cuts, which translates in possibilistic logic [25] into a collection of propositions whose extensions are
all the larger as the proposition is more imperative. In the above example, consider for instance the price criterion. We will
have two goals: g1 = very cheap and g2 = cheap with respective weights 1 and 2. Thus, being cheap is more imperative
than being very cheap as expected. Similarly, r1 = very expensive and r2 = expensive are rejections with respective weights
2 and 1. Note that if an apartment is normally sized, then there will be no argument in favor or against it w.r.t. its size.

In multiple criteria aggregation, criteria may have different levels of importance. Let w i ∈ {0, +1, . . . , +k} be the impor-
tance of criterion ci . Then, we can apply the above translation procedure where, now the importance k − j + 1 of condition
f i(ci(d)) (cid:2) + j is changed into min(w i, k − j + 1). Indeed, if w i is maximal, i.e. w i = +k, the importance is unchanged; in
case the importance w i of criterion ci would be minimal, i.e. w i = 0, then the resulting importance of the associated goal
(the condition f i(ci(d)) (cid:2) + j) is indeed also 0 expressing its complete lack of importance.

In addition to the bases D, C, G and R, the decision maker is also equipped with a stratiﬁed knowledge base K encoding
what he knows. In particular, K contains factual information about the values of the f i(ci(x))’s for the different criteria and
the different candidate decisions. K also contains rules expressing that values in T are linearly ordered, i.e. rules of the form
(cid:9) ∈ T . More generally, K can also contain pieces of knowledge that enable the decision
if ci(x) (cid:2) j, then ci(x) (cid:2) j
maker to evaluate criteria from more elementary evaluation of facts. This may be useful in practice for describing complex
notions, e.g. comfort of a house in our example, which indeed may depend on many parameters. A goal is assumed to be
is associated to a criterion ci by a propositional formula of the form
associated with a unique criterion. Then, a goal g
i refers to the evaluation of criterion ci . Such formulas will be added to Kn. Note
g
that in classical multiple criteria problems, complete information is usually assumed w.r.t. the precise evaluation of criteria.
Clearly, our setting is more general since it leaves room to incomplete information, and facilitates the expression of goals
and rejections.

→ ci meaning just that the goal g

if j (cid:2) j

j
i

j
i

(cid:9)

j

Now that the different bases are introduced, we can apply our general decision system, and build the arguments pros
and cons for any candidate decision. In addition to the completeness of information, it is usually assumed in classical
approaches to multiple criteria decision making that knowledge is consistent. In such a case, it is not possible to have

428

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

conﬂicting evaluations of a criterion for a choice. Consequently, the whole power of our argumentation setting will not be
used, in particular all arguments will be accepted.

4.3. Retrieving some classical multiple criteria aggregations

The aim of this subsection is to show the agreement of the argumentation-based approach with some classical (ele-
mentary) approaches to multiple criteria decision making. It is worth mentioning that until recently, most multiple criteria
approaches use only positive evaluations, i.e. unipolar scales ranging from “bad” to “good” values, rather than distinguishing
genuinely good values from really bad ones that have to be rejected. The argumentation-based approach makes natural the
reference to the distinction between what is favored and what is disfavored by the decision maker for giving birth to ar-
guments for and arguments against candidate decisions. Here, only two types of arguments are needed: positive arguments
pros of type PP, and negative arguments cons of type NC since rejections in this case are just the complement of goals.
Indeed, the negative values of a criterion reﬂect the fact that we are below some threshold while the positive values express
to what extent we are above. Thus, Ap = APP ∪ ANC.

In what follows, the base K is supposed to be consistent, fully certain (i.e. K = Kn), and to contain complete information
w.r.t. the evaluation of criteria. Thus, the set A of arguments is exactly Ae ∪ Ap . Since K is consistent, then the two attack
relations Re and Rm are empty (i.e. Re = Rm = ∅). Consequently, Defe = Defm = ∅, and the set of skeptically accepted
arguments of the decision system AF = (D, A, Def = ∅) is exactly Acc(AF) = A.

The ﬁrst category of classical approaches to multiple criteria decision making that we will study is the one that gives
the same importance to the different criteria of the set C. The idea is to prefer the alternative that satisﬁes positively more
(cid:9)
i(d) = 0 if ci(d) < 0, where ci(d) is the evaluation of choice d by the ith criterion.
criteria. Let c
In order to capture this idea, a particular unipolar principle is used. Before introducing this principle, let us ﬁrst deﬁne a
function Results that returns for a given set B of practical arguments, all the consequences of those arguments, i.e. all
the goals and rejections to which arguments of B refer to.

(cid:9)
i(d) = 1 if ci(d) > 0 and c

Deﬁnition 22. Let d1, d2 ∈ D. d1 (cid:14) d2 iff Results(Fp(d2)) ⊆ Results(Fp(d1)).

Note that in our case, Fp(d) ⊆ APP and Fc(d) ⊆ ANC for a given d ∈ D.

Property 6. Let AF = (D, A, Def) be a decision system. Let d1, d2 ∈ D. When C = Cn, d1 (cid:14) d2 (according to Deﬁnition 22) iff
(cid:5)

(cid:5)

(cid:9)
i(d1) (cid:2)

i c

(cid:9)
i(d2).

i c

If we focus on arguments cons, the idea is to prefer the option that violates less criteria. Let c

(cid:9)(cid:9)
i (d) = 1 if ci(d) < 0. This idea is captured by the following unipolar principle.

c

(cid:9)(cid:9)
i (d) = 0 if ci(d) > 0 and

Deﬁnition 23. Let d1, d2 ∈ D. d1 (cid:14) d2 iff Results(Fc(d1)) ⊆ Results(Fc(d2)).

Property 7. Let (D, A, Def) be a decision system. Let d1, d2 ∈ D. When C = Cn, d1 (cid:14) d2 (according to Deﬁnition 23) iff
(cid:5)

(cid:9)(cid:9)
i (d2).

i c

(cid:5)

(cid:9)(cid:9)
i (d1) (cid:3)

i c

(cid:9)
i(d) with c

When the criteria do not have the same level of importance, the promotion focus principle given in Deﬁnition 7 amounts
(cid:9)
i(d) = 0 if ci(d) < 0 as an evaluation function for comparing decisions.
to use maxic
Recall that the promotion focus principle is based on a preference relation (cid:2)p between arguments. In what follows, we will
propose a deﬁnition of the force of an argument, as well as a deﬁnition of (cid:2)p . The two deﬁnitions are chosen in such a
way that they will allow us to retrieve the above idea on the promotion focus principle.

(cid:9)
i(d) = ci(d) if ci(d) > 0 and c

In our application, the force of an argument depends on two components: the certainty level of the knowledge involved

in the argument, and the importance degree of the goal (or rejection). Formally:

Deﬁnition 24 (Force of an argument). Let δ = (cid:17)S, d, g(cid:18) ∈ APP (resp. δ = (cid:17)S, d, r(cid:18) ∈ ANC). The strength of δ is a pair (Lev(δ),
Wei(δ)) s.t.

• The certainty level of the argument is Lev(δ) = min{i | 1 (cid:3) i (cid:3) n such that S i (cid:13)= ∅}, where S i denotes S ∩ Ki . If S = ∅

then Lev(δ) = n.

• The weight of the argument is Wei(δ) = j if g ∈ G j (resp. r ∈ R j ).

Note that the above deﬁnition is general (and will be re-used as such in the next section). However, here, all arguments
have the same certainty level which is equal to n. The levels of satisfaction of the criteria should be balanced with their
relative importance. Indeed, for instance, a criterion ci highly satisﬁed by d is not a strong argument in favor of d if ci
has little importance. Conversely, a poorly satisﬁed criterion for d is a strong argument against d only if the criterion is
really important. Moreover, in case of uncertain criteria evaluation, one may have to discount arguments based on such

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

429

evaluation. In other terms, the force of an argument represents to what extent the decision will satisfy the most important
criteria. This suggests the use of a conjunctive combination of the certainty level, the satisfaction/dissatisfaction degree and
the importance of the criterion. This requires the commensurateness of the scales.

Deﬁnition 25 (Conjunctive strength). Let δ, δ(cid:9) ∈ APP. δ >p δ(cid:9)

iff min(Lev(δ), Wei(δ)) > min(Lev(δ(cid:9)), Wei(δ(cid:9))).

Property 8. Let AF = (D, A, Def) be a decision system. Let d1, d2 ∈ D. When C = Cn, d1 (cid:14) d2 (according to Deﬁnition 7 and using
Deﬁnition 25 for the relation (cid:2)p ) iff maxic

(cid:9)
i(d1) (cid:2) maxic

(cid:9)
i(d2).

The prevention focus principle (see Deﬁnition 10) amounts to use minic

if ci(d) < 0.

(cid:9)(cid:9)
i (d) with c

(cid:9)(cid:9)
i (d) = 0 if ci(d) > 0 and c

(cid:9)(cid:9)
i (d) = −ci(d)

Property 9. Let AF = (D, A, Def) be a decision system. Let d1, d2 ∈ D. When C = Cn, d1 (cid:14) d2 (according to Deﬁnition 10 and using
Deﬁnition 25 for the relation (cid:2)p ) iff minic

(cid:9)(cid:9)
i (d1) (cid:3) minic

(cid:9)(cid:9)
i (d2).

When each criterion ci is associated with a level of importance w i ranging on the positive part of the criteria scale, the

above c

(cid:9)
i(d) is changed into min(c

(cid:9)
i(d), w i) in the promotion case.

Property 10. Let AF = (D, A, Def) be a decision system. Let d1, d2 ∈ D. d1 (cid:14) d2 (according to Deﬁnition 7 and using Deﬁnition 25
for the relation (cid:2)p ) iff maximin(c

(cid:9)
i(d1), w i) (cid:2) maximin(c

(cid:9)
i(d2), w i).

This expresses that d is all the more preferred as there is an important criterion that is positively evaluated. A similar
proposition holds for the prevention focus principle. Thus, weighted disjunctions and conjunctions deﬁned in [26] are re-
trieved. It would even be possible to provide the argumentative counter-part of a general qualitative weighted conjunction
of the form minimax(ci(d), neg(w i)), where neg is the reversing map of the discrete scale where w i takes its value. However,
this would be quite similar to the qualitative decision making under uncertainty problem which is now discussed in great
detail, and where aggregations having the same structure are encountered.

5. Application to decision making under uncertainty

Decision making under uncertainty relies on the comparative evaluation of different alternatives on the basis of a de-
cision principle, which can be usually justiﬁed by means of a set of rationality postulates. This is, for example, the Savage
view of decision making under uncertainty based on expected utility [45]. Thus, standard approaches for making decisions
under uncertainty consist in deﬁning decision principles in terms of analytical expressions that summarize the whole de-
cision process, and for which it is shown that they encode a preference relation obeying postulates that are supposedly
meaningful. Apart from quantitative principles such as expected utility, another example of such an approach is provided by
the qualitative pessimistic or optimistic decision principles, which have been more recently proposed and also axiomatically
justiﬁed [27,34]. The qualitative nature of these decision evaluations make them more liable to be unpacked in terms of
arguments in favor/against each choice, in order to better understand the underpinnings of the evaluation. We successively
study the pessimistic and optimistic decision principles. Note, however, that these qualitative decision criteria do not make
use of a bipolar univariate scale, so in the following we apply our general decision system with an empty set of rejections.
Thus, we will consider a decision theory T = (cid:17)D, K, G(cid:18). Consequently, the set of practical arguments built from such a
theory is Ap = APP ∪ APC. Recall that arguments of type PP are pro their conclusions whereas arguments of type PC are
cons their conclusions. In classical decision systems, the knowledge base and the goals base are assumed to be consistent.
Thus, in what follows, we will assume that they are consistent as well. Thus, the three defeat relations Defe , Defp and
Defm are empty. Consequently, the decision system that will be used is (D, Ae ∪ Ap, Def = ∅). In such a system, the whole
decision process is reduced to the last step, which consists of ordering pairs of options. This means that in order to show
how pessimistic and optimistic principles are captured, it is suﬃcient to choose the most suitable decision principle among
the ones proposed in Section 2.5.

5.1. Pessimistic criterion

The pessimistic decision criterion is deﬁned as follows: given a possibility distribution πd restricting the plausible states
that can be reached when a decision d takes place, and a qualitative utility function μ, the so-called pessimistic qualitative
decision principle, which estimates a kind of qualitative expectation, is deﬁned as [27]:

E∗(d) = min
ω

max

(cid:6)
μ(ω), neg

(cid:6)
πd(ω)

(cid:7)(cid:7)

(1)

where πd is a mapping from a set of interpretations Ω to a linearly ordered scale U = {0, 1, . . . , n}, and μ is a mapping
from Ω to the same scale U , and neg is the involutive order-reversing map on U = {0, 1, . . . , n} such that neg(0) = n and
neg(n) = 0, where 0 and n are the bottom and the top elements of U . Namely, neg(n − k) = k. Thus, πd(ω) (resp. μ(ω))

430

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

is all the greater as ω is more plausible (resp. satisfactory), 0 standing for the minimal level, and n for the maximal level.
Moreover, πd and μ are assumed to be normalized, i.e. ∃ω ∈ Ω such that πd(ω) = 1, and similarly ∃ω(cid:9) ∈ Ω , μ(ω(cid:9)) = 1.

E∗(d) is all the greater as all the states ω that have some plausibility according to πd are among the most preferred
states according to μ. E∗(d) is in fact a degree of inclusion of the fuzzy set of plausible states (when d is applied) into the
fuzzy set of preferred states. The pessimistic utility E∗(d) is small as soon as there exists a possible consequence of d that
is both highly plausible and has a low satisfaction level with respect to preferences. This is clearly a risk-averse and thus a
pessimistic attitude.

In [25], it has been shown that a stratiﬁed knowledge base has a possibility distribution as semantic counterpart. See
Appendix for a refresher on possibilistic logic. Let Kd be the knowledge base built from the base K to which the decision d
is added to the stratum Kn. Let πd be the possibility distribution associated with Kd, and μ be the possibility distribution
associated with the base G of goals. The normalization of πd and μ is equivalent to the non-emptiness of the highest strata
Kn and Gn. It has been shown in [23] that it is possible to compute E∗(d), as expressed by formula (1), by only using a
classical logic machinery on x-level cuts of the two bases Kd and G.

Proposition 1. (See [23].) E∗(d) is the maximal value of x ∈ U s.t.

(Kd)x (cid:19) (G)neg(x)

(2)

where (B)x (resp. (B)x) is the set of formulas of a base B that appear in the strata x, . . . , n (resp. in the strata x + 1, . . . , n). Mind that
(B)x is a set of strata, while Bx is a stratum. By convention, E∗(d) = 0 if there is no such x.

E∗(d) is equal to n (x = n) if the completely certain part of Kd entails the satisfaction of all the goals, even the ones

with low priorities.

In the pessimistic view, as pointed out by Proposition 1, we are interested in ﬁnding a decision d (if it exists) such that
Kx ∪ {d} (cid:19) G y with x high and y low, i.e. such that the decision d together with the most certain part of K entails the
satisfaction of the goals, even those with low priority (provided that those with higher priority are also satisﬁed).

Example 9 (Surgery example cont.). The example is about having or not a surgery, knowing that the patient has colonic
polyps. The knowledge base is K = Kn ∪Kx, with Kn = {cp, sg → se, ¬sg → ¬se, sg → ¬ll, ca∧¬sg → ll}, and Kx = {cp → ca},
(0 < x < n) where se: having side-effect, ca: cancer, ll: loss of life, sg: having a surgery, cp: having colonic polyps. The integer
x < n refers to a lack of complete certainty.

The goals base is G = Gn ∪ G y with Gn = {¬ll}, and G y = {¬se} (where 0 < y < n). We do not like to have side effects

after a surgery, but it is more important to not lose life.

The set of decisions is D = {sg, ¬sg}.
Note that (Ksg)n (cid:19) (G)n. However, (Ksg)n (cid:3) (G) y . Note also that (Ksg)1 (cid:19) (G)n while (Ksg)1 (cid:3) (G) y . It is clear that (Ksg)n (cid:3)
(G)neg(n) = (G)1. Thus, the only value that satisﬁes Proposition 1 is neg( y). Indeed, (Ksg)neg( y) (cid:19) (G) y . Consequently, E∗(sg) =
neg( y).

In its side, the option ¬sg violates the most important goal (¬ll). Thus, there is no value that satisﬁes Proposition 1.

Consequently, E∗(¬sg) = 0.

We are going to show that the result of Proposition 1 can be captured in terms of arguments pros, i.e., arguments of
type PP are underlying the pessimistic criterion. We ﬁrst relate the absence of PP arguments to some consequences on the
value of E∗. We then conversely relate the value of E∗ to the existence of some PP arguments. Lastly, we show that the
comparison of decisions in terms of criterion E∗ can also be handled directly in terms of PP arguments.

Let us ﬁrst recall the deﬁnition of the strength of an argument already used in the previous section (Deﬁnition 24).

Deﬁnition 26 (Strength of an argument). Let δ = (cid:17)S, d, g(cid:18) ∈ Ap . The strength of δ is a pair (Lev(δ), Wei(δ)) s.t.

• The certainty level of the argument is Lev(δ) = min{i | 1 (cid:3) i (cid:3) n such that S i (cid:13)= ∅}, where S i denotes S ∩ Ki . If S = ∅

then Lev(δ) = n.

• The weight of the argument is Wei(δ) = y s.t. g ∈ G y .

The two following theorems state that the absence of strong PP argument can only weaken E∗(d).

Theorem 4. Let d ∈ D. If ∃g ∈ Gk s.t. (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP then E∗(d) (cid:3) neg(k).

Proof. By reduction ab absurbo. Assume E∗(d) (cid:13)(cid:3) neg(k). Then E∗(d) (cid:2) neg(k) + 1. By Proposition 1, (Kd)neg(k)+1 (cid:19)
(G)neg(neg(k)+1). But neg(neg(k) + 1) = k − 1. Thus, (Kd)neg(k)+1 (cid:19) (G)k. This clearly contradicts the hypothesis that ∃g ∈ Gk
s.t. (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP. (cid:2)

Theorem 5. Let d ∈ D. If ∀g ∈ G, (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP with Lev(δ) > l and l (cid:2) 1, then E∗(d) (cid:3) l.

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

431

Proof. By reduction ab absurbo also. Assume that E∗(d) (cid:13)(cid:3) l. Then E∗(d) (cid:2) l + 1. By Proposition 1, (Kd)(l+1) (cid:19) (G)neg(l+1).
Since l (cid:2) 1, neg(l + 1) (cid:3) n − 2. Thus, (Kd)(l+1) (cid:19) (G)(n−1). This contradicts the hypothesis that ∀g, (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP with
Lev(δ) > l, which means that ∀(cid:17)S, d, g(cid:18) ∈ APP, Lev(δ) (cid:3) l, since (G)(n−1) cannot be empty. (cid:2)

The third theorem states that the value of E∗(d) is determined by the existence of suﬃciently certain PP arguments in
favor of decision d with respect to all important goals whose priority is above some value, and the absence of any more
certain PP argument in favor of decision d with respect to some goal whose priority may be smaller.

Theorem 6. Let d ∈ D. If E∗(d) = x, then ∀g ∈ (G)neg(x)+1, ∃δ = (cid:17)S, d, g(cid:18) ∈ APP, and Lev(δ) (cid:2) x. Moreover, ∃g ∈ G s.t. (cid:2)δ =
(cid:17)S, d, g(cid:18) ∈ APP with Lev(δ) (cid:2) x + 1 and Wei(δ) (cid:2) neg(x).

Proof. Assume that E∗(d) = x. By Proposition 1, (Kd)x (cid:19) (G)neg(x). Thus, ∀g ∈ (G)neg(x)+1, ∃δ = (cid:17)S, d, g(cid:18) ∈ APP and
Lev(δ) (cid:2) x. Besides, (Kd)x+1 (cid:3) (G)neg(x+1), and then ∃g ∈ G s.t. (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP with Lev(δ) > x and Wei(δ) (cid:2)
neg(x + 1) + 1 = neg(x). (cid:2)

However, as illustrated by the following example, E∗(d) = x does not necessarily mean that there does not exist a

suﬃciently good PP argument for some goal in Gneg(x).

Example 10. Let n = 4. Let G4 = {g1}, G3 = {g2}, G2 = {g3}. Assume (K)4 (cid:19) {g2, g3}, and (K)2 (cid:19) {g1}, but (K)3 (cid:13)(cid:19) {g1}. Then
(K)2 (cid:19) (G)3, i.e. (K)2 (cid:19) (G)2, and then E∗(d) = 2. Note that here (K)2 (cid:19) (G)1, but (K)3 (cid:13)(cid:19) (G)1 (here neg(3) = 1), since the
most important goal has only a rather weak proof when d takes place, namely (K)3 (cid:13)(cid:19) {g1}, although stronger proofs exist
for less important goals: (K)2 (cid:19) (G)1, and thus E∗(d) (cid:13)= 3.

The above results show the links between PP arguments supporting candidate decisions and the pessimistic values E∗
assigned to those decisions. In what follows, we will show that an instantiation of our decision system returns the same
ordering on the set D as the one obtained by comparing the pessimistic values of elements of D. As already said, this
amounts to choose the most appropriate decision principle. In the case of pessimistic decision making, the most suitable
principle is the one proposed in Deﬁnition 7. Let us recall that principle:

Deﬁnition 27. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.

d1 (cid:14) d2

iff ∃δ ∈ Fp(d1) ∩ Acc(AF) such that ∀δ

(cid:9) ∈ Fp(d2) ∩ Acc(AF), δ (cid:2)p δ

(cid:9)

.

Note that this principle is based on a preference relation (cid:2)p between practical arguments. For our purpose, this relation
prefers the argument that is based on a subset of K made of beliefs that are more certain and that together entail a goal
having a higher priority. Formally, using the usual Pareto strict partial order between vectors:

Deﬁnition 28 (Comparing arguments of type PP). Let δ, δ(cid:9) ∈ PP. δ is stronger than δ(cid:9)
(Lev(δ), neg(Wei(δ))) >Pareto (Lev(δ(cid:9)), neg(Wei(δ(cid:9)))).

, denoted δ >p δ(cid:9)

,

if and only if

Let us now relate the result of our decision system to that of pessimistic decision making.

Theorem 7. Let T = (cid:17)D, K, G(cid:18) be a decision theory, and AF = (D, Ae ∪ Ap, Def = ∅) be the decision system. If ∀g ∈ G s.t. ∀δ(cid:9) =
(cid:17)S, d2, g(cid:18) ∈ APP, ∃δ = (cid:17)S
, then E∗(d1) (cid:2) E∗(d2).

(cid:9), d1, g(cid:18) ∈ APP and δ >p

8δ(cid:9)

Proof. Assume 0 < E∗(d1) < E∗(d2). Then ∃x s.t. E∗(d1) = x. Then (Kd1 )x (cid:19) (G)neg(x) and (Kd1 )x+1 (cid:13)(cid:19) (G)neg(x+1), while
(Kd2 )x+1 (cid:19) (G)neg(x+1). This contradicts the hypothesis. Indeed ∀δ ∈ APP s.t. Conc(δ) = d1, Lev(δ) < x + 1, but ∃δ(cid:9) ∈ APP
s.t. Conc(δ) = d2 and Lev(δ) = x + 1. Assume 0 = E∗(d1) < E∗(d2). Then (Kd1 )0 (cid:13)(cid:19) (G)n, and (cid:2)δ ∈ APP s.t. Conc(δ) = d1. (cid:2)

The converse of the above theorem is false as shown by the example below, where E∗(d1) > E∗(d2).

Example 11. Let G = {g1, g2}, Gn = {g1}, Gn−1 = {g2}. Assume (Kd1 )n (cid:19) {g1}, (Kd1 )n−2 (cid:19) {g2} (but (Kd1 )n (cid:13)(cid:19) {g2} and
(Kd1 )n−1 (cid:13)(cid:19) {g2}). Similarly, (Kd2 )n (cid:19) {g2}, (Kd2 )n−1 (cid:19) {g1}, and (Kd2 )n (cid:13)(cid:19) {g1}. We can take n = 3. Thus, E∗(d1) = 2 since
(Kd1 )2 (cid:19) (G)2, while E∗(d2) = 1 since (Kd1 )1 (cid:19) (G)1. So, E∗(d1) > E∗(d2). Let δ2 = (cid:17)(K)3, d2, g2(cid:18) ∈ APP, so Lev(δ2) = 3 and
neg(Wei(δ2)) = neg(2) = 1. But, regarding d1, (cid:2)δ1 ∈ APP s.t. Conc(δ1) = d1 and Lev(δ1) = 3 and neg(Wei(δ1)) = 2 = neg(1),
i.e. s.t. δ1 >p δ2 (according to Deﬁnition 28). Indeed, the best arguments for d1 are δ1 = (cid:17)(K)3, d1, g1(cid:18) with Lev(δ1) = 3 and
neg(Wei(δ1)) = 0, and δ(cid:9)

= (cid:17)(K)1, d1, g2(cid:18) with Lev(δ(cid:9)

1) = 2 and neg(Wei(δ(cid:9)

1)) = 1.

1

8 According to Deﬁnition 28.

432

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

Going back to our running example, we have,

Example 12 (Surgery example cont.). In the above example, there is an argument of type PP in favor of sg: δ = (cid:17){sg →
¬ll}, sg, ¬ll(cid:18), and there is an argument of type PP in favor of ¬sg: δ(cid:9) = (cid:17){¬sg → ¬se}, ¬sg, ¬se(cid:18).

is (cid:17)n, σ (cid:18). Thus, δ is preferred to δ(cid:9)

(according to Deﬁnition 28).

The strength of δ is (cid:17)n, n(cid:18), whereas the strength of δ(cid:9)
Consequently, the decision sg is preferred to the decision ¬sg.

(cid:9)

The agreement between the pessimistic qualitative decision criterion and the argument-based view is due to a decom-
posability property of arguments of type PP w.r.t. the conjunction of goals. Namely, Kx ∪ {d} (cid:19) g and Kx ∪ {d} (cid:19) g
is
equivalent to Kx ∪ {d} (cid:19) g ∧ g
. Indeed, the pessimistic evaluation sanctions the fact that all the most important goals are
satisﬁed for sure up to a level where this is no longer true. However, things are not as simple with consistency since one
may have Kx ∪ {d} consistent with both g and g
. This means that
the absence of arguments of type PC is only a necessary condition for consistency w.r.t. the whole set of goals. Thus, the
optimistic criterion can only be approximated in terms of the evaluation of elementary practical arguments. Indeed, as it
will be recalled in the next section the optimistic evaluation refers to the fact that the most certain part of K, and the most
important goals are consistent together in presence of a candidate decision.

separately without having it consistent with g ∧ g

(cid:9)

(cid:9)

(cid:9)

5.2. Optimistic criterion

The optimistic qualitative criterion [27] is given by

∗

E

(d) = max

ω

min

(cid:7)
(cid:6)
μ(ω), πd(ω)
.

(3)

It is a consistency evaluation since it amounts to estimate to what extent the intersection of the fuzzy set of good states
∗(d) corresponds
(in the sense of μ) with the fuzzy set of plausible states (when d is applied) is not empty. The criterion E
to an optimistic attitude since it is high as soon as there exists a possible consequence of d that is both highly plausible
∗(d) is equal to n (is maximal) as soon as one fully acceptable choice ω (i.e., such that μ(ω) = n) is
and highly prized. E
also completely plausible. As for the pessimistic case, the optimistic utility can be expressed in logical terms.

Proposition 2. (See [23].) E

∗(d) is equal to the greatest x ∈ U such that (Kd)neg(x) and (G)neg(x) are logically consistent together.

The above proposition means that in the optimistic point of view, we are interested in ﬁnding a decision d (if it exists)
which is consistent with the knowledge base and the goals (i.e. K ∧ {d} ∧ G (cid:13)= ⊥). This is optimistic in the sense that it
assumes that goals may be attained as soon as their negation cannot be proved.

Example 13 (Surgery example cont.). In this example, E
optimistic case depends on the values x and y.

∗(sg) = neg( y) and E

∗(¬sg) = neg(x). Thus the best decision in the

In order to capture the result of Proposition 2, arguments of type PC are needed. The strength of such arguments is

given using Deﬁnition 24.

Theorem 8. Let d ∈ D. If ∃δ ∈ APC s.t. Conc(δ) = d, then E

∗(d) (cid:3) max(neg(Lev(δ)), neg(Wei(δ))).

∗(d) = maxωmin(πKd (ω), μG (ω)) = max[maxω:ω|(cid:22)(Kd)β min(πKd (ω), μG (ω)), maxω:ω(cid:13)|(cid:22)(Kd)β min(πKd (ω), μG (ω))].
Proof. E
Let δ = (cid:17)S, d, g(cid:18) be a PC argument with Lev(δ) = x and Wei(δ) = y. Then, Kx ∪ {d} (cid:19) ¬g. Thus, ∀ω, ω |(cid:22) (Kd)x, then
πKd (ω) (cid:3) neg(x). Thus, E

∗(d) (cid:3) max[min(n, neg( y)), min(neg(x), n)] = max(neg( y), neg(x)). (cid:2)

Conversely, we have the following theorem

Theorem 9. Let d ∈ D. If E

∗(d) = x, then there is a PCargument δ = (cid:17)S, d, g(cid:18) for d such that Wei(δ) (cid:3) neg(x).

∗(d) = x, then x is the maximal value such as (Kd)neg(x) ∪ (G)neg(x) is consistent, from Proposition 2. This entails

Proof. If E
that (Kd)neg(x(cid:9)) (cid:19) ¬Gneg(x(cid:9)) for neg(x) (cid:2) neg(x

(cid:9)).

Example 14 (Surgery example cont.). In the above example, there is one strong argument against the decision ‘sg’: (cid:17){sg →
se}, sg, ¬se(cid:18). There is also a unique strong argument against the decision ¬sg: (cid:17){cp, cp → ca, ca ∧ ¬sg → ll}, ¬sg, ¬ll(cid:18).

The level of the argument (cid:17){sg → se}, sg, ¬se(cid:18) is n whereas its weight is y. Concerning the argument (cid:17){cp, cp → ca, ca ∧

¬sg → ll}, ¬sg, ¬ll(cid:18), its level is x, and its weight is n.

In this example, the comparison of the two arguments amounts to compare x with y. Namely, if y (the priority of the
goal “no side effect”) is small then the best decision will be to have a surgery. If the certainty degree x of having cancer in

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

433

presence of colonic polyps for the particular patient is small enough then the best optimistic decision will not be to have a
surgery.

In order to retrieve the exact value of E

∗(d) as weight of an argument, we would have to use a non-elementary notion

of arguments, described in [6], that considers as a whole the goals base G.

6. Related works

Different works have combined the ideas of argumentation and decision in artiﬁcial intelligence systems. In particular,
Fox and Parsons [31] have developed an inference-based decision support machinery, which have been implemented in
medical applications (see e.g. Fox and Das in [30]). In this approach the knowledge base is made of recommendation rules
that conclude on candidate decisions. However, in [31,32], no explicit distinction is made between knowledge and goals.
However, in their examples, values (belonging to a linearly ordered scale) are assigned to formulas which represent goals.
These values provide an empirical basis for comparing arguments using a symbolic combination of strengths of beliefs and
goals values. This symbolic combination is performed through dictionaries corresponding to different kinds of scales that
may be used. Only one type of arguments in favor of or against is used. Another recent example of argument-based decision
system that is purely based on an inference system is proposed by Chesnevar et al. in [20] for advising about language usage
assessment on the basis of corpus available on the web.

We now survey works that handle classical multiple decision or decision making under uncertainty problems in an
argumentative manner. This means that recommended decisions have to be found or explained from user’s preferences and
information about the current state of the world. Moreover, no “pre-compiled” rules that explicitly recommend decisions in
a given situation are supposed to be available in these works.

In [14], Bonet and Geffner have also proposed an original approach to qualitative decision, inspired from Tan and Pearl
[47], based on “action rules” that link a situation and an action with the satisfaction of a positive or a negative goal. However
in contrast with the previous work and the work presented in this paper, this approach does not refer to any model in
argumentative inference. In their framework, there are four parts:

(1) a set D of actions or decisions.
(2) a set I of input propositions deﬁning the possible input situation. A degree of plausibility is associated with each input.

Thus, I = {(ki, αi)} with αi ∈ {likely, plausible, unlikely}.
. G+

(3) a set G of prioritized goals such that G = G+ ∪ G−

gathers the positive goals that one wants to achieve and G−

gathers the negative goals that one wants to avoid. Thus, G = {(gi, βi)} with βi ∈ [0, 1, . . . , N].
Note that in our framework what they call here negative goals are considered in our goal base as negative literals.
(4) a set of action rules AR = {( Ai ∧ Ci ⇒ xi, λi), λi (cid:2) 0}, where Ai is an action, Ci is a conjunction of input literals, and xi
is a goal. Each action rule has two measures: a priority degree which is exactly the priority degree of the goal xi , and a
plausibility degree. This plausibility is deﬁned as follows: A rule A ∧ C ⇒ x is likely if any conjunct of C is likely. A rule
A ∧ C ⇒ x is unlikely if some conjunct of C is unlikely. A rule A ∧ C ⇒ x is plausible if it is neither likely nor unlikely.

In this approach only input propositions are weighted in terms of plausibility. Action rules inherit these weights through the
three above rules in a rather empirical manner which depends on the chosen plausibility scale. The action rules themselves
are not weighted since they are potentially understood as defeasible rules, although no non-monotonic reasoning system is
associated with them.

In contrast, our approach makes use of an abstract scale. Moreover, weighted possibilistic clauses have been shown to
be able to properly handle non-monotonic inference in the sense of Kraus, Lehmann and Magidor [36]’ preferential system
augmented with rational monotony. So a part of our weighted knowledge may be viewed as the encoding of a set of default
rules. From the above four bases, reasons are constructed for (against) actions in [14]. Indeed, goals provide reasons for
(or against) actions. Positive goals provide reasons for actions, whereas negative goals provide reasons against actions. The
basic idea behind this distinction is that negative goals should be discarded, and consequently any action which may lead
to the satisfaction of such goals should be avoided. However, the approach makes no distinction between what we call
pessimism and optimism. The deﬁnition of a ‘reason’ in [14] only involves facts. Finally, in Bonet and Geffner’s framework,
decisions which satisfy the most important goals are privileged. This is also true in our approach, but the comparison
between decisions can be further reﬁned, in case of several decisions yielding to the satisfaction of the most important
goals, by taking into account the other goals which are not violated by these decisions.

Amgoud and Prade in [6] have already proposed an argumentation-based reading of possibilistic decision criteria. How-
ever, their approach has some drawbacks from a pure argumentation point of view. In their approach, there was only one
type of arguments pros and one type of arguments cons. Moreover, these arguments were taking into account the goal base
as a whole, and a consequence for a given decision there was at most a unique argument pros and a unique argument
cons. This does not really ﬁt with the way human are discussing decisions, for which there are usually several arguments
pros and cons, rather than a summarized one. On the contrary in this paper, we have discussed all the possible types of
arguments pros and cons in a systematic way, and each argument pertains to only one goal.

434

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

Dubois and Fargier [24] have studied a framework where a candidate decision d is associated with two distinct sets
of positive arguments (pros) and negative arguments (cons). It is assumed that positiveness and negativeness are not a
matter of degree. If one considers that the arguments refers to criteria, this means that an implicit scale {−, 0, +} would
be used for evaluating a candidate decision according to each criterion. Moreover, there is no uncertainty. However, a
function π assesses the level of importance of each argument for the decision maker. Importance ranges on a totally
ordered scale from “no important at all” to a maximal level of importance. If π (x) > π ( y), “the strength of x is considered
at least one order of magnitude higher than the one of y, so that y is negligible in front of x”. The authors provide an
axiomatic characterization of different natural rules in this setting, with a possibility theory interpretation of their meaning.
In particular, a “bipolar lexicographic” preference relation (which is among the decision principles discussed in Section 6) is
characterized. It amounts to compare two candidate decisions d and d
by comparing the difference of the cardinalities of the
sets of positive and negative arguments they have (thus allowing for cancellation between positive and negative arguments),
starting with the highest level of importance; in case of equality at a given level, the level immediately below is considered
to solve the ties and so on. In direct relation to this work, an empirical study of the different decision rules considered
(Bonnefon and Fargier [15]) has shown that the bipolar lexicographic rule is largely favored by humans in practice.

(cid:9)

Another trend of works relating argumentation and decision is mainly interested in the use of arguments for explaining
and justifying multiple criteria decisions once they have been made using some deﬁnite aggregation function. A systematic
study for different aggregation functions can be found in [38,39]. The implemented system developed by Carenini and Moore
[18] is an example of such a use for an aggregation process based on weighted sums associated to value trees.

7. Conclusion

The paper has proposed an abstract argumentation-based framework for decision making. The main idea behind this
work is how to deﬁne a complete preorder on a set of candidate decisions on the basis of arguments. The framework
distinguishes between two types of arguments: epistemic arguments that support beliefs and practical arguments that
justify candidate decisions. Each practical argument concerns only one candidate decision, and may be either in favor of
that decision or against it. The framework follows two main steps:

(1) An inference step in which arguments are evaluated using acceptability semantics. This step amounts to return among

the practical arguments, those which are warranted in the current state of information, i.e. the “accepted” arguments.

(2) A pure decision step in which candidate decisions are compared on the basis of accepted practical arguments.

For the second step of the process, we have proposed three families of principles for comparing pairs of choices. An
axiomatic study and a cognitive validation of these principles are worth developing, in particular in connection with [15,24].
The abstract framework is then instantiated in order to handle decision under uncertainty and multiple criteria decision
making. For that purposes, the framework emphasizes clearly the bipolar nature of the consequences of choices by distin-
guishing goals to be pursued from rejections to be avoided. These bipolar preferences are encoded by two sets of stratiﬁed
formulas stating goals and rejections with their level of importance. In addition, the knowledge about the current state of
the world in encoded in another stratiﬁed base which may be inconsistent. The bipolar nature of the setting has led us to
identify two types of arguments pro a choice (resp. against a choice).

The proposed approach is very general and includes as particular cases already studied argumentation-based decision
systems. Moreover it is suitable for multiple criteria decision making as well as decision making under uncertainty. In
particular, the approach has been shown to agree with qualitative decision making under uncertainty, and to distinguish
between a pessimistic and an optimistic attitude of the decision maker.

Although our model is quite general, it may be still worth extending along different lines. First, the use of default knowl-
edge could be developed. Second, our approach does not take into account rules that recommend or disqualify decisions
in given contexts. Such rules should incorporate modalities for distinguishing between strong and weak recommendations.
Moreover, they are ﬁred by classical argumentative inference. This contrasts with our approach where the only arguments
pertaining to decisions have an abductive structure. Recommendation rules may also turn to be inconsistent with other
pieces of knowledge in practical arguments pros or cons w.r.t. a decision. Lastly, agents may base their decision on two types
of information, namely generic knowledge and a repertory of concrete reported cases. Then, past observations recorded in
the repertory may be the basis of a new form of arguments by exempliﬁcation of cases where a decision has succeeded or
failed. This would amount to relate argumentation and case-based decision.

Acknowledgement

The authors would like to thank the two anonymous referees for their very constructive reviews that helped us to

improve this work.

Appendix. Brief refresher on possibility logic

A possibilistic logic base K can be viewed as a stratiﬁed set of classical logical formulas, such that K = K1 ∪ · · · ∪ Ki ∪
· · · ∪ Kn, with ∀i, j, Ki ∩ K j = ∅. It is assumed that formulas in Ki are associated with a higher level of certainty or priority

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

435

than formulas in Ki−1. Thus, Kn contains the formulas with the highest level, and K1 the ones with the smallest non-zero
level.

Let ρ be a function that returns the rank of stratum to which a formula belongs, i.e. ρ(k j) = i such that k j ∈ Ki .
In the following, ρ(k j) will be denoted for short ρ j . Thus, K can be rewritten as K = {(k j, ρ j); j = 1, l}, as often done in
possibilistic logic [25]. K may represent the available knowledge about the world, or goals having different levels of priority.
The pair (k j, ρ j) is understood as N(k j) (cid:2) ρ j , where N is a necessity measure obeying the characteristic decomposability
axiom N(p ∧ q) = min(N(p), N(q)). Namely (k j, ρ j) encodes that the piece of knowledge “k j is true” is certain or prioritized
at least at level ρ j , where ρ j belongs to a linearly ordered valuation scale whose top and bottom elements are resp. n
and 1. At the semantic level, a possibilistic base K is associated with a possibility distribution deﬁned by

(cid:6)
πK(ω) = min j=1,lmax

(cid:7)
vω(k j), neg(ρ j)
,

where neg is the order reversing map of the scale (0, 1, . . . , n), and where vω(k j) = n if ω is a model of k j and vω(k j) = 0
if ω falsiﬁes k j . An interpretation ω is thus all the less plausible or satisfactory, as it falsiﬁed a proposition k j associated
with a high level ρ j . It rank-orders the more or less plausible states of the world.

The equivalence between a possibilistic logic base and its possibility distribution-based semantic counter-part has been
established in terms of correction and completeness of the inference mechanism that is associated with these represen-
tations [25]. This inference mechanism is governed at the syntactic level by the resolution rule (¬p ∨ q, ρ), (p ∨ r, λ) (cid:19)
(q ∨ r, min(ρ, λ)).

References

[1] T. Alsinet, C. Chesnevar, L. Godo, S. Sandri, Modeling defeasible argumentation within a possibilistic logic framework with fuzzy uniﬁcation, in: Pro-
ceedings of 11th International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU’06), 2006,
pp. 1228–1235.

[2] L. Amgoud, J.-F. Bonnefon, H. Prade, An argumentation-based approach to multiple criteria decision, in: Proceedings of the 8th European Conference

on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU’05), 2005, pp. 269–280.

[3] L. Amgoud, C. Cayrol, Inferring from inconsistency in preference-based argumentation frameworks, International Journal of Automated Reasoning 29 (2)

(2002) 125–169.

[4] L. Amgoud, C. Cayrol, A reasoning model based on the production of acceptable arguments, Annals of Mathematics and Artiﬁcial Intelligence 34 (2002)

197–216.

[5] L. Amgoud, N. Maudet, S. Parsons, Modelling dialogues using argumentation, in: Proceedings of the International Conference on Multi-Agent Systems

(ICMAS’00), 2000, pp. 31–38.

[6] L. Amgoud, H. Prade, Using arguments for making decisions: A possibilistic logic approach, in: Proceedings of the 20th Conference on Uncertainty in

Artiﬁcial Intelligence (UAI’04), 2004, pp. 10–17.

[7] L. Amgoud, H. Prade, A bipolar argumentation-based decision framework, in: Proceedings of the 11th International Conference on Information Process-

ing and Management of Uncertainty in Knowledge-Based Systems (IPMU’06), 2006, pp. 323–330.

[8] L. Amgoud, H. Prade, Comparing decisions in an argumentation-based setting, in: Proceedings of the 11th International Workshop on Non-Monotonic

Reasoning (NMR’06), 2006.

[9] L. Amgoud, H. Prade, Explaining qualitative decision under uncertainty by argumentation, in: Proceedings of the 21st National Conference on Artiﬁcial

Intelligence (AAAI’06), 2006, pp. 219–224.

[10] K. Atkinson, Value-based argumentation for democratic decision support, in: Proceedings of the First International Conference on Computational Mod-

els of Natural Argument (COMMA’06), 2006, pp. 47–58.

[11] K. Atkinson, T. Bench-Capon, P. McBurney, Justifying practical reasoning, in: Proceedings of the Fourth Workshop on Computational Models of Natural

Argument (CMNA’04), 2004, pp. 87–90.

[12] P. Baroni, M. Giacomin, G. Guida, Scc-recursiveness: a general schema for argumentation semantics, Artiﬁcial Intelligence 168 (1–2) (2005) 162–210.
[13] Ph. Besnard, A. Hunter, A logic-based theory of deductive arguments, Artiﬁcial Intelligence 128 (2001) 203–235.
[14] B. Bonet, H. Geffner, Arguing for decisions: A qualitative model of decision making, in: Proceedings of the 12th Conference on Uncertainty in Artiﬁcial

Intelligence (UAI’96), 1996, pp. 98–105.

[15] J.-F. Bonnefon, H. Fargier, Comparing sets of positive and negative arguments: Empirical assessment of seven qualitative rules, in: Proceedings of the

17th European Conference on Artiﬁcial Intelligence (ECAI’06), 2006, pp. 16–20.

[16] M. Bratman, Intentions, Plans, and Practical Reason, Harvard University Press, MA, 1987.
[17] J.T. Cacioppo, W.L. Gardner, G.G. Bernston, Beyond bipolar conceptualizations and measures: The case of attitudes and evaluative space, Personality and

Social Psychology Review 1 (1997) 3–25.

[18] G. Carenini, J.D. Moore, Generating and evaluating evaluative arguments, Artiﬁcial Intelligence 170 (2006) 925–952.
[19] C. Cayrol, V. Royer, C. Saurel, Management of preferences in assumption-based reasoning, in: Lecture Notes in Computer Science, vol. 682, 1993, pp.

13–22.

[20] C. Chesnevar, A. Maguitman, M. Sabaté, An argument-based decision support system for assessing natural language usage on the basis of the web

corpus, International Journal of Intelligent Systems (2006), ISSN 0884-8173.

[21] T. Dean, L. Kaelbling, J. Kirman, A. Nicholson, Planning under time constraints in stochastic domains, Artiﬁcial Intelligence 76 (1–2) (1995) 35–74.
[22] J. Doyle, R. Thomason, Background to qualitative decision theory, Artiﬁcial Intelligence Magazine 20 (2) (1999) 55–68.
[23] D. Dubois, D. Le Berre, H. Prade, R. Sabbadin, Using possibilistic logic for modeling qualitative decision: ATMS-based algorithms, Fundamenta Informat-

icae 37 (1999) 1–30.

[24] D. Dubois, H. Fargier, Qualitative decision making with bipolar information, in: Proceedings of the 10th International Conference on Principles of

Knowledge Representation and Reasoning (KR’06), 2006, pp. 175–186.

[25] D. Dubois, J. Lang, H. Prade, Possibilistic logic, in: Handbook of Logic in Artiﬁcial Intelligence and Logic Programming, vol. 3, 1994, pp. 439–513.
[26] D. Dubois, H. Prade, Weighted minimum and maximum operations, an addendum to a review of fuzzy set aggregation connectives, Information

Sciences 39 (1986) 205–210.

[27] D. Dubois, H. Prade, Possibility theory as a basis for qualitative decision theory, in: Proceedings of 14th International Joint Conference on Artiﬁcial

Intelligence (IJCAI’95), 1995, pp. 1924–1930.

436

L. Amgoud, H. Prade / Artiﬁcial Intelligence 173 (2009) 413–436

[28] P.M. Dung, On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games, Artiﬁcial

Intelligence 77 (1995) 321–357.

[29] M. Elvang-Goransson, J. Fox, P. Krause, Dialectic reasoning with inconsistent information, in: Proceedings of 9th Conference on Uncertainty in Artiﬁcial

Intelligence (UAI’93), 1993, pp. 114–121.

[30] J. Fox, S. Das, Safe and Sound. Artiﬁcial Intelligence in Hazardous Applications, AAAI Press, The MIT Press, 2000.
[31] J. Fox, S. Parsons, On using arguments for reasoning about actions and values, in: Proceedings of the AAAI Spring Symposium on Qualitative Preferences

in Deliberation and Practical Reasoning, Stanford, 1997.

[32] J. Fox, S. Parsons, Arguing about beliefs and actions, in: Applications of Uncertainty Formalisms, 1998, pp. 266–302.
[33] B. Franklin, Letter to J.B. Priestley, 1772, in: J. Bigelow (Ed.), The Complete Works, Putnam, New York, 1887, p. 522.
[34] P.H. Giang, P. Shenoy, Two axiomatic approaches to decision making using possibility theory, European Journal of Operational Research 162 (2) (2004)

450–467.

[35] R. Girle, D. Hitchcock, P. McBurney, B. Verheij, Decision support for practical reasoning, in: C. Reed, T. Norman (Eds.), Argumentation Machines: New

Frontiers in Argument and Computation, in: Argumentation Library, Kluwer Academic, Dordrecht, The Netherlands, 2003.

[36] S. Kraus, D. Lehmann, M. Magidor, Nonmonotonic reasoning, preferential models and cumulative logics, Artiﬁcial Intelligence 44 (1990) 167–207.
[37] S. Kraus, K. Sycara, A. Evenchik, Reaching agreements through argumentation: a logical model and implementation, Artiﬁcial Intelligence 104 (1998)

1–69.

[38] Ch. Labreuche, Argumentation of the results of a multi-criteria evaluation model in individual and group decision aiding, in: Proceedings of the 4th

Conference of the European Society for Fuzzy Logic and Technology (Eusﬂat’05), 2005, pp. 482–487.

[39] Ch. Labreuche, Argumentation of the decision made by several aggregation operators based on weights, in: Proceedings of the 11th International

Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU’06), 2006, pp. 683–690.

[40] J. Pearl, Probabilistic Reasoning in Intelligent Systems, Morgan-Kaufmann, 1988.
[41] J. Pollock, The logical foundations of goal-regression planning in autonomous agents, Artiﬁcial Intelligence 106 (2) (1998) 267–334.
[42] J. Pollock, Rational cognition in OSCAR, in: Proceedings of the International Workshop on Agent Theories and Languages (ATAL’99), 1999.
[43] J. Raz, Practical Reasoning, Oxford University Press, Oxford, 1978.
[44] C. Reed, T. Norman (Eds.), Argumentation Machines: New Frontiers in Argument and Computation, Argumentation Library, Kluwer Academic, Dordrecht,

The Netherlands, 2003.

[45] L.J. Savage, The Foundations of Statistics, Dover, New York, 1954. Reprinted by Dover, 1972.
[46] G.R. Simari, R.P. Loui, A mathematical treatment of defeasible reasoning and its implementation, Artiﬁcial Intelligence and Law 53 (1992) 125–157.
[47] S.W. Tan, J. Pearl, Qualitative decision theory, in: Proceedings of the 11th National Conference on Artiﬁcial Intelligence (AAAI’94), 1994, pp. 928–933.
[48] D. Walton, Argument Schemes for Presumptive Reasoning, Lawrence Erlbaum Associates, Mahwah, NJ, 1996.
[49] M.J. Wooldridge, Reasoning about Rational Agents, MIT Press, Cambridge, MA, 2000.

