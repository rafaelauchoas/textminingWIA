Artificial Intelligence 175 (2011) 694–729Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintSampleSearch: Importance sampling in presence of determinismVibhav Gogate a,∗,1, Rina Dechter ba Computer Science & Engineering, University of Washington, Seattle, WA 98195, USAb Donald Bren School of Information and Computer Sciences, University of California, Irvine, Irvine, CA 92697, USAa r t i c l ei n f oa b s t r a c tThe paper focuses on developing effective importance sampling algorithms for mixedprobabilistic and deterministic graphical models. The use ofimportance sampling insuch graphical models is problematic because it generates many useless zero weightsamples which are rejected yielding an inefficient sampling process. To address thisrejection problem, we propose the SampleSearch scheme that augments sampling withsystematic constraint-based backtracking search. We characterize the bias introduced bythe combination of search with sampling, and derive a weighting scheme which yields anunbiased estimate of the desired statistics (e.g., probability of evidence). When computingthe weights exactly is too complex, we propose an approximation which has a weakerguarantee of asymptotic unbiasedness. We present results of an extensive empiricalevaluation demonstrating that SampleSearch outperforms other schemes in presence ofsignificant amount of determinism.© 2010 Elsevier B.V. All rights reserved.Article history:Received 31 July 2009Received in revised form 1 October 2010Accepted 21 October 2010Available online 5 November 2010Keywords:Probabilistic inferenceApproximate inferenceImportance samplingMarkov chain Monte CarloBayesian networksMarkov networksSatisfiabilityModel countingConstraint satisfaction1. IntroductionThe paper investigates importance sampling algorithms for answering weighted counting and marginal queries over mixedprobabilistic and deterministic networks [1–4]. The mixed networks framework treats probabilistic graphical models such asBayesian and Markov networks [5], and deterministic graphical models such as constraint networks [6] as a single graphicalmodel. Weighted counts express the probability of evidence of a Bayesian network, the partition function of a Markovnetwork and the number of solutions of a constraint network. Marginals seek the marginal distribution of each variable,also called belief updating or posterior estimation in a Bayesian or Markov network.It is straightforward to design importance sampling algorithms [7–9] for approximately answering counting and marginalqueries because both are variants of summation problems for which importance sampling was designed. Weighted counts isthe sum of a function over some domain while a marginal is a ratio between two sums. The main idea is to transform asummation into an expectation using a special distribution called the proposal (or importance) distribution from which itwould be easy to sample. Importance sampling then generates samples from the proposal distribution and approximates theexpectation (also called the true average or the true mean) by a weighted average over the samples (also called the sampleaverage or the sample mean). The sample mean can be shown to be an unbiased estimate of the original summation, andtherefore importance sampling yields an unbiased estimate of the weighted counts. For marginals, importance sampling hasto compute a ratio of two unbiased estimates yielding an asymptotically unbiased estimate only.* Corresponding author.E-mail addresses: vgogate@cs.washington.edu (V. Gogate), dechter@ics.uci.edu (R. Dechter).1 This work was done when the author was a graduate student at University of California, Irvine.0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.10.009V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729695In presence of hard constraints or zero probabilities, however, importance sampling may suffer from the rejection prob-lem. The rejection problem occurs when the proposal distribution does not faithfully capture the constraints in the mixednetwork. Consequently, many samples generated from the proposal distribution may have zero weight and would not con-tribute to the sample mean. In extreme cases, the probability of generating a rejected sample can be arbitrarily close to oneyielding completely wrong estimates.In this paper, we propose a sampling scheme called SampleSearch to remedy the rejection problem. SampleSearch com-bines systematic backtracking search with Monte Carlo sampling. In this scheme, when a sample is supposed to be rejected,the algorithm continues instead with randomized backtracking search until a sample with non-zero weight is found. Thisproblem of generating a non-zero weight sample is equivalent to the problem of finding a solution to a satisfiability (SAT)or a constraint satisfaction problem (CSP). SAT and CSPs are NP-complete problems and therefore the idea of generatingjust one sample by solving an NP-complete problem may seem inefficient. However, recently SAT/CSP solvers have achievedunprecedented success and are able to solve some large industrial problems having as many as a million variables within afew seconds.2 Therefore, solving a constant number of NP-complete problems to approximate a #P-complete problem suchas weighted counting is no longer unreasonable.We show that SampleSearch generates samples from a modification of the proposal distribution which is backtrack-free.The backtrack-free distribution can be obtained by removing all partial assignments which lead to a zero weight sample. Inparticular, the backtrack-free distribution is zero whenever the target distribution from which we wish to sample is zero.We propose two schemes to compute the backtrack-free probability of the generated samples which is required for com-puting the sample weights. The first is a computationally intensive method which involves invoking a CSP or a SAT solverO (n × d) times where n is the number of variables and d is the maximum domain size. The second scheme approximatesthe backtrack-free probability by consulting information gathered during SampleSearch’s operation. This latter scheme hasseveral desirable properties: (i) it runs in linear time, (ii) it yields an asymptotically unbiased estimate, and (iii) it canprovide upper and lower bounds on the exact backtrack-free probability.Finally, we present empirical evaluation demonstrating the power of SampleSearch. We implemented SampleSearch ontop of IJGP-wc-IS [10], a powerful importance sampling technique which uses a generalized belief propagation algorithm [11]called Iterative Join Graph Propagation (IJGP) [12,13] to construct a proposal distribution and w-cutset (Rao-Blackwellised)sampling [14] to reduce the variance. The search was implemented using the minisat SAT solver [15]. We conducted ex-periments on three tasks: (a) counting models of a SAT formula, (b) computing the probability of evidence in a Bayesiannetwork and the partition function of a Markov network, and (c) computing posterior marginals in Bayesian and Markovnetworks.For model counting, we compared against three approximate algorithms: ApproxCount [16], SampleCount [17] and Rel-sat [18] as well as with IJGP-wc-IS, our vanilla importance sampling scheme on three classes of benchmark instances. Ourexperiments show that on most instances, given the same time-bound SampleSearch yields solution counts which are closerto the true counts by a few orders of magnitude compared with the other schemes. It is clearly better than IJGP-wc-ISwhich failed on all benchmark SAT instances and was unable to generate a single non-zero weight sample in ten hours ofCPU time.For the problem of computing the probability of evidence in a Bayesian network, we compared SampleSearch withVariable Elimination and Conditioning (VEC) [19], an advanced generalized belief propagation scheme called Edge DeletionBelief Propagation (EDBP) [20] as well as with IJGP-wc-IS on linkage analysis [21] and relational [22] benchmarks. Ourexperiments show that on most instances the estimates output by SampleSearch are more accurate than those output byEDBP and IJGP-wc-IS. VEC solved some instances exactly, however on the remaining instances it was substantially inferior.For the posterior marginal task, we experimented with linkage analysis benchmarks, with partially deterministic gridbenchmarks, with relational benchmarks and with logistics planning benchmarks. Here, we compared the accuracy ofSampleSearch against three other schemes: the two generalized belief propagation schemes of Iterative Join Graph Prop-agation [12,13] and Edge Deletion Belief Propagation [20] and an adaptive importance sampling scheme called EvidencePre-propagated Importance Sampling (EPIS) [23]. Again, we found that except for the grid instances, SampleSearch consis-tently yields estimates having smaller error than the other schemes.Based on this large scale experimental evaluation, we conclude that SampleSearch consistently yields very good approxi-mations. In particular, on large instances which have a substantial amount of determinism, SampleSearch yields an order ofmagnitude improvement over state-of-the-art schemes.The paper is based on earlier conference papers [24,25]. The present article contains more detailed and general analysis,full proofs, new bounding approximations (described in Section 4.2.1), as well as new experimental results.The rest of the paper is organized as follows. In Section 2, we present notation and preliminaries on graphical models andimportance sampling. In Section 3, we present the rejection problem and show how to overcome it using the backtrack-freedistribution. Section 4 describes the SampleSearch scheme and various improvements. In Section 5, we present experimentalresults and we conclude in Section 6.2 See results of SAT competitions available at http://www.satcompetition.org/.696V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–7292. Preliminaries and backgroundWe denote variables by upper case letters (e.g. X, Y , . . .) and values of variables by lower case letters (e.g. x, y, . . .). Setsof variables are denoted by bold upper case letters (e.g. X = { X1, . . . , Xn}) while sets of values are denoted by bold lowercase letters (e.g. x = {x1, . . . , xn}). X = x denotes an assignment of value to a variable while X = x denotes an assignmentof values to all variables in the set. We denote by Di the set of possible values of Xi (also called as the domain of Xi ). Wedenote the projection of an assignment x to a set S ⊆ X by xS.(cid:2)x∈X denotes the sum over the possible values of variables in X, namely,value EQ [ X] of a random variable X with respect to a distribution Q is defined as: EQ [ X] =x∈ X (x − EQ [ X])2.V Q [ X] of X is defined as: V Q [ X] =(cid:2)(cid:2)x1∈ X1(cid:2)×x2∈ X2× · · · ×(cid:2)(cid:2)xn∈ Xn. The expectedx∈ X xQ (x). The varianceWe denote functions by upper case letters (e.g. F , C , etc.), and the scope (set of arguments) of a function F by scope(F ).Frequently, given an assignment y to a superset Y of scope(F ), we will abuse notation and write F (yscope(F )) as F (y).2.1. Markov, Bayesian and constraint networksDefinition 1 (Graphical models and Markov networks). A discrete graphical model, denoted by G (or a Markov network, de-noted by T ) is a 3-tuple (cid:5)X, D, F(cid:6) where X = { X1, . . . , Xn} is a finite set of variables, D = {D1, . . . , Dn} is a finite set ofdomains where Di is the domain of variable Xi and F = {F 1, . . . , Fm} is a finite set of discrete-valued functions (or poten-tials). Each function F i is defined over a subset Si ⊆ X of variables. A graphical model G represents a joint distribution P Gover the variables X, given by:P G(x) = 1Zm(cid:3)i=1F i(x) where Z =(cid:4)m(cid:3)x∈Xi=1F i(x)where Z is the normalization constant and is often referred to as the partition function.The primary queries over Markov networks are computing the posterior distribution (marginals) over all variables Xi ∈ Xand finding the partition function. Each graphical model is associated with a primal graph which captures the dependenciespresent in the model.Definition 2 (Primal graph). The primal graph of a graphical model G = (cid:5)X, D, F(cid:6) is an undirected graph G(X, E) which hasvariables of G as its vertices and has an edge between any two variables that appear in a scope of a function F ∈ F.Definition 3 (Bayesian or belief networks). A Bayesian network is a graphical model B = (cid:5)X, D, G, P(cid:6) where G = (X, E) is adirected acyclic graph over the set of variables X. The functions P = {P 1, . . . , Pn} are conditional probability tables P i == scope(P i) \ { Xi} is the set of parents of Xi in G. The primal graph of a Bayesian network is alsoP ( Xi|pai), where paicalled the moral graph. When the entries of the CPTs are 0 and 1 only, they are called deterministic or functional CPTs. Anevidence E = e is an instantiated subset of variables.A Bayesian network represents the joint probability distribution given by P B(X) =i=1 P ( Xi|pai) and therefore canbe used to answer any query defined over the joint distribution. In this paper, we consider two queries: (a) computingthe probability of evidence P (E = e) and (b) computing the posterior marginal distribution P ( Xi|E = e) for each variableXi ∈ X \ E.(cid:5)nDefinition 4 (Constraint networks). A constraint network is a graphical model R = (cid:5)X, D, C(cid:6) where C = {C1, . . . , Cm} is a set ofconstraints. Each constraint Ci is a 0/1 function defined over a subset of variables Si , called its scope. Given an assignmentSi = si , a constraint is satisfied if Ci(si) = 1. A constraint can also be expressed by a pair (cid:5)R i, Si(cid:6) where R i is a relationdefined over the variables Si and contains all tuples Si = si for which Ci(si) = 1. The primal graph of a constraint networkis called the constraint graph.The primary query over a constraint network is to decide whether it has a solution, i.e., to find an assignment X = x toall variables such that all constraints are satisfied or to prove that no such assignment exists. Another important query isthat of counting the number of solutions of the constraint network. A constraint network represents a uniform distributionover its solutions.Propositional satisfiabilityA special case of a constraint network is the propositional satisfiability problem (SAT). A propositional formula F is anexpression defined over variables having binary domains: {False, True} or {0, 1}. Every Boolean formula can be convertedinto an equivalent formula in conjunctive normal form (CNF). A CNF formula F is a conjunction (denoted by ∧) of clausesCl1 ∧ · · · ∧ Clt (denoted as a set {Cl1, . . . , Clt }) where a clause is a disjunction (denoted by ∨) of literals (literals are variablesV. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729697or their negations). For example, Cl = (P ∨ ¬Q ∨ ¬R) is a clause over three variables P , Q and R, and P , ¬Q and ¬R areliterals. A clause is satisfied if one of its literals is assigned the value True or 1. A solution or a model of a formula F is anassignment of values to all variables such that all clauses are satisfied. Common queries in SAT are satisfiability, i.e., findinga model or proving that none exists, and model counting, i.e., counting the number of models or solutions.2.2. Mixed networksThroughout the paper, we will use the framework of mixed networks defined in [3,4]. Mixed networks represent allthe deterministic information explicitly in the form of constraints facilitating the use of constraint processing techniquesdeveloped over the past three decades for efficient probabilistic inference. This framework includes Bayesian, Markov andconstraint networks as a special case. Therefore, many inference tasks become equivalent when we consider a mixed net-work view, allowing a unifying treatment of all these problems within a single framework. For example, problems suchas computing the probability of evidence in a Bayesian network, the partition function in a Markov network and countingsolutions of a constraint network can be expressed as weighted counting over mixed networks.Definition 5 (Mixed network). A mixed network is a four-tuple M = (cid:5)X, D, F, C(cid:6) where X = { X1, . . . , Xn} is a set of randomvariables, D = {D1, . . . , Dn} is a set of domains where Di is the domain of Xi , F = {F 1, . . . , Fm} is a set of non-negative realis defined over a subset of variables Si ⊆ X (its scope) and C = {C1, . . . , C p} is a set ofvalued functions where each F iconstraints (or 0/1 functions). A mixed network represents a joint distribution over X given by:PM(x) =(cid:6)(cid:5)1Z0mi=1 F i(x)if x ∈ sol(C)otherwisewhere sol(C) is the set of solutions of C and Z =mi=1 F i(x) is the normalizing constant. The primal graph of amixed network has variables as its vertices and an edge between any two variables that appear in the scope of a functionF ∈ F or a constraint C ∈ C.x∈sol(C)(cid:2)(cid:5)We can define several queries over the mixed network. In this paper, however we will focus on the following twoqueries:Definition 6 (The weighted counting task). Given a mixed network M = (cid:5)X, D, F, C(cid:6), the weighted counting task is to computethe normalization constant given by:Z =(cid:4)m(cid:3)x∈Sol(C)i=1F i(x)Equivalently, if we represent the constraints in C as 0/1 functions, we can rewrite Z as:Z =(cid:4)m(cid:3)p(cid:3)F i(x)C j(x)x∈Xi=1j=1We will refer to Z as weighted counts.(1)(2)Definition 7 (Marginal task). Given a mixed network M = (cid:5)X, D, F, C(cid:6), the marginal task is to compute the marginal distri-bution of each variable. Namely, for each variable Xi and xi ∈ Di , compute:P (xi) =δxi (x)PM(x), where δxi (x) =1 if Xi is assigned the value xi in x0 otherwise(cid:7)(cid:4)x∈XTo be able to use the constraint portion of the mixed network more effectively, for the remainder of the paper, werequire that all zero probabilities in the mixed network are also represented as constraints. It is easy to define such a network aswe show below.Definition 8 (Modified mixed network). Given a mixed network M = (cid:5)X, D, F, C(cid:6), a modified mixed network is a four-tupleM(cid:9) = (cid:5)X, D, F, Ci=1 where(cid:9)(cid:6) where C(cid:7)(cid:9) = C ∪ {H i}m0 if F i(si) = 01 otherwiseH i(Si = si) =(3)H i can also be expressed as a relation. The set of constraints Cdistribution PM.(cid:9)is called the flat constraint network of the probability698V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Clearly, the modified mixed network Mand the original mixed network M are equivalent in that PM(cid:9) = PM. It is easyto see that the weighted counts over a mixed network specialize to (a) the probability of evidence in a Bayesian network,(b) the partition function in a Markov network, and (c) the number of solutions of a constraint network. The marginal taskexpresses the task of computing posterior marginals in a Bayesian or Markov network.(cid:9)2.3. Importance sampling for approximating the weighted counts and marginalsImportance sampling [7,9] is a general Monte Carlo simulation technique which can be used for estimating variousstatistics of a given target distribution. Since it is often hard to sample from the target distribution, the main idea is togenerate samples from another easy-to-simulate distribution Q called the proposal (or trial or importance) distribution andthen estimate various statistics over the target distribution by a weighted sum over the samples. The weight of a sampleis the ratio between the probability of generating the sample from the target distribution and its probability based on theproposal distribution. In this subsection, we describe how the weighted counts and posterior marginals can be approximatedvia importance sampling. For more details on the theoretical results presented in this subsection, we refer the reader to[8,26].We assume throughout the paper that the proposal distribution is specified in the product form along a variable orderingo = ( X1, . . . , Xn) as:Q (X) =n(cid:3)i=1Q i(Xi| X1, . . . , Xi−1)Q is therefore specified as a Bayesian network with CPTs Q = {Q 1, . . . , Q n} along the ordering o. We can generate a fullsample from this product form specification as follows. For i = 1 to n, sample Xi = xi from the conditional distributionQ ( Xi| X1 = x1, . . . , Xi−1 = xi−1) and set Xi = xi . This is often referred to as an ordered Monte Carlo sampler or logic sam-pling [5].Thus, when we say that Q is easy to sample from, we assume that Q can be expressed in a product form and can bespecified in polynomial space, namely,n(cid:3)Q (X) =Q i(Xi| X1, . . . , Xi−1) =n(cid:3)Q i(Xi|Yi)(4)i=1where Yi ⊆ { X1, . . . , Xi−1}. The size of the set Yi is assumed to be bounded by a constant.i=1Throughout the paper, we will often use the notion of biased and unbiased estimators, which we define below.Definition 9 (Unbiased and asymptotically unbiased estimator). Given a probability distribution Q , a statistics θ of Q , and Nsamples drawn from Q , a function (cid:8)θN , defined over the samples is an unbiased estimator of θ if EQ [ (cid:8)θN ] = θ . Similarly,a function (cid:9)θN is an asymptotically unbiased estimator of θ if limN→∞ EQ [ (cid:9)θN ] = θ . Clearly, all unbiased estimators areasymptotically unbiased.Note that we denote an unbiased estimator of a statistics θ by (cid:8)θ , an asymptotically unbiased estimator by (cid:9)θ and anarbitrary estimator by θ .The notion of unbiasedness and asymptotic unbiasedness is important because it helps to characterize the performanceof an estimator which we explain briefly below. The mean-squared error of an estimator θ is given by:MSE(θ) = EQ= EQ(cid:10)EQ=(cid:11)(cid:10)(θ − θ)2(cid:11)(cid:10)θ 2(cid:10)θ 2− 2EQ [θ ]θ + θ 2(cid:10)(cid:11)− EQ [θ ]2+(cid:11)EQ [θ]2 − 2EQ [θ ]θ + θ 2(cid:11)The bias of θ is given by:B Q [θ ] = EQ [θ ] − θThe variance of θ is given by:V Q [θ] = EQ− EQ [θ ]2(cid:11)(cid:10)θ 2From the definitions of bias, variance and mean-squared error, we get:(cid:11)2B Q [θ ]MSE(θ) = V Q [θ ] +(cid:10)(5)(6)(7)(8)In other words, the mean squared error of an estimator is equal to bias squared plus variance. For an unbiased estimator,the bias is zero and therefore one can reduce its mean squared error by reducing its variance. In case of an asymptoticallyunbiased estimator, the bias goes to zero as the number of samples tend to infinity. However, for a finite sample size it mayhave a non-zero bias.699(9)(10)(11)(12)(13)V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–7292.3.1. Estimating weighted countsConsider the expression for weighted counts (see Definition 6).Z =(cid:4)m(cid:3)p(cid:3)F i(x)C j(x)x∈Xi=1j=1If we have a proposal distribution Q (X) such that(cid:5)mi=1 F i(x)(cid:5)pj=1 C j(x)Q (x)Z =(cid:4)x∈XQ (x) = EQ(cid:5)p(cid:5)mi=1 F i(x)(cid:12) (cid:5)mi=1 F i(x)(cid:13)pj=1 C j(x)Q (x)j=1 C j(x) > 0 → Q (x) > 0, we can rewrite Eq. (9) as follows:(cid:5)Given independent and identically distributed (i.i.d.) samples (x1, . . . , xN ) generated from Q , we can estimate Z by:(cid:8)Z N = 1NN(cid:4)k=1(cid:5)(cid:5)mi=1 F i(xk)pj=1 C j(xk)Q (xk)= 1NN(cid:4)k=1(cid:15)(cid:14)xkwwhere(cid:15)(cid:14)xkw=(cid:5)mi=1 F i(xk)(cid:5)pj=1 C j(xk)Q (xk)is the weight of sample xk. By definition, the variance of the weights is given by:(cid:10)(cid:11)w(x)=V Q(cid:4)(cid:14)w(x) − Z(cid:15)2Q (x)x∈XWe can estimate the variance of (cid:8)Z N by:(cid:16)V Q [(cid:8)Z N ] =1N(N − 1)N(cid:4)(cid:14)(cid:14)w(cid:15)xk− (cid:8)Z N(cid:15)2k=1and it can be shown that (cid:16)V Q [(cid:8)Z N ] is an unbiased estimator of V Q [(cid:8)Z N ], namely,(cid:10)(cid:11)(cid:16)V Q [(cid:8)Z N ]EQ= V Q [(cid:8)Z N ]We can show that:1. EQ [(cid:8)Z N ] = Z , i.e. (cid:8)Z N is unbiased.2. limN→∞ (cid:8)Z N = Z , with probability 1 (follows from the central limit theorem).3. EQ [(cid:16)V Q [(cid:8)Z N ]] = V Q [(cid:8)Z N ] = V Q [w(x)]/N.Therefore, V Q [(cid:8)Z N ] can be reduced by either increasing the number of samples N or by reducing the variance of thej=1 C j(x), then for any sample x, we have w(x) = Z yielding anweights. It is easy to see that if Q (x) ∝poptimal (zero variance) estimator. However, making Q (x) ∝j=1 C j(x) is NP-hard and therefore in order to havea small MSE in practice, it is recommended that Q must be as “close” as possible to the function it tries to approximatewhich in our case ismi=1 F i(x)mi=1 F i(x)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)pmi=1 F i(x)pj=1 C j(x).2.3.2. Estimating the marginalsThe marginal problem is defined as:(cid:4)P (xi) =δxi (x)PM(x)x∈Xwhere PM is defined by:PM(x) = 1Zm(cid:3)i=1p(cid:3)F i(x)C j(x)j=1Given a proposal distribution Q (x) satisfying PM(x) > 0 → Q (x) > 0, we can rewrite Eq. (14) as follows:P (xi) =(cid:4)x∈Xδxi (x)PM(x)Q (x)Q (x) = EQ(cid:12)(cid:13)δxi (x)PM(x)Q (x)(14)(15)(16)700V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Given independent and identically distributed (i.i.d.) samples (x1, . . . , xN ) generated from Q , we can estimate P (xi) by:mi=1 F i(xk)Z Q (xk)δxi (xk)PM(xk)Q (xk)(cid:17)P N (xi) = 1Nj=1 C j(xk)= 1Nδxi (xk)N(cid:4)N(cid:4)(cid:5)(cid:5)p(17)k=1k=1Unfortunately, Eq. (17), while an unbiased estimator of P (xi) cannot be evaluated because Z is not known. We cansacrifice unbiasedness and estimate P (xi) by using the notion of properly weighted samples.Definition 10 (A properly weighted sample). (See [26].) A set of weighted samples {xk, w(xk)}Nare said to be properly weighted with respect to a distribution P if for any discrete function H ,k=1 drawn from a distribution G(cid:10)(cid:14)(cid:15)(cid:14)Hxkwxk(cid:15)(cid:11)EG= cEP(cid:10)(cid:11)H(x)where c is a normalization constant common to all samples.Given a set of N weighted samples drawn from P , we can estimate EP [H(x)] as:(cid:2)(cid:10)(cid:11)H(x)(cid:9)EP=Nk=1 H(xk)w(xk)(cid:2)Nk=1 w(xk)Substituting Eq. (15) in Eq. (16), we have:(cid:5)(cid:12)(cid:5)P (xi) = 1ZEQδxi (x)mi=1 F i(x)Q (x)(cid:13)pj=1 C j(x)(18)It is easy to prove that:Proposition 1. Given w(x) = δxi (x)to PM.(cid:5)mi=1 F i (x)Q (x)(cid:5)pj=1 C j (x), the set of weighted samples {xk, w(xk)}Nk=1 are properly weighted with respectTherefore, we can estimate P (xi) by:(cid:2)(cid:18)P N (xi) =Nk=1 w(xk)δxi (xk)(cid:2)Nk=1 w(xk)(19)It is easy to prove that limN→∞ E[(cid:18)P N (xi)] = P (xi), i.e. it is asymptotically unbiased. Therefore, by weak law of large numbersthe sample average (cid:18)P N (xi) converges almost surely to P (xi) as N → ∞. Namely,(cid:18)P N (xi) = P (xi), with probability 1 (from the weak law of large numbers)limN→∞In order to have small estimation error, the proposal distribution Q should be as close as possible to the target distributionPM.3. Eliminating the rejection problem using the backtrack-free distributionIn this section, we describe the rejection problem and show that the problem can be mitigated by modifying the proposaldistribution. Given a mixed network M = (cid:5)X, D, F, C(cid:6), a proposal distribution Q defined over X suffers from the rejectionproblem if the probability of generating a sample from Q that violates the constraints of PM expressed in C is relativelyhigh. When a sample x violates some constraints in C, its weight w(x) is zero and it is effectively rejected from the sampleaverage. In an extreme case, if the probability of generating a rejected sample is arbitrarily close to one, then even aftergenerating a large number of samples, the estimate of the weighted counts (given by Eq. (11)) would be zero and theestimate of the marginals (given by Eq. (19)) would be ill-defined. Clearly, if Q properly encodes all the zeros in M, thenwe would have no rejection.Definition 11 (Zero equivalence). A distribution P is zero equivalent to a distribution P(see Definition 8) are equivalent. Namely, they have the same set of consistent solutions.(cid:9), iff their flat constraint networksClearly then, given a mixed network M = (cid:5)X, D, F, C(cid:6) representing PM and given a proposal distribution Q ={Q 1, . . . , Q n} which is zero equivalent to PM, every sample x generated from Q satisfies PM(x) > 0 and no samplegenerated from Q would be rejected.Because Q is expressed in a product form: Q (X) =i=1 Q i( Xi| X1, . . . , Xi−1) along o = ( X1, . . . , Xn), we can make Qzero equivalent to PM by modifying its components Q i( Xi| X1, . . . , Xi−1) along o. To accomplish that, we have to make theset Q = {Q 1, . . . , Q n} backtrack-free along o relative to the constraints in C. The following definitions formalize this notion.(cid:5)nV. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729701Algorithm 1: Sampling from the backtrack-free distribution.Input: A mixed network M = (cid:5)X, D, F, C(cid:6), a proposal distribution Q along an ordering o and an oracleOutput: A full sample (x1, . . . , xn) from the backtrack-free distribution Q F of Qx = ∅;for i = 1 to n doi ( Xi |x) = Q i ( Xi |x);Q Ffor each value xi ∈ Di doy = x ∪ xi ;if oracle says that y is not globally consistent w.r.t. C thenQ Fi (xi |x) = 0;Normalize Q Fx = x ∪ xi ;i ( Xi |x) and generate a sample Xi = xi from it;12345678910return xDefinition 12 (Consistent and globally consistent partial sample). Given a set of constraints C defined over X = { X1, . . . , Xn},a partial sample (x1, . . . , xi) is consistent if it does not violate any constraint in C. A partial sample (x1, . . . , xi) is globallyconsistent if it can be extended to a solution of C (i.e. it can be extended to a full assignment to all n variables that satisfiesall constraints in C).Note that a consistent partial sample may not be globally consistent.Definition 13 (Backtrack-free distribution of Q w.r.t. C). Given a mixed network M = (cid:5)X, D, F, C(cid:6) and a proposal distributioni=1 Q i( Xi| X1, . . . , Xi−1) along an ordering o, the backtrack-free distribution Q F =Q = {Q 1, . . . , Q n} representing Q (X) ={Q F(cid:5)n1 , . . . , Q Fn} of Q along o w.r.t. C where Q F (X) == α Q i(xi|x1, . . . , xi−1)= 0(cid:7)Q Fi (xi|x1, . . . , xi−1)where α is a normalization constant.(cid:5)i ( Xi| X1, . . . , Xi−1) is defined by:ni=1 Q Fif (x1, . . . , xi) is globally consistent w.r.t. CotherwiseLet xi−1 = (x1, . . . , xi−1) and define the set Bxi−1i(cid:9)= {xi(cid:9)i) is not globally consistent w.r.t. C}. Then, α∈ Di|(x1, . . . , xi−1, xcan be expressed by:α =(cid:2)1 −1(cid:9)Q i(xi(cid:9)xi∈Bxi−1i|x1, . . . , xi−1)We borrow the term backtrack-free from the constraint satisfaction literature [6,27]. An order o is said to be backtrack-freew.r.t. a set of constraints C if it guarantees that no inconsistent partial assignment would be generated along o (i.e., everysample generated would not be rejected). By definition, a proposal distribution Q = {Q 1, . . . , Q n} is backtrack-free alongo w.r.t. its flat constraint network (see Definition 8). The proposal distribution presented in Definition 13 takes a proposaldistribution that is backtrack-free relative to itself and modifies its components to yield a distribution that is backtrack-freerelative to PM.Given a mixed network M = (cid:5)X, D, F, C(cid:6) and a proposal distribution Q = {Q 1, . . . , Q n} along o, we now show how to} of Q w.r.t. C. Algorithm 1 assumes that we havegenerate samples from the backtrack-free distribution Q F = {Q Fan oracle which takes a partial assignment (x1, . . . , xi) and a constraint satisfaction problem (cid:5)X, D, C(cid:6) as input and answers“yes” if the assignment is globally consistent and “no” otherwise. Given a partial assignment (x1, . . . , xi−1), the algorithmi ( Xi|x1, . . . , xi−1) is initialized to Q i( Xi|x1, . . . , xi−1).constructs Q FThen, for each assignment (x1, . . . , xi−1, xi) extending to Xi = xi , it checks whether (x1, . . . , xi−1, xi) is globally consistenti (xi|x1, . . . , xi−1) and generates arelative to C using the oracle. If not, it sets Q Fsample from it. Repeating this process along the order ( X1, . . . , Xn) yields a single sample from Q F . Note that for eachsample, the oracle should be invoked a maximum of O (n × d) times where n is the number of variables and d is themaximum domain size.i ( Xi|x1, . . . , xi−1) and samples a value for Xi as follows. Q Fi (xi|x1, . . . , xi−1) to zero, normalizes Q F1 , . . . , Q FnGiven samples (x1, . . . , xN ) generated from Q F , we can estimate Z (defined in Eq. (2)) by replacing Q by Q F in Eq. (11).We get:(cid:8)Z N = 1NN(cid:4)k=1(cid:5)mi=1 F i(xk)(cid:5)pj=1 C j(xk)Q F (xk)= 1NN(cid:4)k=1(cid:15)(cid:14)xkw Fwherew F (x) =(cid:5)mi=1 F i(x)(cid:5)pj=1 C j(x)Q F (x)is the backtrack-free weight of the sample.(20)(21)702V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729(cid:5)ni=1 Q i ( Xi | X1, . . . , Xi−1) along an ordering o = ( X1, . . . , Xn)Algorithm 2: SampleSearch.Input: A mixed network M = (cid:5)X, D, F, C(cid:6), the proposal distribution Q (X) =Output: A consistent full sample x = (x1, . . . , xn)= D i (copy domains), QSET i = 1, Dwhile 1 (cid:2) i (cid:2) n do(cid:9)1( X1) = Q 1( X1) (copy distribution), x = ∅;(cid:9)i// Forward phase(cid:9)if Di is not empty thenSample Xi = xi from Qif (x1, . . . , xi ) violates any constraint in C then(cid:9)i and remove it from D(cid:9)i ;(cid:9)i ( Xi = xi |x1, . . . , xi−1) = 0 and normalize Q(cid:9)i ;SET QGoto step 3;x = x ∪ xi , i = i + 1, D(cid:9)i= D i , Q(cid:9)i ( Xi |x1, . . . , xi−1) = Q i ( Xi |x1, . . . , xi−1);// Backward phaseelsex = x\xi−1;(cid:9)i−1( Xi−1 = xi−1|x1, . . . , xi−2) = 0 and normalize QSET QSET i = i − 1;(cid:9)i−1( Xi−1|x1, . . . , xi−2);if i = 0 thenreturn inconsistent;elsereturn x;12345678910111213141516Similarly, we can estimate the posterior marginals by replacing the weight w(x) in Eq. (19) with the backtrack-freeweight w F (x).(cid:18)P N (xi) =(cid:2)Nk=1 w F (xk)δxi (xk)(cid:2)Nk=1 w F (xk)(22)Clearly, (cid:8)Z N defined in Eq. (20) is an unbiased estimate of Z while (cid:18)P N (xi) defined in Eq. (22) is an asymptotically unbiasedestimate of the posterior marginals P (xi).In practice, one could use any constraint solver as a substitute for the oracle in Algorithm 1. However, generating samplesusing an exact solver would be inefficient in many cases. Next, we present the SampleSearch scheme which integratesbacktracking search with sampling. In essence, we integrate more naturally sampling with a specific oracle that is based onsystematic backtracking search, hopefully, generating a more efficient scheme.4. The SampleSearch schemeIn a nutshell, SampleSearch incorporates systematic backtracking search into the ordered Monte Carlo sampler so thatall full samples are solutions of the constraint portion of the mixed network but it does not insist on backtrack-freenessof the search process. We will sketch our ideas using the most basic form of systematic search: chronological backtracking,emphasizing that the scheme can work with any advanced systematic search scheme. Indeed, in our empirical work, wewill use an advanced search scheme based on the minisat solver [15].Given a mixed network M = (cid:5)X, D, F, C(cid:6) and a proposal distribution Q (X), the traditional ordered Monte Carlo sam-pler samples variables along the order o = ( X1, . . . , Xn) from Q and rejects a partial sample (x1, . . . , xi) if it violatesany constraints in C. Upon rejecting a sample, the sampler starts sampling anew from the first variable ( X1) in theordering. Instead, when there is a dead-end at (x1, . . . , xi−1, xi) SampleSearch modifies the conditional probability asQ i( Xi = xi|x1, . . . , xi−1) = 0 to reflect that (x1, . . . , xi) is not consistent, normalizes the distribution Q i( Xi|x1, . . . , xi−1) andre-samples Xi from the normalized distribution. The newly sampled value may be consistent in which case the algorithmproceeds to variable Xi+1 or it may be inconsistent in which case the algorithm will further modify Q i( Xi|x1, . . . , xi−1). Ifwe repeat the process we may reach a point where Q i( Xi|x1, . . . , xi−1) is 0 for all values of Xi . In this case, (x1, . . . , xi−1)is inconsistent and therefore the algorithm revises the distribution at Xi−1 by setting Q i−1( Xi−1 = xi−1|x1, . . . , xi−2) = 0,normalizes Q i−1 and re-samples a new value for Xi−1 and so on. SampleSearch repeats this process until a consistent fullsample that satisfies all constraints in C is generated. By construction, this process always yields a consistent full sample.The pseudo-code for SampleSearch is given in Algorithm 2. It can be viewed as a depth first backtracking search (DFS)over the state space of consistent partial assignments searching for a solution to a constraint satisfaction problem (cid:5)X, D, C(cid:6),whose value ordering is stochastically guided by Q . The updated distribution that guides the search is Q. In the for-ward phase, variables are sampled in sequence and a current partial sample (or assignment) is extended by sampling a(cid:9)i (xi|x1, . . . , xi−1) = 0, thenvalue xi for the next variable Xi using the current distribution Q(cid:9)SampleSearch backtracks to the previous variable Xi−1 (backward phase) and updates the distribution Qi−1 by setting(cid:9)i−1 and continues.Q(cid:9)i−1(xi−1|x1, . . . , xi−2) = 0 and normalizing Q(cid:9)i . If for all values xi ∈ Di , Q(cid:9)V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729703Fig. 1. A full OR search tree given a set of constraints and a proposal distribution.4.1. The sampling distribution of SampleSearch(cid:5)nLet I =will show that:i=1 I i( Xi| X1, . . . , Xi−1) be the sampling distribution of SampleSearch along the ordering o = ( X1, . . . , Xn). WeTheorem 1 (Main result). Given a mixed network M = (cid:5)X, D, F, C(cid:6) and a proposal distribution Q , the sampling distribution I ofSampleSearch equals the backtrack-free probability distribution Q F of Q w.r.t. C, i.e. ∀i Q Fi= I i .To prove this theorem, we need the following proposition:Proposition 2. Given a mixed network M = (cid:5)X, D, F, C(cid:6), a proposal distribution Q = {Q 1, . . . , Q n} and a partial assignment(x1, . . . , xi−1) which is globally consistent w.r.t. C, SampleSearch samples values without replacement from the domain Di of Xi untila globally consistent extension (x1, . . . , xi−1, xi) is generated.(cid:9)(cid:9)i ( Xi|x1, . . . , xi−1) be the mostProof. Consider a globally inconsistent extension (x1, . . . , xi−1, xi) of (x1, . . . , xi−1). Let Q(cid:9)i) is sampled then Sample-if (x1, . . . , xrecently updated proposal distribution. Because SampleSearch is systematic,Search would eventually detect its inconsistency by not being able to extend it to a solution. At this point, it will set(cid:9)(cid:9)|x1, . . . , xi−1) = 0 either in step 6 or step 11 and normalize QQi is sampled just once yielding sam-i . In other words, x(cid:9)i ( Xi|x1, . . . , xi−1). On the other hand, again because of its systematic nature, if a globallypling without replacement from Qconsistent extension (x1, . . . , xi) is sampled, SampleSearch will always extend it to a full sample that is consistent. (cid:2)(cid:9)(cid:9)i (xiWe can use Proposition 2 to derive I i(xi|x1, . . . , xi−1), the probability of sampling a globally consistent extension(x1, . . . , xi−1, xi) to a globally consistent assignment (x1, . . . , xi−1) from Q i( Xi|x1, . . . , xi−1) as illustrated in the next ex-ample (Example 1).Example 1. Consider the complete search tree corresponding to the proposal distribution and to the constraints given inFig. 1. The inconsistent partial assignments are grounded in the figure. Each arc is labeled with the probability of generatingthe child node from Q given an assignment from the root node to its parent. Consider the full assignment ( A = 0, B = 2, C =0). Based on Proposition 2, the five different ways in which this assignment could be generated by SampleSearch (calledas DFS-traces) are shown in Fig. 2. In the following, we show how to compute the probability I B (B = 2| A = 0), i.e. theprobability of sampling B = 2 given A = 0. Given A = 0, the events that could lead to sampling B = 2 are shown in Fig. 2,(a) (cid:5)B = 2(cid:6)| A = 0, (b) (cid:5)B = 0, B = 2(cid:6)| A = 0, (c) (cid:5)B = 3, B = 0(cid:6)| A = 0, (d) (cid:5)B = 0, B = 3, B = 2(cid:6)| A = 0, and (e) (cid:5)B = 3, B =0, B = 2(cid:6)| A = 0. The notation (cid:5)B = 3, B = 0, B = 2(cid:6)| A = 0 means that given A = 0, the states were sampled in the order704V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Fig. 2. Five possible traces of SampleSearch which lead to the sample ( A = 0, B = 2, C = 0). The children of each node are specified from left to right inthe order in which they are generated.from left to right (B = 3, B = 0, B = 2). Clearly, the probability I B (B = 2| A = 0) equals the sum over the probability of theseevents. Let us now compute the probability of the event (cid:5)B = 3, B = 0, B = 2(cid:6)| A = 0. The probability of sampling B = 3| A = 0from Q (B| A = 0) = (0.3, 0.4, 0.2, 0.1) is 0.1. The assignment ( A = 0, B = 3) is inconsistent and therefore the distribution(cid:9)(B| A = 0) = (0.3/0.9, 0.4/0.9, 0.2/0.9, 0) = (3/9, 4/9, 2/9, 0). Subsequently,Q (B| A = 0) is changed by SampleSearch to Qis 3/9. However, the assignment ( A = 0, B = 0) is also globally inconsistent andthe probability of sampling B = 0 from Q(cid:9)(cid:9)(B| A = 0) ∝ (0, 4/9, 2/9, 0) = (0, 2/3, 1/3, 0). Next, the probability of samplingtherefore the distribution is changed to Qis 1/3. Therefore, the probability of the event (cid:5)B = 3, B = 0, B = 2(cid:6)| A = 0 is 0.1 × (3/9) × (1/3) = 1/90. ByB = 2 from Qcalculating the probabilities of the remaining events using the approach described above and taking the sum, one can verifythat the probability of sampling B = 2 given A = 0, i.e. I B (B = 2| A = 0) = 1/3.(cid:9)(cid:9)(cid:9)We will now show that:Proposition 3. Given a mixed network M = (cid:5)X, D, F, C(cid:6), an initial proposal distribution Q = {Q 1, . . . , Q n} and a partial assignment(x1, . . . , xi−1, xi) which is globally consistent w.r.t. C, the probability I i(xi|x1, . . . , xi−1) of sampling xi given (x1, . . . , xi−1) usingSampleSearch is proportional to Q i(xi|x1, . . . , xi−1), i.e. I i(xi|x1, . . . , xi−1) ∝ Q i(xi|x1, . . . , xi−1).xi−1ixi−1iProof. The proof is obtained by deriving a general expression for I i(xi|x1, . . . , xi−1), summing the probabilities of all eventsthat can lead to this desired partial sample. Consider a globally consistent partial assignment xi−1 = (x1, . . . , xi−1). Let usxi−1assume that the domain of the next variable Xi given xi−1, denoted by Dwhereixi−1Ri| (x1, . . . , xi−1, xi) is globally consistent} and Bis partitioned into Dxi−1i.= {xi,1, . . . , xi,q}. Let j = 1, . . . , 2q index the sequence of all subsets of Bxi−1i, j ) denote the sequence of all permutations of Bxi−1i, j with πk(Bxi−1i, j ), xi|xi−1) be the probability of generating xi and πk(Bxi−1with Bi, j denoting the j-th elementxi−1i, j ) denoting the k-th element ofxi−1i, j ) given xi−1 by SampleSearch.given xi−1 is obtained by summing over all the events that generate Xi = xiof this sequence. Let π (Bthis sequence. Finally, let Pr(πk(BThe probability of sampling xi ∈ R= {xi ∈ Dxi−1ixi−1ixi−1ixi−1ixi−1ixi−1i= DLet B= R∪ B\ Rxi−1igiven xi−1:I i(xi|xi−1) =2q(cid:4)|π (Bxi−1)|(cid:4)i, jj=1k=1(cid:14)(cid:14)πkPrBxi−1i, j(cid:15)(cid:15)(cid:19)(cid:19)xi−1, xiwhere, Pr(πk(Bxi−1i, j ), xi|xi−1) is given by:(cid:15)(cid:14)(cid:15)(cid:14)(cid:14)(cid:14)PrπkBxi−1i, j, xi(cid:19)(cid:19)xi−1= PrπkBxi−1i, j(cid:15)(cid:19)(cid:19)xi−1(cid:15)Pr(cid:14)xi(cid:14)(cid:19)(cid:19)πk(cid:15)Bxi−1i, j, xi−1(cid:15)Substituting Eq. (24) in Eq. (23), we get:I i(xi|xi−1) =2q(cid:4)|π (Bxi−1)|(cid:4)i, jj=1k=1(cid:14)(cid:14)πkPrBxi−1i, j(cid:15)(cid:15)(cid:19)(cid:19)xi−1(cid:14)Prxi(cid:14)(cid:19)(cid:19)πkBxi−1i, j(cid:15), xi−1(cid:15)(23)(24)(25)V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729705where Pr(xi|πk(Binconsistent. Because, we sample without replacement (see Proposition 2) from Q i , this probability is given by:xi−1i, j ), xi−1) is the probability with which the value xiis sampled given that (πk(Bxi−1i, j ), xi−1) is proved(cid:14)Prxi(cid:14)(cid:19)(cid:19)πk(cid:15)Bxi−1i, j, xi−1(cid:15)=(cid:2)1 −Q i(xi|xi−1)(cid:9)Q i(xxi−1ii, j∈B(cid:9)xi|xi−1)From Eqs. (25) and (26), we get:I i(xi|xi−1) =2q(cid:4)|π (Bxi−1)|(cid:4)i, jj=1k=1(cid:2)1 −Q i(xi|xi−1)(cid:9)Q i(xxi−1ii, j∈B(cid:9)xi(cid:14)(cid:14)PrπkBxi−1i, j(cid:15)(cid:15)(cid:19)(cid:19)xi−1|xi−1)Q i(xi|xi−1) does not depend on the indices j and k in Eq. (27) and therefore we can rewrite Eq. (27) as:(cid:20)I i(xi|xi−1) = Q i(xi|xi−1)|π (Bxi−1)|(cid:4)i, j2q(cid:4)j=1k=1(cid:21)Pr(πk(B(cid:2)(cid:9)xi∈Bxi−1i, jxi−1i, j )|xi−1)(cid:9)Q i(xi|xi−1)1 −(26)(27)(28)The term enclosed in brackets in Eq. (28) does not depend on xi and therefore it follows that if (x1, . . . , xi−1, xi) is globallyconsistent:I i(xi|xi−1) ∝ Q i(xi|xi−1)(cid:2)(29)We now have the necessary components to prove Theorem 1:Proof of Theorem 1. From Proposition 2, I i(xi|xi−1) equals zero iff xi is not globally consistent and from Proposition 3, for all|xi−1). Consequently,other values, I i(xi|xi−1) ∝ Q i(xi|xi−1). Therefore, the normalization constant equals 1 −(cid:2)(cid:9)xi∈Bxi−1i(cid:9)Q i(xiI i(xi|xi−1) =(cid:2)1 −Q i(xi|xi−1)(cid:9)Q i(xxi−1ii∈B(cid:9)xi|xi−1)(30)The right-hand side of Eq. (30) is by definition equal to Q Fi (xi|xi−1) (see Definition 13). (cid:2)4.2. Computing Q F (x)i (xi|xi−1). From Definition 13, we see that to compute the components Q FOnce we have a sample, we still need to compute the weights for estimating the marginals and the weighted counts,i (xi|xi−1)which in turn requires computing Q F(cid:9)for a sample x = (x1, . . . , xn), we have to determine all values x∈ Di which cannot be extended to a solution. One way toiaccomplish that, as described in Algorithm 1 is to use an oracle. The oracle should be invoked a maximum of n × (d − 1)times where n is the number of variables and d is the maximum domain size. Methods such as adaptive consistency [6] orany other exact CSP solver can be used as oracles. But then, what have we gained by SampleSearch, if ultimately, we needto use the oracle almost the same number of times as the sampling method presented in Algorithm 1. Next, we will showhow to approximate the backtrack-free probabilities on the fly while still maintaining some desirable guarantees.4.2.1. Approximating Q F (x)During the process of generating a sample x, SampleSearch may have discovered one or more values in the set Band therefore we can build an approximation of Q Fof Xi that were proved to be inconsistent given xi−1 while generating a sample x. We use the set Aapproximation T Fi (xi|xi−1) as follows. Let Abe the set of values in the domainxi−1ito compute an⊆ Bi (xi|xi−1) of Q Fi (xi|xi−1) as follows:xi−1ixi−1ixi−1ii (xi|xi−1) =T F(cid:2)1 −|xi−1)(31)Finally we compute T F (x) =Q F (x) for computing the weight w F (x) in Eq. (21).i (xi|xi−1). However, T F (x) does not guarantee asymptotic unbiasedness when replacing(cid:9)i) that wereTo remedy the situation, we can store each sample (x1, . . . , xn) and all its partial assignments (x1, . . . , xi−1, xproved inconsistent during each trace of an independent execution of SampleSearch called DFS-traces (for example, Fig. 2shows the five DFS-traces that could generate the sample ( A = 0, B = 2, C = 0)). After executing SampleSearch N timesgenerating N samples, we can use all the stored DFS-traces to compute an approximation of Q F (x) as illustrated in thefollowing example.Q i(xi|xi−1)(cid:9)Q i(xxi−1ii(cid:9)∈Axi(cid:5)ni=1 T F706V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Fig. 3. (a) Three DFS-traces. (b) Combined information from the three DFS-traces given in (a). (c) Two possible approximations of I(B| A = 1).Example 2. Consider the three traces given in Fig. 3(a). We can combine the information from the three traces as shown inFig. 3(b). Consider the assignment ( A = 1, B = 2). The backtrack-free probability of generating B = 2 given A = 1 requiresthe knowledge of all the values of B which are inconsistent. Based on the combined traces, we know that B = 0 andB = 1 are inconsistent (given A = 1) but we do not know whether B = 3 is consistent or not because it is not explored(indicated by “???” in Fig. 3(b)). Setting the unexplored nodes to either inconsistent or consistent gives us the two differentapproximations shown in Fig. 3(c).Generalizing Example 2, we consider two bounding approximations denoted by U FN respectively which are basedon setting each unexplored node in the combined N traces to consistent or inconsistent respectively. As we will show, theseapproximations can be used to bound the sample mean (cid:8)Z N from above and below.3N and L FN ). Given a mixed network M = (cid:5)X, D, F, C(cid:6), an initialDefinition 14 (Upper and lower approximations of Q F by U Fproposal distribution Q = {Q 1, . . . , Q n}, a combined sample tree generated from N independent runs of SampleSearch anda partial sample xi−1 = (x1, . . . , xi−1) generated in one of the N independent runs, we define two sets:N and L F• Axi−1ixi−1N,i⊆ BSampleSearch}.xi−1⊆ DN,ixi−1i• C= {xi ∈ Dxi−1i= {xi ∈ Dxi−1i| (x1, . . . , xi−1, xi) was proved to be inconsistent during the N independent runs of| (x1, . . . , xi−1, xi) was not explored during the N independent runs of SampleSearch}.We can set all the nodes in Cxi−1N,i(i.e. the nodes which are not explored) either to consistent or inconsistent yielding:U FN (x) =n(cid:3)i=1U FN,i(xi|xi−1) where U FN,i(xi|xi−1) =(cid:2)1 −Q i(xi|xi−1)(cid:9)Q i(xxi−1iN,i∈A(cid:9)xi|xi−1)(32)3 Note that it is easy to envision other approximations in which we designate some unexplored nodes as consistent while others as inconsistent basedon the domain knowledge or via some other Monte Carlo estimate. We consider the two extreme options because they usually work well in practice andbound the sample mean from above and below.V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729N (x) =L Fn(cid:3)i=1N,i(xi|xi−1) where L FL FN,i(xi|xi−1) =1 −(cid:2)(cid:9)xiQ i(xi|xi−1)xi−1N,ixi−1N,i∪C∈A(cid:9)Q i(xi|xi−1)707(33)It is clear that as N grows, the sample tree grows and therefore more inconsistencies will be discovered and as N → ∞,xi−1N,i= Bxi−1iand Cxi−1N,i= φ. Clearly then,1 , . . . , xNn )) generated by Sample Search, we canall inconsistencies will be discovered making the respective sets approach AProposition 4. limN→∞ U FN (x) = limN→∞ L FN (x) = Q F (x).As before, given a set of i.i.d. samples (x1 = (x11, . . . , x1estimate the weighted counts Z using the two statistics U F(cid:9)Z UN= 1NN(cid:4)k=1(cid:5)(cid:5)mi=1 F i(xk)U FN (xk)pj=1 C j(xk)= 1NN(cid:4)k=1w UNxkn), . . . , xN = (xNN (x) and L F(cid:14)N (x) by:(cid:15)where(cid:15)(cid:14)xk=w UN(cid:5)(cid:5)pj=1 C j(xk)mi=1 F i(xk)U FN (xk)is the weight of the sample based on the combined sample tree using the upper approximation U FN .(cid:9)Z LN= 1NN(cid:4)k=1(cid:5)(cid:5)pmi=1 F i(xk)L FN (xk)j=1 C j(xk)= 1NN(cid:4)k=1(cid:15)(cid:14)xkw LNwhere(cid:15)(cid:14)xk=w LN(cid:5)(cid:5)pmi=1 F i(xk)L FN (xk)j=1 C j(xk)is the weight of the sample based on combined sample tree using the lower approximation L FN .(cid:2)Similarly, for marginals, we can develop the statistics.N (xk)δxi (xk)N (xk)Nk=1 w U(cid:2)Nk=1 w U(cid:9)P UN (xi) =and(cid:9)P LN (xi) =(cid:2)Nk=1 w L(cid:2)Nk=1 w LN (xk)δxi (xk)N (xk)(34)(35)(36)(37)In the following three theorems, we state some interesting properties of (cid:9)Z Lin Appendix A.N , (cid:9)Z UN , (cid:9)P LN (xi) and (cid:9)P UN (xi). The proofs are providedTheorem 2. (cid:9)Z LN(cid:2) (cid:8)Z N (cid:2) (cid:9)Z UN .Theorem 3. The estimates (cid:9)Z UN (xi) and (cid:9)P L(cid:9)P UN and (cid:9)Z LN (xi) of P (xi) given in Eqs. (36) and (37) respectively are asymptotically unbiased.N of Z given in Eqs. (34) and (35) respectively are asymptotically unbiased. Similarly, the estimatesTheorem 4. Given N samples output by SampleSearch for a mixed network M = (cid:5)X, D, F, C(cid:6), the space and time complexity ofcomputing (cid:9)Z LN (xi) given in Eqs. (35), (34), (37) and (36) is O (N × d × n).N (xi) and (cid:9)P UN , (cid:9)Z UN , (cid:9)P LIn summary, we presented two approximations for the backtrack-free probability Q F which are used to bound thesample mean (cid:8)Z N . We proved that the two approximations yield an asymptotically unbiased estimate of the weighted countsand marginals. They will also enable trading bias with variance as we discuss next.4.2.2. Bias-variance tradeoffAs pointed in Section 2, the mean squared error of an estimator can be reduced by either controling the bias or byincreasing the number of samples. The estimators (cid:9)Z UN (which has abias of zero but requires invoking an exact CSP solver O (n × d) times). However, given a fixed time-bound, we expect thatthe estimators (cid:9)Z UN and (cid:9)Z LN from above and belowand therefore the absolute distance |(cid:9)Z U| is small enough, then we canexpect (cid:9)Z UN have more bias than the unbiased estimator (cid:8)Z FN and (cid:9)Z L| can be used to estimate their bias. If |(cid:9)Z UNN because they can be based on a larger sample size.− (cid:9)Z LN to perform better than (cid:8)Z FN will allow larger sample size than (cid:8)Z FN bound (cid:8)Z F− (cid:9)Z LN . Moreover, (cid:9)Z UN and (cid:9)Z LN and (cid:9)Z LNNN708V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–7294.3. Incorporating advanced search techniques in SampleSearchTheorem 1 is applicable to any search procedure that is systematic, i.e. once the search procedure encounters an assign-ment (x1, . . . , xi), it will either prove that the assignment is inconsistent or return with a full consistent sample extending(x1, . . . , xi). Therefore, we can use any advanced systematic search technique [6] instead of naive backtracking and easilyshow that:Proposition 5. Given a mixed network M = (cid:5)X, D, F, C(cid:6) and an initial proposal distribution Q = {Q 1, . . . , Q n}, SampleSearch aug-mented with any systematic advanced search technique generates independent and identically distributed samples from the backtrack-free probability distribution Q F of Q w.r.t. C.While advanced search techniques would not change the sampling distribution of SampleSearch, in practice, they canhave a significant impact on its time complexity and the quality of the upper and lower approximations. In particular, sinceSAT solvers developed over the last decade are quite efficient, we can represent the constraints in the mixed network usinga CNF formula4 and use minisat [15] as our SAT solver. However, we have to make minisat (or any other state-of-the-artSAT solver e.g. RSAT [29]) systematic via the following changes (the changes can be implemented with minimal effort):• Turn off random restarts and far backtracks. The use of restarts and far backtracks makes a SAT solver non-systematicand therefore they cannot be used.• Change variable and value ordering. We change the variable ordering to respect the structure of the input proposali=1 Q i( Xi| X1, . . . , Xi−1), we order variables as o = ( X1, . . . , Xn). Also, at eachdistribution Q , namely given Q (X) =decision point, variable Xi is assigned a value xi by sampling it from Q i( Xi|x1, . . . , xi−1).(cid:5)n5. Empirical evaluationWe conducted empirical evaluation on three tasks: (a) counting models of a SAT formula, (b) computing probability ofevidence and partition function in Bayesian and Markov networks respectively, and (c) computing posterior marginals inBayesian and Markov networks.The results are organized as follows. In the next subsection, we present the implementation details of SampleSearch.Section 5.2 describes other techniques that we compared with. In Section 5.3, we describe the results for the weightedcounting task while in Section 5.4, we focus on the posterior marginals task.5.1. SampleSearch with Iterative Join Graph Propagation and w-cutset sampling (IJGP-wc-SS)In our experiments, we show how SampleSearch operates on top of an advanced importance sampling algorithm IJGP-wc-IS presented in [10]. We call the resulting scheme IJGP-wc-SS. IJGP-wc-IS uses the generalized belief propagation schemeIterative Join Graph Propagation (IJGP) to construct a proposal distribution and the w-cutset sampling framework [14] toreduce the variance. Below, we outline the details of IJGP-wc-IS followed by those of IJGP-wc-SS.• The proposal distribution: The performance of importance sampling is highly dependent on how close the proposaldistribution is to the posterior distribution [8,30]. In principle, one could use the prior distribution as the proposaldistribution, such as in Likelihood weighting [31,32]. However, when the evidence is unlikely, the prior is a very badapproximation of the posterior [5,30]. In this case, the variance of the sample weights will be large and a few sampleswith large weights will dominate the mean, yielding an inefficient sampling scheme.Several schemes have been proposed to address this problem and below we briefly review two different but com-plementary approaches. In the first approach, which is often referred to as adaptive importance sampling [30,33], thesampling algorithm periodically updates the proposal distribution using the generated samples. As more and moresamples are drawn, the hope is that the updated proposal distribution would get closer and closer to the posteriordistribution; yielding a low variance sampling scheme. In the second approach, the idea is to use a state-of-the-artapproximation algorithm, e.g., Belief propagation [5] to construct a proposal distribution [34,23,10]. In IJGP-wc-IS, weuse the latter approach.In particular, IJGP-wc-IS uses Q = {Q 1, . . . , Q n}, obtained from the output of Iterative Join Graph Propagation (IJGP) [12,13] which was shown to yield good performance in earlier studies [23,10]. IJGP is a generalized belief propagation [11]technique for approximating the posterior distribution in graphical models. It uses the same message passing schemeas join tree propagation [35], but applies it over the clusters of a join graph rather than a join tree, iteratively. A joingraph is a decomposition of the functions of the mixed network into a graph of clusters that satisfies all the propertiesrequired of a valid join tree decomposition except the tree requirement. The time and space complexity of IJGP can becontrolled by its i-bound parameter which bounds the cluster size. IJGP is exponential in its i-bound and its accuracy4 It is easy to convert any (relational) constraint network to a CNF formula. In our implementation, we use the direct encoding described in [28].V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729709Algorithm 3: Implementation details of IJGP-wc-SS (SampleSearch with IJGP based proposal and w-cutset sampling).Input: A mixed network M = (cid:5)X, D, F, C(cid:6), integers i, N and w.Output: A set of N samples globally consistent w.r.t. CCreate a min-fill ordering o = ( X1, . . . , Xn);Create a join graph JG with i-bound i along o using the join graph structuring algorithm given in [12] and run IJGP on JG;Create a w-cutset K ⊆ X using the greedy scheme described in [14,38]. Let K = {K1, . . . , Kt };Create a proposal distribution Q (K) =First, we select a cluster A in JG that mentions K i and has the largest number of variables common with the previous variables {K1, . . . , K i−1}.Then, we construct Q i (K i |K1, . . . , K i−1) by marginalizing out all variables not mentioned in K1, . . . , K i from the marginal over the variables of A;for i = 1 to N doi=1 Q i (K i |K1, . . . , K i−1) from the messages and functions in JG using the following heuristic scheme [10].(cid:5)tApply minisat based SampleSearch on M with proposal distribution Q (K) to get a sample ki ;Store the DFS-trace of the sample ki in a combined sample tree.Output the required statistics (marginals or weighted counts) based on the combined sample tree;12345678generally increases with the i-bound. In our experiments, for every instance, we select the maximum i-bound that canbe accommodated by 512 MB of space as follows.The space required by a message (or a function) is the product of the domain sizes of the variables in its scope. Given ani-bound, we can create a join graph whose cluster size is bounded by i as described in [12] and compute, in advance,the space required by IJGP by summing over the space required by the individual messages.5 We iterate from i = 1until the space bound (of 512 MB) is surpassed. This ensures that IJGP terminates in a reasonable amount of time andrequires bounded space.• w-cutset sampling: As mentioned in Section 2.3, the mean squared error of importance sampling can be reduced byreducing the variance of the weights. To reduce the variance of the weights, we combine importance sampling withw-cutset sampling [14]. The idea is to partition the variables X into two sets K and R such that the treewidth ofthe mixed network restricted to R is bounded by a constant w. The set K is called the w-cutset. Because we canefficiently compute marginals and weighted counts over the mixed network restricted to R conditioned on K = k usingexact inference techniques such as bucket elimination [19], we need to sample only the variables in K. From the Rao-Blackwell theorem [26,36], it is easy to show that sampling from the subspace K reduces the variance.Formally, given a mixed network M = (cid:5)X, D, F, C(cid:6), a w-cutset K and a sample k generated from a proposal distributionQ (K), in w-cutset sampling, the weight of k is given by:(cid:2)(cid:5)w wc(k) =r∈Rmj=1 F j(r, K = k)Q (k)(cid:5)pa=1 Ca(r, K = k)(38)where R = X \ K. Given a w-cutset K, we can compute the sum in the numerator of Eq. (38) in polynomial time(exponential in the constant w) using bucket elimination [19].It was demonstrated that the higher the w-bound [14], the lower the sampling variance. Here also, we select themaximum w such that the resulting bucket elimination algorithm uses less than 512 MB of space. We can choose theappropriate w by using a similar iterative scheme to the one described above for choosing the i-bound.• Variable ordering heuristics: We experimented with three different variable ordering heuristics for constructing the joingraph of IJGP: min-fill ordering, min-degree ordering and the hmetis ordering.6 We performed sampling along thereverse order in which the join graph was constructed. Intuitively, this makes sense because IJGP is akin to variableelimination and sampling is akin to search, and it is known that the best ordering for elimination is the reverse orderingfor search and vice versa. In case of Bayesian networks, we also experimented with topological ordering for sampling.We found (as was also observed before) that the min-fill ordering gives the best performance and therefore for brevity,we only compare the performance of min-fill based IJGP-wc-IS and IJGP-wc-SS with the other solvers. We evaluate theordering heuristics in Section 5.5.The details of IJGP-wc-SS are given in Algorithm 3. The algorithm takes as input a mixed network and integer i, w and Nwhich specify the i-bound for IJGP, w for creating a w-cutset and the number of samples N respectively.7 In steps 1–2, thealgorithm creates a join graph along the min-fill ordering and runs IJGP. Then, in step 3, it computes a w-cutset K for thei=1 Q i(K i|K1, . . . , K i−1)mixed network. Then the algorithm creates a proposal distribution over the w-cutset K, Q (K) =from the output of IJGP using a heuristic scheme outlined in step 4. Finally, in steps 5–8 the algorithm executes minisatbased SampleSearch on the mixed network to generate the required N samples and outputs the required statistics.(cid:5)tHenceforth, we will refer to the estimates of IJGP-wc-SS generated using the upper and lower approximations of thebacktrack-free probability given by Eqs. (34) and (35) as IJGP-wc-SS/UB and IJGP-wc-SS/LB respectively. Note that IJGP-wc-5 Note that we can do this without constructing the messages explicitly.6 This ordering heuristic due to [37] is based on hyper-graph partitioning. To create the partitioning, we use the hmetis software available at: http://www-users.cs.umn.edu/karypis/metis/hmetis and hence the name.7 This is done after we determine the i-bound and the w for the w-cutset.710V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729SS/UB and IJGP-wc-SS/LB bound the sample mean (cid:8)Z N from above and below respectively and not the true mean or the(exact) weighted counts Z .5.2. Alternative schemesIn addition to IJGP-wc-SS and IJGP-wc-IS, we experimented with the following schemes.1. Iterative Join Graph Propagation (IJGP) In our experiments, we used an anytime version of IJGP [12,13] in which we startwith an i-bound of 1, run IJGP until convergence or until 10 iterations, whichever is earlier. Then we increase the i-boundby one and reconstruct the join graph. We do this until one the following conditions is met: (a) i equals the treewidth inwhich case IJGP yields exact marginals or (b) the 2 GB space limit is reached, or (c) the prescribed time-bound is reached.2. ApproxCount and SampleCount Wei and Selman [39] introduced an approximate solution counting scheme called Ap-proxCount. ApproxCount is based on the formal result of [40] that if one can sample uniformly (or close to it) from the setof solutions of a SAT formula F , then one can exactly count (or approximate with a good estimate) the number of solutionsof F . Consider a SAT formula F with S solutions. If we are able to sample solutions uniformly, then we can exactly com-pute the fraction of the number of solutions, denoted by γ that have a variable X set to True or 1 (and similarly to Falseor 0). If γ is greater than zero, we can set X to that particular value and simplify F to F. The estimate of the numberof solutions is now equal to the product of 1γ and the number of solutions of F. Then, we recursively repeat the process,leading to a series of multipliers, until all variables are assigned a value or until the conditioned formula is easy for exactmodel counters like Cachet [41]. To reduce the variance, Wei and Selman [39] suggest to set the selected variable to a valuethat occurs more often in the given set of sampled solutions. In this scheme, the fraction for each variable branching isselected via a solution sampling method called SampleSat [16], which is an extension of the well-known local search SATsolver Walksat [42]. We experimented with an anytime version of ApproxCount in which we report the cumulative averageaccumulated over several runs.(cid:9)(cid:9)SampleCount [17] differs from ApproxCount in the following two ways: (a) SampleCount heuristically reduces the vari-ance by branching on variables which are more balanced, i.e. variables having multipliers 1/γ close to 2 and (b) at eachbranch point, SampleCount assigns a value to a variable by sampling it with probability 0.5 yielding an unbiased estimate ofthe solution counts. We experimented with an anytime version of SampleCount in which we report the unbiased cumulativeaverages over several runs.8In our experiments, we used an implementation of ApproxCount and SampleCount available from the respective authors[16,17]. Following the recommendations made in [17], we use the following parameters for ApproxCount and SampleCount:(a) Number of samples for SampleSat = 20, (b) number of variables remaining to be assigned a value before running Cachet= 100, and (c) local search cutoff α = 100K .3. Evidence Pre-propagated Importance Sampling (EPIS) is an importance sampling algorithm for computing marginals inBayesian networks [23]. The algorithm uses loopy belief propagation [5,43] to construct the proposal distribution. In ourexperiments, we used the anytime implementation of EPIS submitted to the UAI 2008 evaluation [44].4. Edge Deletion Belief Propagation (EDBP) [20] is an approximation algorithm for computing posterior marginals and forcomputing probability of evidence. EDBP solves exactly a simplified version of the original problem, obtained by deletingsome of the edges from the primal graph. Deleted edges are selected based on two criteria: quality of approximation andcomplexity of computation (tree-width reduction) which is parameterized by an integer k, called the k-bound. Subsequently,information loss from the lost dependencies is compensated for by using several heuristic techniques. The implementationof this scheme is available from [20].5. Variable Elimination +++ Conditioning (VEC): When a problem having a high treewidth is encountered, variable or bucketelimination may be unsuitable, primarily because of its extensive memory demand. To alleviate the space complexity, wecan use the w-cutset conditioning scheme [19]. Namely, we condition or instantiate enough variables or the w-cutset sothat the remaining problem after removing the instantiated variables can be solved exactly using bucket elimination [19].In our experiments we select the w-cutset in such a way that bucket elimination would require less than 1.5 GB of space.Again, this is done to ensure that bucket elimination terminates in a reasonable amount of time and uses bounded space.Exact weighted counts can be computed by summing over the exact solution output by bucket elimination for all possibleinstantiations of the w-cutset. When VEC is terminated before completion, it outputs a partial sum yielding a lower boundon the weighted counts.As pre-processing, the algorithm performs SAT-based variable domain pruning that often yields significant performancegains in practice. Here, we first convert all zero probabilities and constraints in the problem to a CNF formula F . Then, for(cid:9)each variable-value pair, we construct a new CNF formula Fby adding a unit clause corresponding to the pair to F and(cid:9)check using minisat [15] if Fis inconsistent then we delete the value from the domain of thevariable. The implementation of this scheme is available publicly from our software website [45].is consistent or not. If F(cid:9)8 In the original paper, SampleCount [17] was investigated for lower bounding solution counts. Here, we evaluate the unbiased solution counts computedby the algorithm.V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729711Fig. 4. Chart showing the scope of our experimental study.Table 1Query types handled by various solvers.Problem typeBayesian networks P (e)Markov networks ZBayesian networks MarMarkov networks MarModel countingIJGP-wc-SSIJGP-wc-IS√√√√√IJGPEDBPEPIS-BNVEC√√√√√√√√√SampleCountApproxCountRelsat√Z : partition function, P (e): probability of evidence and Mar: posterior marginals.6. Relsat9 [18] is an exact algorithm for counting solutions of a satisfiability problem. When Relsat is stopped before com-pletion, it yields a lower bound on the number of solutions.7. ACE10 is a package for exact inference in Bayesian and Markov networks; currently it is state-of-the-art. It first compilesthe Bayesian or Markov network into an Arithmetic Circuit (AC) [46] and then uses the AC to answer various queries overthe network. ACE uses the c2d compiler [47] to compile the network into a d-DNNF [48] and then extracts the AC from thed-DNNF. Note that unlike other exact schemes described until now, ACE is not an anytime scheme. We therefore report onlythe time required by ACE to solve the instance, and use these times as a baseline for comparison.The benchmarks and the solvers for the different task types are shown in Fig. 4. Table 1 summarizes different query’ indicates that the algorithm is able to approximately estimate the√types that can be handled by the various solvers. A ‘query while a lack ofindicates otherwise.√5.3. Results for weighted countsNotation in tables. The first column in each table (see Table 2 for example) gives the name of the instance. The secondcolumn provides various statistical information about the instance such as the number of variables n, the average domainsize k, the number of clauses or constraints c, the number of evidence variables e and the treewidth of the instance w(computed using the min-fill heuristic after incorporating evidence and removing irrelevant variables). The fourth columnprovides the exact answer for the problem instance if available while the remaining columns display the results for thevarious solvers when terminated at the specified time-bound. The solver(s) giving the best results is highlighted in each” next to the output of a solver indicates that it solved the problem instance exactly (before the time-boundrow. A “expired) followed by the number of seconds it took to solve the instance enclosed in brackets. An “X” indicates that nosolution was output by the solver.∗9 Available at http://www.bayardo.org/resources.html.10 Available at http://reasoning.cs.ucla.edu/ace/.712V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Table 2Table showing the solution counts Z and the number of consistent samples M (only for the sampling based solvers) output by IJGP-wc-SS, IJGP-wc-IS,ApproxCount, SampleCount and Relsat after 10 hours of CPU time for 4 Latin Square instances for which the exact solution counts are known.Problem(cid:5)n, k, c, w(cid:6)ls8-norm(cid:5)512, 2, 5584, 255(cid:6)ls9-norm(cid:5)729, 2, 9009, 363(cid:6)ls10-norm(cid:5)1000, 2, 13820, 676(cid:6)ls11-norm(cid:5)1331, 2, 20350, 956(cid:6)Exact5.40E113.80E177.60E245.40E33Sample-Count5.15E+++11165144.49E+1777627.28E+++2438542.08E+342002Approx-Count3.52E+11177401.26E+1784751.17E+2443134.91E+312289ZMZMZMZMRelsat2.44E+081.78E+081.36E+081.09E+08IJGP-wc-SS/LB5.91E+112365103.44E+++171385726.74E+24955673.87E+++3366795IJGP-wc-SS/UB5.91E+112365103.44E+171385726.74E+24955673.87E+3366795IJGP-wc-ISX0X0X0X0Fig. 5. Time versus solution counts for two sample Latin square instances. IJGP-wc-IS is not plotted in the figures because it fails on all the instances.5.3.1. Satisfiability instancesFor the task of counting solutions (or models) of a satisfiability formula, we evaluate the algorithms on formulas fromthree domains: (a) normalized Latin square problems, (b) Langford problems, (c) FPGA-routing instances. We ran each algo-rithm for 10 hours on each instance.Results on instances for which exact solution counts are known. Our first set of benchmark instances come from the normalizedLatin squares domain. A Latin square of order s is an s × s table filled with s numbers from {1, . . . , s} in such a way thateach number occurs exactly once in each row and column. In a normalized Latin square the first row and column are fixed.The task here is to count the number of normalized Latin squares of a given order. The Latin squares were modeled as SATformulas using the extended encoding given in [49]. The exact counts for these formulas are known up to order 11 [50].Table 2 shows the results for Latin square instances up to order 11 for which exact solution counts are known. Approx-Count and Relsat underestimate the counts by several orders of magnitude. On the other hand, IJGP-wc-SS/UB, IJGP-wc-SS/LBand SampleCount yield very good estimates close to the true counts. The counts output by IJGP-wc-SS/UB and IJGP-wc-SS/LBare the same for all instances indicating that the sample mean is accurately estimated by the upper and lower approxima-tions of the backtrack-free distribution (see the discussion on bias versus variance in Section 4.2.2). IJGP-wc-IS fails on allinstances and is unable to generate a single consistent sample in ten hours. IJGP-wc-SS generates far more solution samplesthan with SampleCount and ApproxCount. In Fig. 5(a) and (b), we show how the estimates output by various solvers changewith time for the two largest instances. Here, we can clearly see the superior convergence of IJGP-wc-SS/LB, IJGP-wc-SS/UBand SampleCount over other approaches.Our second set of benchmark instances come from the Langford’s problem domain. The problem is parameterized by its(integer) size denoted by s. Given a set of s numbers {1, 2, . . . , s}, the problem is to produce a sequence of length 2s suchthat each i ∈ {1, 2, . . . , s} appears twice in the sequence and the two occurrences of i are exactly i apart from each other.This problem is satisfiable only if n is 0 or 3 modulo 4. We encoded the Langford problem as a SAT formula using thechanneling SAT encoding described in [51].Table 3 presents the results. ApproxCount and Relsat severely under estimate the true counts except on the instance ofsize 12 (lang12 in Table 3) which Relsat solves exactly in about 5 minutes. SampleCount is inferior to IJGP-wc-SS/UB andIJGP-wc-SS/LB by several orders of magnitude. IJGP-wc-SS/UB is slightly better than IJGP-wc-SS/LB. Unlike the Latin squareinstances, the solution counts output by IJGP-wc-SS/LB and IJGP-wc-SS/UB are different for large problems but the differenceV. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729713Table 3Table showing the solution counts Z and the number of consistent samples M (only for the sampling based solvers) output by IJGP-wc-SS, IJGP-wc-IS,” next to the output of a solver indicates that itApproxCount, SampleCount and Relsat after 10 hours of CPU time for Langford’s problem instances. A “solved the problem exactly (before the time-bound of 10 hours expired) followed by the number of seconds it took to solve the instance exactly.∗Problem(cid:5)n, k, c, w(cid:6)lang12(cid:5)576, 2, 13584, 383(cid:6)lang16(cid:5)1024, 2, 32320, 639(cid:6)lang19(cid:5)1444, 2, 54226, 927(cid:6)lang20(cid:5)1600, 2, 63280, 1023(cid:6)lang23(cid:5)2116, 2, 96370, 1407(cid:6)lang24(cid:5)2304, 2, 109536, 1535(cid:6)Exact2.16E+56.53E+085.13E+115.27E+127.60E+159.37E+16Sample-Count1.93E+0527205.97E+083289.73E+101461.13E+111207.53E+14381.17E+1325Approx-Count2.95E+0446688.22E+066416.87E+082323.99E+091803.70E+12544.15E+1133ZMZMZMZMZMZMRelsat2.16E+++05∗(297 s)6.28E+068.52E+058.55E+04XXIJGP-wc-SS/LB2.16E+059999916.51E+++08149716.38E+++1134312.83E+1229614.17E+1511118.74E+15271IJGP-wc-SS/UB2.16E+059999916.99E+08149717.31E+1134313.45E+++1229614.19E+++1511111.40E+++16271IJGP-wc-ISX0X0X0X0X0X0Fig. 6. Time versus solution counts for two sample Langford instances. IJGP-wc-IS and Relsat are not plotted in the figures because they fail on the giveninstances.is small. IJGP-wc-IS fails on all instances because it does not perform search. Again, we see that IJGP-wc-SS generates farmore consistent samples as compared with SampleCount and ApproxCount. In Fig. 6(a) and (b), we show how the estimatesvary with time for the two largest instances. We clearly see the superior anytime performance of IJGP-wc-SS/LB and IJGP-wc-SS/UB.11Results on instances for which exact solution counts are not known. When exact results are not available, evaluating the ca-pability of SampleSearch or any other approximation algorithm is problematic because the quality of the approximation(namely how close the approximation is to the exact) cannot be assessed. To allow some comparison on such hard in-stances we evaluate the power of various sampling schemes for generating good lower-bound approximations whose qualitycan be compared (the higher the better). Specifically, we compare the lower bounds obtained by combining IJGP-wc-SS/LB,IJGP-wc-IS and SampleCount with the Markov inequality based martingale and average schemes described in our previouswork [52]. These lower bounding schemes [17,52] take as input: (a) a set of unbiased sample weights or a lower boundon the unbiased sample weights and (b) a real number 0 < α < 1, and output a lower bound on the weighted counts Z(or solution counts in case of a SAT formula) that is correct with probability greater than α. In our experiments, we setα = 0.99 which means that the lower bounds are correct with probability greater than 0.99.11 An anonymous reviewer pointed out that the number of solutions of the Langford problem can be estimated using a specialized sampling scheme(he/she also provided a python implementation). This scheme suffers from the rejection problem, but the rejection rate is very small. The scheme oftenyields sample means which are closer to the true mean than the sample means output by SampleSearch, SampleCount and ApproxCount.714V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Table 4Table showing the lower bounds on solution counts Z and the number of consistent samples M (only for the sampling-based solvers) output by IJGP-wc-SS/LB, IJGP-wc-IS, SampleCount and Relsat after 10 hours of CPU time for 5 Latin Square instances for which the exact solution counts are not known. Theentries for IJGP-wc-IS, SampleCount and IJGP-wc-SS/LB contain the lower bounds computed by combining their respective sample weights with the Markovinequality based Average and Martingale schemes given in [52].Problem(cid:5)n, k, c, w(cid:6)Exactls12-norm(cid:5)1728, 2, 28968, 1044(cid:6)ls13-norm(cid:5)2197, 2, 40079, 1558(cid:6)ls14-norm(cid:5)2744, 2, 54124, 1971(cid:6)ls15-norm(cid:5)3375, 2, 71580, 2523(cid:6)ls16-norm(cid:5)4096, 2, 92960, 2758(cid:6)ZMZMZMZMZMSample-Count2.23E+++4310643.20E+545665.08E+652993.12E+791447.68E+9558Relsat1.26E+089.32E+077.1E+072.06E+07XIJGP-wc-SS/LB1.25E+43132751.15E+++5567231.24E+++7034642.03E+++8319352.08E+++981530IJGP-wc-ISX0X0X0X0X0We will show that the samples derived from SampleSearch (IJGP-wc-SS/LB) give rise to superior lower bounds comparedwith other sampling-based schemes. Comparing lower-bounds facilitates a comparative evaluation even on instances forwhich exact weighted counts are not available.12IJGP-wc-SS/UB cannot be used to lower bound Z because it outputs upper bounds on the unbiased sample weights.Likewise, ApproxCount cannot be used to lower bound Z because it is not unbiased. Finally, note that Relsat always yieldsa lower bound on the solution counts with probability one.First we compare the lower bounding ability of IJGP-wc-IS, IJGP-wc-SS/LB, SampleCount and Relsat on Latin square in-stances of size 12 through 15 for which the exact counts are not known. Table 4 contains the results. IJGP-wc-SS/LB yieldsfar better (higher) lower bounds than SampleCount as the problem size increases. Relsat underestimates the counts by sev-eral orders of magnitude as compared with IJGP-wc-SS/LB and SampleCount. As expected, IJGP-wc-IS fails on all instances.Again, we can see that the lower bounds obtained via IJGP-wc-SS/LB are based on a much larger sample size as comparedwith SampleCount.Our final domain is that of the FPGA routing instances. These instances are constructed by reducing FPGA (Field Pro-grammable Gate Array) detailed routing problems into a satisfiability formula. The instances were generated by Gi-JoonNam and were used in the SAT 2002 competition [53]. Table 5 presents the results for these instances. IJGP-wc-SS/LB yieldshigher lower bounds than SampleCount and Relsat on ten out of the fifteen instances. On the remaining five instances Sam-pleCount yields higher lower bounds than IJGP-wc-SS/LB. Relsat is always inferior to IJGP-wc-SS/LB while IJGP-wc-IS failson all instances. SampleCount fails to yield even a single consistent sample on 6 out of the 15 instances. On the remainingnine instances, the number of consistent samples generated by SampleCount are far smaller than IJGP-wc-SS.Next, we present results for Bayesian and Markov networks benchmarks. For the rest of the paper, note a slight change inthe content of each table. In the second column, we also report the time required by ACE to compute the weighted countsor marginals for the instance. The time-bound for ACE was set to 3 hrs.5.3.2. Linkage networksThe Linkage networks are generated by converting biological linkage analysis data into a Bayesian or Markov network.Linkage analysis is a statistical method for mapping genes onto a chromosome [54]. This is very useful in practice foridentifying disease genes. The input is an ordered list of loci L1, . . . , Lk+1 with allele frequencies at each locus and a pedi-gree with some individuals typed at some loci. The goal of linkage analysis is to evaluate the likelihood of a candidatevector [θ1, . . . , θk] of recombination fractions for the input pedigree and locus order. The component θi is the candidaterecombination fraction between the loci Li and Li+1.The pedigree data can be represented as a Bayesian network with three types of random variables: genetic loci variableswhich represent the genotypes of the individuals in the pedigree (two genetic loci variables per individual per locus, onefor the paternal allele and one for the maternal allele), phenotype variables, and selector variables which are auxiliaryvariables used to represent the gene flow in the pedigree. Fig. 7 represents a fragment of a network that describes parents–child interactions in a simple 2-loci analysis. The genetic loci variables of individual i at locus j are denoted by Li, jp andLi, jm. Variables Xi, j , S i, jp and S i, jm denote the phenotype variable, the paternal selector variable and the maternal selectorvariable of individual i at locus j, respectively. The conditional probability tables that correspond to the selector variables12 We still cannot evaluate the quality of the marginals when the exact solution is not known because the Markov inequality based schemes [17,52]cannot lower bound marginal probabilities.V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729715Table 5Table showing the lower bounds on solution counts Z and the number of consistent samples M (only for the sampling-based solvers) output by IJGP-wc-SS/LB, IJGP-wc-IS, SampleCount and Relsat after 10 hours of CPU time for FPGA routing instances. The entries for IJGP-wc-IS, SampleCount and IJGP-wc-SS/LBcontain the lower bounds computed by combining their respective sample weights with the Markov inequality based Average and Martingale schemes givenin [52].Problem9symml_gr_2pin_w6(cid:5)n, k, c, w(cid:6)(cid:5)2604, 2, 36994, 413(cid:6)9symml_gr_rcs_w6(cid:5)1554, 2, 29119, 613(cid:6)alu2_gr_rcs_w8(cid:5)4080, 2, 83902, 1470(cid:6)apex7_gr_2pin_w5(cid:5)1983, 2, 15358, 188(cid:6)apex7_gr_rcs_w5(cid:5)1500, 2, 11695, 290(cid:6)c499_gr_2pin_w6(cid:5)2070, 2, 22470, 263(cid:6)c499_gr_rcs_w6(cid:5)1872, 2, 18870, 462(cid:6)c880_gr_rcs_w7(cid:5)4592, 2, 61745, 1024(cid:6)example2_gr_2pin_w6(cid:5)3603, 2, 41023, 350(cid:6)example2_gr_rcs_w6(cid:5)2664, 2, 27684, 476(cid:6)term1_gr_2pin_w4(cid:5)746, 2, 3964, 31(cid:6)term1_gr_rcs_w4(cid:5)808, 2, 3290, 57(cid:6)too_large_gr_rcs_w7(cid:5)3633, 2, 50373, 1069(cid:6)too_large_gr_rcs_w8(cid:5)4152, 2, 57495, 1330(cid:6)vda_gr_rcs_w9(cid:5)6498, 2, 130997, 2402(cid:6)ZMZMZMZMZMZMZMZMZMZMZMZMZMZMZMExactSampleCount3.36E+5138.49E+++843741.21E+20685.83E+93542.17E+++1391028X02.41E+++87401.50E+++27853.93E+16014.17E+++265167X0X0X0X0X0Relsat3.41E+323.36E+721.88E+564.83E+493.69E+462.78E+477.61E+541.42E+437.35E+381.13E+732.13E+351.17E+491.46E+731.02E+642.23E+92IJGP-wc-SS/LB3.06E+++5362412.80E+82169111.69E+++2358412.33E+++94251619.64E+133483312.18E+++5544911.29E+84141517.16E+2558317.33E+++16019716.85E+25062116.90E+++393267717.44E+++553419511.05E+++18215615.66E+++24611715.08E+++300221IJGP-wc-ISX0X0X0X0X0X0X0X0X0X0X0X0X0X0X0Fig. 7. A fragment of a Bayesian network used in genetic linkage analysis.716V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Table 6Probability of evidence ( Z ) computed by VEC, EDBP, IJGP-wc-IS and IJGP-wc-SS after 3 hours of CPU time for Linkage instances from the UAI 2006 evaluation.For IJGP-wc-SS and IJGP-wc-IS, we also report the number of consistent samples (M) generated in 3 hours.ProblemBN_69BN_70BN_71BN_72BN_73BN_74BN_75BN_76(cid:5)n, k, c, e, w(cid:6)ACE time(cid:5)777, 7, 228, 78, 39(cid:6)ACE (timeout)(cid:5)2315, 5, 484, 159, 35(cid:6)ACE (233 s)(cid:5)1740, 6, 663, 202, 53(cid:6)ACE (timeout)(cid:5)2155, 6, 752, 252, 65(cid:6)ACE (timeout)(cid:5)2140, 5, 651, 216, 67(cid:6)ACE (timeout)(cid:5)749, 6, 223, 66, 35(cid:6)ACE (timeout)(cid:5)1820, 5, 477, 155, 37(cid:6)ACE (timeout)(cid:5)2155, 7, 605, 169, 53(cid:6)ACE (timeout)ZMZMZMZMZMZMZMZMExact5.28E−0542.00E−715.12E−1114.21E−1502.26E−1133.75E−455.88E−914.93E−110IJGP-wc-SS/LB3.00E−−−556.84E+51.21E−−−731.92E+51.28E−−−1117.46E+44.73E−−−1501.53E+52.00E−−−1157.75E+42.13E−−−462.80E+52.19E−−−917.72E+41.95E−−−1112.52E+4IJGP-wc-SS/UB3.00E−−−556.84E+51.21E−−−731.92E+51.28E−−−1117.46E+44.73E−−−1501.53E+52.00E−−−1157.75E+42.13E−−−462.80E+52.19E−−−917.72E+41.95E−−−1112.52E+4VECEDBP1.93E−612.39E−577.99E−826.00E−797.05E−1151.01E−1141.32E−1539.21E−1556.00E−1272.24E−1183.30E−485.84E−485.83E−973.10E−961.00E−1263.86E−114IJGP-wc-ISX0X0X0X0X0X0X0X0Fig. 8. Convergence of probability of evidence as a function of time for two sample Linkage instances. IJGP-wc-IS is not plotted in the figures because itfails on all the instances.are parameterized by the recombination ratio θ . The remaining tables contain only deterministic information. It can beshown that given the pedigree data, computing the likelihood of the recombination fractions is equivalent to computing theprobability of evidence on the Bayesian network that model the problem (for more details consult [21]).We first evaluate the solvers on Linkage (Bayesian) networks used in the UAI 2006 evaluation [55]. Table 6 contains theresults. The exact results for these instances are available from the UAI 2006 evaluation website. We see that IJGP-wc-SS/UBand IJGP-wc-SS/LB are very accurate usually yielding a few orders of magnitude improvement over VEC and EDBP. Becausethe estimates output by IJGP-wc-SS/UB and IJGP-wc-SS/LB are the same on all instances, they yield an exact value of thesample mean. Fig. 8 shows how the probability of evidence changes as a function of time for two sample instances. We seesuperior anytime performance of both IJGP-wc-SS schemes as compared with VEC and EDBP. IJGP-wc-IS fails to output asingle consistent sample in 3 hours of CPU time on all the instances.In Table 7, we present the results on the 18 linkage instances that were used in the UAI 2008 evaluation [44] for whichthe exact value of probability of evidence is known.13 We see that VEC (as an anytime scheme) exactly solves 10 instances(as indicated by a “” in Table 7). On 7 out of the remaining 8 instances, IJGP-wc-SS/LB and IJGP-wc-SS/UB are better thanVEC. EDBP exactly solves 5 instances. On the remaining 13 instances, IJGP-wc-SS/LB and IJGP-wc-SS/UB are better than∗13 The exact value of probability of evidence for instances that ACE and VEC were unable to solve was obtained by running the Bucket elimination (BE)with external memory (BEEM) algorithm [56]. BEEM uses external memory, such as disk storage, to increase the amount of memory available to BE, therebysignificantly improving BE’s scalability.V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729717Table 7Probability of evidence Z computed by VEC, EDBP, IJGP-wc-IS and IJGP-wc-SS after 3 hours of CPU time for Linkage instances from the UAI 2008 evaluation.” next to the output of aFor IJGP-wc-SS and IJGP-wc-IS, each cell in the table also reports the number of consistent samples M generated in 3 hours. A “solver indicates that it solved the problem exactly (before the time-bound expired) followed by the number of seconds it took to solve the instance exactly.∗Problempedigree18pedigree1pedigree20pedigree23pedigree25pedigree30pedigree37pedigree38pedigree39pedigree42pedigree31pedigree34pedigree13pedigree9pedigree19pedigree7pedigree51pedigree44(cid:5)n, k, c, e, w(cid:6)ACE time(cid:5)1184, 2, 386, 0, 26(cid:6)ACE (10 s)(cid:5)334, 2, 121, 0, 20(cid:6)ACE (2 s)(cid:5)437, 2, 147, 0, 25(cid:6)ACE (timeout)(cid:5)402, 2, 130, 0, 26(cid:6)ACE (8 s)(cid:5)1289, 2, 396, 0, 38(cid:6)ACE (timeout)(cid:5)1289, 2, 413, 0, 27(cid:6)ACE (8 s)(cid:5)1032, 2, 333, 0, 25(cid:6)ACE (52 s)(cid:5)724, 2, 263, 0, 18(cid:6)ACE (340 s)(cid:5)1272, 2, 354, 0, 29(cid:6)ACE (timeout)(cid:5)448, 2, 156, 0, 23(cid:6)ACE (timeout)(cid:5)1183, 2, 0, 45(cid:6)ACE (timeout)(cid:5)1160, 1, 0, 59(cid:6)ACE (timeout)(cid:5)1077, 1, 0, 51(cid:6)ACE (timeout)(cid:5)1118, 2, 0, 41(cid:6)ACE (timeout)(cid:5)793, 2, 0, 23(cid:6)ACE (timeout)(cid:5)1068, 1, 0, 56(cid:6)ACE (timeout)(cid:5)1152, 1, 0, 51(cid:6)ACE (timeout)(cid:5)811, 1, 0, 29(cid:6)ACE (timeout)Exact7.18E−797.81E−152.34E−302.78E−391.69E−1161.84E−842.63E−1175.64E−556.32E−1031.73E−311.98E−705.9E−655.44E−323.43E−799.4E−601.49E−651.34E−743.36E−64IJGP-wc-SS/LB7.39E−791.30E+57.81E−153.26E+52.31E−302.31E+52.76E−393.28E+51.69E−−−1161.29E+51.90E−841.14E+51.18E−1174.26E+53.80E−551.63E+56.29E−1031.25E+51.73E−313.26E+52.08E−−−706.7E+43.84E−651.2E+57.03E−−−328.1E+42.93E−798.0E+46.76E−609.5E+41.33E−−−658.3E+42.47E−−−741.0E+53.39E−−−641.7E+5IJGP-wc-SS/UB7.39E−791.30E+57.81E−153.26E+52.31E−302.31E+52.76E−393.28E+51.69E−−−1161.29E+51.90E−841.14E+51.18E−1174.26E+53.80E−551.63E+56.29E−1031.25E+51.73E−313.26E+52.08E−−−706.7E+43.84E−651.2E+57.03E−−−328.1E+42.93E−798.0E+46.76E−609.5E+41.33E−−−658.3E+42.47E−−−741.0E+53.39E−−−641.7E+5ZMZMZMZMZMZMZMZMZMZMZMZMZMZMZMZMZMZMVECEDBPIJGP-wc-IS7.18E−−−79∗(64 s)7.18E−−−79∗(772 s)7.81E−−−15∗(12 s)7.81E−−−15∗(14 s)2.34E−−−30∗(1216 s)6.19E−312.78E−−−39∗(913 s)1.52E−391.69E−−−116∗(318 s)1.69E−−−116∗(2562 s)1.85E−−−84∗(808 s)1.85E−−−84∗(179 s)2.63E−−−117∗(2521 s)5.69E−1245.65E−−−55∗(735 s)8.41E−566.32E−−−103∗(136 s)6.32E−−−103∗(694 s)1.73E−−−31∗(3188 s)8.91E−321.67E−762.58E−762.17E−378.00E−827.97E−−−601.66E−725.56E−852.23E−641.34E−704.30E−−−656.53E−343.13E−−−793.35E−602.93E−666.16E−767.69E−66X0X0X0X0X0X0X0X0X0X0X0X0X0X0X0X0X0X0VEC. Overall, IJGP-wc-SS/LB and IJGP-wc-SS/UB deviate only slightly from the exact value of probability of evidence. Again,IJGP-wc-IS fails on all the instances.5.3.3. Relational instancesThe relational instances are generated by grounding the relational Bayesian networks using the primula tool [22]. Weexperimented with ten Friends and Smoker networks and six mastermind networks from this domain which have between262 to 76212 variables. Table 8 summarizes the results.VEC solves 2 friends and smokers networks exactly while on the remaining instances, it fails to output any answer. EDBPsolves one instance exactly while on the remaining instances it either fails or is inferior to IJGP-wc-SS. IJGP-wc-IS is betterthan IJGP-wc-SS on 3 instances while on the remaining instances it fails to generate a single consistent sample; especiallyas the instances get larger. The estimates computed by IJGP-wc-SS/LB and IJGP-wc-SS/UB on the other hand are very closeto the exact probability of evidence.VEC solves exactly six out of the eight mastermind instances while on the remaining two instances VEC is worse thanIJGP-wc-SS/UB and IJGP-wc-SS/LB. EDBP solves two instances exactly while on the remaining instances it is worse thanIJGP-wc-SS/LB and IJGP-wc-SS/UB.Again, the estimates output by IJGP-wc-SS/LB and IJGP-wc-SS/UB are the same for all the relational instances indicatingthat our lower and upper approximations have zero bias.718V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Table 8Probability of evidence computed by VEC, EDBP, IJGP-wc-IS and IJGP-wc-SS after 3 hours of CPU time for relational instances. For IJGP-wc-SS and IJGP-wc-IS” next to the output of a solver indicates that it solved theeach cell in the table also reports the number of consistent samples generated in 10 hours. A “problem exactly (before the time-bound expired) followed by the number of seconds it took to solve the instance exactly.∗(cid:5)n, k, c, e, w(cid:6)ACE timeProblemFriends andsmokersExactIJGP-wc-SS/LBIJGP-wc-SS/UBVECEDBPfs-04fs-07fs-10fs-13fs-16fs-19fs-22fs-25fs-28fs-29(cid:5)262, 2, 74, 226, 12(cid:6)ACE (4 s)(cid:5)1225, 2, 371, 1120, 35(cid:6)ACE (4 s)(cid:5)3385, 2, 1055, 3175, 71(cid:6)ACE (9 s)(cid:5)7228, 2, 2288, 6877, 117(cid:6)ACE (9 s)(cid:5)13240, 2, 4232, 12712, 171(cid:6)ACE (14 s)(cid:5)21907, 2, 7049, 21166, 243(cid:6)ACE (22 s)(cid:5)33715, 2, 10901, 32725, 315(cid:6)ACE (49 s)(cid:5)49150, 2, 15950, 47875, 431(cid:6)ACE (74 s)(cid:5)68698, 2, 22358, 67102, 527(cid:6)ACE (148 s)(cid:5)76212, 2, 24824, 74501, 559(cid:6)ACE (167 s)Mastermindmm_03_08_03mm_03_08_04mm_03_08_05mm_04_08_03mm_04_08_04mm_05_08_03mm_06_08_03mm_10_08_03(cid:5)1220, 2, 1193, 48, 20(cid:6)ACE (7 s)(cid:5)2288, 2, 2252, 64, 30(cid:6)ACE (12 s)(cid:5)3692, 2, 3647, 80, 42(cid:6)ACE (35 s)(cid:5)1418, 2, 1391, 48, 22(cid:6)ACE (9 s)(cid:5)2616, 2, 2580, 64, 33(cid:6)ACE (19 s)(cid:5)1616, 2, 1589, 48, 28(cid:6)ACE (12 s)(cid:5)1814, 2, 1787, 48, 31(cid:6)ACE (13 s)(cid:5)2606, 2, 2579, 48, 56(cid:6)ACE (27 s)ZMZMZMZMZMZMZMZMZMZMZMZMZMZMZMZMZMZM1.53E−051.78E−157.88E−311.33E−518.63E−782.12E−1092.00E−1467.18E−1899.82E−2376.81E−2549.79E−88.77E−098.89E−118.39E−082.20E−085.29E−071.79E−081.92E−078.11E−061.00E+62.23E−161.00E+62.49E−−−328.51E+53.26E−555.41E+56.04E−791.79E+51.62E−−−1141.90E+54.88E−−−1471.18E+52.67E−−−1899.23E+44.53E−−−2379.35E+49.44E−−−2552.62E+49.87E−085641018.19E−09351017.27E−11104018.37E−083795011.84E−−−08129014.78E−07602011.12E−081133015.01E−−−07108018.11E−061.00E+62.23E−161.00E+62.49E−−−328.51E+53.26E−555.41E+56.04E−791.79E+51.62E−−−1141.90E+54.88E−−−1471.18E+52.67E−−−1899.23E+44.53E−−−2379.35E+49.44E−−−2552.62E+49.87E−085641018.19E−09351017.27E−11104018.37E−083795011.84E−−−08129014.78E−07602011.12E−081133015.01E−−−07108011.53E−−−05∗(1 s)1.53E−−−05∗(2 s)1.78E−−−15∗(708 s)1.11E−16XXXXXXXX7.70E−341.63E−551.32E−82XXXXX9.79E−−−08∗(3 s)9.79E−−−08∗(11 s)8.77E−−−09∗(1231 s)8.90E−−−11∗(1503 s)8.39E−−−08∗(7 s)1.21E−08XXXX5.30E−−−07∗(229 s)5.3E−−−07∗(6194 s)1.80E−−−08∗(2082 s)5.85E−097.79E−082.39E−10IJGP-wc-IS1.52E−052.17E+8X0X01.33E−−−514.67E+78.63E−−−781.37E+7X0X0X0X0X0X0X0X0X0X0X0X0X05.4. Results for the posterior marginal tasks5.4.1. Setup and evaluation criteriaWe experimented with the following four benchmark domains: (a) The linkage instances, (b) the relational instances,(c) the grid instances, and (d) the logistics planning instances. We measure the accuracy of the solvers using averageHellinger distance [57]. Given a mixed network with n variables, let P ( Xi) and A( Xi) denote the exact and approximatemarginals for a variable Xi , then the average Hellinger distance denoted by (cid:8) is defined as:(cid:2)(cid:2)ni=112xi ∈Di√((cid:8) =√P (xi) −nA(xi) )2(39)Hellinger distance lies between 0 and 1 and lower bounds the Kullback–Leibler divergence [58]. A Hellinger distance of0 for a solver indicates that the solver output the exact marginal distribution for each variable while a Hellinger distance of1 indicates that the solver failed to output any solution.As pointed out in [57], Hellinger distance is superior to other choices such as the Kullback–Leibler (KL) divergence, themean squared error and the relative error when zero or infinitesimally small probabilities are present. We do not use theKL divergence because it lies between 0 and ∞ and in practice when the exact marginals are 0 or close to it, floating-pointV. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729719Table 9Table showing the Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-SS, IJGP-wc-IS, IJGP, EPIS and EDBP for Linkage instances fromthe UAI 2006 evaluation after 3 hours of CPU time. For IJGP-wc-IS and IJGP-wc-SS, we also report the number of consistent samples M generated in 3 hours.ProblemBN_69BN_70BN_71BN_72BN_73BN_74BN_75BN_76(cid:5)n, k, c, e, w(cid:6)ACE time(cid:5)777, 7, 228, 78, 39(cid:6)ACE (timeout)(cid:5)2315, 5, 484, 159, 35(cid:6)ACE (233 s)(cid:5)1740, 6, 663, 202, 53(cid:6)ACE (timeout)(cid:5)2155, 6, 752, 252, 65(cid:6)ACE (timeout)(cid:5)2140, 5, 651, 216, 67(cid:6)ACE (timeout)(cid:5)749, 6, 223, 66, 35(cid:6)ACE (timeout)(cid:5)1820, 5, 477, 155, 37(cid:6)ACE (timeout)(cid:5)2155, 7, 605, 169, 53(cid:6)ACE (timeout)IJGP-wc-SSIJGPEPISEDBPIJGP-wc-IS(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M9.4E−−−046.84E+52.6E−−−031.92E+55.6E−−−037.46E+43.6E−−−031.53E+52.1E−−−027.75E+46.9E−042.80E+58.0E−−−037.72E+41.8E−−−022.52E+43.2E−023.3E−021.9E−027.2E−032.8E−024.3E−−−066.2E−022.6E−02111111118.0E−029.6E−022.5E−021.3E−026.1E−024.3E−029.3E−022.7E−021010101010101010precision errors in the exact (or the approximate) solver may yield a false zero when the correct marginal is non-zero andvice versa yielding infinite KL divergence.14 We did compute the error using all other commonly used distance measuressuch as the mean squared error, the relative error and the absolute error. All error measures show similar trends, with theHellinger distance being the most discriminative.Finally, for the marginal task, IJGP-wc-SS/LB and IJGP-wc-SS/UB output the same marginals for all benchmarks that weexperimented with and therefore we do not distinguish between them. This implies that our lower and upper approxima-tions of the backtrack free probability are indeed quite strong and have negligible or zero bias. Therefore, for the rest of thissubsection, we will refer to IJGP-wc-SS/LB and IJGP-wc-SS/UB as IJGP-wc-SS.5.4.2. Linkage instancesIn Table 9, we report the average Hellinger distance between exact and approximate marginals for the linkage instancesfrom the UAI 2006 evaluation [55]. We do not report on the pedigree instances from the UAI 2008 evaluation [44] becausetheir exact marginals are not known. We can see that IJGP-wc-SS is more accurate than IJGP which in turn is more accuratethan EDBP on 7 out of the 8 instances. We can clearly see the relationship between treewidth and the performance ofpropagation based and sampling based techniques. When the treewidth is relatively small (on BN_74), a propagation basedscheme like IJGP is more accurate than IJGP-wc-SS but as the treewidth increases, there is one to two orders of magnitudedifference in the Hellinger distance. EPIS and IJGP-wc-IS do not generate even a single consistent sample in 3 hours of CPUtime and therefore their average Hellinger distance is 1.15 In Fig. 9, we demonstrate the superior anytime performance ofIJGP-wc-SS compared with other solvers.5.4.3. Relational instancesWe experimented again with the 10 Friends and Smoker networks and 6 mastermind networks from the relationalBayesian networks domain [22]. Table 10 shows the Hellinger distance between the exact and approximate marginals after3 hours of CPU time for each solver.On the small friends and smoker networks, fs-04 to fs-13, IJGP performs better than IJGP-wc-SS. However, on largenetworks which have between 13240 and 76212 variables, and treewidth between 12 and 559, IJGP-wc-SS performs betterthan IJGP. EDBP is slightly worse than IJGP and runs out of memory on large instances, indicated by a Hellinger distanceof 1. EPIS is not able to generate a single consistent sample in 3 hours of CPU time indicated by Hellinger distance of1 for all instances. IJGP-wc-IS fails on all but three instances. On these three instances, IJGP-wc-IS has smaller error thanIJGP-wc-SS because it generates many more consistent samples than IJGP-wc-SS (by a factor of 10–200).Discussion. The small sample size of IJGP-wc-SS as compared with its pure sampling counterpart IJGP-wc-IS is due to theoverhead of solving a satisfiability formula via backtracking search to generate a sample. IJGP-wc-IS, on the other hand, uses14 Also see for example the results of the recent UAI evaluation [44]. Dechter and Mateescu [59] proved that IJGP (and EDBP) cannot yield marginalshaving infinite KL distance. However, in many cases these solvers had infinite KL distance because of precision errors.15 The EPIS program does not output the number of consistent samples that were used in computing the marginals. It outputs an invalid marginaldistribution for all variables (for example, it will output a marginal distribution of (0, 0, 0) for a variable having 3 values in its domain) when it generatesno consistent samples within the stipulated time-bound.720V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Fig. 9. Time versus Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-IS, IJGP-wc-SS, IJGP, EPIS and EDBP for two sampleLinkage instances.Table 10Table showing the Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-SS, IJGP-wc-IS, IJGP, EPIS and EDBP for relational instancesafter 3 hours of CPU time. For IJGP-wc-IS and IJGP-wc-SS, we also report the number of consistent samples M generated in 3 hours.ProblemFriends andsmokers(cid:5)n, k, c, e, w(cid:6)ACE timeIJGP-wc-SSIJGPEPISEDBPIJGP-wc-ISfs-04fs-07fs-10fs-13fs-16fs-19fs-22fs-25fs-28fs-29Mastermindmm_03_08_03mm_03_08_04mm_03_08_05mm_04_08_04mm_05_08_03mm_06_08_03mm_10_08_03(cid:5)262, 2, 74, 226, 12(cid:6)ACE (4 s)(cid:5)1225, 2, 371, 1120, 35(cid:6)ACE (4 s)(cid:5)3385, 2, 1055, 3175, 71(cid:6)ACE (9 s)(cid:5)7228, 2, 2288, 6877, 117(cid:6)ACE (10 s)(cid:5)13240, 2, 4232, 12712, 171(cid:6)ACE (14 s)(cid:5)21907, 2, 7049, 21166, 243(cid:6)ACE (23 s)(cid:5)33715, 2, 10901, 32725, 315(cid:6)ACE (49 s)(cid:5)49150, 2, 15950, 47875, 431(cid:6)ACE (74 s)(cid:5)68698, 2, 22358, 67102, 527(cid:6)ACE (149 s)(cid:5)76212, 2, 24824, 74501, 559(cid:6)ACE (168 s)(cid:5)1220, 2, 1193, 48, 20(cid:6)ACE (7 s)(cid:5)2288, 2, 2252, 64, 30(cid:6)ACE (12 s)(cid:5)3692, 2, 3647, 80, 42(cid:6)ACE (35 s)(cid:5)2616, 2, 1391, 64, 33(cid:6)ACE (19 s)(cid:5)1616, 2, 2580, 48, 28(cid:6)ACE (12 s)(cid:5)1814, 2, 1787, 48, 31(cid:6)ACE (13 s)(cid:5)2606, 2, 2579, 48, 56(cid:6)ACE (27 s)(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M5.4E−051.00E+61.4E−−−021.00E+61.2E−028.51E+52.0E−025.41E+51.2E−031.79E+53.1E−−−031.90E+52.5E−−−031.18E+52.5E−−−039.23E+41.3E−−−039.35E+41.9E−−−032.62E+41.1E−−−035.64E+51.1E−−−023.51E+44.0E−021.04E+43.1E−−−021.29E+41.0E−−−026.02E+44.7E−−−031.13E+53.9E−−−021.08E+44.6E−−−081.6E−026.3E−−−036.5E−036.8E−038.8E−038.6E−038.4E−037.4E−037.0E−033.8E−024.4E−023.2E−−−023.5E−023.6E−021111111111111116.4E−023.0E−022.7E−022.3E−021.7E−02111113.8E−011114.0E−023.3E−025.6E−013.2E−015.3E−0218.3E−023.6E−062.17E+810101.4E−−−044.67E+72.1E−−−051.37E+7101010101010101010101010V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729721Fig. 10. Time versus Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-IS, IJGP-wc-SS, IJGP, EPIS and EDBP for two sampleFriends and Smokers networks.Fig. 11. Time versus Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-IS, IJGP-wc-SS, IJGP, EPIS and EDBP for two sampleMastermind networks.the relational consistency [6,59] power of IJGP to reduce rejection as a pre-processing step [10]. This highlights that some-times using constraint-based inference to determine the inconsistencies before sampling is more cost-effective to combiningsearch with sampling. Such constraint based inference schemes are however not scalable and as we can see they fail toyield even a single consistent sample for the larger instances (fs-19 to fs-29). Thus, to take advantage of larger sample size,we can use a simple strategy in which we run conventional sampling for a few minutes and resort to SampleSearch onlywhen pure sampling does not produce any consistent samples.On the mastermind networks, IJGP-wc-SS is the superior scheme followed by IJGP. EPIS fails to output even a singleconsistent sample in 3 hours on 6 out of the 7 instances while IJGP-wc-IS fails on all instances. EDBP is slightly worse thanIJGP on 5 out of the 6 instances. Figs. 10 and 11 show the anytime performance of the solvers demonstrating the clearsuperiority of IJGP-wc-SS.5.4.4. Grid networksThe grid Bayesian networks are available from the authors of Cachet [41]. A grid Bayesian network is a s × s grid, wherethere are two directed edges from a node to its neighbors right and down. The upper-left node is a source, and the bottom-right node is a sink. The sink node is the evidence node. The deterministic ratio p is a parameter specifying the fraction ofnodes that are deterministic (functional in this case), that is, whose values are a function of the values of their parents. Thegrid instances are designated as p-s. For example, the instance 50-18 indicates a grid of size 18 in which 50% of the nodesare deterministic or functional. Table 11 shows the Hellinger distance after 3 hours of CPU time for each solver. Time versusapproximation error plots are shown for six sample instances in Figs. 12 through 14.On grids with deterministic ratio of 50%, EPIS is the best performing scheme on all but two instances. On most instances,IJGP-wc-IS yields marginals having smaller error than IJGP-wc-SS. On four out of the six instances, the sampling schemes722V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Table 11Table showing the Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-SS, IJGP-wc-IS, IJGP, EPIS and EDBP for Grid networks after 3hours of CPU time. For IJGP-wc-IS and IJGP-wc-SS, we also report the number of consistent samples M generated in 3 hours.ProblemDeterministicratio = 50%50-12-550-14-550-15-550-17-550-18-550-19-5Deterministicratio = 75%75-16-575-17-575-18-575-19-575-20-575-21-575-22-575-23-575-24-575-26-5Deterministicratio = 90%90-20-590-22-590-23-590-24-590-25-590-26-590-34-590-38-5(cid:5)n, k, c, e, w(cid:6)ACE time(cid:5)144, 2, 62, 1, 16(cid:6)ACE (3 s)(cid:5)196, 2, 93, 1, 20(cid:6)ACE (3 s)(cid:5)225, 2, 111, 1, 23(cid:6)ACE (6 s)(cid:5)289, 2, 138, 1, 25(cid:6)ACE (211 s)(cid:5)324, 2, 153, 1, 27(cid:6)ACE (timeout)(cid:5)361, 2, 172, 1, 28(cid:6)ACE (timeout)(cid:5)256, 2, 193, 1, 24(cid:6)ACE (7 s)(cid:5)289, 2, 217, 1, 25(cid:6)ACE (9 s)(cid:5)324, 2, 245, 1, 27(cid:6)ACE (11 s)(cid:5)361, 2, 266, 1, 28(cid:6)ACE (14 s)(cid:5)400, 2, 299, 1, 30(cid:6)ACE (11 s)(cid:5)441, 2, 331, 1, 32(cid:6)ACE (60 s)(cid:5)484, 2, 361, 1, 35(cid:6)ACE (78 s)(cid:5)529, 2, 406, 1, 35(cid:6)ACE (420 s)(cid:5)576, 2, 442, 1, 38(cid:6)ACE (228 s)(cid:5)676, 2, 506, 1, 44(cid:6)ACE (timeout)(cid:5)400, 2, 356, 1, 30(cid:6)ACE (8 s)(cid:5)484, 2, 430, 1, 35(cid:6)ACE (7 s)(cid:5)529, 2, 468, 1, 35(cid:6)ACE (9 s)(cid:5)576, 2, 528, 1, 38(cid:6)ACE (6 s)(cid:5)625, 2, 553, 1, 39(cid:6)ACE (7 s)(cid:5)676, 2, 597, 1, 44(cid:6)ACE (10 s)(cid:5)1156, 2, 1048, 1, 65(cid:6)ACE (25 s)(cid:5)1444, 2, 1300, 1, 69(cid:6)ACE (136 s)IJGP-wc-SSIJGPEPISEDBPIJGP-wc-IS(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M4.3E−041.90E+64.9E−049.37E+54.9E−045.24E+58.0E−044.34E+59.3E−043.46E+51.1E−032.87E+56.5E−049.74E+51.4E−037.15E+51.2E−034.47E+59.0E−034.07E+56.2E−044.10E+51.9E−033.13E+53.2E−032.67E+52.0E−031.75E+58.4E−031.29E+52.4E−021.25E+51.6E−038.32E+54.6E−044.42E+52.8E−046.70E+55.0E−047.01E+52.7E−−−077.04E+51.0E−034.13E+58.6E−042.80E+51.6E−021.15E+53.2E−−−072.6E−042.5E−021.8E−021.2E−044.0E−021.0E−022.3E−046.1E−022.1E−022.0E−043.6E−031.9E−023.0E−−−042.1E−033.4E−024.0E−043.4E−−−042.5E−−−071.7E−047.8E−022.6E−−−072.7E−041.2E−033.9E−022.0E−045.0E−034.3E−022.5E−046.7E−−−053.1E−−−071.9E−041.7E−022.9E−−−072.8E−041.5E−022.3E−022.6E−−−042.0E−024.8E−022.3E−−−042.4E−024.3E−022.6E−−−043.5E−025.1E−023.5E−−−045.1E−022.7E−−−072.5E−043.7E−022.8E−−−071.5E−045.1E−023.2E−−−073.9E−041.9E−023.9E−−−073.5E−042.8E−022.7E−−−073.4E−044.6E−021.9E−−−062.3E−043.9E−021.8E−−−073.9E−044.1E−024.3E−−−071.7E−031.6E−011.5E−041.23E+82.1E−048.90E+76.5E−047.68E+71.0E−035.82E+77.6E−045.15E+71.5E−032.80E+71.4E−047.11E+71.6E−045.41E+71.9E−−−045.23E+71.9E−043.93E+72.8E−042.64E+76.2E−042.33E+75.4E−042.12E+77.1E−041.77E+78.9E−042.61E+71.4E−032.20E+76.5E−054.77E+71.0E−043.97E+77.0E−054.00E+79.2E−052.29E+72.7E−−−072.57E+71.9E−042.90E+76.3E−041.37E+71.0E−037.08E+6V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729723Fig. 12. Time versus Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-IS, IJGP-wc-SS, IJGP, EPIS and EDBP for two sampleGrid instances with deterministic ratio = 50%.Fig. 13. Time versus Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-IS, IJGP-wc-SS, IJGP, EPIS and EDBP for two sampleGrid instances with deterministic ratio = 75%.Fig. 14. Time versus Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-IS, IJGP-wc-SS, IJGP, EPIS and EDBP for two sampleGrid instances with deterministic ratio = 90%.724V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729Table 12Table showing the Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-SS, IJGP-wc-IS, IJGP, EPIS and EDBP for Logistics planninginstances after 3 hours of CPU time. For IJGP-wc-IS and IJGP-wc-SS, we also report the number of consistent samples M generated in 3 hours.Problemlog-1log-2log-3log-4log-5(cid:5)n, k, c, e, w(cid:6)ACE time(cid:5)4724, 2, 3785, 3785, 22(cid:6)ACE (1 s)(cid:5)26114, 2, 24777, 24777, 51(cid:6)ACE (58 s)(cid:5)30900, 2, 29487, 29487, 56(cid:6)ACE (23 s)(cid:5)23266, 2, 20963, 20963, 52(cid:6)ACE (68 s)(cid:5)32235, 2, 29534, 29534, 51(cid:6)ACE (727 s)IJGP-wc-SSIJGPEPISEDBPIJGP-wc-IS(cid:8)M(cid:8)M(cid:8)M(cid:8)M(cid:8)M2.2E−051.35E+88.6E−041.49E+61.2E−041.05E+52.3E−021.03E+58.6E−039.73E+3∗(2 s)09.8E−037.5E−031.8E−011.2E−0211111111111010101010yield smaller error than EDBP and IJGP. There is two orders of magnitude difference between IJGP-wc-SS and EDBP/IJGPwhile there is one order of magnitude difference between EPIS and IJGP-wc-IS and IJGP-wc-SS.On grids with deterministic ratio of 75%, IJGP is best on four out of the six smaller grids (up to size 21). EPIS dominateson the larger grids (size 22–26). IJGP-wc-IS is worse than IJGP on the smaller grids (up to size 21) but dominates IJGP onlarger grids. IJGP-wc-IS is consistently worse than EPIS and this is because of the overhead of the exact inference step ofw-cutset sampling and also because of the min-fill ordering used by IJGP-wc-IS. We found that the topological ordering(which is used by EPIS) performs better than the min-fill ordering. Specifically, we found that when we set w = 0 and usetopological ordering, the performance of IJGP-wc-IS and EPIS is almost the same (results not shown).On grids with deterministic ratio of 90%, IJGP is the superior scheme. IJGP-wc-IS is slightly better than EPIS which in turnis slightly better than IJGP-wc-SS. EDBP is the least accurate scheme. Again, we see that there is a two orders of magnitudedifference between the sample size of IJGP-wc-IS and IJGP-wc-SS.The poor performance of IJGP-wc-SS as compared with EPIS and IJGP-wc-IS is because of its search overhead. On gridnetworks, rejection is not an issue for the IJGP-wc-IS and EPIS solvers because the deterministic portion is easy for infer-ence. Although, it may seem on surface that both EPIS and IJGP-wc-IS do not reason about determinism, it is not the case.It is known that Loopy Belief propagation, when run until convergence makes the constraint portion of the mixed networkrelationally-arc-consistent [59]. Therefore, if the constraint network has small treewidth, Loopy BP (or IJGP) may yield aproposal distribution that is either backtrack-free or almost backtrack-free. Note however that enforcing relational consis-tency may reduce but would not completely eliminate the rejection problem. Typically, to guarantee that a backtrack-freedistribution is obtained, one has to use a consistency enforcement scheme whose time and space complexity is bounded bythe treewidth of the constraint portion of the mixed network (see [60], Chapter 2 for details). Overall, when the treewidthof the constraint portion is large, SampleSearch is the only practical alternative available to date.5.4.5. Logistics planning instancesOur last domain is that of logistics planning. Given prior probabilities on actions and facts, the task is to computemarginal distribution of each variable. Goals and initial conditions are observed true. Bayesian networks are generated fromthe plan graphs, where additional nodes (all observed false) are added to represent mutex, action-effect and preconditionsof actions. These benchmarks are available from the authors of Cachet [41].Table 12 summarizes the results. IJGP-wc-IS, EPIS and EDBP fail on all instances. IJGP solves the log-1 instance exactly” in Table 12 while on the remaining instances, IJGP-wc-SS is more accurate than IJGP. In Fig. 15, weas indicated by a “demonstrate the superior anytime performance of IJGP-wc-SS as compared with the other schemes.∗5.5. Impact of ordering heuristicsTable 13 shows the impact of using different variable ordering heuristics on the performance of IJGP-wc-SS measuredin terms of the Hellinger distance between the exact and the approximate marginals. For brevity, we show the results fora few sample instances from each domain. We can clearly see that except for the Grid instances, on average, the min-fillordering performs better than the other schemes. The topological ordering scheme performs the best on the grid instances.hmetis and min-degree ordering are the worst performing schemes.5.6. Summary of experimental evaluationWe implemented SampleSearch on top of an advanced importance sampling technique IJGP-wc-IS presented in [10];yielding a scheme called IJGP-wc-SS. The search was implemented using minisat [15]. For model counting, we comparedIJGP-wc-SS with three other approximate solution counters available in literature: ApproxCount [16], SampleCount [17] andV. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729725Fig. 15. Time versus Hellinger distance (cid:8) between the exact and approximate marginals for IJGP-wc-IS, IJGP-wc-SS, IJGP, EPIS and EDBP for two sampleLogistics planning instances.Table 13Table showing the effect of the four ordering heuristics: min-fill, min-degree, hmetis and topological on the Hellinger distance (cid:8) between the exact andapproximate marginals computed by IJGP-wc-SS. The time-bound used was 3 hours. The best performing scheme is highlighted in each row. For eachordering heuristic, we report its treewidth w.ProblemLinkageBN_69BN_70BN_75BN_76Grids50-18-550-19-575-24-575-26-590-34-590-38-5Relationalfs-28fs-29mm_06_08_03-0015mm_10_08_03-0015(cid:5)n, k, e(cid:6)(cid:5)777, 7, 78(cid:6)(cid:5)2315, 5, 159(cid:6)(cid:5)1820, 5, 155(cid:6)(cid:5)2155, 7, 169(cid:6)(cid:5)324, 2, 1(cid:6)(cid:5)361, 2, 1(cid:6)(cid:5)576, 2, 1(cid:6)(cid:5)676, 2, 1(cid:6)(cid:5)1156, 2, 1(cid:6)(cid:5)1444, 2, 1(cid:6)(cid:5)68698, 2, 67102(cid:6)(cid:5)76212, 2, 74501(cid:6)(cid:5)1814, 2, 48(cid:6)(cid:5)2606, 2, 48(cid:6)IJGP-wc-SSmin-fillmin-degreetopologicalhmetis(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w(cid:8)w9.4E−−−04392.6E−−−03358.0E−03371.8E−−−02379.3E−04271.1E−03284.3E−02382.4E−02448.6E−−−04651.6E−02691.1E−035271.9E−−−035594.7E−−−03313.9E−−−02562.0E−03381.4E−02562.2E−−−03411.5E−01407.1E−03271.3E−03273.8E−02408.0E−02481.6E−03651.6E−02691.3E−035272.1E−035596.1E−03316.5E−02564.7E−031227.5E−031442.1E−021783.2E−023333.3E−−−04213.5E−04211.9E−−−03378E−−−04381.4E−03513.0E−−−03566.4E−−−046326.8E−038031.9E−021738.8E−021852.2E−03398,3E−03515.5E−03461.8E−−−02402.3E−03301.8E−03282.2E−02384.5E−02469.4E−04614.5E−02692.7E−037193.4E−037998.5E−03355.6E−0248Relsat [18] as well as with IJGP-wc-IS on three benchmarks: (a) Latin Square instances, (b) Langford instances, and (c) FPGA-routing instances. We found that on most instances, IJGP-wc-SS yields solution counts which are closer to the true countsby a few orders of magnitude than those output by SampleCount and by several orders of magnitude than those output by726V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729ApproxCount and Relsat. IJGP-wc-IS fails to generate even a single consistent sample on all the SAT instances in 10 hoursof CPU time clearly demonstrating the usefulness of IJGP-wc-SS for deriving meaningful approximations in presence ofsignificant amount of determinism.For computing the probability of evidence in a Bayesian network and the partition function in a Markov network, wecompared IJGP-wc-SS with Variable Elimination and Conditioning (VEC) [19] and an advanced generalized belief propagationscheme called Edge Deletion Belief Propagation (EDBP) [20] on two benchmark domains: (a) linkage analysis and (b) rela-tional Bayesian networks. We found that on most instances the estimates output by IJGP-wc-SS were closer to the exactanswer than those output by EDBP. VEC solved some instances exactly, while on the remaining instances it was substan-tially inferior. IJGP-wc-IS was superior to IJGP-wc-SS whenever it was able to generate consistent samples. However, on amajority of the instances it simply failed to yield any consistent samples.For the posterior marginal task, we experimented with linkage analysis benchmarks, partially deterministic grid bench-marks, relational benchmarks and logistics planning benchmarks. We compared the accuracy of IJGP-wc-SS using theHellinger distance with four other schemes: two generalized belief propagation schemes of Iterative Join Graph Prop-agation [12] and Edge Deletion Belief Propagation [20], an adaptive importance sampling scheme called Evidence Pre-propagated Importance Sampling (EPIS) [23] and IJGP-wc-IS. We found that except on the grid instances, IJGP-wc-SSconsistently yielded estimates having smaller error than EDBP and IJGP. Whenever IJGP-wc-IS and EPIS did not fail, theygenerated more consistent samples and performed better than IJGP-wc-SS. On the remaining instances, IJGP-wc-SS wasclearly superior.6. ConclusionThe paper presented the SampleSearch scheme for improving the performance of importance sampling in mixed proba-bilistic and deterministic graphical models. It is well known that on such graphical models, importance sampling performsquite poorly because of the rejection problem. SampleSearch overcomes the rejection problem by interleaving random sam-pling with systematic backtracking. Specifically, when sampling variables one by one via logic sampling [5], instead ofrejecting a sample when its inconsistency is detected, SampleSearch backtracks to the previous variable, modifies the pro-posal distribution to reflect the inconsistency and continues this process until a consistent sample is found.We showed that SampleSearch can be viewed as a systematic search technique whose value selection is stochasticallyguided by sampling from a distribution. This view enables the integration of any systematic SAT/CSP solver within Sample-Search (with minor modifications). Indeed, in our experiments, we used an advanced SAT solver called minisat [15]. Thus,advances in the systematic search community whose primary focus is solving “yes/no” type NP-complete problems can beleveraged through SampleSearch for approximating much harder #P-complete problems in Bayesian inference.We characterized the sampling distribution of SampleSearch as the backtrack-free distribution, which is a modificationof the proposal distribution from which all inconsistent partial assignments along a specified order are removed. Whenthe backtrack-free probability for a given sampled assignment is too complex to compute, we proposed two approxima-tions, which bound the backtrack-free probability from above and below and yield asymptotically unbiased estimates of theweighted counts and marginals.We performed an extensive empirical evaluation on several benchmark graphical models and our results clearly demon-strate that our lower and upper approximations were accurate on most benchmarks. Overall SampleSearch was consistentlysuperior to other state-of-the-art schemes on domains having a substantial amount of determinism.Specifically, on probabilistic graphical models, we showed that state-of-the-art importance sampling techniques such asEPIS [23] and IJGP-wc-IS [10] which reason about determinism in a limited way are unable to generate a single consistentsample on several hard linkage analysis and relational benchmarks. In such cases, SampleSearch is the only alternativeimportance sampling technique to date.SampleSearch is also superior to generalized belief propagation schemes like Iterative Join Graph Propagation (IJGP) [12]and Edge Deletion Belief Propagation (EDBP) [20]. In theory, these propagation techniques are anytime, whose approximationquality can be improved by increasing their i-bound. However, their time and space complexity is exponential in i and inpractice their memory requirement becomes a major bottleneck beyond a certain i-bound (typically > 22). Consequently, onmost benchmarks, we observed that IJGP and EDBP quickly converge to an estimate which they are unable to improve withtime. On the other hand, as we demonstrated SampleSearch improves with time and yields superior anytime performancethan IJGP and EDBP.Finally, on the problem of counting solutions of a SAT/CSP, we showed that SampleSearch is slightly better than therecently proposed SampleCount [17] technique and substantially better than ApproxCount [16] and Relsat [18].SampleSearch leaves plenty of room for future improvements, which are likely to make it more cost effective in practice.For instance, to generate samples, we solve the same SAT/CSP problem multiple times. Therefore, various goods and no-goods (i.e. knowledge about the problem space) learnt while generating one sample may be used to speed-up the searchfor a solution while generating the next sample. How to achieve this in a principled and structured way is an importanttheoretical and practical question. Some initial related research on solving the similar SAT problems has appeared in thebounded model checking community [61] and can be applied to improve SampleSearch’s performance. A second line ofimprovement is a more efficient algorithm for compactly storing and combining various DFS traces used for deriving thelower and upper approximations. Currently, we store all DFS traces using an OR tree. Instead, we can easily use the AND/ORV. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729727search space [62]. Borrowing ideas from the literature on ordered binary decision diagrams (OBDDs) [63], we could evenmerge together isomorphic traces, and eliminate redundancy to further compact our representation. A third line of futureresearch is to use adaptive importance sampling [30,33,64]. In adaptive importance sampling, one updates the proposaldistribution based on the generated samples; so that with every update the proposal gets closer and closer to the desiredposterior distribution. Because we already store the DFS traces of the generated samples in SampleSearch, one could usethem to dynamically update and learn the proposal distribution.AcknowledgementsThis work was supported in part by the NSF under award number IIS-0713118 and by the NIH grant R01-HG004175-02.Appendix A. ProofsProof of Theorem 2. Because, Bxi−1i⊆ Axi−1N,i(cid:4)(cid:14)(cid:9)xi(cid:15)(cid:19)(cid:19)xi−1Q i(cid:2)xi−1N,i∪ C(cid:4), we have:(cid:14)(cid:15)(cid:19)(cid:19)xi−1(cid:9)xiQ i(cid:9)xixi−1∈Bi(cid:4)(cid:14)(cid:9)xi(cid:15)(cid:19)(cid:19)xi−1Q i(cid:9)xi∈Axi−1N,i(cid:4) 1 −∪Cxi−1N,i(cid:4)∴ 1 −(cid:14)(cid:9)xi(cid:15)(cid:19)(cid:19)xi−1Q ixi−1i(cid:9)∈BxiQ i(xi|xi−1)(cid:9)Q i(xxi−1ii∈B(cid:9)xi(cid:2)∴1 −|xi−1)(cid:2)1 −xi−1N,i(cid:9)xi∈A(cid:2)(cid:9)xi∈Axi−1N,i∪CQ i(xi|xi−1)xi−1N,ixi−1N,i∪C(cid:9)Q i(xi|xi−1)∴ Q Fn(cid:3)i (xi|xi−1) (cid:2) L FN,i(xi|xi−1)n(cid:3)Q Fi (xi|xi−1) (cid:2)N,i(xi|xi−1)L F∴i=1(cid:5)mi=1 F i(x)(cid:5)∴Q F (x)i=1∴ Q F (x) (cid:2) L FN (x)(cid:5)pmi=1 F i(x)j=1 C j(x)L FN (x)(cid:5)(cid:4)pj=1 C j(x)∴ 1NN(cid:4)k=1∴ w F (x) (cid:4) w Fw F(cid:14)(cid:15)xk(cid:4) 1N∴ (cid:8)Z N (cid:4) (cid:9)Z LNL (x)N(cid:4)k=1(cid:15)(cid:14)xkw FLSimilarly, by using Axi−1N,i⊆ Bxi−1i, it is easy to prove that (cid:8)Z FN(cid:2) (cid:9)Z UN . (cid:2)(40)(41)(42)(43)(44)(45)(46)(47)(48)(49)N and L FN in the limit of infinite samples coincide with theProof of Theorem 3. From Proposition 4, it follows that U Fbacktrack-free distribution Q F . Therefore,(cid:5)(cid:5)limN→∞w LN (x) = limN→∞(cid:5)mi=1 F i(x)=pj=1 C j(x)mi=1 F i(x)L FN (x)pj=1 C j(x)(cid:5)Q F (x)= w F (x)Therefore,EQlimN→∞(cid:22)1NN(cid:4)k=1(cid:23)w L(x)= limN→∞1N(cid:4)x∈Xw LN (x)Q (x)N(cid:4)(1)k=1= 1N× N limN→∞(cid:4)x∈Xw LN (x)Q (x)(50)(51)(52)(53)(54)728V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729w F (x)Q (x) . . .(cid:14)(cid:15)from Eq. (52)=(cid:4)x∈X= Z(55)(56)w USimilarly, we can prove that the estimator based on U FN (x) in Eqs. (53)–(56).Finally, because the estimates (cid:9)P UN (xi) and (cid:9)P Lasymptotically unbiased estimators, by definition, they are asymptotically unbiased too. (cid:2)N (xi) of P (xi) given in Eqs. (36) and (37) respectively are ratios of twoN in Eq. (34) is asymptotically unbiased by replacing w LN (x) with(cid:9)Proof of Theorem 4. Because we store all full solutions (x1, . . . , xn) and all partial assignments (x1, . . . , xi−1, xi) that wereproved inconsistent during the N executions of SampleSearch, we require an additional O (N × n × d) space to store thecombined sample tree used to estimate Z and the marginals. Similarly, because we compute a sum or their ratios byvisiting all nodes of this combined sample tree, the time complexity is also O (N × d × n). (cid:2)References[1] R. Dechter, D. Larkin, Hybrid processing of beliefs and constraints, in: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence (UAI),2001, pp. 112–119.[2] D. Larkin, R. Dechter, Bayesian inference in the presence of determinism, in: Tenth International Workshop on Artificial Intelligence and Statistics(AISTATS), 2003.[3] R. Dechter, R. Mateescu, Mixtures of deterministic–probabilistic networks and their AND/OR search space, in: Proceedings of the 20th Annual Confer-ence on Uncertainty in Artificial Intelligence (UAI), 2004, pp. 120–129.[4] R. Mateescu, R. Dechter, Mixed deterministic and probabilistic networks, Annals of Mathematics and Artificial Intelligence (AMAI), Special Issue: Prob-abilistic Relational Learning 54 (1–3) (2008) 3–51.[5] J. Pearl, Probabilistic Reasoning in Intelligent Systems, Morgan Kaufmann, 1988.[6] R. Dechter, Constraint Processing, Morgan Kaufmann, 2003.[7] A.W. Marshall, The use of multi-stage sampling schemes in Monte Carlo computations, in: Symposium on Monte Carlo Methods, 1956, pp. 123–140.[8] R.Y. Rubinstein, Simulation and the Monte Carlo Method, John Wiley & Sons Inc., 1981.[9] J. Geweke, Bayesian inference in econometric models using Monte Carlo integration, Econometrica 57 (6) (1989) 1317–1339.[10] V. Gogate, R. Dechter, Approximate inference algorithms for hybrid Bayesian networks with discrete constraints, in: Proceedings of the 21st AnnualConference on Uncertainty in Artificial Intelligence (UAI), 2005, pp. 209–216.[11] J.S. Yedidia, W.T. Freeman, Y. Weiss, Constructing free energy approximations and generalized belief propagation algorithms, IEEE Transactions onInformation Theory 51 (2004) 2282–2312.[12] R. Dechter, K. Kask, R. Mateescu, Iterative join graph propagation, in: Proceedings of the 18th Conference in Uncertainty in Artificial Intelligence (UAI),Morgan Kaufmann, 2002, pp. 128–136.[13] R. Mateescu, K. Kask, V. Gogate, R. Dechter, Join-graph propagation algorithms, Journal of Artificial Intelligence Research 37 (2009) 279–328.[14] B. Bidyuk, R. Dechter, Cutset sampling for Bayesian networks, Journal of Artificial Intelligence Research (JAIR) 28 (2007) 1–48.[15] N. Sorensson, N. Een, Minisat v1.13–A SAT solver with conflict–clause minimization, in: SAT 2005 Competition, 2005.[16] W. Wei, J. Erenrich, B. Selman, Towards efficient sampling: exploiting random walk strategies, in: Proceedings of the Nineteenth National Conferenceon Artificial Intelligence, 2004, pp. 670–676.[17] C.P. Gomes, J. Hoffmann, A. Sabharwal, B. Selman, From sampling to model counting, in: Proceedings of the 20th International Joint Conference onArtificial Intelligence (IJCAI), 2007, pp. 2293–2299.[18] J. Roberto, J. Bayardo, J.D. Pehoushek, Counting models using connected components, in: Proceedings of 17th National Conference on Artificial Intelli-gence (AAAI), 2000, pp. 157–162.[19] R. Dechter, Bucket elimination: A unifying framework for reasoning, Artificial Intelligence 113 (1999) 41–85.[20] A. Choi, A. Darwiche, An edge deletion semantics for belief propagation and its practical impact on approximation quality, in: Proceedings of theTwenty-First National Conference on Artificial Intelligence (AAAI), 2006, pp. 1107–1114.[21] M. Fishelson, D. Geiger, Optimizing exact genetic linkage computations, in: Proceedings of the Seventh Annual International Conference on Research inComputational Molecular Biology (RECOMB), 2003, pp. 114–121.[22] M.D. Chavira, A. Darwiche, M. Jaeger, Compiling relational Bayesian networks for exact inference, International Journal of Approximate Reason-ing 42 (1–2) (2006) 4–20.[23] C. Yuan, M.J. Druzdzel, Importance sampling algorithms for Bayesian networks: Principles and performance, Mathematical and Computer Modelling(ISSN 0895-7177) 43 (90–10) (2006) 1189–1207.[24] V. Gogate, R. Dechter, SampleSearch: A scheme that searches for consistent samples, in: Proceedings of the 11th Conference on Artificial Intelligenceand Statistics (AISTATS), 2007, pp. 147–154.[25] V. Gogate, R. Dechter, Approximate counting by sampling the backtrack-free search space, in: Proceedings of 22nd Conference on Artificial Intelligence(AAAI), 2007, pp. 198–203.[26] J. Liu, Monte-Carlo Strategies in Scientific Computing, Springer-Verlag, New York, 2001.[27] E.C. Freuder, A sufficient condition for backtrack-free search, Journal of the ACM 29 (1) (1982) 24–32.[28] T. Walsh, SAT v CSP, in: Proceedings of the 6th International Conference on Principles and Practice of Constraint Programming, Springer-Verlag, London,UK, ISBN 3-540-41053-8, 2000, pp. 441–456.[29] K. Pipatsrisawat, A. Darwiche, RSAT 2.0: SAT solver description, Tech. Rep. D-153, Automated Reasoning Group, Computer Science Department, UCLA,2007.[30] J. Cheng, M.J. Druzdzel, AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks, Journal of ArtificialIntelligence Research (JAIR) 13 (2000) 155–188.[31] R.D. Shachter, M.A. Peot, Simulation approaches to general probabilistic inference on belief networks, in: Proceedings of the Fifth Annual Conferenceon Uncertainty in Artificial Intelligence (UAI), 1990, pp. 221–234.[32] R.M. Fung, K.-C. Chang, Weighing and integrating evidence for stochastic simulation in Bayesian networks, in: Proceedings of the Fifth Annual Confer-ence on Uncertainty in Artificial Intelligence (UAI), 1990, pp. 209–220.[33] L. Ortiz, L. Kaelbling, Adaptive importance sampling for estimation in structured domains, in: Proceedings of the 16th Annual Conference on Uncertaintyin Artificial Intelligence (UAI), 2000, pp. 446–454.V. Gogate, R. Dechter / Artificial Intelligence 175 (2011) 694–729729[34] R. Fung, B. del Favero, Backward simulation in Bayesian networks, in: Proceedings of the Tenth Annual Conference on Uncertainty in Artificial Intelli-gence (UAI), 1994, pp. 227–234.[35] K. Kask, R. Dechter, J. Larrosa, A. Dechter, Unifying tree decompositions for reasoning in graphical models, Artificial Intelligence 166 (1–2) (2005)165–193.[36] G. Casella, C.P. Robert, Rao-blackwellisation of sampling schemes, Biometrika 83 (1) (1996) 81–94, doi:10.1093/biomet/83.1.81.[37] A. Darwiche, M. Hopkins, Using recursive decomposition to construct elimination orders, jointrees, and dtrees, in: Trends in Artificial Intelligence, in:Lecture Notes in AI, Springer-Verlag, 2001, pp. 180–191.[38] B. Bidyuk, R. Dechter, On finding minimal w-cutset problem, in: Proceedings of the 20th Conference in Uncertainty in Artificial Intelligence (UAI),2004, pp. 43–50.[39] W. Wei, B. Selman, A new approach to model counting, in: Proceedings of Eighth International Conference on Theory and Applications of SatisfiabilityTesting (SAT), 2005, pp. 324–339.[40] L.G. Valiant, The complexity of enumeration and reliability problems, SIAM Journal of Computation 8 (3) (1987) 105–117.[41] T. Sang, P. Beame, H. Kautz, Heuristics for fast exact model counting, in: Eighth International Conference on Theory and Applications of SatisfiabilityTesting (SAT), 2005, pp. 226–240.[42] B. Selman, H. Kautz, B. Cohen, Noise strategies for local search, in: Proceedings of the Eleventh National Conference on Artificial Intelligence, 1994,pp. 337–343.[43] K.P. Murphy, Y. Weiss, M.I. Jordan, Loopy belief propagation for approximate inference: an empirical study, in: Proceedings of the Fifteenth Conferenceon Uncertainty in Artificial Intelligence (UAI), 1999, pp. 467–475.[44] A. Darwiche, R. Dechter, A. Choi, V. Gogate, L. Otten, Results from the probabilistic inference evaluation of UAI’08, available online at: http://graphmod.ics.uci.edu/uai08/Evaluation/Report, 2008.[45] R. Dechter, V. Gogate, L. Otten, R. Marinescu, R. Mateescu, Graphical model algorithms at UC Irvine, website: http://graphmod.ics.uci.edu/group/Software, 2009.[46] A. Darwiche, A differential approach to inference in Bayesian networks, Journal of the ACM 50 (3) (2003) 280–305.[47] A. Darwiche, New advances in compiling CNF into decomposable negation normal form, in: Proceedings of the 16th European Conference on ArtificialIntelligence (ECAI), 2004, pp. 328–332.[48] A. Darwiche, P. Marquis, A knowledge compilation map, Journal of Artificial Intelligence Research (JAIR) 17 (2002) 229–264.[49] C. Gomes, D. Shmoys, Completing quasigroups or Latin squares: a structured graph coloring problem, in: Proceedings of the Computational Symposiumon Graph Coloring and Extensions, 2002.[50] T. Ritter, Latin squares: A literature survey, available online at: http://www.ciphersbyritter.com/RES/LATSQ.HTM.[51] T. Walsh, Permutation problems and channelling constraints, in: Proceedings of the 8th International Conference on Logic Programming and AutomatedReasoning (LPAR), 2001, pp. 377–391.[52] V. Gogate, B. Bidyuk, R. Dechter, Studies in lower bounding probability of evidence using the Markov inequality, in: Proceedings of 23rd Conference onUncertainty in Artificial Intelligence (UAI), 2007, pp. 141–148.[53] L. Simon, D.L. Berre, E. Hirsch, The SAT 2002 competition, Annals of Mathematics and Artificial Intelligence (AMAI) 43 (2005) 307–342.[54] J. Ott, Analysis of Human Genetic Linkage, The Johns Hopkins University Press, Baltimore, Maryland, 1999.[55] J. Bilmes, R. Dechter, Evaluation of probabilistic inference systems of UAI’06, available online at http://ssli.ee.washington.edu/bilmes/uai06InferenceEvaluation/, 2006.[56] K. Kask, R. Dechter, A. Gelfand, BEEM: bucket elimination with external memory, in: 26th Conference on Uncertainty in Artificial Intelligence (UAI),2010, pp. 268–276.[57] G. Kokolakis, P. Nanopoulos, Bayesian multivariate micro-aggregation under the Hellinger distance criterion, Research in Official Statistics 4 (2001)117–125.[58] S. Kullback, R.A. Leibler, On information and sufficiency, The Annals of Mathematical Statistics 22 (1) (1951) 79–86.[59] R. Dechter, R. Mateescu, A simple insight into iterative belief propagation’s success, in: Proceedings of the 19th Conference in Uncertainty in ArtificialIntelligence (UAI), 2003, p. 175–183.[60] V. Gogate, Sampling algorithms for probabilistic and deterministic graphical models, PhD thesis, University of California, Irvine, 2009.[61] N. Eén, N. Sörensson, Temporal induction by incremental SAT solving, Electronic Notes in Theoretical Computer Science (ISSN 1571-0661) 89 (4) (2003)543–560.[62] R. Dechter, R. Mateescu, AND/OR search spaces for graphical models, Artificial Intelligence 171 (2–3) (2007) 73–106.[63] R.E. Bryant, Graph-based algorithms for Boolean function manipulation, IEEE Transactions on Computers 35 (8) (1986) 677–691.[64] S. Moral, A. Salmerón, Dynamic importance sampling in Bayesian networks based on probability trees, International Journal of Approximate Reason-ing 38 (3) (2005) 245–261.