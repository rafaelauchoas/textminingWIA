Artificial Intelligence 184–185 (2012) 38–77Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintImportance sampling-based estimation over AND/OR search spacesfor graphical modelsVibhav Gogate a,∗, Rina Dechter a,ba Department of Computer Science, The University of Texas at Dallas, Richardson, TX 75080, USAb Donald Bren School of Information and Computer Sciences, University of California, Irvine, CA 92697, USAa r t i c l ei n f oa b s t r a c tIt is well known that the accuracy of importance sampling can be improved by reducingthe variance of its sample mean and therefore variance reduction schemes have beenthe subject of much research. In this paper, we introduce a family of variance reductionschemes that generalize the sample mean from the conventional OR search space to theAND/OR search space for graphical models. The new AND/OR sample means allow tradingtime and space with variance. At one end is the AND/OR sample tree mean which has thesame time and space complexity as the conventional OR sample tree mean but has smallervariance. At other end is the AND/OR sample graph mean which requires more time andspace to compute but has the smallest variance. Theoretically, we show that the varianceis smaller in the AND/OR space because the AND/OR sample mean is defined over a largervirtual sample size compared with the OR sample mean. Empirically, we demonstrate thatthe AND/OR sample mean is far closer to the true mean than the OR sample mean.© 2012 Elsevier B.V. All rights reserved.Article history:Received 23 March 2010Received in revised form 24 February 2012Accepted 1 March 2012Available online 3 March 2012Keywords:Probabilistic inferenceApproximate inferenceGraphical modelsImportance samplingBayesian networksConstraint networksMarkov networksModel countingVariance reduction1. IntroductionImportance sampling [1,2] is a general scheme that can be used to approximate various weighted counting tasks definedover graphical models such as computing the probability of evidence in a Bayesian network, computing the partition func-tion of a Markov network and counting the number of solutions of a constraint network. The main idea is to transform thecounting or the summation task into an expectation using a special distribution called the proposal distribution. Then, thealgorithm generates samples from the proposal distribution and approximates the expectation by a weighted average overthe samples. The weighted average is often called the sample mean. It is well known that the accuracy of the estimate isinversely proportional to the variance of the sample mean and therefore significant research has focused on reducing itsvariance [2,3]. To this effect, in this paper, we propose a family of variance reduction schemes in the context of graphicalmodels called AND/OR importance sampling.The central idea in AND/OR importance sampling is to exploit problem decomposition introduced by the conditional in-dependencies in the graphical model. Recently, graph-based problem decompositions were introduced for systematic searchin graphical models [4,5] and captured using the notion of AND/OR search spaces [6]. The usual way of performing search isto systematically go over all possible instantiations of the variables, which can be organized in an OR search tree. In AND/ORsearch, additional AND nodes are interleaved with OR nodes to capture decomposition into conditionally independent sub-problems.* Corresponding author.E-mail addresses: vgogate@hlt.utdallas.edu (V. Gogate), dechter@ics.uci.edu (R. Dechter).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2012.03.001V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7739We propose to organize the generated samples as a partial cover of a full AND/OR search tree yielding an AND/OR sampletree. Likewise, the OR sample tree is the portion of a full OR search tree that is covered by the samples. The main intuitionin moving from the OR space to the AND/OR space is that at the AND nodes, we can combine samples in independentcomponents to yield a virtual increase in the sample size. For example, if X is conditionally independent of Y given Z , wecan consider N samples of X independently from those of Y given the same value of Z , thereby yielding an effective orvirtual sample size of N 2 instead of the input N. Since the variance reduces as the number of samples increases (cf. [2,3]),the sample mean computed over the AND/OR sample tree has smaller variance than the one computed over the OR sampletree.We can take this idea a step further and look at the AND/OR search graph [6] as the target for compiling the givenset of samples. Since the AND/OR search graph captures more conditional independencies than the AND/OR search tree,its partial cover corresponding to the generated samples, yields an even larger virtual sample size. As a result, the samplemean computed over the AND/OR sample graph has smaller variance than the one computed over the AND/OR sample tree.∗) time-wise and a factor ofHowever, computing the AND/OR sample graph mean is more expensive, by a factor of O (wO (N) space wise, wbeing the treewidth and N being the number of samples. Thus, the AND/OR sample tree and graphmeans allow trading time and space with accuracy.∗We provide a thorough empirical evaluation comparing the impact of exploiting varying levels of problem decom-positions via AND/OR tree and AND/OR graph on a variety of probabilistic and deterministic benchmark networks. Ourexperiments demonstrate that the AND/OR sample tree mean is slightly better than the (conventional) OR sample treemean in terms of accuracy and that the AND/OR sample graph mean is clearly superior to the AND/OR sample tree mean.The rest of the paper is organized as follows. In the next section, we present preliminaries and background. In Section 3we define the AND/OR sample tree mean and in Section 4 we prove that it has smaller variance than the OR sample treemean. The AND/OR sample graph mean is defined in Section 5. Section 6 presents empirical results and we conclude inSection 7. The research presented in this paper is based in part on Gogate and Dechter [7,8].2. Preliminaries and backgroundWe denote variables by upper case letters (e.g., X, Y , . . .) and values of variables by lower case letters (e.g., x, y, . . .).Sets of variables are denoted by bold upper case letters (e.g., X = { X1, . . . , Xn}). We denote by D( Xi) the set of possiblevalues of Xi . D( Xi) is also called the domain of Xi . Xi = xi or simply xi when the variable is clear from the context, denotesthe assignment of xi ∈ D( Xi) to Xi while X = x (or simply x) denotes a sequence of assignments to all variables in X,namely x = ( X1 = x1, X2 = x2, . . . , Xn = xn). D(X) denotes the Cartesian product of the domains of all variables in X, namelyD(X) = D( X1) × · · · × D( Xn). We denote the projection of an assignment x to a set S ⊆ X by xS. Given an assignment y andz to the partition Y and Z of X, x = (y, z) denotes the composition of assignments to the two subsets.(cid:2)(cid:2)x∈D(X)x∈D(X) asExQ [ X] of a random variable X with respect to a distribution Q is defined as: ExQ [ X] =VarQ [ X] of X is defined as: VarQ [ X] =VarQ [ X] as Var[ X], when the identity of Q is clear from the context.· · ·x∈X. The expected valuex∈ X xQ (x). The variancex∈ X (x − ExQ [ X])2 Q (x). To simplify, we will write ExQ [ X] as Ex[ X] andx∈D(X) denotes the sum over all possible configurations of variables in X, namely,(cid:2)xn∈D( Xn). For brevity, we will abuse notation and writexi ∈D( Xi ) asx2∈D( X2)x1∈D( X1)=(cid:2)(cid:2)xi ∈ Xiand(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)We denote (discrete) functions by upper case letters (e.g. F , H , C , I , etc.), and the scope (set of arguments) of a functionF by scope(F ). Given an assignment y to a superset Y of scope(F ), we will abuse notation and write F (yscope(F )) as F (y).Definition 1 (Graphical models or Markov networks). A discrete graphical model or a Markov network denoted by G is a 3-tuple (cid:5)X, D, F(cid:6) where X = { X1, . . . , Xn} is a finite set of variables, D = {D( X1), . . . , D( Xn)} is a finite set of domains whereD( Xi) is the domain of variable Xi and F = {F 1, . . . , Fm} is a finite set of discrete-valued non-negative functions (also calledpotentials). The graphical model represents a joint distribution P G over X defined as:P G(x) = 1Zm(cid:3)i=1F i(x)where Z is a normalization constant, often called the partition function. It is given by:Z =(cid:4)m(cid:3)x∈Xi=1F i(x)We will often refer to Z as weighted counts.(1)(2)The primary queries over Markov networks are computing the partition function (or the weighted counts) and computingThe marginal probability P G ( Xi = xi) is a ratio of two weighted counts. Formally, let I xi be a Dirac-delta function withthe marginal probability P G ( Xi = xi).scope Xi , defined as follows:40V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77(cid:5)I xi (x) =1 if x = xi0 otherwiseThen, by definition, P G (xi) is given by:(cid:2)P G(xi) =I xi (x)P G(x) =(cid:4)x∈X(cid:6)mj=1 F j(x)x∈X I xi (x)Z(3)Notice that the numerator of Eq. (3) is the weighted counts of a graphical model obtained by augmenting G with I xi . Thusalgorithms for computing the weighted counts can be used for computing P G (xi).Each graphical model is associated with a primal graph which captures the dependencies present in the model.Definition 2 (Primal graph). The primal graph of a graphical model G = (cid:5)X, D, F(cid:6) is an undirected graph G(X, E) which hasvariables of G as its vertices and an edge between two variables that appear in the scope of a function.2.1. Bayesian and constraint networksDefinition 3 (Bayesian or belief networks). A Bayesian network is a graphical model B = (cid:5)X, D, G, P(cid:6) where G = (X, E) isa directed acyclic graph over the set of variables X. Each function P i ∈ P is a conditional probability table defined asP i( Xi|pai), where pai= scope(P i) \ { Xi} is the set of parents of Xi in G.The primal graph of a Bayesian network is also called the moral graph. When the entries of a CPT are 0 and 1 only,it is called a deterministic or a functional CPT. An evidence E = e is an instantiated subset of variables. A Bayesian networkrepresents the following joint probability distribution:P B(x) =n(cid:3)i=1P i(x{Xi }|xpai )By definition, given a Bayesian network B the probability of evidence P B(e) is given by:P B(e) =(cid:4)n(cid:3)(cid:7)P i(y, e){Xi }(cid:8)(cid:8)(y, e)pai(cid:9)y∈X\Ei=1(4)(5)It is easy to see from Eqs. (2) and (5) that P B(e) is equivalent to the weighted counts Z over an evidence instantiatedBayesian network. Another important query over a Bayesian network is computing the conditional marginal probabilityP B(xi|e) for a query variable Xi ∈ X \ E.Definition 4 (Constraint networks). A constraint network is a graphical model R = (cid:5)X, D, C(cid:6) where C = {C1, . . . , Cm} is a setof constraints. Each constraint Ci is a 0/1 function defined over its scope. Given an assignment x, a constraint is said to besatisfied if Ci(x) = 1. A constraint can also be expressed by a pair (cid:5)R i, Si(cid:6) where R i is a relation defined over the scope ofCi that contains all tuples for which Ci(si) = 1. The primal graph of a constraint network is called the constraint graph.A solution of a constraint network is an assignment of values to all variables that satisfies all the constraints. The primaryquery over a constraint network is to determine whether it has a solution and if it does to find one. Another importantquery is that of counting the number of solutions K of the constraint network, defined by:K =(cid:4)m(cid:3)x∈Xi=1Ci(x)(6)K is clearly identical to the weighted counts over a constraint network.The Boolean satisfiability problem defines a special type of constraint network in which all variables Xi ∈ X are binarywith domain {0, 1} (or {False, True}) and all constraints are specified using clauses. A clause is a disjunction of literals, wherea literal is a variable or its negation. For example, ( X1 ∨ X2 ∨ ¬ X3) is a clause defined over three literals X1, X2 and ¬ X3,where ¬ denotes negation. Given an assignment, a clause is satisfied when at least one of its literals is set to True. Forexample, the clause ( X1 ∨ X2 ∨ ¬ X3) is satisfied given the assignment ( X1 = 0, X2 = 0, X3 = 0), but is not satisfied giventhe assignment ( X1 = 0, X2 = 0, X3 = 1).V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77412.2. AND/OR search spaces for graphical modelsFig. 1. AND/OR search spaces for graphical models.Given a graphical model G = (cid:5)X, D, F(cid:6), we can compute the weighted counts by accumulating the probabilities (or theweights) while traversing the search space of instantiated variables. In the simplest case, the algorithm traverses an ORsearch tree, whose nodes represent states in the space of partial assignments. This traditional search space does not captureany of the structural properties of the underlying graphical model, however. Introducing AND nodes into the search spacecan capture the conditional independencies in the graphical model.The AND/OR search space is a well-known problem solving approach developed in the area of heuristic search [9] thatexploits the problem structure to decompose the search space. The AND/OR search space for graphical models was intro-duced in Dechter and Mateescu [6]. It is guided by a pseudo tree that spans the original graphical model.Definition 5 (Pseudo tree). Given an undirected graph G = (V, Eis called a pseudo tree if every edge in E(cid:8) \ E is a back-arc, namely it connects a node to an ancestor in T .(cid:8)), a directed rooted tree T = (V, E) defined on all its nodesDefinition 6 (AND/OR search tree). Given a graphical model G = (cid:5)X, D, F(cid:6), its primal graph G and a pseudo tree T of G, theassociated AND/OR search tree, denoted by ψT , has alternating levels of AND and OR nodes. The OR nodes are labeledwith Xi and correspond to the variables. The AND nodes are labeled with xi and correspond to the value assignments. Thestructure of ψT is based on T . Its root is an OR node labeled by the root of T . The children of an OR node Xi are ANDnodes labeled with assignments xi . The children of an AND node xi are OR nodes labeled with the children of Xi in T .Semantically, the OR nodes represent alternative assignments, whereas the AND nodes represent problem decompositioninto independent subproblems, all of which need to be solved. When the pseudo tree is a chain, the AND/OR search treecoincides with the regular OR search tree.Definition 7 (Solution subtree). A solution subtree of an AND/OR search tree (or graph) contains the root node. For every ORnode it contains one of its child nodes and for each of its AND nodes it contains all its child nodes.Example 1. Fig. 1(a) shows a constraint network for a 3-coloring problem over 4 variables. A possible pseudo tree for theconstraint network is given in Fig. 1(b). Fig. 1(c) shows an OR search tree. Fig. 1(d) shows an AND/OR search tree guided bythe pseudo tree given in Fig. 1(b).42V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Fig. 2. Assigning weights to OR-to-AND arcs of an AND/OR search tree.To compute the weighted counts using an AND/OR search tree, all we need is to annotate the OR-to-AND arcs withweights derived from the functions F, such that the product of weights on the arcs of any solution subtree, i.e. a fullassignment x, is equal tomi=1 F i(x). We can formalize this using the notion of a weighted AND/OR tree [6].(cid:6)Definition 8 (Weighted AND/OR tree). Given a graphical model G = (cid:5)X, D, F(cid:6) and its AND/OR search tree along a pseudo treeT , the weight w(a, b) of an arc from an OR node a to an AND node b such that a is labeled with Xi and b is labeled with xi ,is the product of all functions F ∈ F which become fully instantiated by the last assignment from the root to Xi . A weightedAND/OR tree is the AND/OR tree annotated with weights.Example 2. Fig. 2(a) shows a Bayesian network, Fig. 2(b) shows a pseudo tree, and Fig. 2(c) shows the conditional probabilitytables. Fig. 2(d) shows the weighted AND/OR search tree based on the pseudo tree and the Bayesian network. Note that allAND children of OR nodes having an edge label of zero are not drawn (and are not extended). Functions having zeros intheir range express the notion of inconsistent assignments.The weighted counts can be computed by traversing the weighted AND/OR tree in a DFS manner and computing thevalue of all nodes from leaves to the root [6], as defined next. The value of a node is the weighted counts of the subtreethat it roots.Definition 9 (Value of a node for computing the weighted counts). The value of a node is defined recursively as follows. Thevalue of a leaf AND node is “1”. Let chi(n) denote the set of child nodes of a node n and let v(n) denote its value. If n is anOR node then:v(n) =(cid:7)n, n(cid:7)n(cid:4)wv(cid:9)(cid:9)(cid:8)(cid:8)n(cid:8)∈chi(n)V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7743where w(n, n(cid:8)) is the weight of the arc from the OR node n to its child node n(cid:8). If n is an AND node then:v(n) =(cid:3)vn(cid:8)∈chi(n)(cid:9)(cid:7)n(cid:8)Proposition 1. The value of the root node of a weighted AND/OR tree is equal to the weighted counts.Proof. See [6] for a proof. (cid:2)A node n in an AND/OR search tree represents a subproblem of the graphical model restricted to the assignment of valuesalong the path from the root to n. The AND/OR search tree may contain nodes that root identical subproblems. These nodesare unifiable and can be merged yielding a search graph whose size is smaller than the AND/OR search tree. Traversingthe AND/OR search graph requires additional memory, however. A depth first search algorithm can cache previously com-puted results and retrieve them when the same subproblem is encountered. Some unifiable nodes can be identified basedon their context which express the set of ancestor variables in the pseudo tree that completely determine a conditionedsubproblem [6].(cid:8)), the context of a node Xi in T denoted byDefinition 10 (Context). Given a pseudo tree T (X, E) of a primal graph G(X, EcontextT ( Xi) is the set of ancestors of Xi in T , ordered descendingly, that are connected in G to Xi or to descendants of Xi .Example 3. Fig. 1(b) shows a pseudo tree in which each node is annotated with its context. The context of the nodes C , B,D, A is ∅, {C}, {C} and {B} respectively.Definition 11 (Context minimal AND/OR graph). Given an AND/OR search tree, two OR nodes n1 and n2 are context unifiableif they have the same variable label Xi and the assignments of their contexts are identical. In other words, if y and z denotethe partial assignment of variables along the path from the root to n1 and n2 respectively, and if their restriction to thecontext of Xi satisfies ycontextT ( Xi ) = zcontextT ( Xi ), then n1 and n2 are unifiable. The context minimal AND/OR graph is obtainedfrom the AND/OR search tree by merging all the context unifiable OR nodes.Example 4. Fig. 1(e) shows a context minimal AND/OR graph constructed from the AND/OR tree of Fig. 1(d) by merging allcontext unifiable nodes.2.3. Importance sampling for approximating the weighted countsImportance sampling [1,10] is a Monte Carlo simulation technique which can be used for estimating the sum of afunction F over a domain. The main idea is to express the sum as an expectation using an easy-to-sample distribution Q ,which is called the proposal (or trial or importance) distribution. Then, we generate samples from Q and estimate theexpectation (which equals the sum) by a weighted average over the samples, where the weight of a sample x is F (x)/Q (x).The weighted average is often called the sample mean.Following prior work (cf. [11]), we assume that the proposal distribution is specified in a product form along an or-dering o = ( X1, . . . , Xn) of variables. Namely, Q (x) =i=1 Q i(xi|x1, . . . , xi−1). Q is thus a Bayesian network with CPTs{Q 1, . . . , Q n}. Q is often expressed as a Bayesian network because it is easy to generate samples from it using the followinglogic sampling scheme [12]:(cid:6)ni=1 Q i (xi |x1, . . . , xi−1).(cid:6)nAlgorithm 1: Logic Sampling1 Input: A distribution Q (x) =2 Output: An assignment x sampled from Q .3 begin45678Sample xi from Q i ( Xi |x)x = (x, xi )x = ∅For i = 1 to n do:Return x9 endMoreover, we assume that each CPT Q i of Q can be specified in polynomial space. Formally,n(cid:3)Q (x) =Q i(xi|x1, . . . , xi−1) =n(cid:3)Q i(xi|yi)i=1where Yi ⊆ { X1, . . . , Xi−1} and for all i, |Yi| is assumed to be bounded by a constant.1i=1(7)1 Let p = maxi |Y i |. Then, the time complexity of generating a sample from Q is O (np) where n is the number of variables.44V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Definition 12 (Unbiased and asymptotically unbiased estimate). Given a probability distribution Q and a quantity θ definedover Q , an estimate θ N which is function of N random samples drawn from Q , is an unbiased estimate of θ if ExQ [θ N ] = θ .θ N is an asymptotically unbiased estimate of θ if limN→∞ ExQ [θ N ] = θ .By definition, an unbiased estimate of θ is also asymptotically unbiased. However, the converse is not true. We willdenote estimates of θ by either drawing a hat or a line over it (e.g., θ , (cid:10)θ ). The notion of unbiasedness characterizes theperformance of an estimator in terms of its mean squared error (MSE).(cid:11)MSEQ [θ ] = ExQ(cid:11)ExQ=(θ − θ)2(cid:12)(cid:11)θ 2(cid:12)− ExQ [θ ]2(cid:12)(cid:11)ExQ [θ]2 − 2ExQ [θ ]θ + θ 2(cid:12)+The bias of θ is defined as:BiasQ [θ] = ExQ [θ ] − θThe variance of θ is defined as:VarQ [θ ] = ExQ(cid:12)(cid:11)θ 2− ExQ [θ ]2From the definitions of bias, variance and mean-squared error, we have:MSEQ [θ ] = VarQ [θ ] + BiasQ [θ ]2(8)(9)(10)In other words, the mean squared error of an estimator is equal to the bias squared plus the variance. An unbiased estimatorhas zero bias. Therefore one can reduce its MSE (or any estimator that has a constant bias) by reducing its variance.Next, we show how the weighted counts Z can be estimated via importance sampling. Consider the expression for Z(see Eq. (2)):Z =(cid:4)m(cid:3)x∈Xi=1F i(x)Given a proposal distribution Q such thatZ =(cid:4)x∈X(cid:6)mi=1 F i(x)Q (x)Q (x) = ExQ(cid:6)(cid:13) (cid:6)mi=1 F i(x) > 0 ⇒ Q (x) > 0, we can rewrite Eq. (11) as follows:mi=1 F i(x)Q (x)(cid:14)Given independent and identically distributed (i.i.d.) samples (x1, . . . , xN ) generated from Q , we can estimate Z by:(cid:10)Z N = 1NN(cid:4)k=1(cid:6)mi=1 F i(xk)Q (xk)= 1NN(cid:4)k=1(cid:9)(cid:7)xkwwherew(x) =(cid:6)mi=1 F i(x)Q (x)(11)(12)(13)is the weight of sample x. It is easy to show that (cid:10)Z N is unbiased, namely ExQ [(cid:10)Z N ] = Z . Thus, its mean squared error canbe reduced by reducing its variance, which is given by:VarQ [w(x)]NVarQ [(cid:10)Z N ] =(14)Therefore, VarQ [(cid:10)Z N ] can be reduced by either increasing the number of samples N or by reducing the variance of thei=1 F i(x), then for any sample x drawn from Q , we have w(x) = Z ,weights (or both). It is easy to see that if Q (x) ∝yielding an optimal (zero variance) estimator. However, making Q (x) ∝mi=1 F i(x) is NP-hard and therefore in order tohave a small MSE in practice, it is recommended that Q must be as “close” as possible to the distribution that it tries toapproximate which in our case is proportional to(cid:6)(cid:6)(cid:6)mmi=1 F i(x).Next, we present an importance sampling algorithm for estimating the marginal probability P G (xi). Recall that P G (xi) isdefined as:P G(xi) =(cid:2)(cid:6)mj=1 F j(x)x∈X I xi (x)(cid:6)(cid:2)mj=1 F j(x)x∈X(15)V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7745Fig. 3. A Bayesian network and its CPTs.Given a proposal distribution Q (x), we can rewrite Eq. (15) as follows:P G(xi) =(cid:2)(cid:6)x∈X I xi (x)(cid:6)(cid:2)mx∈Xmj=1 F j(x) Q (x)Q (x)Ex=j=1 F j(x) Q (x)Q (x)(cid:6)mj=1 F j (x)(cid:12)(cid:11) I xi (x)(cid:11) (cid:6)ExQ (x)mj=1 F j (x)Q (x)(cid:12) =Ex[I xi (x)w(x)]Ex[w(x)](16)Given independent and identically distributed (i.i.d.) samples (x1, . . . , xN ) generated from Q , we can estimate P G (xi) by:(cid:2)(cid:10)P G,N (xi) =1N(cid:2)Nk=1 I xi (xk)w(xk)(cid:2)Nk=1 w(xk)1N=Nk=1 I xi (xk)w(xk)(cid:2)Nk=1 w(xk)(17)(cid:10)P G,N (xi) is a ratio between two sample means. The numerator equals the sample mean of the samples containing Xi = xiand the denominator is a sample mean of all the samples (which is an estimate of Z ). Thus, the samples used for estimatingZ can be used in a straightforward manner for estimating P G (xi).However, (cid:10)P G,N (xi) is not an unbiased estimate of P G (xi), namely Ex[(cid:10)P G,N (xi)] (cid:14)= P G (xi). Yet, it is asymptotically unbi-ased, namely limN→∞ Ex[(cid:10)P G,N (xi)] = P G (xi), and thus its bias goes down as we increase the sample size. Unfortunately,the variance of (cid:10)P G,N (xi) is harder to analyze because it is a ratio [3].Liu [3] suggests a measure called effective sample size (ESS) to analyze the accuracy of an asymptotically unbiasedestimate. It is defined as:ESS =N1 + VarQ [w(x)](18)The interpretation of ESS is that N samples from the proposal distribution are worth ESS samples from the ideal proposalmi=1 F i(x)). The higher the ESS the higher the accuracy. ESSdistribution (the ideal proposal distribution is proportional tocan be increased by increasing the sample size N or by decreasing the variance VarQ [w(x)] (or both).(cid:6)In summary, the accuracy of the estimates of Z and P G (xi) obtained via importance sampling can be improved byincreasing the sample size N or by reducing the variance of the weights. In the next three sections, we describe two newschemes, AND/OR tree and AND/OR graph importance sampling which improve accuracy by virtually increasing the samplesize N. We will focus our theoretical analysis and empirical evaluation on estimating the weighted counts Z , noting thatour analysis and results can be extended in a straight forward manner to estimating the marginal probabilities P G (xi).3. AND/OR tree importance samplingWe start by discussing computing expectation by parts which forms the backbone of AND/OR importance sampling. Wethen present our first version based on AND/OR tree search. (Note that proofs that do not appear in the body of the paperare deferred to Appendix B.)3.1. Estimating expectation by partsIn Eq. (12), the expectation of a function defined over a set of variables is computed by summing over the Cartesianproduct of the domains of all variables. This method is clearly inefficient because it does not take into account the condi-tional independencies in the graphical model as we illustrate below.Consider the tree Bayesian network given in Fig. 3. Let A = a and B = b be the evidence. By definition, the probability ofevidence P (a, b) is given by:(cid:4)xyz∈ X Y ZP (a, b) =P (z)P (x|z)P (a|x)P ( y|z)P (b| y)(19)46V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Let Q (x, y, z) = Q (z)Q (x|z)Q ( y|z) be a proposal distribution. We can express P (a, b) in terms of Q as:P (a, b) =(cid:4)xyz∈ X Y ZP (z)P (x|z)P (a|x)P ( y|z)P (b| y)Q (z)Q (x|z)Q ( y|z)Q (z)Q (x|z)Q ( y|z)We can now apply some simple symbolic manipulations, and rewrite Eq. (20) as:P (a, b) =(cid:4)z∈ ZP (z)Q (z)Q (z)(cid:4)x∈ XP (x|z)P (a|x)Q (x|z)Q (x|z)(cid:4)y∈YP ( y|z)P (b| y)Q ( y|z)Q ( y|z)By definition of conditional expectation:2(cid:13)Ex(cid:13)ExandP (x|z)P (a|x)Q (x|z)(cid:14)(cid:8)(cid:8)(cid:8) z=(cid:4)x∈ XP (x|z)P (a|x)Q (x|z)Q (x|z)P ( y|z)P (b| y)Q ( y|z)(cid:14)(cid:8)(cid:8)(cid:8) z=(cid:4)y∈YP ( y|z)P (b| y)Q ( y|z)Q ( y|z)Substituting Eqs. (22) and (23) in Eq. (21), we get:P (a, b) =(cid:13)Ex(cid:4)z∈ ZP (z)Q (z)P (x|z)P (a|x)Q (x|z)(cid:14)(cid:8)(cid:8)(cid:8) z(cid:13)ExP ( y|z)P (b| y)Q ( y|z)(cid:14)(cid:8)(cid:8)(cid:8) zQ (z)By definition (of expectation), we can rewrite Eq. (24) as:(20)(21)(22)(23)(24)(cid:14)(cid:14)(cid:8)(cid:8)(cid:8) z(cid:13)(cid:14)(cid:8)(cid:8)(cid:8) z(cid:13)P (a, b) = Ex(cid:13)ExP (z)Q (z)P ( y|z)P (b| y)Q ( y|z)We will refer to equations of the form (25) as expectation by parts. If the domain size of all variables is d = 10, for exam-ple, computing P (a, b) using Eq. (20) would require summing over d3 = 103 = 1000 terms while computing P (a, b) usingEq. (25) would require summing over d + d2 + d2 = 10 + 102 + 102 = 210 terms.P (x|z)P (a|x)Q (x|z)(25)ExWe will now describe how to estimate P (a, b) using Eq. (25). Assume that we are given N samples (z1, x1, y1), . . . ,(zN , xN , y N ) generated from Q . Let {0, 1} be the domain of Z and let Z = 0 and Z = 1 be sampled N0 and N1 timesrespectively. We define two sets S( j) = {k|k ∈ {1, . . . , N} and zk = j} for j ∈ {0, 1} which store the indices of the samples inwhich the value j is assigned to Z .We can estimate Ex[ P (x|z)P (a|x)| z] by replacing the expectation by the sample average. These| z] and Ex[ P ( y|z)P (b| y)Q (x|z)Q ( y|z)unbiased estimates denoted by (cid:10)g X (Z = j) and (cid:10)gY (Z = j); j ∈ {0, 1} are given by:(cid:10)g X (Z = j) = 1N j(cid:10)gY (Z = j) = 1N j(cid:4)i∈S( j)(cid:4)i∈S( j)P (xi| Z = j)P (a|xi)Q (xi| Z = j),P ( yi| Z = j)P (b| yi)Q ( yi| Z = j)(26)(27)Substituting the unbiased estimates for Ex[ P (x|z)P (a|x)estimate of P (a, b):Q (x|z)| z] and Ex[ P ( y|z)P (b| y)Q ( y|z)| z] in Eq. (25), we get the following unbiased(cid:13)(cid:10)P (a, b) = ExP (z)Q (z)(cid:14)(cid:10)g X (Z = j)(cid:10)gY (Z = j)(28)Given samples (z1, . . . , zN ) generated from Q (Z ), we can estimate (cid:10)P (a, b) by replacing the expectation in Eq. (28) by thefollowing sample average:(cid:10)P ao−is(a, b) = 1NN(cid:4)i=1P (zi)Q (zi)(cid:7)(cid:7)(cid:9)(cid:10)gYzizi(cid:9)(cid:10)g X(29)2 Because, the expectation is always taken with respect to the component of the proposal distribution in the denominator, we write ExQ [ X] as Ex[ X]for clarity.V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7747where (cid:10)P ao−is(a, b) stands for an AND/OR estimate of P (a, b). Since, Z = 0 and Z = 1 are sampled N0 and N1 times respec-tively, we can collect together all samples that have Z = j, j ∈ {0, 1} and rewrite Eq. (28) as:(cid:10)P ao−is(a, b) = 1N1(cid:4)j=0N j P (Z = j)Q (Z = j)(cid:10)g X (Z = j)(cid:10)gY (Z = j)(30)It is easy to show that Ex[(cid:10)P ao−is(a, b)] = P (a, b), namely (cid:10)P ao−is is unbiased. Conventional importance sampling, on theother hand, would estimate P (a, b) as follows:(cid:10)P is(a, b) = 1NN(cid:4)i=1P (zi)P (xi|zi)P ( yi|zi)P (a|xi)P (b| yi)Q (zi)Q (xi|zi)Q ( yi|zi)As before, we can collect together all samples that have Z = j, j ∈ {0, 1} and rewrite Eq. (31) as:(cid:10)P is(a, b) = 1N1(cid:4)j=0N j P (Z = j)Q (Z = j)(cid:15)1N j(cid:4)i∈S( j)P (xi| Z = j)P ( yi| Z = j)P (a|xi)P (b| yi)Q (xi| Z = j)Q ( yi| Z = j)(cid:16)For simplicity denote:(cid:10)g X,Y (Z = j) = 1N j(cid:4)i∈S( j)P (xi| Z = j)P ( yi| Z = j)P (a|xi)P (b| yi)Q (xi| Z = j)Q ( yi| Z = j)and rewrite Eq. (32) as:(cid:10)P is(a, b) = 1N1(cid:4)j=0N j P (Z = j)Q (Z = j)(cid:10)g X,Y (Z = j)It is easy to show that (cid:10)g X,Y (Z = j) is an unbiased estimate of Ex[ P (x|z)P ( y|z)P (a|x)P (b| y)| z], namely,Q (x|z)Q ( y|z)(cid:12)(cid:11)(cid:10)g X,Y (Z = j)Ex(cid:13)= ExP (x|z)P ( y|z)P (a|x)P (b| y)Q (x|z)Q ( y|z)(cid:14)(cid:8)(cid:8)(cid:8) z(31)(32)(33)(34)Let us now compare (cid:10)P ao−is given by Eq. (30) with (cid:10)P is given by Eq. (33). The only difference is that in (cid:10)P ao−is, we computea product of (cid:10)g X (Z = j) and (cid:10)gY (Z = j) instead of (cid:10)g X Y (Z = j). The product of (cid:10)g X (Z = j) and (cid:10)gY (Z = j) combines an estimateover two separate quantities defined over the random variables X| Z = z and Y | Z = z respectively from the generatedsamples. While in conventional importance sampling, we estimate only one quantity defined over the joint random variableX Y | Z = z using the generated samples. Because the samples for X| Z = z and Y | Z = z are considered independently inEq. (30), N j samples drawn over the joint random variable X Y | Z = z in Eq. (33) correspond to N j × N j = N 2j virtualsamples in Eq. (30). Since the variance goes down as the sample size increases, our new estimation technique will be moreaccurate than the conventional approach.3.2. Estimating weighted counts using an AND/OR sample treeWe next generalize the above example using an AND/OR search tree [6]. We will define an AND/OR sample tree which isa restriction of the full AND/OR search tree to the generated samples. On this AND/OR sample tree, we define a new samplemean and show that it yields an unbiased estimate of the weighted counts. We start with some required definitions.Definition 13 (Bucket function). Given a graphical model G = (cid:5)X, D, F(cid:6) and a rooted pseudo tree T (X, E), the bucket functionof Xi relative to T , denoted by B T , Xi is the product of all functions in G that mention Xi but do not mention any variablesthat are descendants of Xi in T .Example 5. Fig. 4 shows a possible pseudo tree over the non-evidence variables of the Bayesian network given in Fig. 3.Each variable in the pseudo tree is annotated with its bucket function.3 The bucket function of Z is P (Z ) because P (Z )is the only function that mentions Z but does not mention the descendants X and Y of Z . The bucket function of X isP (a| X) × P ( X| Z ), while that of Y is P (b|Y ) × P (Y | Z ).3 Note that the pseudo tree is defined over the non-evidence variables and the probability of evidence equals the weighted counts over an evidenceinstantiated Bayesian network. After instantiating A to a, the CPT P ( A| X) yields a function P (a| X) having scope X . Similarly, after instantiating B to b,the CPT P (B|Y ) yields a function P (b|Y ) having scope Y . Thus, the actual functions used for computing the weighted counts (probability of evidence) areP (Z ), P ( X|Z ), P (a| X), P (Y |Z ) and P (b|Y ).48V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Fig. 4. A pseudo tree of the Bayesian network given in Fig. 3(a) in which each variable is annotated with its bucket function.Fig. 5. (a) Four samples drawn from a uniform proposal distribution Q ( X, Y , Z ) = Q ( X)Q (Y )Q (Z ) where Q (Z = 0) = Q (Z = 1) = 1/2, Q ( X = 0) = Q ( X =1) = Q ( X = 2) = 1/3 and Q (Y = 0) = Q (Y = 1) = Q (Y = 2) = 1/3, (b) The samples in (a) arranged on a full AND/OR search tree. Dotted edges and nodesare not sampled. Each arc from an OR node to an AND node is labeled by its weight and frequency (see Definition 14).Definition 14 (AND/OR sample tree). Given a graphical model G = (cid:5)X, D, F(cid:6), a pseudo tree T (X, E), a proposal distributioni=1 Q i(xi|yi) such that Yi ⊆ contextT ( Xi),4 a sequence of samples SQ defined relative to the pseudo tree, namely Q (x) =and a complete AND/OR search tree ψT , an AND/OR sample tree ψT ,S is obtained from ψT by removing all nodes and thecorresponding edges which do not appear in S.(cid:6)nLet πn denote the path from the root of ψT ,S to a node n and let A(πn) denote the assignment sequence along thepath πn. We define the arc-label of an arc (n, m) from an OR node n to an AND node m in ψT ,S, where Xi labels n and xilabels m, as a pair (cid:5)w(n, m), #(n, m)(cid:6) where:• w(n, m) = B T , Xi• #(n, m) is the frequency of the arc. Namely, it is equal to the number of times the partial assignment A(πm) occursis the weight of the arc (n, m), where B T , Xi be the bucket function of Xi (see Definition 13).(xi , A(πn))Q i (xi | A(πn))in S.An OR sample tree is an AND/OR sample tree defined relative to a chain pseudo tree.Example 6. Consider again the Bayesian network given in Fig. 3. Assume that the proposal distribution Q ( X, Y , Z ) = Q ( X) ×Q (Y ) × Q (Z ) is a uniform distribution. Fig. 5(b) shows a full AND/OR search tree over the Bayesian network and Fig. 5(a)shows four hypothetical random samples drawn from Q . The AND/OR sample tree is obtained by removing the dotted edgesand nodes which are not sampled from the full AND/OR search tree. Each arc from an OR node to an AND node in theAND/OR sample tree is labeled with appropriate frequencies and weights according to Definition 14. For instance, consider4 For simplicity, we assume that Yi is a subset of context of Xi . When the proposal distribution is specified externally, it may not obey this constraint. Inthat case, we construct a pseudo tree from a graph G obtained by combining the primal graphs of the proposal distribution and the graphical model.V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7749Fig. 6. Value computation on an AND/OR sample tree (see Definition 15). Each OR and AND node is annotated with its value. By definition, the value of aleaf AND node is 1 and is not shown to avoid clutter. The AND/OR sample tree mean (which equals the value of the root OR node Z ) is 0.12096.the arc corresponding to (Z = 0, X = 1) (leftmost sampled arc). The assignment (Z = 0, X = 1) appears only once in thesamples given in Fig. 5(a). Therefore, the frequency of the arc is 1. Given evidence A = 0, the bucket function associatedwith X is P (x|z) × P ( A = 0|x) (see Fig. 4). Since the proposal distribution is uniform, namely Q (x|z) = 1/3 (since X hasthree values in its domain), the weight of the arc is:B T (X = 1, Z = 0)Q (X = 1| Z = 0)= P (X = 1| Z = 0) × P ( A = 0| X = 1)Q (X = 1| Z = 0)= 0.4 × 0.21/3= 0.24Fig. 5(b) shows the derivation of the weight of the arc(Z , 1).We can now compute an approximation of node values by mimicking the value computation on the AND/OR sampletree.Definition 15 (Value of a node). Given an AND/OR sample tree (or graph) ψT ,S, the value of a node n, denoted by v(n) isdefined recursively as follows. The value of a leaf AND node is 1. If n is an AND node then:v(n) =(cid:3)(cid:9)(cid:7)n(cid:8)vn(cid:8)∈chi(n)and if n is an OR node then(cid:2)v(n) =n(cid:8)∈chi(n) #(n, n(cid:2)(cid:8)) × w(n, nn(cid:8)∈chi(n) #(n, n(cid:8))(cid:8)) × v(n(cid:8))We will show that the value of an OR node n is equal to an unbiased estimate of the conditional expectation of thesubproblem conditioned on the assignment from the root to n (see Theorem 1).Definition 16 (AND/OR sample tree mean). The AND/OR sample tree mean is the value of the root node of an AND/OR sampletree.Example 7. Fig. 6 shows the values of nodes computed using Definition 15 for our running example. For instance, the valueof the AND node (which equals the product of values of its child OR nodes) corresponding to Z = 0 is 0.39 × 0.255 =0.09945. The derivation of the value of the root OR node labeled by Z is shown in Fig. 6. The value of the OR nodes X andY given Z = j ∈ {0, 1} is (cid:10)g X (Z = j) and (cid:10)gY (Z = j) respectively, as defined in Eqs. (26) and (27). The value of the root nodelabeled by Z is the AND/OR sample tree mean which is equal to the sample mean computed by parts in Eq. (30).Theorem 1. The AND/OR sample tree mean is an unbiased estimate of the weighted counts.Next, we show that the OR sample tree mean is equal to the conventional importance sampling sample mean given byEq. (13). Recall that an OR sample tree is defined relative to a chain pseudo tree. We can convert any pseudo tree T to achain pseudo tree Tas a topologicallinearization of T . Formally,by forming a chain along a topological (or DFS) ordering of T . We will refer to T(cid:8)(cid:8)50V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Algorithm 2: AND/OR tree importance sampling.Input: A graphical mode G = (cid:5)X, D, F(cid:6) a pseudo tree T (X, E) and a proposal distribution defined relative to T : Q (X) =Output: AND/OR sample tree mean(cid:6)ni=1 Q ( Xi|contextT ( Xi ))1 Generate samples S = (x1, . . . , xN ) from Q ;2 Build an AND/OR sample tree ψT ,S relative to S and T ;3 Initialize all labeling functions (cid:5)w(n, m), #(n, m)(cid:6) on each arc from an OR node n to an AND node m using Definition 14// Start: Value computation phase4 Initialize the value of all leaf AND nodes to 1;5 for every node n from leaves to the root of ψT ,S doLet chi(n) denote the child nodes of node n;6// denote value of a node by v(n)if n is an AND node then(cid:6)n(cid:8)∈chi(n) v(nv(n) =(cid:8));78910else(cid:2)nv(n) =(cid:8) ∈chi(n) #(n,n(cid:2)n(cid:8))×w(n,n(cid:8) ∈chi(n) #(n,n(cid:8))(cid:8))×v(n(cid:8))// End: Value computation phase11 return v(root node of ψT ,S)Definition 17 (Topological linearization of a pseudo tree). A topological linearization of a pseudo tree T (X, E) is a chain pseudo(cid:8)(cid:8)) such that for any two nodes X and Y in X, if X is an ancestor of Y in T then X is also an ancestor of Y in Ttree T.(cid:8)(X, ETheorem 2. Given a graphical model G = (cid:5)X, D, F(cid:6), a pseudo tree T , a proposal distribution Q (X) =i.i.d. samples drawn from Q and a topological linearization Tbased on Tequals the conventional importance sampling estimate (cid:10)Z N defined in Eq. (13).i=1 Q i( Xi|contextT ( Xi)), Nof T , the AND/OR sample tree mean computed on an OR sample tree(cid:8)(cid:8)(cid:6)nAlgorithm AND/OR tree importance sampling is presented as Algorithm 2. In steps 1–3, the algorithm generates samplesfrom Q and stores them on an AND/OR sample tree. The algorithm then computes the AND/OR sample tree mean over theAND/OR sample tree recursively from leaves to the root in steps 4–10 (value computation phase).We summarize the complexity of Algorithm 2 in the following theorem.Theorem 3 (Complexity of AND/OR tree importance sampling). Given N i.i.d. samples drawn from the proposal distribution Q , agraphical model with n variables and a pseudo tree having depth h, the time complexity of computing the AND/OR sample tree mean isO (nN) and the space complexity is O (h).Note that in Algorithm 2 we have separated the sampling and estimation (value computation) phases for pedagogicalreasons. It is possible to interleave the two phases by generating the AND/OR sample tree on the fly via depth-first sampling.This variant is described in Appendix A (see Algorithm 3).3.3. Estimating conditional probabilities using an AND/OR sample treeTo compute an estimate of the conditional probability P G (xi) via AND/OR importance sampling, all we have to do iscompute two AND/OR sample tree means in the value computation phase and output their ratio. The denominator equalsthe AND/OR sample tree mean computed by Algorithm 2. To compute the numerator, we set the values of all child ANDnodes of Xi that are not labeled by xi to zero and compute the AND/OR sample tree mean as before. Both the numerator andthe denominator can be computed in one pass by maintaining two values at each node, one corresponding to the numeratorand the other corresponding to the denominator. Clearly, the estimate of P G (xi) obtained in this way is asymptoticallyunbiased and its computational complexity is the same as that of Algorithm 2.It is also possible to compute the marginal probabilities of all variables in the network by performing two value com-putation passes, upward and downward, over the AND/OR sample tree. The passes mimic the computation for updatingbeliefs over a full AND/OR search tree [6,13]. Each node stores two values, one summarizing the information in its ancestorsand their descendants (excluding its own descendants) and the other summarizing the information in its descendants. Thevalue of a node in the upward pass, which summarizes the information from its descendants is computed as before. Thedownward value of the node is recursively computed as follows.Definition 18 (Downward value of a node). The downward value of the root OR node is 1. The downward value u(n) of aninternal OR node n having an AND parent n(cid:9) (cid:3)is given by(cid:9)(cid:8)u(n) = u(cid:7)n(cid:8)(cid:7)n(cid:8)(cid:8)vn(cid:8)(cid:8)∈chi(n(cid:8)): n(cid:8)(cid:8)(cid:14)=nV. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7751where chi(nof an AND-node n having parent n(cid:8)) is the set of child nodes of n(cid:8)is given by(cid:8)and v(n(cid:8)(cid:8)) is the (upward) value of n(cid:8)(cid:8)(see Definition 15). The downward valueu(n) = u(n(cid:2)(cid:8), n)(cid:8), n)w(n(cid:8))#(nm∈chi(n(cid:8)) #(n(cid:8), m)Given the upward and downward values of all nodes of an AND/OR sample tree, we can compute an estimate of themarginal probability values for all variables in the graphical model using the following equation:(cid:10)P G(xi) =(cid:2)n∈n(xi) v(n)u(n)v(root)(35)where n(xi) is the set of all AND nodes corresponding to the value xi and v(root) is the (upward) value of the root node.The estimate (cid:10)P G (xi) is asymptotically unbiased because it is a ratio of two unbiased estimates [10,3]. The numerator is anunbiased estimate of the weighted counts of the graphical model augmented with the indicator function(cid:5)(cid:9)(cid:7)(cid:8)xiI xi=(cid:8)1 if xi = xi0 otherwiseThe denominator is an unbiased estimate of the weighted counts.Since the size of the AND/OR sample tree is bounded by O (nN), it is straight-forward to show thatTheorem 4. Given N i.i.d. samples drawn from the proposal distribution Q and a graphical model with n variables, the time and spacecomplexity of computing an estimate of all marginal probabilities using an AND/OR sample tree is O (nN).Comparing the space complexities for estimating the weighted counts and all marginal probabilities (Theorem 3 andTheorem 4 respectively), we see that estimating all marginal probabilities is more space intensive, by a factor of O (nN/h).Conventional OR importance sampling, on the other hand, incurs the same space complexity for estimating both. This yieldsanother space versus variance trade-off.In summary, in this section, we defined AND/OR sample tree mean and showed that it yields an unbiased estimate of theweighted counts Z . We proved that the conventional importance sampling sample mean equals the OR sample tree mean.We provided an algorithm for computing the AND/OR sample tree mean and proved that it has the same time complexityas the conventional importance sampling sample mean.4. Variance reductionIn this section, we will prove that the AND/OR sample tree mean has smaller variance than the OR sample tree mean.In other words, we will prove that the variance of the AND/OR sample tree mean is smaller than that of the conventionalimportance sampling sample mean defined by Eq. (12).In fact, we will prove a more general result. Specifically, we will define an iterative process for constructing a topologicallinearization and show that the AND/OR sample tree mean defined relative to the partial linearization at iteration i − 1 hassmaller (or equal) variance than the one defined relative to the partial linearization at iteration i. We begin by formallydefining these partial linearizations.Definition 19 (Topological linearization of a pseudo tree w.r.t. a node). Given a pseudo tree T (X, E), a topological linearization ofT w.r.t. a node X ∈ X is a pseudo tree T X obtained as follows:• If X has at most one child node then T X equals T .• Otherwise, let T X = T and let C be an arbitrary child node of X . Let {O 1, . . . , O k} be the set of child nodes of X notincluding C . Replace each edge ( X, O i) in T X with an edge (C, O i).It is easy to show that:Proposition 2. A topological linearization of a pseudo tree can be obtained by successively applying topological linearization to itsnodes until convergence, namely until all nodes have at most one child node.Example 8. Fig. 7(a) shows a pseudo tree. Each pseudo tree shown in Figs. 7(b)–7(e) is obtained by applying topologicallinearization to a node of the pseudo tree on its left. Fig. 7(e) shows a chain pseudo tree, whose structure cannot be changedby applying topological linearization to any of its nodes.52V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Fig. 7. (a) A pseudo tree. (b) Pseudo tree obtained by applying topological linearization to the node X1 of the pseudo tree given in (a). (c) The pseudotree obtained by applying topological linearization to the node X2 of the pseudo tree given in (b). (d) The pseudo tree obtained by applying topologicallinearization to the node X4 of the pseudo tree given in (c). (e) The chain pseudo tree obtained by applying topological linearization to the node X3 of thepseudo tree given in (d).Fig. 8. (a) Pseudo tree T , (b) AND/OR sample tree ψT ,S based on the pseudo tree given in (a), (c) Pseudo tree T X obtained by topological linearization of Tw.r.t. X , (d) AND/OR sample tree ψT X ,S based on the pseudo tree given in (c).We will now show that the variance of an AND/OR sample tree mean defined relative to S and T is smaller than orequal to the AND/OR sample tree mean defined relative to S and T X , where T X is a topological linearization of T withrespect to X . Since the chain pseudo tree is obtained by successively applying topological linearization to its nodes, ourmain theorem follows immediately from this general result.Lemma 1. Given a pseudo tree T (X, E) of a graphical model G = (cid:5)X, D, F(cid:6), a topological linearization T X of T w.r.t. to a node X ∈ X,and a sequence of samples S = (x1, . . . , xN ) drawn from a proposal distribution Q defined relative to T , the variance of the AND/ORsample tree mean defined relative to S and T is smaller than or equal to the variance of the AND/OR sample tree mean defined relativeto S and T X .V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7753Proof. Without loss of generality, let us assume the following: (a) X is the root of T and has two child nodes U andW , (b) {x1, . . . , xd}, {u1, . . . , ud} and {w 1, . . . , wd} are the domains of X , U and W respectively and (c) Q ( X, U , W ) =Q ( X) × Q (U | X) × Q (W | X) is the proposal distribution over { X, U , W }.Let x j appear N j times in S and let ((x j, u1, w 1), . . . , (x j, u N j , w N j )) be the samples having X = x j . Let ψT ,S and ψT X ,Sdenote the AND/OR sample trees defined relative to (T , S) and (T X , S) respectively. T , T X , ψT ,S and ψT X ,S are shown inFig. 8. For notational convenience, let U and W denote the random variables corresponding to the value of the child nodesU and W respectively of x j in ψT ,S (see Fig. 8). Let U iT be defined as follows:T and W iU iT= B T ,U (x j, ui)Q (ui|x j)(cid:9)(cid:7)ui× v Twhere B T ,U is the bucket function of U and v T (ui) is the value of the AND node corresponding to (x j, ui).W iT= B T ,W (x j, w i)Q (w i|x j)(cid:9)(cid:7)w i× v T(36)(37)where B T ,W is the bucket function of W and v T (w i) is the value of the AND node corresponding to (x j, w i). Note that U iTand W iT are unbiased estimates of the value of the child OR nodes U and W respectively of x j in a full AND/OR search treebased on T .By definition (see Definition 15), the value of the AND node labeled by x j of ψT ,S is given by:v T (x j) = v T (U ) × v T (W )= 1N j= 1N jN j(cid:4)i=1N j(cid:4)i=1B T ,U (x j, ui)Q (ui|x j)(cid:9)(cid:7)ui× v T× 1N jN j(cid:4)i=1B T ,W (x j, w i)Q (w i|x j)(cid:9)(cid:7)w i× v TU iT× 1N jN j(cid:4)i=1W iT(from Eqs. (36) and (37))By definition, the value of the AND node labeled by x j in ψT X ,S is:v T X (x j) = v T X (U )N j(cid:4)= 1N j= 1N ji=1N j(cid:4)i=1B T ,U (x j, ui)Q (x j, ui)(cid:9)(cid:7)uiv T XB T ,U (x j, ui)Q (x j, ui)(cid:9)(cid:7)uiv Tv T X(cid:9)(cid:7)W i(38)(39)(40)(41)(42)(43)The last step follows from the fact that the value of the AND node corresponding to (x j, ui) of ψT X ,S is the product of thevalue of the AND node corresponding to (x j, ui) of ψT ,S and the value of its OR node labeled by W (see Fig. 8). Namely,v T X (ui) = v T (ui) × v T X (W i). For notational convenience, we define:W iT X= v T X(cid:7)(cid:9)W iNote that W iT Xsearch tree based on T X ). From Eqs. (36), (43) and (44), we have:is an unbiased estimate of the value of W in a full AND/OR search tree based on T (as well as in a full AND/ORv T X (x j) = 1N jN j(cid:4)i=1U iTW iT X(45)Since the AND/OR sample tree mean equals the value of the root node labeled by X , to prove the lemma, we have toprove that Var[v T ( X)] (cid:2) Var[v T X ( X)]. From the law of total variance,5 we have:(cid:12)(cid:12)(cid:12)(cid:12)(cid:11)Var(cid:11)Var(cid:12)v T (X)(cid:11)(cid:11)= VarEx(cid:11)(cid:12)= Varv T X (X)v T (x j)(cid:11)Exv T X (x j)(cid:11)+ Ex(cid:12)(cid:12)(cid:11)Varv T (x j)(cid:11)(cid:11)+ ExVarv T X (x j)(cid:12)(cid:12)(44)(46)(47)5 The law states that the variance of a random variable A can be expressed in terms of its conditional variance and expectation w.r.t. to another randomvariable B as follows: Var[ A] = Var[Ex[ A|B]] + Ex[Var[ A|B]].54V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77From Eqs. (46) and (47), we can see that in order to prove that Var[v T ( X)] (cid:2) Var[v T X ( X)], it suffices to prove that:(cid:11)Ex(cid:12)v T (x j)(cid:11)= Ex(cid:12)v T X (x j)(cid:11)and Var(cid:12)v T (x j)(cid:11)(cid:2) Var(cid:12)v T X (x j)We will prove each of the two parts in turn. By definition,(cid:11)Ex(cid:12)v T (x j)(cid:17)= Ex1N jN j(cid:4)i=1U × 1N j(cid:18)N j(cid:4)Wi=1N j(cid:4)N j(cid:4)= 1N 2jEx[UW](1)(1)i=1i=1(cid:11)Ex(cid:12)v T X (x j)Ex[UW]N j N j= 1N 2j= Ex[UW](cid:17)(cid:18)UW1N jN j(cid:4)i=1Ex[UW]N j(cid:4)i=1(1)Ex[UW]N j= Ex= 1N j= 1N j= Ex[UW]From Eqs. (51) and (55), we have:(cid:11)Ex(cid:12)v T (x j)(cid:11)= Ex(cid:12)v T X (x j)This proves the first part.Next, we prove the second part. By definition,(cid:11)Var(cid:12)v T X (x j)(cid:17)= Var(cid:18)UW1N jN j(cid:4)i=1= 1N 2jVar[UW]N j(cid:4)i=1(1)Var[UW]N j= 1N 2jVar[UW]N j=(48)(49)(50)(51)(52)(53)(54)(55)(56)(57)(58)(59)Notice that the random variables U and W are (conditionally) independent of each other (given X = x j). Goodman [14]provides an expression for the variance of product of such independent random variables. Formally, if A and B are twoindependent random variables, then Var[ A B] is given by:Var[ A B] = Var[ A]Ex[B]2 + Var[B]Ex[ A]2 + Var[ A]Var[B]Using this expression, we can derive the expression for Var[v T (x j)] as shown below:(cid:11)Var(cid:12)v T (x j)(cid:17)= Var(cid:17)= Var1N j1N j(cid:17)+ VarU × 1N j(cid:17)(cid:18)UExN j(cid:4)i=1N j(cid:4)i=1(cid:18)N j(cid:4)i=1WN j(cid:4)1N ji=1(cid:17)(cid:18)1N jN j(cid:4)i=1UVar1N j(cid:18)2(cid:17)W+ Var(cid:18)N j(cid:4)i=1W1N jN j(cid:4)i=1(cid:18)(cid:17)WEx(cid:18)21N jN j(cid:4)i=1U(60)(61)(62)It is easy to show that:(cid:17)(cid:18)Var1N jN j(cid:4)i=1U=Var[U]N jV. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7755(cid:17)and Ex1N jN j(cid:4)i=1(cid:18)U= Ex[U]Similarly, it is easy to show that:(cid:17)Var1N jN j(cid:4)i=1(cid:18)W=Var[W]N j(cid:17)and Ex1N jN j(cid:4)i=1(cid:18)W= Ex[W]Substituting these values in Eq. (62), we get,(cid:11)Var(cid:12)v T (x j)=Var[U]Ex[W]2N j+Var[W]Ex[U]2N j+Var[U]Var[W]N 2jUsing the formula for variance of products of independent random variables given in Eq. (60) in Eq. (59), we get:(cid:11)Var(cid:12)v T X (x j)=Var[U]Ex[W]2N j+Var[W]Ex[U]2N j+Var[U]Var[W]N j(63)(64)Notice that the right-hand sides of Eqs. (63) and (64) differ only in the last term (the denominator of the last termin Eq. (63) while it is N j in Eq. (64)). Thus, if N j > 1, then Var[v T (x j)] < Var[v T X (x j)] (assuming that Q does notis N 2jequal the posterior distribution), else if N j = 1, then Var[v T (x j)] = Var[v T X (x j)]. This proves the second part and the prooffollows. (cid:2)As mentioned earlier, since a chain pseudo tree is obtained by applying topological linearizations to nodes of T , thefollowing theorem follows immediately from Lemma 1.Theorem 5. Given a pseudo tree T , a topological linearization Tof T and a set of samples S, the variance of the AND/OR sample treemean defined over the AND/OR sample tree ψT ,S is smaller than or equal to the variance of the AND/OR sample tree mean defined overthe AND/OR sample tree ψT (cid:8),S. In other words, the variance of the AND/OR sample tree mean is smaller than or equal to the variance ofthe OR sample tree mean.(cid:8)4.1. Remarks on variance reductionFrom the proof of Lemma 1, we can see that given a pseudo tree T and its topological linearization T X w.r.t. X , if eachvalue x ∈ D( X) is sampled only once then the value of the AND node x in the AND/OR sample tree defined relative to T willbe equal to its corresponding value in the AND/OR sample tree defined relative to T X , and as a result their variance will bethe same too. We can tie variance reduction to the number of virtual AND/OR tree samples, defined recursively below.Definition 20 (Virtual samples of an AND/OR sample tree). Given an AND/OR sample tree based on a set of samples S, thenumber of virtual samples associated with a leaf AND node l is the number of times the path from the root to l is sampledin S. The number of virtual samples rooted at an internal AND node equals the product of the number of virtual samplesrooted at its child OR nodes. The number of virtual samples rooted at an OR node n equals the sum of the number ofvirtual samples rooted at its child AND nodes. The number of virtual samples of an AND/OR sample tree equals the numberof virtual samples rooted at the root OR node.Note that when each leaf node in the AND/OR sample tree is sampled only once, the number of virtual samples equalsthe number of solution subtrees (see Definition 7).Example 9. Figs. 9(a) and 9(b) show four samples arranged on an OR sample tree and an AND/OR sample tree respectively.The 4 samples correspond to 8 virtual samples on the AND/OR sample tree. The AND/OR sample tree includes for examplethe assignment (C = 0, B = 2, D = 1, A = 1) which is not present in the OR sample tree (because the samples rooted at Bare conditionally independent of the samples rooted at D given C ).The following two propositions are immediate from the definition of virtual AND/OR tree samples and the proof ofLemma 1.obtained by applying topological linearization several times to different nodesProposition 3. Given a pseudo tree T , a pseudo tree Tof T and a set of samples S, the number of virtual AND/OR tree samples of ψT ,S is greater than or equal to the number of virtual AND/ORtree samples of ψT (cid:8),S.(cid:8)56V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Fig. 9. (a) A chain pseudo tree corresponding to a topological linearization of the pseudo tree given in Fig. 1(b). Figures (b) and (c) show 4 samples arrangedon an OR sample tree and on an AND/OR sample tree respectively. The OR sample tree given in (b) is defined relative to the pseudo tree given in (a). TheAND/OR sample tree given in (c) is defined relative to the pseudo tree given in Fig. 1(b). Each node in the tree is annotated with the number of virtualsamples rooted at the node. We can see that the AND/OR sample tree represents 8 virtual samples while the OR sample tree represents only 4 samples.Proposition 4. Given a pseudo tree T , a pseudo tree Tobtained by applying topological linearization several times to different nodesof T and a set of samples S, if the number of virtual AND/OR tree samples of ψT ,S is strictly greater than the number of virtual AND/ORtree samples of ψT (cid:8),S then the variance of the AND/OR sample tree mean defined relative to ψT ,S is strictly smaller than the varianceof the AND/OR sample tree mean defined relative to ψT (cid:8),S (assuming that the proposal distribution Q does not equal P G ).(cid:8)In summary, we proved that for a given set of samples, the variance of the AND/OR sample tree mean is less than orequal to the variance of the AND/OR sample tree mean defined relative to any topological linearization w.r.t. any node inthe pseudo tree, and in particular to the variance of the OR sample tree mean. We also demonstrated how the variancereduction can be tied to the number of virtual samples. Specifically, we showed that variance reduction occurs only at ANDnodes which have at least two child nodes and which appear in the given set of samples at least two times.5. AND/OR sample graph meanNext, we describe an even more powerful strategy for estimating sample mean in the AND/OR space by moving fromAND/OR trees to AND/OR graphs [6]. The idea is similar to AND/OR graph search in that we merge nodes in the AND/ORsample tree, which are unifiable based on context (see Definition 10), to form an AND/OR sample graph. This results in aneven larger number of virtual samples.Definition 21 (AND/OR sample graph). Given a pseudo tree T and a set of samples S, an AND/OR sample graph is obtainedfrom an AND/OR sample tree ψT ,S by merging all OR nodes that have the same context. The frequency of an arc (n, m) froman OR node n labeled with Xi to an AND node m is changed to account for the merging based on context while the weightof (n, m) remains the same. In particular, the frequency of the arc (n, m) in the AND/OR sample graph equals the numberof times the partial assignment A(πm){ Xi }∪contextT ( Xi ) appears in S.6Definition 22 (AND/OR sample graph mean). The AND/OR sample graph mean is the value of the root node of an AND/ORsample graph.Definition 23 (Number of virtual samples of an AND/OR sample graph). The number of virtual samples of an AND/OR samplegraph is the number of virtual samples rooted at its root node.Example 10. Fig. 10 shows an AND/OR sample graph obtained from the AND/OR sample tree given in Fig. 9(c) by merging allcontext unifiable nodes. Notice that the context of A is {B}. Therefore, for each AND node B = i, i ∈ {0, 1, 2}, we can mergeall its child OR nodes labeled by A in the AND/OR sample tree yielding an AND/OR sample graph. The AND/OR sample6 Recall from Definition 14 that A(πm) denotes the assignment sequence along the path πm from the root of ψT ,S to m. Also, recall that the notationA(πm){Xi }∪contextT ( Xi ) denotes the projection of the assignment A(πm) to the set { Xi } ∪ contextT ( Xi ).V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7757Fig. 10. AND/OR sample graph obtained from the AND/OR sample tree given in Fig. 9(c) by merging all context unifiable nodes. (The AND/OR sample treeand graph are based on the pseudo tree given in Fig. 1(b). The context of A, B, C and D is {B}, {C}, ∅ and {C} respectively.) Each node is annotated withthe number of virtual samples rooted at the node. The AND/OR sample graph represents 12 virtual samples. Compare this with the AND/OR sample treegiven in Fig. 9(c) which represents 8 samples.graph represents 12 virtual samples as compared with the AND/OR sample tree which represents only 8 virtual samples.The AND/OR sample graph includes for example the sample (C = 0, B = 2, D = 1, A = 0) which is not part of the virtualsamples of the AND/OR sample tree.Clearly,Proposition 5. The number of virtual AND/OR graph samples is greater than or equal to the number of virtual AND/OR tree samples (ifboth are based on the same underlying pseudo tree).Since the AND/OR sample graph captures more virtual samples, the variance of the AND/OR sample graph mean may besmaller than the variance of the AND/OR sample tree mean. Formally,Theorem 6. The variance of the AND/OR sample graph mean is less than or equal to that of AND/OR sample tree mean.The algorithm for computing the AND/OR sample graph mean is identical to that of AND/OR sample tree mean (Steps4–10 of Algorithm 2). The only difference is that we store the samples and perform value computations over an AND/ORsample graph instead of an AND/OR sample tree.Theorem 7 (Complexity of computing AND/OR sample graph mean). Given a graphical model with n variables, a pseudo tree T with∗) while its spacemaximum context size (treewidth) wcomplexity is O (nN).and N samples, the time complexity of AND/OR graph sampling is O (nN w∗6. ExperimentsIn this section, we demonstrate empirically that moving from OR space to AND/OR space improves the accuracy ofthe estimates as a function of time. The section is organized as follows. We first describe the implementation details andthe experimental set up and then describe our results for various probabilistic and deterministic (constraint) benchmarknetworks.6.1. Experimental setupAs mentioned earlier, the strength of AND/OR-based estimates is that the samples on which the estimates are basedupon can be generated using any importance sampling scheme. Therefore, in order to demonstrate the impact of AND/ORestimation in a non-trivial setting, we generate samples using state-of-the-art importance sampling techniques such asIJGP-IS [15,16] and IJGP-SampleSearch [17–19].IJGP-IS uses the output of a generalized belief propagation scheme called Iterative Join Graph Propagation (IJGP) toconstruct a proposal distribution. It was shown that belief propagation schemes whether applied over the original graph58V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Fig. 11. Figure showing the scope of our experimental study. The algorithm names are abbreviated as follows. IJGP-IS stands for IJGP-based importancesampling while IJGP-SS stands for IJGP-based SampleSearch. or-tree, ao-tree and ao-graph stand for OR tree, AND/OR tree and AND/OR graph respectively.minfill and hmetis are the orderings used for constructing the pseudo trees. For example: ao-graph-IJGP-SS-minfill stands for IJGP-based SampleSearchwhich uses an AND/OR graph constructed along the minfill ordering for deriving the estimates.or on clusters of nodes yield very good approximation to the true posterior than other available choices [20–22] and thussampling from their output is an obvious choice (see [23,15,16] for more details).IJGP [20,24] is a generalized belief propagation scheme which is parametrized by a constant i, called the i-bound, yieldinga class of algorithms IJGP(i) whose complexity is exponential in i, that trade-off accuracy and complexity. As i increases,accuracy generally increases. When i equals the treewidth of the graphical model, IJGP(i) is exact. We use a i-bound of 10and set the number of iterations to 10 in all our experiments to ensure that IJGP terminates in a reasonable amount of time(less than 5 minutes) while requiring bounded space.The variance and therefore the accuracy of AND/OR sample tree mean is highly dependent upon the height of the pseudotree while that of AND/OR sample graph mean is dependent more upon the treewidth of the pseudo tree. We experimentedwith two alternatives for constructing the pseudo tree: one based on the minfill ordering and the other based on hyper-graph partitioning using the hmetis software,7 henceforth called the hmetis ordering. In earlier studies [4,25], it was shownthat the minfill ordering generally yields pseudo trees having smaller treewidth compared with other alternatives while thehmetis ordering yields pseudo trees having smaller height.Finally, on networks having substantial amount of determinism, we generate samples using IJGP-based SampleSearch(IJGP-SS) [17,19] instead of IJGP-IS. It is known that on such networks pure importance sampling generates many uselesszero weight samples which are eventually rejected. SampleSearch overcomes this rejection problem by explicitly searchingfor a non-zero weight sample, yielding a more efficient sampling scheme in such heavily deterministic databases. It wasshown that SampleSearch is an importance sampling scheme which generates samples from a modification of the proposaldistribution which is backtrack-free w.r.t. the constraints. Thus, to derive AND/OR sample tree and graph means from thesamples generated by SampleSearch, all we need is to replace the proposal distribution with the backtrack-free distributionwhile computing the sample weight.We evaluated our algorithms on the weighted counting task defined over mixed probabilistic and deterministic networks(e.g., probability of evidence in a Bayesian network and counting solutions of a constraint network). We experimented withfive sets of benchmarks: (a) alarm networks, (b) grid networks, (c) linkage networks, (d) coding networks, and (e) graphcoloring networks modeled as satisfiability problems. The linkage, coding and the graph coloring networks have strongdeterministic relationships and therefore we generated samples using IJGP-based SampleSearch (IJGP-SS). On the remainingnetworks (namely on the alarm and the grids), we used IJGP-IS. Fig. 11 shows the benchmarks and the various algorithmsthat we experimented with.We organize the results into two subsections. In the next subsection, we describe the results for instances for which theexact weighted counts are known while in Section 6.3 we describe the results for instances for which the exact weightedcounts are not known. The reason for this separation is the difference in the evaluation criteria used.6.2. Results for networks on which the exact value of the weighted counts is known6.2.1. Evaluation criteriaFor networks for which the exact weighted counts are known, we measure performance by comparing the log relativeerror between the exact weighted counts and the approximate ones. If Z is the exact value and Z is the approximate valueof the weighted counts, the log-relative error is defined as:(cid:8)(cid:8)(cid:8)(cid:8)Δ =(65)(cid:8)(cid:8)(cid:8)(cid:8)log(Z ) − log(Z )log(Z )7 Available at: http://www-users.cs.umn.edu/karypis/metis/hmetis.V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7759Table 1Results for the alarm networks. Table showing the average of sample means ((cid:10)Z ), the relative standard deviation of the sample means (RSD) and the averagelog-relative error (Δ) over five runs of or-tree-IJGP-IS-minfill, ao-tree-IJGP-IS-minfill, ao-graph-IJGP-IS-minfill, or-tree-IJGP-IS-hmetis, ao-tree-IJGP-IS-hmetisand ao-graph-IJGP-IS-hmetis after 1 hour of CPU time.Exactminfill orderinghmetis orderingZ∗, h)(w6.24e−06(14, 20)7.96e−18(15, 25)2.46e−04(15, 24)4.78e−03(16, 24)9.66e−10(19, 27)1.99e−06(19, 25)3.59e−18(12, 17)1.84e−19(13, 19)4.29e−26(13, 22)9.63e−08(14, 21)4.08e−03(14, 21)2.72e−04(14, 22)Instance(n, k, f )(e, c)BN_10(85, 2, 85)(17, 0)BN_11(105, 2, 105)(46, 0)BN_12(90, 2, 90)(11, 0)BN_13(125, 2, 125)(9, 0)BN_14(115, 2, 115)(30, 0)BN_15(120, 2, 120)(19, 0)BN_4(100, 2, 100)(51, 0)BN_5(125, 2, 125)(55, 0)BN_6(125, 2, 125)(71, 0)BN_7(95, 2, 95)(30, 0)BN_8(100, 2, 100)(9, 0)BN_9(105, 2, 105)(13, 0)or-treeIJGP-IS-minfill(cid:10)ZRSDΔ6.23e−060.08%1.20e−047.97e−180.15%4.11e−052.46e−040.34%2.57e−044.79e−030.29%4.16e−049.63e−100.56%1.81e−041.99e−060.43%2.53e−043.59e−180.23%4.48e−051.84e−190.09%2.55e−054.29e−260.26%3.18e−059.60e−080.25%1.85e−044.07e−030.28%4.33e−042.72e−040.20%2.58e−04ao-treeIJGP-IS-minfill(cid:10)ZRSDΔ6.24e−060.05%4.87e−057.96e−180.26%4.99e−052.46e−040.20%1.38e−044.78e−030.10%2.30e−049.66e−100.24%9.04e−051.99e−060.23%1.40e−043.59e−180.08%1.85e−051.84e−190.09%3.04e−054.29e−260.12%1.54e−059.60e−080.21%1.69e−044.08e−030.27%3.62e−042.71e−040.18%2.55e−04ao-graphIJGP-IS-minfill(cid:10)ZRSDΔ6.24e−060.05%4.74e−057.96e−180.33%5.77e−052.46e−040.15%1.03e−044.78e−030.08%1.74e−049.66e−100.30%1.20e−041.99e−060.19%1.20e−043.59e−180.07%2.30e−051.84e−190.10%2.90e−054.29e−260.13%2.12e−059.61e−080.19%1.48e−044.08e−030.26%3.90e−042.72e−040.16%2.14e−04∗, h)(w(16, 22)(18, 23)(18, 23)(15, 23)(20, 26)(19, 26)(13, 18)(15, 20)(13, 18)(15, 20)(14, 21)(15, 21)or-treeIJGP-IS-hmetis(cid:10)ZRSDΔ6.23e−060.19%1.63e−047.95e−180.99%2.01e−042.46e−040.20%1.55e−044.79e−030.28%5.29e−049.69e−100.54%2.67e−041.98e−060.42%2.90e−043.59e−180.08%2.53e−051.83e−190.11%6.40e−054.29e−260.29%3.77e−059.62e−080.37%1.35e−044.08e−030.23%3.01e−042.72e−040.21%2.76e−04ao-treeIJGP-IS-hmetis(cid:10)ZRSDΔ6.24e−060.13%8.89e−057.95e−180.36%7.78e−052.46e−040.14%1.34e−044.78e−030.10%2.40e−049.68e−100.45%2.10e−041.99e−060.30%1.66e−043.59e−180.08%2.37e−051.83e−190.09%7.66e−054.29e−260.20%2.53e−059.63e−080.15%7.04e−054.08e−030.09%1.35e−042.72e−040.07%1.50e−04ao-graphIJGP-IS-hmetis(cid:10)ZRSDΔ6.24e−060.13%8.81e−057.95e−180.36%5.72e−052.46e−040.12%1.23e−044.79e−030.12%2.76e−049.68e−100.44%1.91e−041.99e−060.27%1.37e−043.59e−180.08%2.26e−051.83e−190.07%6.34e−054.29e−260.13%1.94e−059.63e−080.20%1.08e−044.08e−030.09%1.26e−042.72e−040.06%1.42e−04We compute the log relative error instead of the usual relative error because when the probability of evidence is extremely−10) or when the solution counts are large (e.g. > 1010) the relative error between the exact and the approximatesmall (< 10answer will be arbitrarily close to 1 and we would need a large number of digits to distinguish between the results.Tables 1–6 contain the results. On each instance, we ran each algorithm 5 times. For each algorithm, we report theaverage log-relative error Δ and the average of the sample means (cid:10)Z 8 over the 5 runs. We also report the relative standarddeviation (RSD) over the 5 runs, where RSD is defined as follows. Let S[(cid:10)Z ] be the standard deviation and (cid:10)Z be the averageof the sample means over k runs of a solver, then8 The sample mean can be recovered from the log-relative error and the exact value of the weighted counts (see Eq. (65)). We report it in each table forthe reader’s convenience.60V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77RSD = 100 × S[(cid:10)Z ](cid:10)Z(66)Relative Standard Deviation (RSD) is a measure of precision and not accuracy. It is a unitless quantity and allows us tocompare standard deviation of two quantities which have different means (cid:10)Z more meaningfully. It is especially relevantwhen two given schemes have roughly the same accuracy. In this case, we would prefer the scheme having the smaller RSD.Also, when RSD is very small (for e.g., < 2%), it indicates that the scheme has very small sample variance and therefore itis likely that the proposal distribution is a very good approximation of the true posterior [2].Notation in tables (see for e.g. Table 1 for reference): The first column shows the instance name and various statisticalinformation about the instance such as the number of variables (n), the average domain size (k), the number of functions( f ), the number of evidence nodes (e), and the number of deterministic functions or constraints (c). The second columngives the exact value of the weighted counts ( Z ) if known, the treewidth (w) and the height (h) of the pseudo tree usedfor the minfill and the hmetis orderings respectively. Columns 3–8 show the average sample mean ((cid:10)Z ), the relative standarddeviation (RSD) and average log-relative error Δ over the 5 runs for each of the six solvers. The average log-relative errorof the best performing scheme for each problem instance is highlighted in bold.∗6.2.2. Results for the alarm networksOur first benchmark domain is that of alarm networks used in the UAI 2006 evaluation [26]. To create these networks, afixed number of copies of the burglar alarm graph described in Pearl’s book [12] are created. One by one, the graph copiesare connected to each of the previously considered copies with some probability. Each variable is then randomly set to behidden or observed.Table 1 shows the results. We make the following observations. First, on most instances the AND/OR sample tree andgraph means are slightly better in terms of log-relative error than the OR sample tree mean. Second, the log-relative errorand RSD values for all schemes are very small indicating that the proposal distribution is very close to the posterior dis-tribution. Third, in most cases the RSD of the AND/OR sample tree and graph means is smaller than the OR sample treemean. Finally, the performance of the schemes that use minfill and hmetis ordering is incomparable, sometimes the minfillis better while at other times hmetis is better.Fig. 12 show log-relative error vs time plots of various schemes for four randomly chosen alarm networks. We can clearlysee the superior anytime performance of AND/OR sample tree and graph means compared with the OR sample tree andgraph means. Note that importance sampling is an anytime algorithm because it needs just one sample to estimate theweighted counts. Moreover, the accuracy of its estimate improves as more samples are drawn.6.2.3. Results for grid networksThe grid networks are available from the authors of Cachet, a SAT model counter [27]. A grid Bayesian network is a s × sgrid, where there are two directed edges from a node to its neighbors right and down. The upper-left node is a source,and the bottom-right node is a sink. The deterministic ratio p is a parameter specifying the fraction of nodes that aredeterministic or functional. The grid instances are designated as p − s. For example, the instance 50–18 indicates a grid ofsize 18 × 18 in which 50% of the nodes are deterministic. Evidence in these networks was set at random.Tables 2, 3 and 4 show the results. The results are quite similar to the alarm networks in that on most instances theAND/OR graph schemes (both hmetis- and minfill-based) are superior in terms of accuracy and RSD to the AND/OR treeschemes which in turn are only slightly superior to the OR tree scheme. Again, the performance of the hmetis-based andminfill-based schemes is incomparable in that one ordering scheme does not strictly dominate the other.The log-relative error vs time plots for six largest grid instances (two for each value of the deterministic ratio) are shownin Figs. 13, 14 and 15 respectively. We clearly see the superior anytime performance of AND/OR graph schemes (bothhmetis-based and minfill-based) compared with the AND/OR tree and the OR tree schemes. The AND/OR tree scheme isonly slightly better than the OR tree scheme.6.2.4. Results for linkage networksThe linkage instances are Bayesian networks that model likelihood computation over a pedigree [28]. These networkshave between 777–2315 nodes with an average domain size of 9 or less. The linkage networks are generated by convertingbiological linkage analysis data into a Bayesian or a Markov network. Linkage analysis is a statistical method for mappinggenes onto a chromosome [29]. This is very useful in practice for identifying disease genes. The input is an ordered list ofloci L1, . . . , Lk+1 with allele frequencies at each locus and a pedigree with some individuals typed at some loci. The goalof linkage analysis is to evaluate the likelihood of a candidate vector [θ1, . . . , θk] of recombination fractions for the inputpedigree and locus order. The component θi is the candidate recombination fraction between the loci Li and Li+1.The pedigree data can be represented as a Bayesian network with three types of random variables: genetic loci variableswhich represent the genotypes of the individuals in the pedigree (two genetic loci variables per individual per locus, onefor the paternal allele and one for the maternal allele), phenotype variables, and selector variables which are auxiliaryvariables used to represent the gene flow in the pedigree. Fig. 16 represents a fragment of a network that describes parents-child interactions in a simple 2-loci analysis. The genetic loci variables of individual i at locus j are denoted by Li, jp andLi, jm. Variables Xi, j , S i, jp and S i, jm denote the phenotype variable, the paternal selector variable and the maternal selectorvariable of individual i at locus j, respectively. The conditional probability tables that correspond to the selector variablesV. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7761Fig. 12. Log-relative error versus time plots for four randomly chosen alarm networks.Fig. 13. Log-relative error versus time plots for the two largest Grid instances with Deterministic ratio = 50%.are parameterized by the recombination ratio θ . The remaining tables contain only deterministic information. It can beshown that given the pedigree data, computing the likelihood of the recombination fractions is equivalent to computing theprobability of evidence on the Bayesian network that model the problem (for more details consult [28]).Table 5 shows the results for linkage networks used in the UAI 2006 evaluation [26]. The AND/OR graph estimates arecloser to the exact value of P (e) than the AND/OR tree and the OR tree estimates except on the BN_69 instance on which62V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Table 2Results for Grid instances with Deterministic ratio = 50%. Table showing the average of sample means ((cid:10)Z ), the relative standard deviation of the samplemeans (RSD) and the average log-relative error (Δ) over five runs of or-tree-IJGP-IS-minfill, ao-tree-IJGP-IS-minfill, ao-graph-IJGP-IS-minfill, or-tree-IJGP-IS-hmetis, ao-tree-IJGP-IS-hmetis and ao-graph-IJGP-IS-hmetis after 1 hour of CPU time.Exactminfill orderinghmetis orderingZ∗, h)(w7.53e−07(8, 31)7.21e−05(8, 37)1.83e−04(13, 38)4.68e−06(11, 49)8.93e−05(19, 55)8.88e−04(25, 62)2.08e−04(14, 46)1.21e−03(28, 77)Instance(n, k, f )(e, c)50-12-5(144, 2, 144)(10, 62)50-14-5(196, 2, 196)(10, 93)50-15-5(225, 2, 225)(10, 111)50-16-5(256, 2, 256)(10, 125)50-17-5(289, 2, 289)(10, 138)50-18-5(324, 2, 324)(10, 153)50-19-5(361, 2, 361)(10, 172)50-20-5(400, 2, 400)(10, 190)or-treeIJGP-IS-minfill(cid:10)ZRSDΔ7.50e−070.51%3.12e−047.22e−050.25%2.08e−041.77e−045.31%6.24e−034.71e−065.88%3.78e−037.85e−0510.00%1.51e−029.99e−0425.40%3.46e−022.09e−043.00%3.06e−031.86e−0363.80%9.69e−02ao-treeIJGP-IS-minfill(cid:10)ZRSDΔ7.52e−070.13%9.44e−057.20e−050.16%1.59e−041.82e−040.97%9.71e−044.68e−061.78%1.13e−038.04e−054.43%1.14e−029.87e−0450.90%4.11e−022.07e−042.58%2.44e−033.18e−03101.00%1.06e−01ao-graphIJGP-IS-minfill(cid:10)ZRSDΔ7.53e−070.20%8.19e−057.21e−050.12%9.62e−051.81e−040.33%7.65e−044.69e−060.28%2.50e−048.93e−052.14%1.80e−038.89e−041.00%1.23e−032.08e−041.06%1.01e−031.21e−034.59%4.77e−03∗, h)(w(14, 21)(9, 18)(17, 29)(20, 30)(19, 36)(24, 41)(17, 31)(27, 46)or-treeIJGP-IS-hmetis(cid:10)ZRSDΔ7.53e−070.08%3.12e−057.21e−050.06%4.90e−051.82e−040.97%8.70e−044.67e−061.31%9.07e−049.29e−052.63%4.43e−038.33e−0434.20%3.76e−022.04e−041.11%2.08e−039.31e−0461.70%8.47e−02ao-treeIJGP-IS-hmetis(cid:10)ZRSDΔ7.53e−070.05%4.36e−057.21e−050.03%2.15e−051.82e−040.39%3.82e−044.67e−060.56%3.03e−048.85e−053.39%3.13e−038.40e−043.90%7.89e−032.08e−040.44%4.06e−041.14e−0330.70%3.57e−02ao-graphIJGP-IS-hmetis(cid:10)ZRSDΔ7.53e−070.05%4.32e−057.21e−050.03%2.15e−051.83e−040.47%4.27e−044.67e−060.35%2.13e−048.93e−050.68%6.30e−048.89e−041.78%2.25e−032.08e−040.32%2.93e−041.20e−032.07%3.06e−03Fig. 14. Log-relative error versus time plots for the two largest Grid instances with Deterministic ratio = 75%.the AND/OR tree scheme is the best. On instances such as BN_69 and BN_74 on which the accuracy of all schemes is roughlythe same, the AND/OR graph estimates have the smallest RSD. Again, we can see that the AND/OR tree estimates are slightlybetter than the OR tree estimates on all instances.V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7763Table 3Results for Grid instances with Deterministic ratio = 75%. Table showing the average of sample means ((cid:10)Z ), the relative standard deviation of the samplemeans (RSD) and the average log-relative error (Δ) over five runs of or-tree-IJGP-IS-minfill, ao-tree-IJGP-IS-minfill, ao-graph-IJGP-IS-minfill, or-tree-IJGP-IS-hmetis, ao-tree-IJGP-IS-hmetis and ao-graph-IJGP-IS-hmetis after 1 hour of CPU time.Exactminfill orderinghmetis orderingZ∗, h)(w4.15e−03(11, 41)7.20e−04(9, 39)4.38e−05(10, 47)3.70e−04(10, 78)3.52e−05(10, 39)5.00e−03(11, 44)1.32e−03(14, 65)2.15e−05(17, 67)7.43e−06(19, 52)3.10e−05(22, 84)6.75e−04(29, 79)Instance(n, k, f )(e, c)75-16-5(256, 2, 256)(10, 193)75-17-5(289, 2, 289)(10, 217)75-18-5(324, 2, 324)(10, 245)75-19-5(361, 2, 361)(10, 266)75-20-5(400, 2, 400)(10, 299)75-21-5(441, 2, 441)(10, 331)75-22-5(484, 2, 484)(10, 361)75-23-5(529, 2, 529)(10, 406)75-24-5(576, 2, 576)(10, 442)75-25-5(625, 2, 625)(10, 455)75-26-5(676, 2, 676)(10, 506)or-treeIJGP-IS-minfill(cid:10)ZRSDΔ4.14e−030.88%1.34e−037.17e−042.78%2.92e−034.39e−051.19%8.07e−043.68e−045.19%4.85e−033.52e−051.79%1.30e−035.00e−032.42%3.64e−031.23e−035.47%1.14e−022.12e−0526.80%1.81e−026.90e−0617.20%1.24e−022.35e−0529.90%3.17e−023.48e−04112.00%1.69e−01ao-treeIJGP-IS-minfill(cid:10)ZRSDΔ4.18e−030.78%1.36e−037.21e−041.58%1.54e−034.39e−050.55%4.17e−043.72e−044.33%4.11e−033.51e−050.61%4.61e−045.01e−030.72%1.17e−031.31e−035.28%6.27e−032.30e−0513.00%9.93e−036.98e−065.58%6.07e−032.76e−0519.70%1.94e−025.55e−04152.00%1.89e−01ao-graphIJGP-IS-minfill(cid:10)ZRSDΔ4.16e−030.51%6.79e−047.21e−040.74%7.82e−044.38e−050.27%1.85e−043.70e−040.90%9.13e−043.52e−050.73%4.77e−045.03e−030.87%1.75e−031.32e−032.13%2.77e−032.14e−051.16%9.00e−047.21e−061.84%2.50e−033.13e−053.07%1.84e−036.50e−048.29%9.74e−03∗, h)(w(17, 24)(17, 33)(15, 27)(18, 31)(13, 26)(16, 33)(15, 31)(26, 42)(20, 39)(25, 45)(29, 53)or-treeIJGP-IS-hmetis(cid:10)ZRSDΔ4.12e−030.30%1.33e−037.14e−041.11%1.44e−034.39e−052.34%2.01e−033.33e−041.80%1.35e−023.52e−051.05%7.85e−045.07e−033.10%5.45e−031.31e−032.28%2.58e−032.14e−055.60%4.16e−037.53e−063.68%2.29e−033.43e−0536.40%2.73e−024.65e−0447.10%6.75e−02ao-treeIJGP-IS-hmetis(cid:10)ZRSDΔ4.15e−030.23%4.28e−047.16e−040.30%6.25e−044.38e−051.71%1.27e−033.27e−040.69%1.55e−023.53e−050.54%4.01e−045.04e−031.22%2.40e−031.32e−030.91%1.06e−032.10e−051.03%2.15e−037.46e−061.19%6.84e−042.95e−055.63%4.84e−034.94e−0416.60%4.45e−02ao-graphIJGP-IS-hmetis(cid:10)ZRSDΔ4.15e−030.18%3.53e−047.16e−040.42%7.43e−044.37e−051.48%1.19e−033.27e−040.75%1.54e−023.53e−050.46%3.54e−045.04e−031.18%1.75e−031.32e−031.11%1.35e−032.13e−051.67%1.24e−037.61e−061.39%2.05e−033.11e−052.66%2.17e−036.80e−047.12%6.51e−03In Fig. 17, we show log-relative error vs time plots for four randomly chosen linkage instances. The AND/OR graphscheme exhibits superior anytime performance compared with the AND/OR tree scheme which in turn is superior to the ORtree scheme.Next, we present results on the (pedigree) linkage instances used in the UAI 2008 evaluation [30]. These are linkageBayesian networks in which evidence is instantiated yielding an un-normalized Bayesian network (namely a Markov net-work). In this subsection, we report on results for the 10 out of the 20 instances which were solved exactly in the UAI 2008evaluation. The results on the remaining 10 instances are presented in the next subsection. Table 6 shows the results. Fig. 18shows log relative error versus time plots for four randomly chosen instances. Again, we see a similar picture, namely theAND/OR graph scheme is superior to the other schemes.6.3. Results on networks for which the exact weighted counts are not knownWhen exact results are not available evaluating the capability of any approximation algorithm is problematic becausethe quality of the approximation (namely how close the approximation is to the exact) cannot be assessed. To allow somecomparison on such hard instances we evaluate the power of the various sampling schemes for yielding good lower-boundapproximations whose quality can be compared (the higher the better) even when the exact solution is not available.64V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Table 4Results for Grid instances with Deterministic ratio = 90%. Table showing the average of sample means ((cid:10)Z ), the relative standard deviation of the samplemeans (RSD) and the average log-relative error (Δ) over five runs of or-tree-IJGP-IS-minfill, ao-tree-IJGP-IS-minfill, ao-graph-IJGP-IS-minfill, or-tree-IJGP-IS-hmetis, ao-tree-IJGP-IS-hmetis and ao-graph-IJGP-IS-hmetis after 1 hour of CPU time.Exactminfill orderinghmetis orderingInstance(n, k, f )(e, c)90-24-5(576, 2, 576)(10, 528)90-25-5(625, 2, 625)(10, 553)90-26-5(676, 2, 676)(10, 597)90-30-5(900, 2, 900)(10, 792)90-34-5(1156, 2, 1156)(10, 1048)90-38-5(1444, 2, 1444)(10, 1300)90-42-5(1764, 2, 1764)(10, 1593)90-46-5(2116, 2, 2116)(10, 1904)90-50-5(2500, 2, 2500)(10, 2264)Z∗, h)(w3.90e−05(6, 35)2.17e−02(7, 25)2.67e−07(4, 16)3.94e−03(8, 45)1.31e−02(11, 59)7.08e−04(11, 60)4.70e−03(14, 65)2.13e−02(19, 87)1.20e−02(16, 79)or-treeIJGP-IS-minfill(cid:10)ZRSDΔ3.87e−050.78%8.94e−042.17e−020.41%9.76e−042.67e−070.35%2.58e−043.92e−032.90%3.98e−031.11e−0212.00%3.82e−026.06e−0429.30%3.59e−024.83e−0365.50%1.11e−011.80e−02111.00%2.34e−017.95e−0333.40%1.03e−01ao-treeIJGP-IS-minfill(cid:10)ZRSDΔ3.89e−050.24%3.40e−042.17e−020.33%8.01e−042.67e−070.28%1.59e−043.92e−030.94%1.45e−031.21e−027.02%1.98e−027.34e−043.98%6.19e−034.36e−0314.30%2.29e−021.63e−0224.40%7.70e−021.09e−0213.40%2.81e−02ao-graphIJGP-IS-minfill(cid:10)ZRSDΔ3.90e−050.23%2.16e−042.17e−020.32%7.55e−042.67e−070.29%1.73e−043.94e−030.62%8.14e−041.30e−021.84%3.30e−037.28e−042.77%4.14e−034.46e−035.24%9.65e−032.18e−0213.50%2.17e−021.18e−026.10%1.17e−02∗, h)(w(11, 18)(23, 50)(14, 23)(12, 23)(16, 26)(17, 32)(16, 49)(27, 51)(23, 60)or-treeIJGP-IS-hmetis(cid:10)ZRSDΔ3.90e−050.19%1.66e−042.17e−020.38%9.46e−042.66e−070.14%8.80e−053.94e−030.76%1.25e−031.32e−0215.80%2.87e−025.49e−046.31%3.53e−023.80e−0343.20%7.62e−021.44e−02121.00%2.88e−019.78e−0325.20%6.18e−02ao-treeIJGP-IS-hmetis(cid:10)ZRSDΔ3.90e−050.15%1.13e−042.16e−020.39%9.28e−042.66e−070.18%8.50e−053.95e−030.41%6.23e−041.28e−024.21%7.24e−035.76e−042.10%2.84e−024.52e−0313.90%2.15e−026.21e−02160.00%1.89e−019.86e−0324.70%5.34e−02ao-graphIJGP-IS-hmetis(cid:10)ZRSDΔ3.90e−050.15%1.12e−042.16e−020.34%8.14e−042.66e−070.17%8.75e−053.95e−030.35%5.43e−041.28e−022.93%6.81e−035.74e−041.47%2.90e−024.50e−036.81%1.21e−022.84e−0243.60%1.05e−011.04e−028.62%3.32e−02Fig. 15. Log-relative error versus time plots for the two largest Grid instances with Deterministic ratio = 90%.V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7765Fig. 16. A fragment of a Bayesian network used in genetic linkage analysis.Fig. 17. Log-relative error versus time plots for four sample linkage instances from the UAI 2006 evaluation.66V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Table 5Results for linkage instances from the UAI 2006 evaluation. Table showing the average of sample means ((cid:10)Z ), the relative standard deviation of the samplemeans (RSD) and the average log-relative error (Δ) over five runs of or-tree-IJGP-SS-minfill, ao-tree-IJGP-SS-minfill, ao-graph-IJGP-SS-minfill, or-tree-IJGP-SS-hmetis, ao-tree-IJGP-SS-hmetis and ao-graph-IJGP-SS-hmetis after 1 hour of CPU time.Exactminfill orderinghmetis orderingZ∗, h)(w5.28e−54(36, 52)2.00e−71(35, 110)5.12e−111(35, 80)4.21e−150(33, 107)2.26e−113(42, 97)3.75e−45(32, 70)5.88e−91(32, 116)4.93e−110(37, 139)6.88e−79(22, 114)Instance(n, k, f )(e, c)BN_69(777, 7, 777)(78, 402)BN_70(2315, 5, 2315)(159, 1290)BN_71(1740, 6, 1740)(202, 920)BN_72(2155, 6, 2155)(252, 1130)BN_73(2140, 5, 2140)(216, 1115)BN_74(749, 6, 749)(66, 374)BN_75(1820, 5, 1820)(155, 1000)BN_76(2155, 7, 2155)(169, 1130)BN_77(1020, 9, 1020)(135, 507)or-treeIJGP-SS-minfill(cid:10)ZRSDΔ2.58e−554.45%2.46e−027.81e−7782.90%7.81e−021.52e−116200.00%6.00e−021.01e−156147.00%5.01e−022.63e−122221.00%9.48e−022.46e−46101.00%3.44e−026.09e−9786.80%6.82e−023.08e−12392.70%1.27e−012.56e−8692.70%9.82e−02ao-treeIJGP-SS-minfill(cid:10)ZRSDΔ2.95e−5510.50%2.36e−021.20e−75185.00%6.78e−021.51e−116190.00%5.83e−026.97e−156192.00%4.70e−025.51e−122223.00%9.48e−023.51e−46152.00%3.30e−027.43e−97121.00%6.81e−024.15e−12386.20%1.23e−014.45e−8682.30%9.54e−02ao-graphIJGP-SS-minfill(cid:10)ZRSDΔ2.90e−552.55%2.37e−023.44e−7525.30%5.34e−021.85e−11358.70%2.26e−022.53e−15067.90%2.20e−033.29e−118201.00%4.98e−022.09e−4670.20%2.98e−021.07e−9599.50%5.47e−021.41e−118101.00%7.99e−021.52e−8481.60%7.39e−02∗, h)(w(27, 55)(44, 90)(39, 101)(35, 88)(39, 93)(34, 67)(37, 76)(38, 108)(57, 125)or-treeIJGP-SS-hmetis(cid:10)ZRSDΔ2.39e−5527.80%2.55e−027.37e−7483.10%3.58e−021.39e−115136.00%4.86e−022.71e−15383.70%2.26e−021.09e−123161.00%9.59e−027.27e−48218.00%9.14e−029.79e−98174.00%8.08e−028.42e−121213.00%1.07e−013.36e−83106.00%6.12e−02ao-treeIJGP-SS-hmetis(cid:10)ZRSDΔ2.83e−5521.10%2.40e−021.47e−7384.10%3.32e−021.11e−115122.00%4.75e−024.13e−15380.60%2.13e−022.80e−123201.00%9.52e−024.90e−48148.00%7.79e−027.03e−98145.00%8.01e−021.77e−120216.00%1.07e−011.69e−8384.10%6.14e−02ao-graphIJGP-SS-hmetis(cid:10)ZRSDΔ2.89e−556.45%2.37e−026.55e−7423.20%3.53e−026.63e−112211.00%1.66e−021.31e−15064.80%3.75e−031.33e−121207.00%8.15e−029.28e−4777.70%3.95e−021.47e−9431.60%4.01e−029.00e−120168.00%9.55e−023.76e−8164.60%3.01e−02Specifically, when the exact weighted counts are not known, we compare the lower bounds obtained by combining thesample means output by various schemes with the Markov inequality based lower bounding scheme presented in [31]. Suchlower bounding schemes, see also [32], take as input: (a) a set of unbiased sample means and (b) a real number 0 < α < 1,and output a lower bound on the weighted counts Z that is correct with probability greater than α.Formally, given a set of unbiased sample means, we can use the following theorem to get a probabilistic lower bound onthe weighted counts Z .Theorem 8. (See [32,31].) Let (cid:10)Z1, (cid:10)Z2, . . . , (cid:10)Zr be the unbiased sample means over “r” independent runs of a solver. Let 0 < α < 1 be aconstant and let β = ( 11r . Let Zlb be given by:1−α )Zlb = 1β×rmini=1(cid:10)Z iThen Zlb is a lower bound on Z with probability greater than α.(67)In our experiments, we set α = 0.99, r = 5 (namely, we run each algorithm five times and our lower bounds are correctr = 2.512. Note that when we evaluate the algorithms in terms of their lowerwith probability greater than 0.99) and β = ( 1bounds, the higher the lower bound the better the corresponding scheme is.1−α )1V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7767Fig. 18. Log-relative error versus time plots for four sample linkage instances from the UAI 2008 evaluation.6.3.1. Results for the linkage instancesTable 7 shows the results for the 10 linkage instances used in the UAI 2008 evaluation for which the exact weightedcounts are not known. Note that in each cell, we report the lower bound Zlb, the average sample mean (cid:10)Z 9 and the RSDover the 5 runs. We can clearly see that on all the instances the AND/OR graph scheme yields substantially higher lowerbounds than the AND/OR tree scheme which in turn yields higher lower bounds than the OR tree scheme. The RSD of theAND/OR graph scheme is also smaller than other schemes.6.3.2. Results for random coding networksThe random coding networks are a class of linear block codes [33]. They can be represented as four-layer belief networks.The second and third layer correspond to input information bits and parity check bits respectively. Each parity check bitrepresents a XOR function of input bits. Input and parity check nodes are binary while the output nodes are real-valued.Each layer has the same number of nodes because a code rate of R = K /N = 1/2 is used, where K is the number of inputbits and N is the number of transmitted bits.Given a number of input bits (K = 128), number of parents ( P = 4) for each XOR bit and channel noise variance (σ =0.40), a coding network structure is generated by randomly picking parents for each XOR node. Then, an input signal issimulated by a assuming a uniform random distribution of information bits, the corresponding values of the parity checkbits are computed, and an assignment to the output nodes is generated by assuming adding a Gaussian noise to eachinformation and parity check bit.Table 8 shows the results. Unlike other benchmarks, we can see that the AND/OR graph scheme is only slightly betterthan the AND/OR tree and the OR tree schemes. The improvement in accuracy is small because the IJGP-based proposal dis-tribution is quite close to the exact posterior distribution as indicated by a relatively smaller RSD (< 2% on most instances).9 Note that the average sample mean is shown for the sake of convenience of the reader. It is not relevant for making comparisons between theperformance of various schemes when the exact weighted counts are not known.68V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Table 6Results for linkage instances from the UAI 2008 evaluation. Table showing the average of sample means ((cid:10)Z ), the relative standard deviation of the samplemeans (RSD) and the average log-relative error (Δ) over five runs of or-tree-IJGP-SS-minfill, ao-tree-IJGP-SS-minfill, ao-graph-IJGP-SS-minfill, or-tree-IJGP-SS-hmetis, ao-tree-IJGP-SS-hmetis and ao-graph-IJGP-SS-hmetis after 1 hour of CPU time.Exactminfill orderinghmetis orderingZ∗, h)(w7.81e−15(20, 51)7.18e−79(24, 93)2.34e−30(25, 58)2.78e−39(29, 56)1.69e−116(26, 82)1.84e−84(27, 89)2.63e−117(29, 56)5.64e−55(17, 69)6.32e−103(26, 87)1.73e−31(27, 52)Instance(n, k, f )(e, c)pedigree1(334, 2, 334)(0, 121)pedigree18(1184, 2, 1184)(0, 386)pedigree20(437, 2, 437)(0, 147)pedigree23(402, 2, 402)(0, 130)pedigree25(1289, 2, 1289)(0, 396)pedigree30(1289, 2, 1289)(0, 413)pedigree37(1032, 2, 1032)(0, 333)pedigree38(724, 2, 724)(0, 263)pedigree39(1272, 2, 1272)(0, 354)pedigree42(448, 2, 448)(0, 156)or-treeIJGP-SS-minfill(cid:10)ZRSDΔ7.70e−153.27%9.13e−041.94e−82119.00%4.86e−029.09e−34114.00%1.33e−012.64e−3916.80%1.69e−034.85e−125222.00%9.05e−023.20e−87224.00%7.01e−022.50e−119100.00%1.88e−023.61e−56212.00%4.01e−027.83e−109216.00%7.02e−021.62e−318.79%1.38e−03ao-treeIJGP-SS-minfill(cid:10)ZRSDΔ7.92e−155.04%1.09e−031.33e−82125.00%5.08e−021.21e−32206.00%1.23e−012.52e−3910.40%1.14e−034.05e−122173.00%5.55e−021.47e−87223.00%6.92e−021.25e−11717.90%2.83e−035.27e−5618.80%1.91e−021.34e−106134.00%4.35e−021.56e−313.45%1.44e−03ao-graphIJGP-SS-minfill(cid:10)ZRSDΔ7.83e−150.98%2.73e−044.04e−7968.20%4.51e−032.18e−3023.40%2.93e−032.90e−395.55%4.68e−041.45e−11621.70%8.51e−046.97e−8524.30%5.16e−031.18e−1175.68%3.01e−031.45e−5517.20%1.10e−025.57e−1038.02%5.45e−041.74e−313.26%3.45e−04∗, h)(w(19, 41)(31, 70)(27, 50)(24, 43)(47, 86)(28, 66)(47, 56)(53, 69)(31, 62)(27, 50)or-treeIJGP-SS-hmetis(cid:10)ZRSDΔ7.45e−159.12%2.39e−031.17e−81123.00%4.26e−024.07e−33218.00%1.65e−012.37e−3915.10%2.05e−032.88e−121175.00%4.81e−022.50e−85218.00%2.45e−021.47e−119201.00%2.53e−022.83e−6285.20%1.41e−013.71e−10482.80%1.32e−021.73e−3110.60%1.27e−03ao-treeIJGP-SS-hmetis(cid:10)ZRSDΔ7.69e−151.66%5.70e−041.53e−81119.00%3.88e−029.11e−32196.00%8.46e−022.72e−392.92%3.55e−041.98e−118154.00%2.31e−028.01e−85122.00%1.20e−021.26e−11796.40%4.59e−031.56e−55138.00%1.72e−023.10e−103115.00%5.32e−031.61e−316.30%1.05e−03ao-graphIJGP-SS-hmetis(cid:10)ZRSDΔ7.75e−151.88%5.23e−047.01e−7930.10%1.42e−031.43e−3038.40%8.07e−032.72e−393.06%3.44e−044.71e−11732.10%4.97e−031.63e−8469.70%3.03e−031.17e−11717.40%3.05e−031.13e−5584.20%1.47e−025.39e−10311.00%6.93e−041.72e−315.95%6.34e−04Consequently, the OR sample tree mean is already quite accurate. Our results are consistent with previous studies [22,21,20]which demonstrated that (generalized) belief propagation yields very good approximation to the true posterior on randomcoding networks.6.3.3. Results for graph coloring problemsOur final domain is that of 4-coloring problems generated using Joseph Culberson’s flat graph coloring generator.10 Here,we are interested in counting the number of solutions of the graph coloring instance. Table 9 shows the results. We observethat AND/OR tree and graph sampling schemes yield higher lower bounds than the OR tree sampling schemes.6.4. Summary of experimentsIn summary, our experiments show that the AND/OR sample graph mean is substantially superior in terms of accuracyand precision to the AND/OR sample tree mean which in turn is only slightly superior to the OR sample tree mean. Inparticular, as the problem size gets larger and instances get harder for exact inference, the AND/OR graph scheme is severalorders of magnitude superior. As expected, when the proposal distribution is close to the posterior distribution (see, forexample, the results on the alarm networks), there is no difference in the performance between the OR and AND/OR10 Available at http://www.cs.ualberta.ca/~joe/Coloring/.V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7769Table 7Results for linkage instances from the UAI 2008 evaluation. Table showing the average of sample means ((cid:10)Z ), the relative standard deviation of the samplemeans (RSD) and the lower bound ((cid:10)Zlb ) on the weighted counts (with 99% confidence) over five runs of or-tree-IJGP-SS-minfill, ao-tree-IJGP-SS-minfill,ao-graph-IJGP-SS-minfill, or-tree-IJGP-SS-hmetis, ao-tree-IJGP-SS-hmetis and ao-graph-IJGP-SS-hmetis after 1 hour of CPU time.Exactminfill orderinghmetis orderingInstance(n, k, f )(e, c)pedigree13(1077, 2, 1077)(0, 343)pedigree19(793, 2, 793)(0, 286)pedigree31(1183, 2, 1183)(0, 389)pedigree34(1160, 2, 1160)(0, 348)pedigree40(1030, 2, 1030)(0, 351)pedigree41(1062, 2, 1062)(0, 346)pedigree44(811, 2, 811)(0, 287)pedigree51(1152, 2, 1152)(0, 383)pedigree7(1068, 2, 1068)(0, 315)pedigree9(1118, 2, 1118)(0, 386)Z––––––––––∗, h)(w(36, 115)(23, 121)(38, 124)(37, 109)(29, 122)(34, 92)(30, 97)(42, 92)(36, 96)(28, 108)or-treeIJGP-SS-minfill(cid:10)ZRSDZlb2.40e−44136.00%2.43e−461.21e−66213.00%9.15e−704.64e−81127.00%4.66e−824.27e−7688.60%1.91e−77ao-treeIJGP-SS-minfill(cid:10)ZRSDZlb6.77e−44138.00%8.89e−476.82e−67206.00%1.17e−692.89e−78196.00%1.99e−807.76e−73143.00%8.45e−741.38e−97219.00%7.71e−1021.40e−97218.00%1.30e−1018.05e−85215.00%8.51e−892.98e−65146.00%1.54e−663.80e−78193.00%1.14e−811.35e−72171.00%5.02e−755.01e−83216.00%5.74e−874.35e−83139.00%4.44e−861.48e−64212.00%2.20e−664.19e−77218.00%2.53e−801.30e−7178.10%1.71e−722.20e−83197.00%7.72e−87ao-graphIJGP-SS-minfill(cid:10)ZRSDZlb7.46e−3367.60%4.70e−347.66e−62125.00%8.14e−635.58e−7387.90%4.67e−741.61e−67177.00%2.22e−693.16e−92168.00%2.08e−944.03e−78135.00%6.49e−803.25e−6436.00%5.93e−659.47e−75125.00%3.10e−762.70e−66120.00%7.92e−682.55e−8068.40%3.04e−81∗, h)(w(48, 72)(35, 63)(43, 77)(39, 69)(36, 86)(39, 74)(31, 59)(46, 87)(39, 74)(36, 68)or-treeIJGP-SS-hmetis(cid:10)ZRSDZlb1.85e−3563.30%3.09e−378.13e−6694.80%7.85e−685.02e−75209.00%5.35e−773.81e−70218.00%2.00e−732.18e−9799.30%3.04e−986.76e−85139.00%1.12e−861.50e−64131.00%1.00e−654.24e−79109.00%4.62e−812.81e−70100.00%8.41e−725.14e−84178.00%2.88e−86ao-treeIJGP-SS-hmetis(cid:10)ZRSDZlb1.06e−3576.40%3.97e−371.33e−65143.00%7.17e−685.21e−75192.00%4.04e−771.42e−68114.00%3.51e−713.52e−97147.00%1.43e−987.22e−8495.10%2.60e−854.20e−64139.00%1.70e−654.85e−77222.00%3.72e−802.33e−67101.00%4.08e−704.03e−83216.00%4.48e−86ao-graphIJGP-SS-hmetis(cid:10)ZRSDZlb9.60e−32159.00%2.37e−335.28e−6247.00%6.81e−632.68e−7367.80%9.13e−758.30e−66200.00%3.48e−683.52e−9299.20%7.12e−958.35e−7889.10%8.65e−791.30e−6428.10%3.32e−656.62e−7687.60%4.91e−773.32e−6618.50%9.06e−672.05e−80110.00%1.90e−81estimates as well as between AND/OR tree and AND/OR graph estimates. We experimented with two orderings, one basedon minfill and the second based on hmetis, for constructing the pseudo trees. It is known and we also observe here thatminfill is superior in generating small treewidth pseudo trees while hmetis is superior in generating small height pseudotrees. We found that the two orderings are not comparable in terms of accuracy of estimation because they yield differentproposal distributions whose relative accuracy compared with the posterior distribution is not well understood at this point.We leave this issue for future research.7. Discussion and related work7.1. Relation to other graph-based variance reduction schemesThe work presented here is related to the work by Hernandez and Moral [34], Kjærulff [35], Dawid et al. [36] whoperform sampling-based inference on a junction tree and organize samples in such a way that more virtual samples aregenerated. The main idea in these papers is to perform message passing on a junction tree by substituting messages whichare too hard to compute exactly by their sampling-based approximations. Kjærulff [35] and Dawid et al. [36] use Gibbssampling while Hernandez and Moral [34] use importance sampling to approximate the messages. Another related work isthat of Bouckaert et al. [37] who use search trees to implement stratified sampling efficiently. Similar to some recent workson Rao–Blackwellized sampling such as [38,39,15], variance reduction is achieved in these junction -tree based samplingschemes because of some exact computations; as dictated by the Rao–Blackwell theorem. AND/OR estimation is based on a70V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Table 8Results for random coding networks from the UAI 2006 evaluation. Table showing the average of sample means ((cid:10)Z ), the relative standard deviation of thesample means (RSD) and the lower bound ((cid:10)Zlb ) on the weighted counts (with 99% confidence) over five runs of or-tree-IJGP-SS-minfill, ao-tree-IJGP-SS-minfill, ao-graph-IJGP-SS-minfill, or-tree-IJGP-SS-hmetis, ao-tree-IJGP-SS-hmetis and ao-graph-IJGP-SS-hmetis after 1 hour of CPU time.Exactminfill orderinghmetis orderingInstance(n, k, f )(e, c)BN_126(512, 2, 512)(256, 384)BN_127(512, 2, 512)(256, 384)BN_128(512, 2, 512)(256, 384)BN_129(512, 2, 512)(256, 384)BN_130(512, 2, 512)(256, 384)BN_131(512, 2, 512)(256, 384)BN_132(512, 2, 512)(256, 384)BN_133(512, 2, 512)(256, 384)BN_134(512, 2, 512)(256, 384)Z–––––––––∗, h)(w(55, 65)(54, 71)(49, 68)(53, 66)(53, 63)(53, 66)(51, 64)(55, 67)(55, 64)or-treeIJGP-SS-minfill(cid:10)ZRSDZlb2.07e−560.64%8.15e−573.04e−587.65%1.12e−584.86e−480.24%1.93e−484.18e−621.16%1.64e−623.78e−580.87%1.49e−582.30e−540.54%9.08e−556.58e−6519.40%2.00e−652.35e−540.95%9.26e−556.10e−570.20%2.43e−57ao-treeIJGP-SS-minfill(cid:10)ZRSDZlb2.05e−560.76%8.08e−573.06e−580.88%1.20e−584.87e−480.17%1.93e−484.19e−620.73%1.65e−623.79e−580.44%1.50e−582.29e−540.57%9.05e−556.90e−654.84%2.60e−652.37e−542.44%9.28e−556.12e−570.28%2.43e−57ao-graphIJGP-SS-minfill(cid:10)ZRSDZlb2.05e−560.65%8.09e−573.07e−580.57%1.21e−584.87e−480.17%1.93e−484.19e−620.79%1.65e−623.79e−580.54%1.50e−582.29e−540.32%9.08e−556.95e−654.42%2.60e−652.37e−542.32%9.29e−556.12e−570.32%2.43e−57∗, h)(w(54, 66)(57, 66)(52, 61)(52, 66)(52, 63)(51, 63)(51, 66)(55, 65)(53, 62)or-treeIJGP-SS-hmetis(cid:10)ZRSDZlb2.05e−560.57%8.08e−573.06e−584.06%1.16e−584.87e−480.15%1.94e−485.66e−6231.10%1.52e−623.79e−580.70%1.50e−582.30e−540.98%9.01e−551.00e−6479.70%1.83e−652.35e−541.06%9.24e−556.11e−570.53%2.42e−57ao-treeIJGP-SS-hmetis(cid:10)ZRSDZlb2.04e−560.39%8.10e−573.07e−581.76%1.20e−584.87e−480.08%1.94e−484.19e−629.08%1.56e−623.79e−580.47%1.50e−582.30e−540.93%9.08e−552.21e−64161.00%2.04e−652.36e−540.88%9.34e−556.11e−570.26%2.42e−57ao-graphIJGP-SS-hmetis(cid:10)ZRSDZlb2.04e−560.19%8.11e−573.06e−581.23%1.20e−584.87e−480.08%1.94e−484.22e−624.33%1.57e−623.80e−580.39%1.50e−582.31e−540.82%9.11e−557.20e−6525.10%2.12e−652.36e−541.17%9.31e−556.11e−570.26%2.42e−57fundamentally different principle; it achieves variance reduction by using conditional independence to derive more virtualsamples. In fact, as we show in Gogate [16], Gogate and Dechter [40], variance reduction due to Rao–Blackwellization isorthogonal to the one achieved by AND/OR-based estimation and therefore the two can be combined to achieve furthervariance reduction.7.2. Hoeffding’s U -statisticsAND/OR-estimates are also closely related to cross match estimates [41] which are based on Hoeffding’s U -statistics. Toderive cross-match estimates, the original function over a set of variables is divided into several marginal functions whichare defined only on a subset of variables. Then, each marginal function is sampled independently and the cross-matchsample mean is derived by considering all possible combinations of the samples. For example, if there are k marginalfunctions and m samples are taken over each function, the cross match sample mean is computed over mk combinations.It was shown in Kong et al. [41] that the cross match sample mean has lower variance than the conventional samplemean; similar to our work. The only caveat in cross match estimates is that it requires exponentially more time O (mk) tocompute the estimates as compared to O (m) for conventional estimates; making their direct application infeasible for largevalues of k. So the authors suggest resampling from the possible O (mk) samples with the hope that the estimates basedon the resampled samples would have smaller variance than the conventional one. Unlike, cross match estimates, the mostcomplex AND/OR estimates are only wis the treewidth, as compared to theconventional estimates, and therefore do not require the extra resampling step.times more expensive time wise, where w∗∗V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7771Table 9Results for 4-coloring instances generated using Joseph Culberson’s flat graph coloring generator. Table showing the average of sample means ((cid:10)Z ), therelative standard deviation of the sample means (RSD) and the lower bound ((cid:10)Zlb ) on the weighted counts (with 99% confidence) over five runs of or-tree-IJGP-SS-minfill, ao-tree-IJGP-SS-minfill, ao-graph-IJGP-SS-minfill, or-tree-IJGP-SS-hmetis, ao-tree-IJGP-SS-hmetis and ao-graph-IJGP-SS-hmetis after 1 hour ofCPU time.Exactminfill orderinghmetis orderingInstance(n, k, f )(e, c)4-coloring1(400, 2, 2026)(0, 2026)4-coloring2(400, 2, 2205)(0, 2205)4-coloring3(800, 2, 4065)(0, 4065)4-coloring4(800, 2, 4419)(0, 4419)4-coloring5(1200, 2, 6455)(0, 6455)4-coloring6(1200, 2, 6641)(0, 6641)Z––––––∗, h)(w(71, 87)(95, 113)(144, 171)(196, 225)(260, 301)(290, 321)or-treeIJGP-SS-minfill(cid:10)ZRSDZlb1.16e+3721.60%3.66e+361.41e+3058.40%3.95e+293.13e+7237.20%5.28e+717.97e+63106.00%6.45e+624.42e+9879.60%6.60e+971.66e+90126.00%1.11e+88ao-treeIJGP-SS-minfill(cid:10)ZRSDZlb1.11e+3717.30%3.72e+361.17e+3035.80%3.50e+293.43e+7230.80%9.00e+718.22e+63127.00%7.76e+624.64e+99203.00%6.65e+971.07e+90108.00%2.22e+88ao-graphIJGP-SS-minfill(cid:10)ZRSDZlb2.05e+3724.50%5.77e+362.92e+3057.70%4.93e+291.96e+7376.00%3.64e+725.84e+6431.80%1.77e+641.60e+101196.00%1.85e+998.56e+91127.00%6.52e+90∗, h)(w(67, 90)(84, 105)(129, 160)(165, 196)(232, 272)(264, 307)or-treeIJGP-SS-hmetis(cid:10)ZRSDZlb1.44e+3726.30%3.85e+361.29e+3057.50%2.67e+294.30e+7260.10%2.52e+712.08e+6356.50%2.06e+625.97e+97120.00%5.37e+962.57e+89109.00%2.82e+88ao-treeIJGP-SS-hmetis(cid:10)ZRSDZlb1.83e+3736.40%4.51e+363.63e+30153.00%1.78e+294.91e+7269.00%2.89e+712.46e+6350.60%4.45e+625.10e+9757.80%4.26e+962.59e+8980.60%3.60e+88ao-graphIJGP-SS-hmetis(cid:10)ZRSDZlb2.23e+3724.10%6.51e+361.55e+3018.00%4.53e+291.12e+7357.00%2.64e+722.86e+6476.30%3.75e+634.04e+99100.00%1.63e+981.53e+91139.00%1.18e+907.3. Problem with large sample sizesGiven that the space complexity of computing the AND/OR sample graph mean and all marginal probabilities is O (nN),the reader may think that as more samples are drawn our algorithms would run out of memory. One can, however, performmulti-stage (adaptive) sampling to circumvent this problem. Here, at each stage we stop storing samples when a pre-specified memory limit is reached. Then the AND/OR sample graph mean is computed from the stored samples and thesamples can be discarded, repeating the process until the stipulated time bound expires or enough samples are drawn. Thefinal sample mean is then simply the average of sample means computed at each stage. It is obvious that the final samplemean will have smaller variance than the OR sample tree mean but will be less accurate compared with the AND/OR samplegraph mean.7.4. Impact of determinism and context specific independenceAND/OR sampling is based on a simple viewpoint: “make the most out of the generated samples”. This is especially use-ful when the graphical model has structural features such as determinism and context specific independence (CSI) [42]. It iseasy to show that the problem of generating a sample that has non-zero weight (namely a useful sample) from a graphicalmodel that has deterministic dependencies is equivalent to the problem of finding a model of a satisfiability formula [43].Because the latter problem is NP-complete, very few useful samples may be generated. Many schemes have been proposedin literature for generating samples from such hard graphical models that have both probabilistic and deterministic relation-ships [44,23,45,17,46]. Out of these, as demonstrated in our prior work [19], SampleSearch is currently the best performingalternative. In this paper, we showed that on many deterministic networks from the linkage analysis domain, AND/OR sam-ple graph mean (computed from the samples generated by SampleSearch) is substantially more accurate than the OR sampletree mean. This shows the power of using AND/OR estimation in hard, deterministic graphical models. Another advantage ofusing AND/OR sample graph mean in such networks is that it can take advantage of many implicit conditional independen-cies that are not elucidated by the primal graph [47,48]. These implicit dependencies can further increase the virtual samplesize resulting in an improved accuracy.8. ConclusionThe primary contribution of this paper is in viewing importance sampling-based estimation in the context of AND/ORsearch spaces for graphical models [6]. Specifically, we viewed sampling as a partial exploration of the full AND/OR search72V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77space (called AND/OR sample tree) and defined a process for computing an unbiased sample mean over an AND/OR sampletree. We proved that the conventional sample mean (running average) is equal to computing the AND/OR sample mean onan OR sample tree and is therefore impervious to problem decomposition. Arranging the same samples over an AND/ORsample tree which is sensitive to problem decomposition yields more virtual samples and therefore a better sample meanthat has smaller variance. Since the AND/OR sample tree mean has the same time complexity and only slightly more spaceoverhead than the OR sample tree mean; it should always be preferred.We extended the AND/OR sample tree mean to AND/OR sample graph mean that further utilizes problem decompositionby merging identical subtrees. The AND/OR sample graph yields more virtual samples than the AND/OR sample tree and∗)therefore reduces variance even further. However, computing the AND/OR sample graph mean requires a factor of O (wmore time and O (N) times more space which introduces various time and space versus accuracy trade-offs.We focused our empirical investigation on the task of computing the probability of evidence in a Bayesian network andthe partition function in a Markov network. The main aim of our evaluation was to compare the impact of exploiting varyinglevels of graph decompositions via (a) OR tree, (b) AND/OR tree, and (c) AND/OR graph on the accuracy of sample mean.Our results demonstrated conclusively that in many cases the scheme that exploits the most decomposition, the AND/ORsample graph mean is consistently superior. Our results also show that AND/OR sample tree mean is slightly better in termsof accuracy than the OR sample tree mean.Future work. The AND/OR sampling framework leaves plenty of avenues for future work. For instance, because AND/ORsampling and Rao–Blackwellization are orthogonal in nature, a combination of the two needs to be explored further. Someinitial results on this combination are presented in the first authors’ thesis [16] and in a recent conference paper [40].A second line of future work is based on the observation that the AND/OR sampling framework only utilizes conditionalindependencies partially, namely, only those uncovered by the primal graph of the graphical model. It is known that theprimal graph captures only a subset of the conditional independencies. New unknown independencies could be elucidatedwhile sampling through the AND/OR space and via AND/OR sampling theory we know that sampling error can only decreaseif we utilize them. How to guide sampling to uncover unknown independencies, however, is still an open problem. A thirdline of future research is to develop AND/OR estimators for other sampling techniques such as Gibbs sampling [49] andstratified sampling [37]. A fourth line of future work is developing memory efficient algorithms for estimating all posteriormarginal probabilities. As discussed in Section 3.3, unlike the conventional OR tree estimator which requires O (n) space,the AND/OR estimator proposed in this paper is memory intensive (complexity O (nN)) and requires storing the full AND/ORsample tree in memory.AcknowledgementsThis work was supported in part by the NSF under award numbers IIS-1065618, IIS-0331707, IIS-0412854 and IIS-0713118 and by the NIH grant R01-HG004175. The authors would like to thank anonymous reviewers and the associateeditor for their valuable comments and suggestions that substantially helped improve our earlier drafts.Appendix A. Algorithm interleaving sampling and estimationAlgorithm 3: Interleaved AND/OR tree importance sampling (IAOTS).Input: A graphical mode G = (cid:5)X, D, F(cid:6) a pseudo tree T (X, E), a proposal distribution defined relative to T : Q (X) =(cid:6)ni=1 Q i ( Xi |contextT ( Xi )), aninteger N > 0, a variable Xi , an assignment x.Output: The value of the OR node corresponding to Xi in an AND/OR sample tree defined relative to G, x, T and Q .1 v( Xi ) = 0;2 Generate N samples of Xi from Q i ( Xi |xcontextT ( Xi ))3 Let {xi,1, . . . , xi,d} be the set of the values of Xi that are sampled N j > 0 times;4 for j = 1 to d do56if Xi is a leaf node of T thenv(xi, j ) = 17891011else(cid:8) = (x, xi, j )xLet {C1, . . . , C p } be the set of child nodes of Xi in T ;(cid:6)(cid:8))v(xi, j ) =k=1 IAOTS(G, T , Q , N j , Ck, xpv( Xi ) = v( Xi ) +B T ,Xi(xi, j ,x)Q i (xi, j |xcontextT ( Xi ))× N j × v(xi, j )12 return v( Xi )/NIn this section, we present a recursive algorithm that interleaves sampling with AND/OR-based estimation (see Al-gorithm 3). The algorithm takes as input a graphical model G, a pseudo tree T , a proposal distribution Q (X) =(cid:6)i=1 Q i( Xi|contextT ( Xi)), a variable Xi to be sampled, an integer N that denotes the number of times Xi should be sam-npled and the current assignment x. The algorithm returns the value of the OR node corresponding to Xi in an AND/ORV. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7773sample tree defined relative to G, x, T and Q . First, given the current assignment x, the algorithm generates N samples ofXi from Q i( Xi|xcontextT ( Xi )). Then, in the for-loop, it computes the numerator of the value of the OR node (see Definition 15)corresponding to Xi by iterating over all sampled values xi, j of Xi and computing their values v(xi, j). The algorithm com-putes v(xi, j) as follows. If Xi is the leaf node of T then by definition (see Definition 15), v(xi, j) equals 1. If Xi is not aleaf node then by definition, v(xi, j) equals the product of the values of its child OR nodes, where there is a child OR nodecorresponding to each child node Ck of Xi in T . The value of each OR node corresponding to Ck is, in turn, computed bycalling the algorithm recursively. Finally, the algorithm returns the value of the OR node corresponding to Xi . It is easy toshow that:Theorem 9. Given a graphical model G = (cid:5)X, D, F(cid:6), a pseudo tree T , a proposal distribution Q (X) =i=1 Q i( Xi|contextT ( Xi)) andan integer N, Algorithm IAOTS(G, T , Q , N, X1, ∅) correctly computes the AND/OR sample tree mean where X1 is the root node of T .(cid:6)nAppendix B. ProofsProof of Theorem 1. We prove by induction that the value of any OR node n is an unbiased estimate of the conditionalexpectation of the subproblem rooted at n (conditioned on the assignment from the root to n).Consider the expression for weighted counts:Z =(cid:4)m(cid:3)x∈Xi=1F i(x)(B.1)Let T (X, E) be a pseudo tree, pathT ( Xi) be the set of variables along the path from root up to node Xi (note thatpathT ( Xi) does not include Xi ) in T and B T , Xi be the bucket function (see Definition 13) of Xi w.r.t. T . For any fullassignment x, we have:n(cid:3)i=1B T , Xi (xi, xpathT ( Xi )) =m(cid:3)i=1F i(x)Recall that xpathT ( Xi ) is the projection of the assignment x onto the subset pathT ( Xi) of X.Substituting Eq. (B.2) into Eq. (B.1), we get:(cid:4)n(cid:3)Z =B T , Xi (xi, xpathT ( Xi ))(B.2)(B.3)x∈Xi=1(cid:6)nLet Q (x) =can express Q as:i=1 Q i(xi|yi) be the proposal distribution where Yi ⊆ contextT ( Xi). Because contextT ( Xi) ⊆ pathT ( Xi), weQ (x) =n(cid:3)i=1Q i(xi|xYi ) =n(cid:3)i=1Q i(xi|xpathT ( Xi ))We can express Z in Eq. (B.3) in terms of Q as:Z =(cid:4)n(cid:3)x∈Xi=1B T , Xi (xpathT ( Xi ))Q i(xi|xpathT ( Xi ))Q i(xi|xpathT ( Xi ))(B.4)(B.5)Using the notation xi = (x1, . . . , xi) and xi,pathT ( X j ) as the projection of xi on pathT ( X j) and migrating the functions tothe left of summation variables which it does not reference, we can rewrite Eq. (B.5) as:Z =(cid:4)x1∈ X1B T , X1 (x1)Q 1(x1)Q 1(x1) × · · ·(cid:4)xi ∈ Xi(cid:4)××xn∈ XnB T , Xi (xi, xi−1,pathT ( Xi ))Q i(xi|xi−1,pathT ( Xi ))Q i(xi|xi−1,pathT ( Xi )) × · · ·B T , Xi (xi, xn−1,pathT ( Xn))Q n(xn|xn−1,pathT ( Xn))Q n(xn|xn−1,pathT ( Xn))(B.6)74V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Using the definition of expectation and conditional expectation, we can rewrite Eq. (B.6) as:(cid:13)Z = Ex(cid:13)× · · · × ExB T , X1 (x1)Q 1(x1)(cid:13)B T , Xn (xn, xn−1,pathT ( Xn))× ExQ n(xn|xn−1,pathT ( Xn))B T , Xi (xi, xi−1,pathT ( Xi ))Q i(xi|xi−1,pathT ( Xi ))(cid:14)(cid:8)(cid:8)(cid:8) xn−1,pathT ( Xn)· · ·× · · ·(cid:8)(cid:8)(cid:8) xi−1,pathT ( Xi )(cid:14)(cid:14)· · ·(B.7)Let chi( Xi) be the set of children of Xi in the pseudo tree T and let us denote the component of conditional expectationat a node Xi , along an assignment xi−1,pathT ( Xi ) by V Xi ( Xi|xi−1,pathT ( Xi )). V Xi ( Xi|xi−1,pathT ( Xi )) can be recursively defined asfollows:(cid:13)V Xi (Xi|xi−1,pathT ( Xi )) = ExB T , Xi (xi, xi−1,pathT ( Xi ))Q i(xi|xi−1,pathT ( Xi ))(cid:3)V X j (X j|xi, xi−1,pathT ( Xi ))(cid:14)(cid:8)(cid:8)(cid:8) xi−1,pathT ( Xi )×X j ∈chi( Xi )It is easy to see that Z equals V X1 ( X1), namely,(cid:13)Z = ExB T , X1 (X1)Q 1(X1)(cid:3)(cid:14)V X j (X j|x1)= V X1 (X1)X j ∈chi( X1)(B.8)(B.9)We will now derive an unbiased estimate of V Xi ( Xi|xi−1,pathT ( Xi )). Assume that for all X j ∈ chi( Xi) in T , we have anunbiased estimate of V X j ( X j|xi, xi−1,pathT ( Xi )) denoted by (cid:10)v X j ( X j|xi, xi−1,pathT ( Xi )). Assume that given xi−1,pathT ( Xi ), we havei ) from Q i( Xi|xi−1,pathT ( Xi )). By replacing the (conditional) expectation by its sample average,generated N samples (x1we get the following unbiased estimate of V Xi ( Xi|xi−1,pathT ( Xi )).i , . . . , xN(cid:10)v Xi (Xi|xi−1,pathT ( Xi )) = 1NN(cid:4)a=1B T , Xi (xaQ i(xaii , xi−1,pathT ( Xi ))|xi−1,pathT ( Xi ))(cid:3)(cid:7)X j(cid:10)v X j(cid:8)(cid:8)xai , xi−1,pathT ( Xi )(cid:9)X j ∈chi( Xi )(B.10)Assume that the domain of Xi is {xi,1, . . . , xi,k}. Also, assume that each value xi, j is sampled Ni, j times. By collecting(cid:2)ka=1 Ni,a, we can rewrite Eq. (B.10) as:together all the samples in which the value xi, j is generated and substituting N =(cid:10)v Xi (Xi|xi−1,pathT ( Xi )) =(cid:2)ka=1 Ni,a(xi,a,xi−1,pathT ( Xi ))B T ,XiQ i (xi,a|xi−1,pathT ( Xi ))(cid:6)X j ∈chi( Xi )(cid:2)ka=1 Ni,a(cid:10)v X j (X j|xi,a, xi−1,pathT ( Xi ))(B.11)Next, we show that given an AND/OR sample tree ψT ,S and the same samples S from which (cid:10)v Xi ( Xi|xi−1,pathT ( Xi )) isderived, the value of an OR node n in ψT ,S labeled by Xi such that A(πn) = xi−1,pathT ( Xi ) is equal to (cid:10)v Xi ( Xi|xi−1,pathT ( Xi )).Let us denote the kth child AND node by mk. By definition the frequencies and weights of the arcs from n to ma are givenby:#(n, ma) = Ni,aw(n, ma) = B T , Xi (xi,a, A(πn))Q i(xi,a| A(πn))=B T , Xi (xi,a, xi−1,pathT ( Xi ))Q i(xi,a|xi−1,pathT ( Xi ))By definition, the value of each AND node ma is given by:v(ma) =(cid:3)(cid:9)(cid:7)n(cid:8)vn(cid:8)∈chi(ma)Similarly by definition, the value of the OR node n is given by:(cid:2)k(cid:2)kv(n) ==a=1 #(n, ma) × w(n, ma) × v(ma)(cid:2)ka=1 #(n, ma)(cid:6)a=1 #(n, ma) × w(n, ma) ×(cid:2)ka=1 #(n, ma)n(cid:8)∈chi(ma) v(n(cid:8))(B.12)V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7775Substituting the expressions for #(n, ma) and w(n, ma) in Eq. (B.12), we get:v(n) =(cid:2)ka=1 Ni,a(cid:6)(xi,a,xi−1,pathT ( Xi ))B T ,XiQ i (xi,a,xi−1,pathT ( Xi ))(cid:2)ka=1 Ni,an(cid:8)∈chi(ma) v(n(cid:8))(B.13)Assuming v(nyielding v(n) = (cid:10)v Xi ( Xi|xi−1,pathT ( Xi )). Namely, we have proved that if the value of a child OR node nof an OR node n is equal to an unbiased estimate of the conditional expectation of the subproblem rooted at nvalue of the OR node n is also an unbiased estimate of the conditional expectation of the subproblem rooted at n.(cid:8)) = (cid:10)v X j ( X j|xi,a, xi−1,pathT ( Xi )), we can see that the right-hand sides of Eqs. (B.11) and (B.13) are equalof a child AND node(cid:8), then the(cid:8)Since this result is true for any OR node, the value of the root OR node is equal to an unbiased estimate of Z = V X1 ( X1),which is what we wanted to prove. (cid:2)Proof of Theorem 2. We prove this theorem by induction over the nodes of the pseudo tree T (X, E).Base Case: Here we prove that the statement of the theorem is true for n = 1. Assume that T has only one variable X1.obtained by any topological linearization of T coincides with T . Given samples1 ) generated from a proposal distribution Q 1( X1) and bucket function B T (cid:8), X1 ( X1) (see Definition 13), theIn this case, the chain pseudo tree TS = (x1conventional importance sampling estimate is:1, . . . , xN(cid:8)(cid:10)Z = 1NN(cid:4)i=1B T (cid:8), X1 (xi1)Q 1(xi1)(B.14)Let {x1,1, . . . , x1,k} be the domain of X1 and N1, j be the number of times the value x1, j appears in S,by collecting together all the samples in which the value x1, j is generated and substituting N =Eq. (B.14) as:(cid:2)ka=1 N1,a(cid:2)ka=1 N1,a(cid:8),X1Q 1(x1,a)(cid:10)Z =(x1,a)B Tj ∈ {1, . . . , k}. Then,(cid:2)ka=1 N1,a, we can rewrite(B.15)(cid:8)Since Thas just one node, the OR sample tree based on T has just one OR node denoted by n. Let m1, . . . , mk be thechild AND nodes of n. By definition, the value of all leaf AND nodes is 1, while the weight and frequency of the arcs (n, ma)are given by:#(n, ma) = N1,aw(n, ma) = B T (cid:8), X1 (x1,a)Q 1(x1,a)Also, by definition, the value of the OR node n is given by:v(n) ==(cid:2)ka=1 #(n, ma)w(n, ma)(cid:2)ka=1 #(n, ma)(x1,a)(cid:8),X1B TQ 1(x1,a)(cid:2)ka=1 N1,a(cid:2)ka=1 N1,a(B.16)From, Eqs. (B.15) and (B.16), we have (cid:10)Z = v(n) which proves the base case. Next, we prove the induction case.Induction case: In this case, we assume that the statement of the theorem is true for n variables { X1, . . . , Xn} and thenprove that it is also true for n + 1 variables { X1, . . . , Xn+1}.Consider a pseudo tree T over n + 1 variables with Xn+1 as the root. Let T(cid:8)be the chain pseudo tree corresponding tothe topological linearization of T . By definition, both T and Thave the same root node Xn+1.(cid:8)n , xNn+1)) generated from the proposal distribution Q (Xn, Xn+1), the conventionaln, x1Given samples S = ((x1n+1), . . . , (xNimportance sampling estimate is given by:n+1j=1 B T (cid:8), X j (xi(cid:6)n+1j=1 Q j(xin, xin, xi(cid:10)Z = 1Nn+1)n+1)N(cid:4)(cid:6)i=1Let Xn+1 have k values in its domain given by {xn+1,1, . . . , xn+1,k} and Nn+1, j be the number of times the value xn+1, jappears in S. Let S(xn+1, j) ⊆ S be the subset of all samples which mention the value xn+1, j . Then, by collecting together allthe samples in which the value xn+1, j is generated and substituting N =(xkn,xn+1,a)|xn+1,a)(cid:2)ka=1 Nn+1,a, we can rewrite Eq. (B.17) as:(cid:2)ka=1 Nn+1,a(cid:6)n(cid:8),X jj=1 B T(cid:6)nj=1 Q j (xkn(cid:8),Xn+1Q n+1(xn+1,a)xk∈S(xn+1, j)1Nn+1,a(xn+1,a)(cid:2)B T(cid:12)(cid:11)(cid:10)Z =(cid:2)ka=1 Nn+1,a(B.17)(B.18)76V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–77Without loss of generality, let X1 be the child of Xn+1 in T. From the induction case assumption, the quantity in thebrackets in Eq. (B.18) is equal to the value of the OR node labeled by X1 given xn+1,a. Let the OR node be denoted by ra.We can rewrite Eq. (B.18) as:(cid:8)(cid:10)Z =(cid:2)ka=1 Nn+1,a(cid:2)ka=1 Nn+1,a(xn+1,a)(cid:8),Xn+1B TQ n+1(xn+1,a) v(ra)(B.19)Consider the root OR node denoted by r of the OR sample tree which is labeled by Xn+1. r has k child AND nodesm1, . . . , mk which in turn have one child OR node each. By definition, the value of an AND node is the product of the valuesof all its child nodes. Since each AND node ma a = 1, . . . , k has only one child OR node denoted by ra in an OR sample tree,the value of ma is equal to the value of ra. Namely,v(ma) = v(ra)By definition, the weights of the arcs between r and ma for a = 1, . . . , k are given by:#(r, ma) = Nn+1,aw(r, ma) =B T (cid:8), Xn+1 (xn+1,a)Q n+1(xn+1,a)By definition, the value of the root OR node r denoted by v(r) is given by:v(r) =(cid:2)ka=1 #(r, ma) × w(r, ma) × v(ma)(cid:2)ka=1 #(r, ma)(cid:2)ka=1 Nn+1,a(cid:2)ka=1 Nn+1,a(xn+1,a)(cid:8),Xn+1B TQ n+1(xn+1,a) v(ra)=(B.20)From Eqs. (B.19) and (B.20), we have (cid:10)Z = v(r), which proves the induction case. Therefore, from the principle of induc-tion, the proof follows. (cid:2)Proof of Theorem 3. Because only N full samples are generated, the number of nodes of the AND/OR sample tree is boundedby O (nN). Because each node of the AND/OR sample tree is processed only once during the value computation phase ofAlgorithm 2 (with each node processed in constant time) the overall time complexity is O (nN). We could perform a depthfirst search traversal of the AND/OR sample tree (i.e. build it on the fly). In this case, we only have to store the current searchpath, whose maximum size is bounded by the depth h of the pseudo tree. Therefore, the space complexity is O (h). (cid:2)Proof of Theorem 6. Given a pseudo tree T and a set of samples S, in an AND/OR sample graph the value of an OR node,denoted by nAOG and labeled by Xiis computed using a subset of the samples that have the same assignment to thecontextT ( Xi) of Xi while in an AND/OR sample tree, the value of the corresponding OR node nAOT is computed using asubset of the samples that have the same assignment to all variables along the path from root to Xi , denoted by pathT ( Xi).Because, contextT ( Xi) ⊆ pathT ( Xi), the value of nAOG is based on a larger (or equal) number of samples compared withthe value of nAOT . Because of its larger virtual sample size, the variance of the value of nAOG is less than (or equal to) thevariance of the value of nAOT . (cid:2)Proof of Theorem 7. Let X j be the child node of Xi in T . Given N samples and maximum context size wof edges emanating from AND nodes corresponding to Xi to OR nodes labeled by X jbounded by O (N wis O (nN w, the numberin the AND/OR sample graph is∗). Since each such edge is visited just once in the value computation phase, the overall time complexity∗). To store N samples it takes O (nN) space and therefore the space complexity is O (nN). (cid:2)∗References[1] A.W. Marshall, The use of multi-stage sampling schemes in Monte Carlo computations, in: Symposium on Monte Carlo Methods, 1956, pp. 123–140.[2] R.Y. Rubinstein, Simulation and the Monte Carlo Method, John Wiley & Sons, Inc., 1981.[3] J. Liu, Monte Carlo Strategies in Scientific Computing, Springer-Verlag, New York, 2001.[4] A. Darwiche, Recursive conditioning, Artificial Intelligence 126 (1–2) (2001) 5–41.[5] F. Bacchus, S. Dalmao, T. Pitassi, Value elimination: Bayesian inference via backtracking search, in: Proceedings of the Nineteenth Conference onUncertainty in Artificial Intelligence, 2003, pp. 20–28.[6] R. Dechter, R. Mateescu, AND/OR search spaces for graphical models, Artificial Intelligence 171 (2–3) (2007) 73–106.[7] V. Gogate, R. Dechter, AND/OR importance sampling, in: Twenty Third Conference on Uncertainty in Artificial Intelligence, 2008, pp. 212–219.[8] V. Gogate, R. Dechter, Approximate solution sampling (and counting) on AND/OR spaces, in: Proceedings of Fourteenth International Conference onPrinciples and Practice of Constraint Programming, 2008, pp. 534–538.[9] N.J. Nilsson, Principles of Artificial Intelligence, Morgan Kaufmann, 1982.[10] J. Geweke, Bayesian inference in econometric models using Monte Carlo integration, Econometrica 57 (6) (1989) 1317–1339.[11] J. Cheng, M.J. Druzdzel, AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks, Journal of ArtificialIntelligence Research 13 (2000) 155–188.V. Gogate, R. Dechter / Artificial Intelligence 184–185 (2012) 38–7777[12] J. Pearl, Probabilistic Reasoning in Intelligent Systems, Morgan Kaufmann, 1988.[13] A. Darwiche, A differential approach to inference in Bayesian networks, Journal of the ACM 50 (2003) 280–305.[14] L.A. Goodman, On the exact variance of products, Journal of the American Statistical Association 55 (292) (1960) 708–713.[15] V. Gogate, R. Dechter, Approximate inference algorithms for hybrid Bayesian networks with discrete constraints, in: Proceedings of the Twenty FirstAnnual Conference on Uncertainty in Artificial Intelligence, 2005, pp. 209–216.[16] V. Gogate, Sampling algorithms for probabilistic graphical models with determinism, Ph.D. thesis, Computer Science, University of California, Irvine,USA, 2009.[17] V. Gogate, R. Dechter, SampleSearch: A scheme that searches for consistent samples, in: Proceedings of the Eleventh Conference on Artificial Intelligenceand Statistics, 2007, pp. 147–154.[18] V. Gogate, R. Dechter, Approximate counting by sampling the backtrack-free search space, in: Proceedings of Twenty Second Conference on ArtificialIntelligence, 2007, pp. 198–203.[19] V. Gogate, R. Dechter, SampleSearch: Importance sampling in presence of determinism, Artificial Intelligence 175 (2) (2011) 694–729.[20] R. Dechter, K. Kask, R. Mateescu, Iterative join graph propagation, in: Proceedings of the Eighteenth Conference in Uncertainty in Artificial Intelligence,2002, pp. 128–136.[21] K.P. Murphy, Y. Weiss, M.I. Jordan, Loopy belief propagation for approximate inference: An empirical study, in: Proceedings of the Fifteenth Conferenceon Uncertainty in Artificial Intelligence, 1999, pp. 467–475.[22] J.S. Yedidia, W.T. Freeman, Y. Weiss, Free constructing energy approximations and generalized belief propagation algorithms, IEEE Transactions onInformation Theory 51 (2004) 2282–2312.[23] C. Yuan, M.J. Druzdzel, Importance sampling algorithms for Bayesian networks: Principles and performance, Mathematical and Computer Model-ing 43 (9–10) (2006) 1189–1207.[24] R. Mateescu, K. Kask, V. Gogate, R. Dechter, Join-graph propagation algorithms, Journal of Artificial Intelligence Research 37 (2010) 279–328.[25] R. Marinescu, AND/OR search strategies for combinatorial optimization in graphical models, Ph.D. thesis, Computer Science, University of California,Irvine, USA, 2008.[26] J. Bilmes, R. Dechter, Evaluation of probabilistic inference systems of UAI’06. Available online at http://ssli.ee.washington.edu/~bilmes/uai06InferenceEvaluation/, 2006.[27] T. Sang, P. Beame, H.A. Kautz, Performing Bayesian inference by weighted model counting, in: Proceedings, The Twentieth National Conference onArtificial Intelligence, 2005, pp. 475–482.[28] M. Fishelson, D. Geiger, Optimizing exact genetic linkage computations, in: Proceedings of the Seventh Annual International Conference on Research inComputational Molecular Biology, 2003, pp. 114–121.[29] J. Ott, Analysis of Human Genetic Linkage, The Johns Hopkins University Press, Baltimore, Maryland, 1999.[30] A. Darwiche, R. Dechter, A. Choi, V. Gogate, L. Otten, Results from the probabilistic inference evaluation of UAI’08. Available online at http://graphmod.ics.uci.edu/uai08/Evaluation/Report, 2008.[31] V. Gogate, B. Bidyuk, R. Dechter, Studies in lower bounding probability of evidence using the Markov inequality, in: Proceedings of the Twenty ThirdConference on Uncertainty in Artificial Intelligence, 2007, pp. 141–148.[32] C.P. Gomes, J. Hoffmann, A. Sabharwal, B. Selman, From sampling to model counting, in: Proceedings of the Twentieth International Joint Conferenceon Artificial Intelligence, 2007, pp. 2293–2299.[33] K. Kask, R. Dechter, A general scheme for automatic generation of search heuristics from specification dependencies, Artificial Intelligence 129 (1–2)(2001) 91–131.[34] L.D. Hernandez, S. Moral, Mixing exact and importance sampling propagation algorithms in dependence graphs, International Journal of ApproximateReasoning 12 (8) (1995) 553–576.[35] U. Kjærulff, HUGS: Combining exact inference and Gibbs sampling in junction trees, in: Proceedings of the Eleventh Conference on Uncertainty inArtificial Intelligence, 1995, pp. 368–375.[36] A.P. Dawid, U. Kjaerulff, S.L. Lauritzen, Hybrid Propagation in Junction Trees: Advances in Intelligent Computing (IPMU), ISBN 3-540-60116-3, 1994,pp. 85–97.[37] R.R. Bouckaert, E. Castillo, J.M. Gutiérrez, A modified simulation scheme for inference in Bayesian networks, International Journal of ApproximateReasoning 14 (1) (1996) 55–80.[38] B. Bidyuk, R. Dechter, Cutset Sampling for Bayesian Networks, Journal of Artificial Intelligence Research 28 (2007) 1–48.[39] M.A. Paskin, Sample propagation, in: Advances in Neural Information Processing Systems, 2003, pp. 425–432.[40] V. Gogate, R. Dechter, On combining graph-based variance reduction schemes, in: Proceedings of the Thirteenth International Conference on ArtificialIntelligence and Statistics, 2010, pp. 257–264.[41] Augustine Kong, Jun S. Liu, Wing Hung Wong, The properties of the cross-match estimate and split sampling, The Annals of Statistics 25 (6) (1997)2410–2432, ISSN 0090-5364.[42] C. Boutilier, N. Friedman, M. Goldszmidt, D. Koller, Context-specific independence in Bayesian networks, in: Proceedings of the Twelfth Annual Confer-ence on Uncertainty in Artificial Intelligence, 1996, pp. 115–123.[43] G.F. Cooper, The computational complexity of probabilistic inference using Bayesian belief networks, Artificial Intelligence 42 (2–3) (1990) 393–405.[44] C. Yuan, M.J. Druzdzel, An importance sampling algorithm based on evidence pre-propagation, in: Proceedings of the Nineteenth Conference in Uncer-tainty in Artificial Intelligence, 2003, pp. 624–631.[45] S. Moral, A. Salmerón, Dynamic importance sampling in Bayesian networks based on probability trees, International Journal of Approximate Reason-ing 38 (3) (2005) 245–261.[46] H. Yu, R. Engelen, Arc refractor methods for adaptive importance sampling on large Bayesian networks under evidential reasoning, International Journalof Approximate Reasoning 51 (7) (2010) 800–819.[47] M. Chavira, A. Darwiche, On probabilistic inference by weighted model counting, Artificial Intelligence 172 (6–7) (2008) 772–799.[48] V. Gogate, P. Domingos, Formula-based probabilistic inference, in: Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence,2010, pp. 210–219.[49] S. Geman, D. Geman, Stochastic relaxations, Gibbs distributions and the Bayesian restoration of images, IEEE Transaction on Pattern analysis andMachine Intelligence PAMI-6 (6) (1984) 721–742.