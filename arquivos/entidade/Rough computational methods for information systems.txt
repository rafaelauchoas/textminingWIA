Artificial Intelligence 105 (1998) 77-103 Artificial Intelligence Rough computational methods for information systems ’ J.W. Guan*, D.A. Bell ’ School of Information and Software Engineering, University of Ulster at Jordanstown, Joraiznstown, BT 37 OQB Northern Ireland, UK Received 16 December 1996; received in revised form 23 December 1997 Abstract Rough set theory in circumstances which are characterized by vagueness and uncertainty. The technique called rough analysis can be applied very fruitfully in artificial intelligence and cognitive sciences. tool for use in computer applications is a relatively new mathematical Although this methodology has been shown to be successful many real-life applications, to consider practical there are still several theoretical problems issues if we want to apply the theory. in dealing with the vagueness of to be solved, and we also need It is the latter set of issues we address here, in the context of handling and analysing sets during the knowledge general problem of finding all “keys”) have been shown to be m-hard. Thus, it is important efficient computational methods for the theory. representation process. Some of the associated problems (for example, large data the to seek In rough set theory, a table called an information system or a database relation is used as a special is defined systems. The use of rough analysis does not involve the details of kind of formal as classifications of information rough set theory directly, but it uses the same basic classification syntactically. Semantically, knowledge to represent knowledge techniques. language We discuss computational methods for the rough analysis of databases. 0 1998 Elsevier Science B.V. All rights reserved. Keywords: Intelligent information systems; Database and knowledge base systems Introduction In this paper we discuss how to apply the rough analysis technique to databases. Rough techniques arising from rough set theory. It provides analysis is one of the main application * Corresponding author. Email: J.guan@ulst.ac.uk. ’ Email: DA.bell@ulst.ac.uk. 0004-3702/98/$ - see front matter 0 1998 Elsevier Science B.V. All rights reserved. PII: SOOO4-3702(98)00090-3 78 J.U? Gum, D.A. Bell/Artijkial Inaelligence 105 (1998) 77-103 for gaining attributes insights in databases. into properties of data, dependencies It has important applications a technique of individual and cognitive sciences, as a tool for dealing with vagueness and uncertainty of facts, and in the application of the technique by classification. The objective of this paper is to enhance designing a series of algorithms context. To study the theory itself, the reader is referred to [6]. and the significance intelligence in a knowledge representation the technique to implement to artificial A sister paper [2] complements the collection of algorithms presented here; it focuses on the use of rough set theory, per se, as a means of discovering knowledge which is latent in the form of in database relations rules, with particular attention being paid to decision (i.e., data mining or knowledge discovery in databases) tables. In this paper, we concentrate upon using rough set theory itself) for the classical database problems of checking dependencies jinding keys for a conventional a view to using the solutions appear in both papers for convenience-being to and (i.e., with no distinguished decision variables) with [l]. One or two algorithms relation in general knowledge discovery the rough analysis technique (as opposed relevant to both studies. is to use the tool for computer applications Our practical motivation in which reasoning [3,14] learning large) collections in deriving evidence are based on (often sites (e.g., near Lake Turkana of sense or other data stored [l] how evidence can be obtained in certain kinds of integrity constraints is that there are realZy two classes-the from this data to support various hypotheses, featured one, and a third class lying between is that there are 3 different classes of skull-a in systems. For example, we have and computers, and perhaps managed by database management for reasoning purposes by examining shown elsewhere (see below). the properties of data as expressed into our distant ancestors, using a set of field notes an investigation Consider in Kenya). Ultimately, we are recorded on paleontological for example, interested in the dataset. One hypothesis, based on prima facie on the classes of skulls described small, finely featured one, a indications, these two. An alternative large, coarsely small and hypothesis (there may be several) medium “classes” being the clearly distinguished female and male specimens of just one class, and the large “class” being a different species. The field data can be recorded as an is given in Example 3.1 below), and rough analysis can information from this data to see which be considered hypothesis in this if we can decide which features, example, and there is also a potential practical benefit exercise (the “core”). A related problem or attributes, are significant is to find which set of attributes or features (perhaps minimal) can be used to distinguish individual this “classical” problem the reasoning and learning applications classification the accumulation data, are clearly very important discovery of keys can also provide insights get by alternative means. exercises. The into the structure of data which are not easy to skulls from one another. This is what we mean by a “key”. The solution is best supported by the data. There is a basic need for classification least in specimens and events, and itself. The classification of evidence theory can provide important support in underpinning learning and other reasoning of (e.g., paleontological) as a possible approach to support hypotheses for deriving evidence system (an illustration in the classification from observation and experimental intelligence-not encountered in database in machine in artificial to In Section 1, we introduce information ization of a database relation. Formally, systems. An information is a general- it consists of two finite sets: one universe U and system J.U! Guan. D.A. Bell /Artificial Intelligence IO5 (1998) 77-103 79 one attribute set A. The rough analysis significant attributes, and the keys-the minimal technique is applied to find the core-the set of identifying sets of attributes. for an attribute The basic tool for rough set theory is classification. In Section 2, we discuss classification for one attribute. Algorithm E with the time complexity 0( 1 U I*) to find the corresponding classification be run in parallel mode to compute concurrently attributes. Algorithm intersection between many attributes is given, where 1 U 1 is the cardinality of U. The algorithm can for many between many attributes. classification In Section 3, we discuss classification I with the time complexity O(lU 1’) to find the corresponding all corresponding intersections classifications is given. Using between dependencies (FD). We present Algorithm 0 with classification, we analyse dependencies functional 0(/U 12) to check two subsets of attributes. Section 4 discusses the time complexity subsets. (ID). We present Algorithm S with the time Section 5 discusses complexity O(lU I*) to check the identity dependency of two attribute subsets. The keys ID subsets of set A. Algorithm K with the time of an attribute set A are the minimal complexity 0(21Al IA) lUl*) to find all keys is presented in Section 6. In order to reduce the the ID significance of an attribute and ID exponential significant attribute subsets. complexity, we need to investigate identity dependencies of two attribute the functional dependency for computing An attribute x in X (& A) is significant if X and X - {x) are not ID. An attribute set if every attribute x E X is significant. Using classification, we analyse the significance measure of attribute x in X. The time is O(lX] x lUl*). Algorithm C with the time set of significant X is significant significances. Section 7 introduces complexity complexity 0(1X1* 1 VI’) attributes. Section 8 discusses keys are significant subsets. Using the significance measure, we present Algorithm A with the time complexity 0( I A I3 I U I*) to find one key. The algorithm can be run in parallel mode to compute all keys concurrently. subsets of attributes. Section 9 shows that the ID ID subsets. Thus, finding keys is equivalent to allow us to find the core-the to finding significant a significance is presented significant 1. Information systems and databases An information system 1 is a system (U, A), where: . . . . uj, . . ., ulul} (I) u = (UI,U2, is a finite non-empty set, called a universe or an object space; elements of U are called identifiers or objects; (2) A = {al,az, . . . , al,. . ., alAI) is also a finite non-empty set; elements of A are called attributes; (3) for every a E A there is a mapping a from U into some space, a : U --F a(U), and is called the domain of attribute a. system An information [6] is also called a knowledge representation attribute-value an information system. An information table (see Table 1). system can be intuitively system, or an in terms of expressed 80 J.W Guan, D.A. Bell /Artijicial Intelligence 105 (1998) 77-103 Table 1 U\A al a2 al alAl Ul u2 ui ,.. al&l) a2(ul) al&) . alAl al (u2) a2(u2) .,. al(u2) alAl (U2) . . . q(4) . . . . a2(Ui) C?r(Ui) alAl@i) . . . . . . . . . . UlUl al(Upl) a2(ulUl) al(ulUI) . . . alAI The time complexity system for computing thereareIUIxIAlvaluesal(ui)tobecomputed,wherei=1,2,...,IUI;1=1,2,...,IAl. an information (U, A) is (U 1 x IAl since The concept of information systems is a generalization of the concept of a reMan in databases. For relational databases, a relational scheme is a finite set of attributes A = {al, a2, . . . , its domain. . . , IA]) has a set of values, Dl, called is a subset of D1 x D2 x . . . x DIAI. A member of they are labelled by different objects uz and urn and for any attribute an information system may consist of duplicate rows they have alAl}. Each attribute al (1 = 1,2,. A relation R on a relation scheme R is called a tuple. relations in databases, Unlike (tuples): identical values [4]. Using rough set theory, we can analyse dependencies of attributes for sets, and (2) intersection of all keys. The core is equal to the set of significant attributes to find (1) the keys-the minimal identity dependent and significancies system an information the core-the (see Fig. 1). for finding the core and keys is of fundamental and cognitive sciences, especially decision The rough analysis technique intelligence acquisition, importance learning, to artificial expert knowledge reasoning, and pattern recognition. Currently, systems, decision support systems, inductive is being used, among other areas, in market research, medical the rough set methodology data analysis, to the design of new composite materials. The analysis of stock market data has confirmed some well-known market rules and has led to the discovery of some interesting new rules [7]. for the purpose of control, and research analysis, knowledge discovery in the areas of machine sensor data analysis from databases, leading @__{-~_- Fig. 1. Rough analysis. J.U! Gum, D.A. Bell /Artijicial Intelligence 10.5 (1998) 77-103 81 This paper describes it. implement the rough analysis technique and proposes a series of algorithms to 2. Classification for one attribute Classification Let (U, A) be an information is the basic tool for rough analysis. system, where U = (ut, ~2, . . . , ulul}, a set of objects, and A={al,az,..., al~l}, a set of attributes. if and only if a(u) = a(v) With each attribute al, ~2, . . . , al , . . . , Ul,q in A we associate an equivalence on U by r&v same value for attribute a. The equivalence relation 6, for all u, v E U; i.e., objects u and u have the (denoted by . . . , XIU/~~} on universe U such that u and v in U are in the same UP, or U/a) 1x1, X2, class X if and only if they have the same value of attribute a, and vice versa. That is, let IPa = (V E u 1 u(u) = u(u)]. Th en we get a collection {u’@ 1 u E U} of subsets of U. The and ut, 1.9 E U are in the same subset (class) collection usa = {V E u 1 u(u) = u(u)] f orau~Uifandonlyifu(vt)=u(v2)=u(u): {uea 1 u E U) is a classification relation 6, gives a cZussi$cution (i) 24’” # 0, (ii) up nuy =0ifuF #uy, (iii) Uueu ueu = U. Thus, we have classification U/u on U for an attribute a; namely, u&v if and only if u(u) = u(u) (see Fig. 2). In Fig. 2, we demonstrate graphically the two classes of U based on attribute a in the first figure, and the three classes on a different attribute, b, in the second. There are two a set which kinds of classification-static has previously been fully input. In dynamic classification is made available is classifying the set to be classified and dynamic. Static classification element by element. incrementally, An algorithm present an algorithm for dynamic classification for static classification. has previously been proposed [2]. Here we Algorithm E. Let (U, A) be an information Given an attribute a E A, this algorithm system, where U = {ul , ~2, . . . , u,} (n > 0). classification U/u such finds the corresponding that u(ui) = u(ui) if and only if u; and uj are in the same class (1 < i, j < n). Fig. 2. Classification of a set of objects can be different for different attributes. J W Gum, D.A. Bell /ArtiJcial Intelligence 105 (1998) 77-103 We use U& 3 uj into class UP. We check . ., un from the first object ut to the last object un. That is, we check object Ui the fact that uj to represent is classified Ul, u2,. fori=1,2 ,..., II. To establish class UP, the idea is to check uj for j = i, i + 1, . . . , II. This classification can be speeded up as follows. When we first meet a j such that a(uj) # a(ui) and uj is j, i + Z) to be checked (to establish not classified, we set this j as the next ui (by I + class ~7). Thus, we need an auxiliary variable I to remember the first j. In the step following this (see (E2) below) we establish class u?. So we set I = i (> 0) and then set Z t 0 when we enter this step. (El) [Startfrom u:]. Set Z t 1 (to check ut and to establish class u”p). (E2) [Start to set u?]. Set i + I, Z t 0. Set j t i (to check ui for j = i, i + 1, i + (E3) (E4) 2 ,..*1 n). Classify U? 3 uj. (First of all, classify u: 3 ui .) [j = n?]. (Is class u? complete?) If j < n (then class u? is not complete) go to step (E5) (to check next uj). If j = 12 then class up is complete, go to (E4). [i = n?]. (Now j = n, class u? complete.) and uj is not classified and we have already set Z t is If i < n and Z > 0 (we have ever met a uj such that a(uj) # U(Q) j at (E9)) go to (E2) (to set i.e., the classification is complete; for up I). next UP by i t If i < n and Z = 0 (we have never met a uj such that both a(uj) # a(ui) and i.e., we only meet such uj that either (1) a(uj) = u(ui), or Uj is not classified; for 0, is completely and uj (2) u(uj) # u(ui) the classification is classified) finished. If i = n (now i = j = n) the classification for 0, is completely finished. (E5) [Increase j]. j t (E6) [u(u~) = u(uj)?]. If a(ui) = a(uj) go to (ElO). Otherwise go to (E7). (E7) [I > O?]. (Now a(uj) # a(ui). Is this the first j?). j + 1. If Z > 0 (then this is not the first j so we simply ignore this j and) go to (E3) (to check next j for u?). Otherwise go to (E8). (ES) [Is uj clussi$ed?] (Now Z = 0.) If uj is classified then go to (E3) (to check next j for u:). [Set Z t If uj is not classified j]. (Now Z = 0 and uj is not classified.) Set Z t then go to (E9). (E9) j and go to (E3) (to check next j for u?). (ElO) [Classify ~7 3 uj]. Classify u? 3 uj. GO to (E3) (to check if class UP is complete). The time complexity of Algorithm E to classify U is O(] U 12). In the worst case, we need to check objects as follows. (1) To establish ut , to check ]U] - 1 objects: ~2, us, . . . , ~1~1. 62 (2) To establish u$, to check ]U] - 2 objects: ~3, ~4,. . . , ~1~1. And so on. (3) To establish uf&t, to check 1 object ~1~1. J.W Guan, D.A. Bell /Artificial Intelligence 105 (1998) 77-103 83 Table 2 Skull information cation), x-j-location, system, where xl--teeth-size, q-type (a crude prima facie classifi- .x-morphology, xg--skull-size (100 cc), &-sex Specimen # XI I#45 #92 #163 #167 #181 4 3 4 3 4 x2 1 2 1 1 2 x3 K J L K J x4 X Y X X Y x5 8.36 5.14 8.38 8.29 5.27 X6 M F M F M So the time complexity of Algorithm E to classify U is (lul-1>+(lul-2)+~~~+1= lUl(lUl - 1) 2 = O(IU12). The algorithm classifications can be run in parallel mode from many attributes. to compute concurrently all corresponding Example 2.1. In a paleontological system (see Table 2). investigation we might have the following information Here we have II = )U) = 5: U = (~1, ~2, ~3, ~4, ug}, where ~1 = #45, ~2 = #92, ~3 = #163, ~4 = #167, ug = #181, and A = {xl, x2, x3, x4, x5, X6). For every attribute a E {xl, x2, x3, x4, x5, X6) U/O, : U/G,, , U/O,,, U/t?,,, U/e,,, U/0,,, U/0,,, U, u E U are in the same class if and only if a(u) = a(v) . for this skull information We can find all classifications l U/Q,, = {VII, Vt2),where in A we can introduce in universe U as follows: a classification two objects system by using Algorithm E: VI1 = (#45,#163, #lSl), Vr2 = {#92, #167 ‘I; 0 u/e,, = I V21, hl, where V2t = {#45, #163, #167), V22 = {#92, #181 l UP,, = {V31> v32, V33)> where V31 = (#45, #167), V32 = {#/92, #1X1), V33 = (#163); 0 up,, = IV41, V42), where V41 = (#45, #163, #167), V42 = (#92, #181); a up,, = {VSI, V52, V53, V54, VSS), where V5l = I#45), l u/e,, = (v61, v62}, where v52 = {#92), V53 = {#163), V54 = {#167), Vs5 = {#181); v61 = {#45, #163, #181), V62 = {#92, #167). Notice that U/8,, = U/e,, and that U/O,, = Up,,. 84 J.W Guan. D.A. Bell /A@icial Intelligence 105 (1998) 77-103 3. Classiiication intersections between many attributes Before discussing classification intersections between many attributes, we need to introduce the intersection operation on classifications. 3.1. The intersection operation on classfications Let 81,& be two equivalence relations on U. The intersection 131 r-1192 of two equivalence relations 81 and 132 is defined as follows: ~(61 II 82)~ if and only if u6Jlv and u&v. 8, denote classification the corresponding For an equivalence by U/O. For a u E U, denote u’ = (v E U I veu), the class in U/e containing u. Then U/e = {ue 1 u E U}, i.e., classification U/O comprises different classes u* for all u E U. For the intersection operation, we have uelne2 = uel n ~82. Let 81,& be two equivalence relations on U. We say that 81 is incEuded in f32, denoted by 81 C 62, if 81 flf92 = 81. We say that 81 is strictly included in 62, denoted by 81 c 62, if e1 n e2 = e1 and e1 # e2. Now, 81 C e2 can be described alternatively by stating that relation 81 is stronger than relation e2: uelv implies ue2v for all u, v E U. We denote 81 C 62 by 81 b 62; denote the identity equivalence by E: UEV if and only if u=vforu,v~U;anddenotetheuniversalequivalenceby6: u6vforallu,v~ U [ll]. Then, E is the least equivalence in the following sense: E s 8 for every equivalence 8; 6 is the greatest equivalence: S 2 8 for every equivalence e. Let U/B1 and U/e2 be two classifications on U with the respective equivalence relations two classifications U/81 and U/e2 is defined 81 and 62 on U. The intersection n between as follows: U/81 n U/C92 = U/B1 n e2 (also called classification “U/e1 AND U/&“). Let U/81 and U/t+ be two classifications relations et,02 on U, respectively. We say that U/Q1 is included in lJ/6’2, denoted by U/B1 5 U/&, if 81 & 62. We say that U/81 is strictly included in U/f&, denoted by U/81 c U/82, if 81 C 62 and e1 # e2. to equivalence corresponding Now, U/B1 C U/O2 if and only if 81 rl 132 = 01; i.e., uOI = ~‘1 n ~82 for all u E U; i.e., ue1Cue2foralluEU. So U/C?1 C U/62 if U/t+ classification U/t31 is included in a class X2 in classification U/02: X1 5 X2. if and only than U/92; is finer i.e., every class X1 in If u/e1 c up2 then lu/el 1 3 p7/e21. We also know 1. ]U/Sl= that U/E = {{u} ( u E U} and lU/.zl = IUI and that U/6 = {U} and 3.2. Intersection for many attribute classijcations Now, consider a subset X C A. There is an equivalence n,,, to X as if and only if a(u) = a(v) for every a E X. Let that U/$x can be denoted by U/X (see Fig. 3). 6, corresponding follows: two objects u, v E U are equivalent us denote f9x = n aEX 6,. Also, remember For the empty set 0, we take 80 = 6. Theorem 3.1 (Grzymala-Busse tion system. Then, for X, Y C A we have 8x rl 0~ = Bxuy. [4], Pawlak and Rauszer [S]). Let (U, A) be an informa- J. W Gum, D.A. Bell /Artificial Intelligence 105 (1998) 77-103 85 U/a Wab Fig. 3. Classification intersection. Proof. We find that n (,~,$>) n ((aEfi,ye4 = ((,,finyea> = (,,fi”yea) n (aEfinyeal = (.,fi,,“) n (,J-Jnyea~ n (,zyea)) n ((a:yeal n (,gyea)) n (.fi,&) =azyea. q Notice conventional as Bx n 8r = &r, that subsets of attributes are often discussed in databases, and XY is a shorthand for X U Y, where X, Y C A. So, this theorem can be rewritten and we have the following algorithm for computing intersection &y. AlgorithmI. Yj, . ..) Y,], 1 <t 6 ]UI. Let lJ/& = (Xl, X2,. . ., Xi, . . . . X,),1 < s < ]UI; lJ/8y = (Yl, Y2,. . ., This algorithm gives classification U/&r We use the following pointers: = [VI, V2, . . . , Vk, . . . , V,}, 1 < r < st. i=1,2,. ..,spointstoXi, j= 1,2,..., t points t0 Yj, r records that we have found r classes VI, V2, . . . , V, of W/& n 62, For every i and every j, we check whether or not Xi n Yj = 0. If Xi n Yj = 0 then we simply ignore it. Otherwise, we establish a new class: r t r + 1, V,. = Xi n Yi . (il) (12) (13) [Initialize]. Set i t 1, j t 1, r t 0. [Xi n Yj = 0?]. If intersection Xi fl Yj = 0 then go to (13) to check intersection. Otherwise, set r t to (13) to check the next intersection. [j = t?]. If j = t then go to (15) to check next i. Otherwise, go to next step (14) to see next j. r + 1 and establish a new class V,. = Xi fl Yj for U/t31 n 02. Go the next (14) [Increase j]. Set j t j + 1. Go to (12). 86 J.W Guan, D.A. Bell /Art&ial Intelligence 105 (1998) 77-103 (15) [i = s?]. If i = s then the classification V,). Otherwise, go to (16). =IV1,V2,..., (16) [Increase i]. Set i t i + 1, j t 1. Go to (12). is completed, and we have U/&y We know that the time complexity of Algorithm I is O(lV 12) since there are s x t (< 1 U 1 x (U 1) intersections Xi n Yj to be calculated. From the definition of intersection and inclusion, we have the following. (1) ForX,YcAwehaveBxuu=BxyC8x,8y. (2) (RefIexivity). For X, Y s A, if X 2 Y then 8x C 8~. (3) (Augmentation). For X, Y C A, if 8~ s 0~ then Bxuz C Bruz for Z C A. (Thus, if 0~ = 8~ then Bxuz = 0yuz for 2 2 A.) Example 3.1. From Example system. information 2.1, the following results are obtained for the skull (1) &, = 0,, and 19,~ = &,. So we take A = {x1,x2,x3,x5}. For convenience, we rewrite A = {VI, ~2, ~3, ~41, where yl = XI, y2 =x2, ~3 =x3, ~4 =x5. (2) Also, notice that 8,, = E, the identity equivalence. So is %,&!I = %2&l = %%Y41 = s. (3) Ul&, = ]Vtt, Vt2L where Vll = {#45,#163, #181], Vt2 = {#92, #167}. (4) UP,, = ] V21, hd, where V21 = {#45, #163, #167], V22 = {#+92, #181}. (5) UP,, = (V31, V32, V331, where V31 = {#45, #167}, V32 = {#92, #181], V33 = {#163}. (6) u/eIy,,y21 = IV51 u V53, V52, V54, V551. (7) U/&,Y3) = WSI, VSZ V53, V54, VSSI. That is, 0{Yl,Y3) is 8. So is @{Y,,Y2,Y3j = &. (8) u/qy& u V% V52 u V55, V531. = ml 4. Functional dependencies Using classification, we can analyze dependencies between system Z = (W, A), let u, u E V and X C A. We denote X(u) = X(v) two subsets of attributes. For if an information andonlyifx(u)=x(u)forallxEX. Definition 4.1. Afunctional in an information in the X(u) = X(v) implies Y(u) = Y(v) (see Fig. 4). two subsets X, Y C A of attributes system Z = (U, A) is a statement, denoted by X -+ Y, which holds that if, for every pair u, v E V we have (FD) between if and only dependency information system 1, Obviously, X 2 Y implies X + Y (see Fig. 5). J.W Guan, D.A. Bell /Artificial Intelligence 105 (1998) 77-103 87 Fig. 4. Functional dependency. Fig. 5. Functional dependency for subsets Theorem 4.1 (Grzymala-Busse dejined as 0~ C ey; i.e., n,,, [4], Pawlak [6]). Aftlnctional dependency X + Y can be 0, G naEY 0,. Proof. By Definition 4.1 we know that X -+ Y, if and only if, for every pair U, v E U we implies u@rU; have that a(u) = a(v) for a E X implies a(u) = a(v) for a E Y; i.e., uexv i.e., u(naCx e& i.e., cn,,, 0,) E (nacY 0,). 0 implies U(naCy e,)v; i.e., (n,,, 0,) F (naEY 0,) by definition; Thus, we have the following algorithm for checking whether or not X + Y for X,YcA. Algorithm 0. Let (U, A) be an information system. Let X, Y c A. Suppose that u/ex = {Xll, x12,. . . 1 ai,. . ., xlln,lh hi G v-4; u/ey=~~21,~22,...~~2j,...,~21ir21~~~~2~ G vi, this algorithm checks whether or not X -+ Y. (01) [Initialize]. Set i t 1, j t 1. J.W Guan, D.A. Bell/ArtiJ?cial Intelligence 105 (1998) 77-103 If Xii C X2j then (for this i we have already found a j such that (02) (03) (04) (05) (06) [Xii c Xzj?]. Xii c X2j SO) go to (03) (to check next i). If Xi1 g X2j then go to (05) (to check next j). [i = 1x1 I?]. If i = 1x11 then the algorithm If i < In1 I then go to (04). [Increase [j = ITQ~?]. If j = 17121 (now we have an 1,2,..., If j < In21 then go to (06). [Increase In21) then the algorithm j + 1, go to (02). i + 1, go to (02). j]. Set j t i]. Set i t is completed with the answer: X -+ Y. i such that Xii g X2j for j = is completed with the answer: X $, Y. Algorithm 0 should make 1x11 x 17~21 < IU12 comparisons in step (02). So its time complexity is 0(lU12). Example 4.1. For the skull information system, we have the following: x1 + x6, x2 -+ x4; i.e., 0,, C &, &, G &,; i.e., q(u) =x1(21) implies X6(u) = X6(U), and x2(u) =x2(21) implies Q(U) =x4(u) for every U, 21 E U. 5. Identity dependencies and keys Definition 5.1. An identity dependency in an information the information two subsets X, Y C A of attributes system Z = (U, A) is a statement, denoted by X -EN Y, which holds in (ID) between system Z, if and only if, both X -+ Y and Y -+ X hold (see Fig. 6). From Theorem 4.1, an identity dependency X -++ Y can be defined as 0x = 8~; i.e., n,,, Qa = n&J 00. Example 5.1. For {x5} t, {xl, x2, x3, x5}, where xl--teeth-size, x5-skull-size and X6-%x. information the skull system: x1 * x6, x2 tf x4, and {xl, x3} * x2--type, x3-location, x4-morphology, Algorithm S. Let (U, A) be an information system. Let X, Y G A. Suppose that Fig. 6. Identity dependency. J.W. Gum, D.A. Bell /Artificial Intelligence 105 (1998) 77-103 89 U/~X=I~11,~12,-~~,~li,~~~,~l~n,~J,l~1l~ IUI; ~/~Y~~~21~~22~---~~2j~~~~~~2~n*~~~I~2l~l~l. (53) 61) 6.9 This algorithm checks whether or not X t, Y. [ZnitiaZize]. Set i t 1, j t 1. [Xti = X;?j?]. If Xti = X2j then (for this i we have already found a j such that Xti = X2j SO) go to (S3) (to check next i). If Xi 1 # X2j then go to (S5) (to check next j). [i = In1 I?]. If i = 1x11 then the algorithm If i -c Inl( then go to (S4). [Increase [j = ]n2]?]. 1,2,..., If j < ]nz( then go to (S6). [Increase If j = ]n2] (now we have an i such is completed with the answer: X y4 Y. is completed with the answer: X + Y. i]. Set i +- i + 1, go to (S2). j]. Set j +- j + 1, go to (S2). In2 I) then the algorithm that Xti # X2j (S4) (S5) for j = (S6) Algorithm S should make 1x1 I x In4 < IUl2 comparisons in step (S2). So its time complexity is 0( ] U 12). Now we introduce keys in an information system. Definition 5.2. Let X be a (non-empty or empty) subset X & A of A. A subset X0 of X is said to be a key of X if Xc satisfies: (1) ID: 19x, = 8~; i.e., X0 f, X; (2) Minimal: if X’ c Xc then 19x c ox!; i.e., if X’ c Xo then X’ + X. functional dependency minimum identity Fig. 7. Keys: minimal ID subsets. 90 J. W Gum, D.A. Bell / Art$cial Intelligence IO5 (I 998) 77-l 03 The empty subset 0 has key 0 (see Example 5.2 below). That is, X0 is a minimal identity dependent subset of X, i.e., we have x t, . . . ++ x0 + x’ -+ . . . for x2... 2 x0 > x’ 2 . . . (see Fig. 7). Example 5.2. Let X = 0. Then X has the unique key X0 = X = 0. Indeed, subset Xo of X satisfies: (1) 8x,=Bx=8n=S; (2) there are no X’ c X0 since X0 = 0. 6. Finding all keys for a database Let Z = (U, A) be an information attributes. We want to find all its keys-that such that: system, where U is universe and A is the set of is, all subsets Ao: Aol, Au2, . . . , Aos of A (1) 6&, = QA; i.e., A0 +-+ A; and (2) if A’ c A0 then 6~ c 6~‘; i.e., if A’ c Ao then A’ $4 A. Algorithm K (Grzymala-Busse singletons to A. [4]). This algorithm finds all keys of A by searching from IfA=0then0istheuniquekeyofA.LetA={at,a2 ,..., aj ,..., aiAl],IA]>O.We need to check all the subsets of A. Let us denote the binomial coefficients by Ck = Z!/k!(Z - k)!. (1) Let us denote CIA’ = 1 A 1 singletons, one-attribute subsets, by A11 = {at], A12=Ia2), . . . . Alj=(aj}, . . . . AlCy = {alAI). We can compute 6A,, = n & @A,2 = 0 &, . ..? ~~AII 0412 oAlj = n &, . . ., ~EAI, eA,cl~l = 1 n 6,. a-Q+ 1 (2) Let us denote Cy’ = 1 A ) (I A I - 1)/2! two-attribute subsets by A21 = (~1, ~21, A22=h,a3J, . ..> A2j={al,ajJ, ...t A,,~I = @fAl-1, alAl). J.W Guan, D.A. Bell /Artificial Intelligence 105 (1998) 77-10.3 91 We can compute @AZ1 = n @?, @AZ2 = n @a, . .., ae.b a~A22 ‘Azj = n ‘,, . . .) ‘A 2c.4 = n 4. aEA2j a~A2Cl~i 2 (3) Generally, let us denote CiA’ = IAl!/t!(lAl -t)! t-attribute subsets by &l=h,a2,...,atl, . . . . A,j, . . . . ArCy = {alAl-t+l, . . . , alAl- alAl}. We can compute ahI a~&2 6A,j = n 6, . . . , 6A tCIAl = n ea. aEA*j aeAlciAl , (4) Notice that CA’ - IA, - 1 (i.e., the unique) IA)-attribute subset is AIAI~ = {al, ~2, . . . . aj, . . .t alAI} = A, and OA = naEA,A,l @a = (-,a&%Z~ The algorithm is to search subsets of A as follows: singletons, two-attribute subsets, _ . . , t-attribute Suppose subsets, and so on. Continue up to the unique 1 A 1 -attribute subset A itself. that we have already found s keys Aol, A02, . . . , Aok,. . . . Aos so far. Then, a t-attribute subset Atj (j = 1,2, . . . , Cl”‘) is the next key Ao,,+I if: . . , s. (Otherwise Atj > Aok implies (1) Atj 2 Aok for k = 1,2, that Afj has a proper subset AO~ such that npEA 0, = naEAok 6,; i.e., 0~ = eAok. So Atj is not a key.) (2) n aEA 6, = n&,, 8,; 1% 0.4 = BAti. includes the following steps, where: check Atj ~5 Aok for all k = 1,2, . . . , s; Therefore, our algorithm (1) Steps (K2)-(K5) (2) Step (KlO) check natA 8, = naEAtj 6,; i.e., 6A = (3) Steps (K6)-(K9) In the successful set the next subset Atj. thlj ; naEA & = naEAlj 0, for this Atj; case (detected by (K2) and (K3)) we go to step (KlO) i.e., 0.4 = oAri. Otherwise to check (detected by (K5) and (KlO)) we go to steps (K6)-(K9) to set the next subset Atj (to check whether or not n&A & = &A,, @a; i.e., 0A = ‘A,j). number of keys we have already found, We use the following variables: l s-the l k-counting l t-we l j-we are currently are currently from 1 to s, searching searching t-attribute subset Atj, the jth subset Atj in all t-attribute subsets Atl,...,A’j’...,A,clAI. (Kl) [Initialize]. Set j L 1, s t 0, t t 1. Compute 0~ = naEA 8,. 92 J.W Guan, D.A. Bell /Artificial Intelligence 105 (1998) 77-103 (K2) (K3) [s = O?]. Ifs = 0 go to (KlO). Ifs > 0 then set k t 1 and go to (K5). [k=s?].Ifk=s(soA?i $Aokforallk=1,2,...,s,where Aol, A02, . . ., Aok, ...I Aos are all keys we have found so far) go to (KlO). If k -c s go to (K4). [Increase k]. Set k t k + 1. (Go to (K5) to check next Auk.) (K4) (K5) [Atj > Aok?]. If Afj > Aok (a key found previously), for it and) go to (K6) (to check the next subset Ali). If then (it is impossible to be a key, so ignore Atj Atj $ Aok go to (K3) (to check next Aok). j + 1. Go to (K2) (to check the next subset Alj: whether (K6) (K7) (K8) (K9) [j = CjA’?]. If j = CiA’ go to (Kg). If j < Ci”’ go to (K7). [Zncrease j]. Set j t ornotAtj>Aokfork=l,2 s). [t = IA I?]. If t = IA 1 then (searching Aot,Ao2 (K9). [Increase whether or not Atj > Aok for k = 1,2, . . . , s). Aos. So) the algorithm t]. Set t t t + 1, j t ,Aok,..., ,..., ,... is complete and all keys are obtained: terminates. If t -c 1 Al go to 1. Go to (K2) (to check the next subset Atj: W10) [OA = ‘Atj?I. If naeAoU = &Atj 0, (then Atj is a key) go to (Kll). If nacAea Atj: whether or not naeA 8, = naeAtj 0,; i.e., 0~ = e&j). fbEA,j c 6, (then Atj is not a key) go to (K6) (to check the next subset (Kl 1) [Foundan Ao]. Found a key: set s t s + 1 and Aos t Atj. Go to (K6) (to check the next subset Afj : whether or not naeA 0, = naeA,; 0,; i.e., #A = @fAtj). Example 6.1. Consider Example 3.1), we rewrite A = (yl, ~2, ~3, y4], where yt = xl, y2 = x2, y3 =x3, y4 =x5. system in Example 2.1. For convenience the skull information (see We can find all keys A01 = {y4), A02 = {yl , ~3) for the skull information system by using Algorithm K. This tells us that the skull size y4 can be used as an identifier of specimens, and that a of teeth size yt and location y3 could serve the same purpose. The latter fact says that there are no two specimens at a given location it is combination is probably more interesting-it which have the same teeth size. This may not mean much to the paleontologists-but easy to envisage applications where such information being studied. into the ontology can give insights The time complexity since in 2A, and ]2AI = 2iAl. Moreover, when the algorithm the algorithm checks all subsets checks one subset Atj 2 A to see if 0~ = @Atj in (KlO), we need to compute equivalence of Algorithm K for finding all keys of A is exponential *Atj = n&A,j We know classification is IU121A(. ea. that of one Q, is I U 12. So the price to find ( Alj ] < ]A I equivalence of Algorithm E for finding the time complexity the corresponding relations 0, TO COIIIpUte naEAtj O,, we need IAtj I - 1 intersections. And the time complexity to compute one intersection (IArjl - 1) x W~12> = W~1214). is O(]IY]~). So the price to find IAtj I - 1 intersections is J.W Guan, D.A. Bell /Art@ial Intelligence 105 (1998) 77-103 93 Thus, the time complexity of Algorithm K is 21A’ x O(IU121AI + IUj*lA() =0(2’A’IAIIU12). 7. Significance and core In order to reduce the exponential complexity of rough analyses, we investigate the ID if X and significance of an attribute x in an attribute set X (s A): attribute x is significant X - (x} are not ID, and x is not significant if X and X - {x) are ID. Intuitively, some attributes are not significant in a representation, removal has no real impact on the value of the representation significant, we can simply remove an attribute from further consideration. the significance of every attribute. Using classification, we can analyze in the sense that their If it is not of objects. Definition 7.1. Let Z = (U, A) be an information A: 0 c X C A. Given an attribute x E X, we say that x is significant and that x is not significant or nonsignzjicant in X if 8~ = ~x+J. system. Let X be a non-empty subset of in X if 8x c 8x_(X); That is, x E X is significant in X if and only if X += X - {x}; x E X is not significant in XifandonlyifXttX-{x). We can introduce a quantitative measure for significance as follows. Definition 7.2. Let X be a non-empty we define the signzjicance of x in X as subset of A: 0 c X L A. Given an attribute x E X, begs- = IuPxl- IUPX-{XII IUI . In the special case where X is a singleton, X = {x}, we also denote sig*(x) by sig(x): sig(x) = sigpj (x) = I~PXI - lUl4 = W/&l IUI So we always have sig(n) > 0 unless Q, = S. - I(Ul = lU/&l - 1 ’ IUI IUI To compute a significance (1) Compute 1x1 partitions u/e, for all x E X. The time complexity is 0(/U I*>. So the time complexity sigx_(xl (x), the following computations for computing are required. for computing 1x1 partitions is each partition WXI x lU12>. (2) To compute U/8x and U/Bx+~, 1x1 - 1 and 1x1 - 2 intersections time complexity forcomputingtheseintersectionsis(IXJ-l+(XI-2)xO(IUI”)=O(IXIx an intersection for computing are needed. The is 0( I U 12). So the time complexity lU12). Summarizing, We know the following: the time complexity for computing a significance is 0( I X I x ( U I 2). (1) 0 < sk?x_&) (2) attribute x is significant < 1 - 1llUl. in X if and only if sigx_(x) (x) > 0. 94 J.W Gum, D.A. Bell/Artificial Intelligence 105 (I998) 77-103 Example 7.1. For the skull information y4) (see Example 3.1), where yt = xi, y2 =x2, y3 = x3, y4 = x5. Also: system in Example 2.1, we have A = {yl , ~2, ~3, (1) yt is not significant in 1~1, ~2, ~3, ~41 since ~(y,,y2.y3.y4~ = &.y3,y4~ = E and ~~g(,,y,,,,,(Yl) = 0. (2) Y2 is not significant Gqy,,y3,y4)(Y2) = 0. in bl, y2, y3, y41 since @{yI,y2,y3.y4) = ~{yI,y3.y4] = & ad (3) ~3 eqy, (4) ~4 is not significant in (~1, ~2, ~3, ~41 since e{yl,y2,y3,y4) = f+y,.yz,y4) = E and ,y2,y4) (Y3) = 0. is not significant in (YI, ~2, ~3, ~4) since ~{y,,y2,y3,y4~ = @(y1,y2,y31 = 6 and &qyl,y2,y3](Y4) = 0. Example 7.2. Let X = (x), a singleton efi = n,,, 8 = 6. s0 (1) x is significant in X if e, # S. Indeed, sigx_lx)(x) = sign(x) > 0 if 6, # 6. in 2A. Notice that ox = n,,, 6, = ox, ox-(x) = (2) x is not significant Indeed, sigx+l(x) in X if e, = S. = sign(x) = 0 if 6, = S. Definition 7.3. Let X be a non-empty x E X which are significant cx = {x E x I Sigx_lxl(x) > 01. subset of A: 0 c X E: A. The set of all attributes is, the core of X, denoted by CX. That in X is called Also, we define Cn = 0 (see Fig. 8). Example 7.3. For in Example Cly,,y2,y3.y41 = 0 since Yi, ~2, ~3, y4 are not significant information the skull system in Iyi, y2, y3, y41. 2.1 (see Example 7.1): Example 7.4. Let X = {x), a singleton in 2A. From Example 7.2, we have the following. (1) Cl,) = (x) if e, # S since x is significant (2) Cl,) = 0 if e, = 6 since x is not significant in X. in X. Knowing what the core is can be useful for efficiency. in A, it would have been the focus of our attention If one of the attributes had been in rough analysis. Here we significant present an algorithm for computing the core. Fig. 8. The core for X. J.W. Gum, D.A. Bell /Artificial Intelligence IO5 (1998) 77-103 95 Algorithm C. Let (Cr, A) be an information universe, and A is the set of attributes. Let X be a non-empty This algorithm computes the core Cx of X. system, where U = {ut , ~42, . . , ulr/l} is the subset of A: !3 c X c A. Cx (i.e., x is significant LetX=[xt,x2,..., x1x1]. The notation Cx 3 x represents in X). [Initialize]. Set i t 1. (Cl) the fact that x is included in cc21 bkx_(x; 1 (Xi) > 03 If sigx_(& 1 (Xi) > 0 go to (C3). 1 (Xi) = 0 go to (C4). If sigx+, [Set Cx 3 xi]. Set Cx 3 xi go to (C4). (C3) (C4) [i = IX]?]. If i = 1x1 then the algorithm is finished and Cx is the core of X. If i < 1x1 then go to (C5). [Increase il. Set i t i + 1. Go to (C2). (C5) This algorithm computes ]X] significancies sigx_(,,l(xi) for i = 1,2,. . , 1x1. The time complexity Algorithm C is 0(]X]2]U]2). for computing one significance is 0( IX] 1 U12). So the time complexity of Example 7.5. Using Algorithm C, for the skull information system we find that %,>Y2.Y3,Y4) = 0. A subset X of A may have many keys X0. Let us denote them by X01, X02, . . . , Xos: s > 1. Now we prove that the intersection of all keys is equal to the core. Theorem 7.1 (Pawlak CX = n;=, XO~, where Xut , X02, . . . , Xui, . . . , Xus are all keys of X (see Fig. 9). [6]). Let (U, A) be an information system. Let X s A. Then Proof. 6) CX S nf=, XOi. S uppose that x E CX. We want to prove that x E nf=, Xui . Assume thatx~Xoiforsomei.ThenXo~~X-(x}cXandso8x,,~~~_l~l~~~. On the other hand, 8x = Bxoi since Xui is a key of X. Thus, we find that &_lxl = 8~; i.e., x is nonsignificant in X; i.e., x $ Cx. This is a contradiction. Summarizing,ifxECxthenxEXojforalli=1,2,...,s;i.e.,xEn~=1Xoi. (ii) CX 1 n;_-, X uI. Suppose that x E n;__, Xui . We want to prove that x E CX . Assume in X and we have OX = @x-lx). that x $ Cx, where x E X. Then x is nonsignificant On the other hand, since keys always exist, we know that X - {x} has a key X0. From X0 C X - {x) we know that x 4 X0. Also, key X0 of X - {x} satisfies: (I 1 ox, = ox-1x1; (2) if X’ c Xu then f3x_lxl c @xl. These two conditions can be rewritten as: (I) 0x0 = ox; (2) if X’ c X0 then 8x c 8x1. 96 J. W Gum, D.A. Bell /ArtQicial Intelligence IO5 (1998) 77-103 Fig. 9. Keys and core. that Xo C X - {x} c X, we know that Xo is a key of X as well. So Xn = Xoi Noting for some i, and x 6 X0. Then, from x 4 Xai for some i we have x $ nf=, Xoi . This is a contradiction. Summarizing, if x E ni=, Xoi then x E CX. q Example 7.6. For the skull information y4} of all attributes has two keys {y4}, {yl, y3}. So it has core CA = {y4} n {yl, ~3) = 0. system (see Example 6. l), the set A = { ~1, ~2, y3, 8. Significant subsets of attributes The time complexity for finding keys can be reduced by analysing the significance of attributes. Let (U, A) be an information attributes. A subset X (C A) is significant system, where U is the universe, if its every attribute is significant. and A is the set of Definition 8.1. Let X be a non-empty is said to be signi$cant or independent [6] if each x E X is significant nonsigni$cant. subset of A: 0 c X E A. The non-empty subset X in X; otherwise X is An empty set 0 is said to be signi@ant. Thus, an attribute set X is significant Let X E A be a (non-empty if and only if X is equal to its core: X = Cx. or empty) subset of A. It is easy to verify the following assertions. (1) If X is nonsignificant, then every superset X U [xl, x2, . . . , xl) of X is nonsignificant, where x1, x2, . . . , xl E A - X. J. W Gum, D.A. Bell / Art$icial Intelligence 105 (I 998) 77-103 97 significant A c-&G3 nonsignificant J V Fig. 10. Significant subsets of attributes. (2) If X is significant, then every subset X - {xl, x2, . . . , x,} of X is significant, where x1, x2,. . . , xS E X (see Fig. 10). Example 8.1. For the skull information can be calculated for subsets of A = {yt, ~2, ~3, ~4) as follows. system (see Examples 2.1 and 3. l), significance (1) 0 is significant. (2) All singletons ent from S. (yt }, (yz), (~31, (~4) are significant since $,, , oYz, GY3, $,, are differ- (3) A = (yl, y2, y3, ~4) is nonsignificant since yt, ~2, ~3, y4 are not significant in A. Now, notice that (U/8,, ( = I( VII, V12)1 = 2, where Vll = (#45, #163, #181), Vt2 = (#92, #167). I~/~,1 = IIV21, V22)l = 2, where V21 = (#45, #163, #167), V22 = (#92, #181). lU/$31 = 11V31, V32, V33)1 = 3, where V31 = (#45, #167), V32 = (#92, #181), V33 = (#163). lul~{y4~l = IIVSI, VIL V53, V54, V55)l = 5, where V51 = (#451, V52 = w), V53 = (#163), V54 = (#167), V55 = (#181). Iu/~(y,,y2]I = I(V51 u V539 V52, V54, V55)l =4. Iu/~(,,,,,)I = I(V51, V52> V53, V54, V55)l =5. J.W Guan, D.A. Bell /Artificial Intelligence IO5 (1998) 77-103 l~P{y*,y~]l = I{VSl u v54, v52 u v55, V53)l = 3. (4) (yl, ~2) is significant since yl, y2 are significant in (~1, ~2): siglyz) (yt ) = 2/5 > 0; @(,,)(Y2) ’ 0. (5) (~21 y3) is nonsignificant = 2/5 since y2 is not significant in {y2, y3) even if y3 is significant in (~2, ~31: ~~glysl(y2) = 0; s@,,l(y3) So its supersets ( yt , ~2, y3), (~2, y3, y4) are nonsignificant. = l/5 > 0. (6) (~3, y4) is nonsignificant since y3 is not significant in (~3, y4) even if y4 is significant its supersets (~2, y3, y4), ( yt , y3, y4) are nonsignificant. since yt is not significant = 0; siglr,l(y4) = 3/5 > 0. in (~3, ~41: sigly,,(y3) = 0; sigbzl(y4) = 2/5 > 0. Again, Similarly, { yl , y4) is nonsignificant is significant in (~1, ~4): sigl,)(yt) Also, its supersets (yt , ~2, y4), (yi , y3, y4) are nonsignificant. Also, (~2, y4) is nonsignificant significant Its supersets in (~2, ~4): sigly4J(y2) = 0; .rigruzl(y4) = 3/5 > 0. (~2, y3, y4), ( yt , ~2, ~4) are nonsignificant. since y2 is not significant in (~1, y4) even if y4 in (~2, y4) even if y4 is (7) (yt , y3) is significant since yt, y3 are significant in (yt, ~3): sigl,,)(yt) = 2/5 > 0; ~~g[~,,)(Y3) = 3/5 ’ 0. Example 8.2. Let X = (x), a singleton in 2A. We have the following. Case A. 6, # 6. X is significant Case B. 19, = 6. X is nonsignificant since x is significant in X. since x is not significant. 9. Finding one key According to the concept of significance, we have the following suggests Fig. 11). finding the significant ID subsets in order to find the minimal fact about keys. It (see ID subsets First of all, it is easy to show the following. Let X be a (non-empty or empty) subset of A: X C A. A subset X0 of X is a key of X if and only if X0 satisfies: (1) ID: ox,, = 8x; (2’) significant: X0 is significant. Thus, from Theorem 7.1, we know that core Cx for every X & A is significant. Also, (1) If X0 s X is a key of X then Xc is significant. (2) If X is significant then X is the unique key of X. Example 9.1. For the skull information system (see Example 8.1), the keys of A = (yi, ~2, ~3, ~41 are as follows. (1) 0 is significant. It is not a key of A since OA = & and so 0.4 # 00 = 6. (2) All singletons (yt), (YZ), (~31, ty4) are significant. Among them (~1)~ (~2)~ (~3) arc not a key of A since 8,, , Q,,, 6,, # 6)~ = E, but (~4) is a key of A since 8, = 6. (3) A = (yt , ~2, y3, ~4) is not a key of A since it is nonsignificant. (4) (yt , ~2) is significant but it is not a key of A since 81y, ,y21 # E. J.W Guan, D.A. Bell /Art$cial Intelligence 105 (1998) 77-103 99 significant A functional dependency minimum identity dependency V nonsignificant X Fig. 11. Keys: significant ID subsets. So it is not a key of A. Its supersets [yr , ~2, y3], (~2, ~3, ~4) (5) (6) (7) It is not a key of A. Its supersets and so they not keys. (~2. ~3) is nonsignificant. are nonsignificant, (~3, ~4) is nonsignificant. are nonsignificant, Similarly, { yl , y3, y4} are nonsignificant, Also, {y2, ~4) is nonsignificant-it {yl? ~2, y4) are nonsignificant, (yt . y3} is significant and is a key of A since 81,,,,,1 = 8. (~1, y4} is nonsignificant-it and so they not keys. so they not keys. so they not keys. (~2, y3, y4), {yl , y3, y4} is not a key of A, and supersets {yl, ~2, y4}, is not a key of A, and supersets (~2, y3, y4}, Example 9.2. Let X = {x}, a singleton in 2A. Case A. 13, # 6. Xu = X is the unique key of X since (1) 8x0 =8x; (2) ifX’CXuthenX’=0and8x=&,CBx/=f&=6. Notice that X0 = {x) (6, # S) is significant. Case B. 0, = 6. X0 = 0 is the unique key of X since (1) ex,=8~=6,=8x=e,=s; (2) there are no X’ c X0 since Xu = 0. Notice that Xu = 0 is significant. We can find one key X0 for every (non-empty or empty) subset X C A as follows. Use that we mathematical can find one key for 1x1 - 1. We want to find one key for subset X. There are two cases depending on whether X is significant or not. induction on [XI. We can find one key when (XI = 0,l. Suppose Case A. X is significant. Then X is the unique key of X. 100 J.N! Gun, D.A. Bell /Artificial Intelligence 105 (1998) 77-103 Case B. X is nonsignificant. xl E X: Bx = for this x1 E X. Let X1 = X - (xl]. Then Bx = 8x, and 1x11 = 1x1 - 1. So we can 8~-+,1 find one key for X1 by the induction hypothesis. That is, X1 has a key Xo such that: In this case, there exists a nonsignificant (1) 8x0 = ox,; (2) Xc is significant. Notice that 8x = 19x,. So, (1) 8x0 =8x; (2) Xo is significant. That is, X has also the key Xo. The mathematical to find one key is the following, accordingly. induction is completed. An algorithm Algorithm A. Let (U, A) be an information a subset of A. This algorithm finds one key of X. system. Let X = {xl, x2, . . . , xj, . . . , xlxl} be Step 1. Compute U/O,, and sigx_(xj 1 (xj) for j = 1,2, . . . , IX I. Choose Xj, such that If sigx_lxj, l (xj, ) > 0 then X is one key of X and the algorithm is completed. Otherwise, go to Step 2. Step 2. Compute sigx_txj, ,xj) (xj) for j = 1,2, . . . , [XI; j # jl. Choose xi2 such that sigX-[Xjl ,Xj2]('h) = j,1,2,~tfnx,; jzj, ’ (sigX-(X,, ,xjl('j)). If sigX-[Xjl ,Xj2) (xj,) > 0 then X - (xji } is one key of X and the algorithm Otherwise, go to Step 3. Step 3. Compute sigx_txj, for j = 1,2, . . . , 1x1; j # ,xjz,,j)(~j) jl, jz. Choose xjj such is completed. that If sigX-[Xjl ,Xj2,Xj3J (Xj3) > 0 then X - {xj,, xj,} is one key of X and the algorithm is completed. Otherwise, go to Step 4. And so on. Step 1x1. Compute S~gX-[~j,,xjz,...,~j,x,_l,~jJ(Xj) = Sig(Xj) for j = I,&. . ., 1x1; j # jl,j2 , . . . , jlxl-1. If sig(xj) > 0 then (xj) is one key of X and the algorithm If sig(xj) = 0 then the empty set 0 is one key of X and the algorithm is completed. is completed. By using this algorithm, At the first step, we check if X is significant; 6x c Bx-(~I? We need to compute 1x1 significancies the time complexity to find one key is polynomial. i.e., if every x E X is significant for j = 1,2,. sigx_txj)(xj) in X: . ., [XI. to compute one significance is 0( 1 XI I U 12). So the price of the first time complexity The step is 1x1 x 0(IXllU12). At Step 2, we need to compute 1x1 - 1 significancies sigx_lxj,,,j)(xj). The time complexity 1) x wIxll~12~. to compute one significance is 0( (X I I U I 2). So the price of Step 2 is (IX I - J.W. Gum, D.A. Bell/Arti&ial Intelligence 105 (1998) 77-10.3 101 At Step 3, we need to compute 1x1 - 2 significancies sigx_txI, ,xjz,xj)(Xj). So the price of Step 3 is (1x1 - 2) x O(IXllU12). And so on. At Step 1x1, we need to compute 1 significance 1 x ww42~. So the total price is sig(.xj). So the price of step 1x1 is (IX1 + (1x1 - 1) + WI - 2) + ... + 1) x o(Ixllu12) = lx”l;l + l) x O(IXllU12) = o(lx131u12). Notice that Algorithm A can be run in parallel mode to compute all keys concurrently. The following is an example. Example 9.3. For the skull information system, from Example 8.2, keys for A = (~1, ~2, ~3, ~41 can be found. Step 1. A is nonsignificant, and %%YLY3)(Y4) = s%%YW‘ll(Yr) = s&I,YW4l(Y2) = ~~gbJ,>y2,.Y4)(Y3) = 0. Step 2A. Choose yj, = yt , Sb?[y2,y3](Y4) = Z/5? ~~g(y,,y4](Y2) = 0, sk[y,,,)(Y3) = 0. Step 3AA. Choose yj, = yt , yjz = y2, %qy,](Y4) = z/5, %{,)(YS) = 0. Step 4AA. Choose Yj, = Yr , yj2 = y2, yj, = Y3, sig(y4) = 4/5 > 0. So (~4) is one key. Go to Step 3AB. Step 3AB. Choose Yj, = yt, yjz = y3, sigIyzl(y4) = 2/5, sigIy41(y2) = 0. So Yj, = y2, and {yjr , yjz, yj,] = (~1, ~2, y3} is the same as that in Step 4AA. Go to Step 2B. Step 2B. Choose yj, = y2, %$11,y3) (Y4) = 0, ~~&f(y,,,)(Yl) = 07 sG+,,yd)(Y3) = 0. Step 3BA. Choose yj, = Y2, Yj, = y4, siglyrl(y3) = 3/5 > 0, sig(yjl(Yl) = 2/5 > 0. So {yt , ~3) is one key. Go to Step 2C. Step 2C. Choose Yj, = ~3, ~~qy,,,,,(Y4) = l/5 ’ 0, sk{yl,y4)(Y2) = 0, ~k[,,,,)(Yl) = 0. Step 3CA. Choose yj, = Y3, yj, = Y2, big = 3/5 ’ 0, ~Gq,)(Yl) =o. 102 J.W Gum, D.A. Bell /Artijicial Intelligence 105 (1998) 77-103 So yj, = yt, and {yj, , yj,, yj,} = {yt, ~2, y3} is the same as that in Step 4AA. GO to Step 2D. Step 2D. Choose yj, = ~4, ~igl~,,~,~(~~) = 215 > 0, ~k+,,~,)(~2) = 0, q,,,y,l(Y3) = ]/5 > 0. SO yj, = ~2, and {yj, , yj,} = (~2, ~4) is the same as that in Step 3BA. The algorithm size or the combination is completed and the keys (~4) and {yt, ~3) are output. So either the skull of teeth size and location can be used to identify specimens. 10. Conclusions of computational methods in the form of classification. In this paper we have presented an investigation for rough for use in such analysis of databases as a step analysis. We suggest a series of algorithms In particular, we suggest the in the discovery of knowledge and keys in data collections. The use use of a significance measure cost of discovery by the of the significance measure allows us to reduce the computational rough analysis method. The use of these algorithms might produce evidence and insights into states of affairs in the world being modelled in other investigation has been used throughout, and ways. A running example of a paleontological data collection can from the corresponding the keys and dependencies in that study. be used as evidence to choose between hypotheses that are not readily available to obtain dependencies that are discovered Acknowledgement The authors wish to thank the anonymous Also, we would like to thank D. Ruan for useful comments on this study. for their constructive reviewers comments. References [I] D.A. Bell, From data properties to evidence, IEEE Transactions on Knowledge and Data Engineering 5 (6) (1993) 965-968. [2] D.A. Bell, J.W. Guan, Computational methods for rough classification and discovery, J. Amer. Sot. Inform. Sci., Special Topic Issue on Data Mining (to appear). [3] P.R. Cohen, Heuristic Reasoning about Uncertainty: An Artificial Intelligent Approach, Pitman Advanced Publishing Program, 1985. [4] J.W. Grzymala-Busse, Managing Uncertainty [5] T.Y. Lin, N. Cercone, Rough Sets and Data Mining: Analysis of Imprecise Data, Kluwer Academic, in Expert Systems, Kluwer Academic, Dordrecht, 1991. Dordrecht, 1996. [6] Z. Pawlak, Rough Sets: Theoretical Aspects of Reasoning about Data, Kluwer, Dordrecht, Netherlands, 1991. [7] Z. Pawlak, J.W. Grzymala-Busse, R. Slowinski, W. Ziarko, Rough sets, Comm. ACM 38 (11) (1995) 89-95. systems, Bull. Polish Acad. Sci. Math. 33 [8] Z. Pawlak, C. Rauszer, Dependency of attributes in information (1985) 551-559. [9] G. Piatetsky-Shapiro, W.J. Frawley (Eds.), Knowledge Discovery in Databases, AAAI/MIT Press, Cambridge, MA, 1991. J.W Gum, D.A. Bell /Art$icial Intelligence 105 (1998) 77-103 103 [lo] R. Ruan (Ed.), Intelligent Hybrid Systems: Fuzzy Logic, Neural Networks and Genetic Algorithms, Kluwer Academic, Dordrecht, 1997. [ 111 R.R. Stoll, Sets, Logic and Axiomatic Theories, W.H. Freeman, San Francisco, CA, 1961. 1121 J. Ullman, Principles of Database Systems, Prentice-Hall, Englewood Cliffs, NJ, 1986. [ 131 W. Ziarko, The discovery, analysis, and representation of data dependencies in databases, in: G. Piatetsky- in Databases, AAAI Press/MIT Press, Cambridge, MA, Shapiro, W.J. Frawley (Eds.), Knowledge Discovery 1991, pp. 177-195. 1141 A. Walker, R.E.F. Leakey, The hominids of East Turkana, Scientific American (August 1978) 54-66. 