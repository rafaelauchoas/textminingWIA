Artificial Intelligence 175 (2011) 220–235Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintDealing with logical omniscience: Expressiveness and pragmatics ✩Joseph Y. Halpern a, Riccardo Pucella b,∗a Cornell University, Ithaca, NY 14853, USAb Northeastern University, Boston, MA 02115, USAa r t i c l ei n f oa b s t r a c tArticle history:Available online 3 April 2010Keywords:LogicKnowledgeLogical omniscienceAwarenessImpossible worldsProbability1. IntroductionWe examine four approaches for dealing with the logical omniscience problem and theirpotential applicability: the syntactic approach, awareness, algorithmic knowledge, and im-possible possible worlds. Although in some settings these approaches are equi-expressiveand can capture all epistemic states, in other settings of interest (especially with proba-bility in the picture), we show that they are not equi-expressive. We then consider thepragmatics of dealing with logical omniscience—how to choose an approach and constructan appropriate model.© 2010 Elsevier B.V. All rights reserved.John McCarthy was a pioneer in the use of reasoning about knowledge in AI. His notion of what “any fool” knows, oneof the earliest uses of common knowledge, goes back to roughly 1970; it first appears in a published paper in [21]. It thusseems particularly appropriate for a paper on logics of knowledge to appear in this special issue of Artificial Intelligencededicated to John McCarthy and his work.Like most authors, McCarthy gave “possible-worlds” style semantics to knowledge. Logics of knowledge based onpossible-worlds semantics have been shown to be useful in many areas of knowledge representation and reasoning, rangingfrom security to distributed computing to game theory. In these models, an agent is said to know a fact ϕ if ϕ is true inall the worlds she considers possible. While reasoning about knowledge with this semantics has proved useful, as is wellknown, it suffers from what is known in the literature as the logical omniscience problem: under possible-world semantics,agents know all tautologies and know the logical consequences of their knowledge.While logical omniscience is certainly not always an issue, in many applications it is. For example, in the context ofdistributed computing, we are interested in polynomial-time algorithms, although in some cases the knowledge neededto perform optimally may require calculations that cannot be performed in polynomial time (unless P = NP) [26]; in thecontext of security, we may want to reason about computationally bounded adversaries who cannot factor a large compositenumber, and thus cannot be logically omniscient; in game theory, we may be interested in the impact of computationalresources on solution concepts (for example, what will agents do if computing a Nash equilibrium is difficult).Not surprisingly, many approaches for dealing with the logical omniscience problem have been suggested (see [10, Chap-ter 9] and [25]). A far from exhaustive list of approaches includes:• syntactic approaches [5,24,18], where an agent’s knowledge is represented by a set of formulas (intuitively, the set offormulas she knows);• awareness [7], where an agent knows ϕ if she is aware of ϕ and ϕ is true in all the worlds she considers possible;✩A preliminary version of this paper appeared in the Proceedings of the 11th Conference on Theoretical Aspects of Rationality and Knowledge, 2007, pp. 169–176.* Corresponding author.E-mail addresses: halpern@cs.cornell.edu (J.Y. Halpern), riccardo@ccs.neu.edu (R. Pucella).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.009J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235221• algorithmic knowledge [12] where, roughly speaking, an agent knows ϕ if her knowledge algorithm returns “Yes” on a• impossible worlds [29], where the agent may consider possible worlds that are logically inconsistent (for example, wherequery of ϕ; andp and ¬p may both be true).Which approach is best to use, of course, depends on the application. One goal of this paper is to elucidate the aspectsof the application that make a logic more or less appropriate. We start by considering the expressive power of theseapproaches. In Section 3, we examine the expressiveness of the approaches for a general epistemic logic. It may seemthat there is not much to say with regard to expressiveness, since it has been shown that all these approaches are equi-expressive and, indeed, can capture all epistemic states (see [31,10] and Section 2). However, this result holds only if weallow an agent to consider no worlds possible. As we show, this equivalence no longer holds in contexts where agents mustconsider some worlds possible.This difference in expressive power is particularly relevant once we have probability in the picture. In Section 4, weexamine the logical omniscience problem in the context of an epistemic logic that can talk explicitly about probability, withformulas of the form K ((cid:3)(Primen) = 1/3), read “the agent knows that the probability that Primen is true is 1/3”. We showthat in the presence of probabilities, the approaches to dealing with logical omniscience that make sense in this setting arenot equi-expressive.But expressive power is only part of the story. We consider here (mainly by example) the pragmatics of dealing withlogical omniscience—an issue that has largely been ignored: how to choose an approach and construct an appropriatemodel. In Section 5, we examine the four main approaches to logical omniscience, and identify some guiding principles forchoosing an approach to model a situation, based on the source of the lack of logical omniscience in that situation. Comingup with an appropriate structure can be nontrivial. As a specific contribution, we illustrate a general approach to deriving animpossible-worlds structure based on an implicit description of the situation, which seems to be appropriate for a numberof situations of interest.2. The four approaches: a reviewWe now review the standard possible-worlds approach and the four approaches to dealing with logical omnisciencediscussed in the introduction. For ease of exposition we focus on the single-agent propositional case. While in many appli-cations it is important to consider more than one agent and to allow first-order features (indeed, this is true in some ofour examples), the issues that arise in dealing with multiple agents and first-order features are largely orthogonal to thoseinvolved in dealing with logical omniscience. Thus, we do not discuss these extensions here.2.1. The standard approachWe define a propositional language LK of knowledge. Starting with a set Φ of primitive propositions, we close off underconjunction (∧), negation (¬), and the K operator. As usual, we consider ϕ ∨ ψ to be an abbreviation for ¬(¬ϕ ∧ ¬ψ), andϕ ⇒ ψ to be an abbreviation for ¬ϕ ∨ ψ . K ϕ will usually be read as “the agent knows ϕ”, but because K ϕ ⇒ ϕ will notalways hold in our models, K ϕ will sometimes have a more natural reading as “the agent believes ϕ”. None of our resultsdepend on the reading of the operator.We give semantics to LK formulas using Kripke structures. For simplicity, we focus on approaches that satisfy the K45(cid:6), π ), where W is a nonemptyaxioms (as well as KD45 and S5).1 In this case, a K45 Kripke structure is a triple (W , W(cid:6) ⊆ W is the set of worlds that the agent considers possible, and π is anset of possible worlds (or worlds, for short), Winterpretation that associates with each world a truth assignment π (w) to the primitive propositions in Φ. Note that theagent need not consider every possible world (that is, each world in W ) possible. Then we have(M, w) |(cid:8) p iff π (w)(p) = true, where p ∈ Φ.(M, w) |(cid:8) ¬ϕ iff (M, w) (cid:10)|(cid:8) ϕ.(M, w) |(cid:8) ϕ ∧ ψ iff (M, w) |(cid:8) ϕ and (M, w) |(cid:8) ψ .(M, w) |(cid:8) K ϕ iff (M, w(cid:6)) |(cid:8) ϕ for all w(cid:6) ∈ W.(cid:6)This semantics suffers from the logical omniscience problem. In particular, one sound axiom is(cid:2)(cid:3)K ϕ ∧ K (ϕ ⇒ ψ)⇒ K ψ,which says that an agent’s knowledge is closed under implication. In addition, the knowledge generalization inference rule issound:From ϕ infer K ϕ.1 We could extend the investigation in this paper to more general structures satisfying weaker axioms, but consider the more standard setting sufficesfor the points we want to make. We expect similar results to the ones we obtain here to hold for more general structures, but have not checked the details.222J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235Thus, agents know all tautologies. As is well known, two other axioms are sound in K45 Kripke structures:K ϕ ⇒ K K ϕand¬K ϕ ⇒ K ¬K ϕ.These are known respectively as the positive and negative introspection axioms. (These properties characterize K45.)In the structures we consider, we allow Wto be empty, in which case the agent does not consider any worlds possible.(cid:6), π ) where(cid:6) (cid:10)= ∅. Thus, in a KD45 Kripke structure, the agent always considers at least one world possible. In KD45 Kripke structures,In such structures, K ϕ is true for all ϕ, including false. A KD45 Kripke structure is a K45 Kripke structure (W , WWthe axiom(cid:6)¬K (false)is sound, which implies that the agent cannot know inconsistent facts. The logic KD45 results when we add this axiom toK45. S5 Kripke structures are KD45 Kripke structures where W = W; that is, the agent considers all worlds in W possible.In S5 Kripke structures, the axiom(cid:6)K ϕ ⇒ ϕ,which says that the agent can know only true facts, is sound. Adding this axiom to the KD45 axioms gives us the logic S5.2.2. The syntactic approachThe intuition behind the syntactic approach for dealing with logical omniscience is simply to explicitly list, at every pos-(cid:6), π , C), where(cid:6), π ) is a K45 Kripke structure and C associates a set of formulas C(w) with every world w ∈ W . The semantics ofsible world w, the set of formulas that the agent knows at w. A syntactic structure has the form M = (W , W(W , Wprimitive propositions, conjunction, and negation is just the same as for Kripke structures. For knowledge, we have(M, w) |(cid:8) K ϕ iff ϕ ∈ C(w).Of course, for syntactic structures, the set of possible worlds plays no role in the semantics of knowledge.2.3. AwarenessAwareness is based on the intuition that an agent should be aware of a concept before she can know it. The formulasthat an agent is aware of are represented syntactically; we associate with every world w the set A(w) of formulas thatthe agent is aware of. For an agent to know a formula ϕ, not only does ϕ have to be true at all the worlds she considers(cid:6), π )possible, but she has to be aware of ϕ as well. A K45 awareness structure is a tuple M = (W , Wis a K45 Kripke structure and A maps worlds to sets of formulas. We now define(cid:6), π , A), where (W , W(M, w) |(cid:8) K ϕ iff(M, w(cid:6)) |(cid:8) ϕ for all w(cid:6) ∈ W(cid:6)and ϕ ∈ A(w).2We can define KD45 and S5 awareness structures in the obvious way: M = (W , W(cid:6), π ) is a KD45 structure, and an S5 awareness structure when (W , Wwhen (W , W(cid:6), π , A) is a KD45 awareness structure(cid:6), π ) is an S5 structure.2.4. Algorithmic knowledgeIn some applications, there is a computational intuition underlying what an agent knows; that is, an agent computeswhat she knows using an algorithm. Algorithmic knowledge is one way of formalizing this intuition. An algorithmic knowledge(cid:6), π ) is a K45 Kripke structure and A is a knowledge algorithm thatstructure is a tuple M = (W , Wreturns “Yes”, “No”, or “?” given a formula ϕ.3 Intuitively, A(ϕ) returns “Yes” if the agent can compute that ϕ is true, “No”if the agent can compute that ϕ is false, and “?” otherwise. In algorithmic knowledge structures,(cid:6), π , A), where (W , W(M, w) |(cid:8) K ϕ iff A(ϕ) = “Yes”.2 In [7], the symbol K is reserved for the standard definition of knowledge; the definition we have just given is denoted as Xϕ, where X stands forexplicit knowledge. A similar remark applies to the algorithmic knowledge approach below. We use K throughout for ease of exposition.3 In [12], the knowledge algorithm is also given an argument that describes the agent’s local state, which, roughly speaking, captures the relevantinformation that the agent has. However, in our single-agent static setting, there is only one local state, so this argument is unneeded.J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235223As in the syntactic approach, the set of possible worlds plays no role in the semantics of knowledge.An important class of knowledge algorithms consists of the sound knowledge algorithms. When a sound knowledgealgorithm returns “Yes” to a query ϕ, then the agent knows (in the standard sense) ϕ, and when it returns “No” to aquery ϕ, then the agent does not know (again, in the standard sense) ϕ. Thus, if A is a sound knowledge algorithm, thenA(ϕ) = “Yes” implies (M, w) |(cid:8) ϕ for all w ∈ Wsuch that (M, w) |(cid:8) ¬ϕ.(When A(ϕ) = “?”, nothing is prescribed.), and A(ϕ) = “No” implies there exists w ∈ W(cid:6)(cid:6)2.5. Impossible worldsThe impossible-worlds approach relies on relaxing the notion of possible world. Take the special case of logical omni-science that says that an agent knows all tautologies. This is a consequence of the fact that a tautology must be true atevery possible world. Thus, one way to eliminate this problem is to allow tautologies to be false at some worlds. Clearly,those worlds do not obey the usual laws of logic—they are impossible possible worlds (or impossible worlds, for short).A K45 (resp., KD45, S5) impossible-worlds structure is a tuple M = (W , WKD45, S5) Kripke structure, W(cid:6)(cid:6) − W a set of formulas. WWmay well include impossible worlds in Wof impossible-worlds structures intermediate between K45 and KD45 impossible-worlds structures. A KD45worlds structure is a K45 impossible-worlds structure (W , Wstructure, we do not require that W(cid:6) ∩ W , π ) is a K45 (resp.,is the set of worlds that the agent considers possible, and C associates with each world in, the set of worlds the agent considers possible, is not required to be a subset of W —the agent(cid:6) − W are the impossible worlds. We can also consider a classimpossible-impossible-worlds(cid:6), π , C), where (W , W(cid:6) ∩ W be nonempty.(cid:6), π , C) where Wis nonempty. In a KD45. The worlds in W−−(cid:6)(cid:6)(cid:6)A formula ϕ is true at a world w ∈ W(cid:6) − W if and only if ϕ ∈ C(w); for worlds w ∈ W , the truth assignment is like thatin Kripke structures. Thus,• if w ∈ W , then (M, w) |(cid:8) p iff π (w)(p) = true;• if w ∈ W , then (M, w) |(cid:8) K ϕ iff (M, w• if w ∈ W(cid:6) − W , then (M, w) |(cid:8) ϕ iff ϕ ∈ C(w).(cid:6)) |(cid:8) ϕ for all w(cid:6) ∈ W(cid:6);We remark that when we speak of validity in impossible-worlds structures, we mean truth at all possible worlds in W inall impossible-worlds structures M = (W , . . .).3. Expressive powerThere is a sense in which all four approaches are equi-expressive, and can capture all states of knowledge. To make thisprecise, define a set Φ(cid:6)is a consistent set of formulas of propositionallogic when we treat formulas of the form K ϕ as primitive propositions (a distinct one for each ϕ). Thus, propositionalconsistency ignores properties of the knowledge operator. We take for granted here a sound and complete axiomatizationof propositional logic [6], and therefore what we call a propositionally consistent set is also a propositionally satisfiable set.of formulas in LK to be propositionally consistent if Φ(cid:6)Theorem 3.1. (See [31,10].) For every finite set F of formulas and every propositionally consistent set G of formulas, there existsimpossible-worlds structure, algorithmic knowledge structure) M =a syntactic structure (resp., K45 awareness structure, KD45(W , . . .) and a world w ∈ W such that (M, w) |(cid:8) K ϕ if and only if ϕ ∈ F , and (M, w) |(cid:8) ψ for all ψ ∈ G.4−Proof. We review the basic idea of the proof, since it will set the stage for our later results.• For syntactic structures, let M = ({w}, ∅, π , C), where C(w) = F and π (w) is such that (M, w) |(cid:8) ψ for all ψ ∈ G.(Since G is propositionally consistent, there must be a truth assignment that makes all the formulas in G true; we cantake π (w) to be that truth assignment.)• For K45 awareness structure, let M = ({w}, ∅, π , A), where A(w) = F and π (w) makes all the formulas in G true.• For KD45in G true.impossible-worlds structure, let M = ({w}, {w(cid:6)) = F and π (w) makes all the formulas(cid:6)}, π , C), where C(w−• For algorithmic knowledge, let M = ({w}, ∅, π , A), where A(ϕ) = “Yes” iff ϕ ∈ F and π (w) makes all the formulas in Gtrue. (cid:2)Despite the name, the introspective axioms of K45 are not valid in K45 awareness structures or K45 impossible-worldsstructures. Indeed, it follows from Theorem 3.1 that no axioms of knowledge are valid in these structures. (Take F tobe the empty set.) In fact, we can show that a formula is valid with respect to K45 awareness structures (or KD45 orimpossible-worlds structures, syntactic structures, algorithmic knowledge structures) if and only if it is a substitutionKD45−4 This result extends to infinite sets F of formulas for syntactic structures, K45 awareness structures, and KD45−impossible-worlds structures. Foralgorithmic knowledge structures, the result extends to recursive sets F of formulas.224J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235instance of a propositional tautology, that is, the result of substituting arbitrary formulas in LK for the primitive propositionsin a propositional tautology. To make this precise, let Prop be the axiomϕ is a substitution instance of a valid formula of propositional logicand MP be the inference ruleFrom ϕ ⇒ ψ and ϕ infer ψ .(Prop)(MP)The inference rule MP is not needed for our next result, but it will be needed later when we look at axiomatizations forLK in different structures, so we introduce it here.Theorem 3.2. {Prop, MP} is a sound and complete axiomatization of LK with respect to K45 awareness structures (resp., K45 andKD45impossible-worlds structures, syntactic structures, algorithmic knowledge structures).−−Proof. Soundness is straightforward to establish in all cases. For completeness, we show that a consistent formula is sat-isfiable. Suppose that ϕ is consistent with {Prop, MP}. It suffices to show that ϕ is satisfiable in a K45 awareness (resp.,K45 and KD45impossible-worlds structure, syntactic structure, algorithmic knowledge structure). Viewing formulas of theform K ψ as primitive propositions, ϕ must be propositionally consistent. Thus, there must be a truth assignment v to theprimitive propositions and formulas of the form K ψ that appear in ϕ such that ϕ evaluates to true under this truth assign-ment. Let F consist of all formulas ψ such that v(K ψ) = true and let G consist of all the propositional formulas ψ suchthat v(ψ) = true. Let M be the structure guaranteed to exist by Theorem 3.1. It is easy to see that (M, w) |(cid:8) ϕ. (cid:2)It follows from Theorem 3.2 that a formula is valid with respect to K45 awareness structures (resp., K45 and KD45impossible-worlds structures, syntactic structures, algorithmic knowledge structures) if and only if it is a substitution in-stance of a propositional tautology. Thus, deciding if a formula is valid is co-NP complete, just as it is for propositionallogic.Theorems 3.1 and 3.2 rely on the fact that we are considering K45 awareness structures and KD45(or K45) impossible-worlds structures. (Whether we consider K45, KD45, or S5 is irrelevant in the case of syntactic structures and algorithmicknowledge structures, since the truth of a formula does not depend on what worlds an agent considers possible.) There areconstraints on what can be known if we consider KD45 and S5 awareness structures and impossible-worlds structures. Theconstraints depend on which structures we consider. To make the constraints precise, we need a few definitions. We say aset of formulas F is downward closed if the following conditions hold:−−(a) if ϕ ∧ ψ ∈ F , then both ϕ and ψ are in F ;(b) if ¬¬ϕ ∈ F , then ϕ ∈ F ;(c) if ¬(ϕ ∧ ψ) ∈ F , then either ¬ϕ ∈ F or ¬ψ ∈ F (or both); and(d) if K ϕ ∈ F , then ϕ ∈ F .We say that F is k-compatible with F(cid:6)if K ψ ∈ F(cid:6)implies that ψ ∈ F .Proposition 3.3. Suppose that M = (W , Wand let F(cid:6)) |(cid:8) ψ}. Then F(cid:6) = {ψ | (M, w(cid:6)(cid:6), . . .) is a KD45 awareness structure, w ∈ W , and w(cid:6) ∈ W(cid:6). Let F = {ϕ | (M, w) |(cid:8) K ϕ}is a propositionally consistent downward-closed set of formulas that contains F .Proof. Suppose that M = (W , Wthe theorem. Clearly F ⊆ F. Since wbeing downward closed. For the last condition, note that if (M, ww(cid:6), . . .) is a KD45 awareness structure. Let w, wis a possible world, it is easy to see that Fmust be propositionally consistent, since w(cid:6)) |(cid:8) ψ . Finally, F, so (M, w(cid:6)(cid:6) ∈ W(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6), F , and F(cid:6)(cid:6)) |(cid:8) K ψ , then we must have (M, wbe as in the statement ofsatisfies the first three conditions of(cid:6)(cid:6)) |(cid:8) ψ for all worldsis a possible world. (cid:2)(cid:6)Proposition 3.4. Suppose that M = (W , W(M, w) |(cid:8) K ϕ} and let F(cid:6) = {ψ | (M, w(cid:6)) |(cid:8) ψ}. Then(cid:6), . . .) is a KD45 impossible-worlds structure, w ∈ W , and w(cid:6) ∈ W ∩ W(cid:6). Let F = {ϕ |(cid:6)(a) F(b) F is k-compatible with F(cid:6).is a propositionally consistent downward-closed set of formulas that contains F ;Proof. The argument for (a) is the same as in the proof of Proposition 3.3, since w, suppose that K ϕ ∈ Fthat F is k-compatible with F(cid:6)that (M, wstructures, since we may not have ϕ ∈ A(w), and therefore we cannot necessarily derive (M, w) |(cid:8) K ϕ.) (cid:2)in this case. For (b), to see(cid:6)) |(cid:8) K ϕ. It follows(cid:6). Hence, (M, w) |(cid:8) K ϕ, so ϕ ∈ F . (Note that this argument does not work for awareness(cid:6)(cid:6)(cid:6)) |(cid:8) ϕ for all ϕ ∈ W, this means that (M, w. By the definition of F(cid:6)(cid:6) ∈ W ∩ W(cid:6)The next result shows that the constraints on F described in Propositions 3.3 and 3.4 are the only constraints on F .J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235225(cid:6)(cid:6)are such that FTheorem 3.5. If F and Fexists a KD45 awareness structure M = ({w, win addition, F is k-compatible with F(M, w) |(cid:8) K ϕ iff ϕ ∈ F and (M, w(resp., S5 impossible-worlds) structure.(cid:6)}, {wis propositionally consistent downward-closed set of formulas that contains F , then there. If,(cid:6)(cid:6)}, π , C) such that, so that M is an S5 awareness(cid:6)}, π , A) such that (M, w) |(cid:8) K ϕ iff ϕ ∈ F and (M, w(cid:6)}, {w, then there exists a KD45 impossible-worlds structure M = ({w, w(cid:6)) |(cid:8) ψ for all ψ ∈ F(cid:6), w(cid:6)(cid:6)) |(cid:8) ψ for all ψ ∈ F, then we can take w = w. Finally, if F = F(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)}, {wthen (M, w(cid:6)). Thus, (M, w(cid:6)) = {ϕ | K ϕ ∈ F(cid:6)}, π , A), where π (wtrue, A(w) = F , and A(w, so, by the induction hypothesis, (M, w(cid:6)) |(cid:8) ψ and, by construction, that ψ ∈ A(w(cid:6)}. We now prove by induction that if ϕ ∈ F(cid:6)) makes all the propositionalProof. In the case of KD45 awareness structures, let M = ({w, w(cid:6)) |(cid:8) ϕ.formulas in FThis is true by construction in the case of primitive propositions and follows easily from the induction hypothesis in thecase of conjunctions. If ϕ has the form K ψ then, since ψ must be in F, it follows from the induction hypothesis that(cid:6)) |(cid:8) K ψ . Finally, if ϕ has the form ¬ψ , we consider the(M, wpossible forms of ψ . If ψ is a primitive proposition it follows from the definition of π (w, then(cid:6)) |(cid:8) ϕ. Similarly, the result follows from the definitionψ (cid:6) ∈ Fof downward closure and the induction hypothesis if ψ has the form ψ1 ∧ ψ2. Finally, if ψ has the form K ψ (cid:6), then the result(cid:6)). It is now immediate that (M, w) |(cid:8) K ϕ iff ϕ ∈ F : if (M, w) |(cid:8) K ϕ then it follows fromfollows from the definition on A(wthe definition of A(w) that we must have ϕ ∈ F . Conversely, if ϕ ∈ F , then ϕ ∈ A(w) and (M, w), so(M, w) |(cid:8) K ϕ.If F = FIn the case of impossible-worlds structures, let M = ({w, win this argument to get an S5 awareness structure.(cid:6)}, {w(cid:6)) makes all the propositional(cid:6)(cid:6)) = F . A proof by induction on the structure of formulas much like that above shows thatto get that ψ ∈ F ,(cid:6)(cid:6)), and,. To deal with the case that ϕ = K ψ , we use the fact that F is k-compatible with F(cid:6)(cid:6)) |(cid:8) ψ . To see that (M, w) |(cid:8) K ϕ iff ϕ ∈ F , first observe that if ϕ ∈ F then, by construction ϕ ∈ C(w(cid:6)(cid:6)) |(cid:8) ϕ if ϕ ∈ F(cid:6)). If ψ has the form ¬ψ (cid:6), then we can take w = w(cid:6)(cid:6)}, π , C), where π (w(cid:6)) |(cid:8) ϕ (since F ⊆ Ftrue and C(w. Hence, (M, w(cid:6)) |(cid:8) ψ (cid:6)(cid:6), w(cid:6)(cid:6)(cid:6)(cid:6)(cid:6), (M, w(cid:6)) |(cid:8) ϕ, so (M, w) |(cid:8) K ϕ. For the converse, if (M, w) |(cid:8) K ϕ, then (M, w(cid:6)(cid:6)) |(cid:8) ϕ, so ϕ ∈ F . (cid:2)formulas in F(M, wso that (M, wsince F ⊆ F(cid:6)We can characterize these properties axiomatically. Let (Ver) be the standard Veridicality axiom, which says that every-thing known must be true:K ϕ ⇒ ϕ.(Ver)Let AXVer be the axiom system consisting of {Prop, MP, Ver}. The fact that the set of formulas known must be a subset of adownward-closed set is characterized by the following axiom:¬(K ϕ1 ∧ · · · ∧ K ϕn)if AXVer (cid:12) ¬(ϕ1 ∧ · · · ∧ ϕn).(DC)The key point here is that, as we shall show, a propositionally consistent set of formulas that is downward closed must beconsistent with AXVer.The fact that the set of formulas that is known is k-compatible with a downward-closed set of formulas is characterizedby the following axiom:(K ϕ1 ∧ · · · ∧ K ϕn) ⇒ (K ψ1 ∨ · · · ∨ K ψm)if AXVer (cid:12) ϕ1 ∧ · · · ∧ ϕn ⇒ (K ψ1 ∨ · · · ∨ K ψm).(KC)Axiom DC is just the special case of axiom KC where m = 0. It is also easy to see that KC (and therefore DC) followfrom Ver. As we now show, DC is strictly weaker than KC. Suppose that Φ = {p} (so that p is the only primitive propositionin the language), and take M = ({w 1, w 2}, w 2, π , A), where π makes p true at both w 1 and w 2, A(w 1) = {K p}, andA(w 2) = {K p, p}. Every instance of DC is valid in M (this is the soundness part of Theorem 3.6(a) below). Now considerthe formula K p ⇒ K p. This is an instance of Prop, so AXVer (cid:12) K p ⇒ K p. Thus, if every instance of KC were valid in M, wewould have K K p ⇒ K p valid in M. It is easy to check that (M, w 1) |(cid:8) K K p, since K p ∈ A(w 1) and (M, w 2) |(cid:8) K p, but(M, w 1) |(cid:8) ¬K p, because p /∈ A(w 1). Therefore the K K p ⇒ K p instance of KC is not in fact valid in M.Let AXDC = {Prop, MP, DC} and let AXKC = {Prop, MP, KC}.Theorem 3.6.(a) AXDC is a sound and complete axiomatization of LK with respect to KD45 awareness structures;(b) AXKC is a sound and complete axiomatization of LK with respect to KD45 impossible-worlds structures;(c) AXVer is a sound and complete axiomatization of LK with respect to S5 awareness structures and S5 impossible-worlds structures.Proof. (a) We first prove soundness. Consider axiom DC. Suppose that AXVer (cid:12) ¬(ϕ1 ∧ · · · ∧ ϕn). Let M = (W , WKD45 awareness structure. For each world wof axiom Ver holds at (M, wshows that, if AXVer (cid:12) ψ , then (M, wmust have (M, w) |(cid:8) ¬(K ϕ1 ∧ · · · ∧ K ϕn).(cid:6), π , A) be a, it easily follows from Proposition 3.3 (taking w = w) that each instance(cid:6)), as does each instance of Prop. An easy argument by induction on the length of proof then(cid:6)) |(cid:8) ¬(ϕ1 ∧ · · · ∧ ϕn). It follows that, for each w ∈ W , we(cid:6)) |(cid:8) ψ . In particular, (M, w(cid:6) ∈ W(cid:6)(cid:6)For completeness, it suffices to show that, given an AXDC -consistent formula ϕ, there exists a KD45 awareness structureM and world w such that (M, w) |(cid:8) ϕ. So suppose that ϕ is AXDC -consistent. Let G be a maximal AXDC -consistent set226J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235(cid:6)containing ϕ. Let F = {ψ | K ψ ∈ G}. We claim that F is AXVer-consistent. If not, then there exist ϕ1, . . . , ϕn ∈ G such thatAXVer (cid:12) ¬(ϕ1 ∧ · · · ∧ ϕn). But then by axiom DC, we have that AXDC (cid:12) ¬(K ϕ1 ∧ · · · ∧ K ϕn), contradicting the fact that G isAXDC -consistent. Thus, F is consistent with AXVer. Let Fbe a maximal AXVer-consistent set extending F . Then it is easy tocheck that Fis a propositionally consistent downward-closed set of formulas that contains F . Thus, by Theorem 3.5, there(cid:6)}, π , A) such that (M, w) |(cid:8) K ψ for all ψ ∈ F . We can assume withoutis a KD45 awareness structure M = ({w, wloss of generality that w (cid:10)= wand that π (w) makes all the primitive propositions in F true. (Note that this would not bethe case if we were dealing with S5 awareness structures.) An easy induction on the structure of formulas then shows that(M, w) |(cid:8) ψ for all ψ ∈ G. In particular, (M, w) |(cid:8) ϕ.(cid:6)}, {w(cid:6)(cid:6)(b) For soundness, essentially the same argument as in part (a) shows that axiom DC is sound in KD45 impossible-worlds structures. A similar argument also shows the soundness of KC with respect to KD45 impossible-worlds structures.(cid:6), π , C) is an impossible-worlds structure, w ∈ W , AXVer (cid:12) (ϕ1 ∧ · · · ∧ ϕn) ⇒ (K ψ1 ∨ · · · ∨ K ψm),For suppose that M = (W , Wand (M, w) |(cid:8) K ϕ1 ∧ · · · ∧ K ϕn. Thus, (M, wis a(cid:6) (cid:10)= ∅, there must bemodel of AXVer, if wsome world w, so(M, w) |(cid:8) K ψ j . It follows that (M, w) |(cid:8) K ψ1 ∨ · · · ∨ K ψm, as desired.(cid:6)(cid:6) ∈ W(cid:6)) |(cid:8) K ψ1 ∨ · · · ∨ K ψm. Moreover, since W ∩ W. It follows that, for some j ∈ {1, . . . , m}, (M, w. But since each world in W ∩ W(cid:6)(cid:6)) |(cid:8) ϕ1 ∧ · · · ∧ ϕn for all w(cid:6)) |(cid:8) K ψ j . Thus, (M, w(cid:6)(cid:6)) |(cid:8) ψ j for all w, we must have (M, w(cid:6) ∈ W ∩ W(cid:6)(cid:6) ∈ W ∩ W(cid:6)(cid:6) ∈ W(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)is consistent with AXVer. Again, let FFor completeness, we use much the same argument as in part (a). Suppose that ϕ is AXKC -consistent. Let G be a maximalAXKC -consistent set containing ϕ. Let F = {ψ | K ψ ∈ G}, and let Gis AXVer-consistent. If not, then there exist K ϕ1, . . . , K ϕn, K ψ1, . . . , K ψm ∈ G such that AXVer (cid:12) (ϕ1 ∧· · ·∧ϕn) ⇒ (K ψ1 ∨· · ·∨ K ψm). Byaxiom KC, we have that AXKC (cid:12) (K ϕ1 ∧· · ·∧ K ϕn) ⇒ (K ψ1 ∨· · ·∨ K ψm), contradicting the fact that G is AXKC -consistent. Thus,(cid:6)G. Then it is easy to check that Fis a propositionally consistent downward-closed set of formulas that contains F ; moreover the construction guarantees that(cid:6)(cid:6)}, π , C)(cid:6)}, {wF is k-compatible with Fsuch that (M, w) |(cid:8) K ψ for all ψ ∈ F . We can assume without loss of generality that w (cid:10)= wand that π (w) makes all theprimitive propositions in F true. An easy induction on the structure of formulas then shows that (M, w) |(cid:8) ψ for all ψ ∈ G.In particular, (M, w) |(cid:8) ϕ.. Thus, by Theorem 3.5, there is a KD45 impossible-worlds structure M = ({w, w(cid:6) = F ∪ {¬ψ | ¬K ψ ∈ G}. We again claim that Gbe a maximal AXVer-consistent set extending G(cid:6), w(cid:6)(cid:6)(cid:6)(cid:6)(cid:6)(c) For soundness, as we have already observed, the soundness of Ver in S5 awareness and impossible-worlds structuresfollows easily from Propositions 3.3 and 3.4.For completeness, let AX = {Prop, MP, Ver}. Suppose that ϕ is consistent with AX. Extend ϕ to a maximally AX-consistentset F of formulas. It suffices to show that F is satisfiable in an S5 awareness structure and in an S5 impossible-worldsstructure. In the case of awareness structures, consider the structure M = ({w}, {w}, π , A), where π (w)(p) = true iff p ∈ Fand A(w) = {ψ | K ψ ∈ F }. We now show by induction on the structure of formulas that (M, w) |(cid:8) ψ iff ψ ∈ F . If ψ is aprimitive proposition, then this is immediate from the definition of π . If ψ has the form ¬ψ (cid:6), then the result is immediatefrom the induction hypothesis. If ψ has the form ψ1 ∧ ψ2, this is immediate from the observation that, since F is a maximalAX-consistent set and propositional reasoning is sound in AX that ψ1 ∧ ψ2 ∈ F iff ψ1 ∈ F and ψ2 ∈ F . If ψ has the formK ψ (cid:6). Forthe converse, if (M, w) |(cid:8) K ψ (cid:6), suppose, by way of contradiction, that K ψ (cid:6) /∈ F . Then, by construction, ψ (cid:6) /∈ A(w). Thus,(M, w) |(cid:8) ¬K ψ (cid:6), note that if K ψ (cid:6) ∈ F then ψ (cid:6) ∈ F (since Ver ∈ F ). By the induction hypothesis, (M, w) |(cid:8) ψ (cid:6). Thus, (M, w) |(cid:8) K ψ (cid:6)To show that F is satisfiable in an S5 impossible-worlds structure, consider the structure M = ({w}, {w, w(cid:6)}, π , C), where(cid:6)) is the same set of formulas as A(w) in the argument for S5π (w)(p) = true iff p ∈ F and C(wawareness structures. An almost identical argument as in the case of S5 awareness structures now shows that (M, w) |(cid:8) ψiff ψ ∈ F . We leave details to the reader. (cid:2)(cid:6)) = {ψ | K ψ ∈ F }. Thus, C(w, a contradiction.Note that in all cases of Theorem 3.6, we proved completeness by constructing, for a given consistent formula, a satisfyingstructure with few worlds. This indicates that awareness structures and impossible-worlds structures are quite flexible—thelack of restrictions on both awareness sets and truth assignments to impossible worlds lets us easily capture states ofknowledge with few worlds.Corollary 3.7. The satisfiability problem for the language LK with respect to KD45 awareness structures (resp., KD45 impossible-worldsstructures, S5 awareness structures, S5 impossible-worlds structures) is NP-complete.Proof. NP-hardness follows immediately from the observation that LK contains propositional logic. The fact that the satis-fiability problem with respect to each of these classes of structures is in NP follows from the construction of Theorem 3.6,which shows that if a formula ϕ is satisfiable with respect to KD45 awareness structures (resp., KD45 impossible-worldsstructures, S5 awareness structures, S5 impossible-worlds structures), then it is consistent with respect to AXDC (resp. AXKC ,AXVer), which in turn implies that it is satisfiable in a KD45 awareness structure (resp., KD45 impossible-worlds structure, S5(cid:6), . . .) with two (resp., three, one) world(s). Without loss ofawareness structure, S5 impossible-worlds structure) M = (W , Wgenerality, we can also assume that, in the case of awareness structures, at each world w ∈ W , A(w) is a subset of Sub(ϕ),the set of subformulas of ϕ, and π (w)(p) = true only if p is a subformula of ϕ; similarly, in the case of impossible-worlds(cid:6)) is a subset of the subformulas of ϕ. (If this is not true(cid:6)structures, we can assume that for each impossible world win M, then we can easily modify M so that this is true without affecting the truth of ϕ or any subformula of ϕ in anyworld.) Thus, we can guess a satisfying structure for ϕ and verify that it satisfies ϕ in time linear in the length of ϕ. (cid:2), C(wJ.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–2352274. Adding probability−While the differences between K45, KD45, and KD45 impossible-worlds structures may appear minor, they turn outto be important when we add probability to the picture. As pointed out by Cozic [2], standard models for reasoning aboutprobability suffer from the same logical omniscience problem as models for knowledge. In the language considered by Fagin,Halpern, and Megiddo [9] (FHM from now on), there are formulas that talk explicitly about probability. A formula such as(cid:3)(Primen) = 1/3 says that the probability that n is prime is 1/3. In the FHM semantics, a probability is put on the set ofworlds that the agent considers possible. The probability of a formula ϕ is then the probability of the set of worlds whereϕ is true. Clearly, if ϕ and ψ are logically equivalent, then (cid:3)(ϕ) = (cid:3)(ψ) will be true. However, the agent may not recognizethat ϕ and ψ are equivalent, and so may not recognize that (cid:3)(ϕ) = (cid:3)(ψ). Problems of logical omniscience with probabilitycan to some extent be reduced to problems of logical omniscience with knowledge in a logic that combines knowledgeand probability [8]. For example, the fact that an agent may not recognize (cid:3)(ϕ) = (cid:3)(ψ) when ϕ and ψ are equivalentjust amounts to saying that if ϕ ⇔ ψ is valid, then we do not necessarily want K ((cid:3)(ϕ) = (cid:3)(ψ)) to hold. However, addingknowledge and awareness does not prevent (cid:3)(ϕ) = (cid:3)(ψ) from holding. This is not really a problem if we interpret (cid:3)(ϕ) asthe objective probability of ϕ; if ϕ and ψ are equivalent, it is an objective fact about the world that their probabilities areequal, so (cid:3)(ϕ) = (cid:3)(ψ) should hold. On the other hand, if (cid:3)(ϕ) represents the agent’s subjective view of the probability of ϕ,then we do not want to require (cid:3)(ϕ) = (cid:3)(ψ) to hold. This cannot be captured in all approaches.To make this precise, we first clarify the logic we have in mind. Let LK ,QU be LK extended with linear inequality formulasinvolving probability (called likelihood formulas), in the style of FHM. A likelihood formula is of the form a1(cid:3)(ϕ1) + · · · +an(cid:3)(ϕn) (cid:2) c, where a1, . . . , an and c are integers. (For ease of exposition, we restrict the ϕ1, . . . , ϕn appearing in likelihoodformulas to be propositional, that is, with no occurrences of (cid:3) and K ; however, the techniques presented here can beextended to deal with formulas that allow arbitrary nesting of (cid:3) and K .) We give semantics to these formulas by extendingKripke structures with a probability distribution over the worlds that the agent considers possible. A probabilistic KD45 (resp.,(cid:6), π ) is KD45 (resp., S5) Kripke structure, and μ is a probabilityS5) Kripke structure is a tuple (W , W. To interpret likelihood formulas, we first define [[ϕ]]M = {w ∈ W | π (w)(ϕ) = true}, for a propositionaldistribution over Wformula ϕ. We then extend the semantics of LK with the following rule for interpreting likelihood formulas:(cid:6), π , μ), where (W , W(cid:6)(M, w) |(cid:8) a1(cid:3)(ϕ1) + · · · + an(cid:3)(ϕn) (cid:2) ciff a1μ(cid:2)[[ϕ1]]M ∩ W(cid:3)(cid:6)+ · · · + anμ(cid:2)[[ϕn]]M ∩ W(cid:3)(cid:6)(cid:2) c.Note that the truth of a likelihood formula at a world does not depend on that world; if a likelihood formula is true at aworld of a structure M, then it is true at every world of M.FHM give an axiomatization for likelihood formulas in probabilistic structures. Aside from propositional reasoning axioms,one axiom captures reasoning with linear inequalities. A basic inequality formula is a formula of the form a1x1 + · · · + akxk +ak+1 (cid:3) b1 y1 + · · · + bm ym + bm+1, where x1, . . . , xk, y1, . . . , ym are (not necessarily distinct) variables. A linear inequalityformula is a Boolean combination of basic linear inequality formulas. A linear inequality formula is valid if the resultinginequality holds under every possible assignment of real numbers to variables. For example, the formula (2x + 3 y (cid:3) 5z) ∧(x − y (cid:3) 12z) ⇒ (3x + 2 y (cid:3) 17z) is a valid linear inequality formula. To get an instance of Ineq, we replace each variable xithat occurs in a valid formula about linear inequalities by a likelihood term of the form (cid:3)(ψ) (naturally, each occurrenceof the variable xi must be replaced by the same primitive expectation term (cid:3)(ψ)). (We can replace Ineq by a sound andcomplete axiomatization for Boolean combinations of linear inequalities; one such axiomatization is given in FHM.)The other axioms of FHM are specific to probabilistic reasoning, and capture the defining properties of probability distri-butions:(cid:3)(true) = 1,(cid:3)(¬ϕ) = 1 − (cid:3)(ϕ),(cid:3)(ϕ ∧ ψ) + (cid:3)(ϕ ∧ ¬ψ) = (cid:3)(ϕ).It is straightforward to extend all the approaches in Section 2 to the probabilistic setting. In this section, we only con-sider probabilistic awareness structures and probabilistic impossible-worlds structures, because the interpretation of bothalgorithmic knowledge and knowledge in syntactic structures does not depend on the set of worlds or any probabilitydistribution over the set of worlds.A KD45 (resp., S5) probabilistic awareness structure is a tuple (W , W(cid:6), π , A) is a KD45 (resp., S5)−(cid:6)awareness structure and μ is a probability distribution over the worlds in W(resp., KD45, S5) probabilistic(cid:6), π , C) is a KD45(cid:6), π , C, μ) where (W , Wimpossible-worlds structure is a tuple (W , W(resp., KD45, S5) impossible-worlds(cid:6)structure and μ is a probability distribution over the worlds in W. Since the set of worlds that are assigned probabilityimpossible-must be nonempty, when dealing with probability, we must restrict to KD45 awareness structures and KD45worlds structures, extended with a probability distribution over the set of worlds the agent considers possible. As we nowshow, adding probability to the language allows finer distinctions between awareness structures and impossible-worldsstructures.(cid:6), π , A, μ) where (W , W. Similarly, a KD45−In probabilistic awareness structures, the axioms of probability described by FHM are all valid. For example, (cid:3)(ϕ) = (cid:3)(ψ)is valid in probabilistic awareness structures if ϕ and ψ are equivalent formulas. Using arguments similar to those in−228J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235Theorem 3.5, we can show that ¬K ¬(cid:3)(ϕ) = (cid:3)(ψ) is valid in probabilistic awareness structures. Similarly, since (cid:3)(ϕ) +(cid:3)(¬ϕ) = 1 is valid in probability structures, ¬K (¬((cid:3)(ϕ) + (cid:3)(¬ϕ) = 1)) is valid in probabilistic awareness structures.We can characterize properties of knowledge and likelihood in probabilistic awareness structures axiomatically. Let Probdenote a substitution instance of a valid formula in probabilistic logic (using the FHM axiomatization). By the observationabove, Prob is sound in probabilistic awareness structures. Our reasoning has to take this into account. There is also anaxiom KL that connects knowledge and likelihood:K ϕ ⇒ (cid:3)(ϕ) > 0.(KL)Ver denote the axiom system consisting of {Prop, MP, Prob, KL, Ver}. Let DC P be the following strengthening of DC,Let AXPsomewhat in the spirit of KC:(K ϕ1 ∧ · · · ∧ K ϕn) ⇒ (ψ1 ∨ · · · ∨ ψm)if AXPVer(cid:12) ϕ1 ∧ · · · ∧ ϕn ⇒ (ψ1 ∨ · · · ∨ ψm)and ψ1, . . . , ψm are likelihood formulas.(DC P )Finally, even though Ver is not sound in KD45 probabilistic awareness structures, a weaker version, restricted to likelihoodformulas, is sound, since there is a single probability distribution in probabilistic awareness structures. Let WVer be thefollowing axiom:K ϕ ⇒ ϕ if ϕ is a likelihood formula.(WVer)= {Prop, MP, Prob, DC P , WVer, KL} be the axiom system obtained by replacing DC in AXDC by DC P and addingLet AXPDCProb, WVer, and KL.Theorem 4.1.(a) AXP(b) AXPDC is a sound and complete axiomatization of LK ,QU with respect to KD45 probabilistic awareness structures.Ver is a sound and complete axiomatization of LK ,QU with respect to S5 probabilistic awareness structures.(cid:6)(cid:6) ∈ W(cid:6)) = μ(WProof. (a) We first prove soundness. We have already argued that Prob is sound in KD45 probabilistic awareness structures.(cid:6), π , A, μ) be a KD45 probabilistic awareness structure, and let w beIt is easy to see that KL is sound: let M = (W , W(cid:6) ∈ W, and therefore, μ([[ϕ]]M ∩a world in W such that (M, w) |(cid:8) K ϕ. This means that ϕ is true at every world w(cid:6), π , A, μ) be a KD45 probabilistic(cid:6)) > 0, that is, (M, w) |(cid:8) (cid:3)(ϕ) > 0. Similarly, WVer is sound: let M = (W , WWawareness structure, and let w be a world in W such that (M, w) |(cid:8) K ϕ, with ϕ a likelihood formula. This means that ϕ is, and because ϕ is a likelihood formula, the truth of ϕ does not depend on the world. Thus, iftrue at every world wϕ is true at some world, it is true at every world; in particular, it is true at w, so that (M, w) |(cid:8) ϕ, as required. Finally, we(cid:6), π , A, μ)show soundness of DC P , using an argument similar to that in the proof of Theorem 3.6. Suppose that M = (W , W(cid:12) (ϕ1 ∧ · · · ∧ ϕn) ⇒ (ψ1 ∨ · · · ∨ ψm), for likelihood formulasis a KD45 probabilistic awareness structure, w ∈ W , AXPVerψ1, . . . , ψm, and (M, w) |(cid:8) K ϕ1 ∧ · · · ∧ K ϕn. Thus, (M, wis a(cid:6)model of AXP. For some(cid:6)) |(cid:8) ψ j . Because ψ j is a likelihood formula, and therefore its truth does not dependj ∈ {1, . . . , m}, we must have (M, won the world, if ψ j is true at some world, then ψ j is true at every world. In particular, (M, w) |(cid:8) ψ j , and it follows that(M, w) |(cid:8) ψ1 ∨ · · · ∨ ψm, as desired.. But since each world in W(cid:6)(cid:6)) |(cid:8) ψ1 ∨ · · · ∨ ψm. Since W(cid:6)(cid:6)) |(cid:8) ϕ1 ∧ · · · ∧ ϕn for all w(cid:6)(cid:6) ∈ W(cid:6)(cid:6) (cid:10)= ∅, let w, we must have (M, wbe an element of WVer, if w(cid:6) ∈ W(cid:6)(cid:6)(cid:6)DC . Let F be a maximal AXPCompleteness follows from combining techniques from the FHM completeness proof with those of Theorem 3.6. Webriefly sketch the main ideas here. Define SubP (ϕ) to be the least set containing ϕ, closed under subformulas, and containing(cid:3)(ψ) > 0 if it contains a propositional formula ψ . It is easy to see that |SubP (ϕ)| (cid:3) 2|ϕ|. Suppose that ϕ is consistent withAXPDC -consistent subset of SubP (ϕ) that includes ϕ. Let S consist of all truth assignments toprimitive propositions. Using techniques of FHM, we can show that there must be a probability measure μ on S that makesall the likelihood formulas in F true. We remark for future reference that the FHM proof shows that we can take the set oftruth assignments which get positive probability to be polynomial in the size of |ϕ|, and we can assume that the probabilityis rational, with a denominator whose size is polynomial in |ϕ|.DC -consistent. Hence there is a maximal AXPthat H must be AXPconstruct a KD45 awareness structure ({w} ∪ Wtruth assignment v such that μ(v) > 0 and a world wμ(cid:6)(w v ) = μ(v). Define π so that π (w v ) = v, π (w)(p) = true iff p ∈ F and π (wso that A(w v ) = ∅, A(wshow that, for each formula ψ ∈ SubP (ϕ) we have that (M, wLet H = {ψ | K ψ ∈ F } ∪ {ψ | ψ ∈ F , ψ is a likelihood formula}. Arguments almost identical to those in Theorem 3.6 showof SubP (ϕ) that contains H . We now(cid:6)corresponding to each(cid:6)) = 0 andso that μ(cid:6)(w. Finally, define A(cid:6)} and A(w) = {ψ | K ψ ∈ F }. Now the same ideas as in the proof of Theorem 3.6and (M, w) |(cid:8) ψ iff ψ ∈ F . Thus, (M, w) |(cid:8) ϕ.(b) The soundness of Ver in S5 probabilistic awareness structures follows easily by induction on the structure of ϕ inK ϕ, using the fact that WVer—the special case of Ver when ϕ is a likelihood formula—is sound in probabilistic awarenessstructures, and the argument for the soundness of Ver in S5 awareness structures.(cid:6), A, μ(cid:6)) as follows. There is a world w v in W(cid:6)(cid:6)on W(cid:6))(p) = true iff p ∈ FDC -consistent subset F(cid:6)) = {ψ | K ψ ∈ F(cid:6)) |(cid:8) ψ iff ψ ∈ F; we define μ(cid:6)corresponding to F(cid:6), W(cid:6)(cid:6)(cid:6)(cid:6)J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235229The proof of completeness is similar in spirit to the proof of completeness in part (a); the modifications required areexactly those needed to prove Theorem 3.6(c). We leave details to the reader. (cid:2)Things change significantly when we move to probabilistic impossible-worlds structures. In particular, Prob is no longersound. For example, even if ϕ ⇔ ψ is valid, (cid:3)(ϕ) = (cid:3)(ψ) is not valid, because we can have an impossible possible world withpositive probability where both ϕ and ¬ψ are true. Similarly, (cid:3)(ϕ) + (cid:3)(¬ϕ) = 1 is not valid. Indeed, both (cid:3)(ϕ) + (cid:3)(¬ϕ) >1 and (cid:3)(ϕ) + (cid:3)(¬ϕ) < 1 are satisfiable in impossible-worlds structures: the former requires that there be an impossiblepossible world that gets positive probability where both ϕ and ¬ϕ are true, while the latter requires an impossible possibleworld with positive probability where neither is true. As a consequence, it is not hard to show that both K ¬((cid:3)(ϕ) = (cid:3)(ψ))and K (¬((cid:3)(ϕ) + (cid:3)(¬ϕ) = 1)) are satisfiable in such impossible-worlds structures.5 In fact, the only constraint on probabilityin probabilistic impossible-worlds structures is that it must be between 0 and 1. This constraint is expressed by the followingaxiom Bound:(cid:3)(ϕ) (cid:2) 0 ∧ (cid:3)(ϕ) (cid:3) 1.(Bound)We can characterize properties of knowledge and likelihood in probabilistic impossible-worlds structures axiomati-imp as being the core of probabilistic reasoning in= {Prop, MP, Ineq, Bound, KL, WVer}. We can think of AXBcally. Let AXBimpimpossible-worlds structures.Let AXBVer denote the axiom system consisting of {Prop, MP, Ineq, Bound, Ver, KL}. Let KC P denote the following extensionof KC:(K ϕ1 ∧ · · · ∧ K ϕn) ⇒ (ψ1 ∨ · · · ∨ ψm)(cid:12) ϕ1 ∧ · · · ∧ ϕn ⇒ (ψ1 ∨ · · · ∨ ψm)if AXPVerand ψ j is either a likelihood formula or of the form K ψ (cid:6), for j = 1, . . . , m.(KC P )Here again, DC P is a special case of KC P . Let AXBKCby KC P and adding Ineq, Bound, WVer and KL.= {Prop, MP, Ineq, Bound, KC P , WVer, KL} obtained by replacing KC in AXKCTheorem 4.2.(a) AXB(b) AXB(c) AXBimp is a sound and complete axiomatization of LK ,QU with respect to KD45KC is a sound and complete axiomatization of LK ,QU with respect to KD45 probabilistic impossible-worlds structures.Ver is a sound and complete axiomatization of LK ,QU with respect to S5 probabilistic impossible-worlds structures.probabilistic impossible-worlds structures.−Proof. (a) We first prove soundness. The argument is similar to the argument for soundness in Theorem 4.1. That KLand WVer are sound in probabilistic impossible-worlds structures follows from the same argument as in Theorem 4.1.To show that Bound is sound, note that for any probabilistic impossible-worlds structure M, [[ϕ]]M ∩ W, so that0 (cid:3) μ([[ϕ]]M ) (cid:3) 1. Because this is independent of the actual world, (M, w) |(cid:8) (cid:3)(ϕ) (cid:2) 0 ∧ (cid:3)(ϕ) (cid:3) 1 holds.(cid:6) ⊆ W(cid:6)For completeness, given a formula ϕ consistent with AXimp, let F be a maximal AXimp-consistent subset of SubP (ϕ)that includes ϕ. Consider the basic likelihood formulas in F . From these, we can get a system of linear inequalities byreplacing each term (cid:3)(ψ) by a variable xψ . We add an inequality 0 (cid:3) xψ (cid:3) 1 for each formula ψ ∈ SubP (ϕ). Using thearguments of FHM, we can show that this set of inequalities must be satisfiable (otherwise F would not be AXimp-∗(cid:3) · · · (cid:3) xψn . Let nconsistent). Take a solution. Without loss of generality, we have subformulas listed so that xψ1∗ = n + 1. Consider a probabilistic impossible-worlds structurebe the least m such that xψm({w}, {w 1, . . . , wn+1, w}, π , C, μ), where we define π , C and μ as follows:= 1; if xψn < 1, then let n(cid:3) xψ2• π (w)(p) = true iff p ∈ F ;• μ(w 1) = xψ1 , μ(w j) = xψ j• C(w j) = {ψ j, . . . , ψn} for j = 1, . . . , n• C(w j) = C(wn∗ ) if j = n;∗ + 1, . . . , n + 1.∗− xψ j−1 for j = 2, . . . , n, and μ(wn+1) = 1 − μ(wn);We leave it to the reader to show that (M, w) |(cid:8) ϕ.(b) We show soundness of KC P with respect to KD45 probabilistic impossible-worlds structures. For suppose that M =(cid:6), π , C, π ) is a KD45 probabilistic impossible-worlds structure, w ∈ W , AXVer (cid:12) (ϕ1 ∧· · ·∧ϕn) ⇒ (ψ1 ∨· · ·∨ψm), where(cid:6)(cid:6)) |(cid:8) ϕ1 ∧ · · · ∧ ϕn for(cid:6)) |(cid:8) ψ1 ∨ · · · ∨ ψm.(W , Weach ψ j either a likelihood formula or of the form K ψ (cid:6). But since each world in W ∩ Wall w, and (M, w) |(cid:8) K ϕ1 ∧ · · · ∧ K ϕn. Thus, (M, w, we must have (M, wis a model of AXP(cid:6) ∈ W ∩ W(cid:6)(cid:6) ∈ W(cid:6)(cid:6)(cid:6)Ver, if w5 We remark that Cozic [2], who considers the logical omniscience problem in the context of probabilistic reasoning, makes somewhat similar points.Although he does not formalize things quite the way we do, he observes that, in his setting, impossible-worlds structures seem more expressive thanawareness structures.230J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235(cid:6) (cid:10)= ∅, there must be some world w(cid:6)) |(cid:8) ψ j .Moreover, since W ∩ WThere are two cases. If ψ j is a likelihood formula, then its truth does not depend on the world, so that if ψ j is true at someworld, then ψ j is true at every world. In particular, (M, w) |(cid:8) ψ j , and it follows that (M, w) |(cid:8) ψ1 ∨ · · · ∨ ψm, as desired.If ψ j is a formula of the form K ψ (cid:6), that is, (M, w) |(cid:8) ψ j . It followsfor all wthat (M, w) |(cid:8) ψ1 ∨ · · · ∨ ψm, as desired.. It follows that, for some j ∈ {1, . . . , m}, (M, w, so (M, w) |(cid:8) K ψ (cid:6)(cid:6) ∈ W ∩ W, then (M, w(cid:6)(cid:6)) |(cid:8) ψ (cid:6)(cid:6)(cid:6) ∈ W(cid:6)(cid:6)The completeness argument is similar in spirit to that of part (a) and left to the reader.(c) For soundness, as in the proof of Theorem 4.1, the soundness of Ver in S5 probabilistic impossible-worlds structuresfollows by induction on the structure of ϕ in K ϕ.The completeness argument is similar in spirit to that of part (a) and left to the reader. (cid:2)Observe that Theorem 4.2 is true even though probabilities are standard in impossible worlds: the probabilities of worldsstill sum to 1. It is just the truth assignment to formulas that behaves in a nonstandard way in impossible worlds. Intuitively,while the awareness approach is modeling certain consequences of resource-boundedness in the context of knowledge, itdoes not do so for probability. On the other hand, the impossible-worlds approach seems to extend more naturally toaccommodate the consequences of resource-boundedness in probabilistic reasoning; see Section 5 for more discussion ofthis issue.Corollary 4.3. The satisfiability problem for the language LK ,QU with respect to KD45 probabilistic awareness structures (resp., S5probabilistic awareness structures, KD45probabilistic impossible-worlds structures, KD45 probabilistic impossible-worlds structures,S5 probabilistic impossible-worlds structures) is NP-complete.−−Ver, AX Bimp, AX BProof. Again, NP-hardness follows immediately from the observation that LK ,QU contains propositional logic. The fact thatthe satisfiability problem with respect to each of these classes of structures is in NP follows from the constructions ofTheorems 4.1 and 4.2, which show that if a formula ϕ is satisfiable with respect to KD45 probabilistic awareness struc-tures (resp., S5 probabilistic awareness structures, KD45probabilistic impossible-worlds structures, KD45 probabilisticimpossible-worlds structures, S5 probabilistic impossible-worlds structures), then it is consistent with respect to AXPDC (resp.,AXPKC , AX BVer) which in turn implies that it is satisfiable in a KD45 probabilistic awareness structure (resp., S5probabilistic awareness structure, KD45probabilistic impossible-worlds structure, KD45 probabilistic impossible-worlds(cid:6), . . .) with a small number of worlds—polynomial in thestructure, S5 probabilistic impossible-worlds structure) M = (W , Wlength of ϕ in each case. Just as in the proof of Corollary 3.7, without loss of generality, we can assume that, in the caseof probabilistic awareness structures, at each world w ∈ W , A(w) is a subset of Sub(ϕ), the set of subformulas of ϕ, andπ (w)(p) = true only if p is a subformula of ϕ; similarly, in the case of probabilistic impossible-worlds structures, we can(cid:6)) is a subset of the subformulas of ϕ. Finally, using the arguments of FHM,assume that for each impossible world wwe can argue without loss of generality that the probability distributions μ are described in size polynomial in the lengthof ϕ. (The probability distributions in all structures can be taken to assign small—polynomial-size—rational probabilities toevery world, where the size of a rational number is the sum of the sizes of the numerator and denominator when they arerelatively prime.) Thus, we can guess a satisfying structure for ϕ and verify that it satisfies ϕ in time polynomial in thelength of ϕ. (cid:2), C(w−(cid:6)5. Pragmatic issuesEven in settings where the four approaches are equi-expressive, they model lack of logical omniscience quite differently.We thus have to deal with different issues when attempting to use one of them in practice. By “in practice”, we meanattempting to use one of the models above to capture a particular scenario about which one wants to reason—as opposed,say, to capturing a scenario using axioms in the logic, and reasoning exclusively using the proof rules of the logic. Issues likethe following can arise: If we are using a syntactic structure to represent a given situation, we need to explain where thefunction C is coming from; with an awareness structure, we must explain where the awareness function is coming from;with an algorithmic knowledge structure, we must explain where the algorithm is coming from; and with an impossible-worlds structure, we must explain what the impossible worlds are.There seem to be three quite distinct intuitions underlying the lack of logical omniscience. As we now discuss, theseintuitions can guide the choice of approach, and match closely the solutions described above. We discuss, for each intuition,the extent to which each of the approaches to dealing with logical omniscience can capture that intuition. While thediscussion in this section is somewhat informal, we believe that these observations will prove important when actuallytrying to decide how to model lack of logical omniscience in practice.5.1. Lack of awarenessThe first intuition is lack of awareness of some primitive notions: for example, an agent reasoning in 2002 about possibleoutcomes of an attack on Iraq may not have even contemplated outcomes like suicide bombers. If “suicide bombers causemany deaths” is a primitive proposition in the language of a more sophisticated modeler, then the agent would not be awareJ.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235231of this proposition, and hence could not even consider it possible that suicide bombers would cause many deaths. (Suchreasoning becomes more interesting if there is more than one agent, and they are aware of different primitive propositions.)This can be modeled reasonably well using an awareness structure where the awareness function is generated by primitivepropositions. We assume that the agent is unaware of certain primitive propositions, and is unaware of exactly those formu-las that contain a primitive proposition of which the agent is unaware. This intuition is quite prevalent in the economicscommunity, and all the standard approaches to modeling lack of logical omniscience in the economics literature [22,23,3,16] can essentially be understood in terms of awareness structures where awareness is generated by primitive propositions[11,14].Of course, how we choose these primitive propositions is critical to the modeling process. What counts as a primitiveproposition is ultimately in the eye of the beholder, and different modelers may well choose different primitive propositionsto capture a particular scenario. However, for many scenarios, the choice of primitive propositions is usually clear anduncontroversial, as witnessed by the popularity of the approach in the economics literature.If awareness is generated by primitive propositions, constructing an awareness structure corresponding to a particularsituation is no more (or less!) complicated that constructing a Kripke structure to capture knowledge without awareness.Determining the awareness sets for notions of awareness that are not generated by primitive propositions may be morecomplicated. It is also worth stressing that an awareness structure must be understood as the modeler’s view of the situa-tion. For example, if awareness is generated by primitive propositions and agent 1 is not aware of a primitive proposition p,then agent 1 cannot contemplate a world where p is true (or false); in the model from agent 1’s point of view, p does notexist.(cid:6)How do the other approaches fare in modeling lack of awareness? To construct a syntactic structure, we need to knowall sentences that an agent knows before constructing the model. This may or may not be reasonable. But it does not helpin discovering properties of knowledge in a given situation. As observed in [10], the syntactic approach is really only arepresentation of knowledge. Algorithmic knowledge can deal with lack of awareness reasonably well, provided that thereis an algorithm Aa for determining what the agent is aware of and an algorithm Ak for determining whether a formula, the set of worlds that the agent considers possible. If so, given a query ϕ, the algorithmicis true in every world in Wapproach would simply invoke Aa to check whether the agent is aware of ϕ; if so, then the agent invokes Ak. For example,if awareness is generated by primitive propositions, then Aa is the algorithm that, given query ϕ, checks whether all theprimitive propositions in ϕ are ones the agent is aware of; and we can take Ak to be the algorithm that does modelchecking to see if ϕ is true in every world of W; see [10].) In impossible-worlds structures, we can interpret lack of awareness of ϕ as meaning that neither ϕ nor ¬ϕ is true at all worlds theagent considers possible. Thus, if there is any nontrivial lack of awareness, then all the worlds that the agent considerspossible will be impossible worlds. However, these impossible worlds have a great deal of structure: we can require thatfor all the formulas ϕ that the agent is aware of, exactly one of ϕ and ¬ϕ is true at each world the agent considerspossible. As we observed earlier, an awareness structure must be viewed as the modeler’s view of the situation. Arguably,the impossible-worlds structure better captures the agent’s view.. (This can be done in time polynomial in W(cid:6)(cid:6)5.2. Lack of computational abilityThe second intuition is computational: an agent simply might not have the resources to compute the required answer.But then the question is how to model this lack of computational ability. There are two cases of interest, depending onwhether we have an explicit algorithm in mind. If we have an explicit algorithm, then it is relatively straightforward. Forexample, Konolige [18] uses a syntactic approach and gives an explicit characterization of C by taking it to be the set offormulas that can be derived from a fixed initial set of formulas by using a sound but possibly incomplete set of inferencerules. Note that Konolige’s approach makes syntactic knowledge an instance of algorithmic knowledge. (See also Pucella [27]for more details on knowledge algorithms given by inference rules.)Algorithmic knowledge can be viewed as a generalization of Konolige’s approach in this setting, since it allows for thepossibility that the algorithm used by the agent to compute what he knows may not be easily expressible as a set ofinference rules over formulas. For example, Berman, Garay, and Perry [1] implicitly use a particular form of algorithmicknowledge in their analysis of Byzantine agreement (this is the problem of getting all nonfaulty processes in a systemto coordinate, despite the presence of failures). Roughly speaking, they allow agents to perform limited tests based onthe information they have; agents know only what follows from these limited tests. But these tests are not characterizedaxiomatically. As shown by Halpern and Pucella [13], algorithmic knowledge is also a natural way to capture adversaries insecurity protocols.Example 5.1. Security protocols are generally analyzed in the presence of an adversary that has certain capabilities fordecoding the messages he intercepts. There are of course restrictions on the capabilities of a reasonable adversary. Forinstance, the adversary may not explicitly know that he has a given message if that message is encrypted using a keythat the adversary does not know. To capture these restrictions, Dolev and Yao [4] gave a now-standard description ofthe capabilities of adversaries. Roughly speaking, a Dolev–Yao adversary can decompose messages, or decipher them if heknows the right keys, but cannot otherwise “crack” encrypted messages. The adversary can also construct new messages byconcatenating known messages, or encrypting them with a known encryption key.232J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235Algorithmic knowledge is a natural way to capture the knowledge of a Dolev–Yao adversary [13]. We can use a knowl-edge algorithm ADY to compute whether the adversary can extract a message m from a set H of messages that he hasintercepted, where the extraction relation H (cid:12)DY m is defined by following inference rules:m ∈ HH (cid:12)DY mH (cid:12)DY {m}k H (cid:12)DY kH (cid:12)DY mH (cid:12)DY m1 · m2H (cid:12)DY m1H (cid:12)DY m1 · m2H (cid:12)DY m2,where m1 · m2 is the concatenation of messages m1 and m2, and {m}k is the encryption of message m with key k.The knowledge algorithm ADY simply implements a search for the derivation of a message m from the messages that theadversary has received and the initial set of keys, using the inference rules above. More precisely, we assume the languagehas formulas has(m), interpreted as “the agent possesses message m”. When queried for a formula has(m), the knowledgealgorithm ADY simply checks if H (cid:12)DY m, where H is the set of messages intercepted by the adversary. Thus, the formulaK (has(m)), which is true if and only if ADY says “Yes” to query has(m), that is, if and only if H (cid:12)DY m, says that the adversarycan extract m from the messages he has intercepted.However, even when our intuition is computational, at times the details of the algorithm do not matter (and, indeed,may not be known to the modeler). In this case, awareness may be more useful than algorithmic knowledge.Example 5.2. Suppose that Alice is trying to reason about whether or not an eavesdropper Eve has managed to decrypt acertain message. The intuition behind Eve’s inability to decrypt is computational, but Alice does not know which algorithmEve is using. An algorithmic knowledge structure is typically appropriate if there are only a few algorithms that Eve mightbe using, and her ability to decrypt depends on the algorithm.6 On the other hand, Alice might have no idea of what Eve’salgorithm is, and might not care. All that matters to her analysis is whether Eve has managed to decrypt. In this case, usinga syntactic structure or an awareness structure seems more appropriate. Suppose that Alice wants to model her uncertaintyregarding whether Eve has decrypted the message. She could then use an awareness structure with some possible worldswhere Eve is aware of the message, and others where she is not, with the appropriate probability on each set. Alice canthen reason about the likelihood that Eve has decrypted the message without worrying about how she decrypted it.What about the impossible-worlds approach? It cannot directly represent an algorithm, of course. However, if there is(cid:6)) = {ϕ |algorithm A that characterizes an agent’s computational process, then we can simply take WA(ϕ) = “Yes”}. Indeed, we can give a general computational interpretation of the impossible-worlds approach. The worlds wsuch that C(w) are precisely those worlds where the algorithm answers “Yes” when asked about ϕ. If neither ϕ nor ¬ϕ is inC(w), that just means that the algorithm was not able to determine whether ϕ was true or false. If the algorithm answers“Yes” to both ϕ and ¬ϕ, then clearly the algorithm is not sound, but it may nevertheless describe how a resource-boundedagent works.(cid:6)} and define C(w(cid:6) = {wThis intuition also suggests how we can model the lack of computational ability illustrated by Example 5.2 using impos-sible worlds. Suppose that we add new primitive propositions of the form cont(m) = c to the language that say that thecontent of a message m is c. Then in a world where Alice cannot decrypt c, neither cont(m) = c nor ¬(cont(m) = c) wouldbe true.5.3. Imperfect understanding of the modelSometimes an agent’s lack of logical omniscience is best thought of as stemming from “mistakes” in constructing themodel (which perhaps are due to lack of computational ability).Example 5.3. Suppose that Alice does not know whether a number n is prime. Although her ignorance regarding n’s pri-mality can be viewed as computationally based—given enough time and energy, she could in principle figure out whethern is prime—she need not be using a specific algorithm to compute her knowledge (at least, not one that she can easilydescribe). Nor can her state of mind be modeled in a natural way using an awareness structure or a syntactic structure.Intuitively, there should at least two worlds she considers possible, one where n is prime, and one where n is not. However,n is either prime or it is not. If n is actually prime, then there cannot be a possible world where n is not prime; similarly, ifn is composite, there cannot be a possible world where n is prime. This problem can be modeled naturally using impossibleworlds. Now there is no problem having a world where n is prime (which is an impossible world if n is actually composite)and a world where n is composite (which is an impossible world if n is actually prime). In this structure, it is also seemsreasonable to assume that Alice knows that she does not know that n is prime (so that the formula ¬K Primen is true evenin the impossible worlds).It is instructive to compare this with the awareness approach. Suppose that n is indeed prime and an external modelerknows this. Then he can describe Alice’s state of mind with one world, where n is prime, but Alice is not aware that n is6 What is required here is an algorithmic knowledge structure with two agents. There will then be different algorithms for Eve associated with differentstates. We omit here the straightforward details of how this can be done; see [12].J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235233prime. Thus, ¬K Primen holds at this one world. But note that this is not because Alice considers it possible that n is notprime; rather, it is because Alice cannot determine whether n is prime using her internal algorithm. If Alice is aware ofthe formula ¬K Primen at this one world, then K ¬K Primen also holds. Again, we should interpret this as saying that Aliceknows that she cannot determine whether n is prime using her internal algorithm.The impossible-worlds approach seems like a natural one in Example 5.3 and many other settings. As we saw, awarenessin this situation does not quite capture what is going on here. Algorithmic knowledge fares somewhat better, but it wouldrequire us to have a specific algorithm in mind; in Example 5.3, this would force us to interpret “knows that a number isprime” as “knows that a number is prime as tested by a particular factorization algorithm”.The impossible-worlds approach can sometimes be difficult to apply, however, because it is not always clear what im-possible worlds to incorporate in a model. While there has been a great deal of discussion (particularly in the philosophyliterature) concerning the “metaphysical status” of impossible worlds (cf. [30]), the pragmatics of generating impossibleworlds has received comparatively little attention. Hintikka [17] argues that Rantala’s [28] urn models are suitable candi-dates for impossible worlds. In decision theory, Lipman [19] uses impossible-worlds structures to represent the preferencesof an agent who may not be able to distinguish logically equivalent outcomes; the impossible worlds are determined bythe preference order. None of these approaches address the problem of generating the impossible worlds even in a simpleexample such as Example 5.3, especially if the worlds have some structure.We view impossible worlds as describing the agent’s subjective view of a situation. The modeler may know that theseimpossible worlds are truly impossible, but the agent does not. In many cases, the intuitive reason that the agent does notrealize that the impossible worlds are in fact impossible is that the agent does not look carefully at the worlds. ConsiderExample 5.3. Let Primen, for various choices of n, be a primitive proposition saying that the number n is prime. Suppose thatthe worlds are models of arithmetic, which include as domain elements the natural numbers with multiplication definedon them. If Primen is interpreted as being true in a world when there do not exist numbers n1 and n2 in that world suchthat n1 × n2 = n, then how does the agent conceive of the impossible worlds? If the agent were to look carefully at a worldwhere Primen holds, he might realize that there are in fact two numbers n1 and n2 such that n1 × n2 = n. But if n is notprime, how do we capture the fact that the agent “mistakenly” constructed a world where there are numbers n1 and n2such that n1 × n2 = n if we also assume that the agent understands basic multiplication?We now sketch a new approach to constructing an impossible-worlds structure that seems appropriate for such exam-ples. The approach is motivated by the observation that the set of worlds in a Kripke structure is explicitly specified, as isthe truth assignment on these worlds. Introspectively, this is not the way in which we model situations. Instead, the set ofpossible worlds is described implicitly, as is the interpretation π , as the set of worlds satisfying some condition.7 This set ofworlds may well include some impossible worlds. The impossible-worlds structure corresponding to a situation, therefore,is made up of all worlds satisfying the implicit description, perhaps refined so that “clearly impossible” worlds are notconsidered. What makes a world clearly impossible should be determined by a simple test; for example, such a simple testmight determine that 3 is prime, but would not be able to determine that 224036583 − 1 is prime.We can formalize this construction as follows. An implicit structure is a tuple I = (S, T , C), where S is a set of possibleworlds, T is a filter on worlds (a test on worlds that returns either true or false), and C associates with every world inS a set (possibly inconsistent) of propositional formulas. Test T returns true for every world in S that the agent considerspossible. An implicit structure I = (S, T , C) induces an impossible-worlds structure M I = (W , W(cid:6), π , C) given by:(cid:4)W =(cid:6) =w ∈ S(cid:4)w ∈ S(cid:5)(cid:5) C(w) is consistent(cid:5)(cid:5) T (w) = true(cid:6)(cid:6),,Wπ (w) = C(w)|Φ for w ∈ W ,C = C|(W (cid:6)−W ).We can refine the induced impossible-worlds structure by allotting more resources to test T . Intuitively, as an agent per-forms more introspection, she can recognize more worlds as being impossible. (Manne [20] investigates a related approach,using a temporal structure at each world to capture the evolution of knowledge as the agent introspects over time.)Consider the primality example again. The agent is likely to care about the primality of only a few numbers, say}. The agent’s inability to compute whether n1, . . . , nk are prime is described im-n1, . . . , nk. Let Φ = {Primen1 , . . . , Primenkplicitly by having worlds where any combination of them is prime. The details of how multiplication works in a worldis not specified in the implicit description. Thus, the implicit structure I = (S, T , C) corresponding to this description willhave S consisting of 2k worlds, where each world is a standard model of arithmetic together with a truth assignment to theprimitive propositions in Φ. The set of formulas C(w) consists of all propositional formulas true under the truth assignmentat w. The agent realizes that all but one of these worlds is impossible, but cannot compute which one is the possible world.Thus, we take T (w) = true for all worlds w. Of course, after doing some computation, the agent may realize that, say, n1 is7 In multiagent settings, where the worlds that the agent considers possible are defined by an accessibility relation, we expect the accessibility relationto be described implicitly as well.234J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235prime and n2 is composite. The agent would then refine the model by taking T to consider possible only worlds in whichn1 is prime and n2 is composite.The use of an implicit description as a recipe for constructing possible (and impossible) worlds is quite general, as thefollowing example illustrates.Example 5.4. Suppose that we have a database of implications: rules of the form C1 ⇒ C2, where C1 and C2 are conjunctionsof literals—primitive propositions and their negations. Suppose that the vocabulary of the conclusions of these rules isdisjoint from the vocabulary of the antecedents. This is a slight simplification of, for example, digital rights managementpolicies, where the conclusion typically has the form Permitted(a, b) or ¬Permitted(a, b) for some agent a and action b, andPermitted is not allowed to appear in the antecedent of rules [15]. Rather than explicitly constructing the worlds compatiblewith the rules, a user might construct a naive implicit description of them. More specifically, suppose that we have a finiteset of agents, say a1, . . . , an, and a finite set of actions, say b1, . . . , bm. Consider the implicit structure I = (S, T , C), whereeach world w in S is a truth assignment to the atomic formulas that appear in the antecedents of rules, augmented withall the literals in the conclusions of rules whose antecedent is true in w; furthermore, take T (w) = true for all w ∈ S,and C(w) to be all propositional formulas true under the truth assignment at world w. Thus, for example, if a rule saysStudent(a) ∧ Female(a) ⇒ Permitted(a, Play-sports), then in a world where Student(a) and Female(a) are true, then so isPermitted(a, Play-sports). Similarly, if we have a rule that says Faculty(a) ∧ Female(a) ⇒ ¬Permitted(a, Play-sports), then in aworld where Faculty(a) and Female(a) are true, ¬Permitted(a, Play-sports) as well. Of course, in a world Faculty(a), Student(a),and Female(a) are all true, both Permitted(a, Play-sports) and ¬Permitted(a, Play-sports) are true; this is an impossible world.This type of implicit description (and hence, impossible-worlds structure) should also be useful for characterizing largedatabases, when it is not possible to list all the tables explicitly.6. ConclusionMany solutions have been proposed to the logical omniscience problem, differing as to the intuitions underlying the lackof logical omniscience. There has been comparatively little work on comparing approaches. We have attempted to fill thisgap here, focusing on two aspects, expressiveness and pragmatics, for four popular approaches.In comparing the expressive power of the approaches, we started with the well-known observation that the approachesare equi-expressive in the propositional case. However, this observation is true only if we allow the agent not to considerany world possible. If we require that at least one world be possible, then we get a difference in expressive power. This isparticularly relevant when we have probabilities, because there has to be at least one world over which to assign probability.Indeed, when considering logical omniscience in the presence of probability, there can be quite significant differences inexpressive power between the approaches, particularly awareness and impossible worlds.Considering the pragmatics of logical omniscience, we identified some guiding principles for choosing an approach tomodel a situation, based on the source of the lack of logical omniscience in that situation. As we show, coming up with anappropriate structure can be nontrivial. We illustrate a general approach to deriving an impossible-worlds structure based onan implicit description of the situation, which seems to be appropriate for a number of situations of interest. Our discussionsuggests that the impossible-worlds approach may be particularly appropriate for representing an agent’s subjective view ofthe world.References[1] P. Berman, J. Garay, K.J. Perry, Towards optimal distributed consensus, in: Proc. 30th IEEE Symposium on the Foundations of Computer Science(FOCS’89), 1989, pp. 410–415.[2] M. Cozic, Impossible states at work: Logical omniscience and rational choice, in: Cognitive Economics: New Trends, in: Contrib. Econ. Anal., vol. 280,Elsevier, 2007, Ch. 2, pp. 47–68.[3] E. Dekel, B. Lipman, A. Rustichini, Standard state-space models preclude unawareness, Econometrica 66 (1998) 159–173.[4] D. Dolev, A.C. Yao, On the security of public key protocols, IEEE Trans. Inform. Theory 29 (2) (1983) 198–208.[5] R.A. Eberle, A logic of believing, knowing and inferring, Synthese 26 (1974) 356–382.[6] H.B. Enderton, A Mathematical Introduction to Logic, Academic Press, 1972.[7] R. Fagin, J.Y. Halpern, Belief, awareness, and limited reasoning, Artificial Intelligence 34 (1988) 39–76.[8] R. Fagin, J.Y. Halpern, Reasoning about knowledge and probability, J. ACM 41 (2) (1994) 340–367.[9] R. Fagin, J.Y. Halpern, N. Megiddo, A logic for reasoning about probabilities, Inform. Comput. 87 (1/2) (1990) 78–128.[10] R. Fagin, J.Y. Halpern, Y. Moses, M.Y. Vardi, Reasoning about Knowledge, MIT Press, 1995.[11] J.Y. Halpern, Alternative semantics for unawareness, Games Econ. Behav. 37 (2001) 321–339.[12] J.Y. Halpern, Y. Moses, M.Y. Vardi, Algorithmic knowledge, in: Proc. 5th Conference on Theoretical Aspects of Reasoning about Knowledge (TARK’94),Morgan Kaufmann, 1994, pp. 255–266.[13] J.Y. Halpern, R. Pucella, Modeling adversaries in a logic for reasoning about security protocols, in: Proc. Workshop on Formal Aspects of Security(FASec’02), in: Lecture Notes in Comput. Sci., vol. 2629, 2002, pp. 115–132.[14] J.Y. Halpern, L.C. Rêgo, Interactive unawareness revisited, in: Theoretical Aspects of Rationality and Knowledge: Proc. Tenth Conference (TARK 2005),2005, pp. 78–91.[15] J.Y. Halpern, V. Weissman, Using first-order logic to reason about policies, ACM Trans. Inform. Syst. Secur. 11 (4) (2008) 1–41.[16] A. Heifetz, M. Meier, B. Schipper, Interactive unawareness, J. Econ. Theory 130 (2006) 78–94.[17] J. Hintikka, Impossible possible worlds vindicated, J. Philos. Logic 4 (1975) 475–484.[18] K. Konolige, A Deduction Model of Belief, Morgan Kaufmann, San Francisco, 1986.J.Y. Halpern, R. Pucella / Artificial Intelligence 175 (2011) 220–235235[19] B.L. Lipman, Decision theory without logical omniscience: Toward an axiomatic framework for bounded rationality, Rev. Econ. Stud. 66 (2) (1999)339–361.[20] K. Manne, Towards a solution to the problem of logical omniscience for resource-bounded agents: An impossible worlds approach to temporal epis-temic logic, unpublished manuscript; an abstract appears in: Proc. 2005 Conference of the Australasian Association of Philosophy, 2005.[21] J. McCarthy, M. Sato, T. Hayashi, S. Igarishi, On the model theory of knowledge, Tech. Rep. STAN-CS-78-657, Stanford University, 1979.[22] S. Modica, A. Rustichini, Awareness and partitional information structures, Theory Dec. 37 (1994) 107–124.[23] S. Modica, A. Rustichini, Unawareness and partitional information structures, Games Econ. Behav. 27 (2) (1999) 265–298.[24] R.C. Moore, G. Hendrix, Computational models of beliefs and the semantics of belief sentences, Technical Note 187, SRI International, Menlo Park, CA,1979.[25] A. Moreno, Avoiding logical omniscience and perfect reasoning: A survey, AI Commun. 11 (2) (1998) 101–122.[26] Y. Moses, M.R. Tuttle, Programming simultaneous actions using common knowledge, Algorithmica 3 (1988) 121–169.[27] R. Pucella, Deductive algorithmic knowledge, J. Logic Comput. 16 (2) (2006) 287–309.[28] V. Rantala, Urn models: A new kind of non-standard model for first-order logic, J. Philos. Logic 4 (1975) 455–474.[29] V. Rantala, Impossible worlds semantics and logical omniscience, Acta Philosophica Fennica 35 (1982) 18–24.[30] R. Stalnaker, Impossibilities, Philos. Topics 24 (1996) 193–204.[31] H. Wansing, A general possible worlds framework for reasoning about knowledge and belief, Studia Logica 49 (4) (1990) 523–539.