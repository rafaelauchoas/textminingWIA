Artificial Intelligence 97 ( 1997) 9-43 Artificial Intelligence Axioms of causal relevance David Galles ‘, Judea Pearl * Cognitive Systems Laboratory, Computer Science Department, University of California, Los Angeles, CA 90024, USA Received October 1995; revised May 1996 Abstract This paper develops axioms and formal semantics for statements of the form “X is causally irrelevant to Y in context Z”, which we interpret to mean “Changing X will not affect Y once Z is held constant”. The axiomization of causal irrelevance is contrasted with the axiomization of informational irrelevance, as in “Finding X will not alter our belief in Y, once we know Z”. Two versions of causal irrelevance are analyzed: probabilistic and deterministic. We show that, unless stability is assumed, the probabilistic definition yields a very loose structure that is governed by just two trivial axioms. Under the stability assumption, probabilistic causal irrelevance is isomorphic to path interception in cyclic graphs. Under the deterministic definition, causal irrelevance complies with all of the axioms of path interception in cyclic graphs except transitivity. We compare our formalism to that of Lewis (1973) and offer a graphical method of proving theorems about causal relevance. @ 1997 Elsevier Science B.V. Keywords: Causality; Graphoids; Causal models; Counterfactuals; Actions 1. Introduction In [ IO], a set of axioms was developed for a class of relations called gruphoids. These axioms characterize the semantics of conditional a parallel set of axioms independence for causal relevance, in probability among observed events based on calculus. This paper develops to that is, the tendency of certain events informational relevance* author. Email: judea@cs.ucla.edu. * Corresponding ’ Email: galles@cs.ucla.edu. 2 “Relevance” will be used primarily will be clear from the context when “relevance” as a genetic name for the relationship of being relevant or irrelevant. to negate “irrelevance”. is intended It 0004-3702/97/$17.00 @ 1997 Elsevier Science B.V. All rights reserved PIISOOO4-3702(97)00047-7 10 D. Galles. J. Pearl/Artificial Intelligence 97 (1997) 9-43 irrelevance Informational affect the occurrence of other events reasoner. independent about X gives us no new information statements of the form “X is causally mean “Changing X will not alter the value of Y, if Z is fixed”. about Y. Causal irrelevant of Y given Z”, which means in the physical world, independent is concerned with statements irrelevance that, given the value of Z, gaining information is concerned with to Y in context Z”, which we take to of the observer- of the form “X is The notion of causal relevance has its roots in the philosophical works of Good [ 451, and Salmon relationships, Suppes cause-effect relevance. Although relevance, a given probability [ 3,5,32]. with no reference to give probabilistic [ 371, who attempted and recognized interpretations causal from statistical the need to distinguish these attempts have not produced an algorithmic definition of causal the consistency of relevance statements against the variables in themselves, and a given distribution for testing statements among temporal ordering relevance or temporal orderings. to underlying probabilities The current paper aims at axiomatizing they led to methods [ 121, to Axiomatic characterization theories of action as well as a guide of causal relevance may serve as a normative for schemes applications. For example, [6], such schemes should enable an agent to examine only direct effects of actions for a given goal and which actions cease to be analyzing (e.g., graphical models) instead of explicitly representation and to infer which actions are relevant relevant once others are implemented. for planning storing all possible effects of an action, as in STRIPS and decision-making for developing representation standard nuances example, in general Another application should assist a machine lies in the area of automatic systems, where machine-generated of causal relevance in complex diagnostic language gener- ation-for explana- tions are loaded with causal utterances. The formalization of causal relevance and causal relationships and selecting proper causes B”, linguistic “B was caused by A”, “A was the cause of B “, “B occurred despite A”, or “B would not if it were not for A” all express some form of causal relevance between have occurred A and B, yet these utterances the appropriate and making of the relation between A and B in the context choice may require careful understanding of the discussion. Axiomization of causal relevance could also be useful in causal conversations. such as “A normally are not entirely in distinguishing to experimental researchers Statements equivalent in suppose we find influence on others conditions, For example, the amount of exercise domains where exact causal models do not exist. If we know, through experimentation, that some variables have no causal determine whether other variables will exert causal experimental or may ask what additional information. growth while no effect on tumor growth while diet is kept constant. We would infer that controlling no influence changing activity, given diet is kept constant when activity in a system, we may wish to influence, perhaps under different such that a rat’s diet has no effect on tumor that exercise has to to be able to exercise) would still have is deciding whether in the cage would have an effect on the rat’s physical that temperature has no effect on activity when has no effect on (the rat’s choice of) diet temperature that we have established only diet (while paying no attention on tumor growth. A more subtle is kept constant and, conversely, inference problem is kept constant. that temperature could provide the ambient experiments like and D. Galles, J. Pearl/Artijicial Intelligence 97 (1997) 9-43 II set of axioms unless further assumptions are made about the underlying If we add the stability assumption (i.e., the nature of the individual processes that no irrelevance in the system), irrelevance. The probabilistic to change very weak; the probability definition, of the effect it does not support a very causal can be destroyed by the same then we obtain of causal irrelevance with inability appeal but is inferentially We provide two formal definitions which equates causal variable, has intuitive expressive model. changing set of axioms in directed graphs. The deterministic inability of axioms without our making path-interception deterministic for probabilistic irrelevance. to change axioms causal the effect variable causal as the set governing path interception irrelevance definition, which equates causal in any state of the world, allows irrelevance with for a rich set the causal model. All of the for directed graphs, with the exception of transitivity, hold for any assumptions about In Section 2, we define causal models, a formal system for interpreting In Section 3, we provide ments. determine which of the graphoid axioms hold under this definition. Finally, definition of causal we give a nonprobabilistic irrelevance. for proving of probabilistic about causal a definition irrelevance statements causal causal state- and in Section 4, and offer a graphical method irrelevance 2. Causal models A causal model given domain; namely, computation) here a definition economics. is a complete specification of the causal relationships it is a mathematical of every causal query about object the domain. Following that provides an interpretation that govern a (and [29] we will adopt and in engineering that generalizes most of the causal models used Definition 1 (Causal model). A causal model is a 3-tuple M= (YU,F), where (i) V={Xl,... (ii) U={Ui,..., disturbances, , Xn} is a set of endogenous variables determined within U,*} is a set of exogenous or background assumptions, abnormalities, variables or boundary conditions, the system, that represent and (iii) F is a set of n nontrivial functions {ft , . . . , f,,}, each having the form Xi = fi(pUi,U), i = 1,. . . ,n, (1) where pUi are the values of a set of variables PAi G V \ Xi (connoting the direct causes of Xi. We will assume solution we can consider the causal model M. called parents), in (iii) has a unique for Xl, . . . , X,,, given any value of the background variables Ut , . . . , U,,,. Thus (I in each variable X E V to be a function XM ( U) of the background that the set of equations The uniqueness assumption is always satisfied predecessors of Xi in some order, but may be violated in recursive models, where PAi are that is, in nonrecursive systems, 12 D. Galles, J. Pearl/Art@cial intelligence 97 (1997) 9-43 V = {X, Y} binary x=ut v ‘y U = { UI } binary y=ut AZ x X- yy Fig. 1. A valid nonrecursive causal model, with unique values for X and Y for all values of U. solutions such functions would be disallowed in nonrecursive models conveys two possible (X = 0, Y = 0)-so requirement systems with feedback. For example, consider The state U = 0 permits 1) and The uniqueness represents all relevant background in one state. Systems possessing dynamic notion of previous supplementing state, and incorporated factors, not modeled V and U [ 91. a deterministic in U. Such physical system conditions U were accounted several equilibrium for X and Y-namely, the equations x = y V u and y = x v u. (X = 1, Y = in a causal model. that F that for, such a system can only be the existence of states the into our analysis as a third kind of variables factors often can be summarized the understanding in equilibrium. if we assume indicate Indeed, by The assumption that there is a unique solution for X1, . . . , X,, while limiting 1, does not prevent the use of causal models to describe The equations do not need to be recursive the scope feedback systems to ensure uniqueness. the causal model shown in Fig. 1 dictates unique values for X and Y for of Definition in stable equilibrium. For example, UI =0 and UI = 1. Drawing arrows between the variables PAi and Xi defines a directed graph G(M), the causal graph of M. In general, G(M) can be cyclic. For some which we call examples of causal models, see Section 2.1. 1 merely provides Definition a description answers of the sentence into a causal model. To fulfill our requirement computing for causal queries, we need interpretation sentence the condition X = x. Thus, Definition pretation of the notion “locally usage. that we can bring about implies of the mathematical objects that enter that a causal model be capable of 1 with an to supplement Definition such a “X = x causes Y = y”. In ordinary discourse, the condition Y = y by locally enforcing inter- enforcing X = x” that is compatible with its common 1 must be supplemented with a formal normally External intervention implies changing the act of enforcing some mechanisms the condition Xi = 0 by connecting the mechanism in the domain. for example, variable Xi to ground amounts a logical circuit, some intermediate that nor- mally determines Xi. If Xi is the output of an OR gate, then after the intervention, Xi would no longer be determined the ground) tional representation, equation, Xi = 0, that represents to the OR gate. In the equa- the equation xi = fi(pai, U) with a new that clamps Xi to 0 regardless of the input by the OR gate but by a new mechanism the grounding of Xi. this amounts to replacing to changing (involving In D. Galles, J. Pearl/Artijicial Intelligence 97 (1997) 9-43 13 of just one equation, not several, sentences of imperative the principle of locality taxes” or “Make told to clean his face, a child does not ask for a razor, nor does he of the modal sentence “do p” of the existing state of affairs, and this, in the reflects such as “Raise pool. The proper interpretation to a a minimal perturbation 1, corresponds to the replacement of the minimal set of equations The replacement into the swimming in the common understanding him laugh”. When jump corresponds context of Definition necessary In general, we will consider several variables involves concurrent action of the form do( X = x), where X in V. 3 This leads to the following definitions. to make p compatible with U. Definition 2 (Submodel). Let M be a causal model, X be a set of variables realization of X. A submodel M, of M is the causal model x be a particular in V, and M, = (U VF,), where F,={_h 1 ~~x}u{x=x}. (2) In words, F, is formed by deleting from F all functions fi corresponding to members of X and replacing submodels is the assumption are useful them with the set of functions X = x. Implicit in the definition of that F, possesses a unique solution for every U. Submodels interpret each function action do( X = x) as the minimal any U, then M, represents differs from M by only for representing fi in F as an independent change the effect of local actions and changes. physical mechanism If we the to make X = x hold true under it in M required and define that results from such a minimal the variables that determine change, since in X. the model those mechanisms Definition 3 (Effect of action). V, and x be a particular by the submodel M,. in realization of X. The effect of action do( X = x) on M is given Let M be a causal model, X be a set of variables Definition 4 (Potential response). V. The potential response of Y to action do( X = x), denoted Y,(U), is the solution Y of the set of equations F,. in V, and let X be a subset of for Let Y be a variable Definition 5 (Countelfactual). The counterfactual interpreted as denoting sentence the potential response Y,(u) . 4 Let Y be a variable let X a subset of V. “The value that Y would have obtained, had X been x” is in V, and 3 The formalization 4 The connection between counterfactuals of conditional actions of the form “do( X = x) if Z = z” is straightforward [ 281. [ 1 ] and Heckerman and Shachter and local actions is made by Lewis [ 171 and is further elaborated [ 141. Readers who are disturbed by the impracticality interpretation of some counterfactuals (e.g., “If 1 were young”) are invited the word “modification” rather interventions, external (see [ 161). Pearl [29, p. 7061 explains the advantage than spontaneous changes, in thinking about causation to replace of and in the by Balke and Pearl of actions the word “action” with using hypothetical counterfactuals. 14 D. Galles. J. Pearl/Artificial Intelligence 97 (1997) 9-43 described transformation The syntactical the old functional mechanisms Xi = fi (PAi, u) with new mechanisms Xi = xi that represent that set the values xi for each Xi E X. As before, we will assume the external each variable Y E V to be a unique function of the background U in any model M,: Y = YM.~ (u) . For brevity, the subscript M is often omitted, in Definition 4 corresponds leaving Y,(U) . to replacing forces that a person can receive. There is a strong connection The notation Y,(U) is sometimes used in the statistical [ 361 to stand for the sentence “The value that Y would take in person u had X been x”, where literature above and our formal counterfactual X stands for a type of treatment the sentence between interprets this abstract, counterfactual Y taking on the value Y,(U) as X changes an individual experimental requirements the type used in the statistical that the process-based Y,(U) that were not formalized literature semantics given but, rather, as the set of attributes u that characterize conditions of Definition the individual, under study, and so on. In fact, every causal model meeting 1 can be translated sentence responsible interpretation in terms of the processes to x. It treats u not as merely of Yx( U) [ 291. Definition 4 for the index of the the statements of [ 29, p. 7031. In Section 4, we will further show in Definition 4 will uncover new properties of into a set of counterfactual in the statistical An explicit translation of intervention was first proposed by Strotz and Wold Graphical pretations Pearl Robins ramifications are explicated of causal and counterfactual [33] and Shafer [ 311. Other formulations [39]. literature. into “wiping out” equations [431 and used by Fisher by Spirtes et al. [41] and Pearl utterances in in the causal model [40]. Inter- in terms of Yx( U) are provided by trees, are given by terms of event [ 71 and Sobel [27]. of causality, Note that Y,(U) is well defined even when U = u and X = x are incompatible the abnormalities modeled or under for actions the input dictates X # x. It is for this reason breakdown or “surgery” feature of our formulation (i.e., X(U) # x) , thus allowing under normal conditions, describes a logic circuit we might wish to intervene though of mechanism The unique the formulations treated as a modality, namely, of the propositions as true. This enables effects of a huge number of action combinations without tend to such combinations. characteristics tion. of actions-the analysis of each individual mechanism theory or decision the causal model it enforces in control Instead, that to enforce propositions in M that are not realized if M and set some voltage X to x, even that one must invoke some notion in U. For example, in the definition of interventions. feature [ 14,381 -is that sets it apart from is that an action the names the to at- the by specifying to predict the model the modeler having is constructed under normal conditions, free of interven- it is not given an explicit name but acquires We can extend the notion of causal models to encode probabilistic information as follows: Definition 6 (Probabilistic causal model). A probabilistic causal model is a pair (MT P(u))* where M is a causal model and P(u) of u. is a probability function defined over the domain D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 15 SPRINKLER RAIN \ J WET 0 x4 0 X5 SLIPPERY Fig. 2. Causal graph illustrating causal relationships among five variables. P(U) , together with the fact that each endogenous variable is a function of U, defines a probability Y C V, we have distribution over the endogenous variables. That is, for every set of variables P(Y) = c P(u). tuIV,)=Y) The probability function Y,(U) induced by the submodel M,: of counterfactual statements is defined (3) in the same manner, through the P(Y, = y) = P(u). (4) c tulY,(u)=B) We note that is, P (Y, = y, Z, = z) that a causal model defines a joint distribution state- ments, for any sets of variables Y, X, Z, W, not necessarily is well defined and is given by C{ulu,cu,=~ar,,(U,=V’} P(u) . Likewise, P (Y, = y, X = x’) is well defined and is given by c{U,Y,(U)=y&x(U)=x’) p(u).5 In particular, P(Y, = y, Y,J = y’) on all counterfactual is defined disjoint. 2.1. Examples Next we demonstrate two familiar els using models. the generality of the mathematical evidential reasoning applications: object defining causal mod- and linear structural equation 5 The existence of such joint distributions has prompted some of the objections to treating counterfactuals to the joint as random variables, because, when x and x’ are incompatible, statement “Y would be y if X were x and X is actually x’“. The definition of Yx in terms of submodel not only avoids such problems but also illustrates using P(u) and F. it is hard to attribute probability that such joint probabilities rather parsimoniously can be encoded 16 D. Galles. J. Peari/Art#cial Intelligence 97 (1997) 9-43 2.1.1. Sprinkler example Fig. 2 is a simple yet sense typical reasoning. in common causal graph used among the sprinkler is slippery the causal relationships the season, whether the pavement It describes the season of the year (Xi ), whether rain falls (X2) during is wet in this graph except the (X4), and whether root variable Xt take a value of either “True” or “False”. Xi takes one of four values: ‘Spring”, link between, that the influence of the season on for example, Xt and Xs, captures our understanding the wetness of the slipperiness the pavement). an autonomous mechanism: of the pavement The corresponding model consists of five functions, is mediated by other conditions “Fall”, or “Winter”. Here, the absence of a direct is on (X3), whether (Xs). All variables each representing the pavement “Summer”, (e.g., x1 =u1 x2 = f2(-vvu2) x3 = f3(XlTU3) x4 = _f4(x3,x2,u4) x5 = _/-5(x47 u5) (5) The disturbances Ut , . . . , Us are not shown explicitly ern the uncertainties with the Bayesian network associated with P( XI, . . . , x5) whenever assumed to gov- associated with the causal relationships. The causal graph coincides in Fig. 2 but are understood to be independent, UillJ the disturbances are \ Ui. A typical specification of the functions {ft, . . . , fs} and the disturbance terms is given by the Boolean model x2 = [(Xi = Winter) V (Xl = Fall) V ab23 A labi x3 = [ (XI = Summer) V (Xl = Spring) V abs] A labi x4 = (x2 V x3 V ab4) A Tub; x5 = (x4 Vub5) A lab; (6) for Xi = true, and ubi and ubi stand, respectively, for (unspecified) For example, ub4 stands abnormalities. the pavement to get wet (x4) when is off (1x2) a pail of water on the pavement), while the sprinkler (e.g., pouring events that will keep the pavement dry (7x4) the sprinkler being on (x2), and ub4 (e.g., covering for triggering events and that might and it does not for stands labi in spite of rain falling the pavement with a plastic where xi stands inhibiting cause rain (unspecified) (x3), sheet). (7x3) To represent the action “turning the sprinkler ON”, or du(X3 = ON), we replace the equation x3 = fs(xt ,ug) in the model of Eq. (5) with X3 = ON. The resulting the effect of the submodel, M,+oN, that the only variables action on the other variables. variable affected by the action are X4 and Xs, that is, the descendants of the manipulated to that of that the operation do( X3 = ON) stands in marked contrast X3. Note, however, It is easy to see from this submodel contains all the information needed for computing D. Galles. J. Pearl/Artificial Intelligence 97 (1997) 9-43 17 Fig. 3. Causal graph illustrating the relationship between supply and demand. the equation involves making the sprinkler ON; the latter for X3, and therefore may potentially jinding removing every variable after observing it probably did not rain, and so on; no such inferences for the action “turning the substitution X3 = ON without in) (the belief seeing and doing: is ON, we may wish to infer that the season is dry, that can be drawn about the reasons in the network. This mirrors that the sprinkler the difference between the sprinkler ON”. influence 2.1.2. Policy analysis in linear econometric models Causal models are often used to predict the effect of policies on systems in dynamic equilibrium. In the economic literature, for example, we find the system of equations q=blp+dli+ul, p = b2q + d2w + 2.42, (7) (8) [ 111. This system of equations where q is the quantity of household demand product A, i is household and 4 represent respectively error terms, namely, unmodeled for a product A, p is the unit price of income, w is the wage rate for producing product A, and ut that affect quantity and price, factors process constitutes a causal model V = {Q, P} and U = {(It, U2, I, W} and assume tonomous shown lJ2 are unobservable U2 are unobserved, rors, which if we define an au- is 3. The causal graph of this model that I and W are known, while Ut and terms UI and the error of these er- to be a Gaussian distribution with the covariance matrix (Definition that each equation the model must be augmented with in the sense of Definition in Fig. 3. It is normally in I and W. Since the distribution independent is usually represents assumed taken and 1) 2, =COV(Ui,Uj). to answer queries such as: We can use this model ( 1) Find (2) Find (3) Given the expected demand the expected demand that the current price (Q) (Q) if the price is controlled if the price is reported at P = PO. to be P = PO. is P = PO, find the expected demand (Q) had the price been P = pl. To find the answer to the first query, we replace Eq. (8) with p = PO, leaving q=blp+dli+ul. P ‘PO. (9) ( 10) 18 D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 The demand from i and the expectation of Ut, giving is then 4 = pabt + dti + ut, and the expected value of Q can be obtained E[Q I611 =E[Ql +bl(p-E[Pl)+dl(i-E[zl). The answer to the second query Eq. (7) on the current observation {P = po, I = i, W = w} and taking is given by conditioning the expectation, E[Q I PO, 6 WI = hpo + dli + E[Ul I PO, i, WI. (11) of E[ Ut 1 po, i, w] is a standard procedure once Sij The computation is given. Note no longer that, although Ut was assumed holds once P = po is observed in the solution and that the observed value pa will affect the expected demand 4 (through E[ Ut 1 po, i, w] ) even when bt = 0, which the conditional [ 2 1 I. Note also that Eqs. (7) and (8) both participate is not the case in the first query. of I and W, this independence of the counter-factual quantity third query independent expectation The QPzP,, given requires the current observations {P = po, I = i, W = w}, namely, HQp,, I po,L WI = hpl + d~i + -%(/I / PO, i, WI. (12) The expected value E[ U1 I po, i, w] is the same in the solutions queries; counterfactual the latter differs only in the term btpt. A general method is described in [ 21. queries to the second and third such for solving 2.1.3. Linguistic notions of causality Causal models provide a precise for defining this section, we provide some brief examples, all relating language intuitive to a given causal model M. causal concepts. In l “X is a cause of Y”, if there exist two values x and x’ of X and a value u of U such that Y,(U) # Yxl(u). 0 “X is a cause of Y in context 2 = z”, if there exist two values x and x’ of X and a value u of U such that Y,, # Yxlz (u). l “X is a direct cause of Y”, if there exist two values x and x’ of X, and a value u of U such that Y,,(U) # Y,l,( U) where r is some realization of V \ X. l “X is an indirect cause of Y”, if X is a cause of Y, and X is not a direct cause of Y. l “Event X = x may have caused Y = x” if (i) X = x and Y = y are true, and (ii) there exists a value u of U such that X(U) = x, Y(u) = y, Y,(u) = y and Y,,(u) # y for some x’ # x. l “The unobserved event X = x is a likely cause of Y = y” if (i) Y = y is true, and (ii) P(Yx=y,Yxt #yIY=y)ishighforsomex’#x l “Event Y = y occurred despite X = x”, if (i) X = x and Y = y are true, and (ii) P(Y, =y> is low. The preceding the flexibility have expressions. list demonstrates of finding that, by varying appropriate formalization the quantifiers of U and X, we for many nuances of causal D. Galles, J. Pearl/Art$cial Intelligence 97 (1997) 9-43 19 1.1 (Symmetry) (XIY ] Z) =+ (YIX ] Z) 1.2 (Decomposition) (X_LYIV ] Z) =+ (XIY 1 Z) 1.3 (Weak union) (XIYW 1 Z) j (XIY 1 ZW) 1.4 (Contraction) (XIY I Z>&(X_LW ) ZY) ==+ (XIYW ) Z) 1.5 (Intersection) Intersection (XIW I ZY)& (XIY 1 ZW) =+ (XUW 1 Z) requires a strictly positive probability distribution. Fig. 4. The graphoid axioms 3. Probabilistic causal irrelevance The existence of a probability distribution over all variables in a probabilistic causal model leads to a natural definition of the probabilistic version of causal irrelevance. Definition 7 (Probabilistic vant to Y given Z, written causal (X f, Y ) Z)p, irrelevance). iff X is probabilistically causally irrele- vx,x’, y,z P(y 1 ?,q = P(y I T,x”). (13) Read: “Once we hold Z fixed change the probability of Y”. (at z), changing X between any two values will not 3.1. Comparison to informational relevance If we remove the “hats” from Definition axioms independence in probability in a different these axioms [ 10,241 given ditional by the graphoid duced axioms were complete. This conjecture proved Nevertheless, relevance: propositions remains the graphoid axioms capture information that conditional independence irrelevant” “Learning irrelevant [ 261. in the system; what was relevant 7, we get the standard definition calculus, denoted (XIY in Fig. 4. Dawid I Z), which [4] and Spohn form, and Pearl and Paz [24] conjectured of con- is governed intro- [42] that these [44], who also has been in probability refuted by Studeny theory has no finite axiomatization. features of informational the most important should not alter the relevance status of other relevant, and what was irrelevant remains between relevance and causal informational is on provides One of the salient differences property of symmetry, Axiom 1.1. Informational to Y, then Y is relevant is relevant sprinkler information learning whether on. This property make the pavement wet, so turning on the sprinkler gives us information of the pavement; of the sprinkler and gives us no information is the if X learning whether the is wet, and, vice versa, is to about the state the pavement has no physical effect on the state the sprinkler was on or off. the sprinkler turning a sprinkler on tends to X as well. For example, on whether the pavement is clearly violated is symmetric, namely, conversely, wetting in causal models: is wet provides about whether the pavement on whether information relevance 20 D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 V = {X, Y} binary U = (Ut} binary Y= 1 if x=ut 0 otherwise P(U,) = 0.5 Fig. 5. An example of P(y) > MAX,P(y ( ?). Another basic difference between informational and causal relevance is that in the former, the rule of the hypothetical middle [26, p. 171 always holds: MIW’(y I x> < P(Y) 6 MAW’(Y I xl. (14) In causal relevance, P ( y ) might be greater than MAX,P ( y 1 2) or less than MIN,P ( y 1 2). Fig. 5 illustrates such a possibility. In Fig. 5, there are two endogenous variables X and Y, as well as an exogenous variable Ut . Without any intervention, X will always have the same value as Ui , thus, Y will have the value 1. If X and Ut have different values, however, then Y will have the value 0. If we intervene and set X = 1, then Y will have the value 1 when CJ1 = 1, which has a probability 0.5, and Y will have the value 0 when lJ1 = 0, which has a probability 0.5: P( Y = 0 1 set( X = 1) ) = P( Y = 1 I set( X = 1) ) = 0.5. Similarly, we can see that =0.5. Thus, MAX,P(y 1%) =0.5, and P(Y=O( P(Y=1)=1>0.5=MAX,P(y(?). set(X=O)) =P(Y= 1 I set(X=O)) Note that, given this violation of the rule of the hypothetical middle (Eq. ( 14)), Definition 7 is not equivalent to v’x,y,z P(y I Z-92) = P(y I a. (15) Read: “Once we hold Z fixed (at z > , controlling X will not affect the probability of Y”. In fact, Eq. (15 is stronger than Definition 7, furthermore, Statement 2.5.2 (left-intersection of Theorem 8, below) follows from the former but not from the latter. The notion of probabilistic causal irrelevance may bring to mind the concept ignor- [35] which is extremely important in analyzing the effectiveness of treatments ability (e.g., drugs, diet, educational programs) from uncontrolled studies. The two concepts are related but different. Ignorability allows us to ignore how X obtained its value x, while irrelevance allows us to ignore which value X actually obtained. Ignorability is defined as the condition P(Yx=y I z) =P(Y=y I LX>, which implies P(y I?;‘> AP(Y,=y) =E,(y I z,x>. (16) (17) D. Galles, .I. Pearl/Artijicial Intelligence 97 (1997) 9-43 21 to relate the potential design Ignorability in experimental probabilities. Central allows an investigator Thus, ignorability conditional a set of observables Z that would make Eq. (16) domain. the problem value A practical criterion a causal model for selecting Z can be obtained the back-door criterion to answer The question we attempt in formal counterfactual (e.g., that Y would obtain had X been x is conditionally in this section in [ 291) . in itself does not provide such a criterion language: “Z can be selected response Y, to observable is the question of how to select true, given causal knowledge of the it does state although if, for every x, the of X, given Z”. from the graph G(M) underlying independent is whether the relation of causal to those governing (A f, B 1 C) p, is governed by a set of axioms similar there are any constraints irrelevance, the relation of informational whether P( y 1 2) P( y 1 2) represents the probability some causal model M. Our finding not totally arbitrary, in Fig. 4. to any pair (X, Y) of variable irrelevance, (AIB that prohibit 1 C). More generally, one may ask the assignment of arbitrary functions of (Y = y) indicate induced by physically sets in V, in total disregard of the fact that setting X = x in ) 2) is axioms such as those the assignment P(y that, although by qualitative it is only weakly constrained 3.2. Axioms of probabilistic causal relevance We have found only two qualitative properties that constrain probabilistic causal irrelevance. Theorem 8. For any causal model, the following two properties must hold: 2.2.1 (Right-Decomposition) (X fi YW 1 Z), j (X + Y 1 Z)p & (X f, W 1 Z)p. 2.5.2 (Left-Intersection) (X j+ Y 1 ZW) p & ( W + Y 1 ZX) p j (XW j+ Y I Z) p. Property 2.2.1 is read: “If changing X has no effect on Y and W considered separately”. This follows it has no effect on either Y or W considered jointly, trivially function, but it does not reflect any quality of then from the fact that P( .) is a probability causation. is read: “If changing X cannot affect P(y) when W is fixed, and then changing X and W together l’(y) when X is fixed, affect Property 2.5.2 changing W cannot cannot affect P(y)“. Many seemingly intuitive properties do not hold, however. For instance, none of the following sentences hold for all causal models. 2.2.2 (Left-Decomposition-l) (XW f, Y I Z), =+ (X ft Y I Z)p V (W f, Y I Z)p. 2.2.3 (Left-Decomposition-2) (XW ft Y 1 Z)p I (X ft Y ) Z)p V (X f+ W j Z)p. 2.2.4 (Left-Decomposition-3) (XW ft Y I Z>P & (XY + z I WP * G f, Y I ZIP v (X f, w I ZIP. 2.3 (Weak Union) (X ft WY I Z)p =+ (X f, Y ) ZW)p. 2.4 (Contraction) (X ft Y I Z)p &(X ft W I ZY)p ==+ (X fi WY I Z)p. 22 D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 x = u1 V = {X, WY} binary w = Ui U= {VI} binary y = Putity( x, w, Ut ) Y P(Ul) = 0.5 Fig. 6. Counterexample to Property 2.2.2. 2.5.1 (Right-Intersection) (X f, Y 1 ZW)p & (X f, W ) XY>p I (X f, WY 1 Z)p. 2.6 (Transitivity) The sentences that symmetry does not hold, which necessitates above were tailored after the graphoid axioms (Fig. 4) with the pro- left and right versions of de- vision and intersection. Many of these sentences have intuitive appeal and yet are composition to the semantics of P ( y ( 2). For example, Property 2.2.2 states, not sound “If changing X has an effect on Y, and changing W has an effect on Y, then changing that refutes X and W simultaneously in Section 3.4 and in this assertion Appendix B, each of these sentences should also affect Y”. A simple real-life example to come by. Still, as will be shown is refuted by some specific causal model. is difficult relative 3.3. Proofs of axioms of probabilistic causal irrelevance We now prove the two sentences of Theorem 8. 2.2.1 (x f, w 1 z)~ - (X ft Y / Z>P 8~ (X b W I -0~ holds trivially. 1 ZIP ++ W (X p ( y ) z, 2) = p( y 1 r, T), which implies (Xft WI ZIP. 0 =+ Pow I ?,a = P( yw I i‘, 2). We can sum over W to get (X f, Y 1 Z)P. A symmetric argument shows 2.5.2 (By contradiction) Assume (X j+ Y 1 ZW)P 8~ (W f, Y 1 z-0~ & l(XW Y 1 Z)p. Since -(XW P(Y (z^,G,F) f, Y I Z) p, by definition of probabilistic causal z P(y Iz^,i7,2').However, (X~,YIZWP P(Y I T,j;,G) = P(y I ~?‘,X,w^l. Furthermore, W’f, Y I .WP f, irrelevance implies implies P(y I ?‘,T,,_) = P(y I T‘,j;l,;;l), / F‘,2’,;3) = P(y / F,z?,G’). which contradicts ~x,x’,w,w’,z Thus Vx,x’,w,w’,z SO kx’,w,w’,z P(y P(y I ?,j;-,@) + P(Y I ?,T,w^‘). 0 P(Y I ~,Zw^) = I / F,Zw^) = P(Y EI~,~,x’,w,w’,z v~,~,x’,z,w ljy,x’,w,w’,z P(y ~,?,G’), 3.4. Counterexample to Property 2.2.2 We now disprove Property 2.2.2 by counterexample. to model a common, necessarily meant that all possible causal models must conform real-life to the property. This counterexample situation. Rather, it disproves is not the claim D. Galles. J. Pearl/Artijicial Intelligence 97 (1997) 9-43 23 2.2.2 (XW f+ Y 1 Z>P 3 Fig. 6 shows a counterexample (X j+ Y 1 Z)P v (W f+ Y 1 ZIP. to this sentence. In this model, j-, Y I 0)P & -(W ++ Y ( 0)P. Th is counterexample its contrapositive form, which would state of Y, and changing X can affect the probability has no effect on the probability if tweaking X has an effect on Y, and (XW f, Y 1 0) p & is more ciear when we the of Y, but changing W and that changing W can affect of Y. This is extremely tweaking W has an effect on Y, we to also counterin- the more flexible option of changing X and W simultaneously -4X consider probability X simultaneously tuitive; would expect affect Y. The key to this counterexample is the fact that setting W removes the connection between W and U,. When we intervene on only X, W takes on the same value as UI, and Y will always have the value of X. When we intervene on both X and W, there is that W and U1 will no longer any connection have the same value is 0.5, and P(y) = 0.5. between U1 and W. Thus, the probability Counterexamples to the other six properties that do not hold for all causal models are in Appendix B. 3.5. Numeric constraints Although Definition 7 imposes only weak constraints the structure of probabilistic which describe nontrivial the effects of actions in the domain, numerical bounds. For instance, the inequality causal irrelevance, the probability (Axioms 2.2.1 and 2.5.2) on assignments P ( y / 2) , by nevertheless are constrained (Y/%2) >P(Y,Z 12) (181 must hold in any causal model. This can easily be shown by the definition of P(y. z 1 2) and P( y 1 2, z^). Recall from Eq. (4) that P(y,z 1% = c P(u)* P(y 1 %a = c P(u). {ulK:(u)=Y&.G(u)=zJ {XIYCZ (u)=.v) the set of all values u of U such that Y,(U) = Y and Z,(u) = z , and Uy , Consider V, the set of all values u’ of U such that Y,, (u’) = y. Since all values u of Uyz already the value of Y. Thus, constrain the value z, fixing Z at z will not affect / l?,z^) b P(yz for all values u of UYz, Yxz(u) = y. Hence, U$ 2 UYz and P(y IL?). This can be shown more formally using Corollary 16 proven in Section 4.2. Additional in [ 301. constraints were explored to have 2 3.6. Axioms of causal relevance for stable models The set of axioms we obtained for causal irrelevance from our intuition expect discrepancy. One possibility rather terministic of cause-effect relations. We have two explanations is that our intuition of causal relevance is much smaller than we would for this is based on a de- than a probabilistic conception of physical reality. This possibility 24 D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 that yields a more complete in Section 4, which gives a deterministic definition set of axioms. The other possibility exploited in Section 3.4 and Appendix B are not commonly life. In this section, we explore what assumptions irrelevance to acquire properties that we intuitively irrele- of causal is that the type in observed need to be made for proba- associate with causal will be explored vance of examples everyday bilistic causal irrelevance. A more expressive set of causal relevance axioms is obtained that is, causal models whose irrelevances remain invariant fi. Our definition of stability employs class 7 is the set of all models arguments. In other words, to changes if we confine the are implied in the the concept of that have the same the functions function class. A replacement to stable causal models, analysis by the structure of the causal model and, hence, forms of each individual a replacement variables V and U, and the same functional are allowed not allowed f,(PAi) E MI and f;(PAi) class that contains the model M. to change between members of 7, but the arguments of these functions to vary. Formally, for any two models Mt , h42 E T and any two functions the replacement E M2, PAi = PA;. The class r(M) represents are We now define stability using replacement classes (see also [ 251 6 ). Definition 9 (Stability). M is stable irrelevances if it is shared by all models in M are stable. Let M be a causal model. An irrelevance (X ft Y 1 Z) p in in r(M). The model M is stable if all of the Stability requires that irrelevance be determined by the structure of the equations, not is not stable if we can to obtain in Section 3.4 merely by the parameters of the functions. Thus, a causal model remove an irrelevance a new model with fewer irrelevance and Appendix B, for instance, would destroy an irrelevance. Note that none of the models presented Appendix by replacing an equation or set of equations in the form of one of the equations In each of the examples in Fig. 6 and the a minor change relationship statements. for example, are stable. One might think that any causal models that contained only additive, monotonic that conjecture. fi would be stable. The causal model of Fig. B.7 refutes stable causal models. All monotonic linear systems, functions is stable. There are, however, many Definition 10 (Path inteerception). Let (X * Y 1 Z)c directed path from X to Y in graph G contains at least one element in Z”. stand for the statement “Every irrel- Theorem 11. If a causal model M is stable, evant to x given Z, in M iff Z intercepts all directed paths from X to Y in the graph G(M) then X is probabilistically defined by M. That is, causally (X f, Y I Z)P e (X -++ y I ZIG(M) 6 The probabilistic notion of stability (also called “DAG-isomorphism”, and “faithfulness” independencies to functional form. [ 411) was used by Pearl and Verma [ 19911 to emphasize “nondegeneracy” the invariance [ 26, p. 3911, of certain D. Galles, J. Pearl/Artijcial Intelligence 97 (1997) 9-43 25 (i) 1 Z)G(M). Assume irrelevance causal that, for some sets of variables X, I: Z, (X ft Y ] Z)p and -(X that there exists a stable causal (X--Y (X ft Y ) Z)P* Proof. (A ft B 1 C)p, and relation model M that induces a probabilistic -+ Y 1 Z)o(,,,,). assume there is a directed path from X to Y that is not intercepted by Z in G(M), we Since in M’. can easily construct a model M’ such that G( M’) = G(M) and -(X that lie on the path from X to Y to We can do this by changing that P(y 1 ?) < 1. Thus, disjunctions if we force X to have the value 1, Y will also have the value 1, and P ( y 1 F, 2) # P( y I T) . By assumption, in a (X f, Y / Z)p, member of r(M). Thus, M is not a stable causal model, a contradiction. in M is not shared all of the functions and then modifying the other functions so an irrelevance f, Y 1 Z), to ensure (ii) (X++Y / Z) G(M) ==+- (X ft Y I Z)p. We will use the following lemma: Lemma 12. For any structural equation fr in a causal model M, if a series of functional substitutions results in a nav function gy such that X is an argument of gy, then there must be a directed path from X to Y in G(M). We will prove Base case: If we make no substitutions be a parent of Y in G(M), from each argument of fy to Y in G(M). this lemma by induction on the number of functional substitutions. into fy, then every argument X of fy must by our definition of G(M). Thus, there is a directed path into fy always results substitutions that n - 1 functional Inductive case: Assume in the new function gr such that for each argument X of gr, there is a directed path from resulting X to Y in G(M). We use this assumption in gk, there as follows: is a directed path from every argument of gk to Y in G(M), When we do a single substitution, we replace a variable with a function of its parents in for in G(M). So, for any new argument X’ that is introduced X, X’ must be a parent of X in G(M). By the inductive hypothesis, there must be a directed path from X to Y in G(M). Thus, there must be a directed path from X’ to Y in G(M). to prove that after n substitutions into gb by substituting We can now prove the implication (X + Y I Z)G(M) ==+ (X f, Y 1 Z)p. We will fr, equation the functional in fy except consider for all variables for X and Z, we are left with a new function gr. By Lemma 12, since there is no directed path from X to Y in G( M,), X is not an argument of gr, so gr is a function of only Z and U. Since gr is a function of only Z and U, and not of X, Yxz(u> = Yz(u), so P(y for Y in M,. After we do a functional I F), and (X fi Y 1 Z),. I jz,?) = P(y substitution 0 Since (X f, Y I Z)p _ irrelevance causal directed graphs. A complete in Fig. 7. is completely (X -H Y I Z)G(M) characterized set of such axioms was developed by the axioms of path in in [ 22,231 and is given interception in stable causal models, probabilistic 4. Deterministic causal relevance The notion of causal irrelevance obtains a deterministic definition when we consider the effects of an action conditioned on a specific state of the world u. 26 D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 3.2.1 (Right-Decomposition) (X-++YW 1 Z)o=+ (X++Y 1 Z)o&(X+W (Z),. 3.2.2 (Left-Decomposition) (XW -M Y 1 Z)o ==+ (X 4 Y 1 Z), & (W ++ Y ( Z)o. 3.4 (Strong Union) (X+ Y 1 Z)o =+ (X* Y 1 ZW)o VW. 3.5.1 (Right-Intersection) (X+Y I ZW)G&(X-+ W) ZY)G* (X+YW Z),. 3.5.2 (Left-Intersection) (X+Y 1 ZW)C&(W++Y 1 ZX)Gq (XW+Y I Z)G. 3.6 (Transitivity) (X ++ Y 1 Z)o =+ (a * Y I Z)o V (X * a ( Z)o Vu 4 XU Z u Y. Fig. 7. Sound and complete axioms for path interception in directed graphs. V = {X, W, Y} binary U = {Vi, Uz} binary ifx=w otherwise \id Y w=x Fig. 8. Example of a causal model that requires the examination of submodels to determine causal relevance. Definition 13 (Causal irrelevance). X is causally irrelevant to Y, given Z, written (Xf,YIZ),ifforeverysetWdisjointofXUYUZ,wehave: Vu, z,x,x’, w &v(u) = yx%v(u). (19) This definition captures the intuition “If X is causally irrelevant than the probabilistic to Y, then X cannot in that definition, affect Y under any circumstance”. It is stronger (XftYIZ)=+(Xf,YIZ)P. Unlike definition the probabilistic implies definition of causal irrelevance (see Eq. ( 15) ) , the deterministic v’u,z,x K,(u) = y,(u). (20) the equality Y,,,(u) To see why we require in every context W = w, consider the causal model of Fig. 8. In this example, Z = {8}, W follows X and, hence, Y follows X, that is, YX=O (u) = YX=I (u) = u2. However, since y( x, w, ~2) is to Y. Only holding W a nontrivial constant would reveal this intuition, we must consider all contexts W = w in Definition relevant to be causally influence of X on Y. To capture function of x, X is perceived the causal = Y,,,,(u) to hold 13. D. G&es, J. Pearl/Art$cial Intelligence 97 (1997) 9-43 21 in [ 141. However, whereas Heckerman of irrelevance This definition siveness presented in terms of limited unresponsiveness vance as a property of the configuration a version of their definition of causality, be a theorem of causal relevance bears some similarity to the idea of limited unrespon- and Shatter define causality to a specific set of actions, we view causal rele- In fact, to of the mechanisms translated into our language, will be shown in a causal model. in Section 4.7.2 (see Eq. (27)). 4.1. Axioms of causal irrelevance With this definition of causal irrelevance, we have the following theorem: Theorem 14. For any causal model, the following sentences must hold: 4.2.1 (Right-Decomposition) (X ft YW ( Z) d (X f, Y 1 Z) & (X f, W 1 Z). 4.2.2 (Left-Decomposition) (XW f, Y 1 Z) =+ (X f, Y 1 Z) & (W ft Y 1 Z). 4.4 (Strong Union) (X + Y 1 Z) 3 (X ft Y 1 ZIV) VW. 4.5.1 (Right-Intersection) (X fi Y I ZIV) & (X ft W I ZY) ==+ (X f, YW 1 Z). 4.5.2 (Left-Intersection) (X j+ Y I ZW) & (W ft Y I ZX) ==+ (XW f+ Y I Z). Comparing to Fig. 7, we see that all axioms of path interception, except transitivity, relative are sound Section 4.4. below. to deterministic causal relevance. The proof of Theorem 14 is in 4.2. Properties of counte$actual statements The axioms of counterfactuals, motivate using listed in the preceding properties namely composition, effectiveness, and reversibility, which we will section are based on three fundamental the action semantics of Definition 3. Composition. causal model, For any two singleton variables Y and W and any set of variables Z in a X,(u) =x-y,,(u) =Yz(u) (21) Composition states that, in any context Z = z, if we force a variable X to a value the intervention will have no then x that it would have had without our intervention, effect on other variables Since composition in the system. allows Y, (u) ) , we need an interpretation naturally, we identify with the variable under no interventions. for the removal of a subscript to for a variable with an empty set of subscripts which, reducing yZx(u) (i.e., Definition 15 (Null action). Y@(u) A Y(u). Corollary 16 (Consistency). For any variables Y and X in a causal model, X(u) =x ==+ Y(u) = Y,(u) (22) 28 D. Galles, J. Pearl/Artijkial Intelligence 97 (1997) 9-43 Corollary 16 follows directly Eq. (22) was called consistency by Robins from composition [ 341. 7 and null action. The implication in Effectiveness. For all variables X and W in a causal model, Xxw(u) = x. Effectiveness specifies that namely, enforcements W = w, X will indeed if we force a variable X to have take on the value x. the effect of an intervention on the manipulated itself, the value x, then regardless of other variable (23) Reversibility. For any two variables Y and W and any set of variables X in a causal model, (Lv(u> = Y> & (W,,(u) = w) =+ Y,(u) = y. (24) Reversibility reflects memoryless behavior-the state of the system, V, tracks the state if forcing W of U, regardless of U’s history. Given a context X = x as in Eq. (24), to a value w results in W = w, in a value y for Y and forcing Y to y results, then W and Y will have the values w and y, respectively, without any intervention. This follows in every context X = x have a unique if we assume a solution W = w and obtain Y = y and, in turn, assuming solution. Thus, a solution Y = y yields W = w, then to the equations. that the equations (W = w, Y = y) the requirement the solution is indeed in turn, from A typical example of irreversibility is a system of two agents who adhere to a tit- the prisoners’ dilemma). Such a system has two stable solutions, (e.g., the reversibility and defection, under condition; for-tat strategy cooperation satisfy in the other agent’s cooperation cooperation product of using a state description determine the state description and reversibility In recursive be seen by noting Thus, reversibility another is trivially forcing X to a value x results true. In non-recursive form of composition, systems, reversibility should include from the start (Y(U) = y, W(u) = w) . Irreversibility, the ultimate state of the system are not included the same external conditions U, and thus it does not results forcing either one of the agents (Y,(U) = y, W,(u) = w), yet this does not guarantee to cooperate in such systems, that is too coarse, one where all of the factors is a that in U. In a tit-for-tat strategy, factors such as the previous actions of the players, is restored once the missing factors are included. follows directly from composition. This can easily that in a recursive system, either Yx,,,(u) = Y,(U) or Wxy(u) = W,(u). reduces to ( Yxw( u) = y ) & (W,(u) = w) =+ Yx( u) = y, which or to (Y,(u) = y) & ( Wxr( u) = w) j reversibility is Y,(u) = y, which loops. If in a value y for Y, and forcing Y to the value y results is a property of causal systems, ’ Consistency and composition are tacitly used in economics [ 361. To the best of our knowledge, Robins was the first to state consistency [ 201 and statistics within model to derive other properties of counterfactuals (personal p.9681. communication, February 1341. Composition was brought 1995). A weak version of composition the so-called Rubin’s formally and to use it to our attention by J. Robins in [ 15, is mentioned explicitly D. Galles, J. Pearl/Artificial Intelligence 97 (I 997) 9-43 29 in X achieving without any intervention. the value x, then X and Y will have the values x and y, respectively, 4.3. Soundness of composition, effectiveness, and reversibility Following relationships tradition the to be sound if that property holds in all causal models. logic, we will consider of standard a property of causal Theorem 17. Composition is sound. solution, the structural equation forming M, and substituting Proof. Since Yz (u) has a unique variables would yield a unique solution we will form M, and examine where W stands all variables without substituting then plug this solution Yz = f (z, x, u). At this point, we can solve for X by substituting other than Z, which then Y,(u) = Yzx(u). q out all other for Y, regardless of the order of substitution. So for Y in M,, Y, = fy( z, w, x, u), for the rest of the parent set of Y. To solve for W, we substitute out in M, into Z, X, and Y and express W as a function of z, x, and u. We into fy to get Y, = fu( z, x, W( z, x, u) , u), which we can write as in Mz except Z,Y, and X. In other words, we substitute out all variables We can now see that if x =X,(U), leaves Yz = f(z,X(u,z),u). out all variables This proof is still valid in cases where X = 8. Theorem 18. Eflectiveness is sound. Proof. This theorem solution for Y of a set of equations under X = x. 0 follows from Definition 1, where Y,(u) is interpreted as the unique Theorem 19. Reversibility is sound. solution, that the solution from the assumption the structural equation follows is unique. Since Y,(u) has a unique for V in every sub- Proof. Reversibility forming M, and substituting out model for Y, regardless of the order of sub- all other variables would yield a unique solution stitution. So, we will form M, and examine for Y in Mx, which in general might be a function of X,W,U, and additional variables: Y, = fy(x, w, z, u), where Z stands for parents of Y not contained in X U W U U. We now solve for Z by out all variables except X,Y, and W. That is, we substitute out all variables substituting into X, W, and Y and express Z as a function of x, w, and in M,, without substituting into fy to get Y, = fy(x, w, Z(x, w, u), u), which we can U. We then plug this solution write as Y, = f (x, w, u). We now consider what would happen for Y in M,,. Since we avoided substituting into W when we solved for Y in M,, we will get the same result as before, namely, Y,, = f (x, w, u). In the same way, we can for y = Yx(u), w = W,(u) show that W, = g(x,y,u) is the same as solving is the same as solving is also a solution for y = f(x, w, u) and w = g(x, y, u), which for y = Y,,(u), w = Wxy (u). Thus, any solution y to y = Y,,(u), w = W,,(u) and Wxy = g(x,y,u). to y = Y*(u) . Cl if we solved So, solving anything 30 D. Galles, .I. Pearl/Arr&iaL Intelligence 97 (1997) 9-43 Given a causal ordering of variables ever Y precedes X in the ordering, one can show that effectiveness complete and effectiveness as long as the uniqueness [ 131 has recently in all causal models, holds. [ 91. Joseph Halpern are complete assumption shown that composition, reversibility, recursive as well as nonrecursive, in V, that is, Y,, (u) = yZ (u) for any set 2 when- are and composition 4.4. Proofs of causal relevartce axioms Using the properties from Section 4.2, we can prove Theorem 14, that the axioms of causal relevance are sound. 4.2.1 Holds trivially. 0 4.2.2 (By contradiction) Assume that there exists a causal model such that (XW ft ft Y 1 Z) ft Y 1 Z) &-(X Y I Z) &-((Z % y I Zl &(W + y I Z)>. s o, either (XW or(XWftYjZ)&~(WftY(Z). First, we consider (XW ft Y 1 Z) & -(X implies -(X ft Y 1 Z) irrelevance, value u of U such u such that Y,, (u) follows: Let w = Wxz(u), w # w’. By composition, Y,,,(u) which contradicts contradiction. (XW that Yx, (u) # Y,/, (u). Now, # Yxjz (u). Using and w’ = WXtz (u). let us consider that there exist ft Y 1 Z). By our definition of causal two values x, x’ of X and some the values x, x’, z, these values, we can determine w and w’ as It does not matter whether w = w’ or Z Yxlzw( u). Thus, 3x, w, z, u Yxwz (u) Z Yntwfz (u), leads to a ft Y 1 Z) & 1(X ft Y ) 2) (XW fi Y 1 Z) _ Thus, We can use a symmetric also leads to a contradiction. argument q to show that (XW f+ Y I Z) & -( W ft Y I Z) 4.4 By our definition of causal irrelevance, (X ft Y I Z) + Y*,(u) = Yxlz (u) of M,,. For an arbitrary W, we consider to have the value w. By our definition of causal all submodels forced all values w. In addition, (X ft Y 1 Z) 3 Yxz (u) = Yxfz (u) for the submodel M, where W is yXzw(u) = y*jzw for irrelevance, for all submodels of M,. Since W was arbitrary, (X ft Y I of M, K&u) (Xft Z) * since = Y,rzw for all submodels Y 1 ZW) for all W. q 4.5.1 (By contradiction) Assume (X ft Y I ZW)&(X ft W I ZY)&l(X 7(X + YW ) Z) W and Y are symmetric, we will only consider Y. Consider and y’ = Yxtz(u). such that Yxz(u) Z Y,),(u). Let y = Y,,(u) implies 3x,x’,z (Yxz(u) f Y,!,(u)) V (Wxz(u> + Wxjz(u)), 74 YW I Z). Since the values of x, x’, z, u By composition, Y,,(u) = Yxzw(u> for w = W,,(u). By assumption, I’&(u) = Ynl,,( u). Also by composition, W,,(u) Wxzy (u) = Wxfzy (u) . By reversibility, Y = yx’zw and w = Wxfzy, then y must also be a solution contradiction. We can use a symmetric leads to a contradiction. = Wxry(u) since y is a solution argument 0 for y = Y,,(u). By assumption, equations to the simultaneous to show that W,, (u) to Yxfz (u). Thus y = y’, a f Wxlz (u) also D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 31 4.5.2 (By contradiction) Assume Y 1 Z). Since However, Y ( ZX) Yxtwz (u) = Yx,w~z (u), -(XW (X ft Y 1 ZW) implies Vx’, w, w’, z Y,~,,(u) (X ft Y 1 ZW) & (W f, Y / ZX) & 7(XW f, Yxwz(u) # Yx,,,,,z(~). ( W fi = Yx$fz (u). Thus, vu’x, x’, w, w’, z Yxwz (U) = = Y,t,t, (u). This contradicts 3x,x’, implies Vx, x’, z, w YxZw(u) = YxtZW(u). Furthermore, f, Y ( Z), by definition &,x’,w,w’,z thus Vx, x’, w, w’, z Y,,,(u) w,w’,z Y&.(u) # Y&Q(u). 0 4.5. Causal relevance and Lewis’ counteflactuals It is instructive version of Lewis’ Rules to compare our framework logic for counterfactual to that of Lewis (from [ 191) . sentences [ 181. We give here a ( 1) If A and A * B are theorems, (2) If (Bl&...)+C) isatheorem, so is B. thensois ((APB])...) rj (A-C) Axioms n-+A) V ((AVB) MB) tautologies. ( 1) All truth-functional (2) Act-+ A. (3) (4) (AD-+B)&(BE-+A)~(AD-+C) ((AVB) & (BE-C)). (5) A-B-A-B. (6) A&B-A-B. The statement A CT+ B stands -(B-C). V (((AVB) 0-C) - (A-C) is careful requirement the obvious to not put any restrictions as well”. Lewis beyond w’ # w. In essence, causal models with local worlds Lewis’ axioms are true for causal models and follow systems) and (for nonrecursive that gives a metric by which reversibility. that world w be no further for “In all closest worlds where A holds, B holds of closest worlds, on definitions from itself than any other define an ordering among to define what worlds are closest. As such, all of interventions from effectiveness, composition, In order to relate Lewis’ axioms to our framework, we need to translate his syntax in a causal model, into the language of causal models. We will equate Lewis ’ “world” with an instantiation of all variables and B in the statements variables “If we force a set of variables the values B”. Let A stand for a set of values XI,. . . ,x, of the variables Xl,. and let B stand for a set of values yt, . . . , y,,, of the variables Yl , . . . , Y,,. Then, such as A in U. Propositions, to the assignment of values to subsets of is to have the values A, a second set of variables will have . . ,X,, the meaning of the statement A cw B in causal models ’ the variables above, will be limited in a model. Thus, including A D-+ B = Y~x,...~,,(u) = YI & L,.&(U) = Y2 & . . . Ex,...x,,(u) = Yn & (25) 32 D. Galles, J. Pearl/Artijicial Intelligence 97 (1997) 9-43 Fig. 9. Example of the failure of reversibility Y = y holds in all closest w-worlds, yet Y + Y and W f w. in Lewis’ framework: W = w holds in all closest y-worlds, and Conversely, we need in Lewis’ notation. Let A stand for the proposition X = x, and B stand for the proposition Y = y. Then, to define what statements such as Y,(U) = y mean Y,(u) = y E A ci+ B (26) in turn. We can now examine each of Lewis’ axioms (1) (2) true. if we force a set of variables is a weaker is the same as effectiveness. Namely, Trivially This axiom X to have the value n, then the resulting value of X is x. That is, Xx(u) = x. form of reversibility, which This axiom cursive causal models. Since actions does not apply. However, under this axiom does hold. This axiom follows directly This axiom follows directly of literals, do( A V B) 3 &I( A) V do(B) from composition. from composition. in causal models are restricted the interpretation to conjunctions is relevant only for nonre- this axiom , (3) (4) composition and effectiveness follow (5) and rule of Lewis’ axiom (2). Thus, causal models do not add any restrictions is a consequence axiom above those imposed by Lewis’ framework, when we are considering When we consider nonrecursive Lewis’ For instance, Y = y may hold y-worlds reversibility and, still, Y = y may not hold in our world. A graphical is given systems, we see that reversibility framework. Lewis’ axiom framework in Fig. 9. in Lewis’ (3), while similar, is not as strong as reversibility. in all closest w-worlds, W = w may hold in all closest example violating from Lewis’ axioms. Composition is Lewis’ (l), while effectiveness statements to counterfactual recursive models. is not enforced by (5) (6) Likewise, 4.6. Why transitivity fails in causal relevance Causal transitivity is a property influence on B, and B has a causal that makes intuitive sense. If a variable A has a causal influence on C, one would think that A would have D. Galles, J. Pearl/Artijicial Intelligence 97 (1997) 9-43 33 Ul i xi P’ i Y V = {x, KY}, x, y E (0, l}, w E (0, 1,2,3} u= {h,U2}, Ul,U2 E {OJ} w=x+2*242 x = u1 y=(w> 1) Fig. 10. Counterexample to transitivity in causal irrelevance. V = {X, WI, W2, W3, W4, Y} binary U = {Ui,U2} binary x = UI WI = 7x&7u2 w2 = x & 7u2 w3 = TX&U2 W4 =X&U2 Y= (w3 & 7w1 & lW2) v (w4 & 1WI & -7w2) P(Ul) = P(u2) =0.5 Fig. Il. Transitivity fails, even when a variable is more completely controlled by its parents than in Fig. 10. the case, however, even in deterministic in Fig. 10. In this example, X is causally to Y. The in irrelevant to Y, but X is causally is that changing X can only cause a minor change on C. This is not always the causal model described a causal influence causality. Consider relevant to W, and W is causally behind intuition W, while Y only responds is deeper than variable W, we still may not be able to achieve of Fig. 11. this example to large changes this. Even when X has more complete relevant in W. However, control over the failure of transitivity the intermediate the causal model transitivity. Consider This model if x + ~2 = 1, W3 is true is the same as the model of Fig. 10 except W has now been split into to W’s four possible values. That is, WI is true if x + 1.42 = 0, if x + z.42 = 2, and W4 is true if x + ~2 = 3. variables WI,. . . , W4 to vari- . . , W4 can affect Y in any state U. However, X has no effect on Y in any WI,. . . , W4, corresponding W2 is true Now, by fixing X, we can cause any of the be false ables WI,. state u. in any given state of the world U. Likewise, each of the intermediate intermediate 34 D. Galles. J. Pearl/Artificial Intelligence 97 (1997) 9-43 4.7. Causal relevance and directed graphs 4.7.1. Causal graphs as irrelevance-maps Comparing Axioms 3.2-3.5 to path interception similar reason about graphs relevances graph G*(M) and irrelevances such that in directed graphs. Since people to Axioms 4.2-4.5, we see that causal irrelevance is quite can easily to create a graph that represents all of the causal like to create a (and machines) of a given causal model. That is, we would it would be useful (i) Each variable X in M corresponds (ii) For all subsets of nodes X*, Y*, Z* in G*(M), to exactly one node X* in G*(M), (X* +Y* 1 Z*)~.(.+.Q j (X ft Y 1 Z), and (iii) For all subsets of variables X, Y, Z in M, (X ft Y 1 Z)=+(X*-tY* In such a graph G*(M), 1 Z*)Q(M). if all directed paths from X* to Y* were intercepted by some irrelevant if a to a set Y given fixed Z, then all paths from to Y in the model M. Likewise, variables set of variables X was causally nodes in Z, then X would be causally irrelevant in X* to nodes in Y” would be intercepted by some variables in Z. is G(M), The obvious choice for G*(M) the graph associated with the causal model (ii) holds, that (X -.+Y 1 Z)ocM) ==+ Y,,(U) = Yz(u>, and thus but does not such irrelevance, (ii) and (iii) hold simultaneously. Nonetheless, we can use directed itself, as defined by Eq. ( 1). If we use G*(M) = G(M), since in Section 3.6 we showed (X f, Y 1 Z) . However, since transitivity in causal always that implications graphs for a given model M there might be no graph G*(M) always holds in path interception to validate candidate as we show below. theorems of causal then implication irrelevance, 4.7.2, Directed graphs as theorem provers Consider an oracle and returns YES if the statement holds in all directed graphs and NO otherwise. We will show that such an oracle can be used to validate or refute sentences about causal relevance. that takes in statements about path interception a language of causal First, let us consider irrelevance statements of the form simple of variables. Second, irrelevance be an implication al & a2 &. . .62 ai q literals consists the sentence * non-negated of non-negated literals. For instance, consider let the canonical form for sentences of a conjunction relevance in which the literals stand for (X ft Y I Z), where X, Y and Z are sets in the language of causal bl V b2 V . . * V bk, whose antecedent and whose consequent consists of (Xf,Y~Z)&~(Xj+Y~0)=+~(Z++Y~0). (27) is not in canonical This sentence is negated and the statement sentence is form because in the consequent ~Xf,~l~~~~~ft~I0~~~~ft~I0~. the second conjunct is negated. The canonical in the antecedent form of this (28) Any causal irrelevance sentence can be written in a unique canonical form using standard logical procedures. * A version of this sentence was chosen in [ 141 as the definition of causality. D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 35 Definition 20 (Horn component). A Horn tence S is a sentence H such that component H of a causal irrelevance sen- (i) H is in canonical (ii) the consequent (iii) H+ S. form, of H contains no disjunctions, and If a sentence S is in the canonical form at &a:! & ’ . . & ai ==+ bl V b2 V. . . V bk, then a Horn component Eq. (28) has no disjunctions irrelevance of S is any sentence of the form at & a2 &. . s &ai ==+ bj. For example, is itself a Horn component. in its consequent and, hence, statement A of the form (X ft Y 1 Z), we will consider statement A,, the graphical translation of A to be the corresponding (X + Y 1 Z)G(M). Using this convention, we can define path-interception For any causal Theorem 21 (Graphical A causal irrelevance sentence S is true for all causal models iff there exists a Horn component H of S such that Hg, the graphical translation of H, is true for all graphs. theorem verification). form of this sentence corresponding For example, consider in Eq. the sentence in Eq. (27). The canonical (28) (28)) for path and interception is itself a Horn component. The sentence is given to Eq. 0)~ ==+ (X -+ Y 1 @)o, states that if all paths from X to Y are intercepted by Z, and there are no paths from Z to Y, then there is no path from X to Y. This sentence is true for all directed graphs, so Eq. (27) is a valid theorem of causal relevance. (X + Y 1 Z)o & (Z * Y 1 in directed graphs, Next, consider transitivity, stated as (X f, Y 1 Z) =+ (a f, Y / Z) V (X f, a I Z). The Horn components of this sentence are H’: H2: (X ft Y l z> - (0 ft y I z>, (Xf,YIZ)=+(Xf,aIZ). (29) (30) Looking at each of the corresponding H; : (X * Y / Z), (a ++ Y 1 Z)o path-interception is not sentences in turn, we find that true for all directed graphs G, and * H;:(X-+YIZ)o-(X if Z intercepts from any other variable transitivity Thus, * a ( Z>o is also not true for all directed graphs G, that is, all paths from X to Y, it is not the case that either Z intercepts all paths all paths from X to any other variable. to Y or Z intercepts is not a theorem of causal relevance. Proof of Theorem 21. First, we prove that if there are no disjunctions of a canonical true for path interception in directed graphs. then the sentence form sentence, is true iff the corresponding in the consequent sentence is We will prove this by contradiction. Assume that there exists some theorem A j B, where A and B are conjunctions l A q B is not a theorem l A, ==+ B, is a theorem Since A, =+ B, is a theorem of literals such that in causal irrelevance, and in path interception in directed graphs. B, from A, using the axioms of path interception in path interception, then we must be able to generate in directed graphs. However, since 36 D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 in causal irrelevance, B is not a theorem every such generation is created. This disjunction must be used the application of the axiom of transitivity. When A I must include is used, a disjunction B,. By assumption, of any of the axioms of path use this disjunction clause. Since A, started with no negated statements, interception the disjunction with anything. Thus, generating B, from A, did not require an application of transitivity, a contradiction. B, does not contain a disjunction. Also, none of the antecedents contain disjunctions. the only way to the disjunction with a negated and none of the axioms of path of B, from A, the axiom of transitivity in the generation can be used to create negated statements, we cannot in the generation of B, is to resolve interception resolve Thus of Next, we prove If A ==+ B V C is a theorem that if a theorem A ==+ B V C is a theorem irrelevance, in causal then we must be able to irrelevance, irrelevance. Since no axiom creates to generate B V C from A we must either generate B from A and add C in causal the axioms of causal irrelevance or A =+ C is a theorem then either A =+ B is a theorem irrelevance. generate B V C from A using a disjunction, or generate C from A and add B. in causal in causal Thus, a causal that corresponds sentence irrelevance to one of the Horn components is a theorem 5. Conclusion iff there is a path-interception of the original sentence. q theorem How do scientists predict the outcome of one experiment run under totally different conditions? Such transfer of experimental from the results of other knowl- languages of in the standard that cannot easily be formalized experiments edge involves logic, physics, or probability. inferences The formalization of such inferences requires a language within which the experimen- can be represented, such that the outcome of in the design and analysis of the next exper- prevailing of experimental and in one experiment can be posed as constraint tal conditions the experiment iment. The description and manipulative sentences, it requires no effect on”, “holding Z fixed”), as distinct independent tation, and axiomatic algebra of equations, Boolean algebra, and probability observational but not manipulative of”, “conditioning characterization. conditions, sentences. in turn, that manipulative from observational phrases phrases involves both observational (e.g., “having (e.g., “being interpre- the to serve semantical including calculus, are all geared It turns out that standard algebras, on Z”), 9 be given formal notation, This paper bases the semantics of manipulative tions that we call a causal model. Unlike ordinary algebraic equations, treats every equation object attached one variable. Actions operator of replacing as an independent mathematical are treated as modalities equations. and are interpreted sentences on a set of structural equa- a causal model to one and only as the nonalgebraic ’ Philosophers, a given Z” [ 291. statisticians, and economists have often confused “holding Z constant” with “conditioning on D. Galles. J. Pearl/Artificial Intelligence 97 (1997) 9-43 31 This semantics permits us to develop an axiomatic characterization of manipulative highlights to Y in context Z”, and the differences between causal irrelevance, statements axiomatization irrelevant not affect our belief to graphical irrelevance transitivity. This affinity relevance modeling. of the form “Changing X will not affect Y if we hold Z constant”. This as in “X is causally as in “Finding X will shows a closer affinity causal definition, in cyclic graphs except theorems about causal talk and causal leads to graphical methods in part, why graphs are so prevalent irrelevance, former the deterministic complies with all of the axioms of path interception in Y, once we know Z”. The the latter. Under and explains, representation informational for proving in causal than Outside of artificial intelligence, our results have interesting framework of counterfactuals thus far, the only accepted ramifications in the fields of cau- formalization [ 33,361, which is a rather cum- and structural equation and the social sciences, are viewed with of these models has not been interpretation causal knowledge. Graphical language of statistics and epidemiology where, sation has been Rubin’s bersome models, popular as they are in econometrics suspicion by statisticians the causal adequately for expressing formalized because [ 8,461. of counterfactuals Our translation into statements about structural the structural and counterfactual equation models and approaches, bases. The soundness of effectiveness used in Rubin’s framework- and unifies 5) generalizes their conceptual and mathematical only properties of counterfactuals (Definition greatly clarifies and composition-the assures theorem that every models. The completeness further assures neous properties beyond unification permits language of causal graphs, use the graphs as inferential machinery the validity of the results. in Rubin’s to express causal knowledge in that framework of effectiveness that the structural is also a theorem of counterfactuals and composition those embodied interpretation investigators in recursive models in structural equations [9] introduces no extra- this appealing in the intuitively and be assured of framework. Most significantly, Acknowledgments This research was partially reviewer NSF grant #IRI-9420306, anonymous first draft of the paper. We also thank Joe Halpern of this paper and for noting framework. for providing insightful supported by Air Force grant #AFOSR/F496209410173, and Rockwell/Northrop Micro grant #/94-100. We thank an and extremely helpful suggestions on the on the first draft for commenting that Property 4.5.1 does not hold in Lewis’ closest-world Appendix A. Independence of composition, effectiveness, and reversibility We show that reversibility, composition, a table of counterfactual statements and effectiveness by creating such that two of the properties hold but the third are independent 38 D. Galles, J. Pearl/Art$cial Intelligence 97 (1997) 9-43 x = 2.41 V = {X, W, Y} binary w= (x & Ul) U = (~7,) binary y = Putizy( x, w, 2.41) Y P(u,) = 0.5 Fig. B.1. Counterexample to Property 2.2.3. does not. We will consider a small model, one with only two binary variables X and Y and a single value for U. A. 1. Composition and effectiveness, not reversibility x=0 xx,=0 xx,1 = 1 xy=o = 0 xy,t = 1 Y=O Yx*=O YX=l = 1 Yy,o=O Yy=I = 1 XX=o,Y=o = 0 YX=o,Y=o = 0 XX=o,Y=l = 0 XX=l,Y=o = 1 XX=l,Y=l = 1 YX=o,Y=l = 1 YX,l,YdJ = 0 YX=l,Y=l = 1 A.2. Effectiveness and reversibility, not composition x=0 xx=0 =o xx=1 = 1 xy,o=o xy=1 = 1 Y=l Yx,o = 1 YX,l = 0 Yy,=O Yy,1 = 1 XX=o,Y=o = 0 YX=o,Y=o = 0 XX=o,Y=l = 0 YX=o,Y=l = 1 XX=I,Y=o = 1 XX,l,Y=] = 1 YX=l,Y=o = 0 YX=l,Y=l = 1 A.3. Composition and reversibility, not effectiveness x=0 xx,=0 xx=1 = 0 xy,, = 0 xy=t = 0 Y=l Yx* = 1 Yx,, = 1 Yy+ = 1 Yy=t = 1 XX=o,Y=o = 0 XX=o,Y=l = 0 YX=o,Y,o = 1 Yx*,y=t = 1 XX,l,Yzcl = 0 YX=l,Y=o = 1 XX=l,Y=l = 0 YX=l,Y=l = 1 Appendix B. Counterexamples I 2.2.3 (XW + Y 1 Z)p In the causal model of Fig. B.l, we can see that (X j+ Y 1 Z)p v (X f, W 1 Z)p. ~~~ft~l0~P~~~~fr~I0~P~~~~ft~I0~P. D. G&es, J. Pearl/Artificial Intelligence 97 (1997) 9-43 39 x = Ul Ul V = {X, W Y} binary y = Parity(x, w, u,) { U = { Ut , U2) binary w = Pa~$Y(x,Y,ul) UI UZ if u2 = 0 I WA ,-y vi4 if u:! = 1 Y f/ ,. if u2 = 0 1 u1 if u2 = 1 P(Ul) = P(u2) = 0.5 Fig. 8.2. Counterexample to Property 2.2.4 In this counterexample, changing X can affect the probability of Y, and changing the probability of W, but changing X and W together cannot affect X can affect probability intervening on Y. However, X does not completely X, 1/t still has some effect on W. Controlling Ut on W. As in the counterexample Ut and W prevents X from having an effect on Y. the of Y. Since changing X affects the value of W, it makes sense to think that on X would not interfere with the effect that X has on W while intervening control W. That is, when we only intervene on the influence of the connection between to Property 2.2.2, removing both X and Y removes (XW+YIZ)p&(XYftWIZ)p=+(Xf,YIZ)pV(XftWIZ)~. 2.2.4 In Fig. B.2, we can see that 0 P(w) = P(y) = 0.5; 0 P(w 1 set(X= 1)) =P(y l P( w 1 9, y^) = 0.5 for all values of 2, j? and 0 P( y 1 2, i3) = 0.5 for all values of L?, w^. 1 set(X= 1)) =0.75; Thus, (XW f, Y I 0)~ c!k (XY f+ W 1 0)~ 8~ -((X f, Y ) 0)~ V (X f, W I @PI. actually contains This counterexample two causal models, each similar to the model of counterexample 2.2.2 (see Section 3.4, Fig. 6). In one, W is a function of X, Y, and 171, and Y is a function of U1. As for Property 2.2.2, X can affect W when Y has the same value as U2, but X has no effect on P(w) when Y is held constant. In the other, W is a function of U1, and Y is a function of X, W, and Ul. Also as in Property 2.2.2, X can affect Y when W has the same value as U1, but it has no effect on P(w) when W is fixed. U2 determines which model on only X can affect P(w) on P(w) is in effect at any given time. While simultaneously changing X and W has no effect on P(y) changing X and Y has no effect , and simultaneously intervening and P(y), . 2.3 (X ft WY ) Z)p In the causal model of Fig. B.3, (X f, YW I 0)~ & -(X In this counterexample, (X f, Y 1 ZW)p. - only act as an inhibitor of Y. When we intervene on W, then it is possible the value 1, and X can affect the probability intervene on W, and X has no effect on W. X does not have any effect on Y since P(y) = 0, and X can for Y to have of Y. Thus, X can only affect Y when we f, W I Y)P. 40 D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 V = {X, W Y} binary x = t.41 w = u2 U = {Ul , U2) binary y=(~ & (wXORu2)) P(q) = P(u2) =os Y Fig. B.3. Counterexample to Property 2.3. U2 1’ 4 i x Y \e W u2 Y’ 4 i Y x UJ W V = {X, U: Y} binary x = u1 Y = u2 U = {VI, lJ2) binary w = Putity( x, y, ll2) P(u,) = P(LQ) = 0.5 Fig. B.4. Counterexample to Property 2.4. V = {X, W, Y} binary x = Ul y = t42 U = (U1,U2) binary w = Purity( x, y, u:!) P(q) = P(u2) = 0.5 Fig. B.5. Counterexample to Property 2.5.1. 2.4 (XftYI Z)P&(X++WI In the causal model of Fig. B.4, (X f, Y ( (b)p & (X f, W 1 Y)P &7(X While changing X can affect P(w) and changing X has no effect on P(y), f, WY / 0)~. (and hence P ( y. w) ) when Y is not held fixed, the effect that X has on W. zY)~e(Xf,~l fixing Y blocks Z>P. (X f+ Y ) zW)p & (X f+ W 1 Zr>p 2.5.1 In the causal model of Fig. B.5, (X f, Y I W)p&(X - (X ++ WY 1 Z)P. f, W I Y)p&-(X f, WY I 0)~. D. Galles, J. Pearl/Art@cial Intelligence 97 (1997) 9-43 41 V = {X, u! I: Z} binary U = {VI, U2) binary x = u1 Y = u2 W = Purizy(x,y,z) z = u:! P(q) = P(u2) = 0.5 Fig. B.6. Counterexample to Property 2.5. I in which each variable in U has a single child. V={X,WY}, x,y E (0, l}, w E (C),1,2,3} U={U,,U2}, Ul,U2E {OJ} x = z41 w=x+2*r4 y = (w > 1) P(Ul = 1) = P(u2 = 1) = 0.5 Fig. B.7. Counterexample to Property 2.6. Ul 6 xt P’ W JI Y Fixing W prevents X from altering the probability of Y, and fixing Y prevents X from the of W (and hence altering probability the probability of W, but X can change of W & Y) if there is no intervention the probability on Y. Up to this point, all of the counterexamples two different children have relied on some exogenous variable since we this is not essential, in which each exogenous variable has exactly one in the model of Fig. B.5, we could replace ZJ2 with Z to get the in V. Obviously, from U having could always create similar examples child. For example, model of Fig. B.6. is required In this model, all of the exogenous variables U have exactly one child, yet Property 2.5.1 still does not hold. There is still an undirected cycle in the underlying causal graph, which are all true for all are true causal models whose causal graphs are trees. In addition, Properties 2.2.1-2.5.2 for all causal models whose causal graphs are polytrees. Property 2.6, as we will see now, is not always for Property 2.5.1 to be false. Properties 2.2.1-2.6 its causal graph to be a polytree. true, even when we restrict 2.6 (X~,YIZ)~~(U~~YIZ)~V(X~,QIZ)P~‘~~XUZUY. In the causal model of Fig. B.7, 42 D. Galles, J. Pearl/Art@cial Intelligence 97 (1997) 9-43 X can only cause a minor change to affect Y. Thus, X can affect W, and W can affect Y, but X has no effect on W. Even if we restrict all variables W could be split into four binary variables WI,. transitivity will not hold. For this counterexample, . . , W4, with fw, = 1(x V uz), in W, while a large change in W is required to be binary, fw, = TX & u2, fw, = x & u2, and fy = w3 V ~4. Section 4.6 elaborates x & 342, counterexample. fw, References = this [ I] A. Balke and J. Pearl, Counterfactual in: R. Lopez de Mantaras and D. Poole, eds., Proceedings 10th Conference on Uncertainty in Artificial Intelligence (Morgan Kaufmann, San Francisco, CA, 1994) 46-54. computation methods, bounds and applications, probabilities: [2] A. Balke and J. Pearl, Counterfactuals and policy analysis in structural models, in: P. Besnard and S. Hanks, eds., Proceedings 11th Conference on Uncertainty in Artificial Intelligence (Morgan Kaufmann, San Francisco, CA, 1995) 11-18. [3] N. Cartwright, Nature’s Capacities and Their Measurement (Clarendon Press, Oxford, UK, 1989). [4] A.P. Dawid, Conditional [ 51 E. tills, Probabilistic Causality (Cambridge University Press, Cambridge, UK, 199 1) . [6] R.E. Fikes and N.J. Nilsson, STRIPS: a new approach solving, Artificial Intelligence 3 ( 1972) 251-284. .I. Roy. Statist. Sot. Ser. A 41 (1979) to the application of theorem proving independence in statistical theory, to problem l-31. [7] EM. Fisher, A correspondence principle for simultaneous equation models, Econometrica 38 ( 1970) 73-92. [ 81 D. Freedman, As others see us: a case study in path analysis (with discussion), J. Educational Statist. 12 (1987) 101-223. 19 ] D. Galles, Causal models: a formalism for modeling actions and counterfactuals, Ph.D. Thesis, University of California, Los Angeles, CA ( 1997). [ 101 D. Geiger, T.S. Verma and J. Pearl, Identifying (John Wiley, Sussex, UK, 1990) 507-534. independence in Bayesian networks, in: Networks, Vol. 20 [ 111 A.S. Goldberger, Models of substance [comment on N. Wermuth, “On block-recursive linear regression equations”], Brazilian J. Probab. Statist. 6 ( 1992) l-56. [ 121 I.J. Good, A causal calculus, Philos. Sci. 11 (1961) 305-318. 113 ] J.Y. Halpem, Axiomatizing [ 141 D. Heckerman Ithaca, NY ( 1997). in: F! Besnard and S. Hanks, eds., Proceedings 11th Conference on Uncertainty in Artificial Intelligence (Morgan Kaufmann, San Francisco, CA, 1995) 262-273. and R. Shachter, A definition and graphical causal structures, Unpublished report, Cornell University, representation of causality, [ 151 P.W. Holland, Statistics and causal inference (with discussion), J. Amer. Statist. Assoc. 81 (396) ( 1986) 945-970. [ 161 E. Learner, Vector autoregression Policy 22 (1985) 255-304. for causal inference?, Carnegie-Rochester Conference Series on Public [ 171 D. Lewis, Causation, .I, Philos. 70 (1973) 556-567. [ 181 D. Lewis, Counter$actuals (Harvard University Press, Cambridge, MA, 1973). [ 191 D. Lewis, Counterfactuals and comparative in: W.L. Harper, R. Stalnaker and G. Pearce, eds., Ifs (Reidel, Dordrecht, The Netherlands, possibility, 1981). [20] CF. Manski, Nonparametric bounds on treatment effects, Amer. Economic Review, Papers and Proceedings 80 (1990) 319-323. [ 2 11 J.S. Meditch, Stochastic Optimul Linear Estimation and Control (McGraw-Hill, New York, 1969). [ 221 A. Paz and J. Pearl, Axiomatic characterization of directed graphs, Tech. Rept. R-234, Computer Science Department, University of California, Los Angeles, CA ( 1994). 1231 A. Paz, J. Pearl and S. Ur, A new characterization of graphs based on interception relations, J. Graph Theory 22 (2) (1996) 125-136. D. Galles, J. Pearl/Artificial Intelligence 97 (1997) 9-43 43 1241 .I. Pearl and A. Paz, Graphoids: a graph-based in: B. du 1987) relations, Amsterdam, about relevance (North-Holland, logic for reasoning Boulay and L. Steels, eds., Advances in Arftjicial Intelligence-II 357-363. J. Pearl and T. Verma, A theory of inferred causation, in: J.A. Allen, R. Fikes and E. Sandewall, eds., Principles of Knowledge Representation and Reasoning: Proceedings 2nd International Conference also in: D. Prawitz, B. Skyrms and D. Westertahl, (Morgan Kaufmann, San Mateo, CA, 1991) 441-452; eds., Logic, Methodology and Philosophy of Science IX (Elsevier, Amsterdam, J. Pearl, Probabilisfic Reasoning in fnfelligent Sysfems (Morgan Kaufmann, San Mateo, CA, 1988). J. Pearl, Graphical models, causality and intervention, Statist. Sci. 8 (3) J. Pearl, A probabilistic in: R. Lopez de Mantaras and D. Poole, eds., Proceedings 10th Conference on Uncertainty in Artificial Intelligence (Morgan Kaufmann, San Francisco, CA, 1994) 454-462. J. Pearl, Causal diagrams J. Pearl, On the testability of causal models with latent and instrumental variables, Hanks, eds., Proceedings 11 th Conference on Uncertainty in Artificial Intelligence (Morgan Kaufmann, San Francisco, CA, 1995) 435-443. (with discussion), Biometrika 82 (4) ( 1995) 669-709. in: D. Besnard and S. calculus of actions, (1993) 266-273. 1994) 789-811. for empirical research 1251 1261 WI 1281 [291 1301 ed., Proceedings 6th Conference (Morgan Kaufmann, San Francisco, 131 I J. Pearl, Causation, action and counterfactuals, in: Y. Shoham, Theoretical Aspects of Reasoning about Knowledge (TARK 1996) CA, 1996) 51-73. 1 J. Pearl, Structural and probabilistic 132 causality, Psychology of Learning and Motivation 34 ( 1996) 393- [33 134 435. J. Robins, A new approach applications J. Robins, Addendum periods-application to causal inference in mortality studies with a sustained exposure period- to control of the healthy workers survivor effect, Marh. Modeling 7 ( 1986) 1393-5 12. to “A new approach in mortality studies with sustained exposure to control of the healthy worker survivor effect”, Comput. Math. Appl. 14 (1987) to causal inference 923-45. [ 35 1 P. Rosenbaum and D. Rubin, The central role of propensity score in observational studies for causal effects, Riometrika 70 (1983) 41-55. [ 361 D.B. Rubin, Estimating causal effects of treatments in randomized and nonrandomized studies, J. Educational Psychology 66 ( 1974) 688701. [ 37 1 W. Salmon, Scientijc Explanation and the Causal Structure of the World (Princeton University Press, Princeton, NJ, 1984). [38 ] L.J. Savage, The Foundations of Statistics, Vol. 1 (John Wiley, New York, 1954). (391 G. Shafer, The Art of Causal Conjecture (MIT Press, Cambridge, MA, 1996). [40] M.E. Sobel, Effect analysis and causation in linear structural equation models, Psychometrika 55 ( 1990) 495-515. [ 411 P. Spirtes, C. Glymour and R. Schienes, Causation, Prediction and Search (Springer, New York, 1993) ( 42 1 W. Spohn, Stochastic J. Philos. Logic 9 ( 1980) and shieldability, independence, independence causal 73-99. I43 I R.H. Strom and O.A. Wold, Recursive versus nonrecursivesystems: an attempt at synthesis, Econometrica 28 (1960) 417-427. [ 44 I M. Studeny, Conditional independence relations have no complete characterization, in: Information Theory Statistical Decision Functions, Random: Trans. of the 11th Prague Conference, 1990 (Kluwer, Dordrecht, The Netherlands, 1992) 377-396. [ 45 ] P. Suppes, A Probabilistic Theory of Causation (North-Holland, Amsterdam, 146 1 N. Wermuth, On block-recursive linear regression equations, Brazilian J. Probab. Statist. 6 ( 1992) 1970) l-56, 