Artificial Intelligence 175 (2011) 1153–1179Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintDecision making with multiple objectives using GAI networksC. Gonzales, P. Perny∗, J.Ph. DubusLIP6 – Université Pierre et Marie Curie, case 169, 4 place jussieu, 75005 Paris, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 28 February 2009Received in revised form 20 July 2010Accepted 15 September 2010Available online 2 December 2010Keywords:Graphical modelsGAI decomposable utilityPreference representationMultiobjective optimizationMultiagent decision makingCompromise searchFairness1. IntroductionThis paper deals with preference representation on combinatorial domains and preference-based recommendation in the context of multicriteria or multiagent decision making. Thealternatives of the decision problem are seen as elements of a product set of attributesand preferences over solutions are represented by generalized additive decomposable (GAI)utility functions modeling individual preferences or criteria. Thanks to decomposability,utility vectors attached to solutions can be compiled into a graphical structure closelyrelated to junction trees, the so-called GAI network. Using this structure, we presentpreference-based search algorithms for multicriteria or multiagent decision making.Although such models are often non-decomposable over attributes, we actually showthat GAI networks are still useful to determine the most preferred alternatives providedpreferences are compatible with Pareto dominance. We first present two algorithms for thedetermination of Pareto-optimal elements. Then the second of these algorithms is adaptedso as to directly focus on the preferred solutions. We also provide results of numerical testsshowing the practical efficiency of our procedures in various contexts such as compromisesearch and fair optimization in multicriteria or multiagent problems.© 2010 Elsevier B.V. All rights reserved.The complexity of decision problems in organizations, the importance of the issues raised and the increasing need toexplain or justify any decision has led decision makers to seek a scientific support in the preparation of their decisions. Formany years, rational decision making was understood as solving a single-objective optimization problem, the optimal deci-sion being implicitly defined as a feasible solution minimizing a cost function under some technical constraints. However,the practice of decision making in organizations has shown the limits of such formulations. First, there is some diversityand subjectivity in human preferences that requires distinguishing between the objective description of the alternatives of achoice problem and their value as perceived by individuals. In decision theory, alternatives are often seen as multiattributeitems characterized by a tuple in a product set of attributes domains, the preferences of each individual being encoded by autility function defined on the multiattribute space measuring the relative attractiveness of each tuple. Hence the objectivesof individuals take the form of multiattribute utility functions to be maximized. Typically, in a multiagent decision problem,we have to deal with several such utility functions that must be optimized simultaneously. Since individual utilities are gen-erally not commensurate, constructing an overall utility function gathering all relevant aspects is not always possible. Hencethe problem does not reduce to a classical single-objective optimization task; we have to solve a multiobjective problem.Moreover, even when there is a single decision maker, several points of views may be considered in the preferenceanalysis, leading to the definition of several criteria. Rationality in decision making is generally not only a matter of costsreduction. In practice, other significant aspects that are not reducible to costs must be included in the analysis; the outcomes* Corresponding author.E-mail addresses: kaveh@river-valley.com (C. Gonzales), patrice.perny@lip6.fr (P. Perny), cvr@river-valley.com (J.Ph. Dubus).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.11.0201154C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179of alternatives must be thought in a multidimensional space. This is the case in the elaboration of public policies wheredifferent aspects such as ecology and environment, education, health, security, public acceptability are considered in theevaluation process. This is also the case for individual decision of consumers. For example, when choosing a new car for afamily, an individual will look at the cost, but will also consider several multiattribute utility functions concerning security inthe car (brake system, airbags, . . . ), velocity (speed, acceleration, . . . ), space (boot size, . . . ), environmental aspects (pollution)and aesthetics (color, shape, brand, . . . ). All these observations have motivated the emergence of multicriteria methodologiesfor preference modeling and human decision support [1–4], an entire stream of research that steadily developed for fortyyears.As for human decision making, automated decision making in complex environment requires optimization proceduresinvolving multiple objectives. This is the case when computers are used for planning actions of autonomous agents or for or-ganizing the workflow in production chains. Various other examples can be mentioned such as web search [5], e-commerceand resource allocation problems. In many of them, however, a decision is actually characterized by a combination of lo-cal decisions, thus providing the set of alternatives with a combinatorial structure. This explains the growing interest formultiobjective combinatorial optimization. Besides the explicit introduction of several possibly conflicting objectives in theevaluation process, the necessity of exploring large size solution spaces is an additional source of complexity. This hasmotivated the development in the AI community of preference representation languages aiming at simplifying preferencehandling and decision making on combinatorial domains.As far as utility functions are concerned, the works on compact representation aim at exploiting preference independenceamong some attributes so as to decompose the utility of a tuple into a sum of smaller utility factors. Different decompositionmodels of utilities have been developed to model preferences. The most widely used assumes a special kind of independenceamong attributes called “mutual preferential independence”. It ensures that preferences are representable by an additivelydecomposable utility [6,7]. Such decomposability makes both the elicitation process and the query optimizations very fastand simple. However, in practice, preferential independence may fail to hold as it rules out any interaction among attributes.Generalizations have thus been proposed in the literature to significantly increase the descriptive power of additive utilities.Among them, multilinear utilities [2] and GAI (generalized additive independence) decompositions [8,9] allow quite generalinteractions between attributes [7] while preserving some decomposability. The latter has been used to endow CP nets withutilities (UCP nets) both under uncertainty [10] and under certainty [11]. GAI decomposable utilities can be compiled intographical structures closely related to junction trees, the so-called GAI networks. They can be exploited to perform classicaloptimization tasks (e.g. find a tuple with maximal utility) using a simple collect/distribute scheme essentially similar tothat used in the Bayes net community or to variable elimination algorithms in CSP [12–15]. In order to extend the useof GAI nets to multiobjective optimization tasks, we investigate the potential of GAI models for representing and solvingmultiobjective optimization problems.As soon as multiple criteria or utility functions are considered in the evaluation of a solution, the notion of optimality isnot straightforward. Among the various optimality criteria, the concept of Pareto optimality or efficiency is the most widelyused. A solution is said to be Pareto-optimal or efficient if it cannot be improved on one criterion without being depreciatedon another one. Pareto optimality is natural because it does not require any information about the relative importance ofcriteria and can be used as a preliminary filter to circumscribe the set of reasonable solutions in multiobjective problems.However, in combinatorial optimization problems, the complete enumeration of Pareto-optimal solutions is often infeasiblein practice [16–18]. For this reason, in many real applications, people facing such complexity resort to artificial simplifica-tions of the problem, either by focusing on the most important criterion (as in route planning assistants), or by performinga prior linear aggregation of the criteria to get a single objective version of the problem, or by generating samples of goodsolutions using heuristics, which in any case does not provide formal guarantees on the quality of the solutions.In this paper, we assume that each objective is represented by a GAI decomposable utility function defined on themultiattribute space describing items. In Section 2, after recalling basic definitions related to GAI nets, we show how theymake it possible to represent vector-valued utility functions in a compact form, thus facilitating preference handling inmultiobjective decision-making problems. In Section 3, we present two exact algorithms exploiting the structure of theGAI net for the determination of Pareto-optimal elements. In Section 4 we propose a refinement of the second algorithmaiming at focusing the search on specific compromise solutions within the Pareto set. We provide exact algorithms forpreference-based search with various preference models. The potential of this approach is illustrated in the context of fairmultiagent optimization or in the context of compromise search in multicriteria optimization. Finally, in Section 5, wepresent numerical tests showing the practical feasibility of the proposed approach on various instances of multiobjectivecombinatorial problems.2. Multidimensional GAI netsWe assume that alternatives are characterized by n attributes x1, . . . , xn taking their values in finite domains X1, . . . , Xnrespectively. Hence alternatives can be seen as elements of the product set of these domains X = X1 × · · · × Xn. In thesequel, N = {1, . . . , n} will denote the set of all the attributes’ indices. By abuse of notation, for any set Y ⊆ N, XY will referi∈Y Xi , and xY will refer to the projection of x ∈ X on XY, that is,to the Cartesian product of the Xi , i ∈ Y, i.e., XY =the tuple formed by the xi , i ∈ Y. We also consider a binary relation (cid:2) over X (actually this is a weak order). Essentially,x (cid:2) y means that x is at least as good as y. Symbol (cid:5) refers to the asymmetric part of (cid:2) and ∼ to the symmetric one.(cid:2)C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791155Under mild hypotheses [19], it can be shown that (cid:2) is representable by a utility function, i.e., by a function u : X (cid:7)→ Z+s.t. x (cid:2) y ⇔ u(x) (cid:3) u( y) for all x, y ∈ X . Actually, all the algorithms proposed in this paper also work with real-valuedutility functions. Our assumption that utilities are integer-valued is only exploited in the proofs of complexity results. Aspreferences are specific to each individual, utilities must be elicited for each agent, which is often impossible due to thecombinatorial nature of X . Moreover, in a recommendation system with multiple regular users, storing explicitly for eachuser the utility of every element of X is prohibitive. Fortunately, agent’s preferences usually have an underlying structureinduced by independencies among attributes that substantially decreases the elicitation effort and the memory needed tostore preferences. The simplest case [6] is obtained when preferences over X = X1 × · · · × Xn are representable by ani=1 ui(xi) for any x = (x1, . . . , xn) ∈ X . This model only requires to store ui(xi) for any xi ∈ Xi , i ∈ N,additive utility u(x) =and it can be effortlessly elicited. However, such a decomposition is not always convenient because it inevitably rules outany interaction between attributes, which is far from being realistic. When preferences are complex, more elaborate modelsare thus needed. Some generalizations of additive utilities have thus been investigated. For instance utility independence onevery Xi leads to a more sophisticated form called a multilinear utility [7]. Such utilities are more general than additive onesbut still cannot cope with many kinds of interactions among attributes. To increase the descriptive power of such models,GAI (generalized additive independence) decompositions have been introduced by [20], that allow more general interactionsbetween attributes [7,9,8,21] while still preserving some decomposability.(cid:3)n2.1. GAI models and GAI netsGAI decomposition is a generalization of the additive decomposition in which subutilities ui are allowed to be definedover overlapping factors. As such, they include additive and multilinear decompositions as special cases. They can be moreformally defined as follows:Definition 1. Let C1, . . . , Ck be subsets of N such that N =w.r.t. the XCi iff there exist functions ui : XCi(cid:4)k(cid:7)→ Z+ such that:i=1 Ci . A utility u(·) representing (cid:2) over X is GAI-decomposableu(x1, . . . , xn) =k(cid:5)i=1ui(xCi ),for all x = (x1, . . . , xn) ∈ X .Example 1. Utility function u(a, b, c, d, e, f , g) = u1(a, b) + u2(b, c, d) + u3(c, e) + u4(b, d, f ) + u5(b, g) defined on A × B ×= B × D × F andC × D × E × F × G is a GAI-decomposable utility, with XC1XC5= B × C × D, XC3= A × B, XC2= C × E, XC4= B × G.GAI decompositions can be represented by graphical structures we call GAI networks [9,21] which are essentially similarto junction graphs used in the Bayesian network literature [22,23]:Definition 2. Let u(x1, . . . , xn) =rected graph G = (C, E) satisfying the following three properties:i=1 ui(xCi ) be a GAI utility function over X . A GAI network representing u(·) is an undi-(cid:3)k}. Vertices XCi are called cliques. To each vertex XCi is associated the corresponding subu-Property 1: C = { XC1 , . . . , XCktility factor ui from the utility function u;Property 2: ( XCi , XC j ) ∈ E ⇒ Ci ∩ C j (cid:12)= ∅. Edges ( XCi , XC j ) are labeled by XSi j , where Si j = Ci ∩ C j . XSi jseparator. Separator XSi j thus corresponds to the attributes that the two cliques XCi and XC j have in common;Property 3: for all XCi , XC j such that Ci ∩ C j = Si j (cid:12)= ∅, there exists a path between XCi and XC j in G such that for everyclique XCh in this path Si j ⊆ Ch (running intersection property).is called aIn the rest of the paper, the XCi will always denote cliques of a GAI network and the XSi j will always denote the separator,i.e., the intersection, between cliques XCi and XC j . By abuse of notation, XSii will refer to clique XCi itself. Cliques are usuallydrawn as ellipses and separators as rectangles. For any GAI decomposition, by Definition 2, the cliques of the GAI networkshould be the sets of attributes of the subutilities. The edges in the network represent the intersections between subsets ofattributes. As the intersections are commutative, the GAI network is an undirected graph. Note that this contrasts with UCPnets, where the relationships between vertices in the network correspond to conditional dependencies, thus justifying theuse of directed graphs for UCP nets. For any clique XCi , Adj( XCi ) will refer to the set of cliques adjacent to XCi .In this paper, we shall only be interested in GAI trees. As we shall see, this is not restrictive as general GAI networkscan always be compiled into GAI trees. The set of edges of a GAI network can be determined by any algorithm preservingthe running intersection property (see the Bayesian network literature on this matter [23]). Fig. 1 shows one possible GAInetwork representing the GAI utility of Example 1. Note that this network is not the unique representation of the utilityfunction.1156C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Fig. 1. A GAI tree.Fig. 2. The GAI trees representing u1 and u2.2.2. Handling multiple objectivesFig. 3. The Markov networks G 1 of u1 and G 2 of u2.Consider now a finite set of objectives M = {1, . . . , m} and assume that any solution x ∈ X is characterized by a utility+ where ui : X → Z+ is the ith utility function. This function measures the relative utility ofvector (u1(x), . . . , um(x)) ∈ Zmalternatives with respect to the ith point of view (criterion or agent) considered in the problem. Hence, the comparisonof alternatives reduces to that of their utility vectors, i.e., instead of comparing alternatives x and y through the numbersassigned to them by utility u as in the preceding subsection, we now compare them through vectors (u1(x), . . . , um(x)) and(u1( y), . . . , um( y)).The ui are functions X → Z+. Hence, separately, they can be considered as single utility functions. Assuming that eachobjective corresponds to a given agent, each ui corresponds to the utility function representing the agent’s preferences andvectors (u1(x), . . . , um(x)) correspond to the utility of the group of agents. Now, if a ui is the utility function of a givenagent, by the preceding subsection, it may be GAI decomposable. Thus, assume that all the ui are decomposable accordingto the same GAI net given in Fig. 1. Then, for any i ∈ M,2(b, c, d) + uiui(a, b, c, d, e, f , g) = ui4(b, d, f ) + ui1(a, b) + ui3(c, e) + ui5(b, g).Note that, even if the values of the uij differ from one agent to another, all these utilities can be stored in the GAI net of2, i ∈ M, in clique BC D, and so on. Hence1, i ∈ M, in clique A B, store all functions uiFig. 1 as follows: store all functions uithe GAI networks described in Section 2.1 can be easily adapted to the multiobjective case. The key property that enablesthis generalization to the multiobjective case is the fact that all the subutilities uij are defined on attribute sets (here XC j )1 are defined on A × B and can thus be stored in any cliqueincluded in at least one clique of the GAI net. For instance, the uicontaining both attributes A and B (here clique A B is the only possible choice).Let us now consider the case where the agents have preferences that are not decomposable according to the same GAInetwork. For simplicity, we will illustrate our point for the two agent/objective case: m = 2. So suppose that u1 and u2 canbe decomposed as follows:u1(a, b, c, d, e, f , g) = u1u2(a, b, c, d, e, f , g) = u21(a, b) + u11(a, c) + u22(b, g) + u12(b) + u23(d, g) + u14(c, d, f ) + u13(d, g) + u24(d, e, f ) + u25(e),5(c, e).(1)These decompositions correspond to the GAI networks of Fig. 2. Note that none of these trees can be used to store bothu1 and u2, the left graph being unable to store u21(a, b). We thus needto construct another GAI tree that can contain both u1 and u2. To construct this new GAI network, we will first createanother graph per ui , called a Markov network. In this graph, each node corresponds to an attribute Xi , and two nodes(cid:7)→ Z+ such that i, j ∈ Ch. In other words,Xi, X j are connected by an edge if and only if there exists a subutility uh : XChthe set of attributes over which uh is defined contains both Xi and X j . Hence, in the Markov network, to each subutility uhcorresponds a clique (a complete subgraph). Fig. 3 displays the Markov networks of u1 and u2 as described in Eq. (1).4(d, e, f ), and the right one being unable to store u1Now, create the union of both graphs, i.e., the Markov network containing an edge between two nodes if and only ifG 1 and/or G 2 contains the same edge. The union of G 1 and G 2 is represented in Fig. 4(a). Next, this graph is triangulatedC. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791157Fig. 4. The creation of the GAI net for (u1, u2).using any triangulation technique [24–26]. Finally, the triangulated graph is mapped into a GAI network: each maximalcomplete subgraph corresponds to a clique of the GAI network. In the latter, edges are added by any algorithm preservingthe running intersection property [23]. In [27], Rose guarantees that whenever the cliques of a GAI net correspond to themaximal complete subgraphs of a triangulated Markov network, then the GAI net is a tree.Hence, given a set u1, . . . , um of utilities, each one having its own GAI decomposition over X , a global GAI tree can(cid:7)→ Z+, i ∈ {1, . . . , m}, in each+ suchbe constructed to store all these utilities. In the sequel, instead of storing m utilities uiclique XC j , we chose an alternative but equivalent representation: we just store one vector-valued utility u j : XC jthat u j(xC j ) = (u1j (xC j ), . . . , umj (xC j )).j : XC j(cid:7)→ Zm3. Pareto search3.1. Problem formulationThe set of all utility vectors attached to solutions in X is denoted by U. We recall now some definitions related todominance and optimality in multiobjective optimization.Definition 3. The weak Pareto dominance relation is defined on utility vectors of Zm+ as: u (cid:2)P v ⇔ [∀i ∈ M, ui (cid:3) v i].Definition 4. The Pareto dominance relation (cid:5)P is defined as the asymmetric part of (cid:2)P : u (cid:5)P v ⇔ [u (cid:2)P v andnot(v (cid:2)P u)].Definition 5. Any utility vector u ∈ U is said to be non-dominated in U (or Pareto-optimal) if (cid:2)v ∈ U such that v (cid:5)P u. Theset of non-dominated vectors in U is denoted ND(U) and is referred to as the “Pareto set”.The problem of determining the Pareto set in X can be stated as follows:Pareto-optimal elements (PO)Input: a product set of finite domains X = X1 ×· · ·× Xn (n finite), m GAI utility functions ui : X → Z+, i = 1, . . . , m (m finite),Goal: determine the entire set of non-dominated vectors in U, and for each utility vector u ∈ ND(U) a corresponding tuplexu ∈ X .This problem is generally intractable on large size instances. Even when m = 2, it may happen that the size of the Paretoset grows exponentially with the number of attributes, as shown by the following example:Example 2. Consider an instance of PO with two objectives (m = 2) on a set X =j = 1, . . . , n.Assume that the objectives are additive utility functions defined, for any Boolean vector x = (x1, . . . , xn) ∈ X , by ui(x) =(cid:3)j (x j) = 2 j−1(1 − x j).j(x j), i = 1, 2, where uinj=1 ui(cid:3)(cid:3)j=1 2 j−1 =Then for all x ∈ {0, 1}n, u1(x) =nn2n − 1. So there exists 2n different Boolean vectors in X , with distinct images in the utility space, all being located on thesame line characterized by equation u1 + u2 = 2n − 1. This line is orthogonal to vector (1, 1) which proves that all thesevectors are Pareto-optimal. Here ND(U) = U.j=1 2 j−1(1 − x j) and therefore u1(x) + u2(x) =j is a marginal utility function defined on X j by u1j=1 X j , where X j = {0, 1},j=1 2 j−1x j and u2(x) =j (x j) = 2 j−1x j and u2(cid:3)n(cid:2)nAlthough pathological, this example shows that the determination of the entire Pareto set may induce prohibitive runtimes in practice on large size instances with two criteria or more. Numerical tests presented in Section 5 will confirm thispoint. We establish now a complexity result concerning problem PO (proofs are given in Appendix A).Proposition 1. As soon as | Xi| (cid:3) 2, i ∈ N, and m (cid:3) 2, deciding whether there exists a tuple in X the utility vector of which weaklyPareto dominates a given utility vector u ∈ Zm+ is a NP-complete decision problem (referred to as problem P u in the sequel).1158C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11793.2. Multiobjective optimization algorithmsDespite the worst case complexity of problem PO, we may expect to solve real instances of reasonable size in admissibletimes. To this end, we introduce below solution algorithms for PO based on message propagation schemes within theGAI network. For clarity reasons, we first present a variable elimination PO algorithm that processes all the vectors of agiven clique before removing it from the GAI network. In the next subsections, we will consider best-first variations of thisalgorithm which will be more efficient for preference-based search.3.2.1. A variable elimination algorithmThe algorithm described below is a direct application of variable elimination to determine the Pareto set. Its principlehas already been used for CSP in [28]. The algorithm extensively relies on the following proposition and its corollaries:Proposition 2. Let (D, E) be a partition of N. Assume that utility u : X (cid:7)→ Zmu1 : XD (cid:7)→ Zm+ and u2 : XE (cid:7)→ Zm(cid:6)(cid:7)+ (here, + unambiguously refers to the pointwise addition over vectors). Then:(cid:8)(cid:9)(cid:8)(cid:9)(cid:6)(cid:7)+ is additively decomposable as u = u1 + u2, with(cid:4) NDu2(xE), xE ∈ XE,(2)ND(U) ⊆ NDu1(xD), xD ∈ XDwhere, for any sets V, W of vectors of Zm+, V (cid:4) W is defined as V (cid:4) W = {v + w, v ∈ V, w ∈ W}.In other words, undominated utility vectors of U can only result from the addition of one undominated utility vectorfrom subset XD and one undominated utility vector from subset XE. For instance, if u : A × B (cid:7)→ Z+ is decomposable asu1( A) + u2(B) where u1 and u2 are defined as:u2 = b1u1 = a1a2(3, 4) (4, 2) (2, 3)a3b2(3, 5) (6, 3) (3, 3)b3Then ND({u1(ai)}) = {u1(a1) = (3, 4), u1(a2) = (4, 2)} and ND({u2(bi)}) = {u2(b1) = (3, 5), u2(b2) = (6, 3)} which, composedwith operator (cid:4), produce the following set:(cid:7)(cid:8)u(a1, b1) = (6, 9), u(a1, b2) = (9, 7), u(a2, b1) = (7, 7), u(a2, b2) = (10, 5).Therefore ND(U) = {u(a1, b1), u(a1, b2), u(a2, b2)}. Note that no Pareto element involves u1(a3) or u2(b3), which are domi-nated in ND({u1(ai)}) and ND({u2(bi)}) respectively.As a consequence, if u is additively decomposable, an efficient procedure to determine ND(U) can be to first determineindependently ND({u(xD), xD ∈ XD}) and ND({u(xE), xE ∈ XE}), then sum-up all these vectors, and finally keep only theundominated resulting vectors.Corollary 1. Let G be a GAI tree with only two cliques XC1 and XC2 and their separator XS12 . Let D1 = C1\S12 and D2 = C2\S12, i.e., theDi are the indices of the attributes that appear in Ci but not in C3−i (or in other words, they appear only on one side of the separator).Then:(cid:10)(cid:6)(cid:6)(cid:7)ND(U) ⊆NDu1(xD1 , xS12 ), xD1xS12∈ XS12(cid:8)(cid:9)(cid:6)(cid:7)∈ XD1(cid:4) NDu2(xD2 , xS12 ), xD2∈ XD2(cid:8)(cid:9)(cid:9).In other words, for each fixed value xS12 of XS12 , if a utility vector u1( yD1 , xS12 ) is Pareto dominated by another vectoru1(xD1 , xS12 ) defined for the same value of the separator, no combination of u1( yD1 , xS12 ) with another vector u2(xD2 , xS12 )can result in a vector of ND(U). Hence, to determine ND(U), first determine the undominated vectors of type u1(xD1 , xS12 )and u2(xD2 , xS12 ) and sum them up for any fixed value xS12 , then keep only those that are undominated.Corollary 2. Let G = (C, E) be any GAI network, with C = { XC1 , . . . , XCk{ XCir+1} be the sets of cliques on each side of separator XSi j and let D =, . . . , XCik}, and let XSi j be any separator. Let { XCi1\Si j and E =(cid:13)(cid:14)(cid:14)(cid:4)kt=r+1 Cit(cid:4)rt=1 Cit, . . . , XCir} and\Si j . Then:ND(U) ⊆(cid:10)xSi j∈ XSi j(cid:11)(cid:11)(cid:12)NDr(cid:5)t=1ut(xCit), xD ∈ XD(cid:4) NDut(xCit), xE ∈ XE.(cid:13)(cid:14)(cid:11)(cid:12)k(cid:5)t=r+1In other words, to determine ND(U), it is sufficient to select any separator, then compute for each fixed value xSi j ofthis separator the undominated utility vectors on each side of the separator and sum-up all these vectors. Finally gather allthese vectors for all the values xSi j and keep only the undominated ones.Corollary 2 can now be exploited recursively to compute the Pareto set over U: consider the GAI network of Fig. 5,in which subutility tables are displayed next to their corresponding clique. The overall utility function u over A × B ×C × D × E × F × G is thus decomposable as: u1( A, B) + u2(C, D) + u3( A, C, E) + u4(C, E, F ) + u5(E, G). Using Corollary 2C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791159Fig. 5. The subutility tables of our example.Fig. 6. The messages Mi .(cid:4)a∈ Awith separator A, we can conclude that vectors in ND(U) can only result from the sum of vectors u2, u3, . . . , to vectorsu1( A, B) that are undominated for fixed values of A. Send the latter as message M A on separator A (see Fig. 6). Similarly,by Corollary 2 with separator C (resp. E), only vectors u2(C, D) (resp. u5(E, G)) that are undominated for fixed valuesof C (resp. E) can lead to vectors in ND(U). Send these vectors as message MC on separator C and message ME onseparator E respectively. Now, apply Corollary 2 with separator C E: vectors in ND(U) can only derive from undominatedvectors u1 + u2 + u3 for fixed values of C E. But we already know that vectors u1 (resp. u2) that are dominated for fixedvalues of A (resp. C ) cannot be part of a vector in ND(U). As a consequence, the undominated vectors u1 + u2 + u3 for fixedvalues of C E can be determined by first computing all the possible vectors u1 + u2 + u3 with vectors u1 and u2 restrictedto M A and MC respectively, and, then, keeping for each value of C E the undominated ones. In other words, we shouldcompute MC E (c, e) = ND(M A(a) (cid:4) {u2(a, c, e)} (cid:4) MC (c)) for every c, e ∈ C × E. This results in an overall separator’smessage MC E = {MC E (c, e): c, e ∈ C × E}. Now there just remains to combine the vectors of MC E , ME and u4(C, E, F )MC E (c, e) (cid:4) u4(c, e, f ) (cid:4) ME (e)).and keep the undominated ones: these are the Pareto set ND(U) = ND(Indeed, this combination corresponds to the combination of all the possible vectors ui except those that are known not tobe part of ND(U). In the end, ND(U) = {u(a1b1c1d1e2 f 2 g2) = (15, 25); u(a1b2c1d1e2 f 2 g2) = (22, 22); u(a1b2c1d2e2 f 2 g2) =(24, 14); u(a2b2c1d1e2 f 2 g2) = (16, 24)}.Note the gain resulting from the application of Corollary 2: for instance, for computing message MC E , we needed only40 additions (16 additions to compute M A (cid:4) MC , out of which 4 vectors were removed because they were dominated,and then 24 additions to combine the 12 remaining vectors with u3) instead of 72 additions if we had computed u1 + u2 +MC E (c, e) (cid:4) u4(c, e, f ) (cid:4) ME (e)) required 8u3 over A × B × C × D × E. Similarly, computing ND(U) = ND(additions to compute u4(c, e, f ) (cid:4) ME (e), and then 30 additions to compute the addition of the result with message MC E .Overall, we computed 78 additions instead of the 298 needed if we had computed u over the whole Cartesian product X .The procedure described above justifies a “collect” algorithm where a clique (here C E F ) collects all the informationfrom its neighbors (via messages Mi ) to compute the Pareto set. To produce these messages the neighbors also collect thenecessary information from their other neighbors, and so on. This results in function Pareto described below. However,to define this algorithm and the next ones more conveniently, we will not work directly with subutility vectors as we didabove but rather with labels:c,e, f ∈C×E×Fc,e, f ∈C×E×F(cid:4)(cid:4)Definition 6. A label is a triple (cid:15)v, xC, XSi jand XSi j is a separator of a GAI network.(cid:16), where v is a vector of Zm+, xC ∈ XC is an instantiation of a set XC of attributes,1160C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Intuitively, a label (cid:15)v, xC, XSi j(cid:16) corresponds to subutility vector v, with the additional information of the partial instanti-ation xC of the attributes that were involved in its construction and the separator XSi j on which the message containing vhas been transmitted. Given a clique XCi the subutility of which is ui : XCi+, we define the set of labels corresponding}. In addition, to handle labels easily, we define for any set of labels V ,(cid:16): xCito ui as Labels( XCi ) = {(cid:15)ui(xCi ), xCi , XCiW , any set of attributes XE and any instantiation xE ∈ XE, the following operators:∈ XCi(cid:7)→ Zm• ND(V) denotes a set of labels of V the utility vectors of which are undominated (actually, we keep in ND(V) only onelabel per undominated vector, that is, we are interested only in one instantiation for each utility vector).• V ⊗ W = {(cid:15)v + w, xC∪D, XE∪F(cid:16): (cid:15)v, xC, XE(cid:16) ∈ V and (cid:15)w, xD, XF(cid:16) ∈ W}, i.e., operator ⊗ aggregates additively labels of Vand W the partial instantiations of which agree on attributes XC∩D. This operator will be used extensively to combineappropriately the aforementioned messages.• V→XE= {(cid:15)v, xD, XE(cid:16): (cid:15)v, xD, XF(cid:16) ∈ V}, i.e., V→XE contains the same labels as V except that their third component issubstituted by XE. This will be used to “move” messages from one separator to another one.=(cid:4)• V[xE] = {(cid:15)v, yD, XF(cid:16) ∈ V: yE = xE}, i.e., V[xE] is the subset of labels of V that “agree” with partial instantiation xE.• V⇓ XEcontains the set of all the labels of V that are undominated by any other labelof V with the same partial instantiation xE. This operator will be used to discard all the labels the combination ofwhich cannot lead to undominated solutions due to Corollary 2.ND(V[xE])→XE , i.e., V⇓ XExE∈ XEGiven these operators, we can now express the basic message-passing algorithm we described above for computing thePareto set1:Algorithm 1: A variable elimination algorithm for computing the Pareto set.Function Pareto_Collect( XCi , XC j )01 message Mi j ← Labels( XCi )02 for all cliques XCk0304 Mi j ← Mi j ⊗ Mki05 donecall Pareto_Collect( XCk , XCi )∈ Adj( XCi )\{ XC j} do06 Mi j ← Mi j ⇓ XSi jFunction Pareto( )01 Let root = XCp be any clique02 call Pareto_Collect( XCp , XCp )03 return ND(Mpp )Proposition 3. Given a GAI tree G, function Pareto() returns precisely the Pareto set.Proposition 4. (See Rollon and Larrosa [28].) Pareto() requires space O (km ×where k is the number of cliques in the GAI network, d is the largest attribute’s domain size, wthe number of variables in the largest clique minus one) and K i is a bound on utility ui .i=1 K i ×dwm(cid:2)∗) and time O (km ×∗∗+1),is the network’s induced width (i.e.,mi=1 K 2i×dw(cid:2)Note that function Pareto_Collect, as described above, is generic and does not impose any ordering on messagesMki ’s combinations. In practice, the number of operations performed during the for loop of lines 02–05 depends on howcombinations are performed. A simple yet very effective strategy consists in, first, computing all the Mki by calling theappropriate Pareto_Collect( XCk , XCi ) and, only then, perform the combinations. The latter can be computed iterativelyby always selecting the pair of messages that produces a message with the smallest dimension. For instance, assume thatwe wish to compute M1i ⊗ M2i ⊗ M3i , with messages M1i , M2i , M3i defined on A × B, A × C and C × D respectively.Then first computing M1 = M1i ⊗ M2i , and then M1 ⊗ M3i produces the same result as computing M2 = M1i ⊗ M3i ,and then M2 ⊗ M2i but the former is faster than the latter since M1 is defined on A × B × C whereas M2 is defined onA × B × C × D.3.2.2. A best-first Pareto search algorithmIn function Pareto() described previously, sending all the subutility vectors that are undominated for fixed separatorvalues in one single message Mi j prevents applying prunings that can significantly speed-up the algorithm. For this reason,we now propose an alternative algorithm that sends the undominated subutility vectors (labels actually) one by one onthe separators. When such vector reaches the root clique, this vector produces new knowledge that can be used to prunethose vectors that have not reached the root yet and that we now know for sure cannot be part of the solution. This∗algorithm is very similar in spirit to the variant of the MOAalgorithm by Mandow and de la Cruz [29] that improves thealgorithm [30–32]. It favors the early detection of partial solutions that will lead to suboptimal solutions andstandard MOAcan therefore discard the corresponding labels hence limiting the combinatorial blowup. The main difference between ourlies in the exploitation of an explicit junction tree structure instead of an implicit state space graph.approach and MOA∗∗1 Recall that, by abuse of notation, XSii= XCi for all i = 1, . . . , k, so that (cid:15)v, xC, XSii(cid:16) corresponds to a label transmitted to clique XCi .C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791161Fig. 7. Making a label move toward root.On one hand, our search algorithm requires satisfying specific constraints imposed by the junction tree. Actually, whenevera label is moved from one separator to the next one, it is combined with other labels stored into adjacent separators; toensure that this combination is meaningful, the partial instantiations of these labels must necessarily be compatible. Thus,usually does. On the other hand, asour approach needs more information about how labels were generated than MOAour best-first search algorithm is based on the tree structure of the GAI network and proceeds from leaves toward theroot clique, a given label can never be generated more than once. Thus, unlike MOA, our algorithm does not need tokeep track of a list of closed labels and, actually, it never stores such a list. In a sense, this feature is close to frontier searchalgorithms [33].More precisely, the idea is to maintain two lists of labels: Lopen, which are the labels that have not reached yet the rootclique, and LPareto, which are essentially those labels that have reached root and are still undominated. At the beginning ofthe algorithm, LPareto should be empty, and Lopen should be basically filled with the labels of the leaves of the GAI network.A nonempty Lopen set means that there still exist labels that can possibly be combined with other labels to produce inthe end undominated labels that should belong to LPareto. So, while Lopen is nonempty, select one of its labels and make itmove toward root (by combining it with other appropriate labels, see below). When a label reaches root, it is of coursediscarded from Lopen and LPareto is updated. When all the labels of interest have reached root, then Lopen becomes emptyand the algorithm has computed the Pareto set.∗∗To illustrate how labels move toward the root, consider an arbitrary label (cid:15)w, xD, XSi j(cid:16) which is currently located onseparator XSi j (see Fig. 7). Like in function Pareto(), this label corresponds to the subutility of a partial instantiation ofthe attributes in the “ A” area of the GAI net. Moving it to separator XS jl should thus logically produce a label correspondingto the instantiation of the attributes in the “B” area of the GAI net. Hence (cid:15)w, xD, XSi j(cid:16) should necessarily be added tocompatible labels located on separators XSk1 j , . . . , XSkr j as well as to compatible labels stored in clique XC j (else someattributes in the “B” area would remain uninstantiated). Among all such possible labels, those that were sent on separatorsXSk1 j , . . . , XSkr j at earlier steps of the algorithm seem to be good candidates. So let us consider the set of labels Mkt j ,t = 1, . . . , r, sent on these separators at earlier steps. We propose to generate all the compatible combinations of these(cid:16)} ⊗ Mk1 j ⊗ · · · ⊗ Mkr j ⊗ Labels( XC j ), and then to project the resulting set of labels on separatormessages, i.e., {(cid:15)w, xD, XSi jXS jl (discarding of course all the dominated labels for fixed values of XS jl ) or, in other words, to compute:(cid:6)(cid:7)V =(cid:15)w, xD, XSi j(cid:8)(cid:16)(cid:9)⊗ Mk1 j ⊗ · · · ⊗ Mkr j ⊗ Labels(XC j ).⇓ XS jl(3)The labels in V thus correspond to a set of labels appropriate for separator XS jl . As such they should be added toLopen since Lopen represents the sets of labels that may potentially be part of the Pareto solutions we look for. In addi-(cid:16) can now be safely removed from Lopen since it has been dealt with (i.e., it has been combinedtion, label (cid:15)w, xD, XSi jwith other labels). The process just described informally can now be described algorithmically by the following func-tion:Algorithm 2: The function for moving labels within the Junction tree.(cid:16))(cid:16)}(cid:16)}Function move_label((cid:15)w, xD, XSi j01 Mi j ← Mi j ∪ {(cid:15)w, xD, XSi j02 V ← Labels( XC j ) ⊗ {(cid:15)w, xD, XSi j03 if XC j04 for all cliques XCk05 V ← V ⊗ Mkj06 done07 if XC j08 return V(cid:12)= root then V ← V⇓ XS jl∈ Adj( XC j ), XCkelse V ← ND(V)(cid:12)= root then let XCl be the clique ∈ Adj( XC j ) s.t. XCl is on the path between XC j and root(cid:12)= XCi and XCk(cid:12)= XCl (if XCl has been defined in line 02) do1162C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Of course, Eq. (3) produces a set of labels corresponding to instantiations of all the attributes of the “B” area ofFig. 7 if and only if all messages Mkt j are nonempty. Hence we should enforce that whenever function move_labelis called, the Mkt j are actually nonempty. A simple way to achieve this is to initialize the Pareto search by function ini-tial_labels below which fills each separator XSi j with exactly one label per value xSi j . The basic idea of the functionis to apply a collect scheme from root toward the leaves of the GAI tree and, each time a separator is encountered, tofill it with one label per value. More precisely, for separators on the leaves of the tree, we compute the set of undom-inated labels per value of the separator, say V⇓ XSi j. As this set may contain several labels per value xSi j , we just keepone label per xSi j (this produces a message Mi j ) and put the other ones into Lopen to be processed later on. Whenlike in Fig. 7, the collect scheme first fills separators XSi j , XSk1 j , . . . , XSkr j with messageswe encounter a separator XS jlMSi j , MSk1 j , . . . , MSkr j respectively. Now, these messages can be combined to produce a message that can be stored on. Again, V may contain several messages per value xS jl , hence weXS jl : V = (MSi jstore only one of them into message M jl and put the other ones into Lopen to be processed later on. Of course, as the labelchosen to be stored into M jl is processed immediately while the others (those added to Lopen) will be processed later,the former should be selected as the one that currently seems best fitted to produce a Pareto element when moved tillthe root. For this reason, we call this element a “most promising” label. Different strategies do exist to define what a mostpromising label should be. In our experiments, we defined it as being the label with the highest utility average (over the Mobjectives). When optimistic heuristics were available, we used the highest average of the sum of the utility vector and theheuristic vector. Of course, alternative characterizations could also have been used such as, e.g., the highest lexicographicutility value (using a lexicographic order over the objectives). Overall, the above algorithm leads to the following functioninitial_labels:⊗ Labels( XC j ))⇓ XS jl⊗ · · · ⊗ MSkr j⊗ MSk1 jAlgorithm 3: The function initializing separator messages with one label per separator’s value.} do∈ Adj( XCi )\{ XC jcall initial_labels( XCk , XCi )Function initial_labels( XCi , XC j )01 V ← Labels( XCi )02 for all cliques XCk0304 V ← V ⊗ Mki05 done06 V ← V⇓ XSi j(cid:4)07 Mi j =xSi j08 Lopen ← Lopen ∪ (V\Mi j )∈XSi jmost promising label of V[xSi j]Let us illustrate this algorithm on the GAI net of Fig. 5: a callto initial_labels(C E F, C E F ) would callinitial_labels(E G, C E F ) and initial_labels( AC E, C E F ) on line 03. The first call first creates set V = {(cid:15)(1, 0),e1 g1, E G(cid:16), (cid:15)(1, 1), e1 g2, E G(cid:16), (cid:15)(1, 1), e2 g1, E G(cid:16), (cid:15)(2, 1), e2 g2, E G(cid:16)} on line 01 and V is reduced to V = {(cid:15)(1, 1), e1 g2, E(cid:16),(cid:15)(2, 1), e2 g2, E(cid:16)} on line 06. As V contains only one label per separator’s value e1,e2, message ME = V as shown inFig. 8. Now initial_labels( AC E, C E F ) calls initial_labels( A B, AC E) and initial_labels(C D, AC E). The firstone will compute V = {(cid:15)(1, 5), a1b1, A(cid:16), (cid:15)(8, 2), a1b2, A(cid:16), (cid:15)(3, 4), a2b2, A(cid:16), (cid:15)(6, 2), a2b3, A(cid:16)} on line 06 (discarding both labels(cid:15)(7, 1), a1b3, A(cid:16) and (cid:15)(2, 3), a2b1, A(cid:16) because they are dominated by (cid:15)(8, 2), a1b2, A(cid:16) and (cid:15)(3, 4), a2b2, A(cid:16) respectively). Here, Vcontains more than one element per ai , so we need select only one element per ai in V to create message M A . Say M A ={(cid:15)(1, 5), a1b1, A(cid:16), (cid:15)(3, 4), a2b2, A(cid:16)} (see Fig. 8). Note that, in this example, to reduce the number of iterations of the algorithm,the most promising label is not always chosen as the one with the highest utility average. The other elements of V , i.e.,{(cid:15)(8, 2), a1b2, A(cid:16), (cid:15)(6, 2), a2b3, A(cid:16)} are thus added to Lopen to be processed later on. Similarly, initial_labels(C D, AC E)computes on line 06 label set V = {(cid:15)(2, 9), c1d1, C(cid:16), (cid:15)(4, 1), c1d3, C(cid:16), (cid:15)(4, 9), c2d2, C(cid:16), (cid:15)(5, 3), c2d3, C(cid:16)}. From V we extractmessage MC = {(cid:15)(2, 9), c1d1, C(cid:16), (cid:15)(4, 9), c2d2, C(cid:16)} and labels (cid:15)(4, 1), c1d3, C(cid:16) and (cid:15)(5, 3), c2d3, C(cid:16) are added to Lopen to beprocessed later on. Now initial_labels( AC E, C E F ) can compute label set [Labels( AC E) ⊗ M A ⊗ MC ]⇓C E , which leads to:(cid:16)(cid:7)(cid:15)(cid:15)(cid:15)(cid:16)V =(cid:15)(6, 17), a1b1c1d1e1, C E(6, 17), a2b2c1d1e2, C E(10, 16), a2b2c1d1e1, C E,(cid:16)(5, 18), a1b1c1d1e2, C E(cid:15),(cid:16)(cid:8)(cid:15)(11, 17), a2b2c2d2e1, C E,(9, 18), a2b2c2d2e2, C E(cid:16),(cid:16),on line 06. From this set, we extract message MC E by selecting one label per value (ci, e j):MC E =(cid:7)(cid:15)(cid:15)(10, 16), a2b2c1d1e1, C E(11, 17), a2b2c2d2e1, C E(cid:16),(cid:16),(cid:15)(cid:16)(6, 17), a2b2c1d1e2, C E(cid:15),(cid:16)(cid:8)(9, 18), a2b2c2d2e2, C Eand add to Lopen labels (cid:15)(6, 17), a1b1c1d1e1, C E(cid:16) and (cid:15)(5, 18), a1b1c1d1e2, C E(cid:16) that were not selected to be part of MC E .Finally, initial_labels(C E F, C E F ) computes V = Labels(C E F ) ⊗ MC E ⊗ ME , i.e.:C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791163Fig. 8. Messages computed by function initial_labels.V =(cid:7)(cid:15)(cid:15)(cid:15)(cid:15)(19, 18), a2b2c1d1e1 f 1 g2, C E F(14, 19), a2b2c2d2e1 f 1 g2, C E F(15, 18), a2b2c1d1e2 f 1 g2, C E F(cid:15)(cid:15)(cid:15)(cid:15)(16, 23), a2b2c1d1e1 f 2 g2, C E F(13, 23), a2b2c2d2e1 f 2 g2, C E F(16, 24), a2b2c1d1e2 f 2 g2, C E F(cid:16),(cid:16),(cid:16),(cid:16)(cid:8)(cid:16)(cid:16)(cid:16),,,(cid:16),(12, 20), a2b2c2d2e2 f 1 g2, C E F(11, 24), a2b2c2d2e2 f 2 g2, C E Fand, as V contains only one label per triple (c, e, f ), MC E F = V . Overall, the created messages Mi j are those of Fig. 8 andLopen is initialized to:(cid:7)(cid:15)Lopen =(cid:16)(cid:15)(cid:15)(8, 2), a1b2, A(cid:16),(5, 3), c2d3, C,(6, 2), a2b3, A(cid:15)(4, 1), c1d3, C(cid:16),(cid:15)(6, 17), a1b1c1d1e1, C E(5, 18), a1b1c1d1e2, C E(cid:16)(cid:8).(cid:15)(cid:16),(cid:16),In addition to filling separators with a single label per separator’s value, which was its primary purpose, function ini-tial_labels has an important feature:Proposition 5. Let XCp be any clique. Call initial_labels( XCp , XCp ). Then, for any utility vector u(x) =one and only one of the following two assertions holds:(cid:3)kj=1 u j(xC j ) ∈ ND(U),1. there exists a label (cid:15)u(x), x, XCp2. there exist some indices j1, . . . , jr such that Lopen contains a label (cid:15)w, xD, XS j1l(cid:16) in message Mpp ;and XC j1is, among XC j1, . . . , XC jr, the clique which is nearest to root.(cid:16) such that w =(cid:3)rt=1 u jt (xC jt), D =(cid:4)rt=1 C jtAccording to Proposition 5, after calling initial_labels( XCp , XCp ), for any u(x) ∈ ND(U), either there exists a labelcorresponding to u(x) in Mpp , or there exists a label in Lopen corresponding to u(x). The latter case can be interpreted asthe fact that the propagation of the label corresponding to u(x) toward root XCp has been temporarily stopped on a givenseparator because this label did not seem, at that time, to be a most promising label to belong to ND(U). We should thussubsequently use function move_label to make it eventually reach root XCp . This justifies algorithm Pareto∗below,which we present for simplicity without pruning rules (those will be given later on):Algorithm 4: Basic best-first Pareto search.Function Pareto∗()01 let root XCp be any clique02 Lopen ← ∅; call initial_labels( XCp , XCp )03 LPareto ← ND(Mpp )04 while Lopen (cid:12)= ∅ dolet (cid:15)w, xD, XSi j05remove (cid:15)w, xD, XSi j(cid:16) from Lopen0607 V ← move_label((cid:15)w, xD, XSi j0809 done10 return LParetoif XSi j(cid:16))(cid:16) be the most promising label in Lopen= root XCp then LPareto ← ND(LPareto ∪ V) else Lopen ← Lopen ∪ VIn our experiments, we defined the “most promising” label of line 05 as being the nearest label to the root clique and,to break ties, that with the highest utility average (over the M objectives). Favoring labels that are nearest to the rootclique is effective because it tends to quickly fill LPareto with feasible labels that can be used subsequently to prune as earlyas possible the labels of Lopen. When optimistic heuristics were available, we broke ties using the highest average of thesum of their utility vectors and their heuristic vectors.1164C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Fig. 9. An optimistic heuristic-based pruning rule.Proposition 6. Given a GAI tree G, function Pareto∗() returns precisely the Pareto set.Fig. 10. The subutility tables of our example.Proposition 7. Pareto∗() requires space O (km ×cliques in the GAI network, d is the largest attribute’s domain size, wthe largest clique minus one) and K i is a bound on utility ui .i=1 K i × dwm(cid:2)∗) and time O (km ×∗∗+1), where k is the number ofis the network’s induced width (i.e., the number of attributes inmi=1 K 2i× dw(cid:2)For the moment, function Pareto∗moves labels one by one toward the root but it applies no pruning rule (except,of course, that given by Corollary 2) and, in the end, it sends precisely the same messages Mi j as those sent by func-tion Pareto in Section 3.2.1. Hence we shall now introduce an additional pruning rule to improve the efficiency of thealgorithm.3.2.3. Pruning ruleConsider the graph of Fig. 9 and apply function Pareto∗. At some step of the algorithm, select on line 05 a label, say(cid:15)w, aib j, A(cid:16), from Lopen to be moved toward separator C E. Assume that we know that there exists some vector v suchthat, for any completion (ck, dl, er, f s, gt) of the attributes of the gray area, v (cid:2)P u2(ck, dl) + u3(ai, ck, er) + u4(ck, er, f s) +∗ (cid:2)P v + w, then labelu5(er, gt). If, in addition, it turns out that LPareto contains at that time a label (cid:15)u(cid:15)w, aib j, A(cid:16) can be safely discarded because it cannot be part of a solution of ND(U). Indeed, if there exists a vector z ∈ Uthe utility vector of an instantiation (ck, dl, er, f s, gt) of the attributes of the gray area, thensuch that w(cid:21) + w should not be added to∗ (cid:2)P v + w (cid:2)P wuND(U).also Pareto dominates z. As a consequence, vector w(cid:21) + w (cid:2)P z and so u(cid:21) + w (cid:2)P z, with w(cid:16) such that u∗, x, XCpThis suggests defining an optimistic heuristic that, given a label like (cid:15)w, aib j, A(cid:16), is able to return a vector or a set ofvectors like the v vector mentioned in the preceding paragraph. For a vector set-valued optimistic heuristic see [30]. In thispaper, for simplicity, we present a single-vector valued heuristic easily computable using the collect algorithm illustrated inFig. 11: on separator E, send message HE containing, for each value ei of E, a utility vector constituted by the max overeach criterion of {u5(ei, g1), u5(ei, g2)}, that is,(cid:15)(cid:6)(cid:7)(cid:15)(cid:6)(cid:16)(cid:8)(cid:9)(cid:9)∗(cid:16)(cid:21)HE ==(cid:7)(cid:15)max{1, 1}, max{0, 1}(cid:16),(1, 1), e1, E(2, 1), e2, E(cid:15), e1, E(cid:16)(cid:8),.max{1, 2}, max{1, 1}, e2, EClearly, by construction, vectors in HE weakly Pareto dominate all possible vectors u5(er, gt), r, t = 1, 2. As a con-sequence, the utility vectors in V = Labels(C E F ) ⊗ HE Pareto dominate Labels(C E F ) ⊗ Labels(E G). Now we can apply thesame process with V : construct a message HC E containing, for each pair (ck, er), a vector constituted by the maxover each criterion of V[ck, er]. Here again, by construction, vectors in HC E weakly Pareto dominate all possible vec-tors in V which, in turn, weakly Pareto dominate all possible vectors in Labels(C E F ) ⊗ Labels(E G). Apply the same pro-cess for constructing message HC and, finally, to construct H A apply again this maximization per criterion scheme onHC ⊗ Labels( AC E) ⊗ HC E . Clearly, by the process of construction, vectors in H A weakly Pareto dominate all subutility vectorsu2(ck, dl) + u3(ai, ck, er) + u4(ck, er, f s) + u5(er, gt), for any completion (ck, dl, er, f s, gt), which was precisely what we werelooking for. This justifies the following recursive algorithm:C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791165Algorithm 5: Computation of the optimistic heuristic.Function Heuristic_Collect( XCi , XC j )01 V ← Labels( XCi )02 for all cliques XCr0304 V ← V ⊗ Hir05 done06 H ji ← Max↓XSi jcall Heuristic_Collect( XCr , XCi )∈ Adj( XCi )\ XC j doVwhere, for any set of attributes XE and any set of labels V , Max↓ XEw i:(cid:16): for all i, v i = max(v 1, . . . , vm), xE, XSi jMax↓ XEV =(cid:7)(cid:15)(cid:7)V is defined as:(cid:9)(cid:15)(cid:6)w 1, . . . , wm, yC, XD(cid:16)(cid:8)(cid:8)∈ V[xE].In other words, for each xE, v i is the max for criterion i of the utilities of the labels agreeing with xE. Now it is clear thatthe following proposition holds:Proposition 8. Call Heuristic_Collect( XCi , XC j ). Then, for any xSi jsubutilities obtained by instantiations of the attributes in cliques Xr such that XCi is not on the path between Xr and root.] weakly Pareto dominates the sums of the∈ XSi j , H ji[xSi jNote that, in Proposition 8, cliques Xr are precisely those located in the gray area of Fig. 9.Proposition 9. Heuristic_Collect( XCi , XC j ) requires space O (km × dwcliques in the GAI network, d is the largest attribute’s domain size, wthe largest clique minus one) and K i is a bound on utility ui .∗∗∗+1), where k is the number ofis the network’s induced width (i.e., the number of attributes in) and time O (km × dwImporting this optimistic heuristic into function Pareto∗essentially requires modifying its line 08 where Lopen andLPareto were updated: now each time LPareto is updated, we should try to prune as well all the labels in Lopen which,when combined with their optimistic heuristic value, are Pareto dominated by some element in LPareto. So let us define thecorresponding dominance operator: for any sets of labels V, W , where W are labels with complete instantiations,NDH (V, W) =(cid:7)(cid:15)v, xD, XSi jis the utility of Hi j[xSi j(cid:16) ∈ V: there exists no (cid:15)w, y, XCp(cid:16) ∈ W: w (cid:2)P v + h, where h(cid:8)] as returned by Heuristic_Collect(XCi , XC j ).Clearly, calling NDH (Lopen, LPareto) in function Pareto∗will remove only the labels from Lopen that, if moved until root,would produce labels weakly Pareto dominated by those of ND(U). As a consequence, it is safe to discard such labels anddoing it as early as possible reduces the run time of the algorithm. This leads to the following Pareto search algorithm:Algorithm 6: Efficient best-first Pareto search algorithm.H()Function Pareto∗01 let root XCp be any clique02 Lopen ← ∅; call initial_labels( XCp , XCp )03 for all Separators XS ji do call Heuristic_Collect( XCi , XC j ) done04 LPareto ← ND(Mpp ); Lopen ← NDH (Lopen, LPareto)05 while Lopen (cid:12)= ∅ dolet (cid:15)w, xD, XSi j06remove (cid:15)w, xD, XSi j(cid:16) from Lopen0708 V ← move_label((cid:15)w, xD, XSi j091011 done12 return LParetoif XSi jelse Lopen ← Lopen ∪ NDH (V, LPareto)(cid:16) be the most promising label in Lopen(cid:16))= root XCp then LPareto ← ND(LPareto ∪ V); Lopen ← NDH (Lopen, V)Proposition 10. Given a GAI tree G, function Pareto∗H() returns precisely the Pareto set.Proposition 11. Pareto∗of cliques in the GAI network, d is the largest attribute’s domain size, win the largest clique minus one) and K i is a bound on utility ui .H() requires space O (km ×i=1 K i × dwm(cid:2)∗) and time O (km ×∗∗+1), where k is the number× dwis the network’s induced width (i.e., the number of attributesmi=1 K 2i(cid:2)Note that, for simplicity of exposition, we chose to call Heuristic_Collect as many times as there are differentseparators. This is not optimally efficient as many messages Hri computed by this function are actually computed severaltimes. However, using techniques similar to inference in Bayesian networks [34], i.e., by using a collect/distribute algorithm,1166C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Fig. 11. Computing the optimistic heuristic for label (cid:15)w, ai b j , A(cid:16).Fig. 12. The optimistic heuristic computed for every separator.Fig. 13. The content of messages Mi j after the completion of initial_labels(C E F , C E F ).all these redundancies can be removed and the overall computational burden to compute the heuristic for all the separatorsis only twice the time required to complete one call to Heuristic_Collect( XCi , XC j ).Let us now see on the GAI network of Fig. 10 how our new pruning rule can effectively improve the run time to computethe Pareto set. First, we display in Fig. 12 the values of the optimistic heuristic computed on every separator.After the completion of initial_labels(C E F , C E F ), messages Mi j are for instance those of Fig. 13 (defining “the mostpromising” vectors appropriately) and the content of Lopen is described below:Lopen =separator instvect vect + H separatorAACCa1b2 (8, 2) (24, 22)a2b3 (6, 2) (24, 22)c1d3 (4, 1) (24, 17)c2d3 (5, 3) (22, 18)C EC EC EC Einstvectvect + Ha1b1c1d1e1 (6, 17) (15, 24)a1b1c1d1e2 (5, 18) (15, 25)a1b1c2d2e1 (6, 16)(9, 22)a2b2c2d2e2 (9, 18) (12, 24)Then LPareto is filled with the undominated labels of MC E F , i.e.:LPareto =(cid:7)(cid:15)(19, 18), a2b2c1d1e1 f 1 g2, C E F(cid:15)(cid:16),(16, 24), a2b2c1d1e2 f 2 g2, C E F.(cid:16)(cid:8)C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791167Fig. 14. The content of messages Mi j after moving label (cid:15)(8, 2), a1b2, A(cid:16).Now, the last instruction of line 04 reduces set Lopen: all the labels which, combined with the optimistic heuristic, aredominated by elements in LPareto are discarded.separator instvect vect + H separatorinstvectvect + HLopen =AACCa1b2 (8, 2) (24, 22)a2b3 (6, 2) (24, 22)c1d3 (4, 1) (24, 17)c2d3 (5, 3) (22, 18)C Ea1b1c1d1e2 (5, 18) (15, 25)Next we enter the while loop of lines 05–11. Let us assume that the most promising element is the label located onseparator C E. Then we move this label up to the root, thus producing V = {(cid:15)(14, 19), a1b1c1d1e2 f 1 g2, C E F (cid:16), (cid:15)(15, 25),a1b1c1d1e2 f 2 g2, C E F (cid:16)}. The second element is thus added to LPareto on line 09, which becomes:(cid:16)(cid:7)(cid:15)(cid:15)(cid:16)LPareto =(19, 18), a2b2c1d1e1 f 1 g2, C E F(cid:15)(15, 25), a1b1c1d1e2 f 2 g2, C E F(16, 24), a2b2c1d1e2 f 2 g2, C E F,(cid:16)(cid:8),,and Lopen remains unchanged. In addition, label (cid:15)(5, 18), a1b1c1d1e2, C E(cid:16) is added to separator C E, thus resulting in messageMC E as described in Fig. 14. Assume the next label selected on line 06 is (cid:15)(8, 2), a1b2, A(cid:16). Then this label is moved toseparator C E. We thus compute V = {(cid:15)(8, 2), a1b2, A(cid:16)} ⊗ Labels( AC E) ⊗ MC , where MC is precisely the message describedin Fig. 13, and the resulting label set V is equal to:V =(cid:7)(cid:15)(cid:15)(13, 14), a1b2c1d1e1, AC E(13, 13), a1b2c2d2e1, AC E(cid:15)(cid:15)(cid:16)(cid:16),,(12, 15), a1b2c1d1e2, AC E(cid:16),(cid:16)(cid:8)(18, 13), a1b2c2d2e2, AC E.Of course, V⇓C E= V and this set is appended to Lopen:separator instvect vect + H separatorLopen =ACCa2b3 (6, 2) (24, 22)c1d3 (4, 1) (24, 17)c2d3 (5, 3) (22, 18)C EC EC EC Einstvectvect + Ha1b2c1d1e1 (13, 14) (22, 21)a1b2c1d1e2 (12, 15) (22, 22)a1b2c2d2e1 (13, 13) (16, 19)a1b2c2d2e2 (18, 13) (21, 19)In addition, label (cid:15)(8, 2), a1b2, A(cid:16) is added to message M A , and the contents of the messages are now those described inFig. 14. Note that LPareto is unaffected.And we execute again the while loop of lines 05–11. Let (cid:15)(12, 15), a1b2c1d1e2, C E(cid:16) be the next label selected on line 06.Moving this label to clique C E F produces a label set V = {(cid:15)(21, 16), a1b2c1d1e2 f 1 g2, C E F (cid:16), (cid:15)(22, 22), a1b2c1d1e2 f 2 g2, C E F (cid:16)}.Only the second element is thus added to LPareto on line 09, which becomes:LPareto =(cid:7)(cid:15)(cid:16)(cid:15)(16, 24), a2b2c1d1e2 f 2 g2, C E F(cid:15)(22, 22), a1b2c1d1e2 f 2 g2, C E F(15, 25), a1b1c1d1e2 f 2 g2, C E F,(cid:16)(cid:8).(cid:16),Note that label (cid:15)(19, 18), a2b2c1d1e1 f 1 g2, C E F (cid:16), which previously belonged to LPareto has been discarded from this set sinceit is dominated by (cid:15)(22, 22), a1b2c1d1e2 f 2 g2, C E F (cid:16). In addition, Lopen is updated as follows (discarding elements with ournew pruning rule):Lopen = separator instvect vect + H separator instAa2b3 (6, 2) (24, 22)Cvect vect + Hc1d3 (4, 1) (24, 17)1168C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Now, we enter the while loop again and select (cid:15)(6, 2), a2b3, A(cid:16) to be moved. However, no element of set V ={(cid:15)(6, 2), a2b3, A(cid:16)} ⊗ Labels( AC E) ⊗ MC is added to Lopen because NDH (V, LPareto) = ∅. Finally, there remains only label(cid:15)(4, 1), c1d3, C(cid:16) to be moved. This creates a set V with 8 elements, out of which only 2 are kept due to our pruningrule:Lopen = separatorinstvectvect + H separatorinstvect + Ha1b2c1d3e2 (17, 4) (24, 14)vectC Ea1b2c1d3e1 (15, 6) (24, 13)C EMoving (cid:15)(17, 4), a1b2c1d3e2, C E(cid:16) toward root will produce a new Pareto element of utility value (24, 14). As a consequenceLopen becomes empty since label (cid:15)(15, 6), a1b2c1d3e1, C E(cid:16), when combined with its optimistic heuristic, is dominated bythis new Pareto element. Therefore, Lopen being empty, the execution of the algorithm is completed. The Pareto set thuscomputed is:(cid:7)(cid:15)LPareto =(16, 24), a2b2c1d1e2 f 2 g2, C E F(cid:15)(22, 22), a1b2c1d1e2 f 2 g2, C E F(15, 25), a1b1c1d1e2 f 2 g2, C E F,(cid:16)(cid:8)(cid:15)(24, 14), a1b2c1d2e2 f 2 g2, C E F.(cid:15)(cid:16),(cid:16),(cid:16)As we can see, the pruning rule is quite effective as it discards many labels that would have been combined without thisrule: on overall, there were actually only 38 labels combinations instead of 78 for function Pareto.4. Preference-based searchAs mentioned above, in a multiagent or multicriteria problem, comparing feasible solutions in X amounts to comparingtheir respective utility profile. The basic preference model to compare solutions is Pareto dominance.The results obtained in Section 3 show that the exact determination of the Pareto set requires, for some instances,prohibitive computation times (see, e.g., Proposition 1). Fortunately, determining the entire set of Pareto-optimal elementsis not always necessary. For example, in multiagent problems, the value of a solution is often measured by a social welfarefunction assessing the overall utility of solutions for the society of agents. For example one can be interested in maximizingthe sum of individual utilities (utilitarianism), or in maximizing the satisfaction of the least satisfied agent (egalitarianism)or any compromise between the two attitudes.One major issue in multiagent decision making processes seeking approval of all agents is fairness of decision procedures.This normative principle generally refers to the idea of favoring solutions that fairly share happiness or utility among agents.More formally, when comparing two utility vectors u and v (one component per agent), claiming that “u is more fairthan v” usually conveys the vague notion that the components of u are “less spread out” or “more nearly equal” thanare the components of v. This intuitive notion leaves room for different definitions of fairness and various models havebeen proposed by mathematicians who developed a formal theory of majorization [35] and by economists who providedaxiomatic foundations of inequality measures (for a synthesis see [36,37]). All these models provide the solution space witha transitive preference structure refining Pareto dominance, which is either a partial weak-order (e.g. Lorenz dominance) ora complete weak-order (e.g. ordered weighted averages). We will see now that the general algorithm Pareto∗H presentedin Section 3.2.3 to determine the Pareto set can be further specialized to focus the search on fair compromise solutions.The need for refining Pareto dominance is also present in single-agent multicriteria decision making problems. In suchproblems, the most preferred solutions are usually those achieving a good compromise between the various conflicting ob-jectives involved in the decision model. We are generally not interested in generating extreme solutions favoring a particularcriterion to the detriment of the others. The standard way of generating compromise solutions within the Pareto set is tooptimize a “scalarizing function” measuring the overall quality of solutions by aggregation of criteria or, more generally,to define an overall preference model refining Pareto dominance and narrowing the initial optimality concept. When pref-erence information is not sufficient to formulate a stable overall preference model, iterative compromise search can stillbe used to explore the Pareto set. One starts with a “neutral” initial preference model used to generate a well-balancedcompromise solution within the Pareto set and the model progressively evolves with feedbacks from the decision makerduring the exploration to better meet its desiderata. Such an interactive process is used in multiobjective programming oncontinuous domains to scan the Pareto set which is infinite, see e.g. [38,1]. The same approach is worth investigating incombinatorial problems when complete enumeration of the Pareto set is not feasible. This will be discussed in Section 4.4.The common problem in all these situations is to determine the most-preferred solutions with respect to a given prefer-ence model (cid:2) refining Pareto dominance. Hence, the rest of this section is dedicated to this general problem. We proposea refinement of Pareto∗H that exploits the GAI structure of utility functions to determine the most preferred solutionswithout resorting to complete enumeration of the Pareto set. For the sake of illustration, we will consider here 3 differentmodels: on one hand Lorenz dominance and ordered weighted averages for fair optimization in multiagent decision makingproblems, on the other hand Tchebycheff distances for compromise search in multicriteria decision making problems. Wewill report numerical tests and provide computation times obtained for these models in Section 5.4.1. Lorenz dominanceLorenz dominance is a refinement of Pareto dominance used in fair optimization problems when utility functionsu1, . . . , um represent the preferences of m agents. In addition to the initial objective aiming at maximizing individual utili-C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791169ties, fairness refers to the idea of favoring Pareto-optimal solutions having a well-balanced utility profile. For this reason, infair optimization problems, we are interested in working with a preference relation (cid:2) satisfying the two following axioms:P-Monotonicity. For all u, v ∈ Zm+, u (cid:2)P v ⇒ u (cid:2) v and u (cid:5)P v ⇒ u (cid:5) v,where (cid:5) is the strict preference relation defined as the asymmetric part of (cid:2). P-Monotonicity is a natural unanimityprinciple enforcing consistency with P-dominance.Transfer Principle. Let u ∈ Zmwhere ei (resp. e j ) is the vector whose ith (resp. jth) component equals 1, all others being null.+ be such that ui > u j for some i, j. Then for all ε such that 0 < ε < ui − u j , u − εei + εe j (cid:5) uThis axiom captures the idea of fairness as follows: if ui > u j for some utility vector u ∈ Zm+, slightly increasing com-ponent u j to the detriment of ui while preserving the sum of individual utilities would produce a better distribution ofutilities and consequently improve the fairness of the solution. For example vector u = (11, 10, 11) should be preferred tov = (12, 9, 11) because there exists a transfer of size (cid:3) = 1 to pass from v to u. Note that using a similar transfer of sizegreater than 12 − 9 = 3 would increase inequality. This explains why the transfers must have a size ε < ui − u j . Such trans-fers are said to be admissible in the sequel. They are known as Pigou–Dalton transfers in social choice theory, where they areused to reduce inequality in the income distribution over a population (see [37] for a survey).Note that the Transfer Principle enables to discriminate between some pair of vectors having the same sum of utilities,but it does not apply in the comparison of utility vectors having different sums. This is the reason why Transfer Principlemust be combined with P-Monotonicity. For example, to compare w = (11, 11, 11) and z = (12, 9, 10) we can use vectorsu and v introduced above and observe that w (cid:5) u (P-Monotonicity), u (cid:5) v (Transfer Principle explained above) and v (cid:5) z(P-Monotonicity). Hence w (cid:5) z by transitivity. In order to better characterize those vectors that can be compared using suchcombinations of P-Monotonicity and Transfer Principle, we recall now the definition of Lorenz vectors and related concepts(for more details see e.g. [35]):Definition 7. For all u ∈ Zm(cid:6)L(u) =u(1), u(1) + u(2), . . . , u(1) + u(2) + · · · + u(m)(cid:9)+, the generalized Lorenz vector associated to u is the vector:where u(1) (cid:5) u(2) (cid:5) · · · (cid:5) u(m) represent the components of u = (u1, . . . , um) sorted by increasing order. The jth componentof L(u) is L j(u) =(cid:3)ji=1 u(i).Definition 8. The generalized Lorenz (weak) dominance relation on ZmL(v) and its strict part (called L-dominance hereafter) is defined by u (cid:5)L v ⇔ L(u) (cid:5)P L(v).+ is defined for all u, v ∈ Zm+, by u (cid:2)L v ⇔ L(u) (cid:2)PThe notion of L-dominance was initially introduced to compare vectors with the same average cost. The generalizedversion of L-dominance considered here is a classical extension allowing vectors with different averages to be compared.Within a set U ⊂ Zm+, any utility vector u is said to be L-dominated when v (cid:5)L u for some v in U , and L-non-dominated whenthere is no v in U such that v (cid:5)L u. The set on L-non-dominated vectors in U is denoted NDL . In order to establish the linkbetween generalized Lorenz dominance and preferences satisfying combination of P-Monotonicity and Transfer Principle werecall a result of Chong [39]:Theorem 1. For any pair of distinct vectors u, v ∈ ZmConversely, if u (cid:5)L v, then there exists a sequence of admissible transfers and/or Pareto improvements to transform v into u.+, if u (cid:5)P v, or if u obtains from v by a Pigou–Dalton transfer, then u (cid:5)L v.For example we have: L(w) = (11, 22, 33) (cid:5)P (9, 19, 31) = L(z) which directly proves the existence of a sequence ofPareto improvements and/or admissible transfers passing from z to w. This theorem establishes L-dominance as the minimaltransitive relation (with respect to set inclusion) satisfying simultaneously P-Monotonicity and Transfer Principle. Hence, thesubset of L-non-dominated elements defines the best candidates to optimality in fair optimization problems. This explainsour interest in solving the following problem:Lorenz-optimal elements (LO)Input: a product set of finite domains X = X1 ×· · ·× Xn (n finite), m GAI utility functions ui : X → Z+, i = 1, . . . , m (m finite),Goal: determine the entire set of L-non-dominated vectors in U, and for each utility vector u ∈ NDL(U) a correspondingtuple xu ∈ X .Unfortunately, although the set of L-non-dominated elements is a subset of the Pareto set, it can still be sufficiently largeto prevent any efficient enumeration as shown by the following result:1170C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Proposition 12. Problem LO is intractable, even when m = 2; it requires a number of operations which grows, in worst case, exponen-tially with the number of attributes.Proposition 13. As soon as | Xi| (cid:3) 2 and m (cid:3) 2, deciding whether there exists a tuple in X the utility of which weakly L-dominates agiven utility vector u in Zn+ is a NP-complete decision problem (referred to as problem Lu in the sequel).Despite the apparent negative results of Propositions 12 and 13, we will see in Section 5 that, in practice, the averagesize of NDL is small as compared to that of ND. This suggests that there might exist algorithms, efficient on average, todetermine the set of L-non-dominated elements. The following subsection is dedicated to such an algorithm.4.2. A focused search algorithm for L-non-dominated elementsWe introduce now a modification of Pareto∗H search algorithm introduced in Section 3.2.3 to determine L-non-dominated elements in X . Since these elements are necessarily Pareto-optimal we might first determine Pareto-optimalelements and then, by pairwise comparisons, determine the L-non-dominated elements. This procedure would not be effi-cient due to the size of the Pareto set. Instead, we prefer using a nice feature of Pareto∗H that computes Pareto optimalsolutions one by one, thus leaving room for pruning rules based on L-dominance. Using such rule, we are going to specializePareto∗H to focus directly on Lorenz-optimal solutions. However this cannot be done naively as shown in the followingexample:Example 3. Consider a GAI network with three Boolean attributes A, B, C and two cliques A B and BC . Assume that there aretwo Pareto-optimal solutions on clique A B: (1, 1) and (0, 1) with utility u A B (1, 1) = (2, 2) and u A B (0, 1) = (3, 1). We haveL(2, 2) = (2, 4) and L(3, 1) = (1, 4). Hence we might be tempted to eliminate vector (3, 1) which is L-dominated by (2, 2) onA B and to send message (0, 1) with utility (2, 2) to the other clique BC . However this would be a mistake. Assume indeedthat the only compatible Pareto-optimal vector on clique BC is (1, 0) with u BC (1, 0) = (1, 3) we would output solution(1, 1, 0) with utility (2, 2) + (1, 3) = (3, 5) with L(3, 5) = (3, 8) whereas there exists a better solution: (0, 1, 0) with utility(3, 1) + (1, 3) = (4, 4) with L(4, 4) = (4, 8).This example shows that we cannot simply substitute Pareto dominance by L-dominance everywhere in a Pareto searchalgorithm to get an admissible algorithm for determining L-non-dominated elements. Lorenz dominance cannot be usedto compare two labels located on a given separator and having the same partial instantiation over this separator (as wassuggested for Pareto dominance by Corollary 2). It can only be used to prune labels by comparison with other labelscorresponding to complete tuples already evaluated. We explain now the exact management of labels for the determinationof L-non-dominated elements.For determining Lorenz non-dominated elements, we will define the counterparts of labels’ functions ND and NDH forthe Lorenz dominance (instead of the Pareto dominance). More formally, let L denote the set of all labels. We define afunction NDL : L (cid:7)→ L which, for any set of labels V , returns a set NDL(V) ⊆ V containing one label per set of labels of(cid:21) =V having the same generalized Lorenz non-dominated vector, i.e., for any V = (cid:15)v, xD, XE(cid:16) ∈ NDL(V), there exists no V(cid:15)w, yF, XG(cid:16) ∈ V such that w (cid:5)L v. In addition, we define NDLH : L × L (cid:7)→ L which, for any pair of label sets (V, W), returnsH (V, W) ⊆ V containing one label per set of labels of V having the same generalized Lorenz vector heuristicallya set NDLH (V, W),undominated by any generalized Lorenz vector of a label in W . In other words, for any V = (cid:15)v, xD, XE(cid:16) ∈ NDL(cid:21) = (cid:15)w, yF, XG(cid:16) ∈ W such that w (cid:5)L v + h, where (cid:15)h, xH, XE(cid:16) is the only label that agrees with xD in HEthere exists no V(as defined in function Heuristic_Collect). Using, NDL and NDLH , we can now provide the counterpart of functionPareto∗H except on lines 04, 09 and 10 whereLorenz dominance is used to discard complete instantiated labels that are dominated. Note in particular that functionmove_label remains unchanged and still uses Pareto dominance and Corollary 2 to prune labels.H for the Lorenz dominance. Function Lorenz∗H is identical to Pareto∗∗ = (10, 20), x, XCpThe pruning rules using function NDL of lines 04, 09 and 10 can be illustrated on a simple example: assume thatwe wish to add on line 10 label (cid:15)w = (8, 3), aib j, A(cid:16) into Lopen. In addition, assume that there exists in LLorenz a label(cid:15)u(cid:16) as shown in Fig. 9. Finally, assume that, for all the instantiations of the attributes of the gray areaof Fig. 9 compatible with ai and b j , the corresponding utility vectors are Pareto dominated by v = (4, 4). Then operator∗ = (10, 20) (cid:12)(cid:2)P w + v = (12, 7). However,ND, as used in the preceding section, cannot be exploited to prune w because u∗) = (10, 30) (cid:2)P L(v + w) = (7, 19). This explains why,Lorenz dominance operator NDL can be used to prune w because L(uin practice, function Lorenz∗H. It is important to note that pruning a vector wcorresponding to a complete assignmentusing L-dominance can only be achieved through a comparison with a vector uof the attributes. As shown in Example 3, we cannot extend this pruning rule to utility vectors ucorresponding to onlypartial assignments of the attributes. As a consequence, in Algorithm 7, functions initial_labels and move_labelmust be the same as in Algorithm 4, i.e., they must use Pareto dominance, not Lorenz dominance. As in Pareto∗H, thereare different possible strategies to define what the most promising label should be. In our experiments, we simply definedit as nearest label to the root clique and, to break ties, that with the highest sum over the M objectives of the Lorenzvector of the sum of its utility vector and its heuristic vector.H is much faster than function Pareto∗∗∗C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791171Algorithm 7: Efficient best-first Lorenz search algorithm.H()Function Lorenz∗01 let root XCp be any clique02 Lopen ← ∅; call initial_labels( XCp , XCp )03 for all Separators XS ji do call Heuristic_Collect( XCi , XC j ) done04 LLorenz ← NDL (Mpp ); Lopen ← NDL05 while Lopen (cid:12)= ∅ dolet (cid:15)w, xD, XSi j06(cid:16) from Lopenremove (cid:15)w, xD, XSi j0708 V ← move_label((cid:15)w, xD, XSi j091011 done12 return LLorenz= root XCp then LLorenz ← NDL (LLorenz ∪ V); Lopen ← NDLif XSi jelse Lopen ← Lopen ∪ NDL(cid:16) be the most promising label in LopenH (Lopen, LLorenz)H (V, LLorenz)(cid:16))H (Lopen, V)Proposition 14. Given a GAI tree G, function Lorenz∗H() returns the Lorenz-optimal set.Proposition 15. Lorenz∗H() requires space O (km ×number of cliques in the GAI network, d is the largest attribute’s domain size, wattributes in the largest clique minus one) and K i is a bound on utility ui .i=1 K i × dwm(cid:2)∗) and time O (km log m ×∗+1), where k is theis the network’s induced width (i.e., the number ofmi=1 K 2i× dw∗(cid:2)4.3. Ordered weighted averagesAlthough L-dominance is a refinement of Pareto dominance used to capture an idea of fairness in comparisons, it isstill a partial relation and as such, not always sufficient to discriminate between multiple feasible solutions, as shown byProposition 12. This is the reason why several inequality measures have been proposed in the literature to refine Lorenzdominance. Among them, preference weak-orders induced by Ordered Weighted Averages (OWA) [40] appear as natural ex-tensions of L-dominance. As shown in [41], under reasonable axioms such as compatibility with L-dominance, completenessof preferences, continuity and comonotonic independence, the only possible model is an ordered weighted average usedwith decreasing weights. This result is consistent with those of Ogryczak [42] that justify the use of OWA operators inequitable optimization. OWA operators are formally defined as follows:Definition 9. The family of Ordered Weighted Averages (OWA) is a class of aggregators that assign weights to ranks and thatperform a linear combination of scores, once they have been ranked. More formally, for any utility vector u ∈ Zm+, the OWAis defined by:OWA(u) =m(cid:5)i=1w i u(i) =m(cid:5)(w i − w i+1)Li(u)i=1where w 1 > w 2 > · · · > wm > wm+1 = 0 and u(1) (cid:5) u(2) (cid:5) · · · (cid:5) u(m) represent the components of u = (u1, . . . , um) sortedby increasing order.Note that the most important weights are attached to least satisfied agents, consistently with the intuitive idea of egali-tarianism. Note also that OWA(u) can be expressed as a linear combination of Lorenz components Li(u), and the coefficientsinvolved in the combination are strictly positive (the weights w i strictly decrease as i increases). In this case, the OWA func-tion obviously provides a weak-order refining L-dominance. Unfortunately, the determination of an OWA-optimal solutionin X is NP-hard:Proposition 16. As soon as | Xi| (cid:3) 2 and m (cid:3) 2, the problem P α consisting of deciding whether there exists an element x ∈ X of utilityvector u(x) such that OWA(u(x)) (cid:3) α, for a fixed positive integer α, is a NP-complete decision problem.The procedure used for Lorenz can easily be adapted to compute optimal OWA elements within X . For computing thebest element according to OWA, we just need to redefine two functions NDOWA and NDOWA, as we did for Lorenz. Wethus define a function NDOWA : L (cid:7)→ L which, for any set of labels V , returns a set NDOWA(V) ⊆ V containing one label per(cid:21) = (cid:15)w, yF, XG(cid:16) ∈ Vset of labels of V having the same OWA, i.e., for any V = (cid:15)v, xD, XE(cid:16) ∈ NDOWA(V), there exists no V: L × L (cid:7)→ L which, for any pair ofsuch that w (cid:5)OWA v, i.e., such that OWA(w) > OWA(v). In addition, we define NDOWA(V, W) ⊆ V containing one label per set of labels of V having the same OWA non-label sets (V, W), returns a set NDOWAdominated by the OWA of any label’s vector of W . In other words, for any V = (cid:15)v, xD, XE(cid:16) ∈ NDOWA(V, W), there exists no(cid:21) = (cid:15)w, yF, XG(cid:16) ∈ W such that w (cid:5)OWA v + h, where (cid:15)h, xH, XE(cid:16) is the only label that agrees with xD in HE (as defined inVfunction Heuristic_Collect). Now, replace in function Lorenz∗respectivelyH the NDL and NDLH by NDOWA and NDOWAHHHHH1172C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Fig. 15. Pruning rule for OWA: prune w whenever OWA(u∗) (cid:2) OWA(w + h(w)).and the resulting algorithm determines the optimal element w.r.t. OWA. The notion of a most promising label must beupdated as well. We simply defined it as the nearest label to the root clique with the highest OWA value of the sum ofis illustrated in Fig. 15: for any label (cid:15)w, aib j, A(cid:16),its utility vector and its heuristic vector. The pruning rule with NDOWAlet h(w) be a vector that Pareto dominates all the utility vectors corresponding to the instantiations of the attributes in∗) (cid:3) OWA(w + h(w)), then w can be safely discarded because it cannot be part of a solution ofthe gray area; if OWA(uNDOWA(U).H4.4. Weighted Tchebycheff distancesThe previous procedures were proposed in the context of fair multiagent decision making. We can easily adapt suchprocedures to single-agent but multicriteria decision problems. In this case, GAI utility functions u1, . . . , um represent criteriadefined from different subsets of attributes referring to different viewpoints about the solutions (e.g., security, velocity,space, aesthetics for a car, as already mentioned in the introduction). In this context, the notion of fairness is replacedby the notion of well-balanced compromise solution. In order to explore the possible compromise solutions in the Paretoset, a classical approach in multicriteria optimization is to generate Pareto-optimal solutions by minimizing the followingscalarizing function (Wierzbicki [43]; Steuer and Choo [38]):(cid:9)(cid:17)(cid:17)(cid:8)(cid:7)(cid:6)f w (x) =¯u − u(x)(cid:17)(cid:17)w∞= maxi∈M(cid:18)(cid:18)(cid:18) ¯ui − ui(x)(cid:18)w iwhere ¯u = ( ¯u1, . . . , ¯um) represents an ideal utility profile and w is a positive weighting vector. The choice of the Tchebycheffnorm focuses on the worst component and therefore guarantees that only feasible solutions close to reference utility vector¯u on every component will receive a good score. This promotes well-balanced solutions. Function f w fulfills two importantproperties [43]:Property 1. If ∀i ∈ M, w i > 0 then all solutions x minimizing f w over the set X are weakly Pareto-optimal (i.e. no feasiblesolution can perform better on all criteria simultaneously). Moreover at least one of them is Pareto-optimal.Property 2. If ∀i ∈ M, ¯ui > supx∈X ui(x), then for any Pareto-optimal solution x ∈ X , there exists a weighting vector w suchthat x is the unique solution minimizing f w over X .Property 1 shows that minimizing f w yields at least one Pareto-optimal solution. Property 2 shows that any Pareto-optimal solution can be obtained with the appropriate choice of parameter w. This second property is very important. Itprevents excluding a priori good compromise solutions. Yet, it is not satisfied by usual linear aggregators:Example 4. Consider a problem with 3 criteria and assume that X = {x, y, z, t} with u1(x) = 0, u2(x) = u3(x) = 100, u2( y) =0, u1( y) = u3( y) = 100, u3(z) = 0, u1(z) = u2(z) = 100, u1(t) = u2(t) = u3(t) = 65. All solutions except t are very bad withrespect to at least one criterion. Thus t is the only reasonable compromise solution and it is Pareto-optimal; yet it cannot beobtained by maximizing a linear combination of individual utilities (with positive coefficients) because it does not belongto the boundary of the convex hull of feasible utility vectors.Fig. 16 represents a feasible area and different Pareto-optimal compromise solutions that can be obtained by minimizinga weighted Tchebycheff distance, for different weights. Among them, only the filled points can be obtained by maximizationof a linear combination of criteria.Example 4 and Fig. 16 explain why f w , as a scalarizing function, is preferred to a weighted sum in multiobjectiveoptimization on non-convex sets [43,38]. This remark is important because the optimization of a weighted sum of criteriawould have been an easier problem. Indeed, a weighted sum of GAI decomposable functions is still GAI decomposable.Hence optimizing a weighted sum of criteria amounts to finding the optimal tuple in a GAI network. As mentioned in[21] this problem can be solved with standard non-serial dynamic programming [12] as for the computation of the mostplausible explanation (MPE) in Bayesian networks [13]. On the contrary, weighted Tchebycheff distances, as introducedabove, are not GAI decomposable. In [44] we show that finding a solution x in X that minimizes the Tchebycheff criterionis a NP-hard problem and we propose a solution procedure. It relies on a ranking algorithm enumerating solutions accordingC. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791173Fig. 16. Compromise solutions minimizing weighted Tchebycheff norms.Table 1Performance of our algorithms on biobjective problems derived from classical benchmarks.FileGEOM30a_4GEOM40_2dubois30dubois50dubois100pret150_25pret150_40pret150_75hailfinderinsurancen3040901503001501501505627∗wPar∗H65333889480.3150.0190.0540.1520.7511.0320.7502.04829.30228.279Lor∗H0.3170.0080.0460.1210.6370.9980.7051.84228.55328.271OWA0.2860.0070.0410.1060.5680.8230.5971.64427.80828.151#L62141162341#ParFile1618386611555565716958kbtree5_2_4_5_10_1kbtree5_2_4_5_30_1kbtree5_2_4_5_50_1kbtree5_2_4_5_70_1kbtree5_2_4_5_90_1cnf2.40.100.730621cnf2.40.100.730623cnf2.80.100.735545cnf2.80.100.735549alarmn62626262624040808037∗wPar∗H Lor∗H OWA5555511126644.3355.1093.5303.0263.6920.7461.6300.0500.0380.1723.8634.6073.1362.7173.3220.7301.6220.0440.0340.1503.1964.2032.7832.3933.0720.6721.4330.0380.0300.128#L33534351223#Par46474242491610161792to the weighted sum of criteria until a boundary condition is reached that guarantees that the optimal solution is found.We propose here an alternative approach based on our Pareto∗H procedure. We first fix the components of the ideal pointas ¯ui = supx∈X ui(x) + 1, i = 1, . . . , m, each value supx∈X ui(x) being obtained by a monocriterion optimization using theGAI net. Then we implement a focused search procedure as for OWA, just by replacing OWA by the Tchebycheff criterion.This approach has been implemented and tested on random instances using a weighting vector w fixed so as to generatewell-balanced compromise solutions within the Pareto set (see [1,45]). The experiments are presented in Section 5. Notethat this approach easily generalizes to any aggregation function, provided it is monotone with respect to Pareto dominance.5. Numerical testsIn order to evaluate the performance of our algorithms, we performed experiments on a 2 GB PC equipped witha 3.6 GHz Pentium 4 running the aGrUM2 graphical model library. For the first set of experiments, we showed thatour algorithms perform well on network structures found in practice. We thus used classical benchmarks available onhttp://carlit.toulouse.inra.fr/cflibtars. As our algorithms return exact – not approximate – solutions, we limited the experi-∗ (cid:5) 15. In the repository, benchmarks are mono-objective problems, hence wements on networks with induced width wmapped them into multiobjective ones. To this end, for each objective we generated a utility decomposable according tothe GAI network of the mono-objective problem. More precisely, for each utility ui of the mono-objective problem, wejcomputed its minimal and maximal values u∗ and ui by∗drawing random values between u∗ and u. In the end, the multiobjective GAI network had thus the same structure as themono-objective one. For biobjective problems, we evaluated the run times of Pareto∗H and OWA (performancesfor Tchebycheff distance are similar to OWA). The results are displayed in Table 1. In this table, n refers to the number ofattributes, wto the induced width of the GAI network (as computed by our triangulation algorithm), columns #L and #Parshow the number of Lorenz-optimal and Pareto-optimal elements respectively. All the other columns report average runtimes in seconds over 100 experiments.As could be expected, Lorenz∗H, essentially because the number of Lorenzundominated elements is much smaller than that of Pareto undominated ones. Note however that most of the attributes ofH and OWA usually outperform Pareto∗, and we generated for each objective j a corresponding utility uH, Lorenz∗∗∗2 See http://agrum.lip6.fr.1174C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Table 2Performance on biobjective problems when all attributes’ domain sizes are 4.FileGEOM30a_4GEOM40_2dubois30dubois50dubois100pret150_25pret150_40pret150_75hailfinderinsurancen3040901503001501501505627∗wPar∗H65333889481.0010.39411.61654.6271047.16–––25.888–Lor∗H1.0370.42310.91151.974949.368–––23.067–OWA0.9360.3529.90448.046905.818–––21.095–#L4441510–––5–#ParFile65643105301451–––178–kbtree5_2_4_5_10_1kbtree5_2_4_5_30_1kbtree5_2_4_5_50_1kbtree5_2_4_5_70_1kbtree5_2_4_5_90_1cnf2.40.100.730621cnf2.40.100.730623cnf2.80.100.735545cnf2.80.100.735549alarmn62626262624040808037Table 3Run times for random problems with 5 criteria.n101112131415161920Par1.5198.19931.51255.833162.425427.1372050.512––Par∗H0.4513.92711.99523.38944.526104.028105.5771620.304–Lor∗H0.2370.3987.5067.9036.05576.94160.467392.702512.344OWA0.1960.3017.1437.3225.02273.70756.931359.253484.233555551112664#L7215922951113∗wPar∗HLor∗H10.18810.2747.51610.5028.169––––11.95511.7819.09912.2999.955––––2.7342.643OWA#L#Par9.4717.5757.0359.4927.074––––2.41685883––––7269199304243254––––103#Par29577134889111 48416 92822 67633 33442 65545 245Tcheb0.2030.3267.2587.4735.23174.41958.092367.835497.613these instances are Boolean, which is seldom the case in decision problems. Significant exceptions are problems hailfinderand insurance, which are actually Bayesian networks with attributes of domain sizes up to 11. For such problems, we cansee that run times are much higher than for the other experiments.To test the behavior of our algorithms in a more decision-theoretic framework, we performed a second round of exper-iments using the same network structures as in Table 1 but now with all attributes of domain size 4. For each instance,we filled the utility tables with numbers drawn randomly between 0 and 20. The run times for biobjective problems aresummarized in Table 2 (“–” indicate when the program failed due to the lack of memory space available). Note that, in suchcontext, the Pareto sets and the run times are much bigger than those of Table 1.Finally, for the last set of experiments, we studied how the algorithms behaved in the presence of multiple conflictingcriteria (actually, we chose 5 criteria/objectives). The experiments of Tables 1 and 2 are not appropriate for this purpose be-cause such problems are much harder to solve than biobjective ones and run times are too prohibitive. Hence, we randomlygenerated GAI networks with all cliques of size 3 and separators of size 2, and all attributes of domain size 4. For eachclique, we generated 5 different utility tables (one for each objective) with numbers drawn randomly between 0 and 20.The run times of the algorithms (in seconds) are displayed in Table 3 (“–” indicate instances where the program faileddue to a 2400 s timeout). Column Tcheb shows the times for Tchebycheff optimization (for a fixed set of weights). Notethat Pareto∗H, OWA and Tchebcompared with Pareto∗H and Pareto. This shows that preference-based optimization can be performed even when thePareto set cannot be computed.H significantly outperforms Pareto (column “Par”). Note also the efficiency of Lorenz∗6. ConclusionWe have shown that GAI networks can be used efficiently to handle preferences in decision problems involving multipleobjectives, provided the objectives can be modeled by GAI decomposable utility functions. In particular, it is possible to storem different GAI functions into a single GAI network endowed with local vector-valued utility tables. Then we have proposeda heuristic search procedure exploiting the GAI structure to compute all Pareto-optimal elements. This procedure bears some[30] used in state space graphs, but it works onsimilarity with labels propagation algorithms such as multiobjective MOAa junction tree and must satisfy compatibility constraints induced by separators. The procedure we propose also bears somesimilarity with multiobjective approaches to constraint satisfaction problems [46–48] with some specificities linked to (i) theuse of heuristic information and (ii) the management of labels candidate to expansion (they can belong to different cliques,we do not require to treat all Pareto-optimal labels of a given clique before propagation). These specificities make it possibleto modify the initial procedure so as to determine the preferred solutions for any preference model compatible with Paretodominance. This is not the case of ranking approaches proposed in [21] that only apply to concave utility functions.∗We have provided various examples where our approach appears to be useful, first with Lorenz dominance and OWAmodels for fair multiagent decision making, and then with weighted Tchebycheff distances for multicriteria problems. Notethat related approaches have been used successfully to perform fair optimization or compromise search in multiobjectiveC. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791175shortest path problems [41,45]. This paper shows that such focused search algorithms specializing Pareto search can alsobe imported into Graphical Models to solve a wide range of multiobjective combinatorial optimization problems involvingsophisticated preferences. Knowing that the size of the Pareto set can be huge in combinatorial domains, a useful comple-mentary study might be to design near admissible algorithms to approximate the Pareto set with performance guarantees.Several recent works on multiobjective combinatorial problems have shown the power of approximations in solving largesize instances [5,49,50]. It is likely that such ideas could be imported with benefit in the world of graphical multiobjectiveutility models. A first step in this direction is proposed in [51].Appendix A. ProofsProof of Proposition 1. We establish the proof for m = 2 (biobjective case). The result obviously extends to problems involv-ing more than two objectives. The decision problem P u associated to PO is clearly in NP. To establish NP-completeness, wereduce the decision version of the Knapsack problem, known as NP-complete [52], to our problem. This problem denotedKP can be stated as follows:Instance: a utility vector (v 1, . . . , vn) ∈ ZnQuestion: does there exist x ∈ {0, 1}n such that(cid:3)nj=1 v j x j (cid:3) V andj=1 w j x j (cid:5) W .(cid:3)n+ and a weight vector (w 1, . . . , wn) ∈ Zn+ and two positive integers V and W .Given an instance of KP, we construct in polynomial time an instance of P u with u = (V ,j=1 w j − W ) as follows: weconsider n Boolean attributes: X j = {0, 1}, j = 1, . . . , n, such that, for all x j ∈ X j , u1(x j) = v j x j and u2(x j) = w j(1− x j). Thus,(cid:3)j=1 w j(1 − x j)). We knowto any vector x ∈ {0, 1}n we associate a utility vector defined by (u1(x), u2(x)) = (nj=1 w j x j (cid:5) W . By construction of the utility functions,j=1 v j x j (cid:3) V andthat the answer to KP is YES if and only ifthese two inequalities are equivalent to u1(x) (cid:3) V and u2(x) (cid:3)j=1 w j − W , meaning that the answer to P u is YES. Thisshows that P u is at least as hard as KP. (cid:2)(cid:3)nj=1 v j x j,(cid:3)n(cid:3)n(cid:3)n(cid:3)nProof of Proposition 2. Assume that u1(xD) (cid:5)P u1( yD), then, by definition, for any xE ∈ XE, u1(xD) + u2(xE) (cid:5)P u1(xD) +u2(xE). Hence, no vector in ND(U) can result from the addition of a vector u2(xE) to u1( yD). For the same reason, novector in ND(U) can result from the addition of a vector u1(xD) to a dominated vector u2(xE). As U = {u(xD), xD ∈ XD} (cid:4){u(xE), xE ∈ XE} since D ∩ E = ∅ and D ∪ E = N, the result obtains. (cid:2)Proof of Corollary 1. By the running intersection property (Property 3 of Definition 2), D1 ∩ D2 = ∅. Hence U =(cid:4)}). Now, when the values of the attributes XS12 are fixed∈ XD2to, say, xS12 , u1(xD1 , xS12 ) and u2(xD2 , xS12 ) become subutilities defined over XD1 and XD2 respectively. As D1 ∩ D2 = ∅, inthis case, u1 + u2 is an additive utility and the application of Proposition 2 completes the proof. (cid:2)} (cid:4) {u2(xD2 , xS12 ), xD2({u1(xD1 , xS12 ), xD1∈ XD1∈ XS12xS12Proof of Corollary 2. The utility function u defined by the GAI network can be decomposed as u = u1 + u2, with u1 : XD ×(cid:3)r). Now we are int=1 ut(xCitXSi jthe conditions of application of Corollary 1 and, thus, the result obtains. (cid:2)+ defined as u1(xD, xSi j ) =(cid:3)kt=r+1 ut(xCit+ and u2 : XE × XSi j) and u2(xE, xSi j ) =(cid:7)→ Zm(cid:7)→ ZmProof of Proposition 3. Function Pareto() is completed in a finite number of steps else function Pareto_Collectwould call itself an infinite number of times. But, by induction, it is easily seen that when Pareto_Collect( XCk , XCi ) iscalled on line 03, clique XCi is “between” XCk and root, so that, as G is a tree, there can be only a finite number of callsof Pareto_Collect.Now,Pareto_Collect( XCi , XC j ) transforms uilet us prove by induction that Pareto() returns ND(U). Consider first the leaves XCi of the GAI tree:. Byinto label set Mi j and projects it on XSi j , that is, it computes Mi j ⇓ XSi jCorollary 2 applied on separator XSi j , only the subutility vectors of ui that are undominated for fixed values xSi j ofXSi j need be taken into account for computing ND(U). This corresponds precisely to label set Mi j ⇓ XSi j. By inductionhypothesis, assume that all the label sets Mki of line 04 correspond to the undominated vectors described in Corol-∈ XSi j ,lary 2 for fixed values of XSki . To apply Corollary 2 on separator XSi j , we should compute, for each value xSi jVxSi j), xD ∈ XD}), where the Cit , t = 1, . . . , r are XCi and all the cliques having XCi on their path to-), xD ∈ XD} butward root, and where D =rather a subset WxSi jwhere the discarded elements are those that are known to be dominated (by Corollary 2). HenceND(VxSi j) = ND(WxSi j). So, each call to Pareto_Collect returns a set of labels that are undominated for each value ofseparator XSi j .t=1 Ct\Si j . The for loop of lines 02–05 does not compute exactly {(cid:3)rt=1 ut(xCit(cid:3)rt=1 ut(xCit= ND({Finally, for each clique XCi , the loop of lines 02–05 parses all the neighbors of XCi except that which leads to root, hencethe whole of the GAI net has been parsed when function Pareto() is completed. As a consequence, the labels in Mpp(cid:4)r1176C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179computed by Pareto() correspond to utility values u of complete instantiations. Moreover, by the recursive applications ofCorollary 2, we know that ND(U) ⊆ Mpp . As the final step returns ND(Mpp), function Pareto() returns u’s Pareto set. (cid:2)Proof of Proposition 4. See the proof of Theorem 3 of [28]. (cid:2)Proof of Proposition 5. Consider a call to initial_labels( XCi , XC j ) where XCi is a leaf of the GAI tree. Then on line 06,V = (Labels( XCi ))⇓ XSi j. By applying Corollary 2 with separator XSi j , we know that it cannot be the case that a label ofLabels( XCi )\V be part of a Pareto element of ND(U). On lines 07 and 08, V is partitioned into message Mi j and W =V\Mi j , the latter being added to Lopen. As a consequence, for any utility vector u(x) ∈ ND(U), either (cid:15)ui(xCi ), xCi , XSi j(cid:16)belongs to W ⊆ Lopen and Property 2 of the proposition holds (with r = 1 and j1 = i), or (cid:15)ui(xCi ), xCi , XSi j(cid:16) ∈ Mi j .Now let XCi be a clique that is not a leaf. Let XC j2path toward clique XCp . Clearly, initial_labels( XCi , XC j ) recursively calls on line 03 initial_labels( XC jtfor t = 2, . . . , r, where XChtinduction hypothesis that, for any vector u(x) ∈ ND(U), one of the two following cases obtains:which is on the path between XC jtdenotes the clique adjacent to XC jtdenote the set of all the cliques that have XCi on their), XChtand root. Assume by, . . . , XC jr(i) there exists a clique XC jt, t ∈ {2, . . . , r}, such that initial_labels( XC jt, XCht) created a new label in Lopen satisfying(cid:16) ∈ M jt ht such that yD = xD, i.e., a label that, when combined appropriately, willProperty 2 of Proposition 5,(ii) there exists a label (cid:15)w, yD, XS jt htproduce (cid:15)u(x), x, XCp(cid:16).. Let u(x) be any element of ND(U). If thereLet us prove that, then, this will also hold for the set of cliques XCi , XC j2, t ∈ {2, . . . , r}, such that (i) holds, then the result is obvious. Hence assume that, for all t ∈ {2, . . . , r},exists a clique XC jt(ii) holds. In particular, it holds for the neighbors of clique XCi , which means that, on line 04, each label set Mki containsa label corresponding to a partial instantiation of x. So, at the end of the for loop of lines 02–05, V necessarily contains(cid:4)a label (cid:15)w, xD, XCirt=2 C jt because the labels corresponding to partial(cid:16) were discarded on line 06, this would mean that it isinstantiations of x can be combined together. If label (cid:15)w, xD, XCidominated by another label for fixed value xSi j of separator XSi j . But this is impossible because, by Corollary 2, this wouldimply that u(x) is necessarily dominated and, thus, that u(x) /∈ ND(U). Consequently, either label (cid:15)w, xD, XCi(cid:16) is insertedinto message Mi j on line 07 or it is inserted into Lopen on line 08.(cid:16) such that w = ui(xCi ) +(cid:3)rt=2 u jt (xC jt) and D = Ci, . . . , XC jrThe application of this induction up to clique XCp completes the proof. (cid:2)Proof of Proposition 6. First, Pareto∗() executes a finite number of steps: clearly the call to initial_labels( XCp , XCp )ends in a finite number of steps since G is a tree. In addition, the number of elements it inserts into Lopen is finite sincethe size of each message Mi jis the domain size of XSi j . Each time we go through the while loop of lines 04–09, anelement is removed from Lopen on line 06, hence, if function Pareto∗() did run infinitely, this would mean that an infinitenumber of new elements would be added to Lopen on line 08. Now this is impossible because these elements, i.e., set V ,are those which result from a move of a given label. But then, by function move_label, these new labels are the only(cid:16) with other labels that are located on neighbors of clique XC j . In other words, these newones yet that combine (cid:15)w, xD, XSi jlabels correspond to new partial instantiations of the attributes. As the number of possible partial instantiations is finite,Pareto∗() terminates in a finite number of steps.Note that a given label can never belong both to Lopen and to a message Mtl. This property clearly holds before thewhile loop because function initial_labels never inserts twice the same label on its lines 07 and 08. In the whileloop of Pareto∗(cid:16) is removed from Lopen before being added to Mi j by function move_label. Finally,label set V created on line 07 of Pareto∗contains only new labels, as mentioned in the preceding paragraph. So we canadd them to Lopen: they do not belong to any message Mtl yet., label (cid:15)w, xD, XSi jLet us now prove that, at each step, one and only one of the assertions of Proposition 5 holds (where Mpp is substitutedby LPareto). Clearly, before the while loop, this holds. Let (cid:15)w, xD, XSi j(cid:16) be the label chosen on line 05. For all vectors u(x) ∈ND(U) such that there exists an agreeing label (cid:15)v, yE, XS j1l(cid:16) as defined in assertion 2 of Proposition 5, with j1 (cid:12)= i, thenafter executing lines 06–08, this label will still exist in Lopen since the only line that removes labels from Lopen is line 06(cid:16) since j1 (cid:12)= i. But, then, there cannot exist a label in LPareto corresponding toand the label removed cannot be (cid:15)v, yE, XS j1lu(x) because this label would correspond to a complete instantiation x and, thus, (cid:15)v, yE, XS j1l(cid:16) would have been combinedwith other labels (which is not the case since it belongs to Lopen). Let now u(x) ∈ ND(U) be a vector such that thereexists only one agreeing label in Lopen and this label is precisely that which is chosen on line 05, that is, (cid:15)w, xD, XSi j(cid:16).(cid:16)) first computes on lines 01–06 the label set V =For such label, using the notations of Fig. 7, move_label((cid:15)w, xD, XSi j(cid:16) ⊗ Mk1 j ⊗ · · · ⊗ Mkr j} and, then, projects this set on line 07. It is easy to prove by induction that{Labels( XC j ) ⊗ (cid:15)w, xD, XSi jeach message Mkt j , t = 1, . . . , r, contains a label agreeing with u(x) else (cid:15)w, xD, XSi j(cid:16) would not be the only label agreeingwith u(x) in Lopen. Hence V necessarily contains a label agreeing with u(x). As u(x) ∈ ND(U), this label cannot be discardedon line 07 of move_label. So the set V on line 07 of Pareto∗contains a label agreeing with u(x) and, on the next line,it is either inserted into LPareto or Lopen, so that one and only one of the assertions of Proposition 5 holds again. Finally,C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791177(cid:21)consider a vector u(x) ∈ ND(U) the label L of which belongs to LPareto. Then label set V mentioned above cannot containthat agrees with u(x) because this one would correspond to the same partial instantiation as L. But then,another label L(cid:21)label Lis a new label.Hence the property also holds in this case.would already have been combined with other labels to produce L, which is impossible since LNow, to complete the proof, we know by the preceding paragraph that for each u(x) ∈ ND(U), either there exists a labelreturns, Lopen is(cid:16) ∈ LPareto or there exists a label in Lopen that agrees with u(x). When function Pareto∗(cid:15)u(x), x, XCpempty, so LPareto contains the Pareto set and, by line 08, it is precisely equal to ND(U). (cid:2)(cid:21)Proof of Proposition 7. Vectors on separators and within Lopen are precisely those sent on the separators by functionPareto. Hence the space complexity of Pareto∗is identical to that of Pareto. As for the time complexity, the combina-tions of sets of labels and their projections differ from Pareto only in the order in which they are done. In addition, labelsselected on line 05 can be determined in O (1). Hence Pareto∗time complexity of is equal to that of Pareto. (cid:2)Proof of Proposition 8. Proof by induction. On the leaves, by construction, labels Hir[xSir] obviously Pareto dominate theur(xSir ). Now, the for loop of lines 02–05 computes V = Labels( XCi ) ⊗ Hir1} = Adj( XCi )\ XC j .Assume as induction hypothesis that each Hirt weakly Pareto dominates the sum of the subutilities of the cliques XCs suchis on their path toward XCi . Then, clearly, V weakly Pareto dominates the sum of the subutilities of XCi and of thethat XCrtcliques XCs such that XCi is on their path toward XC j . Now, by definition of Max↓ XSi j, the same property holds for H ji asdefined on line 06. (cid:2)⊗ · · · ⊗ Hir p , where { XC1 , . . . , XCpProof of Proposition 9. In a “usual” scalar collect algorithm, the space and time complexities are known to be O (k × dwand time O (k × dwHence the overall complexities. (cid:2))∗+1) respectively. Here, the only difference is that we do not manipulate scalars but vectors of size m.∗Proof of Proposition 10. The only difference between Pareto∗is that the former prunes Lopen usingoperator NDH on lines 04, 09 and 10. But, by Proposition 8, the only labels that can be pruned are those that can onlyproduce at the root Pareto dominated labels. Hence, discarding such labels cannot remove any element from ND(U). AsPareto∗was proved to return the Pareto set, Pareto∗H and Pareto∗H must return it as well. (cid:2)(cid:2)Proof of Proposition 11. There are fewer undominated vectors than K =labels on Lopen and on separators than kK dwthan dw∗mi=1 K i . As a consequence, there are fewer possiblebecause there are at most k separators and each separator’s size is lower∗+1 and, for each separator’s value, there are fewer than K undominated vectors. Hence the space complexity.As for the time complexity, if we do not take into account the prunings, we perform the same operations as Pareto∗()except that we actually do them on fewer labels. So, the only difference lies in the additional domination tests. When thelabel moved on line 08 reaches the root, LPareto is updated on line 09, which means that we compare all pairs (x, y)where x ∈ LPareto and y ∈ V . Thus, along the whole execution of the function, we cannot perform more than K 2 tests for∗+1). On line 09, Lopen is updated as well. Note that theeach value of the root clique, hence a complexity of O (mK 2dwlabels in Lopen are those that will be stored later on separators, hence the size of Lopen never exceeds kK dw. In addition,labels of Lopen are always compared on line 09 with new labels, that is, labels that were not yet part of LPareto. As a∗+1. Finally, when Lopen is updated on line 10, noteconsequence, on overall, the time complexity of all these tests is kK 2dwthat the elements in V are some of those sent by Pareto∗on separator XSi j . As a consequence, during the whole executionelements are stored successively in V . As there are fewer elements than K in LPareto,of the algorithm, fewer than kK dwthe time complexity obtains. (cid:2)∗∗(cid:3)nj=1 uij (x j) = 2 j−1x ju1Proof of Proposition 12. We consider instances of LO with two objectives (m = 2) on a set X =j=1 X j , where X j = {0, 1},j = 1, . . . , n. Assume that the objectives are additive utility functions defined, for any Boolean vector x = (x1, . . . , xn) ∈ X ,by ui(x) =j is a marginal utility function defined on X j by:j(x j), i = 1, 2, where uiand u2j (x j) = 2 j(1 − x j) +2n − 1j = 1, . . . , n.(cid:3)n(cid:6)(cid:9)/n,(cid:3)nj=1 2 j−1x j and u2(x) = 2Then for all x ∈ {0, 1}n, u1(x) =j=1 2 j−1x j we get:u1(x) = z and u2(x) = 2(2n − 1) − 2z + 2n − 1 = 3(2n − 1) − 2z. Hence there exist 2n different Boolean vectors in X , withdistinct images in the utility space of the form {(z, 3(2n − 1) − 2z), z ∈ {0, . . . , 2n − 1}}. Note that the second component isalways greater than or equal to the first one for z ∈ {0, . . . , 2n − 1}. Consequently, the corresponding set of Lorenz vectorscan be written as {(z, 3(2n − 1) − z), z ∈ {0, . . . , 2n − 1}}. All these Lorenz vectors have their two components adding to3(2n − 1). Consequently, they are all located on a same line orthogonal to vector (1, 1) which proves that all these vectors arePareto-optimal. Hence all initial utility vectors are Lorenz-optimal which proves that NDL(U) = U. Clearly, in such instances,the size of the set of L-non-dominated elements grows exponentially with the number of attributes, even if the number ofcriteria is fixed to 2. (cid:2)j=1 2 j−1(1 − x j) + 2n − 1. Let z =(cid:2)n(cid:3)n1178C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–1179Proof of Proposition 13. The problem is clearly in NP. To establish NP-completeness, we reduce the partition problem,known as NP-complete [52], to our problem. The partition problem is stated as follows:Instance: finite set A = {a1, . . . , am} of items and a weight s(ai) ∈ N for each ai ∈ A.Question: is it possible to partition A into two sets of objects of equal weights?(cid:3)ai ∈ A s(ai)/2. We construct in polynomial time an instance of Lu with m = 2 criteria and n BooleanLet u = (β, β) with β =j = 1, . . . , n, such that, for all x j ∈ X j , u1(x j) = s(a j)x j and u2(x j) = s(a j)(1 − x j). Thus, to anyattributes: X j = {0, 1},partition of A of type (B, A \ B), B ⊆ A, we associate a Boolean vector xB ∈ X with n = | A| components (xB= 1 if and only(cid:3)iif i ∈ B). By construction, the image of xB in the utility space is vector (a∈ A\B s(a)). Hence, the answer to Lu isYES if and only if the answer to the partition problem is YES. Indeed, if there is a solution to the partition problem, thenthere exists a partition with utility (β, β) and the corresponding Boolean vector in X is a solution of Lu . Moreover, if theanswer to the partition problem is NO, then any partition of A into two subsets is unfair and the corresponding Booleanvector has a utility of type (β − j, β + j), where j is a positive integer no greater than β. Since L(β − j, β + j) = (β − j, 2β)and L(u) = (β, 2β) we have u (cid:5)L (β − j, β + j) and the answer to Lu is NO. (cid:2)a∈B s(a),(cid:3)Proof of Proposition 14. The only differences between Lorenz∗H lie on lines 04, 09 and 10 wherePareto dominance is substituted by Lorenz dominance. Clearly, replacing instructions LPareto ← ND(Mpp) and LPareto ←ND(LPareto ∪ V) by LPareto ← NDL(Mpp) and LPareto ← NDL(LPareto ∪ V) respectively cannot discard any element that is notLorenz dominated. Had we only modified these two instructions, since Lorenz dominance is a refinement of Pareto dom-inance, function Lorenz∗H can be usedanywhere in the algorithm as it prunes labels that we know for sure cannot be part of the solution. Hence Lorenz∗H asdescribed above returns the set of Lorenz-optimal labels. (cid:2)H would thus return the set of Lorenz-optimal elements. As for the heuristic, NDLH and Pareto∗Proof of Proposition 15. The space complexity is the same as Pareto∗the separators. The time complexity is that of Pareto∗to parsing vectors of size m, we also need to sort them, hence a complexity of O (m log m) instead of O (m). (cid:2)H() as we store the same elements in Lopen and inH() × log m because, when we perform dominance tests, in addition(cid:3)Proof of Proposition 16. The proof is similar to the one of Proposition 13. The problem is clearly in NP. To establish NP-completeness, we reduce the partition problem, known as NP-complete [52], to our problem. Let α = (w 1 + w 2)β withβ =ai ∈ A s(ai)/2. From any instance of partition (as introduced in the proof of Proposition 13) we construct in polynomialtime an instance of P α with m = 2 criteria and n Boolean attributes: Xk = {0, 1}, k = 1, . . . , n, such that, for all xk ∈ Xk,u1(xk) = s(ak)xk and u2(xk) = s(ak)(1 − xk). Thus, to any partition of A of type (B, A \ B), B ⊆ A, we associate a Booleanvector xB ∈ X with n = | A| components (xB= 1 if and only if i ∈ B). By construction, the image of xB in the utility spaceiis vector (ai ∈ A\B s(ai)). Hence, the answer to P α is YES if and only if the answer to the partition problemis YES. Indeed, if there is a solution to the partition problem, then there exists a partition with utility (β, β) and thecorresponding Boolean vector in X gets the value OWA(β, β) = w 1β + w 2β = α. Moreover, if the answer to the partitionproblem is NO, then any partition of A into two subsets is unfair and the corresponding Boolean vector has a utility of type(β − k, β + k), where k is a positive integer not greater than β. Then we have OWA(β − k, β + k) = w 1(β − k) + w 2(β + k) =α − k(w 1 − w 2) < α since w 1 > w 2. Hence the answer to P α is NO. (cid:2)ai ∈B s(ai),(cid:3)(cid:3)References[1] R.E. Steuer, Multiple Criteria Optimization: Theory, Computation and Application, John Wiley, 1986.[2] R.L. Keeney, H. Raiffa, Decisions with Multiple Objectives – Preferences and Value Tradeoffs, Cambridge University Press, 1993.[3] B. Roy, Multicriteria Methodology for Decision Analysis, Kluwer Academic Publishers, 1996.[4] M. Ehrgott, Multicriteria Optimization, Springer, 1999.[5] C.H. Papadimitriou, Y. Yannakakis, On the approximability of trade-offs and optimal access of web sources, in: Proceedings of the 41st Annual Sympo-sium on Foundations of Computer Science (FOCS), 2000, p. 86.[6] D. Krantz, R.D. Luce, P. Suppes, A. Tversky, Foundations of Measurement (Additive and Polynomial Representations), vol. 1, Academic Press, 1971.[7] F. Bacchus, A. Grove, Graphical models for preference and utility, in: Proceedings of the 11th Annual Conference on Uncertainty in Artificial Intelligence(UAI), 1995, pp. 3–10.[8] D. Braziunas, C. Boutilier, Local utility elicitation in GAI models, in: Proceedings of the 21st Annual Conference on Uncertainty in Artificial Intelligence(UAI), 2005, pp. 42–49.[9] C. Gonzales, P. Perny, GAI networks for utility elicitation, in: Proceedings of the 9th International Conference on the Principles of Knowledge Represen-tation and Reasoning (KR), 2004, pp. 224–234.[10] C. Boutilier, F. Bacchus, R. Brafman, UCP-networks: A directed graphical representation of conditional utilities, in: Proceedings of the 17th Conferencein Uncertainty in Artificial Intelligence (UAI), 2001, pp. 56–64.[11] R. Brafman, C. Domshlak, T. Kogan, Compact value-function representations for qualitative preferences, in: Proceedings of the 20th Conference inUncertainty in Artificial Intelligence (UAI), 2004, pp. 51–59.[12] U. Bertele, F. Brioschi, Nonserial Dynamic Programming, Academic Press, 1972.[13] D. Nilsson, An efficient algorithm for finding the M most probable configurations in probabilistic expert systems, Statistics and Computing 8 (2) (1998)159–173.C. Gonzales et al. / Artificial Intelligence 175 (2011) 1153–11791179[14] R. Dechter, Bucket elimination: A unifying framework for reasoning, Artificial Intelligence 113 (1999) 41–85.[15] Th. Schiex, H. Fargier, G. Verfaillie, Valued constraint satisfaction problems: hard and easy problems, in: Proceedings of the 14th International JointConference on Artificial Intelligence, Morgan Kaufmann, 1995, pp. 631–637.[16] P. Hansen, Bicriterion path problems, in: G. Fandel, T. Gal (Eds.), Multicriteria Decision Making: Theory and Applications, in: Lecture Notes in Economicsand Mathematical Systems, vol. 177, 1980, pp. 109–127.[17] V. Emelichev, V. Perepelitsa, Multiobjective problems on the spanning trees of a graph, Soviet Mathematics Doklady 37 (1) (1988) 114–117.[18] H. Hamacher, G. Ruhe, On spanning tree problems with multiple objectives, Annals of Operations Research 52 (1994) 209–230.[19] G. Debreu, Continuity properties of Paretian utility, International Economic Review 5 (1964) 285–293.[20] P.C. Fishburn, Interdependence and additivity in multivariate, unidimensional expected utility theory, International Economic Review 8 (1967) 335–342.[21] C. Gonzales, P. Perny, S. Queiroz, GAI networks: Optimization, ranking and collective choice in combinatorial domains, Foundations of Computing andDecision Sciences 32 (4) (2008) 3–24.[22] F. Jensen, An Introduction to Bayesian Networks, Taylor and Francis, 1996.[23] R. Cowell, A. Dawid, S. Lauritzen, D. Spiegelhalter, Probabilistic Networks and Expert Systems, Statistics for Engineering and Information Science,Springer, 1999.[24] U. Kjærulff, Triangulation of graphs – algorithms giving small total state space, Tech. Rep. R-90-09, Department of Mathematics and Computer Science,Aalborg University, 1990.[25] A. Darwiche, M. Hopkins, Using recursive decomposition to construct elimination orders, jointrees, and Dtrees, in: Proceedings of the 6th EuropeanConference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU), 2001, pp. 180–191.[26] F. van den Eijkhof, H.L. Bodlaender, Safe reduction rules for weighted treewidth, in: Proceedings of the 28th International Workshop on Graph-TheoreticConcepts in Computer Science, in: Lecture Notes in Computer Science, vol. 2573, Springer, 2002, pp. 176–185.[27] D. Rose, Triangulated graphs and the elimination process, Journal of Mathematical Analysis and Applications 32 (1970) 597–609.[28] E. Rollon, J. Larrosa, Bucket elimination for multiobjective optimization problems, Journal of Heuristics 12 (4–5) (2006) 307–328.∗[29] L. Mandow, J.P. de la Cruz, A new approach to multiobjective Asearch, in: Proceedings of 19th International Joint Conference on Artificial Intelligence(IJCAI), 2005, pp. 218–223.∗[30] B.S. Stewart, C.C. White III, Multiobjective A[31] R. Carraway, T. Morin, H. Moskowitz, Generalized dynamic programming for multicriteria optimization, European Journal of Operational Research 44 (1), Journal of the Association for Computing Machinery 38 (4) (1991) 775–814.(1990) 95–104.[32] C.C. White III, B.S. Stewart, R.L. Carraway, Multiobjective, preference-based search in acyclic OR-graphs, European Journal of Operational Research 56 (3)(1992) 357–363.[33] R. Korff, W. Zhang, H. Hohwald, Frontier search, Journal of the Association for Computing Machinery 52 (5) (2005) 715–748.[34] G. Shafer, Probabilistic Expert Systems, Society for Industrial and Applied Mathematics, 1996.[35] W. Marshall, I. Olkin, Inequalities: Theory of Majorization and Its Applications, Academic Press, London, 1979.[36] H. Moulin, Axioms of Cooperative Decision Making, Monograph of the Econometric Society, Cambridge University Press, 1988.[37] A. Sen, On Economic Inequality, expanded edition, Clarendon Press, 1997.[38] R. Steuer, E.-U. Choo, An interactive weighted Tchebycheff procedure for multiple objective programming, Mathematical Programming 26 (1983) 326–344.[39] K.M. Chong, An induction theorem for rearrangements, Canadian Journal of Mathematics 28 (1976) 154–160.[40] R. Yager, On ordered weighted averaging aggregation operators in multicriteria decision making, IEEE Transactions on Systems, Man and Cybernetics 18(1998) 183–190.[41] P. Perny, O. Spanjaard, L.-X. Storme, A decision-theoretic approach to robust optimization in multivalued graphs, Annals of Operations Research 147 (1)(2006) 317–341.[42] W. Ogryczak, Inequality measures and equitable approaches to location problems, European Journal of Operational Research 122 (2000) 374–391.[43] A. Wierzbicki, On the completeness and constructiveness of parametric characterizations to vector optimization problems, OR Spektrum 8 (1986)73–87.[44] C. Gonzales, P. Perny, S. Queiroz, Preference aggregation with graphical utility models, in: Proceedings of the 23rd AAAI Conference on ArtificialIntelligence, 2008, pp. 1037–1042.[45] L. Galand, P. Perny, Search for compromise solutions in multiobjective state space graphs, in: Proceedings of the 17th European Conference on ArtificialIntelligence (ECAI), 2006, pp. 93–97.[46] E. Rollon, J. Larrosa, Multi-objective Russian doll search, in: Proceedings of the 22nd AAAI Conference on Artificial Intelligence, 2007, pp. 249–254.[47] S. Bistarelli, F. Gadducci, J. Larrosa, E. Rollon, A soft approach to multi-objective optimization, in: Proceedings of the 24th International Conference onLogic Programming (ICLP), in: Lecture Notes in Computer Science, vol. 5366, Springer, 2008, pp. 764–768.[48] H. Fargier, N. Wilson, Local computation schemes with partially ordered preferences, in: Proceedings of the 10th European Conference on Symbolicand Quantitative Approaches to Reasoning with Uncertainty (ECSQARU), in: Lecture Notes in Computer Science, vol. 5590, Springer, 2009, pp. 34–45.[49] M. Laumanns, L. Thiele, K. Deb, E. Zitzler, Combining convergence and diversity in evolutionary multiobjective optimization, Evolutionary Computa-tion 10 (3) (2002) 263–282.[50] P. Perny, O. Spanjaard, Near admissible algorithms for multiobjective search, in: Proceedings of the 18th European Conference on Artificial Intelligence(ECAI), 2008, pp. 490–494.[51] J.-P. Dubus, C. Gonzales, P. Perny, Multiobjective optimization using GAI models, in: Proceedings of the 21st International Joint Conference on ArtificialIntelligence (IJCAI), 2009, pp. 1902–1907.[52] M. Garey, D. Johnson, Computers and Intractability, W.H. Freeman and Company, 1979.