Artificial Intelligence 297 (2021) 103486Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA unifying look at sequence submodularity ✩Sara Bernardini a,∗a Department of Computer Science, Royal Holloway University of London, Egham, Surrey, TW20 0EX, UKb Department of Mathematical Sciences, Politecnico di Torino, Torino, 10129, Italyc Augmenta Inc., Toronto, M5A 1E1, Canada, Fabio Fagnani b, Chiara Piacentini ca r t i c l e i n f oa b s t r a c tArticle history:Received 11 September 2020Received in revised form 9 January 2021Accepted 16 February 2021Available online 24 February 2021Keywords:SubmodularitySequence submodularityGreedy algorithmsSuboptimal algorithmsDetection problemsSearch-and-trackingEnvironmental monitoringSchedulingRecommender systems1. IntroductionSeveral real-world problems in engineering and applied science require the selection of sequences that maximize a given reward function. Optimizing over sequences as opposed to sets requires exploring an exponentially larger search space and can become prohibitive in most cases of practical interest. However, if the objective function is submodular (intuitively, it exhibits a diminishing return property), the optimization problem becomes more manageable. Recently, there has been increasing interest in sequence submodularityin connection with applications such as recommender systems and online ad allocation. However, mostly ad hoc models and solutions have emerged within these applicative contexts. In consequence, the field appears fragmented and lacks coherence. In this paper, we offer a unified view of sequence submodularity and provide a generalized greedy algorithm that enjoys strong theoretical guarantees. We show how our approach naturally captures several application domains, and our algorithm encompasses existing methods, improving over them.© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).Many real-world applications in engineering and applied science have at their core the selection of sequences of objects that maximize a reward. In information gathering missions, for example, the objects are observations and the goal is to select a sequence of them that maximizes the information gain [1,2]. In a similar fashion, a movie recommender system aims to provide its users with sequences of items that maximize relevance [3,4]. The crucial point in these applications is that the value of the sequence depends not only on the objects belonging to it, but also on their relative order. This is because the value of each object changes based on its position in the sequence.If optimizing over sets is already a daunting task, optimizing over sequences quickly becomes unmanageable when the problem at hand grows. However, the identification of special properties in the objective function helps in making the task more approachable. Submodularity, in particular, has emerged as a powerful feature that can be leveraged to control complexity in the maximization of both set and sequence functions. Submodularity can be understood intuitively as a dimin-ishing return condition. Consider again an information-gathering mission. Each new observation increases the information gain, but it does it to a smaller extent than the previous observations, with gain vanishing at infinity.In areas as variegated as optimization, machine learning, economics, medicine and sensor networks, there has been a vast amount of work on the maximization of submodular set functions (see Section 2). Only recently, the scientific commu-✩This paper is an invited revision of a paper which first appeared at the 2020 International Conference Automated Planning and Scheduling (ICAPS-20).* Corresponding author.E-mail address: sara.bernardini@rhul.ac.uk (S. Bernardini).https://doi.org/10.1016/j.artint.2021.1034860004-3702/© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486nity has started to pay closer attention to sequence submodularity prompted by applications such as online ad allocation [5]and recommendations in online shopping [6], entertainment [3] and courses [7]. However, having arisen in specific applica-tive contexts, the proposed models as well as the corresponding algorithms lack generality and require making restrictive assumptions on the objective function to maintain efficiency.In this paper, to remedy the current ad hoc approach and lack of coherence in the field, we offer a unified view of sequence submodularity. By abstracting away specific applicative details, we show that the optimization problem that lies behind several applications can be captured by a particular type of recursive submodular function. We study its structure and, based on its properties, we propose a generalized greedy algorithm that has theoretical guarantees as strong as its classical counterpart on set functions but does not require unrealistic restrictive assumptions. Our generalized algorithm encompasses and improves the specific algorithms that have been developed for several practical applications. Another property that confers flexibility to our approach is that we can easily enforce constraints on the cardinality of the elements in the sequence (e.g. all elements must be distinct) in the domain description, which is particularly useful in applicative problems.The paper is organized as follows. After discussing related work in Section 2, we state the problem formally and introduce our running example in Section 3. In Section 4, we recall the concept of submodularity for sequence functions and show how, in general, a simple generalization of the classical greedy algorithm from sets to sequences fails to achieve good performance for several optimization problems of practical relevance. Subsequently, in Section 5, we propose and analyze a new greedy algorithm that is proven to achieve the same performance as the classical one for submodular set functions (Theorem 1). In Section 6, we study how this result can be applied to the general class of problems that we are interested in solving (Theorem 2 and Corollary 1) and, in Sections 7 and 8, we present several different application domains, which demonstrate the expressiveness and generality of our approach. Finally, Section 9 provides explicit numerical simulations for two of the applicative setups discussed in the previous two sections, while Sections 10 offers conclusive thoughts.2. Related workWork on submodularity spreads across multiple fields, including optimization [8,9], machine learning [10,11], economics [12,13], medicine [14] and sensor networks [15,16]. This body of work focuses on set functions and, as most of the problems considered are NP-complete, revolves around finding good approximations of the optimal solution via greedy approaches, which are very effective for non-decreasing, submodular functions [9]. We do not review this literature here as set functions are not our focus. For a comprehensive review on this topic, we refer the readers to the literature [17].Only recently, work on sequence submodularity has emerged. Streeter and Golovin [18] first considered this problem in the context of online resource allocation applications. Shortly after, Alaei and Malekian [5] introduced the term sequence submodularity and showed that if the submodular function is non-decreasing and differentiable, a greedy approach always achieves a solution that is at least 1 − 1e of the optimal one for the maximization problem.Zhang et al. [15] consider string submodularity, which is a weaker concept as the submodularity holds for the prefix relationship instead of for any type of subsequence relationship. They improve on Alaei and Malekian’s approximation by introducing additional constraints on the degree of string submodularity (curvature) of the objective function.Other authors have defined sequence submodularity within a graph-based setting. Tschiatschek et al. [4] consider cases in which dependencies between elements of a sequence can be captured via directed acyclic graphs (DAGs) and present an algorithm with theoretical guarantees for them. However, repetitions in the sequence are not allowed and DAG submodular functions are not necessarily string or sequence submodular.Mitrovic et al. [7] extend this graph-based framework to graphs and hypergraphs with bounded in or out degrees.Finally, Qian et al. [19] take a departure from the greedy approach and propose a Pareto optimization method for se-quence selection. They show that, for any class of submodular functions previously studied, their approach can always reach the best known approximation guarantee.Mitrovic et al. [20], on the other hand, consider the case in which the value of a sequence depends not only on the items selected and their order but also on the states of the items, which might be initially unknown (adaptive submodularity).Against the backdrop of this body of work, we aim to show that the submodular functions appearing in practical ap-plications do not satisfy the constraints imposed by the approaches highlighted here. However, they do present a common structure that can be exploited to equip a suitably modified greedy algorithm with strong theoretical guarantees.3. Problem statementIn this section, we formally introduce the optimization problems that we study in this paper. Let (cid:2) be a set and H((cid:2))be the language over (cid:2), i.e. the set of sequences of elements in (cid:2) of any length including the empty sequence ∅. Let Hd((cid:2))denote the sub-language consisting of all sequences in H((cid:2)) with distinct elements. If S = (S 1, . . . , Sn) ∈ H((cid:2)), with S ibeing the element of sequence S in position i, we denote with |S| = n the length of the sequence S. Given R, S ∈ H((cid:2)), we say that R is a subsequence of S (denoted R ≤ S) if R is obtained from S by eliminating some of its elements, i.e. if there exists a strictly increasing function ρ : {1, . . . , |R|} → {1, . . . , |S|} such that R i = Sρ(i) for every i = 1, . . . , |R|. We use the following convention to indicate a specific type of subsequences: if S = (S 1, . . . , Sn) ∈ H((cid:2)) and 1 ≤ a ≤ b ≤ n, we write S|ba= (Sa, Sa+1, . . . , Sb). We put S|ba= ∅, if a > b.2S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486In this paper, we focus on greedy algorithms for maximizing functions defined on H((cid:2)) that present the following recursive form:F g(S) =(cid:3)g(Sk)n(cid:2)k=1F (S|k1) − F (S|k−11(cid:4))(1)for S = (S1, . . . , Sn) ∈ H((cid:2)). In Eq. (1), g : (cid:2) → R+specific order of the elements in S, monotonic and submodular (formal definitions are given in the next section).is any function and F : H((cid:2)) → R is a function independent from the The problem of maximizing these functions (typically on finite sequences with length below a given value) is significant because lies at the heart of several practical applications, ranging from jobs scheduling to web recommendation systems, as we will see in what follows. Note that, for a general g, the functions F g depends on the specific order of the elements of the sequence, with the consequence that the classical results on submodularity of set functions cannot be applied.3.1. Running exampleWe now discuss an illustrative scenario that we will use throughout the paper as a running example. Consider the monitoring of the level of a river subject to flooding. In case of danger, a set of movable floodgates are used to protect roads, bridges and other critical points whose impairment could lead to catastrophic events. Actioning and using these barriers over time and at different points on the river can be costly. A sensing system, e.g. a drone, can be used to observe a specific point more closely to establish whether a floodgate is needed, with a consequent reduction in cost if the drone concludes that the gate is unnecessary to protect that point. We are interested in the problem of finding the sequence of drone observations that allows the maximum reduction in cost. We formalize this scenario as follows.Given a time horizon [0, T ], consider the monitoring of an environment subject to several catastrophic events, which are represented as the elements of a set D. Each of these possible events requires to keep a safety infrastructure system in place (the movable floodgates, in the example above). For simplicity, given an event d in D, we set the cost of the infrastructure to prevent d to one per time unit. In the absence of other information, the total cost of monitoring each event over the time horizon [0, T ] is T and, as there are |D| events, the total cost of monitoring the environment amounts to T |D|. However, over time, the monitoring system acquires information that can be used to rule out the happening of some of the catastrophic events in D. Specifically, we assume that the system can perform a set (cid:2) of experiments. Each σ ∈ (cid:2)has an associated cost c > 0 (in the example above, the cost is associated to the use of a drone to make the observation σ ) and an associated time t(σ ) ∈ [0, T ], representing the moment at which the experiment can be performed. Each experiment σ ∈ (cid:2) is also associated with a subset of the events Dσ ⊆ D with the following meaning: if σ gives a negative result, it can be inferred that none of the events in Dσ will take place. In consequence, the system can stop using the safety infrastructure meant to prevent the events in Dσ from the time of the experiment t(σ ) to the end of the horizon, with a consequence reduction of (T − t(σ ))|Dσ | from the monitoring cost. A positive result of the experiment σ , instead, does not allow the system to rule out any possible event so the monitoring process does not undergo any change.Over time, the monitoring system performs a sequence of experiments S = (S 1, . . . , Sn), which are ordered in such a way that t(S1) < t(S2) < · · · < t(Sn). Let us now calculate the total monitoring cost when no catastrophic event takes place, assuming that, in this case, all experiments will give a negative result. We first define d(S) = | ∪n|, which is the number of events ruled out by the sequence of experiments and d = |D|. The total cost is given by the following expression:DS ii=1(cid:5)(S) =n(cid:2)(t(Sk) − t(Sk−1))(d − d(Sk−11)) + (T − t(Sn))(d − d(Sn1)) + cn(2)k=1interpreting t(S0) = 0. The terms in Eq. (2) have the following interpretation. The expression d − d(Sk−1) is the number of events not yet ruled out at time t(Sk−1). Since the system needs to keep monitoring those events, we have to account for the cost per time they generate over the interval [t(Sk−1), t(Sk)]. The first term is then the monitoring cost up to the last experiment Sn. The second term, (T − t(Sn))(d − d(Sn1)), is the remaining cost up to the time horizon T . Finally, the third term, cn, is the cost for performing n experiments.1Eq (2) can be rewritten as follows:(cid:5)(S) = −n(cid:2)k=1(T − t(Sk))(d(Sk1) − d(Sk−11)) + T d + cnIf we now put F (S) = d(S) and g(σ ) = T − t(σ ), we have that (cid:5)(S) = −F g(S) + T d + cn, and the minimization of the cost for sequences of a given length is equivalent to the maximization of F g (S). The general minimization problem can then be solved as follows:min (cid:5)(S) =|(cid:2)|minn=0[T d + cn − F g(S(n))]where S(n) is a maximum of F g (S) for the sequences of length n.3S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 1034864. Preliminary results on sequence submodularityIn this section, we formally define the concepts of monotonicity and sequence submodularity of a function and describe the greedy algorithm for the maximization of sequence submodular functions originally proposed by Alaei and Malekian [5]. We then discuss some preliminary results concerning the functions of interest in this paper, i.e. those of the type in Eq. (1), and show that Alaei and Malekian’s algorithm does not perform well on them (Example 2). In the next section, we propose a new greedy algorithm, which overcomes the limitations of the original one on such functions.(cid:8) = (S(cid:8)1, . . . , S(cid:8)m) are two elements in H((cid:2)), their Consider the language H((cid:2)) over a set (cid:2). If S = (S1, . . . , Sn) and Sconcatenation is defined as:S⊥S(cid:8) = (S1, . . . , Sn, S(cid:8)1, . . . , S(cid:8)m)For the sake of notational simplicity, concatenations with sequences of length 1 (σ ) will be denoted simply by S⊥σ and σ ⊥S, dropping the parentheses.Definition 1. A function J : H((cid:2)) → R is called forward/backward monotonic if, respectively,J (S⊥σ ) ≥ J (S),J (σ ⊥S) ≥ J (S) ∀S ∈ H((cid:2)), σ ∈ (cid:2)We use instead the term anti-monotonic if the inequalities are inverted.Definition 2. A function J : H((cid:2)) → R is called forward/backward (sequence) submodular if, for every S, R ∈ H((cid:2)), σ ∈(cid:2), respectively,J (S⊥R⊥σ ) − J (S⊥R) ≤ J (S⊥σ ) − J (S)J (σ ⊥R⊥S) − J (R⊥S) ≤ J (σ ⊥S) − J (S)For brevity, we drop sequence as we are only concerned about those functions in this paper. On the subset (cid:2)n ⊆ H((cid:2)) of sequences of length exactly n, there is a natural action of the permutation group Pn (i.e. the set of bijections from {1, . . . , n}to itself):S = (S1, . . . , Sn), θ ∈ Pn −→ θ S := (Sθ (1), . . . , Sθ (n))Definition 3. We define the concept of permutation invariance for sets and functions as follows:• A subset I ⊆ H((cid:2)) is said to be permutation invariant if for every S ∈ I and for every θ ∈ P|S|, it holds θ S ∈ I.• A function J : H((cid:2)) → R is permutation invariant if, for every R ∈ H((cid:2)) and for every θ ∈ P|R|, it holds J (θ R) = J (R).For permutation invariant functions, the backward and forward notions of monotonicity and submodularity always coin-cide and, in that case, we will refer to them as monotonic, anti-monotonic, and submodular functions.Example 1. We now illustrate the concepts presented in this section in the context of our running example, presented in Section 3.1. In that case, we can confer the right interpretation to the function F g(S) only for sequences S such that t S i is monotonically increasing and, in consequence, g(S i) = T − t S i is monotonically decreasing.For the sake of illustration, we consider now the extension of F g(S) to the entire language H((cid:2)). More precisely, we consider the special case in which the experiments in (cid:2) = {σ1 . . . , σr} can be labeled so that the information they ⊆ · · · ⊆ Dσr . Given σ ∈ (cid:2), we denote by k(σ ) = 1, . . . , r its index in convey is monotonically increasing, i.e. Dσ1this ordering, namely k(σ ) is such that σk(σ ) = σ . We put dk = |Dσk| for k = 1, . . . , r. Given S ∈ H((cid:2)), we further put k(S) = max{k(S i) | i = 1, . . . , |S|}, and we notice that⊆ Dσ2|S|(cid:6)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)k=1(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)d(S) =Dσk= |Dσk(S)| = dk(S)If S ∈ H((cid:2)) and σ ∈ (cid:2), thanks to (3), we have that,F g(S⊥σ ) − F g(S) = g(σ )[d(S⊥σ ) − d(S)] =(cid:7)g(σ )dk(σ )0if k(S) < k(σ )if k(S) ≥ k(σ )Notice that k(S) and d(S) are both permutation invariant and monotonic. From expression (4), we obtain that F g is forward monotonic and, using the monotonicity of k(S), that is also forward submodular. Also note that4(3)(4)S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486F g(σi⊥σ j) − F g(σ j) = g(σi)di + g(σ j)d j[1i< j − 1]where 1i< j is 1 if i < j and is 0 otherwise. The expression above shows that in general F g is not backward monotonic: the right hand side can be negative if i > j and g is such that g(σi)di < g(σ j)d j .The following result shows how the function F g behaves with respect to a transposition of two consecutive elements of a sequence S.Lemma 1. Given a permutation invariant and submodular function F and a function g : (cid:2) → R+, consider the function F g as defined in Eq. (1). Let S ∈ H((cid:2)) and k < |S| be such that g(Sk) ≤ g(Sk+1). Let ˜S be the sequence obtained from S by swapping Sk with Sk+1. Then,F g( ˜S) ≥ F g(S)Proof. Thanks to the permutation invariance of the function F , we have that:F g(S) − F g( ˜S) = g(Sk)[F (S|k1) − F (S|k−11− g(Sk)[F (S|k+11) − F (S|k−1= [g(Sk+1) − g(Sk)][F (S|k+1)] + g(Sk+1)[F (S|k+1⊥Sk+1)]) − F (S|k1) − F (S|k−11111⊥Sk+1) + F (S|k−11)]) − F (S|k1)] − g(Sk+1)[F (S|k−11⊥Sk+1) − F (S|k−11)]We conclude the proof by observing that the last term is non-positive since g(Sk) ≤ g(Sk+1) and F is submodular. (cid:2)Remark 1. Notice that, under the stronger assumption that g(Sk) = g(Sk+1), Lemma 1 yields F g( ˜S) = F g(S).Lemma 1 implies that if we want to maximize functions of the type in Eq. (1) over a set of sequences I that is permu-tation invariant, we can always restrict to those sequences S ∈ I for which g(Sk) ≥ g(Sk+1) for every k. To formalize this implies g(σ ) ≤ g(σ (cid:8)). We fact, we consider a total ordering ≺ of the elements of (cid:2) for which g is non-decreasing: σ ≺ σ (cid:8)call this a g-ordering and note that the g-ordering is not unique when g is non-injective.We now present a number of useful concepts related to any fixed total ordering ≺ on (cid:2). We use the notation σ (cid:13) σ (cid:8)to indicate that σ (cid:8) ≺ σ or σ (cid:8) = σ . A sequence S ∈ H((cid:2)) is called ≺-ordered if S1 (cid:13) · · · (cid:13) S|S|. A subset I ⊆ H((cid:2)) is called ≺-ordered if each S ∈ I is ≺-ordered. Given any subset I ⊆ H((cid:2)), we indicate the ≺-ordered subset as follows:I(≺) = {S ∈ I | S is ≺-ordered}(5)When I = H((cid:2)) or I = Hd((cid:2)), we will use the notation H((cid:2), ≺) and Hd((cid:2), ≺), respectively, for I(≺). For the sake of simplicity, a subset I ⊆ H((cid:2)) that is ≺-ordered with respect to a g-ordering ≺, where g is a function g : (cid:2) → R+, will simply be called g-ordered.The following is a direct consequence of Lemma 1.Proposition 1. Consider a permutation invariant and submodular function F and a function g : (cid:2) → R+Then, given any permutation invariant set I ⊆ H((cid:2)) and T ∈ N, it holds that:. Let ≺ be a g-ordering. maxS ∈ I|S| = TF g(S) = maxS ∈ I(≺)|S| = TF g(S)(6)Remark 2. If we maximize F g(S) over Hd((cid:2)) with sequences of maximal length T = |(cid:2)|, the only sequence S of length T belonging to Hd((cid:2), ≺) is a maximum. Because of Lemma 1 and Remark 1, all possible maxima can be obtained from S by arbitrarily permuting the elements of any subsequence (Sh, Sh+1, . . . , Sk) for which g(Sh) = g(Sk). In this case, the maximization problem boils down to a sort problem.In our work, we are interested in investigating the maximization problem of the function F g(S) for the general case when sequences do not necessarily consist of distinct elements or have maximal length. In many applications, this is indeed the case.Let us now fix a value T ∈ N and consider the problem of maximizing a function J : H((cid:2)) → R on the sequences of fixed length T . A popular, simple, suboptimal algorithm for such maximization problems is the greedy algorithm by Alaei and Malekian [5], which generalizes the classical result in Nemhauser and Wolsey [9] to sequence functions. This algorithm produces recursively a sequence S = (S1, . . . , S T ) by adding new elements on the right side of the sequence so that, for every k = 0, . . . , T − 1,J (S|k1⊥Sk+1) ≥ J (S|k1⊥σ ) ∀σ ∈ (cid:2)(7)5S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486Alaei and Malekian [5] give a lower bound on the performance of the greedy algorithm in the presence of monotonicity and submodularity of J . In particular, fix a value T ∈ N and let S T be the sequence generated by the greedy algorithm stopped at step T and O T ∈ H((cid:2)) any maximizing sequence of J restricted to sequences in H((cid:2)) of length T . Now, assume that J is backward monotonic and forward submodular, then,J (S T ) ≥J (O T )(8)(cid:8)(cid:9)1 − 1eConsidering now our function F g , it is simple to see that it is forward monotonic and forward submodular, while, in general, it does not possess the other two complementary properties. Hence, the result in Eq. (8) cannot be applied to it. In Example 2, we show that the classical greedy algorithm can perform arbitrarily bad on such functions.Example 2. Considering Example 1 again, we specify the values of the parameters as follows. We put dk = 2k−1 if k =1, . . . , n − 1, while dn = 2n. We also assume that g(σk) = 2n−k. Then, the value of the function over the single elements of (cid:2) is given by the following expression:F g(σk) = g(σk)d(σk) =(cid:7)2n−12nif k ≤ n − 1if k = nGiven the assumptions made, this expression reaches its maximum for k = n. Consequently, the greedy solution S of length n will necessarily be such that S1 = σn. This implies thatF g(S) = g(σn)d(σn) = 2nas the remaining term will not give any further contribution. A direct check shows that, instead, the optimum among the sequences of length n is reached by O = (σ1, . . . , σn). We can compute as follows:F g(O ) = 2n−1d1 +n(cid:10)k=22n−k(dk − dk−1) = 2n−1 + (n−2)2n4+ 2n 34= 2n n+34Therefore,F g(O )F g(S)= n + 34Hence, no bound of the form in Eq. (8) can possibly hold in this case.5. A new greedy algorithm on fully extendable setsWe now present a new greedy algorithm for the maximization of sequence submodular functions that allows each new element of the sequence under construction to be placed in any position among the elements already in it. This approach differs from Alaei and Malekian’s algorithm [5], which adds new elements only of the right-hand side of the sequence. We show that the new algorithm maintains the same theoretical guarantees as the original one and performs well on functions of the type in Eq. (1).Compared to previous work, our optimization approach is more general as it allows problems to be defined not only over H((cid:2)) (elements can be repeated) and Hd((cid:2)) (elements are all distinct) but also over sets in which the number of repetitions of each element can be constrained to be below a certain value. To allow for such generality, in Section 5.1, we introduce the key concept of fully extendable set of sequences, and we generalize the notions of monotonicity and submodularity by adapting them to fully extendable sets. Our new greedy algorithm, described in Section 5.2, exploits such notions.5.1. Monotonic and submodular functions on fully extendable setsDefinition 4. A subset I ⊆ H((cid:2)) is called fully extendable if the following conditions are satisfied.1. For every σ ∈ (cid:2), (σ ) ∈ I;2. If R ∈ I and Q ≤ R, then Q ∈ I;3. If Q , R ∈ I, there exists U ∈ I such that Q , R ≤ U and |U | ≤ |Q | + |R|.The third property says that, given two sequences Q , R ∈ I, there must exist another sequence U ∈ I of which both are subsequences and whose length is at most the sum of the two lengths. If Q and R do not have any element in common, the only possibility is that U is obtained by intertwining Q and R and then |U | = |Q | + |R|.6S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486We denote by I(Q , R) the subset of sequences U satisfying property 3 defined above. Given Q ∈ I, we also denoteI+(Q ) := {U ∈ I | Q ≤ U , |U | = |Q | + 1}In other words, I+(Q ) consists of the sequences in I that are obtained from Q by adding one element. It follows from properties 1 and 3 and the considerations above that if there exist elements in (cid:2) not appearing in Q , surely I+(Q ) (cid:14)= ∅.We now give some examples of fully extendable sets.Example 3. H((cid:2)) is a fully extendable set.Example 4. Consider any total ordering ≺ in the set (cid:2). Then, the set of ≺-ordered sequences H((cid:2), ≺) (formally defined in Eq. (5)) is a fully extendable set.Finally, we construct a family of fully extendable sets that subsumes the examples above and will be useful in the applications presented in Sections 7 and 8. Those are sets in which the number of repetitions of each element can be constrained to be below a certain value.Example 5. We fix a total ordering ≺ on the set (cid:2). Given S ∈ H((cid:2)), we denote by nσ (S) the number of times the element σappears in the sequence S. The following two properties are a direct consequence of the definition of ≺-ordered sequences:(i) Given non negative integer numbers nσ for every σ ∈ (cid:2), there exists exactly one ≺-ordered sequence S such that nσ (S) = nσ for every σ ∈ (cid:2).(ii) Given two ≺-ordered sequences Q , R, we have that Q ≤ R if and only if nσ (Q ) ≤ nσ (R) for every σ ∈ (cid:2).For every σ ∈ (cid:2), fix a number nσ ∈ {1, 2, . . . } ∪ {+∞} and consider the set of sequencesI = {S ∈ H((cid:2), ≺) | nσ (S) ≤ nσ ∀σ ∈ (cid:2)}(9)Note that H((cid:2), ≺) and Hd((cid:2), ≺) are special cases of I, obtained when, respectively, nσ = +∞ and nσ = 1 for every σ ∈ (cid:2).We have the following result:Proposition 2. The set of sequences I defined in Eq. (9) is fully extendable.Proof. All singleton sequences S = (σ ) are ≺-ordered and respect the repetition constraint (since nσ ≥ 1). Therefore, they are in I and property 1 in Definition 4 holds. Property 2 also holds because any subsequence Q of a sequence in R ∈ I is necessarily g-ordered and satisfies, thanks to property (ii) above, the constraints nσ (Q ) ≤ nσ (R) ≤ nσ for every σ ∈ (cid:2). To check property 3, consider now two sequences Q , R ∈ I and put, for every σ ∈ (cid:2),¯nσ = max{nσ (Q ), nσ (R)}Let U be the only ≺-ordered sequence such that nσ (U ) = ¯nσ (see property (i)). Since by construction ¯nσ ≤ nσ for all σ ∈ (cid:2), we have that U ∈ I. Notice now that both Q and R are subsequences of U because of property (ii). Finally,(cid:2)|U | =nσ (U ) ≤(cid:2)[nσ (Q ) + nσ (R)] = |Q | + |R|σ ∈(cid:2)σ ∈(cid:2)That completes the proof. (cid:2)It follows from Proposition 2 that H((cid:2), ≺) and Hd((cid:2), ≺) are both fully extendable.Example 6. If |(cid:2)| > 1, the set Hd((cid:2)) is not fully extendable. Indeed, let σ1, σ2 ∈ (cid:2) be two distinct elements and consider the sequences R = (σ1, σ2) and S = (σ2, σ1). Any sequence U of which both R and S are subsequences must contains either two copies of σ1 or two copies of σ2 and thus cannot belong to Hd((cid:2)). This reveals that the condition 3 in Definition 4does not hold true.Let us now see how the notions of monotonicity and submodularity change when adapted to fully extendable sets. The novelty is that, in the definitions below, the new element σ that is added to the sequences can appear not only at the beginning and at the end of them, as with the standard notions of monotonicity and submodularity, but also in between the sequences.7S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486Definition 5. Given a fully extendable set I ⊆ H((cid:2)), a function F : I → R is called I-monotonic if for every Q , R ∈ I and σ ∈ (cid:2) such that Q ⊥σ ⊥R ∈ I, it holds:F (Q ⊥σ ⊥R) ≥ F (Q ⊥R)(10)Definition 6. Given a fully extendable set I ⊆ H((cid:2)), a function F : I → R is called I-submodular if for every Q , R, S ∈ Iand σ1, σ2 ∈ (cid:2) such that Q ⊥σ1⊥R⊥σ2⊥S ∈ I, it holds:F (Q ⊥σ1⊥R⊥σ2⊥S) − F (Q ⊥σ1⊥R⊥S) ≤ F (Q ⊥R⊥σ2⊥S) − F (Q ⊥R⊥S)(11)Relation (11) is a way to express the diminishing return property that characterizes the definition of submodularity. Adding an extra element σ2 to a sequence produces a smaller impact on the growth of the function F calculated over the sequence as more elements are added to its prefix, as long as the constructed sequences remain elements of the set I.Remark 3. If I = H((cid:2)), I-monotonic functions are backward and forward monotonic and I-submodular functions are forward submodular.Remark 4. If F : H((cid:2)) → R is permutation invariant, monotonic and submodular, then, for every fully extendable set I ⊆H((cid:2)), the restriction of F to I is I-monotonic and I-submodular.5.2. A new greedy algorithmWe now introduce a generalized greedy algorithm and show that, for functions that are I-monotonic and I-submodular, this new algorithm ensures the same performance as in expression (8). In the next section, we will then show how to apply this result to our problems.Take a function J : H((cid:2)) → R and a fully extendable set I ⊆ H((cid:2)). We fix a value T ∈ N: the goal is to maximize Jover the subset of I of the sequences of length T . Put O T to be any such maximizing sequence for J .We now consider a variation of the greedy algorithm to approximately solve this maximization problem. The algorithm produces recursively an ordered sequence S T = (S T1 , . . . , S TT ) ∈ I in the following way:• S 1 = (S 11) where S 11• Given Sk = (Sk∈ argmaxσ ∈(cid:2)J (σ );k) ∈ I, we define1, . . . , SkSk+1 ∈ argmaxU ∈I+(Sk)J (U )(12)In other words, instead of simply augmenting the sequence on the right hand side as the traditional greedy algorithm does, we allow each new element to be placed in any position among the elements of the previous sequence. We note that, when the argmax in expression (12) is not a singleton, the choice of Sk+1 can be any arbitrary element of it. The performance of the algorithm will not be affected by this choice.The following result shows that this new algorithm satisfies the same performance bound than the classical one (see expression (8)).Theorem 1. Consider a function J : H((cid:2)) → R and a fully extendable set I ⊆ H((cid:2)) and assume that J is I-monotonic and I-submodular. Let O T be a maximizing sequence for J among the sequences in I of length T and let S T be the result of the previous algorithm. Then,J (S T ) ≥J (O T )(cid:8)(cid:9)1 − 1eProof. For simplicity of notation, in the proof, we put O = O T . Fix k < T and consider(cid:7) = (λ1, . . . , λn) ∈ argmaxU ∈I(Sk,O )J (U )We consider a partition of the indices{1, 2, . . . , n} = {i1, i2, . . . , ik} ∪ { j1, j2, . . . , jm}where i1 < i2 < · · · < ik are such that Sklindices.= λil for l = 1, . . . , k and j1 < j2 < · · · < jm with m = n − k are the remaining 8S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486We now consider, for 0 ≤ t ≤ m, the sequence (cid:7)(t) obtained from (cid:7) by removing the elements λ jm , λ jm−1 , . . . , λ jt+1 . Note that, by property 2 of fully extendable sets, (cid:7)(t) ∈ I for every t and that (cid:7)(m) = (cid:7) and (cid:7)(0) = Sk. We can writeJ ((cid:7)) − J (Sk) =m(cid:2)(cid:3)(cid:4)J ((cid:7)(t)) − J ((cid:7)(t−1))t=1(13)Using the property of I-submodularity and removing the elements λ jt−1 , . . . , λ j0 from (cid:7)(t−1) and (cid:7)(t), we obtain thatJ ((cid:7)(t)) − J ((cid:7)(t−1)) ≤ J (U (t)) − J (Sk)(14)for some U (t) ∈ I+(Sk) (a sequence obtained from Sk adding in some position the element λ jt ). Given the definition of the extended greedy solution Sk, it follows that J (U (t)) ≤ J (Sk+1). This fact together with expressions (13) and (14) yields:J ((cid:7)) − J (Sk) ≤ T(cid:3)(cid:4)J (Sk+1) − J (Sk)(15)The assumption of I-monotonicity and the choice of (cid:7) to maximize J on I(Sk, O ) ensure that J ((cid:7)) ≥ J (O ). Using this fact inside expression (15) gives:J (Sk+1) ≥ 1TJ (O ) +(cid:9)(cid:8)1 − 1TJ (Sk)for every k = 0, . . . , T − 1. Applying recursively this relation, we obtain thatJ (S T ) ≥ 1T(cid:9)i(cid:8)T −1(cid:2)i=01 − 1T(cid:11)(cid:8)J (O ) =1 −(cid:12)(cid:9)TJ (O ) ≥(cid:9)(cid:8)1 − 1e1 − 1TJ (O ) (cid:2)6. A detailed analysis of the function F gWe now go back to our original optimization problem on functions of the type of Eq. (1) and study the conditions under which we can apply the theory laid out in the previous section to it. Our aim is to show that, under suitable assumptions, the function F g satisfies the conditions of Theorem 1 on fully extendable g-ordered sets, which – thanks to Proposition 1 – will allow us to obtain a solution with bounded suboptimality to our maximization problem.We start by introducing some additional notation that will become handy in the proof of the main result of this section.(cid:9)F (Q , σ , R) = F (Q ⊥σ ⊥R) − F (Q ⊥R)(cid:9)2 F (Q , σ1, R, σ2, S) = (cid:9)F (Q ⊥σ1⊥R, σ2, S) − (cid:9)F (Q ⊥R, σ2, S)(16)= F (Q ⊥σ1⊥R⊥σ2⊥S) − F (Q ⊥σ1⊥R⊥S) − F (Q ⊥R⊥σ2⊥S) + F (Q ⊥R⊥S)The first expression, (cid:9)F (Q , σ , R), is the ‘first’ variation of F obtained starting from the sequence Q ⊥R and adding one more element σ between the subsequences Q and R. The second expression, (cid:9)2 F (Q , σ1, R, σ2, S), is instead a ‘second’ variation, namely a variation of the first variation, which goes from Q ⊥R to Q ⊥σ1⊥R, relative to an added element σ2. They play an analogous role to, respectively, the discrete derivative and the discrete second derivative of a function; this analogy will become apparent below when we discuss an example of a submodular functions obtained through convex functions.Note that the I-monotonicity of F is equivalent to the requirement that (cid:9)F (Q , σ , R) ≥ 0 for every choice of Q , R and σ such that Q ⊥σ ⊥R ∈ I, while the I-submodularity of F is equivalent to the requirement that (cid:9)2 F (Q , σ1, Rσ2, S) ≤ 0under the assumption that Q ⊥σ1⊥R⊥σ2⊥S ∈ I.We are now ready to state and prove the main result of this section. It asserts that the I-monotonicity and I-submodularity of a function F are transferred to a function F g if the fully extendable set I is g-ordered, namely is ≺-ordered with respect to any g-ordering ≺.Theorem 2. Let g : (cid:2) → R+and let I ⊆ H((cid:2)) be a g-ordered fully extendable set. Given any F : I → R, it holds that1. If F is I-monotonic, then F g is I-monotonic;2. If F is I-submodular, then F g is I-submodular.Proof. Assume that F is I-monotonic. Fix Q , R ∈ I and σ ∈ (cid:2) such that Q ⊥σ ⊥R ∈ I. Put n = |R|. From the definition of F g , we obtain that9S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486(cid:9)F g(Q , σ , R) = F g(Q ) + g(σ )(cid:9)F (Q , σ , ∅)n(cid:2)+g(Rk)(F (Q ⊥σ ⊥R|k1) − F (Q ⊥σ ⊥R|k−11) − F g(Q )k=1n(cid:2)k=1−g(Rk)(F (Q ⊥R|k1) − F (Q ⊥R|k−11))(17)= g(σ )(cid:9)F (Q , σ , ∅) +n(cid:2)k=1g(Rk)(cid:9)F (Q , σ , R|k1) −n−1(cid:2)k=0g(Rk+1)(cid:9)F (Q , σ , R|k1)= [g(σ ) − g(R1)](cid:9)F (Q , σ , ∅) + g(Rn)(cid:9)F (Q , σ , R) +n−1(cid:2)[g(Rk) − g(Rk+1)](cid:9)F (Q , σ , R|k1)k=1By the assumption on I, we have that[g(σ ) − g(R1)] ≥ 0, g(Rn) ≥ 0, [g(Rk) − g(Rk+1)] ≥ 0 ∀k(18)On the other hand, I-submonotonicity of F yields that all the first variation terms appearing in the expression above are non-negative. It thus follow that (cid:9)F g (Q , σ , R) ≥ 0. This proves that F g is I-monotonic.Assume that F is I-submodular. Consider now Q , R, S ∈ I and σ1, σ2 ∈ (cid:2) such that Q ⊥σ1 R⊥σ2⊥S ∈ I. Put m = |S|. Then, from the expressions (16) and (17), we obtain that(cid:9)2 F g(Q , σ1, R, σ2, S) = (cid:9)F g(Q ⊥σ1⊥R, σ2, S) − (cid:9)F g(Q ⊥R, σ2, S)= [g(σ2) − g(S1)] · [(cid:9)F (Q ⊥σ1⊥R, σ2, ∅) − (cid:9)F (Q ⊥R, σ2, ∅)]+ g(Sn)[(cid:9)F (Q ⊥σ1⊥R, σ2, S) − (cid:9)F (Q ⊥R, σ2, S)]m−1(cid:2)[g(Sk) − g(Sk+1)][(cid:9)F (Q ⊥σ1⊥R, σ2, S|k1) − (cid:9)F (Q ⊥R, σ2, S|k1)]+(19)k=1= [g(σ2) − g(S1)](cid:9)2 F (Q , σ1, R, σ2) + g(Sm)(cid:9)2 F (Q , σ1, R, σ2, S)m−1(cid:2)[g(Sk) − g(Sk+1)](cid:9)2 F (Q , σ1, R, σ2, S|k1)+k=1From the inequalities in (18) and the fact that all second variation terms (cid:9)2 F appearing above are, as F is I-submodular, non-positive, it follows that (cid:9)2 F g(Q , σ1, R, σ2, S) ≤ 0 (cid:2)A direct consequence of Theorem 2 is the following corollary, which is crucial to analyze all our applicative examples as we will see in the next section.Corollary 1. Consider a permutation invariant, monotonic, and submodular function F : H((cid:2)) → R, a function g : (cid:2) → R+g-ordered fully extendable set I ⊆ H((cid:2)). Then, F g is I-monotonic and I-submodular., and a Proof. It is a direct consequence of Theorem 2 and of Remark 4. (cid:2)7. Detection and monitoring problemsIn this section, we present a few applicative case studies regarding detection and monitoring and show how they can be framed within the theory developed so far. The case studies revolve around the problem of choosing an optimal sequence of experiments to optimize a quantity of interest. We start with a broad formulation of this problem, focusing on the common characteristics of the case studies. We continue by exploring detection problems, which involve minimizing the detection time of a given event, and we then tackle a particular instance of them, i.e. search-and-tracking (S&T), which is an important applicative domain [2,21,22]. We conclude this section by considering environmental monitoring problems in which the goal is to minimize the cost of the monitoring infrastructure. For each example, we show that its formal representation leads to the maximization of a function F g on a fully extendable set I for which the properties of Corollary 1 are satisfied and, in consequence, our generalized greedy algorithm can be profitably applied.Consider a set of experiments (cid:2). Each experiment σ ∈ (cid:2) is associated with a random variable X σ on a finite space A, which expresses the outcome of the experiment σ , and a time stamp t(σ ), which indicates the time at which that experiment can take place (e.g. because the device to perform the experiment is only available at that time).10S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486We are interested in scenarios where multiple experiments take place, statistically described as follows. For each se-quence S = (S1, . . . , Sn) ∈ H((cid:2)), we consider a sequence of random variables X S = ( X1, . . . , Xn) distributed according to a law P S : An → [0, 1]. P S (ω1, . . . ωn) is the probability that the set of experiments S has a global result (ω1, . . . ωn) ∈ An. Note that the same experiment σ can be repeated within the sequence S.We make two assumptions concerning the experiments: the specific order of disclosing the outcome of the various experiments does not play any role; and ignoring the result of a subset of the experiments is equivalent to not having performed them at all. Formally:1. Permutation covariance: For every S = (S1, . . . , Sn) ∈ H((cid:2)) of length n, ω = (ω1, . . . , ωn) ∈ An, and permutation θ ∈ Pn, it holds thatP θ S (θω) = P S (ω)where θ S = (Sθ(1), . . . , Sθ(n)) and θω = (ωθ(1), . . . , ωθ(n)).2. Coherence: Given R, S ∈ H((cid:2)) with R ≤ S, the law of X R is the same as the law of the subsequence of the random variables ( X S )|R that are obtained from X S considering only those components corresponding to the positions of Rinside S. In particular, the i-th component of X S has the same law than X S i for every i = 1, . . . , |S|.Against the backdrop of the framework outlined above, we now analyze the different problems, which differ in how they exploit the experiments and in the quantity they want to optimize.7.1. Detection problemsGiven the space of the experiment outcomes, let us consider a specific event E ⊆ A, which we call the success event, that represents a desired outcome of one of the experiments (e.g. a positive detection event). The problem is to determine, among all the experiment sequences of a given length, the one that minimizes the expected time to obtain the success event. We now formalize this problem. We fix a (−t)-ordering ≺ on (cid:2) (e.g. σ1 ≺ σ2 implies t(σ1) > t(σ2)) and consider the fully extendable set of ≺-ordered sequences H((cid:2), ≺) (as defined in Eq. (5)). For every S ∈ H((cid:2), ≺) with |S| = n, we define the first detection time as the random variable τS : An → R such that(cid:13)τS (ω) =t(Sk)Tif ωk ∈ E, ω j /∈ E for j < kif ω j /∈ E for every jwhere T ≥ maxσ ∈(cid:2) t(σ ) is a constant playing the role, as we will see below, of a penalty for the fact that detection has not succeeded within S. The quantity that we aim to minimize is E[τS ], i.e. the expected value of τS on the set H((cid:2), ≺). We now show how such a goal involves the maximization of a function of the type in Eq. (1).We put En = {ω ∈ An | ∃i s.t. ωi ∈ E}. Given a sequence S ∈ H((cid:2)) of length |S| = n, we defineF (S) = P S (En)that is the probability that the success event E has happened within the sequence of experiments S. The expected success time can then be computed as follows:E[τS ] =n(cid:2)k=1t(Sk)(F (S|k1) − F (S|k−11)) + T (1 − F (S)) = T +n(cid:2)(t(Sk) − T )(F (S|k1) − F (S|k−11k=1))(20)The minimization of the expected success time is then equivalent to the maximization of the functionn(cid:2)k=1(T − t(Sk))(F (S|k1) − F (S|k−11))(21)which is of the form in Eq. (1) with g(σ ) = T − t(σ ). We observe that the optimization problem in this case is already defined over the fully extendable set I = H((cid:2), ≺). We now show that the properties needed to apply Corollary 1 hold:• Permutation invariance of F follows from the permutation covariance property of the family of probabilities P S and the fact that the success events En are permutation invariant.• Monotonicity of F follows from the following computation:(cid:9)F (S, σ ) = P S⊥σ (En+1) − P S (En) = P S⊥σ (En+1) − P S⊥σ (En × A) = P S⊥σ (E c × · · · × E c × E) ≥ 0(22)where we denote E c = A \ E.11S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486• Submodularity of F follows from the following computation:(cid:9)2 F (S, σ1, σ2) = P S⊥σ1⊥σ2 (E c × · · · × E c × E) − P S⊥σ2 (E c × · · · × E c × E)= P S⊥σ1⊥σ2 (E c × · · · × E c × E) − P S⊥σ1⊥σ2 (E c × · · · × E c × A × E)= −P S⊥σ1⊥σ2 (E c × · · · × E c × E × E) ≤ 0Notice how, in the second equalities of (22) and (23), we use the coherence property.• By definition, the set H((cid:2), ≺) is g-ordered with g(σ ) = T − t(σ ).(23)An important extension of this example is the case when there are multiple detection events of interest E 1, . . . , E s ⊆ A, possibly correlated among themselves. We indicate with τ kS the first detection time of the event Ek. In certain applications, it is natural to consider the global average completion time E[maxk τ k] as the function to minimize; this happens when a Spartial detection of only a subset of the events has no value. This new function has no submodularity properties. However, in those applications where each detection event has an intrinsic value, it is useful to minimize the function E[]. This function fits in our framework. Indeed, if we define F k(S) = P S (Ekk τ kS(cid:10)n) andF (S) =s(cid:2)k=1F k(S)we obtain, computing as in Eq. (20), that the minimization of E[in expression (21).(cid:10)k τ kS] is equivalent to the maximization of the function We now present a specific instance of the general detection problem presented above, which corresponds to an important practical application.7.2. Search-and-trackingAs a specific applicative example of the detection problem presented in Section 7.1, we consider the search-and-tracking (S&T), which is the problem of locating a moving target in a given area and following it to destination. Following a state-of-the-art S&T application in this area [2,22], we assume that the target travels across a large geographical area by following a road network (set of paths (cid:5)), and the observer is a UAV with imperfect sensors. When the UAV loses track of the target, a set of candidate flight search patterns (cid:2) is selected via a Monte Carlo simulation to direct the search towards the areas in which it is more probable to rediscover the target (see [23] for more detail on the Monte Carlo simulation). The UAV, however, has not enough resources to execute all candidates and a subset of patterns needs to be selected and arranged in a feasible sequence for execution.Each pattern σ ∈ (cid:2) provides visibility over a family of paths (cid:5)σ ⊆ (cid:5), i.e. if the target follows a route in (cid:5)σ , the UAV may be able to detect it while performing pattern σ . Each pattern σ ∈ (cid:2) is also associated with: (i) a time stamp t(σ ), indicating the mid-point of a time window during which the target might plausibly be in the area covered by σ ; and (ii) a detection probability φσ with the following meaning: assuming that the target has taken a route in (cid:5)σ , if the UAV performs the pattern σ at time t(σ ), detection will be positive with probability φσ . In all other cases, detection will be negative. We assume an a-priori uniform probability distribution on the routes in (cid:5), as well as independence of the outcomes of the search experiments conditioned to the fact that the target has chosen a specified route.In Bernardini et al. [22], the authors study the problem of selecting the sequence S ∈ H((cid:2)) of length n that minimizes the expected detection time (defined using the reference time stamps t(σ ) for the various elements of the sequence). They allow repetitions of the same search patterns as they correspond to the UAV repeating the search in the same area. Below, we show how this problem fits in the general detection framework presented in this section, while, in Section 9, we provide numerical simulations that highlight how the generalized greedy approach benefits the solution of the problem.To every search pattern σ ∈ (cid:2), we associate a binary random variable X σ on {0, 1} that describes the outcome of the search performed at σ , where 1 expresses the positive detection event (target is found). For each sequence S ∈ H((cid:2)), the probability distribution P S : {0, 1}n → [0, 1] of the outcomes of the corresponding search experiments is constructed on the basis of the topology of the road network and the detection probability of each pattern. More precisely, given S ∈ H((cid:2))of length n, we consider a joint probability distribution ˜P S on {0, 1}n × (cid:5) where ˜P S (ω1, . . . , ωn, γ ) denotes the probability that the target has taken the road γ and the outcome of performing the n search patterns S 1, S2, . . . , Sn have given results, respectively, ω1, ω2, . . . , ωn. The probability ˜P S is univocally described by assuming that its marginal on (cid:5) is the uniform distribution and that˜P S (ω1, . . . , ωn | γ ) =(cid:14)(cid:14)(cid:14)(1 − ωi)φS ii : γ (cid:14)∈ (cid:5)S iωi = 1i : γ ∈ (cid:5)S iωi = 1i : γ ∈ (cid:5)S iωi = 0(1 − φS i )(24)The above equation summarizes our assumptions: given that the target has taken route γ , the outcome of the various experiments in S will give an independent outcome. The patterns S i not compatible with γ will deterministically give a null 12S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486result, while the others will be positive with probability φS i and null with probability 1 − φS i . The probability distribution P S of X S = ( X S1 , . . . , X Sn ) is obtained from ˜P S by averaging over γ ∈ (cid:5). The properties of permutation covariance and coherence for P S follow directly from Eq. (24). The detection event is, in this case, En = {(0, . . . , 0, 1)}. In Bernardini et al. [22], the authors propose an explicit iterative expression for the function F (S) = P S (En), which is useful for computation purposes.As in the general detection problem, here the optimization problem is naturally defined over the set H((cid:2), ≺). However, other choices can be of interest: for instance, we can use Hd((cid:2), ≺), if we want to enforce all search patterns to be distinct or, more generally, we can use the fully extendable sets defined in Eq. (9) to impose specific restrictions on the number of repetitions. Such type of restrictions may reflect the time needed for the UAV to perform a search pattern or the number of UAV’s that are simultaneously available for the search.7.3. Monitoring problemsIn this section, we present monitoring applications by going back to Example 1 (Section 3.1) and framing it within a more general and richer context.Let us consider a system that monitors an environmental phenomenon Z , which is influenced by a set of uncertain factors. As seen in Example 1, the phenomenon could be the level of a river subject to flooding, which is determined by the intensity of the precipitations (among other factors). The phenomenon Z manifests itself by a set D of events. In Example 1, such events are given by whether the river will overflow at some established checkpoints over its course.We now consider a family (cid:2) of Bernoulli experiments, formally modeled as in Section 7.2, that allow the system to observe the events in D. Each experiment σ ∈ (cid:2) is associated with a random variable X σ on {0, 1} and a time stamp t(σ ). The output of a sequence of experiments S ∈ H((cid:2)) performed at the prescribed times, X S = ( X S1 , . . . , X Sl ), is governed by the distribution laws P S (x) as x ∈ {0, 1}l. Such distributions are calculated on the basis of historical data on the phenomenon Z or other information, and we assume that they satisfy the same set of assumptions concerning permutation covariance and coherence that hold true in the case of the detection problems.Each σ ∈ (cid:2) is also associated with a subset Dσ ⊆ D with the following meaning. If the monitoring system performs the experiment σ at its associated time and observes a negative outcome, X σ = 0, the system is guaranteed that none of the events in Dσ will take place, namely Z (cid:14)= E for any E ⊆ D such that E ∩ Dσ (cid:14)= ∅. In our example, the system can observe the level of the river at some of the checkpoints and, for any observation of the water being below a safety threshold, it can exclude that a flooding will happen at that point.We model the cost of handling the phenomenon Z without the aid of the additional experiments in (cid:2) through an D → R+. C is determined by assigning nonnegative numbers ci to each i ∈ D and, then, defining, additive function C : 2(cid:10)given E ⊆ D, C(E) =i∈E ci . This is the global cost that the system incurs to handle all the possible events in E. In Example 1, this cost arises from the use of moveable floodgates, which are lifted as soon as the precipitations increase above a certain threshold. On the other hand, each experiment in (cid:2) is associated to a cost c. In Example 1, this cost emerges from the use of a drone to monitor some of the checkpoints and evaluate whether lifting the corresponding floodgates is needed.Given a sequence S of length l and a binary vector x ∈ {0, 1}l, we putDS,x =(cid:6)j: x j =0DS j(25)Let us now assume that we keep monitoring all the events in D for which the experiments have not given a negative outcome. Over a time horizon [0, T ], the total cost that the system incurs upon performing the sequence of experiments S = (S1, . . . , Sn) with output X = X S is given by the following expression:(cid:15)(cid:15)(cid:16)(cid:16)n(cid:2)(cid:5)(S, X) =(t(Sk) − t(Sk−1))CD \ DSk−11, Xk−11+ (T − t(Sn))CD \ D+ nc1, XnSn1k=1= −n(cid:2)(T − t Sk )k=1(cid:15)(cid:15)DC1, XkSk1(cid:16)(cid:15)− CDSk−11, Xk−11(cid:16)(cid:16)+ T C(D) + cnA natural optimal problem is the minimization of the average global cost (cid:5)(S) = E[(cid:5)(S, X)]. If we put F (S) = E[C(DS, X )]and g(σ ) = T − t(σ ), we have that(cid:5)(S) = −F g(S) + T C(D) + cnMinimizing (cid:5)(S) for sequences of experiments of a given length is equivalent to maximizing F g (S). Indicated with S(n) a maximum of F g(S) for sequences of length n, we are finally left with the following problem:min (cid:5)(S) =|(cid:2)|minn=0[T d + cn − F g(S(n))]13(26)S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486Note that we can reconstruct the special case considered in Section 3.1 by choosing C as the cardinality function and assuming that deterministically Xi = 0 for all i.Below, we show that the assumptions in Corollary 1 are satisfied and hence our theory can be applied to find approx-imate maxima for F g(S) for sequences of experiments of any given length. It is worth noting that the inherent recursive structure of the solution obtained through the greedy algorithm yields a simple iterative solution of the final minimization problem (26).We now discuss the applicability of Corollary 1. The set of sequences on which this maximization takes place is here the set Hd((cid:2), ≺), where ≺ is any (−t)-ordering on the set (cid:2). Concerning the properties of F , we reason as follows.• Permutation invariance is obtained via the following computation. For a fixed S ∈ H((cid:2)) of length n and a permutation θ : {1, . . . , n} → {1, . . . , n}, we have that(cid:2)F (Sθ ) =P Sθ (x)C(DSθ ,x) =x∈{0,1}n(cid:2)x∈{0,1}nP Sθ (xθ )C(DSθ ,xθ ) =(cid:2)x∈{0,1}nP S (x)C(DS,x) = F (S)where the second equality follows from a relabeling of the running variable x and the third equality from the assump-tion of permutation covariance for P S and the definition of DS,x.• Note that, by the coherence property of P S , we can write for every S ∈ H((cid:2)) and σ ∈ (cid:2),F (S⊥σ ) − F (S) =P S⊥σ (x)[C(DS⊥σ ,x) − C(D]|S|S,x1(cid:2)x∈{0,1}|S|+1Since, by construction, Dnonnegative. This proves that F is monotonic.|S|S,x1⊆ DS⊥σ ,x (see Eq. (25)) and C is additive, it follows that the right hand side above is • We now fix the sequences R and S in H((cid:2)) and σ ∈ (cid:2) and we put n = |R| and m = |S|. Denoting the corresponding running output sequences as xR , xS , and xσ and using again the coherence property, we can writeF (R⊥S⊥σ ) − F (R⊥S) − F (S⊥σ ) + F (S) =(cid:2)(cid:2)(cid:2)xR ∈{0,1}nxR ∈{0,1}mxσ ∈{0,1}P R⊥S⊥σ (xR ⊥xS ⊥xσ )[C(DR⊥S⊥σ ,xR ⊥xS ⊥xσ )− C(DR⊥S,xR ⊥xS ) − C(DS⊥σ ,xS ⊥xσ ) + C(DS,xS )]Notice now thatC(DR⊥S⊥σ ,xR ⊥xS ⊥xσ ) − C(DR⊥S,xR ⊥xS ) =(cid:13)if xσ = 10C(Dσ \ DR⊥S,xR ⊥xS ) if xσ = 0whileC(DS⊥σ ,xS ⊥xσ ) − C(DS,xS ) =(cid:13)if xσ = 10C(Dσ \ DSxS ) if xσ = 0Since by construction DS,xS⊆ DR⊥S,xR ⊥xS and C is additive, it follows comparing (28) and (29) thatC(DR⊥S⊥σ ,xR ⊥xS ⊥xσ ) − C(DR⊥S,xR ⊥xS ) ≤ C(DS⊥σ ,xS ⊥xσ ) − C(DS,xS )This implies that the right hand side formula in Eq. (27) is nonpositive and, thus, F is submodular.8. Other applicative domains: job scheduling and recommender systemsIn this section, we present additional applicative examples that can be addressed within our framework.8.1. Job scheduling(27)(28)(29)We tackle a job scheduling problem that was first studied by Stadje [24]. Assume that (cid:2) is a set of jobs that need to be processed by a single machine subject to failure, which is modeled stochastically. We associate a number P (σ ) with each job σ ∈ (cid:2), which represents the probability that the machine does not fail while performing σ . We assume that the machine is not aging so the probability of not failing while performing a sequence of jobs S is simply P (S) =j P (S j). Every job σ is also associated with a reward R(σ ) ≥ 0 and a discount d(σ ) ∈ [0, 1[ (typically, the discount depends on −atσ ). The reward of performing the job σ after the sequence of jobs the time tσ needed to complete job σ , e.g. d(σ ) = e(cid:17)S has been performed is given by d(S)R(σ ), where d(S) =j d(S j). The objective function G on a sequence of jobs S is (cid:17)14S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486the expected total reward under the assumption that the machine keeps processing jobs of the sequence S until it fails. Formally, we haveG(S) =|S|(cid:2)k=1P (S|k−11)d(S|k−11)R(Sk)(30)(with the convention that P (∅)d(∅) = 1). The function G fits the class of functions in Eq. (1) and is formally equivalent to the function considered in the S&T problem described above. To see this, we put D(S) = P (S)d(S) and we note that, by multiplying and dividing the k-th addend in Eq. (30) by 1 − D(Sk), we obtain:G(S) =|S|(cid:2)[D(S|k−11k=1) − D(S|k1)]R(Sk)1 − D(Sk)(31)If we now put F (S) = 1 − D(S) and g(σ ) = R(σ )1−D(σ ) , we observe that G coincides with F g as defined in Eq. (1). The main result reported by Stadje [24] is that, restricting G to sequences of distinct jobs of a fixed length n, the optimal solution is a sequence S for which g is decreasing, namely g(S1) ≥ g(S2) ≥ · · · ≥ g(Sn). This conclusion is also implied by the general result expressed in Proposition 1. We now fix a g-ordering ≺ on (cid:2) and show that the properties needed to apply Corollary 1 hold on the fully extendable set Hd((cid:2), ≺):• D(S) is, by construction, permutation invariant and thus also F (S) = 1 − D(S) is permutation invariant.• Note that(cid:9)D(S, σ ) = D(S⊥σ ) − D(S) = D(S)[D(σ ) − 1]This implies (since D(σ ) ≤ 1 for every σ ) that (cid:9)D(S, σ ) ≤ 0 for every S and σ . Hence, D is anti-monotonic and, consequently, F monotonic.• From the equality(cid:9)2 D(S, σ1, σ2) = D(S⊥σ1⊥σ2) − D(S⊥σ1) − D(S⊥σ2) + D(S) = D(S)[1 − D(σ1)][1 − D(σ2)]it now follows that (cid:9)2 D(S, σ1, σ2) ≥ 0 for every S, σ1, and σ2. This yields the submodularity of F .• By definition, Hd((cid:2), ≺) is g-ordered.8.2. Recommender systemsFinally, we present and extend a recommender system application presented by Ashkan et al. [3]. Numerical simulations for this example are given in Section 9.Assume that (cid:2) is a set of movies and the function g : (cid:2) → [0, 1] attributes the corresponding satisfaction probability of a default user to each of them. Movies are organized under different genres, i.e. there is a set T of genres and a function t such that, for each σ ∈ (cid:2), t(σ ) ⊆ T is the subset of the genres covered by σ . The recommender system generates a sequence S ∈ Hd((cid:2)).The objective function G : Hd((cid:2)) → R is the probability of the user satisfaction assuming the following stochastic model of choice: the user chooses a genre t in T with a probability rt and picks the first item S i in the given sequence for which t ∈ t(S i). We use the notation i(t) to indicate such index i. Formally, we have that i(t) = min{i = 1, . . . , |S| | t ∈ t(S i)}. The user will be satisfied with probability g(S i(t)).We can formally compute G(S) as follows:⎛⎞|S|(cid:2)rt g(S i(t)) =(cid:2)⎝⎠ g(S i)rti=1t: i(t)=iG(S) =(cid:2)t∈TrtP (satisfied | t) =(cid:2)t∈TDefine now F : Hd((cid:2)) → R so that(cid:2)F (S) =rt(cid:22)t∈i t(S i)F (S) represents the probability that the chosen genre shows up in the sequence S and it holds(cid:2)rt = F (S|i1) − F (S|i−11)t: i(t)=iSubstituting the above expression in Eq. (32), we recognize that the function is in the form of Eq. (1).15(32)(33)S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486Ashkan et al. [3] study (for the case when rt are all equal) the optimality of the function F g over the set of sequences of distinct items of maximal length |(cid:2)| and discover that the solution, as in the previous example, is given by any S on which g is monotonically decreasing. In addition, they note that such optimal solution S can be trimmed by iteratively discarding ) = 0. In this way, they obtain the shortest possible recommended sequence of items all items S i for which F (S|istill maximizing the satisfaction probability.1) − F (S|i−11Similarly to the previous example, we now show that the properties needed to apply Corollary 1 hold on the fully extendable set Hd((cid:2), ≺), where ≺ is any fixed g-ordering. The function F (S) is, by construction, permutation invariant. It is known in the literature as a weighted coverage function: interpreting rt as the weight of genre t, F (S) represents the global weight of the genres covered by the sequence of movies S. Such functions represent a well known example of monotonic submodular functions (see the work by Krause and Golovin [17] for details). Finally, by definition, Hd((cid:2), ≺) is g-ordered.In practical applications, as also noted by Ashkan et al. [3], it may be of interest to optimize over sequences that are not necessarily of maximal length. In this direction, we propose a generalization of the above model that also leads to a function of the type of Eq. (1). Instead of assuming that a movie σ ∈ (cid:2) covers a set of genres t(σ ), we associate a probability vector pσ over T with each movie σ , where pσ (t) indicates to which extent movie σ covers genre t. Hence, we assume that the choice mechanism of the user is now the following: once the genre t has been selected, the user will pick S 1 with probability p S1 (t). If S1 is not chosen (which will happen with probability 1 − p S1 (t)), the user will pick S2 with probability p S2 (t) and so on. If S i(t) is the one chosen, the user will be satisfied with probability g(S i(t)). In this case:G(S) = 1|T |(cid:2)t∈TP (sat. | t) = 1|T |(cid:2)|S|(cid:2)t∈Ti=1g(S i)P (i(t) = i | t)whereP (i(t) = i | t) = (1 − p S1 (t)) · · · (1 − p S i−1 (t))p S i (t)If we now define|S|(cid:2)F (S) =(1 − p S1 (t)) · · · (1 − p S i−1 (t))p S i (t)i=1as the probability that one of the items of the sequence S is eventually picked, we have thatP (i(t) = i | t) = F (S|i1) − F (S|i−11)This shows that, in this more general case too, the function G has the same structure of the function in Eq. (1).In regard to the applicability of Corollary 1, note that the function F (S) is identical to the success probability as defined in the detection problem and is thus permutation invariant, monotonic, and submodular. Finally, the fully extendable set on which the maximization takes place is the set Hd((cid:2), ≺) introduced above, which is g-ordered by definition.9. Experimental resultsTo show the potential of our method, we now provide explicit numerical simulations for the S&T and recommender system applications described above.9.1. Search and trackingWe show the advantage of using the generalized greedy algorithm over the standard one by running both algorithms on several, randomly generated S&T problems. To highlight when the two algorithms exhibit different behaviors, we consider scenarios in which the detection probability of each pattern depends on the execution time associated with it. If the pat-terns associated with a lower t have a high detection probability, the standard and the generalized greedy search perform similarly. They prefer these early patterns by placing them at the beginning of the sequence and add the remaining patterns to the end of the sequence. Conversely, if the detection probabilities associated with patterns with a greater t are high enough, the standard greedy immediately places those patterns at the beginning of the sequence, but, as patterns are only added on the right side of the sequence, it never exploits early search patterns. In this way, it constructs short sequences that do not make full advantage of the richness of the set (cid:2). Instead, the generalized greedy, being free to place patterns in any position, manages to exploit both types of patterns.We generate 11,000 realistic problems instances, each with 20 candidate patterns and 40 destinations.1 Each pattern σis associated with a random sample of destinations. Time stamps are generated sequentially by taking a random sequence 1 See supplementary material available online at https://doi .org /10 .5281 /zenodo .3695080.16S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486Fig. 1. Average objective values obtained by the standard and the generalized greedy algorithms.Table 1Average running time (in milliseconds) of the standard and the generalized greedy algorithms.G standardG generalizedmax repetitions = 14 ± 116 ± 6max repetitions = 315 ± 467 ± 25= m · i + q, where m is a value between −1 and 1, and q is such that of all the search patterns ¯S = (σ1, . . . , σn) and imposing that t(σi) = t(σi−1) + r, where r is a random number. The detection probability is a linear function of the indexes of the sequence of search patterns in ¯S with different angular coefficients: i φσi is constant across the scenarios. A scenario φσiwith m = −1 corresponds to the case of patterns with a lower time stamp having higher detection probabilities, while, a scenario with m = 1, represents the case of patterns with higher time stamp having higher detection probabilities. When m = 0, all the patterns have the same detection probability.(cid:10)For each problem, we run the generalized and the standard greedy algorithms over sequences of maximal length 10. We consider two types of sequences: in the first, we establish that all patterns must be distinct; in the second, instead, we allow at most 3 repetitions for each pattern.Fig. 1 shows the average objective values found by the two algorithms for different scenarios: the left plot corresponds to the case of distinct patterns, while the right plot to the case of a maximum of 3 repetitions per pattern. The figure shows that, in all cases, the generalized greedy algorithm dominates the standard algorithm. As expected, the difference in performance is particularly high (considering the ratio) in scenarios where the search patterns associated with a greater execution time have a higher detection probability. The average running time across all scenarios of the two algorithms and across all instances is reported in Table 1. While the generalized greedy algorithm is slightly more time consuming than the standard algorithm, the runtime is acceptable for the real S&T application as the optimization of the objective function is typically performed within a time limit of one minute [2,23].9.2. Recommender systemsFor the recommender systems application, we analyze the performance of our generalized greedy algorithm for the same case study considered by Ashkan et al. in [3], and we make a comparison with the DUM algorithm proposed therein. The general setting is described in Section 8.2. Each movie σ in a set (cid:2) is equipped with a satisfaction probability g(σ ) and a subset of genres t(σ ) ⊆ T . For a fixed positive integer K , Ashkan et al. [3] compare sequences of movies S of length at most K from (cid:2) based on two performance indices: the Intra-List Distance metric (ILD) [25] and the normalized Discounted Cumulative Gain (nDCG) [26]. ILD measures the diversity of a sequence and is formally defined, for a sequence S, asI L D(S) = 1|S|2(cid:2)i, j≤|S||t(S i)(cid:9)t(S j)|where t(S i)(cid:9)t(S j) indicates the symmetric difference between the two subsets t(S i) and t(S j). nDCG is a discounted accu-mulated measure of the level of satisfaction of a sequence, formally defined, for a sequence S, asnDC G(S) = 1C|S|(cid:2)i=1g(S i)log2(i + 1)where C is a normalization constant defined as the ideal gain obtained for a sequence of the same length and maximum satisfaction probability for all its elements.The DUM algorithm considered by Ashkan et al. [3] and explained in Section 8.2 consists in sorting the elements of (cid:2)according to their satisfaction probability g and, then, removing from the sequence all those elements that do not increment 17S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486Table 2Comparison of Dum and Greedy.K = 5DumGreedyILD1.852.20nDCG0.780.83K = 10ILD1.912.12nDCG0.720.82K = 15ILD1.842.01nDCG0.720.82the value of the function F . The sequence obtained in this way has a length that is guaranteed to be below the total number of genres. Since in this case the authors want to produce a sequence of prescribed length K that is in general smaller than the number of genres (|T | = 18 in the considered case study), they apply a modification of their algorithm by substituting the original function F with the following one:F (S) =(cid:2)t∈Tmin{(cid:2)σ ∈(cid:2)|{σ : t ∈ t(σ )}|, Nt}(34)(cid:10)where Nt is a prescribed number of movies of genre t that are forced to appear in the sequence. These numbers Nt are chosen so that t Nt = K and are constructed by making use of the user’s preferences rt of the various genres.Specifically, the case study considered starts from the 1M MovieLens dataset,2 consisting of one million movie ratings from 6040 unique users, from which users with more than 300 ratings are then selected. This results in a dataset of 955 users with 502k ratings for 3644 unique movies, divided into 18 genres. For each user, rated movies are split into a training and a test set with a 2 : 1 ratio. The test set forms the set (cid:2) of recommendable movies. Data in the training set is used to create the user’s interest and genre profiles. The user’s interest profile is generated using matrix factorization via singular value decomposition [27] and provides, for each movie σ ∈ (cid:2), the satisfaction probability g(σ ). The genre profile consists in the empirical distribution rt on the movie genres T obtained from the training set rated by the user.(cid:8)For Dum, the numbers Nt are computed as follows [3]: first, another empirical distribution rt is calculated by sampling 10˜Nt =elements from the original distribution rt on the genre set, and, then, it is established that ˜Nt = (cid:19)r0, then Nt = ˜Nt is set. Otherwise, D more elements t1, . . . , t D are further sampled from the distribution rt , and, finally, it is put that Nt = ˜Nt + |{i = 1, . . . , D | ti = t}.· K (cid:20). If D = K −(cid:10)t(cid:8)tIn our approach, instead, we apply the original function F defined in Eq. (33) using, for each user, the original distribution rt of the genre profile and maximizing directly over sequences of prescribed length K .We perform experiments with different values of K . Every experiment is repeated 3 times, with different training and test set splits. In Table 2, we report the comparison between Dum and our generalized greedy algorithm that maximizes Eq. (32) with respect to ILD and nDCG. As shown in the Table 2, our approach performs better than Dum against both metrics. Our improved performance is achieved thanks to the flexibility of our algorithm, which incorporates the full set of genres in the optimization problem directly, as opposed to Dum that samples a subset of genres before the construction of the sequences.10. ConclusionsIn this paper, we show that, in several applicative domains, the problem of finding a sequence of objects that maximizes a reward can be expressed as the maximization of a recursive function that exhibits the structure captured by Eq. (1). After proving that existing greedy algorithms do not yield strong theoretical guarantees for such a function, we study its properties and generalize the notions of monotonicity and submodularity by adapting them to fully extendable sets of sequences. We then introduce an efficient generalized greedy approach that ensures finding solutions that are O (1 − 1e ) of the optimal. Our method is general and can be applied to any domain with an objective function that can be transformed in the form of Eq. (1). To support this thesis, we present evidence that our technique works across several applications and provide explicit numerical simulations for two domains, S&T and recommender systems. The experiments directly show the power of our new algorithm. Our work contributes to the discussion on submodularity by stepping away from the specific details of practical applications and presenting general properties of functions often encountered in them, which can be exploited to find better solutions more efficiently.Declaration of competing interestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.2 https://grouplens .org /datasets /movielens /1m/.18S. Bernardini, F. Fagnani and C. PiacentiniArtificial Intelligence 297 (2021) 103486AcknowledgementsThis work has been supported by EPSRC Grant EP/S016473/1, Leverhulme Trust Grant VP1-2019-037 and MIUR Grant “Dipartimenti di Eccellenza 2018-2022” (CUP: E11G18000350001). We thank the anonymous reviewers for their detailed and rigorous reviews. Research data used for this paper is available at https://doi .org /10 .5281 /zenodo .3695080.References[1] A. Krause, C. Guestrin, Near-optimal observation selection using submodular functions, in: Proceedings of the 22nd National Conference on Artificial Intelligence - Volume 2, AAAI’07, AAAI Press, 2007, pp. 1650–1654.[2] C. Piacentini, S. Bernardini, C. Beck, Autonomous target search with multiple coordinated UAVs, J. Artif. Intell. Res. 65 (2019) 519–568.[3] A. Ashkan, B. Kveton, S. Berkovsky, Z. Wen, Optimal greedy diversity for recommendation, in: Proceedings of the 24th International Conference on [4] S. Tschiatschek, A. Singla, A. Krause, Selecting sequences of items via submodular maximization, in: Proceedings of the Thirty-First AAAI Conference Artificial Intelligence, IJCAI’15, AAAI Press, 2015, pp. 1742–1748.on Artificial Intelligence, AAAI’17, AAAI Press, 2017, pp. 2667–2673.[5] S. Alaei, A. Malekian, Maximizing sequence-submodular functions and its application to online advertising, in arXiv:1009 .4153v1, 2010, pp. 1–18.[6] J. McAuley, R. Pandey, J. Leskovec, Inferring networks of substitutable and complementary products, in: Proceedings of the 21th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and Data Mining, KDD’15, ACM, New York, NY, USA, 2015, pp. 785–794.[7] M. Mitrovic, M. Feldman, A. Krause, A. Karbasi, Submodularity on hypergraphs: from sets to sequences, in: Proceedings of the International Conference on Artificial Intelligence and Statistics, AISTATS’18, PMLR, 2018, pp. 1177–1184.[8] S. Fujishige, Submodular Functions and Optimization, Annals of Discrete Mathematics, Elsevier, 2005.[9] G.L. Nemhauser, L. Wolsey, An analysis of approximations for maximizing submodular set functions, Math. Program. 14 (1978) 265–294.[10] D. Golovin, A. Krause, Adaptive submodularity: theory and applications in active learning and stochastic optimization, J. Artif. Intell. Res. 42 (1) (2011) 427–486.[11] A. Krause, C. Guestrin, Near-optimal nonmyopic value of information in graphical models, in: Proceedings of the Twenty-First Conference on Uncer-tainty in Artificial Intelligence, UAI’05, AUAI Press, Arlington, Virginia, United States, 2005, pp. 324–331.[12] S. Dughmi, T. Roughgarden, M. Sundararajan, Revenue submodularity, Theory Comput. 8 (2012) 95–119.[13] B. Lehmann, D. Lehmann, N. Nisan, Combinatorial auctions with decreasing marginal utilities, Games Econ. Behav. 55 (2) (2006) 270–296.[14] S.C.H. Hoi, R. Jin, J. Zhu, M.R. Lyu, Batch mode active learning and its application to medical image classification, in: Proceedings of the 23rd Interna-tional Conference on Machine Learning, ICML’06, ACM, New York, NY, USA, 2006, pp. 417–424.[15] Z. Zhang, E.K.P. Chong, A. Pezeshki, W. Moran, String submodular functions with curvature constraints, IEEE Trans. Autom. Control 61 (3) (2016) [16] A. Krause, A. Singh, C. Guestrin, Near-optimal sensor placements in gaussian processes: theory, efficient algorithms and empirical studies, J. Mach. 601–616.Learn. Res. 9 (2008) 235–284.[17] A. Krause, D. Golovin, Submodular Function Maximization, Cambridge University Press, 2014, pp. 71–99, Ch. 3.[18] M. Streeter, D. Golovin, An online algorithm for maximizing submodular functions, in: D. Koller, D. Schuurmans, Y. Bengio, L. Bottou (Eds.), Advances in Neural Information Processing Systems, vol. 21, Curran Associates, Inc., 2009, pp. 1577–1584.[19] C. Qian, C. Feng, K. Tang, Sequence selection by Pareto optimization, in: Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI’18, International Joint Conferences on Artificial Intelligence Organization, 2018, pp. 1485–1491.[20] M. Mitrovic, E. Kazemi, M. Feldman, A. Krause, A. Karbasi, Adaptive sequence submodularity, in: 33rd Conference on Neural Information Processing Systems, NeurIPS’19, 2019, pp. 5353–5364.[21] S. Bernardini, M. Fox, D. Long, C. Piacentini, Deterministic vs probabilistic methods for searching for an evasive target, in: Proceedings of the 31st AAAI Conference on Artificial Intelligence, AAAI’17, 2017, pp. 3709–3715.[22] S. Bernardini, M. Fox, D. Long, C. Piacentini, Leveraging probabilistic reasoning in deterministic planning for large-scale autonomous search-and-tracking, in: Proceedings of the 26th International Conference on Automated Planning and Scheduling, ICAPS’16, 2016, pp. 47–55.[23] S. Bernardini, M. Fox, D. Long, Combining temporal planning with probabilistic reasoning for autonomous surveillance missions, Auton. Robots 41 (1) (2017) 181–203.mender Systems, 2008, pp. 123–130.[24] W. Stadje, Selecting jobs for scheduling on a machine subject to failure, Discrete Appl. Math. 63 (3) (1995) 257–265.[25] M. Zhang, N. Hurley, Avoiding monotony: improving the diversity of recommendation lists, in: Proceedings of the 2008 ACM Conference on Recom-[26] K. Järvelin, J. Kekäläinen, Cumulated gain-based evaluation of IR techniques, ACM Trans. Inf. Syst. 20 (4) (2002) 422–446.[27] D. Scott, S. Dumais, G.W. Furnas, T.K. Landauer, R. Harshman, Indexing by latent semantic analysis, J. Am. Soc. Inf. Sci. Technol. 41 (6) (1990) 391–407.19