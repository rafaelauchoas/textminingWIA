Artificial Intelligence 244 (2017) 70–94Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintConstrained clustering by constraint programmingThi-Bich-Hanh Dao∗, Khanh-Chuong Duong, Christel VrainUniv. Orléans, INSA Centre Val de Loire, LIFO, EA 4022, F-45067, Orléans, Francea r t i c l e i n f oa b s t r a c tArticle history:Received in revised form 12 May 2015Accepted 23 May 2015Available online 29 May 2015Keywords:Constrained clusteringBi-criterion clusteringConstraint programmingModelingGlobal optimization constraintFiltering algorithmConstrained Clustering allows to make the clustering task more accurate by integrating user constraints, which can be instance-level or cluster-level constraints. Few works consider the integration of different kinds of constraints, they are usually based on declarative frameworks and they are often exact methods, which either enumerate all the solutions satisfying the user constraints, or find a global optimum when an optimization criterion is specified. In a previous work, we have proposed a model for Constrained Clustering based on a Constraint Programming framework. It is declarative, allowing a user to integrate user constraints and to choose an optimization criterion among several ones. In this article we present a new and substantially improved model for Constrained Clustering, still based on a Constraint Programming framework. It differs from our earlier model in the way partitions are represented by means of variables and constraints. It is also more flexible since the number of clusters does not need to be set beforehand; only a lower and an upper bound on the number of clusters have to be provided. In order to make the model-based approach more efficient, we propose new global optimization constraints with dedicated filtering algorithms. We show that such a framework can easily be embedded in a more general process and we illustrate this on the problem of finding the optimal Pareto front of a bi-criterion constrained clustering task. We compare our approach with existing exact approaches, based either on a branch-and-bound approach or on graph coloring on twelve datasets. Experiments show that the model outperforms exact approaches in most cases.© 2015 Elsevier B.V. All rights reserved.1. IntroductionConstrained Clustering has received much attention this last decade. It allows to make the clustering task more accurate by integrating user constraints. Several kinds of constraints can be considered. First, constraints may be used to limit the size or the diameter of clusters; second, they can enforce expert knowledge instances that must be or cannot be in the same cluster (must-link or cannot-link constraints). Much work has focused on instance-based constraints and has adapted classi-cal clustering methods to handle must-link or cannot-link constraints. A small number of earlier studies have considered the integration of different kinds of constraints. These studies are based on declarative frameworks and offer exact methods that either enumerate all the solutions satisfying the user constraints, or find a global optimum when an optimization criterion is given. For instance, in [1] a SAT based framework for constrained clustering has been proposed, integrating many kinds of user constraints but limited to clustering tasks into two clusters. A framework for conceptual clustering based on Integer Linear Programming has also been proposed in [2]. In [3], we have presented a model based on Constraint Programming for * Corresponding author.E-mail addresses: thi-bich-hanh.dao@univ-orleans.fr (T.-B.-H. Dao), khanh-chuong.duong@univ-orleans.fr (K.-C. Duong), christel.vrain@univ-orleans.fr(C. Vrain).http://dx.doi.org/10.1016/j.artint.2015.05.0060004-3702/© 2015 Elsevier B.V. All rights reserved.T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9471constrained clustering. This model allows to choose one among different optimization criteria and to integrate various kinds of user constraints. As far as we know, the approach we propose is the only one able to handle different optimization crite-ria and all popular constraints, for any number of clusters. It is based on Constraint Programming (CP): in such a paradigm, a constraint optimization problem or a constraint satisfaction problem is modeled by defining variables with their domains and by expressing constraints on these variables. Solving a CP problem relies on two operations: constraint propagation that reduces the domain of the variables by removing inconsistent values and branching that divides the problem in subprob-lems, by taking an unassigned variable and by splitting its domain into several parts. It is important to notice that modeling a task in Constraint Programming implies several choices, which have a high impact on the efficiency of the approach: the choice of the variables and the choice of the constraints for the model, the development of filtering algorithms dedicated to the task and the use of adapted search strategies for solving the model. A point in favor of CP is that the requirement of getting an exact solution can be relaxed by using metaheuristics or local search methods. For the time being, we have fully investigated exact methods, to push the efficiency of the framework as far as possible. Approximate search strategies could be integrated in the future.In this paper, we propose a new model for Constrained Clustering, still based on Constraint Programming, but signifi-cantly improved compared to the previous model [3]. In the previous model, two sets of variables were introduced, namely a variable for each cluster identifying a cluster by one of its points and a variable for each point expressing its assignment to a cluster. The number of clusters had to be set beforehand. The new model we present here contains only a variable for each point, giving the index of the cluster the point belongs to. As a result, the constraints enforcing the solution to be a partition and breaking symmetries are entirely different. The new model is lighter in terms of the number of variables. It also enables to remove the restriction on the number of clusters; only bounds on the number of clusters are required. Moreover, in order to make this model efficient, we have developed dedicated global constraints for three optimization criteria: minimizing the maximal diameter, maximizing the split between clusters, and minimizing the within-cluster sum of dissimilarities.The approach we propose may be easily embedded in a general process for the task of Constrained Clustering. Consider-ing Data Mining as an iterative and interactive process composed of the classical steps of task formulation, data preparation, application of a tool, thus requiring to set parameters, and validation of the results, a user can specify the task at hand including or not some constraints and decide to change the settings according to the results. He/she may decide to change the constraints, removing or relaxing some constraints, adding or hardening other constraints. The modularity and declara-tivity of our model allow this easily. In this paper, we illustrate the integration of our model in a more complex process by considering a bi-criterion clustering problem, namely finding the Pareto front when minimizing the maximal diameter and maximizing the minimal split. To achieve this, our framework is integrated in an algorithm, which alternatively calls our model to minimize the maximal diameter and then to maximize the split between clusters with adapted constraints.Our contributions are as follows.• We propose a new model based on Constraint Programming, allowing to find an optimal solution for clustering under constraints, given an optimization criterion. This new model improves substantially the previous one, it is more modular (each criterion is implemented by a global constraint) and it is much more efficient.• We show that such a framework can easily be embedded in a more general process and we illustrate this on the problem of finding the optimal Pareto front of a bi-criterion constrained clustering task. As far as we know, this is the first approach to handle bi-criterion clustering in presence of user-constraints.• We propose new global optimization constraints with dedicated filtering algorithms, thus allowing to make the model more efficient.• We compare this model with existing exact approaches, based either on a branch-and-bound approach [4] or on graph coloring [5] on twelve datasets. Experiments show that the model we propose is generally more efficient. Moreover we compare the two models based on CP that we have developed and we show that the different changes (search strategy and development of global constraints) allow to improve the model.The paper is organized as follows. Section 2 is dedicated to preliminaries on Constrained Clustering and Constraint Programming. Related work is presented in Section 3. Section 4 is devoted to the presentation of both CP models, the first one presented in [3] and the new one. The filtering algorithms for the optimization criteria are presented in Section 5. We show in Section 6 how our framework can be easily integrated for solving a bi-criterion constrained clustering task. Experiments are presented in Section 7, showing the performance and the flexibility of our approach.2. Preliminaries2.1. Constrained clusteringCluster analysis is a Data Mining task that aims at partitioning a given set of objects into homogeneous and/or well-separated subsets, called classes or clusters. It is often formulated as the search for a partition such that the objects inside the same cluster are similar, while being different from the objects belonging to other clusters. These requirements are usually expressed by an optimization criterion and the clustering task is usually defined as finding a partition of objects 72T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94that optimizes the given criterion. In the remainder of the paper, we consider a dataset of n objects O = {o1, . . . , on} and a dissimilarity measure d(oi, o j) between any two objects oi, o j ∈ O. A partition (cid:2) of objects into k clusters C1, . . . , Ck is such that: (1) for all c ∈ [1, k],1 Cc (cid:4)= ∅, (2) ∪c Cc = O and (3) for all c (cid:4)= c, Cc ∩ Cc(cid:7) = ∅. The optimization criterion can be among others:(cid:7)• Minimizing the maximal diameter of clusters: the maximal diameter of a partition (cid:2) is the largest dissimilarity between two objects in the same cluster,D((cid:2)) =maxc∈[1,k],oi ,o j ∈Cc,(d(oi, o j)).A clustering task that minimizes this criterion is also called nonhierarchical complete-link clustering.• Maximizing the minimal split between clusters: the minimal split between clusters of a partition (cid:2) is the smallest dissimilarity between two objects of different clusters,S((cid:2)) =minc<c(cid:7)∈[1,k],oi ∈Cc,o j ∈Cc(cid:7)(d(oi, o j)).A clustering task that maximizes this criterion is also called single-link clustering.• Minimizing the within-cluster sum of dissimilarities (WCSD): this sum for a partition (cid:2) is defined asWCSD((cid:2)) =(cid:2)c∈[1,k]12(cid:2)oi ,o j ∈Ccd(oi, o j).For this criterion the dissimilarity d(oi, o j) is usually measured by the squared Euclidean distance between oi and o j .• Minimizing the within-cluster sum of squares (WCSS): in an Euclidean space the within-cluster sum of squares is the sum of the squared Euclidean distances between each object oi and the centroid mc of the cluster containing oi(cid:2)(cid:2)WCSS((cid:2)) =||oi − mc||2.c∈[1,k]oi ∈CcLet us notice that, when the squared Euclidean distance is used for measuring the dissimilarities, the WCSS criterion is mathematically equivalent to the WCSD criterion standardized by the division by the size of each cluster:WCSS((cid:2)) =(cid:2)c∈[1,k]12|Cc|(cid:2)oi ,o j ∈Ccd(oi, o j).Most of the clustering algorithms rely on an optimization criterion. All of these criteria are NP-Hard, except the split crite-rion. In consequence, most of the algorithms search for a local optimum. For instance, the k-means algorithm finds a local optimum for the WCSS criterion as well as FPF (Furthest Point First) [6] for the diameter criterion. Several optima may exist, some may be closer to the one expected by the user. In order to model the task better, but also in the hope of reducing the complexity, user specified constraints are added, leading to Constrained Clustering that aims at finding clusters that satisfy the user constraints. User constraints can be classified into cluster-level constraints, specifying requirements on clusters, or instance-level constraints, specifying requirements on pairs of objects. Most of the attention has been put on instance-level constraints, first introduced in [7]. Commonly, two kinds of constraints are used: must-link and cannot-link. A must-link constraint on two objects oi and o j expresses that they must be in the same cluster: ∀c ∈ [1, k], oi ∈ Cc ⇔ o j ∈ Cc . A cannot-link constraint on two objects oi and o j expresses that they must not be in the same cluster: ∀c ∈ [1, k], ¬(oi ∈ Cc ∧ o j ∈ Cc).Cluster-level constraints impose requirements on the clusters. The minimum capacity constraint requires that each cluster has a number of objects greater than a given threshold α: ∀c ∈ [1, k], |Cc| ≥ α, whereas the maximum capacity constraint requires each cluster to have a number of objects inferior to a predefined threshold β: ∀c ∈ [1, k], |Cc| ≤ β.The maximum diameter constraint specifies an upper bound γ on the diameter of the clusters: ∀c ∈ [1, k], ∀oi, o j ∈Cc, d(oi, o j) ≤ γ . The minimum split constraint, also called the δ-constraint in [8], requires the distance between any two (cid:7) (cid:4)= c, ∀oi ∈ Cc, ∀o j ∈ Cc(cid:7) , d(oi, o j) ≥ δ. points of different clusters to be superior or equal to a given threshold δ: ∀c ∈ [1, k], ∀cAs observed in [8], the maximum diameter constraint can be represented by a conjunction of cannot-link constraints and the minimum split constraint can be represented by a conjunction of must-link constraints.The (cid:7)-constraint introduced in [8] requires for each point oi to have in its neighborhood of radius (cid:7) at least another point belonging to the same cluster: ∀c ∈ [1, k], ∀oi ∈ Cc, ∃o j ∈ Cc, o j (cid:4)= oi and d(oi, o j) ≤ (cid:7). This constraint tries to capture the notion of density, introduced in DBSCAN [9]. We have extended it by proposing a density-based constraint, stronger than the (cid:7)-constraint: it requires that for each point oi , its neighborhood of radius (cid:7) contains at least m points belonging to the same cluster as oi .1 For a discrete variable, [1, k] denotes the set of integers from 1 to k.T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9473Fig. 1. Effect with different criteria. A: intuitive groups; B: complete link; C: single link; D: WCSS.In the last ten years, many works have been done to extend classical algorithms for handling must-link and cannot-link constraints. This is achieved either by modifying the dissimilarity measure, or the objective function or the search strategy. Recently, several works have investigated declarative approaches of constrained clustering, which aim at extending traditional algorithms to different types of user constraints. A presentation of these works is given in Section 3.2.2. Bi-criterion constrained clusteringClustering with the criterion of minimizing the maximal diameter aims at finding homogeneous clusters, but it often suffers from the dissection effect [10], i.e. quite similar objects may be classified in different clusters, in order to keep the diameters small. On the other hand, clustering with the criterion of maximizing the minimal split, which aims at finding well separated clusters, often suffers from the chain effect [11], i.e. a chain of closed objects may lead to group very different objects in the same cluster. The popular WCSS criterion, which minimizes the sum of the squared distances between points and the center of their cluster also suffers from undesirable effects. Considering this criterion, objects that should be in a large group may be classified in different clusters in order to keep this sum small. Fig. 1 gives an illustration of these effects. Image A shows three groups that can be easily identified. Image B shows the obtained solution with the diameter criterion when the number of clusters is set to 3. In this partition, some points are very close but they are classified in two different groups. The partition obtained when considering the split criterion is shown in Image C. Because of the chain effect, the largest group contains points that are very far each from other. The optimal solution with the WCSS criterion is shown in Image D. In this partition, some points that are very close are grouped in different clusters.A good partition with homogeneous and well-separated clusters should have a minimal diameter and a maximal split. Unfortunately, such a partition in general does not exist, since the two criteria are often conflicting. This problem can be modeled by considering the bi-criterion of maximizing the minimal split between clusters and minimizing the maximal diameter, as introduced in [5]. Considering these two criteria together is natural and allows to capture both the homogeneity and the separation requirements for a good clustering. A general approach for handling two optimization criteria is to find the Pareto optimal solutions. A Pareto optimal solution is a solution such that it is not possible to improve the value of one criterion without degrading the value of the other one. A partition (cid:2)(cid:7)dominates a partition (cid:2) if and only if:or(cid:7)(cid:7)D((cid:2)D((cid:2)) ≤ D((cid:2)) and S((cid:2)(cid:7)) > S((cid:2))) < D((cid:2)) and S((cid:2)(cid:7)) ≥ S((cid:2)).A partition (cid:2) is Pareto optimal if and only if there is no partition (cid:2)(cid:7)that dominates (cid:2). Two Pareto optimal solutions (cid:2)1and (cid:2)2 are equivalent if D((cid:2)1) = D((cid:2)2) and S((cid:2)1) = S((cid:2)2). A set P of Pareto optimal solutions is complete if any Pareto optimal solution is either in P or equivalent to an element of P . The set P is minimal if no two partitions of P are equivalent. The Pareto front is the projection of all the Pareto optimal solutions in the criterion space, i.e., the set of pairs (D((cid:2)), S((cid:2))) where (cid:2) is a Pareto optimal solution. If P is a complete and minimal set of Pareto optimal solutions, the set {(D((cid:2)), S((cid:2))) | (cid:2) ∈ P} is equal to the Pareto front. Fig. 2 gives an illustration of the Pareto front. A point in the Pareto front can correspond to several partitions.If the user specifies a function on the criteria to optimize, for example max(S/D) or min[α D − (1 − α)S] with 0 ≤ α ≤ 1, the optimal solution will be among the Pareto optima.Let us consider, for instance, the example given in Fig. 1. When the number of classes is set to 3, a complete and minimal set of Pareto solutions is given in Fig. 3. If the ratio S/D is minimized, the optimal solution is solution 5, which is the one 74T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94Fig. 2. Pareto front.Fig. 3. Pareto optimal solutions.that fits the best the intuitive groups. The user can specify conditions on the desired solutions. If for instance it is specified that points 5 and 14 must be in the same cluster, then only solutions 5 and 6 are found. If another condition is added, requiring the size of each group to be at least 2, only solution 5 is found.A bi-criterion clustering algorithm finding a complete and minimal set of Pareto solutions for different values of the number k of clusters is proposed in [5]. When k = 2, an exact polynomial algorithm is proposed in [12,13]. However, to the best of our knowledge, there is no algorithm dealing with this bi-criterion, while supporting various kinds of user constraints.2.3. Constraint programmingConstraint Programming (CP) is a powerful paradigm to solve combinatorial problems, based on Artificial Intelligence or Operational Research methods. A Constraint Satisfaction Problem (CSP) is a triple (cid:15) X, Dom, C(cid:16) where:• X = (cid:15)x1, x2, . . . , xn(cid:16) is an n-tuple of variables,• Dom = (cid:15)Dom(x1), Dom(x2), . . . , Dom(xn)(cid:16) is a corresponding n-tuple of domains such that xi ∈ Dom(xi),• C = (cid:15)C1, C2, . . . , Ct(cid:16) is a t-tuple of constraints where each constraint Ci expresses a condition on a subset of X .A solution of a CSP is a complete assignment of values from Dom(xi) to each variable xi that satisfies all the constraints of C . A Constraint Optimization Problem (COP) is a CSP with an objective function to be optimized. An optimal solution of a COP is a solution of the CSP that optimizes the objective function. In general, solving a CSP is NP-Hard. Nevertheless, the methods T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9475Fig. 4. Value graph for alldifferent(x1, x2, x3) with Dom(xi ) = {1, 2}.used by the solvers enable to efficiently solve a large number of real applications. They rely on constraint propagation and search strategies.Constraint propagation of a constraint c reduces the domain of the variables of c, by removing some or all inconsistent values, i.e., values that cannot be part of a solution of c. A set of propagators is associated to each constraint, it depends on the kind of consistency required for this constraint. If arc consistency is required, the propagators remove all the inconsistent values in each domain. If bound consistency is required, the propagators modify only the bounds of the domains. The type of consistency is chosen by the programmer when the constraint is defined. Different kinds of constraints are available for the programmer; they can be elementary constraints expressing arithmetic or logic relations, or global constraints expressing meaningful n-ary relations. One of the best known global constraints is the constraint alldifferent(x1, . . . , xn), which imposes the variables xi to be pairwise different. Global constraints benefit from efficient propagation, performed by a filtering algo-rithm exploiting results from other domains as for instance graph theory. From a logical point of view, a global constraint is equivalent to a conjunction of elementary constraints, e.g. the constraint alldifferent(x1, x2, x3) is equivalent to the con-junction of binary constraints x1 (cid:4)= x2 ∧ x1 (cid:4)= x3 ∧ x2 (cid:4)= x3. The interesting point is that a global constraint with its filtering algorithm has much more powerful propagation than the set of propagators of the elementary constraints. Different global constraints are developed, each one aims at exploiting more efficiently an n-ary relation. Filtering algorithms for global constraints use operational research techniques or graph theory to achieve generalized arc consistency or bound consistency with low complexity. A catalog of global constraints with more than 400 inventoried global constraints is maintained in [14].Example 2.1. Let X = {x1, x2, x3} with Dom(xi) = {1, 2}. Let P 1 be a CSP defined on X by the constraints:x1 (cid:4)= x2, x1 (cid:4)= x3, x2 (cid:4)= x3.The arc consistency for each individual constraint xi (cid:4)= x j cannot remove any value from the domains Dom(xi) and Dom(x j), since each value is part of a solution (xi = 1, x j = 2 and xi = 2, x j = 1). The CSP P 1 is however inconsistent, there is no solution that satisfies all the constraints, but the propagation of individual constraints cannot detect it. Let P 2 be the CSP defined on X by the single constraint alldifferent(x1, x2, x3). The filtering algorithm for this constraint [15] maintains the bipartite graph G = (V , E), with V = {x1, x2, x3} ∪ {1, 2} and E = {(xi, v) | v ∈ Dom(xi)}. This bipartite graph, which is also called the value graph of X , is given in Fig. 4. A matching M ⊆ E is a set of disjoint edges, i.e. two edges in M cannot share a vertex. Two important observations on the relationship between the constraint alldifferent(x1, . . . , xn) and matching are introduced in [15]:• There is a matching of cardinality n if and only if the constraint alldifferent(x1, . . . , xn) is satisfiable.• An edge (xi, v) belongs to a matching of cardinality n if and only if the value v is consistent with the constraint.From these observations, the filtering algorithm can detect inconsistencies and can remove all the inconsistent values. In this example, the bipartite graph G has no matching of cardinality 3. The constraint alldifferent(x1, x2, x3) is then inconsistent.In a CP solver, two steps, constraint propagation and branching, are repeated until a solution is found. Constraints are propagated until a stable state, in which the domains of the variables are reduced as much as possible. If the domains of all the variables are reduced to singletons then a solution is found. If the domain of a variable becomes empty, then there exists no solution with the current partial assignment and the solver backtracks. In the other cases, the solver chooses a variable whose domain is not reduced to a singleton and splits its domain into different parts, thus leading to new branches in the search tree. The solver then explores each branch, activating constraint propagation since the domain of a variable has been modified.The search strategy can be determined by the programmer. If a depth-first strategy is used, the solver orders branches following the order given by the programmer and explores in depth each branch. For a constraint optimization problem, a branch-and-bound strategy can be integrated to a depth-first search: each time a solution, i.e. a complete assignment of variables satisfying the constraints, is found, the value of the objective function for this solution is computed and a new 76T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94Fig. 5. Search trees with variable choice S, E, N, D, M, O , T , Y (left) and S, T , Y , N, D, E, M, O (right).constraint is added, expressing that a new solution must be better than this one. Assume that the objective function is represented by a variable y, which is to be minimized. When a solution to the problem is found, its corresponding objective value fis added. This implies that only the first best solution found is returned by the solver. The solver performs a complete search, pruning only branches that cannot lead to a solution and therefore finds an optimal solution. The choice of variables and of values at each branching is very important, since it may drastically reduce the search space and therefore computation time.is computed and the constraint y < fIn the context of constraint optimization problems, an optimization constraint is a global constraint that is linked to the objective function. Each solution induces a “cost” and the global constraint exploits this cost to filter not only the variable which represents the objective function, but also other decision variables inside the constraint. The first filtering algorithm for this kind of global constraints has been proposed in [16]. A well-known example of extension of global constraints to optimization constraints is the constraint cost_gcc [17], which extends the Global Cardinality Constraint with cost. For more details on global constraints, search and more generally on CP, we refer the reader to [18].Example 2.2. Let us illustrate by a simple COP: find an assignment of letters to digits such that SEND + MOST = MONEY, and maximizing MONEY. This problem can be modeled by a COP with eight variables S, E, N, D, M, O , T , Y , having as domain the set of digits {0, . . . , 9} and a variable V of domain integer, which represents the objective function, which is to be maximized. Constraints for this problem are:• the first digits must be different from 0: S (cid:4)= 0, M (cid:4)= 0,• the values of the letters are pairwise different: alldifferent(S, E, N, D, M, O , T , Y ),• (1000S + 100E + 10N + D) + (1000M + 100O + 10S + T ) = 10 000M + 1000O + 100N + 10E + Y ,• V = 10 000M + 1000O + 100N + 10E + Y .The initial constraint propagation leads to a stable state, with the domains: D S = {9}, D E = {2, 3, 4, 5, 6, 7}, D M = {1}, D O = {0}, D N = {3, 4, 5, 6, 7, 8} and D D = D T = D Y = {2, 3, 4, 5, 6, 7, 8}. Since some domains are not reduced to sin-gletons, branching is then performed. At the end of the search, we get the optimal solution with the assignment S = 9, E = 7, N = 8, D = 2, M = 1, O = 0, T = 4, Y = 6, leading to MONEY = 10 876.Strategies specifying the way branching is performed are very important. When variables are chosen in the order S, E, N, D, M, O , T , Y and when values are chosen following an increasing order, the search tree is composed of 29 nodes and 7 intermediary solutions (solutions satisfying all the constraints, better than the previous ones found but not optimal). When variables are chosen in the order S, T , Y , N, D, E, M, O , the search tree has only 13 nodes and 2 intermediary so-lutions. Fig. 5 presents the two corresponding search trees, which are generated by the Gist environment of the Gecode solver [19]. In these search trees, a blue circle represents a stable state but not yet a solution, a red square a fail state (there is no solution), a green diamond is an intermediary solution and the orange diamond (the last diamond) is the optimal solution.3. Related workDue to the hardness of the clustering problem, there are few exact algorithms in the literature, and the algorithms used are often heuristic, metaheuristic or approximation algorithms. Finding a partition maximizing the split between clusters is a polynomial problem [5] but it becomes NP-Hard with user constraints such as cannot-link constraints [20]. Concerning the minimization of the maximal diameter, the problem is polynomial for k = 2, but it becomes NP-Hard when k ≥ 3 [21]. An exact algorithm based on graph coloring is proposed in [21]. Graph coloring is used to check if a distance between two objects can be the maximal diameter. Another exact approach uses a branch-and-bound search [4]. The algorithm uses a hierarchical algorithm to find a good bound and a reordering point strategy to reduce the search space. For the criterion of minimizing the within-cluster sum of dissimilarities, a repetitive branch-and-bound algorithm is presented in [4]. To our knowledge, there is no exact algorithm that supports user constraints with any of those criteria with k > 2.T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9477For the split-diameter bi-criterion optimization without user constraints, an algorithm finding a complete and minimal set of Pareto optimal solutions, which are partitions with at most kmax clusters, is proposed in [5]. It is proved that for npoints, regardless of the number of clusters k, regardless of the partition, the split value can be found among the edges of the minimum weight spanning tree which is constructed from the matrix of dissimilarities between objects. These values are ordered decreasingly and the split s will take value in this order. On the other hand, the diameter value is one of the dissimilarities between two objects. All the dissimilarities are ordered decreasingly and the diameter d will take value in this order. Each couple (s, d) is considered and in case without conflict will induce a graph. Graph coloring on the induced graph helps to find a partition with minimum number of clusters (this number is the chromatic number of the induced graph). The algorithm finds a complete and minimal set of Pareto optimal solutions. Each solution is a partition with at most kmax clusters.In the case of bi-partition (k = 2), an exact polynomial algorithm to find Pareto optimal solutions is proposed in [12,13]. For k > 2, [13] also offers a 2-approximation algorithm. These two algorithms [12,13] are both based on the principle of [5]: a spanning tree is built to find the possible values for split and graph coloring tests are used to verify if a dissimilarity can be the maximal diameter. However none of these bi-criterion cluster analysis approaches does support any kind of user constraints.Most of the attention in constrained clustering has been put on instance-level constraints, i.e. must-link and cannot-link constraints [22]. They were first introduced by Wagstaff [7]. Subsequently, many works have been done to extend classical algorithms for handling must-link and cannot-link constraints, as for instance an extension of COBWEB [7], of k-means [23,24], hierarchical non-supervised clustering [25] or spectral clustering [26,27], etc. When the constraints are tight, most of those algorithms may not find a solution that satisfies all the constraints even if such a solution exists.In recent years, it has been realized that many problems in Data Mining, including constrained clustering, can be solved by generic optimization tools. Recent works investigate generic frameworks such as Constraint Programming, SAT or Integer Linear Programming.In [28], L. De Raedt et al. present a framework in Constraint Programming for k-pattern set mining and show how it can be applied to conceptual clustering. In conceptual clustering, an intentional definition represented by a pattern is associated to each cluster. The objective is to find pairs composed of clusters and patterns, such that the elements in a cluster satisfy the pattern. Constraints are imposed on patterns and clusters. In order to find interesting solutions, some optimization criteria can be introduced. J.-P. Métivier et al. present in [29] a constraint-based language expressing queries to discover patterns in Data Mining. Conceptual clustering tasks can be expressed by queries as well as some kinds of user constraints. The language elements are translated into SAT clauses which are solved by a SAT solver.Davidson et al. propose a SAT framework [1] for constrained clustering, but only for problems with k = 2. Several kinds of constraints are considered: must-link, cannot-link, diameter and split constraints. The algorithm allows to obtain a global optimum with the criterion of diameter or split.Mueller et al. propose in [2] an approach to constrained clustering based on Integer Linear Programming. This approach takes a set of candidate clusters as input and builds a clustering by selecting a suitable subset. It allows different kinds of constraints on clusters or on the set of clusters, but no constraint on individual objects. It integrates different objective functions based on the quality of the clusters composing the clustering. The framework guarantees to find a global optimum but requires a set of candidate clusters. This condition makes the framework less applicable for clustering in general, since finding a good set of candidate clusters is a difficult task as the number of candidate clusters is exponential compared to the number of objects. This approach is experimented for conceptual clustering where candidate clusters might be generated from frequent patterns.Recently, Babaki et al. present in [30] an exact approach for constrained clustering with the criterion of minimizing the within-cluster sum of squares, based on Integer Linear Programming. This approach extends an exact algorithm which uses column generation [31]. It allows must-link, cannot-link and all constraints that are anti-monotone. User constraints are handled within the branch-and-bound search, used for generating new columns. This approach is experimented in [30] with small datasets containing less than 200 objects. For clustering task which maximizes the inter-cluster distances, Kotthoff et al. present in their talk [32] a Constraint Programming approach and also assert the flexibility and opportunities provided by a CP formulation.Other tasks of clustering are based on a similarity graph between objects. Spectral clustering is a clustering task which aims at minimizing the ratio cut criterion2 [33]. Wang et al. present in [27,34] a flexible framework for spectral clustering. The framework integrates different kinds of constraints and allows also to specify a threshold setting a lower bound on how well the given constraints are satisfied. Zhi et al. present in [35] a framework for spectral clustering which integrates logical combinations of constraints. Logical combinations of constraints are expressed as linear equalities and inequalities so that they can be incorporated into various mathematical programming formulations for clustering.Multi-view spectral clustering is an extension of spectral clustering to multi-view datasets. Instead of combining different views into a single objective function, Davidson et al. propose in [36] a natural formulation that treats the problem as a multi-objective problem and solve it using Pareto optimization.2 Based on a similarity measure s(oi , o j ) between the objects, the ratio cut criterion is defined by (1/2) (cid:3)(cid:3)c∈[1,k](1/|Cc |) oi ∈Cc ,o j /∈Ccs(oi , o j ).78T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94Table 1Comparison between the two models.VariablesPartitionUser constraintsOptimization CriterionFirst modelI = [I1, . . . , Ik], Dom(Ic ) = [1, n]G = [G 1, . . . , Gn], Dom(G i ) = [1, n]Second modelG = [G 1, . . . , Gn], Dom(G i ) = [1, kmax]D (diameter), S (split), V (WCSD)precede(G, [1, . . . , kmax])atleast(1, G, kmin)(cid:7) ∈ [1, k], Ic ≤ Ic(cid:7)∀c ∈ [1, k], element(G, Ic , Ic)∀i ∈ [1, n], exactly(1, I, G i )∀i ∈ [1, n], G i ≤ i∀c < cI1 = 1Minimal size α of clusters: ∀i ∈ [1, n], atleast(α, G, G i )Maximal size β of clusters: ∀c ∈ [1, kmax], atmost(β, G, c)Minimal split δ: S ≥ δ, G i = G j , for all i < j st. d(i, j) < δMaximal diameter γ : D ≤ γ , G i (cid:4)= G j for all i < j st. d(i, j) > γDensity constraint: ∀i ∈ [1, n], atleast(m, Ni(cid:7) , G i )Must-link constraint: G i = G j , D ≥ d(i, j)Cannot-link constraint: G i (cid:4)= G j , S ≤ d(i, j)∀i < j ∈ [1, n], d(i, j) > D → (G i (cid:4)= G j )diameter(G, D, d)WCSD criterion: wcsd(G, V , d)Diameter criterion:∀i < j ∈ [1, n], d(i, j) < S → (G i = G j )split(G, S, d)Split criterion:The clustering tasks we are interested in aim at finding a partition of objects. Another clustering approach is hierarchical clustering, which aims at finding a hierarchy of partitions, that is a sequence of nested partitions. The result is a tree diagram, called a dendrogram. A framework formalizing hierarchical clustering as an Integer Linear Programming problem has recently been proposed by Gilpin et al. [37]. Gilpin et al. also propose in [38] a framework based on SAT for hierarchical constrained clustering with different types of user constraints.Another clustering setting is correlation clustering, which is based on a similarity graph between objects, and which aims at finding a partition that agrees as much as possible with the similarities. Berg et al. present in [39] a MaxSAT framework for constrained correlation clustering. In this framework, hard clauses are used to ensure a well-defined clustering and soft clauses are used to encode the cost function.In this paper, we investigate the use of Constraint Programming for constrained clustering. Constraint Programming has already been shown to be a promising approach for Data Mining through various tasks, such as itemset mining [40–44], skypattern mining [45] or decision tree construction [46].4. New CP model for constrained clusteringWe are given a collection of n points and a dissimilarity measure between pairs of points i, j, denoted by d(i, j). Without loss of generality, let us suppose that points are indexed and named by their index (1 represents the first point). The model aims at finding a partition into k clusters satisfying a set of user constraints and optimizing a given criterion.The model we propose is composed of a set of CP constraints. They are used to model partition requirements, the optimization criterion and different kinds of user constraints. Thus, they can be separated into three groups:• the CP constraints expressing that the result must be a partition,• the CP constraints expressing the user constraints,• the CP constraints expressing the criterion to be optimized. Please note that when no optimization criterion is given, the CP solver searches for all the partitions satisfying all the constraints.In a previous work [3], we have presented a CP model for this task. This model was based on a two-level representation: a set of variables for the assignment of a representative to each cluster and a set of variables for the assignment of a representative to each point. Choosing such a representation requires the number of clusters k to be fixed beforehand, since each representative is modeled by a CP variable. In this paper, we introduce a new CP model, which is based only on a set of variables for the assignment of a number (or index) of cluster to each point. As a result, the number of clusters k can be only bounded by kmin and kmax, where kmin and kmax are given by the user. In the following, we present the two models to ease the comparison. In both models, the CP constraints expressing the user constraints are similar but those that express partition requirements and optimization criteria are different. All the optimization criteria in the new model are expressed by new global constraints with a filtering algorithm. The differences between the two models are significant, since the new model has much less variables and constraints, while being more efficient than the previous model. Table 1 summarizes the differences between the two models.T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94794.1. VariablesIn the first model, for each cluster c ∈ [1, k], the point with the smallest index is considered as the representative point of the cluster.3 An integer variable Ic is introduced, its value is the index of the representative point of cluster c; the domain of Ic is therefore the set of integers [1, n]. Let I be the array [I1, . . . , Ik]. Assigning a point to a cluster becomes assigning the point to the representative of the cluster. Therefore, for each point i ∈ [1, n], an integer variable G i ∈ [1, n] is introduced: G i is the representative point of the cluster which contains the point i.Example 4.1. Assume that we have 7 points o1, . . . , o7 and 2 clusters, the first one composed of o1, o2, o4 and the second one composed of the remaining points. The points are denoted by their index (o1 is denoted by 1, o2 by 2 and so on). Then I1 = 1and I2 = 3 (since 1 is the smallest index among {1, 2, 4} and 3 is the smallest index among {3, 5, 6, 7}), G 1 = G 2 = G 4 = 1(since 1 is the representative of the first cluster) and G 3 = G 5 = G 6 = G 7 = 3 (since 3 is the representative of the second cluster).In the new model, clusters are identified by their index, which varies from 1 to k for a partition into k clusters. To represent the assignment of points to clusters, we use integer variables G 1, . . . , Gn whose domain is the set of integers [1, kmax]. An assignment G i = c means that the point i is put into the cluster number c.The domains of the variables G i in the two models are different, but the meaning of these variables is identical: they represent the assignment of points to clusters. Let G denote the array [G 1, . . . , Gn].To represent each optimization criterion, in both models, a float value variable is introduced. It is named D for the diameter criterion, S for the split criterion and V for the WCSD criterion. Their domains are Dom(D) = [mini, j(d(i, j)), +∞), Dom(S) = (−∞, maxi, j(d(i, j))] and Dom(V ) = [0, +∞).4.2. Partition constraints4.2.1. First modelTo express that the result must be a partition, we put the following constraints:• Each representative belongs to its cluster: for each c ∈ [1, k], we put G Ic= Ic . This constraint is represented by the CP constraint element(G, Ic, Ic). The constraint element( A, B, C ) with A an array of variables and B, C variables, sets the relation A[B] = C .• Each point is assigned to a representative: for each i ∈ [1, n], we need c∈[1,k](G i = Ic). This relation can be expressed by #{c | Ic = G i} = 1, which is represented by a CP constraint exactly(1, I, G i). This constraint sets the relation requiring that the value of G i must appear exactly once in the array I.• The representative of a cluster is the point which has the minimal index in the cluster; in other words, the index i of a (cid:4)point is greater or equal to the index of its representative given by G i : for each i ∈ [1, n], we put G i ≤ i.A set of clusters could be differently represented, depending on the order of clusters. For instance, in Example 4.1, we could have chosen I1 = 3 and I2 = 1, thus leading to another representation of the same set of clusters. To avoid this symmetry, the following constraints are added:• The representatives are sorted in an increasing order: ∀c < c• The representative of the first cluster is the first point: I1 = 1.(cid:7) ∈ [1, k], Ic < Ic(cid:7) .4.2.2. Second modelIn this model, clusters are identified by their number (index), and each variable G i gives the index of the cluster that contains point i. A complete assignment of the variables in G represents a partition. However, a partition can be represented by different complete assignments of G. For instance, given a complete assignment of G, if we make a permutation where all the variables G i that have the value c1 take the value c2 and at the same time, all the variables G j having the value c2take the value c1, we get a new assignment for G, which still represents the same partition in terms of clusters. As a second example, when all the variables G i with value c1 receive a value c3 that is not yet used in an assignment of the other variables of G, this leads to a new assignment representing a symmetric solution. Such a situation appears when building the clusters, a new created cluster can receive any value among the remaining cluster numbers.To break this kind of symmetries, the clusters are numbered such that the number 1 is the index of the first created cluster and a new number c, with c > 1, is used if and only if the number c − 1 has been already used. A straightforward way to express this condition is by using a constraint G 1 = 1 and the constraints G i ≤ max j∈[1,i−1](G j) + 1, for i ∈ [2, n]. However, in order to have better interactions and propagations between these relations, a better way is to sum up these relations into one global constraint with a good filtering algorithm. The constraint precede [47] helps to achieve this:3 It allows to have a single representation of a cluster. It must not be confused with the notion of representative in the medoid approach.80T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94precede(G, [1, . . . , kmax]).This constraint imposes that G 1 = 1 and moreover, if G i = c with 1 < c ≤ kmax, there must exist at least an index j < i such that G j = c − 1.The requirement to have at least kmin clusters means that all the numbers among 1 and kmin must be used in the assignment of the variables G i . When using the constraint precede, one only needs to require that at least one variable G iis equal to kmin. This is expressed by the relation #{i | G i = kmin} ≥ 1, which can be represented by the CP constraint:atleast(1, G, kmin).Since the domain of each variable G i is [1, kmax], there will be at most kmax clusters. If the user needs exactly k clusters, all he/she has to do is to set kmin = kmax = k.4.3. User constraintsAll popular user-defined constraints may be straightforwardly integrated. They are expressed the same way in both models, since they rely on the use of the variables in G, which represent the assignment of points to clusters.• Minimal size α of clusters: this means that each point must be in a cluster with at least α points (including itself). For each i ∈ [1, n], the assigned value of the variable G i must then appear at least α times in the array G, i.e. #{ j | G j =G i} ≥ α. Therefore, for each i ∈ [1, n], we put the constraint: atleast(α, G, G i).This constraint helps also to set a bound on the number of possible clusters. Indeed, the number of clusters cannot exceed (cid:20)n/α(cid:21). In the second model, this can be expressed by G i ≤ (cid:20)n/α(cid:21), for all i ∈ [1, n].• Maximal size β of clusters: each number c ∈ [1, kmax] must appear in the array G at most β times (this is still true for an unused value c ∈ [kmin + 1, kmax], since it appears 0 time), i.e. #{i | G i = c} ≤ β. Therefore, for each c ∈ [1, kmax], we put the constraint: atmost(β, G, c).• Minimal split δ: a δ-constraint requires that the split between two clusters must be at least δ. Therefore for each couple i < j ∈ [1, n] such that d(i, j) < δ, the constraint G i = G j is put. The constraint S ≥ δ is also put.• Maximal diameter γ : a diameter constraint requires that the diameter of each cluster must be at most γ . The constraint D ≤ γ is put and for each couple i < j ∈ [1, n] such that d(i, j) > γ , we put: G i (cid:4)= G j .• Density constraint: a density constraint expresses that each point must have in its neighborhood of radius (cid:7), at least mpoints belonging to the same cluster as itself. So for each i ∈ [1, n], the set of variables corresponding to points in its (cid:7)-neighborhood is computed Ni(cid:7) = {G j | d(i, j) ≤ (cid:7)} and this constraint is put: atleast(m, Ni(cid:7) , G i).• Must-link constraint: a must-link constraint on two points i, j is expressed by: G i = G j and D ≥ d(i, j).• Cannot-link constraint: a cannot-link constraint on i, j is expressed by: G i (cid:4)= G j and S ≤ d(i, j).4.4. Optimization criteriaIn the first model, we have proposed to model the optimization criterion by reified CP constraints.• When minimizing the maximal diameter, since D represents the maximal diameter, any two points at a distance greater than D must be in different clusters:∀i < j ∈ [1, n], (d(i, j) > D) → (G i (cid:4)= G j).Since D is a variable and its value is still unknown, these relations are expressed using reified constraints.4• When maximizing the minimal split between clusters, any two points at a distance less than the minimal split S must be in the same cluster:∀i < j ∈ [1, n], (d(i, j) < S) → (G i = G j).Since S is a variable, these relations are also expressed by reified constraints.• When minimizing the Within-Cluster Sum of Dissimilarities (WCSD):(cid:2)V =(G i = G j)d(i, j).i, j∈[1,n]For this relation, we developed a global constraint wcsd(G, V , d) with a filtering algorithm.4 A reified constraint is a logical constraint of the form A → B or A ↔ B, where A and B are also constraints. The reified constraint A → B means the constraint B must be satisfied if A is satisfied, and ¬ A must be satisfied if ¬B is satisfied.T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9481When minimizing the diameter, in [3] we use heuristics provided by the algorithm FPF [6] to get a lower and an upper bound on the diameter without user constraints, and only an upper bound in the presence of user constraints. Such bounds allow to reduce the number of reified constraints that are put in the model.In the new model, for the diameter and split criteria, instead of using reified constraints, we develop two global con-straints diameter(G, D, d) and split(G, S, d), which exploit the measure d and which operate on the array G and on the variable D or S. The filtering algorithms for the three constraints diameter(G, D, d), split(G, S, d) and wcsd(G, V , d) are presented in Section 5.If the user specifies an optimization criterion, an objective function is put in the model, which is:• minimize D in case of minimizing the maximal diameter,• maximize S in case of maximizing the minimal split,• minimize V in case of minimizing the WCSD.When an optimization criterion is specified, if there exist solutions that satisfy all the constraints, the solver finds a global optimal solution. If the user does not specify any optimization criterion, the solver finds all the solutions satisfying all the constraints, if some exist.4.5. Search strategy(cid:7)Symmetry breaking for partition constraints in the two models is based on the indices of points, such as the constraints in the first model or the constraint precede(G, [1, .., kmax]) in the second model. The way points are Ic < Ic(cid:7) for all c < cindexed is therefore really important. Points are then reordered and reindexed, so that points that are far from the others have a small index. In order to achieve this, we rely on the Furthest Point First algorithm [6]. This algorithm starts by choosing a point, marks it as the first head, links all the points to it and iterates until all points are marked. At each iteration, it chooses the point i that is the furthest to its head, marks it as a new head and links to i all the points that are closer to i than to their head.The search strategy in the first model is based on instantiating the variables in I before the variables in G. This means that cluster representatives are identified before assigning points to clusters. Variables in I are chosen from I1 to Ik. Since the representative is the point with the minimal index in the cluster, values for instantiating each Ic are chosen in an increasing order. The choice of variables and values in G depends on the criterion. For the diameter and split criteria, a variable G i with the smallest remaining domain is chosen. Recall that each value in j ∈ Dom(G i) is the index of a cluster representative. All values in Dom(G i) are examined and the value j which corresponds to the smallest value d(i, j) is chosen and two alternatives are created G i = j and G i (cid:4)= j.In the new model, the search strategy is based on the choice of variables and values in G, and depends also on the criterion. For the diameter and split criteria, at each branching point, a variable G i with the smallest remaining domain is chosen. Recall that each value c ∈ Dom(G i) is the number of a cluster. All values in Dom(G i) are examined and the number of the closest cluster to i is chosen. The distance between a point i and a cluster number c is defined as the maximal distance d(i, j) where G jis already instantiated to c. If the cluster number c is empty (there is no point j such that G j = c), the distance between i and the cluster c is set to zero. This means that the assignment of a point to a new cluster is favored if there are unused cluster numbers. Moreover, the smallest remaining number is chosen. The closest cluster c to the point i is chosen and two alternatives are created G i = c and G i (cid:4)= c.Concerning the WCSD criterion, a mixed strategy is used in both models. In order to have a good upper bound for the variable V , a greedy search is used to quickly find a solution. At this step, the chosen variable G i and value c are those such that the assignment G i = c increases V as little as possible. The first solution found is quite good in general. After finding the first solution, the strategy changes to a “first-fail”, which tends to detect failures quickly. In this strategy, a value sic for each point i and each cluster c is defined as the sum of dissimilarities between i and all points j already assigned to the cluster c. At each branching point, for all points i with G i uninstantiated, the minimal value si = minc∈Dom(G i ) sic is computed. The variable G i with the largest value si is then chosen and the value c = arg min sic is chosen.5. Filtering algorithms for optimization criteriaFor each optimization criterion, we have developed a filtering algorithm for the global constraint, which links the variables G representing a partition to the variable representing the objective function (D, S or V ). This kind of global constraints is also called optimization constraint [18]. When a solution is found, its corresponding objective value is com-puted and a constraint expressing that new solutions must have a better value than this one is added. This constraint sets a new upper bound for D or V , which have to be minimized, or a new lower bound for S, which has to be maximized. By reasoning globally on the objective variable and on the variables representing a partition, more interactions between the domains of these variables can be captured and the search subspaces can be pruned before instantiating all the vari-ables.82T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94Algorithm 1: Filtering for constraint diameter(G, D, d).1 stack ← ∅;2 if D.ub has been changed then34for i ← 1 to n where G i is instantiated dostack ← stack ∪ {i}5 else67foreach i that G i has just been instantiated dostack ← stack ∪ {i}8 foreach i ∈ stack dofor j ← 1 to n do91011if d(i, j) ≥ D.ub then delete val(G i) from Dom(G j )if G j is instantiated ∧ val(G i ) = val(G j ) then D.lb ← max(D.lb, d(i, j))5.1. Diameter and split criteriaTo represent the relations between points and the diameter or the split, we develop filtering algorithms for two global constraints diameter(G, D, d) and split(G, S, d), which exploit the dissimilarity measure d between any two points and which operate on the array G and the variable D or S. The constraint diameter(G, D, d) ensures that D is the maximal diameter of the clusters formed by the variables G 1, . . . , Gn. This constraint ensures:∀i < j ∈ [1, n], D < d(i, j) → G i (cid:4)= G j.(1)This kind of relation can be realized by reified constraints, which were indeed used in our previous model [3]. However, a reified constraint is needed for each couple i < j, which implies that the number of reified constraints would be quadratic with respect to the number of points. By developing the constraint diameter(G, D, d), we maintain all these relations in one constraint. The filtering algorithm is presented in Algorithm 1. In this algorithm, Dom(D) is represented by [D.lb, D.ub), where D.lb is the lower bound, which initially can be the minimal dissimilarity between two points, and D.ub is the upper bound, which can be either +∞ or the value of D in the previous solution found. The bound D.ub is strict since in a branch-and-bound search the next solution must have D value strictly smaller than the previous one. The relation (1) is useful when the following cases happen.• The upper bound D.ub has been changed (e.g. by a new found solution or by a diameter constraint). In this case, for each couple i, j, if D.ub ≤ d(i, j), we can conclude D < d(i, j) and by (1) we can infer G i (cid:4)= G j . However, the relation G i (cid:4)= G jis useful to filter the domain of G i (or the domain of G j ) only if the variable G j (or G i , resp.) has been instantiated. Therefore, Algorithm 1 uses a stack to remember the variables G i that are instantiated (lines 1–4) and exploits them to filter (line 10). The lower bound D.lb can possibly be revised (line 11).• Some variables G i have been instantiated. In this case, for each couple i, j such that G i and G j are instantiated and have the same value, we infer D ≥ d(i, j) and can revise D.lb. The stack remembers then the variables G i which have just been instantiated (lines 6–7). This can lead to the revision of the lower bound D.lb (line 11).Let us notice that as soon as the domain of one variable becomes empty, a failure case is detected by the solver. The worst case complexity is O (n2). This algorithm is awaken when the upper bound of D is modified or a variable G i is instantiated. However, because of its complexity, it is scheduled to be effective after other constraints whose propagators are of lower complexity, as for instance constraints representing must-link or cannot-link constraints.The constraint split(G, S, d), on the other hand, maintains that S is the minimal split between the clusters formed by the variables G 1, . . . , Gn. It ensures that:∀i < j ∈ [1, n],S > d(i, j) → G i = G j.(2)The filtering algorithm is presented in Algorithm 2, where Dom(S) = (S.lb, S.ub]. The lower bound S.lb is either −∞ or the value of S in the previous solution found, since S is to be maximized. In the same manner as for the constraint diameter(G, D, d), this algorithm is invoked if the lower bound S.lb has been changed or some variables in G have been instantiated. In this algorithm, if S.lb has been changed, for each couple i, j, if S.lb ≥ d(i, j), by (2) we can infer G i = G j , which is propagated by enforcing Dom(G i) = Dom(G j). Otherwise, if some variables in G have been instantiated, if G i (cid:4)= G jby (2) we infer S ≤ d(i, j), so the upper bound of S can be changed. The worst case complexity is O (n2).5.2. Within-cluster sum of dissimilarities criterionWe have developed a filtering algorithm for a new global optimization constraint wcsd(G, V , d), which links the vari-able V , the array of variables G and which exploits the dissimilarity measure d. This constraint ensures the relation:T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9483Algorithm 2: Filtering for constraint split(G, S, d).1 stack ← ∅2 if S.lb has been changed then stack ← {1, . . . , n}3 else45foreach i that Dom(G i ) has just been changed dostack ← stack ∪ {i}6 foreach i ∈ stack dofor j ← 1 to n do78910if d(i, j) ≤ S.lb thenDom(G i ) ← Dom(G i ) ∩ Dom(G j )Dom(G j ) ← Dom(G i )11if G i and G j are instantiated ∧ val(G i ) (cid:4)= val(G j ) then S.ub ← min(S.ub, d(i, j))(cid:2)V =1≤i< j≤n(G i = G j)d(i, j).(3)where G i = G j is 1 if G i and G j have the same value and 0 otherwise. The filtering algorithm used in the first model as well as motivations, proofs and experiments are presented in [48]. However, in [48] the algorithm is designed for the clustering task with exactly k clusters, as the case considered in the first model. We present below a revision of the algorithm for clustering tasks where the number of clusters is only bounded, as considered in the second model.Let us assume that we have a partial assignment of variables in G. Let K = {i ∈ [1, n] | G i is assigned} and U = {i ∈ [1, n] |G i is unassigned}. We use the computation of a lower bound proposed in [49], which takes into account the unassigned variables. The sum defining V can be split into three parts V = V 1 + V 2 + V 3, where:• V 1 is the sum of dissimilarities between the assigned points:V 1 =(G i = G j)d(i, j)• V 2 is the sum of dissimilarities between the unassigned points and the assigned points:V 2 =(G i = G j)d(i, j)• V 3 is the sum of dissimilarities between the unassigned points:V 3 =(G i = G j)d(i, j)(cid:2)i, j∈K ,i< j(cid:2)i∈U , j∈K(cid:2)i, j∈U ,i< jSince the set K is already known, the exact value of V 1 can be computed. Since the points of U have not been assigned to a cluster, the value of V 2 is unknown. However, a lower bound of V 2, denoted by V 2.lb, can be computed by the sum of the minimal contribution of all unassigned points. For each unassigned point i ∈ U , each value c ∈ Dom(G i) represents an index of cluster to which point i can be assigned to. If point i is assigned to the cluster number c, it will contribute to that cluster the sum of dissimilarities between point i and all the assigned points which are in cluster c, i.e. the sum of dissimilarities d(i, j) for all j ∈ K such that G j = c. The minimal contribution v 2i of the point i is the minimal added amount when considering all values in Dom(G i), with respect to the assigned points:(cid:2)v 2i = minc∈Dom(G i )(d(i, j)).j∈K ,G j =cA lower bound V 2.lb of V 2 can be computed by the sum of v 2i , for all i ∈ U :V 2.lb =(cid:2)v 2i.i∈UThe exact value of V 3 is unknown too and we use a heuristic to compute a lower bound of V 3. We recall that V 3 is the sum of all d(i, j) such that i, j ∈ U and i and j are in the same cluster. Let p be the cardinality of U and let k be the cardinality of the union ∪i∈U Dom(G i). Each value of ∪i∈U Dom(G i) is the index of a possible cluster to which the points in U can be assigned. The number k is then the maximal number of clusters to which the points in U can be assigned. We can see that the minimal number of terms d(i, j) in the sum V 3 is the minimal number of within-cluster pairwise connections, while considering all partitions of p points into at most k clusters.Let m be the quotient of the division of p by k and m(cid:7) − km)/2. It is proved in [48] that the total number of within-cluster pairwise connections for all clusters is greater or equal to f (p, k). The equality the remainder. Let f (p, k) = (km2 + 2mm(cid:7)84T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94Fig. 6. Example of V .lb = V 1 + V 2.lb + V 3.lb.(cid:7)is reached when mclusters have m elements. Therefore, for the set U of unassigned points, if we order increasingly the constants d(i, j) for all i < j ∈ U , a lower bound V 3, denoted by V 3.lb, is then computed by the sum of the f (p, k) first constants in this order.clusters have m + 1 elements and k −m(cid:7)Example 5.1. Let us consider the case given in Fig. 6 with 14 points, which have to be grouped into 2 clusters. Assume that 7 points are grouped and 7 are not. The exact value of V 1 is computed by the sum of solid black lines. The lower bound V 2.lb is the sum of dash lines for each unassigned points. With 7 unassigned points, we have p = 7, k = 2, m = 3(cid:7) = 1, the minimal number of connections is f (7, 2) = 9. The lower bound V 3.lb the sum of dotted lines. Theses lines and mare the 9 smallest lines that connect two unassigned points. The lower bound V 3.lb is heuristic since these lines do not correspond to any case where the 7 unassigned points are grouped into 2 groups.Assume that the domain of variable V is [V .lb, V .ub) where V .lb is the lower bound, which can initially be 0, and V .ubis the upper bound, which can be either +∞ or the value of V in the previous solution found. The upper bound V .ub is strict since in a branch-and-bound search the next solution must be better than the previous solution found. Given a partial assignment of variables in G, a new lower bound of variable V is computed by:V .lb = max(V .lb, V 1 + V 2.lb + V 3.lb).We use this lower bound in the filtering algorithm for wcsd(G, V , d). The algorithm is presented in Algorithm 3. The lower bound V .lb is used for two purposes:• Detecting failure during the branch-and-bound search. A failure happens when V .lb ≥ V .ub, which means that the domain of V becomes empty.• Filtering inconsistent values of unassigned variables. For each unassigned variable G i , for each value c ∈ Dom(G i), under the assumption that G i = c, we propose to revise the lower bound in a constant time. If the revised value is greater or equal than the upper bound V .ub then c is inconsistent and is removed from Dom(G i).Since the constants d(i, j) (i, j ∈ U , i < j) must be ordered increasingly for the computation of V 3.lb, they are ordered once in the array ord, and at the same time the arrays px and py are constructed. For each value pos, ord[pos] gives the value d(i, j) in this order at position pos, and px[pos] (or py[pos]) gives the index i (or j, respectively) of the constant. The arrays ord, px and py are given in input of Algorithm 3. This algorithm computes arrays add and m, where add[i, c] is the added amount if i is assigned to cluster number c (add[i, c] =j∈K ,G j =c d(i, j)) and m[i] is the minimal added amount while considering all possible assignments for i (m[i] = minc∈Dom(G i ) add[i, c]).Lines 1 to 26 computes the lower bound for V , based on a partial assignment of variables in G. Lines 27 to 29 filter the domain of the uninstantiated variables G i as follows. For each uninstantiated variable G i , for each value c ∈ Dom(G i), in (cid:7)3.lb, where:case of assignment of i into cluster number c, a new lower bound for V .lb is revised into V(cid:7)2.lb + V(cid:7).lb = V+ V(cid:3)(cid:7)1(cid:7)1(cid:7)2• V= V 1 + add[i, c] because point i is supposed to be assigned to cluster c, the sum of dissimilarities between instanti-ated points is increased by add[i, c].• V= V 2.lb − m[i] because point i is no more unassigned, the contribution of point i in the computation of V 2.lb must • Vbe removed.(cid:7)3.lb is the sum of the first f (|U \ {i}|, k) elements that are related to U \ {i} in the increasingly ordered array ord. (cid:7)3.lb. Here, V 4 is the sum of the first In order to revise this bound in a constant time, we actually use V 4 instead of Vf (|U | − 1, k) = f (p − 1, k) elements of ord. These elements are related to U , it this therefore possible that some of them (cid:7)3.lb ≥ V 4. The value V 4 can be computed once and independently from i and care related to i. It is evidence that V(line 24).The revised lower bound, in case of assignment of point i to cluster c is:T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9485Algorithm 3: Filtering algorithm for wcsd(G, V , d).input : U : the set of unassigned variables in Gord: array in which all the dissimilarities d(i, j) (i, j ∈ U , i < j) are ordered increasinglypx, py: arrays giving the index i, j wrt. ordoutput: compute a new lower bound of V and filter unassigned variables in G1 V 1 ← 0; V 2.lb ← 0; V 3.lb ← 0; V 4 ← 0;2 for i ← 1 to n where G i is unassigned do345if c ∈ Dom(G i ) then add[i, c] ← 0else add[i, c] ← +∞for c ← 1 to kmax do6 for i ← 1 to n where G i is assigned do78910c ← val(G i )for j ← 1 to n doif G j is assigned and val(G j ) = c and i < j then V 1 ← V 1 + d(i, j)if G j is unassigned and c ∈ Dom(G j ) then add[ j, c] ← add[ j, c] + d(i, j)11 for i ← 1 to n where G i is unassigned do121314m[i] ← +∞foreach value c ∈ Dom(G i ) doif m[i] > add[i, c] then m[i] ← add[i, c]V 2.lb ← V 2.lb + m[i]1516 p ← card(U )17 k ← card(∪i∈U Dom(G i ))18 cpt ← 0; pos ← 1;19 while cpt < f (p, k) do2021222324i ← px[pos]; j ← py[pos];if G i is unassigned and G j is unassigned thencpt ← cpt + 1V 3.lb ← V 3.lb + ord[pos]if cpt ≤ f (p − 1, k) then V 4 ← V 4 + ord[pos]pos ← pos + 12526 V .lb ← max(V .lb, V 1 + V 2.lb + V 3.lb)27 for i ← 1 to n where G i is unassigned doforeach value c ∈ Dom(G i ) do2829if V .lb + add[i, c] − m[i] − V 3.lb + V 4 ≥ V .ub then delete c from Dom(G i )(V 1 + add[i, c]) + (V 2.lb − m[i]) + V(cid:7)3.lbwhich is greater than or equal to:V .lb + add[i, c] − m[i] − V 3.lb + V 4So if this last value is greater than or equal to the actual upper bound of V , point i cannot be assigned to cluster c. The value c is therefore inconsistent and is removed from Dom(G i). The complexity of this algorithm is O (n2 + nk) = O (n2 + nkmax), since the domain of each G i is of size at most kmax. Since kmax ≤ n, the complexity is then O (n2).6. Bicriterion split-diameter constrained clusteringOur CP model represents a general and declarative framework for constrained clustering, where a user can choose one among different optimization criteria and can integrate different kinds of user constraints. This flexibility offers different ways of using our framework. We show in this section how it can be applied to handle bi-criterion constrained clustering tasks.Let us consider a constrained clustering task with a set C of user constraints, which is possibly empty. We aim at com-puting the Pareto front for this constrained clustering task with the bi-criterion (min D, max S). One approach to achieve this is described in Algorithm 4; it is comparable to the (cid:7)-constraint approach presented in [50]. In this algorithm, optimiza-tion steps with a single criterion are iterated, each time with a condition on the value of the other criterion. The function Maximize_Split(C) or Minimize_Diameter(C) means the use of our model with the optimization criterion of maximizing the split or minimizing the diameter, respectively, and with the set of constraints C. It returns an optimal solution which satis-fies all the constraints in C, if there exists one, or NULL otherwise. We prove that this algorithm computes a complete and minimal set of Pareto optimal solutions.Proposition 6.1. Let (cid:2)D1 , (cid:2)S1, . . . , (cid:2)Dm, (cid:2)Sm be the partitions visited by Algorithm 4. We have:1. there is no partition (cid:2) satisfying C such that D((cid:2)) < D((cid:2)D1 ),86T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94Algorithm 4: Algorithm computing a complete and minimal set P = {(cid:2)S1 , . . . , (cid:2)Sm} of Pareto optimal solutions.input : C: a set of user constraintsoutput: P : a complete and minimal set of Pareto optimal solutions1 P ← ∅2 i ← 13 (cid:2)Di4 while (cid:2)Di5(cid:2)SiP ← P ∪ {(cid:2)Sii ← i + 1(cid:2)Di}6789 Return P← Minimize_Diameter(C)(cid:4)= NULL do← Maximize_Split(C ∪ {D ≤ D((cid:2)Di )})← Minimize_Diameter(C ∪ {S > S((cid:2)Si−1)})(cid:4)= NULL then (cid:2)S(cid:4)= NULL,2. if (cid:2)Dii3. for all 2 ≤ i ≤ m, S((cid:2)Si ) > S((cid:2)Si−1),4. for all 1 ≤ i ≤ m, D((cid:2)Si ) = D((cid:2)Di ),5. for all 2 ≤ i ≤ m, D((cid:2)Di ) > D((cid:2)Di−1),6. for all 1 ≤ i < m, there is no partition (cid:2) satisfying C such that S((cid:2)) ≥ S((cid:2)S7. for all 1 ≤ i ≤ m, there is no partition (cid:2) satisfying C such that S((cid:2)) > S((cid:2)Si ) and D((cid:2)) < D((cid:2)Di+1),i ) and D((cid:2)) = D((cid:2)Si ).Proof.1. Since (cid:2)D1 is a partition that minimizes the diameter among all the partitions satisfying the user constraints C (line 3), there exists no partition (cid:2) satisfying C with D((cid:2)) < D((cid:2)D1 ).2. If (cid:2)Di(cid:4)= NULL, since (cid:2)Di )}is not empty. Moreover, this set is finite since the set of all partitions is finite. There exists at least one partition satisfying these constraints and maximizing the split.i satisfies the set C and the condition D ≤ D((cid:2)Di ), the set of partitions satisfying C ∪{D ≤ D((cid:2)Dis among the partitions satisfying C ∪ {S > S((cid:2)Si−1)} (line 8), we have S((cid:2)Di )} and since among all the partitions satisfying C ∪ {D ≤ D((cid:2)Di )}, (cid:2)Sii ) > S((cid:2)Si−1). Since (cid:2)Dsatisfies is the one that maximizes the split i3. Since (cid:2)DiC ∪ {D ≤ D((cid:2)D(line 5), we have S((cid:2)Si ) ≥ S((cid:2)Di ). Therefore S((cid:2)Si ) > S((cid:2)Si−1).1 ) is the minimal diameter of the partitions satisfying the user-constraints C, 4. To prove this we distinguish two cases.1 satisfies C and D((cid:2)D1) ≥ D((cid:2)D1 ). (cid:2)SCase i = 1: (cid:2)Stherefore D((cid:2)SCase i ≥ 2: Since (cid:2)Sithe precedent item, S((cid:2)Sdiameter among all the ones satisfying C ∪ {S > S((cid:2)S5. For i = 2, the set of partitions satisfying C ∪ {S > S((cid:2)Sis a partition satisfying the set C ∪ {D ≤ D((cid:2)Di satisfies C ∪ {S > S((cid:2)Si−1), so (cid:2)Si ) > S((cid:2)S1 belongs to the partitions satisfying D ≤ D((cid:2)Di−1)} (line 8), we have D((cid:2)S1 ) = D((cid:2)D1 ).i ) ≤ D((cid:2)D1 ). So D((cid:2)Si )} (line 5), we have D((cid:2)Si−1)}. Since (cid:2)Di ). As proven in is a partition which minimizes the i ) = D((cid:2)Di ).1)} is a subset of the set of partitions satisfying C. Therefore i−1)} is a subset of the set of partitions satisfying i−1 are the partitions which minimize the diameter among i ). Therefore D((cid:2)Si ) ≥ D((cid:2)Di1 ). For i ≥ 3, the set of partitions satisfying C ∪ {S > S((cid:2)Si−2)}, since S((cid:2)S2 ) ≥ D((cid:2)DD((cid:2)DC ∪ {S > S((cid:2)Sall the ones in these two respective sets, we have D((cid:2)DIn all cases (2 ≤ i ≤ m), we have D((cid:2)Dconstraints entail the same set of partitions and therefore the same maximal split) which contradicts S((cid:2)STherefore D((cid:2)Di−1).i ) = D((cid:2)Di−1), then S((cid:2)Si−2). Since (cid:2)Di−1). If D((cid:2)Di ) ≥ D((cid:2)Di−1) > S((cid:2)Si ) ≥ D((cid:2)Di ) = S((cid:2)Si and (cid:2)Di ) > D((cid:2)Di−1).6. Assume that there exists a partition (cid:2) that satisfies C and such that S((cid:2)) ≥ S((cid:2)Si ) and D((cid:2)) < D((cid:2)DS((cid:2)Si ) and (cid:2)Dhave D((cid:2)) ≥ D((cid:2)Di+1 is a partition which minimizes the diameter among all those satisfying the condition S > S((cid:2)Si+1). This contradicts the fact that D((cid:2)) < D((cid:2)Di+1).7. Assume that there exists a partition (cid:2) satisfying C such that S((cid:2)) > S((cid:2)Si ), so (cid:2) satisfies C ∪ {D ≤ D((cid:2)Di )}. By line 5, (cid:2)SiD((cid:2)DC ∪ {D ≤ D((cid:2)Di )}, so S((cid:2)) ≤ S((cid:2)Si ). This contradicts the fact that S((cid:2)) > S((cid:2)Si ). (cid:2)i ). By point 4, D((cid:2)) =i ) and D((cid:2)) = D((cid:2)Sis a partition which maximizes the split among those satisfying i−1) (line 5: the same i−1). i ) > S((cid:2)Si+1). Since S((cid:2)) ≥i ), we Fig. 7 illustrates the positions of the solutions found by Algorithm 4, according to Proposition 6.1. The left image presents 1 ). By point 7, there is 2 is 1 ). By point 6, there is no 2 ) of the the first two steps. By point 1, there is no partition in the dark zone, except on the line D = D((cid:2)D1 ) of the line D = D((cid:2)Dno partition in the segment S > S((cid:2)Sabove the dotted line (partitions in the white zone below the dotted line are dominated by (cid:2)Spartition in the grey zone, except on the line D = D((cid:2)D2 ). By point 7, there is no partition in the segment S > S((cid:2)S1 ). By point 5 and line 8 of Algorithm 4, the partition (cid:2)DT.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9487Fig. 7. The solutions found by Algorithm 4: (left) by the first two steps, (right) by the whole algorithm.line D = D((cid:2)Dzone. A solution in the white zone is dominated by a solution (cid:2)Si2 ). The right image presents the positions found by the whole algorithm. There exists no partition in the grey ∈ P .Proposition 6.2. The set P = {(cid:2)S1 , . . . , (cid:2)Sm} computed by Algorithm 4 is complete and minimal i.e.:i (1 ≤ i ≤ m) is a Pareto optimal solution,1. (cid:2)S2. for all Pareto optimal solutions (cid:2) satisfying C, there exists i ∈ [1, m] such that D((cid:2)) = D((cid:2)Si ) and S((cid:2)) = S((cid:2)Si ).The set {(D((cid:2)), S((cid:2))) | (cid:2) ∈ P} is therefore the Pareto front.Proof. Algorithm 4 terminates since according to Proposition 6.1, S((cid:2)Sby the maximal dissimilarity of pairs of points.i ) > S((cid:2)Si−1) and these values are discrete and limited i ), or D((cid:2)) < D((cid:2)Si ) and S((cid:2)) > S((cid:2)Si ) and S((cid:2)) > S((cid:2)S1. We prove that for all i ∈ [1, m], there exists no partition (cid:2) satisfying C which dominates (cid:2)SD((cid:2)SD((cid:2)) ≤ D((cid:2)Di ), or D((cid:2)) < D((cid:2)Dpartition which maximizes the split among all those satisfying the condition D ≤ D((cid:2)Dthen (cid:2) does not exist according to point 1 of Proposition 6.1. If i > 1, since S((cid:2)Ssatisfy D((cid:2)) < D((cid:2)Dpartition (cid:2)Sii , i.e. such that D((cid:2)) ≤i ), the partition (cid:2) must satisfy is a i ). For the second case, if i = 1i−1), the partition (cid:2) must i−1). This is impossible according to point 6 of Proposition 6.1. Therefore each i ). The first case is impossible since (cid:2)Si ) and S((cid:2)) ≥ S((cid:2)Si ) and S((cid:2)) ≥ S((cid:2)Si ) and S((cid:2)) > S((cid:2)Si ). Since D((cid:2)Sis Pareto optimal.i ) = D((cid:2)Di ) > S((cid:2)Si1 ) or there exists i ∈ [1, m − 1] such that S((cid:2)S2. Let (cid:2) be any Pareto optimal solution, i.e. (cid:2) is not dominated, satisfying C.m), since (cid:2)Dm). Therefore S((cid:2)) ≤ S((cid:2)SWe cannot have S((cid:2)) > S((cid:2)Ssatisfying C and S((cid:2)) > S((cid:2)SS((cid:2)SConsidering the case where S((cid:2)) ≤ S((cid:2)SD((cid:2)Sdominated, we must have D((cid:2)) = D((cid:2)SIn the other case, where there exists i ∈ [1, m − 1] such that S((cid:2)Swe have D((cid:2)) ≥ D((cid:2)Dtherefore either (D((cid:2)), S((cid:2))) = (D((cid:2)SD((cid:2)) = D((cid:2)Si+1) and by point 4, D((cid:2)) ≥ D((cid:2)Si+1)), or (cid:2)S1) ≤ D((cid:2)). We have either (D((cid:2)), S((cid:2))) = (D((cid:2)Sm). Since {S((cid:2)Si+1).1 ) and S((cid:2)) = S((cid:2)S1).i ) < S((cid:2)) ≤ S((cid:2)S1 ), therefore D((cid:2)Si+1), S((cid:2)Si+1) and S((cid:2)) = S((cid:2)Si+1). (cid:2)m+1 is null (Algorithm 4 terminates) and therefore there exists no partitions i )} is strictly increasing, therefore either S((cid:2)) ≤1 ). By point 1 of Proposition 6.1, D((cid:2)D1 ) ≤ D((cid:2)) and by point 4, D((cid:2)D1 ) =1 dominates (cid:2). Since (cid:2) is not 1 )), or (cid:2)S1 ), S((cid:2)Si ) < S((cid:2)) ≤ S((cid:2)Si+1). By point 6 of Proposition 6.1, i+1) ≤ D((cid:2)), i+1) ≥ S((cid:2)) and D((cid:2)Si+1). We then have S((cid:2)Si dominates (cid:2). Since (cid:2) is not dominated, we must have Minimize_Diameter (resp. Maximize_Split) searches for a partition minimizing the diameter (resp. maximizing the split) among the partitions satisfying the set of constraints given as argument. Let us recall that there may exist several partitions optimizing a criterion but our model returns the first one found. Nevertheless it is later possible to apply our model with no optimization criterion but with the constraint that the diameter of the partitions must be this optimum and the algorithm will enumerate all the partitions satisfying this constraint. In this way, given an element (D i , S i) in the Pareto front, our model without optimization criterion, but with the constraints C ∪ {D = D i, S = S i}, will enumerate all the partitions (cid:2) that satisfy C and such that D((cid:2)) = D i and S((cid:2)) = S i .Multi-objective optimization in Constraint Programming in only one phase of search is proposed in [51]. The idea is to implement a global constraint Pareto(Obj1, . . . , Objm, A), which keeps a set of non-dominated solutions so far computed A and which operates on the variables representing the objective functions Obji . This constraint reduces the domain of a variable Obji if the domains of the other variables enter into the dominated zone of a solution in A. A detailed description of this constraint as well as an extension with Large Neighborhood Search is proposed in [52]. This constraint Pareto can be introduced in our model. Nevertheless this approach for the moment is still much less efficient than Algorithm 4 and therefore deeper studies, as for instance a study on the search strategy, are needed in order to improve the efficiency.88T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94Table 2Properties of datasets.Dataset# Objects# ClassesIrisWineGlassIonosphereUser KnowledgeBreast CancerSynthetic ControlVehicleYeastMultiple FeaturesImage SegmentationWaveform150178214351403569600846148420002000500033724264101073Table 3Performance (measured in seconds) with the criterion of minimizing the maximal diame-ter.DatasetDoptBaBGCCP1CP21.82.342.00.63.71.82.58458.134.978.61.172377.96109.36264.830.67Iris1.4Wine2.0Glass8.1IonoSphere–User Knowledge–Breast Cancer–Synthetic Control–Vehicle–Yeast–Multi Features–Image Segmentation–Waveform–a There was an error in [3] giving 8.6 s instead of 0.4 s for solving the dataset Iono-Sphere.< 0.10.30.90.4a75.00.756.114.32389.9∗< 0.1< 0.10.20.30.20.51.60.95.210.45.750.112 505.5436.415.6––––––589.2∗7. ExperimentsOur model is implemented in Gecode solver version 4.2.1.5 Gecode [19] is an open source Constraint Programming library in C++ and is one of the current state-of-the-art CP solvers. Twelve databases taken from the repository UCI [53] are used in our experiments. They vary on their size and their number of clusters. Table 2 summarizes information on these datasets, which are presented in increasing order of the number of objects. Since the problem we address is finding an exact solution for distance-based clustering, the important factors are the number of objects n and the number of clusters k. For the experiments, we have chosen a wide range of datasets with different values of n and k. The experiments are performed on a 3.4 GHz Intel Core i5 processor with 8G Ram running Ubuntu. All our programs are available at http://cp4clustering.com.7.1. Constrained clustering with a single criterion7.1.1. Minimizing the maximal diameter of clustersPerformance test We compare the performance of our previous model (denoted by CP1) and our new model (denoted by CP2), both relying on Gecode solver, with the branch-and-bound approach [4] (denoted by BaB) and the algorithm based on graph coloring [5] (denoted by GC). The program BaB has been obtained from the author’s website.6 Since no implementation of the algorithm GC was available, we coded it ourselves in C++ using a well-known available graph coloring program [54]. We consider clustering without user constraints since the other algorithms cannot handle them and to our knowledge, there is no exact algorithm handling user constraints with this criterion. In the experiments, the timeout is set to 1 hour and the Euclidean distance is used to compute the dissimilarity between objects. The value of k is set to the ground truth number of clusters given in Table 2 (in the new model kmin = kmax = k).Table 3 shows the results of experiments. For each dataset we present the value Dopt (the optimal diameter) in the second column and the run-time in seconds of each system. The symbol − is used when the system cannot complete the search after 1 hour and the symbol ∗ is used to mark that the computer runs out of memory and cannot finish the search. All the algorithms are exact and they find the same value for the optimum diameter.5 http :/ /www.gecode .org.6 http://mailer.fsu.edu/~mbrusco/.T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9489Fig. 8. Comparison of #nodes in the search tree with different search strategies.Table 4Performance (measured in seconds) of CP2 with two modelings of diameter criterion.Reified constraintsDedicated filtering algorithmIrisWineGlassIonoSphereUser KnowledgeBreast CancerSynthetic ControlVehicleYeastMulti FeaturesImage SegmentationWaveform< 0.1< 0.10.40.315.40.723.611.9574.2∗226.7∗< 0.1< 0.10.20.30.20.51.60.95.210.45.750.1It is clear that with these datasets, our new model (CP2) is the most efficient in all cases. All twelve datasets can be solved by CP2 within one minute. Among the programs, BaB algorithm is the least efficient. It is not able to solve datasets with more than 300 objects. The performance of GC is better than that of BaB but it decreases rapidly when the number n of objects is over 500. BaB algorithm is based on the bounds of the maximal diameter to detect failures during search, while GC algorithm considers all available distances in decreasing order to find the optimum diameter. Our models, which exploits the benefits of Constraint Programming such as constraint propagation and appropriate search strategies, are more efficient.Analysis of search strategy Although our two models are based on the Constraint Programming framework, there are two main reasons that explain the significant difference in performance of the two models: the search strategy and the dedicated filtering algorithm. For analyzing the influence of the search strategy, we use our new model (CP2) with different search strategies: the strategy used in the previous model (CP1) and the one used in the new model (CP2). Fig. 8 presents the number of nodes in the search trees with these two strategies for the last six datasets given in Table 2. It is clear that the new strategy is better, since the search trees are always smaller. The new search strategy always finds quickly the first solution, the maximal diameter of which is close to the optimal diameter. As a result, the solver can remove more unfruitful branches and there are less number of nodes in the search tree.Analysis of the dedicated filtering algorithm In CP1, for modeling the diameter criterion, the number of reified constraints is at most the square of the number of points n. Although our dedicated global constraint has a complexity in the worst case of O (n2), it considers only necessary variables whereas in CP1, at each node, every reified constraint is checked at least one time. In order to study the efficiency of the filtering algorithm, we test our new model (CP2) but using reified constraints (as in CP1) to express the diameter criterion. Table 4 presents the performances obtained when using reified constraints and when using the dedicated filtering algorithm. We can see that when using reified constraints, the solver cannot find optimal solution with the datasets Wave Form and Multi Features. The reason is that there are too many reified constraints and the computer runs out of memory. Table 4 shows that the filtering algorithm boosts the performance and this becomes more and more significant with larger datasets.Analysis of bounds on the number of clusters of the new model We evaluate the influence of the bounds on the number of clusters (k ∈ [kmin, kmax]) on the performance of CP2. In the first experiment, kmax is set to 10 and kmin varies from 2 to 10. The diameter criterion favors a high number of clusters, since the higher the number of clusters, the smaller the value of the optimal diameter. Without user-constraints, the optimal solution with the diameter criterion has always a number of clusters equal to kmax. For all the datasets, the total time in the search tree is constant when kmin changes. It shows that the 90T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94Fig. 9. Analysis of bounds: kmin = 2, kmax = 2, 3, . . . , 10.Table 5Maximizing the minimal split between clusters with a diameter constraint.DatasetIrisWineGlassIonosphereUser Knowledge ModelingBreast CancerSynthetic ControlVehicleYeastMulti FeaturesImage SegmentationWaveformSopt0.5353.331.785.290.32421.9945.1627.060.151107.07228.70–ksolkrealTotal time3372425410107–3372426410107–< 0.1< 0.11.72.411.50.717.429.5639.8267.951.3–propagation of the constraint modeling the diameter criterion is effective. After finding and proving the optimal solution with kmax clusters, the solver can conclude that it does not exist a better solution with less than kmax clusters.In the second experiment, kmin is set to 2 and kmax varies from 2 to 10. Fig. 9 presents the results obtained with the datasets Vehicle, Yeast, Multi Features and Image Segmentation. For each dataset, we report the number of nodes in the search tree, when the bound kmax varies. In general, when kmax increases, more partitions have to be considered. However, we see an interesting trend in Fig. 9: the number of nodes in the search tree does not always increase as kmax increases. Indeed, since kmax is higher, the optimal maximum diameter is smaller, and the propagation of the diameter constraint is more effective. It explains why in some cases, the computation time decreases when kmax increases.7.1.2. Maximizing the minimal split between clustersFinding a partition maximizing the split between clusters is a polynomial problem. However, with user constraints the problem becomes NP-Hard. To our knowledge, there is no exact algorithm for this criterion that supports any kind of user constraints for a general value k ≥ 3. When optimizing the split without user constraints, when the number k of clusters is not set (kmin ≤ k ≤ kmax), the optimal solution has always a number of clusters equal to kmin. However, this is no longer true with user constraints, as for instance with a diameter constraint. We have experimented this point with the new model by adding a diameter constraint. In order to set it, we have used the results given in Table 3 and set an upper bound on the diameter of each cluster to 1.5Dopt. The number of clusters k is not set, it is bounded between kmin = 2 and kmax equal to the actual number of clusters given in Table 2. The results are given in Table 5. For each dataset, we present the optimal split Sopt, the number of clusters of the solution ksol, the actual number of clusters kreal and the total execution time in seconds. Our model is able to solve almost all datasets, with the exception for the dataset Waveform, which is the largest dataset.7.1.3. Minimizing the within-cluster sum of dissimilarities (WCSD)Finding an exact solution for minimizing WCSD is difficult and we compare the performance of our new model in Gecode solver with the Repetitive Branch-and-Bound Algorithm (RBBA) [4]. The program RBBA has been obtained from the author’s website (http://mailer.fsu.edu/~mbrusco/). To our knowledge, it is the best exact algorithm for the WCSD criterion. The dissimilarity between objects is measured by the squared Euclidean distance. Without user constraints, both our model and the RBBA approach can only find the optimal solution with the Iris dataset. Our model needs 4125 s to complete the search whereas RBBA takes 3249 s. RBBA solves the problem by repetitively solving sub-problems: finding the optimal solution with k + 1, k + 2, . . . , n objects. Relying on the optimal value of WCSD computed in the sub-problems, a better lower bound of WCSD can be computed, enabling RBBA to have better performance. However, extending this algorithm to integrate user constraints is difficult.T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9491Fig. 10. WCSD with user constraints on Iris: (left) computation time, (right) Adjusted Rand Index.Our model can handle different kinds of user constraints and appropriate combinations of user constraints can boost the performance. A set of 120 instance-level constraints has been generated from the dataset Iris. The constraints were generated following the method described in [7]: two points are chosen randomly from the dataset, if they belong to the same cluster in the ground truth partition, a must-link constraint is generated, otherwise a cannot-link constraint is generated. The first test is without user constraints, the second one considers the first 30 constraints, the third one takes into account the first 60 constraints and so on. Fig. 10 (left) reports the total time needed to solve the dataset with these user constraints. When there are 30 constraints, the solver takes more computation time. The reason is that, with user constraints, the optimal value of WCSD is higher and the propagation of the WCSD constraint is weaker. However, when more user constraints are integrated, the propagation of must-link and cannot-link constraints is stronger and enables to quickly instantiate variables in G. As a result, the solver takes only 94 s for solving the problem with 60 constraints, and less than 10 s when there are 90 or more constraints.We have also evaluated the quality of the partitions found. For measuring the quality of a partition, we consider the Adjusted Rand Index (ARI). It measures the similarity between two partitions, in this case, the ground truth partition P of the dataset and the partition Pfound by our model. It is defined by:(cid:7)ARI =2(ab − cd)(a + d)(d + b) + (a + c)(c + b), b is the number of pairs of points that where a is the number of pairs of points that are in the same cluster in P and in P, c is the number of pairs of points that are in the same cluster in P , but in different are in different clusters in P and in Pclusters in P. The results of this experiment are reported in Fig. 10 (right). Since the constraints are generated from the ground truth partition of the dataset, the ARI value of the optimal partition is improved when more and more constraints are considered.and d is the number of pairs of points that are in different clusters in P , but in the same cluster in P(cid:7)(cid:7)(cid:7)(cid:7)7.2. Clustering with bi-criterion split-diameter7.2.1. Performance testFor the split-diameter bi-criterion, we compare our new model (CP2) with the bi-criterion clustering algorithm based on graph coloring [5] (denoted bGC). Since no implementation of the algorithm was available, we have coded it in C++. To our knowledge, this is the only exact algorithm for k ∈ [kmin, kmax]. The datasets given in Table 2 are used in the experiments and the timeout is set to 1 hour. The number of clusters k varies between 2 and the ground truth number of clusters. Table 6 gives the results of our experiments. The second column (#Sol) gives the number of Pareto optimal solutions found, or equivalently, the number of elements in the complete Pareto front. The following columns give the run time of each approach in seconds. The two programs are exact and they find the same Pareto front. It is clear that our model is the most efficient in most cases. It takes advantage of the efficient constraint propagation mechanism to reduce the search space. As in the case of GC, the algorithm bGC is limited to datasets with less than 500 points.7.2.2. Bi-criterion clustering with user-constraintsWe have generated a set of 80 instance-level constraints from the dataset Iris, as described in Section 7.1.3 and we have applied Algorithm 4 with CP2 for solving the task of bi-criterion constrained clustering. The first test is without user constraints, the second one is with the first 20 user constraints, the third one with the first 40 constraints and so on. Fig. 11 presents the Pareto front for the five cases, using from 0 to 80 user constraints. As more and more user constraints are added, the number of feasible solutions decreases and as a result, the criterion space changes significantly. Since we have generated user constraints from the ground truth partition, it is obvious that the point (Dr , Sr) corresponding to this partition must be in the region delimited by each Pareto front, including the one with 80 constraints. We can see that without user constraints, there are many points in the Pareto front but all of them are very far from (Dr , Sr). For that reason, it is useful to enable user constraints for the task of bi-criterion clustering. Moreover, given an element (D i, S i) in the Pareto front, our model can be used to enumerate all Pareto optimal solutions that have the maximum diameter D i and the minimum split S i . For example, considering the Pareto front in the case of 80 instance-level constraints, it is composed of two points, which correspond respectively to 8704 and 4352 partitions.92T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94Table 6Comparison of performance (measured in seconds) with bi-criterion Split-Diameter.Dataset#SolIrisWineGlassIonosphereUser KnowledgeBreast CancerSynthetic ControlVehicleYeastMulti FeaturesImage SegmentationWaveform8896167613–158–bGC4.20.921.51.823.6167.5––––––CP2< 0.1< 0.10.42.612.81.16.75.5–229.141.3–8. ConclusionFig. 11. Bi-criterion constrained clustering with dataset Iris.We have presented a new Constraint Programming model for Constrained Clustering. This model is a significantly im-proved version of our previous model within the Constraint Programming framework [3]. It is based on a different choice of variables and constraints. This model is modular in the sense that dedicated global constraints are developed for different optimization criteria. It is more flexible since the number of clusters does not need to be specified; it is sufficient to provide bounds on the number of clusters. It is declarative and general since it allows to choose among different optimization cri-teria and to integrate various kinds of user constraints. We show that thanks to its properties, it can be directly integrated in more general processes, as for instance for handling bi-criterion constrained clustering. Experiments on classical datasets show that our model outperforms existing exact approaches in most cases.We do believe that working on search strategies and on constraint propagation enables to improve substantially the efficiency of the approach. We continue studying these aspects to make the model able to deal with larger datasets. We also investigate the use of approximate search strategies, such as local search methods.The use of our model for bi-criterion split-diameter constrained clustering can be generalized to other bi-criterion prob-lems, the only requirement is that each single criterion must be integrated inside the model.From the Data Mining point of view, we consider integrating other optimization criteria, as for instance the within-cluster sum of squares.AcknowledgementWe gratefully thank the anonymous referees for their helpful comments to improve this article. This work is supported by a Doctoral grant from the French Ministry of National Education, Higher Education and Research.References[1] I. Davidson, S.S. Ravi, L. Shamis, A SAT-based framework for efficient constrained clustering, in: Proceedings of the 10th SIAM International Conference [2] M. Mueller, S. Kramer, Integer linear programming models for constrained clustering, in: Proceedings of the 13th International Conference on Discovery on Data Mining, 2010, pp. 94–105.Science, 2010, pp. 159–173.[3] T.-B.-H. Dao, K.-C. Duong, C. Vrain, A declarative framework for constrained clustering, in: Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, 2013, pp. 419–434.[4] M. Brusco, S. Stahl, Branch-and-Bound Applications in Combinatorial Data Analysis (Statistics and Computing), 1st edition, Springer, 2005.[5] M. Delattre, P. Hansen, Bicriterion cluster analysis, IEEE Trans. Pattern Anal. Mach. Intell. 4 (1980) 277–291.[6] T. Gonzalez, Clustering to minimize the maximum intercluster distance, Theor. Comput. Sci. 38 (1985) 293–306.[7] K. Wagstaff, C. Cardie, Clustering with instance-level constraints, in: Proceedings of the 17th International Conference on Machine Learning, 2000, T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–9493[8] I. Davidson, S.S. Ravi, Clustering with constraints: feasibility issues and the k-means algorithm, in: Proceedings of the 5th SIAM International Conference pp. 1103–1110.on Data Mining, 2005, pp. 138–149.[9] M. Ester, H.P. Kriegel, J. Sander, X. Xu, A density-based algorithm for discovering clusters in large spatial databases with noise, in: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, 1996, pp. 226–231.[10] R. Cormack, A review of classification, J. R. Stat. Soc. A 134 (3) (1971) 321–367.[11] S. Johnson, Hierarchical clustering schemes, Psychometrika 32 (3) (1967) 241–254.[12] Y. Wang, H. Yan, C. Sriskandarajah, The weighted sum of split and diameter clustering, J. Classif. 13 (2) (1996) 231–248.[13] J. Wang, J. Chen, Clustering to maximize the ratio of split to diameter, in: Proceedings of the 29th International Conference on Machine Learning, 2012.[14] N. Beldiceanu, M. Carlsson, J.-X. Rampon, Global constraint catalog, SICS and EMN technical report, http://sofdem.github.io/gccat/.[15] J.-C. Régin, A filtering algorithm for constraints of difference in CSPs, in: Proceedings of the 12th National Conference on Artificial Intelligence, Vol. 1, [16] F. Focacci, A. Lodi, M. Milano, Cost-based domain filtering, in: Proceedings of the 5th International Conference on Principles and Practice of Constraint [17] J.-C. Régin, Arc consistency for global cardinality constraints with costs, in: Proceedings of the 5th International Conference on Principles and Practice [18] F. Rossi, P. van Beek, T. Walsh (Eds.), Handbook of Constraint Programming, Foundations of Artificial Intelligence, Elsevier B.V., Amsterdam, Netherlands, 2006.[19] Gecode Team, http://www.gecode.org/.[20] I. Davidson, S.S. Ravi, The complexity of non-hierarchical clustering with instance and cluster level constraints, Data Min. Knowl. Discov. 14 (1) (2007) 25–61.[21] P. Hansen, M. Delattre, Complete-link cluster analysis by graph coloring, J. Am. Stat. Assoc. 73 (362) (1978) 397–403.[22] S. Basu, I. Davidson, K. Wagstaff, Constrained Clustering: Advances in Algorithms, Theory, and Applications, 1st edition, Chapman & Hall/CRC, 2008.[23] K. Wagstaff, C. Cardie, S. Rogers, S. Schrödl, Constrained k-means clustering with background knowledge, in: Proceedings of the 18th International Conference on Machine Learning, 2001, pp. 577–584.Conference on Machine Learning, 2004, pp. 11–18.[24] M. Bilenko, S. Basu, R.J. Mooney, Integrating constraints and metric learning in semi-supervised clustering, in: Proceedings of the 21st International [25] I. Davidson, S.S. Ravi, Agglomerative hierarchical clustering with constraints: theoretical and empirical results, in: Proceedings of the 9th European Conference on Principles and Practice of Knowledge Discovery in Databases, 2005, pp. 59–70.[26] Z. Lu, M.A. Carreira-Perpinan, Constrained spectral clustering through affinity propagation, in: Proceedings of the 2008 IEEE Conference on Computer 1994, pp. 362–367.Programming, 1999, pp. 189–203.of Constraint Programming, 1999, pp. 390–404.[27] X. Wang, I. Davidson, Flexible constrained spectral clustering, in: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discov-Vision and Pattern Recognition, 2008, pp. 1–8.ery and Data Mining, 2010, pp. 563–572.[28] T. Guns, S. Nijssen, L. De Raedt, k-Pattern set mining under constraints, IEEE Trans. Knowl. Data Eng. 25 (2) (2013) 402–418.[29] J.-P. Métivier, P. Boizumault, B. Crémilleux, M. Khiari, S. Loudni, Constrained clustering using SAT, in: Proceedings of the 11th International Symposium on Advances in Intelligent Data Analysis, 2012, pp. 207–218.[30] B. Babaki, T. Guns, S. Nijssen, Constrained clustering using column generation, in: Proceedings of the 11th International Conference on Integration of AI and oR Techniques in Constraint Programming for Combinatorial Optimization Problems, 2014, pp. 438–454.[31] D. Aloise, P. Hansen, L. Liberti, An improved column generation algorithm for minimum sum-of-squares clustering, Math. Program. 131 (1–2) (2012) 195–220.[32] L. Kotthoff, B. O’Sullivan, Constraint-based clustering, in: Proceedings of the 10th International Conference on Integration of Artificial Intelligence (AI) and Operations Research (OR) Techniques in Constraint Programming, 2013, presentation-only abstract.[33] U. Luxburg, A tutorial on spectral clustering, Stat. Comput. 17 (4) (2007) 395–416.[34] X. Wang, B. Qian, I. Davidson, On constrained spectral clustering and its applications, Data Min. Knowl. Discov. 28 (1) (2014) 1–30.[35] W. Zhi, X. Wang, B. Qian, P. Butler, N. Ramakrishnan, I. Davidson, Clustering with complex constraints – algorithms and applications, in: Proceedings of the 27th AAAI Conference on Artificial Intelligence, 2013.tional Conference on Data Mining, 2013, pp. 234–242.Artificial Intelligence, 2013, pp. 372–378.[36] I. Davidson, B. Qian, X. Wang, J. Ye, Multi-objective multi-view spectral clustering via Pareto optimization, in: Proceedings of the 13th SIAM Interna-2013, pp. 750–757.Intelligence, 2010.Intelligence, 2010, pp. 1109–1110.[37] S. Gilpin, S. Nijssen, I.N. Davidson, Formalizing hierarchical clustering as integer linear programming, in: Proceedings of the 27th AAAI Conference on [38] S. Gilpin, I.N. Davidson, Incorporating SAT solvers into hierarchical clustering algorithms: an efficient and flexible approach, in: Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2011, pp. 1136–1144.[39] J. Berg, M. Jarvisalo, Optimal correlation clustering via MaxSAT, in: Proceedings of the 13th IEEE International Conference on Data Mining Workshops, [40] L. De Raedt, T. Guns, S. Nijssen, Constraint programming for itemset mining, in: Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2008, pp. 204–212.[41] L. De Raedt, T. Guns, S. Nijssen, Constraint programming for data mining and machine learning, in: Proc. of the 24th AAAI Conference on Artificial [42] H. Cambazard, T. Hadzic, B. O’Sullivan, Knowledge compilation for itemset mining, in: Proceedings of the 19th European Conference on Artificial [43] T. Guns, S. Nijssen, L. De Raedt, Itemset mining: a constraint programming perspective, Artif. Intell. 175 (2011) 1951–1983.[44] S. Jabbour, L. Sais, Y. Salhi, The top-k frequent closed itemset mining using top-k SAT problem, in: Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, 2013, pp. 403–418.[45] W.U. Rojas, P. Boizumault, S. Loudni, B. Crémilleux, A. Lepailleur, Mining (soft-) skypatterns using dynamic CSP, in: Proceedings of the 11th International Conference on Integration of AI and OR Techniques in Constraint Programming, 2014, pp. 71–87.[46] C. Bessiere, E. Hebrard, B. O’Sullivan, Minimising decision tree size as combinatorial optimisation, in: Proceedings of the 15th International Conference [47] Y.C. Law, J.H.-M. Lee, Global constraints for integer and set value precedence, in: M. Wallace (Ed.), Proceedings of the 10th International Conference on on Principles and Practice of Constraint Programming, 2009, pp. 173–187.Principles and Practice of Constraint Programming, 2004, pp. 362–376.[48] T.-B.-H. Dao, K.-C. Duong, C. Vrain, A filtering algorithm for constrained clustering with within-cluster sum of dissimilarities criterion, in: Proceedings of the 25th International Conference on Tools with Artificial Intelligence, 2013, pp. 1060–1067.[49] G. Klein, J.E. Aronson, Optimal clustering: a model and method, Nav. Res. Logist. 38 (3) (1991) 447–461.[50] V. T’kindt, J.-C. Billaut, Multicriteria Scheduling, Theory, Models and Algorithms, 2nd edition, Springer, 2006.94T.-B.-H. Dao et al. / Artificial Intelligence 244 (2017) 70–94[51] M. Gavanelli, An algorithm for multi-criteria optimization in CSPs, in: F. van Harmelen (Ed.), Proceedings of the 15th European Conference on Artificial [52] P. Schaus, R. Hartert, Multi-objective large neighborhood search, in: Proceedings of the 19th International Conference on Principles and Practice of Intelligence, 2002, pp. 136–140.Constraint Programming, 2013, pp. 611–627.[53] K. Bache, M. Lichman, UCI machine learning repository, http://archive.ics.uci.edu/ml.[54] A. Mehrotra, M.A. Trick, A column generation approach for graph coloring, INFORMS J. Comput. 8 (1995) 344–354.