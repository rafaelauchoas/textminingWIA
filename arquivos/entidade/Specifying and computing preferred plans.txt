Artificial Intelligence 175 (2011) 1308–1345Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintSpecifying and computing preferred plans ✩Meghyn Bienvenu a, Christian Fritz b, Sheila A. McIlraith c,∗a CNRS & Université Paris-Sud, Franceb Palo Alto Research Center, USAc Department of Computer Science, University of Toronto, Canadaa r t i c l ei n f oa b s t r a c tArticle history:Received 7 April 2009Received in revised form 30 July 2010Accepted 30 July 2010Available online 2 December 2010Keywords:Knowledge representationPreferencesPlanning with preferencesIn this paper, we address the problem of specifying and computing preferred plans usingrich, qualitative, user preferences. We propose a logical language for specifying preferencesover the evolution of states and actions associated with a plan. We provide a semantics forour first-order preference language in the situation calculus, and prove that progressionof our preference formulae preserves this semantics. This leads to the development ofPPlan, a bounded best-first search planner that computes preferred plans. Our preferencelanguage is amenable to integration with many existing planners, and beyond planning,can be used to support a diversity of dynamical reasoning tasks that employ preferences.© 2011 Elsevier B.V. All rights reserved.1. IntroductionResearch in automated planning has historically focused on classical planning – generating a sequence of actions toachieve a user-defined goal, given a specification of a domain and an initial state. However, in many real-world settingssatisficing plans are plentiful, and it is the generation of high quality plans, meeting users’ preferences and constraints, thatpresents the greatest challenge [50].In this paper we examine the problem of preference-based planning – generating a plan that not only achieves a user-defined goal, but that also conforms, where possible, to a user’s preferences over properties of the plan. To address theproblem of preference-based planning, we require a language for specifying user preferences, as well as a means of gener-ating plans that is capable of optimizing for the defined class of preferences. To this end, we propose LPP , a first-orderlanguage for specifying domain-specific, qualitative user preferences. LPP is expressive, supporting the definition of tem-porally extended preferences over the evolution of actions and states associated with a plan. LPP harnesses much of theexpressive power of first-order and linear temporal logic (LTL) [51]. We define the semantics of our first-order preferencelanguage in Reiter’s version of the situation calculus [47,53]. Leveraging this semantics, we also define an extension of LPPthat allows for the specification of preferences over the occurrence of Golog complex actions [44,53]. Golog is an Algol-inspired agent programming language that supports the construction of complex actions using programming language-likeconstructs over primitive and complex actions. Golog has proven of great utility in a diversity of agent programming appli-cations.LPP ’s situation calculus semantics enables us to reason about preferences over situations (corresponding to trajectoriesor partial plans) within the language, which is beneficial for a diversity of reasoning tasks where distinguishing a preferredsituation or trajectory is relevant. Such tasks include but are not limited to plan understanding, diagnosis of dynamical✩The majority of the work presented in this paper was performed while the authors were affiliated with the University of Toronto. Revisions of the paperwere carried out while the first author was at Université Paul Sabatier and Universität Bremen, and the second author at Information Sciences Institute.* Corresponding author.E-mail addresses: meghyn@lri.fr (M. Bienvenu), cfritz@parc.com (C. Fritz), sheila@cs.toronto.edu (S.A. McIlraith).0004-3702/$ – see front matter © 2011 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.11.021M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451309systems, and requirements modeling within software engineering. LPP can also be used to characterize ordered defaultsand norms for default and deontic reasoning.Despite LPP ’s roots in the situation calculus, planning with LPP does not require the use of deductive plan synthesisand a theorem prover. LPP is amenable to use by any state-of-the-art planner that can take LTL-based preferences as input.Indeed, as we will discuss later, work by Baier and McIlraith [2] provides a compilation algorithm that enables preference-based planners that do not accept LTL formulae as input to plan with LPP -like LTL preferences. In this paper, we proposePPLAN, a bounded best-first search forward-chaining planner in the spirit of TLPlan [1] and TALPlanner [43]. PPLAN exploitsprogression to efficiently evaluate LTL preference satisfaction over partial plans. To guide search towards an optimal plan, wepropose an admissible evaluation function that, in concert with A* search, results in the generation of optimal plans.There is a significant body of research on preferences both within artificial intelligence (AI) and in related disciplines.A recent special issue of AI Magazine [36] provides a high-level overview of some of the latest AI research in this field,including research on planning with preferences [6]. In the last four years, there has been growing interest within theplanning community in preference-based planning. This includes study of the specification of preferences for planning (e.g.,Son and Pontelli [59,60] and Delgrande, Schaub, and Tompits [23]), and in particular an extension to the Planning DomainDefinition Language (PDDL) [48] by Gerevini and Long to include preferences (PDDL3) [33]. In 2006, the biennial Interna-tional Planning Competition (IPC-2006) included a track on planning with preferences specified in PDDL3. A number ofpreference-based planners were developed in and around this time, and subsequently (e.g., [24,25,41,61,9,10,29,3,5,35]). Wediscuss this related work in detail in Section 7.This paper is organized as follows. In Section 2 we provide a brief review of the situation calculus. Then in Section 3,we introduce the syntax and semantics of our LPP preference language for planning, illustrating its use through a moti-vating example which is carried throughout the paper. With the semantics of our preference language in hand, we returnto the general problem of planning with preferences. In Section 4, we define the notion of progression over LPP pref-erence formulae and prove that it preserves the semantics of our preferences. We also define an admissible evaluationfunction, which can be used with progression and A* search to generate optimal plans. Then, in Section 5, we describe thePPLAN algorithm, a bounded best-first forward-chaining planner that plans with preferences. We prove the correctness ofthe PPLAN algorithm and present experimental results for a proof-of-concept implementation of the algorithm in Prolog.Finally, in Section 6 we extend LPP to enable definition of preferences over Golog complex actions. We correspondinglyextend our notion of progression to include these new preference formulae. We conclude the paper with a discussion ofrelated work and a summary.2. PreliminariesThe situation calculus is a sorted, logical language with equality designed for specifying and reasoning about dynamicalsystems [53]. The signature of the language is specified in terms of three sorts: the set of action terms, A, consists ofconstants or functions mapping objects and sometimes other actions into elements of type action; the set of situation termsconsists of the constant S0, denoting the initial state of the world, and terms of the form do(a, s) where a is an actionterm and s is another situation term; finally object terms encompass everything that is neither an action nor a situation.In the situation calculus, the state of the world is expressed in terms of functions and relations (called fluents) which arerelativized to a particular situation s, e.g., F ((cid:3)x, s). In this paper, we consider only relational fluents, and we distinguishbetween the set F of fluents (e.g., isSnowing(s)), which are used to model dynamic properties of the world, and the setR of non-fluent relational formulae (e.g., meal(spaghetti)), which describe properties of the world that do not change overtime. A situation s is a history of primitive actions a ∈ A performed from the initial, distinguished situation S 0. The functiondo maps a situation s and an action a into a new situation do(a, s). The theory induces a tree of situations, rooted at S 0.states that situation s precedes situation sA basic action theory D in the situation calculus comprises four domain-independent foundational axioms and a set ofdomain-dependent axioms. The foundational axioms Σ define the situations, their branching structure and the situationpredecessor relation (cid:2). s (cid:2) sin the situation tree. Σ includes a second-orderinduction axiom. The domain-dependent axioms are strictly first-order and are of the general form described below. Thereader is also directed to Appendix A for an example axiomatization of the dinner domain, which we use throughout thispaper to illustrate concepts. Note that we follow the notational convention established by Reiter [53] and assume that freevariables in situation calculus formulae are universally quantified from the outside, unless otherwise noted. In later sections,when discussing preferences and Golog, we also adopt the convention of referring to fluents in situation-suppressed form,e.g., at(home) rather than at(home, s).(cid:5)(cid:5)• A set Dap of action precondition axioms which describe the conditions under which it is possible to execute an actionA in a situation s. An action precondition axiom for an action A takes the form(cid:2)PossA((cid:3)x), s(cid:3)≡ Π A((cid:3)x, s)where Π A((cid:3)x, s) is a formula with free variables among (cid:3)x, s which contains no situation terms other than s. For instance,here is a possible action precondition axiom for the action eat:(cid:2)(cid:3)Posseat(x), s≡ meal(x) ∧ ∃ y(cid:2)(cid:3)at( y, s) ∧ readyToEat(x, y, s)1310M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345• A set DSS of successor state axioms which capture the effects of actions on the truth values of the fluents. A successorstate axiom for a fluent F has the form(cid:3)(cid:3)x, do(a, s)≡ ΦF ((cid:3)x, a, s)(cid:2)Fwhere ΦF ((cid:3)x, a, s) is a formula with free variables among a, (cid:3)x, s which contains no situation terms other than s. Forexample, a successor state axiom for the fluent kitchenClean might be:kitchenClean(cid:2)(cid:3)do(a, s)≡ a = cleanDishes ∨(cid:2)(cid:3)kitchenClean(s) ∧ ∀ y a (cid:11)= cook( y)• A set of axioms DS0 describing the initial situation. These will consist of formulae which only mention S 0, or mentionno situation at all (e.g., for the non-fluent predicates). In this paper, we assume complete information about the initialsituation, i.e., that for every n-ary relation F and every n-tuple of constants (cid:3)c, we have either DS0|(cid:12)¬F ((cid:3)c).|(cid:12) F ((cid:3)c) or DS0• A set Duna of unique names axioms for actions. These have the forms∀(cid:3)x∀(cid:3)y. A((cid:3)x) = A((cid:3)y) → (cid:3)x = (cid:3)ywhere A is an action,∀(cid:3)x∀(cid:3)y A((cid:3)x) (cid:11)= B((cid:3)y)where A, B are actions such that A (cid:11)= B.More details on the form of these axioms can be found in Reiter [53].Definition 2.1 (Planning Problem). A planning problem (cid:5) is a tuple (cid:14)D, G(cid:15) where D is a basic action theory and G is a goalformula, representing properties that must hold in the final situation.Here, a goal formula G denotes a formula that only contains one situation term, which is suppressed. We denote theinstantiation of G in a situation s by G(s).In the situation calculus, planning is characterized as deductive plan synthesis [37]. Given a planning problem (cid:14)D, G(cid:15),the task is to determine a situation s that is executable, and in which the goal holds, i.e.,D |(cid:12) ∃s(cid:2)(cid:3)executable(s) ∧ G(s)(cid:5))). Notice that either the goal is satisfied in the initial situation (i.e., s =where executable(s) def= ∀a∀sS0), or s = do(an, do(an−1, . . . , do(a1, S0)))1 in which case we have shown that executing the sequence of actions a1a2 . . . anfrom S0 enables us to reach a goal state.(cid:5)) (cid:16) s → Poss(a, s(cid:5)(do(a, sWe refer to this situation s as the plan trajectory and the (possibly empty) sequence of actions a1a2 . . . an as the associatedplan. The length of this plan is n. The set of all plans is denoted by Π , and Π k denotes the subset of plans of length (cid:2) k.A planning problem (cid:14)D, G(cid:15) is solvable if it has at least one plan. It is k-solvable if it has a plan of length k or less. Note that,slightly abusing terminology, we will sometimes refer to executable sequences of actions as partial plans, even though notall sequences of actions can be extended into a plan.3. Preference specificationIn this section we describe the syntax and semantics of our first-order preference language. We illustrate the conceptsin this paper in terms of a compelling domain we call the Dinner Domain. An independent contribution of this paper is thecreation of this planning domain which can serve as a benchmarking domain for problems in planning with preferences.In addition to affording a number of natural and compelling temporally extended preferences, the dinner domain is easilyscaled either by increasing the number of objects involved (adding more restaurants, meals, etc.) or by making the eventsmore complex (e.g., buying groceries, cooking, etc.). A complete axiomatization of the dinner domain example used here isprovided in Appendix A.Example 3.1 (The Dinner Domain). It’s dinner time and Claire is tired and hungry. Her goal is to be at home with her hungersated. There are three possible ways for Claire to get food: she can cook something at home, order in take-out food, or goto a restaurant. To cook a meal, Claire needs to know how to make the meal and she must have the necessary ingredients,which might require a trip to the grocery store. She also needs a clean kitchen in which to prepare her meal. Orderingtake-out is much simpler: she only has to order and eat the meal. Going to a restaurant requires getting to the restaurant,ordering, eating, and then returning home.1 We frequently abbreviate do(an, do(an−1, . . . , do(a1, S 0))) to do([a1, . . . , an], S 0), or do((cid:3)a, S 0).M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451311This example is easily encoded in any number of planning systems, and given a specification of Claire’s initial state, aplanner could generate numerous plans that achieve Claire’s goal. Nevertheless, like many of us, Claire has certain prefer-ences concerning where and what she eats that make some plans better than others. It is the definition of these preferencesand the generation of these preferred plans that is the focus of this paper.3.1. A first-order preference languageIn this section we present the syntax of a first-order language for expressing preferences about dynamical systems.Our preference language modifies and extends the preference language PP recently proposed by Son and Pontelli [59].We keep their hierarchy of basic desire formulae (which we rename to trajectory property formulae), atomic preferenceformulae, and general preference formulae, to which we add a new class of aggregated preference formulae. Subsequentreferences to preference formulae refer to aggregated preference formulae, which encompass trajectory property formulae,atomic preference formulae, and general preference formulae. It is such a preference formula that will be given as input toa planner.Definition 3.2 (Trajectory Property Formula (TPF)). A trajectory property formula is a sentence drawn from the smallest set Bwhere:• F ⊂ B,• R ⊂ B,• If f ∈ F , then final( f ) ∈ B,• If a ∈ A, then occ(a) ∈ B,• If ϕ1 and ϕ2 are in B, then so too are ¬ϕ1, ϕ1 ∧ ϕ2, ϕ1 ∨ ϕ2, ∃xϕ1, ∀xϕ1, next(ϕ1), always(ϕ1), eventually(ϕ1), anduntil(ϕ1, ϕ2).TPFs are used to describe properties of trajectories (sequences of actions and states). The TPFs f , r, and final( f ) are usedto describe the static properties of states belonging to a trajectory, while the TPF occ(a) allows one to describe the types ofactions that occur along a trajectory. These basic TPFs then serve as building blocks for creating more complex TPFs usingthe standard Boolean connectives, quantifiers, and temporal operators.We now illustrate the kind of properties that can be expressed by TPFs by giving some sample TPFs from our motivatingexample. The formal semantics of TPFs will be presented in Section 3.2.hasIngredients(spaghetti) ∧ knowsHowToMake(spaghetti)(cid:2)∃x(cid:3)hasIngredients(x) ∧ knowsHowToMake(x)final(kitchenClean)(cid:3)at(home)(cid:2)(cid:2)(cid:3)(cid:3)cook(x)always(cid:2)(cid:2)∃x eventuallyocc(cid:2)∃x∃ y eventuallyocc(cid:2)∃x∃ y eventuallyorderRestaurant(x, y)occ(cid:2)(cid:2)(cid:2)(cid:3)∃x∃ y occ¬∧ isSnowingdrive(x, y)(cid:2)(cid:2)(cid:2)(cid:3)(cid:3)(cid:3)(cid:3)∃x¬eat(x)orderTakeout(x, y)∧ chinese(x)alwaysalwaysocc(cid:3)(cid:3)(cid:2)(cid:2)(cid:3)(cid:3)(cid:3)(cid:3)(P1)(P2)(P3)(P4)(P5)(P6)(P7)(P8)(P9)The first TPF (P1) states that in the initial situation Claire has the ingredients and the know-how to cook spaghetti. (P2)is more general, expressing that in the initial situation Claire has the ingredients for something she knows how to make.Observe that fluents that are not inside temporal connectives refer only to the initial situation. (P3) states that in the finalsituation the kitchen is clean. The TPF (P4) states that Claire remains at home throughout the trajectory. (P5)–(P7) staterespectively that at some point in time Claire cooks, orders in take-out, or orders a meal in a restaurant. The TPF (P8) statesthat at no point does Claire drive while it is snowing. Finally (P9) tells us that Claire never eats any Chinese food.TPFs can be used to express simple “all-or-nothing” preferences. For example, the TPF (P7) can be used to indicate apreference for going to a restaurant, and the TPF (P4) could be used to express a desire to stay at home. However, TPFs donot allow us to express more complex preferences like the fact that Claire prefers cooking to ordering takeout to going to arestaurant. Notice that preferences of the latter type can be satisfied to a certain degree: Claire is happiest when she cooks,less happy if she orders take-out, and least happy when going out to a restaurant. Moreover, Claire might find cooking onlyslightly better than take-out, and take-out much more appealing than going out. This suggests a need to specify preferencesover alternatives in which the user can indicate the level of preference for the different alternatives. These considerationsmotivate the introduction of atomic preference formulae.1312M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345Definition 3.3 (Atomic Preference Formula (APF)). Let V be a totally ordered set with minimal element v min and maximalelement v max. An atomic preference formula is a formula ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18) ϕn[vn], where each ϕiis a TPF, eachv i ∈ V , v i < v j for i < j, and v 0 = v min. When n = 0, atomic preference formulae correspond to TPFs.Note that the requirement that v i < v j for i < j, i.e., that the different values be distinct, is without loss of generalitysince a set of TPFs with the same value could be replaced by their disjunction.An atomic preference formula expresses a preference over alternatives. Each of the alternatives in the APF is annotatedwith a value from a totally ordered set V which describes how far that alternative is from the ideal. The lower the value,the closer to the ideal, the more satisfied the user. In what follows, we let V = [0, 1] for parsimony, but we could just aseasily choose a strictly qualitative set like {best < good < indifferent < bad < worst}.Returning to our example, the following APF expresses Claire’s preference over what to eat (spaghetti, followed by pizza,followed by crêpes)2:(cid:2)(cid:5)occ(cid:3)eat(spaghetti)(cid:5)[0] (cid:18) occ(cid:2)(cid:3)eat(pizza)[0.4] (cid:18) occ(cid:2)(cid:5)(cid:3)eat(crêpes)[0.5](P10)From the values that Claire assigned to the various options, we can see that she has a strong preference for spaghetti butfinds pizza and crêpes about equally appealing. If instead Claire is in a hurry, tired, or very hungry, she may be moreconcerned about how long she will have to wait for her meal, giving rise to the following preference:P6[0] (cid:18) P5 ∧ P4[0.2] (cid:18) P7[0.7] (cid:18) P5 ∧ ¬P4[0.9](P11)This preference tells us that Claire’s first choice is take-out, followed by cooking if it doesn’t involve going out to getgroceries, followed by going to a restaurant, and lastly cooking when it requires leaving her home. We can see here thatClaire really prefers options that don’t involve going out.To reiterate, an atomic preference formula represents a preference over alternatives ϕi . We wish to satisfy the TPF ϕiwith the lowest index i. Consequently, if Claire eats pizza and crêpes, this is no better nor worse with respect to (P10) thansituations in which Claire eats only pizza, and it is strictly better than situations in which she just eats crêpes. Note thatthere is always implicitly one last option, which is to satisfy none of the ϕi , and this option is the least preferred.Atomic preference formulae contribute significantly to the expressivity of our preference language, but we still lack away to combine atomic preferences together. In order to allow the user to specify more complex preferences, we introduceour third class of formulae, which extends our language with conditional, conjunctive, and disjunctive preferences.Definition 3.4 (General Preference Formula (GPF)). A formula Φ is a general preference formula if one of the following holds:• Φ is an atomic preference formula• Φ is γ : Ψ , where γ is a TPF and Ψ is a general preference formula [Conditional]• Φ is one of– Ψ1& . . . &Ψn [General And]– Ψ1 | . . . | Ψn [General Or]where n (cid:3) 1 and each Ψi is a general preference formula.Here are some example general preference formulae:P2 : P5 ∧ P4P10 & P11P10 | P11(P12)(P13)(P14)(P12) states that if Claire initially has the ingredients for something she can make, then she prefers to stay in and cook.Preferences (P13) and (P14) show the two ways we can combine Claire’s food and time preferences. (P13) maximizes thesatisfaction of both Claire’s food and time preferences, whereas (P14) says that she is content if either of the two weresatisfied.Our final class of formulae allows us to combine our general preferences using a number of well-known preferenceaggregation operators (cf., e.g., [26]).Definition 3.5 (Aggregated Preference Formula (AgPF)). A formula Φ is an aggregated preference formula if one of the followingholds:2 For legibility, we abbreviate eventually(occ(ϕ)) by occ(cid:5)(ϕ), and we refer to preference formulae by their labels.M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451313• Φ is a general preference• Φ is one of– lex(Φ1, . . . , Φn)– leximin(Φ1, . . . , Φn)– sum(Φ1, . . . , Φn)[if there is a sum operation associated with V ]where n (cid:3) 1 and each Φi is a general preference formula.Note that summing elements in V (if such an operation exists) could possibly yield values outside of V , e.g., if we usethe standard arithmetic sum + on V = [0, 1], we may obtain numbers greater than 1. All that we require is that the sumoperation is defined for every multi-set of elements from V , and that it outputs elements from a totally ordered set (possiblydifferent from V ).This concludes our description of the syntax of our preference language. Our language extends and modifies the PPlanguage recently proposed by Son and Pontelli [59]. Quantifiers, variables, non-fluent relations, a conditional construct, andaggregation operators (AgPF) have been added to our language. In PP it is impossible to talk about arbitrary action or fluentarguments or their properties, and difficult or even impossible to express the kinds of preferences given above. PP’s APFsare ordinal rather than qualitative making relative differences between ordered preferences impossible to articulate. Finally,our semantics, which we present in the next section, gives a different, and we argue more natural, interpretation of GeneralAnd and General Or. Relative to quantitative dynamical preferences, we argue that our language is more natural for a user.3.2. The semantics of our languageWe appeal to the situation calculus to define the semantics of our preference language. TPFs are interpreted as situationcalculus formulae and are evaluated relative to an action theory D. In order to define the semantics of more complex pref-erence formulae, which can be satisfied to a certain degree, we associate a qualitative value or weight with a situation term,depending upon how well it satisfies a preference formula. Weights are elements of V , with v min indicating complete sat-isfaction and v max complete dissatisfaction. The motivation for introducing values was that purely ordinal preferences (suchas the atomic preference formulae in [59]) can be combined in only very limited, and not necessarily very natural, ways,in addition to leading to great incomparability between outcomes. Replacing ordinal preferences by qualitative preferencesallows us to give a more nuanced representation of the user’s preferences.(cid:5)]Since TPFs may refer to properties that hold at various situations in a situation history, we use the notation ϕ[s, sproposed by Gabaldon [32] to explicitly denote that ϕ holds in the sequence of situations originating in s and terminating(cid:5) = do([a1, . . . , an], s).3 Recall that fluents are represented in situation-suppressed form and F [s] denotes the re-insertionin sof situation term s.Definition 3.6. We define the following set of macros, providing an interpretation of TPFs in the situation calculus [32]4:(cid:4)(cid:4)fr(cid:4)final( f )(cid:4)occ(a)(cid:4)(ϕ ∧ ψ)(cid:4)(ϕ ∨ ψ)(cid:4)(¬ϕ)(cid:4)(∃xϕ)(cid:4)(∀xϕ)(cid:4)eventually(ϕ)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)s, ss, ss, ss, ss, ss, ss, ss, ss, ss, sdef= f [s],for all f ∈ Fdef= r,(cid:4)def= f(cid:5)sfor all r ∈ R(cid:5),for all f ∈ F(cid:5)(cid:5)(cid:5)(cid:5)(cid:4)(cid:4)s, ss, sdef= do(a, s) (cid:16) s(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)∧ ψ∨ ψ(cid:5)(cid:3)(cid:5)s, s(cid:4)s, s(cid:4)s, s(cid:5)(cid:3)(cid:5)(cid:5)(cid:3)(cid:5)(cid:4)(cid:4)s, sdef= ϕdef= ϕs, s(cid:4)(cid:2)def= ¬ϕ(cid:2)def= ∃xϕ(cid:2)def= ∀x(cid:2)ϕdef=∃s1 : s (cid:16) s1 (cid:16) s(cid:4)(cid:3)(cid:5)ϕs1, s(cid:5)(cid:5)(cid:5)s(cid:5), s] is used, where s3 Actually, in [32], the notation ϕ[sto keep with the situation calculus convention that s precedes s4 We use the following abbreviations:(cid:5))Φ def= ∃s1(s (cid:16) s1 ∧ s1 (cid:16) s(cid:5))Φ def= ∀s1((s (cid:16) s1 ∧ s1 (cid:16) s(∃s1 : s (cid:16) s1 (cid:16) s(∀s1 : s (cid:16) s1 (cid:16) s(cid:5) ∧ Φ)(cid:5)) → Φ).(cid:5)(cid:5).is used for the start situation and s for the end situation. We chose to invert the roles of s and1314M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345(cid:4)always(ϕ)(cid:4)next(ϕ)(cid:4)until(ϕ, ψ)s, ss, ss, s(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:2)def=def= ∃a(cid:2)def=∀s1 : s (cid:16) s1 (cid:16) s(cid:2)do(a, s) (cid:16) s(cid:4)(cid:3)(cid:5)ϕ(cid:5) ∧ ϕ(cid:3)(cid:2)(cid:5)(cid:5)(cid:5)s1, s(cid:4)do(a, s), s(cid:4)(cid:5)(cid:5)(cid:3)(cid:5)∃s1 : s (cid:16) s1 (cid:16) s(cid:5)ψs1, s∧ (∀s2 : s (cid:16) s2 (cid:2) s1)ϕ(cid:5)(cid:3)(cid:5)(cid:4)s2, sThat is, a fluent which is not embedded inside some temporal connective holds in a sequence of situations just in the casethat it holds in the first situation in the sequence. For r ∈ R, we have nothing to do as r is already a situation calculusformula. A TPF final( f ) just means that the fluent f holds in the final situation. The TPF occ(a) tells us that the first actionexecuted is a. The Boolean connectives and quantifiers are already part of the situation calculus and so require no specialtranslation. Finally, we interpret all temporal connectives in exactly the same way as in [32]. Since each TPF is shorthandfor a situation calculus expression, a simple model-theoretic semantics follows.Definition 3.7 (Trajectory Property Satisfaction). Let D be an action theory. A trajectory property formula ϕ is satisfied by asituation s just in the case thatD |(cid:12) ϕ[S0, s]We define w s(ϕ) to be the weight of TPF ϕ with respect to situation s. w s(ϕ) = v min if ϕ is satisfied by s, otherwisew s(ϕ) = v max.We can extend this definition to the more general case as follows:Definition 3.8. Let D be an action theory, and let s and sϕ is satisfied by the sequence of situations between s and sbe two situations such that s (cid:16) s(cid:5)just in the case that(cid:5)(cid:5). A trajectory property formulaD |(cid:12) ϕ(cid:5)(cid:5)(cid:4)s, sWe define w s,s(cid:5) (ϕ) to be the weight of TPF ϕ with respect to the situations s and sϕ[s, s(cid:5)], otherwise w s,s(cid:5) (ϕ) = v max.(cid:5). We define w s,s(cid:5) (ϕ) = v min if D |(cid:12)Clearly Definition 3.7 is just a special case of Definition 3.8 since w s is simply short-hand for w S0,s. In most circum-stances, the short-hand w s notation of Definition 3.7 will suffice, with the advantage of being easier to read and understand.Consequently, we use it throughout the paper. Nevertheless, in proving properties of our semantics relative to progression,we will revert to the two-situation notation of Definition 3.8.Example 3.9. We evaluate the example TPFs presented above with respect to the plan trajectory s1 = do([cook(crêpes),eat(crêpes), cleanDishes], S0), and the initial situation S0 in which Claire is at home with a clean kitchen and ingredients forcrêpes, and she knows how to make both spaghetti and crêpes. Recall that for these examples we assume V = [0, 1], i.e.,v min = 0 and v max = 1. (See Appendix A for a more detailed description of S0.)w s1 (P1) = 1,w s1 (P4) = 0,w s1 (P7) = 1,w s1 (P2) = 0,w s1 (P5) = 0,w s1 (P8) = 0,w s1 (P3) = 0w s1 (P6) = 1w s1 (P9) = 0The weight of an atomic preference formula is simply defined to be the value associated with the first satisfied compo-nent TPF:Definition 3.10 (Atomic Preference Satisfaction). Let s be a situation and Φ = ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18) ϕn[vn] be an atomicpreference formula. Then(cid:6)w s(Φ) =v iv maxif D |(cid:12) ϕi[S0, s] and D (cid:11)|(cid:12) ϕ j[S0, s] for all 0 (cid:2) j < iif no such i existsExample 3.11. We evaluate the atomic preferences (P10) and (P11) with respect to the trajectory s1 and initial situation S0from Example 3.9:w s1 (P10) = 0.5,w s1 (P11) = 0.2For the trajectory s2 = do([drive(home, store), buyIngredients(spaghetti), drive(store, home), cook(spaghetti), eat(spaghetti)], S 0),we obtain:M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451315w s2 (P10) = 0,the trajectory s3 = do([drive(home, italianRest), orderRestaurant(spaghetti, italianRest), eat(spaghetti), drive(italianRest,w s2 (P11) = 0.9Forhome)], S0), we have instead:w s3 (P10) = 0,w s3 (P11) = 0.7Finally, for the trajectory s4 = do([orderTakeout(pizza, pizzaPlace), eat(pizza)], S0), we get:w s4 (P10) = 0.4,w s4 (P11) = 0Definition 3.12 (General Preference Satisfaction). Let s be a situation and Φ be a general preference formula. Then w s(Φ) isdefined as follows:(cid:6)• w s(ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18) ϕn[vn]) is defined above• w s(γ : Ψ ) =• w s(Ψ1&Ψ2& . . . &Ψn) = max{w s(Ψi): 1 (cid:2) i (cid:2) n}• w s(Ψ1 | Ψ2 | . . . | Ψn) = min{w s(Ψi): 1 (cid:2) i (cid:2) n}v minw s(Ψ ) otherwiseif w s(γ ) = v maxObserve that the semantics of our generalized Boolean connectives extends the semantics of their Boolean counterparts:a conjunction Ψ1& . . . &Ψn is fully satisfied (i.e., has weight v min) if all of the component preferences Ψi are fully satisfied;a disjunction Ψ1 | . . . | Ψn is fully satisfied if at least one of the disjuncts Ψi is fully satisfied; and a conditional preferenceγ : Ψ is fully satisfied if either the condition γ is false (i.e., has weight v max) or the component preference formula Ψ isfully satisfied. Returning to our example GPFs from page 1312:Example 3.13. We evaluate our general preference formulae with respect to the trajectories s1, s2, s3, s4 and the initialsituation S0 from above (also see DS0 in Appendix A):P12 = P2 : P5 ∧ P4P13 = P10 & P11P14 = P10 | P11s100.50.2s210.90s310.70s410.40We conclude this section with the following definition which shows us how to compare two situations with respect toan aggregated preference formula:Definition 3.14 (Preferred Situations). A situation s1 is at least as preferred as a situation s2 with respect to a preferenceformula Φ, written s1 (cid:4)Φ s2, if one of the following holds:• Φ is a GPF, and w s1 (Φ) (cid:2) w s2 (Φ)• Φ = lex(Φ1, . . . , Φn), and either w s1 (Φi) = w s2 (Φi) for all i, or there is some i such that w s1 (Φi) < w s2 (Φi) and for allj < i, w s1 (Φ j) = w s2 (Φ j)• Φ = leximin(Φ1, . . . , Φn), and either |{i: w s1 (Φi) = v}| = |{i: w s2 (Φi) = v}| for all v ∈ V or there is some v such that|{i: w s1 (Φi) = v}| > |{i: w s2 (Φi) = v}| and for all v(cid:5) < v we have |{i: w s1 (Φi) = v(cid:5)}| = |{i: w s2 (Φi) = v(cid:5)}|• Φ = sum(Φ1, . . . , Φn), andi=1 w s1 (Φi) (cid:2)(cid:7)n(cid:7)ni=1 w s2 (Φi)Strict preference ((cid:19)) and equivalence (≈) are defined in the standard way.Thus, when comparing situations s1 and s2 with respect to the preference lex(Φ1, . . . , Φn), we simply apply the standardlexicographic ordering to the tuples of weights (w s1 (Φ1), . . . , w s1 (Φn)) and (w s2 (Φ1), . . . , w s2 (Φn)). For the preferenceformula leximin(Φ1, . . . , Φn), we check value by value (starting with the minimal value v min) which of the situations hasmore elements of that value, privileging those situations which have more small (= good) values. In other words, wefirst sort the weights in the tuples (w s1 (Φ1), . . . , w s1 (Φn)) and (w s2 (Φ1), . . . , w s2 (Φn)) in ascending order, then apply thelexicographic ordering to the reordered tuples. Finally, if we have the preference sum(Φ1, . . . , Φn), then we simply sum upthe weights in the tuples (w s1 (Φ1), . . . , w s1 (Φn)) and (w s2 (Φ1), . . . , w s2 (Φn)) and then compare the resulting values.Example 3.15. We evaluate some aggregated preference formulae with respect to the trajectories s1, s2, s3, s4 and the initialsituation S0 from above. If Claire places greater importance on satisfying her food preference than her time preference, wemight have the AgPF Φ1 = lex(P10, P11), which gives the following order on situations:1316M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345s3 (cid:19)Φ1 s2 (cid:19)Φ1 s4 (cid:19)Φ1 s1Suppose now that Claire wants to satisfy her food and time preferences, but she accords equal importance to both prefer-ences. This might be expressed by the AgPF Φ2 = leximin(P10, P11), which yields the following preference ordering:s4 (cid:19)Φ2 s3 (cid:19)Φ1 s2 (cid:19)Φ1 s1Since we are using a numerical set of values, the sum operator could also be used to combine Claire’s preferences, Φ3 =sum(P10, P11), allowing her to maximize the average level of satisfaction:s4 (cid:19)Φ3 s1 ≈Φ3 s3 (cid:19)Φ3 s2Notice that because our set V of weights is assumed to be totally ordered, for each general preference formula Φ andevery pair of trajectories s1 and s2, we must have either w s1 (Φ) < w s2 (Φ) or w s1 (Φ) = w s2 (Φ) or w s1 (Φ) > w s2 (Φ). Itfollows that either s1 (cid:4)Φ s2 or s2 (cid:4)Φ s1, i.e., (cid:4)Φ defines a complete pre-order over situations. This continues to hold whenwe replace Φ by an aggregated preference formula.Remark 3.16. Given any aggregated preference formula Φ, the relation (cid:4)Φ defines a complete pre-order over situations.What is interesting about our framework is that we are capable of representing ordinal, qualitative, and simple quantita-tive preferences. For example, if we want to avoid specifying any values at all, we can still apply the aggregation operatorsto a set of TPFs. If we combine them using leximin, we generate a pre-order that ranks plans based on the number ofsatisfied preferences, whereas by using the lexicographic operator (lex), we can classify plans according to criteria of vary-ing importance. By annotating APFs with qualitative values, which is a focus of this paper, the user can specify the relativedifferences in levels of preference for different alternatives, which allows her to combine preferences in a number of naturalways. Finally, our approach works equally well in the case where the relative differences are expressed numerically, in whichcase the user can make use of the sum aggregation operator which allows for compensation between different preferences.4. Planning with preferencesWith a preference language in hand, we return to the problem of planning with preferences.Definition 4.1 (Preference-Based Planning Problem). A preference-based planning problem (cid:5)Pan action theory, G is a goal formula, and Φ is a preference formula.is a tuple (cid:14)D, G, Φ(cid:15), where D isDefinition 4.2 (Preferred Plan). Consider a preference-based planning problem (cid:5)P = (cid:14)D, G, Φ(cid:15), with plan trajectories s1 ands2, and associated plans (cid:3)a1 and (cid:3)a2. We say that plan (cid:3)a1 is preferred to plan (cid:3)a2 iff s1 (cid:19)Φ s2.Following the work on planning with domain control knowledge (e.g., TALPlanner [43], TLPlan [1]), it is interestingto consider the generalized problem of planning with hard constraints, or domain knowledge, combined with preferenceformulae. This constrained planning problem with preferences can be easily accommodated in our framework by simply addinga TPF φc representing the control knowledge and requiring that all plans satisfy φc :Definition 4.3 (Constrained Planning Problem with Preferences). A constrained planning problem with preferences (cid:5)P(cid:14)D, G, φc, Φ(cid:15), where φc is a TPF and D, G, and Φ are as above. A plan for (cid:5)Pφc[S0, do((cid:3)a, S0)].C is any plan (cid:3)a of (cid:14)D, G(cid:15)C is a tuplesuch that D |(cid:12)As an example, a user of the dinner domain in conjunction with any of the previously exemplified preference formulaemay want to rule out any plans that involve leaving the house. She can do so by using φc = always(at(home)).Definition 4.4 (Ideal Plan). Given a (constrained) preference-based planning problem with associated preference formula Φ,an ideal plan is any plan (cid:3)a such that w do((cid:3)a,S0)(Φ) = v min if Φ is a GPF, or such that w do((cid:3)a,S0)(Φi) = v min for all 1 (cid:2) i (cid:2) n, ifΦ = lex(Φ1, . . . , Φn), leximin(Φ1, . . . , Φn), or sum(Φ1, . . . , Φn).Thus, an ideal plan is a plan which fully satisfies all of the user’s preferences.Definition 4.5 (Optimal Plan). Given a (constrained) preference-based planning problem with associated preference formulaΦ, an optimal plan is any plan (cid:3)a such that there does not exist a plan (cid:3)b such that do((cid:3)b, S0) (cid:19)Φ do((cid:3)a, S0).M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451317An optimal plan is thus a plan which best satisfies the user’s preferences among all possible plans. Note that, sinceoptimality is relative, every preference-based planning problem for which at least one plan exists has at least one optimalplan, but ideal plans do not always exist, as it may be impossible to achieve the goal while fully satisfying all of the agent’spreferences.Definition 4.6 (k-Optimal Plan). Given a (constrained) preference-based planning problem with preference formula Φ anda length bound k, a k-optimal plan is any plan (cid:3)a ∈ Π k such that there does not exist another plan (cid:3)b ∈ Π k such thatdo((cid:3)b, S0) (cid:19)Φ do((cid:3)a, S0).Thus, the set of k-optimal plans consists of those plans that best satisfy the user’s preferences among the plans composedof at most k actions.For the purposes of this paper, we restrict our attention to planning problems with finite domains, i.e., problems whoseplanning domain definitions are representable in propositional logic. This restriction to finite domains is consistent with thestate of the art in automated classical planning. Before elaborating on our approach to generating preference-based plans,we digress slightly to discuss issues related to the complexity of preference-based planning.4.1. Complexity of preference-based planningWhile a complete analysis of the computational complexity of preference-based planning is beyond the scope of thispaper, in this subsection we provide insight into some of the key issues related to the complexity of preference-basedplanning.The computational complexity of classical planning is generally examined with respect to two fundamental decisionproblems: (i) the plan existence problem – informally, does there exist a plan; and (ii) the bounded plan existence problem –does there exist a plan of length n or less. A key factor in determining the complexity of these decision problems is theexpressiveness of the planning domain description. It is well established that classical planning with STRIPS using first-order terms is undecidable [28], as is numeric STRIPS [39]. However, when the STRIPS planning domain is propositional,plan existence is PSPACE-complete [19]. It takes severe syntactic restrictions on the planning domain to guarantee even NP-completeness, though such drastic restrictions can yield propositional STRIPS domains that are tractable. The most closelyrelated complexity result to the problem we examine here is that of van den Briel et al. who show that determining whethera propositional partial satisfaction planning problem has a quality of at least k is PSPACE-complete [62]. However as notedin Section 7, while partial satisfaction planning problems are related to the notion of preference-based planning definedhere, their notion of quality (or preference) is defined and evaluated in quite a different way.In order to study the computational complexity of preference-based planning as defined here, we must examine thedecision problems associated with the definitions we provided above. In particular, we must examine the decision problemsassociated with testing whether a given plan is ideal, optimal, or k-optimal. The decision problem relating to ideal plansnecessitates testing whether the sequence of states induced by an action sequence satisfies each of the component TPFsin the preference formula. In contrast, the optimal and (k-)optimality problems not only involve evaluating the preferenceformula with respect to the given plan but also verifying that there does not exist any more preferred plan. Note that oncewe have decided which of the component TPFs is satisfied by a given plan, it is possible to determine all of the possiblecombinations of TPFs that may give rise to more preferred plans.Example 4.7. Consider a GPF Φ of the form(cid:2)(cid:3)ϕ1[0] (cid:18) ϕ2[0.2] (cid:18) ϕ3[0.7](cid:2)&(cid:3)ϕ4[0] (cid:18) ϕ5[0.5] (cid:18) ϕ6[0.9]where ϕ1, . . . , ϕ6 are all TPFs. Suppose we have identified a plan trajectory s which satisfies ϕ1, ϕ5, and ϕ6. The weight ofs is 0.5, so every more preferred plan must have a weight less than 0.5. There are two ways of achieving this: either satisfyϕ1 ∧ ϕ4 (to get a weight of 0) or satisfy ϕ2 ∧ ϕ4 (to obtain a weight of 0.2).Thus, we see that the two key computational problems in our setting are to decide whether a plan satisfies a given TPFand to decide whether there exists a plan that satisfies a given TPF. The latter problem can be seen as a generalization ofthe plan existence problem.Existing complexity results related to classical planning emphasize the expressiveness of the planning domain descriptionas a determining factor in the complexity of plan existence. With respect to preference-based planning, since our preferencelanguage is intended to be used in combination with a diversity of action representations including but not limited tosituation calculus, a proper analysis would require us to parameterize each of the above decision problems by the formof the action theory and perhaps also by the form of the preference formula. In the remainder of this subsection, webriefly discuss the complexity of the plan existence problem for TPFs for the case of propositional STRIPS action theoriesand propositional preference formulae, leaving an investigation of other settings to future work. We begin by noting thatthe complexity of the plan existence problem for temporally extended goals expressed in LTL has been studied previouslyby Baral et al. [7]. They show that the plan existence problem is NP-complete for propositional action theories when a1318M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345polynomial bound is placed on plan length. As their LTL goals are very similar to our TPFs, this result is trivially extendedto our setting.Of greater interest is the case where no length bound is provided (or where the length bound is succinctly encoded) sincethen the minimal-length plans might be exponentially long, preventing us from applying the “guess-and-check” methodexploited by Baral et al. We know from the classical setting that it is PSPACE-complete to determine if an instance of apropositional STRIPS planning problem has any solutions [19]. The PSPACE lower bound clearly transfers to our setting. Theproof of PSPACE membership is based upon a recursive computation of reachability in the transition system induced by theaction theory. Initially, we want to test, for each final state s f , whether s f can be reached from the initial state s0 in at most2n steps, where n is the number of propositional variables. This test can be performed by first guessing an intermediatestate si and testing whether (i) si can be reached from s0 in 2n−1 steps and (ii) s f can be reached from si in 2n−1 steps.The recursion stops when we are only allowed a single step, in which case the two states must be identical or the secondstate must be reachable via a single action from the first state. Since the recursive calls can be processed independently,and there is a recursion depth of log 2n = n, we obtain membership in PSPACE. A crucial point is that we never materializethe transition system, nor even the path from s0 to s f , as both might be exponentially large.At first glance, there appear to be two obstacles to extending the PSPACE upper bound for classical propositional planningto our setting:• In classical planning, one only needs to consider plans in which each state is visited at most once, which is why wecan assume paths of length at most 2n. Such a restriction is not valid in our setting as some states might need to bevisited multiple times in order to satisfy the TPF. This means the length of the shortest plan satisfying a TPF might belarger than the number of states.• The recursive reachability algorithm does not allow us to keep track of the satisfaction of temporal properties.Fortunately, both points have already been addressed by the LTL model-checking community. Indeed, the key to the PSPACEmembership proof for model checking of LTL formulae, originally shown in [54], is the construction of an enhanced tran-sition system (more precisely, a Büchi automaton) in which the states contain not only propositional atoms, but also thetemporal formulae which need to be satisfied at the state. Reachability analysis can be performed on this modified tran-sition system,5 which is still at most exponentially large and can be constructed on-the-fly. The LTL approach cannot bedirectly applied to our setting, since standard LTL semantics treats infinite, rather than finite, state sequences and our TPFscontain some non-standard connectives such as occ and final. However, these differences are largely superficial, and bymodifying the definition of a final state in the transition system to account for finiteness and the final connective andby compiling away the occ connective using fluents, we can obtain membership in PSPACE for TPF plan existence in thepurely propositional setting. A result by Baral et al. shows that verifying that a given plan satisfies a TPF (our second keydecision problem) is feasible in polynomial time. Thus, by combining these results, and applying the reduction from gen-eral preferences to TPFs suggested above, we can show PSPACE-completeness of the optimality problem (and its boundedvariant).4.2. ProgressionWe now return to the question of how to compute preferred plans. Again, recall that we are restricting our attentionto planning problems with finite domains, in keeping with the state of the art in classical planning. In Section 5 we willpresent an algorithm for planning with preferences, based on forward-chaining planning. As has been done with controlknowledge containing linear temporal logic formulae [1,43], we evaluate our preference formulae by progressing them aswe construct our plan. Progression takes a situation and a temporal logic formula (TLF), evaluates the TLF with respect tothe state of the situation and generates a new formula representing those aspects of the TLF that remain to be satisfiedin subsequent situations. In this section, we define the notion of progression with respect to our preference formulae, andprove that the semantics of preference formulae is preserved through progression.Our objective in this section is to develop a method of planning for finite domain problems and as such we defineprogression for preferences ranging over finite domains. This is consistent with previous definitions of progression (e.g.,[1,43]). We believe that it is possible to extend the definition of progression to handle our first-order preference language,at least under some syntactic restrictions, but investigation of this issue goes beyond the scope of the present paper and isleft for future work.In order to define the progression operator, we add the propositional constants TRUE and FALSE both to the situationcalculus and to our set of TPFs, where D |(cid:12) TRUE and D (cid:2) FALSE for every action theory D. To capture the progression ofocc(a), we also add the TPF occLast(a), a ∈ A, whose semantics is defined by occLast(a)[s, s(cid:5)(cid:5)(s = do(a, s(cid:5)] def= ∃s(cid:5)(cid:5))).5 More precisely, we need to determine the emptiness of the Büchi automaton. This can be done by finding a final state s f which is reachable from theinitial state and is reachable from itself.M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451319Definition 4.8 (Progression of a Trajectory Property Formula). Let s be a situation. The progression of a trajectory propertyformula ϕ through s, written ρs(ϕ), is given by:• For all f ∈ F , ρs( f ) def=(cid:8)(cid:8)if D |(cid:12) f [s]TRUEFALSE otherwiseif D |(cid:12) r(cid:8)(cid:5)))(cid:5)(s = do(a, sTRUEFALSE otherwise• For all r ∈ R, ρs(r) def=• ρs(occ(a)) def= occLast(a)if D |(cid:12) ∃sTRUE• ρs(occLast(a)) def=FALSE otherwise• ρs(final(ψ)) def= final(ψ)• ρs(¬ψ) def= ¬ρs(ψ)• ρs(ψ1 ∧ ψ2) def= ρs(ψ1) ∧ ρs(ψ2)• ρs(ψ1 ∨ ψ2) def= ρs(ψ1) ∨ ρs(ψ2)• ρs(∃xψ) def=• ρs(∀xψ) def=• ρs(next(ψ)) def= ψ• ρs(always(ψ)) def= ρs(ψ) ∧ always(ψ)• ρs(eventually(ψ)) def= ρs(ψ) ∨ eventually(ψ)• ρs(until(ψ1, ψ2)) def= (ρs(ψ1) ∧ until(ψ1, ψ2)) ∨ ρs(ψ2)• ρs(TRUE) def= TRUE• ρs(FALSE) def= FALSEc∈C ρs(ψ c/x)c∈C ρs(ψ c/x)(cid:9)(cid:10)where ψ c/x denotes the result of substituting the constant c for all instances of the variable x in ϕ.Example 4.9. With S0 defined as before, we show how to progress some example TPFs:• ρS0 (occ(cook(crêpes))) = occLast(cook(crêpes))• ρS0 (eventually(kitchenClean))= ρS0 (kitchenClean) ∨ eventually(kitchenClean)= TRUE ∨ eventually(kitchenClean) ≡ TRUE(cid:9)• ρS0 (∃xhasIngredients(x)) =c∈C ρS0 (hasIngredients(c))= ρS0 (hasIngredients(crêpes)) ∨ · · · ∨ ρS0 (hasIngredients(pizza))= TRUE ∨ · · · ∨ FALSE ≡ TRUEProgression of atomic and general preference formulae is defined in a straightforward fashion by progressing the indi-vidual TPFs that comprise these more expressive formulae.Definition 4.10 (Progression of Atomic, General, and Aggregated Preference Formulae). Let s be a situation, and let Φ be an atomicor general preference formula. The progression of Φ through s is defined by:• ρs(ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18) ϕn[vn]) def= ρs(ϕ0)[v 0] (cid:18) · · · (cid:18) ρs(ϕn)[vn]• ρs(γ : Ψ ) def= ρs(γ ) : ρs(Ψ )• ρs(Ψ1& . . . &Ψn) def= ρs(Ψ1)& . . . &ρs(Ψn)• ρs(Ψ1 | . . . | Ψn) def= ρs(Ψ1) | . . . | ρs(Ψn)• ρs(lex(Φ1, . . . , Φn)) def= lex(ρs(Φ1), . . . , ρs(Φn))• ρs(leximin(Φ1, . . . , Φn)) def= leximin(ρs(Φ1), . . . , ρs(Φn))• ρs(sum(Φ1, . . . , Φn)) def= sum(ρs(Φ1), . . . , ρs(Φn))Note that progression can lead to a potentially exponential increase in the size of a TPF. In practice, we can (and do)greatly reduce the size of progressed formulae by the use of Boolean simplification and bounded quantification, cf. [1].Definitions 4.8 and 4.10 show us how to progress a preference formula one step, through one situation. We extend thisto the notion of iterated progression.1320M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345Definition 4.11 (Iterated Progression). The iterated progression of a preference formula Φ through situation s = do((cid:3)a, S0),written ρ∗s (Φ), is defined by:ρ∗S0(Φ) def= ρS0 (Φ)(cid:2)ρ∗do(a,s)(Φ) def= ρdo(a,s)(cid:3)ρ∗s (Φ)To prove our progression theorem, we will make use of a more general form of iterated progression, which takes twosituation arguments:Definition 4.12 (General Iterated Progression). The iterated progression of a preference formula Φ starting from situation s1through situation s2 (where s1 (cid:16) s2), written ρ∗(Φ), is defined as follows:s1,s2ρ∗s1,s1(Φ) def= ρs1 (Φ)ρ∗s1,do(a,s3)(Φ) def= ρdo(a,s3)(cid:2)(cid:3)(Φ)ρ∗s1,s3Finally we prove that the progression of our preference formulae preserves their semantics, i.e., that our action theoryentails a preference formula over the situation history of s if and only if it entails the progressed formula up to (but notincluding) s in the state associated with s. We will exploit this in proving the correctness of our algorithm in the section tofollow.Theorem 4.13 (Correctness of Progression). Let s1 and s2 = do([a1, . . . , an], s1) be two situations where n (cid:3) 1, and let ϕ be a TPF.ThenD |(cid:12) ϕ[s1, s2]where s2 = do(an, s3).iff D |(cid:12) ρ∗s1,s3(ϕ)[s2, s2]Proof. Refer to Appendix B. (cid:3)In the context of planning, we will be most interested in the case where s1 = S0:Corollary 4.14. Let s = do([a1, . . . , an], S0) be a situation with n (cid:3) 1, and let ϕ be a TPF. ThenD |(cid:12) ϕ[S0, s](cid:5)).where s = do(an, siff D |(cid:12) ρ∗s(cid:5) (ϕ)[s, s]From Corollary 4.14, we can prove that the weight of a preference formula with respect to a situation (plan trajectory) isequal to the weight of the progressed preference formula with respect to the final situation, disregarding its history.Corollary 4.15. Let s = do([a1, . . . , an], S0) be a situation with n (cid:3) 1 and let Φ be a preference formula. Thenw s(Φ) = w s,s(cid:5)).where s = do(an, s(cid:2)(cid:3)ρ∗s(cid:5) (Φ)4.3. An evaluation function for best-first searchIn this section, we propose an admissible evaluation function for best-first search. To this end, we introduce the notionof optimistic and pessimistic weights of a situation relative to a GPF Φ. These weights provide a bound on the best and worstweights of any successor situation with respect to Φ. As a result, our evaluation function is non-decreasing and will neverover-estimate the actual weight, thus enabling us to define an optimal search algorithm.Optimistic (resp. pessimistic) weights are defined based on optimistic (resp. pessimistic) satisfaction of TPFs. Intuitively,(cid:5)]opt, assumes that any parts of the TPF not yet falsified will eventually be satisfied,optimistic satisfaction, denoted ϕ[s, s(cid:5)(cid:5)(cid:5)] is entailed by the action theory, but without doingsuch that ϕ[s, si.e., that there is a continuation sof situation s(cid:5)]pess, assumes the opposite, namely that anything not yet satisfied willlook-ahead. Pessimistic satisfaction, denoted ϕ[s, seventually be falsified. The definition of optimistic and pessimistic satisfaction largely follows the definition of (normal)satisfaction of TPFs given earlier. The key difference is in the definition of next(ϕ), occ(a), and final(ϕ):(cid:5)(cid:5)M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451321(cid:5)(cid:5)opt def= TRUEs, s(cid:4)final(ϕ)(cid:4)final(ϕ)s, s(cid:4)occ(a)(cid:4)occ(a)s, s(cid:4)next(ϕ)(cid:4)next(ϕ)s, ss, ss, s(cid:5)(cid:5)(cid:5)pess def= FALSE(cid:5)opt def= do(a, s) (cid:16) s(cid:5)(cid:5)pess def= do(a, s) (cid:16) s(cid:2)(cid:5)opt def= ∃a(cid:5)(cid:5)(cid:5)pess def= ∃a(cid:2)(cid:5)(cid:5) ∨ s = s(cid:5)do(a, s) (cid:16) sdo(a, s) (cid:16) s(cid:3)(cid:5)(cid:5)(cid:5) ∧ ϕ(cid:5) ∧ ϕ(cid:4)do(a, s), s(cid:4)do(a, s), s(cid:5)]pess ≡ FALSE and next(ϕ)[s, s(cid:5)opt ∨ s = s(cid:3)(cid:5)pess(cid:5)(cid:5)It follows that when s = sdefine occLast(a)[s, s, occ(a)[s, s(cid:5)]opt/pess def= occLast(a)[s, s(cid:5)(cid:4)eventually(ϕ)(cid:4)always(ϕ)(cid:4)until(ϕ, ψ)opt/pess def= ϕopt/pess def= ϕopt/pess def= ψs, s(cid:4)s, ss, ss, ss, ss, s(cid:5)(cid:4)(cid:5)(cid:5)(cid:5)(cid:4)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)]. We define the other temporal formulae in terms of next.(cid:3)(cid:4)(cid:2)(cid:5)opt/pess ∨ nextopt/pess ∧ next(cid:4)(cid:5)opt/pess ∨(cid:2)(cid:2)ϕs, seventually(ϕ)(cid:5)opt/pesss, s(cid:5)opt/pess(cid:5)(cid:3)(cid:4)always(ϕ)s, s(cid:5)opt/pess ∧ next(cid:5)(cid:2)(cid:3)(cid:4)until(ϕ, ψ)(cid:5)(cid:5)opt/pess(cid:3)s, s(cid:5)]pess ≡ FALSE. For later use with progression, we alsoFor the purpose of creating an admissible evaluation function for planning, we are really only interested in optimisticevaluation. The reason why we also need pessimistic evaluation is simple: the TPF ¬ϕ is optimistically satisfied if and onlyif ϕ is not pessimistically satisfied. That is, it is optimistic to assume that there is a way to falsify ϕ which in turn willsatisfy the negation. We thus define:(cid:5)(cid:2)opt def= ¬(cid:5)(cid:2)pess def= ¬(cid:5)pess(cid:5)opts, ss, sϕ(cid:3)(cid:3)(cid:4)(cid:4)(cid:5)(cid:5)(cid:5)(cid:5)(cid:4)(¬ϕ)(cid:4)(¬ϕ)s, ss, sϕFor all other elements of the language, the definitions are the same as for normal TPF satisfaction.We can now define optimistic and pessimistic weights of TPFs in terms of optimistic and pessimistic TPF satisfaction:(cid:6)w opts,s(cid:5) (ϕ) =if D |(cid:12) ϕ[s, sv minv max otherwise(cid:5)]optand(cid:6)w pesss,s(cid:5) (ϕ) =if D |(cid:12) ϕ[s, sv minv max otherwise(cid:5)]pessFor readability, we abbreviate w optS0,s by w optFor APFs and GPFs the definitions of optimistic and pessimistic weights are straightforward.S0,s and w pessand w pessrespectively.ssDefinition 4.16 (Optimistic/Pessimistic Atomic Preference Satisfaction). Let s be a situation and Φ = ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18)ϕn[vn] be an atomic preference formula. Then(cid:6)opt/pesssw(Φ) =v iv maxif D |(cid:12) ϕi[S0, s]opt/pess and D (cid:11)|(cid:12) ϕ j[S0, s]opt/pess for all 0 (cid:2) j < iif no such i exists.Definition 4.17 (General Preference Satisfaction). Let s be a situation and Φ be a general preference formula. Then w optrespectively w pess(Φ), is defined as follows:ss(Φ),• wopt/pesss• wopt/pesss• w• wopt/pesssopt/pesss(ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18) ϕn[vn]) is defined above(γ : Ψ ) =(γ ) = v maxpess/opts(cid:6)if w(Ψ ) otherwisev minw(Ψ1& . . . &Ψn) = max{w(Ψ1 | . . . | Ψn) = min{wopt/pesssopt/pesssopt/pesss(Ψi): 1 (cid:2) i (cid:2) n}(Ψi): 1 (cid:2) i (cid:2) n}The following theorem describes some of the important properties of our optimistic and pessimistic weight functions.Theorem 4.18. Let s be any situation, let [a1, . . . , an] be an action sequence, and for every i such that 0 (cid:2) i (cid:2) n, let si =do([a1, . . . , ai], s). Let further ϕ be a TPF and Φ a general preference formula. Then for any 0 (cid:2) i (cid:2) j (cid:2) k (cid:2) n:1322M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451. If D |(cid:12) ϕ[s, si]pess, then D |(cid:12) ϕ[s, s j], and if D (cid:11)|(cid:12) ϕ[s, si]opt, then D (cid:11)|(cid:12) ϕ[s, s j],si (Φ) (cid:2) w opt2. w opts j (Φ) and w opts j (Φ) (cid:2) w sk (Φ),(Φ) (cid:3) w pess3. w pesss js jsi(Φ) (cid:3) w sk (Φ).(Φ) and w pessProof. Refer to Appendix C. (cid:3)Intuitively, 1. states that the pessimistic satisfaction of a formula over a sequence of situations implies that any continu-ation of this sequence also satisfies the formula. It further states that a formula that does not hold optimistically cannot bemade true by any continuation. Correspondingly, 2. states that optimistic weight is non-decreasing and bounded from aboveby the real weight of any future situation. Finally, 3. gives the analogue for pessimistic weights: they are non-increasing andbounded from below by the real weight of any future situation.One immediate implication of this is the following:Corollary 4.19. For any two situations s, s(cid:5), such that s (cid:16) s(cid:5):If w opts(Φ) = wpesss(Φ) then w s(cid:5) (Φ) = wopts(Φ) = wpesss(Φ).Since these definitions are compatible with those defined for progression, we have the following corollary to Theo-rem 4.18:Corollary 4.20. Let s = do([a1, . . . , an−1], S0) and ss (ϕ)[sϕ[S0, s(cid:5)]opt/pess iff D |(cid:12) ρ∗(cid:5)]opt/pess.(cid:5), s(cid:5) = do(an, s) be situations where n (cid:3) 1, and let ϕ be a TPF. Then D |(cid:12)Proof. Refer to Appendix D. (cid:3)This corollary states that we can still use progression when computing optimistic and pessimistic weights. Intuitively thisis because the optimistic (pessimistic) part of the evaluation is only concerned with the future whereas progression dealswith the past. Since the past won’t change, there is no room for optimism or pessimism.We can now define our evaluation function fΦ .Definition 4.21 (Evaluation function). Let s = do((cid:3)a, S0) be a situation and let Φ be a general preference formula. Then fΦ (s)is defined as follows:fΦ (s) =(cid:6)w s(Φ)w optsif (cid:3)a is a plan(Φ) otherwiseFrom Theorem 4.18 we see that the optimistic weight is non-decreasing and never over-estimates the real weight. Thus,fΦ is admissible and when used in best-first search, the search is optimal.5. The PPLAN algorithm and implementationIn this section, we describe PPLAN, a bounded best-first search forward chaining planner for computing preferred plans.PPLAN is currently implemented in Prolog and has not been optimized. Rather, its Prolog implementation provides a meansof experimenting with different heuristics and search techniques and a framework for reasoning with preferences in thesituation calculus. As a result of this, PPLAN’s subsequent integration with a Prolog interpreter for the agent programminglanguage Golog was straightforward [58]. In what follows, we describe the PPLAN algorithm and prove properties of thesystem, as well as presenting experiments that illustrate the effectiveness of its heuristic to guide search. The PPLAN codeand test cases are available online [12].The PPLAN algorithm is outlined in Algorithm 1. PPLAN takes as input an initial state init, a goal formula goal, a generalpreference formula pref ,6 and a plan length bound maxLength. The algorithm returns two outputs: a plan and its weightwith respect to pref . The frontier is a list of nodes of the form (w 1, w 2, path, state, pref ), where w 1, w 2 denote weights,which usually hold the optimistic and pessimistic weight, respectively, path is the considered partial plan, state is the statereached by it, and pref denotes the progressed preference formula. Recall that weights are values drawn from the totallyordered set V , and may be qualitative concepts such as “excellent” or “good”, just as easily as numeric values. The frontieris sorted by w 1, then by w 2, and by length (in increasing order). The frontier is initialized to the empty partial plan, itsoptimistic and pessimistic weights, optW and pessW, with respect to the initial situation and the preference formula pref . In6 For simplicity, we present our algorithm for general rather than aggregated preference formulae. We discuss the extension to aggregated preferenceformulae later in the section.M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451323Algorithm 1: PPLAN(D, init, goal, pref, maxLength)12345678910beginfrontier ← [ (optW(pref, [ ], init), pessW(pref, [ ], init), [ ], init, pref) ];if D |(cid:12) goal[init] thennode1 ← (realW(pref, [ ], init), realW(pref, [ ], init), [ ], init, pref);frontier ← sortNmergeByVal( [node1] , frontier);while frontier (cid:11)= ∅ donode ← removeFirst(frontier);if D |(cid:12) goal[node.state] and node.w1 = node.w2 thenreturn node.path, node.w1;successors ← expand(node.path, node.state, node.pref, maxLength);/* expand(path, state, pref, maxLength) returns a list of new nodes to add to the frontier. Eachnode is of the form (w 1, w 2, path, state, pref). If path, the sequence of actions so far, haslength equal to maxLength, expand returns the empty list ([ ]). Otherwise, expand determines allthe executable actions in the given situation and returns a list which contains, for each ofthese executable actions a node(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5), path, state(optW(pref), pessW(pref), pathand for each action leading to a situation that satisfies the goal, a second node), path, stateHere optW, pessW, and realWdenote functions that return the optimistic, pessimistic, and realweight for the given (progressed) preference formula, action sequence, and state.), realW(pref(realW(pref, state, state, state, state, path, path, path, pref, pref).)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)frontier ← sortNmergeByVal(successors, frontier);return “no solution”, ∞;111213end*/the case where the initial situation satisfies the goal, in addition, another node is added to the frontier, representing the realweight for the empty plan (Lines 3–5). On each iteration of the while loop, PPLAN removes the first node from the frontierand places it in node. If the partial plan of node satisfies the goal and its two weights are equal, then PPLAN returns node’spartial plan and weight. Otherwise, we call the function expand using the elements of node as input. If path has lengthequal to maxLength then no new nodes are added to the frontier. Otherwise, expand generates a new set of nodes of the(cid:5)form (optW, pessW, path), one for each action executable in state. For actions leading to goal states, expand alsogenerates a second node of the same form but with optW and pessW replaced by the actual weight achieved by the plan.The reason that we need two nodes is that on the one hand, we need to record the actual weight associated with the planthat we have found, and on the other hand, to ensure completeness, we need to be able to reach the node’s successors. Thenew nodes generated by expand are then sorted by their two weights and length and are merged with the remainder ofthe frontier. If we reach the empty frontier, we exit the while loop and return “no solution”.(cid:5), pref, state(cid:5)A naive implementation of such a planner would require computing alternative plan trajectories and then evaluatingtheir relative weights. This is computationally explosive, requiring computation of numerous plan trajectories, caching ofrelevant trajectory state, and redundant evaluation of preference formula weights. Instead, we make use of Theorem 4.13to compute weights as we construct plans, progressing the preference formula as we go. Exploiting progression enables thedevelopment of a best-first search strategy that orders search by weight, and evaluates preference formulae across sharedpartial plans. Progression is commonly used to evaluate domain control knowledge in forward chaining planners such asTLPlan [1] and TALPlanner [43], where progression of hard constraints prunes the search space. In contrast, we are unableto prune less preferred partial plans, because they may yield the final solution, hence the need for a best-first strategy.Note that the length bound is necessary to prevent the algorithm from exploring long or even infinite action sequencesthat have optimistic weight zero but do not reach a goal state. This can, for instance, happen when a final(ϕ) TPF is usedwith a formula ϕ that cannot ever be achieved using the available actions given the initial state. Since our heuristic doesnot perform look-ahead – approximate or otherwise – it would not be able to detect such branches and could get stuck inan infinite loop.The following theorem asserts both the completeness and the k-optimality of PPLAN.Theorem 5.1 (Correctness of PPLAN algorithm). Given as input a preference-based planning problem P and a length bound k, PPLAN re-turns a k-optimal plan, if P is k-solvable, and returns “no solution” otherwise.Proof. First, we prove that the algorithm terminates. There are two ways that PPLAN halts: either the first node on thefrontier is a plan and has w 1 = w 2 in which case PPLAN returns this plan, or we reach the empty frontier, in which casePPLAN returns “no solution”. Let us then suppose that the first condition is never met. In this case, we will stay in the whileloop, expanding one node on each iteration. But since the successor nodes generated by expand always have length onegreater than their parent, and since expand returns an empty list whenever a node has a partial plan of length equal to k,and there are only finitely many actions to consider in each node, we will eventually run out of nodes and reach the emptyfrontier. Thus, the algorithm always terminates.1324M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345Next, we prove that the output satisfies the conditions of the theorem. This is obvious for the case where P is notk-solvable as in this case, we will never find a plan and thus will stay in the while loop until we reach the empty frontier,finally returning “no solution”.We now treat the case where P is k-solvable. By definition, this means that there exists at least one plan of lengthless than or equal to k. As PPLAN systematically explores the search space, at some point expand will create a node whosepartial plan satisfies the goal and will set w 1 and w 2 to the actual weight. This means that the frontier will contain anode satisfying the conditions of the if-statement, and hence, at some point, we will enter the if-statement and return anon-empty plan. It remains to be shown that the plan returned is k-optimal.(cid:5)of length less than or equal to k which has a weight wSuppose for a contradiction that we return a plan p with weight w which is not k-optimal. This means that there exists(cid:5) < w. There are two possibilities: either (1) we havea plan pand placed it behind p on the frontier, which is a contradiction as the frontier isgenerated a node corresponding to psorted in non-decreasing order by node.w1, which for nodes corresponding to plans is equal to their real weights, i.e., wand wwhich is behind p in the frontier. But this is not possible either,(cid:5) < w (and henceas according to Theorem 4.18, any ancestor of pwould again be before p on the frontier). We have thus shown that if P is k-solvable, the returned plan is k-optimal,concluding the proof. (cid:3)must have an optimistic weight less than or equal to win this case, or (2) there is an ancestor node of p(cid:5)(cid:5)(cid:5)(cid:5)Note that our algorithm is straightforwardly modified to handle aggregated preference formulae. It suffices to associate atuple of optimistic and pessimistic weights with each node, in order to keep track of the optimistic and pessimistic weightsof each of the component GPFs. We then sort the frontier by comparing the tuples according to Definition 3.14. So forinstance, given an AgPF Ψ = lex(Φ1, Φ2, Φ3), our frontier would contain nodes of the form ((w 1w , w 32),2 are the optimistic and pessimistic weights associated with the GPF Φi (for i = 1, 2, 3).path, state, pref), where w iWhen sorting the frontier, we would place a node whose first component is (0, 1, 1) before a node with first component(1, 0, 0), since (0, 1, 1) precedes (1, 0, 0) in the lexicographic ordering. It is easy to show that Theorem 5.1 continues to holdfor the modified algorithm which takes aggregated preference formulae as input.1 and w i1), (w 11, w 31, w 22, w 25.1. ExperimentsAs noted previously, PPLAN was implemented in Prolog as a testbed for planning with rich, temporally extended pref-erences and was not optimized to support large-scale experimental evaluation. As such, its performance is not competitivewith recent state-of-the-art preference-based planners such as those that competed in IPC-2006. We discuss these plannersand their relationship to PPLAN in Section 7.We were interested in evaluating whether the combination of progression to evaluate LTL satisfaction and our proposedadmissible heuristic provided an approach that could help guide a planner toward finding an optimal plan. From the outset,we had two concerns. The first was that while progression and blind search had proven effective in planners like TLPlan,the strength of progression had been rooted in its ability to prune states that did not comply with LTL domain-controlknowledge, thus vastly reducing the search space. With LTL preferences no comparable pruning could be done and as suchthe merit of progression was in question. Further, our objective was ambitious – to generate an optimal plan – and assuch we were using an admissible evaluation function; however it is widely accepted that admissible heuristics often don’tprovide sufficient guidance relative to inadmissible heuristics.To assess the behaviour of our planner, we ran 60 instances of our dinner domain.7 Each instance was run with PPLAN,PPLANC (PPLAN augmented with domain control knowledge – see below), and with depth-first search (DFS) and breadth-first search (BFS) algorithms. In order to facilitate comparison, DFS and BFS were passed the k-optimal weight as a parameterand run until they found a plan with this weight (or ran out of memory). All algorithms were implemented in Prolog usingthe same code base, to the extent possible. Each run was compared with respect to the length of the returned plan and thenumber of nodes expanded. Results are reported in Fig. 1 with instances numbered in order of increasing PPLAN runningtime. Fig. 2 plots test cases against their running time.The 60 individual instances differed with respect to the initial state, the goal, the size and nature of the GPF, and thelength of the optimal plan. In most experiments the agent is initially at home and has at least the goal of being sated. Theinitial state varies with respect to ingredients that are available at home, or things the agent knows how to cook. Preferencesreflect the type of food the agent would like to eat, and how and where the agent obtains her meal. Most GPFs containedmultiple TPFs and APFs, but the domain did not warrant LTL nesting. Most GPFs contained one or more eventually(occ())formulae. GPFs further differed with respect to whether formulae were ground or quantified, contained conditionals, etc.Most optimal plans were of length 6 or less. The preferences expressible as GPFs are too diverse in form and complexity todraw conclusions regarding any correspondence between the size of a GPF formula and the scalability of the planner. Muchdepends on the specifics of the problem instance [12].7 We also ran an early version of PPLAN on the simple school travel example presented in [59], but we were unable to get comparative statistics in orderto compare the two approaches.M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451325PPLANPPLANCBFSDFSPPLANPPLANCBFSDFSTest #123456789101112131415161718192021222324252627282930l232223322422623332236223233333NE7557895255615129231713545164810551087858595515151615l232223322422623332236223233333NE71078922102221192343310211281010547315131015151615l232223322422∗23332234223232332NE61426516171432426615119755161∗61495408421618242624795151408614267140849561l67667777676∗∗∗7776777663677777NE4813954064815104143953954061113406∗∗∗4613893774815903951688406406385481395377389461395Test #313233343536373839404142434445464748495051525354555657585960l323335554455243445372723347663NE1586815408132922292949235972755605928651157577025559737257125451 75316 878l323336654455253446352723346663NE153231511915112219192923169311087111236737163101692219 137858157340l3233324244∗22∗34443∗242234∗4∗3NE426514084264086019756119751975∗7151∗432253720882088432∗51241761615052479∗2599∗432l7637777666772∗777777672737∗7∗∗NE395406385395389432111348111 76711 76715 049510402∗414152610151015414254040616174773954931688∗1597∗∗Fig. 1. Plan length (l) and nodes expanded (NE) by PPLAN, PPLAN augmented by hard constraints (PPLANC ), breadth-first search (BFS), and depth-first search(DFS). The symbol ∗ indicates out of memory (1 GB limit).Overall, the results were quite positive. In 55 of the 60 test cases PPLAN expanded fewer nodes than BFS and DFS,generally by a significant margin and often even an order of magnitude.The four cases where BFS and DFS outperformed PPLAN are all cases where there was a short k-optimal but non-idealplan. In these cases, PPLAN quickly found the plan, but had to continue the search in order to ensure that no other betterplan existed. The poor performance of PPLAN relative to BFS and DFS in these cases is a result of the experimental setupwhich gave BFS and DFS an unfair advantage by supplying them with the k-optimal weight. If PPLAN had received the sameinput, it would not have resulted in a larger number of expanded nodes. In two cases, PPLAN expanded a comparativelylarge number of nodes (> 10,000). This speaks to the difficulty of the task. A good way to cope with this problem is toadd domain-dependent control knowledge to reduce the search space, as was done in TLPlan. In order to test out this idea,we reran PPLAN on the test suite, this time pruning all nodes whose partial plans contained two consecutive drive actionsor those containing orderTakeout, orderRestaurant, or cook actions not immediately followed by an eat action. As the resultsshow, adding these simple pieces of control knowledge allowed PPLAN to find a plan while expanding far fewer nodes inthe process. Taken all together, we feel that these results speak to the effectiveness of our evaluation function in guidingthe search but also to the interest of combining this approach with domain-dependent control knowledge, or some othermeans of pruning the search space. Regarding the running times plotted in Fig. 2, there is a rough correspondence betweenthe numbers of nodes expanded by PPLAN and an instance’s running time. Interestingly, while DFS generally expands manymore nodes than PPLAN, it is still comparatively fast. This, however, is once again a consequence of knowing the k-optimalvalue. If this value had to be found incrementally, long search horizons would need to be explored exhaustively first, beforethe optimal plan/value would be found.It is interesting to note test cases 24, 43, 53, 55, and 60 where PPLAN demonstrates poorer performance than BFS andDFS. Recall that PPLAN’s best-first search explores plans based on weight then length. As a consequence, PPLAN can beled astray, investigating a long plan with low weight, whereas the best plan can end up being a shorter plan with higherweight. However, this behaviour appears to occur infrequently, and the heuristic generally leads to significantly improvedperformance.Also note that BFS sometimes finds shorter plans than PPLAN (see cases 31, 33, 34, 36, 37, 39, 48, 49, 52, and 54). PPLANuses plan length only as a third sorting criterion and hence length is only considered when the first two weights are equal.However, when a goal is found, the second weight of the newly created tuple for this plan is set to the real weight, whichis often lower than the pessimistic weight. Therefore, when a plan is returned there may still be other plans with the sameweight which are shorter. The comparison with DFS shows that this is, however, not the reason for PPLAN’s performanceimprovement over BFS. We conclude that the implemented heuristics provide valuable search guidance.1326M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345Fig. 2. Running-time of PPLAN, PPLAN augmented by hard constraints (PPLANC ), breadth-first search (BFS), and depth-first search (DFS). Missing valuesindicate out of memory (1 GB limit).5.2. Beyond PPLANOur original experimental results with PPLAN were encouraging, but we realized from our experience with IPC plannersthat much more could be done. In particular many state-of-the-art planners use some form of heuristic search, based on arelaxed plan graph (RPG) that provides a good estimate of the distance from the current state to a state in which a goal isreached (e.g., [40]). Of course, when the goal (or in this case, preference) is an LTL formula, it is more difficult to form suchan estimate.Indeed in analyzing earlier PPLAN results, it was clear that our optimistic evaluation function was lacking in two ways.First, in certain cases it could not distinguish between partial plans that made progress towards satisfying preferences andthose that did not. Second, and more importantly, our evaluation function provided no estimate of the number of actionsrequired to satisfy TPFs such as eventually(ϕ) nor did it have a way of determining actions to select that would makeprogress towards satisfaction of preferences. To illustrate the former point, consider how PPLAN evaluates the following GPF,Φ, taken from [5]:(cid:4)(cid:5)eventually(ϕ1) ∧ eventually(ϕ2)[v 1] (cid:18) always(ϕ3)[v 2],where ϕ1 might be occ(clean(kitchen)), ϕ2 might be occ(eat(pizza)) and ϕ3 might be at(home). Our PPLAN optimisticevaluation assumes (optimistically) that each of the component predicates in the TPF can become true as new actionsare added, until proven false. As such, the TPF eventually(ϕ1) ∧ eventually(ϕ2) will be true whether or not either of ϕ1 orϕ2 have actually been satisfied because eventually(ϕi) can never be falsified. There is always hope that ϕi will be achievedin a subsequent state of the plan. Thus, there is no distinction between a partial plan in which one or both of ϕ1 or ϕ2have been achieved and one in which they have not, and as such no measure of progress towards satisfaction of the TPF.This lack of ability to distinguish progress towards satisfaction of a TPF is dependent on the form of the TPF. In contrast, theTPF always(ϕ3) is falsifiable as soon as ϕ3 is false in some state. To evaluate the APF/GPF, we assign a weight equal to thesmallest weight TPF that is optimistically satisfied. Since the TPF eventually(ϕ1) ∧ eventually(ϕ2) is always optimisticallysatisfied, our example Φ is always evaluated to weight v 1.From these observations of the shortcomings of the PPLAN evaluation function, Baier and McIlraith explored whetherlook-ahead heuristics could be developed that would address these deficiencies. The results of this examination were re-ported in [5]. First LPP GPF preferences were decomposed into smaller constituent pieces and then translated, following[2], into parameterized non-deterministic finite-state automata whose accepting conditions corresponded to satisfaction ofthe component preference formulae. For each new planning instance, the planning domain description was augmented witha description of the automata representing the preference formulae – the state of each automaton, and what governedtransitions between automaton states. From here, a set of inadmissible and admissible heuristics were proposed that wereused together to guide search towards satisfaction of the goal and preferences. These heuristics exploited the RPG over theautomata-enhanced domain and thus were able to measure progress towards satisfaction of LTL formulae. Since the use ofinadmissible heuristics caused the system to lose the guarantee that the first plan returned was optimal, an incrementalapproach was used to search for a plan, and it was shown that if the algorithm terminated, the plan was optimal. Leveragingthe insights from PPLAN of the power of pruning, this work developed a sound pruning strategy that allowed inferior partialplans to be identified and pruned from the search space, thus reducing the search space significantly.M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451327Experimental results comparing this work with the PPLAN algorithm presented above showed that in some instancesthere was an order of magnitude reduction in the number of nodes expanded before an optimal plan was found. This isattributed in great part to the heuristics’ ability to guide search going forward, which in turn is the result of compilingthe LTL formulae into a form that can exploit a look-ahead heuristic such as an RPG heuristic. The optimistic evaluationfunction itself operates well in many situations, and indeed was exploited in an IPC-2006 planner to great gain, as discussedin Section 8.6. Specifying preferences over complex actionsIn previous sections we defined LPP , a language for specifying rich, temporally extended preferences, and proposed analgorithm for computing optimal preference-based plans. In this section, we return to this language, proposing an extensionwith complex actions.One of the notable features of LPP is the occ(a) statement, which allows for the specification of preferences for partic-ular actions. We extend LPP with two additional constructs in order to allow for the expression of preferences concerningthe occurrence of complex actions – actions that capture the orchestration of multiple primitive or other complex actionsusing well-known programming constructs. Dine in restaurant might be such a complex action, comprising actions that takethe agent from her current location to the restaurant, where she orders and then eats a meal, and finally returns the agentback to her original location.In many practical circumstances, people think in terms of complex actions when they describe preferred ways of achiev-ing a given goal. It follows that allowing users to describe preferences directly in terms of these complex actions, insteadof requiring them to reformulate their preferences in terms of atomic actions, may help simplify the preference elicitationprocess. Furthermore, the provision of such – procedural – complex actions can be used to effectively guide the searchtowards the goal, as complex actions often take the form of under-constrained prototypical plans.For specifying and reasoning about complex actions, we use the Golog language and semantics [44]. Golog is a pro-gramming language defined in the situation calculus which allows a user to specify programs whose set of legal executionsspecifies a sub-tree of the tree of situations of a basic action theory. Golog has an Algol-inspired syntax extended withflexible non-deterministic constructs, which are later transformed into specific sequences of actions by a planner. This in-tegration of planning and programming has proved useful in a variety of diverse applications including museum tour-guiderobots [18], Web service composition [49], and soccer playing robots [30].6.1. GologThe set of Golog programs (without procedures) is inductively defined using the following constructs, where all appearingδ’s are again Golog programs (without procedures) and the ϕ’s are pseudo-fluent expressions. These represent situationcalculus formulae with all situation terms suppressed. The expression ϕ[s] denotes the instantiation of ϕ with all occurringfluents relativized to situation s.aϕ?(δ1; δ2)if ϕ then δ1 else δ2while ϕ do δ(cid:5)(δ1|δ2)π v.δδ∗primitive actiontest condition ϕsequenceconditionalloopsnon-deterministic choicenon-deterministic choice of argumentnon-deterministic iterationIn addition, we introduce the term any to denote any action. In order to avoid ambiguity, in what follows we will callprograms built from these constructs LPP -programs.As an example, the following program may describe a preferred way of going out to a restaurant:(cid:2)π r.dineInRest(r)?; if close(home, r)(cid:2)(cid:3)walk(home, r); π y.orderRestaurant(r, y); eat( y); walk(r, home)(cid:3)(cid:3)drive(home, r); π y.orderRestaurant(r, y); eat( y); drive(r, home)then(cid:2)else(G1)The program begins by picking a dine-in restaurant. Then, if the chosen place is close to home, it prescribes to walk there,order, eat, and return home. Otherwise, walking is replaced by driving.The next example shows how non-determinism can be used to “achieve” a sub-goal, here hasIngredients(m). Using the∗construct, the program leaves it up to the planner to find a sequence of actions that will satisfy the subsequentanycondition. The following program describes a sensible procedure for fixing a meal at home.(cid:2)(cid:2)πm.(cid:3)meal(m) ∧ knowsHowToMake(m)?; any(cid:3)∗; hasIngredients(m)?; cook(m); eat(m)(G2)1328M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345According to this program, one starts by selecting a meal that one knows how to make and ensuring that one has all thenecessary ingredients, which could involve a trip to the grocery store, and then the meal can be prepared and finally eaten.Originally, the semantics of Golog programs was defined via recursive macro expansion of programs into formulae ofthe situation calculus. Such an “evaluation semantics” requires one to evaluate the entire program at once, which makesit difficult to use in the context of planning. This is the reason that in this paper we adopt an alternative semantics forGolog programs, the so-called “transition semantics”, which was introduced in [21], where it was shown to be equivalentto the evaluation semantics. The transition semantics is defined in terms of possible transitions between program–situationpairs, called configurations. Roughly speaking, a configuration δ, s can lead to a configuration δ(cid:5), s(cid:5)))if by executing a single step of the program δ in situation s, we reach the situation swhere the remaining programis δ(cid:5). A second predicate Final is used to characterize the conditions under which a program has executed completely. Thetransition semantics is well-suited to our purposes as it permits a step-by-step evaluation of programs.(written Trans(δ, s, δ(cid:5), sFormally, the two aforementioned predicates, Trans and Final, are defined using the following axioms [21], where a[s]x denotesdenotes action a with all of its arguments evaluated in s, ϕ[s] denotes the truth value of formula ϕ in s, and δ vthe substitution of all occurrences of v in δ by x:(cid:5)(cid:5)(cid:2)a, s, δTrans(cid:2)Transany, s, δ(cid:2)Trans(cid:2)ϕ?, s, δ(cid:5)δ1; δ2, s, δTrans(cid:5)(cid:5)(cid:5)(cid:5), s, s(cid:5), s(cid:5), s(cid:2)TransTransif ϕ then δ1 else δ2, s, δ(cid:2)while ϕ do δ1, s, δ(cid:2)δ1|δ2, s, δπ v.δ, s, δ∗Trans(cid:2)Trans(cid:2)Transδ, s, δ(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5), s, s, s, s, s(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:5) = nil ∧ s(cid:3)(cid:2)(cid:5) = doa[s], s(cid:2)(cid:5) = doa[s], s(cid:3)(cid:3)(cid:5) = nil ∧ s(cid:2)≡ Poss(a, s) ∧ δ≡ ∃a≡ ϕ[s] ∧ δ≡Poss(a, s) ∧ δ(cid:5) = nilFinal(δ1, s) ∧ Trans∨∃γδ(cid:2)ϕ[s] ∧ Trans(cid:2)(cid:2)≡(cid:2)(cid:2)(cid:2)(cid:3)(cid:3)(cid:5)(cid:5), sδ2, s, δ(cid:2)(cid:5) = (γ ; δ2) ∧ Transδ1, s, γ , s(cid:3)(cid:3)∨δ1, s, δ(cid:3)(cid:2), s(cid:2)(cid:2)(cid:5)(cid:5)(cid:3)(cid:3)(cid:3)(cid:5)Trans(cid:5)(cid:5)(cid:3)(cid:2)≡ ϕ[s] ∧ ∃γ≡ Trans≡ ∃xTrans(cid:2)≡ ∃γTrans(cid:2), s(cid:5)δ1, s, δδ vx , s, δ(cid:2)δ, s, γ , s, s(cid:5)δ1, s, γ , s(cid:2)∨ Trans(cid:3)(cid:5)(cid:3)(cid:5)(cid:3)∧ δ(cid:5) = γ ; δ∗(cid:2)(cid:3)(cid:3)(cid:5)δ2, s, δ¬ϕ[s] ∧ Trans∧ δ(cid:5), s(cid:3)(cid:5) = (γ ; while ϕ do δ1)(cid:5), s(cid:3)(cid:5)δ2, s, δFinal(nil, s)Final(δ1; δ2, s) ≡ Final(δ1, s) ∧ Final(δ2, s)(cid:2)Final(if ϕ then δ1 else δ2, s) ≡(cid:2)Finalwhile ϕ do δ(cid:5), s(cid:2)(cid:3)ϕ[s] ∧ Final(δ1, s)(cid:3), s≡ ¬ϕ[s] ∨ Finalδ(cid:2)(cid:5)(cid:3)(cid:3)¬ϕ[s] ∧ Final(δ2, s)∨Final(δ1|δ2, s) ≡ Final(δ1, s) ∨ Final(δ2, s)Final(π v.δ, s) ≡ ∃xFinal(cid:3)δ vx , s(cid:2)(cid:3)(cid:2)∗Finalδ, s∗Trans and Final enable us to reason about the satisfaction of procedural constraints, similar to the satisfaction of thetemporal constraints expressed as trajectory property formulae described earlier. By using the transitive closure of Trans,, we can define a new predicate Do, which allows one to express the fact that a program δ can terminate indenoted Transsituation s(cid:2)Dowhen executed in situation s:(cid:3)∧ Finaldef= ∃δδ, s, δδ, s, sTrans, s, s(cid:3)(cid:3)(cid:2)(cid:3)(cid:2)(cid:2)∗(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)satisfy δ, or, alternatively, that sWe say that s, sdescribes a complete execution of δ in s. Since in this paper we areonly concerned with finite situation terms, the above is equivalent to saying that there is a sequence of configurations, and for all 1 (cid:2) i < n, D |(cid:12) Trans(δi, si, δi+1, si+1) and D |(cid:12) Final(δn, sn).(δ1, s1), . . . , (δn, sn) such that: δ1 = δ, s1 = s, sn = sGiven that both our preference language and Golog define their semantics over situation trajectories in the situation cal-culus, they can be seamlessly integrated with one another. This enables the specification of preferences over the occurrenceof complex actions, defined as the complete execution of the LPP -programs describing these actions.(cid:5)δ(cid:5)6.2. Preferred programsWe are now ready to define preferences over the occurrence of complex actions. To this end, we extend our preferencelanguage LPP by augmenting the set of trajectory property formulae as follows.Definition 6.1 (Extended Trajectory Property Formula (eTPF)). An extended trajectory property formula is a sentence drawnfrom the smallest set B(cid:5)where:..1. F ⊂ B(cid:5)2. R ⊂ B(cid:5)f ∈ F , then final( f ) ∈ B(cid:5)3.4. If a ∈ A, then occ(a) ∈ B(cid:5)and ϕ25. If ϕ1are..in B(cid:5)M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451329,then so are ¬ϕ1, ϕ1 ∧ ϕ2, ϕ1 ∨ ϕ2, ∃xϕ1, ∀xϕ1, next(ϕ1), always(ϕ1),eventually(ϕ1), and until(ϕ1, ϕ2).6. If δ is an LPP -program and ϕ ∈ B(cid:5), then occC(δ) ∈ B(cid:5)and afterC(δ, ϕ) ∈ B(cid:5).Intuitively, the eTPF occC(δ) states that the program δ is executed starting from the current state. To express that acomplex action δ is executed at some point during a plan, we can use the eventually construct: eventually(occC(δ)). TheeTPF afterC(δ, ϕ) stipulates that if the program δ is executed now, then ϕ holds in the situation where δ terminates. Notethat ϕ may be a temporal formula. For instance, afterC(cook(x), eventually(occ(eat(x)))) describes the set of trajectorieswhere if a dish is cooked, it is eventually eaten afterwards. Using always, it is also possible to state that ϕ holds wheneverδ completes executing (i.e., no matter when execution starts): always(afterC(δ, ϕ)).Extending the semantics of TPFs to eTPFs is straightforward given the above definition of program satisfaction:(cid:4)occC(δ)(cid:4)afterC(δ, ϕ)s, ss, s(cid:5)(cid:5)(cid:5)(cid:5)def=def=(cid:2)(cid:2)∃s1: s (cid:16) s1 (cid:16) s(cid:5)∀s1: s (cid:16) s1 (cid:16) s(cid:5)(cid:3)Do(δ, s, s1)(cid:3)(cid:2)Do(δ, s, s1) → ϕ(cid:5)(cid:3)(cid:5)(cid:4)s1, sNote that temporally extended properties can coexist with the occurrence of complex actions, in which case these prop-erties are also applicable to the actions forming part of the complex action. For instance, the eTPF always(¬cold) ∧eventually(occC(G1)), where (G1) denotes the program specified above, states that eventually the program is executed,while at no time before, during, or after execution, the agent feels cold.Using afterC, we can stipulate, for instance, that if cooking something at home using the above described procedure(G2), the dishes will eventually be cleaned afterwards:(cid:2)(cid:2)G2, eventuallyafterCocc(cleanDishes)(cid:3)(cid:3)The remainder of the hierarchy of preference formulae stays the same, meaning that the change of semantics at the TPFlevel does not require any changes at the higher levels, APFs, GPFs, and AgPFs. To refer to such preference formulae wheneTPFs are used instead of TPFs, we will use the terms eAPFs, eGPFs, and eAgPFs.6.3. Progressing programsThe transition semantics presented in Section 6.1 is a progression of programs in disguise. To make this point clearer, weprovide a formal definition of program progression. In our definition, we make reference to the set(cid:2)(cid:5)(cid:3)δ, do(a, s)=(cid:11)(cid:5)(cid:12)(cid:12) D |(cid:12) Trans(cid:2)∗(cid:5)(cid:3)(cid:13)(cid:5)δ, s, δ, do(a, s)δwhich consists of all possible remaining programs δ(cid:5), after having performed a (sequence of) program transition(s) whoseonly primitive action is the given action a, starting in the given situation s. The reason that there may be a sequence of(program) transitions, rather than just a single transition, is that tests (ψ?) do not change the situation term. Note that inmost cases, the set (cid:5)(cid:5)(δ, s) will either be empty (if there is no possible transition using a) or only contain a single element(when there is a unique possible transition). One rather pathological example in which the set contains more than oneelement is: (cid:5)(cid:5)((a|(a; b)), do(a, S0)) = {nil, b}.Definition 6.2 (Progression of an LPP -program). Let s be a situation, and let δ be an LPP -program. The progression ofoccC(δ) and afterC(δ, ψ) through s is given by:(cid:6)• If ϕ = occC(δ), then ρs(ϕ) =• If ϕ = occCtrans(δ), then ρs(ϕ) =• If ϕ = afterC(δ, ψ), then ρs(ϕ) =• If ϕ = afterCtrans(δ, ψ), then ρs(ϕ) =if D |(cid:12) Final(δ, s);TRUE,occCtrans(δ), otherwise.(cid:9)δ(cid:5)∈(cid:5)(cid:5)(δ,s) ρs(occC(δ(cid:5))).(cid:6)ρs(ψ),afterCtrans(δ, ψ), otherwise.(cid:10)δ(cid:5)∈(cid:5)(cid:5)(δ,s) ρs(afterC(δ(cid:5), ψ)).if D |(cid:12) Final(δ, s);Recall that a disjunction over an empty set is false, whereas a conjunction over an empty set is true. Hence, when thesituation term does not describe an execution of the program δ, then occC(δ) will fail, whereas afterC(δ, ψ) will triviallyhold.In these definitions we make use of auxiliary constructs, similar to occLast, called occCtrans and afterCtrans. These, justlike occLast, are required to do one-step bookkeeping: since the statement occC(a; b) states that the sequence of actions1330M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345a; b is executed in the current situation, and hence regards the future, we need to keep track of this property and evaluate it(partially) in the next step. Thus, e.g., occCtrans(a; b) requires that the last action of the situation term describes a transitionor sequence of transitions of the program, whose only primitive action is a.In both occCtrans and afterCtrans we refer to a set of possible such (partial) program executions. This is mainly for tech-nical reasons. In general, given a sequence of primitive actions, there is only one possible sequence of configurations of theprogram whose contained sequence of situations corresponds to this action sequence. However, in rather pathological cases,there may be several possible such configuration sequences, and hence, several possible remaining programs. For instance,this is the case for the program (a|(a; b)) and the single-action sequence a (cf. above). Here, either the empty program orthe program b remains. Ambiguities of this kind are undesirable in programs, which should be rewritten accordingly – e.g.,to (a; (nil|b)) in our example.6.4. Planning with preferred programsIn order for our planning algorithm to be able to accept preference formulae involving complex actions, we need to de-fine the optimistic/pessimistic satisfaction of extended trajectory property formulae by providing optimistic and pessimisticinterpretations of the new constructs.For the occC construct, we can optimistically assume that any incompletely executed program will eventually be com-pleted.(cid:4)occC(δ)s, s(cid:5)opt def=(cid:5)(cid:2)∃s1 : s (cid:16) s1 (cid:16) s(cid:5)(cid:2)(cid:5)(cid:3)(cid:2)∃δTrans(cid:2)∗δ, s, δ(cid:5), s1(cid:2)(cid:3)∧(cid:3)(cid:2)δ(cid:5), s1Final∨ s1 = s(cid:3)(cid:3)(cid:3)(cid:5)On the other hand, if we are pessimistic, then we would assume that incompletely executed programs will never be com-pleted. Hence, pessimistic evaluation coincides with the original semantics of completed program execution.(cid:4)occC(δ)s, s(cid:5)pess def=(cid:5)(cid:2)∃s1 : s (cid:16) s1 (cid:16) s(cid:3)(cid:5)Do(δ, s, s1)Regarding post-conditions of programs (afterC), the optimistic assumption would be that either the program will notexecute until completion, or, if it already has, to evaluate the condition optimistically.(cid:4)afterC(δ, ψ)s, s(cid:5)opt def=(cid:5)(cid:2)∀s1 : s (cid:16) s1 (cid:16) s(cid:3)(cid:2)(cid:5)Do(δ, s, s1) → ψ(cid:5)opt(cid:3)(cid:5)(cid:4)s1, sThe pessimistic assumption would be that there will be a completed execution after which the condition will not hold.Hence, in order for the condition to be pessimistically satisfied, all possible executions of the program must already termi-nate within the given situation interval.(cid:4)afterC(δ, ψ)(cid:5)pess def=(cid:5)pesss, s(cid:3)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)(cid:5)∀s1 : s (cid:16) s1 (cid:16) s(cid:2)∗(cid:5)(cid:5)∧ (cid:3)δTrans, s(cid:5)Do(δ, s, s1) → ψ(cid:3)(cid:5) (cid:2) s(cid:5)(cid:5)δ, s, δ∧ s, s(cid:5)(cid:5)(cid:2)s1, s(cid:3)(cid:5)(cid:5)Theorem 6.3. Theorem 4.18 continues to hold when ϕ is an eTPF, and Φ is an eGPF.Proof. Refer to Appendix E. (cid:3)Given this theorem, the algorithm described in Section 5 can be readily used to compute preferred plans also for thecase where preferences refer to complex actions, i.e., when preferences are expressed as an extended general preferenceformula.87. Related workThere is growing interest in the issue of how to represent and reason with preferences in AI. In what follows we situatethe work presented in this paper with respect to related work on the representation of preferences, with particular attentionto those designed to represent preferences for planning. We also discuss the literature on the generation of preferred plans,again situating our contributions within the context of this work.7.1. Preference languagesThe literature on preference languages is extensive, much of it originating from the field of economics rather than AI.In comparing our work to other AI preference languages, many of the distinctions we raise relate to whether preferenceformalisms are ordinal, qualitative or quantitative; whether they model temporal preferences or solely static preferences;8 The algorithm can also be easily extended to treat aggregated preference formulae by associating tuples of weights with situations as we explained inSection 5.1.M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451331whether the formalism is propositional or first order; and whether it induces a complete pre-order on the possible outcomesand if not the degree of incomparability in the ordering. In this context, our language is qualitative, models temporalpreferences, is first order, and induces a complete pre-order.As further criteria for comparison, Coste-Marquis et al. [20] evaluate some propositional logic-based preference languageswith respect to expressiveness and succinctness. The issue of expressiveness is concerned with the nature of the pre-ordersthat can be encoded (e.g., all pre-orders, all complete pre-orders). As noted above, LPP , by design, can encode any completepre-order as an atomic preference formula and thus our language can represent all complete pre-orders. We cannot howeverexpress partial (or incomplete) pre-orders. We argue that for preference-based planning the restriction to complete pre-orders is reasonable and even desirable. First, we feel that this restriction is not too great since many preference frameworksadopt this same restriction. For example, in decision theory, the completeness of an agent’s preferences is taken as an axiom[63]. Second, and more importantly, we feel that any disadvantages stemming from the restriction to complete pre-ordersare more than compensated for by the ease of comparison of different plans. Indeed, in some other formalisms wherepartial orders can be encoded, the problem of deciding whether an outcome is preferred to another has been shown to beNP-complete (e.g. [13]).Succinctness evaluates the relative space efficiency of languages, i.e., how succinctly a preference relation (an ordering)can be expressed in a language. In [20], this is done by showing that all orderings expressible in one language can be trans-lated into another language with an at most polynomially large increase in size. With respect to LPP , we can demonstrateusing the following mapping that the preference language R pen [20], which ranks outcomes by the sum of the penalties ofunsatisfied preferences, can be polynomially translated into an equivalent AgPF in our language:(cid:11)(cid:14)αi, G i(cid:15), i = 1, . . . , n(cid:13)(cid:4) sum(cid:2)(cid:3)G 1[0] (cid:18) ¬G 1[αi], . . . , Gn[0] (cid:18) ¬Gn[αi]prio, and R ZThis shows that LPP is at least as succinct as Rpen. Moreover, since there exists polynomial translations of the languagescond into Rpen [20], it follows that LPP is at least as succinct as these languages as well. Overall, theseRboprio, Rlexiare rather positive results: LPP is expressive, being able to generate all complete pre-orders, and is at least as compact asfour of the five preference languages in [20] with the same expressivity.9 We point out however that for domain-dependentapproaches, like our own, comparisons based on domain-independent criteria are less relevant, as the real test is how wellthe language can represent the types of preferences for which it was designed.CP-NetsA widely adopted language for studying user preferences in AI is the propositional CP-nets formalism [13]. CP-netsenable the description of conditional ceteris paribus statements about user preferences (e.g., the user prefers red wine ifmeat is being served and white wine if fish is being served, all other things being equal). User preferences are representedin a graphical notation that is compact and that reflects the conditional independence and dependence of statements.Unlike LPP , CP-nets is restricted to static, ordinal statements about preferences. As such, CP-nets cannot express temporalpreferences, nor can it express relative importance of different preferences. The CP-nets formalism is simple and elegant,however it achieves this at the expense of expressiveness. There is often a high degree of incomparability between differentstates because of the assumption of ceteris paribus. In [64], Wilson extends CP-Nets with stronger statements that enable thestatement of preferences irrespective of the value of other variables. Use of such preference statements supports determining acomplete pre-order on outcomes, which comes closer to the approach proposed in LPP , but is still static and ordinal.QCL, RKBs, and possibilistic logicOther noteworthy work includes that of Brewka on qualitative choice logic (QCL) [17]. This preference framework isdesigned to represent preferences over alternatives and induces a complete pre-order over models. QCL was not developedspecifically for planning and provides a subset of the expressive power of our preference language. In [16], Brewka proposesan ordinal preference language which expresses complex preferences over models in terms of ranked knowledge bases(RKBs). RKBs were originally proposed for default reasoning. In 2006, Feldmann, Brewka, and Wenzel applied this workto planning, proposing two extensions to PDDL that support the definition of preferences using RKBs [29]. In both theseextensions, the preferences are on the final state of a plan. There are no temporally extended preferences. In related earlierwork, Brekwa uses a variant of the QCL language to perform preference-based planning via answer set optimization in alanguage called PLD [15]. The basic elements of PLD are rules that code context-dependent preferences over answer sets.More complex preference formulae are formed using different aggregation operators: sum, (ranked) set inclusion, (ranked)cardinality, pareto, and lexicographic order. Finally, the possibilistic logic approach to preferences [8] is notable in that, likeLPP , it proposes a qualitative preference framework, thus allowing the relative importance of preferences to be specified.The approaches discussed so far do not consider temporal preferences, and hence are unable to express the types ofpreferences that interest us. In what follows, we review some preference languages that have been designed for the task ofpreference-based planning or related tasks.9 We do not know the relationship between LPP and the fifth language R H (which is based on the Hamming distance between models), but most likelythere is no polynomial translation from R H to LPP since R H involves weighted sums, and LPP is not designed to handle any arithmetic operationsbeyond possibly simple sums.1332M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345Planning-oriented preference languagesIn [22,23] Delgrande, Schaub, and Tompits developed a useful framework for expressing preferences for causal reasoningand planning. To this end, they proposed a general query language for histories – sequences of interleaved states andactions. This language is not unlike our TPFs. Building on this, they define a second language that supports the expressionof preferences as a binary relation on histories. From these two base languages, they explored so-called choice and temporalpreferences in further detail, and also extended their language with different aggregate features. One of the argued benefitsof their base framework is its ability to encode other preference languages, and indeed, aside from the obvious distinctionthat our language is first order whereas theirs is propositional, many of the notions and constructs of our language can benicely expressed within this framework. In particular, our TPFs, limited to finite domains, can be encoded in their querylanguage. However our APFs cannot be encoded in their second preference language because of our capacity to denote therelative strength of preferences. Their framework has the capacity to characterize a diversity of aggregation techniques, somesimilar to ours.More generally, the framework proposed in these papers has some fundamental differences to LPP that, in our view,underline the merits of the situation calculus. The histories employed in this framework are finite sequences of alternatingstates and actions. In contrast, the situation calculus foundational axioms induce a tree of situations and, when conjoinedwith a domain-specific action theory, characterize the space of all possible situations that follow from the domain axiomati-zation. As such preferences over situations can be entailed from a domain axiomatization rather than from the comparisonof two specific, finite histories, as in this work.PPMost noteworthy of the related work is that of Son and Pontelli [59,60] who developed a propositional language, PP ,for planning with preferences together with an implementation using answer-set programming (ASP). The original PPlanguage, described in [59], served as a starting point for the development of our language and we adopted their idea ofdefining the language in terms of a hierarchy of formulae, and also adopted some of their nomenclature – BDF (we useTPF), APF, and GPF – and augmenting it with AgPF.Despite the similarity in names, there are significant differences between our preference languages, both in terms ofsyntax and semantics. In particular, our language is first order, which affords us far more compact and simple expressionof preferences. It also enables the expression of preferences over unnamed objects, which is important for online planningwhere groundings may not be known a priori. Planning with Web services is a good example, where the execution of theplan can provide further knowledge of objects that a planner has preferences over (e.g., specific flights or hotels in thecase of Web travel planning). Furthermore, our language is qualitative rather than simply ordinal, allowing us to express,for example, that one TPF (respectively, BDF) is strongly preferred over another, as opposed to just providing an preferenceordering over properties.At the GPF level, our language includes conditional preferences, which are useful (cf. CP-nets). Like PP , our languagehas the notion of General And (Conjunction) and General Or (Disjunction), but we provide a different semantics for theseconstructs. According to PP ’s semantics, in order for a trajectory t1 to be preferred to a trajectory t2 with respect to aGeneral And preference, the trajectory t1 must be strictly preferred to t2 for each of the component preferences. For GeneralOr, they require that t1 be at least as preferred as t2 on all component preferences and strictly preferred to t2 for at leastone component preference. We did not feel that these were natural ways of interpreting conjunction and disjunction. Forexample, one would expect that fully satisfying one of the component preferences should ensure satisfaction of a disjunction,but this does not follow from the PP semantics. In contrast our semantics is more in keeping with the Boolean connectivesthat give these constructs their names. Moreover, our semantics induces a complete pre-order, whereas the semantics ofPP ’s general preferences leads to great incomparability between plans. Finally, at the AgPF level, we provide several furthermethods for aggregating preferences, which those using or reviewing our work have found to be compelling and useful,though our claim of usefulness has not been verified by a usability study. Son and Pontelli have implemented a plannerusing answer-set programming that can be used with a variety of black-box ASP solvers. In their later paper, they alsooverview how to encode their preferences to exploit answer-set optimization engines.PDDL3Also of interest is a comparison of LPP to PDDL3 [34]. Following the description in [36], PDDL3 was developed byGerevini and Long as an extension of the Planning Domain Definition Language, PDDL [48], that provides a rich languagefor defining hard constraints and user preferences for planning. PDDL3 was designed for the 5th International PlanningCompetition (IPC-2006), which was the first international competition to include tracks for preference-based planning.There are a number of commonalities between PDDL3 and LPP but ultimately some fundamental differences. Inparticular, PDDL3 is a quantitative (rather than qualitative) preference language. Plans are evaluated with respect to themaximization (or minimization) of a numeric objective function that is composed of a weighted linear sum of the satisfac-tion or violation of individual preferences. Like LPP , individual preferences in PDDL3 are described as properties of plantrajectories that are either satisfied or violated by a plan. However, preference formulae can be quantified in such a way thata count can be taken of the number of individuals that violate a preference. This is a useful extension that could also beintegrated into LPP . For example, if you have a preference that all ingredients in the meal you are making be fresh, thena plan that uses 5 fresh ingredients and 2 frozen is more desirable than a plan that uses 1 fresh and 6 frozen, even thoughM. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451333neither fully satisfies the preference. This example of course also uncovers a problem that can occur with such counting. Iftwo meals were being compared with different numbers of ingredients, then to achieve the intended interpretation of thecounting, the count would need to be normalized by the total number of ingredients in each of the two different meals –something PDDL3 cannot do. Preferences over plan trajectories in PDDL3 can be temporally extended, though unlike LPP ,PDDL3 does not allow the arbitrary nesting of LTL formulae. It also does not allow for the expression of preferences overaction occurrences, an important feature of LPP . However, PDDL3 has a number of features that LPP does not. In par-ticular, preferences can be related to specific times (e.g., I would like to eat dinner between 8 PM and 9 PM). There arealso precondition preferences, which are state preferences that are desirable to hold in the state in which an action is beingexecuted. One could use these to specify a soft precondition, and record the number of times an action is executed withoutthis preference being satisfied.Recently PDDL3 was extended to include preferences over decompositions of hierarchical task network (HTN) tasks [56].This work and its application to web service composition further motivates the conceptually related extension of LPP withcomplex actions.OtherFinally, there has been a variety of work that uses quantitative preferences for planning or temporal reasoning. Thisincludes Eiter et al.’s work on answer set planning with respect to plan length and numeric action costs [27], work by Rossiand colleagues on reasoning with temporal soft constraints [65], Haddawy and Hanks’ early work using a decision-theoreticutility function to guide planning [38], and of course the extensive research on decision-theoretic planning and MDPs [52].The quantitative nature of these frameworks makes preference elicitation difficult. This is why in our own work we decidedto focus on qualitative preferences, which are more expressive than ordinal preferences yet much easier to elicit thanquantitative preferences. As a useful middle ground, Fritz and McIlraith integrate qualitative and quantitative preferenceswithin an agent programming framework. The authors express their qualitative preferences in a restricted version of LPP[31].7.2. Preference-based plannersThe previous subsection noted some efforts to generate preferred plans, related to the specific languages describedabove. Here we provide a broad overview of some other noteworthy work in the development of preference-based planners.Detailed descriptions of many of these planners is provided in a survey article on preference-based planning by Baier andMcIlraith [6]. The interested reader is directed there for further detail.Work on decision-theoretic planning notwithstanding, one of the first pieces of work on generating preferred plans wasthat of Myers and colleagues at SRI on advisable planners. Myers and Lee [50] proposed a means of generating preferredplans via biases that guided a planner towards plans with certain attributes. This was followed, in and around 2004, bywork on the related problem of partial satisfaction planning (PSP), also called over-subscription planning (e.g., [62,55]).[9],Kambhampati and colleagues have developed a number of PSP planner including Sapaand bbop-lp [10]. In these PSP planners, the planning problem is cast as the task of finding a plan of maximal benefit, givenan association of utility to facts, and costs to actions. The planners differ in how they search for such solutions alternativelyusing forward-chaining, backward-chaining, and incremental branch-and-bound with linear programming.[62], Yochan[62], AltAltPSPSPSAs observed in Section 1, in 2006 the biennial International Planning Competition included a track on planning withpreferences specified in PDDL3. This resulted in the development of several highly optimized preference-based planners.The planners were differentiated with respect to the complexity of the preferences they could handle, starting with final-state preferences, adding temporally extended preferences, and finally extended to include more complex metric preferences.Most planners used some form of heuristic search in order to compute preferred plans. The best comparators to PPLAN arethe planners that could plan with temporally extended preferences. HPlan-P by Baier et al. [4] is one such planner. InHPlan-P, temporally extended preferences are compiled to final-state preferences by representing them as parameterizednondeterministic finite state automata. Planning is performed via branch and bound search, incrementally generating plansof increasing quality. HPlan-P uses a portfolio of admissible and inadmissible heuristics to guide search, together with anadmissible heuristic to soundly prune partial plans that were of poorer quality than the plan previously computed. Bypruning the search space, HPlan-P is able, in some cases, to search the space exhaustively and thus guarantee optimality.Also of note are the two planners by Edelkamp and colleagues: mips-bdd [24] and mips-xxl [25]. The former is an optimalplanner that applies bidirectional breadth-first search, encoding states as binary decision diagrams. The latter is a heuristicplanner based on enforced hill climbing. Both compile temporally extended preferences to grounded Büchi automata so theycan be treated as final-state preferences.Finally, SGPlan5 [41] is also a search-based planner that can plan with temporally extended preferences. Unlike theplanners described above, SGPlan5 searches for a plan by using constraint partitioning, decomposing the original planningproblem into several sub-problems. This technique stems from treating the preference-based planning problem as a standardoptimization problem, where the objective function is to minimize the makespan of the plan.We would be remiss not to mention two other related efforts to build preference-based planners – one using a con-straint satisfaction problem (CSP) solver, and one using a satisfiability (SAT) solver. Like PPLAN, both of these planners arek-optimal. Similarly, neither of these planners can compete with the above state-of-the-art competition-optimized planners.1334M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345However, in contrast to PPLAN, neither of these planners can plan with temporally extended preferences. In 2005, Brafmanand Chernyavsky developed PrefPlan, a preference-based planner using a CSP solver [14]. Preferences were specified overpossible goal states using TCP-nets. A TCP-net is a tradeoff-enhanced CP-net, which allows the user to express prioritiesbetween variables. The most notable limitation of TCP-nets relative to LPP is that they cannot express temporal prefer-ences, and suffer incomparability of states, just as CP-nets do. Their approach to planning is to compile the problem intoan equivalent CSP problem, imposing variable instantiation constraints on the CSP solver, according to the TCP-net. This is apromising method for planning, though it is not clear how it will extend to temporal preferences.satplan(P) [35] by Giunchiglia and Maratea is an extension of the award-winning Satplan planner [42] that is able toplan with final-state preferences by calling an external SAT solver. The approach is similar to PrefPlan in the sense that avariable ordering is imposed on propositional variables corresponding to final-state preferences in such a way that most-preferred plans will be explored first by the SAT solver. Preferences in satplan(P) can be defined either in a qualitative or aquantitative language. In the qualitative language, the preference ordering of plans is induced from a partial order betweenproperties of the final state. In the quantitative language, on the other hand, each preference on the final state has anassociated weight.8. Closing remarksIn this paper we addressed the problem of preference-based planning. To this end, we proposed LPP , an expressivefirst-order language for specifying domain-specific, qualitative user preferences. LPP supports the expression of temporallyextended preferences over states as well as over actions; the actions can be either primitive or complex, in the form of Gologcomplex actions. In contrast to many ordinal or qualitative preference formalisms that yield significant incomparability,LPP provides a complete ordering over plans, which is computationally advantageous for preference-based planning. LPPalso supports specification of the relative strength of a user’s preferences. This is acknowledged by a number of practitionersto be a desirable property of preference languages for real-world applications. The semantics of LPP is described in thesituation calculus. In the situation calculus, each executable situation corresponds to a (possible, partial) plan, and since allexecutable situations are described within one model of the domain theory, it means that preference for one situation overanother can be expressed within the language. The situation calculus semantics facilitated the extension of LPP to Gologcomplex actions.The LPP language, as proposed in [11], has already begun to garner interest from researchers. For example, Fritz andMcIlraith combined LPP with quantitative preferences represented through utility functions within an agent programminglanguage [31]. The resulting program then searched for the quantitatively optimal plan within the space of qualitativelybest plans. LPP has also been exploited in diverse applications including the specification of user preferences for thecustomization of web service composition where it was integrated with a Golog interpreter by Sohrabi et al. [58], andthe specification of goals for (software) requirements engineering by Liaskos et al. (e.g., [45,46]). Giunchiglia and Marateadiscuss the use of LPP in order to extend their work on preference-based planning in satplan(P) with temporally extendedpreferences [35], and Sohrabi and McIlraith integrated LPP into an HTN planner [57]. The extension of LPP to includepreferences over complex actions was not included in any of the above works, but is relevant and would enhance each ofthese applications.In addition to LPP , we proposed an approach to computing preferred plans via bounded best-first search in a forwardchaining planner. Key components of our approach were the exploitation of progression to efficiently evaluate levels ofpreference satisfaction with respect to partial plans, and development of an admissible evaluation function that guaranteesthe optimality of best-first search. We have implemented our planner, PPLAN, and evaluated it experimentally. PPLAN waswritten in Prolog and was not intended to be a state-of-the-art preference-based planner, nor does it perform as one interms of planning time. Nevertheless, experimental evaluation demonstrated that our admissible evaluation function wasinformative, generally expanding far fewer nodes than breadth first search. Also, in contrast to state-of-the-art IPC planners,PPLAN always returns an optimal plan.While PPLAN itself is not a highly optimized planner, aspects of the PPLAN approach are already starting to have someimpact. In particular, the heuristic of our evaluation function (as reported in [11]) has been exploited by HPlan-P, the state-of-the-art preference-based planner that received distinguished mention at IPC-2006. HPlan-P used our heuristic as one ofa portfolio of different heuristics applied to different IPC domains. No one heuristic strategy worked best for all domains,but our heuristic was the best in one of six domains, and was used successfully in combination with other inadmissibleheuristics in other domains [4]. Perhaps more importantly, in cases where HPlan-P used a more informative inadmissibleheuristic to guide search, our (admissible) heuristic was used to soundly prune inferior partial plans as part of a branchand bound search strategy. This enabled HPlan-P to significantly reduce the search space and thus, in some cases, to searchexhaustively for a plan that was provably optimal.The work presented in this paper provides a formal foundation for specifying and generating preference-based plans.Although the work has its basis in the situation calculus, the language and the approach to planning are amenable tointegration with several existing planners, and beyond planning can be used to support a diversity of reasoning tasks thatemploy preferences.M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451335AcknowledgementsWe gratefully acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC)through its Discovery grant program and its Undergraduate Student Research Award (USRA) program. We would like tothank the anonymous referees for their detailed and thorough comments, and Jérôme Lang for helpful comments on anearlier paper describing some of this work. Finally, we gratefully acknowledge Shirin Sohrabi Araghi for her work on theimplementation of PPlan.Appendix A. Axiomatization of the dinner exampleHere we provide a formal axiomatization of the dinner domain as a basic action theory D of the situation calculus.A STRIPS-style specification of the domain can be found at [12]. Axioms marked with (†) are presented for expositionpurposes only, but were not used in the experiments. Recall that a basic action theory comprises four sets of domain-dependent axioms: action precondition axioms, Dap , successor state axioms, DSS, axioms describing the initial situation S0,DS0 , and a set of unique name axioms for actions, Duna. The latter is straightforward to define, and is not shown here.Action precondition axioms (Dap ). These take the form Poss( A((cid:3)x), s) ≡ Π A((cid:3)x, s) where Π A((cid:3)x, s) is a formula with freevariables among (cid:3)x, s. Following the notational convention established by Reiter [53], all free variables in situation calculusaxioms are assumed to be universally quantified from the outside, unless otherwise noted.(cid:2)Poss(†)Possdrive(x, y), s(cid:2)walk(x, y), s(cid:2)Posscook(x), s(cid:2)Posseat(x), sbuyIngredients(x), s(cid:2)Poss(cid:2)orderTakeout(x, y), sPoss(cid:2)orderRestaurant(x, y), sPoss(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)≡ location(x) ∧ location( y) ∧ x (cid:11)= y ∧ at(x, s)≡ location(x) ∧ location( y) ∧ x (cid:11)= y ∧ at(x, s)≡ meal(x) ∧ knowsHowToMake(x) ∧ at(home, s)∧ hasIngredients(x, s) ∧ kitchenClean(s)(cid:2)(cid:2)(cid:3)(cid:3)∃ y≡ meal(x) ∧at( y, s) ∧ readyToEat(x, y, s)≡ meal(x) ∧ ¬hasIngredients(x) ∧ at(store, s)≡ meal(x) ∧ takeOutRest( y) ∧ onMenu(x, y) ∧ at(home, s)≡ meal(x) ∧ dineInRest( y) ∧ onMenu(x, y) ∧ at( y, s)Poss(cleanDishes, s) ≡ at(home, s)Effect axioms, which can be translated into successor state axioms (DSS) using Reiter’s solution to the frame problem [53,pp. 30–32]. We provide effect axioms rather than successor state axioms, as they tend to be easier to understand for humanreaders. These take either the positive form γ +F (a, (cid:3)x, s) → F ((cid:3)x, do(a, s)), or the negative form γ −F (a, (cid:3)x, s) → ¬F ((cid:3)x, do(a, s)),where γ +state the conditions under which action a makes fluent F true, respectively false, when executed fromthe situation s.and γ −(cid:3)do(a, s)(cid:3)x, home, do(a, s)(cid:3)x, do(a, s)(cid:2)(cid:2)(cid:3)y, do(a, s)(cid:2)(cid:2)(cid:3)x, do(a, s)(cid:3)y, do(a, s)(cid:2)(cid:3)x, do(a, s)(cid:2)a = drive(x, y) → ata = drive(x, y) → ¬at(†)a = walk(x, y) → at(†)a = walk(x, y) → ¬at(cid:2)(†)isSnowing(s) ∧ a = walk(x, y) → colda = cook(x) → readyToEata = cook(x) → ¬hasIngredients(cid:2)a = cook(x) → ¬kitchenClean(cid:3)a = eat(x) → sateddo(a, s)at( y, s) ∧ a = eat(x) → ¬readyToEata = buyIngredients(x) → hasIngredients(cid:2)a = orderTakeout(x, y) → readyToEata = orderRestaurant(x, y) → readyToEata = cleanDishes → kitchenClean(cid:3)do(a, s)(cid:2)(cid:2)(cid:2)(cid:2)(cid:3)x, y, do(a, s)(cid:3)x, do(a, s)(cid:2)(cid:3)x, home, do(a, s)(cid:3)x, y, do(a, s)(cid:3)do(a, s)1336M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345Initial theory DS0 . Unless otherwise stated in the text, we use the following values of fluents in the initial state (making aclosed world assumption):at(home, S0) kitchenClean(S0) hasIngredients(crêpes, S0)In addition, we include in DS0 the following axioms about situation-independent relations:• Mealsmeal(x) ≡ x = pizza ∨ x = tacos ∨ x = fajitas ∨ x = spaghetti∨ x = sweetsourpork ∨ x = crêpes ∨ x = duck ∨ x = salad• Types of mealsvegetarian(x) ≡ x = saladitalian(x) ≡ x = spaghetti ∨ x = pizzamexican(x) ≡ x = tacos ∨ x = fajitasfrench(x) ≡ x = crêpes ∨ x = duckchinese(x) ≡ x = sweetsourpork• Locationslocation(x) ≡ x = home ∨ x = store ∨ x = italianRest∨ x = frenchRest ∨ x = chineseRest ∨ x = pizzaPlace(†)close(x, y) ≡ x = home ∧ y = italianRest• Types of restaurantstakeOutRest(x) ≡ x = chineseRest ∨ x = pizzaPlacedineInRest(x) ≡ x = italianRest ∨ x = frenchRest• Restaurant offeringsonMenu(x, y) ≡ y = italianRest ∧ (x = spaghetti ∨ x = pizza)∨ y = frenchRest ∧ (x = crêpes ∨ x = duck)∨ y = pizzaPlace ∧ x = pizza∨ y = chineseRest ∧ x = sweetsourpork• Knowledge of recipesknowsHowToMake(x) ≡ x = crêpes ∨ x = spaghetti ∨ x = tacos∨ x = fajitas ∨ x = saladAppendix B. Proof of Theorem 4.13Proof. The proof proceeds by induction on the structural complexity of ϕ. We assume throughout, unless stated otherwise,that we are given two situations s1 and s2 = do([a1, . . . , an], s1), where n (cid:3) 1 and s2 = do(an, s3).Case 1. ϕ = f ∈ FD |(cid:12) f [s1, s2] iff D |(cid:12) f [s1]iff ρs1 ( f ) = TRUEiff ρ∗s1,s3iff D |(cid:12) ρ∗( f ) = TRUEs1,s3( f )[s2, s2]M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451337The equivalence in line 1 follows from the semantics of TPFs. For the forward direction of the equivalence between lines 1to 2, we use Definition 4.8, and for the backwards direction, we use the fact that a progressed fluent equals TRUE only ifthe fluent is satisfied in the situation. The forwards directions of the equivalences between lines 2 and 3 and lines 3 and 4are obvious, whereas for the backwards directions, we use the fact that progressed fluents are either TRUE or FALSE, plusthe fact that TRUE and FALSE are unaffected by progression.Case 2. ϕ = r ∈ RD |(cid:12) r[s1, s2] iff D |(cid:12) riff ρs1 (r) = TRUEiff ρ∗s1,s3iff D |(cid:12) ρ∗(r) = TRUEs1,s3(r)[s2, s2]For backwards direction, we make use of the fact that non-fluent relations are progressed to either TRUE or FALSE, whichare unaffected by further progression.Case 3. ϕ = occ(a)We prove this case in two steps, first for the case where n = 1 and then the case where n (cid:3) 2.(a) n = 1(b) n (cid:3) 2D |(cid:12) occ(a)[s1, s2] iff s2 = do(a, s1)iff D |(cid:12) occLast(a)[s2, s2](cid:2)iff D |(cid:12) ρ∗(cid:3)occ(a)[s2, s2]s1,s1D |(cid:12) occ(a)[s1, s2] iff D |(cid:12) do(a, s1) (cid:16) s2(cid:3)(cid:5)(cid:5)do(a1, s1), s2= TRUE= TRUE(cid:3)(cid:3)(cid:5)(cid:2)iff do(a1, s1) = do(a, s1)(cid:2)do(a1, s1) = doiff ∃sa, s(cid:4)iff D |(cid:12) occLast(a)iff ρdo(a1,s1)iff ρdo(a1,s1)iff ρ∗s1,s3iff D |(cid:12) ρ∗(cid:3)occLast(a)ρs1(cid:3)occ(a)occ(a)= TRUE(cid:3)occ(a)(cid:2)(cid:2)(cid:2)(cid:2)s1,s3[s2, s2]For the backwards direction, we make use of the fact that the progression of occ(a) through two situations yields eitherTRUE or FALSE, plus the fact that TRUE and FALSE are unchanged by progression.Case 4. ϕ = occLast(a)D |(cid:12) occLast(a)[s1, s2] iff D |(cid:12) ∃s(cid:2)(cid:5)(cid:5)(cid:3)s1 = do(cid:3)occLast(a)(cid:2)a, s= TRUE(cid:3)occLast(a)(cid:2)= TRUE(cid:3)occLast(a)(cid:2)iff ρs1iff ρ∗s1,s3iff D |(cid:12) ρ∗s1,s3[s2, s2]For the backwards direction, we utilize the fact that occLast(a) progresses to a Boolean constant TRUE or FALSE, which isthen stable under further progression.Case 5. ϕ = final( f ) for some TPF f ∈ FD |(cid:12) final( f )[s1, s2] iff D |(cid:12) f [s2]iff D |(cid:12) final( f )[s2, s2](cid:3)iff D |(cid:12) ρ∗[s2, s2]final( f )(cid:2)s1,s31338M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345Case 6. ϕ = ¬ψ for some TPF ψWe assume the result for ψ and show that the result holds for ¬ψ .D |(cid:12) ¬ψ[s1, s2] iff D (cid:11)|(cid:12) ψ[s1, s2]iff D (cid:11)|(cid:12) ρ∗s1,s3iff D |(cid:12) ¬ρ∗iff D |(cid:12) ρ∗s1,s3(ψ)[s2, s2]s1,s3(ψ)[s2, s2](¬ψ)[s2, s2]For the equivalence in line 1, we make use of the fact that the action theory D provides complete information about theinitial situation, which means in particular that for every situation calculus formula γ , either D |(cid:12) γ or D |(cid:12) ¬γ .Case 7. ϕ = ψ1 ∧ ψ2 for TPFs ψ1 and ψ2We assume the result for ψ1 and ψ2 and show that the result holds for ψ1 ∧ ψ2.D |(cid:12) ψ1 ∧ ψ2[s1, s2] iff D |(cid:12) ψ1[s1, s2] and D |(cid:12) ψ2[s1, s2](ψ1)[s2, s2] and D |(cid:12) ρ∗(cid:3)(ψ1) ∧ ρ∗(ψ2)(ψ1 ∧ ψ2)[s2, s2]iff D |(cid:12) ρ∗s1,s3(cid:2)ρ∗iff D |(cid:12)s1,s3iff D |(cid:12) ρ∗s1,s3s1,s3[s2, s2]s1,s3(ψ2)[s2, s2]Case 8. ϕ = ψ1 ∨ ψ2. As ψ1 ∨ ψ2 ≡ ¬(¬ψ1 ∧ ¬ψ2), this follows immediately from cases 6 and 7.Case 9. ϕ = ∃xψWe assume that the result holds for all TPFs of lower structural complexity than ϕ. In particular this means that we canassume the result for the TPFs ψ c/x.D |(cid:12) ∃xψ[s1, s2] iff there exists c ∈ C such that D |(cid:12) ψ c/x[s1, s2](cid:3)ψ c/xiff there exists c ∈ C such that D |(cid:12) ρ∗(cid:2)ψ c/xiff D |(cid:12)(cid:14)s1,s3(cid:2)(cid:3)[s2, s2][s2, s2](cid:15) (cid:14)ρ∗s1,s3(cid:15)c∈Ciff D |(cid:12) ρs3iff D |(cid:12) ρs3iff D |(cid:12) ρ∗s1,s3. . . ρdo(a1,s1)(cid:2)(cid:2). . . ρdo(a1,s1)(∃xψ)[s2, s2](cid:16)(cid:3)(cid:2)ψ c/x(cid:16). . .[s2, s2]ρs1c∈C(cid:3)ρs1 (∃xψ)(cid:3). . .[s2, s2]Note that the backwards direction of the equivalence between lines 2 and 3 uses the fact that D completely defines theinitial situation.Case 10. ϕ = ∀xψ . As ∀xψ ≡ ¬∃x¬ψ , this follows directly from Cases 6 and 9.Case 11. ϕ = next(ψ)We proceed by induction on n, the difference in length between s1 and s2. Our base case is n = 1, i.e., s2 = do(a1, s1):(cid:4)D |(cid:12) next(ψ)(cid:5)s1, do(a1, s1)(cid:5)(cid:4)do(a1, s1), do(a1, s1)iff D |(cid:12) ψiff D |(cid:12) ρ∗(cid:2)next(ψ)s1,s1(cid:3)(cid:4)(cid:5)do(a1, s1), do(a1, s1)Next we assume the result for all pairs of situations s1 and s2 = do([a1, . . . , an], s1) with n < k, and we demonstrate theresult for the case where n = k:D |(cid:12) next(ψ)[s1, s2] iff D |(cid:12) ψiff D |(cid:12) ρ∗iff D |(cid:12) ρ∗iff D |(cid:12) ρ∗(cid:4)do(a1, s1), s2(cid:5)do(a1,s1),s3do(a1,s1),s3(cid:2)(ψ)[s2, s2](cid:2)(cid:2)ρs1(cid:3)next(ψ)next(ψ)[s2, s2](cid:3)(cid:3)s1,s3[s2, s2]M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451339Case 12. ϕ = always(ψ)We assume the result for ψ and prove that the result also holds for always(ψ). The proof proceeds by induction on n, thedifference in length between s1 and s2, with base case n = 1:(cid:4)D |(cid:12) always(ψ)(cid:5)s1, do(a1, s1)(cid:4)(cid:5)s1, do(a1, s1)(cid:4)(ψ)(cid:5)(cid:4)do(a1, s1), do(a1, s1)∧ ψ(cid:5)do(a1, s1), do(a1, s1)(cid:5)(cid:4)do(a1, s1), do(a1, s1)∧ ψs1,s1ρs1 (ψ) ∧ always(ψ)always(ψ)We now suppose that the theorem holds for n < k, and we show that it is also true when s2 = do([a1, . . . , ak], s1):(cid:5)do(a1, s1), do(a1, s1)(cid:5)do(a1, s1), do(a1, s1)s1,s1(cid:3)(cid:4)(cid:3)(cid:4)(cid:2)iff D |(cid:12) ψiff D |(cid:12) ρ∗(cid:2)iff D |(cid:12)iff D |(cid:12) ρ∗(cid:4)D |(cid:12) always(ψ)[s1, s2] iff D |(cid:12) ψ[s1, s2] ∧ always(ψ)(ψ)[s2, s2] ∧ ρ∗(cid:3)ρs1 (ψ) ∧ always(ψ). . . ρdo(a1,s1)(cid:3)ρs1. . . ρdo(a1,s1). . .always(ψ)(cid:2)(cid:3)[s2, s2]always(ψ)iff D |(cid:12) ρ∗iff D |(cid:12) ρs3iff D |(cid:12) ρs3iff D |(cid:12) ρ∗do(a1, s1), s2(cid:2)do(a1,s1),s3s1,s3(cid:2)(cid:3)(cid:3)(cid:2)(cid:2)(cid:2)(cid:2)(cid:5)(cid:3)always(ψ)(cid:3). . .[s2, s2]s1,s3[s2, s2][s2, s2]Case 13. ϕ = eventually(ψ). Given that eventually(ψ) can be rewritten as ¬always(¬ψ), this case follows immediatelyfrom cases 6 and 12.Case 14. ϕ = until(ψ1, ψ2)We assume the result for ψ1 and ψ2 and prove that the result also holds for until(ψ1, ψ2). We prove this by induction onn, the difference in length between s1 and s2. Our base case is s2 = do(a1, s1):(cid:5)s1, do(a1, s1)(cid:4)D |(cid:12) until(ψ1, ψ2)iff D |(cid:12) ∃s(cid:2)(cid:4)iff D |(cid:12) ψ2iff D |(cid:12) ρ∗(cid:5)s, do(a, s1)(cid:5)(cid:5)(cid:3)(cid:3)s(cid:3)(cid:2)(cid:2), do(a, s1)∧ ∀s(cid:4)s1 (cid:16) s ∧ s (cid:16) do(a, s1) ∧ ψ2(cid:4)(cid:5) (cid:2) s(cid:5) ∧ s(cid:5)→ ψ1s1 (cid:16) s(cid:4)(cid:2)(cid:5)∨s1, do(a1, s1)(cid:4)(ψ2)s1,s1(cid:4)∧ ψ2(cid:2)ρs1 (ψ2) ∨do(a1, s1), do(a1, s1)(cid:5)s1, do(a1, s1)(cid:5)ρ∗do(a1, s1), do(a1, s1)s1,s1(cid:3)(cid:3)(cid:4)ψ1(cid:5)(cid:3)∨(cid:2)(cid:2)iff D |(cid:12)iff D |(cid:12) ρs1iff D |(cid:12) ρ∗(cid:2)ρs1 (ψ1) ∧ until(ψ1, ψ2)(cid:5)do(a1, s1), do(a1, s1)(cid:5)do(a1, s1), do(a1, s1)(cid:3)(cid:4)(cid:3)(cid:4)until(ψ1, ψ2)(cid:2)(cid:5)do(a1, s1), do(a1, s1)∧ ψ2(cid:4)do(a1, s1), do(a1, s1)(cid:5)(cid:4)do(a1, s1), do(a1, s1)(ψ1)(cid:5)(cid:3)until(ψ1, ψ2)For the equivalence above between lines 1 and 2, we use the fact that either s = s1 or s = do(a1, s1). In the former case, thefirst line simplifies to ψ2[s1, do(a1, s1)], while in the latter case, we obtain ψ2[do(a1, s1), do(a1, s1)] ∧ ψ1[s1, do(a1, s1)].s1,s1We now prove the result for the case where n = k, under the assumption that the result holds in the case where n < k:D |(cid:12) until(ψ1, ψ2)[s1, s2] iff D |(cid:12) ∃s(cid:2)(cid:5)s(cid:4)(cid:3)(cid:2)(cid:2)(cid:5)(cid:3)(cid:3), s2(cid:5) (cid:2) ss1 (cid:16) s ∧ s (cid:16) s2 ∧ ψ2[s, s2](cid:5) ∧ s(cid:5)→ ψ1∧ ∀ss1 (cid:16) s(cid:4)(cid:2)iff D |(cid:12) ψ2[s1, s2] ∨ψ1[s1, s2] ∧ until(ψ1, ψ2)do(a1, s1), s2iff D |(cid:12) ρ∗s1,s3(cid:2)ρ∗∨s1,s3(cid:2). . . ρdo(a1,s1)(cid:2). . . ρs1(cid:2)(cid:3)until(ψ1, ψ2)do(a1,s1),s3(cid:3)ρs1 (ψ2) ∧ until(ψ1, ψ2)(cid:3)(ψ1)[s2, s2] ∧ ρ∗(cid:2)(cid:2)(cid:3)until(ψ1, ψ2)(ψ2)[s2, s2][s2, s2](cid:3)[s2, s2](cid:3)∨ ρs1 (ψ2)(cid:5)(cid:3)(cid:2)(cid:2). . .(cid:3)until(ψ1, ψ2). . .[s2, s2]iff D |(cid:12) ρs3iff D |(cid:12) ρs3iff D |(cid:12) ρ∗s1,s3(cid:3)[s2, s2]Note that the equivalence between lines 1 to 2 follows from the fact that either s = s1, in which case line 1 simplifies toψ2[s1, s2], or s (cid:11)= s1 in which case line 1 gives us(cid:5)(cid:5)(cid:3)(cid:3)(cid:2)(cid:2)(cid:2)(cid:3)(cid:4)(cid:5)∃sdo(a1, s1) (cid:16) s ∧ s (cid:16) s2 ∧ ψ2[s, s2] ∧ ∀ss1 (cid:16) s(cid:5) ∧ s(cid:5) (cid:2) s→ ψ1s, s2which is another way to write ψ1[s1, s2] ∧ until(ψ1, ψ2)[do(a1, s1), s2].1340M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345Case 15. ϕ = TRUE or ϕ = FALSEObvious as D |(cid:12) TRUE and D (cid:11)|(cid:12) FALSE, and TRUE and FALSE are unaffected by progression. (cid:3)Appendix C. Proof of Theorem 4.18Lemma C.1. Let s be any situation, sn = do([a1, . . . , an], s), n (cid:3) 0, a collection of situations, and ϕ a TPF. Then for any 0 (cid:2) i (cid:2) j (cid:2) n:D |(cid:12) ϕ[s, s j]opt implies D |(cid:12) ϕ[s, si]opt, and D (cid:11)|(cid:12) ϕ[s, s j]pess implies D (cid:11)|(cid:12) ϕ[s, si]pess.Proof. The proof proceeds by induction over the structure of trajectory property formulae. Clearly, for ϕ ∈ F , ϕ ∈ R, andϕ = final(ψ) the assumption holds. Also trivial is the case for ϕ = occ(a): if D |(cid:12) occ(a)[s, s j]opt then either j = 0 whichentails i = 0, or a = a1. In both cases we have D |(cid:12) occ(a)[s, si]opt. Similarly, for the pessimistic case, if D (cid:11)|(cid:12) occ(a)[s, s j]pessthen either j = 0 which entails i = 0, or a (cid:11)= a1. In both cases we have D (cid:11)|(cid:12) occ(a)[s, si]pess.Now suppose the assumption holds for the TPFs ϕ1, ϕ2. Then• For conjunction:D |(cid:12) (ψ1 ∧ ψ2)[s, s j]opt ⇒ D |(cid:12) ψ1[s, s j]opt and D |(cid:12) ψ2[s, s j]opti.h.⇒ D |(cid:12) ψ1[s, si]opt and D |(cid:12) ψ2[s, si]opt ⇒ D |(cid:12) (ψ1 ∧ ψ2)[s, si]optD (cid:11)|(cid:12) (ψ1 ∧ ψ2)[s, s j]pess ⇒ D (cid:11)|(cid:12) ψ1[s, s j]pess or D (cid:11)|(cid:12) ψ2[s, s j]pessi.h.⇒ D (cid:11)|(cid:12) ψ1[s, si]pess or D (cid:11)|(cid:12) ψ2[s, si]pess ⇒ D (cid:11)|(cid:12) (ψ1 ∧ ψ2)[s, si]pess• For disjunctions:D |(cid:12) (ψ1 ∨ ψ2)[s, s j]opt ⇒ D |(cid:12) ψ1[s, s j]opt or D |(cid:12) ψ2[s, s j]opti.h.⇒ D |(cid:12) ψ1[s, si]opt or D |(cid:12) ψ2[s, si]opt ⇒ D |(cid:12) (ψ1 ∨ ψ2)[s, si]optD (cid:11)|(cid:12) (ψ1 ∨ ψ2)[s, s j]pess ⇒ D (cid:11)|(cid:12) ψ1[s, s j]pess and D (cid:11)|(cid:12) ψ2[s, s j]pessi.h.⇒ D (cid:11)|(cid:12) ψ1[s, si]pess and D (cid:11)|(cid:12) ψ2[s, si]pess ⇒ D (cid:11)|(cid:12) (ψ1 ∨ ψ2)[s, si]pessWe remark that the implication from D |(cid:12) (ψ1 ∨ ψ2)[s, s j]opt to D |(cid:12) ψ1[s, s j]opt or D |(cid:12) ψ2[s, s j]opt in line 1 followsfrom the fact that D completely defines the initial situation.• For negation:D |(cid:12) ¬ϕ1[s, s j]opt ⇒ D (cid:11)|(cid:12) ϕ1[s, s j]pess i.h.⇒ D (cid:11)|(cid:12) ϕ1[s, si]pess ⇒ D |(cid:12) ¬ϕ1[s, si]optD (cid:11)|(cid:12) ¬ϕ1[s, s j]pess ⇒ D |(cid:12) ϕ1[s, s j]opt i.h.⇒ D |(cid:12) ϕ1[s, si]opt ⇒ D (cid:11)|(cid:12) ¬ϕ1[s, si]pessNote that to go from D (cid:11)|(cid:12) ¬ϕ1[s, si]pess to D |(cid:12) ¬ϕ1[s, si]opt in line 1, and from D (cid:11)|(cid:12) ¬ϕ1[s, s j]pess to D |(cid:12) ϕ1[s, s j]opt inline 2, we leverage the fact that D contains complete information about the initial situation.• For next:D |(cid:12) next(ϕ1)[s, s j]opt ⇒ D |(cid:12) ϕ1[s1, s j]opt or j = 0(by i.h. and since i (cid:2) j) ⇒ D |(cid:12) ϕ1[s1, si]opt or i = 0 ⇒ D |(cid:12) next(ϕ1)[s, si]optD (cid:11)|(cid:12) next(ϕ1)[s, s j]pess ⇒ D (cid:11)|(cid:12) ϕ1[s1, s j]pess or j = 0(by i.h. and since i (cid:2) j) ⇒ D (cid:11)|(cid:12) ϕ1[s1, si]pess or i = 0 ⇒ D (cid:11)|(cid:12) next(ϕ1)[s, si]pess• The cases for always(ϕ), eventually(ϕ), and until(ϕ1, ϕ2) follow by induction hypothesis and the cases for “∧”, “∨”,and next(ϕ). (cid:3)Lemma C.2. Let s be any situation, sn = do([a1, . . . , an], s), n (cid:3) 0, a collection of situations, and ϕ a TPF. Then for any 0 (cid:2) i (cid:2) j (cid:2) n:D |(cid:12) ϕ[s, s j] implies D |(cid:12) ϕ[s, si]opt and D (cid:11)|(cid:12) ϕ[s, s j] implies D (cid:11)|(cid:12) ϕ[s, si]pess.Proof. The proof of this lemma proceeds analogously to the previous one. Again it is clear that for ϕ ∈ F , ϕ ∈ R, andϕ = final(ψ) the assumption holds. Also trivial is the case in which ϕ = occ(a): if D |(cid:12) occ(a)[s, s j] then a = a1 and thusD |(cid:12) occ(a)[s, si]opt. And, for the pessimistic case, if D (cid:11)|(cid:12) occ(a)[s, s j] then either j = 0 which entails i = 0, or a (cid:11)= a1. Inboth cases we have D (cid:11)|(cid:12) occ(a)[s, si]pess.Now suppose the assumption holds for TPFs ϕ1, ϕ2. ThenM. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451341• For conjunction:D |(cid:12) (ψ1 ∧ ψ2)[s, s j] ⇒ D |(cid:12) ψ1[s, s j] and D |(cid:12) ψ2[s, s j]i.h.⇒ D |(cid:12) ψ1[s, si]opt and D |(cid:12) ψ2[s, si]opt ⇒ D |(cid:12) (ψ1 ∧ ψ2)[s, si]optD (cid:11)|(cid:12) (ψ1 ∧ ψ2)[s, s j] ⇒ D (cid:11)|(cid:12) ψ1[s, s j] or D (cid:11)|(cid:12) ψ2[s, s j]i.h.⇒ D (cid:11)|(cid:12) ψ1[s, si]pess or D (cid:11)|(cid:12) ψ2[s, si]pess ⇒ D (cid:11)|(cid:12) (ψ1 ∧ ψ2)[s, si]pess• For disjunction:D |(cid:12) (ψ1 ∨ ψ2)[s, s j] ⇒ D |(cid:12) ψ1[s, s j] or D |(cid:12) ψ2[s, s j]i.h.⇒ D |(cid:12) ψ1[s, si]opt or D |(cid:12) ψ2[s, si]opt ⇒ D |(cid:12) (ψ1 ∨ ψ2)[s, si]optD (cid:11)|(cid:12) (ψ1 ∨ ψ2)[s, s j] ⇒ D (cid:11)|(cid:12) ψ1[s, s j] and D (cid:11)|(cid:12) ψ2[s, s j]i.h.⇒ D (cid:11)|(cid:12) ψ1[s, si]pess and D (cid:11)|(cid:12) ψ2[s, si]pess ⇒ D (cid:11)|(cid:12) (ψ1 ∨ ψ2)[s, si]pess• For negation:D |(cid:12) ¬ϕ1[s, s j] ⇒ D (cid:11)|(cid:12) ϕ1[s, s j] i.h.⇒ D (cid:11)|(cid:12) ϕ1[s, si]pess ⇒ D |(cid:12) ¬ϕ1[s, si]optD (cid:11)|(cid:12) ¬ϕ1[s, s j] ⇒ D |(cid:12) ϕ1[s, s j] i.h.⇒ D |(cid:12) ϕ1[s, si]opt ⇒ D (cid:11)|(cid:12) ¬ϕ1[s, si]pess• For next:D |(cid:12) next(ϕ1)[s, s j] ⇒ D |(cid:12) ϕ1[s1, s j] and j (cid:3) 1i.h.⇒ D |(cid:12) ϕ1[s1, si]opt ⇒ D |(cid:12) next(ϕ1)[s, si]optD (cid:11)|(cid:12) next(ϕ1)[s, s j] ⇒ D (cid:11)|(cid:12) ϕ1[s1, s j] or j = 0i.h.⇒ D (cid:11)|(cid:12) ϕ1[s1, si]pess or i = 0 ⇒ D (cid:11)|(cid:12) next(ϕ1)[s, si]pess• As before, the cases for always(ϕ), eventually(ϕ), and until(ϕ1, ϕ2) follow from the above cases for “∧”, “∨”, andnext(ϕ). (cid:3)With these lemmas in hand it is straightforward to show the theorem itself. Again we proceed by induction, over thestructure of general preference formulae.Proof of Theorem 4.18. The first item of the theorem follows directly from Lemma C.2. For the second item of the theorem,we consider the component inequalities separately.Inequalities w optsis j(Φ) (cid:3) w pesssi (Φ) (cid:2) w opts j (Φ) and w pess(Φ): Assume w opts j (ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18) ϕm[vm]) = vr with all ϕltrajectory property formulae. Then if r (cid:2) m we have that D |(cid:12) ϕr[S0, s j]opt and D (cid:11)|(cid:12) ϕl[S0, s j]opt, ∀l < r. Thus withLemma C.1 we have D |(cid:12) ϕr[S0, si]opt and thus w optsi (ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18) ϕm[vm]) (cid:2) vr . This trivially alsoholds for the case where none of the ϕl holds optimistically and hence vr = v max. For the pessimistic case in turn,let w pess(ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18) ϕm[vm]) = vr with ϕl trajectory property formulae. Then D (cid:11)|(cid:12) ϕl[S0, s j]pess,∀l < r. Thus by Lemma C.1 we have D (cid:11)|(cid:12) ϕl[S0, si]pess, ∀l < r and thus w pess(ϕ0[v 0] (cid:18) ϕ1[v 1] (cid:18) · · · (cid:18) ϕm[vm]) (cid:3) vr .s jsiNow, for the induction step, suppose the assumption holds for Ψ, Ψi . Then,• For Φ = γ : Ψ and the optimistic case, there are two possibilities:– D (cid:11)|(cid:12) γ [s, s j]pess and thus w opts j (Φ) = 0. Then from Lemma C.1 we have that also D (cid:11)|(cid:12) γ [s, si]pess and thuss j (Ψ ). From induction hypothesis we know that w optsi (Ψ ) (cid:2) w opts j (Ψ ) andw optsi (Φ) = 0.thus again w opt– D |(cid:12) γ [s, s j]pess and w optsi (Φ) (cid:2) w opts j (Φ) = w opts j (Φ).And for the pessimistic case, there are also two possibilities:– D (cid:11)|(cid:12) γ [s, s j]opt and thus w pess– D |(cid:12) γ [s, s j]opt and w pess(Φ) = w pesss j(Φ) = w pesss js jw pesssisi(Φ) = 0 so that immediately w pess(Φ) (cid:3) w pess(Φ).s j(Ψ ). Using Lemma C.1 it follows that also D |(cid:12) γ [s, si]opt and thussi(Ψ ) which by induction hypothesis is greater or equal than w pess(Ψ ).s j1342M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345• For Φ = Ψ1& . . . &Ψmw opts j (Ψ1& . . . &Ψm) = max1(cid:2)l(cid:2)mw opts j (Ψl)i.h.(cid:3) max1(cid:2)l(cid:2)mw optsi (Ψl) = w optsi (Ψ1& . . . &Ψm)andw pesss j(Ψ1& . . . &Ψm) = max1(cid:2)l(cid:2)mw pesss j(Ψl)i.h.(cid:2) max1(cid:2)l(cid:2)mw pesssi(Ψl) = w pesssi(Ψ1& . . . &Ψm)• For Φ = Ψ1| . . . |Ψmw opts j (Ψ1| . . . |Ψm) = min1(cid:2)l(cid:2)mw opts j (Ψl)i.h.(cid:3) min1(cid:2)l(cid:2)mw optsi (Ψl) = w optsi (Ψ1| . . . |Ψm)andw pesss j(Ψ1| . . . |Ψm) = min1(cid:2)l(cid:2)mw pesss j(Ψl)i.h.(cid:2) min1(cid:2)l(cid:2)mw pesssi(Ψl) = w pesssi(Ψ1| . . . |Ψm)Inequalities w opts j (Φ) (cid:2) w sk (Φ) and w pesss j(Φ) (cid:3) w sk (Φ): The proof of these two inequalities proceeds in direct analogy to theprevious ones but uses Lemma C.2 instead of Lemma C.1. (cid:3)Appendix D. Proof of Corollary 4.20Proof. The proof proceeds by induction over the structure of ϕ and using Theorem 4.13.• ϕ = final(ψ): By definition ρ∗s (final(ψ))[s(cid:5), s(cid:5)] = final(ψ)[s(cid:5), s(cid:5)] and final(ψ)[S0, s(cid:5)]opt/pess = final(ψ)[s(cid:5), s(cid:5)]opt/pess, hencethe thesis.• ϕ = occ(a): We have:(cid:5)opt(cid:5)(cid:4)D |(cid:12) occ(a)S0, sD |(cid:12) do(a, S0) (cid:16) sD |(cid:12) do(a, S0) (cid:16) s(cid:2)(cid:3)(cid:4)D |(cid:12) ρ∗occ(a)ssiff(cid:5), s(cid:5)(cid:5).iff (by definition)(cid:5)(cid:5) ∨ S0 = s(cid:2)(cid:5)iff (by assumption n (cid:3) 1)(cid:3)by definition of occ(a) and Theorem 4.13Also, by definition: occ(a)[S0, sNow there are two cases to consider, either ρ∗(cid:5), s(cid:5)]opt/pess, or ρ∗of occLast(a)[s, sfollows trivially.s (occ(a))[s(cid:5)]pess = occ(a)[S0, s(cid:5)] and hence, D |(cid:12) occ(a)[S0, s(cid:5)]pess iff D |(cid:12) ρ∗(cid:5)] = occLast(a), in which case the thesis follows by definitions (occ(a))[s(cid:5)] is equal to a Boolean constant TRUE/FALSE, in which case the thesiss (occ(a))[s(cid:5), s(cid:5), s(cid:5)].• ϕ = next(ψ): Again, since we are assuming n (cid:3) 1 we get that next(ψ)[S0, s(cid:5)]opt/pess = next(ψ)[S0, s(cid:5)]. Hence the thesis• For the remaining temporal formulae the thesis follows from the induction hypothesis due to their definition in termsfollows from Theorem 4.13.of next. (cid:3)Appendix E. Proof of Theorem 6.3Proof. The theorem is easily proven by extending Lemmas C.1 and C.2 to handle the two new constructs in eTPFs as follows.With respect to Lemma C.1 we observe:M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451343• If D |(cid:12) occC(δ)[s, s j]opt, then, by definition,D |(cid:12) (∃s1: s (cid:16) s1 (cid:16) s j)∃δ(cid:2)(cid:5)Trans(cid:2)∗δ, s, δ(cid:5), s1(cid:3)(cid:2)∧s1 (cid:11)= s j → Final(cid:3)(cid:3)(cid:3)(cid:2)(cid:5)δ, s1The chosen s1 is either such that s1 (cid:16) si or the other way round, si (cid:16) s1. In the former case it follows that:(cid:2)D |(cid:12)∃s(cid:5)1: s (cid:16) s(cid:5)1(cid:16) si(cid:2)(cid:3)(cid:5)∃δTrans(cid:2)∗δ, s, δ(cid:5), s(cid:5)1(cid:3)∧ Final(cid:3)(cid:3)(cid:2)(cid:5)δ, s(cid:5)1In the latter case, it follows that:(cid:3)(cid:2)D |(cid:12) ∃δ(cid:5)Trans∗(cid:5)δ, s, δ, siHence, together we obtain:D |(cid:12) (∃s1: s (cid:16) s1 (cid:16) si)∃δ(cid:2)(cid:5)Trans(cid:2)∗δ, s, δ(cid:5), s1(cid:2)(cid:3)∧s1 (cid:11)= si → Final(cid:3)(cid:3)(cid:3)(cid:2)(cid:5)δ, s1and thus D |(cid:12) occC(δ)[s, si]opt.Furthermore: D (cid:11)|(cid:12) occC(δ)[s, s j]pess, i.e., by definition, D (cid:11)|(cid:12) (∃s1 : s (cid:16) s1 (cid:16) s j)Do(δ, s, s1). Since si precedes s j it is obvi-ous that then also D (cid:11)|(cid:12) (∃s1 : s (cid:16) s1 (cid:16) si)Do(δ, s, s1) and hence by definition: D (cid:11)|(cid:12) occC(δ)[s, si]pess.• If D |(cid:12) afterC(δ, ϕ)[s, s j]opt, then, by definition, D |(cid:12) (∀s1 : s (cid:16) s1 (cid:16) s j)Do(δ, s, s1) → ϕ[s1, s j]opt. Since, si precedes s j , itfollows that: D |(cid:12) ∀s1(s1 (cid:16) si → s1 (cid:16) s j), and hence with the above, we derive: D |(cid:12) (∀s1 : s (cid:16) s1 (cid:16) si)(Do(δ, s, s1) →ϕ[s1, si]opt), i.e., D |(cid:12) afterC(δ, ϕ)[s, si]opt.Furthermore: If D (cid:11)|(cid:12) afterC(δ, ϕ)[s, s j]pess, then, by definition,(cid:2)(cid:3)(cid:2)(cid:3)(cid:2)(cid:3)D (cid:11)|(cid:12) (∀s1 : s (cid:16) s1 (cid:16) s j)Do(δ, s, s1) → ϕ[s1, s j]pess ∧(cid:5)(cid:3)δ, s(cid:5)(cid:5)Trans∗(cid:5)(cid:5)(cid:5)δ, s, δ, s∧ s j (cid:2) s(cid:5)(cid:5)Hence, either:1. D |(cid:12) (∃s1 : s (cid:16) s1 (cid:16) s j)(Do(δ, s, s1) ∧ ¬ϕ[s1, s j]pess), or2. D |(cid:12) (∃δ(cid:5), sIn the former case, there are again two cases to distinguish: (a) the chosen s1 is such that si (cid:2) s1 (cid:16) s j , or (b) s1 (cid:16) si . Incase (a) it follows that:(cid:5)(cid:5)) ∧ s j (cid:2) s∗(δ, s, δ(cid:5), s(cid:5)(cid:5))(Trans(cid:5)(cid:5)).D |(cid:12) ∃δ(cid:5)(cid:5)(cid:5)∃s(cid:2)(cid:2)∗δ, s, δ(cid:5), s(cid:3)(cid:5)(cid:5)Trans(cid:3)(cid:5)(cid:5)∧ si (cid:2) sand in case (b) it follows that:(cid:2)D |(cid:12) (∃s1: s (cid:16) s1 (cid:16) si)Do(δ, s, s1) ∧ ¬ϕ[s1, si]pess(cid:3)using induction hypothesis w.r.t. ϕ.In the latter case of the enumeration (2.), it follows immediately from the definition of Transthat∗and the fact that si (cid:2) s jD |(cid:12) ∃δ(cid:5)(cid:5)(cid:5)∃s(cid:2)(cid:2)∗δ, s, δ(cid:5), s(cid:3)(cid:5)(cid:5)Trans(cid:3)(cid:5)(cid:5)∧ si (cid:2) sHence, taking all cases together, we get thatD (cid:11)|(cid:12) (∀s1: s (cid:16) s1 (cid:16) si).Do(δ, s, s1) → ϕ[s1, si]pess ∧(cid:3)(cid:2)(cid:2)(cid:5)(cid:3)δ, s(cid:5)(cid:5)Trans(cid:2)∗δ, s, δ(cid:5), s(cid:3)(cid:5)(cid:5)(cid:3)(cid:5)(cid:5)∧ si (cid:2) si.e, by definition, D (cid:11)|(cid:12) afterC(δ, ϕ)[s, si]pess.With respect to Lemma C.2 we observe:• If D |(cid:12) occC(δ)[s, s j], then, by definition,(cid:2)(cid:2)D |(cid:12) (∃s1: s (cid:16) s1 (cid:16) s j)∃δ(cid:5)∗Transδ, s, δ(cid:5), s1(cid:3)∧ Final(cid:3)(cid:3)(cid:2)(cid:5)δ, s1It then follows from definition of Trans thatD |(cid:12) (∃s1: s (cid:16) s1 (cid:16) si)∃δ(cid:2)(cid:5)Trans(cid:2)∗δ, s, δ(cid:5), s1(cid:2)(cid:3)∧s1 (cid:11)= si → Final(cid:3)(cid:3)(cid:3)(cid:2)(cid:5)δ, s1i.e., by definition, D |(cid:12) occC(δ)[s, si]opt.Furthermore: D (cid:11)|(cid:12) occC(δ)[s, s j], i.e., by definition, D (cid:11)|(cid:12) (∃s1 : s (cid:16) s1 (cid:16) s j)Do(δ, s, s1). Since si precedes s j it is obviousthat then also again D (cid:11)|(cid:12) (∃s1 : s (cid:16) s1 (cid:16) si)Do(δ, s, s1) and hence by definition: D (cid:11)|(cid:12) occC(δ)[s, si]pess.1344M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–1345• If D |(cid:12) afterC(δ, ϕ)[s, s j], then, by definition, D |(cid:12) (∀s1 : s (cid:16) s1 (cid:16) s j)(Do(δ, s, s1) → ϕ[s1, s j]). Since, si precedes s j , itfollows that: D |(cid:12) ∀s1(s1 (cid:16) si → s1 (cid:16) s j), and hence with the above and by induction hypothesis w.r.t. ϕ: D |(cid:12) (∀s1 : s (cid:16)s1 (cid:16) si)(Do(δ, s, sFurthermore: If D (cid:11)|(cid:12) afterC(δ, ϕ)[s, s j], then, D (cid:11)|(cid:12) (∀s1 : s (cid:16) s1 (cid:16) s j).Do(δ, s, s1) → ϕ[s1, s j]. Hence: D |(cid:12) (∃s1 : s (cid:16) s1 (cid:16)s j)(Do(δ, s, s1) ∧ ¬ϕ[s1, s j]). There are two cases to distinguish: (a) the chosen s1 is such that si (cid:2) s1 (cid:16) s j , or (b) s1 (cid:16) si .In case (a) it follows that:(cid:2)(cid:5)1) → ϕ[s1, si]opt), i.e., D |(cid:12) afterC(δ, ϕ)[s, si]opt.(cid:3)(cid:3)D |(cid:12) ∃δ(cid:5)(cid:5)(cid:5)∃s(cid:2)Trans∗(cid:5)(cid:5)(cid:5)δ, s, δ, s∧ s1 (cid:2) s(cid:5)(cid:5)and in case (b) it follows that:(cid:2)D |(cid:12) (∃s1 : s (cid:16) s1 (cid:16) si)Do(δ, s, s1) ∧ ¬ϕ[s1, si]pess(cid:3)using induction hypothesis w.r.t. ϕ. Hence, taken together we get that(cid:3)(cid:2)(cid:2)(cid:3)Do(δ, s, s1) → ϕ[s1, si]pess∧(cid:5)(cid:3)δ, s(cid:5)(cid:5)(cid:2)D (cid:11)|(cid:12) (∀s1 : s (cid:16) s1 (cid:16) si)(cid:2)∗δ, s, δ(cid:5), s(cid:3)(cid:5)(cid:5)Trans(cid:3)(cid:5)(cid:5)∧ si (cid:2) si.e, by definition, D (cid:11)|(cid:12) afterC(δ, ϕ)[s, si]pess.With these extensions in place, the proof proceeds analogously to the proof of Theorem 4.18. (cid:3)References[1] F. Bacchus, F. Kabanza, Using temporal logics to express search control knowledge for planning, Artificial Intelligence 16 (2000) 123–191.[2] J. Baier, S. McIlraith, Planning with first-order temporally extended goals using heuristic search, in: Proceedings of the 21st National Conference onArtificial Intelligence (AAAI-06), 2006, pp. 788–795.[3] J.A. Baier, F. Bacchus, S.A. McIlraith, A heuristic search approach to planning with temporally extended preferences, in: Proceedings of the 20th Inter-national Joint Conference on Artificial Intelligence (IJCAI-07), 2007, pp. 1808–1815.[4] J.A. Baier, F. Bacchus, S.A. McIlraith, A heuristic search approach to planning with temporally extended preferences, Artificial Intelligence 173 (5–6)(2009) 593–618.[5] J.A. Baier, S.A. McIlraith, On domain-independent heuristics for planning with qualitative preferences, in: Proceedings of the 7th Workshop on Non-monotonic Reasoning, Action and Change (NRAC-07), 2007.[6] J.A. Baier, S.A. McIlraith, Planning with preferences, AI Magazine 29 (4) (2008) 25–36.[7] C. Baral, V. Kreinovich, R. Trejo, Computational complexity of planning with temporal goals, in: Proceedings of the 17th International Joint Conferenceon Artificial Intelligence (IJCAI-01), 2001, pp. 509–514.[8] S. Benferhat, D. Dubois, H. Prade, Towards a possibilistic logic handling of preferences, in: Applied Intelligence, vol. 14, Kluwer, 2001, pp. 303–317, .[9] J. Benton, S. Kambhampati, M.B. Do, YochanPS: PDDL3 simple preferences and partial satisfaction planning, in: Proceedings of the 5th InternationalPlanning Competition Booklet (IPC-06), 2006, pp. 54–57.[10] J. Benton, M. van den Briel, S. Kambhampati, A hybrid linear programming and relaxed plan heuristic for partial satisfaction problems, in: Proceedingsof the 17th International Conference on Automated Planning and Scheduling (ICAPS-07), 2007, pp. 34–41.[11] M. Bienvenu, C. Fritz, S. McIlraith, Planning with qualitative temporal preferences, in: Proceedings of the 10th International Conference on KnowledgeRepresentation and Reasoning (KR-06), 2006, pp. 134–144.[12] M. Bienvenu, C. Fritz, S. Sohrabi, S. McIlraith, PPLAN: Code, experiments, http://www.cs.toronto.edu/~sheila/PPLAN, 2006.[13] C. Boutilier, R. Brafman, C. Domshlak, H. Hoos, D. Poole, CP-nets: A tool for representing and reasoning about conditional ceteris paribus preferencestatements, Journal of Artificial Intelligence Research 21 (2004) 135–191.[14] R.I. Brafman, Y. Chernyavsky, Planning with goal preferences and constraints, in: Proceedings of the 15th International Conference on AutomatedPlanning and Scheduling (ICAPS-05), 2005, pp. 182–191.[15] G. Brewka, Complex preferences for answer set optimization, in: Proceedings of the 9th International Conference on Knowledge Representation andReasoning (KR-04), 2004, pp. 213–223.[16] G. Brewka, A rank based description language for qualitative preferences, in: Proceedings of the 16th European Conference on Artificial Intelligence(ECAI-04), 2004, pp. 303–307.[17] G. Brewka, S. Benferhat, D.L. Berre, Qualitative choice logic, Artificial Intelligence 157 (1–2) (2004), Special Issue on Nonmonotonic Reasoning.[18] W. Burgard, A.B. Cremers, D. Fox, D. Hähnel, G. Lakemeyer, D. Schulz, W. Steiner, S. Thrun, Experiences with an interactive museum tour-guide robot,Artificial Intelligence 114 (1–2) (1999) 3–55.[19] T. Bylander, The computational complexity of propositional STRIPS planning, Artificial Intelligence 69 (1–2) (1994) 165–204.[20] S. Coste-Marquis, J. Lang, P. Liberatore, P. Marquis, Expressive power and succinctness of propositional languages for preference representation, in:Proceedings of the 9th International Conference on Knowledge Representation and Reasoning (KR-04), 2004, pp. 203–212.[21] G. De Giacomo, Y. Lespérance, H. Levesque, ConGolog, a concurrent programming language based on the situation calculus, ArtificialIntelli-gence 121 (1–2) (2000) 109–169.[22] J. Delgrande, T. Schaub, H. Tompits, Domain-specific preferences for causual reasoning and planning, in: Proceedings of the 9th International Conferenceon Knowledge Representation and Reasoning (KR-04), 2004, pp. 673–682.[23] J.P. Delgrande, T. Schaub, H. Tompits, A general framework for expressing preferences in causal reasoning and planning, Journal of Logic and Computa-tion 17 (2007) 871–907.[24] S. Edelkamp, Optimal symbolic PDDL3 planning with MIPS-BDD, in: Proceedings of the 5th International Planning Competition Booklet (IPC-06), 2006,pp. 31–33.[25] S. Edelkamp, S. Jabbar, M. Naizih, Large-scale optimal PDDL3 planning with MIPS-XXL, in: Proceedings of the 5th International Planning CompetitionBooklet (IPC-06), 2006, pp. 28–30.[26] M. Ehrgott, Multicriteria Optimization, Springer, Berlin, 2000.[27] T. Eiter, W. Faber, N. Leone, G. Pfeifer, A. Polleres, Answer set planning under action costs, Journal of Artificial Intelligence Research 19 (2003) 25–71.[28] K. Erol, D.S. Nau, V.S. Subrahmanian, Complexity, decidability and undecidability results for domain-independent planning, Artificial Intelligence 76 (1–2) (1995) 75–88.M. Bienvenu et al. / Artificial Intelligence 175 (2011) 1308–13451345[29] R. Feldmann, G. Brewka, S. Wenzel, Planning with prioritized goals, in: Proceedings of the 10th International Conference on Knowledge Representationand Reasoning (KR-06), 2006, pp. 503–514.[30] A. Ferrein, C. Fritz, G. Lakemeyer, On-line decision-theoretic Golog for unpredictable domains, in: Proceedings of 27th German Conference on AI (KI-04),2004, pp. 322–336.[31] C. Fritz, S. McIlraith, Decision-theoretic GOLOG with qualitative preferences, in: Proceedings of the 10th International Conference on Principles ofKnowledge Representation and Reasoning (KR-06), 2006, pp. 153–163.[32] A. Gabaldon, Precondition control and the progression algorithm, in: Proceedings of the 9th International Conference on Knowledge Representationand Reasoning (KR-04), 2004, pp. 634–643.[33] A. Gerevini, P. Haslum, D. Long, A. Saetti, Y. Dimopoulos, Deterministic planning in the fifth international planning competition: PDDL3 and experi-mental evaluation of the planners, Artificial Intelligence 173 (5–6) (2009) 619–668.[34] A. Gerevini, D. Long, Plan constraints and preferences in PDDL3: The language of the fifth international planning competition. Tech. Rep., University ofBrescia, 2005.[35] E. Giunchiglia, M. Maratea, Planning as satisfiability with preferences, in: Proceedings of the 22nd Conference on Artificial Intelligence (AAAI-07), 2007,pp. 987–992.[36] J. Goldsmith, J. Ulrich (Eds.), AI Magazine 29 (4) (2008), Winter 2008, Special Issue on Preferences.[37] C.C. Green, Application of theorem proving to problem solving, in: Proceedings of the 1st International Joint Conference on Artificial Intelligence, 1969,pp. 219–240.[38] P. Haddawy, S. Hanks, Representations for decision-theoretic planning: Utility functions for deadline goals, in: Proceedings of the 3rd InternationalConference on Knowledge Representation and Reasoning (KR-96), 1992, pp. 71–82.[39] M. Helmert, Decidability and undecidability results for planning with numerical state variables, in: Proceedings of the Sixth International Conferenceon Artificial Intelligence Planning Systems (AIPS-02), 2002, pp. 44–53.[40] J. Hoffmann, B. Nebel, The FF planning system: Fast plan generation through heuristic search, Journal of Artificial Intelligence Research 14 (2001)253–302.[41] C.-W. Hsu, B. Wah, R. Huang, Y. Chen, Constraint partitioning for solving planning problems with trajectory constraints and goal preferences, in:Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), 2007, pp. 1924–1929.[42] H.A. Kautz, B. Selman, Unifying SAT-based and graph-based planning, in: Proceedings of the 16th International Joint Conference on Artificial Intelligence(IJCAI-99), 1999, pp. 318–325.[43] J. Kvarnström, P. Doherty, TALplanner: A temporal logic based forward chaining planner, Annals of Mathematics and Artificial Intelligence 30 (2000)119–169.[44] H.J. Levesque, R. Reiter, Y. Lesperance, F. Lin, R.B. Scherl, GOLOG: A logic programming language for dynamic domains, Journal of Logic Program-ming 31 (1–3) (1997) 59–83.[45] S. Liaskos, Acquiring and reasoning about variability in goal models, PhD in Computer Science, Department of Computer Science, University of Toronto,Toronto, Canada, 2008.[46] S. Liaskos, S.A. McIlraith, J. Mylopoulos, Towards augmenting requirements models with preferences, in: Proceedings of the 24th IEEE/ACM InternationalConference on Automated Software Engineering (ASE-09), 2009, pp. 565–569.[47] J. McCarthy, Situations, actions and causal laws. Tech. Rep., Stanford University, 1963.[48] D.V. McDermott, PDDL—The Planning Domain Definition Language, Tech. Rep. TR-98-003/DCS TR-1165, Yale Center for Computational Vision andControl, 1998.[49] S. McIlraith, T. Son, Adapting Golog for composition of semantic web services, in: Proceedings of the 8th International Conference on KnowledgeRepresentation and Reasoning, 2002, pp. 482–493.[50] K. Myers, T. Lee, Generating qualitatively different plans through metatheoretic biases, in: Proceedings of the 16th National Conference on ArtificialIntelligence (AAAI-99), 1999, pp. 570–576.[51] A. Pnueli, The temporal logic of programs, in: Proceedings of the 18th IEEE Symposium on Foundations of Computer Science (FOCS-77), 1977, pp. 46–57.[52] M. Puterman, Markov Decision Processes: Discrete Dynamic Programming, Wiley, New York, 1994.[53] R. Reiter, Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems, MIT Press, Cambridge, MA, 2001.[54] A.P. Sistla, E.M. Clarke, The complexity of propositional linear temporal logics, Journal of the ACM 32 (3) (1985) 733–749.[55] D.E. Smith, Choosing objectives in over-subscription planning, in: Proceedings of the 14th International Conference on Automated Planning andScheduling (ICAPS-04), 2004, pp. 393–401.[56] S. Sohrabi, J.A. Baier, S.A. McIlraith, HTN planning with preferences, in: Proceedings of the 21st International Joint Conference on Artificial Intelligence(IJCAI-09), 2009, pp. 1790–1797.[57] S. Sohrabi, S.A. McIlraith, On planning with preferences in HTN, in: Proceedings of the 4th Multidisciplinary Workshop on Advances in PreferenceHandling (M-Pref-08) at AAAI-08, 2008, pp. 103–109.[58] S. Sohrabi, N. Prokoshyna, S. McIlraith, Web service composition via generic procedures and customizing user preferences, in: Proceedings of the 5thInternational Semantic Web Conference (ISWC-06), 2006, pp. 597–611.[59] T.C. Son, E. Pontelli, Planning with preferences using logic programming, in: Proceedings of the 7th International Conference on Logic Programmingand Nonmonotonic Reasoning (LPNMR-04), 2004, pp. 247–260.[60] T.C. Son, E. Pontelli, Planning with preferences using logic programming, Theory and Practice of Logic Programming 6 (5) (2006) 559–607.[61] P.H. Tu, T.C. Son, E. Pontelli, CPP: A constraint logic programming based planner with preferences, in: Proceedings of the 9th International Conferenceon Logic Programming and Nonmonotonic Reasoning (LPNMR-07), 2007, pp. 290–296.[62] M. van den Briel, R.S. Nigenda, M.B. Do, S. Kambhambati, Effective approaches for partial satisfaction (oversubscription) planning, in: Proceedings ofthe 19th National Conference on Artificial Intelligence (AAAI-04), 2004, pp. 562–569.[63] J. von Neumann, O. Morgenstern, Theory of Games and Economic Behavior, Princeton University Press, 1994.[64] N. Wilson, Extending CP-Nets with stronger conditional preference statements, in: Proceedings of the 19th National Conference on Artificial Intelligence(AAAI-04), 2004, pp. 735–741.[65] N. Yorke-Smith, K.B. Venable, F. Rossi, Temporal reasoning with preferences and uncertainty, in: Proceedings of the 18th International Joint Conferenceon Artificial Intelligence (IJCAI-03), 2003, pp. 1385–1390.