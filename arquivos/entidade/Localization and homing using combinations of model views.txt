Artificial Intelligence 78 ( 1995) 327-354 Artificial Intelligence Localization and homing using combinations of model views Ronen Basri av*,l, Ehud Rivlin b*2,3 a Department of Applied Mathematics, The Weizmann Institute of Science, Rehovot 76100, Israel ’ Computer Science Department, Technion, Haifa 32CO0, Israel Received August 1993; revised November 1994 Abstract identifying Navigation and honing involves recognizing the environment), positioning the current position within the the environment, (the act environment, and reaching particular positions. We present a method for localization (the act of computing the exact coordinates of a of recognizing robot in the environment), (the act of returning to a previously visited position) from visual input. The method is based on representing the scene as a set of 2D views and predicting the appearances of novel views by linear combinations of the model views. The method accurately approximates the appearance of scenes under weak-perspective projection. Analysis of this projection as well as experimental results demonstrate that in many cases this approximation is sufficient to accurately describe the scene. When weak-perspective approximation is invalid, either a larger number of models can be acquired or an iterative solution to account for the perspective distortions can be employed. The method has several advantages over other approaches. It uses relatively rich representations; the representations are 2D rather than 3D; and localization can be done from only a single 2D view without calibration. The same principal method is applied for both the localization and positioning problems, and a simple “qualitative” algorithm for homing is derived from this method. * Corresponding author. E-mail: ronen@wisdom.weizmann.ac.il. ’ This report describes research done in part at the Massachusetts Institute of Technology within the Artificial Intelligence Laboratory and the McDonnell-Pew Center for Cognitive Neuroscience. Support for the laboratory’s artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research Contract NOOO14-91-J-4038. ’ E-mail: ehudr@cs.technion.ac.iI. 3 This repolt describes research done in part at the University of Maryland within the Computer Vision Laboratory in the Center for Automation Research. The second author was suppolted in part by the Defense Advanced Research Projects Agency (ARPA Order No. 8459) and the U.S. Army Engineer Topographic Laboratories under Contract DACA76-92-C-0009. 0004-3702/95/$09.50 @ 1995 Elsevier Science B.V. All rights reserved SSDIOOO4-3702(95)00021-6 328 R. Basri, E. Rivlin /Art&k! fntelligence 78 (1995) 327-354 1. Introduction Basic tasks in autonomous robot navigation are localization, positioning, is the act of recognizing the environment, that is, assigning and homing. consistent locations, to different Localization labels the robot the sense that position place-specific previously visited position. coordinate in the environment. and positioning Positioning is the act of computing is a task complementary the coordinates of to localization, in in a is often specified to a room 911”). Homing is the task of returning (e.g., “1.5 meters northwest of table r’) system (“in A method for localization, positioning, and homing in visually-guided navigation is presented. The method, based on [ 201, represents systems images. Localization is achieved by comparing of model views. The position of the robot is computed by analyzing the linear combination solution to the homing problem using the same scheme the observed is presented. that aligns the model scenes by sets of their 2D image to linear combinations the coefficients of to the image. Also, a simple, qualitative Visually-guided navigation systems can be classified according utilized. We distinguish between that represent images of the scene a representation These invariant from representations and 3D models. Systems generate large range of transformations. projecting measurements observed straightforward way. images and comparing the image data onto a lower dimensional from the data. Localization the obtained two types of representations, to the type of scene signatures the scene using a set of signatures usually that is invariant over a relatively often are obtained by a set of from the in a subspace or by computing signatures signatures with the stored signatures is achieved by generating representations Sarachik and McDermott erates signatures Braunegg map obtained by projecting signatures [ 61 use blurred from averaged orientations [ 171 computes and stores the dimensions of the navigated offices. Engelson [ 141 gen- regions of the image. [4] recovers a depth map of the scene from which he generates an occupancy the 3D edges onto “the floor”. Hong et al. [9] generate images of the scene as signatures. Nelson of edges in different from panoramic views of the scene by projecting them onto a 1D circle. Other systems store complete 3D descriptions of the scene. To recognize that relates between the transformation [I] use a trinocular images. Ayache and Faugeras the systems must first recover incoming the 3D structure of the scene before [ 151 use a stereo system align and their positions image. Fennema images. Gray-scaled the location of these landmarks the stereo image with the model a set of landmarks the transformation the 3D models of the scene landmarks are generated are used to derive et al. [ 71) compare templates of selected to recover a depth map of the observed stereo system the scene the model and the to recover et al. to is first located by the system to the that relates to sequences of 2D from the model, and scene. In order the model is computed by means of correlation and tracking. it is compared with the model. Onoguchi The method presented rather than using explicit 3D descriptions of its 2D images. Predicting the model views. in this paper does not generate signatures of the scene. However, of the scene, the scene is represented by sets the appearances of novel views is obtained by combining Homing was recently addressed in several studies. Nelson [ 141 and Zipser [22] R. Basri, E. Rivlin/Artificial Intelligence 78 (1995) 327-354 329 proposed to handle this problem by generating signatures of the scene from single images and storing them along with vectors directing the robot toward the target location. At runtime whenever the robot encounters a signature similar to one or more of the stored signatures it follows the precomputed direction vectors associated with these signatures. Hong et al. [9] perform homing by comparing signatures obtained from a panoramic view of the scene with a similar signature obtained at the target location. The robot is then instructed to move so as to bring the observed signature and the target signature into alignment. The method for homing presented in this paper differs from previous algorithms by that it does not use signatures to represent the scene. Homing is achieved by moving the robot so as to align the observed images of the scene with an image taken from the target position. Like [ 91, our algorithm computes the direction of motion “on the fly”. The algorithm is qualitative in nature, and it is designed so as to gradually bring the current and the target images into alignment. The rest of the paper is organized as follows. The method for localization is presented in Section 2, where we propose a method that works accurately under weak-perspective approximation and an iterative scheme to account for perspective distortions. Positioning is addressed in Section 3, and the algorithm for homing is described in Section 4. Constraints imposed on the motion of the robot as a result of special properties of indoor environments can be used to reduce the complexity of the method presented here. This topic is covered on Section 5. Experimental results follow. 2. Localization The problem of localization is defined as follows: given P, a 2D image of a place, and M, a set of stored models, find a model M’ E M such that P matches M’. One problem a system for localization should address is the variability of images due to viewpoint changes. The inexactness of practical systems makes it difficult for a robot to return to a specified position on subsequent visits. The visual data available to the robot between visits varies in accordance with the viewing position of the robot. A localization system should be able to recognize scenes from different positions and orientations. Another problem is that of changes in the scene. At subsequent visits the same place may look different due to changes in the arrangement of the objects, the introduction of new objects, and the removal of others. In general, some objects tend to be more static than others. While chairs and books are often moved, tables, closets, and pictures tend to change their position less frequently, and walls are almost guaranteed to be static. Static cues naturally are more reliable than mobile ones. Confining the system to static cues, however, may in some cases result in failure to recognize the scene due to insufficient cues. The system should therefore attempt to rely on static cues, but should not ignore the dynamic cues. We are interested in a system that can recognize the environment from different viewing positions and that can update its representations dynamically to accommodate changes in the scene. A common approach to handling the problem of recognition from different viewpoints is by comparing the stored models to the observed environment 330 R. Bnsri, E. Rivlin/ArfQScia/ Intelligence 78 (1995) 327-354 approach is recovered and compensated after the viewpoint is used in a number of studies of object recognition alignment system based on the “Linear Combinations” into two parts. In the first part (Section 2. I ) we describe under weak-perspective for handling to the problem of localization. Below we describe a localization is divided that works approximation. The second part (Section 2.2) proposes a method for. This approach, called alignment, We apply the [ 201. The presentation the basic system large perspective distortions. [ 3,8,10,13,18,19]. scheme 2.1. Localization under a weak-perspective assumption from the y-coordinates the x-coordinates in correspondence. the feature points of the points. An object The scheme for localization in the image, one contains is the following. Given an image, we construct two view of the (in our case, is modeled by a set of such views, where the points in these views are is predicted of this linear The appearance of a novel view of the object linear combinations are recovered using a small number of mode1 points and their corresponding to the stored views. The coefficients vectors points, and the other contains the environment) ordered by applying combination image points. To verify the match, the predicted appearance image, and the object is recognized line segments) First. viewer-centered models are composed of 2D views of the observed are predicted Formally, is compared with the actual (or is twofold. ones; namely, scene. Second, novel appearances in a simple and accurate way (under weak-perspective given P, a 2D image of a scene, and M, for verification. The advantage of this method if the two match. A large number of points are used rather than object-centered is to find a model M’ E M objective ~,i E R. It has been shown that this scheme accurately predicts objects under weak-perspective limitations of this projection model are discussed a set of stored models, the such that P = cf=, LYjM; for some constants the appearance of rigid [ 201. The (orthographic projection and scale) later in this paper. representations projection). projection are used More concretely, weak-perspective given by let p; = (x;, yi, Zi), 1 < i 6 n, be a set of n object points. Under in the image are the position p,! = (xi, y!) of these points projection, x; = STIIX, + sy12y; + STIJZ, + r,, y: = sr-21x; + sr22yi + smz, + t?, (1) where rij are the components ty are the amounts of horizontal form we obtain vector equation of a 3 x 3 rotation matrix, s is a scale factor, and tx and this in respectively. Rewriting and vertical translation I x = srllx + srl2y + srl37, + t,l, y’ = n-21x + sr22y + sr23z + t!l, (2) where x,y, z, x’, y’ E R” are the vectors of x;. yi, zi, xi and y; coordinates and l=(l,l,..., l)T. Consequently, respectively, x’,y’ E span(x,y, z,l} (3) R. Basri, E. Rivlin/Art#cial Intelligence 78 (1995) 327-354 331 that z’, the vector of depth coordinates or, in other words, x’ and y’ belong (Notice to this subspace. This is spanned by any four scene supply y2 the location vectors of the n points al,a2,a3,aq linearly four such vectors and bl,b2,63,b4 fact is used such that in Section 2.2 below.) A four-dimensional to a four-dimensional linear subspace of R”. of the projected points, also belongs space vectors of the space. Two views of the (See also [ 1 I] .> Denote by xt, y, and x2, independent [ 16,201. in the two images; then there exist coefficients x’ = alxi Y’ = bixt + bzy, + 63x2 + b41. + a2y, + agx2 + a41, (4) (Note that the vector y, already depends on the other four vectors.) Since R is a rotation matrix, two quadratic constraints: the coefficients the following satisfy a:+a$+ai-b:-bi-bz= 2(blb3 - ala3)rll + 2(bzb3 - a2a3)r12, albl + a2b2 + ah + (alb3 + asbl )r11 + (a263 + a362)r12 = 0. (5) these constraints the transformation To derive recovered. This can be done under weak-perspective the constraints mations with affine ones. This usually does not prevent generally scenes are fairly different from one another. can be ignored, between the two model views should be using a third image. Alternatively, rigid transfor- since localization successful in which case the system would confuse Note that we incorporate Points that are not visible the model. We can extend (See images of the scene. that appear in both model images. in the model only points in one of the images due to occlusion from the models with additional points by taking more then two [ 201.) are excluded is quadratic the model that aligns if the quadratic the environment To summarize, we model the effect of noise. After the coefficients constraints may also be stored. Localization by a set of images with correspondence the images. For example, a spot can be modeled by two of its corresponding between views. The corresponding achieved by recovering to the observed the linear combination image. The coefficients are determined using four model points and their corresponding image points by solving a linear set of equations. Three points are sufficient to determine the coefficients are also considered. Additional constraints points may be used to reduce are recovered we use them to predict the appearance the points of the model can be used at this stage. The predicted appearance to verify the match. When can be done, for example, by testing all possible matches of quadruples of feature points in the image. in the model In this case the worst-case time complexity is k(m4n4)m’, where k is the number of models considered, m is the number of model points, n is the number of image points, is typical and m’ is the number of points considered the to alignment constraints proposed the coefficients under an unconstrained by applying to reduce the complexity of recovering of feature points process is then compared are ignored for verification. This complexity the recovery of the coefficients can be reduced considerably schemes. This complexity the quadratic constraints in Section 5. A method to the actual image of the model. All of the localization transformation to quadruples is described in [ 211. 332 R. Basri, E. Rivlin/Artijicial Intelligence 78 (1995) 327-354 The recovery of the alignment coefficients is defined as follows. Denote by M= [Xi,Y,,X2,11 the matrix of model points, and let a and b denote the vectors of coefficients, then a = M+x’, b = M+y’, (6) (7) of M. ( Mf = M-’ when only four where Mt = ( MTM) -’ MT is the pseudo-inverse points are used.) Note that for the recovery stage M, x’, and y’ should contain only the coordinates of those points used for the recovery process, e.g., of the hypothesized match. The sensitivity is determined by the condition number of M. The robustness of the recovery process can be increased by choosing quadruples of model points arising from non-planar the set of matches with additional points to errors of this recovery process to generate an overdetermined and by extending configurations system types of features we assign weights In our scheme we distinguish between static, semi-static, and dynamic cues. To handle their the weights of points, such in the scene in both stages of In the recovery stage, let w be a vector of the different reliability. We can use several different criteria as, the number of occurrences (higher points recovering weights assigned to determine in subsequent visits or the height of points tend more to be static). The weights are incorporated to the model points, and let W = &g(w) to the model points and verification. the coefficients reflecting then a = (WM)+Wx’, b = (WM)+Wy’ In the verification their matched positions stage, distances between predicted positions of model features and in the image are weighed according uses viewer-centered models, to w. that is, representations Our scheme for localization small transformations we avoid the need to handle occlusions It has a number of advantages over methods to represent that full are composed of images. the scene. First, by using viewer-centered models three-dimensional models in the that cover relatively scene. If from some viewpoints the scene appears different because of occlusions we utilize a new model for these viewpoints. Second, viewer-centered models are easier to build and to maintain images and the model images one can find correspondences. By limiting the correspondence If large are changed between visits a new model can be constructed portions of the environment by simply replacing old images with new ones. ones. The models contain only (e.g., epipolar constraints using motion methods than object-centered the transformation that build between [2,12]). The number of models required of the scene. A complex scene depends on the complexity may require a relatively require only a relatively location appears represent sufficient to cover the scene from all possible viewing positions (containing many aspects) large number of views. In practice, however, navigation may its rough small number of models. Specifically, as it to represent the robot can to recognize the appearance of the room from the threshold. One model may therefore be in this case. (See Section 5.) the scheme (An analysis of the weak-perspective the robot may need the access routes only. For example, to recognize the environment a room in the environment from is due to the weak-perspective One problem with using for localization approximation. this scheme assumption under is R. Basri, E. Rivlin/Artificial Intelligence 78 (1995) 327-354 333 given in Appendix A.) In contrast with the problem of object recognition, where we can often assume that objects are small relative to their distance from the camera, in localization the environment surrounds the robot and perspective distortions cannot be neglected. The limitations of the weak-perspective modeling are discussed both math- through the rest of this paper. It is shown that in many ematically and empirically practical cases weak-perspective is sufficient to enable accurate localization. The main reason is that the problem of localization does not require accurate measurements in the entire image; it only requires identifying a sufficient number of spots to guarantee accurate naming. If these spots are relatively close to the center of the image, or if the depth differences they create are relatively small (as in the case of looking at a wall when the line of sight is nearly perpendicular to the wall), the perspective distortions are relatively small, and the system can identify the scene with high accuracy. Also, views related by a translation parallel to the image plane form a linear space even when perspective distortions are large. This case and other simplifications are discussed in Section 5. By using weak-perspective we avoid stability problems that frequently occur in per- spective computations. We can therefore compute the alignment coefficients by looking at a relatively narrow field of view. The entire scheme can be viewed as an accumulative process. Rather than acquiring images of the entire scene and comparing them all to a full scene model (as in [ 41) we recognize the scene image by image, spot by spot, until we accumulate sufficient convincing evidence that indicates the identity of the place. When perspective distortions are relatively large and weak-perspective is insufficient to model the environment, two approaches can be used. One possibility is to construct a larger number of models so as to keep the possible changes between the familiar and the novel views small. Alternatively, an iterative computation can be applied to compensate for these distortions. Such an iterative method is described in Section 2.2. 2.2. Handling perspective distortions The scheme presented above accurately handles changes in viewpoint assuming the images are obtained under weak-perspective projection. Error analysis and experimental results demonstrate that in many practical cases this assumption is valid. In cases where perspective distortions are too large to be handled by a weak-perspective approxima- tion, matching between the model and the image can be facilitated in two ways. One possibility is to avoid cases of large perspective distortion by augmenting the library of stored models with additional models. In a relatively dense library there usually exists a model that is related to the image by a sufficiently small transformation avoiding such distortions. The second alternative is to improve the match between the model and the image using an iterative process. In this section we consider the second option. The suggested iterative process is based on a Taylor expansion of the perspective coordinates. As is described below, this expansion results in a polynomial consisting of terms each of which can be approximated by linear combinations of views. The first term of this series represents the orthographic approximation. The process resembles a method of matching 3D points with 2D points described recently by DeMenthon and Davis [ 51. In this case, however, the method is applied to 2D models rather than 3D 334 R. Basri, E. Rivlin/Art$cial Intelligence 78 (1995) 327-354 ones. In our application they are approximated the 3D coordinates of the model points are not provided; from the model views. instead (x, y) = (fX/Z, p/Z) is the projection in the image, where f denotes the focal length. Consider of some object point, the following Taylor image point An (X, E Z) expansion of l/Z around some depth value Zc: = c --(Z O” 1 (-l)kk! i=. k! Z,k+’ - &)” The Taylor series describing the position of a point x is therefore given by (9) Notice that the zero term contains the kth term of the series: the orthographic approximation for x. Denote by Ack) A(k) fX = _ zo (11) A recursive definition of the above series is given below. l Initialization. x(O) = A (0) = c zo l Iterative step. X(k) =X(k-l) + A(k) where xck) represents highest-order the term in xck). kth-order approximation for x, and Ack) represents the approximation to the orthographic of the model views According combinations approximating X and Z at every step using model points with the image points. The general we estimate x(O) and A(O) by solving both X and Z can be expressed as linear (Eq. (4)). We therefore apply the above procedure, that best aligns the the following. First, case. Then, at each step of the the linear combination idea is therefore the orthographic R. Bash. E. Rivlin /Artificial Intelligence 78 (1995) 327-354 335 iteration we improve the factor the estimate by seeking the linear combination that best estimates Denote by x E R” the vector of image point coordinates, and denote by p = [xI,Y,,n,ll (12) (13) the position of the points an II x 4 matrix containing by P+ = ( PTP) -’ PT the pseudo-inverse denote by a (@ the coefficients combination f are constant the vectors of computed values of x and A at the kth step. An iterative procedure align a model images. Denote Also the linear the X or the Z values. Since ZQ and into the linear combination. Denote by xc’) and Ack’ to of P (we assume P is overdetermined). for the kth step. Pack) represents computed at that step to approximate they can be merged is described below. in the two model to the image computed l Initialization. Solve the orthographic approximation, namely x(O) = 4’0’ = Pa(O) 0 Iterative step. _ #-I)) (k) = (x 4 aw = p+q’k’, L_ A(&-1) A’@ = (pa’@) @A+“, #) = X(k-l) + A(k), where the vector operations 8 and + are defined as to avoid to improve false matches The method presented above is meant the overall match between effects. One problem with applying for errors due to perspective distortion. the this In the two kinds of errors. One possible the model and the image by reducing perspective false matches method is that we may mistake a priori between general, one cannot distinguish way orthographic distortions. Then, extend points which deviate within a predetermined eccentricity of the point in the image and by its expected depth value (using in Appendix A). Finally, is obtained, for reasonable perspective to image bound. The bound will be determined by the the analysis If a poor match the iterative procedure on another match. This procedure guarantees the set of feature points by matching model points the following procedure. First, apply the solution by allowing the iterative procedure solution and evaluate to convergence. is by applying repeat run 336 R. Bash, 15’. Rivlin/Artijicial Intelligence 78 (199.5) 327-354 solution, but it has the disadvantage of increasing a polynomial-time of the correspondence and probabilistic methods may be used to reduce this complexity, and additional cues (such as stereo, color, texture, or previous knowledge) (e.g., sonar) may be used to detect where large variations due to perspective distortion should be anticipated. to the orthographic solution. Heuristics the combinatorics and instruments problem relative 3. Positioning is the problem of recovering in a fixed coordinate Positioning can be specified room coordinates), expressed with respect this section we derive the position of a robot from the alignment the exact position of the robot. This position (i.e., is In in which case location from which the model views were acquired. or it can be associated with some model, system associated with the environment to the position coefficients. image P’, we first align We assume a model composed of two images, Pt and P2; their relative position is local- (i.e., given. Given a novel the robot’s position ization). By considering relative is recovered. To recover the absolute position of the robot in the room the absolute positions of the model views should also be provided. Note that the computation Positions the coefficients of the linear combination (that is, assuming a unit focal length). is done in “image coordinates” the model with the image should be normalized to the model are used. images Assume P2 is obtained the average distance of the camera .?$ + t7 ) .) The coordinates of a point s. (Denote &/( combinations of the corresponding model points in the following way: t = (t,, t,, t, ), and scaling in PI to the scene by &, s is given by (x’, y’), can be written as linear in P’, if world coordinates from PI by a rotation R, translation .x’ = alxl + u2y1 + u3x2 + 04. J” = hxi + b2y1 -t 1)3x2 + 04. Substituting for x2 we obtain x’ = UlXl + a2y1 + ~3(WlXl + sr12yj + sr13z1 + tx) + u4, .v’ = blxl + b2yI + b3(srIIxI + srl2yI + sr13~1 + tx) + b4, and rearranging these equations we obtain x’ = (a~ + a3w1)x1 + (a2 + u3w2)yI + (a3sr13)zl + (a3tx + u4), Y’ = (h + bw+II)xI + (h + b3srl2)yl + (b3sr13)a + (b3t, +a4). (14) (15) (16) these equations we can derive all the parameters of the transformation Using the model and the image. Assume and scaling s,. Using the orthonormality between is obtained by a rotation U, translation t,, constraint we can first derive the scale factor the image 4 = (al -k a3w1)* + (a2 + u3sr12)* + (u3srl3)* =u~+a~+u~s2+2u3s(u~r~~ +u2rl2). (17) R. Basri, E. Rivlin /Art@cial Intelligence 78 (1995) 327-354 337 Note that we can also extract the scale factor by applying the same constraint to the b’s: s; = 6: + 6; + b&v* + 2b3S(blr,l + b2r12). (18) is We can use the two equations to verify that the weak-perspective approximation valid. The orthogonality constraint (Eq. 5) can also be used for the this purpose. From Equations (16) and (17), by deriving the components of the translation vector, t,, we can obtain the position of the robot in the image relative to its position in the model views: Ax = a3tx + a4, Ay = b3t.v + h, Az=t+$(1-f)). (19) Note that AZ is derived from the change in scale of the object. The rotation matrix U between PI and P’ is given by Ull = al + a3wl sil a2 + a3sn2 U12 = St! a3w3 u13 =-) Sll , U21 = U22 = h + b-21 sn b2 + b-22 8, b3 St23 U*3 = -. Sll (20) As has already been mentioned, the position of the robot is computed here relative to the position of the camera when the first model image, PI, was acquired. AX and AZ represent the motion of the robot from PI to P’, and the rest of the parameters represent its 3D rotation and elevation. To obtain this relative position the transformation parameters between the model views, PI and P2, are required. Consequently, positioning, unlike localization, requires calibration of the model images. One should note that the results of the positioning process depend on the precision of the alignment coefficients, which may be erroneous due to either a bad choice of correspondences or to an invalid orthographic approximation. In cases of errors in the coefficients the recovery of Ax and Ay would depend linearly on the errors, while AZ is inversely dependent on the errors. This sensitivity of AZ is typical in processes of recovering depth such as stereo and motion. We should note, however, that positioning in general is performed after localization is achieved, and so the estimate of the coefficients can be improved by using a large number of points. Section 4 below presents an alternative process to lead the robot to desired positions which, due to the use of feedback, is less sensitive to errors and does not require calibration of the model images. 4. Homing The homing problem is defined as follows. Given an image, called the target image, position yourself in the location from which this image was observed. One way to solve this problem is to extract the exact position from which the target image was 338 R. Busri. E. Rivlin/Artijiciul Intelligence 78 (1995) 327-354 obtained more qualitative robot observes Unlike the transformation and direct the robot approach. Under to that position. this approach position In this section we are interested is not computed. the environment and extracts only the direction to the target Instead, in a the location. the recovery of the exact approach, between the method presented here does not require the model views. (In other words, we assume We assume we are given with a model of the environment to take new images as it is moving image. The robot is allowed begin by assuming a horizontally moving platform. degrees of freedom rather than six; the robot is allowed to rotate around and translate horizontally. The validity of this constraint in this section we shall consider homing computation that determines step the robot acquires a new image and aligns alignment next step. The algorithm on one identifiable point and moves along a circular path around the line of sight to this point coincides with the line of sight to the corresponding in the target image. In the second stage the robot advances until together with a target the target. We towards three the vertical axis in Section 5.) Later in the full 3D case. Below we give a simple in the target location. At each time the its into two stages. In the first stage the robot fixates the fixation point until point forward or retreats backward it with the model. By comparing for the target image the robot determines coefficients with the coefficients a path which terminates the target location. is discussed is divided it reaches Given a model composed of two images, PI and P2, P2 is obtained rotation about Given a target translation expressed as (see Fig. 1) the Y-axis by an angle cy, horizontal image P,, P, is obtained tt, and scale s,. Using Eq. (4) translation from PI by a similar from Pi by a t,, and scale factor s. rotation by an angle 0, the position of a target point (xy, y,) can be (21) (The rest of the coefficients are given by the coefficients are zero since the platform moves horizontally.) In fact, al = s,sin(cu - 6) sinff ’ trst sin 6 u4 = t, _ ___ ssina ’ CI? = - st sin 0 s sin (Y ’ b2 = s,. (22) (The derivation At every is given in Appendix B.) time step the robot acquires an image and aligns it with the above model. that an image P,, is obtained as a result of a rotation by an angle 4, translation Assume t,], and scale s,,. The position of a point ( xI, , y,) ) is expressed by (23) -X,’ = ClXl + c3x2 + (‘4, ?‘I> = d2.Y!, where the coefficients are given by R. Basri, E. Rivlin /Artificial Intelligence 78 (I 995) 327-354 339 Fig. 1. Illustration of the hommg task. PI and Pz are the two model images separated by an angle (Y. The target image is separated from PI by an angle 8, and the robot is positioned at an angle 4 of PI. C] = sy sin( ff - 4) sina ’ c4 = t, - t,sp sin 4 ssinff ’ c3 = - sp sin 4 s sin c~ ’ d2 = s,,. The step performed by the robot is determined by That is, s = s sin(a - 4) sin+ - ssin(a - 19) sin6 = ssina(cot4 -cotf3). (24) (25) The robot should now move so as to reduce the absolute value of 6. The direction of motion depends on the sign of LY. The robot can deduce the direction by moving slightly to the side and checking in an increase or a decrease of 6. The results is defined as follows. The robot moves to the right (or to the left, depending on motion which direction ISI) by a step Ax. if this motion reduces A new image P,, is now acquired, its new position by x,. Since the motion in this image. Denote to the image plane the depth values of the point in the two views, Pp and P,, are identical. We now want to rotate the camera so as to return to its original position. The angle of rotation, jl, can be deduced and the fixated point from the equation the fixated point is located is parallel xP =x,cosp+sinfi. (27) This equation has two solutions. We chose the one that counters if translation angle of rotation the In the next time step the new picture P, replaces P,, and the the translation to the left), and that keeps is to the right, small. the camera should (namely, rotate 340 R. Basri, E. Rivlin/Artijicial Inrelligence 78 (1995) 327-354 procedure focus. is repeated until 6 vanishes. The resulting path is circular around the point of Once coincides with or retreat backward be used satisfies to determine the robot arrives at a position that of the target to adjust its position along image, and 4 = 6) for which 6 = 0 (namely, it should now advance its line of sight forward the line of sight. Several measures can is the term c3/us which the direction of motion; one example 2 a3 _ J/’ -- sf (28) when the two lines of sight coincide. The objective at this stage is to bring this measure to 1. t, and scaling s. Given a target A similar process can be formulated of two images, Pr and PI, P2 is obtained vector tion U, translation an image and aligns as a result of a rotation U’, translation a circular path attempting terms t,, and scaling to minimize image Pt, Pt is obtained in the full 3D case. Given a model composed from Pi by a rotation matrix R, translation from PI by a rota- s,. As before, at every time step the robot acquires that an image P,, is obtained takes the absolute value of the four t,,, and scaling simultaneously s,,. Again, the robot it with the above model. Assume As is shown in Appendix B, (30) throughout (k = 1, . . . . 4) the sign of & there exists a circular path that decreases the term ~1-13 depends on the model and so it is constant where putation. The signs of & ponents of the current and the target image. Note that since only nents determine values of all four lar path can be found the direction for searching per. the com- therefore depend only on the rotation com- the rotation compo- the absolute to the sought circu- for Efficient methods in this pa- further in all Sk’s simultaneously. the possible directions will not be discussed that maximizes through through all possible directions for example by searching The direction pointing terms simultaneously. the change Once the robot arrives at a position where & = 0 (k = 1, . ...4) corresponding become equal, namely, U’ = U. This is shown in the following claim. image, P,,, and that corresponding to the current the rotation matrix to the target image, Pt, Claim. & =o (k= 1 , . ...4) implies that (I’ = U. R. Bad, E. RivlidArtificial Intelligence 78 (1995) 327-354 341 Proof. 61 = 0 implies that 41 7=-v u13 a11 u13 and Sz = 0 implies that 42 -=--_. 43 Ml2 u13 As a result, the two following vectors are identical Notice that the top rows of U’ and U are the normalized versions of these two vectors, and so clearly they also must be equal: (4, J&43) = (ullv~12~~13). Similarly, 83 = 84 = 0 implies that the middle rows of U and U’ are equal, namely (4, ,&~u~3> = (u21vu22,u23), and since the third row of a rotation matrix is given by the cross product of the first two rows we obtain that Consequently, after the robot reaches a position where all Sk vanish the line of sight of the robot coincides with the line of sight at the target image. In order to reach the target position the robot should now advance forward or retreat backward to adjust its position along the line of sight. Again, the measure ~/us can be used for this purpose since c3 -=- a3 sp sr (31) when the two lines of sight coincide. The objective at this stage is to bring this measure to 1. 5. Imposing constraints Localization and positioning require a large memory and a great deal of on-line computation. A large number of models must be stored to enable the robot to navigate and manipulate in relatively large and complicated environments. The computational cost of model-image comparison is high, and if context (such as path history) is not available the number of required comparisons may get very large. To reduce this computational cost a number of constraints may be employed. These constraints take advantage of the 342 R. Busri, E. Rivlin/Ar/ijicinl Intelligence 78 (199.5) 327-354 structure of the robot, of the navigation the properties of indoor environments, and the natural properties task. This section examines some of these constraints. One thing a system may attempt the set of models so as to reduce to do is to build in order to avoid performing the system looks satisfy (perpendicular obtained when relatively deep iterative computations. this condition. When perspective distortions subsets of views related by a translation parallel the effect of perspective distortions Views of the environment scene usually may consider modeling plane are roughly equal across all images considered, can be expressed by linear combinations large perspective distortions. This becomes apparent (Xi, x, Z;), and let (.K(. y() be the projected point after applying a rigid transformation. Assuming that Z,’ = Zi we obtain the are large the system to the image In this case the depth values of the points that novel views of two model views even in the presence of from the following derivation. Let in the image to (xi, yi) = (fXi/Z;, I < i 6 II, be a point projected to the line of sight). and it can be shown (assuming .f‘ = 1) fi/Zi), into Z;x: = rllX, + r12Y + r13Z, + f,. Z,y: = r-21X; + r22X + r232, + Iv. Dividing by Z, we obtain .x: = ~IIX, + r12y, I + r13 + t,--, Z, I yJ = r-214 + r22yi + r-23 + rJz, Rewriting this in vector equation form gives x’ = rilx + r12y + t-131 + t,C’, y’ = r2,lx + my + ml + t+-‘, (32) (33) (34) where x, y, x’, and y’ are the vectors of xi, yi, xi, and yi values respectively, 1 is a vector of all Is, and z-’ case, novel views obtained by a translation parallel by linear combinations as in the weak-perspective to the image plane can be expressed is a vector of l/Z; values. Consequently, of four vectors. An indoor environment usually provides the motion of the camera is often constrained in the XZ-plane. instead of the six degrees of freedom support. to rotation about the vertical three degrees of this constraint Such motion has only in the genera1 case. Under to align the mode1 with the image. For example, the robot with a flat, horizontal are required the coefficients u2 = 01 = b3 = b4 = 0. Three points in rather than four the coefficients by solving a linear system. Two, rather than to the possible is the fact that this motion constrains images. This fact can be used to guide the task of correspondence are also considered. Another advantage if the quadratic constraints Consequently, (I’) axis and to translation freedom fewer correspondences Eq. (4) are required three, are required considering epipolar seeking. lines between to determine (above) only horizontal motion Objects in indoor environments ticular, the relatively static objects sometimes appear in roughly planar settings. tend to be located along walls. Such objects In par- include R. Basri, E. Rivlin/Art~$icial Intelligence 78 (1995) 327-354 343 closets and tables. When shelves, pictures, is valid (for example, when windows, projection when the line of sight is roughly perpendicular any two views can be described by a 2D affine transformation. space of views of the scene becomes to three (rather is then reduced to the wall) the robot the assumption is relatively distant the transformation of orthographic from the wall, or between of the than four), and Eq. (4) The dimension x’ = atxt + azy, + a41, y’ = blXl + bzy, + 641. (35) (~1x3 = b3 = 0.) Only one view is therefore sufficient to model the scene. Most office-like indoor environments in such an environment the rooms. Not all points involves maneuvering places where the robot faces a number of options important than other places for navigation. include the thresholds of rooms and the beginnings system would therefore tend to store more models in such an environment are composed of rooms connected by corridors. through the corridors, entering important. its direction, these and ends of corridors. A than for for changing In an indoor environment for these points are equally Navigating and exiting Junctions, are more places navigation others. stepping is about the room for this task include is that they are confined for example from includes it before a decision One important property shared by many junctions the adjacent corridor. When a robot through to relatively the threshold of a room. It is a relatively narrow place small areas. Consider to enter a that separates into the room, room, a common behavior is made to enter the room or to avoid it. The images and identifying relevant that thresholds are narrow these views are related to each other almost exclusively by rotation is relatively easy around from one model view to recover. The position of points only. This is apparent Its Now, consider another position view obtained by a rotation R around the camera. The location of p in the new view is given by (assuming from the following derivation. Consider a point p = (X, I: Z). the set of views of the room from its entrance. Provided the vertical axis. Under perspective projection, is given by (x, y) = (fX/Z, p/Z). in novel views can be recovered in a model view the door, looking such a rotation f = 1) (X’TY’I = rll~ + r12Y + ~32 121x + r22Y + r23z r3,X + r32Y + r33z ’ r31x + r32y + r33z > implying that (x’, y’) = ( rlI x + n2y r31x+r32y+r33’r31x+r32y+r33 + r13 r21 x + r22y + r23 ’ > the relation between Depth is therefore not a factor in determining becomes even simpler if only rotations about the Y-axis are considered: (36) (37) the views. Eq. (37) (x’, y’) = xcos.Ly +sina Y -xsinff ( +cosa’ -xsincr+cosa ’ > (38) where LY is the angle of rotation. correspondence. In this case LY can be recovered merely from a single 344 R. Basri. E. Rivlin/Artijicial lnfelligence 78 (1995) 327-354 6. Experiments The method was implemented to images taken in an indoor environment. Images of two offices, A and B, that have similar structures were taken using a Panasonic and applied Fig. 2. Two model views of office A Fig. 3. Lines extracted the upper three blocks only. Right picture contains from the image. Left picture contains the search blocks. The lines were extracted from the lines found by the Hough transform procedure. Fig. 4. Matching a model of office A to an image of office A (left), and matching a model of office B to the same image (right). R. Basri, E. Rivlin /Artificial Intelligence 78 (I 995) 32 7-354 345 Fig. 5. Matching a model of office A to an image of the same office obtained by a relatively large motion forward and to the right. camera with a focal length of 700 pixels. Semi-static objects, such as heavy furniture and pictures, were used to distinguish between the offices. Fig. 2 shows two mode1 views of office A. The views were taken at a distance of about 4m from the wall. Candidates for correspondence were picked using the following method. The image was divided into six equal-size blocks. Candidates were picked from the upper three blocks only, assuming that the upper portion of the image is more likely to contain static features of the scene. In each block the dominant lines were selected and ranked using a Hough transform procedure. A line was ranked by the sum of the gradient values along its points. The results of this process are shown in Fig. 3. Feature points were then obtained by intersecting the obtained lines. Using the extracted feature points, recovering the coefficients of the linear combination that aligns the model with the image was done in a method similar to [ 8, lo]. Quadruples of image points were matched to quadruples of model points, and the match between the mode1 and the image using these correspondence was evaluated. The best match obtained was selected. The results of aligning the mode1 views to images of the two offices can be seen in Fig. 4. The left image contains an overlay of a predicted image (the thick white lines), constructed by linearly combining the two views, and an actual image of office A. A good match between the two was achieved. The right image contains an overlay of a predicted image constructed from a model of office B and an image of office A. Because the offices share a similar structure the static cues (the wall corners) were perfectly aligned. The semi-static cues, however, did not match any features in the image. Fig. 5 shows the matching of the mode1 of office A with an image of the same office obtained by a relatively large motion forward (about 2m) and to the side (about 1.5m). Although the distances are relatively short most perspective distortions are negligible, and a good match between the mode1 and the image is obtained. The next experiment shows the application of the iterative process presented in Sec- tion 2.2 in cases where large perspective distortion were noticeable. Fig. 6 shows two mode1 views, and Fig. 7 shows the results of matching a linear combination of the mode1 views to an image of the same office. In this case, because the image was taken from Fig. 6. Two model views of office C Fig. 7. Matching the model to an image obtained by a relatively large motion. Perspective distortions can be seen in the table, the board, and the hanger at the upper right. close distance, perspective distortions cannot be neglected. The effects of can be noticed on the right corner of the board, and on the edges the iterative effects were reduced by using this procedure after one and three iterations are shown a relatively perspective distortions of the hanger on the top right. Perspective process. The results of applying in Fig. 8. Another to a corridor an accurate match results still demonstrate two model views of the corridor. Fig. 10 (left) scene. Here, because of the set of experiments was applied are noticeable. Nevertheless, the deep structure of the corridor, perspective distortions in large portions of the image. alignment shows an overlay of a Fig. 9 shows of the model views with an image of the corridor. It can be seen that linear combination the parts that are relatively distant align perfectly. Fig. 10 (right) shows the matching of the corridor model with an image obtained by a relatively (about half of the the relatively near features no longer corridor far edges, however, still match. Fig. 11 align shows the result of applying the iterative process for reducing perspective distortions on the scene. The process converged after three iterations. length). Because of perspective distortions (e.g., the near door edges). The relatively large motion The experimental results demonstrate that the method achieves accurate localization R. Basri, E. Rivlin/Artifcial Intelligence 78 (1995) 327-354 347 Fig. 8. The results of applying the iterative process to reduce perspective distortions after one (left) and three (right) iterations. Fig. 9. Two model views of a corridor. Fig. 10. Matching the corridor model with two images of the corridor. The right image was obtained by a relatively large motion forward (about half of the corridor length) and to the right. Note that the results of alignment when the picture is taken roughly under the conditions of Eq. (34) (left) are better then when these conditions are violated (right). 348 R. Basri. E. Rivlin/Artijicial Intelligence 78 (1995) 327-354 Fig. I 1. Results of applying the iterative process to reduce perspective distortions after three iterations. in many cases, and that when the method an iterative computation can be used to improve fails because of large perspective distortions the quality of the match. 7. Conclusions We presented a method for localization linear combinations and positioning the scene as a set of 2D views and predicting from visual input. The method the appearance of the model views. The method accurately projection. Analysis that in many cases this results demonstrate of scenes under weak-perspective as well as experimental the appearances is sufficient is invalid, either a larger number of models can be acquired or an iterative the scene. When the weak-perspective to accurately describe is based on representing of novel views by approximates of this projection approximation approximation solution can be employed to account for the perspective distortions. Using our method we presented a solution to the homing problem. The solution advantage of the 2D representation. The homing process in a simple and qualitative manner. Specifically, transformation the model between images. takes is done in the image domain the recovery of the it does not require The method presented rich representations; in this paper has several advantages over existing methods. than 3D, and can be done from a single 2D view only without calibration. The same basic and positioning problems. Future work includes indexing the complexity of the localization process, and building maps using is used in both the localization the problem of acquisition to reduce of models, constructing the representations and maintenance are 2D rather It uses relatively localization method handling methods visual input. Appendix A. Projection model-error analysis In this appendix we estimate method assumes a weak-perspective the more accurate perspective projection model. We start by deriving the error obtained by using the localization method. The this assumption with the error between projection model. We compare R. Basri, E. RivlidArtifcial Intelligence 78 (1995) 327-354 349 a true perspective image and its orthographic approximation, and then we compute the error implied by assuming a weak-perspective projection for both the model and the image. A point (X, Y Z) is projected under the perspective model to (x, y) = (fX/Z, p/Z) in the image, where f denotes the focal length. Under our weak-perspective model the same point is approximated by (2, 9) = (sX, sY) where s is a scaling factor. The best estimate for s, the scaling factor, is given by s = f/a, where .Zc is the average depth of the observed environment. Denote the error by E = 12 - xl. The error is expressed by E=lj-X(&+)1. Changitq : to image coordinates E= xz(&-;)l or E= 1. Z x( z-1. I I (A.1) (A.21 (A.3) (A.4) The error is small when the measured feature is close to the optical axis, or when our estimate for the depth, Zc, is close to the real depth, Z. This supports the basic intuition that for images with low depth variance and for fixated regions (regions near the center of the image), the obtained perspective distortions are relatively small, and the system can therefore identify the scene with high accuracy. Figs. A.1 and A.2 show the depth ratio Z/Zc as a function of x for E = 10 and 20 pixels, and Table A.1 shows a number of examples for this function. The allowed depth variance, Z/Zc, is computed as a function of x and the tolerated error, E. For example, a 10 pixel error tolerated in a field of view of up to f50 pixels is equivalent to allowing depth variations of 20%. From this discussion it is apparent that when a model is aligned to the image the results of this alignment should be judged differently at different points of the image. The farther away a point is from the center the more discrepancy should be tolerated between the prediction and the actual image. A five pixel error at position x = 50 is equivalent to a 10 pixel error at position x = 100. So far we have considered the discrepancies between the weak-perspective and the perspective projections of points. The accuracy of the scheme depends on the validity of the weak-perspective projection both in the model views and for the incoming image. In the rest of this section we develop an error term for the scheme assuming that both the model views and the incoming image are obtained by perspective projection. The error obtained by using the scheme is given by E = 1.x - axI - by, - cx2 - dl . (A.51 350 R. Basri. & Rivlin /Art$cial Intelligence 78 (1995) 327-354 Fig. A. I. Z/Z,, as a function of .X for E = IO pixels. 0 so 100 150 200 250 300 Fig. A.2. Z/S& as a function of x for E = 20 pixels. Table A.1 Allowed depth ratios. Z/Z,,, as a function of x (half the width of the field considered) (E, in pixels) and the error allowed d \ F: 2s SO IS 100 5 I.2 I.1 I .07 I .os IO I .4 I 2 1.13 1.1 IS 1.6 1.3 I .2 1.15 20 1.8 I .4 I .21 1.2 the scheme accurately predicts the appearances of points under weak-perspective Since projection, it satisfies A = ~4, - bj$ - ci2 - d, where accented model pictures letters represent orthographic the depth ratios are roughly equal: approximations. Assume (A.61 that in the two (‘4.7) R. Basri, E. Rivlin/Arr$cial Intelligence 78 (1995) 327-354 351 (This condition camera only is satisfied, translates along for example, when between the image plane.) Using the fact that the two model images the x=fif_fx~ ZJ _--==f- z’ ZfJz z we obtain E = Ix - axl - byI - cx2 - dl (A.81 (A.9) to the center of the frame and as the difference between The error therefore depends on two terms. The first gets smaller as the image points get closer the depth ratios of the model and the image gets smaller. The second gets smaller as the translation gets smaller and as the model gets close to orthographic. this analysis, weak-perspective can be used as a projection model when component Following the depth variations in the scene are relatively on the center part of the image. We conclude of the environment, positioning. the linear combinations low and when that, by fixating on distinguished scheme can be used for localization the system concentrates parts and Appendix B. Coefficients values for homing In this appendix we derive the explicit values of the coefficients of the linear combi- for the case of horizontal motion. Consider a point p = (x, y, z ) that is projected from PI by a ro- t,, and scale factor s,,, and P’ is tp and scale s,,. nations by weak-perspective tation about obtained The position of p in the three images to three images, PI, P2, and P’, P2 is obtained the Y-axis by an angle a, translation the Y-axis by an angle 8, translation from PI a rotation about is given by (XltYl) = (KY>, (x2,y2) = (~,,~cosa+s,,zsin~~+f~~,~~~y), (2,~‘) = (s,xcos~+s,zsinB+t,~,s,y). The point (x’, y’) can be expressed by a linear combination of the first two points: 352 R. Basri. E. Rivlin/Artijicial Intelligence 78 (1995) 327-354 I x = UlXl + u2x2 + u3, y’ = by,. Rewriting these equations we get s,,xcosB+s,,zsin8+t,, =a~.r+a~(.s,,,xcoscu+s,,zsincuft,,) +a3, spy = by. Equating the values for the coefficients in both sides of these equations we obtain S,’ cos 0 = u1 + QS,, cos(Y, r,> = a2t,,t + a3. s,’ sin 0 = UPS,, sin LY, .s/, = 0, and the coefficients are therefore given by ut = sp sin( (Y - @) sina ’ (14 = t,, ~- t,,,~,, sin 0 s,,, sin (Y ’ a3 = s,, sin 0 . sn, sin a . b = s,, Similarly, we can derive terms describing model composed of two images PI and P2 and an image Pt, P, is obtained a rotation U, translation (x,, y,) can be expressed as in the 3D case. Given a from PI by f,r fV, tf,) and scaling st, the position of a target point the coefficients t, = (t _. t x/ = alxl + a2yj -I- u3x2 + (14, yr = hxl + bz_m + bm + bq. Using Eq. ( 16) (Section 3) we obtain that the coefficients are given by h=---, stu23 ST13 a4 = t,, - .ytu13 -t so3 x9 Similarly, ( tpx, t,,! , t,,Z > and scaling sp, the position of a point (x,, , yp ) is expressed by from PI by a rotation U’, translation image P,) obtained given an t, = X,’ = CIXI + c2yt + c3x2 + C4. y,, = ~IXI + d2y1 + &x2 + d4. where the coefficients are given by R. Basri, E. Rivlin/Artificial Intelligence 78 (1995) 327-354 353 cI=sp(u;,-u;3~), d,=sp(u;,-u;3~), ci=s,(u:,-u:,$. &=sp(u:2-u;3~)r St& d3 = - w3 ’ d4 = tpg - ety. w/13 c3 = - sr13 ’ c4 = t,, - t w’13 - sr13 ” We define the terms 63=$_$, s,=---, Cl ~3 a1 a3 (j2=c2_a2, a3 ~3 Substituting for the coefficients we obtain that sn3, > u21 ~23 u22 G > sr*3* References III 121 131 141 [51 161 171 [81 191 101 Image Image Understanding Workshop ( 1992) 875-884. from orthographic and perspective projections, system for recognizing world locations with stereo vision, AI-TR-1229, MIT, N. Ayache and O.D. Faugeras, Maintaining representations of the environment of a mobile robot, IEEE Trans. Robotics Automation 5 (1989) 804-819. R. Basri, On the uniqueness of correspondence Proceedings R. Basri and S. Ullman, The alignment of objects with smooth surfaces, Comput. Vision Graph. Process. Image Understanding 57 (3) ( 1993) 33 I-345. D.J. Braunegg, Marvel-a Cambridge, MA ( 1990). D.F. DeMenthon and L.S. Davis, Model-based object pose in 25 lines of code, Proceedings 2nd European Conference on Computer Vision, Genova, Italy ( 1992). S.P Engelson and D.V. McDermott, Image signatures for place recognition and map construction, Proceedings SPIE Symposium on Intelligent Robotic Systems, Boston, MA ( 199 1) . C. Fennema, A. Hanson, E. Riseman, R.J. Beveridge and R. Kumar, Model-directed mobile robot navigation, IEEE Trans. Syst. Man Cybernetics 20 ( 1990) 1352- 1369. M.A. Fischler and R.C. Belles, Random sample consensus: a paradigm for model fitting with application to image analysis and automated cartography, Commun. ACM 24 ( 198 1) 38 I-395. J. Hong, X. Tan, B. Pinette, R. Weiss and E.M. Riseman, Image-based homing, IEEE Control Systems (1992) 38-44. D.P Huttenlocher and S. Ullman, Object recognition using alignment, Int. J. Comput. Vision 5 (2) (1990) 195-212. 1 l] J. Koenderink and A. van Doom, Affine structure from motion, J. Optical Sot. America 8 (2) (1991) 377-385. 121 .I. Lawn and R. Cipolla, Epipole estimation using affine motion parallax, Proceedings British Machine Vision Conference (1993) 379-388. 354 K. Basri. E. Hivlin/Artificial Intelligence 78 (1995j 327-354 [ I3 1 D.G. Lowe. Three-dimensional object recognition from single two-dimensional images, Robotics Research Technical Report 202, Courant ( 1985). Institute of Mathematical Sciences, New York University 1 141 R.N. Nelson, Visual homing using an associative memory, DARPA Image Understanding Workshop (1989) 245-262. ( I5 1 K. Onoguchi, M. Watanabe, Y. Okamoto. Y. Kuno, and H. Asada, A visual navigation system using local map. Proceedings lntrrnutional Conference on Robotics and Automation, on a result by Basri and Ullman, Technical Report 9005-03, IRST, information a multi Cincinnati, OH ( 1990) 767-774. recognition: ( I6 1 T. Poggio, 3D object Italy ( 1990). Povo, ( 17) K.B. Sarachik. Visual navigation: constructing and utilizing simple maps of an indoor environment, AI-TR-1113, MIT, Cambridge, MA ( 1989). 1 18 1 D.W. Thompson and J.L. Mundy. Three dimensional model matching from an unconstrained viewpoint. Proceedings Inrernafionul Confererwe on Robotics ctnd Auromation, Raleigh, NC ( 1987) 208-220, 1 19 1 S. Ullman. Aligning 193-254. pictorial descriptions: an approach to object recognition, Crpifion 32 ( 1989) 1201 S. Ullman and R. Basri, Recognition Intell. 13 ( I99 I ) 992- 1006. Mrrchine by linear combinations of models. IEEE Trans. Pattern Ann/. (21 1 D. Wilkes, S. Dickinson, E. Rivlin and R. Basri, Navigation based on a network of 2-D images, Proceedings Infernutionul Conference on Puttern Recognition (ICPR-94). Jerusalem, Israel ( 1994). 1221 D. Zipser, Biologically and J.L. McClelland plausible models of place recognition and goal location, in: D.E. Rumelhart, the P.D.P. Group, eds.. Purullel Distributed Processing: Explorurions in he Mu~rostrucfure of Co,qnirion 2: Psychologiccd and Biologicul Models (MIT Press, Cambridge, MA. 1986) 432-47 I. 