Artificial Intelligence 72 (1995) 81-138 Artificial Intelligence Learning to act using real-time dynamic programming Andrew G. Barto *, Steven J. Bradtke ‘, Satinder I? Singh2 Department of Computer Science, University of Massachusetts, Amherst, MA 01003, USA Received September 199 1; revised February 1993 Abstract Learning methods based on dynamic programming (DP) are receiving increasing attention in artificial intelligence. Researchers have argued that DP provides the appropriate basis for compiling planning results into reactive strategies for real-time control, as well as for learning such strategies when the system being controlled is incompletely known. We introduce an algorithm based on DP, which we call Real-Time DP (RTDP), by which an embedded system can improve its performance with experience. RTDP generalizes Korf’s Learning-Real-Time-A* algorithm to problems involving uncertainty. We invoke results from the theory of asynchronous DP to prove that RTDP achieves optimal behavior in several different classes of problems. We also use the theory of asynchronous DP to illuminate aspects of other DP-based reinforcement learning methods such as Watkins’ Q-Learning algorithm. A secondary aim of this article is to provide a bridge between AI research on real-time planning and learning and relevant concepts and algorithms from control theory. 1. Introduction The increasing in environments lem solving embedded of dynamic interest of artificial demanding intelligence (AI) researchers real-time performance is narrowing and control engineering. Similarly, machine learning systems are becoming more comparable systems. A growing number of researchers are investigating to methods in systems embedded the gulf between prob- to for the adaptive control learning systems techniques suited author. E-mail: barto@cs.umass.edu. * Corresponding ’ Present address: GTE Data Services, One E. Telcom Parkway, Temple Terrace, FL 33637, USA. 2 Present address: Department Cambridge, MA 02139, USA. of Brain and Cognitive Sciences, Massachusetts Institute of Technology, 0004.3702/95/.$09.50 SSDIOOO4-3702(94)00011-O @ 1995 Elsevier Science B.V. All rights reserved 82 A.G. Burro et d/Artificial Intelligence 72 (1995) 81-138 arguing (DP) algorithms that DP provides the appropriate basis for compiling planning for solving stochastic optimal control results for real-time control, as well as for learning such strategies when algorithms based on DP efficiency of conventional DP versions of DP as incremental and reacting for learning, planning, learning learn [ 83,871 and Watkins and Sutton’s Dyna architecture is based on these principles. The key issue addressed by DP-based how can an agent short- and long-term performance: the computational known. Learning [ 8 1 ] proposed is incompletely for improving based on dynamic programming problems, into reactive strategies the system being controlled employ novel means algorithms. Werbos learning algorithms, [69,70] is the tradeoff between improve long-term mance? DP-based by which autonomous explicit teachers [ 7 11. performance when learning algorithms this may require are examples of reinforcement sacrificing agents can improve skills in environments to short-term perfor- learning methods that do not contain In this article we introduce performance with experience, types of problems. RTDP A* (LRTA*) and we prove is the result of recognizing algorithm3 [ lo]. This novel observation the ideas behind LRTA* so that they apply to real-time problem solving its long-term in several different [38] Learning-Real-Time a learning algorithm based on DP, which we call Real- (RTDP), by which an embedded problem solving system results about Time Dynamic Programming can improve its behavior that Korf’s form of DP known as asynchronous DP generalize involving uncertainty. by Bertsekas to optimal involving suitability of asynchronous DP for parallel processing, we adapt this theory of performing DP concurrently with problem extension of RTDP, called Adaptive RTDP, applicable when information a problem’s to a permits us to tasks In particular, we apply the theory of asynchronous DP developed that RTDP converges tasks types of real-time problem the theory of asynchronous DP was motivated by the to the case solving or control. We also present an is lacking about [IO] and Bertsekas solutions when applied and Tsitsiklis to several uncertainty. Whereas [ 121 to show to its solution. in addition is closely structure solving related Recognizing that the theory of asynchronous DP is relevant algorithm which to provide new insight learning results comparing ventional DP algorithm on several simulated uncertainty. the performance into Watkins’ Q-Learning to learning also permits us [ 81,821 algorithm, another DP-based is being explored by AI researchers. We present simulation and a con- tasks involving of RTDP, Adaptive RTDP, Q-Learning, real-time problem solving Another aim of this article is to discuss some of the important issues that arise in learning algorithms, with particular attention being devoted to indicating using DP-based which aspects of their use have formal we attempt relevant concepts that we believe are most relevant capable of performing from control to clarify justification links between AI research on real-time planning and which do not. In doing theory. We discuss selected concepts from control to the efforts in AI to develop autonomous this, and learning and theory systems in real time and under uncertainty. 3 We use the term real-time following this usage by Korf in which it refers to problems have to be performed under hard time constraints. We do not address details of the scheduling arise in using these algorithms as components of complex real-time systems. in which actions that issues A.G. Barto et al./Art$cial Intelligence 72 (1995) 81-138 83 issues relevant to using DP-based in AI that we do and do not say much is to a wide variety of specific problems, but it is not easy to specify take advantage of these in AI. A formalism systems can best this abstract formalism learning But there remain many applicable not discuss. For example, we adopt a rather abstract about how it might best apply to problems of interest potentially exactly what subproblems within complex methods. learning developing them. In accord with Dean and Wellman as a component sophisticated technology embedded that addresses [ 231, we regard DP-based for some of the issues agents but that by itself does not address all of reinforcement important between Because the reader is unlikely of the proper some concepts in Section 2, followed to be familiar with all of the contributing the necessary background relationship lines of in Section 3 by from AI and control of the theoretical material occupies Sections 4 through 9, with an to a class of stochastic optimal control problems occupying Section 4 and an to conventional DP occupying Section 5. There are two major parts to this for which properties, the additional i.e., when an accurate model learning algorithms research, we provide a discussion theory. Development introduction introduction theoretical development. The first part (Sections 5 and 6) concerns problems are available. Here, we describe RTDP, accurate models and complexity of the problem that are outside the issues In Section 10 we use an example problem conclude some of the open problems. In Section 9 we discuss some of learning algorithms must address. to illustrate RTDP and other algorithms. We in Section 11 with an appraisal of the significance of our approach and discuss is lacking. Section 8 is a brief discussion of DP-based scope of this article. of DP-based its relationship present to LRTA*. The second part in the case of incomplete (Section 7) concerns its convergence implementations the theoretical that practical information, 2. Background A major influence on research leading [61,62] method Samuel of checkers. His method updated board evaluations current board position with an evaluation of a board position game: used to modify a heuristic evaluation by comparing likely to current DP-based algorithms has been function the for the game an evaluation of the to arise later in the . . . we are attempting look like that calculated which most probably occur during actual play. (Samuel for the current board position, for the terminal board position of the chain of moves to make the score, calculated [ 611) As a result of this process of “backing up” board evaluations, improve in its ability should version of this algorithm, Samuel features and adjusted of numerical evaluations of current and predicted board positions. to evaluate the long-term function of moves. In one represented function as a weighted sum the weights based on an error derived from comparing the evaluation the evaluation consequences Because of its compatibility with connectionist learning algorithms, was refined and extended by Sutton [ 67,681 and used heuristically this approach in a number of 84 A.G. Barto et al./Arhjicial Intelligence 72 (1995) 81-138 some solving problem theoretical [ 41. Sutton as a neuron-like Following these methods [ 671) . The algorithm was implemented tasks (e.g., Barto, Sutton, and Anderson ideas in the context of the credit assignment problem (TD) methods and obtained the proposals of Klopf as models of animal single-agent [ 11, and Sutton element called the Adaptive Critic Element Temporal Difference convergence. developed similar systems; Hampson to animal behavior; Christensen for updating evaluation bucket-brigade to Samuel’s method. Tesauro’s recent TD-Gammon together with a connectionist has achieved [4], Anderson connectionist [ 681 later called these algorithms their [72-741 discussed learning them developed some of these ideas and related and Korf [ 161 experimented with a Samuel-like method [ 301 related [ 771, a program using a TD method in playing backgammon, [ 36,371, Sutton and Barto learning. Minsky function coefficients using linear regression; and Holland’s is closely credit in his classifier systems to improve performance [ 281 independently for reinforcement results about for assigning remarkable algorithm success. network [53,54] Independently of the approaches inspired by Samuel’s re- similar algorithms based on the theory of optimal control, where to control problems, DP (a term optimal approximating and stochastic problems. solution methods. As applied [ 91) consists of methods for successively rules for both deterministic checkers player, other in the search space have a compositional suggested and decision form, DP applies searchers important DP provides introduced by Bellman evaluation functions In its most general objects an object of globally minimum Kanal as it applies in problem recurrence ing up state evaluations relations. We discuss several DP algorithms to problems solving or control relations in which the objects are state sequences these optimization tasks. DP solves instead of explicitly searching is the basic step of DP procedures in detail in Section 5. structure problems in which that can be exploited the costs of to find search. Kumar and to DP that can be generated problems by solving in the space of state sequences. Back- these recurrence for solving cost without performing [40] discuss DP at this level of generality. However, we restrict attention exhaustive to optimization in contrast, are explicitly designed Although DP algorithms avoid exhaustive search in the state-sequence they require repeated generation still exhaustive by AI standards because of all possible states. For this reason, DP has not played a significant search algorithms, way. But DP algorithms are relevant are not because adjust a problem’s heuristic evaluation shallow searches. Although estimates of the costs typically do not update goal from each state (the h function). 4 space, they are and expansion role in AI. Heuristic in this to avoid being exhaustive in a way that heuristic search algorithms they the results of repeated such as A* [ 291, update they (A*‘s g function), the cost to reach a some heuristic search algorithms, state to reach states the heuristic evaluation the evaluations of the states; function by incorporating to learning update function estimating they systematically from an initial in effect, Despite it is possible the fact that DP algorithms to arrange their computational are exhaustive above, steps for use during control or real-time in the sense described 4 We have found only a few exceptions M&6 [ 5 I] and Gelperin [ 261. Although functions, they were developed independently of DP to this in the heuristic these algorithms use DP-like backups search literature in algorithms proposed by to update heuristic evaluation A.G. Barto et al. /Artijicial Intelligence 72 (1995) 81-138 85 problem solving. This is the basis of RTDP and the other algorithms we describe in this article. In most cases, convergence to an optimal evaluation function still requires repeated generation and expansion of all states, but performance improves incrementally (although not necessarily monotonically) while this is being accomplished. It is this improvement rather than ultimate convergence to optimality that becomes central. This perspective was taken by Werbos [ 851, who proposed a method similar to that used by the Adaptive Critic Element within the framework of DP He called this approach Heuristic Dynamic Programming and has written extensively about it (e.g., [ 83,86- 88] ). Related algorithms have been discussed by Witten [92,93], and more recently, Watkins [ 811 extended Sutton’s TD algorithms and developed others by explicitly uti- lizing the theory of DP He used the term Incremental Dynamic Programming to refer to this class of algorithms and discussed many examples. Williams and Baird [91] theoretically analysed additional DP-based algorithms suitable for on-time application. We have also come across the work Jalali and Ferguson [ 321, who independently pro- posed a method similar to Adaptive RTDP Sutton, Barto, and Williams [ 751 discussed reinforcement learning from the perspective of DP and adaptive control, and White and Jordan [89] and Barto [2] provide additional background and extensive references to current research. Although aspects of this approach also apply to problems involving continuous time and/or state and action spaces, here we restrict attention to discrete-time problems with finite sets of states and actions because of their relative simplicity and their closer relationship to the non-numeric problems usually studied in AI. This excludes various “differential” approaches, which make use of optimization algorithms related to the connectionist error-backpropagation algorithm (e.g., Jacobson and Mayne [ 3 11, Jordan and Jacobs [ 331, Werbos [ 83,841, White and Jordan [ 891). The relevance of DP for planning and learning in AI was articulated in Sutton’s [69] L?ym architecture. The key idea in Dyna is that one can perform the computational steps of a DP algorithm sometimes using information obtained from state transitions actually taken by the system being controlled, and sometimes from hypothetical state transitions simulated using a model of this system. To satisfy time constraints, this approach interleaves phases of acting with planning performed using hypothetical state transitions. The underlying DP algorithm compiles the resulting information into an efficient form for directing the future course of action. Another aspect of Dyr~ is that the system model can be refined through a learning process deriving training information from the state transitions observed during control. Even without this on-line model refinement, however, executing a DP algorithm concurrently with the generation of actions has implications for planning in AI, as discussed by Sutton in [ 701. In this article, we introduce the fact that the theory of asynchronous DP is applicable to the analysis of DP-based reinforcement learning algorithms. Asynchronous DP algo- rithms differ from conventional DP algorithms in that they do not have to proceed in systematic exhaustive sweeps of the problem’s state set. Bertsekas [ lo] and Bertsekas and Tsitsiklis [ 121 proved general theorems about the convergence of asynchronous DP applied to discrete-time stochastic optimal control problems. However, because they were motivated by the suitability of asynchronous DP for parallel processing, they did not relate these results to real-time variants of DP as we do in this article. To the best 86 A.G. Bar/o ef a/. /Artificial Intelligence 72 (1995) 81-138 of our knowledge, asynchronous DP for real-time control the only other work in which explicit use is made of the theory of is that of Jalali and Ferguson [ 321. Korf’s the current state by generating is a heuristic search algorithm improves with repeated [ 381 LRTA* algorithm that caches state evalua- trials. Evaluations of the states tions so that search performance in a hash table. Each cycle of the algorithm visited by the problem solver are maintained successor all of its immediate proceeds by expanding if they exist in the hash states and evaluating them using previously the function. Assuming table, and otherwise using an initially given heuristic evaluation for each objective the cost of the edge to it from the current neighboring state. The minimum for the current state, which is stored in the hash table.’ Finally, a move is made to this lowest-scoring the same way therefore backs up state evaluations neighboring as do Samuel’s algorithm and DI? In fact, as we shall see in what follows, with a slight caveat, LRTA* specialization of asynchronous DP applied on-line. path to a goal state, a score is computed is to find a minimum-cost is the deterministic the new evaluation stored evaluations to its evaluation state by adding scores becomes of the resulting state. LRTA” in much 3. Heuristic search and the control of dynamic systems relatively Whereas AI has focused on problems having structure, theorists have studied more restrictive classes of problems but have developed theory for example, by Dean the stochastic optimal between theory control correspondingly more detailed are nevertheless and Wellman control heuristic search, real-time heuristic search, and selected concepts theories. Some concepts and methods from control in AI as discussed, to introducing in which our results are cast, we discuss the relationship from control [ 231. In this section, as a prelude to problems of interest little mathematical framework relevant 3.1. Heuristic search and system control Heuristic optimizes (possibly) that maps to state-space search algorithms to states, an initial is to find a sequence of operators apply that map states states, a set of operators states. The objective one of the goal states and the solution path. These components as solving a puzzle, proving a theorem, or planning used in the literature on heuristic deciding what to do next in manipulating some similarities, it refers to the process of manipulating by supplying process, whereas time. Unlike models manipulated search problems defined by a set of state, and a set of goal the initial state to some measure of cost, or merit, of such a robot path. The term control as the process of in question. Despite theory, where in real time the formal search In AI, control specifies system over the behavior of a physical by search algorithms, physical systems cannot be set this is not the meaning of the term control it with appropriate in control a model of some real problem, input signals. it steers the behavior of a physical a model of the problem search and problem solving means in control constitute theory, system 5 In Korf’s 1381 related Real-Time A* (RTA*) algorithm, score to control and DP than is RTA*, we do not discuss RTA*. the second smallest LRTA* is more closely related is stored. Because A.G. Barto et al. /Artificial Intelligence 72 (1995) 81-138 87 systems, not the control of search. states and do not suspend activity to await the controller’s system control problems, called dynamic systems, the passage of time. In what follows, by control we into arbitrary immediately decisions. Models used to formalize are explicit mean in taking the control of dynamic into account In many applications, a symbolic from sequence the methods is the control engineer’s that it is applied the final objective of a heuristic operator Here the result substantially inputs, or actions, produced policy, meaning system’s actual behavior while control or feedback. for producing priate for the given design procedure because under normal circumstances, precedes the execution phase. In terms of control an open-loop to generate a time sequence of actual representation of a sequence of operators search algorithm. The intent may be to execute is not the system. form of control, but this control method differs theory. A sequence of in this way through heuristic search is an open-loop control the addressed by most of control to the system without using to a physical inputs is underway, theory, heuristic control policy from a system model; initial state. Further, under normal circumstances, it is completed before being used to control the planning phase of the problem about information i.e., without execution monitoring, search is a control design procedure is appro- it is an of-line i.e., the system, solving process strictly the policy is a completely accurate model of the physical Open-loop to determine (2) the physical system’s is deterministic, and (4) for some of the problems problems. Any uncertainty, the process of modeling performance. Control of the real system, perhaps to the controller. control works fine when all of the following are true: ( 1) the model used system, the control policy (3) the physical system hold in AI, but they are not true for most realistic control itself or in in the behavior of the physical that closed-loop control can produce better is closed-loop when each action depends on current observations internal initial state can be exactly determined, there are no unmodeled studied either disturbances. These conditions together with past observations and other information the system, implies system for example, by Chapman system. It closely corresponds A closed-loop control policy (also called a closed-loop is a rule specifying about the behavior of the controlled [ 641 as discussed, In control of the controlled ables (a distinction whose significance [ 141) . Although closed-loop counteracts deviations a special case of closed-loop control rule, law, or strategy) each action as a function of current, and possibly past, information to a “universal plan” [ 651. control policy usually specifies each action as a function the current values of observable vari- is discussed by Chapman is closely associated with negative feedback, which is merely from desired system behavior, negative feedback control system’s current state, not just for universal planning [ 271, and Schoppers theory, a closed-loop [ 141, Ginsberg control. control there When control closed-loop is no uncertainty, policy and an initial control. For a deterministic is not in principle more compe- given any tent than open-loop that produces closed-loop the open-loop policy generated by running exactly the system, or simulating it with a perfect model, under control of the given closed-loop policy. But this is not true in the stochastic case, or when there are unmodeled distur- in bances, because the outcome of random and unmodeled state, there exists an open-loop policy the same system behavior, namely, system with no disturbances, events cannot be anticipated 88 A.G. Barto et al./Art@cial Intelligence 72 (1995) 81-138 systems always use closed-loop is a kind of disturbance. A game player always the its next move. For exactly the opponent for this reason: actual previous moves in determining an open-loop policy. Note that game-playing designing control uses the opponent’s same reasons, closed-loop problems of closed-loop acceptable it is used for designing only becomes a practical alternative when it is expensive or impossible controlled control by control engineers: control policy can be significantly control can be better than open-loop control uncertainty. A corollary of this explains system’s behavior with detail sufficient for closed-loop less faithful closed-loop involving control, the almost universal use an to the actual system when control the to monitor instead of open-loop policies. Open-loop the system model used for designing for single-agent for determining that an accurate model of the system theory addresses Most control off-line under the assumption is available. The off-line design procedure method is possible control problems re-design, control objectives, on the other hand, often does require policy re-design. the problem of designing adequate closed-loop policies to be controlled efficient system state. If it policy off-line, as it is in many of the then it is not necessary to perform any additional instances differing only in initial state. Changing studied by engineers, for problem each action as a function of the observed typically yields a computationally to design a complete i.e., re-planning, closed-loop One can also design closed-loop policies on-line through repeated on-line design of the [42,50]. is discarded, a finite-horizon is designed with the current state playing for searching the first action specified by the resulting policy, is repeated requiring on-line design, which in AI corresponds open-loop policies. This approach has been called receding horizon control For each current state, an open-loop policy role of the initial state. The design procedure must terminate within the time constraints imposed by on-line operation. This can be done by designing open-loop to a fixed depth from the current policy, for example, by using a model the remainder of state. After applying state. the policy through Despite projection, a policy. control policy through According in a fixed amount of time to retain projection, but each planning phase has to complete that design the system’s to methods closed-loop in control objectives. to each current system state, i.e., a closed-loop to this view, then, a closed-loop policy can involve explicit planning for the next observed to on-line planning receding horizon control can react on-line to the observed system states. In contrast reactivity policies off-line, receding horizon control produces using a system model, and the design process that is reactive or prediction, to changes 3.2. Optimal control trajectory These are called The most familiar control objective a reference output or tracks a reference of disturbances. regulation optimal control problem, on the other hand, function of the controlled terms of a reference output or trajectory. One typical optimal control problem controlling trajectory. is to control a system so that its output matches in the face as closely as possible In an some in requires and tracking problems the control objective system’s behavior, where this function need not be defined state to a goal state via a minimum-cost respectively. is to extremize to tracking problems-where a system In contrast to go from an initial the desired is part of trajectory A.G. Barto et al. /Artificial Intelligence 72 (1995) N-138 89 trajectory is part of the solution of this optimal control the problem specification-the problem. Therefore, optimal control problems such as this are closely related to the problems to which heuristic search algorithms apply. Specialized solution methods exist for optimal control problems involving linear sys- tems and quadratic cost functions, and methods based on the calculus of variations can yield closed-form solutions for restricted classes of problems. Numerical methods applicable to problems involving nonlinear systems and/or nonquadratic costs include gradient methods as well as DP Whereas gradient methods for optimal control are closely related to some of the gradient descent methods being studied by connection- ists (such as the error-backpropagation algorithm [ 43,86,89] ), DP methods are more closely related to heuristic search. Like a heuristic search algorithm, DP is an off-line procedure for designing an optimal control policy. However, unlike a heuristic search algorithm, DP produces an optimal closed-loop policy instead of an open-loop policy for a given initial state. 3.3. Real-time heuristic search Algorithms for real-time heuristic search as defined by Korf [ 381 apply to state-space search problems in which the underlying model is extended to account for the passage of time. The model thus becomes a dynamic system. Real-time heuristic search algorithms apply to state-space search problems with the additional properties that ( 1) at each time there is a unique current state of the system being controlled, which is known by the searcher/controller, (2) during each of a sequence of time intervals of constant bounded duration, the searcher/controller must commit to a unique action, i.e., choice of operator, and (3) the system changes state at the end of each time interval in a manner depending on its current state and the searcher/controller’s most recent action. These factors imply that there is a fixed upper bound on the amount of time the searcher/controller can take in deciding what action to make if that action is to be based on the most up-to-date state information. Thus, whereas a traditional heuristic search algorithm is a design procedure for an open-loop policy, a real-time heuristic search algorithm is a control procedure, and it can accommodate the possibility of closed-loop control. Korf’s [38] LRTA* algorithm is a kind of receding horizon control because it is an on-line method for designing a closed-loop policy. However, unlike receding horizon control as studied by control engineers, LRTA* accumulates the results of each local design procedure so that the effectiveness of the resulting closed-loop policy tends to improve over time. It stores information from the shallow searches forward from each current state by updating the evaluation function by which control decisions are made. Because these updates are the basic steps of DP, we view LRTA* as the result of interleaving the steps of DP with the actual process of control so that control policy design occurs concurrently with control. This approach is advantageous when the control problem is so large and unstructured that complete control design is not even feasible off-line. This case mathematically requires a partial closed-loop policy, that is, a policy useful for a subregion of the problem’s state space. Designing a partial policy on-line allows actual experience to influence the subregion of the state space where design effort is concentrated. Design 90 A.G. Bcwio et 01. /Artificial htelli~ence 72 (1995) 81-138 effort is not expended actual control. Although in general for a subset of the states unless the design procedure considers possible under certain conditions for LRTA*. for parts of the state space that are not likely to be visited during that is optimal the entire state set, this is theorem such as those required by Korf’s convergence to design a policy it is not possible 3.4. Adaptive control and system is not available interact. A distinction over time as the controller theorists use the term adaptive control Control for cases in which an accurate model to be controlled of the system a policy off-line. These for designing are sometimes called control problems with incomplete information. Adaptive control algorithms design policies on-line based on information that about the control problem is sometimes accumulates made between adaptive control and learning control, where only the latter takes advan- tage of repetitive control experiences that is useful for some types of control over the long and to the kinds of problems problems, we think to what we mean by adaptive control algorithms we consider [61] are though algorithms in this article, even they assume learning the they are not adaptive control algorithms because algorithms, it certainly seems existence of an accurate model of the problem being solved. Although is not ipso facto adaptive, odd to us that a control algorithm this is forced that learns restrictive definition of adaptive control. upon us when we adopt the control engineer’s In Section 7 we describe that have properties of both learning and adaptive control algorithms. from which information this distinction may be useful is limited when applied term. Although its utility like LRTA* and Samuel’s algorithm in this article. According several algorithms is acquired 4. Markovian decision problems The basis for our theoretical framework stochastic versions of the problems is a class of stochastic optimal control prob- that is the simplest class of problems to which heuristic from a well- that include stochastic and the fact that time allowing us to borrow literature. Frameworks in applications present research are important due to the uncertainty lems called Murkovian decision problems. This to include is general enough apply, while at the same search algorithms developed control and operations problems it is the presence of uncertainty over open-loop control. tions, i.e., inputs Although applications of complex behaviors. Many problems of practical Markovian decision problems, this framework can be found an action can be a high-level and extensive to a dynamic an action system, command that gives closed-loop, or reactive, control advantages In a Markovian decision problem, operators is a “primitive ” in the theory, it is important that probabilistically determine take the form of ac- successor states. that in that executes one of a repertoire to understand as importance have been formulated treatment of the theory and application of [ 1 l] and Ross in many books, such as those by Bertsekas 1601. A.G. Barto et al. /Art$cial Intelligence 72 (1995) N-138 91 state, is defined is selected A Markovian then the action decision problem If i is the observed in terms of a discrete-time instants of real time, but until to states, actions, and immediate the controller executes action u E U(i), stochastic dy- namic system with finite state set S = { 1, . . . , n}. Time is represented by a sequence of . In Section 6 introducing RTDP, we treat this as a sequence of time steps t=O,l,... then it is best to treat it merely as an abstract specific the system’s current state and selects sequence. At each time step, a controller observes a control action, or simply an ~crion,~ which is executed by being applied as input from a finite set to the system. U(i) of admissible the system’s actions. When probability pij( u). We further state at the next time step will be j with state-transition that the application of action u in state i incurs an immediate cost ci( u) . 7 When assume costs by the time steps at which necessary, we refer they occur by using s,, u,, and cf to denote, respectively, the state, action, and immediate cost at time step t, where uI E U( s,) and ct = cs, (Us). We do not discuss a significant extension with complete important state and is the scope of this article. it introduces A closed-loop policy specifies each action as a function of the observed state. Such a executes action p(i) E where the controller policyisdenoted~=[~(l),...,~(n)], is a stationary policy because U(i) whenever it does not it observes this article, when we use the term policy, we always change over time. Throughout fp, called the evaluation mean a stationary policy. For any policy function, or the costfunction, corresponding to each state the total the controller uses the policy p starting cost expected to accumulate ,u and state i, we define fp( i) to be the from expected value of the injnite-horizon discounted cost that will accrue over time given ,u and i is the initial state: that the controller uses policy has been studied extensively are beyond the given state. Here, for any policy ,u, there is a function, certainty. Although of this formalism over time when the complexities cannot observe this possibility the controller ,u. It assigns the current in practice, in which to policy i. This state (1) the controller future immediate always uses policy where y, 0 < y < 1, is a factor used to discount expectation assuming the cost of state i under policy ,u. Thus, whereas policy p is ci( p( i) ), the cost of state i under policy p is the expected discounted of all the immediate Theorists such as the function giving average cost per time step, but we do not consider formulations costs, and Eti is the ,u. We refer to f’“(i) simply as the immediate cost of state i under sum from state i. functions, those study Markovian decision problems with other types of evaluation that will be incurred over the future starting costs here. 6 In control used in Al. theory, this is simply called a control. We use the term action because it is the term commonly 7 To be more general, we can alternatively regard the immediate depending on states and actions. of action u in state i, the theory discussed below remains unchanged. In this case, if cl(u) denotes the expecfed costs as (bounded) random numbers immediate cost of the application 92 A.G. Barro et al./Art$cial Intelligence 72 (1995) 81-13R The objective of the type of Markovian this objective that minimizes a policy that achieves is not always unique, we denote corresponds optimal cost function, denoted For each state i, f*(i), for any policy. the same evaluation the cost of each state is an optimal policy which, although decision problem we consider i as defined by Eq. is to find ( 1). A policy it depends on y and To each optimal policy is the optimal evaluation function, or ,u* = [,~*(l),...,p*(n)]. function, which f *; that is, if ,u* is any optimal policy, then ffi’ = f’. the optimal cost of state i, is the least possible cost for state i This infinite-horizon discounted version of a Markovian decision problem is the sim- that the costs of all states are finite ensures because discounting that there is always an optimal policy factor, y, determines how strongly expected plest mathematically for any policy and, further, The discount current control decisions. When y = 0, the cost of any state is just the immediate of the transition E+ [CO/SO = i] = ci( ,u( i) ) . In this case, an optimal policy simply selects actions imize these minimum significant computation. that is stationary.8 influence cost is because O” = I in Eq. ( 1) so that fp(i) = to min- just gives toward one, future costs become more require more optimal actions, and solution methods generally cost for each state, and the optimal evaluation costs. As y increases future costs should that state. This the immediate in determining immediate function from case, (thinking to states), assumptions case because for the undiscounted When y = 1, the undiscounted costs as arc lengths set of states, the cost of a state given by Eq. to produce well-defined are required to problems are closely In these problems, which Bertsekas and Tsitsiklis of immediate there is an absorbing ( 1) need decision the to which heuristic search is [ 121 call stochastic shortest in a graph whose nodes i.e., a set of states that once is never left, and the immediate cost associated with applying an action to any of not be finite, and additional problems. We consider one set of assumptions resulting decision problems related applied. path problems correspond entered the states in the absorbing evaluation set assigns finite taking the system costs to every state even when y = 1. This is true because all but a finite number of the immediate as in that is stationary. The the discounted shortest absorbing solved via path problem, heuristic is to find an optimal closed-loop policy, not just an optimal path from a given initial state. costs incurred by such a policy over time must be zero. Additionally, case, there is always at least one optimal policy and we call it the goal set. However, unlike to the set of goal states in a deterministic imply into the absorbing set is zero. These assumptions search, here the objective set of states corresponds that the infinite-horizon tasks typically for any policy function AI researchers studying reinforcement in which all the immediate is delivered lems “reward” of the stochastic ment [67] formalism we are using. to the controller shortest path problems learning often focus on shortest path prob- costs are zero until a goal state is reached, when a and a new trial begins. These are special kinds the issue of delayed reinforce- in the to negative costs case when all the rewards are of the same that address in a particularly stark form. Rewards correspond In the discounted R In finite-horizon problems, optimal policies arc generally nonstationary because different actions can be optimal for a given state depending on how many actions remain until the horizon is reached. A.G. Barto et al./Arti@ial Intelligence 72 (1995) 81-138 93 A Starting line Finish line 1 Fig. 1. Example race tracks. Panel A: for details. Stwting line Finish line small race track. Panel B: larger race track. See Table 1 (Section 10) shortest path problem an optimal policy produces state. Another magnitude, to this example of a stochastic one except the same positive value instead of zero. In this case, an optimal policy produces a shortest path to a goal state in optimal control the undiscounted problems. a shortest path receiving are examples of minimum-time that all the non-rewarding case. Such problems to a rewarding costs have is identical immediate attention 4.1. An example: the race track problem To illustrate the Markovian decision framework, we formalize Track described by Martin Gardner the game, which we use in Section 10 to compare learning algorithms, by considering [25] that simulates automobile a game called Race racing. We modify the performance of various DP-based only a single car and by making it probabilistic. A race track of any shape is drawn on graph paper, with a starting of designated line at the other consisting squares. Each square within location of the car. Fig. 1 shows two example the car is placed on the starting and a finish boundary of the track is a possible At the start of each of a sequence of trials, a random position, track toward the finish and u squares vertically, in the previous move and v’ squares horizontally, where then the difference between h’ and h is - 1, 0, or 1, and the difference between v’ and v is - 1, 0, or 1. This means or it can slow down or speed up in either dimension by one square per move. If the car hits and moves are made line. Acceleration the car moved h squares horizontally the present move can be h’ squares vertically in which and deceleration its speed in either dimension, that the car can maintain the car attempts to move down are simulated as follows. line at one end the tracks. line at the If 94 A.G. Barto et nl./ArtiJciol Intelligence 72 (1995) 81-138 to zero (i.e., h’ - h and U’ - c’ are considered the track boundary, 9 we move it back to a random position on the starting line, reduce the its velocity trial. The objective line in as few moves as possible. Figs. 2 and 4 show examples of optimal and near-optimal paths for the race tracks shown to be zero), and continue the car so that it crosses in Fig. 1. is to learn to control the finish In addition to the difficulty of discovering very easy for the car to gather too much speed to negotiate matters worse, we introduce a random actual accelerations accelerations actions are executed. One might unpredictably car’s velocity. Although or decelerations or decelerations. Thus, 1 -p faster ways to reach the finish it is the track’s curves. To make factor into the problem. With a probability p, the of the intended intended at a move are zero independently that the controller’s is the probability line, think of this as simulating driving on a track that is slippery so that sometimes braking and throttling up have no effect on the requiring to formulate it is not our learning difficult example of problems it in a way best suited sensory and motor capabilities. the Race Track problem intention vehicle with realistic task as an abstract stochastic is to define time step t = 0, I,. lems, autonomous it as a representative formulate the entire in this formulation system at each s, = ( xt, yr, it, jr). The first two integers are the horizontal of the car’s location, and the second vertical directions. That is, .?( = x, - x,-t t; similarly each state is the set of pairs (u”, uJ’), where uX and uy are both in the set { -l,O, We let 14~ = (u:, u:) denote action admissible is suggestive of robot motion and navigation prob- to the design of an Instead, we regard skills, and we shortest path problem. The first step system being controlled. The state of the as a quadruple of integers and vertical coordinates and in the horizontal speed of the car at time step actions for 1). ,u assigns an is the horizontal (we assume x-t = y-t = 0). The set of admissible the action at time to each state: the action at time step t is two integers are its speeds can be represented t. A closed-loop .vt = yt - yt-t the dynamic policy P(St) E{(--l,--l),(--~,0),(~1.1),(0,-1),(0,0),(0,1),(1,-1), I)}. (l,O),(l. The following I - p, the controller’s equations define action the state transitions of this system. With probability is reliably executed so that the state at time step t + 1 is Xt+l =xr+it+u;, .Yt+t =y,+j,+u:, &,I =xt+u;, jr+l = p, + UT, (2) and with probability time step t + 1 is p, the system ignores the controller’s action, so that the state at ‘) For the computational experiments described in Section 10, this means that the projected path of the car for a move intersects the track boundary at any place not on the finish line. A.G. Barto et al./Arttjicial Intelligence 72 (1995) 81-138 95 (3) Xlfl Yf+l &+1 = Xt + Xt, = yr + P,, = it, P,,l = jr. the point line joining is a randomly that the straight (x,, y,) to the point (x,+1 , yt+t ) This assumes lies entirely within the track, or intersects only the finish line. If this is not the case, then the car has collided with the track’s boundary, and the state at t + 1 is (x, y, 0, O), where the car (x, y) across the finish line is treated as a valid move, but we assume stays in the resulting the track, together with Eqs. (3) and (2)) define the state-transition states and admissible that takes that the car subsequently state until a new trial begins. This method for keeping chosen position on the starting the car on for all line. A move probabilities actions. line, states To complete the finish line from inside in one time step by crossing i.e., all the states (x, y, 0,O) where the set of goal states, and the immediate function defined above, this set is absorbing. The immediate in each state. The set of start states consists of all the zero-velocity (x, y) are coordinates the formulation of the stochastic shortest path problem, we need to define costs associated with states of the line. The set of goal states consists of all states that can the track. According the set of start states, each action on the starting squares making up the starting be reached to the state-transition for all non-goal all non-goal a transition guaranteed For such a policy /L is the expected number of moves for the car to cross the finish line from state i when it is being controlled this cost of to cross the finish line as each state, is therefore a policy by which the car is expected quickly as possible is the smallest expected number of moves of the action actions U. The immediate is zero. If we restrict attention the finish cost i.e., ci(U) = 1 for cost associated with to policies that are to use discounting. states from any goal state to take the car across by a policy Jo. An optimal policy, which minimizes from any state. The optimal cost of a state i, f*(i), cost, fp (i) , of a state i under line, we do not need i and all admissible ,u, the undiscounted to the finish line. infinite-horizon independently starting is one taken, The total number of states depends on the configuration we have not imposed a limit on the car’s speed, set of states that can be reached can be considered to be the state set of the stochastic shortest path problem. it is potentially of the race track, but because the is finite and infinite. However, from the set of start states via any policy 4.2. The optima&y equation To set the stage for discussing DP, we provide more detail about the relationship and evaluation between policies the cost of each state under policy to the best successor greedy policy with respect states as evaluated by fp. to its own evaluation functions. Although the evaluation ,u, p does not necessarily function select actions In other words, p is not necessarily function. ffi gives that lead a To define a greedy policy in this stochastic case we use Watkins’ [ 8 1 ] “Q” notation, which plays a role real-valued function of the states; in the Q-Learning method described it may be the evaluation in Section 7.3. Let f be a for some policy, a function 96 A.G. Rrrrtcl (‘1 iri /Ar-tifi&/ Intrlli,~wzw 72 (199.5) RI-l.38 guess for a good evaluation search). or an arbitrary function function function. For each state i and action II E Or(i), let ( such as a heuristic evaluation Q’ c i. II) = c, c II ) + y c p;, c If) .I‘( ,j ) it.> in heuristic (4) Q”(i, M) is the cost of action 11 in state immediate cost and the discounted states under action u. If the system’s state transitions simplifies It is the sum of the expected value of the costs of the possible successor then Eq. (4) i as evaluated by f. are deterministic, to Q’(i. u) = c;(u) + r.fC,j,. to operator zc). In the deterministic is the successor of state i under action II (i.e., node j where j along the edge corresponding think of @(i, u) as a summary of the result of a one-ply the edge corresponding a generalization having a different probability of being followed. policy, Q-‘( i, u) gives the cost of generating this policy. Using i is the child of node case, one can therefore from node i along to operator u as evaluated by f. The stochastic case requires to each operator, each for some following function action II in state i and thereafter a policy fi is greedy with respect to .f if for all states i, ,u( i) of this view because many edges correspond If .1’ is the evaluation these “Q-values”, lookahead is an action satisfying there can be more than one greedy policy with respect to f if more than one that to many for some state. we let ,CL~ denote any policy to f. Also note that any policy is greedy with respect the set of Q-values Although action minimizes is greedy with respect different evaluation A key fact underlying functions. to their own evaluation all DP methods functions is that the only policies are optimal policies. That is the optima1 evaluation that are greedy with is, if ,u* is any f*, and function then its evaluation function that for any state i, p* (i) satisfies This means respect optimal policy, p* = ,uf * Qj*(i,p*(i)) = min Q’*(i.ll) l&l/(i) any policy i.e., defining it is possible that is greedy with respect to define an optimal policy simply by defining Furthermore, if ,f* is known, Eq. (5), values is any policy each current state. Deeper search is never necessary because the information to ,f* is an optima1 policy. Thus, it satisfy to f*. Due to the definition of Q- to the stochastic case the fact that an optima1 policy that is best-first with respect to f* as determined by a one-ply search from all that such a search would obtain. it to be greedy with respect f* already summarizes this generalizes (Eq. (4)), Letting Q* (i, u) = Qf’ (i, u) to simplify notation, a related key fact is that a necessary is that for each for .f* to be the optimal evaluation function and sufficient state i it must be true that condition A.G. Barto et al./Art@cial Intelligence 72 (1995) 81-138 f*(i) = min Q*(i,u) UEU(i) = min UEU(i) . jES 1 ci(u) +r&j(u)f*(j) 97 (6) is one form of the Bellman Optima& Equation which can be solved for each i E S, by a DP algorithm. It is a set of n (the number of states) equations. The form of the equations depends on the dynamic simultaneous system and the costs underlying the decision problem. Once f* has been found, an optimal action for state i can be determined This f*(i), nonlinear immediate as follows. actions u E V(i) are determined via Eq. (4). steps, where n is the number of states if one knows which in the in the search computational actions from state i are zero (as one usually does less (O(m) lookahead these Q-values amounts for state i. However, can be much to a one-ply of the system’s state-transition probabilities. then the amount of computation this probabilities takes O(mn) The Q-values Q* (i, u) for all admissible In general, and m is the number of admissible of the state-transition case), deterministic deterministic case). Computing from state Using m - 1 comparisons. The computational method of the DP algorithm. requires knowledge these Q-values, i, which an optimal action can be determined takes complexity of finding an optimal action using this f*, i.e., by the complexity via Fq. (5), which is therefore dominated by the complexity of finding 5. Dynamic programming least probabilities, in principle-to it is possible-at i and actions u E U(i), including one called policy several versions of a basic DP algorithm Given a complete and accurate model of a Markovian decision problem ~i,i(~), and the immediate in the form of costs, ci( u), knowledge of the state-transition for all states solve the decision problem off-line by applying one of various well-known DP algorithms. iteration. There We describe are other DP algorithms, algorithms the scope of this article, although we briefly discuss policy based on them are beyond iteration unless oth- iteration is erwise noted. As used approximation a successive func- the Bellman Optimality tion, f*. It is a successive Equation whose basic operation state costs. There are several variations of value are organized. We first describe chronously. is “backing up” estimates of the optimal depending procedure approximation method called value iteration, but learning in Section 8. We treat DP as referring to the optimal evaluation on how the computations for solving Markovian the backup operations decision problems, that converges that applies the version for solving interaction to value iteration value only syn- 5.1. Synchronous dynamic programming Let fk denote the estimate of f* available at stage k of the DP computation, where . At stage k, fk( i) is the estimated optimal cost of state i, which we refer to k=O,l,... 98 A.G. Barto et al./Arti$cial Intelligence 72 (1995) RI-138 even though simply as the stage-k cost of state i; similarly, we refer to fk as the stage-k evaluation function, for any policy. (We use the index k for the stages of a DP computation, whereas we use t to denote the time step of the control problem being solved.) is in terms of fk as follows: defined In synchronous DP, for k = 0, 1, . . ., fk+l it may not actually be the evaluation for each state i, function where fc is some given equation operation the backed-up cost is saved for future use. Here, however, saved by updating initial estimate of f *. We refer to the application of this update is a common that the backed-up cost is always for state in a variety of search algorithms i as backing up i’s cost. Although backing up costs in AI, there it does not always mean the evaluation function. (7) a sequential implementation side of the equation. is synchronous If one The iteration defined by Eq. (7) on the right-hand associated with each state, applying Eq. (7) for all states i means backs up the cost of its state at the same time, using supplied by the other processors. This process updates all values of fk simultaneously. Alternatively, locations The sequential ordering of the backups storage so that all the stage-( k + 1) costs are computed based on the stage-k costs. because no values of fk+i appear imagines having a separate processor that each processor the old costs of the other states to the result. If there are n states and m is the largest number of admissible for any state, then each iteration, which consists of backing up the cost of each state exactly once, requires at most O(mn2) operations in the deterministic it is not desirable it converges iteration of value MIPS processor. in AI and in many control problems, let alone repeat the process until has about 102’ states, a single in this case would take more than 1,000 years using a 1,000 to try to complete even one iteration, to f *. For example, because backgammon in the stochastic case and O(mn) operations case. For the large state sets typical of this iteration is irrelevant temporary iteration requires actions If y < I, repeated synchronous iterations produce a sequence of functions verges to the optimal evaluation cost of a state need not get closer error between fk (i) and f* (i) over all states i must decrease function, to its optimal cost on each iteration, (e.g., [ 111) . f*, for any initial estimate, fo. Although that con- the the maximum a sequence of functions Synchronous DP, as well as the other off-line versions of value iteration we discuss to f* if y < 1, but it does there formed. to f’ and then form a greedy to f*, which would be an optimal policy. But this is not possible Instead, one executes value and then forms a policy from below, generates not explicitly generate a sequence of policies. To each stage-k evaluation corresponds Ideally, one would wait until policy corresponding in practice because value iteration converges asymptotically. it meets a test for approximate iteration until at least one greedy policy, but these policies are never explicitly the sequence converges that converges convergence function A.G. Barto et al. /Art@ial Intelligence 72 (1995) 81-138 99 the resulting evaluation function. lo It is important to note that a function in the sequence of evaluation functions generated by value iteration does not have to closely approximate f* in order for a corresponding greedy policy to be an optimal policy. Indeed, a policy corresponding to the stage-k evaluation function for some k may be optimal long before the algorithm converges to f*. But unaided by other computations, value iteration does not detect when this first happens. This fact is an important reason that the on-line variants of value iteration we discuss in this article can have advantages over the off-line variants. Because the controller always uses a policy defined by the current evaluation function, it can perform optimally before the evaluation function converges to the optimal evaluation function. Bertsekas [ 1 l] and Bertsekas and Tsitsiklis [ 121 give conditions ensuring conver- gence of synchronous DP for stochastic shortest path problems in the undiscounted case ( y = 1) . Using their terminology, a policy is proper if its use implies a nonzero proba- bility of eventually reaching the goal set starting from any state. Using a proper policy also implies that the goal set will be reached eventually from any state with probability one. The existence of a proper policy is the generalization to the stochastic case of the existence of a path from any initial state to the goal set. Synchronous DP converges to f* in undiscounted stochastic shortest path problems under the following conditions: ( 1) the initial cost of every goal state is zero, there is at least one proper policy, and (2) (3) all policies that are not proper incur infinite cost for at least one state. The third condition ensures that every optimal policy is proper, i.e., it rules out the possibility that a least-cost path exists that never reaches the goal set. One condition under which this is true is when all immediate costs for transitions from non-goal states are positive, i.e., Q(U) > 0 for all non-goal states i and actions u E U(i). I1 In the deterministic case, conditions (2) and (3) are satisfied if there is at least one solution path from every state and the sum of the immediate costs in every loop is positive. 5.2. Gauss-Seidel dynamic programming Gauss-Seidel DP differs from the synchronous version in that the costs are backed up one state at a time in a sequential “sweep” of all the states, with the computation for each state using the most recent costs of the other states. If we assume that the states are numbered in order, as we have here, and that each sweep proceeds in this order, then the result of each iteration of Gauss-Seidel DP can be written as follows: for each state i and each k = 0, 1, . . ., ‘” Policy iteration, in contrast, explicitly generates a sequence of policies that converges to an optimal policy after a finite number of iterations (when there am a finite number of states and admissible actions, as we are assuming here). However, policy iteration has other shortcomings which we discuss in Section 8. ” The assumption of positive immediate costs can be weakened to nonnegativity, i.e., ci (a) 2 0 for all i E S and u E U(i), if there exists at least one optimal proper policy [ 121. IO0 A.G. Burro et trl. /Arrijicitrl Intelligence 72 (1995) 81-138 jifki t (i) = min LIEU(i) c,(U) +YCPi.j(U)f’(j) [ iE.7 I (8) where if ,j < i, otherwise. to f* under Unlike synchronous DP, the order in which the states’ costs are backed up influences the same con- the computation. Nevertheless, Gauss-Seidel DP converges ditions under which synchronous DP converges. When y < 1, repeated Gauss-Seidel stochas- sweeps produce a sequence of functions tic shortest path problems, of synchronous DP also ensure convergence of Gauss-Seidel DP [ 121. Because each cost converges backup uses the latest costs of the other states, Gauss-Seidel DP generally it should be clear that some state orderings faster than synchronous DP Furthermore, produce in shortest path problems, from goal states backwards along likely shortest paths usually that converges described on the problem. For example, leads to faster convergence in the forward direction. that ensure convergence than others, depending to f * . For undiscounted faster convergence than sweeping the conditions sweeping above Although Gauss-Seidel DP is not one of the algorithms article, we used it to solve the example problem described as a bridge between synchronous DP and the asynchronous 5.3. Asynchronous dynamic programming interest of direct in this in Section 4.1 and it serves form discussed next. and further developed in terms of systematic [lo] [ 121, asynchronous DP is suitable time delays and without a common to Gauss-Seidel DP in that it does not back up state successive it is not organized by for multi-processor systems clock. For each state i E S Asynchronous DP is similar costs simultaneously. However, sweeps of the state set. As proposed by Bertsekas Bertsekas and Tsitsiklis with communication there is a separate processor dedicated each processor may be responsible processor backs up the cost of its state can be different the cost of its state, each processor uses the costs for other states that are available when it “awakens” utility discuss below lies in the fact that it does not require state costs to be backed up in any systematically organized to backing up the cost of state i (more generally, for a number of states). The times at which each for each processor. To back up to it have obvious for all the algorithms we in asynchronous DP in speeding up DP and thus have practical significance to perform a backup. Multi-processor [44] ). However, our interest (see, e.g., Lemmon implementations fashion. Although in the full asynchronous model, stages does not apply because a processor can awaken at any of a continuum of times, we use it facilitates our discussion of RTDP in the next a notion of an iteration the estimate of f* available at stage section. As in the other forms of DP, let fk denote the notion of discrete computational stage because A.G. Barto et d. /Artificial Intelligence 72 (1995) 81-138 101 and the costs remain unchanged k of the computation. At each stage k, the costs of a subset of the states are backed up synchronously, for the other states. The subset of states whose costs are backed up changes from stage to stage, and the choice of these subsets determines the precise nature of the algorithm. For each k = 0, 1,. . ., if Sk c S is the set of states whose costs are backed up at stage k, then fk+l is computed as follows: fkfl (i) = min Qj”(i,u), uW(i) .fkfi), if i E Sk, otherwise. (9) According to this algorithm, then, fk+l may differ from fk on one state, on many on Sk. Further, unlike Gauss-Seidel DP, the costs states, or possibly none, depending the costs of others are backed of some states may be backed up several as and Gauss-Seidel up once. Asynchronous DP includes special cases: synchronous DP results if Sk = S for each k; Gauss-Seidel DP results when each Sk consists of a single state and the collection of Sk’s is defined to implement . . ., Sn-l = {n}, successive sweeps of the entire state set (e.g., Sa = {l}, St = {2}, the synchronous times before algorithms S,, = {I}, Sn+l = {2}, . . .I. Discounted asynchronous DP converges selection It follows this means to f* provided that the strategy should never eliminate that each state is contained backed up infinitely often, i.e., provided of the subsets Sk, k = 0, 1,. . . . In practice, states for cost backups future. In the undiscounted convergence. asynchronous DP converges cost of each state is backed up infinitely often and the conditions of synchronous DP are met: (1) for convergence (2) zero, is at least one proper policy, and (3) all policies there infinite cost for at least one state. incur to realize that the cost of each state is in an infinite number for selecting in the to ensure that if the shortest path problems in Section 5.1 given the initial cost of every goal state is that are not proper case (y = 1) , additional assumptions from a result by Bertsekas any state from possible that a single backup of a state’s cost in asynchronous DP in it as an estimate of the state’s optimal cost; it may it worse. However, under the cost of each state conditions, to its optimal cost with repeated backups. Further, as in Gauss-Seidel DP, the in a the rate of convergence in undiscounted the appropriate [ 12, p. 4461 and Tsitsiklis are necessary stochastic improve in which states’ costs are backed up can influence way. This fact underlies algorithms by supplying the utility of various strategies for “teaching” experience dictating selected orderings of It is important does not necessarily fact make converges order problem-dependent DP-based the backups learning (e.g., Lin [ 481, Utgoff and Clouse [ 801, and Whitehead [ 901) . 6. Dynamic Programming in real time The DP algorithms described above are off-line algorithms for solving Markovian de- function cision problems. Although through a sequence of stages, these stages are not related to the time steps of the decision the controller performs problem being solved. Here we consider i.e., concurrently with asynchronous DP concurrently with the actual process of control, the optimal evaluation they successively approximate algorithms in which 102 A.G. Barr0 et al./Art@cial Intelligence 72 (1995) 81-138 (2) to which ( 1) control decisions and the process of executing follows: DP computation, selection of states costs have to be stored. The asynchronous due to the flexibility with which interaction, to guide its behavior, and the DP computation are most relevant Real-Time DP (RTDP) present below. Throughout the controller automatically for control as revealed actions. The concurrent DP and control processes information are based on the most up-to-date the state sequences generated during control interact as from the the is applied and whose estimated for this role of this influence version of DP is appropriate its stages can be defined. As a consequence the DP backup operation uses intermediate results of the DP computation can focus on regions of the state set that in the system’s behavior. The algorithm we call that we results when this interaction has specific characteristics in relation is not available. When that there is a complete and accurate model of this section we assume in AI. In [70] discusses the case Sutton the adaptive case, in which a complete and accurate model of the then execution of DP and control can also be carried out in simulation mode, the decision that can have computational to focus on relevant parts of there is a model of the decision problem, is a novel off-line DP computation for the actual system underlying off-line DP due to its ability is used as a surrogate to planning its not being a real-time computation, we regard the concurrent in simulation mode to be a form of learning. This is in fact [ 61,621 and [ 771. Learning occurred during many simulated games in which these learning the real-time use of DP- also applies themselves. Although we emphasize the reader should be aware that our discussion programs of Samuel in the game-playing formulation of the abstract discrete-time in simulation mode. execution of DP and control, we think of the time steps t=O,l,... of a Markovian decision problem as the indices of a sequence of instants of real time at which the controller must execute t, and let k, be the total control actions. Let s, be the last state observed before t. Then fk, is the latest number of asynchronous DP stages completed estimate of the optimal evaluation the controller must select action U, E U(s,). When the controller executes uI, it incurs the immediate cost c,~, (u,), the next action, u,+l, has to be to s,+I. By the time and to yield fk,, , selected, some additional We let Bt denote the set of states whose costs are backed up in these stages. Note that some states in B, might have their costs backed up more than once in these stages. time up to time available when stages of asynchronous DP stages are completed state changes the system’s function the model the decision problem, Section 7, we discuss decision problem the concurrent where problem. The result advantages over conventional the state set. Despite execution of DP and control how learning was accomplished Tesauro systems competed against based algorithms, to the use of these algorithms the concurrent To describe learning 6.1. Real-time DP RTDP refers to cases in which the concurrently executing DP and control processes influence one another as follows. First, greedy with respect the greedy action with respect be resolved to the most recent estimate of f*. This means to fk,. Moreover, any ties in selecting randomly, or in some other way that ensures the continuing that is that U, is always these actions must selection of the controller always follows a policy A.G. Barto et al./Art@cial Intelligence 72 (1995) 81-138 103 the execution of u, and ur+t, the cost of s, is all the greedy actions. Second, between always backed up, i.e., st E B, for all r. In the simplest case, Bt = {st} for all t, i.e., the cost of only s, is backed up at each time step t, but more generally, B, can contain search. any states (in addition search from s, For example, B, might consist of the states generated by an exhaustive forward to some fixed search depth, or it might consist of the states generated by a search to st) such as those generated by any type of lookahead that is best-first according to fk,. that RTDP converges when to f*. Because required We say described the associated always continues t* The conditions ruled out for having case, the only condition in Section 5.3 ensuring asynchronous DP computation that are greedy with respect is that asynchronous DP to visit each state. There are several approaches is to assume, as is often done in the engineering to f* still apply when it is executed concurrently with control. Consequently, the controller always takes actions converges to the current estimate of f*, when RTDP converges, optimal control performance attained. converges in the discounted state is ever completely backs up the cost of the current state, one way to achieve controller this. One approach Markov process resulting nonzero probability RTDP converges under stochastic be absorbing; every state is a goal state. A second way to ensure for it does not allow proper subsets of states to in which for convergence of RTDP is that no its cost backed up. Because RTDP always this is to make sure that the that the from the use of any policy is ergodic. This means that there is a trials. A trial consists of a time RTDP new trial begins. the system state to selected start states, but for many problems and it is always possible when RTDP is used in simulation mode. is to use multiple infinitely often interval of nonzero bounded duration during which is set to a new starting state, and a to set is possible, is performed. After this interval, l3 Obviously, of visiting any state no matter what actions are executed. Discounted this method cannot be used when it is impossible in the trivial stochastic shortest path problem shortest path problems because this assumption. However, that each state is visited it is satisfied only is unsatisfactory this assumption this approach the system to ensuring literature, 6.2. Trial-based RTDP in an infinite series of trials, If the initial states of trials are selected so that every state will be selected every state will be visited often often-if this is to start each trial with a randomly probability initiated an infinite series of trials. Then infinitely infinitely only at the start of an infinite number of trials. A simple way to accomplish selected state, where each state has a nonzero of being selected. By Trial-Based RTDP we mean RTDP used with trials one, be a start state infinitely often in result of noting so that every state will, with probability is an immediate then obviously the following theorem to switch between optimal policies to select among all the greedy actions. This results in a nonstationary optimal policy ‘* When there is more than one optimal policy, the controller will continue because RTDP continues because different optimal actions can be taken from the same state on different occasions. t3 RTDP must be interrupted by the cost of the starting from influencing at the end of a trial so that the cost of the last state in a trial is not influenced caused by the “trainer’ state of the next trial. This prevents the state transitions the evaluation function. 104 A.G. Barto et al. /Artificial Intelligence 72 (1995) 81-138 that, in the discounted DP computation with probability case, Trial-Based RTDP gives rise to a convergent asynchronous one for any method of terminating trials: Theorem 1. For any discounted Markov decision problem any initial evaluation function, Trial-Based RTDP converges (as defined in Section 4) and (with probability one). It is natural to use Trial-Based RTDP in undiscounted stochastic shortest path prob- lems, where trials terminate when a goal state is first reached, or after a predetermined number of time steps. Because Trial-Based RTDP gives rise chronous DP computation with probability path problems under result: asyn- shortest in Section 5.3, we have the following one in an undiscounted to a convergent the conditions enumerated stochastic stochastic shortest path problems, Trial-Based RTDP con- Theorem 2. In undiscounted verges (with probability one) under the following ( 1) the initial cost of every goal state is zero, (2) (3) all policies there is at least one proper policy, and that are not proper conditions: incur infinite cost for at least one state. Trial-Based RTDP is more interesting if we relax the requirement stochastic shortest path problems that it should yield function and a complete optimal policy. Consider a trial- to solving undiscounted in which there subset of start states from which trials always start. We say that a state i if a start state s and an optimal policy exist such that i can be reached from that is optimal states) will never the use of that (or any other) optimal policy. If one somehow knew which save a the controller uses that policy. It suffices then one could apply DP to just the other states (irrelevant to relevant states because these states and possibly to find a policy amount of time and space. But clearly this is not possible because knowing a complete optimal evaluation based approach is a designated is relevant state s when when restricted occur during states were relevant, considerable which states are relevant seeking. requires knowledge of optimal policies, which is what one is However, under certain conditions, without continuing to a function to a policy policy converges to back up the costs of irrel- f* on all relevant that equals that is optimal on all relevant states may not have to be backed up at all. More- trials, the incrementally during for the estimated state costs is allocated evant states, Trial-Based RTDP converges states, and the controller’s states. The costs of some irrelevant over, if memory exhaustive memory RTDP tends to focus computation computation the following requirement of conventional DP can be avoided because Trial-Based restricts in onto the set of relevant states, and eventually this is possible are stated precisely to this set. Conditions theorem, whose proof is given in Appendix A: under which Theorem 3. In undiscounted stochastic shortest path problems, Trial-Based RTDP with the initial state of each trial restricted to a set of start states, converges (with probability to f* on the set of relevant states, and the controller’s policy converges one) to an A.G. Barto et al./Artijicial Intelligence 72 (199.5) 81-138 105 optimal policy (possibly nonstationary) on the set of relevant states, under the following conditions: ( 1) the initial cost of every goal state is zero, (2) there is at least one proper policy, I4 (3) all immediate costs incurred by transitions from non-goal states are positive, i.e., (4) ci(U) > 0 for all non-goal states i and actions u E U(i), and the initial costs of all states are non-overestimating, i.e., fo(i) 6 f*(i) states i E S. for all Condition (4) can be satisfied by simply setting fo( i) = 0 for all i. The significance of Theorem 3 is that it gives conditions under which a policy that is optimal on the relevant states can be achieved without continuing to devote computational effort to backing up the costs of irrelevant states. Under these conditions, RTDP can yield an optimal policy when state and action sets are too large to feasibly apply conventional DP algorithms, although the amount of computation saved will clearly depend on characteristics of the problem being solved such as its branching structure. Moreover, if RTDP is applied on-line instead of in simulation mode, whenever the evaluation function changes so that its greedy policy shows improvement, the controller automatically takes advantage of this improvement. This can occur before the evaluation function is close to f*. Although in both discounted and undiscounted problems, the eventual convergence of RTDP does not depend critically on the choice of states whose costs are backed between the execution of actions (except that the cost of the current state must be backed up), judicious selection of these states can accelerate convergence. Sophisticated exploration strategies can be implemented by selecting these states based on prior knowledge and on the information contained in the current evaluation function. For example, in a trial- based approach to a stochastic shortest path problem, guided exploration can reduce the expected trial duration by helping the controller find goal states. It also makes sense for RTDP to back up the costs of states whose current costs are not yet accurate estimates of their optimal costs but whose successor states do have accurate current costs. Techniques for “teaching” DP-based learning systems by suggesting certain back ups over others [46,80,90] rely on the fact that the order in which the costs of states are backed up can influence the rate of convergence of asynchronous DP, whether applied off- or on-line. A promising approach recently developed by Peng and Williams [58] and Moore and Atkeson [ 571, which the latter authors call “prioritized sweeping”, directs the application of DP backups to the most likely predecessors of states whose costs objective is to facilitate finding change significantly. Exploration such as this-whose an optimal policy when there is a complete model of the decision problem-must be distinguished from exploration designed to facilitate learning a model of the decision problem when one is not available. We discuss this latter objective for exploration in Section 7. I4 If trials am allowed to time out before a goal state is reached, it is possible to eliminate the requirement that at least one proper policy exists. Timing out prevents getting stuck in fruitless cycles, and the time-out period can be extended systematically to ensure that it becomes long enough to let all the optimal paths be followed without intermption. 106 A.G. Barto et cd. /Artijiciul Intelligence 72 (1995) El-138 6.3. RTDP and LRTA * of Korf’s [ 381 convergence Theorem 3 is a generalization theorem for LRTA*. RTDP extends LRTA* in two ways: it generalizes LRTA* to stochastic problems, and it includes the the option of backing up the costs of many states intervals between form of LRTA* operates execution as of actions. Using our notation, first backs up the cost of s, by follows: action uI E Lr( s,), setting f, (s,) to the minimum of the values c,~, (u) + off_ 1 (j) for all actions u E U( s,), is j’s current cost. I5 The costs of where j is s,‘s successor under action u and f,_t (j) all the other states remain action then inputs the same. The controller to the system, observes st+i, and repeats this minimizing the controller the simplest to determine in the time the process. . It differs from in which B, = {st} for all t = 0, 1, . This form of LRTA* is almost the special case of RTDP as applied problem the following way. Whereas RTDP executes an action ff, LRTA* executes an action inconsequential j = st, i.e., when st is its own successor. LRTA* saves computation one minimization also gives the greedy action. However, than one state’s cost during each time interval, of f* to select an action. to a deterministic this special case in to is usually an (j) only when by requiring only the backup in the general case, when RTDP backs up more it makes sense to use the latest estimate at each time step: the minimization that is greedy with respect that is greedy with respect can differ from ft-r difference because in LRTA* ft(j) to ft_t. This to perform required (roughly) to RTDP It applies by setting An extended the costs ft-t the evaluation the backed-up forward search to the minimum that instead of using from s yf to a depth determined form of LRTA* can also be related In his discussion, Korf that the evaluation of a state may be augmented by lookahead search. (j) of sl’s successor states, LRTA* can by the amount of to is in the forward procedure). fr_i (s,), [38] assumes This means perform an off-line resources available. time and computational the frontier nodes and then backs up these costs to st’s immediate done search These backed-up described above, but neither in the forward search are saved. Despite have been computed, the new evaluation However, within costs multiple these backed-up DP. these costs nor the backed-up costs of the states generated costs for many states ft, differs from the old only for s(. it makes sense to store the backed-up the controller will experience to LRTA*, RTDP can save all of defined stages of asynchronous trials with different starting states. In contrast costs in fk, by executing appropriately the fact that backed-up function, (Korf’s “minimin” to update the limits of space constraints, of the costs of its successors cost of each state generated costs of the successor states are then used states as possible, especially when successors. This for as many function ft_i as Specifically, saving responds to executing the depth of the forward search of all the immediate predecessors frontier states), the backed-up a number of stages of asynchronous DP equal costs produced by Korf’s minimin procedure cor- to one less than backs up the costs the current costs of the tree. The first stage synchronously of the frontier states (using the second stage backs up the costs of the states that are the immediate Is Note that because A, = {s,} for all t in LKTA*, k, always equals t. A.G. Barto et al./Artifcial Intelligence 72 (1995) 81-138 107 the computation stage of asynchronous DP to back predecessors of these states, etc. Then one additional of fk,. Not only does this procedure also up the cost of St completes apply in the stochastic case, it suggests that other stages of asynchronous DP might be useful as well. These stages might back up the costs of states not in the forward search tree, or they might back up the costs of states in this tree more than once. For example, noting the forward search might generate a graph with cycles, multiple backups of the costs of these states can further improve All of these possibilities under instances of RTDP and thus converge are basically different that in general the information contained in fk, . the conditions repeated described the function learning trials, With the optimal evaluation and RTDP are indeed However, in control of the system be used in adaptive control problems. they do not directly apply theory, where it applies to be controlled in the theorems above. accumulating information improves algorithms, in the developing estimate of control performance. Consequently, LRTA* as suggested by the name chosen by Korf. as this term is used in which a complete and accurate model In the next section we discuss how RTDP can to adaptive control problems to problems is lacking. 7. Adaptive control The versions of value Gauss-Seidel, iteration described and real-time-require above-synchronous, prior knowledge of the system underlying asyn- the Marko- probabil- and they require knowledge for all states i, j, and all actions u E U(i), chronous, vian decision problem. That is, they require knowledge of the state-transition ities, pij(u), of the immediate this means deterministic, costs for all the admissible policy when this knowledge with incomplete information, and solution methods adaptive control methods. I6 that one must know actions for every state. Finding, or approximating, states and the immediate an optimal is known as a Markovian decision problem for these problems are examples of i and actions u E U(i). is not available If the system for all states the successor costs ci(U) is There are two major classes of adaptive methods for Markovian decision problems the class of possible rest on the assumption distribution accumulate, information. Bayesian methods over this distribution with incomplete systems. priori probability are As observations the expected cost over the set of selected by using DP to find a policy possible in contrast, attempt approaches, to arrive at an optimal policy asymptotically for any system within some pre-specified class of systems. Actions may not be optimal on the basis of prior assumptions accumulated as experience and in the limit the large literature on both classes of that minimizes systems as well as over time. Non-Bayesian but the policy should approach an optimal policy is revised via Bayes’ accumulates. Kumar stochastic dynamic of a known a rule. Actions [ 391 surveys observations, l6 Markovian decision problems with incomplete state information step of control. These are sometimes called partially for many applications, their relevance are beyond are not the same as problems with incomplete the controller does not have complete knowledge of the system state at each time decision problems, which despite information observuble Markovian the scope of this article. in which 108 A.G. Bartr, et al. /Artificial Intelligence 72 (1995) 81-138 methods and conveys existing they are more practical theoretical for large problems. the subtlety of the issues as well as the sophistication of the to non-Bayesian methods because results. Here we restrict attention the current identi&ution the dynamic is the true model of the system typically make control decisions under system being controlled. They use system For both indirect and direct methods, a central system model at any the assumption Two types of non-Bayesian methods are distinguished. model to update parameters whose values determine during control. They current model equivulence principle using explicit system models. They directly estimate a policy or information a system model, such as an evaluation Zndirect methods explicitly algorithms time that the theorists call the certainty (what control [ 111) . Direct methods, on the other hand, form policies without other than ling the system and exploring This in indirect methods as the conflict between conducting model convergence methods also require exploration trol algorithms universally available are reviewed by Kumar been studied by Barto and Singh Sutton from which a policy can be determined, issue is the conflict between control- it better. it appears to achieve an optimal policy. Direct these same issues. Adaptive optimal con- is these problems, but no mechanism results are [ 391, and a variety of more heuristic approaches have [ 631, [ 31, Kaelbling in order to discover how to control iden@ution its behavior the conjict between and control because enough exploration favored. Some of the approaches and the objective of eventually [ 781, and Thrun and Miiller require mechanisms for which rigorous [ 551, Schmidhuber is often called [ 691, Watkins [ 341, Moore for resolving [ 811, Thrun and involve theoretical following function, [ 791. to converge In the following subsections, we describe several non-Bayesian methods for solving these methods can to optimal policies, we do the theory in this that can be proved information. Although Markovian decision problems with incomplete form the basis of algorithms not describe exploration mechanisms with enough rigor for developing direction. We call the first method algorithm updates a system model at each time step of control, and a conventional DP this algorithm of most method’s computational and it serves as a reference of the approaches described point for comparative that is the simplest modification that takes advantage of RTDP. We call this method Adaptive RTDP. The third method we describe method of Watkins purposes. Next, we describe another of the generic is executed at each time step based on the current system model. Although [ 811. We then briefly describe hybrid direct/indirect methods. limits its utility, it is representative indirect method. A system is the direct Q-Learning in the engineering indirect method indirect method identification the generic complexity literature, severely 7.1. The generic indirect method Indirect adaptive methods for Markovian decision problems with incomplete state-transition probabilities and immediate the unknown mation estimate on the history of state transitions and system terms of a parameter, ~9, contained states corresponding interact. The usual approach is to define the state-transition in some parameter i, j E S and each action u E U(i), p( i, j, U, 6) probability to parameter 0 E 0, where the functional dependence on 8 has a known probabilities in space, 0. Thus, for each pair of is the state-transition and immediate costs observed while infor- costs based the controller A.G. Barto et al. /Arti$cial Intelligence 72 (1995) 81-138 109 form. Further, one usually assumes that there is some t9* E 0 that is the true parameter, SO that pij ( U) = p (i, j, u, 8* ) . The identification task is to estimate 8* from experience. A common approach takes as the estimate of 8* at each time step the parameter having the highest probability of generating the observed history, i.e., the maximum-likelihood estimate of f9*. The simplest form of this approach to identification is to assume that the unknown parameter is a list of the actual transition probabilities. Then at each time step t the system model consists of the maximum-likelihood estimates, denoted p$ (u), of the unknown state-transition probabilities for all pairs of states i, j and actions u E U(i). Let n$ ( t) be the observed number of times before time step f that action u was executed when the system was in state i and made a transition to state j. Then ny ( t) = cjEs n;(r) is the number of times action u was executed in state i. The maximum-likelihood state- transition probabilities at time I are (10) If the immediate costs, ci( u), are also unknown, they can be determined simply by memorizing them as they are observed. l7 If in an infinite number of time steps each action would be taken infinitely often in each state, then this system model converges to the true system. As mentioned above, it is nontrivial to ensure that this occurs while the system is being controlled. At each time step t, the generic indirect method uses some (non real-time) DP algorithm to determine the optimal evaluation function for the latest system model. Let f: denote this optimal evaluation function. Of course, if the model were correct, then f: would equal f*, but this is generally not the case. A certainty equivalence optimal policy for time step t is any policy that is greedy with respect to f:. Let denote any such policy. Then at time step t, &( st) is the & = l&(l),..., certainty equivalence optimal action. Any of the off-line DP algorithms described above including asynchronous DP Here it makes sense at each can be used to determine f:, time step to initialize the DP algorithm with final estimate of f* produced by the DP algorithm completed at the previous time step. The small change in the system model from time step t to t + 1 means that f: and fF+, probably do not differ significantly. As pointed out above, however, the computation required to perform even one DP iteration can be prohibitive in problems with large numbers of states. &(n)] What action should the controller execute at time t? The certainty equivalence optimal action, & ( st), appears to be the best based on observations up to time f. Consequently, in pursuing its objective of control, the controller should always execute this action. However, because the current model is not necessarily correct, the controller must also pursue the identification objective, which dictates that it must sometimes select actions other than certainty equivalence optimal actions. It is easy to generate examples in which I7 In problems likelihood action. in which the immediate cost is a random function of the current state and action, the maximum- estimate of an immediate cost is the observed average of the immediate cost for that state and 110 A.G. Barto et al. /Artijicial Intelligence 72 (1995) 81-138 always following to a true optimal policy due to lack of exploration the current certainty equivalence optimal policy prevents convergence (see, for example, Kumar [ 391). One of the simplest ways to induce exploratory behavior is to make the controller that to probabilities policies in which actions are chosen according use randomized depend on the current evaluation the of being executed, with equivalence highest probability. To facilitate comparison of algorithms described in Section 4.1, we adopt the action-selection method based on the Boltzmann distribution that was used by Watkins function. Each action always has a nonzero probability [ 8 1 I, Lin [47], and Sutton the current certainty in the simulations action having optimal [ 691. This method assigns an execution probability to each admissible action for the current is determined by a rating of each action’s utility. We compute state, where this probability a rating, r(u), of each action u E U(s,) as follows: r(u) =Qf~*(s,,u). We then transform probability mass function over the admissible at time step t, the probability these ratings actions using that the controller executes action u E U(s,) the Boltzmann distribution: (which can be negative and do not sum to one) into a is (11) Prob(u) = e-r(u)/T C&/(.s,~ e-r(c’)‘r ’ how sharply these probabilities and as T decreases, where T is a positive parameter controlling certainty equivalence optimal action, p; ( So). As T increases, more uniform, one, while “computational over At “zero certainty equivalence optimal policy, whereas at “infinite at control. the probabilities temperature” it controls of the other actions approach zero. T acts as a kind of [ 351 in which T decreases as used in simulated annealing and control. the necessary identification tradeoff between policy equals the is no exploration, there is no attempt peak at the become ,u;( st) approaches and the randomized temperature” these probabilities the probability of executing temperature” time. Here there in Section 4.1, we introduced described just described In the simulations the method exploratory behavior by using randomized policies, and we let T de- crease over time to a pre-selected minimum value as learning progressed. Our choice of that are this method was dictated by simplicity as “generic” as possible. Without doubt, more sophisticated exploratory behavior would effects on the behavior of these algorithms. have beneficial to illustrate algorithms and our desire for generating 7.2. Adaptive real-time dynamic programming The generic just presented indirect method algorithm until convergence resulting as RTDP as described system some on-line (10); given by Eq. a non real-time DP relies on executing to substitute RTDP, at each time step. It is straightforward in the indirect method we call Adaptive RTDP. This method is exactly the same is updated using method the stages in Section 6.1 except that ( 1) a system model identification method, (2) such as the maximum-likelihood system model in performing the current is used A.G. Barto et al./Artificial Intelligence 72 (1995) 81-138 111 instead of the true system model; and (3) of RTDP determined by the randomized policy given by Eq. ( 1 1 ), or by some other method balances and control objectives. the action at each the identification time step is that iteration that have been to a number of algorithms Adaptive RTDP is related others. Although Sutton’s Dyna architecture based on policy RTDP, as he discusses to Adaptive RTDP algorithm problems of the discounted that is similar in which performance [ 70 1. Lin In the engineering (Section 8), it also encompasses [46,47] literature, by and methods such as Adaptive related [32] describe an they focus on Markovian decision is measured by the average cost per time step instead algorithms also discusses methods closely to Adaptive RTDP, although cost we have discussed. focuses on Q-Learning Jalali and Ferguson investigated [69] in identification, influence as in Adaptive RTDP, the selection of states to that it can be advantageous in the accuracy of the Performing RTDP concurrently with system and direct state-transition to let progress in identification [ 691 suggested is applied. Sutton provides an opportunity which the backup operation to back up the costs of states for which there is good confidence estimated in these estimates most reliable same time, so that the controller low so as to improve that aids identification [ 551, Schmidhuber these and other possibilities. tends the model but can conflict with control. Kaelbling the algorithm information to use a confidence measure state-transition it is possible [ 631, Sutton [ 691, Thrun according probabilities. One can devise various measures of confidence to the states whose cost backups use the to this confidence measure. At the the selection of actions is the confidence to direct to visit regions of the state space where for these regions. This strategy produces exploration [ 341, Lin [47], Moore [ 791 discuss [ 781, and Thrun and Mbller 7.3. Q-learning Q-Learning is a method proposed by Watkins ‘* Unlike information. it is a direct method because [ 811 for solving Markovian decision the indirect adaptive methods discussed it does not use an explicit model of the dynamic problems with incomplete above, system underlying pairs of states and admissible Recall from Eq. (6) is the cost of generating Any policy selecting actions optimal policy. Thus, determined with relatively the decision problem. It directly estimates actions (which we call admissible that Q* (i, u), the optimal Q-value action u in state i and thereafter that are greedy with respect if the optimal Q-values are available, little computation. for pairs). the optimal Q-values state-action for state i and action u E U(i) , an optimal policy. is an an optimal policy can be following to the optimal Q-values a family of Q-learning methods, and what we call Q-learning in this [ 8 1 ] actually proposed tR Watkins article methods are based on a simple however, observed, would be surprising pairs formed estimating these values is the simplest case, which he called “one-step Q-Learning”. He observed that because if no one had studied idea, they had not been suggested previously these problems had been so intensively them earlier. Although that although Q-J-earning as far as he knew. He further it thirty years, for over studied the idea of assigning values to state-action the basis of Denardo’s [24] approach to DP, we have not seen algorithms like Q-Learning for that predate Watkins’ 1989 dissertation. 112 A.G. Bartn et al./Artificial Intelligence 72 (1995) 81-138 We depart somewhat in our presentation [ 691, Barto and Singh others (e.g., Sutton on-line control. To emphasize Q-Learning’s present unique problem. We then describe the basic Q-Learning in not requiring direct access to the state-transition from the view taken by Watkins [ 8 I] and [ 31) of Q-Learning as a method for adaptive relationship with asynchronous DP, we first that is of the decision probabilities the more usual on-line view of Q-Learning. algorithm as an o$Xne asynchronous DP method 7.3.1. OflLine Q-Learning Instead of maintaining for each admissible an explicit estimate of the optimal evaluation as is done by all the methods described above, Q-Learning maintains estimates of the optimal Q-values let Qk(i, U) be the estimate of Q*(i, u) available at stage k of the computation. Recalling for each state (Eq. (6) ), we can think that f* is the minimum of the optimal Q-values of the Q-values at stage k as implicitly defining fk, a stage-k estimate of f”, which is given for each state i by pair. For any state i and action u E U(i), state-action function, (12) Although Q-values define an evaluation the evaluation mation than of Q-values alone, whereas knowledge of the state-transition function in this way, they contain more infor- function. For example, actions can be ranked on the basis function also requires ranking actions using an evaluation probabilities and immediate costs. Instead function to a random to the state-transition of having direct access probabilities, Off-Line Q- Learning only has access samples according to these probabilities. Thus, if a state i and an action u E U(i) are input to this function, so that it returns a state j with probability successor to an accurate model of the j = successor( in the form of its state-transition system does not have access to the probabilities the role of the successor themselves. As we shall see below, function pij ( U) . Let us call this function is played by the system in on-line Q-Learning, i, u). The successor that can generate but Q-Learning probabilities, amounts function itself. updates state-action state-action the Q-values synchronously pairs and leaves unchanged At each stage k, Off-Line Q-Learning of the admissible admissible pairs. The subset of admissible changes from stage to stage, and the choice of these subsets determines of the algorithm. For each k = 0, 1, . . ., let $ 2 {(i, u) 1 i E S, u f U(i)} the set of admissible state-action how much of the new Q-value up value. Let ci!k ( i, u), 0 < cJ!k (i, u) < 1, denote the learning the Q-value of (i, u) at stage k. Then &+I the Q-values of a subset for the other pairs whose Q-values are updated the precise nature denote pairs whose Q-values are updated at stage k. For each that determines is determined by its old value and how much by a backed- for updating if (i, u) E S’f then is computed as follows: to define a learning rate parameter rate parameter it is necessary state-action pair in $, Q~+I (i, u> = ( 1 - ak(i, u> )QdL u) +q(i,u)[Ci(u) +Yfk(successor(i,u))l, (13) where fk is given by Eq. ( 12). The Q-values remain the same, i.e., for the other admissible state-action pairs A.G. Barto et al./ArtijTcial lnlelligence 72 (1995) 81-138 113 for all admissible (i, u) @ $. By a Q-Learning backup we mean the application of Eq. ( 13) for a single admissible state-action pair (i, u). If the Q-value for each admissible state-action pair (i, u) is backed up infinitely often in an infinite number of stages, and if the learning rate parameters ‘ok (i, u) decrease over the stages k in an appropriate way, then the sequence {Qk( i, u) } generated by Off-Line Q-Learning converges with probability one to Q*( i, U) as k -+ 00 for all admissible pairs (i, u). This is essentially proved by Watkins [ 811, and Watkins and [ 821. Appendix B describes a method for meeting Dayan present a revised proof in the required learning rate conditions that was developed by Darken and Moody [ 191. We used this method in obtaining the results for Real-Time Q-Learning on our example problems presented in Section 4.1. One can gain insight into Off-Line Q-Learning by relating it to asynchronous DP The stage-k Q-values for all admissible state-action pairs define the evaluation function fk given by Eq. ( 12). Thus, one can view a stage of Off-Line Q-Learning defined by Eq. ( 13) as updating fk to fk+t , where for each State i, fk+l(i) =Il$itQk+l(i,u). This evaluation function update does not correspond to a stage of any of the usual DP for selected actions algorithms because it is based only on samples from successor determined by the state-action pairs in $!. A conventional DP backup, in contrast, uses the true expected successor costs over all the admissible actions for a given state. I9 It is accurate to think of Off-Line Q-Learning as a more asynchronous version of asynchronous DP. Asynchronous DP is asynchronous at the level of states, and the backup operation for each state requires minimizing expected costs over all admissi- ble actions for that state. The amount of computation required to determine the ex- pected cost for each admissible action depends on the number of possible successor states for that action, which can be as large as the total number of states in stochas- tic problems. Off-Line Q-Learning, on the other hand, is asynchronous at the level of admissible state-action pairs. Although each Q-Learning backup requires minimizing over all the admissible actions for a give state in order to calculate (via Eq. (12)) it does not require computation proportional fk(SUCCeSSor(i,u)) to the number of possible successor states. Thus, in the stochastic case, an asynchronous DP backup can require O(mn) computational steps, whereas a Q-Learning backup Used in &. (13),*’ lg However, stage k of Off-Line Q-Learning has the same effect as the stage of asynchronous DP using Sk in the special case in which (I ) the problem is deterministic, (2) A$ is the set of all admissible state-action pairs for states in Sk, and (3) ok (i, u) = 1 for all admissible state-action pairs (i, 14). M This complete minimization can sometimes be avoided as follows. Whenever a Qk (i, u) is backed up, if its new value, Qk+l (i, u). is smaller than fk( i), then fk+t (i) is set to this smaller value. If it.9 new value is larger than f&i), then if fk(i) = Q,+(i, U) and fk( i) # &(i, U’) for any u’ # u, then fk+] (i) iS found by explicitly minimizing the current Q-values for state i over the admissible actions. This is the case in which u is the sole greedy action with respect to fk( i). Otherwise, nothing is done, i.e., fk+l (i) = fk( i). This procedure therefore computes the minimization in l?q. (12) explicitly only when updating the Q-values for state-action pairs (i, u) in which u is the sole greedy action for i and the Q-value increases. I14 AC. Barto et ~1. /Artificial Intelligence 72 (1995) 81-138 is offset by the increased and the fact that a Q-Learning of requires only O(m). This advantage into account Q-Learning takes less information is comparable than does a backup of asynchronous DP: an asynchronous DP backup required by a Q- to many Q-Learning backups. Nevertheless, because less than that required by an asynchronous DP backup, Learning backup can be much Q-Learning can be advantageous when stages have to be computed quickly despite a large number of possible successor states, as in real-time applications which we discuss next. space complexity the computation backup 7.3.2. Real-Time Q-Learning it con- successor the result can be turned into an on-line identical substitute algorithm by executing is an indirect adaptive method If a current system model provides an approximate is large. However, we use the term Real-Time Q-Learning to Adaptive RTDP for stages of asynchronous Off-Line Q-Learning currently with control. function, tion 7.2) except that stages of Off-Line Q-Learning DP. This can have advantages over Adaptive RTDP when the number of admissible tions discussed by Watkins decision problem and the real system acts as the successor tive algorithm backs up the Q-value of control, where action actually executed. Using Real-Time Q-Learning, optimal policy without problem. the function. This direct adap- pair at each time step for only a single state-action pair consists of the observed current state and the therefore, one can compute an the decision forming an explicit model of the system underlying there is no model of the system underlying for the case originally this state-action in which (Sec- [81] ac- Specifically, assume that at each time step the estimated optimal Q-values produced by all the preceding t the controller observes these estimates Qt( i, u) for all admissible selects an action u, E Lr( s,) using this information receives state sI and stages of state- in the to s~+I. Then Qt+l is computed that allows for exploration. After executing ut, the controller cost cs, ( uy) while the system state changes has available Real-Time Q-Learning. We denote action pairs (i, u). The controller some manner immediate as follows: Qt+l(s,,u,) = (1 - a,(sr,u,))Q,(~,,~t) +a,(~t,~t)[c,v,(~t) +r.fr(st+~)l, (14) .fr(s,+l) = minuE,,s,,,) Q,(s,+l,u) where at time step t for the current state-action the same, i.e., state-action pairs remain Qt+l(Lu) = Qt(i,u), and LY((s~,u~) is the learning rate parameter for all the other admissible pair. The Q-values for all admissible (i, u) As far as convergence in which $, Line Q-Learning up at each step (or stage) Real-Time Q-Learning required by for convergence action must be performed ( sf, u,). This process repeats for each time step. f is concerned, Real-Time Q-Learning the set of state-action is the special case of Off- pairs whose Q-values are backed t, is {(So, ul)}. Th us, the sequence of Q-values generated by converges to the true values given by Q* under the conditions that each admissible in each state infinitely often in an infinite number of control of Off-Line Q-Learning. This means A.G. Barto et al./Artijicial Intelligence 72 (1995) N-138 115 steps. It is also noteworthy, as pointed out by Dayan [ 221, that when there is only one admissible action for each state, Real-Time Q-Learning reduces to the TD(0) algorithm investigated by Sutton [ 681. To define a complete adaptive control algorithm making use of Real-Time Q-Learning it is necessary to specify how each action is selected based on the current Q-values. Convergence to an optimal policy requires the same kind of exploration required by indirect methods to facilitate system identification as discussed above. Therefore, given a method for selecting an action from a current evaluation function, such as the randomized method described above (Eq. ( 11) ) , if this method leads to convergence of an indirect method, it also leads to convergence of the corresponding direct method based on Real- Time Q-Learning. 7.3.3. Other Q-learning methods In Real-Time Q-Learning, to define the real system underlying the decision problem plays the function. However, it is also possible the role of the successor function sometimes by the real system and sometimes by a system model. successor For state-action pairs actually experienced during control, the real system provides the function; for other state-action pairs, a system model provides an approxi- successor mate successor function. Sutton [ 691 has studied this approach in an algorithm called Dyna-Q, which performs the basic Q-Learning backup using both actual state transitions as well as hypothetical state transitions simulated by a system model. Performing the Q-Learning backup on hypothetical state transitions amounts to running multiple stages of Off-Line Q-Learning in the intervals between times at which the controller executes actions. A step of Real-Time Q-Learning is performed based on each actual state tran- sition. This is obviously only one of many possible ways to combine direct and indirect adaptive methods as emphasized in Sutton’s discussion of the general Dyna learning architecture [ 691. It is also possible to modify the basic Q-Learning method in a variety of ways in order to enhance its efficiency. For example, Lin [47] has studied a method in which Real-Time Q-Learning is augmented with model-based Off-Line Q-Learning only if one action does not clearly stand out as preferable according to the current Q- values. In this case, Off-Line Q-Learning is carried out to backup the Q-values for all of the admissible actions that are “promising” according to the latest Q-values for the current state. Watkins [ 811 describes a family of Q-Learning methods in which Q-values are backed up based on information gained over sequences of state transi- tions. One way to implement this kind of extension is to use the “eligibility trace” to back up the Q-values of all the state-action pairs experi- idea [4,37,67,68,72] enced in the past, with the magnitudes of the backups decreasing to zero with in- creasing time in the past. Sutton’s [68] TD( A) algorithms illustrate this idea. At- tempting to present all of the combinations and variations of Q-Learning methods that have been, or could be, described is well beyond the scope of the present article. Barto and Singh [3], Dayan 120,211, Lin [46,47], Moore [57], and Sutton [69] present comparative empirical studies of some of the adaptive algorithms based on Q-Learning. I16 A.G. Barto et d/Artificial Intelligence 72 (1995) 81-138 8. Methods based on explicit policy representations the action at each time step, but the policy so defined algorithms representation All of the DP-based learning cases, use an explicit the Q-values of admissible adaptive function giving in computing stored. There are a number of other real-time in which policies as well as evaluation step of control. Unlike closely related discussed Policy in Section 5. iteration (see, e.g., Bertsekas to the policy the methods addressed functions iteration DP algorithm state-action described pairs. These functions of either an evaluation above, both non-adaptive function and or a are used is not explicitly learning and control methods based on DP are stored and updated at each time these methods are more than the value iteration algorithms in this article, function in which in which the evaluation improvement phase, to the current evaluation uation phase, (2) a policy with respect cuting one of the value iteration algorithms discussed that there is only one admissible the policy being evaluated. Alternatively, Although policy evaluation actions, it can still require More feasible policy evaluation phase phase. Real-time asynchronous is not executed algorithms based on policy does not require too much computation is modified policy iteration Examples of such methods form of modified policy appear [ 111) alternates two phases: for the current policy the current policy is updated function. One way to evaluate a policy ( 1) a policy eval- and is determined, to be greedy is by exe- in Section 5 under the assumption action for each state, namely, the action specified by explicit matrix inversion methods can be used. over all admissible for large state sets. iteration except that the repeated minimizing to be practical [ 591, which is policy to completion before each policy improvement iteration effectively work by executing an iteration concurrently with control. in the pole-balancing [ 1,671) and the LIynu-PI method of Sutton (also [ 4,671 these methods and policy Iteration). Barto, Sutton, and Watkins iteration learning algorithms based on policy as is the theory of learning and Anderson PI means Policy between discuss well understood iteration. However, Williams theory by addressing DP algorithms either asychronous DP or Q-Learning. These algorithms iteration, and modified policy iteration as special cases. Integrating presented here is beyond in some detail. iteration because the scope of this article. that are asynchronous system of Barto, Sutton, [ 691 (where the connection [5,6] discuss In this article we do not is not yet as their theory value to this at a grain finer than that of iteration, policy their theory with that include value algorithms based on asynchronous and Baird 1911 have made a valuable contribution 9. Storing evaluation functions issue of great practical importance in implementing in this article is how evaluation functions are represented results we have described assume a lookup-table An scribed theoretical functions, which-at least in principle-is always possible when any of the algorithms de- and stored.21 The of evaluation the number of states representation ?I All of our comments here also apply to storing the Q-values of admissible state-action pairs. A.G. Barto et al. /Artificial Intelligence 72 (1995) 81-138 I17 (cf. the “boxes” actions and admissible tional DP to problems to discretize representation Barto, Sutton, and Anderson number of state variables, of dimensionality”. Q-Learning havior of Trial-Based RTDP states can reduce trials. is finite, as assumed continuous involving the ranges of the continuous throughout states and/or this article. In applying conven- actions, the usual practice is state variables and then use the lookup-table representation [4]). This leads the situation prompting Bellman The methods described do not circumvent the curse of dimensionality, in stochastic the storage requirement exponential to space complexity used by Michie and Chambers [52] and in the [ 91 to coin the phrase “curse in this article based on asynchronous DP and the focusing be- although start during shortest path problems with designated if memory incrementally is allocated the lookup-table A number of methods exist for making representation more efficient to store the costs of all possible states. Hash table methods, as for LRTA*, permit efficient storage and retrieval when the costs the can provide when it is not necessary assumed by Korf [38] of a small enough kd-tree data structure to access state costs, as explored by Moore efficient storage and retrieval of the costs of a finite set of states from a k-dimensional state space. The because resolved). to these methods are states need to be stored. Similarly, using the integrity of the stored costs subset of the possible in this article extend results described hash collisions they preserve theoretical (assuming [55,56], in Samuel’s configurations. Other approaches to storing evaluation [61] checkers player, functions use function approximation methods the as a weighted sum of the values of a set of features The basic backup operation was performed on to the current cost of a state and its backed-up cost. This approach based on parameterized models. For example, function was approximated evaluation describing checkerboard the weights, not on the state costs themselves. The weights were adjusted the discrepancy between inspired a variety of more recent studies using parameterized The discrepancy functions learning, connectionist Parametric approximations beyond an important to make use of [ 771. they can generalize for states that have not yet been visited, approximations. that approximates is a form of supervised supplies based on a training or learning the training data to supply cost estimates from examples, as shown, the error for any error-correction functions are useful because for example, by Anderson factor for large state sets. [ 11 and Tesauro the natural way set of function samples. This and provides of evaluation procedure to reduce networks function can be adapted for approximating In fact, almost any supervised for learning information, which is derived senting hypotheses, symbolic methods training algorithms. For example, Chapman tree methods, and Mahadevan [94] discusses learning algorithms. approximation function evaluation from examples. These methods also generalize beyond learning method, and its associated manner of repre- functions. This includes the from the backup operations of various DP-based [ 151 and Tan [76] adapt decision- and Kaelbling and Connell [49] use a statistical clustering method. Yee the perspective of its use with DP-based from Despite the large number of studies in which the principles of DP have been combined with generalizing methods presented in this article do not automatically extend to these approaches. Although for approximating the theoretical results evaluation functions, 118 A.G. Barto et al./Art@cial Intelligence 72 (1995) 81-138 [ 811 and illustrated with a simple example by Bradtke an optimal evaluation function, asynchronous DP algorithm, it is as [ 131. can be helpful in approximating of the underlying to the convergence scheme can adequately generalization often detrimental pointed out by Watkins Even if a function approximation function when trained on samples from this function, representation will result from an iterative DP algorithm scheme at each stage. The issues are much solving differential function in the absence of training examples drawn from the true solution. objective its solution. Here, we are interested Equation and not the easier problem of approximating equations. The objective of these problems is to solve approximately the differential that is the solution of a differential solving a solution in approximately equation represent the optimal evaluation it does not follow that an adequate that uses such an approximation the same as those that arise in numerically is to approximate the (for given boundary conditions) equation, not just In other words, the to approximate the Bellman Optimality that is already available. is a challenge To the best of our knowledge, There is an extensive approximation methods and DP, such as [ 71, Bellman, Kalaba, and Kotkin literature on function multigrid methods and methods using splines and orthogonal polynomials and Dreyfus [ 411). However, most of this literature which there this literature algorithms is a complete model of the decision problem. Adapting to produce approximation methods (e.g., Bellman [ 18 I, Kushner and Dupuis in from learning for RTDP and other DP-based to off-line algorithms for future research. [ 81, Daniel is devoted techniques for cases independent of a complete set of linearly [68] and Dayan the problem of representing these results do not address than it would be represented there are only a few theoretical [22] concern using TD methods the use of generalizing methods with DP-based address of Sutton as a linear combination tunately more compactly the problem of learning Q-values but these results are restricted and Yee [66] point out that in the discounted evaluation the performance of control. Without can drastically undermine concerns about combining DP-based research methods can be used effectively with the algorithms described that directly results learning algorithms. The results to evaluate a given policy basis vectors. Unfor- function [ 131 addresses state, regulation problems. However, Singh an in function as the basis errors if true, would raise learning with function approximation. Much more the approximate it might seem plausible of a controller using such a result, to provide a better understanding lead at worst to small decrements (or a function giving Q-values) control performance-a that small evaluation case, small errors to linear quadratic that are quadratic condition which, of how function in approximating of a continuous in this article. table. Bradtke approximation an evaluation in a lookup is needed evaluation functions function 10. Illustrations of DP-based learning We used the race track problem described conventional DP, RTDP, Adaptive RTDP, and Real-Time Q-Learning using tracks shown goal states, and 9,115 states reachable not shown in Section 4.1 to illustrate and compare the two race in Panel A has 4 start states, 87 the start states by any policy. We have line. The in Fig. 1. The small race track shown the squares on which land after crossing the car might the finish from A.G. Barto et al./Artijcial Intelligence 72 (1995) H-138 119 Table 1 Example race track problems. The results were obtained by executing Gauss- Seidel DP (GSDP) Small Track Larger Track Number of teachable states Number of goal states Estimated number of relevant states Optimum expected path length Number of GSDP sweeps to convergence Number of GSDP backups to convergence Number of GSDP sweeps to optimal policy Number of GSDP backups to optimal policy 9,115 87 599 14.67 28 252,784 15 136,725 22,576 590 2,618 24.10 38 835,468 24 541,824 larger race track shown in Panel B has 6 start states, 590 goal states, and 22,576 states reachable from the start states. We set p = 0.1 so that the controller’s intended actions were executed with probability 0.9. We applied conventional Gauss-Seidel DP to each race track problem, by which we mean Gauss-Seidel value iteration as defined in Section 5.2, with y = 1 and with the initial evaluation function assigning zero cost to each state. Gauss-Seidel DP converges under these conditions because it is a special case of asynchronous DP, which converges here because the conditions given in Section 5.3 are satisfied. Specifically, it is clear that there is at least one proper policy for either track (it is possible for the car to reach the finish line from any reachable state, although it may have to hit the wall and restart to do so) and every improper policy incurs infinite cost for at least one state because the immediate costs of all non-goat states are positive. We selected a state ordering for applying Gauss-Seidel DP without concern for any influence it might have on convergence rate (although we found that with the selected ordering, Gauss-Seidel DP converged in approximately half the number of sweeps as did synchronous DP). Table 1 summarizes the small and larger race track problems and the computational effort required to solve them using Gauss-Seidel DP. Gauss-Seidel DP was considered to have converged to the optimal evaluation function when the maximum cost change over all states between two successive sweeps was less than 10m4. We estimated the number of relevant states for each race track, i.e., the number of states reachable from the start states under any optimal policy, by counting the states visited while executing optimal actions for lo7 trials. We also estimated the earliest point in the DP computation at which the optimal evaluation function approximation was good enough so that the corresponding greedy policy was an optimal policy. (Recall that an optimal policy can be a greedy policy with respect to many evaluation functions.) We did this by running lo7 test trials after each sweep using a policy that was greedy with respect to the evaluation function produced by that sweep. For each sweep, we recorded the average path length produced over these test trials. After convergence of Gauss-Seidel DP, we compared these averages with the optimal expected path length obtained by the DP algorithm, noting the sweep after which the average path length was first within lo-* of the optimal. The resulting numbers of sweeps and backups are listed in Table 1 in the rows labeled “Number of GSDP sweeps to optimal policy” and “‘Number of GSDP backups to optimal policy”. 120 A.G. Barto et ul. /Artificial Intelligence 72 (1995) 81-138 functions, in assessing requirements of the real-time initial evaluation the computational off-line value iteration algorithms and requires a considerable in these computations earlier to note that this estimation process than did is Although optimal policies emerged considerably the optimal evaluation it is important not a part of conventional amount of additional computation. ** Nevertheless, useful should allow controllers to the optimal evaluation step, we restricted attention line with zero velocity, where each square on the starting We applied RTDP, Adaptive RTDP, and Real-Time Q-Learning costs are positive, we know the resulting numbers of backups are algorithms, which to follow optimal policies after comparable numbers of backups. to both race track that f’(i) must be problems. Because all the immediate for all states i. Thus, setting the initial costs of al1 the states to zero produces nonnegative function as required by Theorem 3. We applied a non-overestimating in a trial-based manner, starting each trial with the car placed on the real-time algorithms the starting line was selected with equal probability. A trial ended when the car reached a goal state. Thus, according to Theorem 3, with y = I, RTDP will converge function with trials. Although RTDP and Adaptive RTDP can back up the costs of many repeated states at each control they only back up the cost of the current state at each time step. This is the case in which in simulation mode. B, = {s!} for all t. Obviously, random number seeds, where We executed 25 runs of each algorithm using different to zero. initialized a run is a sequence of trials beginning with the evaluation that is, To monitor in line, how many moves these data, we divided each run into a sequence of each trial of each run. To record trials. By an epoch is a sequence of 20 consecutive disjoint epochs, where an epoch path the average of the path lengths generated during an epoch us- ing a given algorithm. Adaptive RTDP and Real-Time Q-Learning were applied under and for these algorithms we induced exploratory conditions behavior by using randomized policies based on the Boltzmann distribution as described in Section 7.1. To control and control, we decreased the tradeoff between the parameter T in Eq. ( 11) after each move until it reached a pre-selected minimum value; T was initialized of each run. Parameter values and additional simulation of each algorithm, we kept track of path lengths, to the finish the starting all of these algorithms were applied to the simplest case in which details are provided length we mean in Appendix B. at the beginning the performance the car took of incomplete identification information, in going function from line line in each graph shows Fig. 2 shows results for RTDP (Panel A), Adaptive RTDP (Panel B), and Real-Time length (Panel C). The central Q-Learning lines averaged over the 25 runs of the corresponding the show &I standard deviation about this average for the sample of 25 runs. Although for the initial several epochs of each algorithm are too large average epoch path lengths for the to note that the average epoch path lengths to show on the graphs, first epoch of RTDP, Adaptive RTDP, and Real-Time Q-Learning are respectively 455, 866, and 13,403 moves. That these initial average path lengths are so large, especially for Real-Time Q-Learning, reflects the primitive nature of our exploration algorithm. The upper and lower the epoch path it is useful strategy. z Policy iteration algorithms address this problem by explicitly generating a sequence of improving policies, its corresponding evaluation function, which is generally a time- but updating a policy requires computing consuming computation. A.G. Bar/o et al. /Artificial Intelligence 72 (1995) 81-138 Epoch number Epoch number learning of three real-time algorithms the 25 runs of the corresponding Fig. 2. Performance Adaptive RTDP Panel C: Real-Time Q-Learning. The central averaged over deviation of the epoch path length for the sample of 25 runs. Exploration was controlled and Real-Time Q-Learning The right side of each panel shows the paths the car would follow in noiseless conditions after effective convergence track. Panel A: RTDP. Panel B: on the small line in each graph shows the epoch path length standard for Adaptive RTDP by decreasin, n T after each move until it reached a pre-selected minimum value. from each start state algorithm. The upper and of the corresponding lines show fl algorithm. lower 122 A.G. Burro et (11. /Artijiciul Intelligence 72 (1995) RI-138 2ei43 3eio3 Epoch number of Real-Time Q-Learning on the small track for 5,000 epochs. The initial part of the Fig. 3. Performance graph shows the data plotted in Panel C of Fig. 2 but at a different horizontal scale. that faster learned learning (numbers information the graphs of moves are given in this problem RTDP (Panel A) and with incomplete in terms of the number of epochs from than Adaptive RTDP and Real-Time Q-Learning, when (and with rate is in Table 2 the versions of the (Panels of RTDP and Adaptive RTDP were so similar despite identification It is clear less variance) measured discussed below). This is not surprising given the differences between problem with complete B and C). That the performances these differences procedure used by the latter algorithm converged low level of stochasticity Time Q-Learning reach a similar Time Q-Learning Adaptive RTDP, a disadvantage of each Q-Learning epochs. in the problem takes very many more epochs level of performance. This reflects less information system rapidly on relevant states due to the (p = 0.1). These graphs also show that Real- than do RTDP and Adaptive RTDP to in Real- in RTDP or simplicity results out to 5,000 the fact that each backup than do the backups backup. Fig. 3 shows the Real-Time Q-Learning somewhat offset by the relative computational the maximum-likelihood takes into account the fact that information reflects and the randomness converged”. We inspected that result from these algorithms A convenient way to show the policies in this way by the policies produced after each algorithm was judged the graphs the average epoch path lengths essentially for RTDP for Real-Time Q-Learning is to show the car would follow from each start state if all sources of randomness were in the problem’s function were turned off. At the right in each panel of Fig. 2 are paths to have to find the smallest epoch numbers at levels: 200 reached (Panel B), and 2,000 for Adaptive RTDP these caution, (Panel C). Treated with appropriate the paths turned off, that is, if both random exploration state-transition generated “effectively which epochs epochs effective convergence The path shown noiseless conditions Panels B and C, on the other hand, were not generated by an optimal policy despite fact that each is a move shorter in in the sense that it was produced for the stochastic problem. The paths in the than the path of Panel A. The control decisions made in comparing in Panel A of Fig. 2 is optimal (Panel A), 300 epochs times are useful their asymptotic that is optimal by a policy algorithms. A.G. Barto et al. /Artificial Intelligence 72 (1995) 81-138 123 Table 2 Summary Time DP (ARTDP), Gauss-Seidel DP (GSDP) of learning performance on the small and Real-Time Q-learning track (RTQ). The amount of computation for Real-Time DP (RTDP), Adaptive Real- required by is included for comparative purposes Average time to effective convergence Estimated path length at effective convergence Average number of backups Average number of backups per epoch % of states backed % of states backed % of states backed up 0 times < 100 times < 10 times up up GSDP RTDP ARTDP 28 sweeps 14.56 252,784 - - - - 200 epochs 14.83 127,538 638 98.45 80.5 1 3.18 300 epochs 15.10 2 18,554 728 96.47 65.41 1.74 RTQ 2,000 epochs 15.44 2,96 1,790 1,481 53.34 6.68 I.56 that toward the end of the track by these suboptimal policies produce higher probability the car will collide with the track boundary under stochastic conditions. Although we do not illustrate it here, as the amount of uncertainty p), optimal policies generate paths that are more “conservative” safer distances (increasing in the sense of keeping from the track boundary lower velocities. and maintaining in the problem increases about purposes, information includes a column the policy produced track. For comparative at effective convergence Table 2 provides additional the performance of the real-time algo- for the table the path length after the effective convergence of RTDP, by executing 500 test trials with learning rithms on the small Gauss-Seidel DP We estimated Adaptive RTDP, and Real-Time Q-Learning of each algorithm. We turned off using also turned off the random exploration used by the latter two algorithms. The row of Table 2 labeled “Estimated path length at effective convergence” gives the average path length over these test trials. 23 RTDP is most directly comparable to Gauss-Seidel DP to the After about 200 epochs, or 4,000 an average of point where a trial 127,538 backups required by Gauss-Seidel function. This number of in the 15 sweeps of Gauss-Seidel DP backups after which took an average of 14.83 moves. RTDP performed in reaching DP to converge function defines an optimal policy to the optimal evaluation this level of performance, to the 136,725 backups the resulting evaluation about half the number control performance is comparable trials, RTDP (Table 1) . improved Another way to compare Gauss-Seidel DP and RTDP is to examine how the backups the cost of every state was backed DP, RTDP focused backups on fewer states. For run, RTDP backed up the costs of than 100 times and 80.51% of the states no more than run. it became even they perform are distributed over the states. Whereas up in each sweep of Gauss-Seidel example, 98.45% of the states no more 10 times; Although we did not collect more focused on the states on optimal paths. the costs of about 290 states were not backed up at all in an average in the first 200 epochs of an average for RTDP after 200 epochs, these statistics *s These path length estimates are somewhat in the graphs of Fig. 2 because convergence graphs over the costs of the start states given by the computed optimal evaluation path length than the average epoch path lengths shown at effective the turned on. For Gauss-Seidel DP, we averaged the estimated smaller they were produced with exploration lengths produced with random exploration turned off, whereas listed in Table 2. show path to obtain function 124 A.G. Barto et al. /Art$ciul Intelligence 72 (1995) 81-138 solving of incomplete Not surprisingly, level of performance the problem under conditions to achieve took 2,000 epochs, or an average of 2,961,790 backups, information took 300 epochs, or an average of 218,554 requires more backups. Adaptive RTDP trials averaging 15.1 moves at effective convergence. Real-time Q- backups, to achieve a somewhat Learning how these backups were less skillful than distributed over states shows that Adaptive RTDP was considerably more focused was Real-Time Q-Learning. In the first 300 epochs Adaptive RTDP backed up 96.47% of the states no more than 100 times and 65.41% of the states no more than 10 times. On for 53.34% the other hand, in 2,000 epochs Real-Time Q-Learning backed up Q-values than 10 of the states no more times.24 Again, of our primitive exploration than 100 times and only 6.68% of the states no more the inadequacy strategy for this algorithm. for Real-Time Q-Learning (see Fig. 3). Examining these results reflect converged effectively Fig. 4 shows results in terms of the number of epochs respectively. That Adaptive RTDP effectively for RTDP, Adaptive RTDP, and Real-Time Q-Learning results for the larger track out to 7,500 epochs. We judged on the information. These results were ob- larger race track, and Table 3 provides additional the same conditions described above for the small track. Fig. 5 shows the tained under that Real-Time Q-Learning converged at 500, 400, RTDP, Adaptive RTDP, and Real-Time Q-Learning than and 3,000 epochs is partially due to the fact that its epochs RTDP tended to have more moves, and hence more backups, than the epochs of RTDP We can see that to achieve slightly suboptimal performance, RTDP required about 62% of the computation for the initial epoch of each algorithm, which are too large to show on the graphs, are 7,198, 8,749, and 180,358 moves, respectively, for RTDP, Adaptive RTDP, and Real-Time Q- Learning. Again, reflect the primitive nature of our exploration each panel of Fig. 4 were generated effective convergence Fig. 4 is optimal that is optimal hand, were generated by slightly suboptimal policies. strategy. The paths shown at the right in in noiseless conditions by the policies produced at in Panel A of in noiseless conditions by a policy for the stochastic problem. The paths in Panels B and C, on the other of conventional Gauss-Seidel DP The average epoch path lengths these large numbers of moves, especially algorithms. The path shown for Real-Time Q-Learning, that it was produced of the corresponding in the sense faster Although these simulations narrow as learning continued. Because to back up the costs of all the states, the real-time algorithms are not definitive comparisons of the real-time algorithms some of their features. Whereas Gauss-Seidel DP with conventional DP, they illustrate focused continued to the control objectives. This focus became on subsets of the states that were relevant for Trial- increasingly Based RTDP applies to the simulations of RTDP, we know that this algorithm eventually would have focused only on relevant states, i.e., on states making up optimal paths. RTDP achieved nearly optimal control performance with about 50% of the computation of Gauss-Seidel DP on the small track and about 62% of the computation of Gauss-Seidel DP on the larger progressively because to problems with as many states as our race track track. Adaptive RTDP and Real-Time Q-Learning fewer states, but we did not run the generic indirect method for comparison it is too inefficient also focused on the convergence to apply theorem strongly 24 We considered a Q-value for a state i to be backed up whenever Q(i, u) was updated for some u E U(i). A.G. Elarto et al. /Artificial hielligence 72 (199.5) 81-138 Epoch number Epoch number C Epoch number Sara* Mm Finilh lbm F’ig. 4. Performance of three real-time learning algorithms on the huger track. Panel A: RTDP Panel B: Adaptive RTDI? Panel C: Real-Time Q-Learning. The central line in each graph shows the epoch path length averaged over the 25 runs of the corresponding algorithm. The upper and lower lines show &I standard deviation of the epoch path length for the sample of 25 runs. Exploration was controlled for Adaptive RTJJP and Real-Time Q-Learning by decreasing T after each move until it reached a pm-selected minimum value. The right side of each panel shows the paths the car would follow in noiseless conditions from each statt state after effective convergence of the corresponding algorithm. 126 A.G. Barto et ctl./Artificinl Intelligence 72 (1995) 81-138 250 0 0 le+03 2e+03 3e+03 4e+03 5e+03 6e+03 7e+O: Epoch number of Real-Time Q-Learning on the larger track for 7,500 epochs. The initial part of the Fig. 5. Performance graph shows the same data as plotted in Panel C of Fig. 4 but at a different horizontal scale. Table 3 Summary of learning performance Real-Time DP (ARTDP), required by Gauss-Seidel DP (GSDP) on the larger track and Real-Time Q-Learning for Real-Time DP (RTDP), Adaptive (RTQ). The amount of computation is included for comparative purposes Average time to effective convergence Estimated path length at effective convergence Average number of backups Average number of backups per epoch % of states backed % of states backed 8 of states backed < 100 times 6 10 times 0 times up up up GSDP RTDP ARTDP RTQ 38 sweeps 500 epochs 400 epochs 3,000 epochs 24. IO 835,468 - 24.62 5 17,356 1,035 97.17 70.46 8.17 24.72 653,714 1,634 90.03 59.90 3.53 25.04 IO,330,994 3,444 52.43 8.28 2.70 problems: contrast, each move was small enough not to have been a limiting It would have to perform at least one complete sweep for each move. In sharp for the amount of computation algorithms factor in the simulations2’ required by each of the real-time strategy that decreased The results described here for Adaptive RTDP and Real-Time Q-Learning were pro- duced by using an exploration in selecting actions by decreasing T after each move until it reached a pre-selected minimum value. Although not described here, we also conducted values and with decreasing T after of the algorithms was much altered (for the worse) by these changes. Although we made no systematic attempt performance controlled. that the is introduced and instead of after moves. Performance experiments with different minimum the effects of various exploration of these algorithms to how exploration is highly sensitive the randomness to investigate it is clear strategies, trials 25 However, that for any action at each move the n divisions under the general conditions of incomplete in implementing Adaptive RTDP on the race track problems, we took advantage of our knowledge to any state. This allowed us to avoid performing of Fq. (10). This is not possible implementation there are only two possible successors in a straightfonvard information. required A.G. Barto et al./Artijicial Intelligence 72 (1995) 81-138 127 How indication for studying of problem details other to extrapolate the algorithms by our simulations. Although some not adequate a function it difficult larger problems continue system with function approximation methods but continued research theoretical of real-time DP algorithms. What time DP algorithms off-line DP algorithms. is an encouraging to use lookup from [77] as to how the algorithms might scale, scale up to larger problems is also not adequately this collection of problems this issue. The variability of an algorithm’s performance addressed the results with the small and the larger race track give is as sets make to if they than its performance the size of its state and action on just two problems. Proceeding of these algorithms functions. Tesauro’s TD-Gammon in conjunction learning is hampered by the large space requirements tables for storing evaluation data point for using DP-based in problems much larger than those described here, complexity is that real- the computational however, is necessary is clear from our simulations, computational advantages over conventional to address can confer significant In concluding our discussion learning to think of our application DP-based misleading the most productive way to apply them to realistic DP-based racing on a specific track. This skill does not transfer with which a track robot navigation and Mahadevan to this formulation applied of the race track problem, we again point out that it is to this problem as tasks. For example, refines skill in to other tracks due to the specificity of DP-based learning algorithms robot navigation of a race track problem learning is represented. More realistic applications to requires more abstract states and actions, as in the work of Lin [45] and Connell [ 491. 11. Discussion Conventional DP algorithms selectively all possible successively approximate in AI, because explores a problem’s state spaces of many problems of interest are of limited utility for problems with large state spaces, they states and storing a cost for each state. Heuristic state space. However, because DP to such as the combinatorial fully expanding require in contrast, search, algorithms learning data structure improves function, some heuristic states from an initial state, they typically do not update the heuristic evaluation estimating in a permanent forward from each state. This information the results of repeated searches evaluation as the algorithm proceeds, ultimately from which one can determine optimal policies with relative ease. Although (such as A*) update an estimate of the cost to reach function the cost to reach a goal state from each state. is not. They effectively in a way that heuristic optimal evaluation search algorithms they are relevant to the optimal converging functions, search cache learning to learning, algorithms because the principles of DP are relevant Although are not really to be applied during problem ence accumulates However, actual or simulated enced by, the ongoing control process. Doing conventional DP algorithms they operate off-line. They are not designed learning occurs as experi- attempts at problem solving or control. to execute an otherwise off-line DP algorithm concurrently with and can be influ- requirements solving or control, whereas (or simulated) this so as to satisfy certain the DP algorithm control, where can influence, during actual it is possible 128 A.G. But-to et cd. /Artijciul Intelligence 72 (1995) 81-138 in the algorithm we call RTDP, a special case of which essentially [ 381 algorithm. This general approach follows previous in which DP principles have been used for problem solving and learning coincides research by (e.g., results with Korf’s LRTA* others [61,69,70,81,87,88]). Our contribution to bring to bear on DP-based in this article has been theory of asynchronous DP as presented by Bertsekas and Tsitsiklis suitability of asynchronous DP for implementation on multi-processor this theory, we have made novel use of these results. Applying the results on stochastic shortest path problems, for DP-based that RTDP retains algorithms, provides algorithms while still ultimately yielding optimal behavior. the learning [ 121. Although the systems motivated these results, especially to RTDP provides a new theoretical basis for asynchronous DP imply and Gauss-Seidel DP to this framework the exhaustive nature of off-line DP and the extension of Korf’s LRTA* convergence under which RTDP avoids algorithms. Convergence the competence of conventional synchronous conditions theorems theorem learning system playing learning the fact simulated We used that DP-based [ 771, and our illustrations instead of actual control. DP-based the term simulation mode to refer to the execution of RTDP and related in during algorithms control [ 61,621, Tesauro’s simulation mode is illustrated by Samuel’s checkers playing system the race track backgammon problem. Despite in simulation mode are actually off-line algorithms, we still treat them as learning algorithms because instead of they incrementally solely like RTDP, that require an accurate model of the decision problem, simulation mode is always an option and has obvious advantages due to the large number of trials often required. Applying RTDP during actual control makes sense when there is not enough time to compute a satisfactory policy by any off-line method before actual control must begin. the application of more abstract computational methods. For algorithms, of RTDP using executed through simulated experience improve control performance algorithms learning through theorem information in selecting to stochastic for improving produce a policy shortest path problems is responsive is applied, that are on optimal paths-eventually states to which the backup operation control performance. The convergence onto parts of the state set for which control Whether applied during actual control or in simulation mode, RTDP can have sig- to over conventional DP algorithms. Because RTDP it is likely for specifies conditions un- nificant advantages the demands of control can focus computation to be most important Trial-Based RTDP applied der which RTDP focuses on states all the other states-to continuing costs of some states even once. Our illustrations RTDP can obtain near optimal policies putation the approach functions mentioned, take more than 1,000 years using a 1,000 MIPS processor. This is true despite that a large fraction of the states of backgammon in normal play. related abandoning that is optimal on these relevant states without to back up the costs of all the states, and possibly without backing up the using the race track problem show that less com- is the fact that to optimal evaluation to which conventional DP cannot be feasibly applied at all. We a single sweep of conventional DP would the fact than is required by conventional DP However, more compelling illustrated by RTDP can form useful approximations in some problems with significantly to Monte Carlo algorithms that achieve computational that in backgammon, are irrelevant for example, in problems is closely RTDP effi- A.G. Bario et al. /Artificial Intelligence 72 (1995) 81-138 129 ciency by automatically allocating computation so that, for example, unimportant terms in a sum correspond to very rare events in the computational process [ 171. For this reason, the computational efficiency of Monte Carlo methods can exceed that of other methods for some classes of problems. However, Monte Carlo methods are generally not competitive with deterministic methods for small problems or when high-precision answers are required. More research is needed to fully elucidate these correspondences and to exploit them in refining DP-based learning methods and understanding their computational complexity. For problems that have very large states sets (such as backgammon), the lookup- table method for storing evaluation functions to which we have restricted attention is not practical. Much of the research on DP-based learning methods has made use of other storage schemes. For problems in which DP-based learning algorithms focus on increas- ingly small subsets of states, as illustrated in our simulations of the race track problem, data structures such as hash tables and M-trees can allow the algorithms to perform well despite dramatically reduced space requirements. One can also adapt supervised leam- ing procedures to use each backup operation of a DP-based learning method to provide training information. If these methods can generalize adequately from the training data, they can provide efficient means for storing evaluation functions. Although some success has been achieved with methods that can generalize, such as connectionist networks, the theory we have presented in this article does not automatically extend to these cases. Generalization can disrupt the convergence of asynchronous DP. Additional research is needed to understand how one can effectively combine function approximation methods with asynchronous DP. In addition to the case in which an accurate model of the decision problem is available, we also devoted considerable attention to Markovian decision problems with incomplete i.e., problems for which an accurate model is not available. Adopting information, the terminology of the engineering literature, these problems require adaptive control methods. We described indirect and direct approaches to these problems. The method we called the generic indirect method is representative of the majority of algorithms described in the engineering literature applicable to Markovian decision problems with incomplete information. A system identification algorithm adjusts a system model on- line during control, and the controller selects actions based on a current estimate of the optimal evaluation function computed by a conventional DP algorithm under the assumption that the current model accurately models the system. The DP algorithm is re- executed whenever the system model is updated. Although this approach is theoretically convenient, it is much too costly to apply to large problems. Adaptive RTDP results from substituting RTDP for conventional DP in the generic indirect method. This means that RTDP is executed using the most recent system model generated by the system identification algorithm. Adaptive RTDP can be tailored for the available computational resources by adjusting the number of DP stages it executes at each time step of control. Due to the additional uncertainty in this case, learning is necessarily slower than in the non-adaptive case when measured by the number of backups required. However, the amount of computation required to select each control action is roughly the same. This means that it is practical to apply Adaptive RTDP to problems that are much larger than those for which it is practical to apply methods, such 130 A.G. Burto et trl. /Art@d Intellipwce 72 (1995) 81-138 as the generic the system model indirect method, is updated. that re-execute a conventional DP algorithm whenever In addition [ 811 Q-Learning function without algorithm, which approximates is an asynchronous DP algorithm forming estimates of state-transition the asynchronous DP algorithm described probabilities. Q-Learning either generated by a system model or observed to indirect adaptive methods, we discussed direct adaptive methods. Direct the decision problem. We the optimal eval- that operates at in Section 5.3. Whereas of asynchronous DP is backing up the cost of a state, requiring the basic oper- methods do not form explicit models of the system underlying described Watkin’s uation instead uses sample state transitions, during actual control. Q-Learning than a finer grain the basic operation computation ation of Q-Learning is backing up the Q-value of a state-action that does not depend on the number of possible successor states. The fine grain of the basic Q-Learning in addition to selected states system. The cost of this flexibility of state-action information to focus on selected actions to the behavior of the controlled to store the Q-values backup does not gather as much as does a complete DP backup operation. backup allows Real-Time Q-Learning pairs and the fact that a Q-Learning is the increased space required to the number of possible in a way that is responsive pair, a computation proportional successor states, and is likely incomplete exploration Sophisticated of both complete strategies are important information most useful to be gained. Knowledgeable a sophisticated the time required strategy can improve control performance in solving Markovian decision prob- lems under conditions information. With complete information, by exploration decreasing to reach goal states or, in the case of RTDP, by focusing DP stages on states from which the evaluation function ordering of backups can accelerate con- vergence of asynchronous DP, whether applied off- or on-line. When is incomplete, for other reasons as well. In this case, exploration about the unknown in the case of complete this kind of ex- for this ploration must be conducted on-line. We discussed how exploration performed reason conflicts with the performance objective of control, at least on a short-term basis, and that a controller that appear to be the best based on its current evaluation structure of the system being controlled. Unlike exploration information, which can be conducted sophisticated strategies must also address should not always execute actions in simulation mode, for improving the necessity information information exploration to gather is useful function. Although we did not use sophisticated exploration strategies in our simulations of the sophisticated in this article strategies. For example, and we made no attempt exploration to analyse strategies will play an essential learning methods practical it should be clear that it is not easy to devise a consistent issues pertinent role in making for larger problems. From what we did mention, race track problem, to exploration, DP-based for however, strategy exploration about the system should in regions of is of low quality (to the that (so the state space where backup operation or (3) probabilities), visit states having successors whose costs are close to their optimal costs (so that the backup operation ( 1) visit states in regions of the state space where information uses accurate estimates of the state-transition (2) visit states is of high quality researchers have argued that an exploration learn more about information these regions), the system Each of these suggestions efficiently propagates cost information). set of desiderata about A.G. Barto et al. /Artificial Intelligence 72 (1995) 81-138 131 however, the current this fact requires It is encouraging, in practice. For example, flow of sensations. Although observable by the controller. Although control policies do not have to distinguish in the proper context, but it is not clear how to design a strategy all of them. that the convergence in this article are compatible with a wide range of exploration this article we have assumed and unambiguously to the theory and operation of all the algorithms we discussed, to satisfy that best results we strategies. that the states of the system being controlled this assump- it can be state of a robot’s world from a list of the robot’s current “sensations”. On the positive side, all possible in the states the has been and many approaches have been studied of DP- to this problem must take the what of the control objectives. The frame- the decision problem” the existence of a single definitive grain with which to de- In actuality, control objectives dictate what their passage. sensations, in the flow of the controller’s are needed makes sense incorporates have presented Throughout are completely tion is critical very difficult is vastly different effective closed-loop sensations. However, exploiting complex subject of research under many guises, based learning methods. Any widely applicable perspective constitutes in this article in which “a dynamic system underlies work adopted is misleading in suggesting lineate events and to mark is important models at different recognized, however, as components between to recognize the ability the problem of state identification a system’s state for purposes of control-indeed not independent and multiple objective-dependent them. If this caution that what constitutes the system in this article should find wide application in extending approach in a variety of disciplines, the algorithms described levels of abstraction the applicability of sophisticated to achieve it remains embedded a critical systems. itself-is factor is Acknowledgment to clarify the relationships into this subject through numerous discussions, The authors thank Rich Yee, Vijay Gullapalli, Brian Pinette, and Jonathan Bachrach for between heuristic search and control. We thank Rich helping Sutton, Chris Watkins, Paul Werbos, and Ron Williams insights for first making us aware of Korf’s research and for his very thoughtful the manuscript. We are very grateful pointing independently thank Harry Klopf, whose of learning problems. This research was supported by grants National Science Foundation Office of Scientific Research, Bolling AFB ( AFOSR-89-0526). their fundamental and we further thank Rich Sutton comments on for in an earlier version of this article. Finally, we in this class encouraged our interest the the Air Force insight and persistence to Dimitri Bertsekas to A.G. Barto from and Steven Sullivan and ECS-9214866) (ECS-8912623 out an error for sharing and Appendix A. Proof of the trial-based RTDP theorem Here we prove Theorem 3, which extends Korf’s [ 381 convergence theorem for LRTA* to Trial-Based RTDP applied to undiscounted stochastic shortest path problems. 132 A.G. Barre et al./Artijicial Intelligence 72 (199.5) 81-138 state is backed up at each time interval, Proof of Theorem 3. We first prove the theorem for the special case in which only the i.e., B, = {st} and k, = t, cost of the current for t = 0, 1, . . . (see Section 6). We then observe that the proof does not change when sy. Let G denote the goal set and let each B, is allowed to be an arbitrary set containing s, , uf, and f, respectively denote function at time step t in an arbitrary functions generated by Trial-Based RTDP starting infinite sequence of states, actions, and evaluation the state, action, and evaluation from an arbitrary start state. First observe 6 f*(i) t, f,(i) all i Z sI and if ft(j) that the evaluation functions remain non-overestimating, for all states i. This is true by induction because i.e., at any time ft+, (i) = ft (i) for < f*(j) for all j E S, then for all t f,+~(s,) = min UEWi) [ c,~,(u) + Cps,,j(u)ft(j) 1 ,iES < u~~j, C,,(U) + Cp,,(u)f*(j) ,iES [ = f*(h), 1 where the last equality restates the Bellman Optimality Equation (Eq. 6). Let I C S be the set of all states that appear infinitely often in this arbitrary sequence; c U(i) actions because for state i that have zero probability the state set is finite. Let A(i) is the set of all actions u E U(i) be the set of I must be nonempty of causing a transition to a admissible such that p;j( u) = 0 for all state not in I, i.e., A(i) there is a finite j E (S - I). Because states in S - I appear a finite number of times, time To after which all states visited are in I. Then with probability one any action chosen an infinite number of times for any state i that occurs after TO must be in A(i) (or else with probability one a transition out of I would occur), and so with probability one there must exist a time T] > TO such that for all t > 7’1, we not only have that sI E I but also that u[ E A($,). We know that at each time step t, RTDP backs up the cost of s, because s, E B,. We can write the backup operation as follows: ft+1 (St) = & 6, CutI + ~ps,,(ut)ft(j) + C p,j(ut>ft(j> 1 . jf(S-I) .IEl (A.1) But for all t > Tl, we know that s, E I and that ps,i( ur) = 0 for all j E S - I because ur E A (s,) . Thus, for t > Tl the right-most in Eq. (A. 1) is zero. This means that the costs of the states in S - I have no influence on the operation of RTDP after T,. Thus, after TI, RTDP performs asynchronous DP on a Markovian decision problem with state set I. summation If no goal states are contained in I, then all the immediate decision problem asynchronous DP must cause this contradicts there is no discounting, that the costs of the states in I to grow without bound. But its optimal cost, the fact that the cost of a state can never overestimate are positive. Because costs in this Markovian it can be shown A.G. Barto et al. /Art@cial Intelligence 72 (1995) 81-138 133 I contains a goal which must be finite due to the existence of a proper policy. Thus state with probability r,, After one. and Tsitsiklis for asynchronous DP applied therefore, Trial-Based RTDP performs shortest path problem with state set I that satisfies to undiscounted theorem (Bertsekas RTDP converges to the optimal problem. We also know that the optimal evaluation to the optimal evaluation because after time rt. [ 12, Proposition evaluation the costs of the states function 3.3, p. 3181). Consequently, function of this stochastic asynchronous DP on a stochastic of the convergence the conditions stochastic shortest path problems Trial-Based shortest path is identical to the states in I in I for this problem restricted function for the original problem in S - I have no influence on the costs of states one I contains to the current evaluation the set of all states reachable Furthermore, with probability from any start state via any optimal policy. Clearly, I contains all the start states because each start state begins an infinite number of trails. Trial-Based RTDP always executes a greedy action with respect ties in such a way that it continues to execute all the greedy actions. Because we know that the number of function policies that are restricted i.e., all the optimal actions. greedy with respect Thus with probability from any start state via any optimal policy, and there is a time after which a controller using RTDP will only execute optimal actions. is finite and that Trial-Based RTDP converges it continues to the optimal evaluation to select all the actions to I, there is a time after which to the optimal evaluation all the states reachable one, I contains and breaks function, function Finally, with trivial revision the above argument holds if RTDP backs up the costs of states other than the current state at each time step, i.e., if each B, is an arbitrary subset of s. cl Appendix B. Simulation details Except for the discount the simulations, factor y, which we set to one and the sets B,, which we set to {sI} for all t, RTDP does not involve any parameters. for its sweeps. We selected Gauss-Seidel DP only requires rate. Both an ordering without concern trials, Adaptive RTDP and Real-Time Q-Learning which we implemented in Section 4.1, we decreased using Eq. ( 11). To generate the parameter T with successive moves as follows: specifying for any influence it might have on convergence require exploration during the data described a state ordering the training throughout T(0) = T~ax, T(k+ 1) =TMin+P(T(R) -T’in)T (B.1) (B.2) where k is the move number TMin = 0.5. (cumulative over trials), p = 0.992, TM,, = 7.5, and Real-time Q-Learning additionally requires sequences of learning LY~( i, U) (Eq. ( 14)) [ 8 1,821. We defined that satisfy the hypotheses of the Q-Learning convergence these sequences as follows. Let (Ye (i, u) denote rate parameters theorem rate the learning 134 A.C. Barto et ~1. /Artificial Intelligence 72 (1995) 81-138 pair (i, U) is backed up at time parameter used when step t. Let n, (i, u) be the number of backups performed on the Q-value of (i, U) up to time step t. The learning rate CY~ (i, U) is defined as follows: the Q-value of the state-action cu,(i,u.) = ffO7 7-t n,(i,u) ’ is the initial learning a search-r&n-converge [ 191. They argue where aa implements and Moody stochastic optimization of the Q-Learning convergence theorem. rate. We set “0 = 0.5 and r = 300. This equation for each cut( i, u) as suggested by Darken in can achieve good performance that such schedules schedule tasks. It can be shown that this schedule satisfies the hypotheses References 1 I 1 C.W. Anderson, Strategy in: Proceedings Fourth S09.3, GTE Laboratories, published 103-I 14). A. Barto, Reinforcement Handbook New York, 1992) 469-49 A. Barto and S. Singh, On the computational J.L. Elman, T.J. Sejnowski Sumnzer School of Intelligent 12 13 learning with multilayer Incorporated, Waltham, MA ( 1987); connectionist representations, Tech. Report TR87- (this is a corrected version of the report Irvine, CA ( 1987) International Conference on Machine Learning, learning and adaptive critic methods, Control: Neuml. I. Fu77t ond Adaptive Approaches in: D.A. White and D.A. Sofge, eds., (Van Nostrand Reinhold, in: D.S. Touretzky, and G.E. Hinton, eds.. Connectionist Models: Proceedings of the 1990 economics of reinforcement learning, 14 I A.G. Barto, R.S. Sutton and C.W. Anderson, Neuronlike (Morgan Kaufmann, San Mateo, CA, 199 I ) 35-44. elements that can solve difficult learning in: J. A. Anderson I.51 161 Foundations of’ Research (MIT Press, Cambridge, MA, 1988). control problems. fEEE Trans. Sysr. Man Cybern. 13 (1983) 835-846; and E. Rosenfeld. Neurowmpufing: A.G. Barto, R.S. Sutton and C. Watkins. Sequential decision problems and neural networks, Touretzky, ed., Advunce.~ m Neural CA. 1990) 686-693. A.G. Barto, R.S. Sutton and C.J.C.H. Watkins, Learning and sequential decision making, and J. Moore, eds., Learning and Computational Neuroscience: Foundations of Adaptive Networks Press, Cambridge, MA, 1990) 539-602. R. Bellman and SE. Dreyfus, Functional in: D.S. Information Processing Sysfems 2 (Morgan Kaufmann, San Mateo, and dynamic programming, Math Gbles in: M. Gabriel (MIT approximations reprinted rmd allocation processes, Math. Camp. 17 ( 1973) 155-161. (Princeton University Press, Princeton, NJ, 19.57). new computational technique in lEEE Trans. Autom. Control 27 ( 1982) 6 I O-6 16. (Prentice-Hall, Englewood and Stochastic Models trnd Disrributed Computation: Numerical Methods (Prentice- in: C.L. Gil%, S.J. Hanson lrzformation Processing 5 (Morgan Kaufmann, San Mateo, regulation, I71 I81 I91 1101 IIll 1121 1131 I141 1151 1161 Deterministic to Computation approximation-a 13 ( 1959) 247-25 Other Aides I. R. Bellman, R. Kalaba and B. Kotkin. Polynomial dynamic programming: R.E. Bellman, Dynamic Programming D.P. Bertsekas, Distributed dynamic programming, D.P. Bettsekas, Dynamic Prqrummin,q: Cliffs, NJ, 1987). D.P. Bertsekas and J.N. Tsitsiklis. Pamfiel Hall, Englewood Cliffs. NJ. 1989). S.J. Bradtke, Reinforcement and J.D. Cowan, eds., Advances CA, 1993) 295-302. D. Chapman, Penquins can make cake. Al Meg. 10 (1989) 45-50. D. Chapman and performance comparisons, J. Christensen and RX. Korf, A unified learning, theory of heuristic evaluation in: Proceedings AAAI-86, Philadelphia, PA ( 1986) 148-152. and L.P. Kaelbling, input generalization to linear quadratic learning applied in: Proceedings in Neural IJCAI-91, Sydney, NSW in delayed reinforcement learning: an algorithm ( 199 1). functions and its application to A.G. Barto et al. /Art#cial Intelligence 72 (1995) 81-138 135 I 17 J J.H. Curtis& A theoretical for computing method Meyer, ed., Symposium on Monre Carlo Methods (Wiley, New York, 1954) 191-233. comparison one component of the efficiencies of two classical methods and a Monte Carlo in: H.A. of the solution of a set of linear algebraic equations, [ I8 I J.W. Daniel, Splines and efficiency [ 191 C. Darken and J. Moody, Note on learning in dynamic programming, rate schedule J.E. Moody and D.S. Touretzky, eds., Advances in Neural Kaufmann, San Mateo, CA, 1991) 832-838. for stochastic optimization, J. Math. Annl. Appl. 54 (1976) 402-407. in: R.P. Lippmann, Information Processing Systems 3 (Morgan [ 201 P. Dayan, Navigating J.E. Moody and D.S. Touretzky, eds., Advances in Neural Information Processing Systems 3 (Morgan Kaufmann, San Mateo, CA, 1991) 464-470. temporal difference, in: R.P Lippmann, through connectionism: [ 21 1 F? Dayan, Reinforcing Edinburgh, Scotland [22] P Dayan, The convergence 123 1 T.L. Dean and M.P. Wellman, Planning nnd Control (Morgan Kaufmann, San Mateo, CA, 199 1). [ 241 E.V. Denardo, Contraction mappings of TD( A) for general A, Mach. kurn. 8 (1992) 341-362. the statistical way, Ph.D. Thesis, University of Edinburgh, in the theory underlying dynamic programming, learning (1991). SIAM Rev. 9 (1967) 165-177. universally bad idea, AI Mug. 10 ( 1989) 40-44. 1251 M. Gardner, Mathematical [26] D. Gelperin, On the optimality of A *, Artif: Intell. 8 (1977) 69-76. [ 27) M.L. Ginsberg, Universal planning: an (almost) [28 1 SE. Hampson, games, Sci. Amer. 228 (1973) 108. Connectionisr Problem Solving: Computational Aspects of Biological Learning (Birkhauser. Boston, MA, 1989). 129 1 PE. Hart, N.J. Nilsson and B. Raphael, A formal basis for the heuristic determination of minimum cost IEEE Trans. Syst. Sci. Cybern. 4 ( 1968) 100-107. paths, [30] J.H. Holland, rule-based Arfljicial Inteitigence Approach. Volume II (Morgan Kaufmann, San Mateo, CA, 1986) 593-623. to J.G. Carbonell and T.M. Mitchell, eds., Muchine Learning: An Escaping brittleness: learning algorithms in: R.S. Michalski, of general-purpose the possibility systems, applied [ 3 1 ] D.H. Jacobson and D.Q. Mayne, Differenrial Dynamic Programming (Elsevier, New York, 1970). [ 321 A. Jalali and M. Ferguson, Computationally for Markov chains, efficient adaptive control algorithms in: Proceedings 28rh Conference on Decision and Control, Tampa, FL ( 1989) 1283- 1288. [ 33 1 M.1. Jordan and R.A. Jacobs, Learning in: D.S. Touretzky, ed., Advances in Neurul Information Processing Systems 2 (Morgan Kaufmann, San Mateo, CA, 1990). to control an unstable system with forward modeling, [ 34 I L.P. Kaelbling, Learning in Embedded Systems (MIT Press, Cambridge, MA, 1991); revised version of: Teleos Research TR-90-04 ( 1990). 1351 S. Kirkpatrick, CD. Gelatt and MI? Vecchi. Optimization by simulated annealing, Sci. 220 (1983) 671-680. 1361 A.H. Klopf, Brain function and adaptive systems-a heterostatic theory, Tech. Report AFCRL-72-0164, Air Force Cambridge Research Laboratories, Bedford, MA (1972); a summary appears in: Proceedings International Conference on Systems, Man, and Cybernetics ( 1974). [ 37 I A.H. Klopf, The Hedonistic Neuron: A Theory of Memory, Learning, and Intelligence (Hemishere, Washington, DC, 1982). [ 381 R.E. Korf, Real-time heuristic search, A@ [ 39) P.R. Kumar, A survey of some results Inrell. 42 ( 1990) 189-211. in stochastic adaptive control, SIAM J. Control Optimizufion 23 (1985) 329-380. I40 ] V. Kumar and L.N. Kanal, The CDP: a unifying formulation in: L.N. Kanal and V. Kumar, eds., Search in Artificial Inrelligence for heuristic search, dynamic programming, (Springer- and branch-and-bound, Verlag, Berlin, 1988) l-37. [ 41 I H.J. Kushner and P. Dupuis, Numerical Methods for Stochastic Control Problems in Continuous 7ime ( Springer-Verlag. New York, 1992). 1421 W.H. Kwon and A.E. Pearson, A modified quadratic cost problem and feedback stabilization of a linear system, fEEE Trans. Aurom. Control 22 ( 1977) 838-842. [ 431 Y. le Cun, A theoretical eds., Proceedings 1988) 21-28. framework in: D. Touretzky, G. Hinton and T. Sejnowski, I988 Connectionisr Models Summer School (Morgan Kaufman% San Mateo, CA, for back-propagation, 136 A.ti. Btrrto PI trl. /Art[jificirzl Intellz~ence 72 (1995) RI-138 I44 I M. Lemmon, Real-time optimal path planning using a distributed computing paradigm, in: Proceedings American Control Conferenre, Boston, MA ( 1991). I45 I L.J. Lin. Programming robots using reinforcement Anaheim, CA (1991) 781-786. learning and teaching, in: Proceedings AAAI-91, I46 I L.J. Lin. Self-improvement based on reinforcement learning, planning and teaching, in: L.A. Bimbaum and G.C. Collins, eds., Muclzing Learning: Proceedings Eighih Internationul Workshop (Morgan Kaufmann, San Mateo. CA, 1991) 323-327. I47 I L.J. Lin, Self-improving reactive agents: case studies of reinforcement learning frameworks, in: From Arzimczls fo Animus: Proceedings Fir.yt International Corzference on Simulation of Adaptive Behavior. Cambridge, MA ( 1991) 297-305. I48 I L.J. Lin, Self-improving Leclrn. 8 ( 1992) 293-32 reactive agents based on reinforcement I, learning, planning and teaching, Much. I49 1 S. Mahadevan and J. Connell, Automatic programming of behavior-based robots using reinforcement learning, Artif Intell. 55 ( 1992) 31 I-36.5. I SO I D.Q. Mayne and H. Michalska, Receding horizon control of nonlinear systems. /EXE Trans. Aufom. Control 35 ( 1990) 8 14-824. IS1 I L. Mdro, A heuristic search algorithm with modifiable estimate, Artif: I S2 I D. Michie and R.A. Chambers, BOXES: an experiment in adaptive control, Intell. 23 ( 1984) 13-27. in: E. Dale and D. Michie, eds.. Muchine lrztelligetzce 2 (Oliver and Boyd, Edinburgh, 1968) 137- 152. 153 1 M.L. Minsky, Theory of neural-analog reinforcement systems and its application to the brain-model problem, Ph.D. Thesis, Princeton University. Princeton, NJ ( 1954). I54 I M.L. Minsky, Steps toward artificial intelligence, Proceedings Institute of Radio Engineers 49 ( 196 I ) 8-30: reprinted in: E. A. Feigenbaum and J. Feldman, eds., Conzpufers and Thought (McGraw-Hill, New York, 1963) 406-490. IS5 I A.W. Moore, Efficient memory-based learning for robot control, Ph.D. Thesis, University of Cambridge, Cambridge, England ( 1990). IS6 I A.W. Moore, Variable resolution dynamic programming: efficiently learning action maps in multivariate real-valued state-spaces, in: L.A. Bimbaum and G.C. Collins, eds., Mac/zing Leurning: Proceedings Eighth /nrernurionul Workshop (Morgan Kaufmann, San Mateo, CA, 1991) 333-337. 157) A.W. Moore and C.G. Atkeson, Memory-based reinforcement learning: efficient computation with prioritized sweeping, in: S.J. Hanson, J.D. Cowan and C.L. Giles, eds., Advances in Neural Informafion Processing 5 (Morgan Kaufmann, San Mateo, CA, 1993). 1581 J. Peng and R.J. Williams, Efficient learning and planning within the dyna framework, Adaptive Behavior 2 ( 1993) 437-454. IS9 j M.L. Puterman and M.C. Shin, Modified problems, Munage. Sci. 24 ( 1978) policy I I27- I 137. iteration algorithms for discounted Markov decision Introduction I 60 I S. Ross, 161 I A.L. Samuel, Some studies reprinted ( 1959) 210-229; (McGraw-Hill, New York, 1963). to Stochastic Dynttmic Pro,qnzmming (Academic Press, New York, 1983). in machine learning using the game of checkers, IBM J. Rex Develop. in: E.A. Feigenbaum and J. Feldman, eds., Computers and Thought I62 I A.L. Samuel. Some studies in machine learning using the game of checkers. II-Recent progress, IBM ( 1967) 601-6 17. Adaptive confidence and adaptive curiosity, Tech. Report FKI-149-91 lnstitut ftir Technische Universitat Miinchen, 800 Miinchen 2, Germany ( 199 I ). I64 I M.J. Schoppers, Universal plans for reactive robots in unpredictable environments, in: Proceedings IJCAI-87, Milan, Italy ( 1987) IO39- 1046. 16.51 M.J. Schoppers, In defense of reaction plans as caches, A/ &fag. 10 (1989) 51-60. (661 S.P. Singh and R.C. Yee, An upper bound on the loss from approximate optimal value functions. technical note, Mach. Lenm. 16 (1994) 227-233. 167 I R.S. Sutton, Temporal Massachusetts, Amherst, MA ( 1984). credit assignment in reinforcement learning, Ph.D. Thesis, University of [ 68 I R.S. Sutton. Learning 1691 R.S. Sutton, Integrated to predict by the method of temporal differences, Mach. Lerzm. 3 ( 1988) 9-44. architectures for learning, planning, and reacting based on approximating dynamic programming, in: Prwerdings Seventh fnterruzfional Conference on Machine Learning (Morgan Kaufmann, San Mateo, CA. 1990) 2 I h-224. J. Res. Dewlo+ I63 I J. Schmidhuber, Informatik, A.G. Barto et al. /Artificial Intelligence 72 (1995) 81-138 137 170 I R.S. Sutton, Planning by incremental dynamic programming, in: L.A. Bimbaum and G.C. Collins, eds., Maching Learning: Proceedings Eighth International Workshop (Morgan Kaufmann, San Mateo, CA, 1991) 353-357. I71 ] R.S. Sutton, ed., A Special Issue c$ Machine Learning on Reinforcement Learning, Mach. Learn. 8 ( 1992); also published as: Reinforcement Learning (Kluwer Academic Press, Boston, MA, 1992). [ 721 R.S. Sutton and A.G. Barto, Toward a modem theory of adaptive networks: expectation and prediction, Psychol. Rev. 88 (1981) 135-170. 1731 R.S. Sutton and A.G. Barto, A temporal-difference model of classical conditioning, in: Proceedings Ninth Annual Conference of the Cognifive Science Society, Seattle, WA ( 1987) [ 741 R.S. Sutton and A.G. Barto, Time-derivative models of Pavlovian in: M. Gabriel and J. Moore, eds., Learning and Computational Neuroscience: Foundations of Adaptive Networks (MIT Press, Cambridge, MA, 1990) 497-537. reinforcement, 1751 R.S. Sutton, A.G. Batto and R.J. Williams, Reinforcement learning is direct adaptive optimal control, in: Proceedings American Control Conference, Boston, MA ( 1991) 2143-2146. [ 761 M. Tan, Learning a cost-sensitive in: L.A. Bimbaum eds., Maching Learning: Proceedings Eighth International Workshop (Morgan for reinforcement representation learning, internal and G.C. Collins, Kaufmann, San Mateo, CA, 1991) 358-362. in temporal difference learning, Mach. Learn. 8 ( 1992) 257-277. 177 1 G.J. Tesauro, Practical [78 1 S. Thrun, The role of exploration issues in: D.A. White and D.A. Sofge, eds., Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches (Van Nostrand Reinhold, New York, 1992) 527-559. in learning control, 1791 S.B. Thrun and K. Moller, Active exploration in: J.E. Moody, S.J. Hanson eds., Advances in Neural infortnafion Processing Sysfems 4 (Morgan Kaufmann, in dynamic environments, and R.P. Lippmann, San Mateo, CA, 1992). [801 PE. Utgoff and J.A. Clouse, Two kinds of training information for evaluation function learning, in: Proceedings AAAI-91, Anaheim, CA ( I99 1) 596-600. (81 1 C.J.C.H. Watkins, Learning from delayed rewards. Ph.D. Thesis, Cambridge University, Cambridge, England ( 1989) 1821 C.J.C.H. Watkins and P Dayan, Q-learning, Mach. Learn. 8 (1992) 279-292. dynamic programming [83] P. Werbos, Approximate in: D.A. White and D.A. Sofge, eds., Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches (Van Nostrand Reinhold, New York, 1992) 493-525. for real-time control and neural modeling, [ 841 P.J. Werbos, Beyond regression: new tools for prediction and analysis in the behavioral sciences, Ph.D. Thesis, Harvard University, Cambridge, MA ( 1974). 1851 P.J. Werbos, Advanced forecasting methods for global crisis warning and models of intelligence, General Systems Yearbook 22 (1977) 25-38. 1861 P.J. Werbos, Applications sensitivity analysis, eds., System Modeling an Optimization ( Springer-Verlag, Berlin, 1982). in nonlinear of advances in: R.F. Drenick and F. Kosin, [ 87 1 PJ. Werbos, Building and understanding adaptive systems: a statistical/numerical approach to factory automation and brain research, IEEE Trans. Syst. Man Cybern. ( 1987). 1881 PJ. Werbos, Generalization of back propagation with applications to a recurrent gas market model, Neural Networks 1 ( 1988) 339-356. [ 891 D. White and M. Jordan, Optimal control: a foundation in: D.A. White and D.A. Sofge, eds., Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches (Van Nostrand Reinhold, New York, 1992) 185-214. for intelligent control, 1901 S.D. Whitehead, Complexity in: L.A. Bimbaum and G.C. Collins, eds., Maching Learning: Proceedings Eighth International Workshop (Morgan Kaufmann, San Mateo, CA, 199 1) 363-367. and cooperation in Q-learning, [ 911 R.J. Williams and L.C. Baird III, A mathematical optimal controls Adaptive and Learning Systems, New Haven, CT (1990) 96-101. dynamic programming, incremental through analysis of actor-critic architectures learning in: Proceedings Sixth YaZe Workshop on for 1921 I.H. Witten, An adaptive optimal controller for discrete-time Markov environments, Infor. Control 34 ( 1977) 286-295. 138 A.G. Barfo er al. /Artijicial lnfelligence 72 (1995) RI-138 I93 1 1.H. Witten, Exploring, modelling and controlling discrete sequential environments, IN. J. Man-Mach. Stud. 9 (1977) 715-735. 1941 R.C. Yee, Abstraction in control learning, Tech. Report 92-16, Department of Computer Science, University of Massachusetts, Amherst, MA (1992). 