Artificial Intelligence 173 (2009) 343–364Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintComputing leximin-optimal solutions in constraint networks ✩Sylvain Bouveret∗, Michel LemaîtreONERA – Centre de Toulouse, 2, avenue Édouard Belin, BP74025, 31055 Toulouse Cedex 4, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 31 October 2007Received in revised form 23 September2008Accepted 31 October 2008Available online 8 November 2008Keywords:LeximinFairnessMultiobjective optimizationConstraint programming1. IntroductionIn many real-world multiobjective optimization problems one needs to find solutionsor alternatives that provide a fair compromise between different conflicting objectivefunctions—which could be criteria in a multicriteria context, or agent utilities in amultiagent context—while being efficient (i.e. informally, ensuring the greatest possibleoverall agents’ satisfaction). This is typically the case in problems implying human agents,where fairness and efficiency requirements must be met. Preference handling, resourceallocation problems are another examples of the need for balanced compromises betweenseveral conflicting objectives. A way to characterize good solutions in such problems isto use the leximin preorder to compare the vectors of objective values, and to select thesolutions which maximize this preorder. In this article, we describe five algorithms forfinding leximin-optimal solutions using constraint programming. Three of these algorithmsare original. Other ones are adapted, in constraint programming settings, from existingworks. The algorithms are compared experimentally on three benchmark problems.© 2008 Elsevier B.V. All rights reserved.In many collective decision making problems implying human agents, one needs to find efficient and fair solutions. Ina social choice context and informally, an efficient solution is a solution ensuring the greatest possible satisfaction to thesociety of agents. Efficiency1 can have several formal definitions, as we will see later. Concerning fairness, this propertyrefers to the need to make compromises between the agents’ objectives, which are often conflicting or even antagonistic.It is impossible to give a widely accepted and formal definition of the notion of fairness, just because it depends on thesituation at stake and on the agents implied. The interested reader can refer to [33,48] for a deep investigation on fairnessin many different contexts. One prominent definition of fairness, to which we will stick in this article, is the one given byegalitarianists: informally, in this context, a fair solution tries to balance the utilities of the agents, or to make the worst-offas well-off as possible. This definition will be formally defined later in the article.Fairness is particularly relevant in areas such as crew or worker timetabling and rostering problems, or the optimizationof long and short-term planning for firemen and emergency services. Fairness is also ubiquitous in resource allocationproblems, like, among others, bandwidth allocation among network users, fair share of airspace and airport resources amongseveral airlines [1], or Earth observing satellite scheduling and sharing problems [24]. As we noticed earlier, the fairness✩This article is an extended version of [S. Bouveret, M. Lemaître, New constraint programming approaches for the computation of leximin-optimalsolutions in constraint networks, in: M.M. Veloso (Ed.), Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), Hyderabad,India, AAAI Press, 2007].* Corresponding author.E-mail addresses: Sylvain.Bouveret@onera.fr (S. Bouveret), Michel.Lemaitre@onera.fr (M. Lemaître).1 Unfortunately, the words “efficiency” and “efficient” have two meanings. In Sections 1 and 2, we refer to the meaning they have in social choice andmicroeconomics contexts (e.g. Pareto-efficiency). From Section 3, we will refer to the computer science meaning (e.g. efficiency of an algorithm). In anycase, this ambiguity can be easily cleared up considering the context in which the words appear.0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.10.010344S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364requirement always comes with an efficiency requirement, at least implicitly: workers must be occupied, resources must beallocated, agents must be satisfied, . . . , “as most as possible”.In spite of the wide range of problems concerned by fairness issues, it often lacks a theoretical and generic approach. Inmany applications, fairness and efficiency are only enforced by specific heuristic local choices guiding the search towardssupposed balanced alternatives or decisions. However, a few works may be cited for their approach of this fairness require-ment. The article [24] investigates a multiagent decision problem which consists in sharing a common property resource ina fair and efficient way, three ways of handling this problem being proposed; the first one gives priority to fairness; the sec-ond one to efficiency; the third one computes a set of compromises based on a linear combination of efficiency and fairnessindices. More recently and in a quite different direction, [36] proposes a new constraint based on statistics, which enforcesthe relative balance of a given set of variables, and can possibly be used to ensure a kind of fairness among a set of agents.Fairness is also studied in operational research, with for example [35], which proposes to model fairness requirements withan Ordered Weighted Average (OWA) aggregation [46] and investigates the way to solve this optimization problem usinglinear programming.Microeconomics and social choice theory provide an important literature on fairness in collective decision making be-tween agents. From this theoretical background we borrow the idea of representing preferences (or satisfaction levels) ofagents by utility levels, and we adopt the leximin preorder on utility profiles2 for conveying the fairness and efficiency−→x = (x1, x2, . . . , xn) andrequirements. Here is an informal definition of the leximin preorder. Consider two utility profiles−→−→x . If mini xi = mini yi , then wey = ( y1, y2, . . . , yn). If mini xi < mini yi then the leximin preorder strictly preferseliminate one occurrence of the lowest value from both profiles, and we continue the process, comparing the second lowestvalues, and so on until either we find unequal minimal values, or the profiles are empty (in which case they are leximin-indifferent). Notice that the only case for which the profiles are leximin-indifferent is when they are equivalent up to apermutation of their elements. For example, the profile (2, 2, 1, 2) is strictly leximin-preferred to the profile (4, 1, 5, 1),while (4, 1, 5, 1) and (1, 1, 4, 5) are indifferent.−→y toBefore introducing it formally in Section 2, we will now give some reasons for which the leximin preorder conveysefficiency and fairness. First of all, leximin-optimal solutions are such that the worst-off agent is made as well-off as possible,thus perfectly matching the definition of fairness we introduced earlier. Secondly, leximin-optimal solutions are also Pareto-efficient, which means that we cannot increase the utility of one agent without decreasing the utility of another agent: itcorresponds to a prominent notion of efficiency in collective decision making problems.Computing a leximin-optimal solution is not a trivial problem, and cannot be easily translated into classical optimizationframeworks. In this article, we will focus on this problem in the constraint programming (CP) framework, which is aneffective and flexible tool for modeling and solving combinatorial problems. We will provide several generic algorithmsfor computing leximin-optimal solutions in this framework, the aim being to benefit from this powerful and expressiveframework, and from existing state-of-the-art solvers, while adapting them to our particular problem.Apart from the fact that it can convey and formalize the concepts of fairness and efficiency in multiagent contexts, theleximin preorder is also a subject of interest in other contexts. This preorder is of particular importance in the context ofmultiobjective or multicriteria decision making and optimization. In this context, it can be used to enforce a good balancebetween criteria or objectives, while ensuring the Pareto-efficiency of the solution. Of course, the algorithms given in thisarticle are generic enough to be applied to multicriteria, multiobjective or multiagent problems. Moreover, we may noticethat the leximin preorder is also of interest in other domains, such as fuzzy CSP [14], and symmetry-breaking in constraintsatisfaction problems [15].This contribution is organized as follows. Section 2 gives a minimal background in social choice theory and justifies for-mally the interest of the leximin preorder as a fairness criterion. Section 3 motivates the use of the constraint programmingframework for dealing with the search for leximin-optimality in a generic way, and defines the search for leximin-optimalityin this framework. The main contribution of this article is Section 4, which describes five algorithms for computing leximin-optimal alternatives, three of them being new, and the other ones adapted from existing works. We also introduce in thissection a general method for designing good heuristics dedicated to the computation of leximin-optimal solutions. All thealgorithms introduced have been implemented within a constraint programming system and their performance compared.Section 5 presents an experimental comparison of these algorithms on three different kinds of randomly generated instances.2. Background on social choice theory−→x ), or between brackets, (e.g. (x1, . . . , xn)). The notation {−→We first introduce some notations. Calligraphic letters (e.g. X ) will stand for sets. Vectors will be written with an arrow−→(e.g.x(i.e. {x1, . . . , xn}), and, unless explicitly specified, f () will stand for the vector↓−→i ) for the ithx rearranged in increasing (resp. decreasing) order. We will write xcomposed by each element of−→↓). Finally, R denotes the set of real numbers; N is the set of non-negative integers; thexelement of vectorinterval of integers between k and l (included) is written (cid:2)k, l(cid:3).x } will be used as a shortcut for the set of elements of(resp.−→x ) for ( f (x1), . . . , f (xn)). Vector(resp. x(resp.−→x−→x−→x↑i↑↑↓2 A utility profile is a vector of agent utilities, each profile corresponding to a given alternative or decision.S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–3643452.1. Collective decision making and welfarismLet N be a set of n agents, and A be a set of admissible alternatives (or decisions) concerning all of them, among whicha benevolent arbitrator—or the society of agents—has to choose one. The most prominent model describing this situationis welfarism (see for example [19,32]): the choice of the arbitrator is made on the basis of the utility levels enjoyed by theindividual agents and on those levels only. Each agent i ∈ N is associated with an individual utility function ui that mapseach admissible alternative a ∈ A to a numerical index ui(a) ∈ R. Therefore to each alternative a can be attached a singleutility profile (u1(a), . . . , un(a)). The model assumes that the individual utilities are comparable between the agents. In otherwords, they are expressed using a common utility scale. According to welfarism, comparing two alternatives is performedby comparing their respective utility profiles.A standard way to compare utility profiles is to aggregate each of them into a collective utility index, standing for thecollective welfare of the agents community. A collective utility function uc maps each alternative a to a collective utilityindex uc(a) = g(u1(a), . . . , un(a)), where g : Rn → R is an aggregation function. The most used aggregation functions are thesum and the minimum, but a wide range of functions are possible, each one conveying different principles [28]. An optimalalternative, that is a most preferred one according to the collectivity, is an alternative maximizing the collective utility overthe set of admissible alternatives.2.2. The leximin preorder as a fairness and efficiency criterionThe main difficulty of fair decision problems is to reconcile the contradictory preferences of the agents. Since generallyno solution fully satisfies everyone, the aggregation function g must lead to fair and efficient compromises. The fairnessrequirement will be discussed in depth in this section. Regarding efficiency requirements, the most widely accepted criterionis Pareto-optimality.−→y Pareto dominatesDefinition 1 (Pareto domination, Pareto-optimality [13]). Let−→−→x ∈ V is Pareto-optimal in Vx if and only if−→if and only if no vector of V Pareto dominatesx . We extend these definitions to alternatives in the following way: let aand b two alternatives, we say that a dominates b if the utility profile of a dominates the utility profile of b. The alternativea ∈ A is Pareto-optimal in A if and only if no alternative of A dominates a.y and ∀i ∈ (cid:2)1, n(cid:3): xi ≤ yi . Let V be a set of vectors of Rn. The vector−→y be two vectors of Rn. We say thatx (cid:7)= −→−→−→x andThe problem of choosing the right aggregation function g for computing a collective utility index corresponding to eachprofile is far beyond the scope of this article. We only describe the two standard ones corresponding to two opposite pointsof view on social welfare:3 classical utilitarianism and egalitarianism. The rule advocated by the defenders of classicalutilitarianism is that the best decision is the one that maximizes the sum of individual utilities (thus corresponding tog = +). However this kind of aggregation function can lead to huge differences of utility levels among the agents, thus−→u = (10, 10, 10)ruling out this aggregation in the context of fair decisions. As an example, consider two utility profiles:−→andv , which is clearly unfair. Consider also that the sumaggregation cannot discriminate between−→v = (1, 1, 29). An utilitarian decision maker will select profile−→u is clearly the fairest.−→w = (1, 1, 28), although−→u = (10, 10, 10) andFrom the egalitarian point of view, the best decision is the one that maximizes the utility of the least satisfied agent(thus corresponding to g = min).−→y ) if and only if minnDefinition 2 (Min preorder, min-optimality). Let−→x ∼minminni=1( yi). We writebe a set of vectors of Rn. A vectori=1(xi) = minni=1( yi). The vector−→−→−→−→x ≺minx (cid:12)miny ory fory of V is said min-optimal in V if and only if ∀−→−→−→y is min-preferred to−→x ∼mini=1(xi) < minn−→x and−→y be two vectors of Rn.−→x and−→y are said min-indifferent (written−→y ) if and only if−→y . The binary relation (cid:12)min is a total preorder. Let V−→x (written−→x ≺minx ∈ V:−→x (cid:12)min−→y .Whereas the min aggregation function is particularly well-suited for problems in which fairness is essential, it hasa major drawback, due to the idempotency of the min operator, and known as “drowning effect” in the community of−→u = (14, 20, 17) andfuzzy CSP [12]: it leaves many alternatives indistinguishable. As an example, consider the profiles−→−→v = (14, 15, 15). They cannot be distinguished by the min aggregation function, althoughv is notPareto-optimal). As a more striking example, the utility profiles (0, . . . , 0) and (1000, . . . , 1000, 0) cannot be discriminated,even if the second one appears to be much better than the first one. In other words, the min aggregation function can selectnon-Pareto-optimal alternatives, which is not desirable.−→u dominates−→v (soThe leximin preorder is a known refinement of the order induced by the min aggregation function that overcomes thisdrawback. It has been introduced in the social choice literature (initially by [41] and discussed in depth in [10,22,32] amongothers) as the social welfare ordering that reconcile egalitarianism and Pareto-optimality, and also in fuzzy CSP [14]. It isdefined as follows:3 Compromises between these two extremes are possible. See [33, page 68] or [46] (OWA aggregation).346S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364−→−→y are said leximin-x andDefinition 3 (Leximin preorder [41], leximin-optimality). Let−→−→−→↑ = −→↑x ≺leximiny )x (writtenyindifferent (written↑−→−→−→x ≺leximin= yif and only if ∃i ∈ (cid:2)0, n − 1(cid:3) such that ∀ j ∈ (cid:2)1, i(cid:3), xy ory forj−→−→−→y of V is saidy . The binary relation (cid:12)leximin is a total preorder. Let V be a set of vectors of Rn. A vectorx ∼leximinleximin-optimal in V if and only if ∀−→−→x and. The vector↑↑i+1 < yj and x−→y be two vectors of Rn.−→y is leximin-preferred to↑i+1. We write−→y ) if and only if−→x ∼leximin−→x (cid:12)leximinx ∈ V:−→y .−→x−→x (cid:12)leximinIn other words, the leximin preorder is the lexicographic preorder over ordered utility vectors. For example, we have(4, 1, 5, 1) ≺leximin (2, 2, 1, 2), because (1, 2, 2, 2) is greater than (1, 1, 4, 5), according to the lexicographic preorder.From the previous definition, it is easy to see that a leximin-optimal vector is also a min-optimal one. It is known that aleximin-optimal vector is also a Pareto-optimal one [32]. This social welfare ordering has a noticeable characterization, whichexplains its central place and huge importance in the theory of cardinal welfarism: it is the only social welfare orderingwhich (1) is independent of the common utility pace and (2) which satisfies the Pigou–Dalton property at the same time(see for example [32, page 40] or [33, page 266]). The independence of the common utility pace property states that for anyincreasing bijection τ : R → R, we have (u1, . . . , un) (cid:12)leximin (v 1, . . . , vn) ⇔ (τ (u1), . . . , τ (un)) (cid:12)leximin (τ (v 1), . . . , τ (vn)); inother words, the individual utilities can be defined up to any increasing dilatation τ without modifying the leximin preorder.The Pigou-Dalton property asks that any sum-preserving transfer of utility from a more satisfied agent to a less satisfied−→v ) such that ∃i (cid:7)= jone that narrows their two utilities leads to a collectively preferred utility profile. More formally: ∀(−→−→such that (1) ui < {v i, v j} < u j , (2) ui + u j = v i + v j , and (3) ∀k /∈ {i, j}, uk = vk, we havev is obviously av .more equitable utility profile, since some amount of utility has been transferred from agent j (the former happier agent) toagent i. Satisfying the Pigou–Dalton property is desirable in a context where fairness is required: it reduces the inequalitiesbetween the agents when it is possible.−→u ,−→u ≺leximin−→x (cid:15)→ −(cid:2)ni=i nA known result is that no collective utility function can represent the leximin preorder,4 unless the set of possible utilityprofiles is countable. This is not a major limitation, since in practice this set is often finite or it can be discretized andreduced to a finite one. In this latter case, the leximin preorder can be represented by the following non-linear functions:5(cid:2)−q−xi (adapted for leximin from one of the alternative approaches proposed in [15]), g2 :ni=1 x,g1 :(cid:2)i↑nwhere q > 0 is large enough [32], or by an Ordered Weighted Average operator [46,47] g3 :i , wherew 1 (cid:16) w 2 (cid:16) · · · (cid:16) wn (where (cid:16) informally means “sufficiently bigger than”). The major drawback of using this kind of−→x in-representation is that it rapidly becomes unreasonable to use it when the upper bound of the possible values ofcreases. Moreover, it hides the semantics of the leximin preorder, and hinders the computational benefits we could possiblytake advantage of. These points will be discussed further in Section 4.−→x (cid:15)→ −i=i w i · u−→x (cid:15)→In the following, we will use the leximin preorder as a criterion for ensuring fairness and efficiency, and we will seekthe set of leximin-optimal alternatives. This problem will be expressed in the next section in a constraint programmingframework.3. Constraint programming and leximin-optimalityThe constraint programming (CP) framework is an effective and flexible tool for modeling and solving many differentcombinatorial problems such as planning and scheduling problems, resource allocation problems, or configuration problems[11,31,39]. Examples of actual CP frameworks and solvers are Ilog Solver, OPL Studio, Comet or Choco (see [16]).The flexibility of the CP framework allows the user to model problems in a mathematical and incremental way, as acooperation of separate constraints linked by shared variables. The CP user is provided with basic constraints such as “==”,“(cid:7)=”, “≤”, as well as so-called “global constraints” [45] such as “scalar product” and “allDifferent”. A CP solver implementsa kernel providing a basic inter-constraint propagation mechanism [3]. In this kernel, intra-constraint specific propagationalgorithms are plugged.The CP framework is a natural support of the search for leximin-optimality, for two complementary reasons. Firstly,constraints are a very convenient and flexible way to define the set of admissible alternatives, and to link decision variablesto individual utilities. Secondly, the search for leximin-optimality can be considered separately in a generic way and castedas a kind of global constraint, hence providing highly re-usable algorithms in different contexts.3.1. Constraint networksThe CP framework is based on the notion of constraint network. A constraint network (X , D, C) consists of a set ofvariables X = {x1, . . . , xp}, a set of associated domains D = {Dx1 , . . . , Dxp}, Dxi being the set of possible values for xi,and a set of constraints C, where each C ∈ C specifies a set of allowed instantiations (the concept of instantiation willbe defined latter) V(C) over a set of variables X (C), called the scope of the constraint. From now on, we will supposethat all the domains are finite subsets of N. A constraint whose scope is of size n will be called a n-ary constraint. Inthe following, we will use bold letters for variables (e.g. x), and for a given variable x, we will respectively write x and x−→x (cid:12)leximin4 In other words there is no g such that5 For g1 and g2, the domain of utilities must be restricted to strictly positive real numbers.−→y ). See [32, page 34].−→x ) ≤ g(−→y ⇔ g(S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364347for min(Dx) and max(Dx). In the algorithms, we will also use the following shortcuts for domain reductions: x ← α forDx ← Dx ∩ (cid:2)α, +∞(cid:2) (all the values under α are removed from the domain of x, notice that if α < x, Dx is not modified),x ← α for Dx ← Dx ∩ (cid:3)−∞, α(cid:3) (all the values over α are removed from the domain of x, notice that if α > x, Dx is notmodified), and x ← α for Dx ← {α} ∩ Dx (all the values different from α are removed from the domain of x, notice that ifα /∈ Dx, then Dx becomes empty).is the restriction of this instantiation to S (cid:20)An instantiation v of a set S of variables is a function that maps each variable x ∈ S to a value v(x) of its domain Dx.If S = X , this instantiation is said to be complete, otherwise it is partial. If S (cid:20) (cid:2) S, the projection of an instantiation v of Sover S(cid:20)and is written v↓S(cid:20) (for any set of instantiations V , we will also writeV↓S(cid:20) the set {v↓S(cid:20) |v ∈ V}). An instantiation v satisfies a constraint C if and only if it is defined on a superset of X (C) andv↓X (C) ∈ V(C). An instantiation is said to be consistent if and only if it satisfies all the constraints. A complete consistentinstantiation of a constraint network is called a solution. The set of solutions of (X , D, C) is written sol(X , D, C).3.2. Constraint Satisfaction Problems and constraint propagationGiven a constraint network, the problem of determining whether it has a solution is called a Constraint SatisfactionProblem (CSP) and is NP-complete. This problem can be solved by using backtracking search techniques [42] associatedto constraint propagation [3,11], the latter aiming at detecting inconsistencies earlier in the searching process. Constraintpropagation is generally based on algorithms for ensuring generalized arc consistency or bound consistency [3,11,45] thatwe now define, after [45]. The constraint C is said generalized arc consistent if and only if, for every x ∈ X (C) and everyvalue α ∈ Dx there exists an instantiation v ∈ V(C) such that v(x) = α. Now, let C be a constraint such that the domain Dxof each variable x ∈ X (C) is an interval domain, that is Dx = (cid:2)x, x(cid:3). Then, C is said bound consistent if and only if for(cid:20) ∈ V(C) such thatevery x ∈ X (C) there exists an instantiation v ∈ V(C) such that v(x) = x and another instantiation v(cid:20)(x) = x.v3.3. The LeximinOptCSPproblemThe CSP can be adapted to become an optimization problem in the following standard way. Given a constraint network(X , D, C) and an objective variable o ∈ X , find the value M of Do such that M = max{v(o) | v ∈ sol(X , D, C)}. We will writemax(X , D, C, o) for the subset of those solutions that maximize the objective variable o.Expressing a collective decision making problem with a numerical collective utility criterion as a CSP with objec-tive variable is straightforward: consider the collective utility as the objective variable o, and link it to the variablesrepresenting individual utilities u1, u2, . . . , un with the constraint o == g(u1, u2, . . . , un), where g is, as explained in Sec-tion 2, the chosen aggregation function. However this cannot directly encode our problem of computing a leximin-optimalsolution, which is a kind of multicriteria optimization problem. We introduce formally the LeximinOptCSP problem as fol-lows:Definition 4 (LeximinOptCSP).Input: a constraint network (X , D, C); a vector of variables−→u such that each element ui is in X , called the objectiveOutput: “Inconsistent” if sol(X , D, C) = ∅. Otherwise a solution (cid:3)v, called a leximin-optimal solution, such that ∀v ∈vector.sol(X , D, C), v(−→u ) (cid:12)leximin(cid:3)v(−→u ).In the following, the set of leximin-optimal solutions of a constraint network (X , D, C) with objective vectorwritten leximinOpt(X , D, C,−→u ).−→u will beWe describe in the next section several generic constraint programming algorithms that solve this problem. The idea isto use the search-tree exploration algorithms and the constraint propagation mechanisms provided by the CP framework astools for computing efficiently a leximin-optimal solution of a constraint network.4. Constraint programming algorithms for leximin optimization4.1. Overview of existing works and contribution of this articleFinding a leximin-optimal solution is algorithmically very easy when the input constraint network has only a few solu-tions: as a leximin-optimal solution is also a min-optimal one, we can compute all the min-optimal solutions (this problemcan be solved as a CSP with a single objective variable) and then compare them to find a leximin-optimal one. It is thesolution suggested in [13, page 162]. This rather naive approach can give good results on some class of problems and shouldnot be completely ignored, as suggested by [34].However, many instances have a huge number of min-optimal solutions, and thus need a cleverer approach to becometractable. The algorithmic aspects linked to the computation of a leximin-optimal solution have been treated in severalworks coming from several fields. Operational researchers have been interested in leximin-optimal solutions in the context348S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364of multicriteria optimization (see e.g. [13]), for example in the context of equitable resource allocation problems [27], lo-cation problems [34], or matrix games [37]. The leximin preorder is also an appealing criterion when dealing with fuzzyCSP, and its algorithmics has been naturally studied in this community [12]. However, the algorithms presented in theseworks are often either restricted to an easy case (e.g. continuous problems with linear or at least convex objective func-tions), or can become rapidly unreasonable in some cases, as we will see later in this section. An exception is the workfrom [34] citing [29] that briefly presents an efficient algorithm for computing a leximin-optimal solution in the discretecase. Section 4.7 revisits this latter work in a constraint programming setting.In this section, we describe five different algorithms for solving the LeximinOptCSP, namely for computing leximin-optimal solutions in constraint networks:• Algorithm 1 (leximin-based branch-and-bound, Section 4.3) is a new branch-and-bound algorithm, adapted to the lex-imin preorder. The main contribution here is the use, for lower bound filtering purposes, of a constraint introduced in[15] in a very different context.• Algorithm 2 (branching on saturated subsets, Section 4.4) is an adaptation, in constraint programming settings, of aknown algorithm due to [12].• Algorithm 3 (based on the Sort constraint, Section 4.5) is a new algorithm, although simple and rather intuitive, basedon the Sort constraint.• Algorithm 4 (based on the AtLeast meta-constraint, Section 4.6) is also a new algorithm. It constitutes the main techni-cal contribution of this article. It contains also a new and specific way to propagate the meta-constraint AtLeast.• Algorithm 5 (using max-min transformations, Section 4.7), as said before, is revisiting a previous work by [29]. However,our presentation of this algorithm points out its interesting connection with comparison networks.Note that all algorithms except the first one are multi-step optimization algorithms: they are based on iterative optimiza-tions, where at each step we maximize the value of one element of the sorted version of the objective vector.During the presentation of the algorithms, we will use the following example to illustrate how the algorithms work:Example 1. Let (X , D, C) be a constraint network and let (u1, u2, u3) ∈ X 3 be an objective vector. We suppose that theset of solutions of the constraint network leads to the following set of admissible values for the objective vector: (1, 1, 0),(5, 5, 3), (7, 3, 5), (1, 2, 1), (9, 5, 2), (3, 4, 3), (5, 3, 6) and (10, 3, 4). Notice that this instance has 5 different min-optimalsolutions, which are (5, 5, 3), (7, 3, 5), (3, 4, 3), (5, 3, 6) and (10, 3, 4), and only one leximin-optimal solution, which is(7, 3, 5). One can also notice that the latter leximin-optimal solution is different from the sum-optimal solution (10, 3, 4),corresponding to the classical utilitarian point of view.All the algorithms presented use two functions solve and maximize (the detail of which is the concern of solvingtechniques for constraints satisfaction problems), which respectively return one solution v ∈ sol(X , D, C) (or “Inconsistent”if sol(X , D, C) = ∅), and an optimal solution (cid:3)v ∈ max(X , D, C, o) (or “Inconsistent” if sol(X , D, C) = ∅).4.2. Difficulties with the leximin represented by a collective utility functionBefore entering into the description of our algorithms, let us first explain why a straightforward approach, which consistsin a standard optimization of a collective utility function representing the leximin preorder, is not effective.As explained in Section 2, the leximin preorder can be represented by a collective utility function if the number ofpossible values for the objective vector is finite (which is our assumption). In that case, finding a leximin-optimal solutionjust comes down to a simple optimization problem with the adequate collective utility function. We here discuss the prac-tical relevance of encoding the leximin preorder by a collective utility function in the CP optimization framework, that is,introducing an objective variable uc that would stand for the collective utility and would be linked to the former objectivevector−→u by a constraint representing the collective utility function.The first matter is the size of the domain of uc, due to the combinatorial nature of the set of admissible solutions. If wesuppose that all the Dui are identical,6 and are of size m, then one can prove that the number of equivalent classes for theleximin preorder (corresponding to the minimal size of Duc ) is:(cid:4)(cid:5)m + n − 1n= (m + n − 1)!(m − 1)! n!This is actually the number of combinations with repetitions of n objects taken from a set of cardinality m (see for example[21, Exercise 1.2.6-60]). It is equivalent to mn when m → +∞. This can become a problem, when m grows up, because it isdifficult for some CP systems to handle very huge domains efficiently.6 This is a reasonable assumption, since for the leximin preorder to be meaningful, the elements of the objective vector must be expressed on a commonscale.S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364349−→u by usingThe second issue is the way one can specify the collective utility function by a constraint between uc and(cid:2)−ui , norone of the aggregation functions introduced in Section 2.2. In the general case, neither the constraint uc = −ni=1 n−q are easy to propagate, which seems to dissuade to use this way for computing a leximin-the constraint uc = −optimal solution. However, for some particular cases of agents’ utilities, this approach can be nevertheless efficient. Forexample, consider the case of a multiagent resource allocation problem where one must give one and only one object toeach agent, an object j given to an agent i producing the utility ui = w i j . In that case, one can directly define the objectivevariables by ui = −wif object j is given to agent i, the −w−qi j being precomputed.(cid:2)ni=1 ui−qi jExample 1a. In the example given at the end of Section 4.1, the collective utility function defined by uc : (u1, u2, u3) (cid:15)→−(u1 + 1)−9 − (u2 + 1)−9 − (u3 + 1)−9 is suitable for representing the leximin preorder. This needs a few explanations. Thechoice of q = 9 comes from the fact that it is the lowest integer (computed numerically) such that uc represents the leximinpreorder on the set of vectors of 3 elements taking their values between 0 and 10. Replacing the ui by (ui + 1) preventsfrom being out of the domain of definition of the function uc (defined for ui > 0). The values for the admissible solutions−6, uc(1, 2, 1) =are (approximately) the following: uc(1, 1, 0) = −1.00, uc(5, 5, 3) = −4.01 × 10−6 and uc(10, 3, 4) =−3.96 × 10−4.33 × 10−3, uc(9, 5, 2) = −5.09 × 10−6. One can check that the leximin-optimal vector (7, 3, 5) has the greatest value.−6, uc(5, 3, 6) = −3.94 × 10−5, uc(3, 4, 3) = −8.14 × 10−6, uc(7, 3, 5) = −3.92 × 10Of course, even if the problem of propagating the constraint specifying the collective utility is solved, the aforementionedproblem of the cardinality of the domain of the collective utility remains. In concrete terms, the constraint programmingexperiments we made showed us that this encoding of the leximin preorder is not very efficient, at least in the CP frame-work.4.3. Algorithm 1: leximin-based branch-and-boundA natural approach to the algorithmics of leximin is to adapt the branch-and-bound algorithm, which is the standardway of optimizing in CP, to the leximin-preorder. Informally speaking, it works as follows (see Algorithm 1): it computesa first solution, then tries to improve it by specifying that the next solution has to be strictly better (in the sense of theleximin preorder) than the current one, and so on until the constraint network becomes inconsistent. It works as if it waslooking for all the solutions (like the naive approach evoked in Section 4.1), but instead of comparing them at the end of thesearch process, it prunes the branches that cannot lead to a better solution than the best one found so far. This approach isbased on the following constraint:Definition 5 (Constraint Leximin). Letthe set {−→−→λ ≺leximin v(x } (variables belonging to−→x ).−→x be a vector of variables,−→x ). The constraint Leximin(−→λ be a vector of integers, and v be an instantiation of−→x } and is satisfied by v if and only ifλ ,x ) holds on {−→−→Although this constraint does not exist in the literature, the works of [15] and [20] introduce an algorithm for enforcinggeneralized arc consistency on a quite similar constraint: the Multiset Ordering constraint, which is, in the context ofmultisets, the equivalent of a leximax7 constraint on vectors of variables. At the price of some straightforward adaptations,the algorithm introduced in this work can be used to enforce the latter constraint Leximin. However, even if the MultisetOrdering constraint is very close to the constraint Leximin, it has never been used, to the best of our knowledge, in thecontext of computing a leximin-optimal (or leximax-optimal) solution. That is why we consider that this Algorithm 1 is newin this context.7 The leximax is based on an decreasing reordering of the values, instead of a increasing one for leximin.350S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364Proposition 1. If function solve is correct and halts, then Algorithm 1 halts and solves the LeximinOptCSP.The proof is rather straightforward, so we omit it.4.4. Algorithm 2: branching on saturated subsetsOur next algorithm, which is our first multi-step algorithm, is based on a recursive solving of successive min-optimalsub-problems. This algorithm has been introduced, in the context of fuzzy CSP, in [12], and is also briefly suggested in [13,page 145]. The idea is to find all the possible sets of “worst” objective variables and to fix explicitly their value (definingwhat is called “strong α-cuts” in the context of fuzzy CSP). By “worst”, we refer to saturated subsets of objective variables:Definition 6 (Saturated subset). Let (X , D, C) be a constraint network andmin-optimal value ofvariables from−→u be a vector of objective variables. Let (cid:3)m be the−→u , that is, (cid:3)m = maxv∈sol(X ,D,C){mini{v(ui)}}. A saturated subset of objective variables is a subset Ssat ofu such that ∃v ∈ sol(X , D, C) with ∀x ∈ Ssat, v(x) = (cid:3)m and ∀y ∈ {−→u } \ Ssat, v(y) > (cid:3)m.−→In our Example 1, the saturated subsets are {u2}, {u3} and {u1, u3}. Among these ones, the cardinality-minimal ones are{u2} and {u3}.Clearly, the only saturated subsets that can lead to a leximin-optimal solution are the cardinality-minimal ones. Theidea of the algorithms introduced in [12] for computing leximin-optimal solutions in the context of fuzzy CSP is based onthe computation of cardinality-minimal saturated subsets of objective variables. The algorithms informally work as follows.Firstly, the min-optimal value (cid:3)m and the cardinality-minimal saturated subsets of objective variables are computed. Thenfor each such subset Ssat, each variable from Ssat is removed from the objective vector, and its value is fixed to (cid:3)m, and thesame is done on the new objective vector, until no variable remains.In the general case, at each step there can be several cardinality-minimal saturated subsets of variables. The algorithmcan therefore be seen as a branching procedure that chooses at each node on which saturated subset it branches. Algo-rithm 2 is the translation in the CP framework of the Depth-First-Search algorithm proposed in [12]. It is based on thefunction explore, which is recursively called to explore the search tree: at each node, it computes the min-optimal value(which corresponds to the function findMinOptimal, not developed here, but which makes one call to maximize),computes the cardinality-minimal saturated subsets (which corresponds to the function findSaturatedSubsets whichmakes several calls to solve), and branches on them. Before returning, the algorithm calls the function leximinOptimalwhich selects a leximin-optimal solution by simple leximin comparisons, because some branches of the search tree can leadto sub-optimal solutions.S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364351Fig. 1. The search tree developed by Algorithm 2 for Example 1.Example 1b. The search tree developed by Algorithm 2 for Example 1 is shown in Fig. 1. On the left side of the figure,one can see the saturated subsets of objective variables, and on the right side the remaining solutions for each node of thesearch tree are shown. The set sol returned by the call to explore in Algorithm 2 is {(5, 3, 6), (7, 3, 5), (5, 5, 3)}.The biggest problem with this algorithm is to find the saturated subsets, and, since in the general case there may beseveral ones, to branch on them. However, there are special known cases where there is, at each step, one saturated subsetthat is included in all the others, that is, only one cardinality-minimal saturated subset. In other words, findMinimal-SaturatedSubsets always returns only one saturated subset. In these cases, the previous algorithm does not produceany branching on the saturated subsets, and it suffices to choose at each step the cardinality-minimal saturated subset. Thisoccurs for example in continuous linear problems, where the set of alternatives is convex (see e.g. [13,27,34,37]), whichexplains the success of this algorithm and why it performs well in this context.4.5. Algorithm 3: based on the Sort constraintHowever, in the context of discrete problems like leximin-CSP, there can be several cardinality-minimal saturated sub-sets at each step, and finding them might be very expensive, which renders this kind of algorithms unusable in practice.A solution to overcome this difficulty is to introduce new variables to replace the objective ones in a way such that (1) itpreserves the leximin-optimal set of solutions, and (2) it guarantees the uniqueness of the cardinality-minimal saturatedsubset.One way to do it is to introduce the sorted version of the objective vector. The leximin-optimal solutions relativelyto the sorted objective vector are clearly the same as the leximin-optimal solutions relatively to the non-sorted objectivevector. Moreover, the only saturated subsets are made of the first elements of the sorted objective vector, and thus theleximin-optimal solutions can be computed by successively maximizing the first elements of the latter sorted vector.−→y of the objective vector−→u can be naturally done in the CP framework by intro-Introducing the entire sorted versionducing a constraint Sort(−→u ,−→y ), which is defined as follows:Definition 7 (Constraint Sort). LetThe constraint Sort(increasing order.−→x ,−→x−→x and(cid:20)) holds on {−→−→(cid:20)xx } ∪ {−→xbe two vectors of variables of the same length, and v be an instantiation.−→(cid:20)}, and is satisfied by v if and only if v(x ) in(cid:20)) is the sorted version of v(−→xThis constraint has been particularly studied in two works, which both introduce a filtering algorithm for enforcing−→x ). The authorsbound consistency on it. The first algorithm comes from [5] and runs in O (n log n) (n being the size of352S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364of [30] designed a simpler algorithm that runs in O (n) plus the time required to sort the interval endpoints ofcan asymptotically be faster than O (n log n).Our new algorithm (see Algorithm 3) intuitively works as follows: having introduced the sorted version−→y of the ob-−→u , it successively maximizes the elements of this vector, provided that the leximin-optimal solution is thejective vectorsolution that maximizes y1, and, given this maximal value, maximizes y2, and so on until yn.−→x , whichExample 1c. Let us go back to our example. At the beginning of the algorithm, 3 new variables (y1, y2, y3) are intro-−→duced, to stand for the sorted version of the objective vector. The admissible instantiations for (y ) are the followingones: ((1, 1, 0), (0, 1, 1)), ((5, 5, 3), (3, 5, 5)), ((7, 3, 5), (3, 5, 7)), ((1, 2, 1), (1, 1, 2)), ((9, 5, 2), (2, 5, 9)), ((3, 4, 3), (3, 3, 4)),((5, 3, 6), (3, 5, 6)), and ((10, 3, 4), (3, 4, 10)).−→u ,• During the first step y1 is maximized (3 is its maximal value), and then it is fixed to its optimal value 3. The remain-ing admissible instantiations are then: ((5, 5, 3), (3, 5, 5)), ((7, 3, 5), (3, 5, 7)), ((3, 4, 3), (3, 3, 4)), ((5, 3, 6), (3, 5, 6)) and((10, 3, 4), (3, 4, 10)).• During the second step y2 is maximized (5 is its maximal value), and then it is fixed to its optimal value 5. Theremaining admissible instantiations are then: ((5, 5, 3), (3, 5, 5)), ((7, 3, 5), (3, 5, 7)) and ((5, 3, 6), (3, 5, 6)).• During the third step y3 is maximized (7 is its maximal value). The unique leximin-optimal solution is: ((7, 3, 5),(3, 5, 7)).Proposition 2. If functions maximize and solve are both correct and both halt, then Algorithm 3 halts and solves the Lex-iminOptCSP.Proof. If sol(X , D, C) = ∅ and if solve is correct, then Algorithm 3 obviously returns “Inconsistent”. We will suppose in thefollowing that sol(X , D, C) (cid:7)= ∅ and we will use the following notations: Si and S(cid:20)i are the sets of solutions of (X (cid:20), D(cid:20), C(cid:20))respectively at the beginning and at the end of iteration i.We have obviously Si+1 = S(cid:20)i for all i ∈ (cid:2)1, n − 1(cid:3), which proves that if Si (cid:7)= ∅, then the call to maximize at line 6 doesnot return “Inconsistent”, and Si+1 (cid:7)= ∅. Thus (cid:3)v(n) is well-defined, and obviously ((cid:3)v(n))↓X is a solution of (X , D, C).We note (cid:3)v = (cid:3)v(n) the instantiation computed by the last maximize in Algorithm 3. Suppose that there is an instantiation↑−→u )i . Then, due−→u ). Following Definition 3, there is+(yi+1). Due to line 7, we have (cid:3)v(yi+1) = (cid:3)v(n)(yi+1) =(i+1)(yi+1) strictly greater than (cid:3)v(i+1)(yi+1),v ∈ sol(X , D, C) such that (cid:3)v(−→to constraint Sort, (cid:3)v(y ) and van i ∈ (cid:2)0, n − 1(cid:3) such that ∀ j ∈ (cid:2)1, i(cid:3), (cid:3)v(yj) = v(cid:3)v(i+1)(yi+1). Thus vwhich contradicts the hypothesis about maximize. (cid:2)−→u ) ≺leximin v(−→y ) are the respective sorted version of (cid:3)v(is a solution in max(X (cid:20), D(cid:20), C(cid:20), yi+1) with objective value vthe extension of v that instantiates each yi to v(+(yj) and (cid:3)v(yi+1) < v−→u ). We define v−→u ) and v+(+(+++4.6. Algorithm 4: based on the AtLeast meta-constraint(cid:2)−→x and a number α,Before presenting this new algorithm, we introduce the following notation: given a vector of numbersi(α ≤ xi) will be the cardinality of the set {i | α ≤ xi}. This notation is inspired by the constraint modeling language OPL[43], where (α ≤ xi) is 1 if the inequality is satisfied and 0 otherwise.The previous algorithm introduced explicitly the sorted version of the objective vector, which required the propagationsto be performed on all its elements. However, it is possible to define the ith element of the sorted objective vector, withoutexplicitly introducing the entire latter vector. The following proposition gives the trick:Proposition 3. Let(cid:6)↑xi= max−→x be a vector of numbers of size n. Then we have:(cid:7)(cid:7)(cid:7)(cid:9)(α ≤ xi) ≥ n − i + 1(cid:8).αiS. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364353In other words, the ith minimum of a vector of numbers of size n is the maximal number α such that at least n − i + 1elements of the vector are greater than or equal to α. This basic proposition is present, explicitly or not, in some otherworks involving sorting. It is explicitly used by [49] in the context of the job-shop problem, and implicitly used by [5] and[30] for propagating the constraint Sort (these two latter works are based on the former). Our Algorithm 4 will make a newusage of this proposition: it allows us to introduce each element of the sorted vector “lazily”, one after another, contrary tothe two other propagation algorithms dedicated to the constraint Sort.The structure of Algorithm 4 is similar to the one of Algorithm 3. Informally it works as follows:• it first computes the maximal value (cid:3)y1 of y1 such that there is a solution v with(cid:2)i( (cid:3)y1 ≤ v(ui)) = n (or in other words(cid:2)• then it fixes y1 to (cid:3)y1 and computes the maximal value (cid:3)y2 of y2 such that there is a solution v withi( (cid:3)y2 ≤ v(ui)) ≥• and so on until, having fixed yn−1 to (cid:3)yn−1, computing the maximal value (cid:3)yn of yn such that there is a solution v with∀i, (cid:3)y1 ≤ v(ui)),n − 1,(cid:2)i( (cid:3)yn ≤ v(ui)) ≥ 1.To enforce the constraint on the ui, we make use of the meta-constraint AtLeast, derived from a cardinality combinatorintroduced by [44]:Definition 8 (Meta-constraint AtLeast). Let (cid:5) be a set of p constraints, and k ∈ (cid:2)1, p(cid:3) be an integer. The meta-constraintAtLeast((cid:5), k) holds on the union of the scopes of the constraints in (cid:5). It is satisfied by an instantiation v if and only if atleast k constraints from (cid:5) are satisfied by v.This Algorithm 4 is now illustrated in our example.Example 1d.• During the first step a variable y1 is introduced, and all the objective variables must have a value which is higher−→u , y1): ((1, 1, 0), 0), ((5, 5, 3), (cid:2)0, 3(cid:3)), ((7, 3, 5), (cid:2)0, 3(cid:3)), ((1, 2, 1), (cid:2)0, 1(cid:3)),than y1. It gives the following solutions for (((9, 5, 2), (cid:2)0, 2(cid:3)), ((3, 4, 3), (cid:2)0, 3(cid:3)), ((5, 3, 6), (cid:2)0, 3(cid:3)) and ((10, 3, 4), (cid:2)0, 3(cid:3)). y1 is fixed to its maximal value 3 (written inbold), which restricts the set of admissible instantiations to the following ones: ((5, 5, 3), 3), ((7, 3, 5), 3), ((3, 4, 3), 3),((5, 3, 6), 3), ((10, 3, 4), 3).• During the second step a variable y2 is introduced, and at least two of the objective variables must have a value−→u , y2) are thus the following ones: ((5, 5, 3), (cid:2)0, 5(cid:3)), ((7, 3, 5), (cid:2)0, 5(cid:3)),which is higher than y2. The solutions for (((3, 4, 3), (cid:2)0, 4(cid:3)), ((5, 3, 6), (cid:2)0, 5(cid:3)) and ((10, 3, 4), (cid:2)0, 4(cid:3)). y2 is fixed to its maximal value 5 (written in bold), whichrestricts the set of admissible instantiations to {((5, 5, 3), 5), ((7, 3, 5), 5), ((5, 3, 6), 5)}.• During the third step a variable y3 is introduced, and at least one of the objective variables must have a value which is−→u , y3): ((5, 5, 3), (cid:2)0, 5(cid:3)), ((7, 3, 5), (cid:2)0, 7(cid:3)) and ((5, 3, 6), (cid:2)0, 6(cid:3)). Thehigher than y3. It gives the following solutions for (maximal value for y3 is 7 (written in bold), which leads to the unique leximin-optimal solution (7, 3, 5).Proposition 4. If functions maximize and solve are both correct and both halt, then Algorithm 4 halts and solves the Lex-iminOptCSP.(cid:20)i, Ci). We willIn the following proofs, we will write soli and sol(cid:20)i)↓X j for the same sets of solutions projected on X j (with j < i). We can notice that sol0 =also write (soli)↓X j and (sol(cid:20)sol(X , D, C), and that ∀i, sol⊆ soli (because of line 8 that restricts the domain of yi).ii , respectively referring to sol(Xi, Di, Ci) and sol(Xi, D(cid:20)354S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364Lemma 1. If sol0 (cid:7)= ∅ then (cid:3)v(n) is well-defined and not equal to “Inconsistent”.(cid:20)(cid:20)(cid:7)= ∅, and let v(i) ∈ solProof. Let i ∈ (cid:2)1, n(cid:3), suppose that soli−1. Then extending v(i) by instantiating yi to min j(uj) leadsi−1to a solution of (Xi, Di, Ci) (only one constraint has been added and it is satisfied by the latter instantiation). Therefore(cid:20)(cid:20)soli (cid:7)= ∅ and, if maximize is correct, (cid:3)v(i) (cid:7)= “Inconsistent” and (cid:3)v(i) ∈ soli . So, soli(cid:7)= ∅. It proves Lemma 1 by induction. (cid:2)Lemma 2. If sol0 (cid:7)= ∅, then ((cid:3)v(n))↓Xi∈ soli, ∀i ∈ (cid:2)0, n(cid:3).(cid:20)⊆ soli , and (soli+1)↓XiProof. We have soli(cid:20)More generally, (soli)↓X j(cid:20)· · · ⊆ (soli+1)↓Xi⊆ soli⊆ soli . (cid:2)⊆ (soli)↓X j , and (soli+1)↓X j(cid:20)⊆ soli (since from (Xi, D(cid:20)i, Ci) to (Xi+1, Di+1, Ci+1) we just add a constraint).⊆(cid:20)i)↓X j , as soon as j ≤ i. Hence, ((cid:3)v(n))↓Xi⊆ (soln)↓Xi(cid:20)n)↓Xi∈ (sol⊆ (solLemma 3. If sol0 (cid:7)= ∅, (cid:3)v(n)(−→y ) is equal to (cid:3)v(n)(−→u )↑.Proof. For all i ∈ (cid:2)1, n(cid:3), ((cid:3)v(n))↓Xi] satisfiesthe cardinality constraint of iteration i, and is then a solution of soli . By definition of function maximize, we thus have(cid:3)v(i)(yi) ≥ (cid:3)v(n)(is a solution of soli by Lemma 2. By Proposition 3, ((cid:3)v(n))↓Xii . Since (cid:3)v(i)(yi) = (cid:3)v(n)(yi), we have (cid:3)v(n)(yi) ≥ (cid:3)v(n)(Since (cid:3)v(n) is a solution of soln, at least n − i + 1 numbers from vector (cid:3)v(n)(−→u ) are greater than or equal to (cid:3)v(n)(yi). At−→u ) must then be greater than or equal to (cid:3)v(n)(yi). These elements include[yi ← (cid:3)v(n)(↑−→u )i .↑−→u )least the n − i + 1 greatest numbers from (cid:3)v(n)(i , which leads to (cid:3)v(n)(yi) ≤ (cid:3)v(n)((cid:3)v(n)(↑−→u )↑−→i , proving the lemma. (cid:2)u )↑−→u )iWe can now put things together and prove Proposition 4.Proof Proposition 4. If sol(X , D, C) = ∅, and if solve is correct, then Algorithm 4 obviously returns “Inconsistent”. Other-wise, following Lemma 1, it outputs an instantiation ((cid:3)v(n))↓X which is, according to Lemma 2, a solution of (X0, D0, C0) =(X , D, C).Suppose that there is a v ∈ sol(X , D, C) such that (cid:3)v(n)(↑−→u )of indices), ∃i ∈ (cid:2)1, n(cid:3) such that ∀ j < i, v(instantiating y1, . . . yi−1 to (cid:3)v(n)(y1), . . .(cid:3)v(n)(yi−1) and yi to v(ing all the previous equalities, we have ∀ j < i v= (cid:3)v(n)(↑−→u )j−→−→u ) ≺leximin v(u ). Then following Definition 3 (up to a substitution↑−→+j and (cid:3)v(n)(i < v(u )(i) be the extension of v respectively↑−→u )j . By gather-↑−→=u )ii . Following Lemma 3, ∀ j, (cid:3)v(n)(yj) = (cid:3)v(n)(↑−→u ))j . We also have v↑−→u )i . Let v+(i)(yj) = (cid:3)v(n)(yj) = v((i)(yi) = v(↑−→+i . By Proposition 3, ∀ j ≤ i at least n − j + 1 numbers from (vu ))(i)(yj), proving+(i) satisfies all the cardinality constraints at iteration i. Since it also satisfies each constraint in C and maps each= (cid:3)v(i)(yi). It contradicts+(i)(that vvariable of Xi to one of its possible values, it is a solution of soli , and vthe definition of maximize, proving Proposition 4. (cid:2)↑−→u )j−→+u )) are greater than or equal to v(i)(+(i)(yi) = v(i > (cid:3)v(n)(↑−→u )i↑−→u )↑−→u )= (v+(i)((v+There are some easy ways to encode the AtLeast((cid:5), k) meta-constraint in a CP framework. A straightforward one is to“reify” each sub-constraint Ci in (cid:5), introducing a new variable ci for each one with domain {0, 1}, then introducing newconstraints holding on each ci, defined as ci = 1 if Ci is satisfied, ci = 0 otherwise, and finally postingi ci ≥ k. Anotherpossibility for our specific use would be to encode AtLeast({y ≤ x1, . . . , y ≤ xn}, k) using the global constraint CardPath[2] or Slide [4]. CardPath(m, [x1, . . . , xn], C), where C is a constraint of arity r, holds if and only if C(xi, . . . , xi+r−1) holdsm times, with 1 ≤ i ≤ n − r + 1. Our specific use of AtLeast could be encoded by CardPath with r = 1 and C defined asC(xi) = (y ≤ xi) with the additional constraint m ≥ k.However in our case where each constraint in the set (cid:5) is of the form y ≤ xi, bound consistency can be enforced usingthe following specialized algorithm (recall that the notation x ← α means that all the values above α are removed from Dx).(cid:2)S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364355This algorithm runs in O (n), since the selection of the kth highest value ofThe algorithm informally works as follows. If the domains of the variables are such that the constraint cannot be satisfiedanymore (line 1), the procedure returns “Inconsistent”. Otherwise, if exactly k variables among the xi can still be greaterthan y then these variables must be greater than y (line 3). In any case the value of y cannot be higher than the kth highestvalue of the xi (line 4).−→x can be done in O (n) [8, page 189]. We cannotice that this algorithm is well-suited for event-based implementation of constraint propagation: in case of an update ofone of the xi, only lines 2–4 need to be run (because the update of y will empty the domain of y if the condition on line 1is not satisfied anymore); in case of an update of y, only lines 1–3 need to be run; any other update does not require toand updating it when one of the xirun the algorithm. The procedure can also benefit from storing the ordered vectorchanges, taking O (n) time. By doing so, we can access↓k in O (1).It can also be noticed that since all its arguments constraints are linear, the meta-constraint AtLeast can be expressedusing a set of linear constraints, therefore allowing our algorithm to be implemented with a linear solver (provided that allother constraints are linear). The usual way [17, page 11] is to express our constraint AtLeast by introducing n 0–1 variables{δ1, . . . , δn}, and a set of linear constraints {y ≤ x1 + δ1y, . . . , y ≤ xn + δny,(cid:2)ni=1 δi ≤ n − k}.−→x−→x↓4.7. Algorithm 5: using max-min transformationsAnother way to make the sorted version of the objective appear, without using specific constraints with associatedpropagation mechanisms like in the two latter algorithms, is to use a set of “max-min transformations”. This solution,introduced in [29] (and cited in [34]) for dealing with leximin-optimization problems with non-convex sets of alternatives,is based on the following idea: replacing two elements ui, uj of the objective vector by two variables m and M respectivelystanding for the minimum and the maximum of the two elements does not change the leximin-optimal set of solutions. Inthe following, we will use the notation (M, m) == MaxMin(x, y) as a shortcut for the couple of constraints M == Max(x, y)and m == Min(x, y).−→u be an objective vector, and ui and uj be two distinct variables−→u . We introduce two new variables M and m, and define: X (cid:20) = X ∪ {M, m}, D(cid:20) = D ∪ {DM, Dm}, with DM = Dm =−→u ,Proposition 5. Let (X , D, C) be a constraint network,from(cid:2)min(ui, uj), max(ui, uj)(cid:3), and C(cid:20) = C ∪ {(M, m) == MaxMin(ui, uj)}. We also defineexcept that ui and uj have been replaced by M and m. We have leximinOpt(X , D, C,−→v the vector made of elements from−→u ) = leximinOpt(X (cid:20), D(cid:20), C(cid:20),−→v )↓X .The proof of this proposition is obvious. By iteratively applying this transformation rule, we can replace the initial−→m(1)):−→u (1) in the following way (introducing a new vector of intermediate variablesu (0) by a new oneu = −→−→objective vector(cid:10)u(1)n , m(cid:10)(1)n−1, mu(1)mn(cid:11)(1)n−1(1)n−2(cid:11)(cid:10)(1)(1)2 , mu1(cid:11)(1)u1(0)== un== MaxMin== MaxMin· · ·== MaxMin(1)== m1which is equivalent to:(cid:10)m(cid:10)m(cid:11)(1)n , u(1)n−1, u(0)n−1(0)n−2(cid:11)(cid:10)m(0)(1)2 , u1(cid:11)(cid:12)(cid:13)(1)u1(1)ui== Min== Max(0)1 , uu(cid:10)(0)i−1, Minu(0)(0)2 , . . . , un(cid:12)(0)ui, u(0)(0)i+1, . . . , un(cid:13)(cid:11), ∀i ∈ (cid:2)2, n(cid:3)Using this reformulation, the minimum of the objective variables appears naturally as a new variable uvariable is clearly the only one cardinality-minimal saturated subset within the new vector of objective variablesin Algorithm 2, this variable u−→u (2), and so on.(1)1 , and this−→u (1). Like(1)1 will be set to its maximal value (cid:3)m, before processing a new vector of objective variablesInterestingly, it was not noticed by previous authors that a max-min transformation can be interpreted as a comparator,and the entire sorting process as a comparison network [8, page 704].Definition 9 (Comparator). A comparator is a device with two inputs x and y and two outputs xfollowing functions: x(cid:20) = min(x, y). A comparator will be represented as follows:(cid:20) = max(x, y) and yand y(cid:20)(cid:20), that performs the356S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364Fig. 2. The comparison network of the sorting algorithm implicitly used in Algorithm 5 for n = 5.In the algorithm proposed here (Algorithm 5), there is an implicit use of a sorting algorithm implemented using acomparison network: the successive reformulations of the objective variables correspond to the use of comparators. Finally,the entire algorithm implicitly uses the sorting algorithm presented in Fig. 2.Each comparator of Fig. 2 is “implemented” by two constraints in Algorithm 5, and each dot corresponds to a differentvariable. Notice that the variables and the constraints are introduced layer by layer, since at one step we only need theminimal variable (the uin the figure and in the algorithm). The layers are introduced at each step by the functionminLayer. As said before, we need to restrict the set of admissible solutions to the ones such that the minimum objectivevariable is maximal before introducing a new layer. This is the role of lines 8 and 9 of Algorithm 5.(i)iExample 1e. Here is an illustration of Algorithm 5 on the example. The table below shows the set of solutions for the initialobjective variables and the new ones. At the first step, the variablesis maximized. Fixing thisvariable to its maximal value restricts the set of solutions (that explains the empty cells in the table). At the second step−→u (1) are introduced, and u(1)1S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364357−→(2)u (2) are introduced, and uthe variables2unique leximin-optimal solution (7, 3, 5).is maximized. And at the last step u(3)3is introduced and maximized, leaving theu1157193510u2u3(1)u1(1)u2(1)u3(2)u2(2)u3(3)u315325433035123640331233315719351015525464553545765746104.8. HeuristicsAll the constraint programming algorithms introduced for dealing with the LeximinOptCSP can benefit from a specificheuristics than exploits the particular semantics of the leximin preorder so as to guide the search process more rapidlytowards good solutions. During the search for a leximin-optimal solution, the lowest element of the objective vector iscrucial: increasing it first immediately gives a better solution. It thus gives the idea of a general method to design efficientvariable and value choice heuristics dedicated to the leximin optimization: at each node of the search tree, the next variableto instantiate and the next value to assign to it should be chosen such that it increases as much as possible the value of thecurrent lowest objective variable.In most collective decision making or resource allocation problems, the objective vector, which is the utility profile,depends on a set of (0–1) decision variables. If it is the case, the next variable to instantiate should be a decision variablethat increases the most the utility of the least satisfied agent or criterion. Of course, these considerations only give a clueto build efficient heuristics for dealing with the LeximinOptCSP, and must be adapted to each kind of problem at stake.As we will see in next section, we applied this idea to design three particular heuristics dedicated to the three kinds ofinstances we used to test our algorithms. These heuristics have also been compared to two classical ones; the results willbe presented in Section 4.8.5. Applications and resultsWe implemented all the algorithms described in this article and we tested them on three different kinds of problems:(1) a simplified and linear model of a real-world application concerning the sharing of a constellation of Earth observationsatellites, (2) fair combinatorial auctions, and (3) a generic model for the allocation problem of indivisible goods, where theagents have complex preferences expressed in weighted propositional logic. The experimental settings are the following:the implementations have been developed in Java 1.6.0, using the constraint programming tool Choco [23]. The tests havebeen conducted on a 2.1 GHz bi-processor PC with 3.8 GB memory and running Gnu/Linux 2.6.21 for the tests concerningthe comparison of heuristics, and on a 1.6 GHz SUNW UltraSPARC-IIIi Sun station with 1 GB memory and runningSolaris 10 for all the other tests.5.1. Allocation of a constellation of Earth observation satellitesDescription This first application concerns the common exploitation of a constellation of agile Earth observation satellites,as described in [25]. From this application we have extracted a simplified multiagent resource allocation problem. In thisproblem, a set of objects O (standing for the resource) must be allocated to a set of agents N . So as to approximate the realphysical constraints (e.g. limited amount of on-board memory, limited agility of the satellite), we have introduced volumeconstraints over different subsets of variables, by attaching for each such constraint a volume to each object of the set, anda maximal admissible volume for this subset. There are also consumption constraints, that restrict the amount of objectsthat can be allocated to the same agent. The individual utility functions are specified by a set of weights w i,o, one per pair(agent, object): given an allocation of the objects, the individual utility of an agent i is the sum of the weights w i,o of theobjects o that she receives. The weights can be generated uniformly or such that they approximate the effect of prioritylevels.The customizable generator of random instances that we have developed for testing our algorithms is available online.8Experiments We conducted three sequences of experiments on this kind of problems, concerning respectively 4, 10 and 20agents and a variable number of objects. For each number of agents and number of objects we tested the algorithms on 208 http://www.cert.fr/dcsd/THESES/sbouveret/benchmark.358S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364Fig. 3. CPU times and number of instances solved within 10 minutes for each algorithm, run on instances of the problem Earth observation satellitessimplified.different instances, with a time limit of 10 minutes. When the number of agents is low (4 agents), the results of the tests(not shown in this article) do not bring to light any significant difference between the algorithms, probably because thiskind of problems is too near to the monoagent case to show any difference between the approaches. However, this is notthe case anymore when the number of agents increases. One can see for example in Fig. 3(a) that the algorithm based onthe exhaustive comparison of all the leximin-optimal solutions is much less efficient than the other ones. One may noticealso that Algorithm 5 based on the max-min transformations is not very efficient either. The best approaches on this kindof instances seems to be those based on the constraint Leximin (Algorithm 1) and on the saturated subsets (Algorithm 2).Finally, we can notice that the solving time of the latter approach increases when the number of objects tends to 0. This isnot very surprising: when the number of objects decreases it becomes impossible to satisfy all the agents, thus creating a lotof equal (zero) utilities in the leximin-optimal profile, leading to make the search for saturated subsets become much harder.Things become even clearer with Fig. 3(b) (20 agents), where the algorithm based on the saturated subsets is completelyinefficient on the instances with a few objects, whereas it has a reasonable solving time when the number of objects ishigher. Here the best algorithms seem to be those based on the constraint Leximin (Algorithm 1), on the meta-constraintAtLeast (Algorithm 4) and on the constraint Sort (Algorithm 3). One can finally notice that the running times of the two lastalgorithms are very close on all the instances, which could be explained by the fact they both compute the sorted vector ofobjective variables, and that the propagation algorithm behind the constraint Sort uses implicitly the alternative definitionof sorting used in Algorithm 4.Heuristics We implemented for the experiments a dedicated variable choice heuristics, based on the principles given inSection 4.8: we choose the next variable to instantiate so as that it gives to the currently least satisfied agent the objectshe rates the most. We compared this heuristics with two classical ones: mindomain (the next variable to instantiate isS. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364359Fig. 4. CPU times and number of instances solved within 10 minutes for four algorithms, with different heuristics, on instances of the problem Earthobservation satellites simplified, with 10 agents and p objects.the one having the smallest domain) and dom/deg (the next variable to instantiate is the one having the smallest rationdomain size/degree). The results are shown in Fig. 4. We can see in this figure that the leximin-specific heuristics is by farmore efficient than the two other heuristics used, on each one of the four algorithms tested (1, 2, 3 and 4). For some setsof instances (see e.g. for 75 objects, Algorithm 1 or 2), the decrease in CPU time can reach one order of magnitude.5.2. Fair combinatorial auctionsDescription Combinatorial auctions (see e.g. [9,40])—auctions in which bidders place unrestricted bids for bundles of goods—are subject of increasing study in the recent years. Their central problem is the Winner Determination Problem (WDP), whichhas been extensively studied. It definitely corresponds to a utilitarian point of view, namely maximizing the revenue of theauctioneer, which is the sum of the selected bids, whoever receives them. Even if fairness does not seem to be a relevantissue in combinatorial auctions, the WDP however inspired us a fair resource allocation problem with indivisible goods,where the agents express their preferences over bundles of items:Definition 10 (Fair CA instance). Given a set of agents N and a set of objects O, a bid b is a triple (s(b), p(b), a(b)) ∈O × N × N (a bundle of objects, a price and an agent). Given a set of non-intersecting bids W and an agent i, the utility2of i regarding W is ui(W) ={p(b) | b ∈ W and a(b) = i}. A fair combinatorial auctions instance is defined as follows:Input: A set of n agents N , a set of objects O and a set of bids B.Output: A set of non-intersecting bids W ⊆ B such that there is no set of non-intersecting bids W (cid:20) ⊆ B with(cid:2)(u1(W), . . . , un(W)) ≺leximin (u1(W (cid:20)), . . . , un(W (cid:20))).360S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364Fig. 5. CPU times and number of instances solved within 10 minutes for each algorithm, run on instances of the problem fair combinatorial auctions withn agents and 100 objects.Experiments We have tested our different approaches for computing the leximin-optimal solution of a constraint networkon an implementation of this problem. The test instances have been generated using CATS [26], which aims at makingrealistic and economically motivated bids for combinatorial auctions, e.g. by simulating some kind of relations such assubstitutabilities and complementarities between the goods. The results shown in Fig. 5 concern a variable number ofagents, 100 objects and a number of bids approximately equal to 10 times the number of agents. For each number of agentswe tested the algorithms on 20 different instances of kind “arbitrary”, with a time limit of 10 minutes. We observe in thefigure that the least efficient algorithm in this case is, like previously, the one based on the exhaustive comparison of allsolutions. It is followed by the approach based on the saturated subsets, which is completely inefficient when the numberof agents increases (that is, when the ratio objects/agents decreases), for the same reasons as before. The best algorithmsare once again those based on the meta-constraint AtLeast and on the constraint Sort. However, one can notice that thealgorithm based on the constraint Leximin is less efficient on this kind of instances than on the instances of the previousproblem. We also compared the solving times of the algorithms with the time required to compute the sum-optimal solution(corresponding to a solution of the classical Winner Determination Problem) using constraint programming. One can see thatthere is no huge difference between the CPU time required to compute a sum-optimal solution and the CPU time requiredto compute a leximin-optimal solution.9Heuristics As previously, we implemented for the experiments a dedicated variable choice heuristics, based on the prin-ciples given in Section 4.8: the next bid to allocate is the one with the higher price, among those of the currently leastsatisfied agent. All the results shown in this subsection use this specific heuristics.5.3. Resource allocation problem with logical preferencesDescription The last kind of problems we used to test our algorithms concerns the allocation of a set of indivisible goodsto a set of agents. The agents may have complex preferences over the set of objects—that is, preferences that involvecomplementarity or substitutability relationships between the objects. Moreover, a set of admissibility constraints restrictsthe set of admissible allocations. A formal model for representing this kind of problems has been introduced in [6]. In thismodel, an instance of the resource allocation problem is defined as follows:9 At least if we use constraint programming to solve the WDP. Of course, the ad hoc solvers dedicated to this problem are far more efficient.S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364361Fig. 6. CPU times and number of instances solved within 10 minutes for each algorithm, run on instances of the resource allocation problem with logicalpreferences with n agents and 20 objects.Definition 11 (Combinatorial resource allocation problem).Input: • a finite set of agents N = {1, . . . , n},• a finite set of objects O,• a set of admissibility constraints C, made of propositional formulae from LallocO , the propositional language overthe variables alloc(o, i) (o ∈ O and i ∈ N ), meaning that the object o is allocated to the agent i,• a set (cid:7)i of weighted formulae per agent i, made of a weight w and a propositional formula from LO , thepropositional language over the variables o ∈ O. Given a set of objects πi allocated to the agent i, one can definethe individual utility ui(πi) of i by the sum of the weights of the formulae from (cid:7)i satisfied by πi .Output: An allocation−→π , with πi ⊆ O for all i, such that:• all the admissibility constraints are satisfied,• there is no other allocation1), . . . , un(π (cid:20)(u1(π (cid:20)n)).−→π (cid:20)also satisfying the admissibility constraints such that (u1(π1), . . . , un(πn)) ≺leximinExperiments We implemented one customizable random generator, the description of which is not detailed here, dedicatedto this model of resource allocation problem. The implementation of the model and the generator, as well as their completedescription, are available online.10We tested the algorithms on a set of instances created by this customizable generator. The results are shown in Fig. 6.The time limit is still 10 minutes, but the number of tested instances is now 10 for each number of agents and number ofobjects. These instances imply a great number of logical constraints, which explains why the algorithms are quite inefficientto solve them. One can see however that the best approaches are still those based on the meta-constraint AtLeast and onthe constraint Sort, which is not the case for the algorithm based on the constraint Leximin.HeuristicsIt is harder to design an efficient variable choice heuristics for this particular problem, since the attributionof an object to an agent is not “direct”, due to the complex semantics of propositional logic. The heuristics we used inour experiments is the following: choose as the next object to allocate the one that appears in the formula with thehighest weight, among those of the currently least satisfied agent. Although the difference between this heuristics and moreclassical ones is less blatant than it was for the two previous problems, it still gives better results. It would be interestingto investigate the potential gain of designing more complex heuristics for this particular application.10 http://www.cert.fr/dcsd/THESES/sbouveret/benchmark2007.362S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–3645.4. Conclusion and discussionThe aim of making the experiments was to find out which algorithms were the most interesting to use in the constraintprogramming framework. The experimental results clearly show that the most efficient algorithms in average are Algo-rithm 4 (based on the AtLeast meta-constraint), Algorithm 3 (based on the Sort constraint), and Algorithm 1 (leximin-basedbranch-and-bound), although the last one is slightly less efficient than the other ones on instances of fair combinatorialauctions and resource allocation with logical preferences, but slightly more efficient on instances of the problem Earthobservation satellites simplified.Unsurprisingly, the approach based on the exhaustive comparison of all solutions should be avoided (except on verysmall instances, where it is not worth using complex constraint propagation mechanisms).The two Algorithms 4 (based on the AtLeast meta-constraint) and 3 (based on the Sort constraint) give very similarresults on all kinds of instances. On the one hand it is not very surprising, because as we noticed in Section 4.6, thesetwo algorithms are based on the same principle of sorting the objective variables, and the constraint Sort is indirectlybased on the same trick as the one used in Algorithm 4. On the other hand, one could have expected that introducing thesorted variables one at a time in Algorithm 4 instead of introducing them at one time in Algorithm 3 would have madea difference. The experiments show that this is not the case. Therefore, one can use these two algorithms indifferently.The choice between the two can thus be driven by the requested implementation effort: Algorithm 3 (based on the Sortconstraint) is very easy to implement if the constraint Sort is provided by the CP system used; Algorithm 4 (based on theAtLeast meta-constraint) is rather easy to implement if the CP system provides some cardinality meta-constraints, or if wechoose to encode these constraints using a set of 0–1 variables and linear constraints.Algorithm 1 (leximin-based branch-and-bound) seems to be quite efficient on instances of the problem Earth observationsatellites simplified, but a little bit less in other instances, for some unknown reasons. In any case, it can be worth to trythis algorithm, as it can give better results than the two latter ones. The fact that this Algorithm 1 gives better results insome cases gives the idea (as suggested by one anonymous reviewer of our previous article [7]) to mix the approach ofAlgorithm 4 (based on the AtLeast meta-constraint) with the approach of Algorithm 1, by using the constraint Leximin withthe constraint AtLeast to provide more filtering. We could have expected that this approach would have been as efficient asthe best of the two algorithms on each instance. However, the results of the experiments (not given here) show that mixingthe two approaches is less efficient than using one of them alone. Our intuition is that the gain of using double filtering(AtLeast and Leximin) is not worth the cost of running the propagation algorithms at each node of the search tree.Concerning the algorithm based on the saturated subsets (Algorithm 2), it seems to be reasonable, and can even be quiteefficient on instances with only a few equal components in the leximin-optimal profile. However, as expected, it explodeswhen the number of saturated subsets increases, which is for example the case when the number of agents and the numberof objects are similar in instances of the problem Earth observation satellites simplified (see e.g. Fig. 3(b), for small numbersof objects). Thus, this algorithm should be used only if we are sure that there is little chance to have equal componentsin the leximin-optimal objective vector. One can notice that it is not very surprising that this algorithm comes from thefuzzy CSP community: in this context, where the constraint satisfaction levels are continuous, it is quite unlikely that twoconstraints have the same satisfaction level.Lastly, Algorithm 5 (using max-min transformations) appears to be quite inefficient. The probable reason is that thenumber of additional variables and constraints it introduces, only for sorting purposes, is too expensive to be efficient, evenif the constraint propagation algorithms associated to the max-min constraints are rather simple.To conclude on the relative performance of the algorithms introduced, it is not very surprising that in the CP frame-work, the most efficient algorithms are the ones that make the full use of constraint propagation algorithms (using globalconstraints). The two different approaches – AtLeast and Sort on the one hand, Leximin on the other hand – give slightdifferences in terms of performance, but it is still unclear which one it is better to use for a given particular instance.Finally, one can see in Fig. 4 that the specific heuristics used, based on the considerations described in Section 4.8, givesquite good results on instances of the problem Earth observation satellites simplified, compared to the two other classicalheuristics used (it is also efficient on the other kinds of problems, although the results are not shown here). In some cases,the gap in CPU-time implied by the use of this heuristics can be worth one order of magnitude. This is a quite interestingresult: the experiments show that using this sort of heuristics (which is often easy to implement for a specific kind ofproblem) leads to a big improvement of the running time, compared to classical heuristics.6. Conclusion and future workFairness is at the base of many real-world applications implying human agents, or seeking for a compromise betweenconflicting interests. We borrowed from the microeconomics field the idea that the leximin preorder is well-suited to addresssuch fairness requirements as well as reconcile them with the crucial notion of Pareto-optimality. More generally, thispreorder is adapted to all kind of multicriteria optimization problems where one has to find good compromises between aset of criteria or objective functions while ensuring Pareto-optimality.Finding a leximin-optimal solution is not a trivial algorithmic problem. In this article we focused on the search for suchsolutions in a constraint network. We proposed a set of constraint programming algorithms, either adapted from otherfields (such as Operational Research) or new, to address this problem. The reasons we invoked to justify the developmentS. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364363of algorithms dedicated to this particular framework is that it provides effective, flexible and efficient tools for modelingand solving a wide range of combinatorial problems. Our approaches can be easily integrated in existing state-of-the-art CPsystems and solvers, and thus heavily depend on the performance of the algorithms provided by these solvers.We tested these algorithms on three different kinds of randomly generated problems: the first one is a linear probleminspired by a real-world application concerning the sharing of a constellation of Earth observing satellites, the second oneis an adaptation of the combinatorial auctions framework to the leximin preorder, and the last one is a generic resourceallocation problem concerning indivisible goods with logical constraints and preferences. Our experiments show that thebest approaches are those based on the meta-constraint AtLeast, on the constraint Sort, followed by the algorithm basedon the constraint Leximin.This article is a contribution to a problem having a wide range of applications. It raises a lot of interesting questions andproblems.First of all, some CP approaches remain to be explored. For example, as one anonymous reviewer suggested, one couldthink of using the global cardinality constraint (see the work from [18,38] that describes fast bound consistency algorithmsfor the global cardinality constraint) to introduce and compute the occurrence vector corresponding to the objective vector.This approach is cited in [20], and in [15] where it is used for decomposing the Multiset Ordering constraint. Havingthis constraint, computing a leximin-optimal solution then comes down to compute a solution whose occurrence vector islexicographically optimal (which can be easily done with a multi-step algorithm). It would be interesting to compare thisapproach with the ones we tested in this article, although the explicit introduction of the occurrence vector is probablyvery expensive in instances where the domains of the objective variables are large (which is the case in the experiments weperformed). One may notice that this limitation also applies to the propagation algorithm introduced in the works from [15,20] for the Multiset Ordering constraint. However the two latter works propose an adaptation of this algorithm to overcomethis limitation. Our propagation algorithm for the constraint Leximin is actually based on this adaptation, which allows usto handle large domains.A natural extension of our work concerns the development of incomplete algorithms dedicated to the computationof leximin-optimal solutions. One could think for example of adapting local search techniques to this particular problem.However it raises some particular and interesting difficulties, such as how to evaluate the quality of a solution, that is, theexpected “distance” from a solution to a leximin-optimal one.Another natural extension of our work is about giving up leximin-optimality (which is sometimes considered as anextreme way to aggregate individual utilities) and focusing on a “softer” modeling of fairness or compromises between theobjective variables. An interesting direction is the use of Ordered Weighted Averages [46] to model fairness. It appears thatmost of the algorithms we introduced could be adapted to compute an OWA-optimal solution.AcknowledgementsAuthors would like to thank the anonymous reviewers for their comments and suggestions. This work has been partlysupported by the project ANR-05-BLAN-0384 “Preference Handling and Aggregation in Combinatorial Domains”, funded byAgence Nationale de la Recherche.References[1] M. Ball, G.L. Donohue, K. Hoffman, Auctions for the safe, efficient and equitable allocation of airspace system resources, in: P. Cramton, Y. Shoham,R. Steinberg (Eds.), Combinatorial Auctions, MIT Press, 2006, pp. 507–538 (Chapter 22).[2] N. Beldiceanu, M. Carlsson, Revisiting the cardinality operator and introducing cardinality-path constraint family, in: Proceedings of the 17th Interna-tional Conference on Logic Programming (ICLP-01), Paphos, Cyprus, 2001.[3] C. Bessière, Constraint propagation, in: F. Rossi, P. van Beck, T. Walsh (Eds.), Handbook of Constraint Programming, Foundations of Artificial Intelligence,Elsevier, 2006, pp. 29–83 (Chapter 3).[4] C. Bessière, E. Hebrard, B. Hnich, Z. Kiziltan, T. Walsh, SLIDE: A useful special case of the CARDPATH Constraint, in: Proceedings of the 18th EuropeanConference on Artificial Intelligence (ECAI-08), Patras, Greece, 2008.[5] N. Bleuzen-Guernalec, A. Colmerauer, Narrowing a block of sortings in quadratic time, in: G. Smolka (Ed.), Proceedings of the 3rd International Confer-ence on Principles and Practice of Constraint Programming (CP-97), Schloss Hagenberg, Austria, Springer, 1997.[6] S. Bouveret, H. Fargier, J. Lang, M. Lemaître, Allocation of indivisible goods: a general model and some complexity results, in: F. Dignum, V. Dignum,S. Koenig, S. Kraus, M.P. Singh, M. Wooldridge (Eds.), Proceedings of the 4th International Joint Conference on Autonomous Agents and MultiagentSystems (AAMAS-05), Utrecht, The Netherlands, ACM, 2005.[7] S. Bouveret, M. Lemaître, New constraint programming approaches for the computation of leximin-optimal solutions in constraint networks, in: M.M.Veloso (Ed.), Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), Hyderabad, India, AAAI Press, 2007.[8] T.H. Cormen, C.E. Leiserson, R.L. Rivest, C. Stein, Introduction to Algorithms, second ed., MIT Press, 2001.[9] P. Cramton, Y. Shoham, R. Steinberg (Eds.), Combinatorial Auctions, MIT Press, 2006.[10] C. d’Aspremont, L. Gevers, Equity and the informational basis of collective choice, Review of Economic Studies 44 (2) (1977) 199–209.[11] R. Dechter, Constraint Processing, Morgan Kaufmann, 2003.[12] D. Dubois, P. Fortemps, Computing improved optimal solutions to max-min flexible constraint satisfaction problems, European Journal of OperationalResearch 118 (1999) 95–126.[13] M. Ehrgott, Multicriteria Optimization, Lecture Notes in Economics and Mathematical Systems, vol. 491, Springer, 2000.[14] H. Fargier, J. Lang, T. Schiex, Selecting preferred solutions in fuzzy constraint satisfaction problems, in: Proceedings of the First European Congress onFuzzy Intelligent Technologies (EUFIT’93), Aachen, 1993.[15] A.M. Frisch, B. Hnich, Z. Kiziltan, I. Miguel, T. Walsh, Filtering algorithms for the multiset ordering constraint, Artificial Intelligence 173 (2) (2009)299–328, this issue.364S. Bouveret, M. Lemaître / Artificial Intelligence 173 (2009) 343–364[16] T. Frühwirth, L. Michel, C. Schulte, Constraints in procedural and concurrent languages, in: F. Rossi, P. van Beck, T. Walsh (Eds.), Handbook of ConstraintProgramming, Foundations of Artificial Intelligence, Elsevier, 2006, pp. 453–494 (Chapter 13).[17] R.S. Garfinkel, G.L. Nemhauser, Integer Programming, Wiley, 1972.[18] I. Katriel, S. Thiel, Fast bound consistency for the global cardinality constraint, in: F. Rossi (Ed.), Proceedings of the 9th International Conference onPrinciples and Practice of Constraint Programming (CP-03), Kinsale, County Cork, Ireland, Springer, 2003.[19] R.L. Keeney, H. Raiffa, Decisions with Multiple Objectives: Preferences and Value Tradeoffs, John Wiley and Sons, 1976.[20] Z. Kiziltan, Symmetry breaking ordering constraints, Ph.D. thesis, Uppsala University, 2004.[21] D.E. Knuth, The Art of Computer Programming, vol. 1, Fundamental Algorithms, Addison-Wesley, 1968.[22] S.-C. Kolm, Justice et Équité, Cepremap, CNRS Paris, 1972, English translation: Justice and Equity, MIT Press, 1998.[23] F. Laburthe, CHOCO: Implementing a CP kernel, in: Proceedings of TRICS’2000, Workshop on Techniques for Implementing CP Systems, Singapore,2000, http://sourceforge.net/projects/choco.[24] M. Lemaître, G. Verfaillie, N. Bataille, Exploiting a common property resource under a fairness constraint: a case study, in: T. Dean (Ed.), Proceedingsof the 16th International Joint Conference on Artificial Intelligence (IJCAI-99), Stockholm, Sweden, Morgan Kaufmann, 1999.[25] M. Lemaître, G. Verfaillie, F. Jouhaud, J.-M. Lachiver, N. Bataille, Selecting and scheduling observations of agile satellites, Aerospace Science and Tech-nology 6 (2002) 367–381.[26] K. Leyton-Brown, M. Pearson, Y. Shoham, Towards a universal test suite for combinatorial auction algorithms, in: Proceedings of the 2nd ACM Confer-ence on Electronic Commerce (EC-00), Minneapolis, MN, ACM, 2000.[27] H. Luss, On equitable resource allocation problems: a lexicographic minimax approach, Operations Research 47 (3) (1999) 361–378.[28] J.-L. Marichal, Aggregation operators for multicriteria decision aid, Ph.D. thesis, Faculté des Sciences de l’Université de Liège, 1999.[29] M. Maschler, J.A.M. Potters, S.H. Tijs, The general nucleolus and the reduced game property, International Journal of Game Theory 21 (1992) 85–106.[30] K. Mehlhorn, S. Thiel, Faster algorithms for bound-consistency of the sortedness and the alldifferent constraint, in: R. Dechter (Ed.), Proceedings of the6th International Conference on Principles and Practice of Constraint Programming (CP-00), Singapore, Springer, 2000.[31] U. Montanari, Network of constraints: Fundamental properties and applications to picture processing, Inf. Sci. 7 (1974) 95–132.[32] H. Moulin, Axioms of Cooperative Decision Making, Cambridge University Press, 1988.[33] H. Moulin, Fair Division and Collective Welfare, MIT Press, 2003.[34] W. Ogryczak, On the lexicographic minimax approach to location problems, European Journal of Operational Research 100 (1997) 566–585.[35] W. Ogryczak, T.´Sliwi ´nski, On solving linear programs with the ordered weighted averaging objective, European Journal of Operational Research 148(2003) 80–91.[36] G. Pesant, J.-C. Régin, sPREAd: A balancing constraint based on statistics, in: P. van Beek (Ed.), Proceedings of the 11th International Conference onPrinciples and Practice of Constraint Programming (CP-04), Sitges, Spain, Springer, 2005.[37] J.A.M. Potters, S.H. Tijs, The nucleolus of a matrix game and other nucleoli, Mathematics of Operations Research 17 (1992) 164–174.[38] C.-G. Quimper, A. Golynski, A. López-Ortiz, P. van Beek, An efficient bounds consistency algorithm for the global cardinality constraint, Constraints 10 (2)(2005) 115–135.[39] F. Rossi, P. van Beck, T. Walsh (Eds.), Handbook of Constraint Programming, Foundations of Artificial Intelligence, Elsevier, 2006.[40] T.W. Sandholm, Algorithm for optimal winner determination in combinatorial auctions, Artificial Intelligence 134 (2002) 1–54.[41] A.K. Sen, Collective Choice and Social Welfare, North-Holland, 1970.[42] P. van Beek, Backtracking search algorithms, in: F. Rossi, P. van Beck, T. Walsh (Eds.), Handbook of Constraint Programming, Foundations of ArtificialIntelligence, Elsevier, 2006, pp. 85–134 (Chapter 4).[43] P. Van Hentenryck, The OPL Optimization Programming Language, The MIT Press, 1999.[44] P. Van Hentenryck, H. Simonis, M. Dincbas, Constraint satisfaction using constraint logic programming, Artificial Intelligence 58 (1–3) (1992) 113–159.[45] W.-J. van Hoeve, I. Katriel, Global Constraints, in: F. Rossi, P. van Beck, T. Walsh (Eds.), Handbook of Constraint Programming, Foundations of ArtificialIntelligence, Elsevier, 2006, pp. 169–208 (Chapter 6).[46] R.R. Yager, On ordered weighted averaging aggregation operators in multicriteria decision making, IEEE Transactions on Systems, Man, and Cybernet-ics 18 (1988) 183–190.[47] R.R. Yager, On the analytic representation of the leximin ordering and its application to flexible constraint propagation, European Journal of OperationalResearch 102 (1) (1997) 176–192.[48] H.P. Young, Equity in Theory and Practice, Princeton University Press, 1994.[49] J. Zhou, A permutation-based approach for solving the job-shop problem, Constraints 2 (2) (1997) 185–213.