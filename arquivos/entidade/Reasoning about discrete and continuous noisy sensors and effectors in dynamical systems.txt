Artificial Intelligence 262 (2018) 189–221Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintReasoning about discrete and continuous noisy sensors and effectors in dynamical systemsVaishak Belle a,b,∗a University of Edinburgh, Edinburgh, United Kingdomb The Alan Turing Institute, London, United Kingdomc University of Toronto, Toronto, Canada, Hector J. Levesque ca r t i c l e i n f oa b s t r a c tAmong the many approaches for reasoning about degrees of belief in the presence of noisy sensing and acting, the logical account proposed by Bacchus, Halpern, and Levesque is perhaps the most expressive. While their formalism is quite general, it is restricted to fluents whose values are drawn from discrete finite domains, as opposed to the continuous domains seen in many robotic applications. In this work, we show how this limitation in that approach can be lifted. By dealing seamlessly with both discrete distributions and continuous densities within a rich theory of action, we provide a very general logical specification of how belief should change after acting and sensing in complex noisy domains.Crown Copyright © 2018 Published by Elsevier B.V. All rights reserved.Article history:Received 27 October 2016Received in revised form 26 March 2018Accepted 5 June 2018Available online 26 June 2018Keywords:Knowledge representationReasoning about actionReasoning about knowledgeReasoning about uncertaintyProbabilistic logical modelsCognitive robotics1. IntroductionOn numerous occasions it has been suggested that the formalism [the situation calculus] take uncertainty into account by attaching probabilities to its sentences. We agree that the formalism will eventually have to allow statements about the probabilities of events, but attaching probabilities to all statements has the following objections:1. It is not clear how to attach probabilities to statements containing quantifiers in a way that corresponds to the amount of conviction people have.2. The information necessary to assign numerical probabilities is not ordinarily available. Therefore, a formalism that required numerical probabilities would be epistemologically inadequate.− McCarthy and Hayes [1].Much of high-level AI research is concerned with the behavior of some putative agent, such as an autonomous robot, operating in an environment. Broadly speaking, an intelligent agent interacting with a dynamic and incompletely known world grapples with two special sorts of reasoning problems. First, because the world is dynamic, it will need to reason about change: how its actions affect the state of the world. Pushing an object on a table, for example, may cause it to fall on the floor, where it will remain unless picked up. Second, because the world is incompletely known, the agent will need to make do with partial specifications about what is true. As a result, the agent will often need to augment what it believes about the world by performing perceptual actions, using sensors of one form or another.* Corresponding author at: University of Edinburgh, Edinburgh, United Kingdom.E-mail addresses: vaishak@ed.ac.uk (V. Belle), hector@cs.toronto.edu (H.J. Levesque).https://doi.org/10.1016/j.artint.2018.06.0030004-3702/Crown Copyright © 2018 Published by Elsevier B.V. All rights reserved.190V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221Fig. 1. A simple robot.For many AI applications, and robotics in particular, these reasoning problems are more involved. Here, it is not enough to deal with incomplete knowledge, where some formula φ might be unknown. One must also know which of φ or ¬φ is the more likely, and by how much. In addition, both the sensors and the effectors that the agent uses to modify its world are often subject to uncertainty in that they are noisy.To see a very simple example, imagine a robot moving towards a wall as shown in Fig. 1, and a certain distance h from it. Suppose the robot can move towards and away from the wall, and it is equipped with a distance sensor aimed at the wall. Here, the robot may not know the true value of h but may believe that it takes values from some set, say {2, . . . , 11}. If the sensor is noisy, a reading of, say, 5 units, does not guarantee that the agent is actually 5 units from the wall, although it should serve to increase the agent’s degree of belief in that fact. Analogously, if the robot intends to move by 1 unit and the effector is noisy, it may end up moving by 0.9 units, which the agent does not get to observe. Be that as it may, the robot’s degree of belief that it is closer to the wall should increase.While many proposals have appeared in the literature to address such concerns (cf. penultimate section), very few are embedded in a general theory of action whilst supporting features like disjunction and quantification. For example, graph-ical models such as Bayesian networks can represent and reason about the probabilistic dependencies between random variables, and how that might change over time. However, it lacks first-order features and a rich account of actions. Re-lational graphical models, including Markov logic networks [2], borrow devices from first-order logic to allow the succinct modeling of relational dependencies, but ultimately they are purely syntactic extensions to graphical models, and do not attempt to address the deeper issues pertaining to the specification of probabilities in the presence of logical connectives and quantifiers. Building on first-order accounts of probabilistic reasoning [3,4], perhaps the most general formalism for dealing with degrees of beliefin formulas, and in particular, with how degrees of belief should evolve in the presence of noisy sensing and acting is the account proposed by Bacchus, Halpern, and Levesque [5], henceforth BHL. Among its many properties, the BHL model shows precisely how beliefs can be made less certain by acting with noisy effectors, but made more certain by sensing (even when the sensors themselves are noisy).The main advantage of a logical account like BHL is that it allows a specification of belief that can be partial or incom-plete, in keeping with whatever information is available about the application domain. It does not require specifying a prior distribution over some random variables from which posterior distributions are then calculated, as in Kalman filters, for example [6]. Nor does it require specifying the conditional independences among random variables and how these depen-dencies change as the result of actions, as in the temporal extensions to Bayesian networks [7]. In the BHL model, some logical constraints are imposed on the initial state of belief. These constraints may be compatible with one or very many initial distributions and sets of independence assumptions. All the properties of belief will then follow at a corresponding level of specificity.Subjective uncertainty is captured in the BHL account using a possible-world model of belief [8–10]. In classical possible-world semantics, a formula φ is believed to be true when φ holds in all possible worlds that are deemed accessible. In BHL, the degree of belief in φ is defined as a normalized sum over the possible worlds where φ is true of some nonnegative weights associated with those worlds. (Inaccessible worlds are assigned a weight of zero.) To reason about belief change, the BHL model is then embedded in a rich theory of action and sensing provided by the situation calculus [1,11,12]. The BHL account provides axioms in the situation calculus regarding how the weight associated with a possible world changes as the result of acting and sensing. The properties of belief and belief change then emerge as a direct logical consequence of the initial constraints and these changes in weights.For example, suppose h is a fluent representing the robot’s horizontal distance to the wall in Fig. 1. The fluent h would have different values in different possible worlds. In a BHL specification, each of these worlds might be given an initial weight. For example, a uniform distribution might give an equal weight of .1 to ten possible worlds where h ∈ {2, 3, . . . , 11}. The degree of belief in a formula like (h < 9) is then defined as a sum of the weights, and would lead here to a value of .7. The theory of action would then specify how these weights change as the result of acting (such as moving away or towards the wall) and sensing (such as obtaining a reading from a sonar aimed at the wall). Naturally, the logical language permits weaker specifications, involving disjunctions and quantifiers, and the appropriate behavior would still emerge.While this model of belief is widely applicable, it does have one serious drawback: it is ultimately based on the addition of weights and is therefore restricted to fluents having discrete finite values. This is in stark contrast to robotics and ma-chine learning applications [13–15], where event and observation variables are characterized by continuous distributions, or perhaps combinations of discrete and continuous ones. There is no way to say in BHL that the initial value of h is any real number drawn from a uniform distribution on the interval [2, 12]. One would again expect the belief in (h < 9) to be .7, V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221191but instead of being the result of summing weights, it must now be the result of integrating densities over a suitable space of values, something quite beyond the BHL approach.So, on the one hand, the BHL account and others like it can be seen as general formal theories that attempt to address important philosophical problems such as those raised by McCarthy and Hayes above. But on the other, a serious criticism leveled at this line of work, and indeed at much of the work in reasoning about action, is that the theory is far removed from the kind of probabilistic uncertainty and noise seen in typical robotic applications.The goal of this work is to show how with minimal additional assumptions this serious limitation of BHL can be lifted.1By lifting this limitation, one obtains, for the first time, a logical language for representing real-world robotic specifications without any modifications, but also extend beyond it by means of the logical features of the underlying framework. In partic-ular, we present a formal specification of the degrees of belief in formulas with real-valued fluents (and other fluents too), and how belief changes as the result of acting and sensing. Our account will retain the advantages of BHL but work seamlessly with discrete probability distributions, probability densities, and perhaps most significantly, with difficult combinations of the two. More broadly, we believe the model of belief proposed in this work provides the necessary bridge between logic-based reasoning modules, on the one hand, and probabilistic specifications as seen in real-world data-intensive applications, on the other.The paper is organized as follows. We first review the formal preliminaries, the BHL model in particular, and introduce definitions for modeling continuous probability distributions. We then show how the definition of belief in BHL can be reformulated as a different summation, which then provides sufficient foundation for our extension to continuous domains. We then discuss how this model can be extended for noisy acting, and combinations of discrete and continuous properties. We conclude after discussing related work.2. A theory of actionOur account is formulated in the language of the situation calculus [1], as developed in [11]. The situation calculus is a special-purpose knowledge representation formalism for reasoning about dynamical systems. Informally, the formalism is best understood by arranging the world in terms of three kinds of things: situations, actions and objects. Situations represent “snapshots,” and can be viewed as possible histories. A set of initial situations correspond to the ways the world can be prior to the occurrence of actions. The result of doing an action, then, leads to a successor (non-initial) situation. Naturally, dynamic worlds change the properties of objects, which are captured using predicates and functions whose last argument is always a situation, called fluents.2.1. The logical languageFormally, the language L of the situation calculus is a many-sorted dialect of predicate calculus, with sorts for actions, situations and objects (for everything else). (We do not review standard predicate logic here; see, for example, [17,18]. We further assume familiarity with the notions of models, structures, satisfaction and entailment.) In full length, let L include:• logical connectives ¬, ∀, ∧, =, with other connectives such as ⊃ understood for their usual abbreviations;• an infinite supply of variables of each sort;• an infinite supply of constant symbols of the sort object;• for each k ≥ 1, object function symbols g1, g2, . . . of type (action ∪ object)k → object;• for each k ≥ 0, action function symbols A1, A2, . . . of type (action ∪ object)k → action;• a special situation function symbol do: action × situation → situation;• a special predicate symbol Poss: action × situation2;• for each k ≥ 0, fluent function symbols f 1, f 2, . . . of type (action ∪ object)k × situation → object;• a special constant S0 to represent the actual initial situation.To reiterate, apart from some syntactic particulars, the logical basis for the situation calculus is the regular (many-sorted) predicate calculus.3 So, terms and well-formed formulas are defined inductively, as usual, respecting sorts. See, for example, [11] for an exposition.We follow some conventions in the ways we use Latin and Greek alphabets: a for both terms and variables of the action sort (the context would make this clear); s for terms and variables of the situation sort (the context would make this clear); and finally, x, u, v, z, n, and y to range over variables of the object sort. We let φ and ψ range over formulas, and (cid:4) over sets of formulas. (These may be further decorated using superscripts or subscripts.)1 A preliminary version of this work was discussed in [16]. That work was limited to noisy sensors, but assumed deterministic (i.e., noise-free) effectors.2 We will subsequently introduce a few more distinguished predicates when modeling knowledge, sensing and nondeterminism.3 For simplicity, only functional fluents are introduced, and their predicate counterparts are ignored. (Distinguished symbols like Poss are an exception.) This is without loss of any generality since predicates can be thought of functions that take one of two values, the first denoting true and the other denoting false.192V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221We sometimes suppress the situation term in a formula φ, or use a distinguished variable now, to denote the current situation. Either way, we let φ[t] denote the formula with the restored situation term t.We will often use the usual “case” notation with curly braces as a convenient abbreviation for a logical formula:(cid:2)z =if ψt1t2 otherwise.= (ψ ⊃ z = t1) ∧ (¬ψ ⊃ z = t2).Finally, for convenience, we often introduce formula and term abbreviations that are meant to expand as L-formulas. For example, we might introduce a new formula A by A .= φ, where φ ∈ L. Then any expression E( A) containing A is assumed to mean E(φ). Analogously, if we introduce a new term t by t = u .= φ(u) then any expression E(t) is assumed to mean ∃u(E(u) ∧ φ(u)).Dynamic worlds are enabled by performing actions, and in the language, this is realized using the do operator. That is, the result of doing an action a at situation s is the situation do(a, s). Functional fluents, which take situations as arguments, may then have different values at different situations, thereby capturing changing properties of the world. As noted, the constant S0 is assumed to give the actual initial state of the domain, but the agent may consider others possible that capture the beliefs and ignorance of the agent. In general, we say a situation is an initial one when it is a situation without a predecessor:Init(s).= ¬∃a, s(cid:11). s = do(a, s(cid:11)).The picture that emerges is that situations can be structured as a set of trees, each rooted at an initial situation and whose edges are actions. More conventions: we use ι to range over such initial situations only, and let α denote sequences of action terms or variables, and freely use this with do, that is, if α = [a1, . . . , an] then do(α, s) stands for do(an, do(. . . , do(a1, s) . . .)).Domains are modeled in the situation calculus as axioms. A set of L-sentences specify the actions available, what they depend on, and the ways they affect the world. Specifically, these axioms are given in the form of a basic action theory [11], reviewed below.2.2. Basic action theoriesIn general, a basic action theory D is a set of sentences consisting of (free variables understood as universally quantified from the outside):• an initial theory D0 that describes what is true initially;• precondition axioms, of the form Poss( A((cid:12)x, s)) ≡ β((cid:12)x, s) that describe the conditions under which actions are executable;• successor state axioms, of the form f ((cid:12)x, do(a, s)) = u ≡ γ f (u, (cid:12)x, s), that describe the changes to fluent values after doing actions;• domain-agnostic foundational axioms, such as a second-order induction axiom for the space of situations and unique name axioms for actions, the details of which need not concern us here [11].The formulation of successor state axioms, in particular, incorporates Reiter’s monotonic solution to the frame problem [19].An agent reasons about actions by means of entailments of a basic action theory D, for which standard first-order (Tarskian) models suffice (although see below). A fundamental task in reasoning about action is that of projection [11], where we test which properties hold after actions. Formally, suppose φ is a situation-suppressed formula or uses the special symbol now. Given a sequence of actions a1 through an, we are often interested in asking whether φ holds after these:D |= φ[do([a1, . . . , an], S0)]?This concludes our review of the basic features of the language. In the subsequent sections, we will discuss how the formalism is first extended for knowledge, and then, degrees of belief against discrete probability distributions and beyond. To prepare for that, the rest of the section will introduce three logical constructions that will be used in our work. First, we will define a class of Tarskian structures. Second, we define a logical term standing for summation in the usual mathematical sense, that is to be understood as an abbreviation for a formula involving second-order quantification. Third, we analogously define a logical term standing for integration in the usual mathematical sense.2.3. R-interpretationsFor our purposes, the notion of entailment will be assumed wrt a class of Tarskian structures that we call R-interpretations. See [17] for a review of Tarskian structures; we assume some familiarity with the underlying notions. Below, for any L-term t and L-interpretation M, we use t M to mean the domain element that t references. If t has a free y to mean the L-term obtained by replacing x in t with y. Finally, for any variable x and y is any L-term, then we write txvariable map μ and variable X , we use μ XY to mean a variable map that is exactly like μ except that for the variable X it takes the value Y .V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221193Definition 1. By an R-interpretation we mean any L-structure where =, <, >, 0, 1, +, ×, /, −, e, π , exponentiation and log-arithms have their usual interpretations.(That is, “1 + 0 = 1” is true in all R-interpretations, if “x > y” is true then “¬( y > x)” is true, and so on.) So, henceforth, when we write (cid:4) |= φ, we mean that in all R-interpretations where (cid:4) is true, so is φ.4Natural numbers can be defined in terms of a predicate by appealing to R-interpretations. LetNatural(x).= ∀P [(P (0) ∧ ∀x(P (x) ⊃ P (x + 1))) ⊃ P (x)].Then:Theorem 2. Let M be any R-interpretation, and c a constant symbol of L. Then, M |= Natural(c) iff c M ∈ N.The proof relies on two lemmas. First, we argue that the set of natural numbers satisfies the antecedent A .= P (0) ∧∀x(P (x) ⊃ P (x + 1)):Lemma 3. For every R-interpretation M and map μ, M, μPN |= A.Proof. Since “0” and “+” have their usual interpretations, and 0 ∈ N, M, μP|= P (x + 1). (cid:2)Since N includes the successors of all its elements, M, μP xN kN |= P (0). Suppose k ∈ N, and so, M, μP xN k|= P (x). Next, we argue that every set satisfying the antecedent includes the set of natural numbers:Lemma 4. For every M and μ as above, and for any set Q , if M, μPQ|= A then N is a subset of Q .Proof. Suppose M, μPQand by assumption M, μPQis, M, μP xQ kfrom 0, every natural number and its successor is in Q , and so Q must include N. (cid:2)|= A. We prove the claim by induction on natural numbers. For the base case, we consider 0 ∈ N, |= P (0), and so 0 is in the set Q . For the hypothesis, assume for any k ∈ N, k is also in Q . That |= P (x + 1). Thus, starting |= P (x). Of course, the successor of k is a natural number and by assumption, M, μP xQ kSo, the main claim is as follows:Proof. Suppose M |= Natural(c), that is, M |= ∀P ( A ⊃ P (c)). Since N is a subset of the domain by assumption, for any map μ, M, μPN |= A ⊃ P (c). By Lemma 3, M, μPN |= P (c). Hence c M ∈ N.N |= A, and so M, μPSuppose instead c M ∈ N. Suppose for any set Q and any map μ, M, μPQ|= P (c). Therefore, M, μP|= A. By Lemma 4, N is a subset of Q and so N |= A ⊃ P (c). Since this holds for any set Q , M |= ∀P [ A ⊃ P (c)], that is, c M is in Q , that is, M, μPQM |= Natural(c). (cid:2)2.4. SummationHere we show how finite summations can be characterized as an abbreviation for an L-term using second-order quan-tification.Let f be any L-function from N to R. Let SUM( f , n), standing for the sum of the values of f for the argument 1 through n, be defined as an abbreviation:SUM( f , n) = z.= ∃g[g(1) = f (1) ∧g(n) = z ∧∀i(1 ≤ i < n ⊃ g(i + 1) = g(i) + f (i + 1))].The variable i is understood to be chosen here not to conflict with any of the variables in n and z. The function g from Nto R is assumed to not conflict with f . (That is, the logical terms are distinct.)This can then be argued to correspond to summations in the usual mathematical sense as follows:4 Alternatively, one could have specified axioms for characterizing the field of real numbers together with (cid:4). Whether or not reals with exponentiation is first-order axiomatizable remains a major open question [20].194V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221Theorem 5. Let f be a function symbol of L from N to R, c be a term, and n be a constant symbol of L. Let M be any R-interpretation. Then,ifnM(cid:3)i=1f M (i) = c M then M |= SUM( f , n) = c.Proof. We prove by induction on nM . For the base case, suppose nM = 1. Then, the antecedent would give us f M (1). As for the consequent, clearly M |= SUM( f , n) = f (1), and so the base case holds.Assume the hypothesis holds for nM , and we prove the case for (nM + 1). (Note that owing to “1” and “+” having their usual interpretations, (n + 1)M = nM + 1.) So supposenM +1(cid:3)i=1f M (i) = c M .On expanding the summation expression, we obtain:nM(cid:3)i=1f M (i) = c M − f M (nM + 1)By induction hypothesis, M |= SUM( f , n) = c − f (n + 1), and so, by definition, M |= SUM( f , n + 1) = c. (cid:2)Henceforth, we write:n(cid:3)i=1tto mean the logical formula SUM(t, n) for a logical term t. Here, i is assumed to not conflict with any of the variables in nand t.5It is worth noting that this logical formula can be applied to summation expressions where the arguments to the terms are not restricted to natural numbers, but taken from any finite set. For example, suppose H is any finite set of terms {h1, . . . , hn}. We can then use terms such as:(cid:3)t(h)h∈Hstanding for an abbreviation, similar to SUM(t, n): let g be a function, and let g(i) = t(hi). Then clearly the above sum (cid:4)ndefines the same number as i=1 g. In the sequel, we sometimes sum over a finite set of situations, or a finite vector of values, which is then understood as an abbreviation in this sense.2.5. IntegrationFinally, we characterize integrals as logical terms.6 We present this first for a continuous real-valued function of onevariable, and then discuss its (straightforward) extension to the many-variable case.We begin by introducing a notation for limits to positive infinity. For any logical term t and variable x, we introduce a term characterized as follows:LIM[x, t] = z(cid:5)(cid:5) < u)).The variables u, m, and n are understood to be chosen here not to conflict with any of the variables in x, t, and z. The abbreviation can be argued to correspond to the limit of a function at infinity in the usual sense:.= ∀u(u > 0 ⊃ ∃m ∀n(n > m ⊃(cid:5)(cid:5)z − txn5 That is, we use the standard mathematical notation to denote sums (and later: limits and integrals) in two ways that context will disambiguate: first, as the usual mathematical expression, and second, as a well-formed logical formula to be understood as an abbreviation as explained above.6 In this article, given a non-negative real-valued function, our notion of an integral of this function is based on the Riemann integral [21], in which case the function is said to be integrable. There are limitations to the Riemann integral; for example, the function f : [0, 1] → R where(cid:2)f (x) =1 if x is rational0 otherwiseis not integrable in the Riemann account. In the calculus community, generalizations to the Riemann integral, such as the gauge integral [22], have been studied that allow for the integration of such functions. We have chosen to remain within the framework of classical integration, but other accounts may be useful.The key idea for moving beyond Riemann integrals would be to amend the logical formula standing for the abbreviations we introduce below. (For example, rather than partitioning the domain of a function, partitioning the range would allow us to formalize Lebesgue integrals.)V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221195Lemma 6. Let g be a function symbol of L standing for a function from R to R, and let c be a constant symbol of L. Let M be any R-interpretation of L. Then we have the following:Iflimx→∞g M (x) = c M then M |= (c = LIM[x, g]).Proof. Suppose the limit holds. Then, by the standard notion of limits [23], for every real number (cid:10) > 0, there is a natural number j such that for all natural numbers k > j:|g M (k) − c M | < (cid:10).Suppose M (cid:16)|= LIM[x, g] = c. Then there is some r > 0 such that M, μu(cid:16)|= ∃m∀n(n > m ⊃ |c − g(n)| < u). By Theorem 2, the rdomain of M includes N, and so let m and n be variables that are mapped to j and k respectively. Then, given that Minterprets arithmetic symbols in the usual way, the claim M, μu m nr j k(cid:16)|= (n > m) ⊃ (|c − g(n)| < u) is a contradiction. (cid:2)Henceforth, we write:limx→∞tto mean the logical formula LIM[x, t].Next, for any variable x and terms a, b, and t, we introduce a term INT[x, a, b, t] denoting the definite integral of t over x from a to b:INT[x, a, b, t].= limn→∞h ·n(cid:3)i=1tx(a+h·i)where h stands for (b − a)/n. The variables are chosen not to conflict with any of the other variables. We now show:Lemma 7. Let g be a function symbol of L standing for a function from R to R, and let a, b, c be constant symbols of L. Let M be any R-interpretation of L. Then we have the following:bM(cid:6)Ifg M (x) dx = c M then M |= (c = INT[x, a, b, g]).aMProof. Suppose (cid:7)bMaM g M (x)dx = c M , that is, g M is integrable. By definition, then:h ·limk→∞k(cid:3)i=1g M (aM + h · i) = c Mwhere h = (bM − aM )/k. By Lemma 6, we haveM |= limn→∞h ·n(cid:3)i=1gx(a+h·i)= cwhere h = (b − a)/n, that is, M |= INT[x, a, b, g] = c. (cid:2)Finally, we define the definite integral of t over all real values of x by the following:(cid:6)t.= limu→∞limv→∞INT[x, −u, v, t].xThe main result for this logical abbreviation is the following:Theorem 8. Let g be a function symbol of L standing for a function from R to R, and let c be a constant symbol of L. Let M be any R-interpretation of L. Then we have the following:(cid:6)g M (x) dx = c M then M |= (c =g(x)).∞(cid:6)If−∞x196V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221Proof. Suppose (cid:7) ∞−∞ g M (x)dx = c M , that is, g M is integrable. By definition,v(cid:6)limu→∞limv→∞−ug M (x) = c.By Lemma 6 and Lemma 7, we obtain:M |= limu→∞(cid:7)limv→∞x g(x) = c. (cid:2)That is, M |=INT[x, −u, v, g] = c.The characterization of integrals for a many-variable function f , from Rk to R, is then an easy exercise. For variables x1, . . . , xk and terms a1, . . . , ak, b1, . . . , bk, and t, we introduce a term MINT[x1, . . . , xk, a1, . . . , ak, b1, . . . , bk, t] denoting the definite integral of t from (a1, . . . , ak) ∈ Rk to (b1, . . . , bk) ∈ Rk:MINT[x1, . . . , xk, a1, . . . , ak, b1, . . . , bk, t].= limn1→∞. . .limnk→∞h1 · · · hk ·n1(cid:3)nk(cid:3). . .i1=1ik=1tx1,...,xk(a1+h1·i1),...,(ak+hk·ik)where h j stands for (b j − a j)/n j . The variables are chosen not to conflict with any of the other variables. Finally, we define the definite integral of t over all real values of x1, . . . , xk by the following:(cid:6)x1,...,xkt.= limu1→∞. . .limuk→∞limv1→∞. . .limvk→∞MINT[x1, . . . , xk, −u1, . . . , −uk, v 1, . . . , vk, t].From this we get, as a corollary to Theorem 8:Corollary 9. Let g be a function symbol of L standing for a function from Rk to R, and let c be a constant symbol of L. Let M be any R-interpretation of L. Then we have the following:∞(cid:6)∞(cid:6)(cid:6)If. . .g M (x1, . . . , xk) dx1 . . . dxk = c M then M |= (c =g(x1, . . . , xk)).−∞−∞3. A theory of knowledge3.1. Knowledgex1,...,xkAn early treatment of knowledge in the situation calculus is due to Moore [24]. The classical possible-world interpretation for knowledge [8,9] is based on the notion that there many different ways the world can be, where each world stands for a complete state of affairs. Some of these are considered possible by a putative agent, and they determine what the agent knows and does not know. Moore’s observation was that situations can be viewed as possible worlds. (Different from standard modal logics [10], however, worlds are reified as part of the syntax, but this is a minor technicality.) A special (cid:11), s) says that binary fluent K , taking two situation arguments, determines the accessibility relation between worlds: K (swhen the agent is at s, he considers spossible. (Also different from standard modal logics, we note that the order of the terms in the accessibility relation is reversed.) Knowledge, then, is simply truth at accessible worlds:(cid:11)Definition 10. (Knowledge) Let φ be any situation-suppressed formula. The agent knowing φ at situation s, written Knows(φ, s), is the following abbreviation:.= ∀sKnows(φ, s), s) ⊃ φ[s. K (s(cid:11)].(cid:11)(cid:11)In English: if in every situation sMoore’s account has been adapted to the arrangement of dynamic laws via basic action theories by Scherl and Levesque [12]. In this scheme, the initial theory is assumed to specify the agent’s initial beliefs. For example, Knows(velocity(obj5) =that is considered possible at s the formula φ holds, then the agent knows φ at s.7(cid:11)7 We do not insist that beliefs are necessarily true in the real world. Perhaps for this reason, “believes” is a more appropriate reading of Knows. The two terms are used interchangeably here. Also, see Scherl and Levesque [12] on how features such as positive and negative introspection can be enabled by constraining the accessibility relation K , in a manner entirely analogous to standard modal logic [10].V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221197Fig. 2. Situations with accessibility relations after sensing. The numbers inside the circles denote the f values at these situations.(cid:11). K (s(cid:11)) = 50; that is, every accessible world initially agrees that the velocity of 50, S0) expands to ∀sobject 5 is 50, which means that the agent knows that the velocity of object 5 is 50. In addition, ¬Knows(velocity(obj6) =60, S0) and Knows( f = 0 ∨ f = 1, S0) say that initially, the agent does not know that the velocity of object 6 is 60, and knows that the functional fluent f takes a value of either 0 or 1. Equivalently, one specifies an initial constraint on K ; for example:(cid:11), S0) ⊃ velocity(obj5, sK (ι, S0) ≡ ( f = 1 ∨ f = 0)[ι](1)characterizes the agent’s knowledge about the fluent f taking a value of either 0 or 1.3.2. SensingTo specify the behavior of K at non-initial situations, Scherl and Levesque provide a successor state axiom for K . This axiom, intuitively, tests whether situations are to remain accessible as actions occur. Without going into full details, assume the provision of domain-specific sensing axioms, also part of the basic action theory D. For example,SF(sensetrue-f, s) ≡ f (s) = 1formalizes the sensing outcome for an action to check whether f has value 1 in situation s. Here SF is a special L-predicate, similar to Poss. Then, for sto be accessible from s, we need the following successor state axiom to be included in D:(cid:11)(cid:11)K (s, do(a, s)) ≡ ∃s(cid:11)(cid:11)This says that if sfrom do(a, s) contingent on sensing outcomes.is the predecessor of s(cid:11)(cid:11)[K (s(cid:11)(cid:11), s) ∧ s(cid:11) = do(a, s(cid:11)(cid:11)) ∧ P oss(a, s(cid:11)(cid:11)) ∧ (SF(a, s(cid:11)(cid:11)) ≡ SF(a, s))].(cid:11), such that s(cid:11)(cid:11)was considered possible at s, then s(cid:11)would be considered possible To appreciate how this axiom works in dynamical domains, assume that for actions without any sensing aspect, such as an action that toggles the value of f , one simply lets SF be vacuously true:SF(toggle-f, s) ≡ true.The idea, then, is that the accessibility between situations would not change as physical actions occur. However, as the agent operates in an environment where it senses various properties, those situations that are incompatible with the real world regarding the sensed value will be deemed impossible after such sensing actions. This leads to a notion of knowledge expansion,8 as the agent becomes more certain about the true nature of the world. See Fig. 2 for an illustration of the are axiom: we imagine three situations s, spossible worlds when the agent is at s). These situations disagree on the value of f , and consequently, the agent does not(cid:11)(cid:11)) is not epistemically related know f ’s value. After executing a sensing action for the truth of f , however, do(sensetrue-f , sto do(sensetrue-f , s). The upshot is that the agent knows the value of f at do(sensetrue-f , s) and believes that this value is 1.that are epistemically related prior to any sensing (that is, sand sand s(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)3.3. Degrees of belief and likelihoodThe Scherl and Levesque scheme, however, lacks constructs to quantify the agent’s uncertainty. One measure to quantify uncertainty is with degrees of belief. What is also lacking in their scheme is the ability to formalize the probabilistic noise in effectors and sensors, as seen in many real-world robotics applications [13]. These limitations, to discrete approximations, were addressed by BHL [5].The BHL scheme builds on Scherl and Levesque’s ideas, especially regarding how accessibility relations between worlds vary as a result of actions. In fact, the reader may observe many parallels between the two extensions. BHL’s remarkably simple proposal consists of introducing two new distinguished fluents, p and l, in addition to K . We present a simpler alternative involving only p and l. As a simple running example, imagine a robot moving towards a wall, as shown in Fig. 1. Its distance to the wall is given by a functional fluent h, and it is assumed to be equipped with a sonar sensor that measures how far the robot is from the wall. In other words, ideally, the sensor’s reading would correspond to the actual value of h. 8 Revising beliefs, where the agent believes φ but acquires information to now believe ¬φ, is not dealt with in this work. See [25] for an account.198V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221(cid:11), s) denotes the relative weight accorded to situation sThe p fluent determines a probability distribution on situations, by associating situations with weights. More precisely, the term p(swhen the agent happens to be in situation s. Of course, p can be seen as a companion to K . As one would for K , the properties of p in initial states, which vary from domain to domain, are specified with axioms as part of D0. For example,p(ι, S0) = u ≡ ((h(ι) = 2 ∨ h(ι) = 3) ∧ u = .5) ∨(cid:11)(2)(h(ι) (cid:16)= 2 ∧ h(ι) (cid:16)= 3 ∧ u = 0)says that those initial situations where h has the integer values 2 or 3 obtain a weight of .5. All other situations, then, obtain 0 weight. We expect, of course, that weights are nonnegative, and that non-initial situations are given a weight of 0 initially. The following nonnegative constraint, also part of D0, ensures this:∀ι, s. p(s, ι) ≥ 0 ∧ (p(s, ι) > 0 ⊃ Init(s))(P1)Note that this is a stipulation about initial situations ι only. But BHL provide a successor state axiom for p, to be listed shortly, that ensures that this constraint holds in all situations.Next, the term l(a, s) is intended to denote the likelihood of action a occurring in situation s. Among other things, l can be used to model noisy sensors. This is perhaps best demonstrated using an example. Imagine a sonar aimed at the wall, which gives a reading for the true value of h. Supposing the sonar’s readings are subject to additive Gaussian noise.9 If now a reading of z were observed on the sonar, intuitively, those situations where h = z should be considered more probable than those where h (cid:16)= z.10 This occurrence is captured using likelihoods in the formalism. Basically, if sonar(z) is the sonar sensing action with z being the value read, we specify a likelihood axiom describing its error profile as follows:l(sonar(z), s) = u ≡ (z ≥ 0 ∧ u = N (z − h(s); μ, σ 2)) ∨(z < 0 ∧ u = 0).(3)This stipulates that the difference between a nonnegative reading of z and the true value h is normally distributed with a variance of σ 2 and mean of μ.11Clearly, the error profile of various hardware devices is application dependent, and it is this profile that is modeled as shown above using l. Notice, for example, when μ = 0, which indicates that the sensor has no systematic bias, then l(sonar(5), s) will be higher when h(s) = 5 than when h(s) = 25. Roughly, then, the idea is that after an observation, the weights on situations would get redistributed based on their compatibility with the observed value.One may contrast such likelihood specifications to (trivial) ones for deterministic physical actions,12 such as an action move(z) of moving towards the wall by precisely z units. For such actions, we simply writel(move(z), s) = u ≡ u = 1in which case the p value of s is the same as that for do(move(3), s). Thus, this is a form of imaging [27], where the weights of worlds are simply “transferred” to their successors.Formally, we add action likelihood axioms to D:Definition 11. Action likelihood axioms for each action type A are sentences of the form:l( A((cid:12)x), s) = u ≡ ψ A((cid:12)x, u, s).Here ψ A((cid:12)x, u, s) is any formula characterizing the conditions under which action A((cid:12)x) has likelihood u in s.In general, likelihood axioms can depend on any number of features of the world besides the fluent that the sensor is measuring. For example, imagine that the sonar’s accuracy depends on the room temperature. We could then specify an error profile as follows:l(sonar(z), s) = u ≡(z ≥ 0 ∧ temp(s) ≥ 0 ∧ u = N (z − h(s); μ, 1)) ∨(z ≥ 0 ∧ temp(s) < 0 ∧ u = N (z − h(s); μ, 16)) ∨(z < 0 ∧ u = 0).(4)That is, the sonar’s accuracy worsens severely when the temperature drops below 0, as seen by the larger variance.9 Note that Gaussians are continuous distributions involving π , e, exponentiation, and so on. Therefore, BHL always consider discrete probability distri-butions that approximate the continuous ones.10 As usual, the reading observed is not in the control of the agent. Here, we assume that the value is given to us, and in that sense, the language is geared for projection (cf. Section 5.2). For example, we might be interested in the beliefs of the agent after obtaining a specific sequence of readings on the sonar. Integrating this language with an online framework that obtains such readings from an external source is addressed in [26].11 We understand N (u; μ, σ 2) as an abbreviation for the mathematical expression e12 Noisy actions will also involve non-trivial likelihood axioms. Their treatment, however, is deferred to a subsequent section.2 · π · σ 2.2·σ 2 /−(u−μ)2√V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221199Fig. 3. Situations with accessibility relations after noisy sensing. The numbers inside the circles denote the h values at these situations. Dotted circles denote a lower weight in relation to their epistemic alternatives.Having introduced the new fluents, we are now ready to provide the successor state axiom for p, which is analogous to the one for K :p(s(cid:11)(cid:11) [s(cid:11), do(a, s)) = u ≡(cid:11) = do(a, s∃su = p(s(cid:11) = do(a, s(cid:11)(cid:11) [s∨ ¬∃s(cid:11)(cid:11)) ∧ Poss(a, s(cid:11)(cid:11))](cid:11)(cid:11), s) × l(a, s(cid:11)(cid:11)) ∧(cid:11)(cid:11)) ∧ Poss(a, s(cid:11)(cid:11))] ∧ u = 0.(cid:11)(P2)(cid:11)(cid:11)times the likelihood This says that the weight of situations srelative to do(a, s) is the weight of their predecessors sof a contingent on the successful execution of a at s(cid:11)(cid:11).(cid:11)To see an application of this axiom using the specifications (2) and (3), consider two situations s and sassociated with the same weight initially, as shown in Fig. 3. These situations have h values 2 and 3 respectively. Suppose the robot obtains a reading of 2 on the sensor. Given a sensor with mean μ = 0 and variance σ 2 = 1, the likelihood ax-iom is such that the weight of the successor of s is higher than that of sbecause the h value at s coincides with the sensor reading. More precisely, the weight for the successor of s is given by the prior weight .5 multiplied by the likelihood factor N (z − h(s); μ, σ 2) = N (2 − 2; 0, 1) = N (0, 0, 1). The weight for the successor of sis obtained analo-gously.(cid:11)(cid:11)(cid:11)(cid:11), s) is 0, then their successors will also not be. Similarly, when a is not executable at sInterestingly, by means of the above axiom, if predecessors are not epistemically related, which is another way of saying , its successor is no longer that p(s(cid:11)accessible from do(a, s). One other consequence of (P1) and (P2) is that (p(sand s share the same history of actions. This is because (P1) insists that initial situations are only epistemically related to other initial ones, and (P2) respects this relation over actions.(cid:11), s) > 0) will be true only when s(cid:11)(cid:11)We are now prepared to define the degree of belief in a formula φ at a situation s, written Bel(φ, s). (Henceforth, whenever a formula φ appears in the context of Bel, we assume that it is either situation-suppressed, or it only mentions the situation term now.) Intuitively, this is simply the weight of accessible situations. Formally:Definition 12. (Degrees of belief) Suppose φ is any situation-suppressed L-formula. Then the degree of beliefabbreviation for:in φ is an Bel(φ, s).= 1γ(cid:3){s(cid:11):φ[s(cid:11)]}p(s(cid:11), s),where γ , the normalization factor, is understood (throughout) as the same expression as the numerator but with φ replaced by true. So, here, γ is (cid:11), s).(cid:4)s(cid:11) p(sNote that we do not have to insist that s(cid:11), s) will be 0 otherwise, as discussed above. The summation term in this logical formula is not a new logical symbol, but simply an abbreviation for a second-order formula, by way of Section 2.4.and s share histories since p(s(cid:11)3.4. DiscussionLet us conclude this section by remarking that since p is a fluent, the syntax of basic action theories allows us to express probabilistic knowledge in a very general way, quite beyond standard probabilistic formalisms [28,7]. For example, to represent categorical uncertainty like (1), we would do:(cid:2)p(ι, S0) =1 if (h = 2 ∨ h = 3)[ι]0 otherwise(5)In English: all initial situations where the value of h is either 2 or 3 are considered epistemically possible, and accorded a weight of 1. All other initial situations are accorded a weight of 0. Observe, however, that this p-specification does not say 200V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221which value of h is more likely. Thus, unlike standard probabilistic frameworks, we do not assume that it is always possible to find a single probability distribution for the robot to use.It is also possible to handle partial specifications. Let us contrast (2) with the following initial axiom for p:(cid:2)p(ι, S0) =.1 if h(ι) ∈ {1, 2, . . . , 10}0otherwiseThen, letting a basic action theory include the sentence:(2) ∨ (6)(6)means that the robot believes h is uniformly distributed on {2, 3} or on {1, . . . , 10} without being able to say which. To reiterate, we do not assume that it is always possible to find a single probability distribution for the robot to use.Of course, a much weaker specification is possible by replacing one of these probabilistic alternatives with categorical ones. For example, suppose the basic action theory were to instead include:(5) ∨ (6).In this case, the agent believes that h may take a value from {2, 3} or is uniformly distributed on {1, 2, . . . , 10} without being able to say which.In sum, the framework allows one to freely combine categorical and probabilistic specifications, leading to a very general model of belief. For simplicity of presentation, we consider a fairly simple set of specifications in this article, and refer readers to [29] for more involved ones.4. Belief reformulatedThe definition for degrees of belief, as given by BHL, is intuitive and simple. It is closely fashioned after the semantics for belief in modal probability logics [30], where the probabilities of formulas is calculated from the weights of possible worlds satisfying the formula. Unfortunately, this definition is not easily amenable to generalizations. Notice, for example, that Bel(cid:11)] holds is finite. This immediately precludes is well-defined only when the sum over all those situations sdomains that involve an infinite set of situations agreeing on a formula. Moreover, the definition does not have an obvious analogue for continuous probability distributions. Observe that such an analogue would involve integrating over the space of situations, which makes little sense. Indeed, it is not certain what the space of situations would look like in general, but even if this was fixed, how such a thing can be tinkered with so as to obtain an appropriate notion of integration is far from obvious.such that φ[s(cid:11)Therefore, what we propose is to shift the calculating of probabilities from situations to fluent values, that is, to the well-understood domain of numbers. The current section is an exploration of this idea. What we will show in this section is that Definition 12 can be reformulated as a summation over numeric indices. That will allow, among other things, a seamless generalization from summation to integration, which is to be the topic of the next section.To prepare for that, in addition to the usual case notation used, for example, in (6), we will introduce another kind of conditional term for convenience. This involves a quantifier and a default value of 0, like in formula (P2). If z is a variable, ψis a formula and t is a term, we use (cid:19)z.ψ → t(cid:20) as a logical term characterized as follows:(cid:19)z.ψ → t(cid:20) = u.=[(∃zψ) ⊃ ∀z(ψ ⊃ u = t)] ∧ [(¬∃zψ) ⊃ u = 0)].The notation says that when ∃zψ is true, the value of the term is t; otherwise, the value is 0. When t uses z (the usual case), this will be most useful if there is a unique z that satisfies ψ .Returning to the task at hand, we will now need a way to enumerate the primitive fluent terms of the language. Intu-itively, these correspond to the probabilistic variables in the language. Perhaps the simplest way is to assume there are nfluents f 1, f 2, . . . , fn in L which take no arguments other than the situation argument,13 and that they take their values from some finite sets. We can then rephrase Definition 12 as follows:13 Essentially, functional fluents in L are assumed to not take any object arguments. More generally, if we assume that the arguments of k-ary fluents are drawn from finite sets, an analogous enumeration of ground functional fluent terms is possible.Understandably, from the point of view of situation calculus basic action theories, where fluents are also usually allowed to take arguments from anyset, including infinite ones, this is a limitation. But in probabilistic terms, this would correspond to having a joint probability distribution over infinitely many, perhaps uncountably many, random variables. We know of no general logical account of this sort, and we have as yet no good ideas about how to deal with it. It remains to be seen whether ideas from probability theory on high dimensions [31,32] and infinite-dimensional probabilistic graphical models [33] can be leveraged for our purposes.V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221201Definition 13. Suppose φ is as before. Let Bel(φ, s) be an abbreviation for:1γ(cid:3)(cid:3)(cid:12)xs(cid:11)(cid:8)p(s0(cid:9)(cid:11), s) iff i(s(cid:11)) = xi ∧ φ[s(cid:11)]otherwisewhere γ is the numerator but with φ replaced by true, as usual.(For readability, we often drop the index variables in sums and connectives when the context makes it clear: in this case, i ranges over the set {1, . . . , n}, that is, the indices of the fluents in L.) Definition 13 suggests that for each possible value of the fluents, we are to sum over all possible situations and for each one, if the fluents have those values and φ holds, then f i(s) = xi into the p value is to be used, and 0 otherwise. Roughly speaking, if one were to group situations satisfying sets for every possible vector (cid:12)x, the union of these sets would give the space of situations. Our claim about the relationship between the two abbreviations can be made precise as follows:(cid:9)Theorem 14. Let D be any basic action theory and φ any L-formula. Then the abbreviations for Bel(φ, s) from Definitions 12 and 13define the same number.Proof. For the proof, we focus solely on the numerators of the two abbreviations. That is,(cid:3){s(cid:11):φ[s(cid:11)]}p(s(cid:11), s)on the one hand, and(cid:3)(cid:3)(cid:8)(cid:11), s) if(cid:9)f i(s(cid:11)) = xi ∧ φ[s(cid:11)]otherwisep(s0(cid:12)xs(cid:11)(†)(‡)on the other. We show that these expressions define the same number. With this, the case for the denominators follows trivially, since true is a special case of φ. Then, the claim is proven.Let S be a set such that s(cid:11)(cid:11) ∈ S iff p(s(cid:11), s) > 0. (That is, for any ground situation term s, this is the set of all ground (cid:11)]. Intuitively, T is the set of all (cid:11) ∈ T iff φ[ssituation terms ssituations that are epistemically related to s, but where φ holds. It is easy to see that(cid:11), s) > 0.) Then, let T ⊆ S be the set such that ssuch that p(s(cid:3)(†) =(cid:11)p(s, s).s(cid:11)∈TSuppose now f i ranges over {c1i, . . . , cki}, and so by extension, suppose (cid:12)f = (cid:19) f 1, . . . , fn(cid:20) ranges over { (cid:12)c1, (cid:12)c2, . . . , (cid:12)ck}. For any (cid:11)) = c ji . That is, T j identifies those situations from T where (cid:12)c j in that set, let T j ⊆ T be a set such that sf i(sfluents satisfy the vector of values (cid:12)c j . Observe that(cid:11) ∈ T j iff (cid:9)k(cid:3)(cid:3)(‡) =(cid:11)p(s, s).j=1s(cid:11)∈T j(cid:10)But, of course, T =T j . Therefore,k(cid:3)(cid:3)j=1s(cid:11)∈T jp(s(cid:11), s) =(cid:3)s(cid:11)∈T(cid:11)p(s, s).Therefore, (†) and (‡) define the same number. (cid:2)Be that as it may, Definition 13 still involves summations over situations. To arrive at a definition that eschews the summing of situations, we start with the case of initial situations. In this matter, we will be insisting on a precise space of initial situations. For this, let us recall the axiomatization of the situation calculus presented in [34] for multiple initial situations, which includes a sentence saying there is precisely one initial situation for any possible vector of fluent values. This can be written as follows:(cid:11)(cid:11)[∀(cid:12)x ∃ιf i(ι) = xi] ∧ [∀ι, ι(cid:11).f i(ι) = f i(ι(cid:11)) ⊃ ι = ι(cid:11)](P3)(Recall that i ranges over the indices of the fluents in L, that is, {1, . . . , n}.) Under the assumption (P3), we can rewrite Definition 12 for s = S0 as202V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221Bel(φ, S0).= 1γ(cid:11)(cid:3)(cid:19)ι.(cid:12)xf i(ι) = xi ∧ φ[ι] → p(ι, S0)(cid:20)The two abbreviations, in fact, are equivalent:(B0)Theorem 15. Let D be any basic action theory, φ any L-formula, and suppose D0 includes (P3). Then the abbreviations for Bel(φ, S0)in Definition 12 and (B0) define the same number.Proof. As in Theorem 14, we focus on the numerators for the two abbreviations. That is,(cid:3){s(cid:11):φ[s(cid:11)]}p(s(cid:11), S0)on the one hand, and(cid:11)(cid:3)(cid:19)ι.(cid:12)xf i(ι) = xi ∧ φ[ι] → p(ι, S0)(cid:20)(†)(‡)on the other. We show that these expressions define the same number. The denominators represent a special case, and so the claim will follow.Let S be the set of initial situations. Suppose f i ranges over (cid:19)c1, . . . , cn(cid:20) for the vector of fluents (cid:19) f 1, . . . , fn(cid:20), there exists a (unique) situation s ∈ S such that such that s ∈ T iff φ[s]. It is easy to see that. By way of (P3), for any vector of values f i(s) = ci . Let T ⊆ S be (cid:9)(cid:12)ci, c(cid:11)i, . . . , c(cid:11)(cid:11)i(cid:13)(‡) =(cid:3)s(cid:11)∈T(cid:11)p(s, S0).Since (P1) ensures that p(s(cid:3)(†) =p(ss(cid:11)∈T(cid:11), S0).(cid:11), S0) > 0 only if s(cid:11)is an initial situation, we get thatTherefore (†) and (‡) define the same number. (cid:2)This shows that for S0, summing over possible worlds can be replaced by summing over fluent values.Unfortunately, (B0) is only geared for initial situations. For non-initial situations, the assumption that no two agree on all fluent values is untenable. To see why, imagine an action move(z) that moves the robot z units to the left (towards the wall) but that the motion stops if the robot hits the wall. The successor state axiom for fluent h, then, might be like this:h(do(a, s)) = u ≡¬∃z(a = move(z)) ∧ u = h(s) ∨∃z(a = move(z) ∧ u = max(0, h(s) − z)).(7)In this case, if we have two initial situations that are identical except that h = 3 in one and h = 4 in the other, then the two distinct successor situations that result from doing move(4) would agree on all fluents (since both would have h = 0). Ergo, we cannot sum over fluent values for non-initial situations unless we are prepared to count some fluent values more than once.It turns out there is a simple way to circumvent this issue by appealing to Reiter’s solution to the frame problem. Indeed, Reiter’s solution gives us a way of computing what holds in non-initial situations in terms of what holds in initial ones, which can be used for computing belief at arbitrary successors of S0. More precisely,Definition 16. (Degrees of belief (reformulated)) Let φ be any L-formula. Given any sequence of ground action terms α = [a1, . . . , ak], letBel(φ, s).= 1γ(cid:3)(cid:12)xP((cid:12)x, φ, s)where if s = do(α, S0) thenP((cid:12)x, φ, do(α, S0)).= (cid:19)ι.(cid:11)f i(ι) = xi ∧ φ[do(α, ι)] → p(do(α, ι), do(α, S0)) (cid:20).V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221203(As before, i ranges over the indices of the fluents in L.) To say more about how (and why) this definition works, we first note that by (P1) and (P2), p will be 0 unless its two arguments share the same history. So the sargument of pin Definition 12 is expanded and written as do(α, ι) in Definition 16. By ranging over all fluent values, we range over all initial ι as before, but without ever having to deal with fluent values in non-initial situations. Of course, we test that the φholds and use the p weight in the appropriate non-initial situation. In particular, owing to p’s successor state axiom (P2), the weight for non-initial situations accounts for the likelihood of actions executed in the history. We now establish the following result:(cid:11)Theorem 17. Let D be any basic action theory with (P3) initially, φ any L-formula, and α any sequence of ground actions terms. Then the abbreviations for Bel(φ, do(α, S0)) in Definition 12 and Definition 16 define the same number.Proof. As in Theorem 14, we will focus on the numerators of the two abbreviations. That is,(cid:3){s(cid:11):φ[s(cid:11)]}p(s(cid:11), do(α, S0))on the one hand, and(cid:11)(cid:3)(cid:19)ι.(cid:12)xf i(ι) = xi ∧ φ[do(α, ι)] → p(do(α, ι), do(α, S0)) (cid:20).(†)(‡)on the other. We show that these expressions define the same number. The denominators represent a special case, and so the claim will follow.∗ = {do(α, s(cid:11)) : s(cid:11) ∈ S}. Let T ⊆ S∗such that s(cid:11)(cid:11) ∈ T iff φ[sLet S be the set of initial situations, as determined by (P3). Let S(cid:11)(cid:11)]. It is easy to see that(cid:3)(‡) =(cid:11)p(ss(cid:11)∈T, do(α, S0)).On the other hand, by (P1) and (P2), p(s, do(α, S0)) = 0 for all s /∈ S∗. This means that(†) =(cid:3)s(cid:11)∈T(cid:11)p(s, do(α, S0)).Therefore (†) and (‡) define the same number. (cid:2)Thus, by incorporating a simple constraint on initial situations, we now have a notion of belief that does not require summing over situations.Readers may notice that our reformulation only applies when we are given an explicit sequence α of actions, including the sensing ones. But this is just what we would expect to be given for the projection problem [11], where we are interested in inferring whether a formula holds after an action sequence. In fact, we can use regression on the φ and the p to reduce the belief formula from Definition 16 to a formula involving initial situations only. See [35] for work in this direction.5. From weights to densitiesThe framework presented so far is fully discrete, which is to say that fluents, sensors and effectors are characterized by finite values and finite outcomes. Belief in φ, in particular, is the summing over a finite set of situations where φ holds. We now generalize this framework. We structure our work by first focusing on fully continuous domains, which is to say that fluents, sensors and effectors are characterized by values and outcomes ranging over R. This section, in particular, explores the very first installment: effectors are assumed to be deterministic, but sensors have continuous noisy error profiles. The next section, then, allows both effectors and sensors to have continuous noisy profiles. Further generalizations are deferred to Section 7.Let us begin by observing that the uncountable nature of continuous domains precludes summing over possible situa-tions. In this section, we present a new formalization of belief in terms of integrating over fluent values. This, in particular, is made possible by the developments in the preceding section.Allowing real-valued fluents implies that there will be uncountably many initial situations. Imagine, for example, the scenario from Fig. 1, and that the fluent h can now be any nonnegative real number. Then for any nonnegative real x there will be an initial situation where (h = x) is true. Suppose further that D0 includes:(cid:2)p(ι, S0) =.1 if 2 ≤ h(ι) ≤ 12otherwise0(8)204V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221which says that the true value of h initially is drawn from a uniform distribution on the interval [2, 12]. Then there are uncountably many situations where p is non-zero initially. So the p fluent now needs to be understood as a density, not as (cid:11)a weight. (That is, we now interpret p(swhen the agent is in s.) In particular, for any x, we would expect the initial degree of belief in the formula (h = x) to be 0, but in (h ≤ 12) to be 1.(cid:11), s) as the density of sWhen actions enter the picture, even if deterministic, there is more to be said. Numerous subtleties arise with p in non-initial situations. For example, if the robot were to do a move(4) there would be an uncountable number of situations agreeing on h = 0: namely, those where 2 ≤ h ≤ 4 was true initially. In a sense, the point h = 0 now has weight, and the degree of belief in h = 0 should be .2. On the other hand, the other points h ∈ (0, 8] should retain their densities. That is, belief in h ≤ 2 should be .4 but belief in h = 2 should still be 0. In effect, we have moved from a continuous to a mixeddistribution on h. Of course, a subsequent rightward motion will retain this mixed density. For example, if the robot were to now move away by 4 units, the belief in h = 4 would then be .2.To address the concern of belief change in continuous domains, we now present a generalization to BHL. One of the advantages of our approach is that we will not need to specify how to handle changing densities and distributions like the ones above. These will emerge as side-effects, that is, shifting density changes will be entailed by the action theory.For our formulation of belief, we first observe that we have fluents f 1, . . . , fn in L as before, that take no argument other than the situation term but which now take their values from R. Then:Definition 18. (Degrees of belief (continuous noisy sensors)) Let φ be any situation-suppressed L-formula, and α =[a1, . . . , ak] any ground sequence of action terms. The degree of belief in φ at s is an abbreviation:Bel(φ, s).= 1γ(cid:6)P((cid:12)x, φ, s)(cid:12)xwhere, as in Definition 16, if s = do(α, S0) thenP ((cid:12)x, φ, do(α, S0)).= (cid:19)ι.(cid:11)f i(ι) = xi ∧ φ[do(α, ι)] → p(do(α, ι), do(α, S0)) (cid:20).That is, the belief in φ is obtained by ranging over all possible fluent values, and integrating the densities of situations where φ holds. If we were to compare the above definition to Definition 16, we see that we have simply shifted from summing over finite domains to integrating over reals. In fact, we could read P as the (unnormalized) density associated with a situation satisfying φ. As discussed, by insisting on an explicit world history, the ι need only range over initial situations, giving us the exact correspondence with fluent values.This completes our new definition of belief. To summarize, our extension to the BHL scheme is defined using a few convenient abbreviations, such as for Bel and mathematical integration, and where an action theory consists of:1. D0 (with (P1)) as usual, but now also including (P3);2. precondition axioms as usual;3. successor state axioms, including one for p, namely (P2), as usual;4. foundational domain-independent axioms as usual; and5. action likelihood axioms, one for each action type.Note that, apart from (P3) and Bel’s new abbreviation, we carry over precisely the same components as would BHL. By and large, the extension, thus, retains the simplicity of their proposal, and comes with minor additions. We will show that it has reasonable properties using an example and its connection to Bayesian conditioning below.In the sequel, we assume, without explicitly mentioning so, that basic action theories include the sentences (P1), (P2)and (P3).5.1. Bayesian conditioningWe now explicate the relationship between our definition for Bel and Bayesian conditioning [7]. Bayesian conditioning is a standard model for belief change wrt noisy sensing [13] and it rests on two significant assumptions. First, sensors do not physically change the world, and second, conditioning on a random variable fis the same as conditioning on the event of observing f .In general, in the language of the situation calculus, there need not be a distinction between sensing actions and physical actions. In that case, the agent’s beliefs are affected by the sensed value as well as any other physical changes that the action might enable to adequately capture the “total evidence” requirement of Bayesian conditioning.The second assumption expects that sensors only depend on the true value for the fluent. For example, in the formulation of (3) the sonar’s error profile is determined solely by h. But to suggest that the error profile might depend on other factors about the environment, as formulated by (4) for example, goes beyond this simplified view. In fact, here, the agent also learns about the room temperature, apart from sensing the value of h.V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221205Thus, our theory of action admits a view of dynamical systems far richer than the standard setting where Bayesian conditioning is applied. Be that as it may, when a similar set of assumptions are imposed as axioms in an action theory, we obtain a sensor fusion model identical to Bayesian conditioning. This connection was demonstrated in BHL for the discrete case. We prove the property formally for continuous variables below.We begin by stipulating that actions are either of the physical type or of the sensing type [12], the latter being the kind that do not change the value of any fluent, that is, such actions do not appear in the successor state axioms for any fluent. Now, if obs(z) senses the true value of fluent f , then assume the sensor error model to be:l(obs(z), s) = u ≡ u = Err(z, f (s))where Err(u1, u2) is some expression with only two free variables, both numeric. This captures the notion established above: the error model of a sensor measuring f depends only on the true value of f , and is independent of other factors. Finally, for simplicity, assume obs(z) is always executable:Poss(obs(z), s) ≡ true.Then we obtain:Theorem 19. Suppose D is any basic action theory with likelihood and precondition axioms for obs(z) as above, φ is any L-formula mentioning only f , and u ∈ {x1, . . . , xn} that f takes a value from. Then we obtain:D |= Bel(φ, do(obs(z), S0)) =(cid:7)(cid:12)x[P((cid:12)x, φ ∧ f = u, S0) × Err(z, u)](cid:7)[P((cid:12)x, f = u, S0) × Err(z, u)](cid:12)xThat is, the posterior belief in φ is obtained from the prior density and the error likelihoods for all points where φ holds given that z is observed, normalized over all points. The proof for the theorem is as follows.Proof. Without loss of generality, assume f takes the value x, and the remaining fluents are f 1, . . . fn that take values from x1, . . . , xn. Let a denote obs(z). From Definition 18, Bel(φ, do(obs(z), S0)) is an abbreviation for:(cid:6)1γ1γ1γ1γ1γ1γ1γP((cid:12)x, φ, do(a, S0)) =n(cid:11)i=1(cid:11)(cid:11)(cid:11)(cid:19)ι. f (ι) = x ∧(cid:19)ι. f (ι) = x ∧(cid:19)ι. f (ι) = x ∧(cid:19)ι. f (ι) = x ∧f i(ι) = xi ∧ φ[do(a, ι)] → p(do(a, ι), do(a, S0))(cid:20) =f i(ι) = xi ∧ φ[ι] → p(do(a, ι), do(a, S0))(cid:20) =f i(ι) = xi ∧ (φ ∧ f = x)[ι] → p(do(a, ι), do(a, S0))(cid:20) =f i(ι) = xi ∧ (φ ∧ f = x)[ι] → p(ι, S0) × Err(z, x)(cid:20) =Err(z, x) × (cid:19)ι. f (ι) = x ∧(cid:11)f i(ι) = xi ∧ (φ ∧ f = x)[ι] → p(ι, S0)(cid:20) =Err(z, x) × P((cid:12)x, φ ∧ f = x, S0).(cid:12)x(cid:6)(cid:6)(cid:12)x(cid:6)(cid:12)x(cid:6)(cid:12)x(cid:6)(cid:12)x(cid:6)(cid:12)x(cid:12)x(a)(b)(c)(d)(e)(f)The arguments underline those parts of the sentences that are being reduced. Step (a) expands P . In step (b), owing to the fact that sensing actions do not change fluent values (by our first assumption tailored to Bayesian conditioning), φ[do(a, ι)]is equivalent to φ[ι]. In step (c), we observe that what appears to the left of the right-arrow in step (b) is equivalent to one where φ[ι] is replaced by (φ ∧ f = x)[ι]. (The main reason for introducing the formula f = x is to allow us to identify those situations where f takes the value x which are all to be multiplied by the error likelihood for observing z when the true value is x.) In step (d), we use (P2) and the fact that a is always executable to replace p(do(a, ι), do(a, S0)) by p(ι, S0) × l(a, ι) = p(ι, S0) × Err(z, x). Now note that the term appearing in the context of an integral is suggesting that if there is an initial situation where a particular condition holds, then a certain value is returned, and otherwise 0 is returned. This allows us to place the term Err(z, x) on the outside in step (f), giving us the required numerator that appears in the claim. The expansion of the denominator is analogous with true being a special case for φ, and so we are done. (cid:2)206V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221The usual case for posteriors are formulas such as b ≤ f ≤ c, which is estimated from the prior and error likelihoods for all points in the range [b, c], as demonstrated by the following consequence:Fig. 4. A robot in a 2-dimensional grid.Corollary 20. Suppose D is any basic action theory with likelihood and precondition axioms for obs(z) as above, f is any L-fluent, and u is a variable from (cid:12)x = (cid:19)x1, . . . , xm(cid:20) that f takes a value from. Then we obtain:D |= Bel(a ≤ f ≤ b, do(obs(z), S0)) =(cid:12)x(cid:7)[P((cid:12)x, f = u ∧ a ≤ u ≤ b, S0) × Err(z, u)][P((cid:12)x, f = u, S0) × Err(z, u)](cid:7)(cid:12)xMore generally however, and unlike many probabilistic formalisms, we are able to reason about any logical property φof the random variable f being measured.5.2. ExampleUsing an example, we demonstrate the formalism and Theorem 19 in particular. To reason about the beliefs of our robot, let us build a simple basic action theory D. We extend the setting from Fig. 1 to a 2-dimensional grid, as shown in Fig. 4. As before, let h be the fluent denoting its horizontal position (that is, its distance to the wall), and let the robot’s vertical position be given by a fluent v. The components of D are as below.• Imagine a p of the form:(cid:2)p(ι, S0) =.1 × N (v(ι); 0, 16)0if 2 ≤ h(ι) ≤ 12otherwise(9)This says that the value of v is normally distributed about the horizontal axis with variance 16, and independently, that the value of h is uniformly distributed between 2 and 12.Note also that initial beliefs can be specified for D0 using Bel directly. For example, to express that the true value of his believed to be uniformly distributed on the interval [2, 12] we might equivalently include the following theory in D0:{Bel(2 ≤ h ≤ 12, S0) = .1, Bel(h ≤ 2 ∨ h ≥ 12, S0) = 0},and analogously for the fluent v.For this example, a simple distribution has been chosen for illustrative purposes. In general, recall from Section 3.4 that the p-specification does not require the variables to be independent, nor does it have to mention all variables.• For simplicity, let us assume that actions are always executable, i.e., that D includesPoss(a, s) ≡ true(10)for all actions a. For this example, we assume three action types: action move(z) that moves the robot z units towards the wall, action up(z) that moves the robot z units away from the horizontal axis, and action sonar(z) that gives a reading of z for the distance between the robot and the wall.• The successor state axiom for h is as in (7), and the one for v is as follows:v(do(a, s)) = u ≡ ¬∃z(a = up(z)) ∧ u = v(s) ∨∃z(a = up(z) ∧ u = v(s) + z).• For the sensor device, suppose its error model is given as follows:l(sonar(z), s) = u ≡ (z ≥ 0 ∧ u = N (z − h(s); 0, 4)) ∨ (z < 0 ∧ u = 0).(11)(12)The error model says that for nonnegative z readings, the difference between the reading and the true value is normally distributed with mean 0 (which indicates that there is no systematic bias) and variance 4.1414 For a more elaborate example involving multiple competing sensors and systematic bias, see [36].V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221For the remaining (physical) actions, we letl(move(z), s) = 1, l(up(z), s) = 1since they are assumed to be deterministic for this section.Then we obtain:207(13)Theorem 21. Let D be a basic action theory that is the union of {(9), (10), (7), (11), (12), (13)}. Then the following are logical entail-ments of D:1. Bel([h = 3 ∨ h = 4 ∨ h = 7], S0) = 0.To see how this follows, let us begin by expanding Bel([h = 3 ∨ h = 4 ∨ h = 7], S0):P((cid:12)x, h = 3 ∨ h = 4 ∨ h = 5, S0).(cid:12)x(a)For the rest of the section, let h take its value from x1 and v take its value from x2. By means of (P3), there is exactly one situation for any set of values for x1 and x2. The P term for any such situation, however, is 0 unless h = 3 ∨ h = 4 ∨ h = 5holds at the situation. Thus, (a) basically simplifies to:.1 × N (x2; 0, 16)0if x1 ∈ {3, 4, 5}otherwise= 0.In effect, although we are integrating a function δ(x1, x2) over all real values, δ(x1, x2) = 0 unless x1 ∈ {3, 4, 7}.2. Bel(h ≤ 9, S0) = .7.We might contrast this with the previous property in that for any given value for x1 and x2, the P term is 0 when x1 > 9. When x1 ≤ 9, however, the p value for the situation is obtained from the specification given by (9). That is, we have:Initial specification given by (9)0(cid:2)if h ≤ 9otherwise.1 × N (x2; 0, 16)0if ∃ι. h(ι) = x1, x1 ∈ [2, 12] and h(ι) ≤ 9otherwise==9(cid:6).1 × N (x2; 0, 16) dx1 dx2The numerator evaluates to .7, and the denominator to 1.3. Bel(h > 7v, S0) ≈ .6.Beliefs about any mathematical expression involving the random variables, even when that does not correspond to well known density functions, are entailed. To evaluate this one, for example, observe that we have.1 × N (x2; 0, 16)0if x1 ∈ [2, 12] and x1 > 7x2otherwise=x1/7(cid:6).1 × N (x2; 0, 16)dx1.4. Bel(h = 0, do(move(4), S0)) = .2.Here a continuous distribution evolves into a mixed distribution. This results from Bel(h = 0, do(move(4), S0)) first ex-panding as:(cid:6)1γP((cid:12)x, h = 0, do(move(4), S0))(cid:12)xThe P term, then, simplifies to:(cid:11)(cid:19)ι.f (ι) = x ∧ (h = 0)[do(move(4), ι)] → p(ι, S0)(cid:20)(a)(b)(cid:6)1γ(cid:2)1γ(cid:6)(cid:12)x(cid:2)(cid:6)(cid:12)x(cid:6)(cid:12)x(cid:6)R2(cid:2)(cid:6)(cid:12)x12(cid:6)−∞21γ1γ1γ1γ1γ208V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221Fig. 5. Belief update for h after physical actions. Initial belief at S0 (in solid red) and after a leftward move of 4 units (in blue with point markers). (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)That is, since move(z) has no error component, l(move(z), s) = 1 for any s in accordance with D. Therefore, p(do(a, ι), do(a, S0)) = p(ι, S0). Now (b) says that for every possible value for h and v, if there is an initial situation where h = 0 holds after moving leftwards, then its p value is to be considered. Note that for any initial situation swhere h(s) ∈ [2, 4], we get h(do(move(4), s)) = 0 by (7). This leaves us with:.1 × N (x2; 0, 16)0(cid:2)if ∃ι. h(ι) = x1, x1 ∈ [2, 12], h(ι) ∈ [2, 4]otherwise=.1 × N (x2; 0, 16)0if x1 ∈ [2, 4]otherwise(c)(cid:2)1γ1γ(cid:6)(cid:12)x(cid:6)(cid:12)xWe can show that γ = 1, which means (c) = .2. This change in beliefs is shown in Fig. 5.5. Bel(h ≤ 3, do(move(4), S0)) = .5.Bel’s definition is amenable to a set of h values, where one value has a weight of .2, and all the other real values have a uniformly distributed density of .1.6. Bel([∃a, s. now =do(a, s) ∧ h(s) >1], do(move(4), S0)) = 1.It is possible to refer to earlier or later situations using now as the current situation. This says that after moving, there is full belief that (h > 1) held before the action.7. Bel(h = 4, do([move(4), move(−4)], S0)) = .2Bel(h = 4, do([move(−4), move(4)], S0)) = 0.The point h=4 has 0 weight initially, as shown in item 1. Roughly, if the agent were to move leftwards first then many points would “collapse”, as shown in item 4. The point would then obtain a h value of 0, and have a weight of .2. The weight is then retained on moving away by 4 units, where the point once again gets h value 4. On the other hand, if this entire phenomena were reversed then none of these features are observed because the collapsing does not occur and the entire space remains fully continuous.8. Bel(−1 ≤ v ≤ 1, do(move(4), S0)) = Bel(−1 ≤ v ≤ 1, S0) = ∫1−1Owing to Reiter’s solution to the frame problem, belief in v is unaffected by a lateral motion. That is, a leftwards mo-tion does not change v in accordance with (11). As per (9), the initial belief in v ∈ [−1, 1] is the area between [−1, 1]bounded by the specified Gaussian.N (x2; 0, 16)dx2.9. Bel(v≤7, do(up(2.5), S0)) = Bel(v≤4.5, S0).After the action up(2.5), the Gaussian for v’s value has its mean “shifted” by 2.5 because the density associated with v = x2 initially is now associated with v = x2 + 2.5. Intuitively, we have:(cid:2)1γ1γ1γ(cid:6)(cid:12)x(cid:6)(cid:12)x(cid:6)(cid:12)x.1 × N (x2; 0, 16)0(cid:2).1 × N (x2; 0, 16)0(cid:2)if ∃ι.v(ι) = x2 and (v ≤ 7)[do(up(2.5), ι)]otherwiseif ∃ι.v(ι) = x2 and v(ι) ≤ 4.5otherwise==.1 × N (x2; 0, 16)0if x2 ≤ 4.5otherwiseV. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221209Fig. 6. Belief change for h at S0 (in solid red), after sensing 5 (in green with circular markers), and after sensing 5 twice (in blue with square markers).10. Bel(h ≤ 9, do(sonar(5), S0)) ≈ .97.Bel(h ≤ 9, do([sonar(5), sonar(5)], S0) ≈ .99.Compared to item 2, belief in h ≤ 9 is sharpened by obtaining a reading of 5 on the sonar, and sharpened to almost certainty on a second reading of 5. This is because the p function, according to (P2), incorporates the likelihood of each sonar(5) action. More precisely, the belief term in the first entailment simplifies to:1γ(cid:6)(cid:12)x(cid:19)ι.h(ι) = x1 ∧ v(ι) = x2 ∧ h(ι) ≤ 9 → p(ι, S0) × N (5 − x1; 0, 4)(cid:20)Note that we have replaced (h ≤ 9)[do(sonar(5), S0)] by (h ≤ 9)[ι] since sonar(z) does not affect h. From (a), we get1γ(cid:6)(cid:12)xN (5 − x1; 0, 4) × (cid:19)ι.h(ι) = x1 ∧ v(ι) = x2 ∧ h(ι) ≤ 9 → p(ι, S0)(cid:20)We know from (9) that those initial situations where h ≤ 2 have p values 0. Therefore, from (b), we get:(a)(b)(cid:6)(cid:12)x(cid:6)(cid:2)N (5 − x1; 0, 4) ×.1 × N (x2; 0, 16)0if x1 ∈ [2, 9]otherwise=9(cid:6)N (5 − x1; 0, 4) × .1 × N (x2; 0, 16)dx1dx2R21γ1γAfter a second reading of 5 from the sonar, the expansion for belief is analogous, except that the function to be inte-grated gets multiplied by a second N (5 − x1; 0, 4) term. It is then not hard to see that belief sharpens significantly with this multiplicand. The agent’s changing densities are shown in Fig. 6.6. Noisy actingIn the presentation so far, we assumed physical actions to be deterministic. By this we mean that when a physical action a occurs, it is clear to us (as modelers) but also the agent how the world has changed on a. Of course, in real-istic domains, especially robotic applications, this is not the usual case. In this section, in a domain that has continuous fluents, we show how our current account of belief can be extended to reason with sensors as well as effectors that are noisy.In line with the rest of this work, effector noise is given a quantitative account. Let us first reflect on what is expected with noisy acting. When an agent senses, as in the case of sonar(z), the argument for this action is not chosen by the agent. That is, the world decides what z should be, and based on this reading of z, the agent comes to certain conclusions about its own state. The noise factor, then, simply addresses the phenomena that the number z returned may differ from the true value of whatever fluent the sensor is measuring.210V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221(cid:11)Noisy acting diverges from that picture in the following sense. The agent intends to do action a, but what actually occurs is athat is possibly different from a. For example, the agent may want to move 3 units, but, unbeknownst to the agent, it may move by 3.042 units. The agent, of course, does not observe this outcome. Nevertheless, provided the agent has an account of its effector’s inaccuracies, it is reasonable for the agent to believe that it is in fact closer to the wall, even if it may not be able to precisely tell by how much. Intuitively, the result of a nondeterministic action is that any number of successor situations might be obtained, which are all indistinguishable in the agent’s perspective (until sensing is performed). Depending on the likelihoods of the action’s potential outcomes, some of these successor situations are considered more probable than others. The agent’s belief about what holds then must incorporate these relative likelihoods. So, in our view, nondeterminism is really an epistemic notion.6.1. Noisy action typesFollowing [5], perhaps the simplest extension to make all this precise is to assume that deterministic actions such as move(x) now have companion action types move(x, y) in L. The intuition is that x represents the nominal value, which is the number of units that the agent intends to move, while y represents the actual distance moved. The actual value of y in any ground action, of course, is not observable for the agent. This simple idea will need three adjustments to our account:1. successor state axioms need to be built using these new action types;2. the formalism must allow the modeler to formalize that certain outcomes are more likely than others, that is, noisy actions may be associated with a probabilistic account of the various outcomes; and3. the notion of belief must incorporate the nominal value, the range of possible outcomes and their likelihoods.First, we address successor state axioms. These are now specified as usual, but using the second argument, which is the actual outcome, rather than the nominal value, which is ignored. For example, for the fluent h, instead of (7), we will now have:h(do(a, s)) = u ≡ ∃x, y[a = move(x, y) ∧ u = max(0, h − y)] ∨¬∃x, y[a = move(x, y)] ∧ u = h(s).(14)The reason for this modification is obvious. If y is the actual outcome then the fluent change should be contingent on this value rather than what was intended. It is important to note that no adjustment to the existing (Reiter’s) solution to the frame problem is necessary.6.2. The Golog approachThe foremost issue now is to use the above idea to allow for more than one possible successor situation. Clearly, we do not want the agent to control the actual outcome in general. So the approach taken by BHL is to think of picking the second argument as a nondeterministic Golog program [11]. Briefly, Golog is an agent programming proposal where one is allowed to formulate complex actions that denote sequential and nondeterministic executions of actions, among others, and is essentially a basic action theory. Given the action move(x, y), for example, the Golog program Move(x) might stand for the abbreviation π y. move(x, y), which corresponds to a ground action move(x, n) where n is chosen nondeterministically. For our purposes, we would then imagine that the agent executes Golog programs.There are some advantages to this approach: namely, we only have to look at the logical entailments, including ones mentioning Bel, of such Golog programs. Since traces of these programs account for many potential outcomes, Bel does the right thing and accommodates all of these when considering knowledge. But the disadvantage is that the resulting formal specification turns out be unnecessarily complex, at least as far as projection is concerned.For projection tasks, we show that we can settle on a simpler alternative, one that does not appeal to Golog. Like BHL, we assume that the world is deterministic, where the result of doing a ground action leads to a distinct successor. Roughly, the intuition then is that when a noisy action is performed, the various outcomes of the action as well as the potential successor situations that are obtained wrt these are treated at the level of belief.6.3. Alternate action axiomsInspired by [37], our approach is based on the introduction of a distinguished predicate Alt. The idea is this: if Alt(a, aholds for ground action a = A((cid:12)c) then we understand this to mean that the agent believes that any instance of amight have been executed instead of a. Here, (cid:12)z denotes the range of the arguments for potential outcomes.(cid:11), (cid:12)z)(cid:11) = A((cid:12)z)To see how that gets used with the new action types such as move(x, y), consider the ground action move(3, 3.1). So, the agent intends to move by 3 units but what has actually occurred is a move by 3.1 units. Since the agent does not observe the latter argument, from its perspective, what occurred could have been a move by 2.9 units, but also perhaps (although less likely) a move by 9 units. Thus, the ground actions move(3, 2.9) and move(3, 9) are Alt-related to move(3, 3.1). (The likelihoods for these may vary, of course.) In logical terms, we might have an axiom of the following form in the background theory:V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221211Fig. 7. Situations with accessibility relations after noisy acting. The numbers inside the circles denote the h values at these situations. Dotted circles denote a lower density in relation to their epistemic alternatives.Alt(move(x, y), a(cid:11), z) ≡ a(cid:11) = move(x, z).(15)This to be read as saying that move(3, z) for every z ∈ R are alternatives to move(3, 3.1). If we required that z is only a certain range from 3, for example, we might have:Alt(move(x, y), a(cid:11), z) ≡ a(cid:11) = move(x, z) ∧ |x − z| ≤ cwhere c bounds the magnitude of the maximal possible error. On the other hand, for actions such as sonar(z), which do not have any alternatives, we simply write:Alt(sonar(x), a(cid:11), z) ≡ a(cid:11) = sonar(z) ∧ z = x.This says that sonar(x) is only Alt-related to itself.(16)With this simple technical device, one can now additionally constrain the likelihood of various outcomes using l. For example:l(move(x, y), s) = u ≡ u = N ( y − x; μ, σ 2)(17)says that the difference between nominal value and the actual value is normally distributed with mean μ and variance σ 2. This essentially corresponds to the standard additive Gaussian noise model in robotics [13].(cid:11)(cid:11)(cid:11)and sTo see an example of how, say, (15) and (17) work together with the successor state axiom (P2) for p, consider three situations s, sassociated with the same density, as shown in Fig. 7. Suppose their h values are 6, 6.1 and 5.9respectively. After attempting to move 3 units, the action move(3, z) for any z ∈ R may have occurred. So, for each of the three situations, we explore successors from different values for z. Assume the motion effector is defined by a mean μ = 0(cid:11)), for example, is obtained from the p-value and variance σ 2 = 1. Then, the p-value of the situation do(move(3, 5.7), smultiplied by the likelihood of move(3, 5.7), which is N (5.7 − 3; 0, 1) = N (2.7; 0, 1). Thus, the successor situation for sdo(move(3, 5.7), s(cid:11)) is much less likely than the successor situation do(move(3, 3), s), as should be the case.In general, we define alternate actions axioms that are to be a part of the basic action theory henceforth:(cid:11)Definition 22. Let A((cid:12)x, (cid:12)y) be any action. Alternate actions axioms are sentences of the form:Alt( A((cid:12)x, (cid:12)y), a(cid:11), (cid:12)z) ≡ a(cid:11) = A((cid:12)x, (cid:12)z) ∧ ψ((cid:12)x, (cid:12)y, (cid:12)z)where ψ is a formula that characterizes the relationship between the nominal and true values.The one limitation with this definition is that only actions of the same type, i.e., built from the same function symbol, are alternatives to each other. This does not allow, for example, situations where the agent intends a physical move, but instead unlocks the door. Nevertheless, this definition is not unreasonable because noisy actions in robotic applications typically involve additive noise [13]. Moreover, this limitation only assists us in arriving at a simple and familiar definition for belief. A more involved definition would allow for other variants.6.4. A definition for beliefWe have thus far successfully augmented successor state axioms and extended the formalism for modeling noisy actions. The final question, then, is how can the outcomes of a noisy action, and their likelihoods, be accounted for? Indeed, a formula might not only be true as a result of the actions intended, but also as a result of those that were not.Consider the simple case of deterministic actions, where the density associated with s is simply transferred to do(a, s). (cid:11)are Alt-related, then the result of doing a at s would lead This is an instance of Lewis’s imaging [27]. In contrast, if a and a212V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221(cid:11), s). Moreover, unlike noisy sensors, a and ato successor situations do(a, s) and do(amay affect fluent values in different ways, which is certainly the case with move(3, 3.074) and move(3, 3) on the fluent h. Thus, the idea then is that when reasoning about the agent’s beliefs about φ, one would need to integrate over the densities of all those potential successors where φ would hold.(cid:11)To make this precise, let us first consider the result of doing a single action a at S0. The degree of belief in φ after doing a is now an abbreviation for:(cid:6).= 1γBel(φ, do(a, S0))(cid:12)x(cid:6)zP ((cid:12)x, z, φ, do(a, S0))whereP ((cid:12)x, z, φ, do(a, S0)).= (cid:19)ι, b.(cid:11)f i(ι) = xi ∧ Alt(a, b, z) ∧ φ[do(b, ι)] → p(do(b, ι), do(a, S0)) (cid:20).(As before, the i ranges over the indices of the fluents in L, that is, {1, . . . , n}.) The intuition is this. Recall that by integrating over (cid:12)x, all possible initial situations are considered by f i(ι) = xi . Analogously, by integrating over z, all possible action outcomes are considered by Alt(a, b, z). Supposing a = A(x, y), for each outcome b = A(x, z),15 we test whether φ holds at the resulting situation do(b, ι) as before, and use its p-value. Here, this p-value is given by p(do(b, ι), do(a, S0)), where the first argument is the successor of interest do(b, ι) and the second is the real world do(a, S0).The generalization, then, for a sequence of actions is as follows:Definition 23. (Degrees of belief (continuous noisy effectors and sensors)) Suppose φ is any L-formula. Then the degree of belief in φ at s, written Bel(φ, s), is defined as an abbreviation:Bel(φ, s).=1γ(cid:6)(cid:6)P ((cid:12)x, (cid:12)z, φ, s)(cid:12)x(cid:12)zwhere, if s = do([a1, . . . , ak], S0), then.=P ((cid:12)x, (cid:12)z, φ, s)(cid:9)(cid:19)ι, b1, . . . , bk.f i(ι) = xi ∧(cid:9)Alt(a j, b j, z j) ∧ φ[do([b1, . . . , bk], ι)]→ p(do([b1, . . . , bk], ι), do([a1, . . . , ak], S0)) (cid:20).(Here, i ranges over {1, . . . , n} as before, and j ranges over the indices of the ground actions {1, . . . , k}.) That is, given any sequence, for all possible (cid:12)z values, we consider alternate sequences of ground action terms and integrate the densities of successor situations that satisfy φ, using the appropriate p-value.6.5. ExampleLet us now build a simple example with noisy actions. Consider the robot scenario in Fig. 1. Suppose the basic action theory D includes the foundational axioms, and the following components.The initial theory D0 includes the following p specification:(cid:2)p(ι, S0) =.50if 10 ≤ h(ι) ≤ 12otherwise(18)For simplicity, let h be the only fluent in the domain, and assume that actions are always executable. The successor state axiom for the fluent h is (14). For p, it is the usual one, viz. (P2).We imagine two actions in this domain, one of which is the noisy move move(x, y) and a sonar sensing action sonar(z). For the alternate actions axioms, let us use (15) and (16).Finally, we specify the likelihood axioms. Let the sonar’s error profile bel(sonar(z), s) = u ≡ (z ≥ 0 ∧ u = N (z − h(s); 0, .25)) ∨ (z < 0 ∧ u = 0).(19)Readers may note that this sonar is more accurate than the one characterized by (12), as it has a smaller variance. Regarding the likelihood axiom for move(x, y), let that be:l(move(x, y), s) = u ≡ u = N ( y − x; 0, 1).This completes the specification of D.(20)15 For ease of presentation, we assume that the nominal and the actual arguments involve a single variable.V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221213Fig. 8. Belief change for h at S0 (in solid red), a noisy move away from the wall (in green with circular markers), and after a second noisy move towards the wall (in blue with square markers).Theorem 24. The following are entailments of D.1. Bel(h ≥ 11, do(move(−2, −2.01), S0)) ≈ .95We first observe that for calculating the degrees of belief, we have to consider all those successors of initial situations wrt move(−2, z) for every z, where φ holds. By (P2), the p value for such situations is the initial p value times the likelihood of move(−2, z), which is N (z + 2; 0, 1) by (20). Therefore, we get.5 × N (z + 2; 0, 1)0(cid:2)if ∃ι. x ∈ [10, 12], h(ι) = x and (h ≥ 11)[do(move(2, z), ι)]otherwise.5 × N (z + 2; 0, 1)0(cid:2)if ∃ι. x ∈ [10, 12], h(ι) = x and h(ι) − z ≥ 11otherwise.5 × N (z + 2; 0, 1)0if x ∈ [10, 12] and x − z ≥ 11otherwise==It is not hard to see that had the action been deterministic, the degree of belief in h ≥ 11 after moving away by 2 units should have been precisely 1. In Fig. 8, we see the effect of this move, where the range of h values with non-zero densities extends considerably more than 2 units.2. Bel(h ≥ 10, do([move(−2, −2.01), move(2, 2.9)], S0)) ≈ .74The argument proceeds in a manner identical to the previous demonstration. The density function is further multiplied by a factor of N (u − 2; 0, 1), from (20) and (P2). More precisely, we have.5 × N (z + 2; 0, 1) × N (u − 2; 0, 1)0if ∃ι. x ∈ [10, 12], h(ι) = x and h(ι) − z − u ≥ 10otherwise=(cid:2).5 × N (z + 2; 0, 1) × N (u − 2; 0, 1)0if x ∈ [10, 12] and x − z − u ≥ 10otherwiseIf the action were deterministic, yet again the degree of belief about h ≥ 10 would be 1 after the intended actions. That is, the robot moved away by 2 units and then moved towards the wall by another 2 units, which means that h’s current value should have been precisely what the initial value was.See Fig. 8 for the resulting density change. Intuitively, the resulting density changes as effectuated by the moves de-grades the agent’s confidence considerably. In Fig. 8, for example, we see that in contrast to a single noisy move, the range of h values considered possible has extended further, leading to a wide curve.3. Bel(h ≥ 11, do([move(−2, −2.01), sonar(11.5)], S0) ≈ .94This demonstrates the result of a sensing action after a noisy move. Using arguments analogous to those in the previous item, it is not hard to see that we have:(cid:2)(cid:6)(cid:6)x(cid:6)z(cid:6)x∞(cid:6)z12(cid:6)−∞101γ1γ1γ(cid:2)1γ1γ(cid:6)(cid:6)(cid:6)x(cid:6)z(cid:6)u12(cid:6)zu10214V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221Fig. 9. Belief change for h at S0 (in solid red), after a noisy move (in green with circular markers), after sensing once (in blue with square markers), and after sensing twice (in solid magenta).1γ(cid:6)(cid:6)xz(cid:2).5 × N (z + 2; 0, 1) × N (x − z − 11.5; 0, .25)0if ∃ι. x ∈ [10, 12], h(ι) = x and h(ι) − z ≥ 11otherwise4. Bel(h ≥ 11, do([move(−2, −2.01), sonar(11.5), sonar(12.6)], S0) ≈ .99In this case, two successive readings around 12 strengthens the agent’s belief about h ≥ 11. The density function is multiplied by N (x − z − 12.6; 0, .25) because of (P2) and (12) as follows:(cid:2)1γ1γ(cid:6)(cid:6)x(cid:6)z(cid:6)zxδ × N (x − z − 11.5; 0, .25) × N (x − z − 12.6; 0, .25)0(cid:2)if ∃ι. x ∈ [10, 12], h(ι) = x, h(ι) − z ≥ 11otherwise=δ × N (x − z − 11.5; 0, .25) × N (x − z − 12.6; 0, .25)0if x ∈ [10, 12] and x − z ≥ 11otherwisewhere δ = .5 × N (z + 2; 0, 1), and γ is(cid:6)(cid:6)(cid:2)zxδ × N (x − z − 11.5; 0, .25) × N (x − z − 12.6; 0, .25)0if x ∈ [10, 12]otherwiseIn Fig. 9, the agent’s increasing confidence is shown as a result of these sensing actions. Note that even though the sensors are noisy, the agent’s belief about h’s true value sharpens because the sensor is a fairly accurate one.7. GeneralizationMany real-world problems have both continuous and discrete components (sensors, fluents, and/or effectors). Not sur-prisingly, discrete sensors can be easily modeled in the current scheme, as they only affect the p-values. Regarding fluents and effectors, it turns out that accommodating the more general case is an easy exercise, where an integration symbol in Bel corresponding to a continuous fluent or action argument is replaced by a summation symbol.To clarify, we proceed as follows. We begin with an example for discrete sensors, introduce a general definition for Belin the above sense, and finally conclude with an example that demonstrates this general setting.7.1. ExampleWe understand a discrete sensor to mean a sensing action that is characterized by a finite number of possible obser-vations. Thus, these observations would be associated with a probability rather than a density. Imagine the robot scenario from Fig. 1. Suppose that instead of a sensor that returns a number indicating the distance to the wall, the robot is equipped with a crude binary version. This latter sensor simply indicates whether the robot is close or far from the wall.Formally, suppose there is a sensing action sensewall(z) where z ∈ {close, far}. A noise-free model for the closeness sensor might be as follows:V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221215l(sensewall(z), s) = u ≡ (z = close ∧ ((h(s) ≤ 1 ∧ u = 1) ∨ (h(s) > 1 ∧ u = 0))) ∨(z = far ∧ ((h(s) > 1 ∧ u = 1) ∨ (h(s) ≤ 1 ∧ u = 0))).For the more interesting case of a noisy sensor, assume the following error profile:l(sensewall(z), s) = u ≡ (z = close ∧ h(s) ≤ 3 ∧ u = 2/3) ∨(z = close ∧ h(s) > 3 ∧ u = 1/3) ∨(z = far ∧ h(s) ≥ 4 ∧ u = 4/5) ∨(z = far ∧ h(s) < 4 ∧ u = 1/5).(21)Notice that the behavior of the sensor differs for the two values. So, for z = close the sensor considers h ≤ 3 as a measure of closeness, and has an accuracy of 2/3. For z = far, however, the sensor takes h ≥ 4 to be a measure of being far, and has an accuracy 4/5.We now build a simple action theory using this sensor. For simplicity, assume noise-free physical actions and a single fluent, h. Let D be the union of the p specification (8), the successor state axiom (7), the above likelihood model (21), in addition to (P1), (P2) and (P3). For the actions A(z) in D, including the sensor, letAlt( A(z), a(cid:11), u) ≡ a(cid:11) = A(u) ∧ u = z.We now analyze belief change on the application of this sensor.Theorem 25. The following are entailments of D:1. Bel(h ≤ 4, do(sensewall(close), S0)) = 3/11(cid:11))), where By (P2), after a = sensewall(close) the term p(do(a, sErr(u1, u2) denotes the error profile (21) of the closeness sensor. To compute the belief term, let us first resolve the normalization factor γ . It is not hard to see that γ evaluates to(cid:11)), do(a, s)) is obtained from p(s(cid:11), s) × Err(close, h(s⎧⎪⎨⎪⎩(cid:6)x.1 × 2/3 if ∃ι. h(ι) = x, x ∈ [2, 12] and h(ι) ≤ 3.1 × 1/3 if ∃ι. h(ι) = x, x ∈ [2, 12] and h(ι) > 30otherwiseSince D0 assigns a non-zero density to only those situations where h ∈ [2, 12] one obtains the above normalization factor. Moreover, after doing sensewall(close), the density of those situations where h ≤ 3 is multiplied by a factor of 2/3, while the density of the remaining situations is multiplied by a factor of 1/3.Formulating the numerator is analogous, except that only those situations where h ≤ 4 are to be considered. To be precise, the degree of belief in h ≤ 4 after the sensing action is:⎧⎪⎨⎧⎪⎨1γ1γ(cid:6)x(cid:6)x1γ1γ(cid:6)x(cid:6)x.1 × 2/3 if x ∈ [2, 12], x ≤ 3, x ≤ 4.1 × 1/3 if x ∈ [2, 12], x > 3, x ≤ 4⎪⎩0otherwise⎧⎪⎨.1 × 2/3 if x ∈ [2, 3].1 × 1/3 if x ∈ (3, 4]⎪⎩otherwise0=This then amounts to 3/11.2. Bel(h ≤ 4, do([move(1), sensewall(close)])) = 5/12We proceed in a manner analogous to the previous item. Using (P2) and (21), we obtain:.1 × 2/3 if ∃ι. h(ι) = x, h(do(move(1), ι)) ≤ 3 and h(do(move(1), ι)) ≤ 4.1 × 1/3 if ∃ι. h(ι) = x, h(do(move(1), ι)) > 3 and h(do(move(1), ι)) ≤ 4⎪⎩0otherwise=⎧⎪⎨.1 × 2/3 if x ∈ [2, 12], x − 1 ≤ 3 and x − 1 ≤ 4.1 × 1/3 if x ∈ [2, 12], x − 1 > 3 and x − 1 ≤ 4⎪⎩0otherwiseThe numerator amounts to 5/3 and the normalization factor γ is 4, leading to a degree of belief of 5/12.216V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–2217.2. A general definition for beliefFig. 10. Robot moving towards a wall that has a window.The main idea is to simply allow the range of some fluents to be taken from finite sets. So, reconsider Definition 18, where the fluents and the action arguments were assumed to take values from R. Then, suppose fluents f 1, . . . , fn takes values from R, and fluents g1, . . . , gm take values from finite sets. Intuitively, f i is to be seen as a continuous probabilistic variable, and g¯i as a discrete probabilistic variable.16 Analogously, suppose a1, . . . , ao are action types such that Alt(a j, b, z)holds for z ∈ R, and suppose d1, . . . , dl are action types such that Alt(d¯j, r, u) holds for u taken from a finite set. Intu-itively, a j is to be seen as a noisy action characterized by a continuous probability distribution, and d¯j as a noisy action characterized by a discrete probability distribution. Then:Definition 26. (Degrees of belief (discrete and continuous fluents, sensors and effectors)) Suppose φ is any L-formula. The degree of belief in φ at s, written Bel(φ, s), is an abbreviation:(cid:6)(cid:6)Bel(φ, s).= 1γ(cid:3)(cid:3)(cid:12)y(cid:12)uP ((cid:12)x · (cid:12)y, (cid:12)z · (cid:12)u, φ, s)(cid:12)zwhere if s = do(α, S0) for α = [a1, . . . , d1, . . . , ao, . . . , dl], then(cid:12)xP ((cid:12)x · (cid:12)y, (cid:12)z · (cid:12)u, φ, s).=(cid:19)ι, b1, . . . , bo, r1, . . . , rl.(cid:9)(cid:9)f i(ι) = xi ∧Alt(a j, b j, z j) ∧∧g¯i(ι) = y¯iAlt(d¯j, r¯j, u ¯j) ∧ φ[do([b1, . . . , rl], ι)](cid:9)(cid:9)→ p(do([b1, . . . , rl], ι), do([a1, . . . , dl], S0)) (cid:20).Here, · is for the concatenation of variables, i ranges over the indices of the continuous fluents, ¯i over the indices of the discrete fluents, j over the indices of the continuous actions, and ¯j over the indices of the discrete actions.Naturally, by means of (P3), there is an initial situation for every possible real number for f 1, . . . , fn and for every possible vector of values for g1, . . . , gm. The intuition is as before. That is, owing to a bijection between situations and the vector of fluent values, for any given value for x1, . . . , xn, y1, . . . , ym, there is a unique initial situation where f i has the value xi and g¯i has the value y¯i . The only difference to what we had earlier is that instead of just integrating over possible values for (cid:12)x, of course, we integrate over values for (cid:12)x and sum over possible values for (cid:12)y while using the p-values of successor situations where φ holds. When a noisy action occurs, the space of possible successor situations is determined by Alt(a j, b, z) for a noisy action with a continuous probabilistic model, and the space is determined by Alt(d¯j, r, u) otherwise.7.3. ExampleTo demonstrate the more intricate case of discrete and continuous fluents, and discrete and continuous action arguments, we consider an example of a robot moving towards the wall with a window, as shown in Fig. 10. Like with Fig. 1, let a fluent h denote the distance to the wall. Let w be a fluent that captures the status of the window in the sense of whether it is opened or closed, with w = 1 meaning that it is open and w = 0 meaning that it is closed.To change the status of the window, we imagine a noisy effector setwin(x, y) where the agent intends on setting w to x, but it is, in fact, set to y. We formally account for this effector using:Alt(setwin(x, y), al(setwin(x, y), s) = u ≡ ((x = 0 ∨ x = 1) ∧ x = y ∧ u = .75) ∨, z, s) ≡ a(cid:11) = setwin(x, z)(cid:11)((x = 0 ∨ x = 1) ∧ ( y = 0 ∨ y = 1) ∧ |x − y| = 1 ∧ u = .25) ∨((¬(x = 0 ∨ x = 1) ∨ ¬( y = 0 ∨ y = 1)) ∧ u = 0).(22)(23)16 Of course, discrete probabilistic variables can also take values from infinite sets, and by way of Section 2.5, limits can be used to define the sum of an infinite sequence of terms, that is, a series. For simplicity of presentation, however, we assume discrete fluents and action arguments take values from finite sets.V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221217This can be seen as saying that {x, y} take values {0, 1}, and the likelihood of x and y agreeing is .75, and that of disagreeing is .25. So, it is very likely that the action succeeds.From this, we then provide the following successor state axiom for w:w(do(a, s)) = u ≡ ∃x, y[setwin(x, y) ∧ u = y] ∨¬∃x, y[setwin(x, y) ∧ u = w(s)].(24)For simplicity, assume that the action move(z) moves the robot towards the wall by z units (that is, it is deterministic), for which we use the successor state axiom (7).Analogously, imagine a noisy sensor seewin(z) that provides a reading of x for the status of the window, but with the following error profile:l(seewin(z), s) = u ≡ (z = 1 ∧ w(s) = 1 ∧ u = .8) ∨(z = 1 ∧ w(s) = 0 ∧ u = .2) ∨(z = 0 ∧ w(s) = 1 ∧ u = .3) ∨(z = 0 ∧ w(s) = 0 ∧ u = .7).(25)That is, when returning 1, the sensor gives the correct reading with a probability of .8, but when returning 0, the sensor gives the correct reading with a probability of .7. Alternate action axioms are specified as usual for sensors:Alt(seewin(x), a(cid:11), z, s) ≡ a(cid:11) = seewin(z) ∧ z = x.(26)To finalize the example, let D be a union of (7), (22), (23), (24), (26), (25), together with (P1), (P2), (P3), and the following initial axiom for p:⎧⎪⎨p(ι, S0) =.5 × .6 if 10 ≤ h(ι) ≤ 12 and w(ι) = 1.5 × .4 if 10 ≤ h(ι) ≤ 12 and w(ι) = 0⎪⎩0otherwise(27)That is, h and w are independent, h is uniformly distributed on [10, 12] and the window being open has a probability of .6.Theorem 27. The following are entailed by D:1. Bel(w = 0, S0) = .4We are to compute the following term, which is easily shown to be equal to .4:⎧⎪⎨1γ(cid:6)x(cid:3)y.5 × .6 ∃ι. x ∈ [10, 12], h(ι) = x, w(ι) = y, y = 1 and w(ι) = 0.5 × .4 ∃ι. x ∈ [10, 12], h(ι) = x, w(ι) = y, y = 0 and w(ι) = 0⎪⎩0otherwise2. Bel(w = 0, do(move(1), S0)) = .43. Bel(w = 0, do(setwin(0, 1), S0)) = .75Owing to Reiter’s solution to the frame problem, belief in the closed window does not change on moving laterally. If we were to expand the belief term, we could get a logical term equivalent to the one in the previous item.After attempting to close the window, the agent’s belief about the window being closed is .75. Not surprisingly, since the action sets the final value of w (as opposed to toggle its value), the degree of belief precisely corresponds to the probability of the action getting successfully executed in (23). Expanding the belief term, we first obtain:⎧⎪⎨(cid:3)(cid:3)y⎪⎩z1γ(cid:6)x.5 × .6 × δ(0, z) ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 1, (w = 0)[do(setwin(0, z), ι)].5 × .4 × δ(0, z) ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 0, (w = 0)[do(setwin(0, z), ι)]0otherwiseHere δ(0, z) is the probability assigned to the substitution of z in setwin(0, z) via (23). Simplifying the above, we get:1γ1γ(cid:6)x(cid:6)x(cid:3)(cid:3)y(cid:3)(cid:3)y⎧⎪⎨⎪⎩z⎧⎪⎨⎪⎩z.5 × .6 × .75 if ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 1, z = 0, (w = 0)[do(setwin(0, z), ι)].5 × .4 × .75 if ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 0, z = 0, (w = 0)[do(setwin(0, z), ι)]0otherwise=.5 × .6 × .75 if x ∈ [10, 12], y = 1, z = 0.5 × .4 × .75 if x ∈ [10, 12], y = 0, z = 0otherwise0218V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221Essentially, there are only two ways that the window can be closed after doing setwin(0, z). Either the window is closed initially and z = 0, or the window is open and again z = 0. This leads to .75.4. Bel(w = 0, do([setwin(0, 1), seewin(0)], S0)) = .875After observing that the window is closed from its sensor, the robot’s belief about the window being closed increases. Proceeding in a manner analogous to above, after simplifications, we get:⎧⎪⎨(cid:3)(cid:3)y⎪⎩z1γ(cid:6)x.5 × .6 × .75 × .7 if ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 1, z = 0, (w = 0)[do(α, ι)].5 × .4 × .75 × .7 if ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 0, z = 0, (w = 0)[do(α, ι)]0otherwisewhere α = [setwin(0, z), seewin(0)]. The numerator simplifies to .75 × .7. It can be shown that γ is:(cid:3)(cid:3)yz(cid:6)x⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩.5 × .6 × .75 × .7 if ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 1, z = 0, (true)[do(α, ι)].5 × .4 × .75 × .7 if ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 0, z = 0, (true)[do(α, ι)].5 × .6 × .25 × .3 if ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 1, z = 1, (true)[do(α, ι)].5 × .4 × .25 × .3 if ∃ι. h(ι) = x, x ∈ [10, 12], w(ι) = y, y = 0, z = 1, (true)[do(α, ι)]0otherwisewhich leads to .75 × .7 + .25 × .3. Thus, the belief in the window being closed is .875.7.4. SummaryThis completes the specification for reasoning about beliefs with discrete and continuous fluents against noisy effec-tors and sensors. To summarize, the generalization of the BHL scheme to arbitrary domains is defined using convenient abbreviations for Bel, sums and integrals, and where an action theory consists of:• D0 describing the initial state, which also includes (P1) and (P3);• successor state axioms as before, including a fixed one for p, namely (P2);• alternate actions axioms for noisy effectors;• likelihood axioms for noisy sensors and noisy effectors; and• precondition axioms and foundational axioms as before.It is perhaps also worth noting that our specification extends BHL in a minimal way. They do not need a fixed space of situations, which we do by (P3), but this is very reasonable [34]. Likelihood axioms are specified for us in much the same manner as they would. Their treatment of noisy actions and sensors is slightly different in expecting the modeler to provide indistinguishability axioms. Briefly, these axioms speculate the set of actions that are observationally indistinguishable to the agent. Roughly, then, this serves the same purpose as our alternate actions axioms. However, their approach requires the successor state axiom for p to include notions of indistinguishability. But perhaps most significantly, as we mentioned earlier, their approach needs to appeal to Golog to reason about belief change after noisy effectors, which we do not.To conclude our technical treatment, let us attempt to simplify Definition 26. We will first abuse notation and use a single symbol to denote either sums or integrals, with the understanding that they expand appropriately for a given term t. For the sake of the discussion, let us use x t to mean the integration of the function t(x) from negative to positive infinity when the function takes values from R, and the sum of terms otherwise. Under this notational convention, let us suppose f 1, . . . , fn are all the fluents in the language, some of which take values from R and others take values from finite sets. Suppose a1, . . . , al are all the action types in the language such that for some action types Alt(a, b, z) holds for z ∈ R, and for the remaining action types, Alt(a, b, z) holds for z taking values from finite sets. Then, we obtain a proposal like Definition 18:(cid:7)Definition 28. (Degrees of belief (simplified and general)) Suppose φ is any L-formula, and let summation over term t as appropriate. Then the degree of belief in φ at s, written Bel(φ, s), is defined as an abbreviation:x t denote integration or (cid:7)(cid:6)Bel(φ, s).=1γP ((cid:12)x, (cid:12)z, φ, s)(cid:12)x · (cid:12)zwhere, if s = do(α, S0) for α = [a1, . . . , ak] and suppose β = [b1, . . . , bk], then(cid:9)(cid:9)P ((cid:12)x, (cid:12)z, φ, s).= (cid:19)ι, b1, . . . , bk.f i(ι) = xi ∧Alt(a j, b j, z j) ∧ φ[do(β, ι)] → p(do(β, ι), do(α, S0)) (cid:20).That is, i ranges over the indices of the fluents, and j over the indices of the ground action terms.V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–2212198. Related workThis article focused on degrees of belief in a first-order dynamic setting. In particular, the existing scheme of BHL was generalized to handle both discrete and continuous probability distributions while retaining all the advantages. Related efforts on belief update via sensor information can be broadly classified into two camps: the literature on probabilistic formalisms, and those that extend logical languages. We discuss them in turn. At the outset, we remark that the focus of our work is on developing a general framework, and not on computational considerations, efficiency or otherwise.From the perspective of probabilistic modeling, graphical models [28], such as Bayesian networks [7], are important formalisms for dealing with probabilistic uncertainty in general, and the uncertainty that would arise from noisy sensors in particular. Mainly, when random variables are defined by a set of dependencies, the density function can be compactly factorized using these formalisms. The significance of such formalisms is computational, with reasoning methods, such as filtering, being a fundamental component of contemporary robotics and machine learning technologies [6,38,13]. On the representation side, however, these formalisms have difficulties handling strict uncertainty, as would arise from connectives such as disjunctions. (Proposals such as credal networks [39] allow for certain types of partial specifications, but still do not offer the generality of arbitrary logical constraints.) Moreover, since rich models of actions are rarely incorporated, shifting conditional dependencies and distributions are hard to address in a general way. While there are graphical model frame-works with an account of actions, such as [40,41], they too have difficulties handling strict uncertainty and quantification. To the best of our knowledge, no existing probabilistic formalism handles changes in state variables like those considered here.This inherent limitation of probabilistic formalisms led to a number of influential proposals on combining logical and probabilistic specifications [42]. (The synthesis of deductive reasoning and the probability calculus has a long and distin-guished history [43,44] that we do not review here; see [45] and references therein.) The works of Bacchus and Halpern [30,3], for example, provide the means to specify properties about the domain together with probabilities about proposi-tions; see [46] for a recent list on first-order accounts of probability. But these do not explicitly address reasoning about actions. As we noted, treating actions in a general way requires, among other things, addressing the frame problem, reason-ing about what happened in the past and projecting the future, handling contextual effects, as well as appropriate semantical machinery. We piggybacked on the powerful situation calculus framework, and extended that theory for reasoning about continuous uncertainty.In a similar vein, from a modal logical perspective, the interaction between categorical knowledge, on the one hand, and degrees of belief, on the other, is further discussed in [30,47,48]. While these are essentially propositional, there are first-order variants [49]. Actions are not explicitly addressed, however.Recently in AI, limited versions of probabilistic logics have been discussed, in the form of relational graphical mod-els, Markov logic networks, probabilistic databases and probabilistic programming [50–56,33,57]. Some have been further extended for continuous probability distributions and temporal reasoning [58–61]. Overall, these limit the first-order expres-siveness of the language, do not treat actions in a general way, and do not handle strict uncertainty. Admittedly, syntactical restrictions in these frameworks are by design, in the interest of tractability (or at least decidability) wrt inference, as they have origins in the richer probabilistic logical languages mentioned above [30,3]. From the point of view of a general-purpose representation language, however, they are lacking in the kinds of features that we emphasize here.From the perspective of dynamical systems, closest in spirit to our work here are knowledge representation languages for reasoning about action and knowledge, which we refer to as action logics. The situation calculus [1,11], which has been the sole focus of this paper, is one such language. There are others, of course, such as the event calculus [62], dynamic logic [63,64], the fluent calculus [65], and formalisms based on the stable model semantics [66].In the situation calculus, a monotonic solution to the frame problem was provided by Reiter [19]. The situation calculus was extended to reason about knowledge whilst incorporating this solution in [12], and to reason about noisy effectors and sensors by BHL.17 Other action logics have enjoyed similar extensions. For example, [68] proposes an extension to dynamic logic for reasoning about degrees of belief and noisy actions, and [69] provides a computational framework for probabilistic reasoning using the stable model semantics, but they are propositional. In [70,71], the fluent calculus was extended for probabilistic beliefs and noisy actions. None of these admit continuous probability distributions.The situation calculus has also been extended for uncertainty modeling in other directions. For example, [72] consider discrete noisy actions over complete knowledge, that is, no degrees of belief. In later work, [73] treat continuous random variables as meta-linguistic functions, and so their semantics is not provided in the language of the situation calculus. This seems sufficient for representing things like products of probabilistic densities, but it is not an epistemic account in a logical or probabilistic sense. A final prominent extension to the situation calculus for uncertainty is the embedding of decision-theoretic planning in Golog [74,75]. Here, actions are allowed to be nondeterministic, but the assumption is that the actual state of the world is fully observable. (It essentially corresponds to a fully observable Markov decision process [76].) In this sense, the picture is a special case of the BHL framework. It is also not developed as a model of belief. While this line of work has been extended to a partially observable setting [77], the latter extension is also not developed as a model 17 Throughout, we operated under the setting of knowledge expansion, that is, observations are assumed to resolve the agent’s uncertainty and never contradict what is believed. The topic of belief revision lifts this assumption [67], but it is not considered here. See [25] for an account of belief revision in the situation calculus.220V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221of belief. Perhaps most significantly, neither of these support continuous probability distributions, nor strict uncertainty at the level of probabilistic beliefs.It is worth noting that real-valued fluents in action logics turn out to be useful for modeling resources and time. See, for example, [78–80]. These are, in a sense, complementary to an account of belief.Outside the logical literature, there are a variety of formalisms for modeling noisy dynamical systems. Of these, partially observable Markov decision processes (over discrete and continuous random variables) are perhaps the most dominant [13]. They can be seen as belonging to the literature on probabilistic planning languages [81,82]. Recent probabilistic planning languages [83], moreover, combine continuous Bayesian networks and classical planning languages. Planning languages, generally speaking, only admit a limited set of logical connectives, constrain the language for specifying dynamic laws (that is, they limit the syntax of the successor state axioms), and do not handle strict uncertainty.In sum, to our knowledge, our proposal is the first of its kind to handle degrees of belief, noisy sensors and effectors over discrete and continuous probability distributions in a general way. The proposal allows for partial and incomplete spec-ifications, and the properties of belief will then follow at a corresponding level of specificity. Moreover, to our knowledge, no other logical formalism for uncertainty deals with the integration of continuous variables within the language.9. ConclusionsMany real-world applications, such as robotics, have to deal with numerous sources of uncertainty, the main culprit be-ing sensor noise. Probabilistic error models have proven to be essential in state estimation, allowing the beliefs of a robot to be strengthened over time. But to use these models, the modeler is left with the difficult task of deciding how the do-main is to be captured in terms of random variables, and shifting conditional independences and distributions. In the BHL model, one simply provides a specification of some initial beliefs, characterizes the physical laws of the domain, and suitable posterior beliefs are entailed. The applicability of BHL was limited, however, by its inability to handle continuous distribu-tions, a limitation we lift in this article. By recasting the assessment of belief in terms of fluent values, we now seamlessly combine the situation calculus with discrete probability distributions, densities and difficult combinations of the two. We demonstrated that distributions evolve appropriately after actions, emerging as a side-effect of the general specification. Our formal framework was then shown to easily accommodate the interaction between discrete and continuous fluents, discrete and continuous noise models, and logical connectives. At a specification level, the framework provides the necessary bridge between logic-based action formalisms and probabilistic ones.Armed with this general specification language, we are in a position to investigate specialized reasoning machinery. To give a few examples, in [35,84], we identified general projection techniques, where we transform a property of belief after a sequence of (noisy) actions and observations to what is believed initially. In later work [85], we provided an efficient implementation of a projection technique under some reasonable assumptions, in service of enabling richer domain axiom-atizations for robotics applications. Finally, a version of Golog was recently embedded in our model of belief [26], in the style of knowledge-based programming [10,86].A major criticism leveled at much of the work in cognitive robotics [87], and logic-based knowledge representation more generally is that the languages are far removed from the kind of uncertainty and noise seen in machine learning and robotics applications. A formal language such as the one considered in this article addresses this concern. It also shows the advantages of appealing to logical machinery: firstly, in admitting natural, rich and intuitive physical laws; secondly, in allowing belief specifications that can exploit the full power of logical connectives, thereby going considerably beyond standard probabilistic formalisms; and thirdly, in alleviating the burden of determining how these probabilistic beliefs are af-fected in dynamical systems. In the long term, we hope it takes steps towards a general-purpose epistemologically-adequate representation language as envisioned by McCarthy and Hayes.References[1] J. McCarthy, P.J. Hayes, Some philosophical problems from the standpoint of artificial intelligence, in: Machine Intelligence, 1969, pp. 463–502.[2] M. Richardson, P. Domingos, Markov logic networks, Mach. Learn. 62 (1) (2006) 107–136.[3] F. Bacchus, Representing and Reasoning with Probabilistic Knowledge, MIT Press, 1990.[4] J. Halpern, An analysis of first-order logics of probability, Artif. Intell. 46 (3) (1990) 311–350.[5] F. Bacchus, J.Y. Halpern, H.J. Levesque, Reasoning about noisy sensors and effectors in the situation calculus, Artif. Intell. 111 (1–2) (1999) 171–208.[6] T. Dean, M. Wellman, Planning and Control, Morgan Kaufmann Publishers Inc., 1991.[7] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, 1988.[8] S. Kripke, Semantical considerations on modal logic, Acta Philos. Fenn. 16 (1963) 83–94.[9] J. Hintikka, Knowledge and Belief: An Introduction to the Logic of the Two Notions, Cornell University Press, 1962.[10] R. Fagin, J.Y. Halpern, Y. Moses, M.Y. Vardi, Reasoning About Knowledge, MIT Press, 1995.[11] R. Reiter, Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems, MIT Press, 2001.[12] R.B. Scherl, H.J. Levesque, Knowledge, action, and the frame problem, Artif. Intell. 144 (1–2) (2003) 1–39.[13] S. Thrun, W. Burgard, D. Fox, Probabilistic Robotics, MIT Press, 2005.[14] Z. Zamani, S. Sanner, P. Poupart, K. Kersting, Symbolic dynamic programming for continuous state and observation POMDPs, in: NIPS, 2012, pp. 1403–1411.[15] K. Murphy, Machine Learning: A Probabilistic Perspective, The MIT Press, 2012.[16] V. Belle, H.J. Levesque, Reasoning about continuous uncertainty in the situation calculus, in: Proc. IJCAI, 2013.[17] H. Enderton, A Mathematical Introduction to Logic, Academic Press, New York, 1972.V. Belle, H.J. Levesque / Artificial Intelligence 262 (2018) 189–221221[18] R. Smullyan, First-Order Logic, Dover Publications, 1995.[19] R. Reiter, The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression, in: Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy, Academic Press, 1991, pp. 359–380.[20] D. Marker, Model theory and exponentiation, Not. Am. Math. Soc. 43 (7) (1996) 753–759.[21] W.F. Trench, Introduction to Real Analysis, Prentice Hall, 2003.[22] C. Swartz, Introduction to Gauge Integrals, World Scientific Publishing Company Incorporated, 2001.[23] H.J. Keisler, Elementary Calculus: An Infinitesimal Approach, Courier Corporation, 2012.[24] R.C. Moore, A formal theory of knowledge and action, in: Formal Theories of the Commonsense World, Ablex, Norwood, NJ, 1985, pp. 319–358.[25] S. Shapiro, M. Pagnucco, Y. Lespérance, H.J. Levesque, Iterated belief change in the situation calculus, Artif. Intell. 175 (1) (2011) 165–192.[26] V. Belle, H.J. Levesque, Allegro: belief-based programming in stochastic dynamical domains, in: IJCAI, 2015.[27] D. Lewis, Probabilities of conditionals and conditional probabilities, Philos. Rev. (1976) 297–315.[28] D. Koller, N. Friedman, Probabilistic Graphical Models – Principles and Techniques, MIT Press, 2009.[29] V. Belle, H.J. Levesque, Robot location estimation in the situation calculus, J. Appl. Log. 13 (4) (2015) 397–413.[30] R. Fagin, J.Y. Halpern, Reasoning about knowledge and probability, J. ACM 41 (2) (1994) 340–367.[31] P. Billingsley, Probability and Measure, 3rd edition, Wiley-Interscience, 1995.[32] G. Da Prato, An Introduction to Infinite-Dimensional Analysis, Universitext, Springer, 2006.[33] P. Singla, P.M. Domingos, Markov logic in infinite domains, in: UAI, 2007, pp. 368–375.[34] H.J. Levesque, F. Pirri, R. Reiter, Foundations for the situation calculus, Electron. Trans. Artif. Intell. 2 (1998) 159–178.[35] V. Belle, H.J. Levesque, Reasoning about probabilities in dynamic systems using goal regression, in: Proc. UAI, 2013.[36] V. Belle, H.J. Levesque, Robot location estimation in the situation calculus, in: Symposium on Logical Formalizations of Commonsense Reasoning, 2013.[37] J.P. Delgrande, H.J. Levesque, Belief revision with sensing and fallible actions, in: Proc. KR, 2012.[38] D. Fox, J. Hightower, L. Liao, D. Schulz, G. Borriello, Bayesian filtering for location estimation, IEEE Pervasive Comput. 2 (3) (2003) 24–33.[39] F.G. Cozman, Credal networks, Artif. Intell. 120 (2) (2000) 199–233.[40] A. Darwiche, M. Goldszmidt, Action networks: a framework for reasoning about actions and change under uncertainty, in: Proc. UAI, 1994, pp. 136–144.[41] H. Hajishirzi, E. Amir, Reasoning about deterministic actions with probabilistic prior and application to stochastic filtering, in: Proc. KR, 2010.[42] N. Nilsson, Probabilistic logic, Artif. Intell. 28 (1) (1986) 71–87.[43] R. Carnap, Logical Foundations of Probability, Routledge and Kegan Paul, London, 1951.[44] H. Gaifman, Concerning measures in first order calculi, Isr. J. Math. 2 (1) (1964) 1–18.[45] J.Y. Halpern, Reasoning About Uncertainty, MIT Press, 2003.[46] Z. Ognjanovic, M. Raškovic, Some first-order probability logics, Theor. Comput. Sci. 247 (1–2) (2000) 191–212.[47] D. Monderer, D. Samet, Approximating common knowledge with common beliefs, Games Econ. Behav. 1 (2) (1989) 170–190.[48] A. Heifetz, P. Mongin, Probability logic for type spaces, Games Econ. Behav. 35 (1–2) (2001) 31–53.[49] V. Belle, G. Lakemeyer, H.J. Levesque, A first-order logic of probability and only knowing in unbounded domains, in: Proc. AAAI, 2016.[50] D. Koller, A. Pfeffer, Probabilistic frame-based systems, in: AAAI/IAAI, 1998, pp. 580–587.[51] L. Getoor, N. Friedman, D. Koller, B. Taskar, Learning probabilistic models of relational structure, in: ICML, 2001, pp. 170–177.[52] S.J. Russell, Unifying logic and probability, Commun. ACM 58 (7) (2015) 88–97.[53] D. Poole, First-order probabilistic inference, in: Proc. IJCAI, 2003, pp. 985–991.[54] P. Domingos, W.A. Webb, A tractable first-order probabilistic logic, in: Proc. AAAI, 2012.[55] L. De Raedt, A. Kimmig, H. Toivonen, ProbLog: a probabilistic Prolog and its application in link discovery, in: Proc. IJCAI, 2007, pp. 2462–2467.[56] D. Suciu, D. Olteanu, C. Ré, C. Koch, Probabilistic databases, Synth. Lect. Data Manag. 3 (2) (2011) 1–180.[57] B. Milch, B. Marthi, S.J. Russell, D. Sontag, D.L. Ong, A. Kolobov, BLOG: probabilistic models with unknown objects, in: Proc. IJCAI, 2005, pp. 1352–1359.[58] J. Choi, A. Guzman-Rivera, E. Amir, Lifted relational Kalman filtering, in: Proc. IJCAI, 2011, pp. 2092–2099.[59] D. Nitti, T. De Laet, L. De Raedt, A particle filter for hybrid relational domains, in: IROS, 2013, pp. 2764–2771.[60] C. Anderson, P. Domingos, D. Weld, Relational Markov models and their application to adaptive web navigation, in: Proc. SIGKDD, ACM, 2002, pp. 143–152.[61] V. Belle, A. Passerini, G. Van den Broeck, Probabilistic inference in hybrid domains by weighted model integration, in: IJCAI, 2015.[62] R. Kowalski, M. Sergot, A logic-based calculus of events, New Gener. Comput. 4 (1986) 67–95.[63] H. van Ditmarsch, W. van der Hoek, B. Kooi, Dynamic Epistemic Logic, 1st edition, Springer Publishing Company, Incorporated, 2007.[64] H. van Ditmarsch, A. Herzig, T. De Lima, From situation calculus to dynamic epistemic logic, J. Log. Comput. 21 (2) (2011) 179–204.[65] M. Thielscher, Reasoning Robots: The Art and Science of Programming Robotic Agents, Appl. Log. Ser., Springer, Dordrecht, 2005.[66] M. Gelfond, V. Lifschitz, Representing action and change by logic programs, J. Log. Program. 17 (2–4) (1993) 301–321.[67] C.E. Alchourròn, P. Gärdenfors, D. Makinson, On the logic of theory change: partial meet contraction and revision functions, J. Symb. Log. 50 (1985) [68] B. Kooi, Probabilistic dynamic epistemic logic, J. Log. Lang. Inf. 12 (4) (2003) 381–408.[69] C. Baral, M. Gelfond, J.N. Rushton, Probabilistic reasoning with answer sets, Theory Pract. Log. Program. 9 (1) (2009) 57–144.[70] M. Thielscher, Planning with noisy actions (preliminary report), in: Proc. Australian Joint Conference on Artificial Intelligence, 2001, pp. 27–45.[71] Y. Martin, M. Thielscher, Integrating reasoning about actions and Bayesian networks, in: International Conference on Agents and Artificial Intelligence, [72] P. Mateus, A. Pacheco, J. Pinto, A. Sernadas, C. Sernadas, Probabilistic situation calculus, Ann. Math. Artif. Intell. 32 (1–4) (2001) 393–431.[73] C. Fritz, S.A. McIlraith, Computing robust plans in continuous domains, in: Proc. ICAPS, 2009, pp. 346–349.[74] C. Boutilier, R. Reiter, M. Soutchanski, S. Thrun, Decision-theoretic, high-level agent programming in the situation calculus, in: Proc. AAAI, 2000, 510–530.Valencia, Spain, 2009.pp. 355–362.[75] C. Boutilier, R. Reiter, B. Price, Symbolic dynamic programming for first-order MDPs, in: Proc. IJCAI, 2001, pp. 690–697.[76] C. Boutilier, T. Dean, S. Hanks, Decision-theoretic planning: structural assumptions and computational leverage, J. Artif. Intell. Res. 11 (1) (1999) 94.[77] S. Sanner, K. Kersting, Symbolic dynamic programming for first-order POMDPs, in: Proc. AAAI, 2010, pp. 1140–1146.[78] H. Grosskreutz, G. Lakemeyer, cc-Golog – a logical language dealing with continuous change, Log. J. IGPL 11 (2) (2003) 179–221.[79] C.S. Herrmann, M. Thielscher, Reasoning about continuous processes, in: AAAI/IAAI, vol. 1, 1996, pp. 639–644.[80] M. Fox, D. Long, Modelling mixed discrete-continuous domains for planning, J. Artif. Intell. Res. 27 (2006) 235–297.[81] N. Kushmerick, S. Hanks, D. Weld, An algorithm for probabilistic planning, Artif. Intell. 76 (1) (1995) 239–286.[82] H. Younes, M. Littman, PPDDL 1. 0: an Extension to PDDL for Expressing Planning Domains with Probabilistic Effects, Tech. Rep., Carnegie Mellon University, 2004.[83] S. Sanner, Relational Dynamic Influence Diagram Language (RDDL): Language Description, Tech. Rep., Australian National University, 2011.[84] V. Belle, H.J. Levesque, How to progress beliefs in continuous domains, in: Proc. KR, 2014.[85] V. Belle, H.J. Levesque, PREGO: an action language for belief-based cognitive robotics in continuous domains, in: Proc. AAAI, 2014.[86] R. Reiter, On knowledge-based programming with sensing in the situation calculus, ACM Trans. Comput. Log. 2 (4) (2001) 433–457.[87] G. Lakemeyer, H.J. Levesque, Cognitive robotics, in: Handbook of Knowledge Representation, Elsevier, 2007, pp. 869–886.