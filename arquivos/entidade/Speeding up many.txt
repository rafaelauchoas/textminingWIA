Artificial Intelligence 204 (2013) 22–29Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintSpeeding up many-objective optimization by Monte CarloapproximationsKarl Bringmann a, Tobias Friedrich b,∗, Christian Igel c, Thomas Voß da Max-Planck-Institut für Informatik, Saarbrücken, Germanyb Friedrich-Schiller-Universität Jena, Germanyc Department of Computer Science, University of Copenhagen, Denmarkd Institut für Neuroinformatik, Ruhr-Universität, Bochum, Germanya r t i c l ei n f oa b s t r a c tArticle history:Received 22 January 2013Received in revised form 26 July 2013Accepted 1 August 2013Available online 20 August 2013Keywords:Evolutionary algorithmMulti-objective optimizationPareto-front approximationHypervolume indicator1. IntroductionMany state-of-the-art evolutionary vector optimization algorithms compute the contribut-ing hypervolume for ranking candidate solutions. However, with an increasing number ofobjectives, calculating the volumes becomes intractable. Therefore, although hypervolume-based algorithms are often the method of choice for bi-criteria optimization, they areregarded as not suitable for many-objective optimization. Recently, Monte Carlo meth-ods have been derived and analyzed for approximating the contributing hypervolume.Turning theory into practice, we employ these results in the ranking procedure of themulti-objective covariance matrix adaptation evolution strategy (MO-CMA-ES) as an exam-ple of a state-of-the-art method for vector optimization. It is empirically shown that theapproximation does not impair the quality of the obtained solutions given a budget of ob-jective function evaluations, while considerably reducing the computation time in the caseof multiple objectives. These results are obtained on common benchmark functions as wellas on two design optimization tasks. Thus, employing Monte Carlo approximations makeshypervolume-based algorithms applicable to many-objective optimization.© 2013 Elsevier B.V. All rights reserved.Multi-objective optimization, also known as multi-criteria or vector optimization, is the basis of multiple criteria decisionmaking [19,22,32,33]. It is concerned with the optimization of vector-valued objective functions. The goal is to find or toapproximate the set of Pareto-optimal solutions. A solution is Pareto-optimal if it cannot be improved in one objectivewithout getting worse in another one. In recent years, it has become apparent that stochastic, population-based searchalgorithms such as evolutionary computing techniques are particularly well suited for solving vector optimization problems(e.g., see [15,16]).Multi-objective evolutionary algorithms (MOEAs) have become broadly accepted methods in multi-criteria decision mak-ing and multiple criteria mathematical programming. It is known that the performance of MOEAs tends to deterioratewith an increasing number of objectives [14]. This is a general problem of vector optimization algorithms. For few ob-jectives, MOEAs relying on the contributing hypervolume as the (second-level) sorting criterion are the methods of choice.These include the evolution strategy with probabilistic mutation for multi-objective optimization (ESP, [26]), the multi-objectivecovariance matrix adaptation evolution strategy (MO-CMA-ES, [27,28,34]), the SMS-EMOA [4], and variants of the indicator-based evolutionary algorithm (IBEA, [41]). Despite the progress in developing algorithms for hypervolume computation* Corresponding author.E-mail address: friedrich@uni-jena.de (T. Friedrich).0004-3702/$ – see front matter © 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2013.08.001K. Bringmann et al. / Artificial Intelligence 204 (2013) 22–2923(e.g., [3,6,7,10,21,38,40]), the computational complexity of calculating the contributing hypervolume prevents the broadapplication of these powerful MOEAs to objective functions with many (say, more than four) objectives.Recently, several approximation algorithms for determining the least hypervolume contributor of a given Pareto-front ap-proximation have been presented, for example, in [8,11] or as part of the HypE MOEA [2]. Because of the good performanceof HypE and encouraged by a preliminary study [36], we hypothesize that using such an approximation instead of the exactcontributing hypervolume will make the aforementioned MOEAs applicable to problems with many objectives and that theresulting algorithms will push the boundaries of MOEAs for many-objective optimization.In theory, the approximation allows for the application of hypervolume-based MOEAs to optimization problems with anarbitrary number of objectives. While there exist comparisons of approximation-based algorithms with other MOEAs [2],the effects of replacing the exact hypervolume calculation with an approximation algorithm on the overall performance ofMOEAs have not been investigated in isolation. Apart from [36], there has been no empirical comparison of state-of-the-art MOEAs in which the exact hypervolume computation has been replaced by an approximation while fixing the othercomponents of the algorithm.Against this background, we employ the approximation within the steady-state MO-CMA-ES while all other componentsare kept fixed to empirically investigate whether the Monte Carlo approximation is actually useful in practice. In our exper-iments, using approximations indeed considerably reduced the computation time in the case of multiple objectives withoutimpairing the quality of the obtained solutions.The remainder of the document is structured as follows. The next section introduces the problem of determining the leasthypervolume contributor. We briefly review results on Monte Carlo approximation of the least hypervolume contributor aswell as the exact hypervolume algorithm. Then, we present our empirical evaluation before concluding with the results ofour experiments and suggestions for future work.2. Vector optimization and the least hypervolume contributorWe consider multi-objective optimization problems of the form(cid:3)f : X → Rm,(cid:3)f (x) (cid:5)→(cid:2)(cid:3)f 1(x), . . . , fm(x),where X denotes the search space of the optimization problem and m refers to the number of objectives. Without loss ofgenerality, we assume that all objectives are to be minimized. The number of objectives of all considered test problemsis m (cid:2) 3. Pareto-dominance is the fundamental concept for comparing candidate solutions of a multi-objective optimization(cid:6)problem. The candidate solution xweakly dominates x and we write x(cid:6) (cid:7) x if∀i ∈ {1, . . . , m}: f i(cid:6)The solution x∃ j ∈ {1, . . . , m}: f jhold.(cid:3)(cid:2)(cid:6)x(cid:3) f i(x).(cid:2)(cid:3)(cid:6)x< f j(x)strictly dominates x and we write x(cid:6) ≺ x if additionallyUsing the notion of dominance, the goal of multi-objective optimization can be defined as finding or approximating theset(cid:4)(cid:6) =Xx ∈ X(cid:5)(cid:5) (cid:2)x(cid:6) ∈ X: x(cid:6)(cid:6) ≺ x⊆ X,which is called the Pareto-optimal set. The image of X(cid:6)under (cid:3)fis referred to as the corresponding Pareto-optimal front.The concept of dominance can be extended to sets. Let A and B be sets of candidate solutions. Then A weakly dominatesB and we write A (cid:7) B if every element in B is weakly dominated by at least one element in A.2.1. Evolutionary vector optimizationEvolutionary algorithms (EAs, [20]) are iterative direct search heuristics that maintain a set of μ candidate solutions,the so-called parent population. In each iteration, λ new offspring solutions are generated. Then a new parent populationis assembled from both the offspring and the former parent population. Candidate solutions with better objective functionvalues are preferentially selected. In the elitist EA considered in this study, the parent population of the next generationis formed by the best μ of the new solutions and their parents. This requires sorting the solutions. However, the Pareto-dominance relation does not establish a total order. Therefore, incomparable candidate solutions need to be sorted by aso-called second-level sorting criterion. Given two incomparable individuals a and b (i.e. neither a (cid:7) b nor b (cid:7) a holds) and aPareto front F , the second-level sorting criterion determines whether a or b is more valuable in the context of F (cf. [41]).The contributing hypervolume is one of the most popular second-level sorting criteria due to its attractive theoretical prop-erties, and it is deployed in most recent multi-objective evolutionary algorithms.24K. Bringmann et al. / Artificial Intelligence 204 (2013) 22–292.2. The contributing hypervolumeThe hypervolume measure or S-metric (see [42]) of a population A is the volume of the union of regions of the objectivespace which are dominated by A and bounded by some appropriately chosen reference point (cid:3)r ∈ Rm, that is,HYP( A) := VOL(cid:7) (cid:8)(cid:9)f 1(a), r1(cid:10)(cid:9)× · · · ×fm(a), rm,(cid:11)(cid:10)a∈ Awith VOL(·) being the Lebesgue measure. One of the unique features of the hypervolume indicator is that it is, up toweighting objectives, the only known indicator which is strictly Pareto compliant [43], that is, given two sets A and Bthe indicator values A higher than B if A dominates B. It has further been shown by Bringmann and Friedrich [13] thatthe worst-case approximation factor of all possible Pareto fronts obtained by any hypervolume-optimal set of fixed size μis asymptotically equal to the best worst-case approximation factor achievable by any set of size μ, namely Θ(1/μ) foradditive approximation and 1 + Θ(1/μ) for relative approximation. The authors have shown in [23] that by consideringa transformed variant of the hypervolume indicator, the logarithmic hypervolume indicator, a close-to-optimal multiplicativeapproximation ratio can be achieved. For these reasons, the hypervolume indicator is a popular second-level sorting criterionin many recent multi-objective evolutionary algorithms (MOEAs).When using the hypervolume as a second-level sorting criterion for comparing incomparable individuals, we measurethe respective contribution of each individual to the total hypervolume. The contributing hypervolume of an individual a ∈ Ais given by(cid:2)CON(a, A) := HYP( A) − HYPA \ {a}(cid:3).Note that the contributing hypervolume of a dominated individual is zero. Thus, in the following we assume that all domi-nated individuals have been removed from A before contributing hypervolumes are computed or estimated. The contributionCON(a, A) is an important measure since instead of using the hypervolume directly, most hypervolume-based algorithmssuch as the steady-state MO-CMA-ES or the SMS-EMOA remove, in the selection step, the individuala1 := argmina∈ A(cid:2)(cid:3)CON(a, A)contributing the least hypervolume to the population A.13. Computing the contributing hypervolumeIn this section, we summarize an exact algorithm and two approximation schemes for calculating the contributing hy-pervolume.3.1. Exact computationIn order to determine a1, the usual way is calculating HYP( A) and HYP( A \ {a}) for all a ∈ A. This can be done by(| A| + 1) hypervolume calculations with one of the many available hypervolume algorithms. Unfortunately, as the problem is#P-hard [9], none of them can run in time polynomial in m unless P = NP. In fact, assuming the widely believed exponentialtime hypothesis [30], the runtime of all algorithms computing the hypervolume must be | A|Ω(m) [12]. Note that this onlyholds in the worst-case. The average-case complexity can be polynomial in the number of objectives [12].Many algorithms have been present recently for calculating the hypervolume (e.g. [3,6,7,10,21,38,40]). We use the algo-rithm of Bringmann and Friedrich [10] which computes all contributions CON(a, A), a ∈ A, in only one pass. This saves afactor of | A| compared to most other hypervolume algorithms and gives a total runtime of O(| A|m/2 log(| A|)) to computeall | A| hypervolume contributions. For dimension m = 3 there is an even faster algorithm by Emmerich and Fonseca [21],which computes all hypervolume contributions in time O(| A| log(| A|)). As we are more interested in higher-dimensionalproblems (m (cid:2) 5), we use the algorithm of Bringmann and Friedrich [10] for all dimensions.3.2. Probably approximately correct approximationIn our experiments we want to compare this exact calculation of the hypervolume with an approximation algorithm.It is known that the hypervolume can be approximated very efficiently by an FPRAS (fully polynomial-time randomizedapproximation scheme) [9]. Unfortunately, an approximation of the hypervolume does not yield an approximation of CON.Even worse, CON(a, A) is not only #P-hard to calculate exactly, it is also NP-hard to approximate by a factor of 2m1−εfor all1 To be precise, the algorithms consider not the whole population A but the subset A(cid:6)sorting [17]. This set is constructed iteratively starting from Aremoved from Ahypervolume to A. This removal process is iterated until the remaining set A(cid:6)is removed from the population A.(cid:6)(cid:6)(cid:6) = A as follows. If Aof individuals having the worst rank w.r.t. non-dominatedcontains dominated individuals, all non-dominated individuals aredoes not contain dominated individuals, and the individual contributing least(cid:6)K. Bringmann et al. / Artificial Intelligence 204 (2013) 22–2925ε > 0 [8,11]. Though CON is therefore not approximable in time polynomial in | A| and m, there are still a few approximationalgorithms for CON (see [2,8,11]). The approximation algorithms presented in [2,8,11] are Monte Carlo algorithms basedon different sampling techniques. The algorithm HypE [2] is a MOEA using hypervolume estimations at a user-specifiedconfidence level to guide the search. However, here we want to compare a standard MOEA with and without hypervolumeapproximation to study the effects of the approximation. For this, we use the approximation algorithm of Bringmann andFriedrich [8,11]. We now briefly describe this approach, which we will later improve in Section 3.3. This Monte Carloalgorithm returns, for a population A and arbitrary small ε, δ > 0, an individual (cid:12)a1 with the property(cid:2) 1 − δ.(cid:9)(cid:10)CON((cid:12)a1, A) (cid:3) (1 + ε) CON(a1, A)Pr(1)The algorithm samples in the minimal bounding boxes of all contributions and conducts a race between the differentcandidates until an individual (cid:12)a1 is found which (with high probability) has a contribution very close to the hypervolumecontribution of a1.The runtime of this algorithm is bounded by O(m | A| (| A| + H)), where H is a measure of hardness of the instance. It ispolynomial in m and | A| for most practical instances, but unbounded in the worst-case. More precisely, H is defined as(cid:7)H :=BB(a1, A)CON(a2, A) − CON(a1, A)(cid:11)2+(cid:7)(cid:13)a1(cid:13)=a∈ ABB(a, A)CON(a, A) − CON(a1, A)(cid:11)2,(2)where a1 denotes the individual with the smallest contribution, a2 denotes the individual with the second smallest contri-bution, and BB(a, A) denotes the volume of the smallest bounding box of the contribution CON(a, A). By definition, H isunbounded and can even be undefined if there is no unique least contributor. However, in such cases an abortion criterionbounds the runtime. In general, H is small if BB(a, A) ≈ CON(a, A) and CON(a1, A) (cid:15) CON(a, A) for all a ∈ A \ {a1}. Onthe other hand, H is large if either (i) there is an individual with a large bounding box BB(a, A) but a small contributionCON(a, A), or (ii) there are two or more boxes contributing the minimal contribution or only slightly more than it, that is,for all CON(a, A) − CON(a1, A), a (cid:13)= a1, is very small.3.3. Fast approximate computationAs has been pointed out, approximating the least hypervolume contributor is an NP-hard problem [8,11]. For fixed errorbounds and error probabilities, the above described approximation scheme can degenerate to an exponential runtime. Forguiding the search in a randomized search heuristic this seems inappropriate. Despite the fact that the approximationalgorithm is reported to have a very fast empirical average-case performance [8,11], we observed that difficult situations doindeed occur for typical benchmark problems, and sometimes very many samples are needed to achieve the specified errorbound and error probability. These slow instances have a very large hardness value H (see Eq. (2)), for example, becausethe contribution is extremely small compared to the bounding box and most samples do not lie in the contribution. This isunavoidable for a probably approximately correct approximation, but undesirable for a practical optimization algorithm.To address these situations, we propose a heuristic that stops the overall selection process whenever a certain thresholdof total samples has been reached. On early stopping, the current estimate of the least contributor is considered furtherby the selection scheme. Note that, in contrast to the approximation scheme described in Section 3.2, this algorithm is notanymore probably approximately correct with parameters ε and δ (cf. Eq. (1)), but it results in a very fast algorithm whichstill gives competitive results with respect to the quality of the final Pareto-front approximation.This new approximation scheme comes with an additional threshold parameter. In order to determine a good value forthis parameter, we conducted a preliminary study of the threshold parameter for the DTLZ benchmark set [18]. As a testbed,we chose the objective function DTLZ 2 with eight objectives and considered a maximum number of 103, 104, 105 and 106samples for the hypervolume approximation scheme. We conducted 25 independent trials for every parameter setting andanalyzed the quality of the resulting Pareto-front approximations in terms of the absolute hypervolume indicator. Moreover,we recorded the required running time for every parameter setup.The results of the parameter study are presented in Fig. 1. Limiting the number of samples directly affects the runningtime of the selection scheme, and thus, of the overall algorithm. For a threshold of 103 and 104 samples, respectively, thequality of the resulting Pareto-front approximations is negatively affected and the absolute hypervolume starts to fluctuateas it nears the Pareto-optimal front (see Fig. 1, bottom). In case of higher thresholds (105 and 106 samples), the qualityremains stable and is on par with the quality obtained when considering the approximation scheme without a samplethreshold. We therefore use sample threshold 105 for our fast approximation algorithm.4. Empirical evaluationWe compared the two approximated hypervolume indicators to the exact hypervolume indicator w.r.t. the influence onthe performance (in terms of the quality of the final Pareto-front approximation) and the running time of MOEAs. To thisend, we deploy all three indicators in the steady-state variant of the multi-objective covariance matrix evolution strategy(MO-CMA-ES, [27,34,37]). However, we expect the results to carry over to any MOEA also relying on the hypervolumeindicator as second-level sorting criterion.26K. Bringmann et al. / Artificial Intelligence 204 (2013) 22–29Fig. 1. Comparison of different sample thresholds (103:) for the benchmark function DTLZ 2 with eight objectives. Both thequality of the Pareto-front approximations (in terms of the absolute hypervolume) and the runtime requirements for the different parameter settings areshown as the median of 25 independent trials., 104:, 106:, 105:4.1. MO-CMA-ESThe MO-CMA-ES relies on the Pareto-dominance relation and a second-level sorting criterion for selection, which isstate-of-the-art in MOEAs since [17]. The algorithm builds on the principles of the single-objective covariance matrix adap-tation strategy (CMA-ES, [24,31,34]), which is a variable metric algorithm adapting the shape and strength of its Gaussiansearch distribution. The claim that the “CMA-ESs represent the state-of-the-art in evolutionary optimization in real-valuedRn search spaces” [5] is backed up by many performance comparisons across different suites of benchmark problems (e.g.,see the competition results in [1,25]). Here, we use the most recent variant of the (μ + 1)-MO-CMA-ES presented in [37].For empirical evaluations of the MO-CMA-ES see, for example, [27,28,37].4.2. Experimental setupWe compared the MO-CMA-ES using an approximation of the least hypervolume contributor to the results of the originalMO-CMA-ES with exact hypervolume computation. More precisely, we compare the following indicators:• Exact computation as described in Section 3.1.• Probably approximately correct as described in Section 3.2.• Fast approximation with sample threshold 105 as described in Section 3.3.This allows us to isolate the influence of the indicator by altering only the environmental selection procedure. We appliedthe algorithms to several classes of benchmark functions that are scalable to an arbitrary number of objectives m. Weconsidered the seven constrained functions DTLZ 1–7 [18] with search space dimension 30. The number of objectives waschosen to be 3, 5, and 7. In addition, we considered two real-world many-objective optimization problems. The first oneK. Bringmann et al. / Artificial Intelligence 204 (2013) 22–2927Table 1Experimental results for DTLZ benchmark set obtained by the MO-CMA-ES with exact and two approximate calculations of the hypervolume indicator after50,000 objective function evaluations. The differences in achieved hypervolumes were not statistically significant according to a Wilcoxon rank-sum testat a confidence level of α = 0.05 for all test functions except DTLZ 7 with 7 objectives. The median running time that each of the algorithms required tocomplete one independent trial is listed and the average overall speed-up is summarized.ExactcomputationProbably approximatelycorrect approximationFast approximation withsample threshold 105DTLZ 1DTLZ 1DTLZ 1DTLZ 2DTLZ 2DTLZ 2DTLZ 3DTLZ 3DTLZ 3DTLZ 4DTLZ 4DTLZ 4DTLZ 5DTLZ 5DTLZ 5DTLZ 6DTLZ 6DTLZ 6DTLZ 7DTLZ 7DTLZ 7AirfoilPump(3 objectives)(5 objectives)(7 objectives)(3 objectives)(5 objectives)(7 objectives)(3 objectives)(5 objectives)(7 objectives)(3 objectives)(5 objectives)(7 objectives)(3 objectives)(5 objectives)(7 objectives)(3 objectives)(5 objectives)(7 objectives)(3 objectives)(5 objectives)(7 objectives)(6 objectives)(8 objectives)7 min4 h 2 min3 d 19 h 12 min14 min18 h 6 min23 d 0 h 43 min3 min54 min5 d 4 h 48 min8 min2 h 20 min3 d 16 h 48 min32 min5 h 14 min9 d 3 h 24 min15 h 46 min6 d 19 h 40 min41 d 18 h 14 min4 min2 h 44 min8 d 21 h 7 min6 d 12 h 57 min>100 d16 min24 min21 h 21 min1 min31 min1 d 11 h 44 min1 min21 min9 h 50 min1 min41 min2 d 3 h 4 min1 min34 min20 h 18 min1 min43 min1 d 3 h 1 min1 min36 min1 d 5 h 23 min1 d 5 h 45 min8 d 2 h 13 min(2× slower)(10× faster)(4.3× faster)(14× faster)(35× faster)(16× faster)(3× faster)(2.6× faster)(13× faster)(8× faster)(3.4× faster)(1.7× faster)(32× faster)(9.2× faster)(11× faster)(946× faster)(228× faster)(37× faster)(4× faster)(4.6× faster)(7.3× faster)(5.3× faster)(>12× faster)16 min24 min13 h 31 min1 min31 min17 h 53 min1 min21 min8 h 36 min1 min41 min2 h 3 min1 min34 min29 min1 min43 min23 min1 min35 min(10 h 7 min)23 h 34 min4 d 19 h 27 min(2× slower)(10× faster)(6.7× faster)(14× faster)(35× faster)(31× faster)(3× faster)(2.6× faster)(15× faster)(8× faster)(3.4× faster)(43× faster)(32× faster)(9.2× faster)(454× faster)(946× faster)(228× faster)(2614× faster)(4× faster)(4.7× faster)(21× faster)(6.7× faster)(>21× faster)deals with the design of 2D airfoil shapes that are encoded by 10 real-valued parameters, the so-called PARSEC 10 parameterset. An airfoil shape is then evaluated with respect to six objectives by means of a computational fluid dynamics simulation(see [39]). The second one considers the optimization of centrifugal pump designs with respect to eight objectives (see [35]).For all experiments, the number of parent individuals was set to μ = 50. We conducted 25 independent trials with−2 and δ =50,000 objective function evaluations each. For both approximation algorithms, we used the parameters ε = 10−2 (cf. Eq. (1)). The experiments were carried out on a cluster of technically equivalent workstations with 2.93 GHz Intel10Quad-Core Xeon processors running Linux. We used the GNU compiler chain and enabled compiler optimizations accordingto the following command line arguments: -O3 -ffast-math -msse4 -mtune=core2. The reported runtimes referto the overall CPU times.We monitored the performance of the algorithms after every 5,000th objective function evaluation and carriedout the statistical evaluation after 25,000 and 50,000 function evaluations. We relied on the hypervolume indica-tor to compare the Pareto-front approximations obtained by the three optimizers.In case of the benchmark func-tions DTLZ 1–7 and the centrifugal pump design problem, the reference point was determined from the union ofall Pareto-front approximations. In case of the airfoil shape optimization problem, we chose the reference point (cid:3)r =(0.00516, 0.00606, 0.00982, 0.30806, 0.92314, 0.65460) as suggested in [39]. We applied the statistical evaluation proce-dure described in [37] to evaluate our experiments employing the Wilcoxon rank-sum test to verify statistical significance.All experiments were implemented using the Shark machine learning library [29], which is available online.4.3. ResultsTable 1 summarizes our experimental results. In terms of the achieved hypervolume, the MO-CMA-ES relying on anapproximation of the least hypervolume contributor performed at least on par with the variant employing the exact hyper-volume indicator across the set of benchmark functions. More precisely, the differences in achieved hypervolumes were notstatistically significant according to a Wilcoxon rank-sum test at a confidence level of α = 0.05 for all test functions exceptDTLZ 7 with 7 objectives. The final objective values are therefore omitted in Table 1.Table 1 shows clearly that the MO-CMA-ES achieves an enormous speed-up if the indicator is only approximated, espe-cially for more than 4 dimensions. The speed-up is the largest for the heuristically improved approximation introduced inSection 3.3 of this paper. However, we also observe that for very complicated test functions such as the disconnected frontsof DTLZ 7 in 7 dimensions, a sample threshold of 105 might not suffice. Here, we observe a tradeoff between speed (samplethreshold 105) and quality (probably approximately correct computation).28K. Bringmann et al. / Artificial Intelligence 204 (2013) 22–29), with approximate indicatorFig. 2. Achieved absolute hypervolume (the higher the better) for the objective function DTLZ 3 with seven objectives for the MO-CMA-ES with exactindicator (. Shown are the medians and thecorresponding lower and upper quartiles. The areas between both quartiles are shaded. Note that the 75% percentile and median are very close. In theupper plot, where the x-axis is the number of fitness function evaluations, the two algorithms behave similarly. After roughly 25,000 fitness evaluationsthey are very close to the Pareto front. The lower plot shows the hypervolume depending on the actual running time on a logarithmic scale, demonstratingthe performance gain by using the Monte Carlo approximations. Plots for all other objective functions with five or more objectives look similar.and with its heuristic variant with sample threshold 105Fig. 2 illustrates the typical behavior of the algorithms as a function of time and number of objective function evaluations.Shown are the medians over the 25 trials and the corresponding lower and upper quartiles (i.e., 25th and 75th percentiles).As expected, the hypervolume as a function of the number of objective function evaluations behaves similarly. On theother hand, both variants with approximated hypervolume finished all 50,000 objective functions evaluations in less than10 hours, while the exact version needed more than 10 hours for only the first 5000 objective function evaluations.A closer look at Fig. 2 reveals that the algorithms using the Monte Carlo approximation perform even a bit better.The final medians are slightly higher, and, more prominently, the lower quartiles are larger. That is, there are fewer trialsreaching only low hypervolume values within the given budget. A possible explanation might be that the approximationadds a little noise to the otherwise deterministic selection operator, which turns out to be beneficial for the evolutionaryprocess.5. ConclusionsWe empirically investigated the effects of replacing the exact hypervolume indicator with two different Monte Carloapproximations on the performance of multi-objective evolutionary algorithms (MOEAs). We evaluated whether a state-of-the-art MOEA relying on hypervolume-indicator-based selection is affected by the potential errors made by approximatingthe least hypervolume contributor. The results show that the performance of the algorithms in terms of the quality ofthe Pareto-front approximation given a budget of objective function evaluations does not suffer from the additional noiseintroduced by the Monte Carlo approximation. In some trials, the approximation, which introduces noise into the other-K. Bringmann et al. / Artificial Intelligence 204 (2013) 22–2929wise deterministic, greedy indicator-based selection scheme, led to better performance. We observed a vast reduction ofthe running time even for few objectives, when relying on the approximation scheme. In general, the higher the numberof objectives the more pronounced the performance advantage of using Monte Carlo approximation becomes. Hence, byemploying Monte Carlo approximations, hypervolume-based MOEAs become applicable to many-objective optimization.References[1] A. Auger, N. Hansen, Performance evaluation of an advanced local search evolutionary algorithm, in: Proceedings of the IEEE Congress on EvolutionaryComputation (CEC), IEEE Press, 2005, pp. 1777–1784.[2] J. Bader, E. Zitzler, HypE: An algorithm for fast hypervolume-based many-objective optimization, Evol. Comput. 19 (1) (2011) 45–76.[3] N. Beume, S-metric calculation by considering dominated hypervolume as Klee’s measure problem, Evol. Comput. 17 (4) (2009) 477–492.[4] N. Beume, B. Naujoks, M. Emmerich, SMS-EMOA: Multiobjective selection based on dominated hypervolume, Eur. J. Oper. Res. 181 (3) (2007)1653–1669.[5] H.-G. Beyer, Evolution strategies, Scholarpedia 2 (8) (2007) 1965.[6] L. Bradstreet, L. While, L. Barone, A fast incremental hypervolume algorithm, IEEE Trans. Evol. Comput. 12 (6) (2008) 714–723.[7] K. Bringmann, An improved algorithm for Klee’s measure problem on fat boxes, Comput. Geom. 45 (5–6) (2012) 225–233.[8] K. Bringmann, T. Friedrich, Approximating the least hypervolume contributor: NP-hard in general, but fast in practice, in: Proceedings of the 5thInternational Conference on Evolutionary Multi-Criterion Optimization (EMO), in: LNCS, vol. 5467, Springer, 2009, pp. 6–20.[9] K. Bringmann, T. Friedrich, Approximating the volume of unions and intersections of high-dimensional geometric objects, Comput. Geom. 43 (2010)601–610.[10] K. Bringmann, T. Friedrich, An efficient algorithm for computing hypervolume contributions, Evol. Comput. 18 (3) (2010) 383–402.[11] K. Bringmann, T. Friedrich, Approximating the least hypervolume contributor: NP-hard in general, but fast in practice, Theor. Comput. Sci. 425 (2012)104–116.[12] K. Bringmann, T. Friedrich, Parameterized average-case complexity of the hypervolume indicator, in: Proceedings of the 15th Annual Conference onGenetic and Evolutionary Computation Conference (GECCO), ACM Press, 2013, pp. 575–582.[13] K. Bringmann, T. Friedrich, Approximation quality of the hypervolume indicator, Artif. Intell. 195 (2013) 265–290.[14] D. Brockhoff, Theoretical aspects of evolutionary multiobjective optimization, in: A. Auger, B. Doerr (Eds.), Theory of Randomized Search Heuristics:Foundations and Recent Developments, World Scientific Publishing, 2011, pp. 101–139.[15] C. Coello Coello, G.B. Lamont, D.A. van Veldhuizen, Evolutionary Algorithms for Solving Multi-Objective Problems, 2nd edition, Springer, 2007.[16] K. Deb, Multi-Objective Optimization using Evolutionary Algorithms, Wiley, 2001.[17] K. Deb, A. Pratap, S. Agarwal, T. Meyarivan, A fast and elitist multiobjective genetic algorithm: NSGA-II, IEEE Trans. Evol. Comput. 6 (2002) 182–197.[18] K. Deb, L. Thiele, M. Laumanns, E. Zitzler, Scalable multi-objective optimization test problems, in: Proceedings of the Congress on Evolutionary Com-putation (CEC), IEEE Press, 2002, pp. 825–830.[19] M. Ehrgott, Multicriteria Optimization, 2nd edition, Springer, 2010.[20] A.E. Eiben, J.E. Smith, Introduction to Evolutionary Computing. Natural Computing, Springer, 2008.[21] M.T.M. Emmerich, C.M. Fonseca, Computing hypervolume contributions in low dimensions: Asymptotically optimal algorithm and complexity results,in: Proceedings of the Evolutionary Multi-Criterion Optimization (EMO), in: LNCS, vol. 6576, Springer, 2011, pp. 121–135.[22] J. Figueira, S. Greco, M. Ehrgott (Eds.), Multiple Criteria Decision Analysis: State of the Art Surveys, International Series in Operations Research &Management Science, vol. 78, Springer, 2004.[23] T. Friedrich, K. Bringmann, T. Voß, C. Igel, The logarithmic hypervolume indicator, in: H. Beyer, W.B. Langdon (Eds.), Proceedings of the 11th Interna-tional Workshop on Foundations of Genetic Algorithms (FOGA), ACM Press, 2011, pp. 81–92.[24] N. Hansen, A. Ostermeier, Completely derandomized self-adaptation in evolution strategies, Evol. Comput. 9 (2) (2001) 159–195.[25] N. Hansen, A. Auger, R. Ros, S. Finck, P. Pošík, Comparing results of 31 algorithms from the black-box optimization benchmarking BBOB-2009, in:Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation Conference (GECCO), ACM, 2010, pp. 1689–1696.[26] S. Huband, P. Hingston, L. While, L. Barone, An evolution strategy with probabilistic mutation for multi-objective optimisation, in: Proceedings of theIEEE Congress on Evolutionary Computation (CEC), vol. 4, 2003, pp. 2284–2291.[27] C. Igel, N. Hansen, S. Roth, Covariance matrix adaptation for multi-objective optimization, Evol. Comput. 15 (1) (2007) 1–28.[28] C. Igel, T. Suttorp, N. Hansen, Steady-state selection and efficient covariance matrix update in the multi-objective CMA-ES, in: Proceedings of the 4thInternational Conference on Evolutionary Multi-Criterion Optimization (EMO), in: LNCS, vol. 4403, Springer, 2007, pp. 171–185.[29] C. Igel, T. Glasmachers, V. Heidrich-Meisner, Shark, J. Mach. Learn. Res. 9 (2008) 993–996.[30] R. Impagliazzo, R. Paturi, The complexity of k-SAT, in: Proceedings of the 14th IEEE Conference on Computational Complexity (CCC), 1999, pp. 237–240.[31] S. Kern, S. Müller, N. Hansen, D. Büche, J. Ocenasek, P. Koumoutsakos, Learning probability distributions in continuous evolutionary algorithms –A comparative review, Nat. Comput. 3 (2004) 77–112.[32] K. Miettinen, Nonlinear Multiobjective Optimization, Kluwer’s International Series in Operations Research & Management Science, vol. 12, KluwerAcademic Publishers, 1999.[33] Y. Sawaragi, H. Nakayama, T. Tanino, Theory of Multiobjective Optimization, Mathematics in Science and Engineering, vol. 176, Academic Press, 1985.[34] T. Suttorp, N. Hansen, C. Igel, Efficient covariance matrix update for variable metric evolution strategies, Mach. Learn. 75 (2) (2009) 167–197.[35] R.K. Ursem, Centrifugal pump design: Three benchmark problems for many-objective optimization, Technical Report 2010-01, Grundfos ManagementA/S, 2010.[36] T. Voß, T. Friedrich, K. Bringmann, C. Igel, Scaling up indicator-based MOEAs by approximating the least hypervolume contributor: A preliminary study,in: Proceedings of the Genetic and Evolutionary Computation Conference (GECCO): Workshop on Theoretical Aspects of Evolutionary MultiobjectiveOptimization, ACM Press, 2010, pp. 1975–1978.[37] T. Voß, N. Hansen, C. Igel, Improved step size adaptation for the MO-CMA-ES, in: Proceedings of the 12th Annual Conference on Genetic and Evolu-tionary Computation Conference (GECCO), ACM Press, 2010, pp. 487–494.[38] L. While, L. Bradstreet, L. Barone, A fast way of calculating exact hypervolumes, IEEE Trans. Evol. Comput. 16 (1) (2012) 86–95.[39] U.K. Wickramasinghe, R. Carrese, X. Li, Designing airfoils using a reference point based evolutionary many-objective particle swarm optimization, in:Proceedings of the IEEE Congress on Evolutionary Computation, IEEE Press, 2010, pp. 1857–1864.[40] H. Yıldız, S. Suri, On Klee’s measure problem for grounded boxes, in: Proceedings of the ACM Symposium on Computational Geometry (SoCG), 2012,pp. 111–120.[41] E. Zitzler, S. Künzli, Indicator-based selection in multiobjective search, in: Proceedings of the International Conference on Parallel Problem Solving fromNature (PPSN), in: LNCS, vol. 3242, Springer, 2004, pp. 832–842.[42] E. Zitzler, L. Thiele, Multiobjective evolutionary algorithms: A comparative case study and the strength Pareto approach, IEEE Trans. Evol. Comput. 3 (4)(1999) 257–271.[43] E. Zitzler, L. Thiele, M. Laumanns, C.M. Fonseca, V. Grunert da Fonseca, Performance assessment of multiobjective optimizers: An analysis and review,IEEE Trans. Evol. Comput. 7 (2) (2003) 117–132.