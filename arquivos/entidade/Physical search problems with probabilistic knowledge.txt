Artificial Intelligence 196 (2013) 26–52Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintPhysical search problems with probabilistic knowledge ✩Noam Hazon a,∗,1, Yonatan Aumann b, Sarit Kraus b, David Sarne ba Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USAb Department of Computer Science, Bar-Ilan University, Ramat Gan, Israela r t i c l ei n f oa b s t r a c tArticle history:Received 31 August 2011Received in revised form 30 September2012Accepted 24 December 2012Available online 3 January 2013Keywords:Graph searchEconomic searchThis paper considers the problem of an agent or a team of agents searching for a resourceor tangible good in a physical environment, where the resource or good may possiblybe obtained at one of several locations. The cost of acquiring the resource or good ata given location is uncertain (a priori), and the agents can observe the true cost onlywhen physically arriving at this location. Sample applications include agents in explorationand patrol missions (e.g., an agent seeking to find the best location to deploy sensingequipment along its path). The uniqueness of these settings is in that the cost of observinga new location is determined by distance from the current one, impacting the considerationfor the optimal search order. Although this model captures many real world scenarios, ithas not been investigated so far.We analyze three variants of the problem, differing in their objective: minimizing the totalexpected cost, maximizing the success probability given an initial budget, and minimizingthe budget necessary to obtain a given success probability. For each variant, we firstintroduce and analyze the problem with a single agent, either providing a polynomialsolution to the problem or proving it is NP-complete. We also introduce a fully polynomialtime approximation scheme algorithm for the minimum budget variant. In the multi-agentcase, we analyze two models for managing resources, shared and private budget models.We present polynomial algorithms that work for any fixed number of agents, in the sharedor private budget model. For non-communicating agents in the private budget model, wepresent a polynomial algorithm that is suitable for any number of agents. We also analyzethe difference between homogeneous and heterogeneous agents, both with respect to theirallotted resources and with respect to their capabilities. Finally, we define our problem inan environment with self-interested agents. We show how to find a Nash equilibrium inpolynomial time, and prove that the bound on the performance of our algorithms, withrespect to the social welfare, is tight.© 2013 Elsevier B.V. All rights reserved.1. IntroductionFrequently, in order to successfully complete its task, an agent may need to explore (i.e., search) its environment andchoose among different available options. For example, an agent seeking to purchase a product over the Internet needs toquery several electronic merchants in order to learn their posted prices; a robot searching for a resource or a tangible goodneeds to travel to possible locations where the resource is available and learn the configuration in which it is available, as✩This paper extends two earlier conference papers (Aumann et al., 2008 [6]; Hazon et al., 2009 [31]).* Corresponding author.E-mail addresses: noamh@cs.cmu.edu (N. Hazon), aumann@cs.biu.ac.il (Y. Aumann), sarit@cs.biu.ac.il (S. Kraus), sarned@cs.biu.ac.il (D. Sarne).1 This work was done while the author was at Bar-Ilan University.0004-3702/$ – see front matter © 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.12.003N. Hazon et al. / Artificial Intelligence 196 (2013) 26–5227well as the difficulty of obtaining it there. In these environments, the benefit associated with an opportunity is revealedonly upon observing it. The only knowledge available to the agent prior to observing the opportunity is the probabilityassociated with each possible value of each prospect.While in virtual environments the exploration can sometimes be considered costless, in physical environments travelingand observing typically entails a cost. Furthermore, traveling to a new location may increase or decrease the distance toother locations, so the cost associated with exploring other unexplored locations changes. For example, consider a Roverrobot with the goal of mining a certain mineral. Potential mining locations may be identified based on satellite imaging,each location associated with some uncertainty regarding the difficulty of mining there. In order to assess the amount ofbattery power required for mining at a specific location, the robot needs to physically visit there. The robot’s battery isthus used not only for mining the mineral but also for traveling from one potential location to another. Consequently, anagent’s strategy in an environment associated with search costs should maximize the overall benefit resulting from thesearch process, defined as the value of the option eventually used, minus the costs accumulated along the process, ratherthan merely finding the best valued option.In physical environments, it is common to use a team of agents rather than a single agent. Extending the single agentsolution to multi-agent strategy may require subdividing the search space among the different agents. However, if agentshave means of communication, then they may not wish to become too distant, as they can call upon each other for assis-tance. For example, even if a Rover does not have sufficient battery power for mining at a given location, it may be usefulfor it to travel to the site in order to determine the exact mining cost, and call for other robots that do have the necessarybattery power. In this case, the scheduling of the robots’ travel times is key, and must be carefully planned. If the agents arenot fully cooperative, a selfish behavior should also be considered. Each one of the agents will try to minimize its travelingcosts while still achieving the group’s goal.Finally, agents may be of different types, or with different amounts of resources. For example, Rover robots may beentering the mission with differing initial battery charges. They may also differ in their capabilities, like a team of Rovers inwhich some were specifically designed for mining missions, and thus require less battery power for the same mining task.This paper aims at taking the first steps in understanding the characteristics of such physical search environments, bothfor the single and multi agent cases, and developing efficient exploration strategies for the like. Our main focus is on the casewhere the opportunities are aligned along a path, as in the case of perimeter patrol [60,19,2,3]. We note that many singleand multi-agent coverage algorithms convert their complex environment into a simple long path [52,25,32]. Furthermore,we show that the problem in more general metric spaces is NP-complete, even for a tree graphs. For exposition purposes,in the remainder of the paper we use the classical procurement application where the goal of the search is purchasing aproduct and the value of each observed opportunity represents a price. Of course, this is only one example of the generalsetting of exploration in a physical environment, and the discussion and results of this paper are relevant to any suchsetting, provided that exploration and fulfilling the task consume the same type of resource.We consider three variants of the problem, differing in their objective. The first (Min-Expected-Cost) is the problem ofan agent that aims to minimize the expected total cost of completing its task. The second (Max-Probability) considers anagent that is given a budget for the task (which it cannot exceed) and aims to maximize the probability it will completethe task (e.g., reach at least one opportunity with a budget large enough to successfully buy the product). In the last variant(Min-Budget) the agent is required to guarantee a pre-defined probability of completing the task, and aims to minimize theoverall budget that will be required to achieve the said success probability. We also consider the multi-agent extensionsof these variants. While the first variant fits mostly product procurement applications, the two latter variants fit well intoapplications of robots engaged in remote exploration, operating with a limited amount of battery power (i.e., a budget).1.1. Summary of resultsWe first consider the single agent case. We prove that in general metric spaces all three problem variants are NP-hard.Thus, as mentioned, we focus on the setting where all locations are located along a path. For this setting we provide poly-nomial algorithms for the Min-Expected-Cost problem. We show the other two problems (Min-Budget and Max-Probability)to be NP-complete even for the path. Thus, we consider further restrictions and also provide an approximation scheme. Weshow that both problems are polynomial if the number of possible prices is constant. Even with this restriction, we showthat these problems are NP-complete on a tree graph. For the Min-Budget problem, we provide an FPTAS (fully-polynomial-time-approximation-scheme), that provides a (1 + (cid:2)) approximation for any (cid:2) > 0, in time O (poly(n(cid:2)−1)), where n is thesize of the input.For the multi-agent case, we first analyze a shared budget model, where all the resources and costs are shared amongall the agents. We show that if the number of agents is fixed, then all of the single-agent algorithms extend to k-agents,with the time bounds growing exponentially in k. Therefore the computation of the agents’ strategies can be performedwhenever the number of agents is relatively moderate, a common scenario in many physical environments where severalagents cooperate in exploration and search. If the number of agents is part of the input then the multi-agent versions ofMin-Budget and Max-Probability are NP-complete even on the path and even with a single price.We then investigate a model of private budgets, where each agent has its own initial budget. We again assume thatthe number of possible prices is bounded. In this case, we separately consider the setting where agents can communicateand the setting where they cannot. For non-communicating agents we show a polynomial algorithm for the Max-Probability28N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52Table 1Summary of results: n is the input size, m – the number of points (store locations), d – the number of different possible prices, w.p. (with phone) – theversion of Min-Expected-Cost with the ability to purchase by phone (see Section 1.3), ¯d = d + 1, k – the number of agents, n/a – the problem was not definedin that case or there is no need for a solution, f – the polynomial function which is defined in Lemma 22.General metric spacesTreesPathSingle priced pricesGeneral case(1 + (cid:2)) approximationk agentsGeneral case(1 + k(cid:2)) approximationNo-communicationk fixedk parameterWith-communicationk fixedk parameter(a) Single agentMin-Expected-CostNP-hardn/aO (d3m4) (O (d2m2) w.p.)O (d3m4) (O (d2m2) w.p.)n/aMax-ProbabilityNP-completeNP-completeO (m)O (m2d( e·mNP-complete2d )2d)(b) Multi-agent, shared budget, on the pathMin-Expected-CostO (d2k+12k( mk )4k) (O (d22k( mk )2k) w.p.)Max-Probability2kd )2kd)O (m2kd( emNP-completen/a(c) Multi-agent, private budget, on the pathMin-BudgetNP-completeNP-completeO (m)O (m2d( e·mNP-completeO (n(cid:2)−6)2d )2d)Min-Budget2kd )2kd)O (m2kd( emNP-completeO (n(cid:2)−6k)Max-ProbabilityMin-BudgetidenticalMin-BudgetdistinctO (m3k2)O (m3k2)O (m3k2n)O (m3k2n)(cid:2)(cid:2)m2k¯dfem2¯d(cid:3)2k¯d, k, d, k(cid:3)∈ PO (m2kd( emNP-complete2d )2kd)problem that is suitable for any number of agents. For the Min-Budget problem with non-communicating agents, we presenta polynomial algorithm for the case that all agents must be allotted identical resources, but show that the problem is NP-hard for the general case (unless the number of agents is fixed). Next we consider agents that can communicate, and cancall upon each other for assistance. As noted above, in this case the scheduling of the different agents’ moves must also tobe carefully planned. We present polynomial algorithms for both the Max-Probability problem and the Min-Budget problemthat work for any constant number of agents (but become non-polynomial when the number of agents is not constant).We then move to the self-interested agents setting, where the agents seek to obtain an item but each one of them tries tominimize the use of its own private budget for traveling. We define two games, a sequential game, Min-Expected-Cost-Game,and simultaneous game, Min-Budget-Game. We show that when the number of possible prices is bounded and the numberof agents is fixed, the strategy that maximizes the social welfare can be found in polynomial time. We also show that inMin-Budget-Game this strategy is a Nash equilibrium, but this is not always the case in Min-Expected-Cost-Game. We thuspresent a polynomial algorithm that guarantees finding a strategy which is a Nash equilibrium. Furthermore, we show anupper bound on the algorithm’s performance, and prove that it is tight.Finally, we extend our results to the case of heterogeneous agents with different capabilities, and discuss the assumptionsthat we made throughout the paper. Table 1 presents a summary of the results. Empty entries represent open problems.1.2. Related workModels of a single agent search process with prior probabilistic knowledge have attracted the attention of many re-searchers in various areas, mainly in economics and operational research, prompting several reviews over the years [40,42].These search models have developed to a point where their total contribution is referred to as search theory. Probably themost famous problem within this field is the “secretary problem”, which has a remarkably long list of articles that havebeen dedicated to its variations (see [22] for an extensive bibliography).2 Nevertheless, these economic-based search mod-els, as well as their extensions over the years into multi-agent environments [35,46,36,48], assume that the cost associatedwith observing a given opportunity is stationary (i.e., does not change along the search process). While this permissiveassumption facilitates the analysis of search models, it frequently does not capture the real situation in the physical world,as illustrated in Fig. 1. Therefore, in this paper, we assume that the cost associated with observing a given opportunity maychange along the search process. The use of changing search costs suggests an optimal search strategy structure differentfrom the one used in traditional economic search models; other than merely deciding when to terminate its search, theagent also needs to integrate exploration sequence considerations into its decision process.2 While the “secretary problem” is a classical optimal-stopping online problem, it does not involve search costs and the goal is to maximize the probabilityof finding the best candidate rather than minimizing the expected overall cost in our case.N. Hazon et al. / Artificial Intelligence 196 (2013) 26–5229Fig. 1. An example of how the cost associated with observing a given opportunity may change along the search process. When the agent is located at u3,the cost of observing the prices at u2 and u5 are 5 and 9, respectively. When the agent moves to u4, the cost of observing the prices at u2 and u5 become10 and 5, respectively.Search with changing search costs has been previously considered in the computer science domain in the contexts ofprize-collecting traveling salesman problems [9], the orienteering problem [55] and the graph searching problem [39].In Prize-Collecting Traveling Salesman Problems (PC-TSP) we are given a graph with non-negative “prize” values associ-ated with each node, and a salesman needs to pick a subset of the nodes to visit in order to minimize the total distancetraveled while maximizing the total prize collected. Since there is a tradeoff between the cost of a tour and how muchprize it spans, several variants have been developed. All the variants of PC-TSP are NP-hard, as they are generalizations ofthe famous Traveling Salesman Problem (TSP). One variant of PC-TSP is the k-TSP, where every node has a prize of one andthe goal is to minimize the total distance traveled, while visiting at least k nodes. Over the years, several constant-factorapproximations have been developed for the k-TSP [5,28,16,8,29]. The Orienteering Problem (OP) is another well-studiedvariant of PC-TSP, where the goal is to maximize the total prize collected, while keeping the distance traveled below a cer-tain threshold. Several stochastic variants of the OP have been considered. Campbell et al. [17] investigated the OrienteeringProblem with Stochastic Travel and Service times (OPSTS). Another stochastic variant of OP is the Orienteering Problem withStochastic Profits (OPSP) [34].These variants of the TSP and OP, while related, fundamentally differ from our model in that the traveling budget andthe prizes in these models are distinct, with different “currencies”. Thus, using up travel budget does not, and cannot affectthe prize collected at a node. In our work, in contrast, traveling and buying use the same resource (e.g. battery power).This fundamental difference is perhaps best exemplified when considering the situation on a path. On the path, solving theTSP, PC-TSP, OP and OPSP is trivial, while, as we show, two of the problems we consider remain NP (and none seems totallytrivial). As such, also the methods developed for solving PC-TSP and OP variants, which focus on more general metric spaces,are less relevant to our problem, which focuses on the path metric.In the Graph Searching Problem (GSP), an agent seeks a single item that resides at some node of a graph, and a distri-bution is defined over the probabilities of finding the item at each node of the graph. The goal is to minimize the expectedcost, as in our Min-Expected-Cost problem. The GSP was shown to be strictly related to a classic well-studied problem, theMinimum Latency Problem (MLP) [47], also called the traveling repairman problem [1], the school-bus driver problem [59],and the delivery man problem [23,43]. In this problem an agent is supposed to visit the nodes of a graph in a way thatminimizes the sum of the latencies to the nodes, where the latency of a node is the distance traveled by the agent beforevisiting the node. The minimum latency problem was shown to be NP-complete even when the metric space is induced bya tree [51], but can be solved in linear time when the underlying graph is a path [1,27]. In the operations research commu-nity, there are several exact exponential time algorithms for the MLP, e.g. [61,23,50,14,41]. Researchers have also evaluatedvarious heuristic approaches [58,56]. In the computer science community, there is a large branch of research dealing withapproximate solutions to the MLP. For general metrics, Blum et al. [15] gave the first constant factor approximation. Thiswas improved by Goemans and Kleinberg [30], and later by Chaudhuri et al. [18]. Koutsoupias et al. [39] provided a con-stant factor approximation for the unweighted case (i.e. for a shortest path metric on an unweighted graph), and Aroraand Karakostas [4] gave a quasi-polynomial O (n O (log n)) time approximation scheme for weighted trees and points in Rd.The MLP was also generalized to multi-agent settings (with k repairmen) by Fakcharoenphol et al. [20,21]. Koutsoupias etal. [39] and later Ausiello et al. [7] showed how to extend results obtained for the MLP to the GSP. For example, in somecases, approximation developed for the MLP can be applied to the GSP. Extensions of the GSP to scenarios where the itemis mobile are of the same character [26,38]. The GSP models fundamentally differ from our model in that in GSP there is asingle item located somewhere in the graph, which needs to be found, and thus a single binary probability at each node. Inour model, the item may be simultaneously available at many nodes, but at different prices, and the for each node there isa distribution over the multiple prices (as in economic search theory). Additionally, in our model traveling and purchasingconsume the same resource, an element lacking from the GSP model.This paper thus tries to bridge the gap between classical economic search theory (which is mainly suitable for virtualor non-dimensional worlds) and the changing search cost constraint imposed by operating in physical multi-agent environ-ments.From a broad perspective, our search problems lay within the field of planning under uncertainty. Some of the importantand relevant models in this field are Markov Decision Process (MDP) [12,45], Decentralized Markov Decision Process (DEC-30N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52MDP) [13] and Stochastic Games [49]. In all of these models (Stochastic Games being the most general, MDP is the mostrestricted) the goal is to maximize expected cumulative reward over a finite or infinite number of steps. This is the objectivewith the Min-Expected-Cost problem, and we indeed use an MDP formulation to solve the Min-Expected-Cost problem on thepath (see Section 2.1.2). The other two problems we consider are not concerned with expected rewards, and hence the MDPformulations, and variants, are inapplicable.There are also other fields in AI (and in general) that consider problems which are closely related to our problems. Onesuch field is the work in path planning when there is uncertain knowledge about the environment. In these settings the∗agent reveals the traversability of the edges only upon reaching them. The Focused Dynamic A) algorithm [53] is themost popular heuristic search method that repeatedly determines a shortest path from the agent’s current position to the∗goal, and is able to replan quickly as the knowledge of the terrain changes. Consequent alternatives to Dlite [37]and DSA[54], which are also able to handle the case where the goal changes over time in addition to the traversability ofedges. Unlike our model, in this line of work there are neither uncertainties regarding values nor costs at the possible goallocations that need to be considered. Therefore, Dlite and DSA* algorithms cannot be used to solve our problem.∗are D∗(D∗∗Another related line of work is scheduling, and in particular scheduling under uncertainty, which considers similartypes of objectives [10,11,24]. Much research has been carried out in this field (see [33] for a survey), considering theproblem of optimal allocation of resources to activities over time, where some of the parameters are uncertain. Our problemscan also be seen as a scheduling problem, as we assign the agents to visit different stores over time. However, in thescheduling domain the chosen plan does not affect the way that the uncertain parameters (i.e., the processing time orthe job length) are determined. In our case, the cost of visiting each store (i.e., the processing time) and the probabilityof buying there (i.e., the probability distribution over the job length) depends on the selected plan. Moreover, in manyscheduling under uncertainty problem formulations the underlying deterministic problem is itself NP-hard, and researchfocus is on developing heuristics to cope with the probabilistic version of the problem. In our case there is no underlyingdeterministic problem – if all the prices are known, Min-Budget and Max-Probability are not defined and Min-Expected-Costhas a trivial solution. In addition, we concentrate in analyzing the cases where there is an exact polynomial solution or atleast a proven approximation (with guarantees on the distance from the optimal solution).1.3. Terminology and definitionsWe are provided with a set of m points – S = {u1, . . . , um}, which represent the locations where the item may beavailable, which we call stores, together with a distance function dis : S × S → R, which determines the travel costs betweenany two locations.3 We are also provided with the agents’ initial locations, which are assumed WLOG (without loss ofgenerality) to be at one of the stores (the product’s price at this store may be ∞). With a single agent, there is one initial(k)location, us; with k agents we are provided with a vector of initial locations (us ). In addition, we are providedwith a cost probability function pi(c) – stating the probability that the cost of obtaining the item at store i is c. Let D bethe set of distinct prices with non-zero probability, and d = |D|. We assume that the actual price at a store is only revealedonce an agent reaches the store. Given these inputs, the goal is roughly to obtain the product at the minimal total cost,including both travel costs and purchase price. Since we are dealing with probabilities, this rough goal can be interpretedin three different concrete formulations:, . . . , u(1)s+1. Min-Expected-Cost: minimize the expected cost of obtaining the product.2. Min-Budget: given a success probability psucc minimize the budget necessary to guarantee obtaining the product withprobability at least psucc .3. Max-Probability: given a total budget B, maximize the probability of obtaining the product.In all the above problems, the optimization problem entails determining the strategy (order) in which to visit the differentstores, and when to terminate the search. In Min-Budget and Max-Probability, the search is always terminated when theproduct is available for a price no greater than the remaining budget, and the agent is located at the store where theproduct is purchased. In Min-Expected-Cost the budget is not allocated in advance and therefore the agent may decide tobuy the product at one store even when currently located at a different store. In this case, the agent will have to return tothe specific store, thus paying the “return” costs. This model (which includes returning costs) is the basic Min-Expected-Costmodel we consider. In addition, following standard assumption in economic search literature, we also consider a modelin which there are no returning costs. In a physical environment this can be justified if the product can be purchasedby phone (following the first physical visit at the store). We analyze both variants of the problem, and refer to them asMin-Expected-Cost and Min-Expected-Costphone problems, respectively.Technically, it is sometimes easier to work with the failure probability instead of the success probability. In order tocompute the failure probability there is a need to only multiply the failure probabilities in each node that has been visited.For the success probability, one needs to use addition and multiplication. Therefore, instead of maximizing psucc we mayphrase our objective as minimizing the failure probability.3 We therefore have a metric space.N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52312. Single agentWe start by analyzing the single agent case, for general distance functions (e.g. the stores are located in a generalmetric space). Unfortunately, in these settings, all three of the above mentioned problems are NP-hard; Min-Budget andMax-Probability remain hard even if the metric space is a tree. Thus, we focus on the case that the stores are all locatedon a single path. We would like to emphasize that even in a general metric spaces the stores are along some path thatcan be traced through them. However, the agent can freely move from every store to every other store directly (with anassociated travel cost). When we say that the stores are located on a path we mean that the agent’s movement is morerestricted – if the agent is located in a store it can move directly only to the two adjacent neighbors of its current location.We denote these problems Min-Budget (path), Max-Probability (path), and Min-Expected-Cost (path), respectively. In thiscase we can assume that, WLOG all points are on the line, and do away with the distance function dis. Rather, the distancebetween ui and u j is simply |ui − u j|. Furthermore, WLOG we may assume that the stores are ordered from left-to-right, i.e.u1 < u2 < · · · < um. For exposition purposes, the analysis throughout the paper includes only sketch of proofs for hardness.The detailed proofs are given in Appendices A and B.2.1. Minimize-Expected-CostWe prove that the Min-Expected-Cost variant is hard for general metric spaces. To prove this we first convert the problem+,·(·), and a maximum expected cost M. The problem is to decideinto its decision version. In Min-Expected-Cost-Decide we are given a set of points S, a distance function dis : S × S → Ran agent’s initial location us, a price-probability function pwhether there is a policy to obtain the product with an expected cost of at most M.2.1.1. Hardness in general metric spacesTheorem 1. For general metric spaces Min-Expected-Cost-Decide is NP-hard.The reduction used is from Hamiltonian path, and we build the instance in such a way that the agent needs to visit allthe stores, but only once, to obtain the product with the target expected cost.We note that in the proof the number of possible prices, d, does not depend on the input. Thus, for general metricspaces Min-Expected-Cost-Decide is hard even if d is bounded. Furthermore, even if we assume that an agent can purchasethe product by phone after leaving the store, Min-Expected-Cost-Decide is still hard (the proof is essentially identical, exceptthat we remove the returning cost of 2n from M).Before continuing with the analysis, we point out that if the cost of arriving to a location is fixed (e.g., in a vir-tual environment, such as e-commerce, where the cost of visiting each store does not depend on the last visited store),Min-Expected-Cost can be solved in polynomial time. The optimal search strategy in this case, as proved by Weitzman [57],is based on setting a reservation value (i.e., a threshold) to each store according to the distribution characterizing its valueand the cost of revealing that value. The searcher should continue the search as long as the best value obtained so faris above the lowest reservation value among those associated with stores that have not been explored yet. Formally, thereservation value R i of a store associated with a fixed cost F ci and a distribution pi(c) can be extracted from the followingequation:F ci =(cid:4)c<R i(R i − c)pi(c)(1)Intuitively, R iis the value where the searcher is precisely indifferent: the expected marginal benefit from revealing theactual value of a store (right-hand side) exactly equals the cost of doing so (left-hand side). The use of variable explorationcosts due to physical constraints that need to be factored in thus dramatically increases the complexity of determining theagents’ optimal strategies, precluding simple solutions such as the above.2.1.2. Solution for the pathWhen all stores are located on a path, Min-Expected-Cost and Min-Expected-Costphone problems can be modeled as afinite-horizon Markov Decision Process (MDP), as follows. For exposition purposes, we start with Min-Expected-Costphone.Note that on the path, at any point in time the points/stores visited by the agent constitute a contiguous interval, whichwe call the visited interval. Clearly, the algorithm need only make decisions at store locations. Furthermore, decisions can belimited to times when the agent is at one of the two stores at the edges of the visited interval. At each such location, theagent has only three possible actions: “go right” – extending the visited-interval one store to the right, “go left” – extendingthe visited-interval one store to the left, or “stop” – stopping the search and buying the product by phone at the best priceso far. Also note that after the agent has already visited the interval [u(cid:3), ur], how exactly it covered this interval does notmatter for any future decision; the costs have already been incurred. Accordingly, the states of the MDP are quadruplets[(cid:3), r, e, c], such that (cid:3) (cid:2) s (cid:2) r, e ∈ {(cid:3), r}, and c ∈ D, representing the situation that the agents visited stores u(cid:3) through ur ,it is currently at location ue , and the best price encountered so far is c. The terminal states are Buy(c) and all states of the32N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52(cid:6)) to transition to state [(cid:3), r + 1, r + 1, cform [1, m, e, c], and the terminal cost is c. For all other states there are two or three possible actions – “go right” (providedthat r < m), “go left” (provided that 1 < (cid:3)), or “stop”. The cost of “go right” on the state [(cid:3), r, e, c] is (ur+1 − ue), whilethe cost of “go left” is (ue − u(cid:3)−1). The cost of “stop” is always 0. Given the state [(cid:3), r, e, c] and move “go right”, there is(cid:6) < c. With the remaining probability, the transition is to(cid:6)], for cprobability pr+1(cstate [(cid:3), r + 1, r + 1, c]. Transition to all other states has zero probability. Transitions for the “go left” action are analogous.Given the state [(cid:3), r, e, c] and the action “stop”, there is probability 1 to transition to state Buy(c). This fully defines theMDP. The optimal strategy for finite-horizon MDPs can be determined using dynamic programming (see [45, Ch. 4]). In ourcase, the number of entries in the dynamic programming table is at most m · m · 2 · d (since this is an upper bound onthe number of possible states) and it takes at most O (d) steps to compute each entry. Therefore, the complexity of solvingMin-Expected-Costphone is O (d2m2) steps (using O (dm2) space).For Min-Expected-Cost (no phone) we will need a larger MDP, but the basic structure is similar. First note that if aninterval [u(cid:3), ur] has been visited, and the item not yet purchased, then any future purchase within the interval (if thereshould be such a purchase) will be with the agent coming from outside the interval into the interval, and moving directlyto a store for purchasing. In addition, there is a unique store uxr , such that any searcher coming from anywhere to the rightof ur and purchasing within [u(cid:3), ur] purchases at uxr , and similarly a unique store ux(cid:3) , for purchases coming from the left ofu(cid:3). The reason is that the cost of purchasing at a store is the sum of price and distance. So, any fixed additional distance doesnot change the minimum. Thus, if the price at uxr is lowest for someone coming from ur , it is also the minimum for anyonecoming from ur(cid:6) to the right of ur . Therefore, the states of the MDP for Min-Expected-Cost are septuplets [(cid:3), r, e, c(cid:3), x(cid:3), cr, xr],representing the situation that the agent visited stores u(cid:3) through ur , it is currently at location ue , e ∈ {(cid:3), r}, and the bestprice encountered so far for someone coming from the left (respectively right) is c(cid:3)(cr ), which can be found at store ux(cid:3) (uxr ).The terminal states are Buy(c, e, x) with a terminal cost of c + |ue − ux|, and all states of the form [1, m, e, c(cid:3), x(cid:3), cr, xr]with a terminal cost of c(cid:3) + |ux(cid:3)| if e = m. The actions are the same as in the MDP forMin-Expected-Costphone, but the transition probabilities are different. Given the state [(cid:3), r, e, c(cid:3), x(cid:3), cr, xr] and move “go right”,there is probability pr+1(c|). With the remainingprobability, the transition is to state [(cid:3), r + 1, r + 1, c(cid:3), x(cid:3), cr, xr]. Transition to all other states has zero probability. Transitionsfor the “go left” action are analogous. Given the state [(cid:3), r, e, c(cid:3), x(cid:3), cr, xr] and the action “stop”, there is probability 1 totransition to state Buy(c(cid:3), e, x(cid:3)) if e = (cid:3) and Buy(cr, e, xr) if e = r. This fully defines the MDP. Using the same analysis asbefore, we get that the complexity of solving Min-Expected-Cost is O (d3m4) steps (using O (d2m4) space).(cid:6)) to transition to state [(cid:3), r + 1, r + 1, c(cid:3), x(cid:3), c− u1| if e = 1 and cr + |um − uxr(cid:6) < (cr +|ur+1 − uxr(cid:6), r + 1], for c2.2. Min-Budget and Max-Probability2.2.1. NP completenessUnlike the Min-Expected-Cost problem, the other two problems are NP-complete even on a path. To prove this weagain convert the problems into their decision versions. In the Min-Budget-Decide problem, we are given a set of points·(·), a minimum successS, a distance function dis : S × S → Rprobability psucc and maximum budget B. We have to decide whether a success probability of at least psucc can be obtainedwith a budget of at most B. The exact same formulation also constitutes the decision version of the Max-Probability problem., an agent’s initial location us, a price-probability function p+Theorem 2. The Min-Budget-Decide problem is NP-complete even on a path.The reduction is from 0–1 Knapsack. We build the instance such that the agent needs to go back and forth along theline in a zigzag movement. Before any direction switch the agent needs to decide weather to keep moving to a close store,to “collect” some more probabilities, or to keep its budget and head to the other direction. Each such decision correspondsto the decision of whether to spend some space and insert an item to the knapsack, or save it to the next item.Thus, we either need to consider restricted instances or consider approximations. We do both.2.2.2. Restricted case: Bounded number of pricesWe consider the restricted case when the number of possible prices, d, is bounded. For brevity, we focus on theMin-Budget (path) problem. The same algorithm and similar analysis work also for the Max-Probability (path) problem.Consider first the case where there is only one possible price c0. At any store i, either the product is available at this price,with probability pi = pi(c0), or not available at any price. In this setting we show that the problem can be solved in O (m)steps. This is based on the following lemma, stating that in this case, at most one direction change is necessary.Lemma 3. Consider a price c0 and suppose that in the optimal strategy starting at point us the area covered while the remainingbudget is at least c0 is the interval [u(cid:3), ur]. Then, WLOG we may assume that the optimal strategy is either (us (cid:3) ur (cid:3) u(cid:3)) or(us (cid:3) u(cid:3) (cid:3) ur).Proof. Any other route would yield a higher cost to cover the same interval. (cid:2)Using this observation, we immediately obtain an O (m3) algorithm for the single price case: for each interval [u(cid:3), ur],consider both possible options and for each compute the total cost and the resulting probability. Choose the option whichN. Hazon et al. / Artificial Intelligence 196 (2013) 26–5233Algorithm 1 OptimalPolicyForSinglePrice(Success probability psucc, single price c0).1: ur ← leftmost point on right of us s.t. 1 −2: (cid:3) ← s← |ur − us|3: B RLmin4: while (cid:3) (cid:2) 0 and r (cid:2) s do5:6:i=s(1 − pi ) (cid:2) psucc(cid:5)r(cid:5)rB ← 2|ur − us| + |us − u(cid:3)|if B < B RLmin then← BB RLminr ← r − 1while (cid:3) (cid:2) 0 and 1 −7:8:9:10:11: u(cid:3) ← rightmost point to left of us s.t. 1 −12: r ← s← |us − ul|13: B LRmin14: while r (cid:3) m and (cid:3) (cid:3) s do15:16:(cid:3) ← (cid:3) − 1i=(cid:3) 1 − pi < psucc do(cid:5)B ← 2|us − u(cid:3)| + |ur − us|if B < B LRmin then← BB LRmin(cid:3) ← (cid:3) + 1while r (cid:3) m and 1 −(cid:5)r17:18:19:20:21: return min{B RLr ← r + 1min, B LRmin} + c0i=(cid:3) 1 − pi < psucc dosi=(cid:3) 1 − pi (cid:2) psuccrequires the lowest budget but still has a success probability of at least psucc. With a little more care, the complexity can bereduced to O (m). First note that since there is only a single price c0, we can add c0 to the budget at the end, and assumethat the product will be provided at stores for free, provided that it is available. Now, consider the strategy of first movingright and then switching to the left. In this case, we need only consider the minimal intervals that provide the desiredsuccess probability, and for each compute the necessary budget. This can be performed incrementally, in a total of O (m)operations for all such minimal intervals, since at most one point can be added and one deleted at any given time. Similarlyfor the strategy of first moving left and then switching to the right. The details are provided in Algorithm 1.Next, consider the case that there may be several different available prices, but their number, d, is fixed. We provide apolynomial algorithm for this case (though exponential in d). First note that in the Min-Budget problem, we seek to minimizethe initial budget B necessary so as to guarantee a success probability of at least psucc given this initial budget. Once thebudget has been allocated, however, there is no requirement to minimize the actual expenditure. Thus, at any store, if theproduct is available for a price no greater than the remaining budget, it is purchased immediately and the search is over.If the product has a price beyond the current available budget, the product will not be purchased at this store under anycircumstances. Denote D = {c1, c2, . . . , cd}, with c1 > c2 > · · · > cd. For each ci there is an interval I i = [u(cid:3), ur] of pointscovered while the remaining budget was at least ci . By definition, for all i, I i ⊆ I i+1. Thus, consider the incremental areacovered with remaining budget ci , (cid:4)i = I i − I i−1 (with (cid:4)1 = I1). Each (cid:4)i is a union of an interval at left of us and aninterval at the right of us (both possibly empty). The next lemma, which is the multi-price analogue of Lemma 3, states thatthere are only two possible optimal strategies to cover each (cid:4)i :Lemma 4. Consider the optimal strategy and the incremental areas (cid:4)i (i = 1, . . . , d) defined by this strategy. For ci ∈ D, let u(cid:3)i bethe leftmost point in (cid:4)i and uri the rightmost point. Suppose that in the optimal strategy the covering of (cid:4)i starts at point usi . Then,(cid:3) uri ). Furthermore, the starting point forWLOG we may assume that the optimal strategy is either (usicovering (cid:4)i+1 is the ending point of covering (cid:4)i .(cid:3) u(cid:3)i ) or (usi(cid:3) u(cid:3)i(cid:3) uriProof. The areas (cid:4)i fully determine the success probability of the strategy. Any strategy other than the ones specified inthe lemma would require more travel budget, without enlarging any (cid:4)i . (cid:2)Thus, the optimal strategy is fully determined by the leftmost and rightmost points of each (cid:4)i , together with the choicefor the ending points of covering each area. We can thus consider all possible cases and choose the one with the lowestbudget which provides the necessary success probability. There are m2d2d )2d ways for choosing the external points ofthe (cid:4)i ’s, and there are a total of 2d options to consider for the covering of each. For each option, computing the budgetand probability takes O (m) steps. Thus, the total time is O (m2d( em2d )2d). Similar algorithms can also be applied for theMax-Probability (path) problem. In all, we obtain:(2d)! (cid:2) ( emTheorem 5. Min-Budget (path) and Max-Probability (path) can be solved in O (m) steps for a single price and O (m2d( emprices.2d )2d) for dIn summary, the effect of bounding the number of prices was indeed surprising. Indeed, as we showed in Lemma 4,the number of prices determines incremental intervals for covering by the agent. Each price induces only one interval and34N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52there are only two optimal ways to cover each interval. Therefore, if the number of prices is fixed, even if we increase thenumber of stores we can still enumerate and check all the possible ways to cover the intervals optimally. Unfortunately,moving beyond the path, Min-Budget-Decide turns hard, even with a bounded number of possible prices, and even on a tree.Theorem 6. The Min-Budget-Decide problem is NP-complete on a tree, even with a bounded number of prices.The reduction is from 0–1 Knapsack. In the proof we build a star tree, where the agent is located at the root and allthe other stores around him. We use only two possible prices, 0 and inf, so the only difference between stores is theirdistance from the root (which corresponds to the knapsack items size) and the probability of purchasing the product (whichcorresponds to the knapsack item value).2.2.3. Min-Budget approximationNext, we provide an FPTAS (fully-polynomial-time-approximation-scheme) for the Min-Budget (path) problem. The ideais to force the agent to move in quantum steps of some fixed size δ. In this case the tour taken by the agent can be dividedinto segments, each of size δ. Furthermore, the agent’s decision points are restricted to the ends of these segments, exceptfor the case where along the way the agent has sufficient budget to purchase the product at a store, in which case it does soand stops. We call such a movement of the agent a δ-resolution tour. Note that the larger δ the less decision points there are,and the complexity of the problem decreases. Given 0 < (cid:2) < 1, we show that with a proper choice of δ we can guarantee a(1 + (cid:2)) approximation to the optimum, while maintaining a complexity of O (n poly(1/(cid:2))), where n is the size of the input.Our algorithm is based on computing for (essentially) each initial possible budget B, the maximal achievable successprobability, and then pick the minimum budget with probability at least psucc. Note that once the interval [(cid:3), r] has beencovered without purchasing the product, the only information that matters for any future decision is (i) the remainingbudget, and (ii) the current location. The exact (fruitless) way in which this interval was covered is, at this point, immaterial.This, “memoryless” nature calls, again, for a dynamic programming approach. We now provide a dynamic programmingalgorithm to compute the optimal δ-resolution tour. WLOG assume that us = 0 (the initial location is at the origin). Forintegral i, let w i = iδ. The points w i , which we call the resolution points, are the only decision points for the algorithm.Set L and R to be such that w L is the rightmost w i to the left of all the stores and w R the leftmost w i to the rightfail[·, ·, ·, ·] and act[·, ·, ·, ·], such that for all (cid:3), r, L (cid:2) (cid:3) (cid:2) 0 (cid:2) r (cid:2) R, e ∈ {(cid:3), r} (oneof all stores. We define two tables,of the end points), and budget B, fail[(cid:3), r, e, B] is the minimal failure probability achievable for purchasing at the storesoutside [w(cid:3), wr], assuming a remaining budget of B, and starting at location we . Similarly, act[(cid:3), r, e, B] is the best actto perform in this situation (“left”, “right”, or “stop”). Given an initial budget B, the best achievable success probability is(1 − fail[0, 0, 0, B]) and the first move is act[0, 0, 0, B]. It remains to show how to compute the tables. The computation ofthe tables is performed from the outside in, by induction on the number of remaining points. For (cid:3) = L and r = R, there areno more stores to search and fail[L, R, e, B] = 1 for any e and B. Assume that the values are known for i remaining points,we show how to compute for i + 1 remaining points. Consider cost[(cid:3), r, e, B] with i + 1 remaining points. Then, the leastfailure probability obtainable by a decision to move right (to wr+1) is:F R =1 −fail[(cid:3), r + 1, r + 1, B − δ](cid:6)(cid:6)(cid:7)pr+1(c)(cid:7)p(cid:3)−1(c)(cid:4)c(cid:3)B−δ(cid:4)c(cid:3)B−δSimilarly, the least failure probability obtainable by a decision to move left (to w(cid:3)−1) is:F L =1 −fail[(cid:3) − 1, r, (cid:3) − 1, B − δ]max/δ, the furthest point reachable with budget BδThus, we can choose the act providing the least failure probability, determining both act[(cid:3), r, e, B] and fail[(cid:3), r, e, B]. Inpractice, we compute the table only for B’s in integral multiples of δ. This can add at most δ to the optimum. Also, we mayplace a bound Bδmax/δand w R = Bδmax on the maximal B we consider in the table. In this case, we start filling the table with w L = −Bδmax.Next, we show how to choose δ and prove the approximation ratio. Set λ = (cid:2)/9. Let α = min{|us − us+1|, |us − us−1|}– the minimum budget necessary to move away from the starting point, and β = m2|um − u1| + max{c: ∃i, pi(c) > 0}– an upper bound on the total usable budget. We start by setting δ = λ2α and double it until δ > λ2β, performing thecomputation for all such values of δ. For each such value of δ, we fill the tables (from scratch) for all values of B’s in= 2λ−2δ. We now prove that for at least one of the choices of δ we obtain a (1 + (cid:2))integral multiples of δ up to Bδapproximation.maxConsider a success probability psucc and suppose that optimally this success probability can be obtained with budgetBopt using the tour T opt. By tour we mean a list of actions (“right”, “left” or “stop”) at each decision point (which, in thisopt, as follows. For any i (cid:4) 0, when T opt moves for thecase, are all store locations). We convert T opt to a δ-resolution tour, T δopt moves all the way to w i+1. Similarly, for i (cid:2) 0, when T opt moves for the first time tofirst time to the right of w i then T δthe left of w i then T δopt moves all the way to w i−1.N. Hazon et al. / Artificial Intelligence 196 (2013) 26–5235Note that T δopt makes t turns (i.e. t direction changes). Then, the total additional travel cost of the tour T δopt requires additional travel costs only when it “overshoots”, i.e. when it goes all the way to the resolutionpoint while T opt would not. This can either happen (i) in the last step, or (ii) when T opt makes a direction change. Type (i)can happen only once and costs at most δ. Type (ii) can happen at most once for each resolution point, and costs at most2δ. Suppose that T δopt overopt with budget Bopt + (2t + 1)δ then at anyT opt is at most (2t + 1)δ. Furthermore, if we use T opt with budget B opt and T δstore, the available budget under T δopt is a δ-resolution tour that with budgetat most Bopt + (2t + 1)δ succeeds with probability (cid:4) psucc. Hence, our dynamic algorithm, which finds the optimal such(cid:2) Bopt + (2t + 2)δ obtaining at least the same success probability. Noteδ-resolution tour will find a tour with budget Bδthat we include one additional δ, for the integral multiples of δ in the tables.opt is at least that available with T opt. Thus, T δoptSince T δopt has t-turns, T opt must also have t-turns, with targets at t distinct resolution segments. For any i, the i-th suchturn (of T opt) necessarily means that T opt moves to a point at least (i − 1) segments away, i.e. a distance of at least (i − 1)δ.Thus, for B opt, which is at least the travel cost of T opt, we have4:Bopt (cid:4)t(cid:4)i=1(i − 1)δ = (t − 1)(t)δ (cid:4) t24δ2On the other hand, since we consider all options for δ in multiples of 2, there must be a ˆδ such that:−2 ˆδ (cid:4) Bopt (cid:4) λ−22λˆδCombining (2) and (3) we get that t (cid:2) 2λ−1. Thus, the approximation ratio is:(cid:2) 1 + 2(t + 1) ˆδλ−2 ˆδ/2ˆδoptBoptB(cid:2) Bopt + 2(t + 1) ˆδBopt(cid:3)8λ + 4λ2(cid:2) 1 +(cid:2)(cid:2) 1 + (cid:2)(2)(3)(4)(5)Also, combining (3) and (5) we get thatˆδoptB(cid:2) Bopt(1 + (cid:2)) (cid:2) 2λ−2 ˆδ = BˆδmaxHence, the tables with resolution ˆδ consider this budget, and Bˆδopt will be found.It remains to analyze the complexity of the algorithm. For any given δ there are Bδmax/δ = 2λ−2 budgets we consider andat most this number of resolution points at each side of us, for each, there are two entries in the table. Thus, the size ofthe table is (cid:2) 8λ−6 = O ((cid:2)−6). The computation of each entry takes O (1) steps (see the discussion above). We consider δ inpowers of 2 up to β (cid:2) 2n, where n is the size of the input. Thus, the total computation time is O (n(cid:2)−6). We obtain:Theorem 7. For any (cid:2) > 0, theMin-Budget ( path) problem can be approximated with a (1 + (cid:2)) factor in O (n(cid:2)−6) steps.3. Multi-agent, shared budgetSince even the single agent case is hard for general metric spaces, with the multi-agent case we focus solely on situationsin which all the stores are on a single path. We assume k agents operating in the same underlying physical setting as inthe single agent case, i.e. a set of stores S and a price probability function for each store. We assume that the goal is notindividualized; the agents seek to obtain only one item and having multiple goods is not beneficial. Furthermore, since theagents are fully collaborative, they do not care which agent will obtain the item.We begin by analyzing the shared budget multi-agent model, where all the resources and costs are shared among all theagents. In theory, agents may move in parallel, but since minimizing time is not an objective, we may assume WLOG thatat any given time only one agent moves. When an agent reaches a store and finds the price at this location, the optimalstrategy should tell whether to purchase the product (and where) and if not what agent should move next and to where.Therefore, in the k-Shared-Min-Expected-Cost problem the agents try to minimize the expected total cost, which includesthe travel costs of all agents plus the final purchase price (which is one of the prices that the agents have sampled). Ink-Shared-Min-Budget and k-Shared-Max-Probability, the initial budget is for the use of all the agents, and the success probabil-ity is for any of the agents to purchase, at any location. Since all the agents use the same budget in this model, inter alia, fortraveling costs, we assume the agents can communicate with each other to coordinate their moves. In k-Shared-Min-Budgetand k-Shared-Max-Probability the agents only need to announce to the other agents when they reach a specific store. Ink-Shared-Min-Expected-Cost the agents also need to communicate the price they find at the location they have reached.4 Assuming that t > 1. If t = 0, 1 the additional cost is small by (3).36N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52In general, the algorithms for the single-agent case (for the path) can be extended to the multi-agent case, with theadditional complexity of coordinating between the agents. The proofs are relegated to Appendix B, as they are very similarto the single agent case. We obtain:Theorem 8. With k agents, k-Shared-Min-Expected-Costphone can be solved in O (d22k( mcan be solved in O (d2k+12k( mk )4k).k )2k), and k-Shared-Min-Expected-CostTheorem 9. With k agents, k-Shared-Min-Budget and k-Shared-Max-Probability with d possible prices can be solved inO (m2kd( em2kd )2kd).Theorem 10. With k agents, for any (cid:2) > 0, k-Shared-Min-Budget can be approximated to within a factor of (1 + k(cid:2)) in O (n(cid:2)−6k)steps (for an arbitrary number of prices).While the complexity in the multi-agent case grows exponentially in the number of agents, in most physical environ-ments where several agents cooperate in exploration and search, the number of agents is relatively moderate. In these casesthe computation of the agents’ strategies is efficiently facilitated by the principles of the algorithmic approach presented inthis paper.If the number of agents is not fixed (i.e. part of the input) then, the complexity of all three variants grows exponentially.Most striking perhaps is that k-Shared-Min-Budget and k-Shared-Max-Probability are NP-complete even on the path with asingle price. This is in contrast to the single agent case where the single price case can be solved in O (n) steps. To provethis we again formulate the problems into a decision version – k-Shared-Min-Budget-Decide – given a set of points S on the·(·), a minimum success probability psuccpath, initial locations for all agents (uand a maximum budget B, decide if success probability of at least psucc can be achieved with a maximum budget B.), a price-probability function p, . . . , u(1)s(k)sTheorem 11. k-Shared-Min-Budget-Decide is NP-complete even on the path with a single price.The reduction is (again) from 0–1 Knapsack. We build the instance such that the number of agents corresponds to thenumber of possible knapsack items. Each agent can only visit one store, since we set the distances to the other stores abovethe (shared) budget. As before, the probabilities at the stores correspond to the knapsack items values, and the distancescorrespond to the knapsack items sizes. The problem is to decide which agents will move given the initial budget, whichcorresponds to the decision which items to insert to the knapsack given its size.4. Multi-agent, private budgetWe now investigate a model ofprivate budgets , wherein each agent j has its own initial budget B j (unlike the previousshared budget model). If the objective is to minimize the total expected cost, the private budgets model is equal to theshared budget model since the agents are cooperative. Therefore in this case we have two concrete problem formulations:1. k-Private-Max-Probability: given initial budgets B j , for each agent j, maximize the probability of obtaining the item.2. k-Private-Min-Budget: given a target success probability psucc, minimize the agents’ initial budgets necessary to guaranteeacquisition of the item with a probability of at least psucc.Since the corresponding single-agent problems are hard even for the path, we again assume that the number of possibleprices, d, is bounded. In the k-Private-Min-Budget problem it is also important to distinguish between two different agentmodels:• Identical budgets: the initial budgets of all the agents must be the same. The problem is to minimize this initial budget,and we denote the problem as k-Private-Min-Budgetidentical.• Distinct: the agents’ initial budgets may be different. In this case the problem is to minimize the average initial budget,and we denote the problem as k-Private-Min-Budgetdistinct.4.1. Non-communicating agentsWe first consider the case where agents cannot communicate with each other. In this case agents cannot assist eachother. Hence a solution is a strategy comprised of a set of ordered lists, one for each agent, determining the sequence ofstores this agent must visit.The success probability of a strategy is the probability that at least one of the agents will succeed in its task. Technically,in this case, it is easier to calculate the complementary failure probability: the probability that all the agents will notsucceed in their tasks. For example, suppose that the stores and agents are located as illustrated in Fig. 2, and consider thedepicted strategy. This strategy fails if for both agents and each of the stores they visit the cost of the item is higher thanN. Hazon et al. / Artificial Intelligence 196 (2013) 26–5237Fig. 2. A possible input with a suggested strategy. The numbers on the edges represent traveling costs. The table at each store ui represents the costprobability function pi (c). The strategy of each agent is illustrated by the arrows.their remaining budget. This will happen with probability ( 12is 1920 .We begin by considering the k-Private-Max-Probability problem. We prove:5 ) = 1· 44 ) · ( 1· 1220 . Hence, the success probability of this strategyTheorem 12. In the no communication case if the number of possible costs is constant then k-Private-Max-Probability can be solvedin polynomial time for any number of agents.The proof is based on the following definitions and lemmata. The key idea is that in most cases the stores will be visitedby only one agent in the optimal strategy. However, there are some cases where the same store will be visited by morethan one agent. We identify these cases and show that there are only a fixed number of them. We are thus able to providea dynamic programming algorithm to find the optimal strategy.Note that multiple strategies may result in the same success probability. In this case we say that the strategies areequivalent. In particular there may be more than one optimal strategy.Definition 13. Let S be a strategy. Agents i and ¯i are said to be separated by S if each store that is reached by i is notreached by ¯i.Lemma 14. If agents i and ¯i are not separated by any optimal strategy, then in at least one optimal strategy at least one of these agentsmust pass the initial location of the other.Proof. WLOG assume that i is on the right side of ¯i. Consider an optimal strategy S. Let r be the rightmost store that isreached by i and ¯l the leftmost store that is reached by ¯i. Assume by contradiction that none of the agents passes theinitial location of the other in S. Thus, there is at least one store between their initial locations that is reached by bothagents. WLOG assume that ¯i reaches at least one store with a higher budget than i’s remaining budget when reaching it,and denote by ¯rthe rightmost such store. Consider the following modified strategy: i goes according to S till the stage it∗has to reach ¯rit goes all the way straight to r. Otherwise, it stops just. If ¯i did not reach ¯l yet then after reaching ¯rbefore reaching ¯ritgoes all the way straight to ¯l. Otherwise, it stops after reaching ¯r. Agents i and ¯i are separated by this strategy and it hasat least the same success probability as S, in contradiction. (cid:2)∗. If i did not reach r yet then instead of reaching ¯r. ¯i goes according to S till the stage it has to reach ¯r∗∗∗∗∗Based on this lemma, we now show that if two agents are not separated by any optimal policy, their movement has aspecific structure.Lemma 15. Suppose that agents i and ¯i are not separated by any optimal strategy. Let S be an optimal strategy. Suppose that in Sagent i passes the initial location of agent ¯i and agent ¯i does not stay in its initial location. Then, there is an optimal strategy such thatone of the following holds:• ¯i moves only in one direction which is opposite to the final movement’s direction of i. Furthermore, if the final movement’s directionof i is right (left) then ¯i passes the leftmost (rightmost) store that is reached by i.• Either i or ¯i does not move.Proof. WLOG assume that i is on the right side of ¯i. Let [l, r] be the interval of stores covered by i. Since i passes the initiallocation of ¯i, l is located on the left of u(¯i)s and r is located on the right of uFirst we show that we may assume that ¯i reaches at least one store outside the interval [l, r]. If it does not, considerthe following two cases. If i’s remaining budget at each store is always as high as ¯i’s remaining budget then ¯i does notbe the rightmost store where ¯i’s remaining budget is higher thanhave to move and the theorem holds. Otherwise, let ¯r(¯i)s∗.38N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52Fig. 3. The only three cases where a pair of agents may not be separated.∗∗∗∗i’s remaining budget. If ¯ris on the left side of i’s initial location, then as in the proof of Lemma 14, the agents can beseparated. If ¯ris on the right side of i’s initial location and it equals r, there is no need for i to reach r since at each stores , r], ¯i has at least the same budget as i. Thus, there is an optimal strategy where either i does not move or it moves(i)in [uonly to the left, so ¯i passes the rightmost store that is reached by i. If ¯ris on the right side of i but on the left side of r. Since it has more budget than i at this location, ¯i can move to l while i movesthen there is no need for ¯i to go beyond ¯rto r. Thus, again, there is an optimal strategy where either i does not move or it moves only to the right, so ¯i passes theleftmost store that is reached by i. Thus, we may assume that ¯i reaches at least one store outside the interval [l, r].WLOG assume that i’s final movement’s direction is left and suppose that ¯i reaches at least one store outside the interval[l, r] to the left of l. If ¯i’s budget at l is higher than i’s remaining budget there, then it is also higher at u, and again theagents can be separated. If ¯i’s budget at l is not higher than i’s remaining budget, then ¯i does not have to move since i canreach the same stores to the left of l.Now suppose that ¯i moves to the right (which is the opposite direction of i’s final movement) and passes u, but italso changes its direction. The only reason for ¯i to change directions is to reach a store on the left side of its initial location,with a higher budget than i has at this store, or to reach a store that i does not reach at all. In both cases ¯i must reach eachstore in [l, u] with at least the same budget as i has at the same location, so either S is not optimal, or we can modify Sby letting only ¯i to move while i does not move at all. (cid:2)(¯i)s(i)s(i)sUsing these lemmata we observe that for any two agents, there is only a constant number of possible cases where theagents are not separated by the optimal strategies. Fig. 3 illustrates the three core cases (the others are symmetrical). Here,agents 1 and 3 are non-separated agents. Note that every agent between them, like agent 2, does not have to move at allin the optimal strategy.Therefore we can use a dynamic programming approach to find an optimal strategy whereby all the agents are separated,but we also check the non-separated strategies individually.N. Hazon et al. / Artificial Intelligence 196 (2013) 26–5239Recall that in our problem the objective is to maximize the success probability, given the initial budgets. Technically, itis easy to work with the failure probability instead of the success probability.Definition 16. fail[ui, j] is the minimal failure probability if the only reachable stores are in the interval [u1, ui], and onlyagents 1, . . . , j are allowed to move. act[ui, j] is the optimal strategy achieving fail[ui, j], under the same conditions.5Note that where ui < u, fail[ui, j] is not defined. Given act[ui, j], fail[ui, j] can be easily computed in O (m) steps.For technical reasons we add another agent, 0, with a budget of zero and set its initial location to the leftmost store, i.e.u= u1. fail[ui, 0] = 1 for all i, and this agent doesn’t affect the failure probability of any policy.We are now ready to prove Theorem 12, by showing a polynomial time algorithm for k-Private-Max-Probability.(0)s( j)s(¯j)sProof of Theorem 12. We use dynamic programming to calculate fail[um, k] and act[um, k]. For fail[ui, 1] and act[ui, 1], whichis the single agent case, we employ the polynomial algorithm obtained from Theorem 5.(¯j)l(cid:2) uor not at all. Let u(l)sGiven any agent ¯j we first consider the case where ui = u. In this case in the optimal strategy ¯j moves only to the left,be the leftmost store visited by ¯j with the optimal strategy for the given interval, and agent l be the(¯j)(l may equal 0). Each agent t such that l < t < ¯j does not move in the optimal strategy. Otherwise,lone such that uagents t and ¯j are not separated and according to Lemma 15 agent t must pass the rightmost store u(¯j)possible. The same argument shows that each agent t such that t (cid:2) l does not reach ul(¯j)(¯j)l−1, l], which are already known, together with the movement of agent ¯j to uof act[ulsteps.(¯j), which is nots. Therefore act[ui, ¯j] is composedtakes O (m). Thus, computing u(¯j)l(¯j)l(¯j)l(l)s , uat all. Let u(cid:2) uNext, consider the case where ui > u. In this case, in the optimal strategy ¯j may move in both directions, or not movebe the leftmost store visited by ¯j with the optimal strategy for this interval, and agent l is the one such that. First note that each agent t, t (cid:2) l, and ¯j are separated by the optimal policy, or ¯j does not move. Otherwise,(l)usaccording to Lemma 14 t must pass the initial location of ¯j but according to Lemma 15 ¯j must reach a store outside the] which does not occur. Since ¯j passes the initial locations of every agent t, l < t < ¯j, if one of them movesinterval [uit goes only in the opposite direction of the final movement direction of ¯j according to Lemma 15, and as illustrated inFig. 3. Since they all must move in the same direction, according to the same lemma at most one of them moves in theoptimal policy. Therefore, to compute act[u¯i, ¯j] we check only the following options, and choose the best one:1. ¯j does not move, and act[u¯i, ¯j] = act[u¯i, ¯j − 1].2. Each agent t, l < t < ¯j, does not move. Thus, act[u¯i, ¯j] is composed of act[u(¯j)l−1, l], with the optimal movement of agent(¯j)s(¯j)s¯j in the interval [u(¯j)l, ui].The previous two options assume that ¯j and every other agent are separated. Otherwise:(¯t)3. One agent t, l < t < ¯j, moves. Let ul(l)(cid:2) uand agent l is the one such that us(¯t)agents ¯j and t in the interval [u, ui].lbe the leftmost store visited by either agent t or ¯j, with the optimal strategy,(¯t)(¯t)l−1, l], with the optimal movement of the twol. act[u¯i, ¯j] is composed of act[uThere are at most m possible options for u.Therefore for each agent j and store ui act[ui, j] can be found in O (m2k) steps, and act[um, k] can be found in O (m3k2)time steps using O (mk) space. (cid:2). In each option we check for at most k agents m possible options for u(¯t)l(¯j)lWe can use the algorithm for the k-Private-Max-Probability problem to obtain a polynomial time algorithm for thek-Private-Min-Budgetidentical problem:Theorem 17. In the no communication setting, if the number of costs is constant, k-Private-Min-Budgetidentical can be solved inpolynomial time for any number of agents.Proof. By Theorem 12, given a budget ¯B, we can calculate the maximum achievable success probability. Thus we can runa binary search over the possible values of ¯B to find the minimal one that still guarantees a success probability psucc.5 There may be more than one strategy with the same failure probability, act[ui , j] is one of them.40N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52Fig. 4. A possible input with suggested moves. The numbers on the edges represent traveling costs. The table at each store ui represents the cost probabilityfunction pi (c). The moves are illustrated by the arrows.The maximum required budget is 2 · |u1 − um| + max D, which is part of the input. Thus the binary search will require apolynomial number of steps. (cid:2)Surprisingly, the results for k-Private-Min-Budgetdistinct are different. We note that in the k-Private-Min-Budgetdistinct prob-lem the objective is to minimize the average budget, which is the same as minimizing the total budget. Therefore, if thenumber of agents is fixed, we can use the same polynomial time algorithm that is used in the proof of Theorem 9 (ex-cept that the visited intervals of the agents are not disjoint). If the number of agents is a parameter, the hardness ofk-Private-Min-Budgetdistinct follows from that of the k-Shared-Min-Budget problem. We obtain:Theorem 18. If the number of agents is fixed, k-Private-Min-Budgetdistinct with no communication can be solved in O (m2kd( em2d )2kd).If the number of agents is a parameter, k-Private-Min-Budgetdistinct with no communication is NP-complete even for a single possiblecost.4.2. Communicating agentsOnce communication is added agents can call upon each other for assistance and the relative scheduling of the agents’moves must also be considered. In this case a solution is an ordered list of moves, where each move is a pair stating anagent and its next destination.The success probability of a solution is now calculated according to the order of moves. For example, suppose that thestores and agents are located as illustrated in Fig. 4.Consider the following solution: agent 2 first goes to u4 and then agent 1 goes to u2. Agent 2 is the only one whichcan succeed at u4, with a probability of 0.8. With probability of 0.2 it will not succeed and agent 1 has a probability of0.2 to succeed at u2. Hence, the success probability is 0.8 + 0.2 · 0.2 = 0.84. If we switch the order of the moves we get aprobability of 0.9 to succeed at u2 with the first move, since agent 2 will be called for assistance if the cost required is lessthan 100. If not, agent 2 will move to u4 as before. Hence, this solution success probability is 0.9 + 0.1 · 0.8 = 0.98.When the number of agents is not fixed, k-Private-Max-Probability, k-Private-Min-Budgetidentical and k-Private-Min-Budgetdistinct are not known to be solvable in polynomial time. However, in many physical environments where severalagents cooperate in exploration and search, the number of agents is relatively small. In this case we can show that all thethree problems can be solved in polynomial time. We show:Theorem 19. In the setting of communicating agents, if the number of agents and the number of different costs is fixed thenk-Private-Max-Probability, k-Private-Min-Budgetidentical and k-Private-Min-Budgetdistinct can be solved in polynomial time.For brevity, we focus on the k-Private-Max-Probability problem. The same algorithm and similar analysis work also forthe other two problems.First note that as in Max-Probability, in k-Private-Max-Probability we need to maximize the probability of obtaining theitem given the initial budgets B i , but there is no requirement to minimize the actual resources consumed (in contrast tok-Shared-Max-Probability). Thus, at any store, if agents can obtain the item for a cost no greater than its remaining budget,the search is over. Furthermore, if the cost is beyond the agent’s available budget, but there is another agent with a sufficientbudget to both travel from its current location and to obtain the item, then this agent is called upon and the search is alsoover. Otherwise, the item will not be obtained at this store under any circumstances. Thus, the basic strategy structure,which determines which agent goes where, remains the same. Unless the search has to be terminated, the decision of oneagent where to go next is not affected by the knowledge gained by others. Using a similar argument as in the proof ofTheorem 9, we get the following result. For brevity, we denote ¯d instead of d + 1.N. Hazon et al. / Artificial Intelligence 196 (2013) 26–5241Fig. 5. A possible input with a suggested strategy. The numbers on the edges represent traveling costs. The table at each store ui represents the costprobability function pi (c). The moves of each agent are illustrated by the arrows.Proposition 20. For k agents, one needs only to consider O (m2k¯d( em2¯d)2k¯d) number of options for the set of moves of the agents.i( j)= I1 ). Each (cid:4)( j)is a union of an interval at left of u( j)= [u(cid:3), ur] of pointsProof. Let c1 > c2 > · · · > cd be the set of costs. For each agent j and for each ci there is an interval Ii( j)( j)⊆ Ii+1. Thus, considercovered while the agent’s remaining budget is at least ci . Furthermore, for each j and for all i, Ii( j)for each agent the incremental area covered when its remaining budget is ci but less than ci−1, (cid:4)( j)= Ii−1 (withi(cid:4)( j)( j)(both possibly empty).s1Since there is communication, an agent may continue to reach a store even if it does not have any chance of obtaining theitem there, in order to reveal the cost for the use of other agents. Thus, the optimal strategy may define also an interval( j)j is greater than 0. By Lemma 31 (see Appendix B), theI¯dmoves of each agent are fully determined by the leftmost and rightmost stores of each (cid:4)( j)the ending points of covering each area. Therefore, for each j there are at most (m)2¯d(2¯d)!external stores of the (cid:4)( j)of options for the set of moves is O (2k¯d( em2¯d, together with the choice for)2¯d possible choices for the¯d options to consider for the covering of each. Thus, the total number= [u(cid:3), ur] of points covered while the remaining budget of)2k¯d), which is polynomial (in m). (cid:2)and an interval at the right of u’s, and there are a total of 2i(cid:2) ( em2¯d− I( j)i( j)siIt thus remains to consider the scheduling between the moves, i.e. their order. Theoretically, with n moves there are n!different possible orderings. We show, however, that for any given set of moves, we need only to consider a polynomialnumber of possible orderings.Consider a given set of moves M, determining the sets (cid:4)( j)of M is said to be a prefix of M, if for each agent the moves in Mi. Note that for each agent, M fully determines the order ofare a prefix of theis a prefix. We now inductively define the notion of a(cid:6)(cid:6)is a suffix of M if M − Mthe moves of this agent. A subset Mmoves of this agent in M. A subset Mcascading order:(cid:6)(cid:6)1. The trivial order on moves of a single agent is cascading.2. Let M be a set of moves, and let ci0 be the highest cost (of the product) that any agent can pay. An order S on M iscascading if M and S can be decomposed in the form M = Mpre ∪ Mmid ∪ Mpost and S = Spre ◦ Smid ◦ Spost, such that:• Mpre is a prefix of M consisting only of moves of agents with budget less than ci0 and Spre is a cascading order onMpre.• There exists an agent j(cid:6)with budget at least ci0 such that Mmid consists of all the moves of j(cid:6))(cid:6)in (cid:4)( ji0and Smid isthe (one possible) order on these moves.• Mpost are the remaining moves in M and Spost is a cascading order on them.To visualize this definition, consider the example in Fig. 5. Given the depicted set of moves, the highest cost thatan agent can pay is 60, by agent 2 at u4. Therefore, a possible cascading order S can decompose M such that Mpre ={agent 3 goes to u7}, Mmid = {agent 2 goes to u4} and Mpost = {agent 1 goes to u1 and to u3}. Since each group containscan decompose M such thatonly moves of a single agent, S is a cascading order on M. Another possible cascading order SMpre = {agent 1 goes to u1 and to u3, agent 3 goes to u7}, Mmid = {agent 2 goes to u4} and Mpost = { }. Sis a cascading or-(cid:6)= {agent 1 goes first, agent 3 goes second}. We◦ Spost are trivial, and Sder on M if Snow prove (by induction) that cascading orders are optimal.(cid:6)post, where S(cid:6)mid and S(cid:6) = S(cid:6)mid(cid:6)pre(cid:6)pre◦ S(cid:6)(cid:6)Lemma 21. For any set of moves M there exists a cascading order with optimal success probability.42N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52(cid:6)Proof. The proof is by induction on the number of agents and the number of moves in M. If there is only one agent movingin M then the order is cascading. Otherwise, consider any other order S on M and let Ai0 be the set of agents with budgetat least ci0 . Let jand let t0 be the time it completes covering it. Mpre includesin (cid:4)( ji0all the moves taken by agents not in Ai0 prior to t0; Mmid includes all the moves of j; and Mpost the rest of themoves in M. We show that we do not decrease the success probability by first making all moves of Mpre then all thoseof Mmid, and finally those of Mpost. By the inductive hypothesis Spre, Smid and Spost are optimal for Mpre, Mmid and Mpost,respectively and the result follows.be the first agent in Ai0 to cover its (cid:4) ji0(cid:6))(cid:6)(cid:6)Before t0 all agents in Ai0 have a higher budget than any agent not in Ai0 . Thus, before t0 agents of Ai0 will never callupon those not in Ai0 . Thus, if we let the agents that are not in Ai0 take their moves first the success probability will notdecrease. We can thus allow all of the moves of Mpre to be performed first.Also, before t0 no agents of Ai0 needs to call upon each other for assistance (since they are all in the same resourcebracket). Thus, we may allow them to take their moves independently without decreasing the success probability. In par-ticular, we can allow jbefore any other member of Ai0 moves. Thus, we get that firsthaving the moves of Mpre and then of Mmid does not decrease the success probability. The moves of Mpost are the remainingmoves. (cid:2)to complete its covering of (cid:4)( ji0(cid:6))(cid:6)Finally we show that the number of cascading orders is polynomial:Lemma 22. For fixed k and d and any set of moves M there are a polynomial number of cascading orders on M.is a polynomial in n. Since (cid:3) (cid:2) k, the result follows. Clearly, for any (cid:3),Proof. Let f (n, k, d, (cid:3)) be the number of cascading orders with k agents, n moves, d costs and (cid:3) agents in Ai0 . We prove byf (n, k, 0, (cid:3)) = (cid:3)! (all of which areinduction that fuseless). Then, by the definition of cascading orders f (n, k, d, (cid:3)) (cid:2) (cid:3)nk−(cid:3) f (n, k − (cid:3), d − 1, k − (cid:3)) f (n, k, d, (cid:3) − 1) (the nk beingfor the choice of Mpre). By the inductive hypothesis f (n, k − (cid:3), d − 1, k − (cid:3)) and f (n, k, d, (cid:3) − 1) are polynomials in n. Thus,so is f (n, k, d, (cid:3)). (cid:2)Together with Proposition 20 we get that the total number of options to consider is polynomial, proving the k-Private-Max-Probability part of Theorem 19. The proof for the other two problems is similar.5. Self-interested agentsIn this section we consider the strategic behavior that may occur when the agents are self-interested. We assume kagents, operate in the same underlying physical setting as in the previous multi-agent case with private budgets, i.e. thestores are all on a single path, the number of possible prices, d, is bounded, and there is a fixed number of agents. Howeverin the self-interested agents setting, the agents seek to obtain the item but do not want to spend their individual budgetson travel costs; we assume the purchase price is equally shared among all the agents. In this case we define two games,a simultaneous game, Min-Budget-Game, and a sequential game, Min-Expected-Cost-Game.5.1. Min-Budget-GameIn the Min-Budget-Game we are given a target success probability psucc, and each agent’s objective is to minimize itsinitial budget necessary to guarantee that the item will be acquired with a probability of at least psucc. To avoid the casewhere each agent will set its initial budget at zero, we set the utility of not guaranteeing the success probability psucc so lowthat it will always be worthwhile to attain it. We assume the game is a simultaneous game; the agents can only choose theirinitial budgets. After this phase, the agents calculate the (collaborative) strategy that will maximize their success probability(given their chosen budgets) and follow it. The only decision point in this game is when an agent needs to choose itsbudget.Since the number of agents and the number of different costs is fixed, the optimal solution for k-Private-Min-Budgetdistinctcan be found in polynomial time, whether the agents can or cannot communicate (Theorem 19). Let Balgbe the initialbudget that was assigned to agent i by the algorithm from Theorem 19. This solution ofk-Private-Min-Budget distinct, whichis optimal, can be directly translated into a strategy, denote Optsoc: each agent i should individually choose its initial budgetto be B alg. Obviously, Optsoc maximizes the social welfare and it can be computed in polynomial time. Furthermore, Optsocis also a Nash equilibrium [44, p. 14]. Clearly, for each agent i, there is no incentive to deviate and to choose a budgetfor itself which is larger than Balgthe success probability psucc is already guaranteed (assuming the otheragents will not deviate). On the other hand, since the algorithm of Theorem 19 is optimal, there is no incentive for eachagent i to deviate and choose a budget for itself which is smaller than B alg, as psucc will not be achieved (recall that theutility of not guaranteeing the success probability is very low). We obtain:, since with BalgiiiiiN. Hazon et al. / Artificial Intelligence 196 (2013) 26–5243Theorem 23. In the Min-Budget-Game, the strategy that maximizes the social welfare, Optsoc, can be found in polynomial time and itis also a Nash equilibrium.5.2. Min-Expected-Cost gameIn the Min-Expected-Cost-Game each agent’s objective is to minimize its total expected cost. As in the previous game, toavoid the case where each agent will not want to make any move, we set the utility of not obtaining the item so low thatit is always worth traveling to at least one store to purchase the product. The Min-Expected-Cost-Game is a sequential gameand the rules are as follows. At each time step, only one of the agents is allowed to move to the next store, but it can alsodecide not to move at all. Then there is a decision phase, where every agent is allowed to buy the product, to opt-out, or todo nothing. If at least one agent decides to buy the product, it is purchased and then the game is over (even if other agentsdecide to opt-out). No matter how many agents decide to buy the product, only the one with minimal price is purchased.If no agent decides to buy the product and at least one agent decides to opt-out then the game is over without buyingthe product. Otherwise, the decision phase ends and the game proceeds by allowing the next agent (according to a fixed,pre-defined cyclic order) to move. The pre-defined order of movement phases well-define the game, but it has no essentialmeaning; the agents have the option not to move during their turns, so actually any order of movements may occur.In order to find the strategy that will maximize the social welfare, Optsoc, we need to run the algorithm from The-it will run in polynomial time. However, unlike in the Min-Budget-Game, the solution foundorem 8. In our setting,cannot be directly translated into a strategy. First, we need to translate the movements. At any stage, if the algorithmfor k-Shared-Min-Expected-Cost decides that a specific agent should move, for instance agent i, then the strategy forMin-Expected-Cost-Game defines that until it is agent i’s turn to move, any other agent will not move during its move-ment phase, and all the agents will do nothing during the decision phase. We also need to handle the case where one agentdoes not move according to this strategy. For this purpose, we determine that in any case where one of the agents deviatesfrom its determined policy in the movement phase, the other agents purchase the product during the decision phase thatfollows. If it is not possible, i.e. the product is not available where the agents are located, the other agents opt-out duringthe decision phase. The translation of the algorithm’s decision to buy is straightforward; the strategy defines that in thecorresponding decision phase all the agents decide to buy, and an agent that cannot buy opt-out. We also do not needto handle the case where one agent deviates in a decision phase, since the game will be over in that case. In conclusion,Optsoc, the strategy for Min-Expected-Cost-Game that maximizes the social welfare, can be found in polynomial time usingthe algorithm from Theorem 8. However, Optsoc is not always a Nash equilibrium, as will be shown in Example 24. For easeof notation, when describing Optsoc or any other strategy we omit the movement and decision phases when the agents donothing.Example 24. Suppose that the stores and agents are located as illustrated in Fig. 6. The traveling costs between u2 and u3and between u4 and u5 are so high, that the only reasonable moves are according to the illustrated arrows. Optsoc for thisexample is that agent 1 will go to u2. If the price is 6 the product will be purchased. Otherwise, agent 3 will go to u6and if the price is 6 the product will be purchased. Otherwise, agent 2 will go to u4 and the product will be purchasedat the minimal sampled price (which can be 12 or 27). The expected cost of this strategy is 14.375, but it is not a Nashequilibrium. Clearly, if the product was not purchased after the moves of agents 1 and 3, then the minimal sampled pricewill be 27. At this stage, if agent 2 deviates and decides not to move, the product will be purchased and the private cost ofagent 2 will be 9 (the purchase price is equally shared among all the agents). If agent 2 proceeds according to Optsoc, itsexpected cost will be 4 + 0.5 · 4 + 0.5 · 9 = 10.5 > 9. Therefore, agent 2 will have an incentive to deviate from Optsoc.If we switch the movement order of agents 2 and 3, the expected cost will be higher, 15.125, but this strategy isa Nash equilibrium. Clearly, agent 1 will not deviate during its turn since the other agents will opt-out. Agent 2 willnot deviate during its turn since its private cost will be 10, and if it will follow the strategy its expected cost will be4 + 0.5 · 4 + 0.5 · (0.5 · 2 + 0.5 · 9) = 8.75 < 10 (assuming the other agents will not deviate). Agent 3 will not deviate duringits turn either, since its private cost will be 10, and if it will follow the strategy its expected cost will be 4 + 0.5 · 2 + 0.5 · 9 =9.5 < 10.Example 24 demonstrates that Optsoc is not always a Nash equilibrium. We now show a polynomial algorithm that alwaysreturns a strategy which is a Nash equilibrium. Furthermore, we show an upper bound on the algorithm’s performance, andprove that it is tight.Theorem 25. There is a polynomial algorithm for finding a Nash equilibrium for the Min-Expected-Cost-Game.Proof. The algorithm works as follows. It divides all the buying costs by k, and then solves the finite-horizon MDP as inthe proof of Theorem 8. The solution is then translated into a strategy for the Min-Expected-Cost-Game, in the same waywe translated the optimal solution of k-Shared-Min-Expected-Cost to Optsoc. We denote this strategy by OptNash. Since thepre-process takes O (d) operations the algorithm for finding OptNash is polynomial.44N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52Fig. 6. A possible input with suggested moves. The numbers on the edges represent traveling costs. The table at each store ui represents the cost probabilityfunction pi (c). The reasonable moves are illustrated by the arrows.Any strategy S consists of traveling costs and buying costs, denoted by {ti} and {bi}, respectively. We can then write thei are the associated probabilities. We also write ti ∈ j if thei , pb· bi), where ptexpected cost of S as E[S] =traveling cost ti was credited to the movement of agent j.· ti) +i(pbii(pti(cid:8)(cid:8)We now analyze the steps of OptNash. First note that if the product is not available to any agent, there is no incentive todeviate since the other agents will opt-out and the game will be over. We thus assume that the product is available. Thelast step of OptNash is a decision step, where the product is purchased. By definition, there is an incentive to purchase theproduct in this step. In any other decision phase the strategy of OptNash is not to purchase the product. However, if agent jdeviates in a movement phase, the product will be purchased in the decision phase that follows. Therefore, we only needto consider the movement phases. Now, consider agent j and a movement phase r, and suppose that j needs to move in r.If j deviates (does not move), his expected cost will be c/k, the best price available now divided by the number of agents.Since OptNash is optimal (with respect to the modified buying costs),(6)(7)c/k (cid:4)(cid:4)(cid:2)(cid:3)+pti· ti(cid:4)(cid:2)(cid:3)pbi· bi/ki(cid:2)ri(cid:2)rIn addition,(cid:4)(cid:2)pti(cid:3)(cid:4)· ti(cid:4)(cid:2)(cid:3)pti· tii(cid:2)ri(cid:2)r,ti ∈ jCombining (6) and (7) we get,(cid:4)(cid:4)(cid:2)(cid:3)+pti· ti(cid:2)(cid:3)pbi· bi/kc/k (cid:4)i(cid:2)r,ti ∈ ji(cid:2)rwhere the right term is the expected cost of agent j if it follows OptNash. Therefore, agent j has no incentive to deviate. Thesame analysis shows that j does not have an incentive to deviate if it does need to move in r. (cid:2)OptNash is a Nash equilibrium, but it does not maximize the social welfare. Furthermore, there may be another Nashequilibrium which will yield a larger social welfare. For example, recall the settings in Example 24. In these settings, OptNashpolicy is that agent 1 will go to u2. If the price is 6 the product will be purchased. Otherwise, agent 3 will go to u6 andbuy the product at the minimal price (6 or 27). This is indeed a Nash equilibrium with an expected cost of 15.25. However,we already showed a better Nash equilibrium with an expected cost of 15.125. We now prove an upper bound on theperformance of OptNash; the expected cost of OptNash is no more than k times worse than the expected cost of Optsoc.Theorem 26. E[OptNash] (cid:2) k · E[Optsoc].] > k · E[OptsocProof. Suppose that E[OptNash(cid:4)(cid:2)(cid:3)(cid:4)(cid:2)]. Therefore,(cid:9)(cid:4)(cid:3)E[OptNash] =pti· ti+pbi· bi> k ·(cid:2)ptj· t j(cid:4)(cid:2)(cid:3)+pbj· b j(cid:10)(cid:3)= k · E[Optsoc]iijjThen,(cid:4)(cid:2)pti· ti/k(cid:4)(cid:2)(cid:3)+pbi· bi/k(cid:3)(cid:4)(cid:2)>(cid:4)(cid:2)(cid:3)+(cid:3)pbj· b jptj· t jiijjN. Hazon et al. / Artificial Intelligence 196 (2013) 26–5245Since ti > ti/k and b j > b j/k then,(cid:4)(cid:2)(cid:4)(cid:2)(cid:3)+pbi· bi/k(cid:3)(cid:4)(cid:2)>ptj· t j(cid:4)(cid:2)(cid:3)+pti· ti(cid:3)pbj· b j/kiijjThe left and right terms are the expected costs of OptNash and Optsoc, respectively, where all the buying costs are divided byk. Therefore, OptNash is not optimal in these settings. Contradiction. (cid:2)As for the lower bound, consider the following example.(1)Example 27. For any (cid:2) > 0, suppose that the price at u2 = uis k with a probability of 1, and the price at the leftmostsstore, u1 is 0 with a probability of 1. The traveling cost from u2 to u1 is 1 + (cid:2). In all other stores the price is very high andthe traveling costs between any other store to u2 is also very high, for instance 2k. Optsoc for this example is that agent 1will go left and buy the product at u1. The cost of this strategy is 1 + (cid:2), but it is not a Nash equilibrium. Clearly, agent 1will prefer to buy the product in its initial location, u2, since its own cost will be 1, instead of 1 + (cid:2) in Optsoc. The total costfrom this strategy will be k, and it is the only Nash equilibrium. Therefore, for any algorithm that finds a strategy S whichis a Nash equilibrium, E[S] ∈ Ω( k1+(cid:2)]), and the bound from Theorem 26 is tight.· E[Optsoc6. Heterogeneous agentsThe analysis so far assumes that all agents are of the same type, with identical capabilities. Specifically, the cost ofobtaining the item at any given store is assumed to be the same for all agents. However, agents may be of different typesand hence with different capabilities. For example, some agents may be equipped with a drilling arm, which allows themto consume less battery power while mining. In this section we consider situations of heterogeneous agents, and show thatthe results can be extended to such settings.While agents may have different capabilities, in many cases it is reasonable to assume that if one agent is more capablethan the other at one location, it is also more capable at all other locations (or at least not less capable). Hence the followingdefinition:Definition 28. We say that agents are inconsistent if there exist budgets B, Blocation i with budget B(cid:6), agents j, j(cid:6), and locations i, i(cid:6), such that at(cid:11)Pr[ j can obtain the item] < Pr(cid:6)j(cid:12)can obtain the itembut at location i(cid:6)with budget B(cid:6)(cid:11)Pr[ j can obtain the item] > Pr(cid:6)j(cid:12)can obtain the itemWe now show that the results of Section 4.1 can be extended to heterogeneous agents.Theorem 29. In the private budget and no communication setting, if the number of different costs for each agent is constant, thenk-Private-Max-Probability and k-Private-Min-Budgetidentical can be solved in polynomial time with any number of heterogeneousagents, provided that the agents are consistent.The algorithm is essentially the same dynamic programming algorithm described in Section 4.1. The consistency assump-tion is necessary for Lemmata 14 and 15 to remain true.In any other case, we can do away with the consistency assumption. Clearly, however, we do need to assume that uponreaching a site, agents can assess the cost for obtaining the item for all other agents. Otherwise, communication would bemeaningless. We obtain:Theorem 30. In the setting of communicating agents, with a constant number of agents, and a constant number of different costs foreach agent, k-Shared-Min-Expected-Cost, k-Shared-Min-Expected-Costphone, k-Shared-Max-Probability, k-Shared-Min-Budget,k-Private-Max-Probability, k-Private-Min-Budgetidentical and k-Private-Min-Budgetdistinct can be solved in polynomial time evenwith inconsistent heterogeneous agents.The algorithms and proofs remain essentially the same as those for the case of homogeneous agents.46N. Hazon et al. / Artificial Intelligence 196 (2013) 26–527. DiscussionIn this paper we mainly analyzed the case where the stores are located along a path (either closed or non-closed).We see fair applicability of the proposed model to real-life applications. The most important/appropriate application, asdiscussed in the paper, is robot patrolling (see [60,19,2,3]). The reason is that a common patrolling scheme is patrollingalong the surroundings of the area of interest, and the robot’s movement is, by the task definition, restricted to be alonga path even if there are shortcuts. However, there are additional important families of applications that the model can bemapped to – exploration along a path. Typical applications of this kind include:• Finding a place for camping along a path – consider an expedition or a group of campers that follows a trail or travelalong a river. When deciding on where to set their night camp, the group should consider different locations along theirtrail/river, each associated with some uncertainty related to the benefit from spending the night there.• Positioning scouts – consider a border-control squad, which needs to position itself for scouting (e.g., based on someinformation that illegal infiltrators will arrive during the next few hours). Many possible locations along the bordersegment can potentially be used by the squad, each offering a different visibility level and accessibility to differentpoint from where the infiltrators may arrive, which are a priori unknown (e.g., due to visibility conditions, terrainconditions and human factors).• Deciding on a restaurant – consider a group that wants to dine together in one of the numerous restaurants on BalboaBlvd in Newport Beach. Assume that the true utility from dining in any of the restaurants can be observed only oncegetting to it (e.g., after observing the menu, how crowd it is and getting an impression of the general atmosphere).• Deciding where to place bets – consider a visitor arriving to Las Vegas Blvd, interested in gambling in one of the casinosthere. Similar to the restaurants’ example, the gambler will be able to learn about the utility from gambling (not theexpected payoff, which is likely to be similar in all places, but rather the gambling experience in terms of atmosphere,crowd, excitement, etc.) in any given casino only upon visiting it.• Buying souvenirs – consider a tourist visiting the pier in Key West, Florida. Walking along the pier, the tourist will findmany souvenir stores, selling practically the same items, however in different prices. In this case, the tourist shouldconsider the tradeoff between the potential saving in cost and the alternative cost of time when going back and forth,visiting the different stores.• Finding the best place to install a spying device at some place along a communication line – assume there are severalpossible locations along the line that are applicable for installing the equipment, each characterized by a differentchance of being discovered or with a different chance of success.All the above applications are characterized by a physical search along a line with potential locations that can be explored,where there is a distribution of potential gains/utilities for each location. Indeed, numerous physical environments may onlybe represented by a planar graph. Theorems 1 and 6 show that physical search problems are hard even on planar graphsand trees, even with a single agent, but finding a heuristic is of practical interest nonetheless. It seems that the first stepsin building such a heuristic will be to utilize our results. For example, one should try to avoid repeated coverage as much aspossible and restrict the number of cases where such coverage is necessary, as we showed in Theorem 12. Another idea isto convert the complex graph structure into a path, where each site on the path represents a region of strongly-connectednodes on the original graph. Many graphs which represent real physical environments consist of some regions with strongly-connected nodes, but few edges connect these regions (for example, cities, have many roads inside but are connected byonly a few highways). A heuristic algorithm for these graphs may use our algorithm to construct a strategy for the sitesalong the path, and use an additional heuristic for visiting the sites inside a region.We also considered the case where mining costs are rounded/estimated to one of a constant number of possible options.We believe that this assumption is appropriate since the given input for our problems includes prior probabilistic knowledge.Usually, this data comes from some sort of estimation so it is reasonable to assume that the number of options is fixed; Ifthere are too many possible values, an accurate estimation is hard to achieve. Nevertheless, if the number of costs will notbe a constant it can be rounded to a fixed number of costs, which yields a PTAS (polynomial-time approximation scheme)for our problems.We also assume that the agents seek only one item. As soon as more than one item is needed, our results do not hold,and seemingly the problems become NP-complete.8. Conclusions and future workThis paper considers single and multi-agent physical search problems, with prior probabilistic knowledge. This integrationof changing search cost into economic search models is important, as it improves the realism and applicability of the mod-eled problem. At the same time, it also dramatically increases the complexity of determining the agents’ optimal strategies,precluding simple solutions such as easily computable reservation values (see for example “Pandora’s problem” [57], thatwas briefly discussed in Section 2.1.1). Indeed, we showed that our problems are hard on a metric space, sometimes even ifit is a tree. We then focused on the path case, presenting polynomial algorithms for the two variants of Min-Expected-CostN. Hazon et al. / Artificial Intelligence 196 (2013) 26–5247problem, and proving hardness for Max-Probability and Min-Budget problems. We provided FPTAS for Min-Budget in thatcase, and showed that both problems are polynomial if the number of possible prices is bounded.For the multi-agent case, we analyzed shared and private budget models. In the case of the shared budget, we showedhow all of the single-agent algorithms extend to k-agents, with the time bounds growing exponentially in k. We provedthat this is also the case with the private budget model, if the agents can communicate. In the case of the private budgetwith no communicating agents, we presented a polynomial algorithm that is suitable for any number of agents. We alsoextended the analysis to heterogeneous agents.Finally, we considered the self-interested agents setting, showing how to find a Nash equilibrium for Min-Budget-Gameand Min-Expected-Cost-Game in polynomial time. In both cases, we showed an upper bound on the ratio between thissolution to the optimal one (the one which maximizes the social welfare) and proved that it is tight.For future work, there are still many interesting open problems. With a single agent, the complexity of Min-Expected-Costproblem on a tree is yet to be explored. This case is interesting since it can be shown that Min-Expected-CostIn the shared budget model, the complexity ofis easy for a specific tree, namely star graph, where d is bounded.k-Shared-Min-Expected-Cost problem where k is part of the input is still open. In the private budget model, the complexity ofall the problems with a non-constant number of communicating agents is open. In addition, there are interesting extensionsto consider. We showed that most of our results can be extended to heterogeneous agents, with different buying capabilities.The next step is to analyze our results with heterogeneous agents with different traveling capabilities. Another direction isto add time constraint, which will possibly result with completely different optimal strategies. Currently, we assume thatcommunication between the agent is free and reliable; a possible extension is to integrate communication costs to ourmodel, and to handle the consequences of non-reliable communication. The Min-Budget-Game can also be extended; insteadof defining the utility of achieving psucc as a step function (“high” if achieving psucc, and “low” if not), it could be definedas a linear function of psucc. Finally, metric spaces beyond the line remain an open challenge. As discussed in Section 7, thetechniques and results given in this paper can facilitate the development of approximations and/or heuristics for the generalmetric space.Appendix A. Proofs for Section 2Theorem 1. For general metric spaces Min-Expected-Cost-Decide is NP-hard.(cid:8)nj=1 2Proof. The proof is by reduction from Hamiltonian path, defined as follows. Given a graph G = (V , E) with V ={v 1, . . . , vn}, decide whether there is a simple path (v i1 , v i2 , . . . , v in ) in G covering all nodes of V . The reduction is asfollows. Given a graph G = (V , E) with V = {v 1, . . . , vn}, set S (the set of stores) to be S = {us} ∪ {u1, . . . , un}, whereis the designated start location, and {u1, . . . , un} correspond to {v 1, . . . , vn}. The distances are defined as follows.usFor all i, j = 1, . . . , n, dis(us, ui) = 2n, and dis(ui, u j) is the length of the shortest path between v i and v jin G. Set−n(n! + n − 1 + 2n). For all i, pi(0) = 0.5, and pi(M) = 0.5, and for us, ps(n!) = 1.M = 2n +− j( j − 1) + 2Suppose that there is a Hamiltonian path H = (v i1 , v i2 , . . . , v in ) in G. Then, the following policy achieves an expectedcost of exactly M. Starting in us move to ui1 and continue traversing according to the Hamiltonian path. If at any point uialong the way the price is 0, purchase and stop. Otherwise continue to the next node on the path. If at all points along thepath the price is M, return to us and purchase there, where the price is n!. The expected cost of this policy is as follows.The price of the initial step (from us to ui1 ) is a fixed 2n. For each j, the probability to obtain price 0 at ui j but not before−n, in which case theis 2purchase price is n!, plus n − 1 wasted steps along the Hamiltonian path and a cost of 2n for returning to us. The totalexpected cost is thus exactly M.− j . The cost of reaching ui j from ui1 is j − 1. The probability that no u j has a price of 0 is 2Conversely, suppose that there is no Hamiltonian path in G. Clearly, since the price at us is so large, any optimal strategymust check all nodes/stores {u1, . . . , un} before purchasing at us. Since there is no Hamiltonian path in G, any such explo-ration would be strictly more expensive than the one with a Hamiltonian path. Thus, the expected cost would be strictlymore than M. (cid:2)Theorem 2. The Min-Budget-Decide problem is NP-complete even on a path.Proof. Given an optimal policy it is easy to compute its total cost and success probability in O (n) steps, thereforeMin-Budget-Decide is in NP. The proof of NP-hardness is by reduction from the 0–1 Knapsack problem, defined as fol-lows. Given a knapsack of capacity C > 0 and N items, where each item has value v i ∈ Z+, determinewhether there is a selection of items (δi = 1 if selected, 0 if not) that fits into the knapsack, i.e.i=1 δi si (cid:2) C , and the totalvalue,and size si ∈ Z+(cid:8)(cid:8)NNi=1 δi v i , is at least V .Given an instance of 0–1 Knapsack we build an instance for the Min-Budget-Decide problem as follows. We assumeWLOG that all the points are on the line. Our line consists of 2N + 2 stores. N stores correspond to the knapsack items,denoted by uk1 , . . . , ukN . The other N + 2 stores are denoted u g0 , u g1 , . . . , u gN+1 , where u g0 is the agent’s initial location. LetT = 2 ·i=1 si and maxV = N · maxi v i . For each odd i, u gi is to the right of u g0 and u gi+2 is to the right of u gi . For eacheven i (i (cid:12)= 0), u gi is to the left of u g0 and u gi+2 is to the left of u gi . We set |u0 − u1| = |u0 − u2| = T and for each i > 0(cid:8)N48N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52Fig. 7. Reduction of 0–1 Knapsack to Min-Budget-Decide problem used in the proof of Theorem 2, for N = 3.(cid:8)− u gi+2− ukiWe set B = T ·also, |u gi| = T . If N is odd (even) ukN is on the right (left) side of u gi and it is the rightmost (leftmost) point. As forthe other uki points, uki is located between u gi and u gi+2 , if i is odd, and between u gi+2 and u gi otherwise. For both cases,|u gi| = si . See Fig. 7 for an illustration.N+1j=1 j + 2C + 1 and for each i set X i = T ·i−1j=1 s j . At store u gN+1 either the product is−maxV , or not available at any price. On any other store u gi , either theavailable at the price of 1 with probability 1 − 2product is available at the price of B − X i with the same probability, or not available at all. At any store uki , either the−v i , or not available at any price. Finally, we setproduct is available at the price of B − X i − si , with probability 1 − 2psucc = 1 − 2−maxV·(N+1) · 2j=1 j + 2 ·−V .(cid:8)(cid:8)iN(cid:8)(cid:8)i=1(i · T + δi · 2si) + (N + 1) · T = T ·Suppose there is a selection of items that fit the knapsack with a total value of at least V , and consider the followingpolicy: go right from u g0 to u g1 . Then for each i = 1, 2, . . . , N, if δi = 0 (item i was not selected) change direction and goto the other side to u gi+1 . Otherwise, continue in the current direction to uki and only then change direction to u gi+1 . ThisN+1i=1 i + 2C = B − 1, thus the agent has enough budgetpolicy’s total travel cost isto reach all u gi , and uki with δi = 1. When the agent reaches u gi , i < N + 1 it has already spent on traveling cost exactly−maxV to purchase the product at this store. WhenT ·it reaches u gN+1 it is on the end of its tour and since the agent’s total traveling cost is B − 1, here it also has a probabilityi−1of 1 − 2j=1(δ j · s j) + si (cid:2)−v i to purchase the product in this store. In total, the success probability isX i + si so the agent has a probability of 1 − 21 − (2Ni=1 2−maxV to purchase the product. When it reaches uki it has already spent exactly T ·i−1j=1(δ j · s j) (cid:2) X i so the agent has a probability of 1 − 2−V ) = psucc as required.−v i ·δi ) (cid:4) 1 − (2−maxV·(N+1) · 2−maxV·(N+1) ·j=1 j + 2 ·j=1 j + 2 ·(cid:8)(cid:8)(cid:8)(cid:8)(cid:5)iiSuppose there is a policy, plc with a total travel cost that is less than or equal to B, and its success probability is at−V . Since maxV = N · maxi v i , plc must reachleast psucc. Hence, plc’s failure probability is at most 1 − psucc = 2all the N + 1 stores u gi with enough budget. Hence, plc must go right from u g0 to u g1 and then to each other u gi beforeu gi+1 . Therefore plc goes in a zigzag movement from one side of us to the other side and so on repeatedly. plc also has toselect some uki to reach with enough budget. Thus, plc has to reach these uki right after the corresponding store u gi . Weuse γi = 1 to indicate the event in which plc selects to reach uki right after u gi , and γi = 0 to denote the complementaryevent. plc’s total traveling cost is less than or equal to B − 1 to be able to purchase the product also at the last store,−V (cid:2)u gN+1 , so T ·1 − 2i=1 v i · γi . Setting δi = γi gives a selection of items that fit theknapsack. (cid:2)j=1 γ j · s j (cid:2) C . Also, psucc = 1 − 2j=1 γ j · s j (cid:2) T ·−V (cid:2)Ni=1 2N+1j=1 j + 2 ·(cid:5)Ni=1 2N+1j=1 j + 2C . Thus,−maxV·(N+1) · 2−maxV·(N+1) · 2−v i ·γi ⇒ V (cid:4)−maxV·(N+1) ·−v i ·γi ⇒ 2(cid:8)(cid:8)(cid:8)(cid:8)(cid:8)(cid:5)NNNTheorem 6. The Min-Budget-Decide problem is NP-complete on a tree, even with a bounded number of prices.Proof. Membership in NP is immediate as in the proof of Theorem 2. The proof of NP-hardness is by reduction from the0–1 Knapsack problem.Given an instance of 0–1 Knapsack we build an instance for the Min-Budget-Decide problem as follows. We have N + 2stores. N stores corresponds to the knapsack items, denoted by uk1 , . . . , ukN . The other 2 stores are u0 and ue , where u0 isthe agent’s initial location. The stores are placed on a star, which is a tree with one internal node, u0, and N + 1 leaves. Thedistance to any uki is defined according to the item size, dis(u0, uki ) = si/2, and dis(u0, ue) = C . At any store uki , either the−v i , or not available at any price. At store u0 the product is notproduct is available at the price of 0 with probability 1 − 2−maxV , maxV = N · maxi v i ,available, and at store ue either the product is available at the price of 0 with probability 1 − 2or not available at any price. Finally, we set psucc = 1 − 2−V , and B = 2 · C .−maxV · 2N(cid:8)Suppose there is a selection of items that fit the knapsack with a total value of at least V , and consider the followingpolicy: for each i = 1, 2, . . . , N, if δi = 1 (item i was selected) go from u0 to uki and then back to u0. Finally, go from u0i=1(δi · si) + C (cid:2) 2 · C = B. If the product is available at any store, its price is 0. Thus, theto ue . This policy’s travel cost issuccess probability of this policy is 1 − (2−v i ·δi ) (cid:4) 1 − (2Suppose there is a policy, plc with a total travel cost that is less than or equal to B, and its success probability is at−V . Since maxV = N · maxi v i , plc must reachleast psucc. Hence, plc’s failure probability is at most 1 − psucc = 2store ue . plc also has to select some uki to reach, but since dis(u0, ue) = C and B = 2 · C , plc must reach these uki beforereaching ue . We use γi = 1 to indicate the event in which plc selects to reach uki , and γi = 0 to denote the complementaryj=1 γ j · s j (cid:2) C . Also, psucc =event. plc’s traveling cost before going to ue is less than or equal C , to be able reach ue , so−V ) = psucc as required.−maxV · 2−maxV · 2−maxV ·Ni=1 2(cid:8)(cid:5)NN. Hazon et al. / Artificial Intelligence 196 (2013) 26–5249Ni=1 2−v i ·γi ⇒ 2−V (cid:2)(cid:5)Ni=1 2−v i ·γi ⇒ V (cid:4)(cid:8)Ni=1 v i · γi . Setting δi = γi gives a selection of−maxV · 2−maxV ·1 − 2items that fit the knapsack. (cid:2)−V (cid:2) 1 − 2(cid:5)Appendix B. Proofs for Section 3Theorem 8. With k agents, k-Shared-Min-Expected-Costphone can be solved in O (d22k( mcan be solved in O (d2k+12k( mk )4k).k )2k), and k-Shared-Min-Expected-CostProof. We start with k-Shared-Min-Expected-Costphone. Since the stores are on the path, at any point in time the points/storesvisited by the agents constitute a set of k disjoint contiguous intervals, which we call the visited intervals. Clearly, thealgorithm need only make decisions at store locations. Furthermore, decisions can be limited to times when the agents areat one of the two stores edges of the visited interval. At each such location, each agent has only three possible actions:“go right” – extending its visited-interval one store to the right, “go left” – extending its visited-interval one store to theleft, or “stop” – stopping the search and buying the product at the best price so far. Also note that after each agent i hasalready visited its interval [u(cid:3)(i) , ur(i) ], how exactly it covered this interval does not matter for any future decision; the costshave already been incurred. Accordingly, the states of the MDP are quadruplets [L, R, E, c], such that L = ((cid:3)(1), (cid:3)(2), . . . , (cid:3)(k)),R = (r(1), r(2), . . . , r(k)), E = (e(1), e(2), . . . , e(k)) and c ∈ D. For each agent i, (cid:3)(i) (cid:2) s(i) (cid:2) r(i) and e(i) ∈ {(cid:3)(i), r(i)}. Every suchstate represents the situation that each agent i visited stores u(cid:3)(i) through ur(i) , it is currently at location ue(i) , and the bestprice encountered so far is c. Since the intervals are disjoint, r(i) < (cid:3)(i+1) for every i.(cid:6)], for cThe terminal states are Buy(c) and all states where all the stores were visited. The terminal cost is c. For all other statesthere are at most 2k + 1 possible actions – “agent i go right” (provided that r(i) < (cid:3)(i+1) and r(i) < m), “agent i go left”(provided that r(i−1) < (cid:3)(i) and 1 < (cid:3)(i)), or “stop”. The cost of “agent i go right” is (ur(i)+1− ue(i) ), while the cost of “agenti go left” is (ue(i) − u(cid:3)(i)−1). The cost of “stop” is always 0. Given a vector V , let V i( j) be the same vector but with valuej at index i. Given the state [L, R, E, c] and move “agent i go right”, there is probability pr(i)+1(c(cid:6)) to transition to state(cid:6) < c. With the remaining probability, the transition is to state [L, R i(r(i) + 1), E i(r(i) +[L, R i(r(i) + 1), E i(r(i) + 1), c1), c]. Transition to all other states has zero probability. Transitions for the “agent i go left” actions are analogous, whilewith the action “stop” there is probability 1 to transition to state Buy(c). This fully defines the MDP. The optimal strategyfor finite-horizon MDPs can be determined using dynamic programming (see [45, Ch. 4]). In our case, the complexity canbe brought down to O (d22k( mk )2k) space).We now move to k-Shared-Min-Expected-Cost. Like in the single agent case, if an interval [u(cid:3)(i) , ur(i) ] has been visited byagent i and the item not yet purchased, then any future purchase within the interval (if there should be such a purchase)will be with an agent coming from outside the interval into the interval, and moving directly to a store for purchasing. Notethat even if the interval [u(cid:3)(i) , ur(i) ] has been visited by agent i, the purchaser may also be one of its immediate neighbors(i.e., agents i − 1 and i + 1). In addition, there is a unique store u, such any purchaser coming from anywhere to the rightof ur(i) and purchasing within [u(cid:3)(i) , ur(i) ] purchases at u, for purchases coming from theleft of u(cid:3)(i) . Therefore, the states of the MDP for k-Shared-Min-Expected-Cost are septuplets [L, R, E, C(cid:3), X(cid:3), Cr, Xr], such thatL = ((cid:3)(1), (cid:3)(2), . . . , (cid:3)(k)), R = (r(1), r(2), . . . , r(k)), E = (e(1), e(2), . . . , e(k)), C(cid:3) = (c(cid:3)(1) , c(cid:3)(2) , . . . , c(cid:3)(k) ), X(cid:3) = (x(cid:3)(1) , x(cid:3)(2) , . . . , x(cid:3)(k) ),Cr = (cr(1) , cr(2) , . . . , cr(k) ), and Xr = (xr(1) , xr(2) , . . . , xr(k) ). Every such state represents the situation that each agent i visitedstores u(cid:3)(i) through ur(i) , it is currently at location ue(i) , e(i) ∈ {(cid:3)(i), r(i)}, and the best price encountered so far in [u(cid:3)(i) , ur(i) ]when coming from the left (respectively right) is c(cid:3)(i) (cr(i) ), which can be found at store ux(cid:3)(i) (uxk )2k) steps (using O (d2k( mGiven a state [L, R, E, C(cid:3), X(cid:3), Cr, Xr], let BuyAt(i) be the minimum cost of purchasing within the visited interval of agenti, i.e., BuyAt(i) = min{ce(i) + |ue(i) − ux|, cr(i) + |ue(i+1) − ur(i) |, cl(i) + |ue(i−1) − ul(i) |}. The terminal states are Buy(i) witha terminal cost of BuyAt(i), and all states where all the stores were visited, with a terminal cost of mini∈k BuyAt(i). Theactions are the same as in the MDP for k-Shared-Min-Expected-Costphone, but the transition probabilities are different. Given(cid:6)) to transition to state [L, R i(r(i) +the state [L, R, E, C(cid:3), X(cid:3), Cr, Xr] and move “agent i go right”, there is probability pr(i)+1(c1), E i(r(i) + 1), C(cid:3), X(cid:3), C i|). With the remaining probability, the transitionis to state [L, R i(r(i) + 1), E i(r(i) + 1), C(cid:3), X(cid:3), Cr, Xr]. Transition to all other states has zero probability. Transitions for the“agent i go left” action are analogous. Given the state [L, R, E, C(cid:3), X(cid:3), Cr, Xr] and the action “stop”, there is probability 1 totransition to state Buy(i), i = arg mini∈k BuyAt(i). This fully defines the MDP. Using the same analysis as before, we get thatthe complexity of solving k-Shared-Min-Expected-Cost is O (d2k+12k( m(cid:6) < (cr(i) + |ur+1(i) − ux(cid:6)), X i(r(i) + 1)], for c, and similarly a unique store uk )4k) steps (using O (d2k2k( mk )4k) space). (cid:2)r(i) ).r(c(i)x(cid:3)(i)xr(i)xre(i)r(i)Theorem 9. With k agents, k-Shared-Min-Budget and k-Shared-Max-Probability with d possible prices can be solved inO (m2kd( em2kd )2kd).Proof. For brevity, we focus on the k-Shared-Max-Probability problem. The same algorithm and similar analysis work alsok-Shared-Min-Budget problem. Let c1 > c2 > · · · > cd be the set of costs. For each agent j and for each ci there is an interval( j)( j)i+1.Ii( j)Thus, consider for each agent the incremental area covered with remaining budget ci but less than ci−1, (cid:4)( j)i−1( j)= [u(cid:3), ur] of points covered while the remaining budget is at least ci . Furthermore, for each j and for all i, Ii= I⊆ I− I( j)ii50N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52Fig. 8. Reduction of the 0–1 Knapsack problem to the Multi-Min-Budget-Decide problem used in the proof of Theorem 11, for N = 3.( j)= I1 ). Each (cid:4)( j)(with (cid:4)( j)(both possibly empty).1The next lemma, which is the multi-agent Max-Probability analogue of Lemma 4 states that there are only two possibleoptimal strategies to cover each (cid:4)( j)is a union of an interval at left of uand an interval at the right of u( j)s( j)s:iiLemma 31. Consider the optimal solution and the incremental areas for each agent j, (cid:4)( j)For i ∈ 1, . . . , d, let uof (cid:4)( j)i( j)(usiri ). Furthermore, the starting point for covering (cid:4)( j)be the leftmost store in (cid:4)( j)starts at location u(cid:3) u(cid:3) uand u( j)(cid:3)i( j)ri( j)si( j)i( j)(cid:3)ii+1 is the ending point of covering (cid:4)( j)i.. Then, WLOG we may assume that the optimal strategy for each j is either (u(i = 1, . . . , d) defined by this solution.the rightmost store. Suppose that in the optimal strategy the coveringi( j)si(cid:3) u( j)ri(cid:3) u( j)(cid:3)i) orProof. Any strategy other than the ones specified in the lemma would reach all the stores covered by the optimal solutionwith at most the same available budget. (cid:2)together with the choice for the ending points of covering each area. For each two agents j1,( mk )2d(2d)! (cid:2) ( emBy the previous lemma, the moves of each agent are fully determined by the leftmost and rightmost stores of each (cid:4)( j),j2, the intervals of covered∩ I2kd )2d possible choices for the’s, and there are a total of 2d options to consider for the covering of each. For each option,2kd )2kd), which is polynomial( j1)points are disjoint, i.e. Idexternal stores of the (cid:4)( j)computing the budget and probability takes O (m) steps. Thus, the total time is O (m2kd( em(in m). (cid:2)= ∅. Therefore, for each j there are at most( j2)diiTheorem 10. With k agents, For any (cid:2) > 0, k-Shared-Min-Budget can be approximated to within a factor of (1 + k(cid:2)) in O (n(cid:2)−6k)steps (for arbitrary number of prices).Proof. For k agents, we extend the dynamic programming algorithm, which calculates fail[·, ·, ·, ·] and act[·, ·, ·, ·] tables, inthe same way we extended the single agent algorithm in the proof of Theorem 8. We now save k disjoint intervals, thusthe tables size becomes O ((cid:2)−6k). The rest of the approximation algorithm remains essentially the same. We still considerδ in powers of 2 up to β (cid:2) 2n, where n is the size of the input. Thus, the total computation time is O (n(cid:2)−6k). Since theapproximation ratio in each interval is guaranteed to be (1 + (cid:2)), we get a total ratio of (1 + k(cid:2)). (cid:2)Theorem 11. k-Shared-Min-Budget-Decide is NP-complete even on the path with a single price.Proof. An optimal policy defines for each time step which agent should move and in which direction. Since there are atmost 2m time steps, it is easy to compute the success probability and the total cost in O (m) steps, therefore the problemis in NP. The NP-hard reduction is from the 0–1 Knapsack problem.We assume WLOG that all the points are on the line. We use N agents and our line consists of 2N stores. N stores}i=1,..,N .(i+1)scorrespond to the knapsack items, denoted uk1 , . . . , ukN . The other N points are the starting point of the agents, {u(i)We set the left most point to usright after uki . Set |uand the right most point to ukN . For all 1 (cid:2) i (cid:2) N − 1 set uki right after u| = B + 1. See Fig. 8 for an illustration.The price at all the nodes is c0 = 1 and pki (1) = 1 − 2For every agent i, the only possible move is to node pki , denote by γi = 1 if agent i moves to pki , and 0 if not. Therefore,Ni=1 δi v i , is at least V iff there is a selection of−v i . Finally, set B = C + 1 and psucc = 1 − 2i=1 δi si (cid:2) C , and the total value,(cid:5)there is a selection of items that fit, i.e.,| = si and |uki(i)sand u− uki(i+1)s−V .− u(1)s(i)s(cid:8)(cid:8)(cid:8)NNi=1 γi si (cid:2) B, and the total probability 1 −Ni=1 γi2−v i , is at least psucc = 1 − 2−V . (cid:2)agents that move such thatReferences[1] F. Afrati, S. Cosmadakis, C. Papadimitriou, G. Papageorgiou, N. Papakonstantinou, The complexity of the traveling repairman problem, TheoreticalInformatics and Applications 20 (1986) 79–87.[2] N. Agmon, S. Kraus, G.A. Kaminka, Multi-robot perimeter patrol in adversarial settings, in: Proceedings of the 2008 IEEE International Conference onRobotics and Automation (ICRA-2008), 2008, pp. 2339–2345.N. Hazon et al. / Artificial Intelligence 196 (2013) 26–5251[3] N. Agmon, D. Urieli, P. Stone, Multiagent patrol generalized to complex environmental conditions, in: Proceedings of the Twenty-Fifth AAAI Conferenceon Artificial Intelligence (AAAI-2011), 2011, pp. 1090–1095.[4] S. Arora, G. Karakostas, Approximation schemes for minimum latency problems, in: Proceedings of the Thirty-First Annual ACM Symposium on Theoryof Computing (STOC-1999), 1999, pp. 688–693.[5] S. Arora, G. Karakostas, A 2 + (cid:2) approximation algorithm for the k-MST problem, in: Proceedings of the Eleventh Annual ACM-SIAM Symposium onDiscrete Algorithms (SODA-2000), 2000, pp. 754–759.[6] Y. Aumann, N. Hazon, S. Kraus, D. Sarne, Physical search problems applying economic search models, in: Proceedings of the Twenty-Third AAAIConference on Artificial Intelligence (AAAI-2008), 2008, pp. 9–16.[7] G. Ausiello, S. Leonardi, A. Marchetti-Spaccamela, On salesmen, repairmen, spiders, and other traveling agents, in: Algorithms and Complexity, Springer,Berlin/Heidelberg, 2000, pp. 1–16.[8] B. Awerbuch, Y. Azar, A. Blum, S. Vempala, Improved approximation guarantees for minimum weight k-trees and prize-collecting salesmen, SIAMJournal on Computing 28 (1) (1999) 254–262.[9] E. Balas, The prize collecting traveling salesman problem, Networks 19 (1989) 621–636.[10] J.C. Beck, N. Wilson, Job shop scheduling with probabilistic durations, in: Proceedings of the Sixteenth European Conference on Artificial Intelligence(ECAI-2004), 2004, pp. 652–656.[11] J.C. Beck, N. Wilson, Proactive algorithms for job shop scheduling with probabilistic durations, IEEE Transactions on Robotics 28 (1) (2007) 183–232.[12] R. Bellman, A Markovian decision process, Indiana University Mathematics Journal 6 (4) (1957) 679–684.[13] D. Bernstein, D. Givan, N. Immerman, S. Zilberstein, The complexity of decentralized control of Markov decision processes, Mathematics of OperationsResearch 27 (4) (2002) 819–840.[14] L. Bianco, A. Mingozzi, S. Ricciardelli, The traveling salesman problem with cumulative costs, Networks 23 (2) (1993) 81–91.[15] A. Blum, P. Chalasani, D. Coppersmith, B. Pulleyblank, P. Raghavan, M. Sudan, The minimum latency problem, in: Proceedings of the Twenty-SixthAnnual ACM Symposium on the Theory of Computing (STOC-1994), 1994, pp. 163–171.[16] A. Blum, R. Ravi, S. Vempala, A constant-factor approximation algorithm for the k-MST problem, Journal of Computer and System Sciences 58 (1)(1999) 101–108.[17] A.M. Campbell, M. Gendreau, B.W. Thomas, The orienteering problem with stochastic travel and service times, Annals of Operations Research 186 (1)(2011) 61–81.[18] K. Chaudhuri, B. Godfrey, S. Rao, K. Talwar, Paths, trees, and minimum latency tour, in: Proceedings of the 44th Annual IEEE Symposium on Foundationsof Computer Science (FOCS-2003), 2003, pp. 36–45.[19] Y. Elmaliach, A. Shiloni, G.A. Kaminka, A realistic model of frequency-based multi-robot fence patrolling, in: Proceedings of the 7th International JointConference on Autonomous Agents and Multiagent Systems (AAMAS-2008), 2008, pp. 63–70.[20] J. Fakcharoenphol, C. Harrelson, S. Rao, The k-traveling repairman problem, in: Proceedings of the Fourteenth Annual ACM–SIAM Symposium onDiscrete Algorithms (SODA-2003), 2003, pp. 655–664.[21] J. Fakcharoenphol, C. Harrelson, S. Rao, The k-traveling repairmen problem, ACM Transactions on Algorithms 3 (4) (2007) 40.[22] T.S. Ferguson, Who solved the secretary problem?, Statistical Science 4 (3) (1989) 282–289.[23] M. Fischetti, G. Laporte, S. Martello, The delivery man problem and cumulative matroids, Operations Research 41 (1993) 1055–1064.[24] N. Fu, P. Varakantham, H.C. Lau, Towards finding robust execution strategies for RCPSP/max with durational uncertainty, in: Proceedings of the Twen-tieth International Conference on Automated Planning and Scheduling (ICAPS-2010), 2010, pp. 73–80.[25] Y. Gabriely, E. Rimon, Spanning-tree based coverage of continuous areas by a mobile robot, Annals of Mathematics and Artificial Intelligence 31 (2001)77–98.[26] S. Gal, Search Games, Academic Press, 1980.[27] A. García, P. Jodrá, J. Tejel, A note on the traveling repairman problem, Networks 40 (2002) 27–31.[28] N. Garg, A 3-approximation for the minimum tree spanning k vertices, in: Proceedings of the 37th Annual IEEE Symposium on Foundations of ComputerScience (FOCS-1996), 1996, pp. 302–309.[29] N. Garg, Saving an epsilon: a 2-approximation for the k-MST problem in graphs, in: Proceedings of the Thirty-Seventh Annual ACM Symposium onTheory of Computing (STOC-2005), 2005, pp. 396–402.[30] M. Goemans, J. Kleinberg, An improved approximation ratio for the minimum latency problem, in: Proceedings of the Seventh Annual ACM-SIAMSymposium on Discrete Algorithms (SODA-1996), 1996, pp. 152–158.[31] N. Hazon, Y. Aumann, S. Kraus, Collaborative multi agent physical search with probabilistic knowledge, in: Proceedings of the Twenty-first InternationalJoint Conference on Artificial Intelligence (IJCAI-2009), 2009, pp. 164–167.[32] N. Hazon, G.A. Kaminka, Redundancy, efficiency, and robustness in multi-robot coverage, in: Proceedings of the 2005 IEEE International Conference onRobotics and Automation (ICRA-2005), 2005, pp. 735–741.[33] W. Herroelen, R. Leus, Project scheduling under uncertainty: Survey and research potentials, European Journal of Operational Research 165 (2) (2005)289–306.[34] T. ˙Ilhan, S.M.R. Iravani, M.S. Daskin, The orienteering problem with stochastic profits, IIE Transactions 40 (4) (2008) 406–421.[35] J. Kephart, A. Greenwald, Shopbot economics, Autonomous Agents and Multi-Agent Systems 5 (3) (2002) 255–287.[36] J. Kephart, J. Hanson, A. Greenwald, Dynamic pricing by software agents, Computer Networks 32 (6) (2000) 731–752.[37] S. Koenig, M. Likhachev, Fast replanning for navigation in unknown terrain, IEEE Transactions on Robotics 21 (3) (2005) 354–363.[38] B.O. Koopman, Search and Screening: General Principles with Historical Applications, Pergamon Press, 1980.[39] E. Koutsoupias, C.H. Papadimitriou, M. Yannakakis, Searching a fixed graph, in: Proceedings of the 23rd International Colloquium on Automata, Lan-guages and Programming (ICALP-1996), 1996, pp. 280–289.[40] S. Lippman, J. McCall, The economics of job search: A survey, Economic Inquiry 14 (1976) 155–189.[41] A. Lucena, Time-dependent traveling salesman problem – the deliveryman case, Networks 20 (6) (1990) 753–763.[42] J. McMillan, M. Rothschild, Search, in: R. Aumann, S. Amsterdam (Eds.), Handbook of Game Theory with Economic Applications, Elsevier, 1994, pp. 905–927 (Chapter 27).[43] E. Minieka, The delivery man problem on a tree network, Annals of Operations Research 18 (1–4) (1989) 261–266.[44] M.J. Osborne, A. Rubinstein, A Course in Game Theory, The MIT Press, 1994.[45] M.L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, Wiley–Interscience, 1994.[46] I. Rochlin, D. Sarne, M. Laifenfeld, Coordinated exploration with a shared goal in costly environments, in: Proceedings of the 20th European Conferenceon Artificial Intelligence (ECAI-2012), 2012, pp. 690–695.[47] S. Sahni, T. Gonzales, P-complete problems and approximate solutions, in: Proceedings of the 15th Annual Symposium on Switching and AutomataTheory (SWAT-1974), 1974, pp. 28–32.[48] D. Sarne, S. Kraus, Managing parallel inquiries in agents’ two-sided search, Artificial Intelligence 172 (4–5) (2008) 541–569.[49] L.S. Shapley, Stochastic games, Proceedings of the National Academy of Sciences of the USA 39 (10) (1953) 1095–1100.[50] D. Simchi-Levi, O. Berman, Minimizing the total flow time of n jobs on a network, IIE Transactions 23 (3) (1991) 236–244.52N. Hazon et al. / Artificial Intelligence 196 (2013) 26–52[51] R. Sitters, The minimum latency problem is NP-hard for weighted trees, in: Proceedings of the 9th Conference on Integer Programming and Combina-torial Optimization (IPCO-2002), 2002, pp. 230–239.[52] S.V. Spires, S.Y. Goldsmith, Exhaustive geographic search with mobile robots along space-filling curves, in: First International Workshop on CollectiveRobotics, 1998, pp. 1–12.∗[53] A. Stentz, The focussed Dalgorithm for real-time replanning, in: Proceedings of the 14th International Joint Conference on Artificial Intelligence(IJCAI-1995), 1995, pp. 1652–1659.∗[54] X. Sun, W. Yeoh, S. Koenig, Dynamic fringe-saving ASystems (AAMAS-2009), 2009, pp. 891–898., in: Proceedings of the 8th International Joint Conference on Autonomous Agents and Multiagent[55] T. Tsiligirides, Heuristic methods applied to orienteering, Journal of the Operational Research Society 35 (9) (1984) 797–809.[56] I.R. Webb, Depth-first solutions for the deliveryman problem on tree-like networks: an evaluation using a permutation model, Transportation Sci-ence 30 (2) (1996) 134–147.[57] Martin L. Weitzman, Optimal search for the best alternative, Econometrica 47 (3) (May 1979) 641–654.[58] R.J.V. Wiel, N.V. Sahinidis, Heuristic bounds and test problem generation for the time dependent traveling salesman problem, Transportation Sci-ence 29 (2) (1995) 167–183.[59] T. Will, Extremal results and algorithms for degree sequences of graphs, PhD thesis, University of Illinois at Urbana–Champaign, 1993.[60] K. Williams, J. Burdick, Multi-robot boundary coverage with plan revision, in: Proceedings of the 2006 IEEE International Conference on Robotics andAutomation (ICRA-2006), 2006, pp. 1716–1723.[61] C. Yang, A dynamic programming algorithm for the travelling repairman problem, Asia-Pacific Journal of Operations Research 6 (10) (1989) 192–206.