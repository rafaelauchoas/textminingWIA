ELSEVIER Artificial Intelligence 82 (1996) 21-44 Artificial Intelligence Probably approximately optimal satisficing strategies * Russell Greiner a-*, Pekka Orponen b ’ Siemens Corporate Research, 75.5 College Road, East Princeton, NJ 08540, USA h Department of Computer Science, University of Helsinki, PO. Box 26, FIN-00014 Helsinki, Finland Received September 1990; revised December 1992 Abstract A satis$cirzg search problem consists of a set of probabilistic of successes experiments to be performed and failures. The expected cost and on the order configuration of the individual experiments, the experiments strategy, which specifies that minimizes that compute optimal strategies in some order, seeking a satisfying of the search depends both on the success probabilities are to be performed. the search in which the expected cost is optimal. Earlier work has provided “optimizing A strategy functions” from the success for certain classes of search problems probabilities of the individual experiments. We extend those results by providing a general model optimal strategy when of such strategies, and an algorithm PA0 first estimates the probability values are not known. The algorithm from a number of trials of each undetermined and then uses these estimates, and the proper experiment, to identify a strategy whose cost is, with high probability, close to optimal. optimizing the PA0 We also show algorithm can also “learn while doing”, the search. that if the search problem can be formulated that identifies an approximately statistics while performing the relevant probabilities as an and-or the necessary i.e. gather tree, then function, * Some of this work was performed while the authors were at the University of Toronto, supported by an operating grant Academy of Finland. The authors useful comments on earlier versions of this paper. Preliminary versions of parts of the work have appeared the conference the National Science and Engineering Research Council of Canada, and by the referees for their in thank Dale Schuurmans, Tom Hancock and the anonymous [ 191 and [ IO]. respectively reports from * Corresponding author. E-mail: greiner@scr.siemens.com. 0004-3702/96/$15.00 SSD10004-3702(95)00010-0 @ 1996 Elsevier Science B.V. All rights reserved 22 R. Greinec P. OrponedArtificial Intelligence 82 (1996) 21-44 1. Introduction Consider the following situation: there are two reliable tests for deciding whether an the blood to obtain a diagnosis. Using strategy 01 = (blood, individual has hepatitis; one involves a blood test and the other a liver biopsy. Assuming a doctor there can be false negatives but no false positives, first can follow perform If liver, and conclude his diagnosis based on the not, he would the patient’s performs result of that biopsy. The doctor’s other option, strategy 02 = (liver, these tests in the other order-first the blood test. blood), if necessary, the liver test and then, only there are two “strategies” the patient has hepatitis if that test is positive. test and conclude then examine he would liver), is a strategy Which strategy is better? Our goal the measurement, we assume that will perform well in practice. that the to evaluate. We can then define a strategy’s expected cost as the to perform To quantify doctor will be asked average cost required patients. Assuming, for now, that these tests (blood strategy 01 is clearly better if the probability of a positive blood test (pi) liver test (pi); the probability these tests, averaged over the distribution of anticipated have the same cost, and liver) is larger than strategy 02 is preferable. is a distribution of a positive of patients otherwise, there Earlier research on this decision making model has produced a number of “optimizing that each identify a strategy optimal for a specific values of the relevant experiments is that the probability [ 6,7,18,21,22]. values are in practice testing situation, given the A limitation of typically not that are to identify the number of trials of each experiment to obtain estimates of these probability values that are good enough strategy, with high confidence. It also addresses the complexities of from the doctor’s situation and defines strategies, the PA0 algorithm, trials of each experiment approximately to identify optimal. The algorithm presumes for the class of search structures and-or trees, the PA0 and optimal strategies, a general process to a general class of for these that uses a is, with the existence of an considered. When dealing with can “learn while a strategy whose cost algorithm An extended version of this paper, available statistics while solving relevant performance tasks. report of the basic algorithm presented here. as a technical [ 1 I], discusses techniques, functions” success probability these known a priori. This paper specifies required a nearly-optimal observing however, this many Section 2 below trials. first generalizes structures” “decision arbitrary structures. Section 3 then specifies set of observed high probability, optimizing certain doing”, search structures, i.e. gather the necessary function notably several variants and applications 2. Framework 2.1. Decision structures The doctor’s task presented (term due to Simon and Kadane problem configuration of events: in Section 1 is a simple example of a satisjcing search [ 2 1 ] ) , as his goal is to find a single satisfactory of test results. Other combination in this case, an informative R. Greiner; l? Orlmnen/Arrijicial Intelligence 82 (1996) 21-44 23 [ Patient has hepatitis] f- el f e2 --l Fig. I An and-or tree representation of a decision structure GI examples whether a product a position treasure chests general, can constraints. involve of such problems specimen include, is satisfactory [6], competing for prizes at a quiz show [ 211, and performing inference [ 61, screening employment [6], mining in simple expert systems e.g., performing a sequence of tests such tasks may involve searching through general “decision an arbitrary number of experiments, constrained candidates to decide for in In structures”, which by various precedence for gold buried [ 10,221. and the arcs encode the precedence trees: More general versions of this diagnostic task can be represented trees, such as G1 in Fig. 1. Here, the nodes {A, el, . . . , eg} correspond the doctor cannot serum) and found that el succeeds. The experiment associated with the A node is formally from a given line, e.g., the graph in Fig. 1 states that the relationships-e.g., the patient’s blood reacts with a particular the e2-e5 arc to the e2-e6 arc). Hence, to succeed. The set of arcs descending to draw blood from the patient) (here indicated by a horizontal i.e. it is guaranteed el (attempted or conjunctive e3 (test whether experiment And-or decision by and-or decision to experiments, perform experiment until he has performed moreover, degenerate, node can be either disjunctive, connecting patient has hepatitis holds. The number near each arc designates costs units incremental arcs that must be traversed. “collapsed” reduce el cost of performing the top eo node to e3 (test I unit to further iff the condition to simpler to reduce two-level to the el subgoal the cost of traversing it and 2 more and so forth. The is the sum of the costs of the additional (This cost specification means such trees cannot always be the blood against serum-A), each experiment that arc-hence (draw blood), [ el A (e3 V e4) ] V [ e2 A e5 A e6] on the experiments trees.) trees: The general class of decision than and-or trees. First, and-or structures we shall consider trees can encode only simple Beyond and-or strictly more general mulae, which can include each experiment only once, and whose connections “and”s and “or”s. In general, we may want to express more complicated ships of the experiments; experiments”. ships; some complicated Third, and-or of performing performed. can only be performed if has succeeded or failed. cost have been In general, we may want the cost to depend also on whether e, and/or vari- interrelation- or “at least 3 of 5 specified relation- simple precedence e.g., the XOR of m experiments, relatively e can depend only on which other experiments that an experiment of other experiments in general, we may want to specify in which the incremental is for- are only form of cost function, trees use a restricted boolean combination trees only permit Second, and-or experiment 24 R. Greiner: I? OrponedArtijicial Intelligence 82 (1996) 21-44 ous prior experiments, more complicated ways of computing experiment; see the extended paper [ 111. have been successful. There are also situations which require yet a particular cost of performing the incremental these extensions, we define a more general class of “decision structure involve an arbitrary can constraints To accommodate tures”. A decision with general precedence formed until after certain other specified experiments (success or failure) specified can correspond has hepatitis) failures of any subset of these experiments, of experiments “decision structures” defined below. can be given by an arbitrary nondecreasing result. The overall to an arbitrary boolean combination test result have been performed with struc- set of experiments W = {ei}y=l, from being per- the the patient of the successes and a sequence function. This leads to the (e.g., whether that can prevent an experiment and the costs of performing Notation: Given two sequences, (T = (al,. formed by concatenating CT and r-i.e. is extended to the sequence The definition manner. A sequence all i = 1, . . . , II, for some monotonically is trivially a subsequence of any sequence u. (T is a subsequence of sequence CT. r = (al,. . . , a,) and Q- = (71,. . . , T,,), let g. 7 refer . . , cnr 71,. . . , r,,). in the obvious (T r r, if Ci = rh(i) for () function h. The empty sequence r, denoted increasing to the case where u or r are single elements Definition 1 (Decision where structures). A decision structure is a four-tuple G = (x F; R, c) 0 W = {e] , . F C [Wx{+,-}I* . , e,,} is a set of experiments. x W is a precedence iments can be performed given (E.g., F( (), el) means e2) means was successful, following that experiment conditions use The the results of a previous relation that specifies which further exper- sequence of experiments. (ex-)), and F( ((el+), initially; that ei may be performed e2 may be performed after et has been performed and and then es has been performed but was unsuccessful.) (abbreviated ei E W, each sequence must satisfy “l/es”). This ii E {+,-}, the notion of a legal is a sequence of the form ((et&t), and no e E W appears more labeled experiment sequence . . . , (ek&)), where each the specified by the F relation: a sequence holds for than once. Furthermore, the precedence constraints if F( ((elkI), is a lles only . . . , (ek*k)) , k. The collection of all such sequences . . . , (enr_trt,_t)),e,,) is denoted LLIS(G). ! = ((el*i), l,... all m= l R : CLSS(G) + {S, F,U} is th e result function renders sequence labeled experiment legal R maps each lles to one of {S, F,U} require R to be monotonic, R(a) = .F 3 R( IJ. T) = 3 whenever c and cr. r are lles. lRt is the cost function to be nondecreasing: in the sense l c : LL&S( G) + cost. It is required lles. the overall that specifies whether a given i.e. test successful or not; (for Success, Failure, and Undecided). We that R(U) = S + R((T . 7) = S and that maps each lles to its nonnegative c(a real .T) 2 c(u) whenever g and g. 7 are We let DS refer to the class of all such decision structures. To illustrate these definitions: the diagnostic tree of Fig. 1 can be encoded structure Gi = ({ ei , . , es}, FI , RI, cl), where e.g. es corresponds as a to “a patient’s blood R. Greiner, P. OrponedArtijicial Intelligence 82 (1996) 21-44 2s to mean that es (respectively, reacts with Serum-A” ical test”. The F, relation, and FI (0, ez) but not F,(((el-)),e3) dicate ceeded. The RI function R~(((ez+), and RI (((el-), of performing 1 +2+ (es+), Notice reachable, subsequence conditions FI (((el -)),e2) When that the structure there and e5 corresponds encoding the precedence to “a patient’s that et or e2 can be performed (respectively Fl(((e~+)),ed) e4) can be performed includes Rl(((el+),(es+))) if and only relationships, initially; but not FI(((~I-)),e4)) and FI (((el+)), liver has a positive cytolog- includes FI ( (), el ) e3) to in- if et has already suc- = S, = 3, cost = the incremental = S, Rl(((e1+),(e4+))) cl encodes (e6+))) and (ez+))) = 24, etc. The cost function = s so forth, as well as Rl(((el-),(ez-))) any sequence of experiments: 1 =4 and cr(((er+),(es-),(ez+),(e6+j))) that for standard graph-like decision structures, whenever an example becomes cr (((er +), (e3+), for instance, = 1 +2+2+ (e4-))I 1 =6. it stays so until performed, i.e. F(a,e) of r and r is a lles that does not include experiment implies F(7, e) whenever (T is a e. For instance, both the condition FI (0, e2 ). for every experiment, we say f o 11 ow from condition is a unique minimal and FI (((e~+)),ez) reachability is “tree-like”. Formally, we define: Definition tree-like path(e) 2 (Tree-like decision structures). A decision if we can identify with each experiment , that encodes the necessary and sufficient conditions e E W a single minimal structure G = (W, F, R, c) is lles, denoted for reaching e; i.e. Ve E W 3path(e) E CC&S(G) V’a E fXCES(G) F(cT,e) u [path(e) C a]. The class of all tree-like decision structures is denoted ‘TDS. When the structure G represents an and-or decision tree, the lles path(e) corresponds to the unique path leading to the experiment goes through ea and el, path(e4) = ((eat-), e in the tree: e.g., as the path to eq in Gt (et+)). 2.2. Satisficing search strategies A “search strategy” for a satisficing the associated decision structure-in through the doctor when to perform which tests to determine whether A strategy can be represented as a binary tree; for example search problem the order of traversal the sample application Gi of Fig. 1, it tells specifies strategy 01 for the decision structure. The strategy specifies tree is labeled with an experiment in any given situation. For example, right side of Fig. 2 represents one possible Each internal node in the strategy some node in the decision to be performed the experiment +-labeled that test succeeds, Or advances up to the S-labeled node, signifying with success. Alternatively, to the tree rooted definition of this process in the e4-labeled node, is as follows: arc to the strategy subtree rooted if e3 fails, 01 then follows et associated with @i’s root ui. If et succeeds, 01 then performs the --labeled the sequence of experiments first performs the in the es-labeled node, and performs e3. If the 01 strategy then follows that 01 terminates arc, descending e4, and so forth. A general the patient has hepatitis. the tree shown on the structure Gi. to that corresponds 26 R. Greinec l? Orponen/Artijiciul Intelligence 82 (1996) 21-44 In each node of the strategy associated to the node is indicated tree below, the experiment ei together with the name of the node, uk. + s 0 IWI t s + rzo1 + s _ ?= r::, _ t _ a [&I r:;1 _ 3 3 S + e1 1111 1 -dd 69 Fig. 2. Decision structure GI (above) and an associated strategy tree 01 (below). binary Definition 3 (Search strategies). A strategy for a decision structure G = (q E R, c) is tree 0 = (N, A, Z,V, Z,J), where N is the set of nodes and a node- and arc-labeled A C N x N is the set of arcs connecting 1~ maps each internal node n E N in the tree to an experiment node to either S or 3. The arc-labeling internal node must have exactly e E W, and each leaf 1~ maps each arc a E A to either + or -. Each to their descendants. The node-labeling arcs, one labeled + and the other -. two descending nodes A path T in 0 is an alternating sequence of nodes and arcs leading from the root a sequence of the form r = (121 ,a1,2,n2,U2,3,. of the tree to a leaf-i.e. nk) where each ni E N, and each Ui,i+t = (ni,ni+t) E A. Each such path has an associated . . . , (1N(nk_,)zA(Uk__l,k))). For labeled experiment 0 to be a proper strategy for G, the following conditions must be fulfilled by each path ?r=(nr,ar,2...,nk) sequence E(r) = ((E~(nr)Z.4(at.2)), in& . . ,ak-l,k. ( 1) l(r) E U&S(G) (i.e. l(rr) must be a legal labeled experiment sequence); (2) (3) R. Greiner. P OrponedArtijcial Intelligence 82 (1996) 21-44 21 = R( l(r)) IN(Q) ‘Lsuccess” or “failure”) E {S, F} ; and (i.e. the label of the final node must be either = U for al] j < k where Zt ,_.., j(r) R(Zt ,..,,, i(r)) (1N(n;)lA(ai,.j+l))) IS t e sequence proper prefix of a path can be conclusive). h of the first j elements = ((lN(fi1)iA(ai,2f)t.. of Z(n) , (i.e. no We let path(@) let SS(G) SS(Ds) structures. refer = (&S(G) to the set of all such proper paths refer to the set of all strategies defined for the decision to the class of all strategies in the strategy 0. We also structure G, and for all decision 1 G E Ds} refer For an illustration of these notions, side of Fig. 2. There are 10 paths dicated by the letters S and F (1~1,a~,~~,u~~,a~~,1s,~~~,a~5.~6,~~6), forwhichthecorrespondingllesisl(rrl6) in the figure). For instance, one such path see the strategy in 01, one corresponding tree 01 shown on to each leaf node the right (in- is ~16 = =((e~-), (0+)3 (es-)). 2.3. Optimal strategies We wish to identify the best strategy for traversing a given decision structure, strategy whose expected cost is minimal. As this depends on the success probabilities experiments, the individual To state this more precisely, we define: different strategies will be optimal for different distributions. i.e. the of cost of a strategy). Definition 4 (Expected ture G = (W,ER,c), experiment distribution strategy, weighted by its probability, to its success probability. The (expected) p, denoted C, (0), Let 0 be a strategy for the decision struc- that maps each to the is defined as the sum of the cost of each path in the function cost of strategy 0 relative and p : W + [0, l] be a distribution i.e. C,,(O) = c epath(@) P(Z(T)) x c(Z(r)). Here the probability of a path rr is defined as p(Z(rr)) = n(r,*,jEI(,rj Pan (ei), where p*(e) is p(e) if f equals +, and 1 -p(e) if * equals -. Definition 5 (Optimizing structures D C VS is a function OSS that maps any decision structure G = (w F; R, c) E is D, together with a distribution minimal. That is, p E [0, 1 ] w, to a strategy in SS(G) whose cost for a class of decision An optimizing functions). function VG E D vp E [O, llW v’o E SS(G) C,,(OSS(G,p)) < C,,(O). For brevity, when denote by O,, the optimal a given distribution p. the decision structure G is understood from strategy OSS(G,p) provided by the optimizing the context, we often for function 28 R. Greinec l? Orponen/Art$cial intelligence 82 (1996) 21-44 While these definitions assume that the experiments are independent both these definitions plicated situations. and the theorems below could be extended of each other, to handle more com- Since we are only dealing with finite decision structures, optimal strategies can always impractical, such as and-or be found by exhaustive and if we are dealing with decision trees, the optimal strategies may not even have polynomial-size structures with concise encodings, search. Of course, exhaustive representations. is in general search strategies Nevertheless, are allowed; cf. also optimal special cases. Garey the constraints in polynomial for finding as a regular “or tree” subgoals and no multiple predecessors [ 211 later extended can be determined [6] provided an algorithm can be represented teresting strategy when junctive Kadane this algorithm cial case where success at any intermediate global success is currently not known whether optimal strategies can be found and-or presents an efficient algorithm case. and Smith the more general case of and-or dag’s, the problem probabilities time in many in- the optimal search (i.e. no con- [ 221). Simon and to deal with directed acyclic graphs in the spe- (In dag’s where node implies global success. [ 81.) It time for [ 181 in this In is NP-hard even when all the success results on this question exist: for instance, Natarajan search strategies reaching a specified goal node, the problem for finding optimal “serial strategies”. for finding optimal “depth-first” [22] provides an algorithm trees. Some partial in polynomial are 1 [ 201. is NP-hard requires 3. The PA0 algorithm Each of the above-mentioned algorithms assumes optimization that the precise success are known, which of course is not the case in most real- a these probabilities by observing and then use these estimates to compute a near-optimal is that the strategies computed estimates: lead to drastically different strategies. Fortunately, to errors in the probability of the experiments probabilities life situations. The best one can do then is to estimate set of trials of the experiments, strategy. A potential pitfall by any of the above algorithms small changes even though cost of the strategy obtained this paper, as it means the choice of the actual strategy is not. This realization in the estimates may are very sensitive in this approach, however, is very sensitive errors, is one of the main contributions to estimation the of strategy. Below, we describe an algorithm PA0 that can be used in conjunction with any opti- that we can use our estimates to obtain a near-optimal and outlines function OSS. Section 3.1 first formally defines the task of finding approximately strategies the algorithm. The following issues in more detail. First, in Section 3.2 we compute mizing optimal technical of this task: how many samples of each experiment high level of confidence, to optimal. then discuss the the sample complexity to guarantee, with a estimates will be close that a strategy based on the resulting are needed sections Section 3.3 then addresses a second problem: guaranteeing that the PAO algorithm to obtain a sufficient number of samples of each experiment. The main in the context of arises that the (Fig. 1 ), the sample complexity analysis may suggest the precedence For example, constraints. example from will be able complication our diagnostic R. Greiner; I? Orpnen/Ar@cial Intelligence 82 (1996) 21-44 29 Algorithm PAO( G : DS, E : R+, 6 : (0, l] ) p^ + GS(G,a,@ I* GS may call oracle 0 a polynomial number of times *I & + OSS(G,@) Return 8 End PAO Fig. 3. Outline of the PA0 algorithm doctor needs impossible e2 in structure Gi never succeeds. for general “tree-like” decision general. to obtain 100 samples of the test “CytologicalTest if he is never able to perform a biopsy on any patients; In Section 3.3 we provide a solution structures, but also observe that the task is intractable (liver)“. This is i.e. if the experiment to this problem in 3.1. The PA0 task A PA0 problem instance consists of: a decision structure G = (W, E R, c) E DS; a the required confidence S E (0, 1 ] . The from some samples drawn at random that produces bound on the allowed excess E E iR+; and also uses an oracle 0 algorithm fixed but unknown distribution. the PA0 is, with high probability, For each algorithm instance, expected O,, = OSS (G, p) be the optimal with probability than the cost of this optimal strategy, cost returns close at least 1 - 8, the cost of the strategy O,,,, i.e. strategy for a true, but unknown, distribution to optimal. Stated more precisely, a strategy O,,, E SS( G), whose let p. Then than E higher is no more We split the PA0 task into two subtasks: statistics, and OSS, which uses those statistics Fig. 3. subroutine GS, which gathers to produce an appropriate the relevant strategy; see The GS subroutine takes as input the decision structure G and the parameters E them. The subroutine to the oracle c3 (specified the specified number produces for to obtain . . , j$,), where each @i is the estimate below) estimates, p^ = (fit,. of the ith experiment that we do not know a priori and 6; it computes how many samples are required, and makes of calls a vector of probability the success probability we are assuming the experiments. values directly, and not bother with the estimation.) The PA0 algorithm function OSS on these estimated probabilities @, by running instead of the unknown part of the PA0 algorithm, GS; for the OSS functions, we rely on the ones provided by earlier researchers. of any of to know some of the values, we can simply use those then concludes true values. We concentrate here on the sample-gathering ei E W. (To simplify our description, an appropriate optimizing the success probabilities If we happen 30 R. Greinec P: OrponedArtifkial Intelligence 82 (1996) 21-44 3.2. Sample complexity We first analyze can always perform Here we assume access the sample complexity of the PA0 task in the simple case where we the experiments whose success probabilities we need to estimate. to an oracle 0 that will, upon request, produce a sample ~,i from the population, together with its complete labeling C(K,~) = (t{, . . . ,!i), where ei E W, and 0 otherwise. The GS routine performs .$ is 1 if ~.i passes experiment number M (specified below) calls to this c3 oracle, and, returns a vector of probability estimates 6 = (61 , . . . ,@,?), where each p^i = 1 /M cy=, $; function that the cost of the then uses these values. We prove strategy O,,, = OSS(G,p^) at least 1 - 6, whenever (Corollary A.1 in Appendix A) is within E of the optimal, with reliability the OSS optimizing a M= p(!$)21n?gn!, where C is the worst-case cost of performing in W. In fact, we can improve on the constant C somewhat. Let us denote by D(e) any sequence of experiments maximal cost of any concluding Formally: sequence of experiments beginning with experiment the e. Definition we define 6. Let G = (W, F: R, c) be a decision structure. For each experiment e E W D(e) = max{c(c-u . (e*) . p) - c(a) 1 ff. (ef) p E fXES(G)}. (The 3~ above indicates that the max should range over both + and - values.) We can then let C = maxeEw{D( e)} be the maximum remainder cost starting with any values are quite easy to compute when the underlying structure e E W. These D(e) is an and-or experiment decision sum of the costs of all of the tree’s arcs and c(path(e)) unique path in the and-or (For D(e4) = 10 - 2 = 8.) instance, tree that leads from the root to experiment in the tree Gt of Fig. 2 we have Ct,t = 10 and c(path(e4)) tree: here, D(e) = CtOt - c(path(e)), where Crot is the the e; see Definition 2. = 2, so is the cost of path(e), result that also takes the experiments; The sample complexity bound ( 1) is derived into account more general (i.e. performing simple case: we first prove that after M samples, we are at least 1 - 6 confident probability directly on the definition of the cost of a strategy and independent function the cost of the obtained in Appendix A as Corollary A.1 of a the samples the difficulty of labeling the proof for this that each of the correct value pi; we then show, based of which optimizing is used, that this precision of the probability see below). To very briefly outline estimates suffices to guarantee estimate @i is within e/2nC is within E of the optimal, i.e. that strategy that R. Greiner; l? OrponedArtificial Intelligence 82 (1996) 21-44 31 3.3. “Learning while doing” in tree-like structures The simple PA0 algorithm presented above assumes that the oracle 0 produces a (e,, labeling for each sample, Instead, component i.e. it returns a complete vector C(K) = the learning the statistics system must collect e; values of fZ( K) ) while watching a performance complete . . . , !,,) E (0, 1 }” on each query. In practical situations, however, such an oracle will typically not it needs be available. the individual element perform large set of samples. In the context of our diagnostic example, its task, over a sufficiently recording how the learning module would observe the many of these patients pass the various learner would compute to the approximately itself. ’ We view this as a “learning while doing” use this O,,,, strategy, and terminate the learning phase protocol (here, examining [ 161, as the overall system is performing useful work during the doctor as he examines patients, optimal strategy @,,ao, instruct tests. After gathering enough information, the doctor patients). (i.e. From now on, we assume our oracle 0, when queried, provides only an unlabeled In order the value of any label !; on sample K, the GS subroutine must then actually than the full labelings of that sample, (e.g., a patient K), rather ,(Z( K). sample to determine “reach” and perform experiment e, on K. is problematic when these I?; values in the case of our decision Computing for instance, determine whether a patient’s blood will react draw blood from the patient. Hence, our learning probability blood reacts (i.e. if Pr[Draw blood] = 0). of event “blood to serumA” there are intermediate structure Gi, the doctor cannot experiments: immediately he must to serum-A; system will be unable to the if the doctor is never able to extract first be able to estimate there Fortunately, of “reaching” is a way around let p( ei) be the probability (This notion is the fol- the execution ef during lowing: is very small, we will of a strategy. the be unlikely is to the smaller that we also need fewer samples of value of the success probability e;. In the limit, if there is no chance of reaching ei (i.e. p(e;) = 0), then we will also need no samples of it (i.e. OSS can produce an optimal strategy even if Ij_$ - pI 1 = 1) this problem. The critical observation an experiment If p(ei) to obtain samples of this experiment. However, the cost of the optimal strategy to reach ei and hence p (e;), which means the value of p( ei), the less sensitive formally below.) is defined Definition 7. Let G = (W, F, R, c) E 27)s be a decision a distribution strategy 0 = (N, A, IN, IA) E SS( G), and any experiment probability that maps each experiment that 0 will reach e, i.e. function structure, and p : W --f [ 0, 1 ] to its success probability. For any e E W, let p( e, 0) be the P(e,@) = C p(l(n-(n))), ,7:Il$(n)=r ’ We are still considering only “one-shot learning”, learning phase. We are not considering ways of modifying mentally better; but see [ 9 1. Also, this issue differs from the “Exploration-Exploitation” in the context of the Bandit problem cost of the learning and performance (cf. 13, I7 I) as we are not concerned with minimizing together, over an infinite sequence of samples. systems in which the learner sets the strategy only once, after the incre- the strategy gradually over time to become trade-off discussed the cumulative 32 R. Greiner: II Or~xmen/Artijkial Intelligence 82 (1996) 21-44 is over nodes n in the strategy 0 labeled with e, r(n) the sum where 0 that leads Definition 4. Finally, to n, and the probability let p(e) = max{p( e, 0) of this path p(Z(r(n) ( 0 E &S(G)}. )) is as defined above is the path in in ie The formula for p(e) reduces to a articularly simple form when the decision structure In this case p(e) = fli=, p*‘(fi), where path(e) = ((ftztt), G is tree-like. is the unique path Fig. 1, we have path(e4) = ((eo, +), (et, +)), and so p(e4) = p(eo) x p(et ). to e in G. For instance, in the Gt decision that leads . . . , (fk&)) structure of Now let O,, = OSS( G,p) be the actual optimal correct probability vector p = (pi,. our PA0 algorithm will produce, based on the estimates j? = (J?, ) lemma need only ensure . . . , a,). We wish to bound the cost difference C, (0~) -C, shows in place of obtaining that, that the product p( ei) x Ipi - @i 1 is small for each ei. 2 precise estimates of the probabilities . , p,), and 06 = OSS(G, j3) be the strategy strategy based on the unknown that the GS subroutine has obtained, (Or,). The following pi, we , pn) be a vector of success probabilities Lemma 8. Let G = (w F, R, c) be a decision structure with 1 WI = n experiments. Let a . , a) to p be be the strategy based on the estimated for K and j? = (81,. p = (PI,... vector of their estimates. Let the optimal search strategy for G with respect @,, = OSS( G,p), probabilities. let 0~ = OSS(G,p^) Then and C,,(@p) - C,,(@,,) < 2kDCeil X p(ei> X IPi -@il. i=l true distribution A further complication now arises from the fact that the p(ei) values actually depend these values p. Fortunately, we can also approximate the estimates of the pi. In essence, we need only “aim for ei” a time we reach ei, we improve our estimate of pi (i.e. the Ipi - PiI “error bars”) and each time our path to ei is blocked, we can, with on the unknown as we are obtaining certain number of times: each reduce confidence, the value of p( e;). reduce The rest of this subsection first shows how to estimate the products p( ei) x lpi - Ijil in computing near-optimal in tree-like decision strategies in more general structures. structures, then discusses the difficulties (e2*2), . . . , (ek&)) that path(e) = ((eiztr), Dealing with tree-like decision structures: Given an experiment e in a tree-like decision is the unique minimal structure G, recall lles that determines when e can be performed. We say that a strategy 0 E SS(G) direct strategy for e if it contains of 0 is labeled with theexperiment fi equals experiment es, and so on, down to a node labeled ek, whose &labeled with e. We denote example, is a in the sense that the root arc if to a node labeled with the to a node labeled with arc leads to a node labeled e by &S(e). As an to et and hence 01 E SS(et ); et, and its *t-labeled arc if&t equals -) descends this lles as an initial segment, arc from that node descends the class of direct strategies the strategy 0, shown in Fig. 2 goes directly arc (i.e. the +-labeled e2, and the &-labeled f, and the --labeled for an experiment 2 Appendix A contains the proofs for all lemmata, theorems and corollaries presented in the text. R. Creiner; P. OrponedArtijicial Intelligence 82 (1996) 21-44 33 Algorithm GS( G: IDS, ForEach eEW E: lR+, 6: (0, 1 ] ) do Find some 0, E SS(e) tot(e) c 0 t 0 sue(e) End ForEach While se such that m(e) > 0 do Get sample K from oracle c? Execute strategy 0, on sample K After performing each experiment e,i: tot(ej) +- tot(e,j) + 1 i?Z( ej) + ??Z(e.j) - 1 If ej succeeds: suc(ej) t~~c(ej)+l If t?j'S result e cannot or m(e) +- m(e) reached: (success failure) be means - 1 End While ForEach B(ei) + e; E W do suc( ei) ~ tot( ej) 1 z if tot(ei) > 0 otherwise End ForEach Return P= @(ei),...,@(e,,)) End GS Fig. 4. A GS algorithm for tree-like decision structures. it also contains the other hand, similarly 01 @’ SS(e2) The GS algorithm the direct route to ea as an initial segment, and hence 01 E SS(e3). On the strategy “digresses” to consider es before e4, and so 01 @’ SS(e4); as 01 considers et before e2. shown first identifies in Fig. 4 can deal with any tree-like decision a direct strategy 0, E SS(e) The algorithm can in general be many such strategies, performing common ternatives. Nor does it consider observe that for e.g. and-or the tree structure.) After selecting with each experiment, number of times experiment and the number of attempts GS updates each of these counters by: tot( ei), suc( ei) and m( ei), ei has been performed, that remain the cost of identifying outside different experiments structure G. for each e E W. (There their the al- any of these strategies, except to from three counters the that will record, the number of times ei succeeded, the instances, time GS performs respectively, tot(ei) each trees they can be constructed quite efficiently, directly this set of strategies, GS associates to be performed. As it processes incrementing initial path to e; this paper does not consider how to choose between 34 R. Greiner; I? OrponedArtijicial Intelligence 82 (1996) 21-44 e; incrementing experiment menting m( ei) each time GS has attempted e,, or by using the strategy 0, but failing suc(ei) The remaining challenge to reach ei. is to identify when to use which strategy. each time the experiment to reach experiment and decre- e succeeds; ei either by performing trials of the different experiments throughout.) On each sample, GS first identifies not, in general, be able to observe enough same strategy i.e. those e’s for which m(e) > 0. If there are none, samples, and so can terminate, will pass to the OSS algorithm. Otherwise, GS selects one of the needy experiments and executes Notice the needy experiments, enough the obtained vector of estimates p^, which PA0 e, the associated that GS decrements then GS has collected at least one m(e) strategy 0,. returning (Clearly GS will if it uses the e to which the mo( e) are the initial values of the counters), counter on each sample, viz. the one aiming. Hence, after at most all of requires only a it is currently it therefore terminate; (The algorithm may of course use far fewer samples, the m(e,i) values for several different experiments e,j. counters will be zero and GS will associated with the experiment (where c c,EW mo( e) samples the m(e) polynomial as most 0, strategies will reduce GS can also be changed unreachable behavior of the algorithm: number of samples. to decrease the counters of all experiments in the process of following O,.) The following e’ that are deemed the lemma characterizes Lemma 9. Let G = (W, F: R, c) be a tree-like decision structure with 1 WI = n experi- ments, and let p = (PI,. . . , p,,) be a vector of success probabilities for the experiments. let E, 6 > 0 be any given constants, and let j? = ($1, . . . , a,,) be a vector Furthermore, estimates computed by the GS algorithm of Fig. 4. Then of probability \v’ei E W Pr [D(ei) X p(ei) X Ipi-pIil 3 &] 6 f. (While our analysis uses the p( ei) values, notice that the GS algorithm never actually the following the results of Lemmas 8 and 9, we obtain them.) Combining computes theorem: . . , p,,) be a vector of success probabilities Theorem 10. Let G = (W, F, R, c) be a tree-like decision structure with 1 WI = n experi- for the experiments. ments, and let p = (PI,. Furthermore, let E, S > 0 be any given constants, and let BP,, = PAO( G, E, S) be the strategy produced by the PA0 algorithm using the GS subroutine of Fig. 4. Then, with probability is the optimal strategy for probability - C!,( 0,) < E, where 0, = OSS(G,p) at least 1 - 6, C,, (O,,,) vector p. tree-like decision structures: While the specific GS algorithm presented above that there can be other related algorithms to tree-like decision Beyond applies only can learn strategies p( e;), as required fact that there can be many distinct ways of reaching an experiment structure. for other decision structures. The main challenge the product p( ei) x Ipi - p^i/, which is complicated by the in a genera1 decision is in estimating structures, to bound To address this task, recall from Definition 7 that p(e) e, where the maximum is the maximum probability of is taken over all possible strategies. We the experiment reaching R. Greinec P: OrponedArtificial Intelligence 82 (1996) 21-44 3s at least 1 - S/]&S(G) the maximum of these values: this value by first estimating p( e, 0) for every possible strategy if each estimate $( e, 0) is within E of 1, then the value b(e) = max{b(e, 0) 1 can always approximate 0, and then taking p( e, 0) with probability .a of p(e) = max{p( e, 0) 1 0 E SS( G)} with probability 0 t SS( G)} will be within for a given decision the number of strategies though at least 1 - S. Even structure G can be exponential the structure of G, and hence of SS( G), to limit the number of @(e, 0) values that need to be considered. From this point of view, the GS algorithm is based on the observation necessarily yield use the same GS algorithm whenever strategy 0, This in the size of G, there can be ways of exploiting e. In fact, one can e with a structure G, the direct strategies 0, E SS(e) the largest values of p( e, 0,) is not always straightforward. for which p( e, 0,) = p(e). to identify each experiment for tree-like structures that for any tree-like The extended paper for any experiment it is possible in SS(G) that uses dynamic programming each “layer” of certain result shows exponential that the computational in the number of experiments. techniques to sequentially types of decision structures. Unfortunately, complexity of any such algorithm [ 1 l] estimate includes an algorithm the probabilities of the following general to be is likely time algorithms with one-way error, cf time algorithm, and consequently no deterministic polynomial Theorem 11. Assume RP # NP. (RP is the class of problems solvable by probabilistic [ 121.) Then there is no probabilistic polynomial time algorithm polynomial e E W a distribution that, given a decision structure G = (W, E R, c), an experiment function p : W + the value p(e) within F with probability at least 1 - 6. l] , and parameters E, S > 0, can estimate [0, to 4. Conclusion The results presented required search strategies some specifically the user to supply precise success probability [2,6,18,21,22]. defined class of decision in this paper have been motivated by, and extend, various other objective of finding a provably good search strategy Each of satisficing structures for the in three ways. First, we have strategies”, which and “search in this lines of research. The underlying comes from the work on optimal these earlier papers considered and, moreover, experiments. Our work extends defined a general encompasses very general setting, estimates. Third, we have provided an efficient algorithm the probability RP = NP) to errors in the probability for finding good estimates of values in the case of tree-like decision structures, and proved that (unless this body of research structures” the models used before. Second, we have analyzed, the sensitivity of optimal search strategies there can be no efficient algorithm for this task for general structures. and generalizes of “decision framework values Our approach also resembles [ 5,14,15] learning” the work on speed-up and “chunking” tion-based suggest a way of improving systems, however, use only a single example those works by showing how to use a set of samples and by describing, [ 131)) as it uses previous solutions system. Most speed-up learning to suggest an improvement; we extend the the speed of a performance furthermore, to learning (including both “explana- 36 R. Greiner; P Orponen/Arti$cial Intelligence 82 (1996) 21-44 exact number of samples on purely heuristic considerations, we use mathematically that our new strategies will be close to optimal, with provably high probability. required. Also, while most speed-up learning systems are based to guarantee sound techniques Finally, this work derives many of its mathematical methods, as well as its title, from [23]. We hope to have enriched learning” of the PAC framework outside of its traditional an application approximately correct the field of “probably this field by providing setting of concept learning. Appendix A. Proofs This appendix contains the proofs of the results mentioned in the body of the paper. Lemma 8. Let G = (K E R, c) be a decision structure with 1 WI = n experiments. Let . . ,a,,) a to p be be the strategy based on the estimated p = (Pl>...>P,,) vector of their estimates. Let the optimal search strategy for G with respect 0, = OSS( G,p), probabilities. be a vector of success probabilities and let 0, = OSS(G,/?) for G, and p^ = (al,. Then C,7(@p) - C,(@,) < ZeD(ei) X p(ei) X /Pi -PiI. i=l the vectors p and ~5, let p(‘) denote . . . , p,,), Proof. Given and as special cases, p(O) = p and p(“) = 8. We shall prove below that for any strategy 0 for G, and for every i = 0,. . . , n - 1, the vector (at , . . , @it pi+l, /C,,,~I(O) -C,,,,,l,(O>I < D(ei) X p(ei) X /Pi-@iI+ (A.1) which implies that IC,,(@) - CI;(@)I < )JC,jc~,(@) -C,jcr-lb(0)I <CD(ei) Xp(ei) X I$i-ppiI. i=l i=l Applying CD (0,)) < 0, then yields this bound the desired result: to the strategies O,> and OF, and noting that by optimality Cb (Ot) - C,,(@,) - C,,(@,,) = ]C,,(@p) - C/?(@,)l + [Cfi(@p) - Cp(@p>l + [C/7(@,) - Cp<@p,l G kD(ei) [ i=l X P(ei) X lpi-pi =2kD(ei) X p(ei) X Ifii-PiI i=l 1 [ i=l +0+ kD(ei) Xp(ei) X I$i-PiI 1 R. Greiner, P. Orponen/Artijicial Intelligence 82 (1996) 21-44 u.i Uj' + A U.7 $I . . . 0 W! rjf = (uj,+) + Tjt . \ \ \ . . . t(uj> / / / . . . Fig. A. I. Illustration of notation In proving inequality (A.l), we shall make use of the following given any node u,, in a strategy 0, let Tj denote the path Fig. A.l): the root of 0 briefly by c(rrl). the final entry extend to U,i. For any such path Tj, we denote the associated cost c(l(T,i)) (Here, we have extended the I( .) function in T,i, does not have to be a leaf node to partial sequences: u,i, tree.) We also in the strategy the cost function to incomplete paths in the strategy tree by defining notation leading (cf. from C((~~i~~i,i+I3~i+l~. .,Uk)) = ~~(~O~~O,l~~~~~~~~~i~~;,i+l~~i+l~~~~~~~)~~~~(~O~~O,l~~l~~~~~~i)~~ where ua is the root node of the strategy the tree; furthermore, connected path through tree and (ua,ao,t, ut , . . . , uk-l,k,uk) is any for single nodes we define c( Ui) = c( (ui)) . the set of leaf nodes Let uf be the +-descendant of node U,i, and let t(Ui) denote below u,. For each U[ E t(u.i), let Tjl denote the path from uj to ~1, and if LQ E t( I*;), let T$ denote the +-subtree the path from node u.7 to Z.Q. Let C,(Of) of u,i”: denote the “expected cost of Analogous definitions hold for u.7, t( ui), p(r_l), and C, (07). For a given experiment ei E W, let N(ei) = iuj)j=t,,,,,k be the set nodes in 0 labeled with e,. Let 0 / ei be the subtree within 0 consisting of the paths from the root of 0 of through a node to a leaf; and let 0 ( Pi be the subtree consisting in N(ei) down 38 R. Greinec P: Orponen/Art@cial Intelligence 82 (1996) 21-44 the other paths. Notice paths in O+ according any strategy 0, an experiment a leaf node), and use this representation the influences of ei’s success probability that C, (0) = Cu( 0 1 ei) + C, (0 1 Fi>. We may partition to which of the u,i nodes each passes the that in e can occur at most once on any path from the root to through (recall to obtain a very simple formula expressing pi on the function C, (0 1 ei) (and hence on C,,(O)): = = p(m)c(m) 1 P(Tj)PiP(r,$) c(T,j) + C(Ltj+) +c(~i) + C P(Ti)(l LI/Et(u,-) k r =CPtr.i) [Pi .j= I C u/Et(u;) -Pi)P(r.i) ( C(Tj) +C(Uj) +C(r,i) , P(T$) ( C(rj) + C(U.7) + C(?rj:) \ J + (l -Pi) C P(T,T)(C(Tj) +C(U,F) +C(?Ti)) u,Et(u, f =$ I[( P(T’) P’ Cc(rj) + C(U,t>> C p(Ti> + &p(T$)C(T:) u/w;) j=l )I 1 > + (l -Pi) (c(r,j) +C(u/)) C p(T,;) ( u/Et(u,-) R. Greiner; l? Orponen/Artificial Intelligence 82 (1996) 21-44 39 = k c j=l P(r,j> Pi(C(rj> [ +Cp(@,‘)) + (1 -Pi)(C(rj) +CpC@~~)) I PtT.j) CCnj) +PiCp(@,T) + (1 -Pi)Cp(@,‘) =$ ( . 1 (Near the end of the proof we have simplified system of elementary probabilities sums to 1: in this case the formulas using the fact that a complete c P($) = c h-j) wwu; ) ulEt(u,-) = 1.) An analogous formula can be derived (Pl,.‘.,Pi-l,d;,Pi+l,... ith value. Using now the facts that CP( 0 1 pi) = Ce, (0 1 gi) and CP(@,T) = CD!@), ,pn) is the probability for the cost function Cb,( 0 / ei), where vector that differs from JJ only $, = in the as none of the substrategies following bound: 0 1 Ei, 07, j # i, involve the experiment e,, we obtain the IC,,C@) -Cfiz(@)l = ~(C,I<@ 1 ei) +C,(O j a,)) - (Ci,(O / ei) +Cjj,(O I Zi))i = I(Cp(@ ( ei) - Cp;(@ / e;)) + (Cp(O I Fi) - Cfi,(O I Zi))l k G CPb-j) x IPi -PiI j=l < P(ei) X JPi-P^i) x pox_ max , {C,, ( 07 ), C,> ( 0,; ) } The last line of the calculation p( e,, 0) p(ej) = ltlaX{p(ei, is the probability All that remains that this strategy 0 will reach ei, and hence E &S(G)}. 0) is to show that the value maxl(uj)=Y,{CP( OJ’), C,,(@,;)} I 0 uses the facts that CIJ( .) 3 0 and that Cr, p(r,j) = is bounded by is bounded by D (e;). TO see this, consider any U,j such that 1( uj) = ei. Then <c(q) + ( max c(rj:) u!Er(u,+) 1 (A.21 [ 1,4] : 40 R. Greinec P. Orponen/Arti$cial Intelligence 82 (1996) 21-44 = ctu,:) + max ~(97;) u/Er(u;) = max C(~TJ) U/Ef(Uj ) < Dte;>. Similarly, C,,(@,;) < maxl,,Ef(L,-j c(7rjl) < D(ei). I 0 Lemma 9. Let G = (w F, R, c) be a tree-like decision structure with merits, and fet p = (PI,. E, 6 > 0 be any given constants, and let ~5 = (@I,. . . ,$,,) be a vector of probability estimates computed by the GS algorithm of Fig. 4. Then . . ,p,,) be a vector of success probabilities. Furthermore, lW\ = n experi- let Ye; E W Pr [D(e;) x p(e;) x Ipi -piI > f-1 < 8. Proof. We use Hoeffding’s let (X;} be a set of independent, whose common mean samples. Then inequality, which is a simple form of Chernoff bounds identically distributed random Bernoulli variables, is p. Let SC”“) = 1 /h4 CE, Xi be the sample mean after taking M Pr[ SCM) > p + A] < e-2Mh2, Pr[ SCM) > p - A] < e-*‘+I’*. (A.3) To prove that inequality cases, depending on whether if F( (), e;) and consequently the experiment p(ei) = I), (A.2) holds for each experiment can be performed ei E W, we consider two initially or not. If so (i.e. then the GS algorithm will perform at least trials of e;. As the samples are drawn at random inequality after m( e;) samples, (A.3): (A.4) from a fixed distribution, we can use Pr p(e,) x Ipi -piI 2 E 2nD( ei) 1 Pr llii-pil = 3 & (2m,,i, (A)*) j- - n’ Now consider an experiment ei that is not immediately F( 0, e;) does not hold). Here, the GS algorithm will attempt path a total of M times, where reachable that to reach ei along a direct (i.e., such (A.5) R. Creiner; l? Orponen/Art$cial intelligence 82 (1996) 21-44 41 Of these, GS reaches and performs ei some number k times; and does not reach e; the value of the product p(ej) x lb; - piI to show that for any value of k, the remaining M - k times. Denote by j(k) assuming that GS succeeds exactly k times. It suffices at least 1 - s/n. with probability j(k) < El(2nWe;)) Using the fact that /j( e,) = k/M is the estimated value of p( ei) here (as GS has suc- ceeded k times out of M) together with inequality at least I - 6/2n, (A.3)) we know that with probability de;) < b(e) + and with probability at least 1 - 6/2n, If% - Pil < G &lnf. Of course, each of these terms therefore define tops off at 1 (since p( ei) 6 1 and Ip^i - piI < 1). We and observe that j(k) < g(k) with probability at least ( 1 - S/2n)* 3 1 - s/n. We now need to bound the largest possible value of g(k). First, note that g(k) can be bounded by when 0 6 k < ko, whenko<k<M, ln(4n/6). (The two expressions where ko = l/2 As the first expression has a positive largest allowed value of k, viz. ko. Using second expression maximal at either k = ko or k = M. for g(k) have the same value at k = ko.) to k, it is largest at its first and second derivatives, we see that the is upwards concave on the interval k = [ ko, M], and thus its value is first derivative with respect Hence, the largest value of g( k) is bounded by the value of the second expression at either k = ko or k = M, i.e. by the larger of the values g(ko)=&lnF+ J g(M) =A&-$$+ &-i$. By inspection, both of these values, and hence all values of g(k), are below 42 R. Greiner; P. Orponen/Artijicial Intelligence 82 (1996) 21-44 As p(ei) X Ipi - pi( = j(k) < gM, we need only find an M sufficiently large so that g,+r < e/(2nD(ei)). Solving gw = e/(2nD(ei)) for M yields M= ({G- lj’ln$. the m( ei) value from Eq. (AS) gM value will be yet smaller), To see that corresponding ( 1 + y) / ( r2) holds for any y > 0, so in particular taking at least m(ei) sufficiently Notice samples, we can be confident small, as desired. than is larger just observe this M (and hence the - 1) -* 6 for y = e/( nD( ei) ) . Hence, after is that the product p( ei) x that (dm Ipi - pii that in the typical situation where E is small relative to D(e), the m( ei) value obtained here is only slightly larger than the value obtained from Eq. (A.4). q Theorem 10. Let G = (W F, R, c) be a tree-like decision structure with WI experi- PAO( G, E, 6) be the strategy produced by the PA0 algorithm using the GS subroutine of Fig. 4. Then, with probability optimal strategy at least 1 - C,)(O,,) 6 where O,, probability vector p. C,, (@,a,) OSS(G,p) is Proof. Let $ = (p^t , . . . ,$,,) be the vector of probability GS subroutine. By Lemma 9 each of the products D( ei) x p( e;) x bounded by the value e/2n with probability they are all less than e/2n 0 Lemma 8. at least 1 - s/n. Hence, estimates produced by the - p^il is upper that from this by is at least 1 - 6. The theorem’s claim follows the probability lpi Corollary A.l. G = (W, E R, c) is any decision structure with probability at least I - 6, C,) (O,,,) optimal strategy, based on the correct probability Let @pa, = PAO( G, E, S) be the result of the PA0 algorithm, where in DS, and E, 8 > 0 are given constants. Then, is the - C, (0,)) < E where 0, = OSS( G, p) vector p. Proof. This result easy first part of Lemma 9, and the observation value A4 in Eq. ( 1) is larger than the value m( ei) in Eq. (A.4). q immediately follows that C > D (ei) guarantees from the proof of Theorem 10, using only the that the Theorem 11. Assume RP # NP. Then there is no probabilistic gorithm distribution p(e) that, given a decision function p : W + structure G = (W E R, c), an experiment [ 0, 1 ] , and parameters E, 8 > 0, can estimate to within E with probability at least 1 - 6. polynomial time al- e E W a the value Proof. Assume e, 6 > 0; say F = l/3 = 8. We show that this algorithm could also be used to decide that such an algorithm exists for some fixed values of the to the contrary R. Greiner; P Orponen/Artijicial Intelligence 82 (1996) 21-44 43 satisfiability is NP-complete, of boolean formulas (SAT) with reliability it would follow by standard arguments 1 - 6. 3 As the SAT problem that RP = NP. Let (D be a boolean decision formula over the variables XI,. . . ,x,. We show how to construct structure G, = (W,, F+,, R,, cq), such that the formula 40 has if the thus if and only 0). We could g in W, is 1 (respectively to its variables assignment does not have) a satisfying for a specific experiment the satisfiability of formula sp, with reliability 1 - 6, by running our hypothetical on structure G, and experiment g, and checking whether the estimate it a corresponding (respectively value p(g) decide algorithm provides for p(g) is greater than 1 - E or less than E. (That in general permitting is, F,( a, ek+l) and F+,(a, ek+l) both hold dence relation Fp permits exactly one of et or Zr to be performed one of ez or Z2, and so on; experiment. The structure G, has 2n + 1 experiments W, = {et, et, . . . , e,, Z,,, g}. The prece- then exactly initially, exactly one of ek or E?k as the kth iff cy is of the form se- (ZZ+), . . . , (Z,,+)), can be identified with false) relation F finally if this if and only ((6 +)> (z2+)>. quence of the P-type experiments, a truth assignment if and only specifies truth assignment Zi = Zi) in (Y. The precedence (i.e. F,( cr,g) holds) w h ere each Ei is either ei or a;.) Now each complete . . ,x,, whereby xi is true (respectively that experiment satisfies if d; = ei (respectively to the variables xl,. (Y = ((Zt+), . , (zk+)), g can be performed the formula then the trivial probability rp. distribution and recall that p(g) Consider all experiments, g using any strategy. Given constraints there exists a strategy 0 for reaching g if and only assignment only and otherwise p(g) = 0. q it is clear that truth for qo, and any such strategy will have p(g, 0) = 1. Hence p(g) = 1 if and if there exists a satisfying is the maximum probability of reaching experiment that assigns success probability if cp is satisfiable, specified above, the precedence 1 to References N. Alon, J.H. Spencer and F! Erdos, The Probabilistic Method (Wiley, New York, 1992). J.A. Barnett, How much is control knowledge worth?: a primitive example, Arf$ Infell. 22 (1984) for tests of a hypothesis 77-89. D.A. Berry and B. Fristedt, Bandit Problems: Sequential Allocation of Experiments (Chapman & Hall, London, 1985). efficiency H. Chemoff, A measure of asymptotic observations, Ann. Math. Stat. 23 (1952) 493-507. G. DeJong and R. Mooney, Explanation-based 145-176. M.R. Garey, Optimal II. Geiger and J.A. Barnett, Optimal (1991). R. Greiner, Finding (1991) 95-116. R. Greiner and I. JuriSica, A statistical AAAI-92, San Jose, CA (1992). learning: an alternative view, Mach. Learn. 1 (1986) in: Proceedings AAAI-91, Anaheim, CA constraints, Discrete Math. 4 ( 1973). task sequencing with precedence in a redundant knowledge the EBL utility problem, the optimal derivation in: Proceedings base, Art$ Intell. 50 the sums of tree searches, to solving based on satisticing approach strategy III 121 131 141 151 161 171 181 191 - ’ We are indebted to Tom Hancock for suggesting this reduction. 44 R. Greiner; t? Orpnen/Arti$ciul Intelligence 82 (1996) 21-44 I IO I R. Greiner and I? Orponen, Probably approximately optimal derivation strategies, eds., Proceedings KR-91, Cambridge, MA (Morgan Kaufmann, in: J.A. Allen, R. Fikes San Mateo, CA, and E. Sandewall, 1991). I 1 I I R. Greiner and P Orponen, Probably approximately optimal satisficing strategies, Tech. Rept., Siemens Corporate Research ( 1993). I 12 I D.S. Johnson, A catalog of complexity in: J. van Leeuwen, Cornpter Science A: Algorithms and Complexity (Elsevier, Amsterdam, classes, ed., Handbook 1990) 67-l 86. of Theoretical [ 13 I J.E. Laird, P.S. Rosenbloom and A. Newell, Universal Subgoaling and Chunking: The Automatic Generation and Learning of Goal Hierarchies (Kluwer Academic Publishers, Hingham, MA, 1986). I 14 I S. Minton, J. Carbonell, C.A. Knoblock, D.R. Kuokka, 0. Etzioni and Y. Gil, Explanation-based learning: a problem solving perspective, Artif Intell. 40 (1989) 63-119. I IS I T.M. Mitchell, R.M. Keller and S.T. Kedar-Cabelh, Example-based generalization: a unifying view, Mncll. Leurn. 1 ( 1986) 47-80. I 161 T.M. Mitchell, S. Mahadevan and L.I. Steinberg, LEAP: a learning apprentice for VLSI design, in: Proceedings IJCAI-85, Los Angeles, CA (1985) 573-580. I 17 I KS. Narendra and M.A.L. Thathachar, Learning Automata: An Introduction (Prentice-Hall, Englewood Cliffs, NJ, 1989). I I8 I KS. Natarajan, Optimizing depth-first search of AND-OR trees, Research Report RC-I 1842, IBM T.J. Watson Research Center ( 1986). I I9 I P. Orponen and R. Greiner, On the sample complexity of finding good search strategies, in: Proceedings COLT-90, Rochester I20 1 S. Sahni, Computationally 121 I H.A. Simon and J.B. Kadane, Optimal problem-solving (1990) 352-58. related problems, SIAM J. Cornput. 3 ( 1974) 262-279. search: all-or-none solutions, Arf$ Intell. 6 ( 1975) 23.5-247. I’.?2 1 D.E. Smith, Controlling [ 23 I L.C. Valiant, A theory of the learnable, Commun. ACM 27 ( 1984) 1134-l 142. inference, Artif Intell. 39 ( 1989) 145-208. backward 