Artificial Intelligence 174 (2010) 1460–1480Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintAgent decision-making in open mixed networksYa’akov Gal a,b,∗, Barbara Grosz b, Sarit Kraus c,d, Avi Pfeffer e, Stuart Shieber ba Department of Information Systems Engineering, Ben-Gurion University of the Negev, Israelb School of Engineering and Applied Sciences, Harvard University, USAc Computer Science Department, Bar Ilan University, Israeld Institute for Advanced Computer Studies, University of Maryland, USAe Charles River Analytics, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 14 December 2008Received in revised form 18 August 2010Accepted 27 August 2010Available online 21 September 2010Keywords:Human–Computer decision-makingNegotiationComputer systems increasingly carry out tasks in mixed networks, that is in group set-tings in which they interact both with other computer systems and with people. Partic-ipants in these heterogeneous human–computer groups vary in their capabilities, goals,and strategies; they may cooperate, collaborate, or compete. The presence of people inmixed networks raises challenges for the design and the evaluation of decision-makingstrategies for computer agents. This paper describes several new decision-making modelsthat represent, learn and adapt to various social attributes that influence people’s decision-making and presents a novel approach to evaluating such models. It identifies a range ofsocial attributes in an open-network setting that influence people’s decision-making andthus affect the performance of computer-agent strategies, and establishes the importanceof learning and adaptation to the success of such strategies. The settings vary in the ca-pabilities, goals, and strategies that people bring into their interactions. The studies deploya configurable system called Colored Trails (CT) that generates a family of games. CT isan abstract, conceptually simple but highly versatile game in which players negotiate andexchange resources to enable them to achieve their individual or group goals. It provides arealistic analogue to multi-agent task domains, while not requiring extensive domain mod-eling. It is less abstract than payoff matrices, and people exhibit less strategic and morehelpful behavior in CT than in the identical payoff matrix decision-making context. By notrequiring extensive domain modeling, CT enables agent researchers to focus their attentionon strategy design, and it provides an environment in which the influence of social factorscan be better isolated and studied.© 2010 Published by Elsevier B.V.1. IntroductionComputer systems are increasingly being deployed in group settings in which they interact with people to carry out tasks[3,46,42,44,28]. To operate effectively in such settings, computer agents need capabilities for making decisions and negoti-ating with other participants—both people and computer-based agents—about the procurement and allocation of resourcesnecessary to complete their tasks. For example, in a civil disaster like an earthquake, rescue personnel and equipment aredispersed geographically and may be under the jurisdiction of various dispatchers. Dispatchers might depend on computeragents to allocate these limited resources to affected locations quickly and to alert them about changing environmentalconditions, such as wind speed and traffic. In turn, computer agents might depend on people to provide up-to-the-minute* Corresponding author at: Department of Information Systems Engineering, Ben-Gurion University of the Negev, Israel.E-mail address: kobig@bgu.ac.il (Y. Gal).0004-3702/$ – see front matter © 2010 Published by Elsevier B.V.doi:10.1016/j.artint.2010.09.002Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801461information about the availability of personnel and equipment. In another realm, in some electronic auction settings, bothpeople and computer agents (representing groups or individuals) might participate not only to acquire items of value, butalso to exchange information about the reliability of others.1First response and e-commerce are two different kinds of examples of open mixed networks. By “open” we mean that theautonomous agents in the network may be designed by or represent different individuals or organizations. By “mixed” wemean that the participants of the network may be computer agents or people. Computer agents operating in open, mixednetworks may support people in their work (e.g., collaborative human–computer interfaces [47,3]), serve as proxies forpeople or institutions (e.g., electronic commerce [25,44]), or interact with other agents to carry out tasks for which they areresponsible (e.g., robots in rescue operations [46,36]). These examples exhibit several key characteristics of mixed networksettings: (1) the participants are both human and computer-based; (2) they depend on each other to make decisions; (3)they may need to exchange resources and information; (4) they have different, complementary roles.Open mixed network settings present a range of challenges for agent designers. First, the participants in these networks—whether people or computer agents—are loosely coupled and not under the control of any single entity. Agent designersare unlikely to know a priori the strategies that people or agents designed by others will adopt, and they cannot forceothers’ agents to adopt a particular strategy. Second, people’s decision-making behavior in group settings does not followthe strategies of classical economic or game theoretic models, but is affected by such social and psychological factors ascognitive biases, social preferences, and framing effects [12,7,4]. It is difficult to measure the effects of such factors directly,and preferences are hard to elicit explicitly from people [9,35]. Third, agents may differ in their goals and plans, so agentdesigners need to develop strategies that are flexibly able to accommodate different levels of cooperation or competitiveness.For these reasons, it is at best challenging, and at worst, impossible, to construct effective agent strategies purely analytically.An alternative approach is to learn and evaluate agent strategies empirically. However, past empirical investigations ofcomputer agent strategies such as the Trading Agent Competition [1] and RoboCup soccer [2] have typically required a fullyspecified domain model. The need for extensive modeling of domain specific knowledge in such settings makes it difficultto distinguish among possible causes of agents’ failures and successes, such as the way agents model the specifics of thedomain or the way they make decisions more generally.On the other hand, completely abstract settings such as the payoff matrices or decision trees traditionally used in studiesin the behavioral sciences collapse the structure of a domain into a list of choices that does not capture the essentialrelationships among tasks, goals and resources.2 Such relationships often play an important role in decision making.The investigations in this paper were done in an environment that represents an intermediate approach. They use theCT (Colored Trails) system [20] which provides an analogue to the ways in which goals, tasks and resources interact in real-world settings, but abstracts away the complexities of real-world domains. CT supports comparisons of the performanceof different computational strategies for interacting in groups comprising people and computer agents as well as solelycomputer agents.3This paper presents several new decision-making models that represent, learn and adapt to various social attributes ofnegotiation in open, mixed-network settings. We consider in particular, social factors that influence possible negotiationdeals (e.g., joint benefit and inequality of outcome), traits of individual negotiators (e.g., altruism, trustworthiness, helpful-ness) and group structure (e.g., solidarity, hierarchy). Our results show that (1) people exhibit more helpful behavior andincrease their social welfare in CT settings than in payoff-matrix types of settings; and (2) computer agents that modeland learn the social factors that influence human negotiation strategies can outperform traditional game-theoretic equilibriastrategies when interacting with people and other computer agents in mixed networks.The contributions of the paper are four-fold: it presents new multi-agent decision-making models, ones that are ableto learn and adapt to the social attributes that affect behavior in open mixed networks; it presents a novel approach toevaluating such models; it shows empirically that agents using these models outperform traditional game-theoretic equi-libria strategies. Lastly, it describes CT more completely than before as a new environment for investigating the design andperformance for negotiation strategies in open-mixed networks. It integrates earlier reports of initial CT studies [17,16,48]and describes a broader range of experimental investigations which demonstrate the flexibility of the CT infrastructure tosupport different agent-design studies.The purpose of the work reported in this paper was not to design “plug-and-play” strategies for specific applicationssuch as first response or electronic commerce. Rather, the studies we describe show empirically that agents will be betterable to negotiate with people if they take into account social factors. In this respect, our results relate to recent work inthe social sciences that point to societal and cultural factors that people “bring into the game”, as influencing the way theybehave in negotiation settings [41,8]. Our studies differ from these in providing and evaluating computational models fordecision-making in these settings. In particular, they identify the influence of factors that have not been addressed in pasthuman–computer decision-making studies and show the influence of these factors on agent-strategy performance.The next section of this paper describes CT and the ways in which it corresponds to real-world task settings; it com-pares the CT environment with alternative test-bed environments used in multi-agent system and discusses related work.1 One example of an existing application like this are sniper agents that bid on e-bay as proxies for their human users.2 For a comprehensive account of behavioral economics experiments in decision-making, see Camerer [7].3 CT is open-source software, which has been made available for download at http://www.eecs.harvard.edu/ai/ct.1462Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–1480Fig. 1. Snapshots of CT GUI.Section 3 presents a study that establishes that people make different negotiation choices when interacting in a CT gamethan when doing so in the payoff-matrix settings used in the behavioral sciences. Section 4 describes a model for learningthe different types of social factors that affect people’s behavior in a simple negotiation setting of complete information.Sections 5 and 6 compare the performance of different computational strategies for adapting to other agents’ decision-making in a repeated negotiation setting. Section 7 discusses the implications of this work for the design of agent-strategiesin open, mixed networks and presents several open research questions.2. The Colored Trails gameColored Trails (CT) is a game played by two or more participants on a board of colored squares. The CT system is highlyconfigurable, allowing for the specification of games that reflect a wide variety of task environments and decision-makingsituations. The basic CT board includes players’ icons and goal squares, but configurations may also include traps or otherfeatures needed to model a task situation. Players are typically allocated a set of chips of colors chosen from the samepalette as the board squares. To move a piece into an adjacent square a player must turn in a chip of the same color as thesquare. The board state may be dynamic, for instance with goals moving or traps appearing and disappearing. A player’sperformance in CT is determined by a scoring function which is a parameter of the game configuration. The score can bedefined to depend on such factors as a player’s distance from its goal-square when the game ends, the number of movesmade, the number of chips the player possesses at the end of the game, or the number of goals achieved. In addition, torepresent incentives for cooperation or social good, an individual player’s score can be made to depend on the performanceof other players.In the canonical use of CT, paths through the board represent doing a task, with each square in the path corresponding toa subtask. Chips represent resources needed for doing the tasks. Typically, at least one player does not have the chips neededto reach its goal. The heart of the game is players’ abilities to negotiate over these resources. Chips may be exchanged bythe players, and the conditions of exchange may be varied to model different decision-making situations.Humans play CT using a graphical user interface, while computer agents use a set of application program interfaces (API).Snapshots of the CT GUI for one of the canonical games used in this paper is shown in Fig. 1. The Main Game panel (Fig. 1a)includes the board game, the goal square, represented by an icon displaying the letter G, and two icons, “me” and “sun”,Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801463representing the location of the two players on the board at the start of the game.4 The bottom part of the Main Gamepanel, titled “Chips”, shows the chip distributions for the players. In the game shown here, neither player has sufficientchips to get to the goal square. The Decision Aid panel (Fig. 1c) provides decision support tools to be used by players duringthe game. It displays a list of possible paths to the goal for players and the chips required to fulfill each path. Players canview this information for the chips that they currently possess, or for any hypothetical chip set for each of the players. Forexample, the “me” player is lacking a purple chip to get to the goal in the path that is highlighted in the Main Game panel(this is the shortest possible path for the “me” player to get to the goal). Similarly, the “sun” player is lacking a cyan chipto get to the goal (using its shortest path to move two squares up). The Proposal Panel (Fig. 1b) can be used by players tonegotiate exchanges. In the example shown here, the “me” player has offered to give one cyan chip to the “sun” player inreturn for one purple chip. This offer is displayed in the Message History Panel (Fig. 1d), and also includes the response ofthe “sun” player, which accepted the offer.52.1. Analogy with task settingsCT provides a realistic analog to task settings, highlighting the interactions among goals, tasks required to achieve thesegoals, and resources needed for completing tasks. Chips correspond to agent capabilities and skills required to fulfill tasks.Different squares on the board represent different types of tasks. A player’s possession of a chip of a certain color corre-sponds to having the skill available for use at a time. Not all players possess chips in all colors, much as different agentsvary in their capabilities. Traversing a path through the board corresponds to performing a complex task whose constituentsare the individual tasks represented by the colors of each square.CT is parametrized in ways that allow for increasing complexity along various dimensions that influence the performanceof different strategies and algorithms for decision making. It allows for specification of different reward structures, enablingexamination of such trade-offs as the one between the performance of the group as a whole and the outcome of anindividual. It also allows to examine the cost-benefits of collaboration-supporting actions.Game parameters such as the number of players, the size of the board, and the number of chips may be set to vary thetask complexity. The amount of information available to agents can be controlled by varying the information revealed onthe board and about players’ chips. Uncertainty can be introduced by changing features on the board and chips during play.Two kinds of inter-dependencies among players can be varied in CT: task and score dependence. A task dependence ariseswhenever players lack the chips they need to reach their goals and must depend on other players to supply those chips.A score dependence arises when players’ scores depend on each other’s performance. The degree to which a player’s scoredepends on the performance of other players may be used to distinguish collaborative teamwork from situations in whichgroup members act independently. For example, a fully cooperative setting can be modeled in CT by setting the score foreach player as the sum of all players’ scores.To illustrate the task analogy, we present an example of the way CT corresponds to the rescue domain described inSection 1. Players in CT correspond to fire-engine dispatchers. There are different ways to assign players to teams, such asby representing a dispatcher’s affiliation with other dispatchers in their geographical vicinity by designating a shared goalsquare. A goal square might represent a mission to accomplish, such as rescuing people from areas afflicted with fire andsmoke or assisting other rescue teams in different locations. Paths on the board represent the completion of tasks, suchas clearing safe passage in a building or bringing special equipment to aid in the rescue. Chips represent resources, suchas fire-engines, ladders and personnel. Negotiation over these resources by the dispatchers is needed to have an efficientdeployment of resources. Players’ scores in this game may be set to depend solely on their individual performance asdispatchers or may include, at some level, the score of their teammates.2.2. Related work: decision-making environmentsA variety of simulation systems have been developed to enable evaluation and comparison between computational strate-gies for decision-making in particular domains. Prime examples of such test-beds are (1) RoboCupRescue, which simulatesfirst-response strategies that integrate disaster information, planning, and human interfaces [30]; (2) the Trading AgentCompetition (TAC), that facilitates the study of bidding strategies for the procurement of resources as well as supply-chainmanagement [1], (3) the Agent Reputation Test-bed (ART), for studying reputation and trust in environments of varyingagent-capabilities and reliability [14], and (4) the Electric Elves system for immersing agent technology in scheduling tasksin organizations [43].CT is distinguished from these systems in several ways. First, these test-beds require that significant domain knowledgebe represented in computer agents (e.g., modeling stock prices prior to bidding, estimating the number of people in dangerin a burning building). In contrast, CT allows agent designers to focus on such general properties of interactions betweenhumans and computers such as people’s perception of the usefulness of an interruption request for information [26] andthe way people form teams to carry out tasks in strategic environments [50]. This abstraction also has advantages for4 A “me” icon on the main board panel is used to enable a human player to easily distinguish between the player’s own icon and those of other players.5 Note that the panel is displayed from the point of view of the “sun” player, and therefore a “me” icon is displayed as the recipient of the offer.1464Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–1480experiments that involve people: first, human subjects need not be domain experts. Second, CT enables the specification ofdifferent reward structures, allowing system-designers to vary the importance of different decision-making factors such asthe performance of others or the group as a whole to the outcome of an individual. As a result, CT is novel in addressingthe need to have better ways of evaluating computer agent strategies in contexts in which systems are participants in groupactivities that include people.Although CT abstracts away from such domain specific details as fire engines and evacuation routes, it provides a task-like context in which decisions are made. This task context means that decisions are presented less abstractly than payoffmatrices or trees, the canonical forms used to present outcomes in behavioral economic experiments. These canonical formsexplicitly specify the payoffs to all players for each potential strategy. Real-world decision-making seldom presents choicesthis starkly. CT immerses people in an environment in which underlying relationships among tasks, goals and resourcesmatter. It thus places decision-making in a more real context.3. The effects of decision-presentation on human negotiation behaviorThis section provides empirical evidence that people’s behavior is significantly more cooperative when they negotiateusing CT than when they are presented with payoff-only representations of the sort common to studies in behavioraleconomics. Recent work in the social sciences has demonstrated that people cooperate despite the lack of direct short-termbenefits from cooperative behavior [11,39]. This section shows the use of CT induces such cooperative behavior and thus itprovides support for CT as being the right kind of test-bed with which investigate decision-making in open-mixed networks.3.1. Empirical designWe presented subjects with identical multi-agent decision-making problems in two conditions and measured their be-havior and outcomes. In one condition, called the “task condition”, people made decisions in the context of a CT game. Inthe other, the “table condition”, they were given the decision in the context of a payoff-only table representation.For the task condition, we used a 2-player CT setting that varied a configuration of 4 × 4 boards, the chip allocationsfor each player, and the placement of the goal square. The study comprised a one-shot negotiation setting for which oneplayer was designated the proposer and the other was designated the responder. Players had full view of the board andeach others’ chips. The proposer player could make of offer to exchange a particular subset of its chips for some subsetof the responder’s chips. The responder player could accept or reject the proposer’s offer. If no offer was made (there wasa 3-minute deadline for proposers to make offers), or if the offer was declined, then both players were left with theirinitial allocation of chips. If the offer was accepted, the chip exchanges were enforced by the game controller. Following thisinteraction, both players’ icons were automatically moved as close as possible to the goal square given the chips in theirpossession and their computed score.The scoring function for players depended solely on their own performance: 100 points bonus for reaching the goal(otherwise, 50 points bonus); 5 points for each chip left in a player’s possession at the end of the game; 10 points deductedfor any square in the shortest path between a player’s final position in the game and the goal-square. These parameterswere chosen so that getting to the goal was by far the most important component, but if a player could not get to the goalit was preferable to get as close to the goal as possible, rather than hoard chips.An example of one of the boards used in the study is given in Fig. 1a. In this example, neither player can get to the goalby using the original chip allocation. The games used in the study all involved at least one of the players having insufficientchips to get to the goal, but not necessarily both.The table condition consisted of a payoff matrix representing potential offers that could be selected by the proposerplayer in a CT game. Each offer was represented as a pair of payoffs for the proposer and the responder players. Fig. 2shows a snapshot of a game in this representation as seen from the point of view of a proposer player. Each entry in thetable represents an offer, and selecting one of the entries corresponds to choosing the offer that was associated with thepayoffs inside the entry. One of the entries contained the no-negotiation alternative score and was presented to players asthe default outcome of the interaction.The score that each player received if no offer was made was identical to the score each player received if the offer wasrejected by the responder. We refer to this score as the “no-negotiation alternative” score and refer to the score that eachplayer received for an offer that was accepted by the responder as the “proposed outcome” score.A total of 32 subjects participated in the experiment. They were equally divided between the two conditions. Participantsin the task condition interacted with each other using the CT environment, whereas those in the table condition interactedwith each other using the payoff matrix representation. In both conditions, participants were compensated in a mannerthat depended solely on their individual scores, aggregated over all rounds of interaction. To prevent confounding effects ontheir behavior, participants were randomly divided into the two condition groups and seated in two rooms, such that noparticipant interacted with another participant seated in the same room. Participants only interacted with others in theircondition group and were not provided any information about each other. We trained each group of subjects and testedtheir proficiency using a pre-study questionnaire.66 Subjects that did not score full points on the questionnaire were given a show-up fee and did not participate in the experiment.Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801465Fig. 2. Snapshot of an interaction in the table condition. Each entry in the matrix lists a pair of payoffs representing the score to the proposer player (left)and the responder player (right).Table 1Average benefit of exchange.TaskTableOffer benefit toProposer82.398Responder47.636For each CT round that was played in the task condition, an equivalent round was played in the table condition. Eachentry in the table listed a payoff pair representing the payoffs to the proposer and responder player for a possible exchange.For example, the payoff matrix shown in Fig. 2 is equivalent to the CT game shown in Fig. 1a.We use the term “table proposers” and “task proposers” to refer to the participants that were designated with theproposer role in the table and task condition respectively (and similarly for responder players). We use the term “offerbenefit” to refer to the difference between the proposed outcome for an offer and the no-negotiation alternative scorefor the game. We measured people’s behavior in the experiment using two features. The degree to which proposers werehelpful to responders was measured in terms of the average offer benefit they proposed to responders. Similarly, the degreeto which proposers were selfish was measured in terms of the average offer benefit they proposed to themselves. Thesefeatures are not independent. For example, proposers can exhibit both a degree of selfishness and a degree of helpfulnessbased on the average benefit of their offers.3.2. Analysis of proposer behaviorTable 1 presents the average offer benefit to participants in the task and table conditions for proposers and responders.Table proposers offered significantly more benefit to themselves than did task proposers (t-test p < 0.05). Also, they offeredsignificantly less benefit to table responders than task proposers offered to task responders (t-test p < 0.01). Thus, proposerswere more likely to engage in helpful behavior, in the task setting, and less likely to engage in selfish behavior.Another result arising from the table is that proposers offered significantly more to themselves than they did to respon-ders in both conditions (t-test p < 0.05). However, the difference between the benefit for proposer and responders wassignificantly larger in the table condition than in the task condition (t-test p < 0.05). We can thus conclude that althoughproposers were competitive in both conditions, they were significantly more competitive in the table condition than in thetable condition.Table 2 shows the exchange benefit to proposers and responders averaged over all accepted proposals, as well as thetotal accumulated benefit in each condition. The benefit to responders from accepted proposals was significantly higher in1466Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–1480Table 2Average benefit for accepted exchanges.TaskTableProposer79.585.6Responder56.440.7Total135.9126.3Fig. 3. Benefit from proposed exchanges vs. NE exchanges.the task condition than in the table condition, and conversely for the proposers (t-test p < 0.05). Thus, task respondersoutperformed table responders, whereas table proposers outperformed task proposers.Our results also found that the CT task setting had a positive effect on the combined performance of participants. Asthe rightmost column shows, the total performance (combined proposers and responders scores) was higher in the taskcondition than in the table condition (t-test p < 0.1). The benefit for accepted exchanges is a measurement of performance,because the outcome of each round of interaction was fully determined by the action of the responder. Although this resultwas not significant at the p < 0.05 confidence level, it highlights that CT did not only have an effect on people’s helpfulbehavior tendencies, but also that this helpful behavior was beneficial overall for the participants.3.3. Comparison with Nash-equilibrium strategiesThis section compares the offers that were made in the CT and table conditions with the offers dictated by the exchangecorresponding to the Nash equilibrium strategy. We use the term NE exchange of a round to refer to the exchange prescribedby the Nash equilibrium strategy profile for the round. This exchange offers the maximum benefit for the proposer, out ofthe set of all of the exchanges that offer non-negative benefits to the responder. For the CT scenario used in our experiments,the Nash equilibrium offer is the one that maximizes the proposer’s benefit out of the set of all possible exchanges thatoffered any benefit, however small, to the responder. Thus, the NE exchange is a competitive offer, which is more beneficialto proposers than to responders.Because proposers were more competitive in the table setting, we expected table proposers to be more likely to offer NEexchanges than task proposers. We found that the number of NE offers made in the table condition (57) was significantlygreater than the number of NE offers made in the task condition (13) (chi-square t < 0.01). To compare the extent to whichthe exchanges made by proposers in the two conditions differed from the NE exchange, we plotted the average benefitoffered by NE exchanges and by proposed exchanges for both task and table conditions, as shown in Fig. 3.The difference between the average benefit to responders from the NE offer and the average proposed exchange was closeto zero in the table condition, and large and positive in the task condition (t-test p < 0.05). Similarly, the difference betweenthe benefit to proposers from the NE offer and the average proposed exchange was close to zero in the table condition, andlarge and negative in the task condition (t-test p < 0.05). The Euclidean distance between the points representing the NEbenefit and the proposed exchange was significantly larger in the task condition than in the table condition. In fact, therewas no statistically significant difference between proposed exchanges in the table condition and NE offers; participantswho make decisions in the table condition were more likely to follow the equilibrium choice.These results align with recent work in the social sciences that has found differences between people’s behavior in fieldstudies and their behavior in controlled laboratory experiments [21]. However, they differ from classical findings whichshow that people do not adhere to traditional game theoretic equilibria in one-shot games such as prisoners’ dilemma [45].One possible explanation for the adherence to game theoretic exchanges in the table condition may be the fact that subjectswere guaranteed the no-negotiation alternative score in each game (unlike the canonical behavioral economics experiments,in which the outcome for no negotiation was zero). The guarantee of a positive payoff to proposers could have made themY. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801467less likely to fear rejection by the responders. Thus they offered exchanges that were more selfish (more game-theoretic) ascompared to prior studies.3.4. The context hypothesis and related approachesThe results of the experiment described above support the hypothesis that the way in which a decision is representedinfluences people’s negotiation behavior. While considerable amount of research in the social sciences has studied the effectsof framing on behavior this is the first work that has explored the particular role played by task settings. In particular, theresult that people are more helpful and less competitive in the task-like CT setting than when deciding in a payoff-onlycontext, suggests that task settings have a positive effect on the way people (and in turn, the agents that interact withpeople) behave. However, there are several alternative explanations of this behavior. In this section, we rule out two obviousalternatives.First, the competitive behavior exhibited by table proposers might be attributable to the fact that the specific payoffs foreach possible strategy was explicitly presented to them, whereas proposers in the CT-task setting were not given the payoffsexplicitly. To rule out this hypothesis, we ran an additional experiment in the CT game context. In this experiment, we useda CT setting like the one used in the original task context, with one difference: subjects were able to access an extendedversion of the path-finder panel that allowed them to view the payoffs for potential paths to the goal. This setting preservedthe task environment, but also allowed for explicit display of the payoffs for both players associated with agreements. Weused the same set of games that were used in the original experiment. The results showed that there was no significantdifference in the average benefit allocated to proposers and responders in this intermediate representation than in the CT-task condition (t-test t(29) = 2.34, p < 0.05). Thus, we can preclude the lack of explicitly presented payoff information inthe CT context as an explanation of the different proposers’ behavior.Second, the different proposer behaviors might be attributable to differences in the cognitive demand on subjects makingdecisions in two different settings. In both of these settings, the number of possible offers for proposer players in each gamewas large, and bounded by 28 = 256. Typically, there were between thirty and forty offers in each game associated with adistinct benefit to both proposer and responder players. All of these strategies were explicitly presented to table proposers,while the task proposers needed to infer the possible strategies (and payoffs) by reasoning about the task setting. Bothconditions potentially required large cognitive effort that could confound players’ reasoning about the degree of helpfulbehavior to exhibit. For the original set of experiments, we provided decision support tools for each decision representationsetting, that were designed to mitigate these potential cognitive load problems. In the CT game, subjects could use theDecision Aid panel, shown in Fig. 1c, to query the system for suggestions about the best paths to take given any hypotheticalchip distribution and the chips required for these paths. In the table condition, subjects were able to sort the table by theirown benefit or by benefit to the other player. Thus, in both cases, we reduced the effects of the decision-making complexityon the behavior exhibited by subjects.Thus, the task context provided by CT remains the best explanation of the more helpful, less competitive behaviorexhibited by subjects in that condition. Although we have not yet established how many of the elements essential forstudying decision-making trade-offs of actual open, mixed network application settings are reflected in the decision-makingenvironment provided by CT, these results show that the CT environment differs for non-trivial reasons from payoff-onlysettings and in ways that make decision-making in CT closer to the types of task settings that occur in the real world.Lastly, we note that this study is distinguished from work on collaborative user interfaces and communication protocolsfor facilitating tasks such as preference acquisition in e-commerce [35,25], turn-taking [10] and diagram generation [5].Many aspects of interfaces affect the way people interact; In designing both the task interface and the table interface weattempted to create the most natural representations while restricting the modalities to a graphic display. The fact that ourresults did not change even when the payoffs were available to subjects using CT implies that the difference in behaviorshould not be attributed to the payoffs themselves, but to other aspects relating to the context that CT provides, such asthe explicit presentation of tasks, goals and resources. We do not mean to imply that the difference in behavior can beattributed solely to one aspect or the other, such as the communication protocol.4. Learning social preferences in negotiationThis section describes an investigation of the hypothesis that computer agents that model and learn the influence ofsocial factors on people’s negotiation strategies will outperform agents using traditional equilibrium strategies. It definesthree particular social factors—aggregate benefit, advantage of outcome, advantage of trade—which together with individualbenefit (the sole characteristic taken into account by traditional equilibrium strategies) are used to characterize an offer.It then describes an algorithm for learning these factors from data of people’s performance in a 2-player CT setting thathighlights the task-resource relationship, and presents the results of empirical studies comparing this learned model toequilibrium strategy approaches. The CT setting used in this study was identical to the one described in Section 3.4.1. A model for learning social preferencesThis section describes a model that can be used to enable a computer agent playing the role of the proposer to makeoffers that take into account social factors that influence people’s decisions when they are responders in this CT scenario.1468Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–1480The model addresses three essential challenges. First, people vary in the extent to which they are affected by social factorswhen they negotiate. For example, an offer that is rejected by a competitive responder might be accepted by a responderwho is more altruistic. In addition, people sometimes make mistakes, so they may not behave consistently and thus may attimes deviate from the model of their typical behavior. Third, people’s behavior has been shown to depend on their inter-dependence. For example, the extent of generosity reflected in a proposer’s offer may depend on whether the proposerneeds chips from the responder to get to the goal.The formal decision-making model used in this study is defined as follows: let k represent a CT game, associated with aset Ck of possible proposals. For each proposal ckR denote the no-negotiation alternative scores forjthe proposer and responder. These are the scores that the proposer and responder would receive if no agreement is reachedin the game, and the players use the chips they were allocated at the onset of the game to move towards the goal-square.k, jR denote the proposed outcome scores for the proposer and responder for proposal ckj .k, jP and PO∈ Ck, let N NkP and N NkLet POFor each proposal ckj of game k, we represent four social factors xkj= {xkj,1, . . . , xkj,4} that affect the behavior of theresponder agent.j,1• Individual benefit xk• Aggregate benefit xk• Advantage of outcome xk• Advantage of trade xkk, j= PORk, j= (POP= POk, jRj,3= (PO− N NkR .k, jR ) − (N Nk+ POk, jk, j− POP .RP ) − (PO− N Nkj,2j,4P+ N NkR ).k, jR− N NkP ).The individual benefit of an offer measures the extent to which the offer is beneficial to the responder. The aggregatebenefit of an offer measures the extent to which the offer is beneficial to both players. The other two social factors representdifferent potential sources of inequality. The advantage of outcome for an offer measures inequality in the proposed outcomescores for the offer, without reference to the no-negotiation alternative scores. The advantage of trade for an offer measuresinequality in the benefits to the agents of the offer. We illustrate these factors using the offer in the game shown in Fig. 1,in which the “me” player (the proposer) offers one cyan chip to the “sun” player (the responder) in return for one purplechip. This exchange provides both players with the chips they need to get to the goal.7 It would allow the “sun” player tochoose a shortest path of length two to the goal. This player would have two chips left in its possession, and earn a scoreof 120 points. Similarly, the offer would allow the “me” player to choose a shortest path of length three to the goal (thepath that is outlined in Fig. 1a). This player would have one chip left in its possession and earn a score of 110 points. Theno-negotiation alternative score for the “me” player in the game is 40 points, while the no-negotiation alternative score forthe “sun” player in the game is 50 points. Therefore the exchange above provides an individual benefit to the “sun” playerof 70 points, an aggregate benefit of (70 + 60) = 130 points, and an advantage of outcome and of trade of 10 points.To enable learning of people’s decision-making strategies in this CT game, we introduce the notion of “responder types”and use them to represent ways that responders may differ in the extent to which the various social factors affect theirdecision-making. Each type represents a particular weighting of these factors, reflecting the different ways in which re-sponders make decisions. There is a finite set of responder types T, and a prior probability distribution over types, denotedby P (T). The weight w i,l denotes the weighting of social preference xl for responder type ti . These weights measure therelative importance for the responder of each of the social preferences.Let k denote a CT game instance. Given a proposal ckj , possible social preferences xkj and responder type ti that is selectedfrom P (T), we define a social utility ui for the responder as a weighted sum(cid:3)(cid:2)xkjui=4(cid:4)l=1w i,l · xkj,lj) > 0. The probability of acceptance of an exchange ckj . A responder that always follows its social utility function ui would agree to anyj at game k by a responder of type ti isLet rk denote the response to exchange ckexchange ckj such that ui(xkdefined by the following sigmoid function(cid:5)(cid:5) xk(cid:2)rk = accept=P(cid:3)j, ti1−ui (xkj )1 + e(1)There are two advantages to using this function to model the probability of acceptance using this function. First, itcaptures the fact that people may make mistakes with respect to their utility function. Accepting a proposal is more likelywhen the utility ui is a large positive number than a small positive number. In particular, the probability of acceptance7 The exchange provides a higher individual benefit to the “sun” player than to the “me” player, according to the scoring function. This is because the“sun” player is initially located closer to the goal square than the “me” player, and has the same number of four chips at the onset of the game. Thereforeit will be left with more chips than the “me” player if it accepts the exchange and moves towards the goal.Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801469converges to one as the utility becomes large and positive, and to zero as the utility becomes large and negative. Whenthe utility is close to zero, the decision is less clear-cut, and the probability of acceptance is close to 0.5, meaning thatmistakes, or decisions that are inconsistent with the utility function are more likely to be made by the responder. Second,the mathematical properties of the sigmoid function facilitate the derivation of the learning rules, as we will soon describe.The expected utility to the proposer for an exchange ckj of type ti can be written as(cid:2)ckj(cid:3)(cid:5)(cid:5) ti= PEU P(cid:3)(cid:2)rk = accept(cid:2)(cid:5)(cid:5) xkj, ti(cid:2)rk = accept1 − P+· PO(cid:5)(cid:5) xkk, jPj, ti(cid:3)(cid:3)· N NkP(2)The proposer does not get to observe the type of the responder it is interacting with at each game. As a result, the best-response strategy for the proposer, as specified by Eq. (3), will equal the unique proposal that maximizes the proposer’sexpected score in the game for all the different types of possible responders.(cid:5)(cid:5) tiP (ti) · EU P(cid:4)(3)(cid:3)(cid:2)ckj∗ = argmaxckckj∈Cti ∈T4.2. Learning a mixture modelTo use Eq. (3) in the model for a proposer agent’s offer strategy, it is necessary to learn the distributions over the typespace of responders and the likelihood that each type agrees to a particular offer. Let D = {d(1), . . . , d(N)} denote a set ofinstances, each instance representing a single CT game. The proposal ckj in CT game k comprises a (possibly empty) subsetof the chips of the proposer to be exchanged with some (possibly empty) subset of the chips of the responder. We definea data instance d(k) to consist of the following features: a CT game k, a set Ck of possible offers in game k, the chosen} associated with the chosen proposal, and the response rk.proposal ck∗,4The goal of the learning task is to use the data set D to estimate the distribution over the responder types and thestrategy of the responder for each type. As specified by Eq. (1), the strategy for the responder depends on the socialpreference weights w 1, . . . , w 4. The likelihood of the response at game k is defined as P (rk | xki denote an errorterm for predicting whether the proposal ck(cid:5)(cid:5) xk∗, ti). Let errk∗ at game k is accepted by responder type ti , defined as follows:∗ ∈ Ck, the set of social preferences xk∗,1, . . . , xk= 1 − P∗ = {xk(cid:2)rk(4)(cid:3)∗, tierrkiThis term comes from assuming that if the model was a perfect predictor of the responder, it would have predicted the trueresponse rk with probability 1. The difference between 1 and the P (rk | xk∗, ti) factor is the degree of error for the model,given that the responder is using block ti .We can minimize errki by taking the derivative of this function with respect to each weight w j . Employing a gradientdescent technique, we get the following update rule for each weight w j , where α is the learning rate.w j = w j + α(cid:2)1 − P(cid:2)rk(cid:3)(cid:3)(cid:5)(cid:5) xk∗tixkj(cid:4)k∈DHowever, the responder type ti in a CT game is unknown. Therefore we define a mixture model that is comprised of aprobability distribution P (T) over the type space, and an associated set of weights w i,1, . . . , w i,4 for each type ti . We sumover the type space to compute errk, the expected error of the model at each instance k for all types(cid:4)errk =(cid:2)tiP(cid:5)(cid:5) rk, xk∗(cid:3)· errki(6)ti ∈Twhere p(ti | rk, xki . This can becomputed using Bayes rule. The degree to which a training instance k can be used to learn the weights w i,1, . . . , w i,4 isproportional to the probability that each type ti generated instance k. We get the following update rule for weight w i, j oftype ti .∗) is the posterior probability of type ti given response rk and the set of social preferences xkw i, j = w i, j + α · P(cid:2)ti(cid:5)(cid:5) rk, xk∗(cid:3)(cid:4)(cid:2)xkj1 − P(cid:2)rk(cid:5)(cid:5) xk∗ti(cid:3)(cid:3)k∈DComputing the new values of the P (T) parameters is performed using an on-line version of the EM algorithm [38], astandard stochastic learning paradigm for estimating distributions in the presence of missing information. We compute theexpected sufficient statistics for each type ti using the current parameter settings and normalize to get a new distributionover the type space.(cid:4)P (t) = 1Z(cid:5)(cid:5) rk, xk∗(cid:2)tiP(cid:3)k∈Dwhere Z is a normalizing factor.(5)(7)1470Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–14804.3. Empirical studyIn this section we compare the performance of an agent that used a social preference model to negotiate with people toagents using traditional equilibrium strategies. A total of 42 subjects participated in the experiment, 32 in the data collectionphase and ten in the evaluation phase.The CT games we used were randomly generated, but filtered to meet the following conditions: (1) at least one of theplayers could reach the goal after trading with the other player; (2) it was not the case that both players could reachthe goal without trading. In this way, the games we trained and tested on characterized a wide range of task dependencyrelationships between players. For each board we used in the study, we recorded the board and chip settings, as well as theactions of both agents. We ran two instances of the data collection phase, each with different subjects, collecting 192 gamesin total. The data obtained in this phase was then used to learn a model of human play.We learned separate models for one, two and three possible types of responders, referred to as M1, M2 and M3, respec-tively. The parameters in these models represented different convergence points of the learning algorithm in the parameterspace of possible values for the social preferences. For all models, we used random initial values for the distribution overresponder types. For M1 we also set random values for all of the social preference weights. For M2 and M3 we assignedeach responder type initial values that highlighted certain social preferences by giving them significantly higher initial valuethan others. Specifically, in M2, one of the responder types highlighted advantage-of-outcome and advantage-of-trade, whilethe other highlighted aggregate benefit. In M3, responder types highlighted advantage-of-outcome, aggregate benefit, andadvantage-of-trade separately. We ran each model on the data from the data collection phase.To keep from over-fitting the data, we stopped the learning process after no improvement had been recorded on aheld-out validation set. This occurred after about 20 epochs. We obtained the following posterior parameter values foreach model. Model M1, which had a single responder type, learned social preference weights (7.00, 5.42, 0.40, 4.00) forindividual benefit, aggregate benefit, advantage of outcome and advantage of trade, respectively. This model thus describes aresponder who cares about both players’ outcome, but also likes to do better than the proposer. In M2, the distribution overresponder types was (0.36, 0.63) and the social preference weights were (3.00, 5.13, 4.61, 0.46) and (3.13, 4.95, 0.47, 3.30)for each type respectively. This model thus describes two partially altruistic responders, both of whom have high weightsfor social welfare, while still caring about their own benefit. One of the types cares more about advantage of outcome,and the other type cares more about advantage of trade. In M3, the distribution over responder types assigned minusculeprobability for the third type, and resembled M2 in all other parameter values. We therefore decided to use M2 in theevaluation phase.Ten people participated as subjects in the evaluation study, which compared the performance of three different computeragents. The computer agents were always in the proposer role. Each of these agents was capable of mapping any CT gameposition to some proposed exchange. The agent using the social preference model, denoted SP, proposed the exchangematching the best-response strategy of Eq. (3). The second agent, denoted NE, proposed the exchange corresponding to theNash equilibrium strategy for the proposer role. A proposer that uses a Nash equilibrium approach will make the offer thatmaximizes the proposer’s benefit out of the set of all possible exchanges that offer non-negative benefit to the responder.∗ =ck(cid:2)POk, jP− N NkP(cid:3)argmaxk, j−N NkRc j|(POR )>0The third agent, denoted NB, proposed the exchange corresponding to the Nash bargaining strategy [37] for the proposer.8The subjects were divided into two groups with the members of each group playing four rounds in a round-robinfashion. At each round, four concurrent games were played by each group. One of the (human) subjects and the threecomputer agents were designated as proposers; the other four (human) subjects were designated as responders. In eachround, the game settings—including board layout, start and goal positions and initial chip distributions—were the same forall of the games played by members of the same group. Through the round-robin, each of the (human) subjects played eachof the computer agents at least once as well as some other human player(s). No two people played each other more thanonce, but people could play the same computer agent more than once.Participants were given the same instructions and tutorial as in the data collection experiment. To make the conditionsin both collection and evaluation phases identical, we did not tell subjects they would be playing computer agents as wellas people.9 We recorded the settings of each game, as well as players’ proposals and responses. Altogether we collected 21rounds, where each round consisted of four games with different proposers, as explained above.4.4. ResultsThe evaluation criterion for the SP agent was the total accumulated reward received by the agent as compared to theagents that used other models to guide their play. We also evaluated whether the offers that the SP agent made wereaccepted more often by people than the offers made by other agents.8 The Nash Bargaining solution maximizes the product of the agents’ utilities and is always Pareto optimal.9 Approval was obtained from the Harvard Human Subjects Committee for this procedure.Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801471Table 3Total score achieved by agents.ModelTotal rewardProposalsacceptedProposalsdeclinedNooffersSPNENBHU28802100240024401613141658210054Table 4An example of a proposal made by the SP agent.ModelNNASPProposerscore4570Responderscore170150Table 3 presents the results of the evaluation phase for each of the agents used in the experiment. It lists the totalmonetary reward, the number of proposals accepted, the number of proposals rejected, and the number of times no offerwas proposed.The computer proposer labeled NE always proposed the exchange that corresponded to the proposer’s strategy in theNash equilibrium of each CT game. In essence, this resulted in offering the best exchange for the proposer, out of the set ofall of the exchanges that are not worse off for the responder. Many of the exchanges proposed by this agent were declined.We hypothesize this is because they were not judged as fair by the responder. This result closely follows the findings ofbehavioral game theory. The performance of NE was the worst of the four.The computer proposer labeled NB always proposed the exchange that corresponded to the proposer’s strategy in theNash Bargaining profile. In particular, this always corresponded to an exchange that was Pareto optimal with respect to theset of feasible allocations, that is, the proposed outcome was not worse off than the no-negotiation alternative for bothagents. This exchange consistently offered more to the responder than the NE agent did for the same game, when the boardand chip distribution enabled it to do so. Because NB tended to offer quite favorable deals to the responder, they wereaccepted more than the other computer agents, provided that an offer was made. This player did not make any offer forthose games in which there was no Pareto optimal offer.The results in the row labeled HU in the table combine the monetary rewards for all of the human agents. The identityof the human proposer was different at each round. With one exception, human offers were accepted whenever they weremade.The computer proposer that followed our expected utility model, labeled SP, achieved a significantly higher reward thanthe NE, NB and HU (t-test comparison for mean reward was p < .05, p < .05, p < .1, respectively). SP and HU had thehighest number of accepted proposals. Interestingly, the SP agent proposed the same offer as the human proposer in 4 ofthe games, whereas the Nash equilibrium agent did not match a human proposal in any game, and the Nash bargainingagent matched human proposals in 2 games.The distinguishing behavior of the SP agent is illuminated by the examples in Tables 4 and 5 in which the NNA head-ing gives the no-negotiation alternative scores. Table 4 presents a round in which SP proposed an exchange which wasaccepted; the (human) responder was altruistic in this case. This example is an interesting illustration of generalization inthe learning systems. There was only one observation in the learning phase in which a responder altruistically agreed tosuch an exchange. In the evaluation, the SP agent proposed exchanges that were asking such a favor from a respondent whowas much better off four times, and these offers were consistently accepted by the (human) subjects. The ability of the SPagent to make the right kind of trade-offs—when to ask for favors and when not to ask for favors from the responder—wasa contributing factor to its success.Table 5 displays an example in which the proposed outcome of the exchange proposed by NE, while beneficial forthe responder, was lower than the exchange proposed by SP. The NE exchange was rejected, while the SP exchange wasaccepted. This difference seems to indicate that the responders in this game cared about the equality of outcomes. Notethat in this exchange, the SP exchange and the exchange proposed by the human were equal.4.5. Related work: learning from peopleThe results reported in this section relate to several different strands of prior research that address, in a variety ofcontexts, the importance of learning for agents working with people. Past works in AI have used heuristics, equilibriumstrategies, and opponent modeling approaches toward building computer agents that negotiate with people. For a recentcomprehensive review, see Lin and Kraus [33]. Within repeated negotiation scenarios, Kraus et al. [32] modeled humanbilateral negotiations in a simulated diplomatic crisis characterized by time constraints and deadlines in settings of completeinformation. They adapted equilibrium strategies to people’s behavior using simple heuristics, such as considering certain1472Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–1480Table 5An example of the offers made by all agents in a single game.ModelNNASPNENBHUProposerscore75170180150170Responderscore150170160190170AcceptedYesNoYesYesnon-optimal actions. Jonker et al. [24] designed computer strategies that involve the use of concession strategies to avoidimpasses in the negotiation. Byde et al. [6] constructed agents that bargain with people in a market setting by modeling thelikelihood of acceptance of a deal and allowing agents to renege on their offers. Traum et al. [49] constructed agents for thetraining of individuals to develop leadership qualities and interviewing capabilities. Recent approaches have used learningtechniques to model the extent to which people exhibit different social preferences when they accept offers in multipleinteraction scenarios [40,34].Our work is also related to approaches for elicitation of users’ preferences for automated negotiation [35,9]. In theseworks, people were asked to disclose information about their preferences over various negotiation deals, and evaluatevarious acquisition models that trade-off the importance of different features of an agreement to learn a parsimoniousrepresentation of users’ preferences. However, the participants of open, mixed networks directly negotiate in task settingsand it is not possible to ask people about their preferences directly. Indeed, people are generally reluctant to reveal theirgoals and preferences when they negotiate in task settings [18]. Thus we took a different approach, namely to infer people’sutility functions from their negotiation behavior.Lastly, this work is also related prior work include adaptation to user preferences for improved human–computer inter-face design or computer-supported collaborative agents that infer users’ goals and intentions [23]. These user models informagent-design for applications such as office assistants [5], and scheduling and meeting management [43]. Our work differsfrom these in its focus on learning people’s decision-making in strategic—rather than collaborative—settings, and adaptingto the social factors that guide their play.5. Repeated negotiation in settings of full informationThis section describes a novel agent design for a mixed-network setting that extends the one described in the previoussection to include multiple negotiation rounds and non-binding agreements. Such settings characterize many types of real-world scenarios, including diplomatic relations, trade agreements and contract negotiations, but have not been consideredbefore in the design of agents that interact with people. These settings pose additional challenges to the design of effec-tive computer negotiation strategies. In particular, when agreements are not enforceable, negotiators may initially behavereliably, but eventually take advantage of others that trust them.The study described in this section focuses on negotiation between computer agents and humans under conditions offull information, while Section 6 focuses on negotiation solely between computer agents under conditions of incompleteinformation. Together, they demonstrate CT’s flexibility in allowing to configure more complex negotiation protocols thatsupport repeated negotiation and allow to control the extent to which participants abide to agreements. To date, all work onhuman–computer negotiation assumes that agreements are binding, and uses constrained negotiation protocols that do not allowpeople to reveal information or to argue their positions when they negotiate. An exception is an agent proposed by Krausand Lehmann [31] that was developed for a specific domain, that of diplomacy, and which did not reason about behavioraltraits of participants.We hypothesized that to be successful in settings where agreements are non-binding, agents need to (1) model be-havioral traits that affect the negotiation behavior of other participants; and (2) adapt its negotiation strategy to the waythese traits change over time. To this end, we designed an agent that explicitly modeled the extent to which its negotiationpartners were reliable and helpful. We begin the section by describing the CT setting, then outline the strategy used by theagent and provide an empirical evaluation of this strategy when negotiating with other people.5.1. The CT settingThe CT setting described in Section 3.1 was extended as follows: first, players were allowed to renege on their agreementsin part or in full. This was done by introducing a transfer phase which immediately followed the negotiation phase, andrequired chips to be transferred manually. In contrast, in the CT setting described in Section 3.1 the chip transfer wasautomatic. Second, the negotiation protocol included a recurring sequence of negotiation interactions that allowed playersto make and agree to offers, transfer chips and move about the board.This study used two different board configurations. In all of these boards there was a single distinct path from eachplayer’s initial location to its goal square. One of the board configurations exhibited a symmetric dependency relationshipbetween players: neither player could reach the goal given its initial chip allocation, and there existed at least one exchangeY. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801473Fig. 4. Snapshots of CT GUI.such that both players could reach the goal. We referred to players in this game as task co-dependent. The other boardtype exhibited an asymmetric dependency relationship between players: one of the players, referred to as task independent,possessed the chips it needed to reach the goal, while the other player, referred to as task dependent, required chips fromthe task independent player to get to the goal. An example of the task co-dependent board is shown in Fig. 4. In this gameboth “me” and “O” players are missing three chips to get to the goal. The relevant path from the point of view of the “me”player is outlined.At the onset of the game, one of the players was randomly allocated the role of proposer, while the other was given therole of responder. The interaction proceeded in a recurring sequence of communication, transfer and movement phases. Inthe communication phase the proposer could make an offer to the responder, who could accept or reject the offer. In thetransfer phase, both players could choose chips to transfer to the other player. The transfer action was done simultaneously,such that neither player could see what the other player transferred until the end of the phase. In particular, players werenot required to fulfill agreements: a player could choose to transfer more chips than were agreed, or any subset of the chipsthat were agreed, including transferring no chips at all. In the movement phase, players could manually move their icons onthe board across one square by surrendering a chip in the color of that square. At the end of the movement phase, a newcommunication phase began. The players alternated roles, such that the previous proposer was designated as a responder,and vice versa.These phases repeated until the game ended, which occurred when one of the following conditions held: (1) at leastone of the participants reached the goal square; or, (2) at least one of the participants did not move for three consecutivemovement phases. At this point, both participants were automatically moved as close as possible to the goal square usingthe chips in their possession and their score was computed as follows: 100 points bonus for getting to the goal square,5 points bonus for any chip left in a player’s possession; 10 points penalty for each square left in the path from a player’sfinal possession and the goal square.5.2. The personality-based agentThe agent that we constructed for this setting, called the Personality Based (PURB) agent, modeled other participants interms of two behavioral traits: helpfulness, and reliability.10 The helpfulness measure of participants, denoted h, representedthe extent to which they shared resources with their negotiation partners through initiating and agreeing to proposals. Thiswas computed as the percentage of proposals in the game in which participants offered more chips to their negotiationpartners than they requested for themselves. The reliability measure of participants, denoted r, represented the extent towhich they kept commitments with their negotiation partners. This was computed as the ratio between the number ofchips transferred by participants and the number of chips they actually offered, averaged over all proposals in the game.Together, we refer to the pair (h, r) as the cooperativeness measure of a participant.10 We use the term “participants” to refer to both people and computer agents.1474Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–14805.3. Social utility functionThe PURB agent used a social utility function to negotiate with people which was a weighted combination of the severalfeatures. For the remainder of this section we will use the term “agent” to refer to the PURB agent, and “person” to refer toits negotiation partner.1. The expected future score for the PURB agent. This score was estimated using a heuristic function that estimated thebenefit to i agent from a potential exchange. It depended on the probability that PURB will get to the goal given thatproposal O is fulfilled at a state s which comprises a board game, the positions of both participants on the board, andthe chips in their possession. We denote this probability as P (G | s, O ), and define the expected future score to i as(cid:2)P (G | s, O ) · 100(cid:3)(cid:2)+1 − P (G | s, O ) · 10 · d(cid:3)+ c · 5where 100 is the number of bonus points to get to the goal according to the CT scoring function; d is the Manhattandistance of the agent from its final position on the board and the goal square, given that the agreement was fulfilledthe p; 10 is the number of penalty points for each square in the distance from the final position of PURB and thegoal square; c is the number of chips left in the player’s possession after it advances to the goal using the shortestpossible path, and 5 is the number of points awarded to the player for each chips left in its possession at the end ofthe game. The probability P (G | s, O ) to get to the goal at state s given proposal O was estimated as the ratio betweenthe number of chips that the other participant delivered to the PURB agent, and the number of chips that the PURBagent was missing to get to the goal at state s given that O was fulfilled.2. The expected future score for the other participant (computed in the same way as for the PURB agent).3. The cooperativeness of the other participant (in terms of helpfulness and reliability).4. The perceived cooperativeness of the other participant. This feature represented the PURB agent’s model of the otherparticipant’s beliefs about its own reliability and helpfulness.The weights for the features of the social utility function were set by hand, and depended on the dependency relationshipsbetween participants as well as their cooperative measures. Generally, as the other participant increased its cooperativenessmeasures, the weighting in the social utility function for PURB that was associated with the score of the other participantwere increased. This was to provide an incentive to the PURB agent to be more generous when its negotiation partnerwas cooperative. Each time an agreement was reached and transfers were made in the game, the PURB agent updated thehelpfulness and reliability measures of both agents. Using this social utility allows the PURB agent to vary its strategy basedon its estimate of the other participant’s cooperativeness measure. For example, if the reliability of the other participantwas high, this would increase the social utility of actions that favor the other participant.115.4. Rules of behaviorThe second component of PURB’s decision-making paradigm was a set of rules that narrowed the search space of possibleactions to be considered by the agent when using its social utility. These rules depended on aspects relating to the state ofthe game (e.g., the number of chips each agent had, whether a participant can independently reach the goal). At each stepof the game, the PURB agent used its social utility function to choose the best action out of the set of possible actions thatwere constrained by the rules. The rules were designed such that the PURB agent begins by acting reliably, and adapts overtime to the individual measure of cooperativeness that is exhibited by its negotiation partner. These rules are based in parton a decision-making model designed to adapt to people’s negotiation behavior in different cultures [19].To enable to specify a finite set of rules for different measures of reliability and helpfulness, the possible values thatthese traits can take were divided into three equal intervals representing low, medium or high measures. For example,low reliability measures ranged from 0 to 13 . We then defined the cooperativeness of an agent to depend on the extent towhich it was reliable and helpful. Specifically, we defined the cooperativeness of a participant to be high when it exhibitedhigh helpfulness and high reliability measures, or high helpfulness and medium helpfulness measures; the cooperativenessmeasure of a participant was medium when it exhibited medium reliability and medium helpfulness measures, or mediumreliability and high helpfulness measures; the cooperativeness measure of a participant was low when it exhibited lowreliability measures (regardless of its helpfulness measure) or medium reliability measures and low helpfulness measure.These values were tuned by hand on games that were not considered in the evaluation.We now list the set of rules used by the PURB agent in combination with its social utility function:(a) Making Proposals The PURB agent generated a subset of possible offers and non-deterministically chose any proposal outof the subset that provided a maximal benefit (within an epsilon interval) according to its social utility function.11 Although this strategy may resemble the principle behind the Tit-for-Tat paradigm, depending on PURB’s model, its strategies can be more nuanced.For example, depending on the dependency relationships that hold in the game, the PURB agent may offer a generous exchange in response to a selfishoffer made by a person.Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801475Before outlining the rules by which the set of possible proposals were generated, we will introduce the followingnotation: we say an agent i is “stronger” than agent j if i is able to reach the goal independently of j, or if it requiresless chips to reach the goal than j. Let O i= j represent the number of proposals in which agent i asks for as many chipsas it receives; O i> j represents the set of proposals in which i asks for more chips than it receives; O j>i represents theset of proposals in which i asks for less chips than it receives.Offers were generated by PURB in a way that considered which participant was stronger than the other. Let i denotethe PURB agent and j denote the other participant. When participants were co-dependent, the set of possible offersi considered included those offers that favored the stronger agent. If i was stronger than j, then the set O i> j wasconsidered (i.e., i requested from j more chips than i proposed to j) and conversely for the case in which j wasstronger than i. In both cases, the set O i= j was also generated and considered (i asks for as many chips as it sends).In the other dependency roles, the offers that were generated depended on the cooperativeness measure of j:(1) When the cooperativeness of j was high or medium, then if i was stronger than j, then the set of possible offersthat i considered included O i> j . This is because that when the reliability of j was high, there was a higher like-lihood that j would keep its commitments, and thus the set of possible exchanges for i included exchanges thatwere highly favorable to i. However, if j was stronger than i, then offers were chosen from the set O j>i . This wasbecause that i wished to minimize the chances that j would reject its offers given that j did not need i to get tothe goal.(2) When the cooperativeness of j was low, then offers were chosen from the set O i> j , regardless of which agent wasstronger. This was because i did not expect j to fulfill its agreements, and thus it proposed offers that were lessbeneficial to j.(cid:4)(b) Accepting Proposals As a responder, the PURB agent accepted an offer if it was more advantageous to it than the offer itwould make as a proposer in the same game state, or if accepting the offer was necessary to prevent the game fromterminating. To state this formally, let ui(O , accept | s) denote the social utility for i from an offer O made by j at statedenote the offer that agent i would make at state s according to the rules in (a). Agent i accepted an offer Os. Let O(cid:4), accept | s). In addition, i would accept any proposal that prevented the game from ending,if ui(O , accept | s) (cid:2) ui(Owhich occurs when the following conditions hold: (1) the chips in the possession of agent i do not allow it to move onthe board at state s; (2) the offer O j allows agent i to move; and (3) if i rejects the offer, the limit for dormant turnswill be reached and the game would end.(c) Transferring Chips These rules specify the extent to which the PURB agent fulfilled its agreements in the game. Thisbehavior directly depended on its model of the other’s reliability:If the reliability of j was low, it was likely that the other participants would not fulfill its agreement. Therefore i didnot send any of its promised chips. However, if both agents were task dependent, and the agreement resulted in bothagents becoming task independent then i sent half of the promised chips with a given probability, because it was notcertain that j would fulfill the agreement.If the reliability of j was high, then i sent all of the promised chips.If the reliability of j was medium, then the extent to which i was reliable depended on the dependency relationships inthe game:(1) If j was task dependent, and the agreement resulted in j becoming task independent, then i sent the largest set ofchips such that j remained task dependent.(2) If the exchange resulted in the PURB agent becoming task independent, and j remaining task dependent, then thePURB agent sent all of the promised chips, or two thirds of its promised chips, depending on its confidence level ofj’s reliability measure being medium. This confidence level depended on the number of times in which the PURBagent interacted with j and j exhibited a medium measure of reliability.(3) If both agents were task dependent, and the agreement resulted in both agents becoming task independent, then isent all of the promised chips.Combining the PURB agent’s social utility function with these rules allows it to adapt its negotiation behavior to that of theother participant.5.5. Empirical methodology and resultsFifty-four human subjects participated in the study drawn from a pool of undergraduate computer science students atBar Ilan University. Each participant was given an identical 30 minute tutorial on Colored Trails that did not disclose theactual boards used in the study. Each participant was seated in front of a terminal for the duration of the study, and couldnot speak to any of the other participants.There were two conditions in the study. In the first condition, people played other people while in the second condition,people played the PURB agent. In all, there were 110 games that were played by people, and 58 games that were played bythe PURB agent and people. Subjects were not told whether they would be playing a person or a computer agent.1212 The exact wording given to subjects were “you may be interacting with a computer agent or with a person”.1476Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–1480Table 6Performance for different dependency conditions.PURB agent vs. peoplePeople vs. peopleCo-dep.(n = 28) 163(n = 50) 131Task ind.(n = 12) 187(n = 30) 181Task dep.(n = 18) 82(n = 30) 102Total(n = 58) 143(n = 110) 136Table 7Actual reliability (left) and probability of acceptance (right) measures.PURB agent vs. peoplePeople vs. peopleCo-dep.(0.79, 0.27)(0.53, 0.56)Task ind.(0.92, 0.62)(0.63, 0.58)Task dep.(0.74, 0.62)(0.41, 0.58)Total(0.81, 0.38)(0.49, 0.53)Table 8Other-benefit (left) and self-benefit (right) measures.PURB agent vs. peoplePeople vs. peopleCo-dep.(74, 86)(40, 52)Task ind.(79, 23)(66, 15)Task dep.(10, 81)(14, 89)Total(54.33, 63.33)(40, 52)Unless otherwise stated, the following results compare the behavior of the PURB agent playing people with that of peopleplaying people. We list the number of observations and means for each result. Significance of results were confirmed forp < 0.05 using parametric statistical tests. Table 6 shows the average performance for the PURB agent and people, measuredas the average score obtained in all games played.5.5.1. Analysis of performanceAs shown by the “total” column in the table, the PURB agent was able to negotiate as well as people: there wasno statistically significant difference between its total average performance (143 points) and people’s performance (136points). However, there were distinct difference in performance for different dependency relationships between players.When players were co-dependent, the PURB agent significantly outperformed people (163 points versus 131 points). Peopleoutperformed the PURB agent in the task dependent condition (102 versus 82 points), but the difference was not statisti-cally significant. There was also no significant difference in performance between the PURB agent and people in the taskindependent condition. In addition (and not shown in the table), in the co-dependent setting, the PURB agent was able toreach the goal 90% of the time, significantly more often than people, who reached the goal 67% of the time. There was nosignificant difference between people and the PURB agent in the extent to which the goal was reached when one of theplayers was task independent and the other player was task dependent.5.5.2. Analysis of behaviorThe purpose of this section is to analyze the extent to which both people and the PURB agent fulfilled their commitmentsin negotiation. For any two participants i and j, let Ci denote the set of chips in possession of i at round n in the game. Let⊆ Ci beO = (O i, O j) denote a proposal at round n, where O i ⊆ Ci was the set of chips that i agreed to send to j. Let O∗j .) Let ri({Ci ∪ O j}) denotethe set of chips actually sent by i following the agreement. (And similarly define C j, O j , and Othe score to player i in the case that j sent all of its promised chips O j , and i did not send any of its chips. We assume thatthe score is computed using the scoring function for the CT game that is described in Section 5.1. We refer to this value as∗the score that was promised by j in proposal O . The factor ri({Ci ∪ Oj thatj actually delivered to i. We refer to this as the actual score to i given the chips that j transferred. The actual reliability of jgiven proposal O at round n is the ratio between the promised score to i from the chips O j in proposal O and the actual}) denotes the score to player i given the chips O∗i∗jscore to i given the chips that O∗j chose to transfer. This is computed as∗j})ri ({O i ∪Ori {Ci ∪O j } . Note that this score-based measure ofreliability is different than the chip-based measure of reliability that was used by the PURB agent (Section 5).Table 7 compares the actual reliability and probability of acceptance for the PURB agent playing other people, and peopleplaying other people. As shown in the Table, for all dependency relationships the PURB agent was consistently more reliablethan people. Also shown by the table is that the PURB agent accepted offers significantly less often than did people (38%vs. 53%). In addition there was a significant correlation of 0.42 between the PURB agent’s reliability and performance, whilethere was no such effect for people. (This result is not shown in the table.) Thus, although the performance of the PURBagent was similar to people in score, the negotiation strategy used by the PURB agent was different from that of people inseveral ways. First, the PURB agent was significantly more reliable than people; and second, the PURB agent was less likelyto accept offers than people.We also compared the types of offers made by people and those made by the PURB agent, as well as how oftenthese offers were accepted. Table 8 shows the average benefit from all proposals in a game that are associated with theproposing player (self-benefit), and the responding player (other-benefit). These results show that when both players wereY. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801477co-dependent, the PURB agent made offers that Pareto dominated (these offers were significantly more beneficial to bothparticipants) the offers made by people.Although the PURB agent we designed was able to negotiate as well as people, our analysis showed that its negotiationstrategy differed from people: when it was task co-dependent its offers Pareto dominated those of people. In this condition,people exhibited medium reliability, and the PURB agent responded to this by generating offers that benefited itself morethan people, as described in its strategy. Interestingly, the proposals made by the PURB agent also offered more to peoplethan did proposals made by people (74 versus 52 points). On average, the PURB agent was less likely to accept offersthan people; also, it was more reliable than people in all conditions, and there was a significant correlation between thereliability of the PURB agent and its performance.We conclude this section by describing an example that highlights the way the PURB agent was able to adapt to thedifferent behavioral traits. The example is taken from two games in which the PURB agent was task independent, and itsnegotiation partner was task dependent. In both of these games, there was an identical offer made by the human participantthat would allow both players to reach the goal. This example consisted of the person asking 1 chip from the PURB agentand offering 3 chips in return. The proposal was accepted by the PURB agent in both games. In the first game, the PURBagent chose to fulfill the agreement and sent the promised chip. However, in the second game, the PURB agent chose notto fulfill the agreement and did not send the promised chip. This was because the reliability of the person in the first gamewas considerably lower than the reliability of the person in the second game.6. Repeated negotiation in settings of incomplete informationIn this section, we describe a variant of the PURB agent that was designed to negotiate in a setting that modifiedthe one in Section 5 as follows: (1) the participants were agent-based, some of which were designed by human subjects;(2) the participants in the negotiation lack pertinent information about each other’s resources when they make decisions;and (3) the negotiation included multi-party interactions of four participants. The relevance of this mixed-network settingto negotiations of the type that may occur in the real world is in its inclusion of agents playing unknown negotiationstrategies. Such negotiations are already commonplace in electronic commerce applications such as ebay, where people canconstruct automatic bidders to serve as proxies and represent themselves in the bidding process. The object of the studywas to build an agent that is able to negotiate proficiently in this setting, despite its uncertainty about others’ resources aswell as their negotiation strategy.Because our setting included numerous negotiation partners and limited information, the agent-design we describe inthis section modified the PURB design in two ways: first, it was endowed with a separate adaptation mechanism for eachnegotiation partner, and second, it estimated the cooperativeness of the negotiation partners without knowing the actualbenefit associated with potential actions in the game.6.1. Experimental designWe modified the CT repeated interaction setting described in Section 5 so that each participant could observe its ownchips, but not the chips of the other participants. Three types of agents were used in the study. The first type of agentswere Peer-Designed (PD) agents, created by graduate-level computer science students at Bar Ilan University who were notgiven any explicit instructions beyond designing an agent that will represent themselves in a negotiation.The second type of agent was a Constant Personality (CP) agent. This agent used a technique to estimate the cooperative-ness of the other participants that was similar to the PURB agent described in Section 5.The third type of agent was a Varying Personality (VP) agent. This agent was a variant of PURB that extended its utilityfunction described in Section 5 to adopt a separate measure of cooperativeness for different levels of cooperativeness ex-hibited by the other participants. Table 9 specifies a match between cooperativeness measures exhibited by the VP agentgiven the cooperativeness measures exhibited by its negotiation partners. The matching process was done empirically, us-ing a held-out test-set of PD agents that were not used again in the evaluation process. Both VP and CP agents wereadaptive: they changed their behavior as a function of their estimate of others’ cooperativeness, given the history of theirobservations. However, the VP agent adopted a unique measure of cooperativeness for each player, whereas the measure ofcooperativeness for the CP agent was constant.A series of repeated games were played between the VP agent and with the other agents in the systems. Each agent’sfinal outcome was the aggregate of its scores in all of the games it participated in. For purpose of analysis, we classified PDand CP agents as either “cooperative” or “non-cooperative” according to the following: cooperative CP agents were those thatengaged in helpful exchanges more than 50% of the time and reneged on their commitments less than 20% of the time. Weexpected cooperative agents to realize opportunities for exchange with each other more often than non-cooperative agentsand to exceed them in performance, as measured by the score in the game. We also expected that in some cases, non-cooperative agents would be able to take advantage of the vulnerability of those cooperative agents that allow themselvesto be exploited. Additionally, we hypothesized that the VP agent would be able to identify and reciprocate cooperativeagents more quickly than CP or PD agents, while staying clear of agents that are non-cooperative. As a result, the VP agentwould perform better than all other agents in the game.1478Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–1480Table 9Matching scheme used by VP agent to adapt itscooperativeness to the cooperativeness of its ne-gotiation partners.PerceivedcooperativenessMatchedcooperativenessLLLMLHMMMHHMHHLLLMLMLMMMHMMMTable 10Average performance of VP agent against cooperative/non-cooperative agents (3 re-peated games).CooperativeNon-cooperativeVP agent170.6142.5PD and CP agents114.898.2Table 11Percentage of exchange types proposed by VP agent.ExchangetypeReciprocalIdleCooperativeagents60%20%Non-cooperativeagents25%39%We ran 5040 games in our experiment, played in 1080 rounds of three consecutive games each. The board games weused in each round varied the task dependency relationships between players. There were 4 players in each game, consistingof a VP agent, two CP agents, and one of the PD agents. The boards used were generated to span all possible task dependencyrole (dependent, co-dependent). Table 10 presents the average score for the VP agent when playing against cooperative andnon-cooperative agents across all games. The scores reported in the table sum over the other players in the game.As shown by the table, the average score achieved by the VP agent was significantly higher than all other agents,regardless of their level of cooperativeness. Also, the VP agent’s score when playing against cooperative agents (170.6)was higher than its score when playing against non-cooperative agents (142.5). Cooperative agents also benefited fromcooperating with the VP agent: their performance was significantly higher than their non-cooperative counterparts (114.8vs. 98.2).In addition, the VP agent engaged in cooperative exchanges with cooperative agents significantly more often than theother agents, while the amount of time the VP agent remained idle when dealing with non-cooperative agents was longerthan the amount of time other agents remained idle (Table 11).To conclude, in repeated game settings, the VP agent, which conditioned its own personality based on its estimate ofothers, outperformed all of the other agents in the system. Some types of agents escaped identification in intermediaterounds, resulting in an increase in their scores. However, the general performance of the VP agent was not affected. Itperformed better than all of the other agents in all of the games we played, and increased its score from game to gameduring the final rounds of the experiment. This shows that adopting a separate measure of cooperativeness for differentagents is key to agents’ success in task settings of incomplete information.6.2. Related work: adapting to agents’ behavioral traitsThe results reported in this section relate to recent approaches for learning to adapt to others’ negotiation behaviorin repeated interactions under uncertainty.13 One strand of research involves learning and adapting to agents’ behavioraltraits. Zhang et al. [51] explored the trade-off between selfishness and helpfulness in environments in which agents wereuncertain about the helpful nature of others in the system. They showed that although realizing every opportunity forcooperation was impossible, selfish agents do better than helpful agents as the rate of uncertainty in the system grows.Hogg and Jennings [22] proposed a model in which agents’ utilities were a weighted summation of each others’ expectedoutcomes. By learning these weights from observations, agents changed their measure of helpfulness over time. Whenall agents in the system were adaptive, high exploration rates led agents to seek out new negotiation opportunities and13 Other works have considered negotiation settings that allow reputation and revelation mechanisms, which does not directly relate to our work.Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–14801479increased the overall social welfare of the group. All these models allowed agents to change their measure of helpfulnessover time as a function of their model of others, and investigated the effects of this behavior on agents’ cooperation andsystem performance. However, these works assumed that players fulfill their commitments following agreements, and didnot model the behavioral types of others.7. Conclusion and future workThis paper has presented studies which demonstrate the importance of learning and of incorporating social factors intoagents’ decision-making models when they operate in open, mixed networks. The studies focused on the design of computeragents for negotiation with each other, or with people, in settings that varied the availability of information, the negotiationprotocol and the interdependence’s that held between agents. For settings of complete information, our findings showedthat• Computer agents can successfully learn the extent to which different people are affected by social factors, and that thisimproves their decision-making.• Agents that learn can outperform computational agents using traditional game- and decision-theoretic strategies.For settings in which complete information about agents’ resources is not available, or in which agents can renege onagreements, we have shown that• Computer agents that adapt their level of cooperation to the varying degree of helpful behavior exhibited by othersoutperform computer agents that do not adapt.• This adaptation process also facilities successful interaction among computer agents and people.These studies used different configurations of the CT test-bed, a framework designed to investigate decision-making insuch networks. The advantage of using CT is that it presents decisions to people in a context that mirrors task settings inthe real world. People are more likely to engage in cooperative behavior in CT environments than when in the context oftraditional representations such as payoff matrices.Additional work by the co-authors and others further demonstrate the usefulness of the CT framework in investigatinga variety of questions about decision-making in group settings. In particular, the Colored Trails system has been usedto construct and evaluate computational models of human reciprocity [15], to investigate the role of gender and socialrelationships in people’s negotiation behavior [29], the way people reason about belief hierarchies in negotiation [13] andthe way people respond to interruptions from computers [27]. CT has proved to support the rapid prototyping and easeof analysis of different kinds of decision-making models that is made possible when using CT. It has also been used as apedagogical tool in for teaching agent design to students in courses at Harvard, Ben-Gurion and Bar-Ilan Universities.AcknowledgementsWe thank Yael Blumberg and Yael Ejgenberg for their help in programming the PURB agent. The work reported in thispaper was supported in part by the Air Force Office of Scientific Research under grants FA9550-05-1-0321 and W911NF-08-1-0144. Development and dissemination of the Colored Trails framework has been supported by the National ScienceFoundation under Grants CNS-0453923, IIS-0705406 and 0705587.References[1] R. Arunachalam, N.M. Sadeh, The supply chain trading agent competition, Electronic Commerce Research and Applications 4 (2005) 63–81.[2] M. Asada, P. Stone, H. Kitano, A. Drogoul, D. Duhaut, M. Veloso, H. Asamas, S. Suzuki, The RoboCup physical agent challenge: Goals and protocols forphase I, in: Robocup-97: Robot Soccer World Cup I, 1998, pp. 42–61.[3] T. Babaian, B.J. Grosz, S.M. Shieber, A writer’s collaborative assistant, in: Intelligent User Interfaces Conference, 2002, pp. 7–14.[4] M. Bazerman, Judgment in Managerial Decision Making, Wiley Publishers, 2001.[5] S.R. Bocionek, Agent systems that negotiate and learn, International Journal of Human–Computer Studies 42 (1995) 265–288.[6] A. Byde, M. Yearworth, K.Y. Chen, C. Bartolini, N. Vulkan, Autona: A system for automated multiple 1–1 negotiation, in: Proceedings of the 4th ACMConference on Electronic Commerce, 2003, pp. 198–199.[7] C.F. Camerer, Behavioral Game Theory. Experiments in Strategic Interaction, Princeton University Press, 2003.[8] J.-C. Cardenas, T.K. Ahn, Elinor Ostrom, Communication and cooperation in a common-pool resource dilemma: A field experiment, in: Steffen Huck(Ed.), Advances in Understanding Strategic Behaviour: Game Theory, Experiments, and Bounded Rationality: Essays in Honour of Werner Güth, Palgrave,2004.[9] J.J. Castro-Schez, N.R. Jennings, X. Luo, N.R. Shadbolt, Acquiring domain knowledge for negotiating agents: a case of study, International Journal ofHuman–Computer Studies 61 (1) (2004) 3–31.[10] A. Chan, K. MacLean, J. McGrenere, Designing haptic icons to support collaborative turn-taking, International Journal of Human–Computer Studies 66(2008) 333–355.[11] A. Dreber, D. Rand, D. Fudenberg, M. Nowak, Winners don’t punish, Nature 452 (7185) (2008) 348–351.[12] A. Falk, U. Fischbacher, A theory of reciprocity, Games and Economic Behavior 54 (2) (2006) 293–315.[13] S.G. Ficici, A. Pfeffer, Simultaneously modeling humans’ preferences and their beliefs about others’ preferences, in: Proc. 7th International Joint Confer-ence on Autonomous Agents and Multi-agent Systems (AAMAS), 2008.1480Y. Gal et al. / Artificial Intelligence 174 (2010) 1460–1480[14] K. Fullam, T. Klos, G. Mueller, J. Sabater, A. Schlosser, Z. Topol, K.S. Barber, J. Rosenschein, L. Vercouter, M. Voss, The agent reputation and trust (ART)testbed competition, Technical report, University of Texas at Austin, 2004.[15] Y. Gal, A. Pfeffer, Modeling reciprocity in human bilateral negotiation, in: Proc. 22nd National Conference on Artificial Intelligence (AAAI), 2007.[16] Y. Gal, A. Pfeffer, F. Marzo, B. Grosz, Learning social preferences in games, in: Proc. 19th National Conference on Artificial Intelligence (AAAI), 2004.[17] Y. Gal, B. Grosz, S. Shieber, A. Pfeffer, A. Allain, The effects of task contexts on the decision-making of people and computers, in: Proc. of the SixthInternational Interdisciplinary Conference on Modeling and Using Context, Roskilde University, Denmark, 2007.[18] Y. Gal, S. D’souza, P. Pasquier, I. Rahwan, S. Abdallah, The effects of goal revelation on computer-mediated negotiation, in: Proceedings of the Annualmeeting of the Cognitive Science Society (CogSci), Amsterdam, The Netherlands, 2009.[19] Y. Gal, S. Kraus, M.J. Gelfand, Y. Blumberg, E. Salmon, An adaptive agent for negotiating with people in different cultures, Harvard University TechnicalReport 2010-102.[20] B. Grosz, S. Kraus, S. Talman, B. Stossel, The influence of social dependencies on decision-making. Initial investigations with a new game, in: Proc. 3rdInternational Joint Conference on Autonomous Agents and Multi-agent Systems (AAMAS), 2004.[21] J. Henrich, R. Boyd, S. Bowles, C.F. Camerer, H. Gintis, R. McElreath, E. Fehr, In search of Homo economicus: Experiments in 15 small-scale societies,American Economic Review 91 (2) (2001) 73–79.[22] L.M.J. Hogg, N.R. Jennings, Socially intelligent reasoning for autonomous agents, IEEE Transactions on Systems, Man and Cybernetics – Part A 31 (5)(2001) 381–399.[23] E. Horvitz, Principles of mixed-initiative user interfaces, in: Proc. of ACM SIGCHI Conference on Human Factors in Computing Systems (CHI), 1999.[24] C.M. Jonker, V. Robu, J. Treur, An agent architecture for multi-attribute negotiation using incomplete preference information, in: Proc. 6th InternationalJoint Conference on Autonomous Agents and Multi-agent Systems (AAMAS), 2007.[25] E. Kamar, E. Horvitz, C. Meek, Mobile opportunistic commerce: Mechanisms, architecture, and application, in: Proc. 7th International Joint Conferenceon Autonomous Agents and Multi-agent Systems (AAMAS), 2008.[26] E. Kamar, Y. Gal, B.J. Grosz, Incorporating helpful behavior into collaborative planning, in: Proc. 8th International Joint Conference on AutonomousAgents and Multi-agent Systems (AAMAS), 2009.[27] E. Kamar, Y. Gal, B.J. Grosz, Modeling user perception of interaction opportunities for effective teamwork, in: IEEE Conference on Social Computing,IEEE Computer Society, 2009, pp. 271–277.[28] R. Katz, S. Kraus, Efficient agents for cliff edge environments with a large set of decision options, in: Proc. 5th International Joint Conference onAutonomous Agents and Multi-agent Systems (AAMAS), 2006.[29] R. Katz, Y. Amichai-Hamburger, E. Manisterski, S. Kraus, Different orientations of males and females in computer-mediated negotiations, Computers inHuman Behavior 24 (2) (2008) 516–534.[30] H. Kitano, RoboCup rescue. A grand challenge for multi-agent systems, in: Fourth International Conference on Multi-Agent Systems (ICMAS), 2000.[31] S. Kraus, D. Lehmann, Designing and building a negotiating automated agent, Computational Intelligence 11 (1) (1995) 132–171.[32] S. Kraus, P. Hoz-Weiss, J. Wilkenfeld, D.R. Andersen, A. Pate, Resolving crises through automated bilateral negotiations, Artificial Intelligence 172 (1)(2008) 1–18.[33] R. Lin, S. Kraus, Can automated agents proficiently negotiate with humans? Communications of the ACM 53 (1) (2010).[34] R. Lin, S. Kraus, J. Wilkenfeld, J. Barry, Negotiating with bounded rational agents in environments with incomplete information using an automatedagent, Artificial Intelligence Journal 172 (6–7) (2008) 823–851.[35] X. Luo, N.R. Jennings, N. Shadbolt, Acquiring user tradeoff strategies and preferences for negotiating agents: A default-then-adjust method, InternationalJournal of Human–Computer Studies 64 (2006) 304–321.[36] R.R. Murphy, Human–robot interaction in rescue robotics, IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews 34 (2)(May 2004).[37] J. Nash, The bargaining problem, Econometrica 18 (1950) 155–162.[38] R.M. Neal, G.E. Hinton, A view of the EM algorithm that justifies incremental, sparse, and other variants, Learning in Graphical Models 89 (1998)355–368.[39] M.A. Nowak, Five rules for the evolution of cooperation, Science 314 (5805) (2006) 1560.[40] Y. Oshrat, R. Lin, S. Kraus, Facing the challenge of human–agent negotiations via effective general opponent modeling, in: Proc. 8th International JointConference on Autonomous Agents and Multi-agent Systems (AAMAS), 2009.[41] E. Ostrom, R. Gardner, J. Walker, Rules, Games, and Common-Pool Resources, Univ. of Michigan Press, 1994.[42] M.E. Pollack, Intelligent technology for an aging population: The use of AI to assist elders with cognitive impairment, AI Magazine 26 (9) (2006).[43] D.V. Pynadath, M. Tambe, Y. Arens, H. Chalupsky, Y. Gil, C. Knoblock, H. Lee, K. Lerman, J. Oh, S. Ramachandran, et al., Electric elves: Immersing anagent organization in a human organization, in: Proceedings of the AAAI Fall Symposium on Socially Intelligent Agents, 2000.[44] D. Rajarshi, J.E. Hanson, J.O. Kephart, G. Tesauro, Agent–human interactions in the continuous double auction, in: Proc. 17th International Joint Confer-ence on Artificial Intelligence (IJCAI), 2001.[45] D. Sally, Conversation and cooperation in social dilemmas, Rationality and Society 7 (1) (1995) 58–92.[46] N. Schurr, P. Patil, F. Pighin, M. Tambe, Using multiagent teams to improve the training of incident commanders, in: Proc. 5th International JointConference on Autonomous Agents and Multi-agent Systems (AAMAS), 2006.[47] S.M. Shieber, A call for collaborative interfaces, Computing Surveys 28A (1996) (electronic), URL http://www.eecs.harvard.edu/~shieber/Biblio/Papers/collab-interface-stmt.html.[48] S. Talman, Y. Gal, M. Hadad, S. Kraus, Adapting to agents’ personalities in negotiation, in: Proc. 4th International Joint Conference on AutonomousAgents and Multi-agent Systems (AAMAS), 2005.[49] D. Traum, J. Rickel, J. Gratch, S. Marsella, Negotiation over tasks in hybrid human–agent teams for simulation-based training, in: Proc. 2nd InternationalJoint Conference on Autonomous Agents and Multi-agent Systems (AAMAS), 2003.[50] A. van Wissen, Team formation among self-interested human actors, Master’s thesis, Utrecht University, 2009.[51] X. Zhang, V. Lesser, T. Wagner, Integrative negotiation in complex organizational agent systems (research abstract), in: Proc. 1st International JointConference on Autonomous Agents and Multi-agent Systems (AAMAS), 2002.