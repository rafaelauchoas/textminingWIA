Artificial Intelligence 172 (2008) 234–264www.elsevier.com/locate/artintConjunctive and disjunctive combination of belief functions inducedby nondistinct bodies of evidence ✩Thierry DenœuxUMR CNRS 6599 Heudiasyc, Université de Technologie de Compiègne, BP 20529, F-60205 Compiègne cedex, FranceReceived 2 August 2006; received in revised form 20 May 2007; accepted 25 May 2007Available online 5 June 2007AbstractDempster’s rule plays a central role in the theory of belief functions. However, it assumes the combined bodies of evidence tobe distinct, an assumption which is not always verified in practice. In this paper, a new operator, the cautious rule of combination,is introduced. This operator is commutative, associative and idempotent. This latter property makes it suitable to combine belieffunctions induced by reliable, but possibly overlapping bodies of evidence. A dual operator, the bold disjunctive rule, is alsointroduced. This operator is also commutative, associative and idempotent, and can be used to combine belief functions issuesfrom possibly overlapping and unreliable sources. Finally, the cautious and bold rules are shown to be particular members ofinfinite families of conjunctive and disjunctive combination rules based on triangular norms and conorms.© 2007 Elsevier B.V. All rights reserved.Keywords: Evidence theory; Dempster–Shafer theory; Transferable belief model; Distinct evidence; Idempotence; Information fusion1. IntroductionDempster’s rule of combination [3,28] is known to play a pivotal role in the theory of belief functions, togetherwith its unnormalized version introduced by Smets in the Transferable Belief Model (TBM) [30], hereafter referred toas the TBM conjunctive rule. Justifications for the origins and uniqueness of these rules have been provided by severalauthors [8,21,22,30]. However, although they appear well founded theoretically, the need for greater flexibility througha larger choice of combination rules has been recognized by many researchers involved in real-world applications.Two limitations of Dempster’s rule and its unnormalized version seem to be their lack of robustness with respectto conflicting evidence (a criticism which mainly applies to Dempster’s rule), and the requirement that the items ofevidence combined be distinct.The issue of conflict management has been addressed by several authors, who proposed alternative rules which,unfortunately, are generally not associative (see, e.g., [11,25,40], and reviews in [27] and [37]). The disjunctive ruleof combination [9,31] (hereafter referred to as the TBM disjunctive rule) is both associative and more robust thanDempster’s rule in the presence of conflicting evidence, and its use is appropriate when the conflict is due to poor✩ This paper is an extended version of [T. Denœux, The cautious rule of combination for belief functions and some extensions, in: Proceedingsof the 9th International Conference on Information Fusion, Florence, Italy, July 2006, Paper #114].E-mail address: Thierry.Denoeux@hds.utc.fr.0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.05.008T. Denœux / Artificial Intelligence 172 (2008) 234–264235reliability of some of the sources. It may also be argued that problems with Dempster’s rule (and, to a lesser extent,with the TBM conjunctive rule) are often due to incorrect or incomplete modelisation of the problem at hand, and thatthese rules often yield reasonable results when they are properly applied [17]. In [37], an expert system approach isadvocated in case of large conflict, to determine its origin and revise the underlying hypotheses accordingly.The other, and perhaps more fundamental, limitation of Dempster’s rule lies in the assumption that the items of evi-dence combined be distinct or, in other words, that the information sources be independent. As remarked by Dempster[3], the real-world meaning of this notion is difficult to describe. The general idea is that, in the combination process,no elementary item of evidence should be counted twice. Thus, nonoverlapping random samples from a populationare clearly distinct items of evidence, whereas “opinions of different people based on overlapping experiences couldnot be regarded as independent sources” [3]. When the nature of the interaction between items of evidence can bedescribed mathematically, then it is possible to extend Dempster’s rule or the TBM conjunctive rule so as to includethis knowledge (see, e.g., [8,29]). However, it is often the case that, although two items of evidence (such as, e.g.,opinions expressed by two experts sharing some experiences, or observations of correlated random quantities) canclearly not be regarded as distinct, the interaction between them is ill known and, in many cases, almost impossible todescribe.In such a common situation, it would be very helpful to have a combination rule that would not rely on the distinct-ness assumption. An early attempt to provide such a rule is reported in [26], but it was limited to the combination ofsimple belief functions (i.e., belief functions having at most two focal sets, including the frame of discernment). Thismethod was extended to separable belief functions (i.e., belief functions that can be decomposed as the conjunctivesum of simple belief functions) in [15]. However, not all belief functions are separable, and the justification for thisapproach was unclear.A natural requirement for a rule allowing the combination of overlapping bodies of evidence is idempotence. Thearithmetic mean does possess this property, but it is not associative, another requirement often regarded as essential.Following an approach initiated by Dubois and Prade in [8], Cattaneo [1] studied a family of rules generalizing theTBM conjunctive rule, based on the definition of a joint belief function on a product space, whose marginals are thebelief functions to be combined. Inside this family, he proposed a rule minimizing the conflict, which happens to beidempotent. However, he showed that, within this particular family of rules, associativity is incompatible both withidempotency, and with conflict minimization.In contrast, associative and idempotent operators exist in possibility theory, based on the minimum triangular normand its dual, the maximum triangular conorm. Dubois and Yager [14] showed that aggregation operators for possibilitydistributions (or, equivalently, fuzzy set connectives) can be deduced from assumptions on multi-valued mappingsunderlying the possibility distributions viewed as consonant belief functions. This approach, however, has not madeit possible to extend possibilistic aggregation operators to arbitrary belief functions while maintaining such propertiesas associativity and idempotency. New operators satisfying these properties are proposed in this paper, following acompletely different approach based on some ideas suggested to the author by the late Philippe Smets [35].The rest of this paper is organized as follows. The underlying fundamental concepts, including the canonical de-composition and the relative information content of belief functions, are first recalled in Section 2. The cautiousconjunctive rule and its dual, the bold disjunctive rule are then introduced in Sections 3 and 4, respectively. The cau-tious and bold rules are shown in Section 5 to be particular members of infinite families of conjunctive and disjunctivecombination rules based on triangular norms and conorms. Finally, the efficiency of the cautious rule to combineinformation from dependent features in a classifier fusion problem is demonstrated experimentally in Section 6, andSection 7 concludes the paper.2. Fundamental conceptsIn this section, the main building blocks of new combination rules defined later are introduced. The basic con-cepts and terminology related to belief functions are first summarized in Section 2.1. Section 2.2 then focuses on thecanonical conjunctive decomposition of nondogmatic belief functions, which allows their representation in the formof conjunctive weight functions taking values in (0, +∞). This section is essential, as the cautious conjunctive ruleintroduced in this paper will be expressed as a function of conjunctive weights. Finally, Section 2.3 recalls knowndefinitions and results related to the ordering of belief functions according to their information content; a new partial236T. Denœux / Artificial Intelligence 172 (2008) 234–264ordering relation based on conjunctive weights is also introduced. This ordering relation will play an important rolein the derivation of the new combination rules.2.1. Basic definitions and notationsIn this paper, the TBM [33,38] is accepted as a model of uncertainty. An agent’s state of belief expressed on a finiteframe of discernment Ω = {ω1, . . . , ωK } is represented by a basic belief assignment (BBA) m, defined as a mappingfrom 2Ω to [0, 1] verifyingA⊆Ω m(A) = 1. Subsets A of Ω such that m(A) > 0 are called focal sets of m. A BBAm is said to be(cid:2)• normal if ∅ is not a focal set (this condition is not imposed in the TBM);• subnormal is ∅ is a focal set;• dogmatic if Ω is not a focal set;• vacuous if Ω is the only focal set;• simple if it has at most two focal sets and, if it has two, Ω is one of those;• categorical if it has only one focal set;• Bayesian if its focal sets are singletons.A subnormal BBA m can be transformed into a normal BBA m∗ by the normalization operation defined as follows:A simple BBA (SBBA) m such that m(A) = 1 − w for some A (cid:6)= Ω and m(Ω) = w can be noted Aw (theadvantage of this notation will become apparent later). The vacuous BBA can thus be noted A1 for any A ⊂ Ω, and acategorical BBA can be noted A0 for some A (cid:6)= Ω. A BBA m can equivalently be represented by its associated belief,implicability, plausibility and commonality functions defined, respectively, as:(cid:3)∗m(A) =k · m(A)if A (cid:6)= ∅,0otherwise,for all A ⊆ Ω, with k = (1 − m(∅))−1.(cid:4)bel(A) =m(B),∅(cid:6)=B⊆A(cid:4)m(B) = bel(A) + m(∅),b(A) =B⊆A(cid:4)pl(A) =m(B),B∩A(cid:6)=∅andq(A) =(cid:4)B⊇Am(B),for all A ⊆ Ω. BBA m can be recovered from any of these functions. For instance:m(A) =andm(A) =(−1)|B|−|A|q(B),∀A ⊆ Ω,(−1)|A|−|B|b(B),∀A ⊆ Ω,(cid:4)B⊇A(cid:4)B⊆Awhere |A| denotes the cardinality of A.The negation (or complement) m of a BBA m is defined as the BBA verifying m(A) = m(A) for all A ⊆ Ω, whereA denotes the complement of A [9]. It may easily be shown that the implicability function b associated to m and thecommonality function q associated to m are linked by the following relation:b(A) = q(A),∀A ⊆ Ω.(8)(1)(2)(3)(4)(5)(6)(7)T. Denœux / Artificial Intelligence 172 (2008) 234–264237A BBA m is said to be consonant if its focal sets are nested. This is known to be equivalent to the followingcondition [28]:pl(A ∪ B) = pl(A) ∨ pl(B),∀A, B ⊆ Ω,where ∨ denote the maximum operator. The above equation defines a possibility measure [41]. Consequently, aconsonant BBA uniquely defines a possibility measure. The corresponding possibility distribution is then given byπ(ω) = pl(cid:6)(cid:5){ω}(cid:5){ω}(cid:6),= q∀ω ∈ Ω.Given a BBA m and a coefficient α ∈ [0, 1], the discounting of m with discount rate α yields the new BBA αmdefined by:αm = (1 − α)m + αmΩ ,where mΩ denotes the vacuous BBA [28, p. 252]. The discounting operation is used to model a situation where asource S provides a BBA m, and the reliability of S is measured by 1 − α. If S is fully reliable (1 − α = 1), then mis left unchanged. If S is not reliable at all, m is transformed into the vacuous BBA. In intermediate situations, m isreplaced by a convex combination of m and the vacuous BBA.The TBM conjunctive rule and Dempster’s rule are noted ∩(cid:14) and ⊕, respectively. They are defined as follows. Letm1 and m2 be two BBAs, and let m1 ∩(cid:14)2 and m1⊕2 be the result of their combination by ∩(cid:14) and ⊕. We have:m1 ∩(cid:14)2(A) =m1(B)m2(C),∀A ⊆ Ω,(cid:4)B∩C=A(cid:7)and, assuming that m1 ∩(cid:14)2(∅) (cid:6)= 1:m1⊕2(A) =0m1 ∩(cid:14)2(A)1−m1 ∩(cid:14)2(∅)if A = ∅,otherwise.(9)(10)Dempster’s rule is just equivalent to the TBM conjunctive rule followed by normalization using (1). Both rules arecommutative, associative, and admit a unique neutral element: the vacuous BBA. They both assume the combineditems of evidence to be distinct. Let Aw1 and Aw2 be two SBBAs with the same focal element A (cid:6)= Ω. The resultof their ∩(cid:14)-combination is the SBBA Aw1w2 . The ⊕ operator yields the same result as long as A (cid:6)= ∅. The TBMconjunctive rule has a simple expression in terms of commonality functions: with obvious notations, we have:q1 ∩(cid:14)2 = q1 · q2.(11)In the TBM, conditioning by B ⊆ Ω is equivalent to conjunctive combination with a categorical BBA mB focusedon B. The result is noted m[B], with m[B] = m ∩(cid:14) mB . This conditional BBA quantifies our belief on Ω, in a contextwhere B holds.Let us now assume that m1 ∩(cid:14)2 has been obtained by combining two BBAs m1 and m2, and then we learn thatm2 is in fact not supported by evidence and should be “removed” from m1 ∩(cid:14)2. This “decombination” operation wasintroduced in [32]. It is well defined if m2 is nondogmatic. Denoting (cid:6)∩(cid:14) this operator, we can write:m1 ∩(cid:14)2 (cid:6)∩(cid:14) m2 = m1.Decombination can easily be computed for any two BBAs m1 and m2 using the corresponding commonality functionsas:q1 (cid:6)∩(cid:14)2(A) = q1(A)q2(A),∀A ⊆ Ω.(12)Note that q2(A) > 0 for all A as long as m2 is nondogmatic. One should also be aware that the quotient of twocommonality functions is not always a commonality function. Consequently, m1 (cid:6)∩(cid:14) m2 is not necessarily a BBA.A disjunctive rule of combination ∪(cid:14) also exists [9,31]: it is defined asm1 ∪(cid:14)2(A) =m1(B)m2(C),∀A ⊆ Ω.(13)(cid:4)B∪C=A238T. Denœux / Artificial Intelligence 172 (2008) 234–264This rule, called the TBM disjunctive rule, is also commutative and associative. It has a simple expression in terms ofimplicability functions, which is the counterpart of (11):b1 ∪(cid:14)2 = b1 · b2.As for the TBM conjunctive rule, an inverse operation may also be defined for the TBM disjunctive rule:b1 (cid:6)∪(cid:14)2(A) = b1(A)b2(A),∀A ⊆ Ω.(14)(15)This operation is well-defined as long as m2 is subnormal (in which case we have b2(A) > 0 for all A). However, itdoes not necessarily produce a belief function. Its interpretation is similar to that of (cid:6)∩(cid:14): it removes, or “decombines,”evidence which has been combined disjunctively with prior knowledge.The dual nature of ∩(cid:14) and ∪(cid:14) becomes apparent when one notices that these two operators are linked by DeMorgan’s laws [9]:m1 ∪(cid:14) m2 = m1 ∩(cid:14) m2,m1 ∩(cid:14) m2 = m1 ∪(cid:14) m2,for all m1 and m2.(16)(17)As remarked by Smets [31], the TBM conjunctive rule is based on the assumption that the belief functions to becombined are induced by reliable sources of information, whereas the TBM disjunctive rule only assume that at leastone source of information is reliable, but we do not know which one. Both rules assume the sources of information tobe independent (i.e., they are assumed to provide distinct, nonoverlapping pieces of evidence).In the TBM, combination rules belong to the credal level where evidence aggregation takes place, whereas deci-sions are made at the pignistic level [38], where each BBA m is mapped to a pignistic probability function Betpmdefined by(cid:4)Betpm(ω) =m∗(A)|A|{A: ω∈A}where m∗ denotes the normalized version of m.,∀ω ∈ Ω,(18)2.2. Canonical conjunctive decomposition of a belief functionShafer [28, Chapter 4] defined a separable BBA as the result of the ⊕ combination of SBBAs. For every separableBBA in the sense of Shafer, one has:(cid:8)m =Aw(A),(19)∅(cid:6)=A⊂Ωwith w(A) ∈ [0, 1] for all A ⊂ Ω, A (cid:6)= ∅. This representation is unique if m is nondogmatic. Shafer named thisrepresentation the canonical decomposition of m.The concept of separability can be extended to subnormal BBAs in two ways:• We will say that a BBA m is u-separable (where “u” stands for “unnormalized”) if we havem = ∩(cid:14)A⊂ΩAw(A),with w(A) ∈ [0, 1] for all A ⊂ Ω.(cid:8)∗ =mAw(A),• We will say that a BBA m is n-separable (where “n” stands for “normalized”) if we have(20)(21)∅(cid:6)=A⊂Ωwhere w(A) ∈ [0, 1] for all A ⊂ Ω, A (cid:6)= ∅, and m∗ is the normalized form of m.Again, the decompositions (20) and (21) are unique as long as m is nondogmatic. Clearly, (20) implies (21), but theconverse is not true, as will be shown below. Consequently, u-separability is a stronger notion than n-separability.T. Denœux / Artificial Intelligence 172 (2008) 234–2642392.2.1. Extension to nondogmatic BBAsThe canonical decomposition of a separable BBA was extended to any nondogmatic BBA by Smets [32]. The keyto such a generalization is the notion of generalized simple BBA (GSBBA), defined as a function μ from 2Ω to Rverifyingμ(A) = 1 − w,μ(Ω) = w,μ(B) = 0 ∀B ∈ 2Ω \ {A, Ω},(24)for some A (cid:6)= Ω and some w ∈ [0, +∞). Any GSBBA μ can thus be noted Aw for some A (cid:6)= Ω and w ∈ [0, +∞).When w (cid:2) 1, μ is a SBBA. When w > 1, μ is not a BBA, since it is no longer a mapping from 2Ω to [0, 1]. Such afunction can be referred to as an inverse simple BBA (ISBBA), using a terminology similar to that used in [32]. TheTBM conjunctive rule can be trivially extended to combine SBBAs and ISBBAs alike. In particular, the relationshipAw1 ∩(cid:14) Aw2 = Aw1w2 still holds for w1, w2 ∈ [0, +∞).In [32], Smets proposed an interpretation of an ISBBA as representing a state of belief in which one has somereasons not to believe in A. By combining Aw for some w > 1 with the SBBA A1/w using the TBM conjunctive rule,one obtains the vacuous bba A1. Hence, the ISBBA Aw corresponds to a situation where the agent has a “debt ofbelief” in A, and some evidence has to be accumulated before it starts to believe in A.Using the concept of GSBBA, and extending Shafer’s approach, Smets showed that any nondogmatic BBA can beuniquely represented as the conjunctive combination of GSBBAs:m = ∩(cid:14)A⊂ΩAw(A),(25)with w(A) ∈ (0, +∞) for all A ⊂ Ω. Eq. (25) is clearly an extension of (19). It defines the canonical conjunctivedecomposition of m (we will see in Section 4.1 that a canonical disjunctive decomposition also exists). The weightsw(A) for every A ⊂ Ω can be obtained from the commonalities using the following formula:(cid:9)w(A) =q(B)(−1)|B|−|A|+1B⊇A⎧(cid:14)⎪⎨(cid:14)=⎪⎩(cid:14)(cid:14)B⊇A,|B| /∈2N q(B)B⊇A,|B|∈2N q(B)B⊇A,|B|∈2N q(B)B⊇A,|B| /∈2N q(B)if |A| ∈ 2N,otherwise,where 2N denotes the set of even natural numbers. Eq. (26) can be equivalently written(cid:4)ln w(A) = −(−1)|B|−|A|ln q(B),∀A ⊂ Ω.(22)(23)(26)(27)(28)B⊇AOne notices the similarity with (6). Hence, any procedure for transforming q to m (such as the Fast Möbius Transform[20] or matrix multiplication [34]) can be used to compute ln w from − ln q.The function w : 2Ω \ {Ω} → (0, +∞) (hereafter referred to as the conjunctive weight function) is thus yet anotherequivalent representation of any nondogmatic BBA (together with bel, pl, q, etc.). This concept of conjunctive weightTable 1A BBA with its commonality and weight functionsA∅{a}{b}{a, b}{c}{a, c}{b, c}Ωm(A)q(A)w(A)0000.3000.50.210.510.50.70.20.70.2117/42/5112/7240T. Denœux / Artificial Intelligence 172 (2008) 234–264function can be extended to a dogmatic BBA m by discounting it with some discount rate (cid:6) and letting (cid:6) tend towards0 [32]. However, this extension requires some mathematical subtleties. Furthermore, it may be argued that most (ifnot all) states of belief, being based on imperfect and not entirely conclusive evidence, should be represented bynondogmatic belief functions, even if the mass m(Ω) is very small. For instance, consider a coin tossing experiment.It is natural to define a BBA on Ω = {Heads, Tails} as m({Heads}) = 0.5 and m({Tails}) = 0.5. However, this assumesthe coin to be perfectly balanced, a condition never exactly verified in practice. So, a more appropriate BBA may bem({Heads}) = 0.5(1 − (cid:6)), m({Tails}) = 0.5(1 − (cid:6)) and m(Ω) = (cid:6) for some small (cid:6) > 0.Example 1. Let Ω = {a, b, c} be a frame of discernment, and m the BBA shown in Table 1. The weights can becomputed from the commonalities using (26) as follows:= 0.5 × 1 × 0.7 × 0.21 × 0.5 × 0.2 × 0.7= 1,(cid:6)(cid:6)(cid:6)ww(cid:5){b}(cid:5){a, b}w(∅) = q({a})q({b})q({c})q({a, b, c})q(∅)q({a, b})q({a, c})q({b, c})(cid:5)= 0.5 × 0.2= q({a, b})q({a, c}){a}0.5 × 0.2q({a})q({a, b, c})= 0.5 × 0.7= q({a, b})q({b, c})q({b})q({a, b, c})1 × 0.2= q({a, b, c})q({a, b})= q({a, c})q({b, c})q({c})q({a, b, c})= q({a, b, c})q({a, c})= q({a, b, c})q({b, c})= 0.2= 2/5,0.5= 0.2 × 0.20.7 × 0.2= 0.20.1= 0.20.7(cid:5){a, c}(cid:5){b, c}= 2/7.(cid:5){c}= 1,wwww(cid:6)(cid:6)(cid:6)= 1,= 7/4,= 1,We can see that m can be represented as the conjunctive combination of two SBBAs {a, b}2/5 and {b, c}2/7, and anISBBA {b}7/4.2.2.2. Special casesIn the following two propositions, we provide analytical formulas for the conjunctive weight functions associatedto two important classes of BBAs.Proposition 1. Let A1, . . . , An be n subsets of Ω such that Ai ∩ Aj = ∅ for all i, j ∈ {1, . . . , n}, and let m be a BBA(cid:2)nk=1 m(Ak) (cid:2) 1, so that ∅ may also be a focalon Ω with focal sets A1, . . . , An, and Ω. We assume that m(Ω) +set. The conjunctive weight function associated to m is:⎧⎨w(A) =⎩m(Ω)m(Ak)+m(Ω) ,(cid:5)(cid:14)n1 + m(Ak)m(Ω)k=1m(Ω)1,A = Ak,(cid:6), A = ∅,otherwise.Proof. We have:(cid:7)q(A) =m(Ak) + m(Ω), A ⊆ Ak,A = ∅,1,m(Ω),otherwise.Consequently, m may be expressed as a function of q as follows:m(Ak) = q(Ak) − q(Ω),m(Ω) = q(Ω),m(∅) = q(∅) − q(Ω) −k = 1, . . . , n,n(cid:4)(cid:5)(cid:6)q(Ak) − q(Ω)m(A) = 0,k=1∀A /∈ {A1, . . . , An, Ω, ∅}.,(29)(30)(31)(32)T. Denœux / Artificial Intelligence 172 (2008) 234–264241As explained above, ln w may be obtained from − ln q using any procedure that transforms q to m. Consequently,we may, in the above equations, replace m by ln w and q by − ln q (except in (30), because w(Ω) is not defined). Weobtain from (29):ln w(Ak) = − ln q(Ak) + ln q(Ω) = lnq(Ω)q(Ak),which impliesw(Ak) =m(Ω)m(Ak) + m(Ω),k = 1, . . . , n.Now, from (31) we getln w(∅) = − ln q(∅) + ln q(Ω) +(cid:15)= lnq(Ω)1−nn(cid:9)q(Ak),(cid:5)(cid:6)ln q(Ak) − ln q(Ω),n(cid:4)k=1(cid:16)(cid:15)= lnk=1n(cid:9)m(Ω)1−n(cid:5)(cid:6)m(Ω) + m(Ak)(cid:16),k=1which impliesw(∅) = m(Ω)(cid:17)n(cid:9)k=1(cid:18).1 + m(Ak)m(Ω)Finally, (32) implies that w(A) = 1, for all A /∈ {A1, . . . , An, Ω, ∅}. (cid:2)The BBAs studied in Proposition 1 may be termed “quasi-Bayesian”, as they can be obtained by discountingBayesian BBAs defined on a coarsening of Ω. This class of BBAs is closed under the TBM conjunctive rule. Quasi-Bayesian BBAs are defined by a small number of masses, and are frequently encountered in applications.Another important case concerns consonant BBAs, whose focal sets are nested. The following proposition providesformulas to compute the weight function associated to a consonant BBA.Proposition 2. Let m be a consonant BBA, with associated possibility distribution π(ωk) = q({ωk}), k = 1, . . . , K.We note πk = π(ωk) and we assume that the elements of Ω have been arranged in decreasing order of plausibility,i.e., we have1 (cid:3) π1 (cid:3) π2 (cid:3) · · · (cid:3) πK > 0.Let Ak = {ω1, . . . , ωk}, k = 1, . . . , K. The focal sets of m are in {A1, . . . , AK , ∅} (m is subnormal if π1 < 1, and it isnondogmatic since we have assumed πK > 0). The conjunctive weight function associated to m is:w(A) =π1πk+1πk1,A = ∅,, A = Ak, 1 (cid:2) k < K,otherwise.(cid:7)⎧⎪⎨Proof. As shown in [7], m can be computed from π1, . . . , πK as:⎪⎩m(A) =1 − π1,A = ∅,πk − πk+1, A = Ak, 1 (cid:2) k < K,A = Ω,πK ,otherwise.0Since πk = q({ωk}), we may deduce thatln π1,− ln πk + ln πk+1, A = Ak, 1 (cid:2) k < K,0ln w(A) =otherwise,A = ∅,(cid:7)242T. Denœux / Artificial Intelligence 172 (2008) 234–264from which the desired expression of w can be easily derived. (cid:2)2.2.3. Normalization and combinationIt may be remarked that normalizing a subnormal BBA m using (1) amounts to combining it with the ISBBA ∅k:∗ = m ∩(cid:14) ∅k.mConsequently, the weight function w∗ associated to m∗ is identical to w, except for the weight assigned to ∅. Ifm = ∩(cid:14)A⊂Ω Aw(A), we have(cid:19)∗ = ∅k ∩(cid:14) ∅w(∅) ∩(cid:14)m(cid:19)= ∅k·w(∅) ∩(cid:14)= ∩(cid:14)A⊂ΩAw∗(A)∩(cid:14)∅(cid:6)=A⊂Ω(cid:20)Aw(A)(cid:20)∩(cid:14)∅(cid:6)=A⊂ΩAw(A),with w∗(∅) = k · w(∅) and w∗(A) = w(A) for all A ∈ 2Ω \ {∅, Ω}. We can write, equivalently:∗ =m(cid:8)∅(cid:6)=A⊂ΩAw(A).(33)As a direct consequence of the above remark, it is easy to formulate criteria for u-separability and n-separability:• A BBA m is u-separable iff w(A) (cid:2) 1, for all A ⊂ Ω;• A BBA m is n-separable iff w(A) (cid:2) 1, for all A ⊂ Ω, A (cid:6)= ∅.For instance, quasi-Bayesian BBAs studied in Proposition 1 are n-separable, but they are not u-separable in general(we may have w(∅) > 1). In contrast, consonant BBAs are u-separable, since they satisfy the condition w(A) (cid:2) 1 forall A ⊂ Ω.The w representation appears particularly interesting when it comes to combining BBAs using the TBM conjunc-tive rule or Dempster’s rule. Indeed, let m1 and m2 be two BBAs with weight functions w1 and w2. We have:m1 ∩(cid:14) m2 =(cid:20)(cid:19)Aw1(A)∩(cid:14)(cid:19)∩(cid:14)A⊂ΩAw2(A)∩(cid:14)A⊂Ω(cid:20)= ∩(cid:14)A⊂ΩAw1(A)w2(A).We can thus write, with obvious notations:w1 ∩(cid:14)2 = w1 · w2,(34)(35)which is reminiscent of (11). The inverse TBM conjunctive rule (cid:6)∩(cid:14) also has a simple expression in the w-space,similar to (33): we have w1 (cid:6)∩(cid:14)2 = w1/w2. Hence,m1 (cid:6)∩(cid:14) m2 =(cid:20)(cid:19)Aw1(A)(cid:6)∩(cid:14)(cid:19)∩(cid:14)A⊂ΩAw2(A)∩(cid:14)A⊂Ω(cid:20)= ∩(cid:14)A⊂ΩAw1(A)/w2(A).Finally, using (33), it is easy to see that(cid:8)m1 ⊕ m2 =Aw1(A)w2(A).∅(cid:6)=A⊂Ω(36)(37)(38)T. Denœux / Artificial Intelligence 172 (2008) 234–2642432.2.4. Latent belief structureLet m be a nondogmatic belief function, and w its associated conjunctive weight function. For each weight w(A)let us define the following two quantities:wc(A) = 1 ∧ w(A),andwd (A) = 1 ∧ 1w(A),where ∧ denotes the minimum operator. It is clear that we havew(A) = wc(A)wd (A).Consequently, we can writeAwc(A)/wd (A)m = ∩(cid:14)A⊂Ω(cid:19)∩(cid:14)A⊂Ω= mc (cid:6)∩(cid:14) md .=Awc(A)(cid:20)(cid:19)(cid:6)∩(cid:14)(cid:20)Awd (A)∩(cid:14)A⊂Ω(39)(40)(41)(42)(43)(44)Any nondogmatic BBA m can thus be decomposed into two u-separable BBAs mc and md called, respectively, itsconfidence and diffidence components. The pair (mc, md ) forms what Smets called a latent belief structure (LBS) [32].He proposed to interpret mc as representing positive evidence, i.e., good reasons to believe in various propositionsA ⊆ Ω, and md as representing negative evidence, i.e., good reasons not to believe in the same propositions. The BBAm is obtained by removing the negative evidence md from the positive evidence mc. Note that we have the following1, mdproperty with respect to the TBM conjunctive rule: if (mc2 ) are two LBSs associated to nondogmatic2 ) is a LBS associated to m1 ∩(cid:14) m2.∩(cid:14) mdBBAs m1 and m2, respectively, then (mc11 ) and (mc2, md12, md∩(cid:14) mc2.3. Informational comparison of belief functionsIn the TBM, the Least Commitment Principle (LCP) plays a role similar to the principle of maximum entropy inBayesian Probability Theory. As explained in [31], the LCP indicates that, given two belief functions compatible witha set of constraints, the most appropriate is the least informative. To make this principle operational, it is necessaryto define ways of comparing belief functions according to their information content. Three such partial orderings,generalizing set inclusion, were proposed in the 1980s by Yager [39] and Dubois and Prade [9]; they are defined asfollows:• pl-ordering: m1 (cid:18)pl m2 iff pl1(A) (cid:2) pl2(A), for all A ⊆ Ω;• q-ordering: m1 (cid:18)q m2 iff q1(A) (cid:2) q2(A), for all A ⊆ Ω;• s-ordering: m1 (cid:18)s m2 iff there exists a square matrix S with general term S(A, B), A, B ∈ 2Ω verifying(cid:4)S(A, B) = 1,∀A ⊆ Ω,B⊆ΩS(A, B) > 0 ⇒ A ⊆ B, A, B ⊆ Ω,such thatm1(A) =(cid:4)B⊆ΩS(A, B)m2(B),∀A ⊆ Ω.(45)The term S(A, B) may be seen as the proportion of the mass m2(B) which is transferred (“flows down”) to A.Matrix S is named a specialization matrix [22,34], and m1 is said to be a specialization of m2.244T. Denœux / Artificial Intelligence 172 (2008) 234–264As shown in [9], these three definitions are not equivalent: m1 (cid:18)s m2 implies m1 (cid:18)pl m2 and m1 (cid:18)q m2, but theconverse is not true. Additionally, pl-ordering and q-ordering are not comparable. However, in the set of consonantBBAs, these three partial orders are equivalent. The interpretation of these ordering relations is discussed in [9] froma set-theoretical perspective, and in [12] from the point of view of the TBM. Whenever we have m1 (cid:18)x m2, withx ∈ {pl, q, s}, we will say that m1 is x-more committed than m2.Another concept which leads to an alternative definition of informational ordering is that of Dempsterian special-ization [22]. m1 is said to be a Dempsterian specialization of m2, which we note m1 (cid:18)d m2, iff there exists a BBA msuch that m1 = m ∩(cid:14) m2. As shown in [22], this is a stronger condition than specialization, i.e., we have m1 (cid:18)d m2 ⇒m1 (cid:18)s m2, but the converse is false. If m1 = m ∩(cid:14) m2, then there is a specialization matrix Sm defined as a function ofm, called a Dempsterian specialization matrix, allowing the computation of m1 from m2 using relation (45).Finally, we can think of one more definition of informational ordering based on the weight function recalled inSection 2.2: given two nondogmatic BBAs m1 and m2, we can say that m1 is w-more committed than m2, whichwe note m1 (cid:18)w m2, iff w1(A) (cid:2) w2(A), for all A ⊂ Ω. Because of (35), this is equivalent to the existence of a u-separable BBA m, with weight function w = w1/w2, such that m1 = m ∩(cid:14) m2. Consequently, w-ordering is strictlystronger than d-ordering. The meaning of (cid:18)d and (cid:18)w is clear: if m1 (cid:18)d m2 or m1 (cid:18)w m2, it means that m1 is theresult of the combination of m2 with some BBA m; consequently, m1 has a higher information content than m2. Inthe case of (cid:18)w, the requirement that m be u-separable may seem artificial. However, it may be argued that most belieffunctions encountered in practice result from the pooling of simple evidence, and are therefore u-separable. As shownin Section 2.2.2, this is also the case for consonant belief functions, a class of belief functions often encountered inapplications because of its simplicity. Furthermore, we will see that w-ordering happens to be a simpler and moreconvenient notion, for some purposes, than other orderings. A slightly weaker notion based on n-separability will bedefined later in Section 3.3. We defer the introduction of this additional notion for clarity of presentation.In summary, we thus have, for any two nondogmatic BBAs m1 and m2:(cid:3)m1 (cid:18)w m2 ⇒ m1 (cid:18)d m2 ⇒ m1 (cid:18)s m2 ⇒m1 (cid:18)pl m2,m1 (cid:18)q m2,(46)where all implications are strict.orderings (cid:18)x with x ∈ {pl, q, s, d}, i.e., we haveThe vacuous BBA mΩ (with weight function wΩ (A) = 1, for all A ⊂ Ω) is the unique greatest element for partialm (cid:18)x mΩ ,∀m, ∀x ∈ {pl, q, s, d}.In contrast, mΩ is only a maximal element for (cid:18)w, i.e., we have the following weaker propertymΩ (cid:18)w m ⇒ m = mΩ .The BBAs that are w-less specific than mΩ are the u-separable ones. Non-u-separable BBAs are not comparable withmΩ according to relation (cid:18)w.As emphasized by Dubois and Prade in [9], relations (cid:18)x with x ∈ {pl, q, s} generalize set inclusion: if mA andmB are two categorical BBAs such that mA(A) = 1 and mB (B) = 1, then mA (cid:18)x mB , with x ∈ {pl, q, s}, if and onlyif A ⊆ B. The same is true for relation (cid:18)d . For relation (cid:18)w, this property does not hold, since categorical BBAs,being dogmatic, cannot be compared according to (cid:18)w. However, we can still have a similar property if we consider acategorical BBA as the limit of a sequence of nondogmatic BBAs. More precisely, let ((cid:6)n), n = 1, 2, . . . , ∞, be a realsequence such that (cid:6)n ∈ [0, 1] for all n, and limn→∞ (cid:6)n = 0. For any A ⊆ Ω, let mnA the BBA with following weightfunction:wn(cid:6)n1A(C) =if C ⊇ A,otherwise,for all C ⊂ Ω. It is clear that mnfor all C (cid:6)= A. This means that the categorical BBA mA may seen as the limit of the sequence (mnbeing uniquely defined, given ((cid:6)n). Using this representation, we can state the following proposition.A(A) (cid:3) 1 − (cid:6)n. Consequently, we have limn→∞ mnA(A) = 1 and limn→∞ mnA(C) = 0,A), this sequence(cid:3)Proposition 3. Let A and B be two subsets of Ω, mA and mB the categorical BBAs focused on A and B, and (mnA)and (mnB ) their representations as sequences of BBAs as defined above. Then, we haveA ⊆ B ⇔ mnA(cid:18)w mnB ,∀n (cid:3) 1.T. Denœux / Artificial Intelligence 172 (2008) 234–264245Proof. Assume that A ⊆ B. Let C be an arbitrary subset of Ω:• if C ⊇ B, then C ⊇ A, and we have wn• if C (cid:6)⊇ B, then wnB (C) = 1 (cid:3) wnA(C).A(C) = wnB (C) = (cid:6)n;Conversely, assume that wnand A ⊆ B. (cid:2)A(C) (cid:2) wnB (C) for all C ⊂ Ω. Then wnA(B) (cid:2) wnB (B) = (cid:6)n. Consequently, wnA(B) = (cid:6)n,Equipped with these definitions of the relative information content of belief functions, it is possible to give anoperational meaning of the LCP. Let M be a set of BBA compatible with a set of constraints. The LCP dictates tochoose a greatest element in M, if one such element exists, according to one of the partial ordering (cid:18)x , for somex ∈ {pl, q, s, d, w}. These partial orderings seem equally well justified and reasonable and, in the absence of anydecisive argument to discard any of them, considerations of simplicity, existence of a solution and tractability ofcalculations can be invoked to choose a particular ordering for a given problem. For instance, q-ordering is adopted in[12] to derive the expression of the q-least committed BBA with given pignistic probability function. In the followingsection, the same principle is used to derive a rule of combination, using partial ordering (cid:18)w.3. The cautious conjunctive rule3.1. Derivation from the LCPJust as relations (cid:18)x may be seen as generalizing set inclusion, it is possible to conceive conjunctive combinationrules generalizing set intersection, by reasoning as follows. Assume that we have two sources of information, one ofwhich indicates that the true value of the variable of interest ω lies in A ⊆ Ω, while the other one indicates that it liesin B ⊆ Ω, with A (cid:6)= B. If we consider both sources as reliable, then we can deduce that ω lies in some set C such thatC ⊆ A, and C ⊆ B. The largest of these subsets is the intersection A ∩ B of A and B.Let us now assume that the two sources provide BBAs m1 and m2, and the sources are both considered to bereliable. The agent’s state of belief, after receiving these two pieces of information, should then be represented bya BBA m12 more informative than m1, and more informative than m2. Let us denote by Sx(m) the set of BBAsm(cid:21) such that m(cid:21) (cid:18)x m, for some x ∈ {pl, q, s, d, w}. We thus have m12 ∈ Sx(m1) and m12 ∈ Sx(m2) or, equivalently,m12 ∈ Sx(m1) ∩ Sx(m2). According to the LCP, one should select the x-least committed element in Sx(m1) ∩ Sx(m2).This defines a conjunctive combination rule, provided that an x-least committed element (i.e., a greatest element withrespect with partial order (cid:18)x ) exists and is unique.In [12], this approach was used to justify the minimum rule for combining possibility distributions, from the pointof view of the TBM. Let m1 and m2 be two consonant BBAs, and let q1 and q2 be their respective commonalityfunctions. Then, the consonant BBA m12 with commonality function q12(A) = q1(A) ∧ q2(A) for all A ⊆ Ω isclaimed in [12] to be the s-least committed element in the set Ss(m1) ∩ Ss(m2). This approach, however, cannot beblindly transposed to nonconsonant BBAs, since the minimum of two commonality functions is not, in general, acommonality function.However, applying this approach with the (cid:18)w ordering does yield an interesting solution, as shown by the followinglemma and proposition.Lemma 1. Let m by a nondogmatic BBA with conjunctive weight function w, and let w(cid:21) be a mapping from 2Ω \ Ωto (0, +∞) such that w(cid:21)(A) (cid:2) w(A) for all A ⊂ Ω. Then w(cid:21) is the conjunctive weight function of some BBA m(cid:21).Proof. We have(cid:21)w(A) = w(A) · w(cid:21)(A)w(A),∀A ⊂ Ω.Since w(cid:21)(A)/w(A) (cid:2) 1 for all A ⊂ Ω, w(cid:21)/w is the weight function of a u-separable BBA. Consequently, w(cid:21) is theweight function of a BBA m(cid:21) obtained by combining m with a u-separable BBA using the TBM conjunctive rule. (cid:2)246T. Denœux / Artificial Intelligence 172 (2008) 234–264Proposition 4. Let m1 and m2 be two nondogmatic BBAs. The w-least committed element in Sw(m1) ∩ Sw(m2) existsand is unique. It is defined by the following weight function:w1 ∧(cid:14)2(A) = w1(A) ∧ w2(A),∀A ⊂ Ω.(47)Proof. For i = 1 and i = 2, we have m ∈ Sw(mi) iff w(A) (cid:2) wi(A) for all A ⊂ Ω. Hence, m ∈ Sw(m1) ∩ Sw(m2) iffw(A) (cid:2) w1(A) ∧ w2(A) for all A ⊂ Ω. Let us consider function w1 ∧(cid:14)2 defined by w1 ∧(cid:14)2(A) = w1(A) ∧ w2(A), forall A ⊂ Ω. This is the conjunctive weight function of a valid BBA, as a consequence of Lemma 1. Consequently, itcorresponds to the unique w-least committed element in Sw(m1) ∩ Sw(m2). (cid:2)Eq. (47) defines a new rule which can be formally defined as follows.Definition 1 (Cautious conjunctive rule). Let m1 and m2 be two nondogmatic BBAs. Their combination using thecautious conjunctive rule (or cautious rule for short) is noted m1 ∧(cid:14)2 = m1 ∧(cid:14) m2. It is defined as the BBA with thefollowing weight function:w1 ∧(cid:14)2(A) = w1(A) ∧ w2(A),∀A ⊂ Ω.We thus havem1 ∧(cid:14) m2 = ∩(cid:14)A⊂ΩAw1(A)∧w2(A).(48)Note that this rule happens to generalize a method proposed by Kennes [19] for combining u-separable BBAsinduced by nondistinct items of evidence, based on an application of category theory to evidential reasoning. Usingthe canonical decomposition of nondogmatic belief functions and the concept of w-ordering, the new rule describedin this paper proves to be well justified for combining the wider class of nondogmatic belief functions.As another remark, it must also be emphasized that the cautious rule provides a BBA m1 ∧(cid:14) m2 which is the w-leastcommitted in the set Sw(m1) ∩ Sw(m2) of BBAs that are w-more committed than both m1 and m2. When either m1or m2 is not u-separable, then m1 ∩(cid:14) m2 does not belong to that set (because, e.g., w1(A)w2(A) > w1(A) wheneverw2(A) > 1). Consequently, we do not have m1 ∩(cid:14) m2 (cid:18)w m1 ∧(cid:14) m2 in general, except when both m1 and m2 areseparable.In practice, the cautious combination of two nondogmatic BBAs m1 and m2 can thus be computed as follows:• Compute the commonality functions q1 and q2 using (5);• Compute the weight functions w1 and w2 using (26);• Compute m1 ∧(cid:14)2 = m1 ∧(cid:14) m2 as the ∩(cid:14) combination of GSBBAs Aw1(A)∧w2(A), for all A ⊂ Ω such that w1 ∧w2(A) (cid:6)= 1.Example 2. Table 2 shows the weight functions of two BBAs m1 and m2 on Ω = {a, b, c}, a well as the combinedweight function w1 ∧(cid:14)2 and BBA m1 ∧(cid:14) m2. In this case, m1 ∧(cid:14)2 is obtained as the TBM conjunctive combination ofthree SBBAs: {b}0.7, {a, b}2/5 and {b, c}2/7. By combining the first two we get a mass 0.3 on {b}, 3/5 × 0.7 = 0.42on {a, b} and 2/5 × 0.7 = 0.28 on Ω. Combination with {b, c}2/7 then yieldsTable 2Combination of two BBAs using the cautious ruleA∅{a}{b}{a, b}{c}{a, c}{b, c}Ωm1(A)0000.3000.50.2w1(A)117/42/5112/7m2(A)000.30000.40.3w2(A)110.71113/7w1 ∧(cid:14)2(A)110.72/5112/7m1 ∧(cid:14)2(A)000.60.12000.20.08m1 ∩(cid:14)2(A)000.420.09000.430.06T. Denœux / Artificial Intelligence 172 (2008) 234–264247= 0.3 × (2/7 + 5/7) + 0.42 × 5/7 = 0.6,(cid:6)(cid:6)(cid:5){b}(cid:5){a, b}(cid:6)(cid:5){b, c}m1 ∧(cid:14)2m1 ∧(cid:14)2m1 ∧(cid:14)2m1 ∧(cid:14)2(Ω) = 0.28 × 2/7 = 0.08.= 0.42 × 2/7 = 0.12,= 0.28 × 5/7 = 0.2,The result of the combination of m1 and m2 using the TBM conjunctive rule directly from (9) is shown in the lastcolumn of Table 2 for comparison.3.2. PropertiesProposition 5. The cautious conjunctive rule has the following properties:Commutativity: for all m1 and m2, m1 ∧(cid:14) m2 = m2 ∧(cid:14) m1;Associativity: for all m1, m2 and m3, m1 ∧(cid:14) (m2 ∧(cid:14) m3) = (m1 ∧(cid:14) m2) ∧(cid:14) m3;Idempotence: for all m, m ∧(cid:14) m = m;Distributivity of ∩(cid:14) with respect to ∧(cid:14): for all m1, m2 and m3,m1 ∩(cid:14) (m2 ∧(cid:14) m3) = (m1 ∩(cid:14) m2) ∧(cid:14) (m1 ∩(cid:14) m3).Proof. Commutativity, associativity and idempotence result directly from corresponding properties of the minimumoperator. Distributivity of ∩(cid:14) with respect to ∧(cid:14) is a consequence of distribution of the product with respect to theminimum:w1(w2 ∧ w3) = (w1w2) ∧ (w1w3),∀w1, w2, w3.(cid:2)The last property (distributivity) is actually quite important, as it explains why the cautious rule can be consideredto be more relevant than the TBM conjunctive rule ∩(cid:14) when combining nondistinct items of evidence: if two sourcesprovide BBAs m1 ∩(cid:14) m2 and m1 ∩(cid:14) m3 having some evidence m1 in common, the shared evidence is not countedtwice.The following proposition is linked to the notion of LBS introduced in Section 2.2.4. It will be useful to explainsome additional properties of the cautious rule.Proposition 6. Let m1 and m2 be two nondogmatic BBAs with conjunctive weight functions w1 and w2. Let (mcand (mccorresponding weights. Then the LBS (mc2 ) denote the LBSs associated to m1 and m2, respectively, and let (wc1, wd1 ∧(cid:14)2) associated to m1 ∧(cid:14) m2 is defined by1 ) and (wc1, md1 )2 ) denote the1 ∧(cid:14)2, md2, md2, wdmc1 ∧(cid:14)2md1 ∧(cid:14)2= ∩(cid:14)A⊆Ω= ∩(cid:14)A⊆ΩAwc1∧wc2 ,Awd1∨wd2 ,where ∨ denotes the maximum operation.Proof. For any A ⊂ Ω, we havewc1 ∧(cid:14)2(A) = 1 ∧ w1 ∧(cid:14)2(A) = 1 ∧ w1(A) ∧ w2(A)(cid:6)(cid:5)∧1 ∧ w1(A)1(A) ∧ wc(cid:6)(cid:5)1 ∧ w2(A)== wc2(A),andwd1 ∧(cid:14)2(A) = 1 ∧= 1 ∧1w1 ∧(cid:14)2(A)(cid:17)1w1(A)= 1 ∧∨ 1w2(A)1w1(A) ∧ w2(A)(cid:17)(cid:18)1 ∧ 1=w1(A)(cid:18)(cid:17)∨(cid:18)1 ∧ 1w2(A)= wd1 (A) ∨ wd2 (A).(cid:2)248T. Denœux / Artificial Intelligence 172 (2008) 234–264We thus see that, using the cautious rule, the confidence parts are combined conjunctively, whereas the diffidenceparts are combined disjunctively by taking the maximum of the two weight functions wd2 . Note that such adisjunctive combination is well defined only for u-separable BBAs (see Section 4 for further discussion on this issueand the definition of a disjunctive counterpart of the cautious rule). Combining the diffidence components disjunctivelydoes seem to make sense, as shown by the following informal argument. According to Smets [32], md (A) should beinterpreted as the strength of evidence that one should not believe A. If I receive two pieces of evidence, one ofwhich tells me not to believe A while the other tells me not to believe B, then I am inclined not to believe A ∪ B,hence the disjunctive nature of the combination. Consequently, there seems to be some form of duality between theconfidence and diffidence components of a LBS, which translates into different mechanisms for combining each ofthe two components.1 and wdAs is well known, the vacuous BBA is the neutral element for the TBM conjunctive rule, whereas it is an absorbingelement for the TBM disjunctive rule. As a consequence of the dual conjunctive/disjunctive nature of the cautiousrule, cautious combination of a BBA m with the vacuous BBA has the effect of absorbing the diffidence component,while leaving the confidence component unchanged. This is formalized in the following proposition.Proposition 7. For any nondogmatic BBA m with corresponding LBS (mc, md ):m ∧(cid:14) mΩ = mc.Proof. This is a direct consequence of Proposition 6. Let wc and wd denote, respectively, the weight functions of mcand md . The weights associated to the confidence component of m ∧(cid:14) mΩ are wc(A) ∧ 1 = wc(A) for all A ⊂ Ω,whereas those associated to the diffidence component are wd (A) ∨ 1 = 1 for all A ⊂ Ω. (cid:2)The following proposition follows directly from the previous one.Proposition 8. For any nondogmatic BBA m, mΩ ∧(cid:14) m = m iff m is u-separable.Proof. m is u-separable iff it is equal to its confidence component, i.e. m = mc, hence the result. (cid:2)Proposition 8 implies that, when combining a BBA m with the vacuous BBA using the cautious rule, one doesnot in general recover m but only a u-separable approximation in the form of its confidence component. This is aconsequence of Proposition 6, which shows that, for non-u-separable BBAs, the cautious rule is not purely conjunctiveas it combines the diffidence components in a disjunctive manner. Furthermore, it is easy to see that the cautiousconjunctive rule has no neutral element, since the only BBA m0 such that m ∧(cid:14) m0 = m for any u-separable BBA mis the vacuous BBA, and this property is not satisfied for non-u-separable BBAs.Note that, in practice, the cautious rule will often behave in a purely conjunctive fashion because most belieffunctions encountered in applications are u-separable. This is the case, in particular, for belief functions elicited fromexperts, which are often obtained by discounting logical propositions. This is also the case for consonant BBAs, asshown in Section 2.2.2, and for belief functions inferred using the General Bayesian Theorem [2,31] (see Section 6) orthe evidential case-based reasoning approach [4,5], two widely used mechanisms for reasoning with belief functionsin diagnosis and classification applications [6].To conclude this description of the main properties of the cautious rule, it is important to mention some implicationsof selecting the (cid:18)w ordering in its definition. As a consequence of (46), we have, for any bbas m1 and m2;Sw(m1) ∩ Sw(m2) ⊆ Sd (m1) ∩ Sd (m2) ⊆ Ss(m1) ∩ Ss(m2) ⊆ Spl(m1) ∩ Spl(m2)andSw(m1) ∩ Sw(m2) ⊆ Sd (m1) ∩ Sd (m2) ⊆ Ss(m1) ∩ Ss(m2) ⊆ Sq (m1) ∩ Sq (m2),with the subset relations being usually strict. Choosing the combined bba in Sw(m1) ∩ Sw(m2), as done by the cautiousrule, then comes down to choosing the smallest set of possible combined bbas in the above relations. In that set, thecautious rule selects the w-least committed element, which exists and is unique, as stated by Proposition 4. In thatsense, it may be termed “cautious” as it is derived from the LCP. However, it must be kept in mind that the choiceT. Denœux / Artificial Intelligence 172 (2008) 234–264249of Sw(m1) ∩ Sw(m2) imposes severe restrictions on the combination. As a consequence, bbas x-less committed thanm1 ∧(cid:14) m2 (with x ∈ {w, d, s, pl, q}) may exist outside Sw(m1) ∩ Sw(m2). In particular, when m1 or m2 is not u-separable, m1 ∩(cid:14) m2 does not belong to Sw(m1) ∩ Sw(m2), and it is possible to have m1 ∧(cid:14) m2 (cid:18)w m1 ∩(cid:14) m2, as shownby the following example.1(cid:7)m1(A) =Example 3. Let us consider the following bbas on Ω = {a, b, c, d, e}:0.4 if A = {a, b} or A = {b, c},0.2 if A = Ω,otherwise,00.4 if A = {b, d} or A = {b, e},0.2 if A = Ω,otherwise.0m2(A) =(cid:7)The corresponding weight functions are(cid:7)w1(A) =(cid:7)w2(A) =1/31.811/31.81if A = {a, b} or A = {b, c},if A = {b},otherwise,if A = {b, d} or A = {b, e},if A = {b},otherwise.It can easily be checked that w1(A) ∧ w2(A) (cid:2) w1(A)w2(A) for all A ⊂ Ω and, consequently, m1 ∧(cid:14) m2 (cid:18)w m1 ∩(cid:14) m2.As illustrated by the previous example, the cautious rule is not more “cautious” than the TBM conjunctive rulewhen applied to non-u-separable bbas, even in the sense of the (cid:18)w ordering. As will be shown in Section 5, thesetwo rules actually belong to two different families of rules with distinct algebraic properties, and as such they cannoteasily be compared.The strong constraint imposed to the cautious rule seems to be the price to pay for its ease of calculation and goodproperties (associativity, idempotence, distributivity of ∩(cid:14) with respect to ∧(cid:14)) as outlined above. It would, of course, bepossible to consider a larger set Sx(m1) ∩ Sx(m2), with x ∈ {d, s, pl, q} as the search space for the combination of twobbas m1 and m2. However, the existence and unicity of a x-least committed element would then no longer be verifiedin general, making it impossible to apply the LCP. One could also consider selecting a combined bba maximizingan uncertainty measure (see, e.g., [24]). Finding both a search space and an uncertainty measure ensuring interestingproperties of the combination seems to be a very difficult problem which is left for further research.3.3. The normalized cautious ruleA normalized version of the cautious rule introduced in the previous section may be defined by replacing theconjunctive rule ∩(cid:14) by Dempster’s rule ⊕ in (48). Denoting this rule by ∧(cid:14)∗, we have:Aw1(A)∧w2(A).m1 ∧(cid:14)∗2 = m1 ∧(cid:14)∗m2 =(cid:8)(49)We thus have∅(cid:6)=A⊂Ωm1 ∧(cid:14)∗2(A) = k · m1 ∧(cid:14)2(A),∀A ⊆ Ω, A (cid:6)= ∅,with k = (1 − m1 ∧(cid:14)2(∅))−1, and m1 ∧(cid:14)∗2(∅) = 0. Note that we can never have m1 ∧(cid:14)2(∅) = 1, because the cautiouscombination of two nondogmatic BBAs can never be dogmatic. As shown in Section 2.2, the weight functions ofm1 ∧(cid:14)∗ m2 and m1 ∧(cid:14) m2 differ only by the weight assigned to the empty set: with obvious notations, we havew1 ∧(cid:14)∗2(A) = w1 ∧(cid:14)2(A) = w1(A) ∧ w2(A),∀A ∈ 2Ω \ {∅, Ω},1 This example was suggested to the author by Frédéric Pichon.250T. Denœux / Artificial Intelligence 172 (2008) 234–264Table 3Combination of two BBAs using the normalized cautious ruleA∅{a}{b}{a, b}{c}{a, c}{b, c}Ωm1(A)0000.3000.50.2w1(A)117/42/5112/7m2(A)00.3000.4000.3w2(A)7/50.5113/711w1 ∧(cid:14)2(A)10.512/53/712/7m1 ∧(cid:14)2(A)0.610.0610.0920.0370.1100.0610.025m1 ∧(cid:14)∗2(A)00.160.240.0940.2900.160.063and w1 ∧(cid:14)∗2(∅) = k · w1 ∧(cid:14)2(∅). Clearly, this rule has the same properties as its unnormalized counterpart: it is commu-tative, associative and idempotent. When combining several BBAs m1, . . . , mn using the normalized cautious rule, wemay either compute their unnormalized cautious combination and normalize the end result, or normalize intermediateresults in the computation. The same property is known to hold for Dempster’s rule [28].The normalized cautious rule ∧(cid:14)∗ can be justified using the LCP with a suitable ordering relation. Let (cid:18)∗w denotew m2 iff w1(A) (cid:2) w2(A), for all A ⊂ Ω, A (cid:6)= ∅.the following partial order between nondogmatic BBAS: m1 (cid:18)∗Obviously, we havem1 (cid:18)w m2 ⇒ m1 (cid:18)∗w m2,for all m1 and m2, and the implication is strict. When the condition m1 (cid:18)∗w m2 holds, we will say that m1 is w∗-morecommitted than m2. Using the same line of reasoning as in Section 3.1, it is easy to show that m1 ∧(cid:14)∗ m2 is the w∗-leastcommitted BBA among all normal BBAs which are w∗-more committed than m1 and m2.The following proposition is a counterpart to Proposition 8:Proposition 9. For any nondogmatic normal BBA m, mΩ ∧(cid:14)∗ m = m iff m is n-separable.Proof. Let m be a nondogmatic normal BBA and w is weight function. Then w(A) ∧ 1 = w(A) for A ⊂ Ω, A (cid:6)= ∅implies that w(A) (cid:2) 1 for A ⊂ Ω, A (cid:6)= ∅, i.e., m is n-separable. The converse is obvious. (cid:2)Example 4. Table 3 shows intermediate steps for the computation of the normalized cautious combination of twoBBAs m1 and m2. Their weight functions are first computed and combined using the minimum operator. The corre-sponding BBA is then computed, and normalized. It can be checked that the weight function w1 ∧(cid:14)∗2 is equal to w1 ∧(cid:14)2,except for w1 ∧(cid:14)∗2(∅) = 1/(1 − m1 ∧(cid:14)2(∅)) = 2.57.Note that we have, in Example 4, m1 ∧(cid:14)2(∅) = 0.61, whereas it can be checked that m1 ∩(cid:14)2(∅) = 0.27. This illustratesthe fact that the cautious rule is not related to conflict minimization, contrary to, e.g., the rule proposed in [1].4. The bold disjunctive ruleJust as the cautious rule extends set intersection, as shown in Section 3.1, one may wonder whether the sameprinciple could be used to derive a disjunctive operator generalizing set union. Just as the union of two sets A and B isthe smallest set containing both A and B, we could attempt to define the disjunction of two BBAs m1 and m2 issuedfrom two sources as the most x-committed BBA, among the set of BBAs less x-committed than m1 and m2. Thisapproach seems appropriate when it is only known that at least one of the two sources is reliable, but we do not knowwhich one. The combined BBA should then be less informative than each of the BBAs provided by the individualsources.Formally, let us denote as Gx(m) the set of BBAs m(cid:21) such that m (cid:18)x m(cid:21). If the set Gx(m1) ∩ Gx(m2) possesses amost x-committed element, then this element could, by definition, be equated to the disjunction of m1 and m2. Sincethis BBA would be the most committed one, among those which are less informative than m1 and m2, such a rulecould be named a bold disjunctive rule.T. Denœux / Artificial Intelligence 172 (2008) 234–264251If we adopt w-ordering as the definition of inclusion, then Gw(m1) is the set of BBAs m such that w(A) (cid:3) w1(A)for all A ⊂ Ω. Similarly, Gw(m2) contains the BBAs m such that w(A) (cid:3) w2(A) for all A ⊂ Ω. The intersectionGw(m1) ∩ Gw(m2) thus contains the set of BBAs for which w(A) (cid:3) w1(A) ∨ w2(A) for all A ⊂ Ω, where ∨ denotesthe maximum operator. The most w-committed element in that set, if it exists, has the weight function w1 ∨ w2. Thisapproach is valid in the case where m1 and m2 are both separable BBA: in that case, we still have w1(A) ∨ w2(A) (cid:2) 1for all A ⊂ Ω, and w1 ∨ w2 defines a separable belief function [19]. However, this rule cannot be used to combinearbitrary nondogmatic BBAs, because w1 ∨ w2 does not always correspond to a belief function, as shown by thefollowing counterexample.Example 5. Consider the two BBAs m1 and m2 of Example 2, and let w = w1 ∨ w2 be the weight function obtainedby taking the maximum of the weight functions of m1 and m2. We have w({b}) = 7/4 ∨ 0.7 = 7/4, w({b, c}) =2/7∨3/7 = 3/7, and w(A) = 1 for all other A ⊂ Ω. The corresponding mass function is thus the TBM combination ofISBBA {b}7/4 and SBBA {b, c}3/7. We get m({b}) = −3/4, m({b, c}) = 7/4 × 4/7 = 1, and m(Ω) = 7/4 × 3/7 = 3/4,which does not correspond to a belief function.In the rest of this section, we will show that the above approach does allow to define a disjunctive counterpart of thecautious rule, provided it is based on a proper informational ordering of belief functions. To define such an ordering,we will need to introduce a canonical disjunctive representation of belief functions, dual to the “conjunctive” oneintroduced in [32] and recalled in Section 2.2. This representation is presented in the following section.4.1. Canonical disjunctive decomposition of a subnormal BBALet m be a subnormal BBA. Its complement m is nondogmatic and can be decomposed asm = ∩(cid:14)A⊂ΩAw(A).Consequently, m can be writtenm = ∩(cid:14)A⊂ΩAw(A) = ∪(cid:14)A⊂ΩAw(A).We recall that Aw(A) denotes the GSBBA assigning a mass w(A) > 0 to Ω and a mass 1−w(A) to A. Consequently, itscomplement Aw(A) is a generalized BBA with two focal sets: A with a mass 1 − w(A), and ∅ with a mass w(A). Sucha mapping can be called a negative GSBBA, as it is the negation of a GSBBA, and noted Av(A), with v(A) = w(A).We can thus writem = ∪(cid:14)A⊂Ω= ∪(cid:14)A(cid:6)=∅Av(A)Av(A).(51)(50)We have proved the following proposition:Proposition 10 (Canonical disjunctive decomposition). Any subnormal BBA m can be uniquely decomposed as the∪(cid:14)-combination of negative generalized BBAs Av(A) assigning a mass v(A) > 0 to ∅, and a mass 1 − v(A) to A, forall A ⊆ Ω, A (cid:6)= ∅:m = ∪(cid:14)A(cid:6)=∅Av(A).(52)Function v : 2Ω \ {∅} → (0, +∞) will be referred to as the disjunctive weight function. It is related to the conjunctiveweight function w associated to the negation m of m by the equationv(A) = w(A),∀A (cid:6)= ∅.(53)A comparison between Eqs. (8) and (53) shows that the relation between v and w parallels that between b and q.As a consequence, v can be obtained from b using a formula similar to (28), as expressed in the following proposition.252T. Denœux / Artificial Intelligence 172 (2008) 234–264Table 4Computation of disjunctive weightsA∅{a}{b}{a, b}{c}{a, c}{b, c}Ωm(A)m(A)0.1000.3000.6000.6000.3000.1w(A)2.80.1429110.2511v(A)110.25110.14292.8Proposition 11. Let v and b the disjunctive weight and implicability functions associated to a subnormal BBA m.They are related by the following equation:(cid:4)ln v(A) = −(−1)|A|−|B|ln b(B).(54)B⊆AProof. Using (53) and (28), we haveln v(A) = ln w(A)= −(cid:4)(−1)|B|−|A|ln q(B).B⊇ANow, (8) implies that q(B) = b(B). Observing that B ⊇ A ⇔ B ⊆ A, and |B| − |A| = |A| − |B|, we get the desiredresult. (cid:2)Note that relation (54) between ln v and − ln b has the same form as relation (7) between m and b. Consequently,any procedure for transforming b to m can be used to compute ln v from − ln b.Example 6. Table 4 illustrates the computation of disjunctive weights using (53).Just as the TBM conjunctive rule can be easily computed using conjunctive weights using (35), the TBM disjunctiverule has a simple expression in terms of the disjunctive weights, as shown by the following proposition.Proposition 12. Let m1 and m2 be two subnormal BBAs with disjunctive weight functions v1 and v2. The disjunctiveweight function v1 ∪(cid:14)2 associated to m1 ∪(cid:14) m2 is given by v1 ∪(cid:14)2 = v1v2.Proof. It is easy to check that we have Av ∪(cid:14) Av(cid:21) = Avv(cid:21) . Consequently, we have:m1 ∪(cid:14) m2 =Av1(A)∪(cid:14)(cid:19)∪(cid:14)A(cid:6)=∅= ∪(cid:14)A(cid:6)=∅(cid:20)(cid:19)(cid:20)Av2(A)∪(cid:14)A(cid:6)=∅(cid:2)Av1(A)v2(A).(55)(56)It follows directly that the inverse (cid:6)∪(cid:14) of the TBM disjunctive rule also has a simple expression in the v-space, asv1 (cid:6)∪(cid:14)2 = v1/v2.Finally, the concept of latent belief structure recalled in Section 2.2.4 also has a disjunctive counterpart. For eachdisjunctive weight v(A) let us define the following two quantities:andvc(A) = 1 ∧ v(A),vd (A) = 1 ∧ 1v(A).(57)(58)253(59)(60)(61)(62)T. Denœux / Artificial Intelligence 172 (2008) 234–264It is clear that we havev(A) = vc(A)vd (A).Consequently, we can writem = ∪(cid:14)A⊂Ω(cid:19)∪(cid:14)A⊂Ω=Avc(A)/vd (A)(cid:20)Avc(A)(cid:6)∪(cid:14)(cid:19)∪(cid:14)A⊂Ω(cid:20)Avd (A)(cid:6)∪(cid:14) mddis.= mcdisdis, mdThe pair (mcLBS associated to m.dis) is the disjunctive counterpart of the LBS introduced in 2.2.4, and can be named the disjunctive4.2. Informational ordering based on disjunctive weightsThe concept of disjunctive weight function defined above makes it possible to define a new partial ordering relationbetween belief functions, which is the counterpart of (cid:18)w introduced in Section 2.3.Let m1 and m2 be two subnormal BBAs with disjunctive weight functions v1 and v2. Assume that v1(A) (cid:3) v2(A),for all A (cid:6)= ∅. Let v = v2/v1, and m the corresponding BBA (it corresponds to a belief function, since v(A) (cid:2) 1 forall A (cid:6)= ∅). We thus have m2 = m1 ∪(cid:14) m, which implies that m1 is a specialization of m2. In that sense, m1 is moreinformative than m2. Consequently, the following new informational ordering can be introduced:m1 (cid:18)v m2 ⇔ v1(A) (cid:3) v2(A),∀A (cid:6)= ∅.If m1 (cid:18)v m2, we will say that m1 is v-more committed than m2.Just as (cid:18)v is a counterpart of (cid:18)w as a result of the duality between the conjunctive and disjunctive decompositions,we may observe that it is also possible to define a dual to the d-ordering (Dempsterian specialization) recalled inSection 2.3. Assume that m2 = m1 ∪(cid:14) m for some arbitrary bba m. Then m1 is a particular kind of specialization ofm2, which we can write as: m1 (cid:18)dd m2. This new ordering is obviously stronger than (cid:18)s , but weaker than (cid:18)v. Thetwo new ordering relations (cid:18)v and (cid:18)dd allow us to complete the picture drawn in Section 2.3 as follows:(cid:3)(cid:21)m1 (cid:18)w m2 ⇒ m1 (cid:18)d m2m1 (cid:18)v m2 ⇒ m1 (cid:18)dd m2⇒ m1 (cid:18)s m2 ⇒m1 (cid:18)pl m2,m1 (cid:18)q m2,where again all implications are strict.4.3. Derivation of the bold disjunctive ruleUsing the general approach outlined at the beginning of this section, we can define a disjunctive rule based on the(cid:18)v ordering, as shown by the following proposition.Proposition 13. Let m1 and m2 be two subnormal BBAs. The v-most committed element in Gv(m1) ∩ Gv(m2) existsand is unique. It is defined by the following disjunctive weight function:v1 ∨(cid:14)2(A) = v1(A) ∧ v2(A),∀A ∈ 2Ω \ ∅.(63)Proof. The proof is similar to that of Proposition 4. For any m ∈ Gv(m1) ∩ Gv(m2), we have v(A) (cid:2) v1(A) andv(A) (cid:2) v2(A), hence v(A) (cid:2) v1(A) ∧ v2(A) for all nonempty subset A of Ω. The v-most committed element inGv(m1) ∩ Gv(m2) is obtained by taking the minimum of v1(A) and v2(A) for all A. One can verify that it correspondsto a belief function, as a consequence of a counterpart of Lemma 1 for disjunctive weights. (cid:2)Eq. (63) introduces a new rule which can be formally defined as follows.254T. Denœux / Artificial Intelligence 172 (2008) 234–264Table 5Combination of two BBAs using the bold disjunctive ruleA∅{a}{b}{a, b}{c}{a, c}{b, c}Ωm1(A)0.1000.3000.60v1(A)1111/4111/714/5m2(A)0.100.50000.40v2(A)111/61110.61v1 ∨(cid:14)2(A)111/61/4111/71m1 ∨(cid:14)2(A)0.006000.02980.1071000.21430.6429m1 ∪(cid:14)2(A)0.0100.050.18000.640.12Definition 2 (Bold disjunctive rule). Let m1 and m2 be two subnormal BBAs. Their combination using the bolddisjunctive rule is noted m1 ∨(cid:14)2 = m1 ∨(cid:14) m2. It is defined as the BBA with the following disjunctive weight function:v1 ∨(cid:14)2(A) = v1(A) ∧ v2(A), A ∈ 2Ω \ ∅.We thus havem1 ∨(cid:14) m2 = ∪(cid:14)A(cid:6)=∅Av1(A)∧v2(A).(64)Example 7. Table 5 shows two subnormal BBAs, together with their v-representation and their bold disjunction. Theresulting BBA m1 ∨(cid:14)2 may be computed from v1 ∨(cid:14)2 by combining the three BBAs {b}1/6, {a, b}1/4 and {b, c}1/7 usingthe disjunctive rule ∪(cid:14). The result of the combination using the TBM disjunctive rule is shown in the last column forcomparison.The fact that the bold disjunctive rule is only applicable to subnormal BBAs may appear as a severe restriction, asmost belief functions encountered in practice are normal (and subnormality often arises when combining conflictingitems of evidence using the TBM conjunctive rule). However, practically, it is always possible to “denormalize” aBBA by transferring a very small proportion of the unit mass of belief to the empty set. In the TBM, the mass m(∅)may be interpreted as being committed to the hypothesis than none of the elementary hypotheses in the frame ofdiscernment is true. Assigning even an infinitesimal mass to this hypothesis may be justified in most cases, as a wayto acknowledge the fact that the adopted model might not be complete, unlikely as it may be.4.4. Properties of the bold disjunctive ruleThe bold disjunctive rule has properties which parallel those of the cautious conjunctive rule, due to the dual natureof these two rules. These properties are listed below.Proposition 14. The bold disjunctive rule has the following properties:Commutativity: for all m1 and m2, m1 ∨(cid:14) m2 = m2 ∨(cid:14) m1;Associativity: for all m1, m2 and m3, m1 ∨(cid:14) (m2 ∨(cid:14) m3) = (m1 ∨(cid:14) m2) ∨(cid:14) m3;Idempotence: for all m, m ∨(cid:14) m = m;Distributivity of ∪(cid:14) with respect to ∨(cid:14): for all m1, m2 and m3,m1 ∪(cid:14) (m2 ∨(cid:14) m3) = (m1 ∪(cid:14) m2) ∨(cid:14) (m1 ∪(cid:14) m3).Proof. The proof is similar to that of Proposition 18. (cid:2)The following proposition is the counterpart of Proposition 6, and can be derived in the same manner. It shows thatthe bold disjunctive rule treats differently the two components of disjunctive LBSs.T. Denœux / Artificial Intelligence 172 (2008) 234–264255Proposition 15. Let m1 and m2 be two subnorm BBAs with disjunctive weight functions v1 and v2. Let (mcand (mcdenote the corresponding disjunctive weights. Then the disjunctive LBS (mcis defined bydis,2) denote the disjunctive LBSs associated to m1 and m2, respectively, and let (vcdis,1)2, vd1, vd2 )dis,1 ∨(cid:14)2) associated to m1 ∨(cid:14) m2dis,1, md1 ) and (vcdis,1 ∨(cid:14)2, mddis,2, mdmcdis,1 ∨(cid:14)2mddis,1 ∨(cid:14)2= ∪(cid:14)A⊆Ω= ∪(cid:14)A⊆ΩAvc1∧vc2,Avd1∨vd2.Finally, the following proposition shows that the ∨(cid:14) and ∧(cid:14) operations are dual to each other with respect tocomplementation, i.e., they are linked by De Morgan laws analogous to (16) and (17).Proposition 16 (De Morgan’s laws). Let m1 and m2 be two subnormal BBAs. We have:m1 ∨(cid:14) m2 = m1 ∧(cid:14) m2,for all subnormal BBAs m1 and m2, andm1 ∧(cid:14) m2 = m1 ∨(cid:14) m2for all nondogmatic BBAs m1 and m2.Proof. Let m1 and m2 be two subnormal BBAs. We havem1 ∨(cid:14) m2 ==(cid:22)A(cid:6)=∅(cid:23)A(cid:6)=∅Av1(A)∧v2(A) =(cid:23)Av1(A)∧v2(A)A(cid:6)=∅(cid:23)Aw1(A)∧w2(A) =Aw1(A)∧w2(A)A⊂Ω= m1 ∧(cid:14) m2.The proof of (66) is similar. (cid:2)(65)(66)5. General combination rules based on triangular norms and conormsAs we have seen, the cautious and TBM conjunctive rules are based on pointwise combination of conjunctiveweights (using, respectively, the minimum and the product), whereas the bold and TBM disjunctive rule are basedon similar combination of disjunctive weights. One may wonder whether such operations on weights could be gen-eralized to define other combination rules with interesting properties. To simplify the discussion, only operations onconjunctive weights will be considered here; extension to disjunctive weights is obvious.Let m1 and m2 be two nondogmatic BBAs with conjunctive weight functions w1 and w2. We have seen that eachweight w(A) may be decomposed into two components in [0, 1]: a confidence component wc(A) = 1 ∧ w(A) and adiffidence component wd (A) = 1 ∧ (w(A))−1, with w(A) = wc(A)/wd (A).Both the TBM conjunctive rule and the cautious rule can be described in terms of operations on conjunctive anddisjunctive weights:• The TBM conjunctive rule combines the confidence and diffidence components of the weights using the product:1 ∩(cid:14)2(A) = wcwc1 ∩(cid:14)2(A) = wdwd1(A) · wc1 (A) · wd2(A),2 (A);(67)(68)• The cautious rule combines the confidence components using the minimum, and the diffidence components usingthe maximum:256T. Denœux / Artificial Intelligence 172 (2008) 234–2641 ∧(cid:14)2(A) = wcwc1 ∧(cid:14)2(A) = wdwd1(A) ∧ wc1 (A) ∨ wd2(A),2 (A).(69)(70)Can these operations be generalized? To answer this question, we may observe that, in the interval [0, 1], theproduct and the minimum are triangular norms (t-norms for short), whereas the maximum is a triangular conorm(or t-conorm) [23]. We recall that a t-norm is a commutative and associative binary operator (cid:22) on the unit intervalsatisfying the monotonicity propertyy (cid:2) z ⇒ x(cid:22)y (cid:2) x(cid:22)z,∀x, y, z ∈ [0, 1],and the boundary condition x(cid:22)1 = x, ∀x ∈ [0, 1]. A t-conorm ⊥ has the same three basic properties (commutativity,associativity, monotonicity) and differs only by the boundary condition x⊥0 = x. Because of their different boundaryconditions, t-norms and t-conorms are usually interpreted, respectively, as generalized conjunction and disjunctionoperators in fuzzy logic.A first consequence of this observation is that the TBM conjunctive rule and the cautious rule combine the dif-fidence weights using operations with different algebraic properties and, in that respect, they should be regarded asbelonging to different families of combination rules. However, it does seem possible to define new combination ruleswith interesting properties by generalizing the cautious rule and the TBM conjunctive rule separately. This is done inthe rest of this section, with emphasis on the generalization of the cautious rule, which is the main topic of this paper.5.1. Generalized cautious rulesThe following proposition shows that new rules for combining nondogmatic belief functions can be defined by re-placing the minimum and the maximum in (69) and (70) by, respectively, a positive t-norm and a t-conorm. (A t-norm(cid:22) is said to be positive iff x > 0 and y > 0 implies x(cid:22)y > 0.)2, wd1 ) and2 ) their decompositions into confidence and diffidence components. Let w1∗2 be the mapping from 2Ω \ Ω toProposition 17. Let m1 and m2 be two nondogmatic BBAs, w1 and w2 their weight functions, and (wc(wc(0, +∞) defined as:1(A)(cid:22)wcwc1 (A)⊥wdwdw1∗2(A) =2(A)2 (A)∀A ⊂ Ω,1, wd(71),where (cid:22) is a positive t-norm, and ⊥ a t-cornorm. Then:• Function w1∗2 is the conjunctive weight function of a nondogmatic BBA m1∗2;• We have m1∗2 (cid:18)w m1 ∧(cid:14) m2.Proof. It is known [23] that the minimum is the largest t-norm, while the maximum is the weakest t-conorm. Con-2 (A), hence w1∗2(A) (cid:2)sequently, we have wc2(A) and wdw1(A) ∧ w2(A) for all A. Using Lemma 1, this proves that w1∗2 corresponds to a belief function. It is obviouslyw-more committed than m1 ∧(cid:14) m2. Additionally, positivity of (cid:22) ensures w1∗2(A) > 0 and, consequently, that m1∗2 isnondogmatic. (cid:2)2 (A) (cid:3) wd1 (A) ∨ wd2(A) (cid:2) wc1(A) ∧ wc1 (A)⊥wd1(A)(cid:22)wcNote that each combined weight w1∗2(A) can be expressed directly as a function of w1(A) and w2(A) as w1∗2(A) =w1(A) ∗(cid:22),⊥ w2(A), where ∗(cid:22),⊥ is the following operator in (0, +∞):⎧⎨x ∗(cid:22),⊥ y =⎩x(cid:22)yx ∧ y(cid:5)⊥ 11yx(cid:6)−1if x ∨ y (cid:2) 1,if x ∨ y > 1 and x ∧ y (cid:2) 1,otherwise,(72)for all x, y > 0.operator (cid:4)(cid:22),⊥ asGiven a positive t-norm (cid:22) and a t-cornorm ⊥, Proposition 17 allows us to define a belief function combinationm1 (cid:4)(cid:22),⊥ m2 = ∩(cid:14)A⊂ΩAw1(A)∗(cid:22),⊥w2(A),T. Denœux / Artificial Intelligence 172 (2008) 234–264257where ∗(cid:22),⊥ is defined by (72). Note that the cautious rule corresponds to (cid:4)∧,∨.The (cid:4)(cid:22),⊥ operator has some interesting properties. We start with the following lemma.Lemma 2. For any positive t-norm (cid:22) and any t-conorm ⊥, the operator ∗(cid:22),⊥ defined by (72) is commutative, asso-ciative, and satisfies the monotonicity propertyy (cid:2) z ⇒ x ∗ y (cid:2) x ∗ z,∀x, y, z > 0.Proof. Commutativity results directly from the commutativity of (cid:22), ∧ and ⊥. For associativity, we may considerdifferent cases:• If x (cid:2) 1, y (cid:2) 1 and z (cid:2) 1, then(x ∗(cid:22),⊥ y) ∗(cid:22),⊥ z = (x(cid:22)y)(cid:22)z = x(cid:22)(y(cid:22)z) = x ∗(cid:22),⊥ (y ∗(cid:22),⊥ z);• If x > 1, y > 1 and z > 1, then(cid:17)(cid:24)(cid:17)(x ∗(cid:22),⊥ y) ∗(cid:22),⊥ z =(cid:18)−1(cid:25)−1(cid:18)−1⊥ 11yx(cid:24)⊥ 1z(cid:17)(cid:25)(cid:18)−1=⊥1x1y= x ∗(cid:22),⊥ (y ∗(cid:22),⊥ z);⊥ 1z(cid:17)=(cid:25)⊥ 1y(cid:18)−1⊥ 1z(cid:25)−1(cid:17)(cid:24)=(cid:24)(cid:17)1y1x⊥ 1z(cid:18)−1(cid:18)−1⊥1x• If x (cid:2) 1, y (cid:2) 1 and z > 1, then(x ∗(cid:22),⊥ y) ∗(cid:22),⊥ z = (x(cid:22)y) ∧ z = x(cid:22)y,andx ∗(cid:22),⊥ (y ∗(cid:22),⊥ z) = x(cid:22)(y ∧ z) = x(cid:22)y;• If x > 1, y > 1 and z (cid:2) 1, then(x ∗(cid:22),⊥ y) ∗(cid:22),⊥ z = (x ∗(cid:22),⊥ y) ∧ z = z,andx ∗(cid:22),⊥ (y ∗(cid:22),⊥ z) = x ∧ (y ∧ z) = z.The other cases can be deduced from the above last two cases using the commutativity property. Finally, monotonicitycan be proved in a similar manner, by considering the different cases:Case 1: y (cid:2) z (cid:2) 1. Then:Case 2: y (cid:2) 1 < z. Then:• if x (cid:2) 1, then x ∗(cid:22),⊥ y = x(cid:22)y and x ∗(cid:22),⊥ z = x(cid:22)z, and x(cid:22)y (cid:2) x(cid:22)z by the monotonicity of (cid:22);• if x > 1, then x ∗(cid:22),⊥ y = y and x ∗(cid:22),⊥ z = z, and the result follows directly.• if x (cid:2) 1, then x ∗(cid:22),⊥ y = x(cid:22)y and x ∗(cid:22),⊥ z = x ∧ z. Now, x(cid:22)y (cid:2) x ∧ y (cid:2) x ∧ z, since (cid:22) is dominated by ∧;• if x > 1, then x ∗(cid:22),⊥ y = y and x ∗(cid:22),⊥ z = ((1/x)⊥(1/z))−1. Now, we have 1/x < 1/y and 1/z < 1/y, hence(1/x)⊥(1/z) < 1/y and ((1/x)⊥(1/z))−1 > y.Case 3: 1 < y (cid:2) z. Then:• if x (cid:2) 1, then x ∗(cid:22),⊥ y = x, x ∗(cid:22),⊥ z = x and the result follows directly;• if x > 1, then x ∗(cid:22),⊥ y = ((1/x)⊥(1/y))−1 and x ∗(cid:22),⊥ z = ((1/x)⊥(1/z))−1. Now, since 1/y (cid:3) 1/z, we have(1/x)⊥(1/y) (cid:3) (1/x)⊥(1/z) by the monotonicity of ⊥, hence ((1/x)⊥(1/y))−1 (cid:2) ((1/x)⊥(1/z))−1. (cid:2)Proposition 18. The (cid:4)(cid:22),⊥ rule has the following properties:Commutativity: for all m1 and m2, m1 (cid:4)(cid:22),⊥ m2 = m2 (cid:4)(cid:22),⊥ m1;258T. Denœux / Artificial Intelligence 172 (2008) 234–264Associativity: for all m1, m2 and m3,m1 (cid:4)(cid:22),⊥ (m2 (cid:4)(cid:22),⊥ m3) = (m1 (cid:4)(cid:22),⊥ m2) (cid:4)(cid:22),⊥ m3;Monotonicity with respect to (cid:18)w: for all m1, m2 and m3,m1 (cid:18)w m2 ⇒ m1 (cid:4)(cid:22),⊥ m3 (cid:18)w m2 (cid:4)(cid:22),⊥ m3.Proof. These properties follow directly from corresponding properties of ∗(cid:22),⊥ expressed in Lemma 2. (cid:2)5.2. DiscussionWe thus have defined a family of commutative and associative combination operators, which also have the propertyof monotonicity with respect to (cid:18)w. The latter property means that, if a BBA m1 is less informative than a BBA m2according to the (cid:18)w ordering, then this order is unchanged after combination with a third BBA. The cautious rule isthe only idempotent rule in this family, since the minimum and the maximum are, respectively, the only idempotentt-norm and t-conorm.We may remark that, normalized combination rules can be defined in the same way, by combining the weights ofnon empty subsets A of Ω, and normalizing the result. These normalized combination rules are also commutative,associative, and monotonic with respect to (cid:18)∗w.The same approach can also be used to generalize the TBM conjunctive rule, by replacing the product in (67)and (68) by two t-norms (cid:22)1 and (cid:22)2, respectively. However, some cautious should be exercised here, because theresulting combined weight function is not guaranteed to correspond to a belief function for any choice of (cid:22)1 and (cid:22)2.For instance, choosing (cid:22)1 = (cid:22)2 = ∧ does not yield a belief function in general. A sufficient condition for obtaininga belief function is to choose (cid:22)1 and (cid:22)2 such that (cid:22)1 (cid:2) Π (cid:2) (cid:22)2, where Π denotes the product t-norm. Deeperinvestigation of this topic is beyond the scope of this paper.One may object that these new combination rules, in spite of their interesting properties outlined in Proposition 18,are only weakly justified. However, we may remark that the same situation prevails in Possibility theory [10], wherethere are as many conjunctive and disjunctive operators as t-norms and t-conorms. Although this multiplicity ofoperators may be seen as a weakness of the axiomatic foundations of Possibility theory, it also proves beneficialfrom a practical point of view as it provides considerable flexibility to adjust the behavior of a system to user-definedrequirements [13] or to learning examples. In contrast, Dempster–Shafer theory has sometimes been criticized for itslack of flexibility in the choice of combination operator [8], a criticism which, in light of the new results presented inthis paper, appears to be unjustified.5.3. Combination rule optimizationAs shown in the previous section, a commutative and associative operator based on conjunctive (or disjunctive)weights can be associated to each pair of a t-norm and a t-conorm. By choosing parameterized families of t-normsand t-conorms [23], it is thus possible to defined parameterized families of belief function combination rules. Thisintroduces the possibility to learn a combination rule from examples, as shown in the following simple illustrativeexample.Example 8. Assume that the BBAs m1 and m2 shown in Table 6 have been provided by two sensors, and expertknowledge regarding the true value of the variable of interest is represented by BBA me also shown in Table 6. Forthis simple illustrative example, me was artificially constructed by combining m1 and m2 using the (cid:4)(cid:22),⊥ operatorbased on the Frank t-norm and dual t-conorm with parameter s = 0.5, and adding a small amount of random noise.We recall that the Frank family of t-norms [23] is defined by⎧⎨x(cid:22)sy =⎩x ∧ yxy(cid:5)1 + (sx −1)(sy −1)logss−1(cid:6)if s = 0,if s = 1,otherwise,for all x, y ∈ [0, 1], where s is a positive parameter. The dual t-conorm ⊥s is defined by x⊥sy = 1 − (1 − x)(cid:22)s(1 − y).T. Denœux / Artificial Intelligence 172 (2008) 234–264259Table 6BBAs of Example 8∅0.110.080.3965{a}0.100.190.1210{b}0.130.160.1446{a, b}0.010.050.0236{c}0.1000.0871{a, c}0.070.100.0498{b, c}0.190.110.0934{a, b, c}0.290.310.0841m1m2meFig. 1. Distance between the target BBA and the combined BBA, as a function of parameter s of the Frank family of t-norms.We wish to find a combination rule of the form (cid:4)(cid:22)s ,⊥s , where (cid:22)s and ⊥s are, respectively, the Frank t-norm andt-conorm with parameter s, such that the combination of m1 and m2 yields a BBA as close as possible to me. Note that,in real applications such as classifier fusion problems, a large number of such learning instances would typically beavailable. Parameter s was varied between 0 and 1, and for each s value the discrepancy between m12 = m1 (cid:4)(cid:22)s ,⊥s m2and me was measured by Jousselme’s distance [18] defined:(cid:26)d(m12, me) =12(m12 − me)t D(m12 − me),where m12 and me are 2|Ω|-dimensional vectors of basic belief masses corresponding to m12 and me, and D is thesquare matrix of size 2|Ω| defined by(cid:3)D(A, B) =1|A∩B||A∪B|if A = B = ∅,otherwise.Distance d is plotted as a function of s in Fig. 1. The best fit between the combined BBA m12 and the target BBAme is obtained for s ≈ 0.33, which is an estimate of the true value s = 0.5.6. Application to classifier fusionAlthough the cautious rule and its relatives have nice mathematical properties, their usefulness in applications ofbelief functions might be questioned. In this section, we present numerical experiments showing the efficiency of thecautious rule and a t-norm based generalization to combine classifiers built from dependent features.6.1. Problem statement and formalizationLet us consider a classification problem with K classes and d continuous features X1, . . . , Xd . Assume that eachfeature Xi has a known conditional probability distribution fk(xi) in each class ωk (k = 1, . . . , K). The class priorprobabilities are unknown. Additionally, nothing is known concerning the correlations between features.260T. Denœux / Artificial Intelligence 172 (2008) 234–264This problem can be tackled in the TBM using the General Bayesian Theorem (GBT) [2,6,31]. Assume that theknown probability density fk(xi) of feature Xi in ωk is interpreted as the pignistic probability of some unknownconditional belief function on R. For simplicity, fk(xi) will be assumed to be unimodal and symmetric. As shown in[36], the q-least committed belief function on R associated with a unimodal symmetric pignistic probability densityfk with mode νk is consonant (and, consequently, equivalent to a plausibility measure). The corresponding possibilitydistribution (called “contour function” by Shafer [28, p. 221]) is:plk(xi) =(cid:7)2(xi − νk)fk(xi) + 22(νk − xi)fk(xi) + 2fk(ti) dti(cid:27) +∞xi(cid:27)xi−∞ fk(ti) dtiif xi (cid:3) νk,otherwise.(73)The quantity plk(xi) is the plausibility that feature Xi takes value xi , given that the object belongs to class ωk.Assume that the value xi of feature Xi has been observed for a certain object. What is our belief state concerningthe class of this object? In the TBM, the answer is provided by the GBT. The induced BBA on the set of classesΩ = {ω1, . . . , ωK }, conditional on Xi = xi , is u-separable [6]. It is given by:mΩ [xi] =K∩(cid:14)k=1{ωk}plk(xi ).(74)If the features Xi are assumed to be conditionally independent given the class, then the evidence of the d featurevalues can be considered as distinct and, as such, can be combined by the TBM conjunctive rule:mΩ [x1, . . . , xd ] ==d∩(cid:14)i=1K∩(cid:14)k=1mΩ [xi](cid:14){ωk}di=1 plk(xi ).If conditional independence is not assumed, then the cautious rule may be more appropriate. We then have:mΩ [x1, . . . , xd ] ==d∧(cid:14)i=1K∩(cid:14)k=1mΩ [xi](cid:28){ωk}di=1 plk(xi ).(75)(76)(77)(78)Note that the possibility distributions plk(xi) are combined using the product t-norm in (76), whereas they are com-bined using the minimum t-norm in (78). Using a generalized cautious rule (cid:4)(cid:22),⊥ such as introduced in Section 5.1amounts to combining the plk(xi) using t-norm (cid:22). If (cid:22) is chosen in the Frank family, then the cautious rule is recov-ered for s = 0, whereas the TBM conjunctive rule is recovered for s = 1. Choosing s between 0 and 1 results in acombination rule somewhere between these two extremes.6.2. Experimental resultsNumerical simulations were performed for a particular instance of the above problem, with K = 2 classes andd = 10 features. The conditional distribution of feature vector (X1, . . . , Xd ) in class ωk was assumed to be multivariatenormal with mean μ1 = (0, . . . , 0) in class ω1 and μ2 = (1, . . . , 1) in class ω2, and with common variance matrixΣ =⎜⎜⎜⎜⎜⎜⎜⎝⎛1 ρρ1ρρ. . .ρ ρ......ρ ρ . . .. . .00⎞⎟⎟⎟⎟⎟⎟⎟⎠,. . . ρ 0. . . ρ 0.......... . . ρρ01. . .10with ρ ∈ [0, 1]. Conditionally on each class, the last feature X10 was thus assumed to be independent from all otherfeatures, whereas the correlation coefficient between any two features Xi and Xj , i, j ∈ {1, . . . , 9}, was equal to ρ.For any observed feature vector x = (x1, . . . , x10), a decision was computed as follows:T. Denœux / Artificial Intelligence 172 (2008) 234–264261Fig. 2. ROC curves for the TBM conjunctive, cautious and generalized cautious rules, for ρ = 0 (top), ρ = 0.5 (middle) and ρ = 0.9 (bottom).262T. Denœux / Artificial Intelligence 172 (2008) 234–264• The plausibilities plk(xi) were computed using (73), for each i and each k;• The BBAs mΩ [xi] on Ω given feature Xi were computed using (74);• The combined BBA mΩ [x1, . . . , x10] was computed using the conjunctive rule using (76), the cautious rule using(78), or the generalized cautious rule (cid:4)(cid:22),⊥ with (cid:22) equal to the Frank t-norm with s = 0.01;• The pignistic transformation (18) was applied to mΩ [x1, . . . , x10], and the pignistic probability of class ω1 wascompared to some threshold.The above procedure was repeated for n = 5000 test vectors in each class, for ρ = 0, ρ = 0.5 and ρ = 0.9. The falsepositive rate (proportion of test vectors from class ω1 wrongly classified as ω2) and the true positive rate (proportionof test vectors from class ω2 correctly classified as ω2) were estimated for each combination rule and each value ofρ. The corresponding ROC curves (plot of the true positive rate as a function of the false positive rate) are shown inFig. 2. In this representation, a higher curve corresponds to higher performance (higher true positive rate for any falsepositive rate).As expected, the TBM conjunctive rule achieves higher performance in the case of independent features. However,it is outperformed by the cautious rule when features are no longer independent. The generalized cautious rule with theFrank t-norm for s = 0.01 has intermediate performances in all three situations. This is an experimental verificationof the validity of the cautious rule in the case of nondistinct evidence.7. ConclusionTwo new commutative, associative and idempotent operators for belief functions have been introduced. The cau-tious conjunctive rule ∧(cid:14) has been derived from the Least Commitment Principle with a suitable informationalordering: the ∧(cid:14)-combination of two nondogmatic BBAs m1 and m2 has been defined as the least committed BBAaccording to the (cid:18)w ordering, among those which are more committed than m1 and m2, according to the same order-ing. Symmetrically, the combination of two subnormal BBAs m1 and m2 using the bold disjunctive rule ∨(cid:14) has beendefined as the most committed BBA according to (cid:18)v, an ordering dual to (cid:18)w, among BBAs that are less committedthan m1 and m2.Contrary to the TBM conjunctive and disjunctive rules ∩(cid:14) and ∪(cid:14), these two operators do not require the assumptionof independence, or distinctness of the information sources from which BBAs are derived. Independently from thisdistinctness assumption, conjunctive operators ∩(cid:14) and ∧(cid:14) are appropriate when all sources are believed to be reliable,whereas disjunctive operators should be used when one only assumes that at least one of the sources is reliable. Thechoice of one operator among ∩(cid:14), ∪(cid:14), ∧(cid:14) and ∨(cid:14) thus depends on assumptions regarding both the distinctness andreliability of the sources, as summarized in the following table:SourcesDistinctAll reliable∩(cid:14)At least one reliable∪(cid:14)Nondistinct∧(cid:14)∨(cid:14)The cautious and bold rules have also been shown to belong to infinite families of conjunctive and disjunctiveoperators based on t-norms and t-conorms. Using parametrized families of t-norms and t-conorms, correspondingfamilies of conjunctive and disjunctive operators can be defined. All these operators are commutative and associative,but only ∧(cid:14) and ∨(cid:14) are idempotent. Although these operators do not appear to be as well justified as the cautious andbold rules, they may be useful in classification or information fusion applications where the behavior of a combinationrule can be tuned to optimize a given performance measure [5,16,42]. In any case, it appears that, contrary to aso far widely accepted opinion [8], the richness of potential combination operators is not lower in the theory ofbelief functions than it is in possibility theory, which opens new perspectives for applying belief functions theory toinformation fusion problems.AcknowledgementThis work is dedicated to the memory of Professor Philippe Smets, who inspired most of the ideas expressed inthis paper. The author thanks the anonymous referees for their careful reading of the manuscript and their constructiveT. Denœux / Artificial Intelligence 172 (2008) 234–264263comments that helped to improve this paper. Thanks also to Frédéric Pichon for stimulating discussions on the cautiousrule and the canonical decompositions.References[1] M.E.G.V. Cattaneo, Combining belief functions issued from dependent sources, in: J.M. Bernard, T. Seidenfeld, M. Zaffalon (Eds.), Pro-ceedings of the Third International Symposium on Imprecise Probabilities and Their Applications (ISIPTA’03), Carleton Scientific, Lugano,Switzerland, 2003, pp. 133–147.[2] F. Delmotte, Ph. Smets, Target identification based on the transferable belief model interpretation of Dempster–Shafer model, IEEE Transac-tions on Systems, Man and Cybernetics A 34 (4) (2004) 457–471.[3] A.P. Dempster, Upper and lower probabilities induced by a multivalued mapping, Annals of Mathematical Statistics 38 (1967) 325–339.[4] T. Denœux, A k-nearest neighbor classification rule based on Dempster–Shafer theory, IEEE Transactions on Systems Man and Cybernet-ics 25 (05) (1995) 804–813.[5] T. Denœux, A neural network classifier based on Dempster–Shafer theory, IEEE Transactions on Systems, Man and Cybernetics A 30 (2)(2000) 131–150.[6] T. Denœux, P. Smets, Classification using belief functions: the relationship between the case-based and model-based approaches, IEEE Trans-actions on Systems, Man and Cybernetics B 36 (6) (2006) 1395–1406.[7] D. Dubois, H. Prade, On several representations of an uncertainty body of evidence, in: M.M. Gupta, E. Sanchez (Eds.), Fuzzy Informationand Decision Processes, North-Holland, New York, 1982, pp. 167–181.[8] D. Dubois, H. Prade, On the unicity of Dempster’s rule of combination, International Journal of Intelligent Systems 1 (1986) 133–142.[9] D. Dubois, H. Prade, A set-theoretic view of belief functions: logical operations and approximations by fuzzy sets, International Journal ofGeneral Systems 12 (3) (1986) 193–226.[10] D. Dubois, H. Prade, Possibility Theory: An Approach to Computerized Processing of Uncertainty, Plenum Press, New York, 1988.[11] D. Dubois, H. Prade, Representation and combination of uncertainty with belief functions and possibility measures, Computational Intelli-gence 4 (1988) 244–264.[12] D. Dubois, H. Prade, Ph. Smets, New semantics for quantitative possibility theory, in: S. Benferhat, Ph. Besnard (Eds.), Proc. of the6th European Conference on Symbolic and Quantitative Approaches to Reasoning and Uncertainty (ECSQARU 2001), Toulouse, France,Springer-Verlag, 2001, pp. 410–421.[13] D. Dubois, H. Prade, R. Yager, Merging fuzzy information, in: J.C. Bezdek, D. Dubois, H. Prade (Eds.), Fuzzy Sets in Approximate Reasoningand Information Systems, Kluwer Academic Publishers, Boston, 1999, pp. 335–401.[14] D. Dubois, R.R. Yager, Fuzzy set connectives as combination of belief structures, Information Sciences 66 (1992) 245–275.[15] Z. Elouedi, K. Mellouli, Pooling dependent expert opinions using the theory of evidence, in: Proc. of the Seventh Int. Conf. on InformationProcessing and Management of Uncertainty in Knowledge-Based Systems (IPMU 98), vol. 1, Paris, France, July 1998, pp. 32–39.[16] Z. Elouedi, K. Mellouli, Ph. Smets, Assessing sensor reliability for multisensor data fusion within the transferable belief model, IEEE Trans-actions on Systems, Man and Cybernetics B 34 (1) (2004) 782–787.[17] R. Haenni, Are alternatives to Dempster’s rule of combination real alternatives?: Comments on “about the belief function combination and theconflict management problem”—Lefèvre et al., Information Fusion 3 (3) (2002) 237–239.[18] A.-L. Jousselme, D. Grenier, E. Bossé, A new distance between two bodies of evidence, Information Fusion 2 (2) (2001) 91–101.[19] R. Kennes, Evidential reasoning in a categorical perspective: conjunction and disjunction of belief functions, in: B. D’Ambrosio, P. Smets,P.P. Bonissone (Eds.), Proceedings of the 7th Conference on Uncertainty in Artificial Intelligence, Morgan Kaufman, San Mateo, CA, 1991,pp. 174–181.[20] R. Kennes, Ph. Smets, Computational aspects of the Möbius transformation, in: P.P. Bonissone, M. Henrion, L.N. Kanal, J.F. Lemmer (Eds.),Proceedings of the 6th Conference on Uncertainty in Artificial Intelligence, Elsevier Science Publishers, 1990, pp. 401–416.[21] F. Klawonn, E. Schwecke, On the axiomatic justification of Dempster’s rule of combination, International Journal of Intelligent Systems 7(1992) 469–478.[22] F. Klawonn, Ph. Smets, The dynamic of belief in the transferable belief model and specialization–generalization matrices, in: D. Dubois, et al.(Eds.), Proc. of the 8th conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann, San Mateo, CA, 1992, pp. 130–137.[23] E.P. Klement, R. Mesiar, E. Pap, Triangular Norms, Kluwer Academic Publishers, Dordrecht, 2000.[24] G.J. Klir, M.J. Wierman, Uncertainty-Based Information. Elements of Generalized Information Theory, Springer-Verlag, New York, 1998.[25] E. Lefèvre, O. Colot, P. Vannoorenberghe, Belief function combination and conflict management, Information Fusion 3 (2) (2002) 149–162.[26] X.N. Ling, W.G. Rudd, Combining opinions from several experts, Applied Artificial Intelligence 3 (1989) 439–452.[27] K. Sentz, S. Ferson, Combination of evidence in Dempster–Shafer theory, Technical report, SANDIA National Laboratories, 2002.[28] G. Shafer, A Mathematical Theory of Evidence, Princeton University Press, Princeton, NJ, 1976.[29] Ph. Smets, Combining nondistinct evidences, in: Int. Conf. of the North American Fuzzy Information Processing Society (NAFIPS’86), NewOrleans, USA, 1986, pp. 544–549.[30] Ph. Smets, The combination of evidence in the transferable belief model, IEEE Transactions on Pattern Analysis and Machine Intelli-gence 12 (5) (1990) 447–458.[31] Ph. Smets, Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem, International Journal of ApproximateReasoning 9 (1993) 1–35.[32] Ph. Smets, The canonical decomposition of a weighted belief, in: Int. Joint Conf. on Artificial Intelligence, Morgan Kaufman, San Mateo, CA,1995, pp. 1896–1901.264T. Denœux / Artificial Intelligence 172 (2008) 234–264[33] Ph. Smets, The Transferable Belief Model for quantified belief representation, in: D.M. Gabbay, Ph. Smets (Eds.), Handbook of DefeasibleReasoning and Uncertainty Management Systems, vol. 1, Kluwer Academic Publishers, Dordrecht, 1998, pp. 267–301.[34] Ph. Smets, The application of the matrix calculus to belief functions, International Journal of Approximate Reasoning 31 (1–2) (2002) 1–30.[35] Ph. Smets, Personal communication, 2005.[36] Ph. Smets, Belief functions on real numbers, International Journal of Approximate Reasoning 40 (3) (2005) 181–223.[37] Ph. Smets, Analyzing the combination of conflicting belief functions, Information Fusion 8 (4) (2007) 387–412.[38] Ph. Smets, R. Kennes, The transferable belief model, Artificial Intelligence 66 (1994) 191–243.[39] R.R. Yager, The entailment principle for Dempster–Shafer granules, International Journal of Intelligent Systems 1 (1986) 247–262.[40] R.R. Yager, On the Dempster–Shafer framework and new combination rules, Information Sciences 41 (1987) 93–137.[41] L.A. Zadeh, Fuzzy sets as a basis for a theory of possibility, Fuzzy Sets and Systems 1 (1978) 3–28.[42] L.M. Zouhal, T. Denœux, An evidence-theoretic k-NN rule with parameter optimization, IEEE Transactions on Systems, Man and CyberneticsC 28 (2) (1998) 263–271.