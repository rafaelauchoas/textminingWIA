Artificial Intelligence 174 (2010) 479–499Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintRepresenting uncertainty on set-valued variables using belief functionsThierry Denœux∗, Zoulficar Younes, Fahed AbdallahHEUDIASYC, UTC, CNRS, Centre de Recherche de Royallieu, BP 20529, F-60205 Compiègne, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 20 April 2009Received in revised form 2 February 2010Accepted 3 February 2010Available online 6 February 2010Keywords:Dempster–Shafer theoryEvidence theoryConjunctive knowledgeLatticeUncertain reasoningMulti-label classification1. IntroductionA formalism is proposed for representing uncertain information on set-valued variablesusing the formalism of belief functions. A set-valued variable X on a domain Ω is a variabletaking zero, one or several values in Ω. While defining mass functions on the frame 22Ωis usually not feasible because of the double-exponential complexity involved, we proposean approach based on a definition of a restricted family of subsets of 2Ω that is closedunder intersection and has a lattice structure. Using recent results about belief functionson lattices, we show that most notions from Dempster–Shafer theory can be transposedto that particular lattice, making it possible to express rich knowledge about X with onlylimited additional complexity as compared to the single-valued case. An application tomulti-label classification (in which each learning instance can belong to several classessimultaneously) is demonstrated.© 2010 Elsevier B.V. All rights reserved.An important concept in knowledge representation is that of variable. Usually, we associate to each variable X a domain(or frame of discernment) Ω , and we assume that X takes one and only one value in Ω . For instance, in conventionalclassification problems, X denotes the class of an object, and each object is assumed to belong to one and only one classamong a set Ω of classes.There are cases, however, where it is convenient to consider a variable X taking zero, one or several values in a domainΩ . In such cases, X may be called a set-valued, or conjunctive variable [8,33]. For instance, in diagnosis problems, Ω maydenote the set of faults that can possibly occur in a system, and X the faults actually occurring at a given time. In textclassification, Ω may denote a set of topics, and X the list of topics dealt with in a given text, etc.A straightforward approach to the above problem is, of course, to consider a set-valued variable X on Ω as a single-valued variable on the power set Θ = 2Ω . However, this approach often implies working in a space of very high cardinality.If, as done in this paper, we assume Ω to be finite with size K , then the size of Θ is 2K . If we want to express impreciseinformation about X , we will have to manipulate subsets of Θ . As there are 22Kof these subsets, this approach rapidlybecomes intractable as K increases.In this paper, we consider the problem of representing partial knowledge about a set-valued variable X with domain Ωusing the Dempster–Shafer theory of belief functions [26,30]. Our approach will be based on a simple representation of aclass C(Ω) of subsets of Θ = 2Ω which, endowed with set inclusion, has a lattice structure. Using recent results about belieffunctions on lattices [14], we will be able to generalize most concepts of Dempster–Shafer theory (including the canonicaldecompositions and the cautious rule [5]) in this setting. This formalism will be shown to allow the expression of a widerange of knowledge about set-valued variables, with only a moderate increase of complexity (from 2K to 3K ) as comparedto the usual single-valued case.* Corresponding author. Fax: +33 03 44 23 44 77.E-mail address: tdenoeux@hds.utc.fr (T. Denœux).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.02.002480T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499The rest of this paper is organized as follows. Background notions on belief functions in the classical setting and ingeneral lattices will first be recalled in Sections 2 and 3, respectively. Our approach will then be introduced in Section 4,and some relationships with previous work will be outlined in Section 5. An application to multi-label classification will bepresented in Section 6, and Section 7 will conclude the paper.2. Belief functionsThe basic concepts of the Dempster–Shafer theory of belief functions, as introduced in [26], will first be summarized inSection 2.1. The canonical decomposition and the cautious rule will then be recalled in Section 2.2.2.1. Basic definitionsLet Ω be a finite set. A mass function on Ω is a function m : 2Ω → [0, 1] such that(cid:2)m( A) = 1.A⊆ΩThe subsets A of Ω such that m( A) > 0 are called the focal elements of m. The set of focal elements of m will be denotedF (m). m is said to be normal if ∅ is not a focal element, and dogmatic if Ω is not a focal element.A mass function m is often used to model an agent’s beliefs about a variable X taking a single but ill-known value ω0in Ω [30]. The quantity m( A) is then interpreted as the measure of the belief that is committed exactly to the hypothesisω0 ∈ A. Full certainty corresponds to the case where m({ωk}) = 1 for some ωk ∈ Ω , while total ignorance is modeled by thevacuous mass function verifying m(Ω) = 1. Probabilistic uncertainty corresponds to the case where all focal elements aresingletons, in which case m is equivalent to a probability distribution on Ω .To each mass function m can be associated an implicability function b and a belief function bel defined as follows:(cid:2)b( A) =B⊆ Abel( A) =m(B),(cid:2)m(B) = b( A) − m(∅).(1)(2)B⊆ A,B(cid:2) AThese two functions are equal when m is normal. However, they need to be distinguished when considering non-normalmass functions. Function bel has easier interpretation, as bel( A) corresponds to a degree of belief in the proposition “The truevalue ω0 of X belongs to A”. However, function b has simpler mathematical properties. For instance, m can be recoveredfrom b asm( A) =(−1)| A\B|b(B),(3)(cid:2)B⊆ Awhere | · | denotes cardinality. Function m is said to be the Möbius transform of b. For every function fsuch that f (Ω) = 1, the following conditions are known to be equivalent [26]:from 2Ω to [0, 1]1. The Möbius transform m of fis positive and verifies(cid:3)A⊆Ω m( A) = 1.2.fis totally monotone, i.e., for any k (cid:2) 2 and for any family A1, . . . , Ak in 2Ω ,(cid:6)(cid:2)Ai(cid:4)fk(cid:5)i=1(cid:2)(−1)|I|+1 f∅(cid:7)=I⊆{1,...,k}(cid:7) (cid:8)(cid:9)Ai.i∈IHence, b (and bel) are totally monotone.Other functions related to m are the plausibility function, defined as(cid:2)pl( A) =m(B)B∩ A(cid:7)=∅= 1 − b( A)and the commonality function (or co-Möbius transform of b) defined asq( A) =(cid:2)m(B).B⊇ Am can be recovered from q using the following relation:m( A) =(−1)|B\ A|q(B).(cid:2)B⊇ A(4)(5)(6)(7)T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499481Functions m, bel, b, pl and q are thus in one-to-one correspondence and can be regarded as different facets of the sameinformation.A special case of interest is that where the focal elements of m are nested: m is then said to be consonant. In this case,we havepl( A ∪ B) = max(cid:10)(cid:11)pl( A), pl(B), ∀ A, B ⊆ Ω.The plausibility function is thus a possibility measure, with corresponding possibility distribution defined by π (x) = pl({x})for all x ∈ Ω . Conversely, to each possibility distribution corresponds a unique consonant mass function [26].Let us now assume that we receive two mass functions m1 and m2 from two distinct sources of information assumed tobe reliable. Then m1 and m2 can be combined using the conjunctive sum (or unnormalized Dempster’s rule of combination)defined as follows:(m1 ∩(cid:12) m2)( A) =(cid:2)m1(B)m2(C).B∩C= A(8)This rule is commutative, associative, and admits the vacuous mass function as neutral element. It is conjunctive as theproduct of m1(B) and m2(C) is transferred to the intersection of B and C . The quantity (m1 ∩(cid:12) m2)(∅) is referred to as thedegree of conflict between m1 and m2.Let q1 ∩(cid:12) 2 denote the commonality function corresponding to m1 ∩(cid:12) m2. It can be computed from q1 and q2, the common-The normalized Dempster’s rule ⊕ [26] is defined as the conjunctive sum followed by a normalization step:ality functions associated to m1 and m2, as follows:q1 ∩(cid:12) 2( A) = q1( A) · q2( A), ∀ A ⊆ Ω.(cid:12)0(m1 ⊕ m2)( A) =(m1 ∩(cid:12) m2)( A)1−(m1 ∩(cid:12) m2)(∅)It is clear that m1 ⊕ m2 is defined as long as (m1 ∩(cid:12) m2)(∅) < 1.if A = ∅,otherwise.the choice of the union operator results in the disjunctive sum [28]:(m1 ∪(cid:12) m2)( A) =It can be shown that(cid:2)m1(B)m2(C).B∪C= Ab1 ∪(cid:12) 2( A) = b1( A) · b2( A), ∀ A ⊆ Ω,Alternatives to the conjunctive sum can be constructed by replacing ∩ by any binary set operation in (8). For instance,(9)(10)(11)(12)which is the counterpart of (9). Dubois and Prade [10] have also proposed a “hybrid” rule intermediate between the con-junctive and disjunctive sums, in which the product m1(B)m2(C) is assigned to B ∩ C whenever B ∩ C (cid:7)= ∅, and to B ∪ Cotherwise. This rule is not associative, but it usually provides a good summary of partially conflicting items of evidence.In [30], Smets proposed a two-level model in which items of evidence are quantified by mass functions and combinedat the credal level, while decisions are made at the pignistic level (from the Latin pignus meaning a bet). Once a decisionhas to be made, a mass function m is thus transformed into a pignistic probability distribution p. The pignistic transformationconsists in normalizing m (assuming that m(∅) < 1), and then distributing each normalized mass m( A)/(1 − m(∅)) equallybetween the atoms ωk ∈ A:(cid:2)p(ωk) ={ A⊆Ω,ωk∈ A}m( A)(1 − m(∅))| A|, ∀ωk ∈ Ω.(13)Other authors have suggested the so-called plausibility transformation for transforming a mass function into a probabil-ity distribution, by normalizing the plausibilities of singletons [3]. In a decision making context, this approach results inselecting the most plausible single hypothesis.2.2. Canonical decompositions and idempotent rulesAccording to Shafer [26], a mass function is said to be simple if it has the following formm( A) = 1 − w0,m(Ω) = w0,for some A ⊂ Ω and some w 0 ∈ [0, 1]. Let us denote such a mass function as A w0 . The vacuous mass function may thus benoted A1 for any A ⊂ Ω . It is clear thatA w 0 ∩(cid:12) A w(cid:15)0 = A w 0 w(cid:15)0 .482T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499A mass function may be called separable if it can be obtained as the result of the conjunctive sum of simple mass functions.It can then be written:m = ∩(cid:12)(14)A⊂Ω A w( A),with w( A) ∈ [0, 1] for all A ⊂ Ω .Smets [29] showed that any non-dogmatic mass function m can be uniquely expressed using (14), with weights w( A)now in (0, +∞). This is referred to as the conjunctive canonical decomposition of a mass function. Note that, when w( A) > 1,A w( A) is no longer a mass function, but the conjunctive sum can be extended to such “generalized mass functions” in anobvious way.Function w is called the conjunctive weight function associated to m [5]. It is a new equivalent representation of a non-dogmatic mass function, which may be computed directly from q as follows:w( A) =q(B)(−1)|B\ A|+1, ∀ A ⊂ Ω,or, taking logarithms,ln w( A) = −(−1)|B\ A|ln q(B), ∀ A ⊂ Ω.(cid:13)B⊇ A(cid:2)B⊇ A(15)(16)In [29] and [5], w( A) was defined for all strict subsets A of Ω . However, function w can be extended to 2Ω by using (15)for A = Ω . We then have:w(Ω) = 1q(Ω)= 1m(Ω)=and(cid:13)A⊆Ωw( A) = 1.(cid:7) (cid:13)(cid:9)−1w( A)A⊂Ω(17)With this convention, (16) can be extended to all A ⊆ Ω . We notice that (16) then has exactly the same form as (7), i.e.,the formula for computing ln w from − ln q is the same as the one for computing m from q. Conversely, ln q can thus becomputed from − ln w using a formula similar to (6):(cid:2)ln q( A) = −ln w(B), ∀ A ⊆ Ω.B⊇ AWe note that function w has a simple property with respect to the conjunctive sum. Let w 1 and w 2 be two weightfunctions, and let w 1 ∩(cid:12) 2 denote the result of their ∩(cid:12) -combination. Then the following relation holds:w 1 ∩(cid:12) 2( A) = w 1( A)w 2( A), ∀ A ⊆ Ω.(18)In [5], Denœux introduced the cautious rule, noted ∧(cid:12) , which is obtained by replacing the product by the minimum in (18),for all A ⊂ Ω :(cid:10)w 1 ∧(cid:12) 2( A) = min(cid:11)w 1( A), w 2( A).(19)The value of w 1 ∧(cid:12) 2(Ω) can then be determined to satisfy the normalization condition (17). This rule is obviously com-mutative, associative and idempotent. As shown in [5], it is suitable for combining conjunctively non-independent items ofevidence. As the conjunctive sum, the cautious rule has a normalized version defined by(cid:12)(cid:10)m1 ∧(cid:12)∗m2(cid:11)( A) =0(m1 ∧(cid:12) m2)( A)1−(m1 ∧(cid:12) m2)(∅)if A = ∅,otherwise.(20)As shown in [5], the conjunctive canonical decomposition also has a disjunctive counterpart. Any mass function m suchthat m(∅) > 0 can be decomposed disjunctively as follows:m = ∪(cid:12)A⊃∅ A v( A),(21)where A v( A) is a generalized mass function assigning a mass v( A) > 0 (possibly greater than 1) to ∅, and 1 − v( A) to A,for all A ⊆ Ω , A (cid:7)= ∅. This defines a new function v, called the disjunctive weight function, which can be computed from b asfollows:v( A) =b(B)(−1)| A\B|+1, ∀ A ⊆ Ω, A (cid:7)= ∅,(22)(cid:13)B⊆ AT. Denœux et al. / Artificial Intelligence 174 (2010) 479–499orln v( A) = −(cid:2)B⊆ A(−1)| A\B|ln b(B), ∀ A ⊆ Ω, A (cid:7)= ∅.As before, the above equations can be extended to A = ∅, which leads to(cid:7) (cid:13)(cid:9)−1v( A)A(cid:7)=∅v(∅) = 1b(∅)= 1m(∅)=and (cid:13)A⊆Ωv( A) = 1.The disjunctive rule (11) has a simple expression as a function of disjunctive weights:v 1 ∪(cid:12) 2( A) = v 1( A)v 2( A), ∀ A ⊆ Ω.483(23)(24)(25)By replacing the product by the minimum in the above equation, we can define a new rule, denoted ∨(cid:12) and called the boldrule in [5]:v 1 ∨(cid:12) 2( A) = min(cid:10)(cid:11)v 1( A), v 2( A),(cid:14)(26)A(cid:7)=∅ v 1 ∨(cid:12) 2( A))−1. This rule is obviously commutative, associative and idempotent; it is suitable for com-A ⊆ Ω, A (cid:7)= ∅,and v 1 ∨(cid:12) 2(∅) = (bining disjunctively non-independent items of evidence.3. Extension to general latticesAs shown by Grabisch [14], the theory of belief function can be extended from the Boolean lattice (2Ω , ⊆) to any lattice,not necessarily Boolean. We will first recall some basic definitions about lattices in Section 3.1. Grabisch’s results used inthis work will then be summarized in Section 3.2.3.1. LatticesA review of lattice theory can be found in [21]. The following presentation follows [14].Let L be a finite set and (cid:3) a partial ordering (i.e., a reflexive, antisymmetric and transitive relation) on L. The structure(L, (cid:3)) is called a poset. We say that (L, (cid:3)) is a lattice if, for every x, y ∈ L, there is a unique greatest lower bound (denotedx ∧ y) and a unique least upper bound (denoted x ∨ y). Operations ∧ and ∨ are called the meet and join operations,respectively. For finite lattices, the greatest element (denote (cid:20)) and the least element (denoted ⊥) always exist. We say thatx covers y if x > y and there is no z such that x > z > y. An element x of L is an atom if it covers only one element andthis element is ⊥. It is a co-atom if it is covered by a single element and this element is (cid:20).(cid:15)are isomorphic if there exists a bijective mapping f from L to Lsuch that x (cid:3) y ⇔ f (x) (cid:3) f ( y).For any poset (L, (cid:3)), we can define its dual (L, (cid:2)) by inverting the order relation. A lattice is autodual if it is isomorphic toits dual.Two lattices L and LA lattice is distributive if (x ∨ y) ∧ z = (x ∧ z) ∨ ( y ∧ z) holds for all x, y, z ∈ L. For any x ∈ L, we say that x has a(cid:15) = (cid:20). L is said to be complemented if any element has(cid:15) = ⊥ and x ∨ xcomplement in L if there exists xa complement. Boolean lattices are distributive and complemented lattices. Every Boolean lattice is isomorphic to (2Ω , ⊆)for some set Ω . For the lattice (2Ω , ⊆), we have ∧ = ∩, ∨ = ∪, ⊥ = ∅ and (cid:20) = Ω .(cid:15) ∈ L such that x ∧ x(cid:15)A closure system C on a set Θ is a family of subsets of Θ satisfying the following properties:1. Θ ∈ C.2. C1, C2 ∈ C ⇒ C1 ∩ C2 ∈ C.As shown in [21], any closure system (C, ⊆) is a lattice with the following meet and join operationsC1 ∧ C2 = C1 ∩ C2,C1 ∨ C2 =(cid:8){C ∈ C | C1 ∪ C2 ⊆ C}.3.2. Belief functions on lattices(27)(28)Let (L, (cid:3)) be a finite poset having a least element, and let f be a function from L to R. The Möbius transform of f is thefunction m : L → R defined as the unique solution of the equation:f (x) =m( y), ∀x ∈ L.(29)(cid:2)y(cid:2)x484T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499Function m can be expressed as:(cid:2)m(x) =μ( y, x) f ( y),y(cid:2)xwhere μ(x, y) : L2 → R is the Möbius function defined inductively by:⎧⎨μ(x, y) =⎩(cid:3)1−0,x(cid:2)t< y μ(x, t)if x = y,if x < y,otherwise.The co-Möbius transform of f(cid:2)q(x) =m( y),y(cid:3)xis defined as:and m can be recovered from q as:(cid:2)m(x) =μ(x, y)q( y).y(cid:3)x(30)(31)(32)(33)Let us now assume that (L, (cid:3)) is a lattice. Following Grabisch [14], a function b : L → [0, 1] will be called an implicabilityfunction on L if b((cid:20)) = 1, and its Möbius transform is non-negative. The corresponding belief function bel can then bedefined as:bel(x) = b(x) − m(⊥), ∀x ∈ L.Note that Grabisch [14] considered only normal belief functions, in which case b = bel. As shown in [14], any implicabilityfunction on (L, (cid:3)) is totally monotone, i.e., for any k (cid:2) 2 and for any family x1, . . . , xk in L,(cid:4)(cid:6)b(cid:2)xik(cid:18)i=1(cid:2)(−1)|I|+1b∅(cid:7)=I⊆{1,...,k}(cid:7) (cid:19)(cid:9)xi.i∈INote, however, that the converse does not hold in general: a totally monotone function may not have a non-negative Möbiustransform.As shown in [14], most results of Dempster–Shafer theory can be transposed in the general setting of lattices. Forinstance, the conjunctive sum (8) becomes:(cid:2)(m1 ∩(cid:12) m2)(x) =m1( y)m2(z), ∀x ∈ L,(34)y∧z=xand the following relation between commonality functions still holds:q1 ∩(cid:12) 2(x) = q1(x) · q2(x), ∀x ∈ L.(35)The normalized Dempster’s rule ⊕ can still be defined, as in the classical case, by dividing each number (m1 ∩(cid:12) m2)(x) withx (cid:7)= ⊥ by 1 − (m1 ∩(cid:12) m2)(⊥), provided that (m1 ∩(cid:12) m2)(⊥) < 1.Using a similar line of reasoning as that followed in [14], we can also extend the disjunctive rule (11) as:(m1 ∪(cid:12) m2)(x) =m1( y)m2(z), ∀x ∈ L,(cid:2)y∨z=xand (12) becomes:b1 ∪(cid:12) 2(x) = b1(x) · b2(x), ∀x ∈ L.(cid:13)w(x) =q( y)−μ(x, y), ∀x ∈ L, x (cid:7)= (cid:20),y(cid:3)xwhich generalizes (15). Obviously, we still havew 1 ∩(cid:12) 2(x) = w 1(x)w 2(x), ∀x ∈ L, x (cid:7)= (cid:20).Grabisch [14] also extended the conjunctive canonical decomposition of belief functions in the general lattice setting. Heshowed that any mass function m on L such that m((cid:20)) > 0 can be decomposed asm = ∩(cid:12)x<(cid:20)xw(x),(38)where xw(x) is a simple mass function assigning 1 − w(x) to x and w(x) to (cid:20), with w(x) ∈ (0, +∞). Clearly, (38) general-izes (14). As in the classical case, function w : L \ {(cid:20)} → (0, +∞) can be computed from q using the following equation:(36)(37)(39)(40)T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499The existence of the w function also allows us to define the cautious rule in the general lattice setting as(cid:10)w 1 ∧(cid:12) 2(x) = min(cid:11)w 1(x), w 2(x), ∀x ∈ L, x (cid:7)= (cid:20).485(41)The normalized cautious rule ∧(cid:12) ∗provided that w 1 ∧(cid:12) 2(⊥) < 1.is defined as in the classical case, by dividing each w 1 ∧(cid:12) 2(x) for x (cid:7)= ⊥ by 1 − w 1 ∧(cid:12) 2(⊥),Although Grabisch did not consider the disjunctive canonical decomposition, it can also be extended in the general latticesetting. The proof parallels that given in [14] for the conjunctive case. We will only state the main result here. Let xv(x) bea mass function on L assigning 1 − v(x) to x and v(x) to ⊥, with v(x) ∈ (0, +∞). Any mass function m on L such thatm(⊥) > 0 can be decomposed asm = ∪(cid:12)x>⊥xv(x).The function v : L \ {⊥} → (0, +∞) can be computed from b using the following equation:(cid:13)v(x) =b( y)−μ( y,x), ∀x ∈ L, x (cid:7)= ⊥,y(cid:2)xwhich generalizes (22). We still havev 1 ∪(cid:12) 2(x) = v 1(x)v 2(x), ∀x ∈ L, x (cid:7)= ⊥,and the existence of the v function allows us to define the bold rule as(cid:11)v 1(x), v 2(x)(cid:10)v 1 ∨(cid:12) 2(x) = min, ∀x ∈ L, x (cid:7)= ⊥.(42)(43)(44)(45)The extension of other notions from classical Dempster–Shafer theory may require additional assumptions on (L, (cid:3)). Forinstance, the definition of the plausibility function pl as the dual of b using (5) can only be extended to autodual lattices[14]. The definition of pl from (4) remains possible in the other cases, but the relationship between pl and b (or bel) is lost.Also, probability measures cannot be defined on arbitrary lattices. Consequently, the pignistic probability (13) can only beextended in restricted settings.Remark 1. Although our approach relies essentially on Grabisch’s work, we may note the existence of another line ofresearch that aims at extending results of Probability Theory to some classes of residuated lattices, which are more generalthan Boolean algebra. In particular, there have been many developments about probability measures on MV-algebra (alsocalled states), see, e.g., [2,16,17,22] as well as in Gödel algebras [1]. In the course of revising this paper, we also becameaware of recent work on defining belief functions on MV-algebras [18].14. Belief functions on set-valued variablesIn this section, the main concepts of Dempster–Shafer theory recalled in Section 2 will be extended to the case wherewe want to describe the uncertainty regarding a set-valued variable X on a finite domain Ω . The key to this extensionwill be the definition of a closure system C(Ω) of Θ = 2Ω , i.e., a set of subsets of Θ that is closed under intersection.Each element of C(Ω) will be shown to have a simple description as a pair of disjoint subsets of Ω . Belief functions andassociated notions will then be defined on the lattice (C(Ω), ⊆), resulting in a simple framework for uncertain reasoningabout set-valued variables.4.1. The lattice (C(Ω), ⊆)In the rest of this paper, X denotes a set-valued variable on a finite domain Ω , i.e., a variable taking values in Θ = 2Ω .Let A0 ⊆ Ω denote the unknown true value of X . We want to describe partial knowledge about that value in the belieffunction framework.As explained in the introduction, the formalism recalled in Section 2 could be applied without modification to thiscase, by defining a mass function mΘ on Θ . However, such a brute force approach would require the storage of up to|Θ| = 22numbers for each mass function. Basic operations such as the conjunctive or disjunctive sums would have2double-exponential complexity, making the approach inapplicable except for sets Ω with very small cardinality.|Ω|As an alternative, we propose to define mass functions and associated functions on a subset of 2Θ that forms a latticewhen equipped with the inclusion relation. The intuitive idea underlying our approach is the fact that, when expressingknowledge about a set-valued variable X , it is often convenient to specify sets of values that are certainly taken by X , andsets of values that are certainly not taken by X . This can be illustrated by the following example.1 We are indebted to one of the anonymous referees for bringing this work to our attention.486T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499Fig. 1. Two subsets of Ω (broken lines) containing A and not intersecting B. The set of all such subsets is denoted by ϕ( A, B).Example 1. Let X denote the languages spoken by John, defined on the (very large) set Ω of existing languages. If weknow for sure that John can speak English and French (because he was brought up in the US and he stayed in France for along time), and that he can speak neither Japanese nor Chinese (because he never traveled to Asia), then all subsets of Ωcontaining A = {English, French} and not intersecting B = {Japanese, Chinese} are possible values of X .As shown by this example, some families of subsets of Ω or, equivalently, some subsets of Θ = 2Ω can be convenientlydescribed by two subsets A and B of Ω such that A ∩ B = ∅ (Fig. 1).More generally, let Q(Ω) = {( A, B) ∈ 2Ω × 2Ω | A ∩ B = ∅Ω } be the set of ordered pairs of disjoint subsets of Ω , where∅Ω denotes the empty set of Ω . For any ( A, B) ∈ Q(Ω), let ϕ( A, B) denote the following subset of Θ = 2Ω :ϕ( A, B) = {C ⊆ Ω | C ⊇ A, C ∩ B = ∅Ω }.(46)ϕ( A, B) is thus the subset of Θ composed of all subsets of Ω including A and not intersecting B. Equivalently, it is the setof all subsets of Ω that include A and are included in B:ϕ( A, B) = {C ⊆ Ω | A ⊆ C ⊆ B}.(47)It is thus the interval [ A, B] in the lattice (Ω, ⊆).Let C(Ω) denote the set of all subsets of Θ of the form ϕ( A, B), completed by the empty set of Θ , noted ∅Θ :∪ {∅Θ }.ϕ( A, B) | A ⊆ Ω, B ⊆ Ω, A ∩ B = ∅ΩC(Ω) =(cid:20)(cid:21)C(Ω) is thus a subset of 2Θ . For a reason that will become evident later, we will also use ϕ(Ω, Ω) as an alternative notationfor ∅Θ . Function ϕ is thus a bijective mapping from Q∗(Ω) = Q(Ω) ∪ {(Ω, Ω)} to C(Ω). The following proposition statesthat C(Ω) is a closure system and, consequently, has a lattice structure.Proposition 1. C(Ω) is a closure system of Θ , and(cid:15), B ∪ B(cid:22)(cid:11)(cid:10)ϕ( A, B) ∩ ϕ(cid:15)A(cid:15), B=ϕ( A ∪ A∅Θ(cid:15)) in Q∗(Ω).for all ( A, B) and ( A(cid:15), B(cid:15))(cid:15)) ∩ (B ∪ Bif ( A ∪ Aotherwise,(cid:15)) = ∅Ω ,Proof. It is obvious that Θ = ϕ(∅Ω , ∅Ω ) ∈ C(Ω). Now,(cid:11)(cid:20)(cid:10)ϕ( A, B) ∩ ϕ(cid:15)A, B(cid:15)==(cid:20)C ⊆ Ω | C ⊇ A, C ⊇ A(cid:11)(cid:15)C ⊆ Ω | C ⊇(cid:10)(cid:15)(cid:10)(cid:15) = ∅Ω, C ∩ B = ∅Ω , C ∩ B(cid:21)(cid:15)= ∅Ω, C ∩.(cid:15)) is thus equal to ϕ( A ∪ AB ∪ B(cid:11)A ∪ A(cid:15), B(cid:15)) = ∅Ω then ϕ( A, B) ∩ ϕ( A(cid:21)If ( A ∪ Ainclude A ∪ A(cid:15)) ∩ (B ∪ B(cid:15)and have an empty intersection with B ∪ B; consequently, ϕ( A, B) ∩ ϕ( A(cid:15)(cid:15), B(cid:15)) = ∅Θ . (cid:2)(cid:15), B ∪ B(cid:15)). Otherwise, no subset C of Ω canAs recalled in Section 3.1, any closure system endowed with the inclusion relation has a lattice structure with ∧ = ∩ and∨ defined by (28). Here, the inclusion relation has the following simple expression using the ϕ( A, B) representation:ϕ( A, B) ⊆ ϕ(cid:15)A, B(cid:15)(cid:10)(cid:11)⇔ A ⊇ A(cid:15)and B ⊇ B(cid:15).(48)The least element is ⊥ = ϕ(Ω, Ω) = ∅Θ . We note that (48) remains valid when A = B = Ω , which explains the interest ofthe notation ϕ(Ω, Ω) = ∅Θ . The greatest element is (cid:20) = ϕ(∅Ω , ∅Ω ) = Θ . The atoms are of the form ϕ( A, A) for A ⊆ Ω ,and the co-atoms are of the form ϕ({x}, ∅Ω ) or ϕ(∅Ω , {x}) for x ∈ Ω . We can see that the number of atoms is not equal tothe number of co-atoms, which shows that (C, ⊆) is not autodual. This lattice is also not complemented; consequently, it isnot Boolean.T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499487As a consequence of (48), it is easy to see that the meet operation ∨ is the following operation, hereafter denoted (cid:24):(cid:10)(cid:11)(cid:10)ϕ( A, B) (cid:24) ϕ(cid:15)A, B(cid:15)= ϕA ∩ A(cid:15), B ∩ B(cid:15)(cid:11).It must be noted that (cid:24) is not identical to set union. The following proposition states the relation between these twooperators.Proposition 2. For all ( A, B) and ( A(cid:10)ϕ( A, B) ∪ ϕ(cid:15)A, B(cid:15)(cid:11)(cid:15), B⊆ ϕ( A, B) (cid:24) ϕ(cid:15)) in Q∗(Ω),(cid:11)(cid:10)(cid:15)(cid:15), BA.Proof. For every C in ϕ( A, B) ∪ ϕ( AC ⊇ A and C ⊇ A(cid:15), B(cid:15) ⇒ C ⊇ A ∩ A(cid:15)), we have(cid:15)andC ∩ B = ∅Ω and C ∩ B(cid:15)). (cid:2)(cid:15), B ∩ Bhence C ∈ ϕ( A ∩ A(cid:15) = ∅Ω ⇒ C ∩(cid:10)B ∩ B(cid:11)(cid:15)= ∅Ω ,(49)(50)One can notice that the implications in (49) and (50) are strict. Consequently, ϕ( A, B) ∪ ϕ( A(cid:15)) is usually a strict(cid:15)). As the lattices (C(Ω), ⊆) and (2Θ , ⊆) do not have the same join operator, (C(Ω), ⊆) is not a(cid:15), B(cid:15), Bsubset of ϕ( A, B) (cid:24) ϕ( Asublattice of (2Θ , ⊆), although it is a subposet.As noticed in [15], any ordered pair ( A, B) of disjoint subsets of Ω = {ω1, . . . , ωK } can be represented by a vector(u1, . . . , u K ) ∈ {−1, 0, 1}K , with⎧⎨ui =⎩if ωi ∈ A,1−1 if ωi ∈ B,otherwise.0Consequently, any ϕ( A, B) ∈ C(Ω) such that ( A, B) (cid:7)= (Ω, Ω) can be represented in the same way. For ϕ(Ω, Ω) = ∅Θ , aspecial representation can be adopted, e.g., (∗, . . . , ∗). This encoding makes it possible to implement the ∩ and (cid:24) operationsin a simple way using generalized truth tables. It also makes it clear that the cardinality of C(Ω) is equal to 3K + 1.4.2. Belief functions on C(Ω)The general theory recalled in Section 3.2 can be applied directly to the lattice (C(Ω), ⊆).Let m : C(Ω) → [0, 1] be a mass function on C(Ω). The notation m(ϕ( A, B)) will be simplified to m( A, B). For this reason,m will be called a two-place mass function. We assume that(cid:2)( A,B)∈Q∗(Ω)m( A, B) = 1.The implicability, belief and commonality functions can be computed from m using the following formula:(cid:2)(cid:2)b( A, B) =m(C, D) =ϕ(C,D)⊆ϕ( A,B)bel( A, B) = b( A, B) − m(Ω, Ω),(cid:2)q( A, B) =m(C, D) =C⊇ A,D⊇B(cid:2)ϕ(C,D)⊇ϕ( A,B)C⊆ A,D⊆Bm(C, D),m(C, D),(51)(52)(53)where all pairs ( A, B) and (C, D) are understood to belong to Q∗(Ω) (the same convention will be adopted throughout thispaper). The conjunctive sum operation in C(Ω) is defined as follows:(cid:2)(m1 ∩(cid:12) m2)( A, B) ==m1(C, D)m2(E, F )ϕ(C,D)∩ϕ(E,F )=ϕ( A,B)(cid:12) (cid:3)(cid:3)C∪E= A,D∪F =B m1(C, D)m2(E, F )(C∪E)∩(D∪F )(cid:7)=∅Ωm1(C, D)m2(E, F )It can be computed using the commonality functions as:q1 ∩(cid:12) 2( A, B) = q1( A, B) · q2( A, B), ∀( A, B) ∈ Q∗(Ω).Similarly, the disjunctive sum can be computed as:if A ∩ B = ∅Ω ,if A = B = Ω.(54)(55)(56)488T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499(m1 ∪(cid:12) m2)( A, B) =(cid:2)m1(C, D)m2(E, F )=ϕ(C,D)(cid:24)ϕ(E,F )=ϕ( A,B)(cid:2)m1(C, D)m2(E, F ),C∩E= A,D∩F =Bor using implicability functions:b1 ∪(cid:12) 2( A, B) = b1( A, B) · b2( A, B), ∀( A, B) ∈ Q∗(Ω).(57)(58)It is also possible to define a rule expressing a consensus among items of evidence, somehow in the same spirit as theDubois–Prade rule recalled in Section 2.1. Assume that we learn from two sources that the value of X is in ϕ(C, D) and inϕ(E, F ), but that ϕ(C, D) ∩ ϕ(E, F ) = ∅Θ , i.e., (C ∪ E) ∩ (D ∪ F ) (cid:7)= ∅Ω , so that the two pieces of information are in conflict.It may still be safe to keep (C ∪ E) \ (D ∪ F ) as positive information, and (D ∪ F ) \ (C ∪ E) as negative information. Denotingby (cid:25) the following operation on C(Ω):ϕ(C, D) (cid:25) ϕ(E, F ) = ϕ(cid:10)(cid:11)(C ∪ E) \ (D ∪ F ), (D ∪ F ) \ (C ∪ E),we may define a new combination rule as(cid:2)(m1 (cid:4) m2)( A, B) =ϕ(C,D)(cid:25)ϕ(E,F )=ϕ( A,B)m1(C, D)m2(E, F ).(59)This rule will be referred to as the consensus rule. We note that operations (cid:25) and (cid:4) are not associative. However, they arequasi-associative, as it is possible to define a n-ary version of (cid:25) as:ϕ(C1, D1) (cid:25) · · · (cid:25) ϕ(Cn, Dn) = ϕ(cid:4)n(cid:5)(cid:23) n(cid:5)(cid:23) n(cid:5)(cid:23) n(cid:5)CiD i,D iCi.i=1i=1i=1i=1(cid:6)To compute functions m, w and v from q or b using (30), (33), (39) and (43), we need the expression of the Möbiusfunction μ. It is given in the following proposition.Proposition 3. The Möbius function on (C(Ω), ⊆) is given, for any ( A, B) and ( A(cid:15)),(−1)| A\ A0if ϕ( A, B) ⊆ ϕ( Aotherwise.ϕ( A, B), ϕ(cid:15)|+|B\B(cid:10)μ(cid:15), B, B(cid:11)(cid:11)=(cid:22)A(cid:10)(cid:15)|(cid:15)(cid:15)(cid:15), B(cid:15)) in Q∗(Ω) byProof. The proof is similar to that of Theorem 2 in [15] with simple adaptations, due to the similarity between two-placebelief functions on C(Ω) and bi-capacities (see Remark 2 below). (cid:2)This result allows us to compute m from b as:m( A, B) =and from q asm( A, B) =(−1)|C\ A|+|D\B|b(C, D),(−1)| A\C|+|B\D|q(C, D).(cid:2)C⊇ A,D⊇B(cid:2)C⊆ A,D⊆B(cid:13)The conjunctive and disjunctive weight functions may be computed, respectively, as:w( A, B) =q(C, D)(−1)| A\C|+|B\D|+1, ∀( A, B) (cid:7)= (∅Ω , ∅Ω ),C⊆ A,D⊆Bandv( A, B) =(cid:13)C⊇ A,D⊇Bb(C, D)(−1)|C\ A|+|D\B|+1, ∀( A, B) (cid:7)= (Ω, Ω),which makes it possible to use the cautious and bold rules in this context.(60)(61)(62)(63)Example 2. Let X now denote the set of languages spoken by Bernard. Assume that we are 100% sure that Bernard speaksno other language than Dutch (d), English (e) and French ( f ), so that we can restrict the domain of X to Ω = {d, e, f }.Suppose that we have the following items of evidence:T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499489Table 1Computation of the conjunctive sum of m1 and m2 in Example 2. The columns and the lines correspond to the focal elements of m1, and m2, respectively.Each cell contains the intersection of a focal element of m1 and a focal element of m2. The mass of each focal element is indicated below it.({e}, ∅)0.7({ f }, ∅)0.15(∅, ∅)0.15({d}, { f })0.27({d, e}, f )0.7 × 0.27∅Θ0.15 × 0.27({d}, { f })0.15 × 0.27({ f }, {d})0.29({e, f }, {d})0.7 × 0.29({ f }, {d})0.15 × 0.29({ f }, {d})0.15 × 0.29({ f , d}, ∅)0.34({e, f , d}, ∅)0.7 × 0.34({ f , d}, ∅)0.15 × 0.34({ f , d}, ∅)0.15 × 0.34(∅, ∅)0.1({e}, ∅)0.7 × 0.1({ f }, ∅)0.15 × 0.1(∅, ∅)0.15 × 0.11. Bernard is Belgian. Approximately 60% of Belgians are Dutch-speaking, and 40% of Belgians are French-speaking (weneglect here the small German-speaking community for simplicity). According to a recent survey, approximately 20%of French-speaking Belgians declare to have good knowledge of Dutch, whereas around 50% of members of the Dutchspeaking community claim to have good knowledge of French.2. Bernard studied three years in Canada, where most universities are English-speaking, and some are French speaking.Based on available evidence, we have a 0.7 degree of belief that Bernard studied in an English-speaking university, anda 0.15 degree of belief that he studied in a French-speaking one.Each of these two items of evidence can be represented by a mass function on C(Ω). According to the first item of evidence,approximately (0.6 × 0.5) × 100 = 30% of Belgians speak Dutch and no French, approximately (0.4 × 0.8) × 100 = 32% speakFrench and no Dutch, and the rest speak both languages. Knowing that Bernard belongs to this population (and nothingelse), and assuming these figures to be accurate, this would lead to the following mass function:= 0.32,{ f , d}, ∅= 0.38.{d}, { f }{ f }, {d}= 0.3,(cid:11)(cid:10)(cid:10)(cid:10)(cid:11)(cid:11)m1m1m1To account for inaccuracy of these figures, we may discount this mass function [26] by transferring a fraction of the mass(say, 10%) to the greatest element of C(Ω), i.e., ϕ(∅, ∅). We thus have(cid:10)(cid:10)m1m1(cid:11){d}, { f }{ f , d}, ∅= 0.3 × 0.9 = 0.27,(cid:11)= 0.38 × 0.9 ≈ 0.34,(cid:10)m1(cid:11){ f }, {d}= 0.32 × 0.9 ≈ 0.29,m1(∅, ∅) = 0.1.The second item of evidence can be represented by a mass function m2 defined as:(cid:10)(cid:11)(cid:10)(cid:11)m2{e}, ∅= 0.7,m2(∅, ∅) = 0.15.Assuming these two items of evidence to be distinct, they should be combined using the conjunctive sum operation ∩(cid:12) .This may be achieved in two ways:= 0.15,{ f }, ∅m21. We may compute the intersection between each focal element of m1 and each focal element of m2 and apply for-mula (54). The computations may be presented as in Table 1.2. Alternatively, we may compute the commonality functions q1 and q2 using (53), multiply them, and convert the resultinto a mass function using (61). The intermediate and final results are shown in Table 2.We may check that both approaches yield the same result. In particular, we can see that the empty set ∅Θ receives a massequal to 0.15 × 0.27 = 0.0405, which can be interpreted as a degree of conflict between m1 and m2. Using the consensusrule (cid:4) (59), the mass 0.15 × 0.27 would be transferred to(cid:11)(cid:10)(cid:11)(cid:11)ϕ{ f }, ∅(cid:25) ϕ(cid:10){d}, { f }(cid:10){d}, ∅,= ϕresulting in a normal, conflict-free mass function.Table 2 also shows the normal mass function computed using the normalized Dempster’s rule ⊕, and Table 3 displays theintermediate steps and final results for computing the combinations of m1 and m2 using the unnormalized and normalizedcautious rules.Remark 2. We may remark here that the concept of two-place mass and belief functions defined here bears some similaritywith bi-capacities introduced by Grabisch and Labreuche [15]. A bi-capacity as defined in [15] is an increasing functiondefined on the lattice (Q(Ω), (cid:27)), where (cid:27) is the partial ordering on Q(Ω) defined by ( A, B) (cid:27) (C, D) if A ⊆ B and C ⊇ D.In [15], Grabisch and Labreuche introduce various concepts related to bi-capacities, with application to cooperative gametheory. In [19], they introduce the concept of bi-belief function, defined as a totally monotone bi-capacity from Q(Ω)to [0, 1]. They suggest an interpretation in terms of bipolar representation of uncertainty for the case of a single-valuedvariable. Bi-belief functions and two-place belief functions as introduced here are thus two distinct classes of belief functionsbuilt on different lattices, with different interpretations.490T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499Table 2Computation of m1 ∩(cid:12) m2 and m1 ⊕ m2 in Example 2.A{de f }∅∅{ f }∅∅{ f }{e}{e}{e f }∅∅{ f }∅∅{ f }{e}{e}{e f }{d}{d}{df }{d}{d}{df }{de}{de}{de f }B{de f }{de f }{de}{de}{df }{d}{d}{df }{d}{d}{e f }{e}{e}{ f }∅∅{ f }∅∅{e f }{e}{e}{ f }∅∅{ f }∅∅m10000000.2900000000.100000000.2700.34000q110.10.10.390.10.10.390.10.10.390.10.10.10.10.10.10.10.10.10.370.10.440.370.10.440.370.10.44Table 3Computation of m1 ∧(cid:12) m2 and m1 ∧(cid:12) ∗m2 in Example 2.A{de f }∅∅{ f }∅∅{ f }{e}{e}{e f }∅∅{ f }∅∅{ f }{e}{e}{e f }{d}{d}{df }{d}{d}{df }{de}{de}{de f }B{de f }{de f }{de}{de}{df }{d}{d}{df }{d}{d}{e f }{e}{e}{ f }∅∅{ f }∅∅{e f }{e}{e}{ f }∅∅{ f }∅∅m10000000.2900000000.100000000.2700.34000w 16.349111110.25611111111011111110.27010.227111m2000000000000000.150.1500.70000000000m2000000000000000.150.1500.70000000000q210.150.150.30.150.150.30.850.8510.150.150.30.150.150.30.850.8510.150.150.30.150.150.30.850.851w 2111111111111116.670.510.1761.7111111111q1 ∩(cid:12) 210.0150.0150.1170.0150.0150.1170.0850.0850.390.0150.0150.030.0150.0150.030.0850.0850.10.05550.0150.1320.05550.0150.1320.31450.0850.44w 1∧21111110.2561111111719.60.510.17611110.27010.227111m1 ∩(cid:12) m20.0405000000.087000.20300000.0150.01500.0700000.040500.1020.18900.238m1 ∧(cid:12) m20.864000000.00806000.037600000.001390.0013900.006490.006490000.0037500.009450.017500.0441m1 ⊕ m20000000.091000.21200000.0160.01600.0700000.042200.1060.19700.248m2m1 ∧(cid:12) ∗0000000.0591000.27600000.01020.010200.04770.04770000.027500.06940.12900.324Remark 3. Another remark concerns decision making. As noted in the previous section, the lattice (C(Ω), ⊆) is not Boolean,so that the notion of pignistic probability cannot be defined in that lattice. In the classical setting, a common alternativeto the rule of maximum pignistic probability for decision making is that of maximum plausibility: it consists in selectingthe element of Ω with the greatest plausibility or, equivalent, with the greatest commonality (as these two functionscoincide on singletons). In C(Ω), we may propose as a reasonable decision rule to select the atom ϕ( A, A) with the highestT. Denœux et al. / Artificial Intelligence 174 (2010) 479–499491Table 4Commonalities of atoms according to m1 ⊕ m2, m1 (cid:4) m2 and m1 ∧(cid:12) ∗m2 in Example 2.( A, A)(∅, {de f })({ f }, {de})({e}, {df })({e f }, {d})({d}, {e f })({df }, {e})({de}, { f })({de f }, ∅)q1⊕2( A, A)0.01560.1220.08890.4060.05780.1380.3280.459q1(cid:2)2( A, A)0.0150.1170.0850.390.0960.1730.3550.481q1∧∗ 2( A, A)0.01020.07960.05780.4510.03770.08980.2140.509commonality. Table 4 shows the commonalities of the atoms computing from m1 ⊕m2, m1 (cid:4)m2 and m1 ∧(cid:12) ∗m2 in Example 2.In that particular case, we can see that the three rules lead to the same conclusion, which is that Bernard speaks all threelanguages. The second most likely hypothesis is that Bernard speaks English and French, but no Dutch. However, it is clearthat different combination rules may, in general, result in different decisions.The following section will be devoted to a review of previous work on uncertainty representation for set-valued variables.5. Relation to previous workThis section discusses the relation between the notions introduced above and related concepts or other formalismsalready proposed for handling set-valued variables.5.1. Disjunctive vs. conjunctive bodies of evidenceYager [32,33] was among the first authors to emphasize the fundamental difference between single-valued and set-valuesvariables, and to develop specific formalisms for reasoning with the latter. In [33], a distinction is made between disjunctiveand conjunctive information using set-based representations. Given a variable X taking a single value in Ω , a statement“ X is A” with A ⊆ Ω means that X takes some value in A, but we do not know which one. In contrast, if X is multiple-valued, the same statement is understood to mean that X takes all values in A (and possibly other values outside A). Thecorresponding piece of information is called “disjunctive” in the former case, and “conjunctive” in the latter. Yager thenproceeds by observing that there is some kind of duality between disjunctive and conjunctive knowledge. For instance, thestatement P 1: “ X is A” implies P 2: “ X is B” whenever B ⊇ A in the disjunctive case, whereas P 2 can be deduced from P 1whenever B ⊆ A in the conjunctive case. If we know that P 1 and P 2 both hold, then we can deduce “ X is A ∩ B” in thedisjunctive case, and “ X is A ∪ B” in the conjunctive case, etc.Viewing mass functions as generalized sets, Dubois and Prade [8] remarked that the same distinction holds in the belieffunction framework. They pointed out that, when a mass function m represents a body of evidence pertaining to a set-valued variable (referred to as a conjunctive body of evidence), the commonality function q is more appropriate than b forrepresenting degrees of belief, and the disjunctive sum (11) should be used for combining information conjunctively.The formalism developed in Section 4 sheds new light on this duality between conjunctive and disjunctive knowledge.The conjunctive statement “ X is A” corresponds to the proposition ϕ( A, ∅). Let m be a mass function on C(Ω) whose focal(cid:15)( A) = m( A, ∅) for all A. Using (51), we then have, forelements are all of the form ϕ(B, ∅) for some B ⊆ Ω . We can note mall A ⊆ Ω :b( A, ∅) =(cid:2)B⊇ Am(B, ∅) =(cid:2)B⊇ A(cid:15)m(B) = q(cid:15)( A),where q(cid:15)is the commonality function corresponding to m(cid:2)(cid:2)q( A, ∅) =m(B, ∅) =(cid:15)(B) = b(cid:15)( A).B⊆ AmB⊆ A(cid:15). Conversely,As a consequence, let m1 and m2 be two mass functions on C(Ω) with focal elements of the form described above, andassume that we want to combine them conjunctively using (56). We getq1 ∩(cid:12) 2( A, ∅) = q1( A, ∅)q2( A, ∅) = b(cid:15)1( A)b(cid:15)2( A) = b(cid:15)1 ∪(cid:12) 2( A)for all A ⊆ Ω , which explains why the disjunctive sum seems to be used when combining conjunctive bodies of evidence ina conjunctive manner.492T. Denœux et al. / Artificial Intelligence 174 (2010) 479–4995.2. Random setsRandom sets are defined as random elements taking values as subsets of some space [20,23]. In the finite case, a randomA⊆Ω m( A) = 1, which is mathematically equivalent to aset is thus defined by a probability function m on 2Ω such thatDempster–Shafer mass function on Ω [24]. However, as noted by Smets [27], the semantics of random sets and (standard)belief functions are different, as random sets model random experiments with set-valued outcomes, whereas standard belieffunctions quantify beliefs regarding a variable taking a single, but unknown value.(cid:3)In contrast, random sets are recovered as a special class of belief functions on set-valued variables introduced in thispaper. Let m be a mass function on C(Ω), and assume that the focal elements of m are atoms of C(Ω), i.e., if they are of(cid:15)( A) = m( A, A) for all A ⊆ Ω is a random set.from 2Ω to [0, 1] such that mthe form ( A, A). In that case, the function mRandom sets are thus equivalent to mass functions on C(Ω) with atomic focal elements, just as probability distributions onΩ are equivalent to mass functions on 2Ω with singleton focal elements.(cid:15)5.3. Veristic variablesIn [34,35], Yager develops a theory of veristic variables, which can be defined as fuzzy set-valued variables. Let X denotea fuzzy set-valued variable on a domain Ω , i.e., a variable taking a single value in the set I Ω of fuzzy subsets of Ω . For anyx ∈ Ω and any A ∈ I Ω , we denote by A(x) the degree of membership of x in A. Let A0 ∈ I Ω denote the unknown true valueof X . Yager considers four ways of associating variable X with a fuzzy set A ∈ I Ω , using the following statements:1. X isv A, meaning that A ⊆ A0, where ⊆ denotes standard fuzzy set inclusion, i.e., A(x) (cid:3) A0(x) for all x ∈ Ω ;2. X isv(n) A, meaning that A0 ⊆ A, where A denotes the complement of A with membership function A(x) = 1 − A(x) forall x ∈ Ω ;3. X isv(c) A, meaning that A0 = A;4. X isv(c, n) A, meaning that A0 = A.In the above expressions, the copula isv has two parameters: c for closed (exclusive) and n for negative. We observe thatthe statement X isv(c, n) A is equivalent to X isv(c) A. Consequently, we need only to consider the first three cases.As remarked by Yager, each of these basic types of statements can be interpreted as specifying a set W of fuzzy subsetsof Ω , i.e., a crisp subset of I Ω . W contains the possible values of variable X . It is defined as follows for the three types ofstatements:(cid:20)−→ W =X isv AX isv(n) A −→ W =X isv(c) A −→ W = { A}.(cid:20)F ∈ I Ω | F (x) (cid:2) A(x), ∀x ∈ ΩF ∈ I Ω | F (x) (cid:3) A(x), ∀x ∈ Ω(cid:21),(cid:21),Yager also associated to W two functions from Ω to [0, 1], called the verity and rebuff distributions, and defined asfollows:F (x),Ver(x) = minF ∈WRebuff(x) = 1 − maxF ∈WF (x) = minF ∈W1 − F (x) = minF ∈WF (x).Ver(x) is thus the minimal degree of membership of x to any possible value of X : it can be interpreted as the minimalsupport for x being one of the values taken by X . In contrast, Rebuff(x) can be interpreted as the minimal support for xnot being one of the values taken by X . These two distributions have the following expressions for the three basic types ofstatements:−→ Ver(x) = A(x), Rebuff(x) = 0,X isv AX isv(n) A −→ Ver(x) = 0,X isv(c) A −→ Ver(x) = A(x), Rebuff(x) = 1 − A(x).Rebuff(x) = A(x),Clearly, a major difference between Yager’s approach and ours is the fact that Yager represents each piece of knowledgeabout X as a set of fuzzy subsets of Ω , whereas we use a set of crisp subsets of Ω . However, the kinds of statementsconsidered by Yager as well as the associated verity and rebuff distributions have very close representations in our approach.To begin with, let us provisionally assume that A is a crisp subset of Ω . Then, each of the three types of statements canbe expressed by categorical mass functions on C(Ω) as follows:−→ m( A, ∅) = 1,X isv AX isv(n) A −→ m(∅, A) = 1,X isv(c) A −→ m( A, A) = 1.T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499It is easy to see that, in each of these three cases:(cid:11)(cid:11)(cid:10){x}, ∅(cid:10)∅, {x}bb= Ver(x),= Rebuff(x)493(64)(65)for all x ∈ Ω . The verity of x is thus the belief that x is one of the values taken by X , whereas the rebuff of x is the beliefthat x is not a value taken by X . This interpretation can be shown to remain true when A is a fuzzy subset of Ω . In thatcase, the function x → A(x) can be seen as a possibility distribution, which is known to be equivalent to a consonant massfunction m(cid:10)on Ω with focal elements A1 ⊆ · · · ⊆ An. The corresponding plausibility function pl(cid:11)verifies(cid:2)(cid:15)(cid:15)(cid:15)pl{x}=(cid:15)mAi (cid:28)x( Ai) = A(x), ∀x ∈ Ω.For instance, let us consider the statement X isv A, and let us translate it as the following two-place mass function:m( Ai, ∅) = m(cid:15)( Ai),i = 1, . . . , n.We have(cid:10){x}, ∅b(cid:11)=(cid:2)Ai (cid:28)xm( Ai, ∅) =(cid:2)Ai (cid:28)xand(cid:11)(cid:10)∅, {x}b= 0 = Rebuff(x).(cid:15)m( Ai) = A(x) = Ver(x)By handling the two other cases similarly, it can be verified that Eqs. (64) and (65) hold in all cases.We may thus conclude that, although based on a slightly different interpretation, Yager’s framework can be easily trans-lated into the formalism of two-place belief functions, which is more general. However, this is only true at the static level,i.e., as long as we do not combine different pieces of information. For instance, as shown by Yager, the conjunctive com-bination of two statements X isv A and X isv B in the veristic framework results in a new statement X isv A ∪ B, where ∪denotes fuzzy set union. This is consistent with our approach only as long as A and B are crisp sets. If A and B are fuzzy,then translating the two statements as two-place mass functions and combining them using either the conjunctive sum orthe cautious rule does not, in general, yield a consonant mass function corresponding to a veristic constraint on X . The twoformalisms thus differ when combining statements involving fuzzy subsets.5.4. Two-fold fuzzy setsTo complete this review of previous work on uncertainty representation for set-valued variables, we need to mention therepresentation of incomplete conjunctive information using a pair of fuzzy sets introduced in [9].In this work, Dubois and Prade proposed to represent partial knowledge about a set-valued variable as a possibilitydistribution π on 2Ω . This is equivalent to defining a fuzzy set of crisp subsets of Ω , which contrasts with Yager’s approachwho defines a crisp set W of fuzzy subsets of Ω . To make such a representation more easily tractable, Dubois and Prade+) as follows. Let Ai , i ∈ I , be the family of subsets of Ω−, Athen proposed to approximate π by a pair of fuzzy sets ( Asuch that π ( Ai) > 0. Let(x) = 1 − supi: x /∈ Aiπ ( Ai)A−and+A(x) = supi: x∈ Aiπ ( Ai).−+(x)The degree of membership of x to A+), referred to as a two-fold fuzzy set, constitutescorresponds to the possibility of finding an Ai containing x. The pair ( Aan approximation of π in the sense that it is a simpler, but incomplete representation: several possibility distributions πcorrespond to the same two-fold fuzzy set. However, Dubois and Prade showed that the least specific possibility distributionπ ∗is thus the extent to which is impossible to find an Ai not containing x, while A+) can be expressed as π ∗(∅) = 1 − sup A, π ∗(Ω) = inf A−, A−, A, and−+induced by a two-fold fuzzy set ( A(cid:24)(B) = minπ ∗+(cid:10)(cid:11)(cid:25)−1 − A, ∀B ∈ 2Ω \ {∅, Ω}.A(x)infx∈BTo each two-fold fuzzy set ( A(x), infx /∈B+) can thus be associated a fuzzy subset A of 2Ω , with membership function equal to π ∗.We note that this approach has some similarity with ours, since it is based on the representation of a subset of 2Ω byare crisp, then the corresponding crisp subset A of 2Ω is exactly equal to−, A+). However, in the general case, the two-fold fuzzy set representation is based on a pair of possibility distributions,a pair of subsets of Ω . Actually, if Aϕ( Ai.e., consonant belief functions on Ω , whereas our approach is based on a single two-place belief function on C(Ω).−, Aand A−+494T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499−, B−, A−, A− ∪ BWhat can be seen as a limitation of the two-fold fuzzy set approach arises when combining information from several+) and (B+) representing knowledge about two set-valued variables X and Y , Duboissources. Given two pairs ( A+), while the knowledge of X ∪ Y can+ ∩ B−, Aand Prade showed that the knowledge of X ∩ Y can be represented by ( A+). Applications of this kind of reasoning to database query evaluation is discussed in [9].be represented by ( AHowever, a different and maybe more common problem in uncertain reasoning is the situation where we have two items+) andof evidence about a single set-valued variable X , and we want to combine these two items of evidence. If ( A+) correspond, respectively, to fuzzy subsets A and B of 2Ω , the result of the combination should ideally correspond(Bto A ∩ B or to A ∪ B, depending on the choice of a conjunctive or disjunctive combination mechanism. However, none ofthese two fuzzy subsets of 2Ω generally admits a two-fold fuzzy set representation, which restricts the use of this formalismfor reasoning with set-valued variables.− ∩ B+ ∪ B−, A−, BWe have shown that the formalism of two-place belief functions introduced in this paper seems to compare favorably interms of expressive power with existing formalisms for representing and reasoning with uncertain conjunctive information.In the next section, we will demonstrate the usefulness of this formalism for a certain category of classification problems.6. Application to multi-label classificationIn this section, we present an application of the framework developed in this paper to multi-label classification.2 Inthis kind of problems, each object may belong simultaneously to several classes, contrary to standard single-label problemswhere objects belong to only one class [13,38,12,36]. Multi-label classification tasks arise in many real-world problems. Forinstance, in image retrieval, each image may belong to several semantic classes such as beach and urban. In text categoriza-tion, each document may belong to several topics, etc. In such problems, the learning task consists in predicting the valueof the class variable for a new instance, based on a training set. As the class variable is set-valued, the framework developedin this paper may be used.In order to construct a multi-label classifier, we generally assume the existence of a labeled training set, composed ofn examples (xi, Y i), where xi is a feature vector describing instance i, and Y i is a label set for that instance, defined asa subset of the set Ω of classes. In practice, however, gathering such high quality information is not always feasible at areasonable cost. In many problems, there is no ground truth for assigning unambiguously a label set to each instance, andthe opinions of one or several experts have to be elicited. Typically, an expert will sometimes express lack of confidencefor assigning exactly one label set. If several experts are consulted, some conflict will inevitably arise, which again willintroduce some uncertainty in the labeling process.The formalism developed in this paper can easily be used to handle such situations. In the most general setting, theopinions of one or several experts regarding the set of classes that pertain to a particular instance i may be modeled bya mass function mi on C(Ω). A less general, but arguably more workable option is to restrict mi to be categorical, i.e., tohave a single focal element ϕ( Ai, B i), with Ai, B i ⊆ Ω and Ai ∩ B i = ∅. The set Ai is then the set of classes that certainlyapply to example i, while B i the set of classes that certainly do not apply. In a multiple expert setting, Ai might representthe set of classes indicated by all (or most) experts as relevant to describe instance i, while B i would be the set of classesmentioned by none of the experts (or only a few of them). The usual situation of precise labeling is recovered in the specialcase where B i = Ai .For instance, assume that instances are songs and classes are emotions generated by these songs, as in the emotiondataset that will be used in Section 6.3 below. Upon hearing a song, an expert may decide that this song certainly evokeshappiness and certainly does not evoke sadness, but may be undecided regarding the other emotions (such as quietness,anger, surprise, etc.). In that case, the song cannot be assigned a single label set, but we can associate to it the set of alllabel sets containing “happiness” and not containing “sadness”, which has the form suggested above.In [4,39], we introduced a single-label k-nearest neighbor (NN) classifier based on Dempster–Shafer theory. This methodwill be briefly recalled in Section 6.1, and will be extended to multi-label classification tasks in Section 6.2. An experimentalcomparison with the multi-label k nearest neighbor (ML-kNN) method introduced in [38] using real-world data will thenbe presented in Section 6.3.6.1. Single-label evidential k-NN classificationThe evidential k-NN method introduced in [4] for single-label classification problems can be summarized as follows. LetL = {(x1, A1), . . . , (xn, An)} be a learning set of n instances, where xi is a p-dimensional attribute vector describing instancei, and Ai ⊆ Ω = {ω1, . . . , ωK } is a set of possible classes for instance i. We emphasize the fact that, in the context consideredhere, each instance i actually belongs to one and only one class, but this class is only known to lie somewhere in Ai .Let x denote the feature vector for a new object with unknown class y. We want to guess the value of y based onevidence provided by the learning set L. For that purpose, we consider the set Φk(x) of the k nearest neighbors of x,according to some distance measure d (usually, the Euclidean one). Each learning instance (xi, Ai) with xi ∈ Φk(x) can then2 A preliminary version of the application described in this section was presented in [37].T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499495be regarded as a piece of evidence regarding the unknown value of y, represented as the following simple mass functionon Ω :mi( Ai) = α expmi(Ω) = 1 − α exp(cid:10)(cid:11)−γ d(x, xi),(cid:10)(cid:11)−γ d(x, xi),(66)(67)with 0 < α < 1 and γ > 0. Parameter α is usually fixed at a value close to 1 such as α = 0.95, whereas γ should dependon the scaling of distances and can be either fixed heuristically or optimized [39]. The evidence of the k NNs is then pooledusing the conjunctive sum:m = ∩(cid:12)i: xi ∈Φ(x)mi,(68)and the class with highest plausibility or pignistic probability is selected. As remarked in [7] and [6], this method can beeasily extended to the case where each learning instance in L is labeled by a general mass function on Ω .6.2. Multi-label evidential k-NN classificationLet us now come back to the multi-label classification problem, in which objects may belong simultaneously to severalclasses. Let L = {(x1, A1, B1), . . . , (xn, An, Bn)} be the learning set, where Ai ⊆ Ω = {ω1, . . . , ωK } denotes a set of classesthat surely apply to instance i, and B i ⊆ Ω a set of classes that surely do not apply to the same instance. If Y i ⊆ Ω denotesthe true label set of instance i, we thus only know that Y i ∈ ϕ( Ai, B i).As before, let Φk(x) denote the set of k nearest neighbors of a new instance described by feature vector x, and xi anelement of that set with label ( Ai, B i). This item of evidence can be described by the following simple two-valued massfunction:mi( Ai, B i) = α expmi(∅, ∅) = 1 − α exp(cid:10)(cid:11)−γ d(x, xi),(cid:10)(cid:11)−γ d(x, xi),(69)(70)with, as before, 0 < α < 1 and γ > 0. These k mass functions are then combined using the conjunctive sum (68) as in thesingle-label case.For decision making, different procedures can be used. The following simple and computationally efficient rule wasimplemented. Let (cid:26)Y be the predicted label set for instance x. To decide whether to include each class ω ∈ Ω or not, wecompute the degree of belief bel({ω}, ∅) that the true label set Y contains ω, and the degree of belief bel(∅, {ω}) that itdoes not contain ω. We then define (cid:26)Y as(cid:26)Y =(cid:20)ω ∈ Ω | bel(cid:10){ω}, ∅(cid:11)(cid:10)(cid:2) bel∅, {ω}(cid:11)(cid:21).6.3. ExperimentsTo study the above procedure experimentally, three real datasets3 were used:• The emotion dataset, presented in [31], consist of 593 songs annotated by experts according to the emotions they gen-erate. The emotions are: amazed-surprise, happy-pleased, relaxing-calm, quiet-still, sad-lonely and angry-fearful. Eachemotion corresponds to a class. There are thus 6 classes, and each song was labeled as belonging to one or severalclasses. The average size of the label set for each song is 1.87 ± 0.67. Each song was also described by 8 rhythmic fea-tures and 64 timbre features, resulting in a total of 72 features. The data was split into a training set of 391 examplesand a test set of 202 examples.• The yeast dataset contains data regarding the gene functional classes of the yeast Saccharomyces cerevisiae [11,25]. Itdescribes 2417 genes each represented by 103 features. There are 14 possible classes and the average size of the labelset for each gene is 4.24 ± 1.57. The data was split into a learning set of 1500 examples and a test set of 917 examples.• The scene dataset consists of 2407 natural scene images, where a label set is manually assigned to each image. Thereare 6 classes and 294 attributes. The average cardinality is 1.074 ± 0.26 (only 7.35% of observations are labeled by morethan one class). The data was split into a training set of 1211 examples and a test set of 1196 examples.Each of these three datasets was constructed in such a way that each instance i is assigned a single set of labels Y i . Asexplained above, this choice may be questioned since, at least for the emotion and scene datasets, there is no ground truthand the data have been labeled subjectively by a pool of experts. To assess the performances of our approach in learningfrom data with imprecise labels such as postulated in Section 6.2 above, we randomly simulated an imperfect labeling processby proceeding as follows.3 These datasets can be downloaded from http://mlkd.csd.auth.gr/multilabel.html.496T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499Fig. 2. Mean accuracy (plus or minus one standard deviation) over 5 trials as a function of k for the emotions dataset with the following methods: EML-kNNwith imprecise labels ( Ai , B i ), EML-kNN with noisy labels and ML-kNN with noisy labels.Let yi = ( yi1, . . . , yi K ) be the vector of {−1, 1}K such that yik = 1 if ωk ∈ Y i and yik = −1 otherwise. For each instancei and each class ωk, we generated a probability of error pik between 0 and 0.5 by drawing a random number from a betadistribution with parameters a = b = 0.5 (this is a bimodal distribution with modes at 0 and 1) and dividing it by two. We(cid:15)then changed yik to − yik with probability pik, resulting in a noisy label vector yi . The imprecise label vector was finallydefined as y(cid:15)(cid:15)= ( yi(cid:22)(cid:15)iky0(cid:15)(cid:15)i K ) with(cid:15)(cid:15)i1, . . . , yif pik < 0.2,otherwise.(cid:15)(cid:15)iky=(cid:15)(cid:15)ik(cid:15)(cid:15)ikAs remarked in Section 4.1, such a vector of {−1, 0, 1}K encodes an ordered pair ( Ai, B i) of disjoint subsets of Ω such thatAi = {ωk ∈ Ω | y= 1} and B i = {ωk ∈ Ω | y= −1}.The intuition behind the above model may be described as follows. Each number pik represents the probability that themembership of instance i to class ωk will be wrongly assessed by the expert. This number may be turned into a degree ofconfidence ci by the transformation cik = 1 − 2pik. We assume that these numbers can be provided by the expert, whichallows us to label each instance i by a pair of sets ( Ai, B i). The set Ai then contains the classes ωk that can be definitelyassigned to instance i with a high degree of confidence (cik (cid:2) 0.6), while B i is the set of classes which are definitely notassigned to instance i. The remaining set Ω \ ( Ai ∪ B i) contains those classes about which the expert is undecided (cik < 0.6).(cid:15)i and withimprecise labels ( Ai, B i). The features were normalized so as to have zero mean and unit variance. Parameters α and γwere fixed at 0.95 and 0.5, respectively, for all three datasets. We note that γ could easily be determined automaticallyby cross-validation. However, the results are not very sensitive to the value of γ , so that this parameter could be fixedmanually.Our method (hereafter referred to as EML-kNN) was applied to the three datasets, both with noisy labels yAs a reference method, we used the ML-kNN method introduced in [38], which was shown in [38] to have good per-formances as compared to most existing multi-label classification algorithms. It is also the closest to our method, as bothmethods are based on nearest neighbors. The ML-kNN algorithm was applied to noisy labels only, as it is not clear howimprecise labels could be handled using this method.For evaluation, we used accuracy as a performance measure, defined as:Accuracy = 1nn(cid:2)i=1|Y i ∩ (cid:26)Y i||Y i ∪ (cid:26)Y i|,where n is the number of test examples, Y i is the true label set for examples i, and (cid:26)Y i is the predicted label set for thesame example. This measure takes values between 0 and 1, with higher values corresponding to better performance.Figs. 2 to 4 show the mean accuracy plus or minus one standard deviation over five generations of noisy and impreciselabels for the three datasets, with the following methods: EML-kNN with imprecise labels ( Ai, B i), EML-kNN with noisylabels and ML-kNN with noisy labels. The results are consistent over the three datasets: the EML-kNN method with noisylabels outperforms the ML-kNN trained using the same data, while the EML-kNN algorithm with imprecise labels clearlyyields the best performances for the three problems.T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499497Fig. 3. Mean accuracy (plus or minus one standard deviation) over 5 trials as a function of k for the yeast dataset with the following methods: EML-kNNwith imprecise labels ( Ai , B i ), EML-kNN with noisy labels and ML-kNN with noisy labels.Fig. 4. Mean accuracy (plus or minus one standard deviation) over 5 trials as a function of k for the scene dataset with the following methods: EML-kNNwith imprecise labels ( Ai , B i ), EML-kNN with noisy labels and ML-kNN with noisy labels.These preliminary results demonstrate the ability of our approach to handle imprecise labels in multi-label classificationtasks. More generally, they illustrate a practical situation where mass functions on a lattice (C(Ω), ⊆) are a natural modelfor expert knowledge and can be successfully exploited for uncertain reasoning with set-valued variables.It should be noted, however, that these encouraging results are only a first step towards a comprehensive assessmentof our approach in multi-label classification tasks. A more complete study would require more extensive comparisons witha wider range of algorithms and datasets, and more sophisticated schemes for tuning hyperparameters. Such a study goesbeyond the scope of the present paper, and is left for future work.7. ConclusionWe have presented a formalism for quantifying uncertainty on a set-valued variable X defined on a domain Ω in thebelief function framework. This approach relies on the definition of a family C(Ω) of subsets of 2Ω that is closed underintersection and has a lattice structure. Each element C in this family is indexed by two subsets A and B, and is defined498T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499as the set of subsets of Ω containing A and not intersecting B. The number of such elements (including the empty set of2Ω ) is equal to 3K + 1, where K is the size of Ω : it is thus much smaller than the size of 22Ω, while being rich enough toexpress evidence about X in many realistic situations.Using recent results about belief functions on general lattices reported in [14], we have shown that most notions fromDempster–Shafer theory can be defined on C(Ω) with only a moderate increase in complexity as compared to the single-valued case, which contrasts with the double-exponential complexity encountered when working in 22Ω. This formalismhas been shown to be more general than previous attempts to apply the Dempster–Shafer framework to this problem. Ithas also been shown to be somewhat similar to, but arguably more general and flexible than other approaches introducedin the possibilistic framework.Finally, our formalism has been applied to multi-label classification with imprecise labels, using an extension of thesingle-label evidential k nearest neighbor rule. Preliminary experimental results with real data and simulated uncertainlabeling suggest that the proposed approach allows for the development of powerful classification procedures and canbe applied to solve complex real-world problems. Further investigations into the belief function approach to multi-labelclassification, including extensive comparison with other methods, are currently under way and will be reported in futurepublications.References[1] S. Aguzzoli, B. Gerla, V. Marra, De Finetti’s no-Dutch-book criterion for Gödel logic, Studia Logica 90 (1) (2008).[2] R.L.O. Cignoli, I.M.L. D’Ottaviano, D. Mundici, Algebraic Foundations of Many-Valued Reasoning, Trends in Logic – Studia Logica Library, vol. 7, KluwerAcademic Publishers, Dordrecht, 2000.[3] B.R. Cobb, P.P. Shenoy, On the plausibility transformation method for translating belief function models to probability models, International Journal ofApproximate Reasoning 41 (3) (2006) 314–330.[4] T. Denœux, A k-nearest neighbor classification rule based on Dempster–Shafer theory, IEEE Transactions on Systems, Man and Cybernetics 25 (05)(1995) 804–813.[5] T. Denœux, Conjunctive and disjunctive combination of belief functions induced by non-distinct bodies of evidence, Artificial Intelligence 172 (2008)234–264.[6] T. Denœux, P. Smets, Classification using belief functions: the relationship between the case-based and model-based approaches, IEEE Transactions onSystems, Man and Cybernetics B 36 (6) (2006) 1395–1406.[7] T. Denœux, L.M. Zouhal, Handling possibilistic labels in pattern classification using evidential reasoning, Fuzzy Sets and Systems 122 (3) (2001) 47–62.[8] D. Dubois, H. Prade, A set-theoretic view of belief functions: logical operations and approximations by fuzzy sets, International Journal of GeneralSystems 12 (3) (1986) 193–226.[9] D. Dubois, H. Prade, On incomplete conjunctive information, Computers and Mathematics with Applications 15 (10) (1988) 797–810.[10] D. Dubois, H. Prade, Representation and combination of uncertainty with belief functions and possibility measures, Computational Intelligence 4 (1988)244–264.[11] A. Elisseeff, J. Weston, A kernel method for multi-labelled classification, in: T.G. Dietterich, S. Becker, Z. Ghahramani (Eds.), Advances in Neural Infor-mation Processing Systems, vol. 14, MIT Press, 2002, pp. 681–687.[12] J. Fürnkranz, E. Hüllermeier, E.L. Mencia, K. Brinker, Multilabel classification via calibrated label ranking, Machine Learning 73 (2) (2008) 133–153.[13] S. Godbole, S. Sarawagi, Discriminative methods for multi-labeled classification, in: Proceedings of the 8th Pacific–Asia Conference on KnowledgeDiscovery and Data Mining, PAKDD 2004, Sidney, Australia, 2004, pp. 22–30.[14] M. Grabisch, Belief functions on lattices, International Journal of Intelligent Systems 24 (2009) 76–95.[15] M. Grabisch, C. Labreuche, Bi-capacities – I: definition, Möbius transform and interaction, Fuzzy Sets and Systems 151 (2005) 211–236.[16] T. Kroupa, Conditional probability on MV-algebras, Fuzzy Sets and Systems 149 (2) (2005) 369–381.[17] T. Kroupa, Representation and extension of states on MV-algebras, Archive for Mathematical Logic 45 (4) (2006) 381–392.[18] T. Kroupa, Belief functions on formulas in Lukasiewicz logic, in: T. Kroupa, J. Vejnarová (Eds.), 8th Workshop on Uncertainty Processing (WUPES ’09),Liblice, Czech Republic, 2009.[19] C. Labreuche, M. Grabisch, Modeling positive and negative pieces of evidence in uncertainty, in: T.D. Nielsen, N.L. Zhang (Eds.), Symbolic and Quanti-tative Approaches to Reasoning with Uncertainty (Proceedings of ECSQARU ’03), Aalborg, Denmark, Springer, 2003, pp. 279–290.[20] G. Matheron, Random Sets and Integral Geometry, Wiley, New York, 1975.[21] B. Monjardet, The presence of lattice theory in discrete problems of mathematical social sciences. Why, Mathematical Social Sciences 46 (2) (2003)103–144.[22] D. Mundici, Averaging the truth-value in Lukasiewicz logic, Studia Logica 55 (1) (1995) 113–127.[23] H. Nguyen, An Introduction to Random Sets, Chapman and Hall/CRC Press, Boca Raton, Florida, 2006.[24] H.T. Nguyen, On random sets and belief functions, Journal of Mathematical Analysis and Applications 65 (1978) 531–542.[25] P. Pavlidis, J. Weston, J. Cai, W.N. Grundy, Combining microarray expression data and phylogenetic profiles to learn functional categories using supportvector machines, in: Proceedings of the Fifth Annual International Conference on Computational Biology, 2001, pp. 242–248.[26] G. Shafer, A Mathematical Theory of Evidence, Princeton University Press, Princeton, NJ, 1976.[27] P. Smets, The Transferable Belief Model and random sets, International Journal of Intelligent Systems 7 (1992) 37–46.[28] P. Smets, Belief functions: the disjunctive rule of combination and the generalized Bayesian theorem, International Journal of Approximate Reasoning 9(1993) 1–35.[29] P. Smets, The canonical decomposition of a weighted belief, in: Int. Joint Conf. on Artificial Intelligence, Morgan Kaufman, San Mateo, CA, 1995,pp. 1896–1901.[30] P. Smets, R. Kennes, The transferable belief model, Artificial Intelligence 66 (1994) 191–243.[31] K. Trohidis, G. Tsoumakas, G. Kalliris, I. Vlahavas, Multi-label classification of music into emotions, in: Proc. 9th International Conference on MusicInformation Retrieval (ISMIR 2008), Philadephia, PA, USA, 2008.[32] R.R. Yager, On different classes of linguistic variables defined via fuzzy subsets, Kybernetes 13 (1984) 103–110.[33] R.R. Yager, Set-based representations of conjunctive and disjunctive knowledge, Information Sciences 41 (1987) 1–22.[34] R.R. Yager, Reasoning with conjunctive knowledge, Fuzzy Sets and Systems 28 (1988) 69–83.[35] R.R. Yager, Veristic variables, IEEE Transactions on Systems, Man and Cybernetics B 30 (1) (2000) 71–84.[36] Z. Younes, F. Abdallah, T. Denœux, Multi-label classification algorithm derived from k-nearest neighbor rule with label dependencies, in: 6th EuropeanSignal Processing Conference (EUSIPCO ’08), Lausanne, Switzerland, 2008.T. Denœux et al. / Artificial Intelligence 174 (2010) 479–499499[37] Z. Younes, F. Abdallah, T. Denœux, An evidence-theoretic k-nearest neighbor rule for multi-label classification, in: Proceedings of the 3rd InternationalConference on Scalable Uncertainty Management (SUM 2009), Washington, DC, USA, in: LNAI, vol. 5785, Springer-Verlag, 2009, pp. 297–308.[38] M.-L. Zhang, Z.-H. Zhou, ML-KNN: a lazy learning approach to multi-label learning, Pattern Recognition 40 (7) (2007) 2038–2048.[39] L.M. Zouhal, T. Denœux, An evidence-theoretic k-NN rule with parameter optimization, IEEE Transactions on Systems, Man and Cybernetics C 28 (2)(1998) 263–271.