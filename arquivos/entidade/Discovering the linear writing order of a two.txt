Artificial Intelligence 170 (2006) 409–421www.elsevier.com/locate/artintDiscovering the linear writing order of a two-dimensional ancienthieroglyphic scriptShou de Lin ∗, Kevin KnightInformation Sciences Institute, University of Southern California, USAReceived 8 February 2005; received in revised form 28 November 2005; accepted 6 December 2005Available online 17 January 2006AbstractThis paper demonstrates how machine learning methods can be applied to deal with a real-world decipherment problem wherevery little background knowledge is available. The goal is to discover the linear order of a two-dimensional ancient script, Hi-eroglyphic Luwian. This paper records a complete decipherment process including encoding, modeling, parameter learning,optimization, and evaluation. The experiment shows that the proposed approach is general enough to recover the linear orderof various manually generated two-dimensional scripts without needing to know in advance what language they represent and howthe two-dimensional scripts were generated. Since the proposed method does not require domain specific knowledge, it can beapplied not only to language problems but also order discovery tasks in other domains such as biology and chemistry. 2005 Elsevier B.V. All rights reserved.Keywords: Ancient script; Decipher; Luwian; Hieroglyphic; Unsupervised learning; Estimation-maximization; Linear order; Discovery; Writingsystem; Natural language process1. IntroductionThe Hieroglyphic Luwian script is a two-dimensional script discovered in middle Asia, preserved on rock datingback to 700–1300 BCE [4]. Unlike modern scripts that possess a clear linear order for reading (for example, left-to-right, top-down for English), the Luwian symbols seem not to be arranged regularly enough to indicate any specificwriting order (e.g., see the upper line in Fig. 1). This paper discusses a general process to discover the writing order(e.g., see the second line in Fig. 1) for this type of two-dimensional script.So far, there is no convincing evidence about the precise order of this script. A linguistic authority says “it is asystem which may leave in doubt the correct order of reading” [4]. Up to the present day there has not yet beenmuch effort focused on applying machine learning methods for decipherment. Knight and Yamada [5] propose a gen-eral framework to discover the text-to-speech relationships for unknown scripts by applying a finite-state transducermodel together with the EM (Expectation-Maximization) algorithm to learn parameters. Sproat [7] claims that theorthography of a language represents a consistent level of linguistic representation, and the mapping from this levelto surface spelling is a regular relation. He proposes to use a rule-based system (realized by finite state automata)* Corresponding author.E-mail addresses: sdlin@isi.edu (S. de Lin), knight@isi.edu (K. Knight).0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.12.001410S. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421Fig. 1. A snapshot of the two-dimensional Luwian script (upper) and one sample linear order of symbols (lower).to encode the regularities. Both works aim at finding the relationships among writing symbols and their sounds. Wehave not yet seen computational approaches for order discovery in any ancient writing system, including HieroglyphicLuwian.The challenges of discovering the linear order for unknown scripts are threefold:(1) Modeling: Without any knowledge or examples of linear order, we have to abandon enlisting help from eithersupervised training or rule writing. This task is as difficult as asking somebody who knows no English to figureout the linear order of a set of English characters written in two-dimensional manner. The challenge lies in howto model this problem as something that can be handled by machine using unsupervised techniques.(2) Complexity: In general the number of possible orderings increases exponentially with the number of symbols.Hence we face a huge search space.(3) Evaluation: As with all kinds of human discovery problems, there is no easily obtained linear data for this script,which makes it hard to verify the results.The next section describes how we model the linear-order discovery task as an unsupervised learning problem.We also provide an intuition indicating how the proposed approach resembles what human beings might do for thistask. In Section 3, we show how the model described in Section 2 can be applied (and refined) to deal with a real-world problem. Additionally, we will discuss how to reduce the search complexity to polynomial. We describe severalevaluation strategies and results in Section 4 and conclude in the last section.2. ModelingOur general strategy is as follows: we model the way the Luwians generated the script with Shannon’s noisy-channel model [6], which allows us to further translate the problem into a traveling-salesman-like problem. We thenapply the EM algorithm to learn the parameters (i.e., a Luwian language model) within the model. Finally we applythese parameters to compute the associated probability for each plausible order in order to extract the one with thehighest probability as the result.2.1. Noisy channel modelWe start by exploiting the noisy-channel model as shown in Fig. 2.We assume that the ancient people have the original linear script in mind before writing, and then they carve thescripts in a two-dimensional manner on the stone. When time passed (as did the Luwians), the only remains are thetwo-dimensional scripts on the rocks as observed. Based on Shannon’s model, the original linear script is the inputX to the noisy channel. The way the Luwians wrote down the script can be treated as a noisy channel that perturbsthe original one-dimensional source text X into a two-dimensional appearance Y . This noisy channel (represented asP(Y | X) here) is a black box to us since we do not know how those two-dimensional scripts were generated. OnceS. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421411Fig. 2. The noisy channel model representing how Luwian people wrote the script.Fig. 3. (a) The two-dimensional English. (b)–(d) Three arbitrary linear orders.the problem is modeled as a noisy channel, it is obvious that this order-discovery task can be represented as findingthe X that maximizes P(Y | X).This formula can further be decomposed into two components, P(X) and P(Y | X), by Bayes’ rule (see Eq. (1)).1P(X) can be generated by the language model of the script, which essentially stands for how frequently this scriptshould occur in a large sample of ancient Luwian literature. P(Y | X) can be treated as the noisy channel that representshow the Luwian people wrote down the script. Shannon’s model together with Bayes’ rule tells us that the desiredlinear order X should not only appear frequently in the Luwian literature (i.e., high P(X)), but also possess a highchance of producing the observed two-dimensional script Y .2.2. Linear order for known languageWe would like to demonstrate the idea described in Section 2.1 with a toy example: we assume that we are askedto find the linear order of a known two-dimensional language, say the English in Fig. 3(a), without being told how itwas generated.argmaxXp(X | Y ) = argmaxp(X) ∗ P(Y | X)X(1)According to Eq. (1), we need to search for an order X that maximizes P(X) ∗ P(Y | X). In this example, we ignoreP(Y | X), since we assume there is no indication of how this script was generated. Therefore we only need to maximizeP(X). In other words, whichever permutation of sequence appears to be valid English is a plausible candidate. Peoplecan scramble the characters mentally, and sooner or later they will find out it probably means “good job!” (Fig. 3(b)),as this permutation occurs more frequently to English speakers than others (e.g., gdbojoo! in 3(c) and goodj!ob in3(d)). What happens in the human mind mimics the procedure of maximizing P(X), where meaningful sentencespossess higher P(X) than meaningless ones.This toy example demonstrates that our model, to some extent, reflects how human beings solve the same puzzle.A machine can also perform such an analysis, if it knows which English orders are more probable than others. One wayto compute this is to exploit an n-gram English language model, which can be easily obtained by a simple statisticalanalysis of a large English corpus. Assuming that English letter-bigram probabilities are given, we can program themachine to enumerate all possible orders and compute their associated probabilities (e.g., p(X) = p(goodjob!) =p(g) ∗ p(o | g) ∗ p(o | o) ∗ p(d | o) ∗ p(j | d) ∗ p(b | j) ∗ p(b | o) ∗ p(! | b)). After that, we extract the sequence withthe highest probability as the result. The probability of 3(c) and 3(d) should be much lower than 3(b) since p(d | b)in 3(c) and p(o |!) in 3(d) do not occur too often in English. In this sense, if the bigram language model is known,then the linear-order discovering problem is similar to the traveling salesman problem in a graph (where the nodesrepresent letters and the weighted links represent the corresponding bigram probability between letters), except thatthe salesman does not have to go back to the origin.1 In fact there should be an extra term P(Y ) in Bayes’ rule deduction. However, P(Y ) can be ignored in the optimization process since it modelsthe probability of the observed output, which should be identical for all X.412S. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421Fig. 4. Applying EM to refine the language model and P(X) recursively.2.3. EM for unknown scriptsSection 2.2 shows that it is possible to recover the linear order of a known script from its two-dimensional form.However, this approach is not applicable to the decipherment of the Luwian script, whose language model is unknown.On the other hand, when the order probability P(X) is known, it is possible to apply the method of “fractionalcounting” to generate the n-gram probabilities by counting relatively how many times each n-gram appears in eachpossible ordering. For example, if there are three possible orders with associated probability 20%, 30%, and 50%, andthe bigram P(e | s) occurs once, once, and twice respectively in these three sequences, the fractional count of P(e | s)will be 1 ∗ 20% + 1 ∗ 30% + 2 ∗ 50%. Finally, the language model can be constructed by normalizing the fractionalcount values. Statistically speaking, “learning the language model” and “learning the associated probability for theordered sequences P(X)” are dual problems, in the sense that one can provide sufficient information to understandthe other. Unfortunately we know neither for Luwian. In this case we propose to use the idea of EM [2] to learn bothmodels simultaneously, as shown in Fig. 4.For the initialization, we assign certain probabilities (e.g., a uniform distribution) to all the n-gram parametersin the language model. In the M step of EM, we enumerate all the possible linear orders X and use the imperfectlanguage model learned so far to generate the associated probabilities P(X). In the E step of EM, we count howfrequently each n-gram appears for all possible orders X, weighted by their associated probability P(X), to generatea refined language model. Then the new language model can be exploited to create a more precise P(X). It has beenproven that if we execute the E step and M step iteratively, EM will ultimately converge on a locally optimal solution(i.e., the parameter settings for the language model will lead to a locally optimal P(X)). Once the language model islearned, we can finally apply it to compute the associated probability for each order and extract the most probable one.2.4. DiscussionIn fact, what our program does is similar to asking a human to learn the linear order for some script he or shehas never seen. Why is it intuitively possible at all? In this section, we would like to provide an insight showing thatour proposed method is a generalization of one plausible decipherment process. Thinking of what methods humanbeings might use to deal with such an ordering-decipherment task, first we might try to group symbols that co-occurfrequently in the same neighborhood. Based on the co-occurrence frequencies, we might be able to hypothesize someplausible order in certain less ambiguous areas (e.g., corner areas). Then we could propagate the order informationfrom those less-ambiguous areas to the remaining areas and refine our belief in the co-occurrence frequencies. Thesesteps could be applied repeatedly either until one plausible order is determined or until we learn that the currenthypotheses are not applicable to the remaining areas, in which case we would need to backtrack by choosing otherhypotheses.This decipherment process is a simplified version of what our program does. The process of choosing the mostplausible direction based on current beliefs (e.g., what symbols co-occur with others frequently) corresponds to the“maximization” step in the expectation-maximization (EM) algorithm. Using the currently plausible order informationto modify the existing beliefs resembles the “estimation” step in EM. The major difference is that human beings cantake only a small amount of highly possible choices into account, but the machine is able to take all situations intoaccount (and furthermore, weight them by the possibilities of occurrence) to make a better decision.S. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421413Fig. 5. The symbols with faces in adjacent rows always face oppositely.The power of our approach comes from its ability to capture hidden subtle regularities. Although the programknows nothing about a language in the beginning, it starts to absorb information and refine its understanding aboutthe language after more and more perturbed scripts are seen.3. DecipheringWhile Section 2 illustrates the basic idea of applying the noisy-channel model and EM to handle the order-discoveryproblem, in this section we address practical issues we have faced while implementing the deciphering program.3.1. AssumptionsIn order to simplify the computation, we make two assumptions. The first assumption is that the Luwians alwayswrite the next symbol in the neighborhood of the current symbol. The definition of neighborhood symbols (say, Xand Y ) is twofold: the straight line between the center of X and Y cannot touch any other symbol, and X and Yhave to be in the same or adjacent columns (e.g., c1, c2, and c3 in Fig. 5 are columns). Under this assumption wecannot jump across a symbol to connect to the next symbol. This assumption is plausible since it is the case for mostwriting systems.2 With this assumption, complexity can be reduced from O(n!) to O(bn), where b is the averagebranching factor (average number of neighbors for Luwian Hieroglyphics per symbol), since for each symbol thereare on average b candidates to be its successor.Furthermore, linguists agree that the big picture (external appearance) follows the so-called “boustrophedon”,which means that alternate rows progress in opposite directions, as in plowing a field, shown by the dotted line inFig. 5. This hypothesis is convincing, since the symbols with human or animal shapes tend to face different directionsin subsequent rows, as circled in Fig. 5. This assumption implies that we cannot move backward horizontally withinone row. In the boustrophedon writing the first row can be either left-to-right or right-to-left. We choose “right-to-left”in the experiment because in the data we used there seems to be a clear starting point in the upper-right corner of thescript.With these two assumptions, one can further reduce the total number of plausible orders to O(2k) where k is thetotal number of columns. Since the external appearance is known, our program has to determine only the preciseprogress direction locally. The first assumption implies that there are only two choices (top-down or bottom-up)for each column, and thus the plausible number of orders is O(2k). It is still exponential, but as will be shown inSection 3.4, these two assumptions allow us to apply the forward-backward algorithm for fractional counting, furtherreducing the overall complexity down to polynomial.2 There are only very few writing systems that violate this assumption, for example, the “honorific inversion” for Egyptian writing.414S. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421Fig. 6. Digitizing the Luwian script.Fig. 7. The graph representation of the of the data in Fig. 6.3.2. EncodingEncoding of the Luwian text reproductions into computer friendly format was done manually. There are 9 rows(separated by rigid horizontal lines) and 753 symbols total (73 distinct) in the page we tested on. As will be shownin the evaluation section, 753 symbols are essentially enough for determining the order for many languages. Foreach symbol, we first recognize it (i.e., assign it a symbol identifier such as “arrow” or “carrot”) and then record itstwo-dimensional position (as shown in the middle table in Fig. 6).Once the positions of the symbols are determined, we can generate the relative direction from one symbol tothe other automatically (we use the clock direction as shown in the upper right part of Fig. 6). The true input toour discovery program is the table with recognized symbol identifiers and their relative directions to neighboringsymbols, as demonstrated in Fig. 7. The arrows show the plausible directions to proceed. For example, there is a two-way channel between symbols S1 (arrow) and S2 (underscores). Note that there is no link in between a middle nodeand a node in the other column (e.g., no connection between S4 and S1) since such progression will inevitably violatethe assumptions we make. Also, there is no cycle involving nodes from different columns, or one of our assumptionswill again be violated.3.3. Modeling the weight of the linksOnce the information is recorded as in Fig. 6, the next step is to generate the associated weight automaticallyfor each link such that the sum of the weights in a path reflects the possibility of occurrence for that path. We haveS. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421415two models for the weights from one symbol Wn to its neighbor Wn+1, borrowed and modified from what we havedescribed in Section 2.The first (“model 1”) is to model the weight as the bigram Luwian probability P(Wn+1 | Wn), which is exactly thesame as we used in Section 2.2. The limitation is that we ignore the noisy channel (i.e., how the Luwians generatedthe two-dimensional script) and rely completely on how frequently a sequence occurs in the literature.To compensate for this limitation, in the second model (“model 2”) we choose to use P(Wn+1 | Wn) ∗ P(Dn,n+1)to represent the weight. P(Dn,n+1) is the direction from the current symbol to the next one. The idea behind thismodel is that we assume the ancient Luwian people preferred progressing in some directions more than others, andthat this preference is independent of the symbols themselves. From the decipherment point of view, this model usestwo types of independent information to compensate for the weakness of each. That is, if several symbols are equallyprobable as successors, then the model will choose the one in the preferred direction; on the other hand, if severaldirections are equally probable for progressing, the model will pick the symbol that is most plausible to follow thecurrent one. For both models, we have to apply the EM algorithm to learn the associated parameters. For initialization,we tried uniform distribution for all weights as well as random assignments. The results show that EM converges tovery similar results for various initializations we have tried.3.4. ComplexityAs illustrated in Section 3.1, even under our two assumptions, the complexity of this deciphering task is stillexponential. This is because in the EM phase, we need to generate the probabilities for all 2k different orders beforeperforming the fractional counting. The script used for the experiment has on average 30 columns per row, andunfortunately, 230 is computationally intractable.However, we can benefit from our assumptions and do much more than ease the complexity from a higher orderexponential O(bn) to a lower level exponential O(2k). We have found that with these two assumptions, it is possibleto apply the forward-backward algorithm [1] for fractional counting, without having to list all plausible orders, whichreduces the complexity to polynomial.The forward-backward algorithm is a dynamic programming algorithm. We propose to use it to store the alphaand beta values for each link in the graph. The alpha value is the aggregated probability from the starting point tothe source of the link while the beta value is the aggregated probability from the destination of this link to the endingpoint. In other words, the alpha value of a link is the sum of all the path probabilities that reach this link from thestarting point, and the beta value is the sum of the probabilities of all paths leaving from the link and terminating atthe ending point. The fractional count of a link is equal to its own weight times its alpha value times its beta valuedivided by alpha[end], which is the alpha value of the last node.3There are two types of links in the graph. One is the link that progresses horizontally (e.g., link S3 → S5 in Fig. 8),the other is the link that progress vertically (e.g,. link S5 → S1). For the link traverse horizontally, its alpha valueinherits only from the previous vertical link that is in the same column (e.g., Eq. (2a) shows the alpha value for linkS3 → S5). Although there are three incoming links to node S3 (i.e., S1 → S3, S2 → S3, S4 → S3), the other two donot contribute to the alpha value of link S3 → S5, since coming from S1 or S2 to S3, the next node we have to visitis S4 instead of S5, under our second assumption. In other words, it is illegal to progress from S3 to S5 unless all theother nodes (i.e., S4) in the same column are visited, so the alpha value of S3 → S4 can be based on just the alphavalue of S4 → S3. Similarly the beta value of S3 → S5 can be generated from the beta value of S5 → S1 link, but notthe S5 → S6 or S5 → S4 link, because the second assumption tells us that after moving from S3 to S5, the next nodeto visit has to be S1 rather than S4 or S6 (see Eq. (2b)). Eqs. (2a) and (2b) tells us that for horizontal links, the alphavalue and beta value are independent of each other, thus the dynamic programming condition holds. Likewise, for thelinks that progress vertically, such as S5 → S1, the alpha value is the weighted sum of all the incoming alpha valuesfrom the previous column (Eq. (2c)). The beta value of S5 → S1 can be produced based on the beta value of S1 →S2 (Eq. (2d)). Eqs. (2c) and (2d) show that for a vertical link, its choice of previous route (i.e., alpha value) cannotaffect its choice of future route (i.e., beta value), thus forward-backward algorithm can be applied.3 Note that since there is one long sequence in our computation, the alpha[end] value for every fractional count is the same. Since the goal offractional count is to calculate the relative occurrence frequency of the links, it is not necessary for us to truly compute alpha[end] during thecomputation.416S. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421Fig. 8. How the alpha value propagates in the graph.Alpha(S3 → S5) = Alpha(S4 → S3) ∗ Pr(S3 | S4)Beta(S3 → S5) = Beta(S5 → S1) ∗ Pr(S1 | S5)Alpha(S5 → S1) = Alpha(S3 → S5) ∗ P(S5 | S3) + Alpha(S4 → S5) ∗ P(S5 | S4)Beta(S5 → S1) = Beta(S1 → S2) ∗ P(S2 | S1)(2a)(2b)(2c)(2d)The above analysis shows that given our two assumptions, both alpha and beta values can be propagated in a first-order Markov manner without interfering with each other. Once the alpha-beta values are computed and stored, wecan calculate the amount each specific link contributes to its associated bigram parameter be multiplying its transitionprobability with both the alpha and beta values. The ultimate bigram probability is the accumulation of all the identicalbigram link probabilities. Similarly, the fractional count for the P(Dn,n+1) table can be computed by aggregating theP(Dn,n+1) times alpha and beta.3.5. DecodingAs shown in the previous section, an alpha value depends only on its neighbor’s alpha values under our assumptions.Therefore, after the EM converges, we can apply dynamic programming again to extract the path of the highestprobability. The output order generated by model 2 on our experimental page is shown in Fig. 9. In the decodingphase, we modified the dynamic programming algorithm a bit by recording not only the best probability in each step,but also the second-best probability. Therefore, once the dynamic programming algorithm finishes walking throughthe nodes in the graph, we not only have the order with the highest probability, but also the second-best one, whichwe will need for further evaluation.4. EvaluationsEvaluation is always the trickiest part for any machine discovery problem. The chicken-and-egg paradox arisesin the sense that to evaluate the results, we need to know the precise order; on the other hand it is because that wewere not aware of the precise order therefore it is necessary to pursue such discovery. In this section we will describeseveral indirect strategies to evaluate our results.4.1. Checking the machine’s confidenceOne method of verification is to check how much confidence the machine has toward the order it picks. We proposeto measure confidence by comparing the probability between the best and the second-best order returned by themachine. In view of the fact that the machine makes its decision based on probability, closeness between the bestand second-best probabilities implies that they might be virtually the same from the machine’s point of view. Thissituation could indicate lack of data or under-fitting of the model. Table 1 records the best and second-best negativelog-probability for each row in each model.From this table one can learn that in the first model (bigram only) only 2/9 rows have distinguishable probabilitieswhile in model 2 (bigram plus independent direction) all the rows have distinguishable probability. This impliesS. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421417Fig. 9. The writing order discovered by our second model (9 rows, 753 symbols).that with only bigram information, the system does not feel comfortable with its choice. It gains more confidencewhen the direction factor is considered. Although this evidence does not necessarily show that the second model cangenerate accurate results, it does indicate that it provides sufficient information for the machine to make a confidentdecision.4.2. Checking the consistencyOne typical way to evaluate a natural language learning program is to check its learning curve over data, i.e., toexamine how much the accuracy can be improved when more data is added to training. In our problem, generatingthe learning curve over data is not possible because the gold standard answer is not available. However, we can drawa similar graph to see how the results change as new data are introduced. Fig. 10 shows how much the result alters asdata are added line by line. For both models, the similarity of the outputs remains steady and reaches 90% after the7th line is added, which implies a stabilization of the system.418S. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421Table 1The best and second-best log probabilities (bold pairs are distinguishable ones)Rowsrow1row2row3row4row5row6row7row8row9ModelsModel 1best1491931651931601501571781892nd150194165193161247260179190Model 2best2243062773092502402532882842nd230315282318257251257293292Fig. 10. How the system changes while the data increases.Fig. 11. Ten different patterns to generate two-dimensional scripts.4.3. Evaluating the methodologyInstead of verifying the results directly, in this section we propose to verify the methodology by checking whetherit works for known languages and writing systems. The basic idea is to first have someone collect a set of meaningfulsentences for various known languages, secretly overlay them one by one on top of symbols in the two-dimensionalLuwian scripts (without violating the two assumptions), then execute our program again to see if it can recover theoriginal linear order. This time we do know the true order for verification.We use 5 natural languages (Chinese, English, Arabic, Latin, Spanish) and one programming language (Java) fortesting. We collected a paragraph of writing for each language. We need only 753 letters (753 characters in Chinese)since the tested Luwian script has a total of 753 symbols. To generate two-dimensional scripts, we apply ten differentpatterns (Fig. 11). Note that the tenth pattern is a random one that does not follow any rules. Then we replace thesymbols on the Luwian page by the letters in the paragraph, as exemplified in Fig. 12.S. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421419Fig. 12. Replacing Luwian symbols with English letters according to pattern 1.Table 2The accuracy of 10 different writing patterns in model one and twoModelPattern12345678910Avg753 letters in Latin753 letters in English753 letter in Arabic753 letters in Spanish753 characters in Chinese753 letters in JavaLuwian ScriptsBaseline12121212121212Random guess73%78%77%74%77%78%76%75%85%85%84%84%84%86%87%85%79%98%76%97%77%99%72%82%87%83%85%85%100% 100% 91% 80% 87% 84% 89% 88% 79% 77% 88%76%76%77%78%79%89% 77% 82% 79% 85% 87% 74% 72% 84%98%74%76%73%71%84% 72% 78% 78% 79% 77% 68% 66% 79%95%76%75%76%77%88% 72% 75% 77% 80% 82% 75% 73% 82%99%65%60%70%66%76% 67% 68% 72% 71% 74% 67% 67% 73%87%82%85%87%86%84%100% 100% 90% 84% 83% 87% 88% 86% 83% 84% 89%52%60%54%52%52%54% 57% 65% 44% 45% 63% 53% 50% 54%80%50%50%50%50%49%28%50%69%56%66%66%56%67%50%56%55%67%61%50%50%50%50%72%76%50%76%85%75%85%72%50%85%76%86%75%76%76%84%These newly generated two-dimensional scripts (there are totally 60 different ones since we have 6 different lan-guages and each has 10 different ways of encoding) can then serve as the input to both models to generate the linearresults. We evaluate the results by counting how many of the columns returned by our system have the correct order.The results are shown in Table 2. Note that the baseline (randomly guess a direction at each decision point) reaches50% for all the patterns.Table 2 shows that both models achieve better performance than the baseline, for all writing patterns. For eachcase, it takes less than 1 minute for our program (running on 1 GHz PC) to converge to a stable result. If the scriptwas written according to pattern 1 or 2 (both have strong tendency for directions), the second model can discoverthe order almost perfectly except for Chinese. We believe it is not perfect in Chinese because unlike others, Chinesescript is logographic, and the number of distinct symbols in logographic writing is much higher than in alphabeticor syllabic systems. The left part of Table 3 describes the P(Dn,n+1) learned by model 2 for pattern 1. This showsthat the program does learn the fact that the system prefers progressing in the 6 o’clock direction much more than12 o’clock direction. Also, it learns another distinguishable factor for pattern 1, which is that the 10 or 11 o’clockdirection is preferable to 7 or 8 o’clock. The right column in Table 3 indicates that for pattern 3, the model learnedthat pr(6) is almost the same as pr(12), while pr(9) has a much higher probability than any other horizontal directions,which matches the intrinsic characteristic of that pattern. As to the other patterns that have less-strong preference forthe directions, model 2 still reaches 80% accuracy for most of the languages. Table 2 also points out that for model 1the accuracy ranges from 70% to 85% (except Chinese), depending on the language, but not on the writing pattern.This is reasonable, since only the language model is used there, and we did not model the noisy channel. It also tellsus that even if the script was written randomly as pattern 10, our approach could still reproduce the linear order withaccuracy higher than 75%.Next we show some linear English (written in pattern 3) recovered by model 2: “While Section 2 iullartsets thebasic ideas of combining the nosihc-y annel model and EM to handle the order-discovery problem, in this sectionwe uowl dleki to dadress mose practical issues we have faced while trying to implement thed icephering program.”420S. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421Table 3P(Dn,n+1) learned by model 2 in LatinPattern 1pr(6) = 0.620pr(7) = 0.001pr(8) = 0.005pr(9) = 0.030pr(10) = 0.123pr(11) = 0.224pr(12) = 0.001Pattern 3pr(6) = 0.312pr(7) = 0.020pr(8) = 0.031pr(9) = 0.259pr(10) = 0.048pr(11) = 0.024pr(12) = 0.308as well as the linear English (written in pattern 10) recovered by our model 1, which is the most difficult challengefor our program as well as for human beings: “While Section 2 illuartsets the basic edias oc fombining thon esihc-yennal moled and EM to handle the odrer-dicsove yrorplb, mein this section we uowl dlike to address some practicalisusse we have faced while yrting ti ompleme tnthed icerehping program.” The recovered scripts are not perfect butunderstandable.We perform a similar human study by giving human subjects4 two-dimensional scripts written in some languagethey do not know according to the above patterns, and ask them to figure out the linear order of each. The resultsshow that among ten samples we sent out, ninety percent of subjects perform better than 50%. The average accuracyis 58% while the highest is 73%, which is far below our program’s 79% average and 97% highest score. The resultdemonstrates that although people can learn certain regularity from the data and perform slightly better than thebaseline, to get better results in this task, the use of EM algorithm together with the powerful computational capabilityof a machine can be very helpful.The bottom of Table 2 demonstrates the accuracy of our Luwian result assuming that it was written according toeach of the ten previous patterns.5 For example, if it was written according to pattern 2, then the results our programgenerates would reach only 28% accuracy (since pattern 2 is written in bottom-up manner, while our discovery showsthat most of the lines are top-down). It is clear that pattern 1 possesses a much higher accuracy (80% top-down bymodel 2) than all the others. This shows that if we trust the validity of our program, then we can conclude that mostof the Luwian scripts are written top-down.After manually checking the results shown in Fig. 9, we found that most of our bottom-up writings come eitherfrom the blurred area or because the boundaries of columns are unclear. Our theory is that the Luwians might ingeneral follow a top-down manner during writing. However, every Luwian symbol has a different shape and size, sothere are situations now and then (e.g., a desire to make better use of the space) preventing the writer from followingthe general rule. Dr. Hawkins addressed similar issues by saying “signs arranged in one or more vertical columns fromtop to bottom. . . . But the signs come in a variety of shapes, long and slender, wide and flat etc. a problem for us is thatit is by no means always clear in what order the signs are to be read” [3]. The major contribution of this paper is notonly to strengthen the hypothesis that the general writing pattern is top-down, but also that our program can provide aplausible solution for the order in the ambiguous areas. It finds the regularities as well as the exceptions (areas wherethe writing pattern was abandoned), while the latter is a very hard task for human beings. Table 4 shows the learnedparameters for P(Dn,n+1) and certain bigram probabilities in Luwian Hieroglyphic.5. ConclusionIn this paper we describe an unsupervised method for discovering the linear order of the two-dimensional Hiero-glyphic Luwian script. We represent the problem as Shannon’s noisy-channel model and apply EM to learn the twodifferent parameter sets within the models. Finally, we apply dynamic programming algorithm to extract the mostprobable order.4 The subjects’ background including computer science, computational linguistics, psychology, and signal processing.5 Note that the sum of the probabilities for opposite patterns (e.g., pattern 1 and pattern 2) is sometimes more than 100%. It is because that thereare a few columns with only single symbol, and for these columns both top-down and bottom-up direction are regarded as correct.S. de Lin, K. Knight / Artificial Intelligence 170 (2006) 409–421421Table 4The P(Dir) and bigram probabilities for the Luwian scriptpr(6) = 0.498pr(7) = 0.009pr(8) = 0.004pr(9) = 0.146pr(10) = 0.086pr(11) = 0.138pr(12) = 0.122p(small | triangle) = 0.25p(four | triangle) = 0.249p(buffalo | triangle) = 0.249p(swan | triangle) = 0.250p(crocodile | three) = 0.250p(deer | three) = 0.249p(three | three) = 0.499We propose several ways to evaluate this discovery problem: We examine whether the machine has confidenceabout the results it produced by comparing the best and second-best results. The results imply that model 2 mightbe a better fit. We also generate a consistency curve to examine the sufficiency of the data. Finally, we evaluate ourmethodology by applying it to various known languages. The results show that our program, having neither any pre-requisite knowledge about the language nor how the two-dimensional script was generated, can still recover the linearscript with more than 80% accuracy. As to the Luwian scripts, our experiment shows that in general it was writtentop-down, but it also identifies certain exceptional areas.The lesson we have learned is that with the modern computational power and a proper model, machines are capableof grasping the hidden regularities and dealing with tasks that might be extremely difficult for human beings.The contributions of our study are not only about finding the writing order of an unknown ancient script, butalso to demonstrate how a general, unsupervised approach can be designed to deal with a situation where very littlebackground knowledge is available for learning. Furthermore we propose several indirect but general strategies toevaluate such a discovery system. Since no prior knowledge is necessary in our approach, it has potential to be appliednot only to natural language problems but also to tasks related to order discovery in areas such as biology, chemistry,and others.References[1] L.E. Baum, An inequality and associated maximization in statistical estimation for probabilistic functions of Markov processes, Inequali-ties 627 (3) (1972) 1–8.[2] A.D. Dempster, N.M. Laird, D.B. Rubin, Maximum likelihood for incomplete data via the EM algorithm, J. Roy. Statist. Soc. Ser. B 39 (1977)1–38.[3] J.D. Hawkins, Corpus of Hieroglyphic Luwian Inscription, vol. I, Walter de Gruyter, 1999.[4] J.D. Hawkins, The Luwians: Scripts and texts, in: C. Melchert (Ed.), The Luwians, Leiden, 2003.[5] K. Knight and K. Yamada, A computational approach to deciphering unknown scripts, in: Proceedings of the ACL Workshop on UnsupervisedLearning in Natural Language Processing, 1999.[6] C. Shannon, A mathematical theory of communication, Bell Syst. Tech. J. 27 (3) (1948) 379–423.[7] R. Sproat, A Computational Theory of Writing Systems, Cambridge University Press, Cambridge, 2000.