Artificial Intelligence 171 (2007) 1161–1173www.elsevier.com/locate/artintHuman-level artificial general intelligence and the possibility of atechnological singularityA reaction to Ray Kurzweil’s The Singularity Is Near,and McDermott’s critique of KurzweilBen GoertzelNovamente LLC, San Francisco, CA, United StatesAvailable online 10 October 2007AbstractAn analysis of Ray Kurzweil’s recent book The Singularity Is Near is given, along with Drew McDermott’s recent critique.The conclusion is that Kurzweil does an excellent job of fleshing out one particular plausible scenario regarding the future of AI,in which human-level AI first arrives via human-brain emulation. McDermott’s arguments against the notion of Singularity viaiteratively self-improving AI, as described by Kurzweil, are considered and found wanting. However, it is pointed out that thescenario focused on by Kurzweil is not the only plausible one; and an alternative is discussed, in which human-level AI arrives firstvia non-human-like AI’s operating virtual worlds.© 2007 Elsevier B.V. All rights reserved.Keywords: Strong AI; AGI; Self-modifying software; Singularity virtual worlds; Language acquisition1. Narrow AI versus AGIThe AI field started out with grand dreams of human-level artificial general intelligence. During the last half-century, enthusiasm for these grand AI dreams—both within the AI profession and in society at large—has risen andfallen repeatedly, each time with a similar pattern of high hopes and media hype followed by overall disappointment.Throughout these fluctuations, though, research and development have steadily advanced on various fronts within AIand allied disciplines.Averaging across the various historical fluctuations, we may generalize that the original vision of human-level AIhas been dampened over time due to various coupled factors, including most prominently• overoptimistic promises by early AI researchers, followed by failures to deliver on these promises [6,8];• a deeper understanding of the underlying computational and conceptual difficulties involved in various mentaloperations that humans, in everyday life, consider trivial and simple [23,24].E-mail address: ben@goertzel.org.0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.10.0111162B. Goertzel / Artificial Intelligence 171 (2007) 1161–1173These days most R&D carrying the label “AI” pertains to some sort of very narrowly-defined problem domain,shying away from the ambitious goals that are associated with AI in the popular media.Recently, in the first years of the 21st century, AI optimism has been on the rise again, both within the AI field andin the science and technology community as a whole. One possibility is that this is just another fluctuation—anotherinstance of excessive enthusiasm and hype to be followed by another round of inevitable disappointment. Anotherpossibility is that AI’s time is finally near, and what we are seeing now is the early glimmerings of a rapid growthphase in AI R&D, such as has not been seen in the field’s history to date.As evidence of the recent increase in AI optimism in some relevant circles, I note that the last few years haveseen an increasing number of conference special sessions and workshops focused on “Human-Level AI,” “ArtificialGeneral Intelligence” and related topics; for example (this is not a comprehensive list):• Integrated Intelligent Capabilities, Special Track of AAAI since 2006 (and planned to continue annually into theindefinite future);• Roadmap to Human-Level Intelligence. Special Session at WCCI, July-2006;• Building & Evaluating Models of Human-Level Intelligence, CogSci, July-2006;• Artificial General Intelligence Workshop, Bethesda MD, May-2006;• Between a Rock and a Hard Place: Cognitive Science Principles Meet AI-Hard Problems, AAAI Spring Sympo-sium Series, Mar-2006;• Towards Human-Level AI, NIPS Workshop, Dec-2005;• Achieving Human-Level Intelligence through Integrated Systems and Research, AAAI Fall Symposium Series,Oct-2004.In March 2008, the University of Memphis will host AGI-08, the first AI conference focusing specifically onartificial general intelligence and human-level AI, co-organized by the author and several colleagues, including StanFranklin, a long-established leader in the AI field. There have also been specific attempts to focus publications onhuman-level artificial general intelligence, e.g. a “Human-Level AI” issue of AI Magazine edited by Nick Cassimatis[5], and a series of edited volumes focused on AGI [15,16].And in the popular press, futurist pundits are more and more often heard proclaiming that the age of real AI is,this time, really coming soon—not like the false hopes of the past. The most widely-read example of this sort offuturist AI optimism is probably Ray Kurzweil’s recent book The Singularity Is Near (TSIN [21]), which projects theachievement of human-level AI by roughly 2029, followed by an AI-triggered radical transformation of mind, societyand economy by roughly 2045.Of course, the vast majority of academic and industry AI researchers remain deeply skeptical of the sort of opti-mistic rhetoric and perspective that Kurzweil’s book typifies. The annual AAAI conference remains focused on thesolution of important, fascinating, but very narrowly-defined technical problems, most of which are only loosely (if atall) connected to the problem of creating artificial general intelligence at the human level or beyond. More ambitiousAI ideas remain at the periphery.Kurzweil has called the currently mainstream sort of AI research “narrow AI”—meaning AI that focuses on thecreation of software solving specific, narrowly constrained problems. A narrow AI program need not understanditself or what it is doing, and it need not be able to generalize what it has learned beyond its narrowly constrainedproblem domain. For example, a narrow-AI program for playing chess need not be able to transfer any of its strategicor methodological insights to Shogi (Japanese chess) or checkers . . . and probably not even to Fisher random chess(though a human programmer might be able to take some of the knowledge implicit in a narrow-AI chess playingprogram and use this to make a better program for playing other games; in this case the general intelligence existsmainly in the human being not the programs). A narrow-AI program for driving a car in the desert need not be able toutilize its knowledge to drive a car in the city or a motorcycle in the desert. A narrow-AI program for parsing Englishcannot learn any other language, whether or not the other language has a similar syntactic and semantic structure.A narrow-AI program for diagnosing kidney cancer will always be useless for diagnosing gall bladder cancer (thoughthe same narrow-AI framework may be used by humans to create narrow-AI programs for diagnosing various sorts ofcancers).Kurzweil contrasts narrow AI with “strong AI,” but I find this terminology confusing due to its overlap withSearle’s better-known usage of the term “strong AI,” so I prefer the term AGI or “Artificial General Intelligence” forB. Goertzel / Artificial Intelligence 171 (2007) 1161–11731163the opposite of Kurzweil’s “narrow AI.” A related term sometimes used is “Human-Level AI,” but I consider this termless preferable for two reasons:• In the space of all possible minds, humans are not necessarily all that smart, so that the “human level” constraintactually may pose an overly strict limitation on the scope of future AGI work.• Defining what “human level” means is actually difficult, when one starts thinking about potential highly-general-intelligence AI systems with fundamentally non-human-like architectures. If one has an AGI system with verydifferent strengths and weaknesses than humans, but still with the power to solve complex problems across avariety of domains and transfer knowledge flexibly between these domains, it may be hard to meaningfully definewhether this system is “human-level” or not.For the rest of this essay I will stick with the term AGI, though using “Human-Level AI” when that is specificallywhat I mean.Now, one might argue that the position of AGI R&D on the margins of contemporary AI research is only correctand proper, since we don’t really know how to do human-level AGI yet; and the bulk of contemporary AI researchfocuses on more narrowly-defined research directions that have the benefit of far more easily leading to scientificallydemonstrable and/or pragmatically useful results. However, there are other branches of contemporary science wherethe overall modus operandi is not so conservative. For instance, physics currently devotes a significant amount ofattention to speculative mathematical theories of unified physics, which are no more soundly proven than anybody’scurrent approach to AGI. This work is considered justified because it is advancing understanding, and seems likely(according to the very theories being developed, which are not yet proven) to yield exciting experimental results infuture. And, the biopharmaceutical industry has devoted a huge amount of funding to areas such as gene-therapy basedmedicine, which has not yet led to any dramatic practical successes, and still involves a huge amount of uncertainty (forinstance, how to effectively deliver modified genes to the appropriate part of the organism being healed). Quantumcomputing is the focus of much excitement in spite of the fact that all known quantum computers are extremelyspecialized or extremely small (a handful of qubits), and the number of fundamental quantum computing algorithmsknown can be counted on the fingers of one hand. Modern science, in other areas, is willing to take a medium andlong term view and focus significant amounts of attention on the big problems. Other examples abound. But the fieldof AI, due to its particular history, has settled into a rather conservative pattern. Which makes the contrast betweenthe goings-on at the average AAAI conference session, and the prognostications of a futurist AI pundit like Kurzweil,particularly dramatic, poignant and amusing.My own view is that AGI should be the focus of a significant percentage of contemporary AI research; and thatdramatic progress on AGI in the near future is something that’s reasonably likely (though by no means certain) tohappen. In the remainder of this essay I’ll present this view mainly via reviewing some of Ray Kurzweil’s recentarguments, and their attempted rebuttal in a recent article by Drew McDermott [22]. I will reframe Kurzweil’s ideasand predictions within a larger perspective of scenario analysis, review their strengths and weaknesses in this context,and then consider them side-by-side with other scenarios that are also plausible and in my view at least equallyexciting. Among the many important issues to be considered along the way is the nature of the “Singularity” eventthat Kurzweil refers to in the title of his book, and the relationship between AGI and the Singularity. I realize theseissues are fairly far “out there” compared to what most contemporary AI researchers think about from day to day—butI contend that this is something that should, and will, change.2. Scenario analysis and the future of AGIThe future of AGI is a big, hard, complicated issue. No one can sensibly claim to know what’s going to happen.Dealing with issues like this is difficult; there is uncertainty that is irreducible (at least from the perspective of merehuman-level minds!).One methodology that has been crafted for dealing with very difficult problems involving complex systems andhigh levels of uncertainty is “scenario analysis.” Originally crafted by Shell Oil planning pundits in the 1970s, scenarioanalysis has been expanded into a more general methodology, and has been used profitably to deal with a variety ofcritical real-world situations, such as the establishment of a new order in South Africa following the abolition ofapartheid [20].1164B. Goertzel / Artificial Intelligence 171 (2007) 1161–1173The basic idea of scenario analysis is simple. Rather than trying to make definite predictions with specific prob-abilities, or trying to arrive at elegant abstractions binding the past, present and future, one tries to lay out a seriesof specific future scenarios for a complex system. Ideally each scenario should be fleshed out by a group of indi-viduals with expertise and intuition regarding different aspects of the system in question, and different opinions onkey, relevant controversial issues. Applying scenario analysis to the South African political situation at the time ofthe abolition of apartheid, for example, involved organizing meetings involving a wide variety of parties, includingblack anti-apartheid activists, government and business leaders, and so forth. Rather than arguing about what will orwon’t happen, the goal of the team is to use their collective intuition, knowledge and expertise to flesh out a varietyof particular possible scenarios. Once this is done, then analytic and evaluative effort can be expended assessing theplausibility and desirability of various scenarios.I think this would be an excellent way to confront the question of the future of AGI and its relationship to therest of technology and society. In the ideal approach, one would convene a series of meetings involving a variety ofparties involving AGI researchers, narrow AI researchers, technology pundits, social and business leaders, artists andpsychologists, and so forth. The goal would be to explore in detail a variety of plausible scenarios regarding what thefuture of AGI may hold, both scientifically and in terms of broader implications.Lacking such an explicit scenario analysis approach to exploring AGI, however, I think scenario analysis can also befruitfully deployed as a way of structuring and evaluating the thinking and speculating about AGI and its future that’salready been done by various knowledgeable individuals and groups. Example of plausible categories of scenariosregarding the future of AGI would be the following (this is certainly not a complete list!):Steady Incremental Progress Scenarios. Narrow-AI research continues incrementally, as it’s doing now—graduallyand slowly becoming less and less narrow. Explicit AGI research doesn’t really get anywhere till narrow AI has builtup a lot more knowledge about how to solve particular sorts of problems using specialized algorithms. Eventually,maybe hundreds of years from now, narrow AI research becomes sufficiently general that it reaches human-levelAGI. By that point, AI tech at various levels of generality is already thoroughly integrated into human society, andpotentially even into human organisms via brain-computer interfacing technology (as reviewed by Kurzweil in TSIN).Dead-End Scenarios. Narrow-AI research continues and leads to various domain-specific successes but doesn’tsucceed in progressively moving toward AGI. Explicit AGI research also fails to progress. The human mind provesincapable of understanding, replicating or improving on human-level intelligence.Each of two these scenario-categories branches into multiple, radically variant scenarios for the future as a whole,depending on one’s assumptions about the advances of other futuristic technologies besides AGI, such as neuromodifi-cation and nanotechnology (extensively discussed by Kurzweil in TSIN; for the classic discussion of nanotech see [7]).For instance, if the creation of something vaguely like a Drexlerian molecular assembler becomes possible within thenext couple centuries, then the above scenario-categories correspond to scenarios in which human life changes dra-matically and unpredictably, but AGI doesn’t play a major role in it. On the other hand, if molecular assemblersand other hoped-for radical technologies prove just as difficult as AGI, then these scenario-categories correspond tooverall scenarios of incremental social, technological, political and psychological evolution (scenarios which, as ithappens, are relatively straightforward to explore, although clearly we have a pretty weak track record at predictingthe evolution of human systems even under these relatively “easy” conditions).On the other hand, a more exciting category of scenarios are theAGI-Based Singularity Scenarios. A human-level AGI is achieved, and this AGI succeeds at both progressivelyincreasing its own intelligence and creating various other radical technologies, thus leading to a massive and massivelyunpredictable transformation of the conditions in our region of the universe.The term “Singularity” was introduced in this context by Vernor Vinge [26], who used it to refer to the point atwhich scientific and technological progress occur so fast that, from the perspective of human cognition and perception,the rate of advancement is effectively infinite. The knowledge-advancement curve becomes effectively vertical, froma human perspective.The term Singularity in this context also gains some semantic flavor by analogy to singularities in mathematicalphysics, particularly black holes. This analogy invokes the concept of the “event horizon” which shields the singularityB. Goertzel / Artificial Intelligence 171 (2007) 1161–11731165from outside observation. This point of view is informed by the notion that it is difficult for humans to predict whatwill be done by intellects which exceed our own.Slightly less dramatically, J. Storrs Hall [17] has referred to the possibility of a coming “Intellectual Revolution,”loosely analogous to the Industrial Revolution. But whether we think about a revolution or a Singularity, many featuresof the situation remain the same. There is still a dramatic, irreducible uncertainty attached to the development ofany future technology as radical as human-level AGI. Whether the change is conceived as a Singularity or a mererevolution, the character of the human condition following such a major change is substantially unknown, and theoutcome may well depend critically on the nature of the AGI programs that enable it.Within the category of Singularity Scenarios, there are, again, various particular scenarios, differentiated by multi-ple factors including the manner in which AGI is achieved. As examples we may look atSkynet Scenario. Named after the malevolent AGI in the Terminator movies. A powerful AGI is created, improvesitself, develops amazing new technologies (backwards time travel, in the movies), and enslaves or annihilates uspesky little humans. In the modern futurist literature there is recurrent talk of superhuman AI’s transforming all themolecules of their region of the universe into “computronium”—i.e. into processors and memory intended to enhancetheir own intelligence. After all, from a superhuman AI’s point of view, isn’t a superhuman intelligence a better useof available mass-energy resources than a bunch of stupid little atavistic humans?Kurzweil Scenario. The scenario Kurzweil envisions in TSIN is essentially that AGI is achieved via scanning humanbrains, figuring out the nature of human thought from these scans, and then replicating human brain function on mas-sively powerful computer hardware (whose existence is predicted by Moore’s Law and its brethren). Furthermore, heviews this human-like human-level AGI as integrating richly with human society, so that it’s not an “Us versus Them”type scenario, but rather a scenario in which the boundary between Us and Them is fluid, evolving and impossible todefine.While Kurzweil is enthused about human-brain emulation (HBE), this is obviously not the only possible path toAGI, nor will it necessarily be the first path to succeed. Furthermore, once HBE is achieved, this may relatively quicklylead to other forms of advanced AGI (a point I’ll elaborate on below).In The Path to Posthumanity [14], the author and Stephan Vladimir Bugaj suggest that the most likely future isone in which human-level AGI is first achieved via integrative methods (synthesizing insights from computer science,cognitive science and other disciplines) rather than via emulation of the human brain.In this vein, a number of contemporary AGI researchers are pursuing computer-science-centric approaches, in-spired only loosely by cognitive neuroscience (and generally more so by cognitive psychology proper; e.g. Franklin’s[9] LIDA system which is based closely on Bernard Baars’ work and SOAR [19] and ACT-R [1] which follow variouspsychological studies very rigorously).From a Singularitarian perspective, non-human-emulating AGI architectures may potentially have significant ad-vantages over human-emulating ones, in areas such as robust, flexible self-modifiability, and the possession of arational normative goal system that is engineered to persist throughout successive phases of radical growth, develop-ment and self-modification. This leads to a variety of possible futurological scenarios, including three which The Pathto Posthumanity identifies as particularly interesting:Sysop Scenario. Yudkowsky [28] described the “Sysop Scenario”—a scenario in which a highly powerful AI ef-fectively becomes a “system operator” for this region of the physical universe. Goertzel and Bugaj, tongue-in-cheek,referred to a closely related “AI Buddha” scenario, focusing on the Sysop’s need to display a practical compassionfor humans and other beings who are far its inferiors. The basic feature is the existence of an AGI with dramaticallysuperhuman powers, and a will and mandate to make the universe (or its region thereof) a hospitable place for thevarious other minds and life-forms resident within it, as well as advancing its own self in directions it finds desirable.This is the ultimate extension of the archetype of the “benevolent dictator.”AI Big Brother Scenario. The task of creating an AGI that will evolve into a Sysop or something else reasonablybeneficent may prove an arduous one, perhaps even intractable, which brings up the possible desirability of preventingthe advent of super-advanced AI technologies. One way to do this would be to freeze technological development at itscurrent levels; but this seems difficult to do, consistent with contemporary ideals of capitalism and personal freedom.1166B. Goertzel / Artificial Intelligence 171 (2007) 1161–1173If these ideals are sacrificed, on approach that might be effective would be to create an AGI with greater-than-humancapabilities, but without a mandate for progressive self-improvement; rather, with a mandate for preserving the humanstatus quo, using other technology such as nanotechnology (XX) as appropriate.Singularity Steward Scenario. Finally, there is a variant of the above two scenarios in which a powerful AGI iscreated with a temporary goal of easing the human race through its Singularity in a smooth and beneficial way. Thisis the sort of future alluded to in Damien Broderick’s novel Transcension [3].Finally, another solution, fascinating though fanciful, is outlined in Yudkowsky [29].Coherent Extrapolated Volition Scenario.In this scenario, the human race realizes that choosing the right futurescenario is too hard, so it creates a specialized narrow-AI optimization process whose goal is to figure out whathumans really want, and issue a report (which we then are free to put in a file cabinet and ignore, of course!).Obviously, the list of future scenarios given above is far from complete. However, I think it is long enough to givea concrete flavor of the raw indeterminacy associated with the possibility of the technological Singularity. As Vingepointed out when he first articulated the Singularity idea, the key point about the creation of superhuman intelligenceis that, once it’s here, in all probability we measly humans simply have no way to predict what happens next.3. Kurzweil’s singularity scenario and McDermott’s critiqueGetting back to Kurzweil, then: One thing that he has accomplished in TSIN and related writings is to articulate,in impressive detail and with admirable articulacy, one possible Singularity scenario (the one I have labeled the“Kurzweil scenario” above). In this vein he does three things, all of them well:1. He describes what this scenario might be like for those living through it;2. He describes the scientific path by which it might come about;3. He gives his own estimates of the probability (very high) and the expected timeline (human-level AI by 2029,Singularity by 2045).Much of his effort seems to have gone into the detailed graphs and charts supporting the timeline estimates (item 3);as I’ll elaborate below, I think these extrapolations are worthy as well as attention-grabbing, but would be moreinteresting if they came with equally carefully estimated confidence intervals. On the other hand, I think he has donea tremendous service by carrying out item 2 in the above list, and describing, for the general reader, the scientific pathby which a Kurzweilian Singularity might pragmatically be brought to pass.In 2006, this journal published a review by Drew McDermott entitled “Kurzweil’s argument for the success of AI”,which presents a fairly harsh critique of contemporary AI optimism, focusing heavily on The Singularity Is Near.While McDermott’s critique does contain some cogent points, it also seems to miss some of the key points Kurzweil’sbook tries to get across. It seems worthwhile to briefly pause here to consider some of McDermott’s counter-argumentsand their strengths and shortcomings.One of McDermott’s main points is that Kurzweil does not provide any proof that an AI-driven Singularity isupon us. This is certainly the case. However, nor does Kurzweil claim to provide proof—he strives only to providea convincing rational argument of likelihood. Obviously, in any extrapolation of the future of a complex real-worldsystem coupled with other complex real-world systems, there can be no such thing as proof, only at best “probablyapproximately correct” prediction.I do tend to feel that Kurzweil underestimates the uncertainty involved in predicting the future of complex, opensystems like human societies (and the scientific and technological development taking place within them). The ten-dency of the human mind toward overconfidence is well-researched and well-documented in the cognitive psychologycommunity [10], and in reading Kurzweil’s book, it’s hard to escape the feeling that in spite of the rigor of his dataanalysis procedures, he has succumbed to this tendency to a certain extent. Where are the confidence intervals aroundhis prognostications? Sure, it’s useful to have a specific date in mind, to make a prediction concrete and palpable—buthow certain could a date like “2045 for human-level AI” possibly be? Even if 2045 is a good estimate of the mean ofthe distribution of dates for human-level AI, what’s the variance? Clearly it must be pretty large—because there areplausible scenarios in which human-level AI is delayed beyond 2100, and also (a more controversial statement, I re-alize, but one that I’ll stand by) plausible scenarios in which it’s achieved by 2020 or even 2015. Even “human-levelB. Goertzel / Artificial Intelligence 171 (2007) 1161–11731167AI by 2010” has a probability non-negligibly greater than zero, I would venture. There is a lot more uncertainty in thefuture than Ray Kurzweil wants to recognize.Taking a scenario-analysis point of view, however, gives a different perspective on Kurzweil’s achievement as afuturologist. What he has done is to give a very clear portrayal of a particular scenario for our future, and then to arguewhy this scenario is in fact a reasonably likely one. The fact that he may display a bit of overconfidence in estimatingthe probability of this scenario over other ones, or the date of the arrival of particular events within this scenario,doesn’t detract from the crux of his achievement.The most interesting point in McDermott’s critique is his attempted rebuttal of Kurzweil’s argument as to whyhuman-level AGI is likely to occur by 2029. To get this figure, Kurzweil extrapolates not from contemporary progressin the AI field, but rather from contemporary progress in computer hardware and brain scanning. He argues that by2029 we will have computers powerful enough to host a detailed emulation of the human brain, and brain-scannerspowerful enough to mine the brain-data needed to create such an emulation. So according to this argument, even if theAI approaches currently being pursued are all wrong, we’ll still get to human-level AI soon enough just by copyingthe brain.The most important thing to understand about this argument is that Kurzweil intends it centrally as a sort of“existence proof.” He’s not saying that human-brain emulation is the only route to human-level AGI, he’s mostlyjust saying that it’s a plausible route—and a route that one can argue for by extrapolating historical growth curvesof various non-AI technologies. My own view is that Kurzweil is essentially correct in his extrapolations, and thatbrain-scanning and computer-hardware will conspire to allow effective human brain emulation sometime in the nextfew decades. Just recently, the newspapers contained reports that IBM had “simulated half a mouse brain” [2]. It turnsout that the simulation ran only briefly, ran at 1/10 the speed of an actual mouse brain—and, most limitingly, was infact just a simulation of the same number of neurons believed to comprise half a mouse brain, but with largely randominterconnectivities (because brain scanning doesn’t yet tell us how a mouse’s neurons are interconnected). But still,this sort of development is highly consistent with Kurzweil’s projections.My own guess happens to be that we will achieve human-level AGI via other means, well before the brain-scanningroute gets us there. But, this is not an argument against Kurzweil’s projections. I think that computer hardware andbrain-scanning are simply more predictable technologies than non-human-brain-based AI software. The latter, I sug-gest, is more likely to experience a sudden and generally-surprising revolutionary advance; but also more likely to getstuck on unforeseen obstacles and drag on a long time with disappointing progress.The main dispute McDermott has with the brain-scanning route to AGI is that, even if we succeed in scanning thebrain into a computer, this still won’t give us any real understanding of how intelligence (human or otherwise) works.In his words,Obviously improvements in brain-scanning technology are important and exciting, but they get us somewhere onlyif accompanied by deepening of our understanding of the computations neurons do. So the possibility of scanningis a very weak argument that our understanding is sure to deepen.. . .[E] ven if we succeeded in duplicating a person to the point where we couldn’t tell the copy from the original, thatwouldn’t even confirm AI, let alone contribute to it.(p. 6)More technically, McDermott claims that simulating a brain in computer hardware wouldn’t suffice as a demon-stration of “computationalism,” which he defines as the assertion thata) there is such a thing as the ‘computational state’ of a creature, which can be characterized independently of thephysical substrate used to implement it’b) what computational state a system is in complete determines its mental stateAnd relatedly, he takes issue withthe idea that once we achieve ‘strong AI’ we will ‘soar’ on to what I will call superstrong AI. Once more, I don’tthink the case is proven.1168B. Goertzel / Artificial Intelligence 171 (2007) 1161–1173arguing against Kurzweil’s vision of recursively self-improving AI via accusing it of embodying circular reasoning:“ ‘Machine intelligence will improve its own abilities in a feedback cycle that unaided human intelligence will notbe able to follow.’ The last argument is, alas, circular, as far as I can see. Until machine intelligence gets past thehuman level, it won’t be able to do all that smart stuff.”These objections do, I think, have some point to them. But the point was already understood and addressed byKurzweil in his book. Note that Kurzweil foresees human-level AGI via brain emulation in 2029, and Singularityin 2045. This is in part because Kurzweil understands that being able to emulate a brain in computer hardware isdifferent from having a real understanding of how to flexibly create human-level artificial intelligences. And Kurzweilunderstands that the human brain architecture—even ported to digital hardware—is probably not amenable to rapidrecursive self-improvement.Indeed, digitally emulating a human brain is not logically equivalent to “solving the human-level AGI problem”,let alone to solving the problem of creating a Singularity-enabling self-improving AGI. But even so, there is a quiteplausible-sounding path from the former to the latter, such that from our present perspective, it seems very reasonableto surmise that having uploaded human brains to study would make the latter a heck of a lot easier.An uploaded, digitized human brain is not really equivalent to a biological human brain, because it can be ma-nipulated and studied a lot more flexibly and thoroughly. Of course, experimenting with sentient minds without theirpermission raises ethical questions; however, it seems likely that there will be no lack of willing volunteers, amongsoftware minds, for cognitive experimenation. Personally, as a mind scientist, if I were uploaded into a computer, Iwould happily volunteer copies of myself as subjects for non-painful experimentation. Currently, the legal systemsof most countries do not allow individuals to volunteer themselves for scientific experiments; but it seems likely thatthese laws will change in a scenario where individuals can be replicated and reconstituted.The ability to observe everything that happens inside an uploaded brain, and to make flexible manipulations andobserve their consequences, is a huge difference from the situation with biological brains that have a disturbingtendency to break when you poke and prod them too much. For this reason, it seems highly likely to me that uploadedbrains will—within years or at most decades, not centuries—lead us to a real science of human-level intelligence. Ifhuman-level AGI doesn’t already exist by that point via other means, the knowledge gained from probing uploadedhuman brains will allow us to create it (thus, among many other more interesting things, providing the practicaldemonstration of computationalism that McDermott mentions).Furthermore, it’s worth noting that, even given merely human-level AGIs, Moore’s law and related extrapolationssuggest that the cost of human-level intelligence will then decrease by orders of magnitude per decade, a (very)significant economic impact. Relatedly, using human-level AGI scientists, an “artificial scientific community” couldincrease in scope exponentially at extremely manageable cost. It seems quite plain that with this sort of rapidlyexpanded scientific capability, the step from digitally emulated human scientists to a principled understanding ofhuman-level AGI should not be nearly so slow and difficult as the process of creating the first human-level AGI orhuman-brain emulation in the first place.McDermott complains about Kurzweil’s projection that“ ‘Machines will be able to reformulate their own designs and augment their own capacities without limit’. Arethere really principles of intelligence that would support limitless insight the way Bernoulli’s principle supportsthe design of various kinds of lifting surfaces? It just seems to me that intelligence can’t be boxed in that way.”and I agree that the “without limit” part seems an unnecessary speculation. Who knows what limitations may bediscovered by vastly superhuman intelligences? The laws of physics and the properties of the physical universe imposetheir own limitations. But, I also note that a continual process of self-modification leading to unbounded increases inintelligence (within the constraints posed by the physical world) does not require universal “principles of intelligence”of the type McDermott mentions, and doubts. One can envision a series of ever-improving minds M1, M2, M3, . . . ,where Mn understands enough to create a smarter Mn+1 and not necessarily much more. No universal theory is neededhere. Different chains of ever-improving minds might discover different sorts of theories explaining different regionsof mind-space. These are wild speculations, yes, but the point is that, contra McDermott, Kurzweilian “unlimitedB. Goertzel / Artificial Intelligence 171 (2007) 1161–11731169intelligence increase via progressive self-improvement” does not require any “boxing-in” of intelligence via universallaws.A good analogy to invoke here, as pointed out by J. Storrs Hall [18], is the scientific community itself—which is acontinually self-improving collective intelligence composed internally of human-level intelligences. At each stage inits development, the scientific community knows enough to give rise to its next generation. Improvement may continueprogressively, incrementally and perhaps unboundedly (within the constraints of the physical universe), without theneed for any universal principles of scientific endeavor to be charted out in advance.So, in spite of what McDermott says, there is no circular reasoning underlying the notion of Singularity approachedvia progressive self-improvement of human-level AGI systems. And Kurzweil does not commit the fallacy of assumingthat emulating a human brain in a computer is logically equivalent to solving the AGI problem. Rather, Kurzweilcorrectly views human brain emulation as one route toward Singularity-enabling AGI, a route that involves:1. Scanning human brains.2. Creating human brain emulations in advanced computer hardware.3. Studying these emulations to understand the principles underlying them.4. Creating various AGI systems embodying these principles, including AGI systems capable of radical self-modification and self-improvement.Kurzweil does not pose things in exactly this way but I believe it is a fair reading of TSIN. Appropriately (giventhe level of uncertainty associated with the relevant technologies), he does not make any commitments regarding howsimilar these Singularity-enabling, self-modifying AGI systems are going to be to the original human brain emulations.4. Virtual-world embodiment as an alternate plausible route to human-level AGIScreen capture of humanoid avatar w/non-talking parrot in the Second Life virtual world.But what other plausible routes to AGI are there, aside from the brain-emulation route on which Kurzweil focusesmost of his attention? Of course there are plenty, and since this is a brief essay I wish to avoid debating variouscontemporary paradigms on AGI design and their merits and shortcomings. My own detailed approach to AGI design,1170B. Goertzel / Artificial Intelligence 171 (2007) 1161–1173engineering and teaching can be found in references such as [11–13] if the reader is curious. What I want to discusshere is a general approach to developing and teaching AGI’s and launching them into society that is quite differentfrom what Kurzweil conceives in the context of whole-brain emulation. The approach I’ll discuss is based on virtualembodiment, and is interesting in the present context both because of what it suggests about the possibility of a non-Kurzweilian timeline for the development of human-level AGI, and because of the unique flavor of the Singularityscenarios to which it naturally leads.The issue of the necessity for embodiment in AI is an old one, with great AI minds falling on both sides of the debate(the classic GOFAI systems are embodied only in a very limited sense; whereas Brooks [4] and others have arguedfor real-world robotic embodiment as the golden path to AGI). My own view is somewhere in the middle: I thinkembodiment is very useful though probably not strictly necessary for AGI, and I think that at the present time, it maybe more generally worthwhile for AI researchers to spend their time working with virtual embodiments in digitalsimulation worlds, rather than physical robots. Toward that end some of my current research involves connecting AIlearning systems to virtual agents in the Second Life virtual world.The notion of virtually embodied AI is nowhere near a new one, and can be traced back at least to Winograd’s[27] classic SHRDLU system. However, technology has advanced a long way since SHRDLU’s day, and the powerof virtual embodiment to assist AI is far greater in these days of Second Life, Word of Warcraft, HiPiHi, Creatures,Club Penguin and the like. To concretely understand the potential power of virtual embodiment for AGI, considerone potential project I’ve been considering undertaking during the next few years: a virtual talking parrot. Imaginemillions of talking parrots spread across different online virtual worlds—all communicating in simple English. Eachparrot has its own local memories, its own individual knowledge and habits and likes and dislikes—but there’s also acommon knowledge-base underlying all the parrots, which includes a common knowledge of English.Next, suppose that an adaptive language learning algorithm is set up (based on one of the many available paradigmsfor such), so that the parrot-collective may continually improve its language understanding based on interactions withusers. If things go well, then the parrots will get smarter and smarter at using language, as time goes on. And, ofcourse, with better language capability, will come greater user appeal.The idea of having an AI’s brain filled up with linguistic knowledge via continual interaction with a vast numberof humans, is very much in the spirit of the modern Web. Wikipedia is an obvious example of how the “wisdom ofcrowds”—when properly channeled—can result in impressive collective intelligence. Google is ultimately an evenbetter example—the PageRank algorithm at the core of Google’s technical success in search, is based on combininginformation from the Web links created by multi-millions of Website creators. And the intelligent targeted advertisingengine that makes Google its billions of dollars is based on mining data created by the pointing and clicking behaviorof the one billion Web users on the planet today. Like Wikipedia and Google, the mind of a talking-parrot tribeinstructed by masses of virtual-world residents will embody knowledge implicit in the combination of many, manypeoples’ interactions with the parrots.Another thing that’s fascinating about virtual-world embodiment for language learning is the powerful possibilitiesit provides for disambiguation of linguistic constructs, and contextual learning of language rules. Michael Tomasello[25], in his excellent book Constructing a Language, has given a very clear summary of the value of social interactionand embodiment for language learning in human children. For a virtual parrot, the test of whether it has used Englishcorrectly, in a given instance, will come down to whether its human friends have rewarded it, and whether it has gottenwhat it wanted. If a parrot asks for food incoherently, it’s less likely to get food—and since the virtual parrots willbe programmed to want food, they will have motivation to learn to speak correctly. If a parrot interpret a human-controlled avatar’s request “Fetch my hat please” incorrectly, then it won’t get positive feedback from the avatar—andit will be programmed to want positive feedback.The intersection between linguistic experience and embodied perceptual/active experience is one thing that makesthe notion of a virtual talking parrot very fundamentally different from the “chatbots” on the Internet today. The othermajor difference, of course, is the presence of learning—chatbots as they currently exist rely almost entirely on hard-coded lists of expert rules. But the interest of many humans in interacting with chatbots suggests that virtual talkingparrots or similar devices would be likely to meet with a large and enthusiastic audience.Yes, humans interacting with parrots in virtual worlds can be expected to try to teach the parrots ridiculous things,obscene things, and so forth. But still, when it comes down to it, even pranksters and jokesters will have more funwith a parrot that can communicate better, and will prefer a parrot whose statements are comprehensible.B. Goertzel / Artificial Intelligence 171 (2007) 1161–11731171And of course parrots are not the end of the story. Once the collective wisdom of throngs of human teachers hasinduced powerful language understanding in the collective bird-brain, this language understanding (and the com-monsense understanding coming along with it) will be useful for other purposes as well. Humanoid avatars—bothhuman-baby avatars that may serve as more rewarding virtual companions than parrots or other virtual animals; andlanguage-savvy human-adult avatars serving various useful and entertaining functions in online virtual worlds andgames. Once AI’s have learned enough that they can flexibly and adaptively explore online virtual worlds (and theInternet generally) and gather information according to their own goals using their linguistic facilities, it’s easy toenvision dramatic acceleration in their growth and understanding.A baby AI has a lot of disadvantages compared to a baby human being: it lacks the intricate set of inductive biasesbuilt into the human brain, and it also lacks a set of teachers with a similar form and psyche to it . . . and for thatmatter, it lacks a really rich body and world. However, the presence of thousands to millions of teachers constitutes alarge advantage for the AI over human babies. And a flexible AGI framework will be able to effectively exploit thisadvantage.Now, how does this sort of vision relate to Kurzweil, the Singularity and all that? It’s simply a very differentpathway than human-brain emulation. Google doesn’t emulate the human brain, yet in a sense it displays considerableintelligence. To demonstrate this, I just typed the query “How tall is a giraffe?” into Google. The snippet shown bythe first search result reads:Giraffe are the tallest animals in the world. Males can reach as high as 6 meters (19 feet) tall and female giraffecan measure up to 5 meters (17 feet) tall. Their neck accounts for the added height and is by itself approximately2.4 . . .This sort of experiment indicates that Google is a pretty decent natural-language question-answering system.Granted, it is not an incredibly smart question-answerer; for instance when I ask it “how tall is a giraffe that hasbeen flattened by a steamroller?” its responses are irrelevant. But its responses to simple questions are often quiteuseful, due to the combination of the masses of information posted online, the inter-page links created by various Webauthors, and the clever narrow-AI algorithms created by Google staff that make use of this text and these links. Whatwe have here is considerable and useful artificial intelligence, achieved via the wisdom of crowds, combined with theingenuity of the authors of Google’s AI algorithms. Wikipedia is an interesting and related example: synergeticallywith Google or other search tools, it enhances the intelligence of the Web, in a way that specifically and cleverlyleverages the collectivity of human intelligence.It seems possible to harness the “wisdom of crowds” phenomenon underlying the Internet phenomena such asGoogle and Wikipedia for AGI, enabling AGI systems to learn from vast numbers of appropriately interacting humanteachers. There are no proofs or guarantees about this sort of thing, but it does seem at least plausible that this sort ofmechanism could lead to a dramatic acceleration in the intelligence of virtually-embodied AGI systems, and maybeeven on a time-scale faster than brain-scanning and hardware advances lead to human-brain emulation. The human-brain-emulation route is better for drawing graphs—it forms a better “existence proof”—but personally I find thevirtual-worlds approach more compelling as an AGI researcher, in part because it gives me something exciting towork on right now, instead of just sitting back and waiting for the brain-scanner and computer-hardware engineers.Singularity-wise, one key difference between human-brain emulation and virtual-parrots-and-the-like has to dowith the assumed level of integration between AI’s and human society. Kurzweil already thinks that, by the timeof the Singularity, humans will essentially be inseparable from the AI-incorporating technological substrate theyhave created. The virtual-agents pathway makes very concrete one way in which this integration might happen—in fact it might be the route by which AGI evolves in the first place. Right now many people consider themselvesinseparable from their cellphones, search engines, and so forth. Suppose that in the Internet of 2015, Websites andword processors are largely replaced by some sort of 3D immersive reality—a superior Second Life with hardwareand software support far beyond what exists now in 2007—and that artificially intelligent agents are a critical part ofthis “metaversal”1 Internet. Suppose that the AGI’s involved in this metaverse become progressively more and moreintelligent, year by year, due to their integration in the social network of human being interacting with them. When1 See e.g. the Metaverse Roadmap, http://metaverseroadmap.org/.1172B. Goertzel / Artificial Intelligence 171 (2007) 1161–1173the AGI’s reach human-level intelligence, they will be part of the human social network already. It won’t be a matterof “us versus them”; in a sense it may be difficult to draw the line. Singularity-scenario-wise, this sort of path to AGIlends itself naturally to what I above called the “AI Big Brother” and “Singularity Steward” scenarios, in which AGIsystems interact closely with human society to guide us through the future.I don’t care to pose an argument regarding the probability of the “virtual parrots” route versus the brain-emulationroute or some other route. I consider both these routes, and others, highly plausible and worthy of serious consider-ation. The key point is that we can now explore these sorts of possibilities with dramatically more concreteness thanwe could do twenty or thirty years ago. As Kurzweil has adeptly argue, technology has advanced dramatically in waysthat are critically relevant to AGI, and seems very likely to continue to do so—and this is one of the key reasons Ibelieve the time for human-level AGI really is near.5. ConclusionMcDermott, in his critique of Kurzweil, levies the following complaint:I wish he would stop writing these books! . . . The field of AI is periodically plagued by ‘hype hurricanes’. . . Mostof these storms are started or spun up by journalists, venture capitalists, or defense-department bureaucrats. Butthere are a very small number of people within the field who contribute to the problem, and Kurzweil is the worstoffender.My attitude could hardly be more opposite! Though I disagree with Ray Kurzweil on a number of points of em-phasis, and a few points of substance, I believe he is doing pretty much exactly the right thing, by getting the generalpublic enthused about the possibilities AGI is very likely going to bring us during the 21st century. I think the time ismore than ripe for the mainstream of AI research to shift away from narrowly-constrained problem-solving and backto explicit pursuit of the general-intelligence goals that gave the field its name in the first place. In the last few years Ihave seen some signs that this may in fact be happening, and I find this quite encouraging.The difficulty of predicting the future is not just a cliche’, it’s a basic fact of our existence. And part of the hypoth-esis of the Singularity is that this difficulty is just going to get worse and worse. One of my main critiques of Kurzweilis that he is simply too pat and confident in his predictions about the intrinsically unpredictable—quite differentlyfrom Vinge, who tends to emphasize the sheer unknowability and inscrutability of what’s to come.In spite of all this uncertainty, however, we must still plan our own actions, our own lives and our own researchcareers. My contention is that the hypothesis of the Singularity is a serious one, worthy of profound practical con-sideration; and even more emphatically, that the pursuit of human-level AGI deserves to be taken very seriously, atvery least as much so as grand pursuits in other areas of science and engineering (gene therapy; building artificialorganisms a la Craig Venter’s work on mycoplasma genitalium; quantum computing; unifying physics; etc.). Yes, cre-ating AGI is a big and difficult goal, but according to known science it is almost surely an achievable one. There aresound though not absolutely confident arguments that it may well be achievable within our lifetimes, via one of manyplausible routes—and that its achievement may lead to remarkable things, for instance, one of the many Singularityscenarios discussed above, or other related possibilities not yet articulated.References[1] J.R. Anderson, Cognitive Psychology and Its Implications, fifth ed., Worth Publishing, New York, 2000.[2] BBC News, Mouse Brain Simulated on Computer, 27 April 2007, http://news.bbc.co.uk/2/hi/technology/6600965.stm.[3] D. Broderick, Transcension, Tor Books, 2003.[4] R. Brooks, Cambrian Intelligence, MIT Press, 1999.[5] N.L. Cassimatis, E.K. Mueller, P.H. Winston (Eds.), Special Issue of AI Magazine on Achieving Human-Level Intelligence through IntegratedSystems and Research, AI Magazine 27 (2) (2006).[6] D. Crevier, AI: The Tumultuous Search for Artificial Intelligence, BasicBooks, New York, 1993.[7] K.E. Drexler, Nanosystems: Molecular Machinery, Manufacturing, and Computation, Wiley, New York, 1992.[8] H. Dreyfus, What Computers Still Can’t Do, MIT Press, 1992.[9] S. Franklin, A foundational architecture for artificial general intelligence, in: B. Goertzel, P. Wang (Eds.), Advances in Artificial GeneralIntelligence, IOS Press, Amsterdam, 2007, pp. 36–54.[10] Gilovich, Griffin, Kahneman, Heuristics and Biases: The Psychology of Intuitive Judgment, Cambridge University Press, 2002.B. Goertzel / Artificial Intelligence 171 (2007) 1161–11731173[11] B. Goertzel, Virtual easter egg hunting: A thought-experiment in embodied social learning, cognitive process integration, and the dynamicemergence of the self, in: B. Goertzel, P. Wang (Eds.), Advances in Artificial General Intelligence, IOS Press, Amsterdam, 2007, pp. 36–54.[12] B. Goertzel, Patterns, hypergraphs and general intelligence, in: Proceedings of International Joint Conference on Neural Networks, IJCNN2006, Vancouver, CA, 2006.[13] B. Goertzel, M. Looks, C. Pennachin, Novamente: An integrative architecture for artificial general intelligence, in: Proceedings of AAAISymposium on Achieving Human-Level Intelligence through Integrated Systems and Research, Washington, DC, August 2004.[14] B. Goertzel, S. Bugaj, The Path to Posthumanity, Academica Press, 2006.[15] B. Goertzel, C. Pennachin (Eds.), Artificial General Intelligence, Springer, 2006.[16] B. Goertzel, P. Wang (Eds.), Advances in Artificial General Intelligence, IOS Press, 2007.[17] J. Storrs Hall, Beyond AI: Creating the Conscience of the Machine, Prometheus Books, 2007.[18] J. Storrs Hall, Self-Improving AI: An Analysis, lecture given at AI@50, Dartmouth, July 2006; journal version to appear in Minds andMachines.[19] R.M. Jones, R.E. Wray, Toward an abstract machine architecture for intelligence, in: Proceedings of the 2004 AAAI Workshop on IntelligentAgent Architectures: Combining the Strengths of Software Engineering and Cognitive Systems, 2004. For a complete list of publicationsregarding SOAR, see http://winter.eecs.umich.edu/soarwiki/Soar_Publications.[20] A. Kahane, Solving Tough Problems, Berrett-Koehler, 2007.[21] R. Kurzweil, The Singularity Is Near, Viking, 2005.[22] D. McDermott, Kurzweil’s argument for the success of AI, Artif. Intell. 170 (18) (2006) 1227–1233.[23] M. Minsky, The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind, Simon & Schuster,New York, 2006.[24] P. Thagard, Mind: Introduction to Cognitive Science, MIT Press, Cambridge, MA, 2005.[25] M. Tomasello, Constructing a A Language, Harvard University Press, 2003.[26] V. Vinge, The coming technological singularity, online at http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html. The original versionof this article was presented at the VISION-21 Symposium sponsored by NASA Lewis Research Center and the Ohio Aerospace Institute,March 30–31, 1993. A slightly changed version appeared in the Winter 1993 issue of Whole Earth Review.[27] T. Winograd, Understanding Natural Language, Academic, San Diego, 1972.[28] E. Yudkowsky, Creating friendly AI, http://www.singinst.org/upload/CFAI.html, 2002.[29] E. Yudkowsky, Coherent extrapolated volition, http://www.singinst.org/upload/CEV.html, 2004.