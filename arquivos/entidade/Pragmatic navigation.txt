ELSEVIER Artificial Intelligence 100 ( 1998) 275-322 Artificial Intelligence Pragmatic navigation: reactivity, heuristics, and search Susan L. Epstein * Deprrttnent of Computer Science, Hunter College und The Graduate School of The City University (f New York, 695 Park Avenue, New York, NY 10021. USA Received I7 April 1997; revised 15 October 1997 Abstract techniques is an architecture the Right Reasons) set of solution methods first has the opportunity for learning and problem solving (the serial testing of known, triggered incomplete and overlapping it represents that inte- to address complex problems. some facet of domain expertise, may vary in reliability and FORR (For grates a possibly Each method, although speed. The principal contribution of this paper is the extension of FORR to include situation-based behavior reactivity and heuristic based, and addresses problem reasoner activates a set of reactive If they, too, fail to produce a response, rationales. All three components experiments, strate how each component plays an important of this paper include a FORR-based, pragmatic, cognitively plausible approach learned heuristic through there. Empirical evidence demonstrates and guidelines for problem solving in a domain) with reasoning. FORR categorizes methods as reactive, heuristic, or situation- solving with one category of methods at a time. A hierarchical the reasoner tailored to specific situations. among heuristic In a series of is shown to be effective and efficient. Ablation experiments demon- role in problem solving. Additional contributions to navigation with territory and travel experience interact is both effective and efficient, to other domains are provided. @ 1998 Elsevier Science B.V. search procedures resorts learned the reasoner reference knowledge it, and a careful study of how situation-based to collaboration from experience. to react correctly. If no ready reaction behavior, reactivity, and heuristics that the resultant system triggers for time-limited for generalization this architecture two-dimensional approximations that describe is computed, Keywords; AI architectures; Heuristic search; Machine solving; Satisficing; Situation-based search; Spatial representation learning; Multistrategy learning; Navigation; Problem * Email: epstein@roz.hunter.cuny.edu 0004-3702/98/$19.00 PIf SOOO4-3702(97)00083-O @ 1998 Elsevier Science B.V. All rights reserved. 216 S.L. Ep.~tein/Artijiciul Intelligence 100 (1998) 275-322 Naive geographic reasoning is probably the most common and basic form of human intelligence-Egenhofer & Mark, 1995 rational behavior are applied of the world reasoning principles is, however, another is the serial testing of known, trigger action without conscious in a domain. The principal contribution search space, people employ a variety of devices to is automatic; certain reasoning. AI researchers this automaticity with reactive systems. Other portions of this behavior in some combination. limitedly When confronted with an intractable make what they hope will be expert decisions. Some of this behavior perceptions have modeled are heuristic; There Situation-based solving (For it integrates a complex problem, FORR heuristic expertise, may vary in reliability and speed. Additional contributions a pragmatic, of how situation-based evidence important mechanism people use, a kind of restricted search. for problem of this paper is the extension of FORR so that reasoning. To solve set of some facet of domain of this paper include to navigation with FORR; a careful study there; and empirical a possibly solution methods. Each method, although an architecture behavior with reactivity structures reactivity, and heuristics is both effective and efficient. the Right Reasons), situation-based incomplete it represents that the resultant and overlapping and heuristic and problem for learning cognitively techniques behavior, approach triggered plausible solving, interact system Consider example. situation-based first how reactivity, and a decision constitutes behavior, and heuristics would behave In the grid world studied here, the robot knows and those of the goal. (Other kinds of grid worlds could be treated in four line to any visible for a robot, with situation-based separately on a path-finding its own coordinates similarly.) The robot has no map; it can “see” only orthogonal directions, location, Fig. 1 shows a goal at (9,6) the legal moves behavior, or heuristics would encounter difficulties here. Without experience cyclic behaviors. Without every relevant decision-making could err, resorting And without efficient decision making, heavy search costs. record of its recent to learn, a purely reactive system could become mired in local rule, a purely heuristic system to either random choice or search when no heuristic was applicable. incur a move in a straight locations of reactivity, from RI striped. Unilateral to the nearest obstruction and three possible system would and the ability situation-based application a purely k?.Y obstl q legal G goal uction moves from Rl R 1, R2, R3 robot locations Fig. I. Robot R seeks to move to goal G. S.L. Ep.wein/Art@iciul Intelligence IOa (1998) 275-322 277 In contrast, FORR strikes an economical balance among reactivity, situation-based responds it gives priority first to correct reactions, behavior, and heuristics. Rather than exhaustively deliberate on a complete model of the in a variety of efficient ways to a partial, feature-based model. world, FORR As a three-tiered then to situation- architecture, to the model learned during based behaviors, and finally it is an inexpensive way to problem solving. Reactivity make the right obvious decisions it is costly, situation-based of computational limited manner. Heuristics right decision the partial model. relies upon triggers to justify cycles on search, and relies upon restricted to search in a very resort, applied when the in restricted search to heuristics, all with reference top priority because is given the appropriate expenditure routines and to avoid the wrong obvious ones. Although is not obvious and the problem solver cannot thus become a last, albeit frequent, behavior justify but had been a move to (7,5), a correct reaction would be to eliminate Consider how FORR would deal with each of the robot locations at RI contemplated it was a dead-end, consideration. Now consider a robot at R2 with no immediate little progress on its task, and recognized was an intervening circumnavigate with the goal, say at (9,9) with the path in Fig. 1. If a robot that that move from any further If it had made that it was aligned with the goal but there to the robot obstruction, FORR could activate a time-limited If the search algorithm were able to realign there before and remembered search algorithm that obstruction. reactions. ((636) (697) (7,7) (798) (878) (839) (939)) the goal (Happily, performance and executed any particular as implemented in two-dimensional situation. Aligning as a way of addressing that entire path would be proposed situation. Finally, consider a robot at R3, one that has no immediate then recognized is and does not recognize generally a good heuristic, so (6,6) might be a good choice. Then again, if the robot is having difficulty on the trip, trying a location might be helpful. Of course, be good choices. Since long-step heuristic might value them more highly. Achieving a consensus such as these is nontrivial. Pragmatic navigation, it has not yet visited, say (4,8) or (6,5), and (8,8) might two of those choices are in the general direction of the goal, a large steps speed travel, so (3, S), (6,3), the reactions the robot with the dilemma at R3 will be resolved here by FORR, aspires space, performance in origin and destination. learns its way around a new territory, a pragmatic navigator is resilient for a particular from a series of trips through competent time and expertise territory differ. Just as a person who detailed description pragmatic navigator of a detailed map, pragmatic navigation or make territory, a pragmatic navigator learns features of the territory its performance. As envisioned accurate; travels efficiently trip or remember trips whose origins and destinations through a campus does not retain a tree and rock, a of each the location of each ignores much travel history and many topographical details. Instead travel into a room or an extended wall. In a new initially performs as a competent novice, and then, as it from traveling to improve there, it uses that knowledge rather than absolutely here, most features are heuristic that describe a territory and travel experience they are useful approximations that support efficient relies upon features it more difficult, such as a door to provide that improves Instead of pre-engineered from heuristics in Section 4.) robust, across the territory, to changes 278 S.L. Epstch /Art(jicid lmlligence 100 (1998) 275-322 1 2 3 4 5 6 7 6 9 1011121314 1 2 3 4 5 6 7 a 9 10 11 12 13 14 Fig. 2. A problem and its solution in a maze it. Such representation through storage, retrieval, and computation. deliberately sacrifices detail in exchange for efficient travel learned about it appropriately through unmapped tasks for a perceptually-limited to context, both in a cognitively situation-based features there. This approach works because It is the thesis of this work that expert navigation territory can behavior, and care- the territory allocates control and plausible way. The first section of this paper in unknown agent find that so difficult, and pro- the that balances search, reactivity, and how for decision making, those navigation principles with learned that demonstrate responsible including guidelines these ideas in other domains. The final sections address related and future be achieved as the integration of correct reactions, fully balanced heuristics, many of which reference during responds frames navigation as a set of travel territory, explains why traditional AI search algorithms poses pragmatic navigation for learning and problem solving underlying architecture and heuristics. Section 3 identifies territory they can be learned. Section 4 summarizes and offers examples of how FORR coordinates knowledge. Section 5 formulates and reports on a series of experiments the strengths of pragmatic navigation its performance. Section 6 provides a full discussion of the results, for applying work. Algorithmic as an alternative. The second section describes FORR, features navigation principles for pragmatic navigation the FORR components details are reserved to the Appendix. and identify for 1. The task and an approach task was first suggested as a problem that would challenge [ 251. The robot’s world is a maze, a discrete, rectangular The pragmatic navigation the power of search algorithms grid with external walls and internal obstructions obstructed experiments (r, c) in a maze is the position array. A problem that is 30% the runs; this one.) A lacufion in the rth row and cth column, addressed as if it were an is to travel from an initial robot location R to some goal location G in the examples are on substantially in this paper are taken from actual in Fig. 2. (All themselves like the 14 x 14 maze larger mazes than S.L. Eptein /Artificial Intelligence 100 (1998) 275-322 219 its own coordinates, is to travel from (9,s) the coordinates of the goal, the dimensions that is, to find a (not necessarily optimal) path to the goal. a sequence of legal moves, to ( 1, 14). In any state, the robot senses In Fig. 2 the problem of the maze, and only or to the goal. The the distance north, south, east, and west to the nearest obstruction any robot does not sense while moving, only before a move. The robot also remembers useful knowledge is similar to privet-hedge mazes such as the one grown for royalty at Hampton Court, and to the computer game Maze Wars.) The robot knows in the current problem, but it is not given, and does not construct, an explicit, detailed map of the maze in Section 3. As the problem like the one in Fig. 2. Rather, level increases, the path it has thus far traversed this maze. (This domain trips through in previous acquired Intuitively, a legal move passes through any number of unobstructed vertical or horizontal the robot is at (r, c) to one where it is at (r’,c’) is true: line. More formally, a legal move is a transition it learns features of that map as described this task, even with a map, is nontrivial. in a locations from a state where such that exactly one of the foilowing l r=r’,c<c’andunobstructed(r,c+I),...,(r,c’), l r=r’,c>C’andunobstructed(r,c-l),...,(r,c’), l c=c’,r<r’andunobstructed(r+I,c),...,(r’,c), l c=c’, r>r’andunobstructed (r- The robot through if and only if there exists some path in Fig. 2 has 1 I legal moves: north to (7,8) and (8,8), east to (9,9) (9, 14), south to ( 10,8), and west to (9,6) and (9,7). A problem is solvable l,c),...,(r’,c). (R = lock nzovel loq . . loci-i nzove; lot; . . loc,_i movep lot,, = G) such that I?iove; is a legal move from Ioc;-i of a solvable problem minimum the level of difficulty of a problem right turns Manhattan distance it, with Manhattan distance 16, is indicated is the minimum value of p for which there is a solution, number of legal moves with which the robot can reach the goal. (Effectively, to loci for 1 < i < p. The level ofdifficulzj~ that is, the the robot must make to reach the goal.) Note that this is different from R to G. Fig. 2 is a level-6 problem; one six-move solution number of left or from the for is one more than the minimum there: ((9,8) (9,13) (4,13) (4,12) (2712) (2314) (I, 14)). Interchanging heading of the task is the subset of {north, east, south, west} that describes from R to G. The heading of the task in Fig. 2 is {north,east}. the robot and the goal produces another problem at the same level. The the direction is expected in the same maze. Lower-level problems to perform quickly. Speed can be measured as elapsed computation traveled. The robot task has several performance This problems is expected number of decisions, or path length expected locations is expected important distinction its performance here between criteria. The robot to solve multiple should be easier to solve. The robot time, is also can be measured as the number of distinct the robot is an the number of decisions and number of moves. As improve with experience. in a path. Finally, to perform efficiently. Efficiency visited or the percentage of repeated (Manhattan to learn; distance) locations (There should 280 S. L. Eptein /Artijicial Intelligettce 100 (1998) 275-322 below, described solution but consumes a decision solution.) is a step the system does some search resources. Thus, a move appears taken during problem that may not be part of the problem in the solution path, while in the solving, whether or not it appears exhaustive backtracking; is not amenable search requires search visits it visits a high proportion of nodes As Section 5 demonstrates, the task formulated above substantial too many nodes; on most hard problems search while large structure a very it requires knowledge search with a “sensible” is easily misled by deceptive problems where proximity AI techniques. Depth-first and repetitive. Breadth-first approaches space and maintains inapplicable because backwards. Best-first distance to the goal, valid robot must move away from the goal to reach it eventually. For example, placing robot at (9,4) For a large maze, explicit search would be extremely With FORR, pragmatic navigation offers an alternative. is not a (The goal might be hidden behind a long wall so that the the for open paths. Means-ends about to traditional its paths are long it in the search is analysis to reason such as Euclidean level-6 problem.) intractable. in Fig. 2 produces a deceptive the vicinity of the goal and the goal at (8,5) indicator of progress. inefficient, perhaps evaluation function, 2. The underlying architecture FORR furnished is intended that models the transition is a problem-solving and learning architecture experience where a sequence of moves to specific expertise, and capitalizes on methods from that people use [ lo]. solving as a sequence of reasonable decisions. A tusk is a to reach a desired from R to G in Fig. 2. A problem class is a set of related rooms or mazes that simulate office suites. is a set of related problem classes, such as grid-world mazes. A FORR-based general expertise FORR approaches problem problem-solving state, such as the robot moving tasks, such as mazes that simulate A domain system begins with a domain and some domain-specific knowledge, gradually acquires useful knowledge, problem-class-specific and probably should enhance program data that is potentially useful in a particular maze, but problem-class-independent a FORR-based such as “avoid dead-ends”. With task experience, such as dead-ends system. Although most useful knowledge useful knowledge items to pragmatic navigation of a FORR-based there are broadly is tailored that FORR provides and learns by default. Those is applicable relevant the number of moves made. Total learning experiences Openings, this paper terms such as “few ” “recent”, or “x%” indicate parameters the user. A full listing of the parameter values used in the experiments appears is the number of tasks attempted. stored as a tree, are the first x% of the moves recorded for a task. (Throughout that are preset by described here correct. This useful knowledge, after each task. Average to a specific domain, in the Appendix.) the performance are calculated task length , FORR’s three-tier hierarchical model of the reasoning process is a domain-specific but problem-class-independent, decision-making is shown in Fig. 3. An rationale, implemented to your destination”. Each Advisor is a “right reason”, procedure. Input to each Advisor is the current state of the world, Advisor such as “get closer as a time-limited 281 Tier 7: Reaction from perfect knowledge Tier 2: Search and inference triggered by situatian recognition Tier 3: Heuristic reactions Fig. 3. How FORR makes decisions. actions the current permissible from that state, and any learned useful knowledge about the current problem class. Each Advisor outputs any number of comments that support the actian commented or discourage permissible upon, and a strength, an integer and direction of the Advisor’s opposition.) Although themselves, procedures the world and respond with a rapid computation, there are no constraints on the nature of the comment-generating a FORR-based to sense the current state of as support, below 5 as above 5 are construed actians. A comment lists the Advisor, search. the current state of the world and what they know about that is, to avoid extensive Tier-l Advisors sense that measures the intensity is inrended in [0, lo] (Strengths opinion. system if they make a decision, fixed order. Each Advisor may have the authority problem class; predetermined, alone or to eliminate reactive and reference only correct useful knowledge. An important FORR-based to make a decision a legal action from any further consideration. Tier-l Advisors are in a navigator would be “if you see the goal directly ahead, go to it”. Only it is fast and correct. They are consulted tier-l Advisor the in a 282 S. L. Epstein /Art$&l lf~telligence 100 (I 998) 275-322 for or against one or more actions. Tier-3 Advisors are reactive when the first tier of a FORR-based to the next tier. Tier 1 is like the sense-compute-execute and perfectly accurate Tier-3 Advisors, in contrast, are not necessarily reactive system. system fails to make a decision does control default loop of a carefully constructed specialized view of reality correct in the full context of the state that can make a too, their reasoning process nor the useful knowledge to tier 3, all tier-3 is relegated is made. The decision before any choice In a FORR-based a to the goal”. Although a tier-3 rule in an expert system or a term in an evaluation there are two important differences. First, because a tier- to the same state may change with its reactions navigator, (incorrect or irrelevant) a heuristic, to comment correct. Once control they rely is guaranteed space. Each of them embodies plausible argument but far less trustworthy, because neither on which Advisors have an opportunity they arrive at is the action with highest good tier-3 Advisor would be “minimize is reminiscent Advisor for a search algorithm, function 3 Advisor may rely on useful knowledge, experience. Second, a tier-3 Advisor may have an inappropriate perspective but also learn to refuse to allocate computational control for decision making with an example As the results in Sections 4 and 5. in Section 5 indicate, total strength. your distance of a heuristic resources for a particular problem class, and FORR may learn not only to disregard it, to it. As a result, appropriate in tier 3 is neither obvious nor trivial. Further details appear rescue rescue, enough; and reliably some search an emergency situation-based the commander is necessary. FORR quickly time-limited, is based upon psychologists’ reports about human experts team is called from a sign after jumping is semiconscious. During debriefing [ 231. For example, suicide, where a person dangles is limited and the person tiers 1 and 3 alone do not solve the most difficult implements problems searches with the Advisors of tier 2. Situation-based certain in resource-limited behavior to the scene of an situations from a highway attempted after a overpass. Time secured successful to lift her to safety. He the semiconscious woman’s arms and legs, but then needed mentally the and tested four devices team lifted, one device at a time. When a device failed in his mental simulation, he ran times in simulation without an apparent the next. When flaw, he began the predominance evidence judges key features, procedural and nuclear power plant operators are that a situation and that those responses are not tested in parallel. it in the real world. Klein and Calderwood the fourth scenario to execute of the team describes how they immediately and cite additional in deliberation, Its triggers a set of for the purposes of this discussion, from studies of army commanders, that could hold her while responses, not solutions, setting bail, highway of this situation-based in 32 such incidents, business executives, ran several instantiated, engineers, retrieved, behavior describe juries [23]. Each tier-2 Advisor has a reactive highly-constrained set of possible solution a tier-2 Advisor as a sequence of decisions, from the “sense-compute-execute” that its method may be directly robot Advisor paths to circumnavigate is aligned with the goal but there and instantiates related trigger and a procedure that generates and tests a fragments. A solution emerges from rather than a single reactive one, a digression it recognizes triggers when fragment loop. A tier-2 Advisor to the current situation, for example, when the is an intervening wall. Execution of a tier-2 for example, like tier 1, but lacks tests one or more possible the intervening wall. Tier 2 is prioritized fragments, solution S.L. Epstein/Am&id lntellipm 100 (1998) 275-322 283 any guarantee tier-2 Advisor fragment. Once some executed control the outcome, robot, whether or not any fragment by the Advisor sequence of recommended FORR is implemented is returned of correctness. Until one of them produces a solution is ceded control and given limited that triggers tier-2 Advisor constructs a solution (one move at a time, subject to override fragment, each time to develop a solution is from tier 1) and then, regardless of to the that sequence fragment, to tier 1. Decisions during search are charged step becomes a move because it is recommended triggers or produces a as a result of the search. If no tier-2 Advisor steps, tier 3 will make the decision. in Common Lisp. To apply FORR to a domain, one specifies the its useful knowledge, and procedures its problem classes, is implemented domain, navigation King Minos of Crete, told Theseus how to find his way protected a great treasure.) The next two sections detail Ariadne’s useful knowledge a set of features principles to learn it. Pragmatic (Ariadne, daughter of that the labyrinth as space, and sketch Ariadne’s Advisors, navigation as the FORR-based for two-dimensional for path finding. system Ariudne. through 3. Learning to represent territory These Instead of a map, pragmatic navigation those that support efficient travel cfacilitutors) features constitute Ariadne’s useful knowledge, territory: (obsrructors). for a specific problem class from experience. Although knowledge is expected described program must be prespecified by the system designer, learning This section describes what Ariadne time limit, and learnin, 0 schedule in the preceding relies upon two kinds of features to describe a and those make it more difficult and are learned useful items in a FORR-based its learning algorithm, (after a decision, a task, or a set of tasks). it may be approximate, to enhance performance. Other than the default learns, when it learns, and how it does so. section, each kind of useful knowledge in FORR including 3. I. Facilitators Ariadne identifies three kinds of facilitators: gates, bases, and corners. A gate has, in from one large segment of space to another. A choice in successful paths. A corner offers theory, the ability base repeatedly appears as a counter-intuitive the possibility to provide a transition of a new direction. the dimensions it knows tests whether that offers a transition its quadrant has changed, of the maze, Ariadne can calculate quadrants. A gate from one quadrant of the maze to another. After each that is, if it has moved through a Since is a location move, Ariadne gate. If so, the robot’s current and the previous one. A gate may not always be helpful; between quadrants 3 and 4 in Fig. 4, but it offers access gate is stored with its exterzf (rectangular can be reached) extent of (8, 10) in Fig. 4, for example, learns (9, S), and (6,5). Ariadne only the current quadrant (8, 10) is a gate to little of quadrant 3. Each it in a hash table whose key is the sorted pair of quadrant numbers. The (9, lo), in Fig. 4 is the rectangle with vertices the gates it visits, so many is learned as a gate between of the locations approximation for example, from which locations location (6, lo), 284 S. L, E/w/cirr /Artificitrl Irxtelli~ence 100 (1998) 275-322 Quadrant 2 1 2 3 4 5 6 7 6 9 1011121314 Quadrant 1 m q obstruction ETJ gate Quadrant 3 Quadrant 4 Fig. 4. After IO tasks, gates learned for a simple maze. 1 2 3 4 5 6 7 8 9 1011171.114 1 2 3 z 6 7 0 9 10 11 12 13 14 1 2 3 4 5 6 7 6 9 1011121314 m obstruction - path 0 R G # derived base robot goal base frequent plan 1 2 3 4 5 6 7 6 9 10 11 '12 13 14 (a) Fig. S. (a) A solution path and the bases bases. that arise from it. (b) A plan for a task formulated from learned @I the definition that satisfy during some trip, the robot moved into (14, 10) from quadrant 3, and therefore (14, IO) as a gate. Although and therefore was not learned as a gate. (That position as another kind of facilitator.) The subdivision quadrants) was deliberate. Specifying to manage, while fewer areas provide of gate were not learned and are not marked. For example, learned as such, identified below (the II areas produces as many as ,,CZ gate categories information. too little transition is also a gate, it was never experienced into only four areas of the maze is, however, (14,9) A base is a location in a maze that appears town, people regularly give directions beginning to have been a key to a successful “First you path. In the author’s home go to the Claremont Diner.” Although Diner burned down 15 years ago, and there it served memorable cheesecake, is nothing particularly the Claremont about significant S.L. Epstcin/Art$ciuI Intelligence IW (1998) 275-322 285 I 2 3 4 5 6 7 6 9 1011121314 3 obstruction dead-end hallway Fig. 6. After IO tasks, corridors learned for a simple maze. it. What is significant first corrects (not necessarily that has replaced that affords ready the path to eliminate shortest path) access the car dealership location a IO-mile radius. A base is such a location. Bases are learned after a successful the algorithm base is a location base is not a dead-end or G itself, and solution that circumvent walls contribute only their most extreme positions opposite headings. Bases are stored in a hash table with theirfrequency have been are circled became bases. is that the diner was at a in to other locations task; digressions. A from R to G. A fragments constructed by tier-2 Advisors the original (the number of times they from one solution path corners in that corrected path that was not in the heading identified in Fig. 5(a), where the heading was {north} and the eastern-most in different problems). The bases learned loops and unnecessary Bases facilitate in Ariadne some primitive, high-level planning is a sequence for Ariadne’s decision making. (ba bi 62 . . . b;_l b; b;+l . . . 6, b,+l) where bo is the A plan location, b,,+I is the goal, b; is a base for i = 1,. . . ,n, bi is aligned robot’s current vertically or horizontally with b;+l, and either b;_l is closer to bo than b; is, or else bi+l is closer to G than b; is. Plans are constructed search on aligned bases, for bases of higher frequency, as if there were no obstructions. A plan with preference that prevents fails when in formulated to b;+,. An example of a plan Ariadne its move tasks in the same maze are indicated Fig, 5 (b) , where the bases learned after 20 level-6 by their frequency values. In Fig. 5(b), are the keys to the only route between the eastern and western portions of the maze. Others, such as (7, 13) and (9,9) obstruction for a task is shown intersections, is a passageway of width one that either has a single exit (a dead-end) is at some bi and there is an intervening some bases, such as (14,5) like the Claremont Diner. from bidirectional lie at important A corridor and (14,9) the robot is a straight hallway, In Fig. 6, there is a hallway lie in the same row or is a hallway. A yiye or the same column. and a pipe from ( 14, 10) to ( 14, 1 I ) . Some pipes offer a view of the space after their far end. For example, to the ends of the pipe in Fig. 6 the robot can move not only from it to (14,12). Other pipes do not offer a view of the space after their far end, but since a pipe is not a dead-end, (14,9) to (14, 1 I), but also beyond that is, its endpoints that promises from (13,5) to (14,B) (14,lO) from 286 S. L. Epstein /Art$cid Intei/igencr 100 (I 998) 275-322 from (4, 10) in Fig. 6 the robot can see a turn in some other direction. For example, it; that promises a turn at both ends of the (length one) pipe at (4, 13) but not beyond far end of a pipe is called a corner. A corner can the far end. Such a turn-promising in which the robot actually be helpful when the move to it is orthogonal in Fig. 6 and the goal seeks to travel. If, for example, to were to the south, moving east to the known corner at (4, 13) promises travel either north or south. A corridor is learned when, from the current state, the robot has only one or two moves. The two endpoints of a corridor serve as the keys to a hash table which also indicates whether or not they are for a dead-end. Corridors are enlarged and merged that Ariadne experiences are learned. together as necessary. Like gates, only corridors the robot were located at (4,lO) to a direction the ability 3.2. Obstructors Ariadne that are less narrow spaces is a linear approximation four kinds of obstructors: and bottles are circumscribing identifies corridors. Chambers stricted of contiguous barrier obstruct movement it is a dead-end, it is a non-corner pipe. In the latter case, movement locations because from is no reason there; if traveling east efficiently, one would go through its internal there afford access only either because rectangular chambers, bottles, barriers, and certain of re- than a corridor, and have one or more exits. A obstructed positions. A corridor may such as (2,9) in Fig. 6, or because into it necessitates an extra decision approximations to pause at ( 14, IO) or ( 14,11) unless to each other. For example, that pipe, not pause within ( 14,9) Learning an obstructor through is often space. There are several measures of such confinement, the robot has been hampered triggered when in Fig. 6 the goal lies it. in its all to move ability termed recently constrained: l The area of the territory covered by the last x% of the moves was less than y% of the total maze area. l All the legal moves have been visited at least once. l The last x% of the moves was less than y% of the possible maze locations. l 2% of the recent moves were visited at least once before. A chamber is an irregularly shaped space with an access point and an approximate rectangle, the chamber is a bounding the robot moved that affords a view outside a compact, heuristic approximation to access point reachable of the in each direction one can go in the chamber. The access point is a location it, but need not be on the border of the shows a chamber with extent 1 north, 13 east, 5 south, and 9 west. to the south. really constitute limited and the task has been in its location before, there are few legal moves to locations not yet visited on this task location was not the result of a tier-2 fragment. the dimensions extent. The extent furthest within extent. Fig. 7(a) When In principle, a single room-like. The learning underway current or the goal is remote, and the current The to its for a chamber algorithm current sensing, and then tries to move to a location where the chamber appears both all locations large chamber, but the robot has been recently constrained from the chambers (4, 13) it saw beyond is triggered when learns are more initial position for a chamber and has been first estimates that Ariadne the robot’s that extent for some according algorithm learning time, S. L. Epwitl /ArtiJiciul Intelligence 100 (1998) 275-322 287 1 2 3 4 5 6 7 8 9 1011 121314 1 2 3 4 5 6 7 8 9 1011 121314 Kev q obstruction fl extent A access point N bottleneck (4 Fig. 7. (a) A learned chamber, with its extent and access point. (b) A learned bottle with its neck and extent. identifies legal locations, its current location offering the robot scans once horizontally, location the largest view scans once again vertically. higher and wider. From from the scanned are no horizontally-adjacent procedure the extent, at least one of which chamber’s extent and access point on a list. The scan for the chamber from (4, 12). A new chamber may subsume an old one, in which case it replaces the list. Otherwise, one access point. and then (If there first.) If the that enlarge the in Fig. 7 began it on chambers are not merged, and they may overlap or have more than the vertical scan is performed (access points) a sequence of one or two locations is previously unvisited during this task, it records A bottle contiguous several spots, is not corridor-like, in both directions along however, are learned deliberately that was visited more than once, and is repeatedly is another useful knowledge description of a constrained the path by immediately neighboring and does not ultimately similar subspace, to a chamber. Chambers, by search, whereas bottles are learned without search from analysis of the entire path after a task is completed. A potential bottle begins with a location extended it includes than x% of the area of the maze. Once a bottle its neck (not necessarily entry and/or stored in a hash table as an extent and a neck. Fig. 7(b) north, 6 east, 14 south, and 1 west, and neck (6,2). is a linear approximation IO tasks learned positions only if encompass more is identified and its extent computed, is identified. Bottles are shows a bottle with extent 12 A barrier the barriers ( I I, 13), for example, is an approximation corner of the maze. Barriers are learned different ways, depending context in which clearly prioritized circumnavigate of the goal. and another treat dead-ends fragments, of a wall that obstructs movement. Fig. 8 shows to right the search they arise. There are four tier-2 Advisors whose search paths have preferences to in which side an intervening obstructions. All of these searches solution In each case, in a simple maze. The barrier of the irregular wall they move. Two attempt to shift space. Whether or not these Advisors produce for the direction obstruction, follows along contiguous barriers are detected to the opposite their searches in the lower as obstructed one attempts exit point) the paths followed. (13, 10) from from upon after 288 XL. E~~.rteir~/Artijicitrl Intelligence 100 (1998) 275-322 1 2 3 4 5 6 7 8 9 1011121314 Kev obstruction - barrier Fig. 8. After IO tasks, barriers computed for a simple maze. the learning algorithm search path. Experience encountered; vertical wall between program encountered object with endpoints, recourse and Ariadne’s in the rationale is a function of the preferences the to tier 2 determine which barriers are the irregular to describe the eastern and western portions of the maze in Fig. 8, had the difficulty when required is an slope, intercept, and length. it. Each retained barrier that produced to avoid learned thus there could have been some barriers 4. Pragmatic navigation in action rationale; in general, An Advisor in Ariadne, an Advisor is a narrow decision-making is characterized by the tier in which it resides: rather than for some particular maze. As described is designed for maze navigation in Section 2, an Advisor reactive Advisors in tier I, situation-based Advisors too the dimensions of the maze, that input to an Advisor the location of the robot, the location of the goal, the legal moves for the robot, the trip history, and the features of the maze. Although every Advisor has access to the entire store, most apply no more than one or two items. useful knowledge is always the same: in this domain, in tier 2, and heuristic Advisors in tier 3. Recall Table 1 lists Ariadne’s 32 Advisors with the useful knowledge in Section 6.) Advisors of the Advisors’ appears order. Full descriptions appear the experiments are perfectly correct, are situation-based a sequence of moves on the origin of Advisors discussion 2 appear in their prioritized their parameter values during Advisors 2 Advisors to produce trigger the identified solution by a tier-2 Advisor are tested serially. The 20 tier-3 Advisors are reactive, heuristics may reactive procedures procedures or oppose any number of legal moves that they then mandate. Each that signals fragments that embody path-finding situation. The solution and a search method that do time-limited and do no search its applicability that attempts commonsense recommend to address they reference. (A in tiers 1 and and behavior in the Appendix. The four tier-l that decide quickly. The eight tier- search in an attempt tier-2 Advisor has a to compute fragments generated time-limited in the maze. Each that have not already been S. L. Epsrcitl /Art#ciul lntelli~enence ICQ (I 998) 275-322 289 Table I Ariadne’s Advisors with tiers I and 2 in prioritized order Tier Advisor Rationale Useful knowledge I I I I 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Victory Can Do No Way Pipeline Roundabout Outta Here Probe Patchwork Other Side If the goal is reachable by a legal move, go there. Move adjacent to the goal. Avoid dead-ends. Avoid internal locations in straight corridors. - - Corridors Corridors Circumnavigate intervening obstructions. Barriers, corridors Exit chamber or dead-end not containing the goal. Chambers, corridors Determine the current extent and try to leave it. Repair plans. Bases Move robot to the opposite side of the goal. - - Super Quadro Search for entry into the goal’s quadrant or into Gates a new quadrant. Wander Move far in an L-shaped path. Barriers, average task steps, bases, corridors Humpty Dumpty Seek barriers. Adventure Move to thus far unvisited locations, preferably Barriers toward the goal. Been There Discourage returning to a location already visited during this task. Chamberlain Move into a chamber that contains the goal; avoid Chambers Contract Take large steps when far from the goal, and small a chamber that does not. Cork Corner Crook steps when close to it. Move into a bottle that contains the goal; avoid a Bottles bottle that does not. Move to the end of a pipe that promises a turn. Move to the end of a crooked corridor. Corridors Corridors Cycle Breaker Stop repeated visits to the same few spots. Detour Done That Move away from barriers that obstruct the goal. Barriers Discourage moving in the same direction as be- fore from a previously-visited location. Giant Step If recently confined, take a long step, preferably Goal Column Align the robot vertically with the goal, if it is toward goal. Goal Row Align the robot horizontally with the goal, if it is not already. not already. - - - - - - - - Home Run Move toward bases. Bases Hurry Take big steps early and small steps late. Average task steps Leap Frog Execute opportunistic plans. Bases Mr. Rogers Move into the neighborhood of the goal. Opening Plod Quadro Begin as a previously successful path did. Openings Take a one-unit step, preferably toward the goal. Move into the goal’s quadrant or into a new Gates - - quadrant. 290 XL f+sfei~z/Art~ciul Itrrelligence 100 (1998) 275-322 Move Comments (Advisor and strength) Score (3.8) (4,X) (538) (6.3) (6.4) Giant Step 8, Adventure 6 Giant Step 8, Adventure 6 Adventure 6, Plod 6 Home Run IO, Giant Step 10 Home Run 8, Mr. Rogers 6, Giant Step 10, Adventure 8 (635) Home Run 8, Mr. Rogers 7, (636) (67) (7.8) (88) Giant Step 10, Adventure 8 Home Run 8, Mr. Rogers 8, Giant Step 10, Adventure 8, Goal Column 10 Mr. Rogers 7. Adventure 8, Plod 8 Mr. Rogers 9, Been There 4, Plod 8 Mr. Rogers IO, Giant Step 10, Been There 4 4 4 2 10 12 13 19 8 6 9 Kev q obstruction G goal R robot Fig. 9. A state in the midst of problem solving, and how tier 3 votes on the next move. The strengths were converted from [ 0, 10 I to 1 -S, S I and then summed to produce the scores. by No Way. Although eliminated for navigation, the simple ever run out of time on level 10 problems. none should be trusted them support ideas behind every tier-3 Advisor captures a reasonable rationale to decide alone. All of them vote together, and rapid computation. Given 10 seconds, none has Pragmatic navigation with all the Advisors of Table decision may be mandated by tier fragment from tier 2, or a top-ranked move selected by vote in tier 3. is achieved by the execution of FORR’s Fig. 3 decision process tier in turn. A single and executed I, or a solution is delegated I. Control submitted to each The nature of the voting process that chose to comment. The best supported move is clearly (6,6), at R3 from Fig. 1, a state in the middle of the fourth in tier 3 is best explained with an example. Fig. 9 trip Ariadne this particular maze. The program has, by this time, some useful knowl- it to all the tier-3 Advisors, along with a list of for this state, produced by the this territory, and forwards the position was key in a previous problem recaps the dilemma made through edge about the IO legal moves. Fig. 9 shows all 30 tier-3 comments seven tier-3 Advisors for a variety of reasons: closer to the goal than several other of the legal moves (Mr. Rogers), than several other moves (Giant Step), task (Adventure), is not “on the way” to the goal, once Ariadne task, Roundabout to (9,9), triggered and drove Fig. 9 demonstrates thesis: to make good decisions. Thus Ariadne does not seek to construct explanation that circumnavigates from which that many “right” for its decisions, only to make satisticing ones. the robot FORR’s (a tier-2 Advisor repositioned reasons may (Home Run), it is it is a larger step the current (6,6) an intervening the problem was quickly obstruction) solved. indeed combine logical a flawless and it aligns the robot with the goal (Goal Column). Although it has not been visited before during the robot there during this For the problem in Fig. IO, Table 2 demonstrates matic navigation can have surprising how the satisficing nature of prag- results. Table 2 is an annotated version of three S.L. E~~stei~l/Art~ficiul hltelli~ence 100 (1998) 275-322 291 1 2 3 4 5 6 7 8 9 1011121314 uction move obstrl q legal R robot G goal Fig. IO. A level-b problem. Table 2 Three solutions to the problem in Fig. IO Generator’s path Second Ariadne solution First Ariadne solution 8 decisions 26 path length IO decisions 22 path length (14, 14) (13, 14) (13, 12) ( 14. 14) (13.14) ( 13, 12) identical (( 12, 12)) Patchwork to generator fragment (14, 12) (lZl2) 42 decisions 58 path length ( 14,14) (13.14) (IO, 12) (l3,12) ( IO, 12) ( 14.12) a good, but indecisive choice ((IO, 13) (7,13)) Wander tries to help (l4,9) (9.9) (9.2) (4,2) (4.4) (IO, 12) a shortcut! (IO, 13) (IO, 13) (7.13) ((7, 13) (797) Patchwork forces a perfect plan (6,7) (634) (434)) (3, 13) overshooting (7.13) (7,7) (637) (6.4) (4,4) (3, 12) partial correction (4,12) alignment (4.8) too far toward the goal Other Side’s fragment to (5,3), interrupted by Victory at (6.4) (4.4) to Fig. 10’s level-8 problem. The first column solutions tion, the third column solution after 10 different In Ariadne’s somewhat baroque plan from the bases it had learned: try at it, and the second column tasks, including as soon as the robot moved is the problem generator’s solu- is Ariadne’s this one, in the same maze. a to (13,14) initial learning second solution, it formulated is Ariadne’s level-8 ((13,14) (13,121 (12,121 (12.5) (9,5)(9,13) (7913) (7>7) (67) (64) (4,4)). 292 XL. Epstein /Am&id lnteiligpm 100 (1998) 275-322 then the original to ( 10, 12), to ( 12, 12) and Ariadne executed from ( 12, 12) to ( 12,5) the ordinary decision-making triggered at ( 13, 12). in its allotted search (( 12, 12)). Ariadne resumed. Ariadne recognized (7,13), the task. The resultin g solution entailed more decisions, but produced (The problem generator’s In contrast, Ariadne’s fragment helped, but the more the first step of the plan, and then Patchwork The movement is illegal, and Patchwork, time, did not find a patch to the plan, so it forced only the fragment moved forged ahead that and completed a shorter path length solution first solution began well, if a bit indecisively. Wander’s primitive an alignment then shifted plan was now fully executable led the robot to a premature alignment with the goal at (3,13), that knowledge helped avoid the second in number of turns, not in path length.) than the problem generator’s to ( 10,13). Patchwork the newly-accessible is optimal only tier-3 Advisors time around. triggered, solution. process from 5. Empirical design and results The data described here was produced when Ariadne generated a maze and tested there of different the performance results from 10 runs (i.e., 10 randomly-generated mazes) were averaged experiment. Throughout 95% confidence this section, cited differences are statistically reasoning agents. Because FORR is nondeterministic, to produce an at the significant Experiments were performed level unless otherwise stated. for problems II is generated by selecting at some fixed a random, unoccupied level of difficulty II. A for R. location at level the algorithm extends the robot’s legal move. Each newly marked problem location, marking all locations On the first iteration, location becomes an element of reachable by a single iterations, each element of the fringe is extended and removed thefringe. On subsequent from the fringe but remains marked, and a new fringe fails, the robot location R is discarded and the process begins anew. After n iterations, every for G and one is selected at random and element of the fringe locations. The marked. The problem generator’s number of locations is estimated in the maze while the problem is generated. that would have been visited by a breadth-first as the total number of locations ever marked is thus the sequence of marked search algorithm If any iteration is a possible is formed. location solution 5.1. The ablation experiments ability after demonstrates Ariadne’s The first set of experiments and explores which components it has never before experienced, problems gram are necessary random mazes, that is, 20 x 20 mazes 4, 6, 8, and IO. Dimensions possible problems at the specified mazes with sqme fixed percentage of obstruction has no routines designed hard problems, in Fig. 10 for example, irregular wall running to solve of the pro- to achieve such performance. These experiments were performed on levels to provide enough the generation of square to formulate the lengthy, and the deceptive nature of the distance that were 30% obstructed, with problem it offers ample challenges. Note, level of difficulty. Although frequency were selected and obstruction to (6,9), learning ( l(4) from S.L. Epstein/Art$&l Intelligence 100 (1998) 275-322 293 complexities and (7,4), the full version of Ariadne was given 20 learning problems (8,3) between In each maze, pairs) at a fixed level of difficulty. Then 10 newly-generated same maze and level of difficulty were offered to all the agents with learning After learning, l Ariadne. l The Reactive agent, an ablated version of Ariadne that the problem generator exploits quite nicely. (R and G for the turned off. the following agents were tested: that used only the tier-l Advisors reactive decision making alone. If more than one legal move to simulate correct, was left after tier 1, this agent made a randomly-selected move. testing problems l The Reactive+Searclz agent, an ablated version of Ariadne that used only the tier- 1 and tier-2 Advisors behavior but without heuristic agent made a randomly-selected move. to simulate reasoning. reactive decision making with situation-based If no decision was made after tier 2, this agent, an ablated version of Ariadne to simulate correct and heuristic that used only the reactive decision making that applied all the Advisors but made no useful l The Reactive+Heuristic tier- 1 and tier-3 Advisors without situation-based behavior. l The No-Learn agent, an algorithm available. l The No-Plan agent, an algorithm knowledge the Advisors Patchwork ). except those involved with planning that applied all the useful knowledge and all (Leap Frog, Home Run, and experiments Separate (equivalent Euclidean distance to blind search), also tested a random agent and a best-first agent that selected that did best-first random legal moves search with the to the goal as an evaluation function. The learning problems established a useful knowledge base for those Advisors that it reached the decision agent reached the reasoning included all exploration during depend on it. All agents using such Advisors had equal access to the learned knowledge. the goal A problem of either kind was terminated when or when tier-2 step limit which limit was set to 200 on level 4, 300 on level 6, and 400 on level 8. On search. This to solve each problem. level IO, it was set to 1000 to give the agents ample opportunity The IOOO-decision so that Ariadne acquired additional useful knowledge In preliminary limit permitted more experience, to support better comments. testing on IO random mazes the efficacy of the random trips per maze with a IOOO-decision cutoff, agent, Ariadne was permitted 20 learning and then Ariadne and the random agent were tested on 10 trips per maze with a 200- decision cutoff. Although Ariadne solved 99% of the level-4 in the 10 randomly-generated Ariadne’s 27 I .86) and entailed The random agent was therefore eliminated mazes, to solved problems were significantly the random agent was only able to solve 36%. In addition, to (17.53 as opposed than the random agent’s, from the full ablation experiment. (23. IO instead of 166.06) testing problems fewer decisions to determine solutions shorter Table 3 reports the results in each experiment. is the Manhattan through one or more 10 runs “Distance” move difficulty. for the ablated agents and Ariadne averaged across In Table 3, a “location” along distance the in the grid. square is a distinct to the goal. Since a step may locations, path length varies among problems of the same tier-2 search or solution, taken during the path “Decisions” is the number of steps 294 Table 3 .“.I>. El'.~reir~/Ai-t~~citrl lntellip=nce 100 (1998) 275-322 The performance of Ariadne and ablated versions of it after learning in a particular 20 x 20 milze in Ariadne’s world. Results are averaged over runs in IO mazes Agent Distance Decisions Moves Locations Triggers Time Solved 4-step problems (generator path length 12.47) Reactive Reactive+Search Reactive+Heuristic No-Learn No-Plan Ariadne 124.32 3 I .99 47.1s 2.5.10 16.95 16.38 7.5.78 5 I .07 18.66 53.98 25.14 23.90 46.4 I 16.38 16.82 13.01 IO.49 10.02 24.69 15.38 9.39 9.9 I 9.82 9.36 6-step problems (generator path length 19.19) Reactive+Search Reactive+Heuristic No-Learn No-Plan Ariadne 53.06 62.63 Sl.34 30.14 26.36 102.60 37.48 I 19.46 47.98 39.04 28.9 I 24.00 28.56 26.98 18.71 IS.87 13.10 17.81 17.05 14.58 g-step problems (generator path length 25.62) Reactive+Heuristic I IS.37 No-Plan Ariadne 43.74 41.3s 99.14 73.59 83.89 41.64 26.72 2.5.98 19.42 24.47 23. I2 IO-step problems (generator path length 3 I .23) 4.64 - 3.18 1.65 1.44 8.78 - 7.29 2.61 2.20 - 5.56 4.59 I.58 81% (0%) 0.5 1 96% (0%) I .I2 0.78 0.43 0.44 0.99 3.97 I .59 0.9s 0.91 99% (0%) 91% (0%) 98% (0%) 98% (51%) 88% (0%) 95% (0%) 86% (0%) 99% (0%) 97% (38%) 18.00 84% (0%) 2.22 2.20 95% (0%) 93% (14%) No-Plan Ariadne 96.3 I 72.48 79.68 60.44 39.3 I 35.13 24. I4 23.70 14.94 13.95 8.88 5.95 98% (0%) 97% (19%) those moves is reported as “locations”. tier-2 Advisor executed. Distance, moves, and locations is the number of steps in the solution. The number of distinct while “moves” locations the actually visited during reliance of the system on tier 2; it is the number of passes through Fig. 3 during which are computed only over any look somewhat better than solved problems. time is execution they actually per testing problem, solved by Ariadne is also listed, with the number than the problem generator’s in parentheses. solution “Time” in seconds. The percentage of testing problems solved as well or better the ablated agents the easier problems.) (This are, because “Triggers” measures they solve to make tends from testing on all subsequent the decision-step levels if it solved fewer limit at any given level. On level 4, An ablated agent was eliminated than 90% of the problems within the Reactive agent solved a surprising 8 1% of the testing problems, but with solutions than Ariadne’s, and required that were significantly typically when large portions of far more execution the randomly-generated the way the upper right corner of the Reactive agent’s behavior was more likely maze were unreachable is in Fig. 1. In such a maze the substantial random component to be effective. Mazes where Pipeline longer, entailed many more decisions time. The Reactive agent succeeded from the robot’s starting point, S. L. Epstein /Art@irrl Intelligence 100 (1998) 275-322 295 Table 4 Percentage of the maw visited by Ariadne and by breadth-first search Level Breadth-first Ariadne 4 6 8 IO 40.26 64.62 86.25 95.63 3.56 5.58 8.83 9.19 was able to veto many choices also supported this agent well. Nonetheless the Reactive agent was eliminated after level 4. as it substitutes agent produced agent and the No-Learn On level 6, Reactive+Search significantly (Note the increased number of triggers longer paths that required many more decisions. after level for No-Learn, 6. Despite it was able to solve 95% of the level-6 problems, but on level 8 its inadequacy became clear. There, its paths were far longer and its solution greater. On level 10, Ariadne is clearly solves, although once it failed to solve a problem on which No-Plan succeeded. search for knowledge.) Both were eliminated agent was retained because its long paths, the ReactivefHeuristic shorter paths for the problems time substantially than No-Plan, faster overall and produces it There In reality, however, is one important benefit of planning. As discussed generator produces a problem whose difficulty than path length. more turns, and Ariadne found a solution often did so, more level 8, and 19% on level solutions distance. in Section 4, the problem is predicated on number of steps rather to find a shorter solution with that none of the ablated agents ever In contrast, Ariadne than half the time on level 4, 38% of the time on level 6, 14% on that most of the rest of Ariadne’s outlier or two driving up the average are near optimal, with an occasional than the problem generator’s. frequently does so. Observe it is often possible as good or better IO. Inspection indicates the robot search and heuristic search with an evaluation to the goal. Table 4 compares the efficacy of Ariadne’s problem solving, compare breadth-first it with two standard to the equal the fraction of the search In from to the robot from its starting position visited by breadth-first To measure AI techniques: Euclidean distance locations accessible with that visited by Ariadne. Ariadne only visits a small fraction of the locations. contrast, breadth-first large fraction of the maze. This somewhat understates breadth-first first learned on 20 problems, best- in a separate experiment function was first search with only able to solve only 26% on level 10 within 1000 steps, and averaged path lengths of 87.65 on these solved problems, versus Ariadne’s 62.99 path length with a 93% success rate. Best-first search averaged 164.48 seconds per problem, Ariadne 4.23 seconds. the same problems while exploring an increasingly executed search, whose many repetitive subpaths go uncounted here. By comparison, to the goal as its evaluation of 10 runs where Ariadne the cost of a physically the Euclidean distance search solved In summary, as the problems become more difficult, Tables 3 and 4 show that several search reaches an increasing percentage of the accessible and things happen: breadth-first unobstructed trigger more frequently, the search-oriented tier-2 Advisors locations, 296 S. L. E).win /Art(ficitr/ Intelligence 100 ( 1998) 275-322 Fig. I I. Some non-random environments: (a) warehouse, (b) for clarity. furnished room, (c) office. Grids are omitted (b) of to solve the ablated agents tier 2 offers a measure of reliability the ability FORR with lack. Although the successful paths of the ablated agents are extremely in place, Ariadne gets the robot the way. fewer alternatives inferior. the other versions solutions, tiers to the goal more often, more quickly, and considers of suboptimal long. With all of FORR’s this work was predicated on the acceptability becomes markedly and achievement the problems along 5.2. Peqormance in non-random environments Although random mazes present ments which are not random. To test Ariadne’s intended furnished must be quite large to produce enough even somewhat challenging face environ- real navigators three other classes of mazes to model more realistic worlds were constructed. These maze classes represent rooms, warehouses, and office suites on a 40 x 40 grid. The non-random mazes challenges, robustness, interesting problems. The warehouse maze models a single room, much like the typical basement, garage, specify (rectangular The furnished for a warehouse maze places objects room maze models a room whose obstructions attic, or storeroom. The generator the size and number of obstructions) these objects, them, and the minimum distance an object must be from the outer wall of the grid. Objects may not overlap, but may be contiguous if the parameters permit. An example of a warehouse maze appears at random on an empty grid. Parameters the amount of space around in Fig. 11 (a). are objects that do rows of desks or a not require a particular by this model. room with furniture the size of the objects and the amount of Like the warehouse maze, parameters first space between the perimeter, and then in the center section. Although precise along (but not touching) the perimeter are placed sequentially locations are chosen at random, so that there is some measure of balance. Objects along the perimeter are also allowed to overlap. As a result such as a sectional couch. An example of a furnished the objects often resemble various pieces of furniture, regularity. Thus a classroom with regular a television would not be encompassed room maze in two passes: them. Objects are placed the objects along in Fig. 11 (b). in a furnished room appears surrounding specify S. L. Epstein /Art[jicitrl lntellipmce 100 (1998) 275-322 297 The ofice jnaze models a single floor in an office building. Such an environment (the ofices) layouts are possible, should make good use of space and make all subspaces though several outer rectangle of offices (those gular hallway. The space framed by the hallway Parameters determine most with a door to the hallway. As in the real world, corner offices are somewhat and oftices adjacent hallway) the inner core are slightly Office mazes example of an office maze appears accessible. Al- the generator here produces only mazes with an that could have windows) bordered within by a rectan- is an inner core of connected offices. the size of the hallway, and the number of offices along each wall, larger, than the to model private spaces accessible only through adjacent offices. Offices within larger than those on the outer rectangle, and have more doors. room. An include a few objects placed as if an office were a furnished to a corner office open only onto the corner office (rather in Fig. 11 (c). problems in mazes the program Ariadne was developed learned on 20 different problems (e.g., a warehouse or a furnished for random mazes. Without any changes edge or the Advisors, we tested problem class on each run a new maze generated. Ariadne on 10 previously-unseen the offices and warehouses, problems at any higher perimeter and the furniture provides mazes were 4 times mained at 1000. Ariadne solved all the furnished as well or better than the problem generator’s problems in the useful knowl- like those in Fig. 11. In each room) was in that maze, and then was tested in the same maze. Level-8 problems were run on room with enough the these new limit re- room problems easily, 49% of the time It also solved all the warehouse ready access to most locations. Although the decision-step readily, 4 1% as well or better than the problem generator. larger than those in earlier experiments, level than 4, presumably because the required gap between but it was difficult to find a furnished solution. than the 1000 decision-step the problem generator’s, The office mazes presented a greater challenge. Only 5% of Ariadne’s solutions were and in several cases Ariadne did not limit. (In contrast, Ariadne devoted to furnished to warehouse that in most cases the solution was near at hand, but the search. One obvious for linked chambers, where the access point it in offices. that grid worlds, as good or better solve the test problem within an average of 42.83 decisions problems.) indicated Inspection corner offices had been sufficiently deceptive solution would be to create a representation for one chamber would not have been appropriate Other Ariadne’s knowledge and that its satisficing approach scales. is in the extent of the next. Although to add it to Ariadne this is readily programmable, to facilitate solutions for most two-dimensional to demand additional that, this testing room problems, to be evidence in non-random representation environments is adequate and 84.79 appears than 6. Discussion 6.1. Why does FORK work? FORR works because react, to search, or to guess priate, it allocates control appropriately. Control here is whether to is appro- foolish errors and guarantees easy right reaction intelligently. When a quick and obvious tier I responds correctly. Tier 1 prevents 298 S. L. Eprein /Artijicitr/ Irztelligence 100 (I 998) 275-322 needs that should be addressed with a particular search answers. When pre-identified tier 2 does so. Tier 2 provides appropriate, more thoughtful, more costly routine arise, tier responses. Rather tier-2 1 monitors if neces- tier 1 has prevented obvious mistakes and search should not be an option, sary. When tier 3 formulates In this way, search is minimized, and no egregious errors are committed. the safeguards of tier their execution, decisions are timely and well-founded, than reproduce fragments tier-2 Advisor, them a guess as a compromise I in every reasonable heuristics. and can interrupt among during FORR also works because it responds to context appropriately. Context here is how the in the domain, and how those reasons should in the tiers). A FORR-based system knows (Advisors) to one another for doing (their assignment what one knows affects how and what one decides. A FORR-based right reasons relate system also knows what is worth learning to apply acquired the FORR-based on general, prespecified principles but also on information it (again, in many ways, and applied system knows exactly when it. Decisions acquired (useful knowledge), to and sequence the Advisors). Useful knowledge can be represented in many ways. That flexibility brings how to learn it, and how in many ways, to bear what are based not only from experience. it needs things to note It is important that the Advisors of Table 1 and the useful knowledge of Section 3 were all developed only for random mazes. The other problem classes are quite recent and no changes of any kind were made to adjust for them. Domain knowledge tool. Of course, (here, useful knowledge, Advisors, and learning methods) of an architecture does not entirely validate it. The interested a one-domain demonstration two-person, is referred reader to play [9,10]. To date Hoyle has learned perfect to play 18 information, different games as well or better the best human opposition. The games Hoyle simple; none of them has a search space larger than several billion plays are relatively and in many ways play them states. People, however, find these games quite challenging (tiers 1 and 3 very much here), a set of tier-2 Advisors [ 361. Although Hoyle’s Advisors are all reactive is currently under development. to Hoyle, a FORR-based games than is a powerful for learning finite-board like Hoyle program from location planners How would Ariadne extend to other path-finding domains? Path finding on a printed to reason backward the robot. This suggests additional the goal’s as well as forward that direct the robot’s approach with the facilitators and obstructors already map would work from correct knowledge and provide the opportunity from Advisors place, such as tier-2 bidirectional legal steps from the goal. Path finding part process: long-distance like those currently performed. One local search would join R to the chosen highway goal location G. The long-distance algorithms or be done in a FORR-based to facilitate knowledge in that would target positions one or two known require a 3- in large scale space would probably travel through a network of highways plus two local searches location route to the segment could either rely on standard graph search is structured solution would require additional useful travel, a FORR-based (such as highway system. Since a highway network the second would join the chosen highway and Advisors interchanges) long-distance the starting to exploit route; robot it. XL.. Epstein/Artijiciul Intelligence IO0 (1998) 275-322 299 Ariadne imposes approach. computation to reference is deliberately the fundamental the random domain to navigate. Because readily be augmented to learn other descriptions, (such as a central distribution learns instances of those descriptions the same spatial descriptions of experience on both random grids and and applies those impoverished, without point or station as a consistent R), these descriptions must be very general. Ariadne such as landmarks. to learn it, and it. In other words, a richer domain would make this task easier more realistic ones. The program instances landmarks or tasks that reflect some regularity a docking could, however, One would only need to add a category of useful knowledge, a method Advisors but not invalidate If unlimited for many domains, however, are readily accessible, application or if opti- mality were essential, FORR’s approach would not be appropriate. The right reasons and FORR’s and useful knowledge systems facilitates modularity are currently as- signs in the course of a to use a host of tier-3-like heuris- work day. Human experts at this task are observed Initial tics, but we believe version are quite promising. The second system constructs results with a preliminary from its formula. Here the Advisors are fewer a three-dimensional and more complex, and weight (described important. under development. The first, for transportation model of a protein learning that tier-2 Advisors will make a substantial development. Two substantial FORR-based to pick up and deliver about 1000 loads in Section 6.7) will be particularly if failure were intolerable, time were available, resource scheduling, trucks and drivers contribution. 6.3. How a search space engenders a FORR-based system It is possible In this sense, Ariadne’s visible space provides a metaphor search is blind, just like the ablated version edge of its geography. state space search. Unintelligent the goal without using Ariadne’s been simulated with an evaluation tive problems subtleties proximity rowed from work in genetic algorithms this section exploit knowledge to reason about the traversal of a search space without complete knowl- for that sought intelligence has that measures proximity to the goal. Decep- function overlooks evaluation the “obvious” that “deceptive” was defined here as “when the term “deceptive” was bor- to address similar difficulties.) The purpose of is to suggest some additional ways to think about a search space, and to system. limited vision. function foil this approach because (Recall in the problem is not a valid indicator of progress”. In many AI artifacts, it in a FORR-based Indeed, space. about the ideas behind Ariadne The kind of knowledge available about a domain should its tier. To instantiate inspire an Advisor, and also typically determines in another domain, the reader should consider what can be learned or observed, at what cost, and how it is the cost of best exploited. The experiments rooms search. The surprise in Ariadne there is and warehouses the robot little point for distance that way anyway, and, when it does not behave that way, there is usually often behaves to begin a set of good reasons and situation- with Brooks’ it is rarely necessary in a tier-2 hill-climbing Advisor is that in non-random to plan described above took great care to monitor to reach the goal. For example, for that behavior. The reader is therefore encouraged to the goal location; 141, but to leaven recommendations like furnished environments them with learning 300 S. L. Epdt~ /Arf[ficid Intelligence 100 (1998) 275-322 based search. Save search ameliorating them. for clearly defined situations with efficient algorithms for testing to construct the Advisors a FORR-based and tier-3 Advisors system, do so gradually, remembering too it on a difficult problems. Begin with tier-l and tier-3 Advisors, and add they that tier-l the frequency with which If you decide suite of increasingly Advisors only as the need for them arises. Monitor require, Monitor comment; an Advisor problem dependent, rationale need not partition overlap and to keep each premise simple. This minimizes Advisors, which effectively give one Advisor others, a poor choice. should be broadly applicable. Although Irrelevant Advisors will not and should be quickly dropped. Take pains not to make for a particular maze; a valid tier-3 Advisors try to minimize of repetitive than the the possibility to vote more strongly the rationales behind in the domain, for good decision making for example, designed they are unnecessary should be relatively for the resources they comment. inexpensive. the reasons the ability to automate solving by a FORR-based Individual Advisors often encapsulate what people consider commonsense, such as Mr. Rogers’ “get closer” or Giant Step’s “take long steps”. The kinds of mistakes made suggest during problem system under development to the rationale behind existing ones. It is also possible new Advisors, or corrections in the discovery of tier-3 Advisors and gradually phase them into some domains decision making. Pattern-oriented, learned there [ 141. These from visual cues Advisors were based on a limited vocabulary during extensive play experience and then generalized. Extendibility of the vocabulary becomes to learn Advisors one would begin with a crucial and then and “constrained”) a vocabulary postulate Advisors in useful ways. Such a program could learn to manage corner oflice suites as a sequence of connected components. tier-3 Advisors [ 13 1 and have been shown to improve performance terminology that would combine domain, (such as “connect” for game playing have been issue here. In a path-finding that was instantiated of shapes of spatial typically terms those the principles of Ariadne or FORR For those attempting the remainder they apply to extend to other appli- of this section categorizes Ariadne’s Advisors by the kind of the state space of the maze world. Full to maneuver individual Advisors appear in the Appendix by tier; the purpose of cations, knowledge details on Ariadne’s this section the reader will note, are actually about search and not restricted search. Many of the Advisors, the maze world. is to characterize their approach to intelligent through 6.3.1. Correct conditions as good as shallow search A correct condition can generate one or more choices can be used to generate good single actions instead of searching than examine all the legal actions, an Advisor predicated on a correct for them. Rather then and condition for example, test to see if that action the goal. Most domains have some intrinsic version makes a single move that reaches that immediately wins a game. Ariadne’s Can Do, as a of Victory, such as a decision to the second example, goal, setting Victory up for the next decision. Some domains have a version of Can Do; for example, since the actual win occurs when the king is captured. Victory does not look at all possible next states to see if the robot reaches to a location vertically or horizontally the legal moves. Ariadne’s Victory, in chess Can Do is checkmate, the problem description forces a move is among adjacent from S. L. Epwin /Arrijiciul Intelligence 100 (1998) 275-322 301 interprets visibility its legal options. Similarly, Can Do interprets the goal in one of them, nor does Can Do look two moves away in the search space. Instead Victory and seeks the goal-achieving move as accessibility, among and seeks a one-from-the-goal move among exploits execute properly, Advisor a condition access that correctly that a chosen operator will the that correctly exploits in tier 1, where it offers quick and easy search assumes that is, no other agent or aspect of the environment will prevent its purported post-condition. An Advisor as good as shallow search belongs its legal options. An Advisor as good as shallow to nearby solutions. as near-accessibility, from achieving a condition adjacency a constrained subspace is a set of states 63.2. Constrained subspaces and hindrances subspace A constrained can be used domains, are primarily to each other, and very little access a bottle, and a corridor a constrained (but not exclusively) formally, access world, a chamber, less visually-oriented neighbors among states where two players shuttle No intelligent believed subspace More that a goal state lay there; instead. formally, them. An example of a constrained let S be a set of states agent would deliberately search. repetitive in a state space to curtail unnecessarily In- that afford ready to other portions of the space. In the maze In other, represent constrained subspace would be a set of states whose so that search would cycle is a set of game the same piece or two between a small set of locations. in another domain themselves, subspaces. subspace search within a constrained subspace unless it it would seek an operator that left the constrained let S, denote in a state space and lie in S. A subspace S is constrained the set of immediate neighbors of s E S, those accessible by a single operation on s. Let an exit from S be a state s E S such that S,Y - S f 0, that is, a state not all of whose if it has relatively neighbors few exits proportional subspaces have at least one exit, such as the exit from a dead-end or the access point of a chamber. Bear in mind, however, for this concept; constrained Learning that visible space in Ariadne subspaces can occur in any domain. the extent of a constrained if and only to (SI. By definition, Ariadne’s constrained has a tier-2 (search-based) Advisor called Probe that delineates what it perceives a chamber, a constrained triggered when is appropriately terminated when an exit is identified. Confinement of the territory It is also possible through search. Thus Ariadne to be subspace with at least one exit, its access point. Such search in the search space, and in Ariadne by how little it has been to recently. critique of the path for bottles. the search space. This is the essence of the learning algorithm the robot can see or how few distinct the agent senses confinement is measured locations space with a post-solution is only a useful metaphor subspace often requires to induce a confined Once a constrained subspace and its exit(s) are identified, an Advisor can exploit it by refusal a goal state. dead-end nor in the pipe, increase of locations to move through an exit into a constrained In Ariadne, to pause in a pipe not believed for example, subspace agent has no reason the goal. Unless an intelligent to enter a is the goal the pipe cannot ever provide a new option, and can only treats a pipe as a set to contain that does not contain the number of steps in any solution. This is why Ariadne locations within to be avoided. Advisors that prune search with knowledge about constrained 302 S. L.. Epstein /Arr@%d Intellipme 100 (I 998) 275-322 can appear and Pipeline keeps it from pausing stumble subspace, or heuristically subspaces dead-end, constrained subspace does, for example, searching Chamberlain bottles, respectively. in any tier. In tier 1, No Way keeps the robot from entering in a pipe. Since an agent may begin into one, Advisors to leave a constrained a in a that does not contain a goal state are also necessary. This is what Outta Here in tier 2, and what for an exit from a chamber or dead-end to knowledge about chambers and and Cork do in tier 3 by their reactions location is a hindrance, in another domain that prunes search Yet another kind of knowledge in the state. In the maze world, the presence of a barrier between a hindrance. An example of a hindrance a set of states in that the goal is not the robot and is the to move a particular playing piece to a particular place on a game board, such search may visit states with hindrances can be quite solving may not the for example, agent can address particular search from the current seeking them. Ariadne’s Detour, Roundabout, Patchwork, and respectively. Because knowledge a state space which share a property whose presence guarantees achieved the goal constitutes inability in chess. In any domain, as the king’s until quite close to the goal, but the elimination productive. Since all the hindrances be predictable into a set of subgoals problem does not know where all the barriers hindrances state, or with situation-based out hindrances Humpty Dumpty about hindrances tier 3. are examples of these approaches, is likely as they arise, either reactively, or with situation-based search for plan revision, or even by deliberately from the initial state, an intelligent agent cannot merely decompose that can arise during problem of a particular hindrance to be heuristic, Advisors that rely on them reside that are non-hindrance conditions. Ariadne, and then avoiding lie. An intelligent in tier 2 or 6.3.3. Transition regions Although many of us are accustomed to imagining state space as a vast and somewhat transition that permits movement its search space in quadrants, which suggests formless graph, it may well have a shape that suggests solutions. A transition region is a set of locations represents Fig. 12 identifies from Fig. separately, a single decision any of E = { (6,3), F in quadrant 4 in one step. Q, denotes region. from one portion of a space to another. Ariadne them. for the maze regions are shown if from in quadrant 3 the robot could move east to region i not in a transition respectively. A location from it would move the robot to another quadrant. For example, I. For clarity, in Figs. 12(a) and 12(b), the horizontal and vertical pairs of transition regions A through N among the portion of quadrant regions between is in a transition the quadrants transition region (6,5)} (6,4), The summary graph in Fig. 12(c) it quite clear, for example, in common or if a single decision could shuttle graph makes and that travel through reduce powerful planning state space. Instead. a single representative region D in the first quadrant. Because the number of nodes tool. Its construction, from N to the rest of the fourth quadrant links two transition regions the robot between if they have a location them. The summary that travel to regions B or N is of limited value, (Qd) will require movement the summary graph can substantially however, requires exhaustive search through of a transition region can expedite movement in the search space (from 64 to 17 in this case), it is a the S. L. Ep.wh /Art@cicd Intelligence 100 (I 998) 2 75-322 12 3 4 5 67 8 910 1234 5 6 7 8 910 ‘\ D K L Ql M r-4 \ E F- Q4 H G 03 Fig. 12. The horizontal (a) and vertical (b) pairs of transition regions. (c) The summary graph for the same maze the state space. Ariadne’s gates are a heuristic approximation learning through graph the relationship the overlap of its extent with Ql and Q,+. Thus one way to approach a problem of the summary as a gate in Fig. 12 would describe through in the for a maze. For example, among regions C, D, and F, as well as how to reach (5,7) (5,7) 304 XL, Epvei~~ /ArtiJicicd Intelligence 100 (I 998) 275-322 maze world would be to search for an intervening precede movement movement implemented from N; Quadro here. transition region, is a reactive version. A tier-2 planning Advisor the way Qt must for idea, one not through a sequence of quadrants would also be a reasonable 6.3.4. Other classifications of (search) space between the relationship to a set of conditions It is also possible to classify If search has not been productive, a state space according the states visited characterize goal state. state whose classification Other Side monitors has remained, is a tier-2 Ariadne Advisor whose classification moving mechanism is based on the notion of moving away from a small set of states. Advisors like these, that use knowledge derived from costly search procedures, should be triggered only when that thus far during search and the for a for example, a tier-2 Advisor called to the goal. If the robot to say, consistently it north of the goal. Wander south of the goal, Other Side may devote resources is different. the robot’s position it can be constructive in space with respect In Ariadne, to search there is some likelihood of their success, and assigned the same superficially arc primarily heuristic, to tier 2. “right” When decisions describing what states. Therefore, like FORR, more general classifications in the past can be quite useful. Advisors with such a historical perspective repeatedly draw the agent to the same non-productive approach cided typically have counterparts That discourages Advisor suggests states entirely new in the current applied in many domains. Been There discourages repeating in tier 3. Ariadne’s Been There, Done That, Cycle Breaker, and Adventure a state; Done is a tier-3 in the current context, while Adventure task. A similar, exploratory heuristic has been the same transition out of a state. Cycle Breaker to suggest alternatives that are different revisiting reasons may in a satisficing the agent has de- are for an Advisor Ariadne’s Opening supports has a counterpart Run and Leap Frog are both dependent on bases, historically useful generalizations states goal.) to take a historical perspective on a set of tasks. successful path. It in game playing. Home over the location of the the reuse of the beginning of a previously in most domains, most obviously (A base is a generalization in the space. for openings it ignores because Still other classilications actions and select extremes. Ariadne’s Giant Step and Plod, for example, advocate extreme step size, the former large steps and the latter small ones. A similar heuristic was employed in AM and Eurisko compare alternative [ 29,301. 6.3.5. The role of reactivity Reactivity is an important component in both Ariadne and FORR. Inexpensive reasoning can actually combine to solve the most difficult problems efficiently, ing and simplistic fast and wise decisions. Although enough tant component. Advisors heuristics concern alignment with the goal, Mr. Rogers and Contract goal, Corner and Crook react to what is visible. for tier 3, as are all such Advisors that favor simple environment sens- to produce a remarkable number of that these were not they did prove to be an impor- reactive in Ariadne: Goal Row and Goal Column the sensors are typically react to distance from the ablation experiment proved to Go [32]. It is also possible 6.4. Cognitive plausildity 305 There is no claim here that Ariadne is a cognitive model of a human navigator, plausible. Several of the Advisors do recognize: Plod’s [8] all of us readily curiosity, and Hurry’s anxiety. In addition, cognitive scientists [ 181 and the rationales of several reveals no tests of human search, however, rationale of naive geographic that many of its features are cognitively reasoning only model principles tentativeness, Adventure’s have found empirical evidence of the other Advisors [20]. A literature subjects on problems as difficult as these. heuristics, for Contract’s of reactivity, integration The and search as situation-based [23]. There in people behavior is increasing is the detection of all three approaches based upon evidence human problem psychologists to make decisions inconsistent, architecture rationales to accomplish The use of a learning that FORR, Ariadne’s underlying solving. architecture, In a variety of domains, report that people integrate multiple, parallel, possibly conflicting captures aspects of to game playing, strategies [ 2,6,37]. During problem solving, people describe multiple, possibly to a single goal [ 361. The brain appears to use a modular from circuit design accurately relevant integration of information architecture, [ 7,461. than a prespecified one, is supported by that people evolve superior performance. Human expertise develops only from experience to set of problem solving classes. For example, in unfamiliar is applicable at problem [ l-17,33]. rather This expertise an expert path finder and not about immediately wonder about dead-ends of domain knowledge the color of ob- for a source of fo- rely upon a foundation to provide a baseline ignorance; total the methods level of competence. Thus the architecture it is reasonable to provide to specialize it, for example, by learning it with general do- feature evidence repeated a related territory will structions. Experts cus and direction need not begin with main knowledge and instances. represent inferences form Ariadne’s facilitators representations its cognitive map, of the visual world representations that are based upon, but more abstract and general and obstructors its world. These features are consistent with the ways humans to make use constructed representations [44]. These [45]. They are integrated with many other kinds of information the human user does not require multiple representations geographers use of levels that people gauge distance There are two ways and its tolerance tell us, is much like the naive geography (number of turns) that way [ 381. to view Ariadne’s for degree of problem difficulty distort visual perception task as resource-limited. for inconsistent systematically to be complete or consistent. Ariadne’s and even incorrect its representation of space. People about space, than, perception recall that reliance upon information, to facilitate to form a model that people rely on [ 81. Even the is supported by results If CPU time is a then the agent that makes the fewest passes in Fig. 3 is best. If travelin g time or fuel is a scarce resource, scarce resource, structure that constructs synergy a variety of routes on a campus was also explained by distance, [ 181. turns through FORR’s decision then the agent the full FORR agent achieves a lack. The behavior of human subjects asked to traverse time, and number of the shortest paths is best. On both metrics that the ablated versions (here, problem level) 306 S.L. Epstein/Artijicid ltztelli~ence 100 (1998) 275-322 6.5. Situation-based behavior as a compromise on search to forget philosophy The initial tier 2, it is easy that. Tier 2 Advisors amount of computing in two ways. First, FORR only allocates each Advisor, impulse behind reactive programming was to avoid search, to make instead by search) and local in space (restricted tier the reactive Advisors of tier 1 and the search in any too long to to intended fragments, but identified from decisions that were local in time (unconfirmed to current perception). When one augments 3 with minimization tier, a limited construct will not arise. Second, address their particular subgoals. These routines generate and test solution the proposed partial solutions by the trigger while preserving a combinatoric search effective. saves the tier-2 Advisor in the Appendix, Roundabout’s is quite deep, but it is also severely curtailed by knowledge; tier-2 Advisors have hand-coded resources. This constraint explosion. For example, are highly constrained, that take routines that is why it is are kept within time. Solution the situation as detailed to address fragments and then applied a procedure that proposed a kind of solution deemed appropriate in Section 2, tier-2 Advisors are intended decision making tailored a particular category of situation to simulate behavior observed [23]. The decision makers the (in Ariadne, type (in Ariadne, the to that situation category. reasoning are indexed and relevant cases are retrieved, and an attempt is made behavior In CBR, experiences is not case-based to that situation such situation-based in common. the current problem [24]. Although of the current state that could have been used behavior does not retrieve specific solutions to to generate solution situation-based solution generation, but CBR does it by the knowledge fragments. Situation-based it by searching inherent they study that CBR has are very that the human experts (This is not a claim that it is less likely to be used when resources behavior does emphasize their problem solving as reminding. studying although time-limited interpretation, As discussed by psychologists described how they first recognized trigger), search) In FORR’s (CBR), they have much stored. Later, one or more potentially to modify their solutions behavior as an index be modified, only procedures behavior from old solutions, while situation-based in its procedures. Klein and Calderwood do not perceive no parallel limited.) is triggered by an abstraction for CBR. situation-based and CBR both constrain in people, only to solve intended Situation-based intended behavior is not the same as planning is a set of ac- to reach a specific goal [43]. The Advisors of tier 2 are not planners recom- (one to even if they do not eventually as many as eight L-shaped paths second step) before it chooses one either. A plan they actually their behavior, execute it. For example, Wander can investigate step in each direction plus a possible tions because mend longest execute. Rather than planners, seize control of a FORR-based that time elapses, a sequence of actions whose execution like Pengi maker, much in its world; it is not held to an explicit decision standard steps”. situation-based Advisors program’s [ 11. The principal difference resources the situation-based Advisor either returns control it requires. Tier 3 constitutes are procedures that reactively for a fixed period of time. When to tier 3 or returns a reactive decision is living in 1000 decision is that Pengi’s problem like “solved XL. Ep.win/Artificiol lntellipnce 100 (1998) 275-322 307 Situation-based behavior is a resource-grabbing, fragment, not a production duce a solution of a tier-2 Advisor could be the condition of a production tor’s response the action part. A macro-operator successful procedure, whereas a situation-based Advisor to a situation. over several kinds of behavior appropriate is a generalization is too complex (particularly since rule or a macro-operator. Although intended heuristic digression to pro- the trigger rule, the comment genera- it can be overridden by tier 1) to be in a the variables entailed is a procedural generalization across situation-based Finally, and reactivity resentation of the moves have been the aligned lies between this domain, the representation component in a reactive learner. behavior sheds some light on the ongoing debate about rep- includes “the last 30% [22]. Ariadne’s conceptual knowledge in the maze” and “a wall that, at least in is an essential in no more than 5% of the locations robot and the goal”. This paper demonstrates of conceptual as a context knowledge 6.6. The interaction among reactivity, heuristics, and search among is a reactive the Reactive+Heuristic Ablation shows the interdependence reactivity, heuristics, and situation-based determined tier 2, FORR intervening wall; it because of an system augmented it; it needs maneuvers because most of their to the goal but cannot reach circumnavigation trigger significantly more often with ReactivetSearch repertoire of behaviors. The situation-based Advisors by learned useful agent, however, simply are not in regions where Giant Step cannot ex- like Wander’s L-shaped path to get out. It also regularly it needs to get closer. The situation-based Advisors behaviors. Without knowledge. The results with good enough. This agent regularly gets stuck tricate gets close Roundabout’s of tier 2 make a clear contribution when combined with tier 1 as Reactive-tSearch, but they have a limited sufficient on their own. They than with Ariadne, something tively, a device example, Wander wall”. Subgoals programmer. There inherent the situation-based Advisors and Goal Column move the robot where Roundabout based Advisors of tier 2 set up the heuristic puts the robot where many comments. Another useful knowledge, sets up in tier 2 so that they can trigger. For example, Goal Row can trigger. In turn, the situation- in tier 3. For example, Wander to make newly constructive the robot experiences more often with ReactivefSearch. subgoals. The subgoal tries to “get out of here”, and Roundabout are detected by the program, but their nature lack of recent progress, Tier 2 is, effec- for the by the is the opposite of the trigger, tries to “get around is predetermined the tiers. Tier 1 offers task. Tier 3 tries to avoid search and effectively tier-3 Advisors are more likely side-effect of the search is a complex in any problem-solving in tier 2 is the acquisition in every tier can benefit. from which Advisors the commonsense triggers measure relationship to execute important reasoners are in- among of 6.7. The role qf learning in pragmatic navigation As limited the data indicate, learning is essential resources. The ablated No-Learn agent to an agent failed facing hard problems with to solve many of the hardest 308 S. Id. Epwirr /A@&1 Intelligence 100 (1998) 275-322 the full version could perform quite well after only 20 learning problems, while Even on level 6, No-Learn was slower to solve problems it to meander more about the nraze. of knowledge forced than Ariadne because trials. its lack as Table 1 indicates, One hallmark of Ariadne’s pragmatic navigation, is the variety the flexible use is applied by seven different the experience of for the same useful knowledge. Corridors demonstrate of procedures of knowledge; the same data Advisors. Barriers demonstrate four different Advisors provides is there apparently in a single representation the flexible acquisition of knowledge; fodder for learning any danger of learning them. so much useful knowledge static map of the maze would have been a more efficient that a Nor than detailed, in 20 x 20 random Ariadne’s mazes, there were on average only 32.5 barriers, 80.5 bases, 1.6 bottles, 3.9 chambers, 49.3 corridors, and 18.0 gates. As a graph, however, such a maze would have 280 nodes and approximately features. After 20 level-8 problems learned heuristic representation for example, 1485 edges. Perhaps the most interesting Even among randomly-generated goal behind wall-like compares of triggers, program’s overall performance to have the program that FORR uses to learn weights was formulated to a model of expertise learner. Although initial paths would autonomy. challenge is that of balancing the Advisors’ comments. mazes there are different kinds. Some may conceal the structures, others place it at the end of a tortuous path. When one in Table 3 the number of decisions used to solve a problem with the number it is clear that most of Ariadne’s decisions were made in tier 3, and that the is could still be improved. The key to this, we believe, for a domain to apply to the voting learn to value tier-3 Advisors appropriately. AWL is an algorithm in tier 3 [ 111. AWL, however, (game playing) where the learner necessarily has access is an autonomous that weights trained on the problem generator’s improve performance, we are currently adapting AWL to retain Ariadne’s (its opposition). As described here, Ariadne testing indicated 6.8. The role of plunnir~g in pragmatic navigation level-10 distinct) effective in Ariadne tool. Although is a surprisingly they are inexpensive The simple planning are naive, themselves sors (Home Run, Leapfrog, and Patchwork) per problem during computation (not necessarily of 77.0 available bases. Because bases are strengthened with each re-identification, three different Advisors use them, over time Ariadne develops habitual way people do. It is not uncommon to see Ariadne This also means that, unless some fortuitous choice occurs, develop shortcuts on its own; those would have to be taught. the plans to generate. The three plan-related Advi- together average only 0.15 17 seconds of 14.58 from an average and routes, just the stage, it in its entirety. to is unlikely testing. On average Ariadne constructs plans for a level- 10 problem during testing, or even late in the learning plan fairly early and execute the program a reasonable formulate testing, during Because a plan in Ariadne to the goal, a plan instead, a reaction the knowledge is not reactive is a sequence of positions location is, to the fact that it currently has no applicable plan and that it has for from the robot’s current sense 1391. Ariadne’s planning to construct one. The preference from which to attempt in Schoppers’ (the bases) S.L. Epstein/Artificiul Intellipnce 100 (1998) 275-322 309 (their orthogonal moves) and to be readily patched by a circumnavigation (bases) instead of considering from strategic to the approach locations in [40]. Ariadne’s plans are deliberately planning similar legal travel routine. Using bases to plan also makes the overly optimistic assumptions travel experience routes are correct and that the robot has had adequate maze. This is generally more efficient is why not too much time is invested to replan. in plan construction that Ariadne’s the throughout it or correction; all possible choices is to support structured The component ripe for development in Ariadne barriers, gates, or corridors to facilitate efficiently with these useful knowledge long as it was appropriately wary of the heuristic nature of the information. into the corner offices of the office mazes. An Advisor items could make a significant in plan construction, or links chambers and bottles is planning. No Advisor considers together that reasoned as contribution, travel 6.9. The real world Although Ariadne would require adaptation before it could direct a real-world territory is distant navigation dangerous (deep sea terrain), that roboticists would do well embodies principles (a planet), correct and complete maps may be unavailable pragmatic When warehouse), becomes quite practical. The nature of the maze already permits sections along their edges, as in Fig. 1. territory, since the mazes may have unreachable For the real world, the fineness of the grid would also have to be determined. Given the heuristic nature of useful knowledge and the inaccuracies of measuring devices and robotic controls, however, one really does not want a very fine grid: approximations in pragmatic navigation and a feature orientation irregularly-shaped are tolerable and tolerated. or dynamic robot, to consider. (a issue The primary to allow a margin of error. Continuous, in adaptation would be the sensors. Their inaccuracies would rather than discrete, sensing require modifications the robot to sense and travel in only four orthogonal would be more costly. Permitting than the equipment on most robots, but not far from human directions was more limiting [ 191. The equidistant placement of the four sensors was judged appropriate proclivities for a task where forward motion was often no more productive than lateral or backward movement. A more generous number of sensors and directions should do better, although it could revision of some of the algorithms. Finally, it may drift or attempt avoidance maneuvers. Standard AI search methods have no provision for this. No real difficulty if G were to move, say, one unit in a randomly chosen direction nice fit with the opportunistic after each two of Ariadne’s moves. The instability of G should be a the goal is not completely and would necessitate increased computation real-world problems is foreseen stationary; in many planner. require 7. Related work This work has some clear counterparts with Korf’s analysis of heuristic the tile puzzles [25]. His minimin search “strategy of least commitment” search in is shared by 310 XL.. Epsrein/Artijiciul Intelligence 100 (1998) 275-322 lengthens the Euclidean it of necessity loop prevention the number of steps distance heuristic much in the tile puzzles, and with the same disappointing too, there may be better ways to reach the goal. Korf’s permission in a solution. For Ariadne’s Plod, but if the correct, unobstructed move is from ( I, 1) to ( 1, IO), plodding will take example, 9 moves to get there, although an immediate move to ( 1, 10) would be legal. Ariadne’s the way Korf tried heuristic Mr. Rogers uses results. Once you get node ordering close in this domain, to backtrack with Roundabout. Ariadne has no foolproof and Done That discourage Korf indicates were limited only by how far the robot could see ahead, the Reactive+Heuristic should solve few problems, expected performance its general maze-traveling level of difficulty uncertainty solver. in loop prevention, but Been There, Cycle Breaker, than tree) search space, If the search horizon agent it sees only one step ahead. That agent’s better than to its useful knowledge about a particular maze and the there remains some the ability of the problem loops. In Ariadne’s graph locally optimal in Ariadne’s problems, but with the tile puzzles to some of the decision making about how the level of difficulty that one cannot expect in the tier-3 Advisors. (rather solutions. impacts upon is attributable It is possible is analogous knowledge to dictate since different to the definition of recent machine learnin g, it is important to note that the program’s I, would be a single problem. Although Ariadne’s maze problems may be reminiscent learning task and fun- [ 3 1,421. Such programs seek convergence In contrast, Ariadne has no mechanism satisticing paths for a set of problems, applying knowledge attempt instead of from one problem-solving work in reinforcement damental approach are significantly in Sec- to an optimal path through repeated solution of what, according tion that would guarantee optimality, and will quickly settle upon the same route in most cases. Ariadne constructs from one to another attempt problem at the same problem. The complexity of a maze problem learners is a function of both goal strength and the number of state-action pairs (the number of from them). The complexity of a reachable locations and directed one-step movements problem of too. Ariadne the size of the maze and the strength of the goal. Memory use is different, learns abstractions, while for the value of each one-step move attempted for Ariadne, on the other hand, is the number of turns required, for the reinforcement the reinforcement from each state. refine estimates to the others, independent learners learned recently A second, learning in a set of abstraction suggested for the grid world that operated and abstracted solution paths adne’s warehouses, which present few dead-ends, narrow-necked effective barriers between easy. approach was a case-based planning method spaces, and stored both detailed [ 31. These mazes were somewhat simpler versions of Ari- chambers or bottles, or large regions. Once again, Ariadne would find them relatively One way to characterize an approach to learning navigation for a robot is by the degree it is engineered, in this area ranges that is, to which from the the robot’s abilities are preprogrammed. to the thoroughly in [34] [5]. Although Ariadne’s rum approach tabula initial knowledge it is certainly engineered, conflicts among knowledge-based in tier 3, that is, no tier-3 Advisor is deliberately about what to learn and how to are left to principles to outweigh any formulated to which Work prescribed learn the voting other. S.L. E~~stei~l/A~t~~cicll Intelligence 100 (I 998) 275-322 311 (particularly an approach to learning navigation into general cognition A second way to characterize them, and a survey map of global limited short-term memory, neither of which that integrates path finding landmarks, to human sensory orientation and its position) for a robot is by the it reflects what is known about human perception and behavior. PLAN [5]. PLAN a route map of place sequences information. All three maps system has strong to see more than 180” for is necessary that such maps are therefore based on In contrast, (e.g., a gate) (e.g.. a base). Although developed its gateways are to Ariadne’s bottles and chambers. PLAN from regional maps. PLAN’s to be in maps whose scale and level of difficulty are far traverses with ease, and the authors offer no statistics on either its degree to which is a connectionist model learns a topological map of experienced and directions between are based on a visual scene rather than an aerial view. The resultant biases about intelligent experience, but the experience Ariadne and reasoning independently, similar makes its global overview explicit, and plans hierarchically expertise, however, appears those Ariadne below efficiency or its effectiveness. about a sequence some of PLAN’s features are quite similar they record is pure1 y visual or rote experience. to Ariadne’s gates, its regions similar robot navigation. The authors claim that represents both momentary retains knowledge of experiences in its refusal to Ariadne’s: experience is by for a robot an approach representations to characterize and quadrants) and connectors a rigorous approach that have boundaries to learning navigation [ 2 11. He too envisions of its world. Hayes has constructed to reasoning of space into pieces (like Ariadne’s (like Ariadne’s access points, and gates). There is, as yet, no implemented version, however. that are sensorily distinctive. Ariadne’s such A third way its representation about space bottles, chambers, bottlenecks, [ 271 and Qualnav TOUR landmarks. As a grid world, as originally theories about what might substitute, Ariadne be useful from completed problems either, whereas TOUR and PLAN both keep a topological network of routes these systems, Ariadne’s between places and it plans naively global overview graph opportunistically. on Ariadne’s gates and bases, the reactive approach described here is preferred, for both its computational [ 281 have landmarks postulated by Korf, did not provide in the useful knowledge it would be simple enough in travel. Ariadne does not store routes or route fragments it learns; to learn a route-fragment efficiency and its limited storage. has gates and bases predicated is implicit Although that can be manipulated to find a path. Unlike upon SSH (Spatial Semantic Hierarchy) for real-world spatial knowledge topological level as learned places and chambers). Kuipers envisions path-finding robots is a four-level ontological features [ 26 I. Ariadne’s (e.g., gates and bases) this level as a representation hierarchy lie primarily and regions to provide at SSH’s (e.g., bottles and for exploration to his work. is thus complementary There has been some work on learning heuristics problems. Ariadne transformations affords access only risko sought an approach tractive path of available this approach does best when content of the concepts, Eurisko addressed. Prieditis’ Absolver as it did of existing heuristics to interesting transformations. Since the representation linked concepts for problem solving. Lenat’s Eu- to find new, useful ones [ 301. Such to others by some at- is a syntactic process, the semantic [29] but less so in some of the domains that could in some way reflects transformation II sought an abstraction of a problem in AM 312 XL. Epreh /Artijicid Intelligence 100 (1998) 275-322 representation. The heuristics transformations [35]. Absolver then used as an admissible the number of rooms a robot visited the problem description a good problem heuristic in a variety of grammatical be sped up and tails from also required to prune search, but to be used one at a time. One of the problems Absolver dressed was a “Rooms World” problem mized is analogous Chamberlain prespecihed, solving. Although classes, ence. II removed de- that intended II ad- that mini- task. This relies on are for the most part they rely is gathered during problem and problem from experi- to entering and Outta Here. In contrast, Ariadne’s heuristics although a heuristic in the course of a box moving for which Ariadne to a variety of problems induced chambers only when necessary, it also makes them vulnerable to incorrect generalizations it learned were them applicable the information it computed this makes for which on which Finally, although optimum paths in space have a variety of well-documented algo- the IZ points [ 411, they eventually face the 0( n2) complexity that cost. The 24 Advisors search there would be 0(n2) that comes from the underlying of experience people formulate in the space. If Ariadne were to construct rithms a map based on graph among the its heuristic knowledge, to kind of spatial descriptions in tiers 1 and 3 must react to a finite set of at most subvert 2r1- 1 legal moves and the available useful knowledge u within some fixed time t. The in tier 2 spend at most some finite time t’ in search. For a problem with a 8 Advisors - 1)~ + 8t’) time that Ariadne spends at most 1(24(2n solution step limit 1, this means in search of a solution. The key is to control U, that is, to keep the cost of referenc- ing useful knowledge sublinear. The result is occasionally optimal, and usually quite good, performance. too. Instead, Ariadne employs in a satisficing approach and obstructors) (facilitators 8. Conclusion tailored reasoning, navigation for a particular such as “closer Human navigators is an implemented is better” or “when into a search mode react quickly and correctly is between me and the goal, so I have to get around pragmatic as if it were learning to clearly productive or unhelpful oppor- tunities, such as “there’s the goal” or “not that dead-end again”. They entertain a variety far away, take a long, straight step”. of heuristics, situation, They also digress for example, it”. “this obstacle that epitomizes Ariadne of naive geographic or town. Ariadne knowledge the same than more difficult problems. Compared problems quickly, as measured and path length. It also performs efficiently, as measured by number of distinct visited and the percentage of repeated it applies previous experience in a variety of more realistic world models. Ariadne does some primitive planning, controls this kind the way around a new campus and uses that in in fewer steps and with fewer resources it solves these time, number of decisions, locations learns, so that to hitherto unseen problems, both in random mazes and and learns a variety of features about a new environment, to solve multiple It solves travel the easier ones the allocation of search resources in elapsed problem-solving solves multiple problems in a path. And Ariadne to acquire knowledge. search techniques, there. Ariadne to traditional locations territory. system tasks S.L. El,steirl/Art~~cicll Intelligence 100 (I 99U) 275-322 313 Pragmatic navigation as detailed here was shaped by an underlying architecture, features. a long-range learning methods FORR, a robust, adaptive detailed map, pragmatic correct, possibly useful has one or more of executing a journey representation, rather than mandate, path selection. Despite and decision making, tasks, Ariadne Ariadne’s plan, pragmatic to outperform at a time. empirical integration of reactivity, heuristics, navigation represents and search. Instead of a Instead of a uniform to acquire learning territory-specific of probably the world as a collection technique, each feature instances of it. Instead part of selects one reasonable navigation pragmatic navigation opportunistically Instead of planning with heavy search costs in a fine-grained constructs naive plans that suggest, the heuristic nature of Ariadne’s knowledge few learning that, after relatively results demonstrate is able to solve novel, difficult problems quickly. clear ability and time-limited ablated versions of itself demonstrates subset of FORR’s reasoning methods suffices in this domain. Reactivity, heuristics, ing, planning, to good performance, advocates pragmatic navigation proach robust, and applicable that no learn- are each essential them all in an appropriate manner. This paper ap- cognitively plausible, developmental is viable, efficient, to path finding. Ariadne demonstrates to a broad range of environments. search based on situation that such an approach and FORR connects as a satislicing, recognition Acknowledgments David Sullivan’s preliminary work on Tonka convinced me that a FORR-based version far more primitive, version of Ariadne was insight and many furnished then, Barry Schiffman provided much as well as the generators that represent for mazes in [ 121. Since of Ariadne could be a success. An earlier, reported upon constructive rooms, offices, and warehouses. Thanks compare Ariadne with best-first Hayes, Ben Kuipers, and Barbara Tversky suggestions, search, and too to Mike Pazzani to to Jack Gelfand, Alice Greenwood, Pat for the suggestion for their shared wisdom. Appendix A. Ariadne’s Advisors by tier A.I. Tier I Advisors appear Victory has absolute in the order in which they are consulted. authority; if the goal is reachable by a legal move, Victory makes it. Curz Do forces a move adjacent On the next decision cycle, Victory will turn toward the goal and reach it. to a location vertically or horizontally to the goal. No Way vetoes unnecessary moves to see if it resides that the extent move (Recall so, No Way eliminates the robot is already in the extent of a dead-end into a dead-end. This Advisor checks each legal the goal. If by any other Advisor, unless so this is a conservative that could not contain approach.) is a bounding the move from further consideration rectangle, in the dead-end and therefore needs to get out. 314 S. L. Epstein/Artj&id Intelli~mce 100 (1998) 275-322 1 2 3 4 5 6 7 8 9 1011121314 Kev obstruction q dead-end - path R robot G goal Fig. A. I. Roundabout circumnavigates an obstruction. Pipeline vetoes a move opposite exit is in view, unless the goal. For example, to ( 14, IO) and ( 14, 1 I); not pause within it. to any location within a pipe the is aligned with in Fig. 6 (see p. 285) from ( 14,9) Pipeline would veto moves that pipe, if traveling east efficiently, one would go through the robot is in the pipe or the location if the location beyond A.2. Tier 2 faced is an between attempts algorithm; the robot the robot the situation that attempts to go around the obstruction is in the same to circumnavigate obstruction. When it and the goal. When, in the order in which they are consulted. It triggers when intervening is a tier-2 Advisor the goal. there is not a traditional wall-following the goal) and a secondary direction an obstruction between row or column is so aligned, for took the robot around in Fig. A.], Roundabout horizontal obstruction before stopping at (2, 10) where the goal was in a primary to the primary and In Fig. A. 1, the primary direction Advisors are discussed Roundabout the robot and as the goal but Roundabout example, Ariadne the intervening sight. Roundabout direction (toward not toward a barrier between was north and the secondary direction was east. Avoiding known dead-ends, ((4,9) (5,8)) direction when possible, otherwise goal is in view or until backup directions) would exceed the original alignment search fails to produce a solution first in the secondary direction, to circumnavigate attempt it succeeds, Roundabout until search may fail to circumnavigate than it had been when it started but without actually bringing Roundabout’s such as in Fig. A. I, search repeatedly moves toward the goal, in the primary in the secondary direction or their opposites until the and secondary (permitted if this the algorithm will shift the robot laterally, and large shifts Its to the goal the goal into sight. All of learning. in the opposite of the secondary direction, it iterates with increasingly starch paths, successful or not, serve as input for barrier from there. Although is time-limited the robot and the goal). like any tier-2 Advisor. perhaps getting closer coordinates. While to the primary the obstruction, time permits, and heuristic it establishes (orthogonal fragment, opposite (5,9) then S. L. Epstei~~ /Art(ficid Intelligence 100 (I 998) 275-322 315 Kev q obstruction - - plan patch R robot G goal Fig. A2 Patchwork repairs a plan small (and may that attempts triggers when cover a relatively Outta Here is a tier-2 Advisor learn a chamber as a side-effect) is in a chamber, Outta Here scans If Outta Here were to trigger in Fig. 7(a) the goal. Outta Here locations recent the robot to be in a dead-end or chamber not containing to leave a chamber or dead-end the task is well underway if it does and either fraction of the total area of the the is in a dead-end, Outta Here marches out with a sequence of steps the way the its exit. If the robot it before routine does the access point (see p. 287) when the robot the path ((4, 12) (4, 13) (10, 13)). Outta Here is not it not contain the robot’s maze or Ariadne believes goal. If the robot that lead through chamber-learning returns a sequence of up to three steps that move the robot out through of the chamber. was at (2, 12), it could generate guaranteed has already visited during is a tier-2 Advisor to leave that space. Its trigger is the learning condition described Probe attempts Probe scans for the current chamber and then orthogonally during upon its recent experience, Probe actually generated a path out and thereby chamber shown. Probe and then for chambers. the view, that is, it tries to move the robot to an access point leave the chamber. Chambers are learned in Fig. 7(a), depending these scans as a side-effect of this search. From (4,12) to find an access point, however, and may return the current that determines the extent of the robot’s confinement the robot to a location to learn a chamber. is not guaranteed learned the to improve task. triggered that attempts is a tier-2 Advisor routine as Roundabout, that naively disregards to repair a current plan with the same if the plan fails because of an obstruction. Patchwork circumnavigation Fig. A.2 shows a typical Patchwork plan (5, 10). When Patchwork the fragment ((4,1 (7, 10). Patchwork number solutions when simpler approaches would suffice. When Patchwork succeeds one step in a plan and the result contains Patchwork need no correction, one circumnavigation I) (4, 12) (5,12) to move from (4, IO) to triggers when there are valid current plans and 30% of the expected overly elaborate in repairing this task, at least one location new during that is, it executes all the initial steps of the plan that sequence as a patch, and the next set of steps has been exhausted. This prevents premature, the indicated plan by inserting in Fig. A.2, it repaired the obstruction that fragment, of decisions (7,13)) forces (6,13) (5,13) at 316 S. L. El~steirz/Art~~icirrl Intelligence 100 (1998) 275-322 1 2 3 4 5 6 7 8 9 1011121314 Kev obstruction - path R robot G goal Fig. A.3. Other Side shifts the robot’s orientation to the goal. that need no correction. the goal. In this way Patchwork moved the robot in Fig. A.2 directly to state. Other Side the other side of the goal is within the task the robot has been on one side cycles, Other Side is costly, so its trigger describes is not the result of a just-executed that attempts Other Side is a tier-2 Advisor to the other. In decision location throughout location before, and but also fairly desperate goal appropriate are no current plans, current this right, above, or below) of the goal. Other Side the other side (right, an Other Side path appears The robot began running north-south at ( 11,9) and had always been directional the goal had been at (4,2) to ( I 1,2) contribution. if the goal comes attempt fragment, it is late, then attempts triggers when the edges of the maze, the robot has been to shift the robot from one side of the an there the in (left, to of the goal. An example of task on this maze. learning the irregular barrier to penetrate the robot was its If, for example, (12,2) still a constructive the robot triggered, to move instead, into view, as it did from (12,2). instead, Other Side’s path would have gone from left, below, or above, respectively) in Fig. A.3, from an early to the right of the goal. Other Side will abandon then halted at ( I I, 1) to the left of the goal to reach the goal. When Other Side finally the task at (9, 12) and kept trying Super Qua&w is a tier-2 Advisor that attempts to change the robot’s quadrant. It in its current quadrant scans It and the goal’s quadrant) for some time. Super-Quadro the extent of a gate that would change the task is well underway and the robot has been triggers when (or its current quadrant to find a move into tries to find a sequence of orthogonal the goal quadrant preferably taken from an early learning through (2,4), Super Quadro generated into the goal’s quadrant, but cannot right solution.) Super Quadro has no heuristics that would require even further search), so its solution always be constructive. if it has not been there recently. From task where Ariadne had not yet found the path (( 5,6) is different, steps to a location whose quadrant (5,5) in Fig. A.4, its way to the goal This takes the robot the eventually (since like this one, may not for preferring one gate to another lead it to the goal. (Ariadne the robot’s quadrant. fragments, (5,ll)). found S. L. Epstein /Art@ciul Intelligence 100 (1998) 275-322 317 1 2 3 4 5 6 7 6 9 1011 121314 Kev q obstruction El gate - path R robot G goal Fig. A.4. Super Quadro responds to its trigger. 1 2 3 4 5 6 7 6 9 1011 121314 Kev obstruction - Wander’s path Fig. A.S. A situation that triggered Wander. Wander is a tier-2 Advisor that attempts new location, one as far from the robot’s current only when location was not just as more bases are identified; the result of a solution the robot’s behavior is judged constrained location as possible. Wander to find an L-shaped path that leads it to a triggers and the current less likely fragment. Wandering becomes and repetitive, its trigger is therefore stochastic, with probability I- [bases/ 0. I /unobstructed maze locations/ Wander orthogonal directions. As a side-effect, Wander L. On the second with does it move solution to recommend tests up to eight L-shaped paths that are pairs of longest possible steps in two learns dead-ends on the first leg of the the robot that would align this task, nor it and the goal. Wander often produces several one. It prefers farthest from the one in the direction of the goal that ends in a location leg of the L, it does not pass a location fragments but, like any tier-2 Advisor, the robot has already visited toward a barrier between it can only recommend that location during the goal unless 318 XL. Epsteitr/Arti&ial htelli~ence 100 (1998) 275-322 1234 567 8910 Kev obstruction - - path barrier R robot Fig. A .6. Part of a Humpty Dumpty path and the barrier learned from it. location, has been rarely visited, and has not been visited recently. When and made little progress. is the least reliable of the tier-2 in Fig. A.5, the robot had begun at (8,2) ( 14,5)). Wander triggered robot’s current Wander Now Wander Advisors, but it often makes a dramatic Humpty Dumpty is a tier-2 Advisor forced the path (( 10,5) available through in its vicinity. The fragment Humpty Dumpty might be avoided by judicious move selection. of the locations the current barriers immediate in the data sors. A Humpty Dumpty path fragment Fig. A.6 area along contiguous tier 2, but obstructions. it collects and serves as important impact when one is sorely needed. that goes in search of barriers, hindrances that It triggers if the robot has been to most legal moves and there are few known the algorithm to several other Advi- in It has the least goal-directed is one that traverses the barrier extracted it appear returns input from A.3. Tier 3 Advisors in tier 3 are consulted simultaneously. Their order of presentation here is only to facilitate discussion. to align that align Goal Row and Goal Column attempt favors moves Row favors moves that do so horizontally. Giant Step and Plod advocate steps, respectively. Late in a task, Hurry proportionately the five longest steps. When Step encourages single unit moves, more strongly the goal and when the robot is close to G. the robot with the goal. Goal Column the robot vertically with the goal only if it is not already; Goal large and small those moves with to a small area, Giant strengths. Plod advocates for those toward strength) the five longest moves with proportionate (with a higher comment the robot has recently been confined encourages Mr. Rogers and Contract address the shortest distances supports, with strengths proportional that is, those that produce advocates large steps when robot is close to it. Closeness possible distance. Both Advisors to the goal, while Contract uses proximity steps draw the robot closet- to the goal. is measured the robot the Euclidean to the goal. Mr. Rogers to their result, moves into the goal’s neighborhood, distance is far from relative to the goal. Contract, on the other hand, the goal, and small steps when the to the maze’s diagonal, its maximum address proximity, but Mr. Rogers seeks to be close step size, whether or not those to determine S. L. Epsteirl /Artijiciul lntelli~ence 100 (1998) 275-322 319 (a visited returning strength) representation, lower comment locations. With careful the current this task. Been There discourages task, with more discouragement Been There, Done That, Cycle Breaker, and Adventure all address prior behavior to a location already visited for more can respond for example, would there. in the same direction one did before from a previously the robot repeatedly visits the same few spots in its most recent that, while in contrast, encourages during during frequently as if they were reactive. One way to make Been There reactive, be to note locations Done That discourages moving visited location. When moves, Cycle Breaker attempts not necessarily innovation. Adverlture task, with greater strengths C/zunzber/uin discourages for those in the direction of the goal. a move into the extent of a chamber to intervene by suggesting in the current context. Adventure, task in an array and “sense” thus far unvisited during recommends moves already visited such Advisors in the current new, is novel an alternative to locations this if the goal might be there. If the robot encourages moves is less powerful than Probe because such a move the goal is not, Chamberlain exit). Chamberlain and encourages chamber where support a subsequent search and can only recommend for bottles. Any the bottle. When bottle whose extent the neck of a bottle or into the bottle the goal. When location the robot is outside the robot is inside indicates a single move. Cork is the tier-3 version of Chamberlain from which a bottle’s neck can be seen provides an exit from into a the goal, and encourages moves into that it can contain the bottle, Chamberlain that it cannot contain discourages moves itself whose extent the bottle, Cork reverses indicates this advice. if the goal is not there, in a is already (to it does not to its access point Detour avoids moving goal. Jf a barrier already toward barriers intervenes, Detour encourages moves to avoid it. that lie directly between a location and the to known gates into the goal’s quadrant, moves Quadra is a simpler version of Super Quadro. It encourages, with decreasing strengths, into the extent of a known gate into moves into the goal’s quadrant, moves the extent of a known gate into another quadrant. to known gates into another quadrant, and moves Home Run and Leap Frog both use bases. Home Run encourages moves to bases and, to locations next to bases. Leap Frog advocates a move to with lesser strengths, moves the legal location closest to the end of any current plan. Corner advocates a move the pipe is in the direction of the goal, less so when it is opposite goal. Crook advocates a move to the near exit of a corridor dead-end, again more strongly when the corridor when the far exit of the corridor to the far exit of a pipe that turns, more strongly when the direction of the that is neither straight nor a is in the direction of the goal, less so the direction of the goal. is opposite Opening encourages the reuse of previously such moves may seem odd if the goal is in a different cable. Although heuristic works well if the old path was successful because area that offered good access to other parts of the maze. successful path beginnings when appli- the to an it began by moving location, A.4. Parameters for Advisor applicabilit!~ One common component of the triggers for tier-2 Advisors relative to available resources. Each task in a FORR-based is a measure of lateness if system can be terminated 320 XL. Epstein /Art@ckd Intelligence 100 (1998) 275-322 limit upon the decision Depending in the partial solution, far exhausted the average described here: the trigger, is met or the computation time allocated is exhausted. lateness may be measured as the number of actual moves thus to in moves), or the ratio of actual moves limit or computation to a problem time the percentage of the decision (whether or not it resulted task length. The following parameters were applied during the experiments l Giant Step: The most recent 30% of the history is confined to no more than 10% of the maze area. l Humpty Dumpty: The most recent 30% of the history includes at least 80% of the there are fewer than 3 known walls in the robot’s vicinity, and 50% legal actions, of the decision limit has been exhausted. l Opening: The first 15% of the moves constitute l Other Side: 60% of the decision l Outta Here: The most recent 30% of the history limit has been exhausted. the opening. is confined to no more than 10% of the maze area. l Wander: The most recent 30% of the history is confined to no more than 10% of the maze area. References 1 I 1 P.E. Agre. D. Chapman, What are plans for?, Robotics and Autonomous Systems 6 (1990) [ 2 1 G. Biswas, S. Goldman. D. Fisher, B. Bhuva. G. Glewwe, Assessing design activity 17-34. in complex CMOS (Eds.), Cognitively Diagnostic Assessment, circuit design. Lawrence Erlbaum, Hillsdale, NJ, 199.5. 13 ) L.K. Branting. D.W. Aha, Stratified case-based in: P. Nichols, S. Chipman, R. Brennan in: Proceedings Morgan Kaufmann, San Mateo. CA. 1995. pp. 384-390. 14th International Joint Conference on Artificial reasoning: Reusing hierarchical problem solving episodes, (IJCAI-95). Montreal, Que., Intelligence (41 R.A. Brooks, ( 5 I E. Chown. S. Kaplan, D. Kortenkamp, Prototypes, Location, and Associative Networks representation. Artificial Intelligence 47 (1991) Intelligence without 139-160. (PLAN): towards a unified theory of cognitive mapping, Cognitive Science 19 ( 1995) 1-51. 16 I K. Crowley, R.S. Sieglrr. Flexible strategy use in young children’s tic-tat-toe, Cognitive Science 17 (4) (1993) SRI-S6l. 171 E. DeYoe. D. Van Essen, Concurrent processing streams in the monkey visual cortex, Trends in Neuroscience I I (1988) 219-226. 181 M.J. Egenhofer, D.M. Mark, Naive Geography, Technical Report 95-8, National Center for Geographic Information and Analysis, 1995. 191 S.L. Epstein, Prior knowledge strengthens J. Intelligent Systems 7 (1992) 547-586. learning to control search in weak theory domains, Internat. ( 101 S.L. Epstein, For the Right Reasons: the FORR architecture for learning in a skill domain, Cognitive Science I8 (3) (1994) 479-51 I. I I I I S.L. Epstein, Collaboration and interdependence in limitedly rational agents, in: Proceedings AAAI FaII Symposium on Rational Agency, AAAI Press. Cambridge, MA, 1995. 1 121 S.L. Epstein, On heuristic Conference on Artifcial 1995. pp. 454-46 I. reasoning, Intelligence reactivity, Joint in: Proceedings (IJCAI-9.5). Montreal, Que., Morgan Kaufmann, San Mateo, CA, 14th International and search, I I3 1 S.L. Epstein, J. Gelfand, J. Lesniak, Pattern-based decision-making a multi-agent. expert, Computational learning and spatially-oriented concept formation with Intelligence 12 (1) ( 1996) 199-221. 1 141 S.L.Epstein. appear. J. Gelfand. E.T. Lock, Learning game-specific spatially-oriented heuristics, Constraints, to XL. Epstein/Artijiic.iul Intelligence 100 (1998) 275-322 321 I IS I K.A. Ericsson, N. Charness, Expert performance: Its structure and acquisition, American Psychologist 49 (8) (1994) 725-747. 1 16 1 K.A. Ericsson. R.T. Krampe, C. Tcsch-Riimer, The role of deliberate practice in the acquisition of expert performance, Psychological Revirw 100 (3) ( 1993) 363-406. 1171 K.A. Ericsson, J. Smith, Toward a General Theory of Expertise-Prospects and Limits, Cambridge University Press, Cambridge, UK, 1991. [ I8 1 R.G. Golledge, Path selection and route preference in human navigation: a progress report, in: d’itineraires, in: Proceedings ‘95). Semmerling, cognitive des descriptions en Sciences Cognitives, La Motte d’Aveillans, France, 1994. Proceedings 2nd International Conference on Spatial Information and Theory (COSIT Austria, Lecture Notes in Computer Science, Springer, Berlin, 1995, pp. 207-222. A. Gryl, Analyse Chercheurs A. Cry], Analyse et modelisation des processus discursifs mis en oeuvre dans la description d’itindraires. Ph.D. Thesis, Universite Paris Xl Orsay, 1995. P.J. Hayes, The second naive physics manifesto, in: J.R. Hobbs, R.C. Moore of the Commonsense World. Ablex Publishing, Norwood, NJ, 1988, pp. l-36. P.J. Hayes, K.M. Ford. N. Agnew, On babies and bathwater: (1994) G.S. Klein. R. Calderwood, Decision models: some lessons from the field, IEEE Trans. Systems Man Cybernet. 21 (5) J.L. Kolodner. Issue on Case-Based Reasoning, Machine Learning (1991) Introduction (Eds.), Formal Theories tale, AI Magazine Premier Colloque to the Special a cautionary 1018-1026. I5 (4) IO (3) Jeunes 15-26. 1191 1201 1211 1221 1231 1241 1251 1261 1271 1281 1291 1301 131 I 1321 1331 (341 1351 L36l 1371 1381 1391 1401 141 I l-5. I3 (I) (1988) (1993) learning learning, 129-153. 103-130. approach sweeping: intelligence Intelligence to discovery reinforcement in large-scale and mapping initial results, Ph.D. Thesis, in mathematics, Intelligence 2-The in the game of Go: space, AI Magazine 9 (2) learning with less data and less time, spatial knowledge, Cognitive Science 2 (1978) that learns new heuristics and domain concepts, Artificial Intelligence 42 ( 1990) 189-21 I. in robot in: J. Connell, (Eds.), Robot Learning, Kluwer Academic Publishers, Boston, MA, 1993, pp. 141-170. (1993) R. Korf. Real-time heuristic search. Artificial B. Kuipers, R. Froom, W.-Y. Lee, D. Pierce, The semantic hierarchy S. Mahadevan B.J. Kuipers. Modeling B.J. Kuipers, T.S. Levitt, Navigation 25-43. D.B. Lenat. AM: an artificial Department of Computer Science, Stanford University, 1976. D.B. Lenat. EURISKO: a program 26 (1983) 61-98 A.W. Moore, C.G. Atkeson, Prioritized Machine Learning B. Pell. Exploratory Heuristic Programming in Artificial Chichester. UK. 1991. pp. 137-152. J. Piaget, B. Inhelder, The Child’s Conception of Space, W.W. Norton, New York, 1967. D. Pierce. B.J. Kuipers. Learning on Artiticial 1271. A.E. Prieditis. Effective admissible heuristics, Machine Learning 12 ( l-3) M.J. Ratterman. S.L. Epstein, Skilled like a person: a comparison of human and computer game playing, in: Proceedings 17th Annual Conference of the Cognitive Science Society, Lawrence Erlbaum, Pittsburgh, PA, 1995. pp. 709-714. M.J. Ratterman. S.L. Epstein, Game-playing E.K. Sadalla. S.G. Magel, The perception of traversed distance, Environment 6.5-79. M.J. Schoppers, S.J.J. Smith, D.S. Nau, T.A. Throop, Total-order multi-agent in: Proceedings 13th National Conference Press, Cambridge, MA, 1996. pp. IOS- I 13. E. Stefanakis. M. Kavouras, On the determination International Conference Notes in Computer Science, Springer, Berlin, 1995, pp. 241-257. in: D.N.L. Levy, D.F. Beal (Eds.), Second Computer Olympiad, Ellis Horwood, 12th National Conference Press, Cambridge, MA, 1994, pp. l264- 2nd ‘95). Semmerling, Austria, Lecture In defense of reaction plans as caches, Al Magazine 10 (4) to explore and build maps, in: Proceedings for contract bridge, Portland, OR, MIT of the optimum path in space, and Behavior 12 ( 1980) Seattle, WA, AAAI/MlT Information Theory in young children (in preparation). in: Proceedings (1989) 51-60. on Artificial task-network (AAAI-94). (AAAI-96), Intelligence Intelligence on Spatial expertise planning I17-141. (COSIT ( 1993) 322 XL. Epsrei~~/Art~fificicrl intelligence 100 (1998) 275-322 I42 1 R.S. Sutton, programming, Integrated architectures for learning, planning, and reacting based on approximating dynamic in: Proceedings 7th International Conference on Machine Learning, Austin, TX, Morgan Kaufmann, San Mateo. CA, 1990, pp. 216-224. 1431 A. Tate, J. Hendler. M. Drummond, A review of AI planning techniques, in: J. Allen, J. Hendler, A. Tate (Eds. ). Readings in Planning, Morgan Kaufmann, San Mateo, CA, 1990, pp. 26-49. 144 1 B. Tversky. Spatial mental models, The Psychology of Learning and Motivation 145 1 B. Tversky, Distortions 131-138. 1461 L. Ungerleider. M. Mishkin, Two cortical visual systems, in cognitive maps. Geoforum 23 (2) (1992) Analysis of Visual Behavior, MIT Press, Cambridge, MA, 1982, pp. 548-586. in: D. Ingle, M. Goodale, R. Mansfield (Eds.), 27 ( 1991) 109-145. 