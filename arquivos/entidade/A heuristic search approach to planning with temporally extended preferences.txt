Artificial Intelligence 173 (2009) 593–618Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA heuristic search approach to planning with temporally extendedpreferencesJorge A. Baier a,b,∗, Fahiem Bacchus a, Sheila A. McIlraith aa Department of Computer Science, University of Toronto, Canadab Department of Computer Science, Pontificia Universidad Católica de Chile, Chilea r t i c l ei n f oa b s t r a c tArticle history:Received 27 October 2007Received in revised form 28 November 2008Accepted 28 November 2008Available online 6 December 2008Keywords:Planning with preferencesTemporally extended preferencesPDDL3Planning with preferences involves not only finding a plan that achieves the goal,itrequires finding a preferred plan that achieves the goal, where preferences over plansare specified as part of the planner’s input. In this paper we provide a technique foraccomplishing this objective. Our technique can deal with a rich class of preferences,including so-called temporally extended preferences (TEPs). Unlike simple preferences whichexpress desired properties of the final state achieved by a plan, TEPs can express desiredproperties of the entire sequence of states traversed by a plan, allowing the user to expressa much richer set of preferences. Our technique involves converting a planning problemwith TEPs into an equivalent planning problem containing only simple preferences. Thisconversion is accomplished by augmenting the inputed planning domain with a new set ofpredicates and actions for updating these predicates. We then provide a collection of newheuristics and a specialized search algorithm that can guide the planner towards preferredplans. Under some fairly general conditions our method is able to find a most preferredplan—i.e., an optimal plan. It can accomplish this without having to resort to admissibleheuristics, which often perform poorly in practice. Nor does our technique require anassumption of restricted plan length or make-span. We have implemented our approachin the HPlan-P planning system and used it to compete in the 5th International PlanningCompetition, where it achieved distinguished performance in the Qualitative Preferencestrack.© 2008 Elsevier B.V. All rights reserved.1. IntroductionClassical planning requires a planner to find a plan that achieves a specified goal. In practice, however, not every planthat achieves the goal is equally desirable. Preferences allow the user to provide the planner with information that it canuse to discriminate between successful plans; this information allows the planner to distinguish successful plans based onplan quality.Planning with preferences involves not just finding a plan that achieves the goal, it requires finding one that achieves thegoal while also optimizing the user’s preferences. Unfortunately, finding an optimal plan can be computationally expensive.In such cases, we would at least like the planner to direct its search towards a reasonably preferred plan.In this paper we provide a technique for accomplishing this objective. Our technique is able to deal with a rich classof preferences. Most notably this class includes temporally extended preferences (TEPs). The difference between a TEP and aso-called simple preference is that a simple preference expresses some desired property of the final state achieved by the* Corresponding author at: Department of Computer Science, University of Toronto, Canada.E-mail addresses: jabaier@cs.toronto.edu (J.A. Baier), fbacchus@cs.toronto.edu (F. Bacchus), sheila@cs.toronto.edu (S.A. McIlraith).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.11.011594J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618plan, while a TEP expresses a desired property of the sequence of states traversed by the plan. For example, a preferencethat a shift worker work no more than 2 overtime shifts in a week is a temporally extended preference. It expresses acondition on a sequence of daily schedules that might be constructed in a plan. Planning with TEPs has been the subject ofrecent research (e.g. [6,12,35]). It was also a theme of the 5th International Planning Competition (IPC-5).The technique we provide in this paper is able to plan with a class of preferences that includes those that can bespecified in the planning domain definition language PDDL3 [23]. PDDL3 was specifically designed for IPC-5. It extendsPDDL2.2 to include, among other things, facilities for expressing both temporally extended and simple preferences, wherethe temporally extended preferences are described by a subset of linear temporal logic (LTL). It also supports quantifying thevalue of achieving different preferences through the specification of a metric function. The metric function assigns to eachplan a value that is dependent on the specific preferences the plan satisfies. The aim in solving a PDDL3 planning instanceis to generate a plan that satisfies the hard goals and constraints while achieving the best possible metric value, optimizingthis value if possible or at least returning a high value plan if optimization is infeasible.Our technique is a two part approach. The first part exploits existing work [2] to convert planning problems with TEPsto equivalent problems containing only simple preferences defined over an extended planning domain. The second part,and main contribution of our work, is to develop a set of new heuristics, and a search algorithm that can exploit theseheuristics to guide the planner towards preferred plans. Many of our heuristics are extracted from a relaxed planninggraph a technique that has previously been used to compute heuristics in classical planning. Previous heuristics for classicalplanning, however, are not well suited to planning with preferences. The heuristics we present here are specifically designedto address the tradeoffs that arise when planning to achieve preferences.Our search algorithm is also very different from previous algorithms used in planning. As we will show, it has a numberof attractive properties, including the ability to find optimal plans without having to resort to admissible heuristics. This isimportant because admissible heuristics generally lead to unacceptable search performance. Our method is also able to findoptimal plans without requiring a restriction on plan length or make-span. This is important because such restrictions donot generally allow the planner to find a globally optimal plan. In addition, the search algorithm is incremental in that itfinds a sequence of plans each one improving on the previous. This is important because in practice it is often necessary totrade off computation time with plan quality. The first plans in this sequence of plans can often be generated fairly quicklyand provide the user with at least a working plan if they must act immediately. If more time is available the algorithmcan continue to search for a better plan. The incremental search process also employs a pruning technique to make eachincremental search more efficient. The heuristics and search algorithm presented here can easily be employed in otherplanning systems.An additional contribution of the paper is that we have brought all of these ideas together into a working planningsystem called HPlan-P. Our planner is built as an extension of the TLPlan system [1]. The basic TLPlan system uses LTLformulae to express domain control knowledge; thus, LTL formulae serve to prune the search space. However, TLPlan has nomechanism for providing heuristic guidance to the search. In contrast, our implementation extends TLPlan with a heuristicsearch mechanism that guides the planner towards plans that satisfy TEPs, while still pruning those partial plans that violatehard constraints. We also exploit TLPlan’s ability to evaluate quantified formulae to avoid having to convert the preferencestatements (many of which are quantified) into a collection of ground instances. This is important because grounding thepreferences can often yield intractably large domain descriptions. We use our implementation to evaluate the performanceof our algorithm and to analyze the relative performance of different heuristics on problems from both the IPC-5 Simple andQualitative Preferences tracks.In the rest of the paper we first provide some necessary background. This includes a brief description of the featuresof PDDL3 that our approach can handle. In Section 3 we describe the first part of our approach—a method for compilinga domain with temporally extended preferences into one that is solely in terms of simple (i.e., final state) preferences.Section 4 describes the heuristics and search algorithm we have developed. It also presents a number of formal propertiesof the algorithm, including characterizing various conditions under which the algorithm is guaranteed to return optimalplans. Section 5 presents an extensive empirical evaluation of the technique, including an analysis of the effectiveness ofvarious combinations of the heuristics presented in Section 4. Section 6 presents a discussion of the approach and Section 7summarizes our contributions and discusses related work after which we provide some final conclusions.2. BackgroundThis section reviews the background needed to understand this paper. Section 2.1 presents some basic planning defi-nitions and a brief description of the planning domain definition language PDDL. Section 2.2 describes a variation of thewell-known approach to computing domain-independent heuristics based on the computation of relaxed plans that is usedby our planner to compute heuristics. As opposed to most well-known approaches, our method is able to handle ADL do-mains directly without having to pre-compile the domain into a STRIPS domain. Section 2.3 describes the planning domaindefinition language PDDL3, a recent version of PDDL that enables the definition of hard constraints, preferences, and metricfunctions.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–6185952.1. An overview of planning formalisms and languagesA classical planning instance is a tuple I = (Objs, Oper, Init, Goal), where Objs is a finite set of objects, Oper is a finite setof planning operators, Init is the initial state, i.e., a finite set of ground literals—or simply, facts—describing the initial state,and Goal describes the set of goal states.In STRIPS planning instances [19], the set Oper contains operator descriptions of the form (pre(o), add(o), del(o)), wherepre(o) is a list of precondition facts for operator o, add(o)—the add list—is a list of facts that are positive effects of operator o,and del(o)—the delete list—is a list of facts that are negative effects of operator o. Finally, Goal is a set of goal facts.In the more expressive ADL formalism [31], operators still describe preconditions and effects, but these can now bemore than simple lists of ground literals. ADL preconditions can be arbitrary boolean formulae, existentially or universallyquantified over the set of objects Objs. ADL effects can be conditional, which means that adds and deletes can be conditionedon the satisfaction of arbitrary boolean formulae. Effects can also be universal in the sense that they affect all objects thatsatisfy a certain condition. For example, assume we are describing a domain where objects can contain other objects.Further, assume action move(x, y, z) moves object x from location y to location z and in the process moves all objects in xto z as well. The precondition for this action is just at(x, y); i.e., the object x has to be at location y, while its effects canbe defined by the list:(cid:2)Eff =add at(x, z), ∀v(cid:3)(cid:4)in(v, x) ⇒ add at(v, z), del at(x, y), ∀vin(v, x) ⇒ del at(v, y).(cid:3)(cid:4)(cid:5)Thus, the location of the object x and all objects inside x changes to z.In addition to more expressive preconditions and effects, ADL also allows for the representation of functions. This meansthat states can contain, in addition to propositional facts, sentences of the form f ((cid:5)c) = z, where fis a function name, c isa tuple of objects in Objs, and z is an object in Objs. Actions can change the functions by assigning f ((cid:5)c) a different value asan add effect.Finally, in ADL, Goal can be any formula (possibly quantified) that describes a condition that must be satisfied by a goalstate. For more details on ADL we refer the reader to [31].Although STRIPS and ADL can be used to provide formal descriptions of classical planning instances, they cannot be usedas a standard input language for planners since their precise syntactical form has never been standardized. The PlanningDomain Definition Language (PDDL) [30], on the other hand, was specifically designed to provide a uniform syntax fordescribing planning problems in the context of the 1998 International Planning Competition. PDDL is currently a de factostandard for describing planning problems, and it has been extended and used in all subsequent versions of IPC.Recent versions of PDDL enable the definition of planning instances in a superset of ADL. For example, PDDL2.1 [20]extends ADL by enabling explicit representation of time. Among other features, it allows the specification of actions withduration. On the other hand, PDDL2.2 [16] extends PDDL2.1 by allowing derived predicates (i.e., predicates defined axiomati-cally), and timed literals (i.e., literals that will become true at a specified time instant). PDDL3, as we describe in Section 2.3,extends PDDL2.2 with hard constraints, preferences, and metric functions.The planning problem in both the STRIPS and the ADL settings is the problem of finding a legal sequence of actions—ground operators—that, when executed in the initial state, will lead to a state in which the goal condition Goal is satisfied.2.2. Planning as heuristic searchMany state-of-the-art domain-independent planners use domain-independent heuristics to guide the search for a plan.Heuristics estimate the cost of achieving the goal from a certain state. They can be used with standard search algorithms,and are usually key to good performance. They are typically computed by solving a relaxed version of the original problem.One of the most popular domain-independent relaxations corresponds to ignoring the negative effects of actions. This is theapproach taken by many planners (e.g., hsp [8] and ff [27], among others). In the STRIPS formalism, this corresponds toignoring delete lists.In this paper we exploit heuristic search to plan with preferences. The heuristics presented here are based on the well-known technique of computing a relaxed planning graph [27], which is the graph that would be generated by Graphplan[7] on the STRIPS relaxed planning instance that ignores negative effects. This graph is composed of fact layers—or relaxedworlds—and action layers. The action layer at level n contains all actions that are possible in the relaxed world at depth n.The relaxed world at depth n + 1 contains all the facts that hold at layer n + 1 and is generated by applying all the positiveeffects of actions in action layer n. The graph is expanded until the goal is satisfied by the final relaxed world or a fixedpoint is reached.Once the graph is expanded, one can compute a relaxed plan for the goals by regression from the goal facts in the graphto the initial state. The length of this plan can then be used as a heuristic estimator of the cost for achieving the goal. Inthe rest of the paper we assume familiarity with the extraction of relaxed plans. For more details we refer the reader to thearticle by Hoffmann and Nebel [27].2.2.1. Relaxed plans for function-free ADL domainsTo compute heuristics for function-free ADL domains one can first transform the domain to STRIPS, using a well-knownprocedure described by Gazen and Knoblock [21], and then compute the heuristic as usual. This is the approach taken596J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618by some systems (e.g. ff) but unfortunately this procedure can lead to a considerable blow up in the size of the originalinstance.Our planner handles ADL domains, but takes a different approach. In particular, it computes the relaxed planning graphdirectly from the ADL instance, using an approach similar to that taken by the Marvin planning system [11]. To effectivelyhandle relaxed ADL domains (in which effects can be conditioned on negative facts), the relaxed worlds represent both thefacts that become true and the facts that become false after executing a set of actions. To that end, the relaxed worlds aredivided into two parts: a positive part, that represents added facts, and a negative part, that represents deleted facts.+n−0+0−k+n , F−n ), with F−0 ), . . . , (FWhen computing a relaxed planning graph for a state s, the set of relaxed worlds is a sequence of pairs of fact sets+= sc , where sc is the set of facts not in s (i.e., the complement of s). Further-(F0 , Fmore, if action a appears in the action layer at depth n, all facts that are added by a are included in the positive relaxed−world at depth Fk+1. Moreover, all facts in layer k are copied to layer⊆ Fk + 1 (i.e. F+k+1, whereas facts that are deleted by a are added to F+k+1 and F= s and F−k+1).⊆ FSpecial care has to be taken in the evaluation of preconditions and conditions in conditional effects for actions, becausenegations could appear anywhere in those conditions. To evaluate a formula in a relaxed world, we evaluate its negationnormal form (NNF) instead. In NNF, all negations appear right in front of atomic formulae. A formula can easily be convertedto NNF by pushing negations in using the standard rules ¬∃. f ≡ ∀.¬ f , ¬∀. f ≡ ∃.¬ f , ¬( f 1 ∧ f 2) ≡ ¬ f 1 ∨ ¬ f 2, ¬( f 1 ∨ f 2) ≡¬ f 1 ∧ ¬ f 2, and ¬¬ f ≡ f .Now assume we want to determine whether or not the formula φ is true in the relaxed state (F−k ) in the graph withbe the NNF of φ. To evaluate φ we instead evaluate φ(cid:11)relaxed worlds (Frecursively in the standard way, interpreting quantifiers and boolean binary operators as usual. When evaluating a positive+fact f , we return the truth value of f ∈ Fk . On the other hand, when evaluating a negative fact ¬ f , we return the truth−value of f ∈ Fk . In short, ¬ fis true at depth k if f was deleted by an action or was already false in the initial state. Moreformally,n ). Furthermore, let φ(cid:11)−k ) · · · (F−0 ) · · · (F+k , F+k , F+0 , F+n , F−Definition 1 (Truth of an NNF formula in a relaxed state). Let the relaxed planning graph constructed from the initial state s in+−+−0 ) · · · (Fk , Fa problem where the set of objects of the problem is Objs be (Fk ). The following cases define when φ is0 , F−+k ) |(cid:13) φ.k , Ftrue at level k of the relaxed planning graph, which is denoted as (F+• If φ is an atomic formula then (Fk , F• If φ = ¬ f , where fis an atomic formula, then (F−+−+k ) |(cid:13) ψ and (Fk ) |(cid:13) φ iff (F• If φ = ψ ∧ ξ , then (Fk , Fk , F+−+−+k ) |(cid:13) φ iff (F• If φ = ψ ∨ ξ , then (Fk ) |(cid:13) ψ or (Fk , Fk , Fk , F+−+k ) |(cid:13) φ iff for every o ∈ Objs (F• If φ = ∀x.ψ , then (Fk , Fk , F−k ) |(cid:13) φ iff φ ∈ F+k , F+k .−k ) |(cid:13) φ iff φ ∈ F−+k ) |(cid:13) ξ .k , F−k ) |(cid:13) ξ .−k ) |(cid:13) ψ(x/o), where ψ(x/o) is the formula ψ with all free−k .instances of x replaced by o.1• If φ = ∃x.ψ , for some o ∈ Objs (F+k , F−k ) |(cid:13) ψ(x/o).The standard relaxed plan extraction has to be modified slightly for the ADL case. Now, because actions have conditionaleffects, whenever a fact fis made true by action a there is a particular set of facts that is responsible for its addition, i.e.those that made both the precondition of a and the condition in its conditional effect true. When recursing from a subgoalf we add as new subgoals all those facts responsible for the addition of f (which could be in either part of the relaxedworld).As is the case with STRIPS relaxed planning graphs, whenever a fact fis reachable from a state by performing a certainsequence of legal actions, then f eventually appears in a fact layer of the graph. The same happens in these relaxed planninggraphs. This is proven in the following proposition.−−Proposition 2. Let s be a planning state, R = (F1 ) · · · (Fm ) be the relaxed planning graph constructed from s up toa fixed point, and φ be an NNF formula. If φ is true after performing a legal sequence of actions a1 · · · an in s, then there exists somek (cid:2) m such that (F−0 )(F+0 , F+1 , F+m , F+k , F−k ) |(cid:13) φ.Proof. See Appendix A. (cid:2)This proposition verifies that the relaxed planning graph is in fact a relaxation of the problem. In particular, it says thatif the goal is not reachable in the relaxed planning graph then it is not achievable by a real plan.Besides being a desirable property, this reachability result is key to some interesting properties of our search algorithm.In particular, as we see later, it is essential to proving that some of the bounding functions we employ will never prune anoptimal solution (under certain reasonable assumptions).1 In our implementation, bounded quantification is used so that this condition can be checked more efficiently. In particular, this means that not everyobject in Objs need be checked.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–6185971.2.3.4.5.6.s0s1 · · · sn |(cid:13) (always φ)s0s1 · · · sn |(cid:13) (sometime φ)s0s1 · · · sn |(cid:13) (at end φ)s0s1 · · · sn |(cid:13) (sometime-after φψ)s0s1 · · · sn |(cid:13) (sometime-before φψ)s0s1 · · · sn |(cid:13) (at-most-once φ)sn |(cid:13) φiff ∀i : 0 (cid:2) i (cid:2) n, si |(cid:13) φiff ∃i: 0 (cid:2) i (cid:2) n, si |(cid:13) φiffiff ∀i if si |(cid:13) φ then ∃ j: i (cid:2) j (cid:2) n, s j |(cid:13) ψiff ∀i if si |(cid:13) φ then ∃ j: 0 (cid:2) j < i, s j |(cid:13) ψiff ∀i: 0 < i (cid:2) n, if S i |(cid:13) φ then ∃ j:j (cid:3) i, ∀k: k > j, sk |(cid:13) ¬φFig. 1. Semantics of PDDL3’s temporally extended formulae that do not mention explicit time. The trajectorys0s1 · · · sn represents the sequence of states that results from the execution a sequence of actions a1 · · · an .2.3. Brief description of PDDL3PDDL3 was introduced by Gerevini and Long [23] for the 5th International Planning Competition. It extends PDDL2.2 byenabling the specification of preferences and hard constraints. It also provides a way of defining a metric function that definesthe quality of a plan dependent on the satisfaction of the preferences.The current version of our planner handles the non-temporal and non-numeric subset of PDDL3, which was the languageused for the Qualitative Preferences track in IPC-5. In this subset, temporal features of the language such as durative actionsand timed fluents are not supported. Moreover, preference formulae that mention explicit times (e.g., using operators suchas within and always-within) are not supported. Numeric functions (PDDL fluents) are not supported either. The restof this section briefly describes the new elements introduced in PDDL3 that we do support.2.3.1. Temporally extended preferences and constraintsPDDL3 specifies TEPs and temporally extended hard constraints in a subset of a quantified linear temporal logic (LTL)[32]. These LTL formulae are interpreted over trajectories, which in the non-temporal subset of PDDL3 are sequences of statesthat result from the execution of a legal sequence of actions. Fig. 1 shows the semantics of LTL-based operators that canbe used in temporally extended formulae. The first two operators are standard in LTL; the remaining ones are abbreviationsthat can be defined in terms of standard LTL operators.2.3.2. Temporally extended preferences and constraintsPreferences and constraints (which can be viewed as being preferences that must be satisfied) are declared using the:constraints construct. Each preference is given a name in its declaration, to allow for later reference. By way ofillustration, the following PDDL3 code defines two preferences and one hard constraint.(:constraints(and(preference cautious(forall (?o - heavy-object)(sometime-after (holding ?o)(at recharging-station-1))))(forall (?l - light)(preference p-light (sometime (turn-off ?l))))(always (forall ?x - explosive) (not (holding ?x)))))The cautious preference suggests that the agent be at a recharging station sometime after it has held a heavy ob-ject, whereas p-light suggests that the agent eventually turn all the lights off. Finally, the (unnamed) hard constraintestablishes that an explosive object cannot be held by the agent at any point in a valid plan.When a preference is externally universally quantified, it defines a family of preferences, containing an individual pref-erence for each binding of the variables in the quantifier. Therefore, preference p-light defines an individual preferencefor each object of type light in the domain. Preferences that are not quantified externally, like cautious, can be seenas defining a family containing a single preference.Temporal operators cannot be nested in PDDL3. Our approach can however handle the more general case of nestedtemporal operators.2.3.3. Precondition preferencesPrecondition preferences are atemporal formulae expressing conditions that should ideally hold in the state in which theaction is performed. They are defined as part of the action’s precondition. For example, the preference labeled econ belowspecifies a preference for picking up objects that are not heavy.(:action pickup :parameters (?b - block)(:precondition (and (clear ?b)(:effect (holding ?b)))(preference econ (not (heavy ?b)))))598J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618Precondition preferences behave something like conditional action costs. They are violated each time the action is exe-cuted in a state where the condition does not hold. In the above example, econ will be violated every time a heavy blockis picked up in the plan. Therefore these preferences can be violated a number of times.2.3.4. Simple preferencesSimple preferences are atemporal formulae that express a preference for certain conditions to hold in the final state ofthe plan. They are declared as part of the goal. For example, the following PDDL3 code:(:goal (and (delivered pck1 depot1)(preference truck (at truck depot1))))specifies both a hard goal (pck1 must be delivered at depot1) and a simple preference (that truck is at depot1).Simple preferences can also be externally quantified, in which case they again represent a family of individual preferences.2.3.5. Metric functionThe metric function defines the quality of a plan, generally depending on the preferences that have been achieved by theplan. To this end, the PDDL3 expression (is-violated name), returns the number of individual preferences in the namefamily of preferences that have been violated by the plan. When name refers to a precondition preference, the expressionreturns the number of times this precondition preference was violated during the execution of the plan.The quality metric can also depend on the function total-time, which, in the non-temporal subset of PDDL3, returnsthe plan length, and the actual duration of the plan in more expressive settings. Finally, it is also possible to define whetherwe want to maximize or minimize the metric, and how we want to weigh its different components. For example, the PDDL3metric function:(:metric minimize (+ (total-time)(* 40 (is-violated econ))(* 20 (is-violated truck))))specifies that it is twice as important to satisfy preference econ as to satisfy preference truck, and that it is less impor-tant, but still useful, to find a short plan.In this article we focus on metric functions that mention only total-time or is-violated functions, since we donot allow function symbols in the planning domain.3. Preprocessing PDDL3As described in the previous section, PDDL3 supports the definition of temporally extended preferences in a subset ofLTL. A brute force method for generating a preferred plan would be to generate all plans that realize the goal and thento rank them with respect to the PDDL3 metric function. However, evaluating plans once they have been generated is notefficient because there could be many plans that achieve the goal. Instead, we need to be able to provide heuristic guidanceto the planner to direct it towards the generation of high-quality plans. This involves estimating the merit of partial plans byestimating which of the TEPs could potentially be satisfied by one of its extensions (and thus estimating the metric valuethat could potentially be achieved by some extension). With such heuristic information the planner could then direct thesearch effort towards growing the most promising partial plans.To actively guide the search towards plans that satisfy the problem’s TEPs we develop a two-part approach. The firstcomponent of our approach is to exploit the techniques presented by Baier and McIlraith [2] to convert a planning domaincontaining TEPs into one containing an equivalent set of simple (final-state) preferences. Simple preferences are quite similarto standard goals (they express soft goals), and thus this conversion enables the second part of our approach, which is toextend existing heuristic approaches for classical goals to obtain heuristics suitable for guiding the planner toward theachievement of this new set of simple preferences. The development and evaluation of these new heuristics for simplepreferences is one of the main contributions of our work and is described in the next section. That section also presents anew search strategy that is effective in exploiting these heuristics.In this section we describe the first part of our approach: how the techniques of Baier and McIlraith [2] can be exploitedto compile a planning domain containing TEPs into a domain containing only simple preferences. Besides the conversion ofTEPs we also describe how we deal with the other features of PDDL3 that we support (i.e., those described in the previoussection).3.1. Temporally extended preferences and constraintsBaier and McIlraith [2] presented a technique that can construct an automaton Aϕ from a temporally extended formula ϕ.The automaton Aϕ has the property that it accepts a sequence of states (e.g., a sequence of states generated by a plan) ifand only if that sequence of states satisfies the original formula ϕ. The technique works for a rich subset of first-orderJ.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618599linear temporal logic formulas that includes all of PDDL3’s TEPs. It also includes TEPs in which the temporal operatorsare nested, which is not allowed in PDDL3. To encode PDDL3 preference formulae, each preference formula is representedas an automaton. Reaching an accepting condition of the automaton corresponds to satisfying the associated preferenceformula.The automaton Aϕ can then be embedded within the planning domain by extending the domain with new predicatesrepresenting the state of the automaton. Thus, in the initial state of the planning problem these predicates will capturethe fact that the automaton, starting from its initial state, has just inputed the initial state of the problem. The techniquealso modifies the domain’s actions so that they can properly update the “automata-state” predicates. When a sequenceof actions is applied starting in the initial state, the automata-state predicates are updated to capture the progress theseactions have made towards satisfying the preference formula to which the automaton corresponds. Hence we can determineif a sequence of actions has satisfied ϕ by simply testing if the automata-state predicates in the final state arising fromthese actions indicate that the automaton is in an accepting state. In other words, the technique allows one to convert atemporally extended condition (ϕ) into a condition on the final state (the automaton state predicates indicate that Aϕ is ina accepting state).One important feature of the compilation technique we exploit is that it can construct parameterized automata. That is,we do not need to expand a quantified first-order temporal extended formula ϕ into a larger propositional formula (bycomputing all ground instantiations). This means that the technique generates compact domains, by avoiding grounding ofquantified preferences. Generating a compact compiled problem is key for good performance, as we will see in Section 5.Although in general the size of the automaton that results from compiling an arbitrary LTL formula ϕ can be exponentialin |ϕ|, in case of the restricted subset of LTL allowed by PDDL3 (in which formulae do not allow nestings of temporaloperators) an exponential blowup cannot occur.Baier and McIlraith’s original paper was aimed at planning with temporally extended goals, not preferences. Up to theconstruction of the automata for each temporally extended formula, our approach is identical to that taken by them. How-ever, Baier and McIlraith [2] then propose using derived predicates to embed the automata in the planning domain. In ourwork we have chosen a different approach that is more compatible with the underlying TLPlan system we employed inour implementation. In the rest of the section, we give some more details on the construction of automata and the way weembed these automata into a planning domain. Further details on automata construction can be found in [2].3.1.1. Parameterized finite state automataThe compilation process first constructs a parameterized nondeterministic finite-state automaton (PNFA) Aϕ for eachtemporally extended preference or hard constraint expressed as an LTL formula ϕ. The PDDL3 operators presented in Fig. 1that are abbreviations are first expanded into standard LTL operators following Gerevini and Long [23].The PNFA represents a family of nondeterministic finite-state automata. Its transitions are labeled by first-order formulae,and its input language is the set of all strings of plan states. A PNFA Aϕ accepts a sequence of plan states iff such a sequencesatisfies ϕ. Fig. 2 shows some examples of PNFA for first-order LTL formulae.Parameters in the automaton appear when the LTL formula is externally quantified (e.g., Fig. 2(b)). The intuition is thatdifferent objects (or tuples of objects) can be in different states of the automaton. Tuples of objects can transition from athat is labeled by awhen the automaton reads a plan state s iff there is a transition between q and qstate q to a state qformula that is satisfied in s.(cid:11)(cid:11)As an example, consider a transportation domain with two packages, A and B, which are initially not loaded in anyvehicle. Focusing on the formula of Fig. 2(b), we see that both objects start off in the initial state q0. Then the automatoninputs the initial state of the planning problem. That state satisfies the formula (implies (loaded ?x) (delivered?x)) for both packages A and B since neither is loaded in the initial state. Hence the packages transition to state q2 as wellas stay in state q0 (the automata is nondeterministic). This means that initially both objects satisfy the temporal formula,since both are in the automaton’s accepting state q2. That is, the null plan satisfies the formula (b) of Fig. 2. Now, assumewe perform the action load( A, Truck). In the resulting state, B stays in q0 and moves once again from q0 to q2 while Anow moves from q0 to q1. Hence, A no longer satisfies the formula; it will satisfy it only if the plan reaches a state wheredeli vered( A) is true.A PNFA is useful for computing heuristics because it effectively represents all the different paths to the goal that canachieve a certain property; its states intuitively “monitor” the progress towards satisfying the original temporal formula.Therefore, while expanding a relaxed planning graph for computing heuristics, one is implicitly considering all possible(relaxed) ways of satisfying the property.3.1.2. Representing the PNFA within the planning problemAfter the PNFA has been constructed it must be embedded within the planning domain. This is accomplished by ex-tending the original planning problem with additional predicates that represent the state of the automaton in each planstate. If the planning domain has multiple TEPs (as is usually the case), a PNFA is constructed for each TEP formula andthen embedded within the planning domain with automaton-specific automata-state predicates. That is, the final planningproblem will contain distinct sets of automata-state predicates, one for each embedded automaton.600J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618Fig. 2. PNFA for (a) (sometime (exists (?c) (and (cafe ?c) (at ?c)))), and (b) (forall (?x) (sometime-after (loaded ?x)(delivered ?x))). In both PNFA q0 is the initial state and the accepting states are indicated by a double circle border.(a)(b)To represent an automaton within the domain, we define a predicate specifying the automaton’s current set of states.When the automaton is parameterized, the predicate has arguments, representing the current set of automaton states for aparticular tuple of objects. In our example, the fact (aut-state q0 A) represents that object A is in automaton state q0.Moreover, for each automaton we define an accepting predicate. The accepting predicate is true of a tuple of objects if theplan has satisfied the temporal formula for the tuple.Rather than modify the domain’s actions so that the automata state can be properly updated as actions are executed (aswas done by Baier and McIlraith [2]) we instead modified the underlying TLPlan system so that after every action it wouldautomatically apply a specified set of automata updates. Automata updates work like pseudo-actions that are performedautomatically while a new successor is generated. When generating the successor to s after performing action a, the plannerby adding and deleting the effects of a. When this is finished, it processes the automata updatesbuilds the new state sis then regarded as the actual successor of s after performing a. Theover scompilation process can then avoid changes to the domain’s actions and instead insert all of the conditions needed totransition the automata state in one self-contained addition to the domain specification., generating a new successor s. The state s(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)Syntactically, the automata updates are encoded in the domain as first-order formulae that contain the add and delkeywords, just like regular TLPlan action effect specifications. For the automata of Fig. 2(b), the update would include rulessuch as:(forall (?x) (implies (and (aut-state q0 ?x) (loaded ?x))(add (aut-state q1 ?x))))That is, an object ?x moves from state q0 to q1 whenever (loaded ?x) is true.Analogously, we define an update for the accepting predicate, which is performed immediately after the automataupdate—if the automaton reaches an accepting state then we add the accepting predicate to the world state.In addition to specifying how the automata states are updated, we also need to specify what objects are in what au-tomata states in the initial state of the problem. This means we must augment the problem’s initial state by adding acollection of automata facts. Given the original initial state and an automaton, the planner computes the states that everyrelevant tuple of objects can be in after the automaton has inputed the problem’s initial state, and then adds the corre-sponding facts to the new problem. In our example, the initial state of the new compiled problem contains facts stating thatboth A and B are in states q0 and q2.If the temporally extended formula originally described a hard constraint, the accepting condition of the automaton canbe treated as an additional mandatory goal. During search we also use TLPlan’s ability to incrementally check temporalconstraints to prune from the search space those plans that have already violated the constraint.3.2. Precondition preferencesPrecondition preferences are very different from TEPs: they are atemporal, and are associated with the execution ofactions. If a precondition preference p is violated n times during the plan, then the PDDL3 function (is-violated p)returns n.Therefore, the compiled problem contains a new domain function is-violated-counter-p, for each preconditionpreference family p. This function keeps track of how many times the preference has been violated. It is initialized to zeroand is (conditionally) incremented whenever its associated action is performed in a state that violates the atemporal pref-erence formula. In the case where the preference is quantified, the function is parameterized, which allows us to computethe number of times different objects have violated the preference.For example, consider the PDDL3 pickup action given above. In the compiled domain, the original declaration is re-placed by:J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618601(:action pickup :parameters (?b - block)(:precondition (clear ?b))(:effect (and (when (heavy ?b)(increase (is-violated-counter-econ)1)))(holding ?b))) ;; add (holding ?b)3.3. Simple preferencesAs with TEPs, we add new accepting predicates to the compiled domain, one for each simple preference. We also defineupdates, analogous to the automata updates for these accepting predicates. Accepting predicates become true iff the prefer-ence is satisfied. Moreover, if the preference is quantified, these accepting predicates are parameterized: they can be true ofsome tuples of objects and at the same time be false for other tuples.3.4. Metric functionFor each preference family name, we define a new domain function is-violated-name. The return values of thesefunctions are defined in terms of the accepting predicates (for temporally extended and simple preferences) and in termsof the violation counters (for precondition preferences). If preference p is quantified, then the is-violated-p functioncounts the number of object tuples that fail to satisfy the preference.By way of illustration, the TLPlan code that is generated for the preference p-light defined in Section 2.3.2 is:(def-defined-function (is-violated-p-light)(local-vars ?x)(and (:= ?x 0)(forall (?l) (light ?l);; ?x is a local variable;; ?x initialized to 0(implies (not (preference_p-light_satisfied ?l))(:= ?x (+ ?x 1))));; increase ?x by 1 if preference not satisfied(:= is-violated-p-light ?x))) ;; return total sumwhere preference_p-light_satisfied is the accepting predicate defined for preference p-light. Note our translationavoids grounding by using quantification to refer to all objects of type light.If the original metric function contains the PDDL3 function (total-time), we replace its occurrence by the TLPlanfunction (plan-length), which counts the number of actions in the plan. Thus, actions are implicitly associated a unitaryduration.The metric function in the resulting instance is defined just as in the PDDL3 definition but by making reference tothese new functions. If the objective was to maximize the function we invert the sign of the function body. Therefore, wehenceforth assume that the metric is always to be minimized.In the remainder of the paper, we use the notation is-violated(p, N) to refer to the value of is-violated-p ina search node N. We will sometimes refer to the metric function as M, and we will use M(N) to denote the value of themetric in search node N.4. Planning with preferences via heuristic searchStarting with the work of unpop [29], hsp [8], and ff [27], forward-chaining search guided by heuristics has proved tobe a powerful and useful paradigm for solving planning problems. As shown above, the automata encoding of temporallyextended preferences allows us to automatically augment the domain with additional predicates that serve to keep trackof the partial plans’ progress towards achieving the TEPs. The central advantage of this approach is that it converts theplanning domain to one with simple preferences. In particular, now the achievement of a TEP is marked by the achievementof an accepting predicate for the TEP, which is syntactically identical to a standard goal predicate.This means that, in the converted domain, standard techniques for computing heuristic distances to goal predicatescan be utilized to obtain heuristic distances to TEP accepting predicates. For example, the standard technique based on arelaxed planning graph [27], which approximates the distance to each goal and each TEP accepting predicate can be used toheuristically guide a forward-chaining search.Nevertheless, although the standard methods can be fairly easily modified in this manner, our aim here is to developa search strategy that is more suitable to the problem of planning with TEPs. In particular, our approach aims to providea search algorithm with three main features. First, the planner should find good plans, which optimize a supplied metricfunction. Second, it should be able to generate optimal plans, or at least be able to generate an improvement over anexisting plan. Finally, since in some contexts it might be very hard to achieve an optimal plan—and hence a great deal ofsearch effort could be required—we want the algorithm to find at least one plan as quickly as possible.Heuristic search with non-admissible heuristics, like the relaxed goal distances employed in planners like ff can bevery effective at quickly finding a plan. However, they offer no assurances about the quality of the plan they find. Onthe other hand, if an admissible heuristic is used, the plan found is guaranteed to be optimal (assuming the heuristic602J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618is admissible with respect to the supplied plan metric). Unfortunately, admissible heuristics typically perform poorly inpractice [8]. Hence, with an admissible heuristic the plan often fails to find any plan. This is typically unacceptable inpractice.In this section we develop a heuristic search technique that exploits the special structure of the translated planningdomains in order to (a) find a plan fairly rapidly using a non-admissible heuristic and (b) generate a sequence of improvedplans that, under some fairly general conditions, terminates with an optimal plan by using a bounding technique. In partic-ular, our search technique allows one to generate better plans—or even optimal plans—if one has sufficient computationalresources available. It also allows one to improve on an existing plan and sometimes prove a plan to be optimal.In the rest of the section we begin by describing a set of different heuristic functions that can serve to guide the searchtowards satisfying goals and preferences. Then, we describe our search algorithm and analyze some of its properties.4.1. Heuristics functions for planning with preferencesOur algorithm performs a forward search in the space of states guided by heuristics. Most of the heuristic functionsgiven below are computed at a search node N by constructing a relaxed planning graph as described in Section 2.2.1. Thegraph is expanded from the planning state corresponding to N and is grown until all goal facts and all preference facts (i.e.,instances of the accepting predicates) appear in the relaxed state or a fixed point is reached. The goal facts correspond tothe hard goals, and the preference facts correspond to instantiations of the accepting predicates for the converted TEPs.Since in our compiled domain we need to update the automata predicates, the procedure in Section 2.2.1 is modifiedto apply automata updates in action layers after all regular actions have been performed. On the other hand, becauseour new compiled domain has functions, in addition we modify the procedure in Section 2.2.1 to ignore all effects thatdirectly affect the value of a function. This means that in the relaxed worlds, all preference counters will have the samevalue as in the initial state s. Note that since preference counters do not appear in the conditions of conditional effectsor in the preconditions of actions, Proposition 2 continues to hold for relational facts; in particular, it holds for acceptingpredicates.Below we describe a suite of heuristics that can be computed from the relaxed planning graph and can be used for plan-ning with preferences. They are designed to guide the search towards (1) satisfying the goal, and (2) satisfying highly valuedpreferences, i.e., those preferences that are given a higher weight in the metric function. However, highly valued preferencescan be very hard to achieve and hence guiding the planner towards the achievement of such preferences might yield un-acceptable performance. To avoid this problem, our approach tries to account for the difficulty of satisfying preferences aswell as their value, ultimately attempting to achieve a tradeoff between these two factors.4.1.1. Goal distance function (G)This function returns an estimate of the number of actions needed to achieve the goal (planning problems often containa hard “must achieve” goal as well as a collection of preferences). G is the same as the heuristic used by the ff planner butmodified for the ADL case. The value returned by G is the number of actions contained in a relaxed plan that achieves thegoal.4.1.2. Preference distance function ( P )This function is a measure of how hard it is to reach the various preference facts. It is based on a heuristic proposed byZhu and Givan [37] for conjunctive hard goals, but adapted to the case of preferences. Let P be the set of preference factsthat appear in the relaxed planning graph, and let d( f ) be the depth at which f first appears during the construction of thegraph. Then P (N) =f ∈P d( f )k, for some parameter k. Notice that unreachable preference facts (i.e., those not appearingin the graph) do not affect P ’s value.(cid:6)4.1.3. Optimistic metric function (O )The O function is an estimate of the metric value achievable from a search node N in the search space. O does notrequire constructing the relaxed planning graph. Rather, we compute it by assuming (1) no further precondition preferenceswill be violated in the future, (2) TEPs that are violated and that can be proved to be unachievable from N are regardedas false, (3) all remaining preferences are regarded as satisfied, and that (4) the value of (total-time) is evaluated tothe length of the plan corresponding to N. To prove that a TEP p is unachievable from N, O uses a sufficient condition.It checks whether or not the automaton for p is currently in a state from which there is no path to an accepting state.Examples of LTL formulae that can be detected by this technique as always being falsified in the future are those of theform (always ϕ). Indeed, as soon as ϕ becomes false, from no state in the automaton’s current set of states will it bepossible to reach an accepting state.Although O clearly underestimates the set of preferences that can be violated by any plan extending N it is not neces-sarily a lower bound on the metric value of any plan extending N. It will be a lower bound when the metric function isnon-decreasing in the number of violated preferences. As we will see later, lower bounds for the metric function can beused to soundly prune the search space and speed up search.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618603Definition 3 (NDVPL metric functions). Let I be a (preprocessed) PDDL3 planning instance, let the set Γ contain its prefer-ences, and let length(N) be the length of the sequence of action that generated N. A metric function M is non-decreasingin the number of violated preferences and in plan length (NDVPL) iff for any two nodes N and Nit holds that:(cid:11)(1) If length(N) (cid:3) length(N(2) If(total-time) appears in M, and length(N) > length(N(cid:11)), and for every p ∈ Γ , is-violated(p, N) (cid:3) is-violated(p, N(cid:11)), and(cid:11)), and for every p ∈ Γ , is-violated(p, N) (cid:3)(cid:11)), then M(N) (cid:3) M(Nis-violated(p, N(cid:11)), then M(N) > M(N(cid:11)).NDVPL metrics are natural when the objective of the problem is to minimize the metric function (as in our preprocessedinstances). Problems with NDVPL metrics are those in which violating preferences never improves the metric of the plan.Furthermore, adding more actions to a plan that fail to satisfy any new preferences can never improve its metric. Below, inRemark 16, we see that additive metrics, which were the only metrics used in IPC-5, satisfy this condition.Proposition 4. If the metric function is NDVPL, then O (N) is guaranteed to be a lower bound on the metric value of any plan extend-ing N.Proof. The optimistic metric only regards as violated those preferences that are provably violated in every successor of N(i.e., in every state reachable from N by some sequence of actions). It regards as satisfied all remaining preferences. That(cid:11)is, O is evaluating the metric in a hypothetical node N O such that for any node Nreachable from N and for every(cid:11)). Furthermore, because O evaluates the plan length to that of N, ourp ∈ Γ is-violated(p, N O ) (cid:2) is-violated(p, N(cid:11)). Since thehypothetical node is such that length(N O ) = length(N) and hence we have length(N O ) (cid:2) length(N(cid:11)). It follows thatmetric function is NDVPL, it follows from Definition 3 that for every successor NO (N) returns a lower bound on the metric value of any plan extending N. (cid:2)of N, M(N O ) (cid:2) M(N(cid:11)The O function is a variant of the “optimistic weight” heuristic in the PPlan planner [6]. PPlan progresses LTL preferences(as defined by Bacchus and Kabanza [1]) through every node of the search space. The optimistic weight assumes as falsifiedonly those LTL preferences that have progressed to false.4.1.4. Best relaxed metric function (B)The B function is another estimate of the metric value achievable by extending a node N. It utilizes the relaxed planninggraph grown from the state corresponding to N to obtain its estimate. In particular, we evaluate the metric function ineach of the relaxed worlds of the planning graph and take B to be the minimum among these values. The metric functionevaluated in a relaxed world w, M(w), evaluates the is-violated functions directly on w, and evaluates (total-time)as the length of the sequence of actions that corresponds to N.For the case of NDVPL metric functions, B is similar to O , but can return tighter estimates. Indeed, note that the lastlayer of the relaxed planning graph contains a superset of the preference facts that can be made true by some successorto the current state. Also, because the counters for precondition preferences are not updated while expanding the graph,the value of the is-violated functions for precondition preferences is constant over the relaxed states. This representsthe implicit assumption that no further precondition preferences will be violated. The metric value of the relaxed worldsdoes not increase (and sometimes actually decreases), since the number of preference facts increases in deeper relaxedworlds. As a result, the metric of the deepest relaxed world is the one that will be returned by B. This value correspondsto evaluating the metric function in a relaxed state where: (1) is-violated functions for precondition preferences areidentical to the ones in N, (2) preference facts that do not appear in the relaxed planning graph are regarded as violated,and (3) all remaining preferences are regarded as satisfied. This condition (2) is stronger than condition (2) in the definitionof O above. Indeed, no preference that is detected as unsatisfiable by the method described for O can appear in the relaxedplanning graph, since there is no path to an accepting state of that preference. Hence, no action can ever add the acceptingpredicate for the preference.By using the relaxed planning graph, B can sometimes detect preferences that are not satisfiable by any successor of Nbut that cannot be spotted by O ’s method. For example, consider we have a preference ϕ = (sometime f), and considerfurther that fact f is not reachable from the current state. The myopic O function would regard this preference as satisfiable,because it is always possible to reach the final state of the automaton for formula ϕ (the automaton for f looks like theone in Fig. 2(a)). On the other hand, f might not appear in the graph—because f is unreachable from the current state—andtherefore B would regard ϕ as unsatisfiable.These observations lead to the conclusion that B(N) will also be a lower bound on the metric value of any successor ofN under the NDVPL condition.Proposition 5. If the metric function is NDVPL, then B(N) is guaranteed to be a lower bound on the metric value of any plan extend-ing N.Proof. Proposition 2 implies that all preference facts that could ever be achieved by some successors of N will eventuallyappear in the deepest relaxed world. Because the metric is NDVPL, this implies that the metric value of the deepest relaxed604J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618world is also the minimum, and therefore such a value will be returned by the B function. Now we can apply the sameargument as in the proof for Proposition 4, since the returned metric value corresponds to evaluating the metric in ahypothetical node in which all is-violated counters are lower or equal than those of any plan extending N. (cid:2)4.1.5. Discounted metric function (D(r))The D function is a weighting of the metric function evaluated in the relaxed worlds. Assume w 0, w 1, . . . , wn are therelaxed worlds in the relaxed planning graph, where w i is at depth i and the w 0 = (s, sc), i.e., the positive and negativefacts of the state where D(r) is being evaluated. Then the discounted metric, D(r), is:D(r) = M(w 0) +(cid:9)M(w i+1) − M(w i)ri,n−1(cid:7)(cid:8)i=0(1)where M(w i) is the metric function evaluated in the relaxed world w i and r is a discount factor (0 (cid:2) r (cid:2) 1).The D function is optimistic with respect to preferences that appear earlier in the relaxed planning graph (i.e., prefer-ences that seem easy) and pessimistic with respect to preferences that appear later (preferences that seem hard). Intuitively,the D function estimates the metric value of plans extending the current state by “believing” more in the satisfaction ofpreferences that appear to be easier. Observe that M(w i+1) − M(w i) is the amount of metric value gained when passingfrom relaxed world w i to w i+1. This amount is then multiplied by ri , which decreases as i increases. Observe also that,although the metric gains are discounted, preferences that are weighted higher in the PDDL3 metric will also have a higherimpact on the value of D. That is, D achieves the desired tradeoff between the ease of achieving a preference and the valueof achieving it.A computational advantage of the D function is that it is easy to compute. As opposed to other approaches, this heuristicnever needs to make an explicit selection of the preferences to be pursued by the planner.Finally, observe that when r is close to 1, the effect of discounting is low, and when it is close to 0, the metric is quicklydiscounted. When r is close to 0 the D function is myopic in the sense that it discounts heavily those preferences thatappear deeper in the graph.4.2. The planning algorithmOur planning algorithm searches for a plan in a series of episodes. The purpose of each of these episodes is to find aplan for the goal that has a better value than the best found so far. In each planning episode a best-first search for a planis initiated using some of the heuristics proposed above. The episode ends as soon as it finds a plan whose quality is betterthan that of the plan found in the previous episode. The search terminates when the search frontier is empty. The algorithmis shown as Algorithm 1.When search is started (i.e., no plan has been found), the algorithm uses the goal distance function (G) as its heuristic ina standard best-first search. The other heuristics are ignored in this first planning episode. This is motivated by the fact thatthe goal is a hard condition that must be satisfied. In some problems the other heuristics (that guide the planner towardsachieving a preferred plan) can conflict with achieving the goal, or might cause the search to become too difficult.1:function Search-HPlan-P(initial state init, goal formula goal, a set of hard constraints hConstraints,metric function MetricFn, heuristic function UserHeuristic)(cid:2) initialize search frontier(cid:2) pruning by bounding(cid:2) search restartedfrontier ← InitFrontier(init)closed ← ∅bestMetric ← worst case upper boundHeuristicFn ← Gwhile frontier is not emptycurrent ← Best element from frontier according to HeuristicFnif ¬Closed?(current, closed) and current satisfies hConstraints thenif MetricBoundFn(current) < bestMetric thenif current satisfies goal and its metric is < bestMetric thenOutput plan for currentif this is first plan found thenHeuristicFn ← UserHeuristicFnfrontier ← InitFrontier(init)Reinitialize closed Listend ifbestMetric ← MetricFn(current)end ifsucc ← successors of currentfrontier ← merge succ into frontierclosed ← closed ∪ {current}2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:end if23:24:end while25: end functionend ifAlgorithm 1. HPlan-P’s search algorithm.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618605After finding the first plan, the algorithm restarts the search from scratch, but this time it uses some combination of theabove heuristics to guide the planner towards a preferred plan. Let UserHeuristic() denote this combination. UserHeuris-tic() could be any combination of the above heuristic functions. Nevertheless, in this paper we consider only a small subsetof all possible combinations. In particular, we consider only prioritized sequences of heuristics, where the lower priorityheuristics are used only to break ties in the higher priority heuristics.Since achieving the goal remains mandatory, UserHeuristic() always uses G as the first priority, together with some ofthe other heuristics at a lower priority. For example, consider the prioritization sequence G D(0.3)O . When comparing twostates of the frontier, the planner first looks at the G function. The best state is the one with lower G value (i.e., lowerdistance to the goal). However, if there is a tie, then it uses D(0.3) (the best state being the one with a smaller value).Finally, if there is still a tie, it uses the O function to break it. In Section 5, we investigate the effectiveness of several suchprioritized heuristics sequences.4.2.1. Pruning the search spaceOnce we have completed the first planning episode (using G) we want to ensure that each subsequent planning episodeyields a better plan. Whenever a plan is found, it will only be returned if its metric is lower than that of the last plan found(line 10).Moreover, in each episode we can use the metric value of the previously found plan to prune the search space, andthus improve search performance. In each planning episode, the algorithm prunes from the search space any node N thatwe estimate cannot reach a better plan than the best plan found so far. This estimate is provided by the function Met-ricBoundFN(), which is given as an argument to the search algorithm. MetricBoundFN(N) must compute or estimate alower-bound on the metric of any plan extending N.Pruning is realized by the algorithm in line 1, when the condition in the if becomes false. As the value of bestMetric getsupdated (line 17), the pruning constraint imposes a tighter bound causing more partial plans to be rejected.The O and B heuristic functions defined above are well-suited to be used as MetricBoundFN(). Indeed, we tried bothof them in our experiments. On the other hand, it is also simple to “turn-off” pruning by simply passing a null function asMetricBoundFN().4.2.2. Discarding nodes in closed listUnder certain conditions, our algorithm will also prune nodes that revisit a plan state that has appeared in a previouslyexpanded node. This is done for efficiency, and allows the algorithm to avoid considering plans with cycles.The algorithm keeps a list of nodes that have already been expanded in the variable closed, just as in standard best-first search. Furthermore, when current is extracted from the search frontier, its state is checked against the set of closednodes (line 8). If there exists a node in the closed list with the same state and a better or equal heuristic value (i.e.,Closed?(current, closed) is true), then the node current will be pruned from the search space.Note that for two states to be identicalin the compiled planning instance every boolean predicate has to co-incide and, moreover, values assigned to each ground function also have to coincide.In particular, this means thatis-violatedcounters in two identical states are also identical, i.e., the preferences are equally satisfied. Nevertheless,two search nodes with identical states can still be assigned different heuristic values. Given the way we have defined User-Heuristic(), different heuristic values will be assigned to nodes with identical states only when the metric function dependson (total-time). If the (total-time) function appears positively in the metric (i.e., the metric is such that for other-wise equally preferred plans, longer ones are never preferred to shorter ones), then discarding of nodes cannot prune anynode that leads to an optimal plan. We discuss this further in the next section.Finally, note that the cycles we are eliminating are those that occur in the compiled instance, not those occurring inthe original instance. Indeed, in the original instance there might be LTL preferences that can be satisfied by visiting thesame state twice. For example consider the preference: eventually turn the light switch on and sometime after turn it off. Anyplan that contains the action turn-on immediately followed by turn-off satisfies the preference but also visits the same statetwice. In our compiled domains however such a plan will not produce a cycle, and therefore will not be pruned. This isbecause the set of current states of the preference’s automaton—represented by the automata domain predicates—changeswhen performing those actions; indeed it changes from a non-accepting state to an accepting state.4.3. Properties of the algorithmIn this section we show that under certain conditions our search algorithm is guaranteed to return optimal and k-optimalplans. We will prove this result without imposing any restriction on the UserHeuristic() function. In particular, we can stillensure optimality even if this function is inadmissible. In planning this is important, as inadmissible heuristics are typicallyrequired for adequate search performance.The first requirement in our proofs is that the pruning performed by the algorithm is sound.Definition 6 (Sound Pruning). The pruning performed by Algorithm 1 is sound iff whenever a node N is pruned (line 1) themetric value of any plan extending N exceeds the current bound bestMetric.606J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618When Algorithm 1 uses sound pruning, no state will be incorrectly pruned from the search space. That is, node N isnot pruned from the search space if some plan extending it can achieve a metric-value superior to the current bound. Toguarantee that the algorithm performs sound pruning it suffices to provide a lower-bound function as input to the algorithm.Theorem 7. If MetricBoundFN(N) is a lower bound on the metric value of any plan extending N, then Algorithm 1 performs soundpruning.Proof. If node N is not in closed and is pruned from the search space then (a) MetricBoundFN(N) (cid:3) bestMetric. If Met-ricBoundFN() is a lower bound on the metric value of any plan extending N, then (b) MetricBoundFN(N) (cid:2) M(N p) for anysolution node N p extending N. By putting (a) and (b) together we obtain that if N is not in closed and it is pruned, thenM(N p) (cid:3) bestMetric, for every solution node N p extending N, i.e., pruning is sound. (cid:2)As proven previously in Section 4.1, if the metric function is NDVPL, O and B will both be lower bound functions, andtherefore provide sound pruning. Notice also that “turning off” pruning by having MetricBoundFN() return a value that isalways less than bestMetric, also provides sound pruning.The second requirement for optimality has to do with the discarding of closed nodes performed in line 8. To preserveoptimality, the algorithm must not remove a node that can lead to a plan that is more preferred than any plan that can beachieved by extending nodes that are not discarded. Formally,Definition 8 (Discarding of Closed Nodes Preserves Optimality). The discarding of nodes by Algorithm 1 preserves optimality ifffor any node N that is discarded in line 8, there is either already an optimal node (i.e., plan) N O in the closed list or thereexists a node N in frontier that can be extended to a plan with optimal quality.The condition defined above holds when using NDVPL metrics under fairly general conditions. In particular, it holds forany NDVPL metric that is independent of (total-time). It also holds if the NDVPL metric depends on (total-time),and O or B is used as a first tie breaker after G or P in UserHeuristic(). Finally, it will hold if D is used as the first tiebreaker for NDVPL metric functions that are additive on total-time.Definition 9 (Additive on total-time (ATT)). A metric function M is additive on total time (ATT) iff it is such that M(N) =M P (N) + M T (N), where M P (N) is an expression that does not mention the function (total-time), and M T (N) is anexpression whose only plan-dependent function is (total-time).Intuitively, an ATT metric is a sum of a function that only depends on the is-violated functions, and a function thatincludes (total-time) but does not include any is-violated functions. Now we are ready to state our result formally.Theorem 10. The discarding of nodes done by Algorithm 1 preserves optimality if the algorithm performs sound pruning, the metricfunction M is NDVPL and:(1) M is independent of (total-time), or(2) M is dependent on (total-time) and O or B are used as the first tie breaker in UserHeuristic() after G or P , or(3) M is ATT and D is used as the first tie breaker in UserHeuristic() after G or P .Proof. See Appendix B. (cid:2)An important fact about sound pruning is that it never prunes optimal plans from the search space, unless anotheroptimal plan has already been found. An important consequence of this fact, is that the search algorithm will be able tofind optimal plans under fairly general conditions. Our first result says that, under sound pruning, optimality is guaranteedwhen the algorithm terminates.Theorem 11. Assume Algorithm 1 performs sound pruning, and that its node discarding preserves optimality. If it terminates, the lastplan returned, if any, is optimal.Proof. Each planning episode has returned a better plan, and the algorithm stops only when the final planning episode hasrejected all possible plans. Since the algorithm never prunes or discards a node that can be extended to an optimal unlessan optimal plan has already been found then no plan better than the last one returned exists. (cid:2)Theorem 11 still does not guarantee that an optimal solution will be found because the algorithm might never terminate.To guarantee this we must impose further conditions that restrict the explored search space to be finite. Once we have theseconditions, optimality is easy to prove since the search must eventually terminate.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618607Theorem 12. Assume the following conditions hold:(1) The initial value of bestMetric (worst case upper bound) in Algorithm 1 is finite;(2) The set of cycle-free nodes N such that MetricBoundFN(N) is less than the initial value of best Metric is finite;(3) Algorithm 1 performs sound pruning;(4) Node discarding in Algorithm 1 preserves optimality.Then Algorithm 1 is guaranteed to find an optimal plan, if one exists.Proof. Each planning episode only examines nodes with estimated metric value—given by MetricBoundFN—that is less thanbestMetric. By assumption 2, this is a finite set of nodes, so each episode must complete and the algorithm must eventuallyterminate. Now the result follows from Theorem 11. (cid:2)In Theorem 12, condition (1) is satisfied by any implementation of the algorithm that uses a sufficiently large numberfor the initial value of bestMetric. Moreover, Theorem 7 shows how condition (3) can be satisfied, and Theorem 10 showshow condition (4) can be satisfied. Condition (2), however, can sometimes be falsified by a PDDL3 instance. In particular,the metric function can be defined in such a way that its value improves as the number of violated precondition preferencesincreases. Under such a metric function the plans’ metric values might improve without bound as the plan length increases.This would mean that the number of plans with metric value less than the initial bound, bestMetric, becomes unbounded,and condition (2) will be violated. We can avoid cases like this when the metric function is bounded on precondition prefer-ences.Definition 13 (BPP metrics). Let the individual precondition preferences for a planning instance P be Γ , and let U denotethe initial value of bestMetric. A metric function is bounded on precondition preferences (BPP) if there exists a value ri for eachprecondition preference pi ∈ Γ such that in every node N with MetricBoundFN(N) < U , pi is never violated more than ritimes.BPP metrics are such that the is-violated functions are always smaller than a fixed bound in every node withmetric value lower than U . This property guarantees that there are only a finite number of plans with value less than U ,and ultimately enables us to prove another optimality result:Corollary 14. Assume that the metric function for planning instance P is BPP and assume conditions (1), (3), and (4) in Theorem 12hold. Then Algorithm 1 finds an optimal plan for P .Proof. We need only prove that the set of nodes N with MetricBoundFN(N) < bestMetric is finite. This will satisfy condi-tion (2) and allow us to apply Theorem 12. The BPP condition ensures that each precondition function pi in N can only havea value in the range 0–ri (for some fixed value ri ). Since the precondition functions are the only functions in the planninginstance (the remaining elements of the state are boolean predicates), this means that only a finite number of differentstates can have this property. (cid:2)Note that the NDVPL property, which we could use to satisfy condition (4) in Theorem 12, does not imply necessarilythe BPP property. As an example suppose a domain where precPref is a precondition preference, and goalPref1 andgoalPref2 are final-state preferences. Assume we are using the B function as MetricBoundFN and that the metric for anode N is defined as:M(N) = is-violated(goalPref1, N) ∗ is-violated(precPref, N) + is-violated(goalPref2, N).(2)M is clearly NDVPL since it cannot decrease as plans violate more preferences. However, M does not necessarily increase asmore preferences are violated, which can lead to situations in which we have an infinite set of goal nodes with the samemetric value. Indeed, assume goalPref2 is an unreachable preference that cannot be detected by the relaxed planninggraph (i.e., it is such that it won’t be detected by our B bounding function). Moreover, assume the planner has found anode that satisfies goalPref1. Assuming precPref can be violated by some action in the planning instance, there mightbe infinite plans that could be generated that violate precPref repeatedly while still satisfying goalPref1. Because theis-violated functions are represented within the state, those plans cannot be eliminated by the algorithm since theywill not produce cycles.The BPP and NDVPL properties are quite natural conditions on the metric function. Indeed, it is reasonable to assumethat violated preferences are undesirable. Hence, a plan should become (arbitrarily) worse as the number of preferences itviolates becomes (arbitrarily) larger. Such a property is sufficient to guarantee both the NDVPL and the BPP conditions. Theadditive family of metric functions satisfies both conditions, and it is defined as follows.Definition 15 (Additive metric function). A PDDL3 metric function is additive,is-violated(pi), where ci (cid:3) 0.ifit has the form M =(cid:6)ni=0 ci ×608J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618Remark 16. Additive metric functions satisfy the NDVPL condition and satisfy the BPP condition when MetricBoundFN iseither B or O .Additive metric functions were used in all of the problems in the qualitative preference track of IPC-5. Therefore, ouralgorithm—when using O or B for pruning—is guaranteed to find an optimal solution for these problems, given sufficienttime and memory. In practice, however, due to restrictions of time and memory, the algorithm finds the optimal solutiononly in the most simple problems. On the other larger problems it returned the best plan its completed planning episodesfound in the time alloted.4.3.1. k-OptimalityInstead of searching for an optimal plan among the set of all valid plans, one might be interested in restricting attentionto a subset of the valid plans. For example, there might be resource usage limitations that might further constrain the setof plans that one is willing to accept. This might be the case when a shift worker cannot be asked to work more than oneovertime shift in three days, or a plane cannot log more than a certain number of continuous kilometers. If the set of plansone is interested in can be characterized by a temporally extended property, it suffices to add such a property to the set ofhard constraints. The optimality results presented above, will allow the planner to find the optimal plan from among therestricted set of plans, regardless of the property used.For some interesting properties, however, we can find optimal plans under weaker conditions on the metric functionthan those required in the general case above. This is the case, for example, when we are interested in plans whose lengthis bounded by a certain value.Several existing preference planners are able to find plans that are optimal among the set of plans with restricted lengthor makespan. For example, PPlan [6] when given a bound k is able to find an optimal plan among those with length kor less. Similarly, both the system by Brafman and Chernyavsky [10] and Satplan-P [24] return optimal plans among thoseplans of makespan n, where n is a parameter. It should be noted, however, that such plans need not be globally optimal. Thatis, there could be plans of longer length or makespan that have higher value than the plan returned by these systems. Ouralgorithm, on the other hand, can return the globally optimal plan under conditions described above. If we are interested,however, in plans of restricted length then our algorithm can return k-optimal plans under weaker conditions.Definition 17 (k-optimal plan). A plan is k-optimal iff it is the optimal among the set of plans of length i (cid:2) k.To achieve k-optimality, we force the algorithm to search in the space of plans whose length is smaller than or equalto k, by imposing an additional hard constraint that restricts the length of the plan.Theorem 18. Assume Algorithm 1 uses sound pruning, and that the set of initial hard constraints contains the formula (total-time)(cid:2) k. Then, the returned plan (if any) is k-optimal.Proof. Since the space of plans of length up to k is finite, each planning episode will terminate with an improved plan (ifany exists). Because of sound pruning, no node can be wrongly pruned from the search space. Hence, the last returned plan(if any) is optimal. (cid:2)Note that this result does not require restrictions on the metric function such as condition 2 in Theorem 12. Thus, thisresult is satisfied by a broader family of metric functions than those that satisfy Theorem 12; for example, it is satisfiedwhen using NDVPL metrics such as the one in Eq. (2).5. Implementation and evaluationWe have implemented our ideas in the planner HPlan-P. HPlan-P consists of two modules. The first is a preprocessorthat reads PDDL3 problems and generates a planning problem with only simple preferences expressed as a TLPlan domain.The second module is a modified version of TLPlan that is able to compute the heuristic functions and implements thealgorithm of Section 4.Recall that two of the key elements in our algorithm are the iterative pruning strategy and the heuristics used forplanning. In the following subsections we evaluate the effectiveness of our planner in obtaining good quality plans usingseveral combinations of the heuristics. As a testbed, we use the problems of the qualitative preferences track of IPC-5,all of which contain TEPs. The IPC-5 domains are composed of two transportation domains: TPP and trucks, a productiondomain: openstacks, a domain which involves moving objects by using machines under several restrictions: storage, andfinally, rovers, which models a rover that must move and collect experiments (for more details, we refer the reader to theIPC-5 booklet [13]). Each domain consists of 20 problems. The problems in the trucks, openstacks, and rovers domains havehard goals and preferences. The remaining problems have only preferences. Preferences in these domains impose interestingrestrictions on plans, and usually there is no plan that can achieve them all.At the end of the section, we compare our planner against the other planners that participated in IPC-5. The results arebased on the data available from IPC-5 [22] and our own experiments.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–6186095.1. The effect of iterative pruningTo evaluate the effectiveness of iterative pruning we compared the performance of three pruning functions: the optimisticmetric (O ), the best relaxed metric (B), and no pruning at all. From our experiments, we conclude that most of the timepruning can only produce better results than no pruning, and that, overall, pruning with B usually produces better resultsthan pruning with O .To compare the different strategies, we ran all IPC-5 problems with O and no pruning, with a 30-minute timeout. Theheuristics used in these experiments were the four top-performing strategies on each domain, under pruning with B.The impact of pruning varies across different domains. In three of the domains, the impact of pruning is little. In thestorage and TPP domains, pruning has no effect, in practice. In the rovers domain, the impact is slim: O performs as goodas B does, and no pruning, on average, produces solutions with a 0.05% increase on the metric. An increased impact isobserved in the trucks domain, where the top-performing heuristics improve the metric of the first plan found by 30.60%under B pruning, while under O pruning the metric is improved by 28.02% on average, and under no pruning by 21.33% onaverage. Finally, the greatest impact can be observed on the openstacks domain. Here, B produces 13.63% improvement onaverage, while both no pruning and pruning with O produce only 1.62% improvement.In general, pruning has a noticeable impact when, during search, it can be frequently proven that certain preferenceswill not be satisfied. In the case of the openstacks domain for example, most preferences require certain products (whichare associated with orders) to be delivered. On the other hand, the goal usually requires a number of orders to be shipped.To ship an order one is required to start the order, and then ship it. However, to deliver a product associated with order o,one needs to make the product after o has been started and before the o has been shipped. Thus, whenever an order ois shipped, the B function automatically regards as unsatisfiable all preferences that involved the delivery of an unmadeproduct associated with o. This occurs frequently in the search for plans for this domain. The initial solution, which ignorespreferences, produces a plan with no make-product actions. As the search progresses, states that finish an order early areconstantly pruned away, which in turn favours adding make-product actions.A side effect of pruning is that it can sometimes prove (when the conditions of Theorem 11 are met) that an optimalsolution has been found. Indeed, the algorithm stops on most of the simplest problems across all domains (therefore, provingit has found an optimal plan). If no pruning was used the search would generally never terminate.5.2. Performance of heuristicsTo determine the effectiveness of various prioritized heuristic sequences (Section 4.1) we compared 42 heuristic se-quences using B as a pruning function, allowing the planner to run for 15 minutes over each of the 80 IPC-5 probleminstances. All the heuristics had G as the highest priority (therefore, we omit G from their names). Specifically, we experi-mented with O , B, O P , P O , B P , P B, and B D(r), D(r)B, O D(r), D(r)O for r ∈ {0, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9, 1}.In general, we say that a heuristic is better than another if it produces plans with better quality, where quality ismeasured by the metric of the plans. To evaluate how good a heuristic is, we measure the percent improvement of themetric of the last plan found with respect to the metric of the first plan found. Thus, if the first plan found has metric100, and the last has metric 20, the percent improvement is 80%. Since a first plan is always found using G, its metricvalue is always the same, regardless of the heuristic we choose. Hence this measure can be used to objectively compareperformance.Table 1 shows the best and worst performing heuristics in each of the domains tested. In many domains, several heuris-tics yield very similar performance. Moreover, we conclude that the heuristic functions that use the relaxed planning graphare key to good performance. In all problems, save TPP, the heuristics that used the relaxed planning graph had the bestperformance. The case of TPP is pathological in the qualitative preference track. However, upon looking at the actual planstraversed during the search we observed that it is not the case that O is a good heuristic for this problem, indeed O isalmost totally blind since in most states O is equal to 0. Rather, it turns out that heuristics based on the relaxed planninggraph are poor in this domain, misguiding the search. In Section 6, we explain scenarios in which our heuristics can performbadly, and give more details on why TPP is one of these cases.5.3. Comparison to other approachesWe entered HPlan-P in the IPC-5 Qualitative Preferences track [22], achieving second place behind SGPlan5 [28]. DespiteHPlan-P’s distinguished standing, SGPlan5’s performance was superior to HPlan-P’s, sometimes finding better quality plans,but generally solving more problems and solving them faster. SGPlan5’s superior performance was not unique to the pref-erences tracks. SGPlan5 dominated all 6 tracks of the IPC-5 satisficing planner competition. As such, we conjecture that theirsuperior performance can be attributed to the partitioning techniques they use, which are not specific to planning withpreferences, and that these techniques could be combined with those of HPlan-P. This is supported by the fact that HPlan-P has similar or better performance than SGPlan5 on simple planning instances, as we see in experiments shown at the endof this section.HPlan-P consistently performed better than mips-bdd [17] and mips-xxl [15]; HPlan-P can usually find plans of bet-ter quality and solve many more problems. mips-bdd and mips-xxl use related techniques, based on propositional Büchi610J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618Table 1Performance of different heuristics in the problems of the Qualitative Preferences track of IPC-5. The second column shows the number of problems whereat least one plan was found. The third, shows how many of these plans were subsequently improved upon by the planner. The average percent metricimprovement with respect to the first plan found is shown in square brackets.DomainopenstackstrucksstorageroversTPP1 Plan185161120499>1 Plan14Best heuristicsBP[13.77], DO(1)[13.63], DB(1)[13.63],BD(1)[13.63], B[13.63]Worst heuristicsD(0)B[7.56], for r ∈ {0.01, 0.05, 0.1}:DO(r)[7.63] and DB(r)[7.63]D(0)O[30.68], OD(0)[30.68]PB[5.35], OP[5.35], PO[5.35], O[12.02]BO[37], OB[37], B[37], O[37], BD(0.05)[35.62],OD(0.05)[35.55], BD(0)[35.42]PO[21.04], PB[21.04], BP[24.18],OP[24.18]D(0.1)O[17.15], D(0.1)B[17.15], D(0.3)B[16.91],D(0.3)O[16.91], O(0.01)D[16.47],O(0.05)D[16.47]20O[40.32], BO[32.02], B[32.02], OB[33.97]BP[6.97], OP[7.16], B[10.85],OB[10.85], BO[10.85], O[10.85]for r (cid:2) 0.9: BD(r)[9.03],OD(0.9)[10.98]Table 2Relative performance of HPlan-P’s best heuristics for simple preferences, compared to other IPC-5 participants. Ratio compares the performance of theparticular planner and HPlan-P’s. Ratio > 1 means HPlan-P is superior, and Ratio < 1 means otherwise. #S is the number of problems solved. “∗” meansthe planner did not compete in the domain.DomainHPlan-PSGPlan5PSYochanmips-bddmips-xxlTPPopenstacksstoragepathways#S20202020Ratio1111#S20202020Ratio0.78–0.80.89–0.920.74–0.760.77#S11∗54Ratio1.02–1.07∗3.86–3.951.02#S92410Ratio0.94–0.992.510.79#S918416Ratio1.68–1.786.45–6.8115.411.19–1.21automata, to handle LTL preferences. We think that part of our superior performance can be explained because our compi-lation does not ground LTL formulae, avoiding blowups, and also because the heuristics are easy to compute. For example,mips-xxl and mips-bdd were only able to solve the first two problems (the smallest) of the openstacks domain, whereasHPlan-P could quickly find plans for almost all of them. In this domain the number of preferences was typically high (thethird instance already contains around 120 preferences). On the other hand, something similar occurs in the storage do-mains. In this domain, though, there are many fewer preferences, but these are quantified. More details can be found onthe results of IPC-5 [22].While we did not enter the Simple Preferences track, experiments performed after the competition indicate that HPlan-Pwould have done well in this track. To perform a comparison, we ran our planner for 15 min2 on the first 20 instances3 ofeach domain. In Table 2, we show the performance of HPlan-P’s best heuristics compared to all other participants, in thosedomains on which all four planners solved at least one problem. HPlan-P was able to solve 20 problems in all domains,except trucks, where it could only solve the 5 simpler instances (see Table 3 for details on the trucks domain). In the table,#S is the number of problems solved by each approach, and Ratio is the average ratio between the metric value obtainedby the particular planner and the metric obtained by our planner. Thus, values over 1 indicate that our planner is findingbetter plans, whereas values under 1 indicate the opposite. The results for HPlan-P were obtained on an Intel(R) Xeon(TM)CPU 2.66 GHz machine running Linux, with a timeout of 15 min. Results for other planners were extracted from the IPC-5official results, which were generated on a Linux Intel(R) Xeon(TM) CPU 3.00 GHz machine, with a 30 min timeout. Memorywas limited to 1 GB for all processes.We conclude that SGPlan5 typically outperforms HPlan-P. SGPlan5, on average, obtains plans that are no more than 25%better in terms of metric value than those obtained by HPlan-P. Moreover, in the most simple instances usually HPlan-PPSdoes equally well or better than SGPlan5 (see Table 3). HPlan-P can solve more instances than those solved by Yochan,mips-xxl and mips-bdd. Furthermore, it outperforms Yochanand mips-xxl in terms of achieved plan quality. HPlan-P’sperformance is comparable to that of mips-bdd in those problems that can be solved by both planners. Finally, we againobserved that the best-performing heuristics in domains other than TPP are those that use the relaxed planning graph, and,in particular, the D heuristic.PSWe ran a final comparison between SGPlan5 and HPlan-P on the openstacks-nce domain [25]. openstacks-nce is a re-formulation of the original openstacks simple-preferences domain that does not include actions with conditional effects.These two domains are essentially equivalent in the sense that plans in one domain have a corresponding plan with equalquality in the other. The results are shown in Table 4. We observe that HPlan-P consistently outperforms SGPlan5 across2 In IPC-5, planners where given 30 min on a similar machine.3 Only the pathways domain has more than 20 problems.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618611Table 3Plan quality (metric) of three of HPlan-P’s heuristics compared to the IPC-5 Simple Preferences participants on the simpler, non-metric problems. “ns” meansthat the instance what not solved by the planner. “∗” means the planner did not compete in the domain.InstancePSYochanmips-bddmips-xxlSGPlan5HPlan-PTPP-01TPP-02TPP-03TPP-04TPP-05TPP-06TPP-07openstacks-01openstacks-02openstacks-03openstacks-04openstacks-05openstacks-06openstacks-07trucks-01trucks-02trucks-03trucks-04trucks-05storage-01storage-02storage-03storage-04storage-05storage-06storage-07pathways-01pathways-02pathways-03pathways-04pathways-05pathways-06pathways-0722362445103133124∗∗∗∗∗∗∗030016114951165nsns2333nsnsns16242935891101261212nsnsnsnsns0000ns3569nsnsns233278111624293522327532263638898133133285000nsns1837158197nsnsns354.7310.212.912.5162429357910110013161226363367100005814178712416023326.5108O162429391031201246436472521870003035699716127423328.512.912.5OD(r = 0.5)OD(r = 0)OD(r = 1)1624293579118135643044211874000103569130195281243.72912.912.51624293587114135643645252187000303569130195307243.7210.212.912.51624294210512013564304921187400040356997161274243.7210.212.912.5Table 4Metric values obtained by four of HPlan-P’s heuristics and SGPlan5 on the openstacks and openstacks-nce [25] domains.InstanceSGPlan5HPlan-Popenstacks-nceSGPlan5HPlan-P01020304050607080910111213141516171819207070901001401403006206201201201532236521021045093015811348O11738484835981401543036801904712513322458815811348OD(.5)111142494841981521552526811722212313325555815811348openstacks-nceOD(0)11737464834981481543036801814712513326992915811348OD(1)111141494841981481542022731742412613325455715811348131612263633671231212021234860000254424O6436472521878610919195217132747420955715811348openstacksOD(.5)6430442118747812311224516723676317946415811348openstacksOD(0)6436452521878610910234516721676717946415811348OD(1)6430492118747812313125116721676318049315811348612J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618all instances of this domain, obtaining plans that are usually at least 50% better in quality. We also observe that the perfor-mance of HPlan-P is consistent across the two formulations, which is not the case with SGPlan5.6. DiscussionIn previous sections, we proposed a collection of heuristics that can be used in planning with TEPs and simple prefer-ences in conjunction with our incremental search algorithm. In our experimental evaluation we saw that in most domainsthe heuristics that utilize the relaxed planning graph are those that provide the best performance. Given the limited num-ber of domains in which we have had the opportunity to test the planner, it is hard—and might be even be impossible—toconclude which is the best combination of heuristics to use. It is even hard to give a justified recipe for their use. However,some situations in which our heuristics perform poorly can be identified and analyzed. Below we describe two reasons forpotential poor performance.The first reason for potentially poor performance is due to our choice of using prioritized sequences of heuristics. Wehave chosen the goal distance G to appear as the first priority to guide the planner towards satisfying the must-achievegoals for a pragmatic reason: the goal is the most important thing to achieve. However, this design decision sometimesmakes the search algorithm focus excessively on goal achievement to the detriment of preference satisfaction. This issuebecomes particularly relevant when there are interactions between the goal and the preferences. Consider, for example, asituation in which a preference p can only be achieved after achieving the goal. Furthermore, assume the goal g is theconjunction f 1 ∧ f 2, and assume that prior to achieving p one has to make f 2 false. In cases like this, after the algorithmfinds a plan for the goal, it can hardly find a plan that also satisfies p. When extending any plan for g, the planner willf 2 over an action that invalidates f 2, if such an action isalways choose an action that does not invalidate the subgoalavailable. This is because the goal distance (G) of any search node in which f 2 is false is strictly greater than the goaldistance in which both f 1 and f 2 are true. As a consequence, the algorithm will have trouble achieving p, and actually willonly achieve p when extending a plan for g when no actions that invalidate f 2 are available. Unfortunately the only way ofgetting into such a situation implies exhausting the search space of plans that extend a plan for g without invalidating g.The second source for poor performance is the loss of structure in which we incur by computing our heuristic in aplanning instance in which the action’s deletes (i.e., negative effects) are ignored. The inaccurate reachability informationprovided by this relaxation might significantly affect the performance of all our heuristics based on the relaxed planninggraph (i.e., P , B, and D). Consider for example an instance in which there are no hard goals and there are two preferences,p1 and p2. Assume further that p2 is a preference that is rather easy to achieve from any state but that has to be violated inorder to achieve p1. Assume that we are in a state in which p2 is satisfied but p1 is not, and in which we need to performat least three actions to achieve both p1 and p2. Let those actions be a, b, and c, such that a makes p2 false and p1 true, andfinally action b followed by c reestablish p1, as shown in Fig. 3. Moreover, assume that action e is applicable in s, and thatit leads to s2—a state from which p1 and p2 can be reached by the same sequence of three actions. Because the D heuristicis computed on the delete relaxation, D will always prefer to expand s2 instead of s1. A relaxed solution on s2 may achieveboth preferences at depth 1, since the preference p2 is already satisfied at depth 0. On the other hand, a relaxed solutionon s1 may achieve both preferences at depth 2, since in s1 two actions are needed to reestablish p2. Once the algorithmexpands s2, there could be another action applicable in s2, analogous to e, that would steer the search away from s3.It is precisely a situation similar to that described above that makes the heuristics based on the relaxed planning graph(especially D and P ), perform poorly in the TPP domain. TPP is a transportation problem in which trucks can move betweenmarkets and depots transporting goods. A good can be put into the truck by performing a load followed by a store. Storedgoods can be unloaded from the truck performing an unload. Once in a market, one has to buy an object before it becomesready to load. In problems of the TPP domain there is a preference that states that any good must be eventually loaded onsome truck (p1). On the other hand, there is a preference that states that all trucks should be unloaded at the end of theplan (p2). Once we have considered moving a truck to a market and bought a certain good, say good1, our plan prefix hasachieved p2 but not p1. A reasonable course of action to achieve both preferences would be to load good1 on the truck,followed by a store, and followed by an unload. However, the state that results from performing a load is never preferredby the planner, since just like in Fig. 3, a load invalidates p2 while making p1 true. Instead, an action that preserves theFig. 3. A situation in which our D heuristics prefers a node that does not lead to the quick satisfaction of both p1 and p2.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618613p2 property (e.g., a buy of another good) is always preferred. This leads the planner to consider all possible combinationsof sequences that buy a good before considering a load. Even worse, after performing all possible buys, for a similar reasonthe search prefers to use other truck to move to another market to keep on buying products.7. Related workThere is a significant amount of work on planning with preferences that is related, in varying degrees, to the methodwe have presented here. We organize this work into two groups: first, planners that are able to plan with preferences innon-PDDL3 preference languages or using soft goals; second, work that focuses on the PDDL3 language. In the rest of thesection we review the literature in these two categories.7.1. Other preference languagesPPlan [6] is a planning system that exploits progression to plan directly with TEPs using heuristic search. In contrast toHPlan-P, which is incremental, PPlan always returns an optimal plan whose length is bounded by a plan-length parameter(i.e., it is k-optimal). Unfortunately, PPlan uses an admissible heuristic that is far less informative than the heuristics pro-posed here. As such, it is far less efficient. The heuristic in PPlan is similar to our O heuristic, and thus does not provide anestimate of the cost to achieving unsatisfied preferences. PPlan was developed prior to the definition of PDDL3 and exploitsits own qualitative preference language, LPP, to define preferences. LPP supports rich TEPs, including nested LTL formulae(unlike PDDL3) and rather than specifying a metric objective function, the LPP objective is expressed as a logical formula.PPlan’s LPP language is an extension and improvement over the PP language proposed by Son and Pontelli [35].The HPlan-QP planner [3] was proposed as an answer to some of the shortcomings of PPlan. It is an extension to theHPlan-P system, allowing planning for qualitative TEPs guided by heuristics similar to those that have been proposed in thispaper. The preference language is based on LPP, the language used by PPlan. HPlan-QP guides the search actively towardssatisfaction of preferences (unlike PPlan), and like HPlan-P, guarantees optimality of the last plan found given sufficientresources.Also related is the work on partial satisfaction planning problems (PSPs) (over-subscription planning) [34,36]. PSPs canbe understood as a planning problem with no hard goals but rather a collection of soft goals each with an associatedutility; actions also have costs associated with them. Some existing planners for PSPs [14,33] are also incremental and usepruning techniques. However in general, they do not offer any optimality guarantees. Recently, Benton et al. [5] developedan incremental planner, bbop-lp, that uses branch-and-bound pruning for PSP planning, similar to our approach. bbop-lpis able to offer optimality guarantees given sufficient resources. However, in contrast to HPlan-P, it uses very differenttechniques for obtaining the heuristics. To compute heuristics it first relaxes the original planning problem and creates aninteger programming (IP) model of this new problem. It then computes heuristics from a linear-programming relaxation ofthe IP model. Lastly, Feldmann et al. [18] propose a planner for PSPs that iteratively invokes Metric-FF to find better plans.Bonet and Geffner [9] have proposed a framework for planning with action costs and costs/rewards associated withfluents. Their cost model can represent PSPs as well as the simple preferences subset of PDDL3. They propose admissibleheuristics and an optimal algorithm for planning under this model. Heuristics are obtained by compiling a relaxed instanceof the problem to d-DNNF, while the algorithm is a modification of A. The approach does not scale very well for largeplanning instances, in part because of its need to employ an admissible heuristic.∗Finally, there has been work that casts the preference-based planning problem as an answer set programming problem(ASP), as a constraint satisfaction problem (CSP), and as a satisfiability (SAT) instance. The paper by Son and Pontelli [35]proposed one of the first languages for preference-based planning, PP, and cast the planning problem as an optimization ofan ASP problem. Their PP language includes TEPs expressed in LTL. Brafman and Chernyavsky [10] proposed a CSP approachto planning with final-state qualitative preferences specified using TCP-nets. Additionally, Giunchiglia and Maratea [24] pro-posed a compilation of preference-based planning problems into SAT. None of these approaches exploits heuristic searchand thus are fundamentally different form the approach proposed here. The latter two approaches guide the search for asolution by imposing a variable/value ordering that will attempt to produce preferred solutions first. Because these worksare recasting the problem into a different formalism, they explore a very different search space than our approach. Notealso that the conversion to ASP, CSP or SAT requires assuming a fixed bound on plan length limiting the approach to at bestfinding k-optimal plans.7.2. IPC-5 competitorsMost related to our work are the approaches taken by the planners that competed in IPC-5, both because they usedthe PDDL3 language and because many used some form of heuristic search. Yochan[4] is a heuristic planner for simplepreferences based on the Sapaps system [36]. Our approach is similar to theirs in the sense that both use a relaxed planninggraph to obtain a heuristic estimate. Yochanis also an incremental planner, employing heuristics geared towards classicalgoals. However, to compute its heuristic, it explicitly selects a subset of preferences to achieve then treats this subset as aclassical goal. This process can be very costly in the presence of many preferences.PSPS614J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618mips-xxl [17] and mips-bdd [15] both use Büchi automata to plan with temporally extended preferences. While theapproach to compiling away the TEPs also constructs an automata (as in our approach), their translation process generatesgrounded preference formulae. This makes the translation algorithm prone to unmanageable blow-up. Further, the searchtechniques used in both of these planners are quite different from those we exploit. mips-xxl iteratively invokes a modifiedMetric-FF [26] forcing plans to have decreasing metric values. mips-bdd, on the other hand, performs a cost-optimal breath-first search that does not employ a heuristic.Finally, the winner of the preferences tracks at IPC-5, SGPlan5 [28], uses a completely different approach. It partitionsthe planning problem into several subproblems. It then uses a modified version of ff to solve those subproblems andfinally integrates these sub-solutions into a solution for the entire problem. During the integration process it attempts tominimize the metric function. SGPlan5 is not incremental, and seems to suffer from some non-robustness in its performanceas shown by the results given in Table 4 (where its performance on an reformulated but equivalent domain changes quitedramatically).8. Conclusions and future researchIn this paper we have presented a new technique for planning with preferences that can deal with simple preferences,temporally extended preferences, and hard constraints. The core of the technique, our new set of heuristics and incrementalsearch algorithm, are both amenable to integration with a variety of classical and simple-preference planners. The compi-lation technique for converting TEPs to simple preferences can also be made to work with other planners, although themethod of embedding the constructed automata we utilize here might need some modification, dependent on the facilitiesavailable in that planner. Our method of embedding the constructed automata utilized TLPlan’s ability to deal with numericfunctions and quantification. In particular, TLPlan’s ability to handle quantification allowed us to utilize the parameterizedrepresentation of the preferences generated by the compilation, leading to a considerably more compact domain encoding.We have presented a number of different heuristics for planning with preferences. These heuristics have the featurethat some of them account for the value that could be achieved from unsatisfied preferences, while others account for thedifficulty of actually achieving these preferences. Our method for combining these different types of guidance is quite simple(tie-breaking), and more sophisticated combinations of these or related heuristics could be investigated. More generally, thequestion of identifying the domain features for which particular heuristics are most suitable is an interesting direction forfuture work.We have also presented an incremental best-first search planning algorithm. A key feature of this algorithm is that it canuse heuristic bounding functions to prune the search space during its incremental planning episodes. We have proved thatunder some fairly natural conditions our algorithm can generate optimal plans. It is worth noting that these conditions donot require the algorithm to utilize admissible heuristics. Nor do they require imposing a priori restrictions on the plan size(length or makespan) which would allow the algorithm to only achieve k-optimality rather than global optimality.The algorithm can also employ different heuristics in each incremental planning episode, something we exploit duringthe very first planning episode by ignoring the preferences and only asking the planner to search for a plan achieving thegoals. The motivation for this is that we want at least one working plan in hand before trying to find a more preferredplan. In our experiments, however, the remaining planning episodes are all executed with one fixed heuristic. More flexibleschedules of heuristics could be investigated in future work.Finally we have implemented our method by extending the TLPlan planning system and have performed extensiveexperiments on the IPC-5 problems to evaluate the effectiveness of our heuristic functions and search algorithm. While noheuristic dominated all test cases, several clearly provided superior guidance towards good solutions. In particular, thosethat use the relaxed planning graph in some way proved to be the most effective in almost all domains. Experiments alsoconfirmed the essential role of pruning when solving large problems. HPlan-P scales better than many other approachesto planning with preferences, and we attribute much of this superior performance to the fact that we do not ground ourplanning problems.Although the proposed heuristics perform reasonably well in many of the benchmarks we have tested, we have identifiedcases in which they perform poorly. In some cases, computing heuristics over the delete relaxation can provide bad guidancein the presence of preferences. The resolution of some of the issues we have raised above open interesting avenues for futureresearch.AcknowledgementsWe gratefully acknowledge funding from Natural Sciences and Engineering Research Council of Canada (NSERC) andfrom the Ontario Ministry of Research and Innovation Early Researcher Award. We also thank Christian Fritz for helpfuldiscussions during the development of our planner, and the anonymous reviewers for their useful feedback.Appendix A. Proof for Proposition 2In this section we prove Proposition 2. First, we prove three intermediate results that will be used by the final proof.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618615The first intermediate result says that if an NNF formula φ over P is true in a state s (denoted as s |(cid:13) φ), then φ will also−) if every proposition that is true in s is also true in such a relaxed state. This is proven+, Fbe true in a relaxed state (Fin the following lemma.Lemma 19. Let P be a set of propositions, φ be an NNF formula, and s, Fthat:+, F− ⊆ P be states. Then if s |(cid:13) φ, and (F+, F−) is such(1) (F(2) (F+, F+, F−) |(cid:13) p, for every p ∈ s, and−) |(cid:13) ¬p, for every p ∈ sc ,then (F+, F−) |(cid:13) φ.Proof. The proof that follows is by induction on the structure of φ.Base cases (φ = p or φ = ¬p) They both follow directly from the conditions of this lemma.Induction We have the following cases:• If φ = ψ ∧ ξ , then s |(cid:13) ψ and s |(cid:13) ξ . By inductive hypothesis, also (F+, F−) |(cid:13) ψ and (F+, F−) |(cid:13) ξ . It follows fromDefinition 1 that (F+, F−) |(cid:13) φ.• If φ = ψ ∨ ξ , then the proof is analogous to the previous case.• If φ = ∀x.ψ , then for every o ∈ Objs we have that s |(cid:13) ψ(x/o). By inductive hypothesis, for every o ∈ Objs then(F+, F−) |(cid:13) ψ(x/o), hence by Definition 1, we have that (F• If φ = ∃x.ψ , the proof is analogous to the previous case. (cid:2)+, F−) |(cid:13) φ.The final intermediate result is actually a version of Proposition 2 but for simple facts.−Lemma 20. Let s be a planning state, R = (Fm ) be the relaxed planning graph constructedfrom s up to a fixed point. Moreover, let sn be the state that results after performing a legal sequence of actions a1 · · · an in s, then thereexists some k (cid:2) m such that (F−k ) |(cid:13) f , for every f ∈ s, and such that (F−k ) |(cid:13) ¬ f for every f ∈ sc .+k , F+k , F+m , F−1 ) · · · (F−m−1)(F+m−1, F−0 )(F+1 , F+0 , F−m , and m > 0. Moreover, assume that theProof. Since R has been constructed to a fixed point, Fset of states generated by performing the action sequence over s is s1 · · · sn (i.e., state siis generated after performingthe sequence of actions a1 · · · ai over s). The following proof for the lemma is by induction on the length of the actionsequence, n.+m and F= F= FBase case (n = 0) We prove that in this case we can consider k = 0. In this case the sequence of actions performed on s= s and F= sc . Let f be an arbitrary fact.is empty. By definition of the construction of R, F= F= F+m−1−m−1+k+0−0−k(1) f ∈ s. Then, by Definition 1, (F(2) f ∈ sc . Then, again by Definition 1, we obtain (F+k ) |(cid:13) f , for k = 0 concluding the proof for this case.−k ) |(cid:13) ¬ f , for k = 0.+k , F+k , FInduction Let us assume that the theorem is true for n − 1. We now prove that it is also true for n. We divide this proof intofour cases. Again, assume fis an arbitrary fact.(1) f ∈ sn and f ∈ sn−1. This case is trivial, since by inductive hypothesis we have that (F+(2) f /∈ sn and f /∈ sn−1. Again, by induction hypothesis (Fk , F(3) f ∈ sn and f /∈ sn−1. Then, an must have added fact−k ) |(cid:13) ¬ f for some k (cid:2) m.f when performed in sn−1. We now prove that action an is(cid:11) (cid:2) m − 1 of the relaxed planning graph, and that it will add fact f to the graph at levelexecutable at some level k(cid:11) + 1 (cid:2) m.kLet us assume that the precondition of action an is ϕP and that the condition of the conditional effect that adds f is ϕc .Then since both formulae are satisfied in sn−1, we have that+k , F−k ) |(cid:13) f for some k (cid:2) m.sn−1 |(cid:13) ϕP ∧ ϕc.Moreover, by inductive hypothesis, we have that there exists a k(cid:11) (cid:2) m such that(F(F+k(cid:11) , F+k(cid:11) , F−k(cid:11) ) |(cid:13) p,−k(cid:11) ) |(cid:13) ¬p,for every p ∈ sn−1,for every p ∈ scn−1.(A.1)(A.2)(A.3)At this point, we can safely assume also that k(cid:11) = m − 1, because the graph has been constructed to a fixed point.k(cid:11) < m, because if k(cid:11)were equal to m, then (A.2) and (A.3) also hold for616J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618Now, we combine Eqs. (A.2), (A.3), and (A.1) with Lemma 19 to conclude that (Fexecutable at level kf−k(cid:11) ) |(cid:13) ϕP ∧ ϕc. Action an is thereforeof the relaxed planning graph, and the condition ϕc , which enables the conditional effect that adds(cid:11) + 1 (cid:2) m, concluding the proof for this case.is added to the graph at level k = kis also true at level k. Therefore, f+k(cid:11),, F(cid:11)(cid:11)(4) f /∈ sn and f ∈ sn−1. Proof is analogous to previous case. (cid:2)Now we are ready to prove our result.Proof for Proposition 2. By Lemma 20, we know that there exists a k (cid:2) m such that for each p ∈ sn, (F−k ) |(cid:13) ¬p. Because sn |(cid:13) φ it follows from Lemma 19 that (Feach p ∈ sc then (F−k ) |(cid:13) φ. (cid:2)+k , F+k , F+k , F−k ) |(cid:13) p, and forAppendix B. Proof for Theorem 10Before we start our proof we prove a lemma which establishes that, under the conditions of Theorem 10, if two nodeswith exactly the same state have different B, D, or O metric value, then their lengths must also differ analogously.Lemma 21. Let N1 and N2 be two search nodes that correspond to the same planning state s. Furthermore, let the metric M of theinstance be NDVPL and depend on (total-time). If R(N1) (cid:2) R(N2), and:(1) R is either O or B, or(2) M is ATT and R is D.then length(N1) (cid:2) length(N2).Proof. We divide the proof in two cases.Case 1: R is either O or B. Then R(N1) = M(Nwhich possibly more preferences are satisfied. Analogously, R(N2) = M(NTherefore,(cid:11)1), where N(cid:11)1 is a hypothetical node with the same length as N1 but in(cid:11)2 with the same length as N2.(cid:11)2) for a node NM(N(cid:11)1) (cid:2) M(N(cid:11)2).(B.1)(cid:11)Because the planning state associated to N1 and N2 are identical, we know that N1 are such that they satisfyexactly the same preferences, i.e., if Γ is the set of preferences of the planning instance, for all p ∈ Γ we have that(cid:11)is-violated(p, N2). Now, using the contra-positive of implication (2) in the NDVPL definition(cid:11)2). This implies that length(N1) (cid:2) length(N2), and(Definition 3) and Eq. (B.1), we have that length(Nconcludes the proof for this case.(cid:11)1) = is-violated(p, N(cid:11)1) (cid:2) length(N(cid:11)2 and NCase 2: R is D and M is ATT. Because M is ATT, then by Eq. (1), D(N1) = M(N1)+ R1, where R1 is an expression that doesnot depend on (total-time), i.e. it only depends on N1’s state. Likewise, D(N2) = M(N2) + R2, where R2 only dependson the state of N2. Since both the states corresponding to N1 and N2 are equal, we have that R1 = R2. Hence, becauseD(N1) (cid:2) D(N2) we have that M(N1) (cid:2) M(N2), which by the contra-positive of implication (2) in the NDVPL definition(Definition 3) implies that length(N1) (cid:2) length(N2). This concludes this case, finishing the proof. (cid:2)Now we are ready to prove our result. First, note that the search is restarted from scratch after the first plan isfound. This also means that the closed list is reinitialized. Second, note that if two nodes N1 and N2 have the samestate associated to them then both the G and the P functions evaluated on these nodes return the same value. There-fore, if UserHeuristic(N1) (cid:2) UserHeuristic(N2), then this means that the tie breaker functions used, say R, is such thatR(N1) (cid:2) R(N2) where R is either O , B or D.The sketch of the proof is as follows. We assume that a node N that leads to an optimal plan is discarded by thealgorithm. Then we prove that if this happens then either the optimal was found or there is a node in the frontier that canbe extended to another optimal plan.Assume there exists an optimal plan p1 = a1a2 · · · an that traverses the sequence of states s0s1 · · · sn. Let N1 be a nodeformed by applying p1 on s0. Because the metric is NDVPL, we assume that this plan contains no cycles (otherwise, hadthe plan contained any cycles, by removing them we could not make it worse). Suppose further that at some point in thesearch, there is a node N that is generated by applying a1a2 · · · a j in the initial state (with j < n) and that is discarded bythe algorithm in line 8. This means that there exists another closed node, say NC that is associated the same state as N,and that is such thatUserHeuristic(NC ) (cid:2) UserHeuristic(N).(B.2)Both nodes are associated the same state s j , hence the is-violated counters are identical for each preference. Thismeans that NC is constructed from s0 by a sequence of actions b1b2 · · · bk. This sequence of actions gets to the samestate s j , hence the sequence p2 = b1b2 · · · bka j+1 · · · an is also a plan.J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618617Let N2 be a node that would be constructed by applying p2 in s0. Now we prove that N2 also corresponds to an optimalplan. We have two cases.Case 1: The metric depends on (total-time). Because inequality (B.2) implies that R(NC ) (cid:2) R(N), where R is ei-ther O , D or B, by Lemma 21, we have that length(NC ) (cid:2) length(N), and therefore k (cid:2) j. We clearly have thatlength(N2) (cid:2) length(N1), furthermore because all precondition counters are identical, it follows from the NDVPL condi-tion that M(N2) (cid:2) M(N1). Given that N1 represents an optimal plan, we conclude that M(N2) = M(N1), and therefore N2also represents an optimal plan.Case 2: The metric does not depend on (total-time). Therefore, because node N2 reaches the same state as N1 doesand M only depends on properties encoded in the state, M(N1) = M(N2) and hence N2 also represents an optimal plan.This concludes case 2.Now, we know that since NC , a predecessor of N2 was expanded by the algorithm, one of the following things happen:(1) A successor of NC is in frontier. In this case, the condition of Definition 8 follows immediately.(2) N2 is in the closed list. This implies that the condition of Definition 8 is also satisfied.(3) A successor of NC has been discarded by the algorithm. In this case, such a successor also leads to an optimal plan. Thismeans that we could apply the same argument in this proof for such a node, leading to eventually satisfy the conditionof Definition 8 since the algorithm has visited finitely many nodes.References[1] F. Bacchus, F. Kabanza, Planning for temporally extended goals, Annals of Mathematics and Artificial Intelligence 22 (1–2) (1998) 5–27.[2] J.A. Baier, S.A. McIlraith, Planning with first-order temporally extended goals using heuristic search, in: Proceedings of the 21st National Conference onArtificial Intelligence (AAAI), Boston, MA, 2006, pp. 788–795.[3] J.A. Baier, S.A. McIlraith, On domain-independent heuristics for planning with qualitative preferences, in: 7th Workshop on Nonmonotonic Reasoning,Action and Change (NRAC), 2007.[4] J. Benton, S. Kambhampati, M.B. Do, YochanPS: PDDL3 simple preferences and partial satisfaction planning, in: 5th International Planning CompetitionBooklet (IPC-2006), Lake District, England, July 2006, pp. 54–57.[5] J. Benton, M. van den Briel, S. Kambhampati, A hybrid linear programming and relaxed plan heuristic for partial satisfaction problems, in: Proceedingsof the 17th International Conference on Automated Planning and Scheduling (ICAPS), Providence, RI, September 2007, pp. 34–41.[6] M. Bienvenu, C. Fritz, S. McIlraith, Planning with qualitative temporal preferences, in: Proceedings of the 10th International Conference on KnowledgeRepresentation and Reasoning (KR), Lake District, England, 2006, pp. 134–144.[7] A. Blum, M.L. Furst, Fast planning through planning graph analysis, Artificial Intelligence 90 (1–2) (1997) 281–300.[8] B. Bonet, H. Geffner, Planning as heuristic search, Artificial Intelligence 129 (1–2) (2001) 5–33.[9] B. Bonet, H. Geffner, Heuristics for planning with penalties and rewards using compiled knowledge, in: Proceedings of the 10th International Conferenceon Knowledge Representation and Reasoning (KR), 2006, pp. 452–462.[10] R. Brafman, Y. Chernyavsky, Planning with goal preferences and constraints, in: Proceedings of the 15th International Conference on Automated Planningand Scheduling (ICAPS), Monterey, CA, June 2005, pp. 182–191.[11] A.I. Coles, A.J. Smith, Marvin: A heuristic search planner with online macro-action learning, Journal of Artificial Intelligence Research 28 (February2007) 119–156.[12] J.P. Delgrande, T. Schaub, H. Tompits, Domain-specific preferences for causal reasoning and planning, in: Proceedings of the 14th International Confer-ence on Automated Planning and Scheduling (ICAPS), Whistler, Canada, June 2004, pp. 63–72.[13] Y. Dimopoulos, A. Gerevini, P. Haslum, A. Saetti, The benchmark domains of the deterministic part of ipc-5, http://zeus.ing.unibs.it/ipc-5/, July 2006.[14] M.B. Do, J. Benton, M. van den Briel, S. Kambhampati, Planning with goal utility dependencies, in: Proceedings of the 20th International Joint Confer-ence on Artificial Intelligence (IJCAI), Hyderabad, India, 2007, pp. 1872–1878.[15] S. Edelkamp, Optimal symbolic PDDL3 planning with MIPS-BDD, in: 5th International Planning Competition Booklet (IPC-2006), Lake District, England,July 2006, pp. 31–33.[16] S. Edelkamp, J. Hoffmann, PDDL2.2: The language for the classical part of the 4th International Planning Competition, Tech. Rep. 195, Computer ScienceDepartment, University of Freiburg, 2004.[17] S. Edelkamp, S. Jabbar, M. Naizih, Large-scale optimal PDDL3 planning with MIPS-XXL, in: 5th International Planning Competition Booklet (IPC-2006),Lake District, England, July 2006, pp. 28–30.[18] R. Feldmann, G. Brewka, S. Wenzel, Planning with prioritized goals, in: Proceedings of the 10th International Conference on Knowledge Representationand Reasoning (KR), Lake District, England, July 2006, pp. 503–514.[19] R. Fikes, N.J. Nilsson, STRIPS: A new approach to the application of theorem proving to problem solving, Artificial Intelligence 2 (3/4) (1971) 189–208.[20] M. Fox, D. Long, PDDL2.1: An extension to PDDL for expressing temporal planning domains, Journal of Artificial Intelligence Research 20 (2003) 61–124.[21] B.C. Gazen, C.A. Knoblock, Combining the expressivity of UCPOP with the efficiency of graphplan, in: ECP97. Toulouse, France, September 1997, pp.221–233.[22] A. Gerevini, Y. Dimopoulos, P. Haslum, A. Saetti, 5th International Planning Competition, http://zeus.ing.unibs.it/ipc-5/, July 2006.[23] A. Gerevini, D. Long, Plan constraints and preferences for PDDL3, Tech. Rep. 2005-08-07, Department of Electronics for Automation, University ofBrescia, Brescia, Italy, 2005.[24] E. Giunchiglia, M. Maratea, Planning as satisfiability with preferences, in: Proceedings of the 22nd AAAI Conference on Artificial Intelligence (AAAI),Vancouver, British Columbia, 2007, pp. 987–992.[25] P. Haslum, Openstacks SP-NCE domain, http://users.rsise.anu.edu.au/~patrik/ipc5.html, 2007.[26] J. Hoffmann, The Metric-FF planning system: Translating ignoring delete lists to numeric state variables, Journal of Artificial Intelligence Research 20(2003) 291–341.[27] J. Hoffmann, B. Nebel, The FF planning system: Fast plan generation through heuristic search, Journal of Artificial Intelligence Research 14 (2001)253–302.[28] C.-W. Hsu, B. Wah, R. Huang, Y. Chen, Constraint partitioning for solving planning problems with trajectory constraints and goal preferences, in:Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), Hyderabad, India, January 2007, pp. 1924–1929.[29] D.V. McDermott, A heuristic estimator for means-ends analysis in planning, in: AIPS96, 1996, pp. 142–149.[30] D.V. McDermott, PDDL—The Planning Domain Definition Language, Tech. Rep. TR-98-003/DCS TR-1165, Yale Center for Computational Vision and Con-trol, 1998.618J.A. Baier et al. / Artificial Intelligence 173 (2009) 593–618[31] E.P.D. Pednault, ADL: Exploring the middle ground between STRIPS and the situation calculus, in: Proceedings of the 1st International Conference ofKnowledge Representation and Reasoning (KR), Toronto, Canada, May 1989, pp. 324–332.[32] A. Pnueli, The temporal logic of programs, in: Proceedings of the 18th IEEE Symposium on Foundations of Computer Science (FOCS), 1977, pp. 46–57.[33] R. Sanchez, S. Kambhampati, Planning graph heuristics for selecting objectives in over-subscription planning problems, in: Proceedings of the 15thInternational Conference on Automated Planning and Scheduling (ICAPS), Monterey, CA, 2005, pp. 192–201.[34] D.E. Smith, Choosing objectives in over-subscription planning, in: Proceedings of the 14th International Conference on Automated Planning andScheduling (ICAPS), Whistler, Canada, 2004, pp. 393–401.[35] T.C. Son, E. Pontelli, Planning with preferences using logic programming, in: V. Lifschitz, I. Niemela (Eds.), Proceedings of the 7th International Confer-ence on Logic Programming and Nonmonotonic Reasoning (LPNMR), in: LNCS, vol. 2923, Springer, 2004, pp. 247–260.[36] M. van den Briel, R.S. Nigenda, M.B. Do, S. Kambhampati, Effective approaches for partial satisfaction (over-subscription) planning, in: Proceedings ofthe 19th National Conference on Artificial Intelligence (AAAI), 2004, pp. 562–569.[37] L. Zhu, R. Givan, Simultaneous heuristic search for conjunctive subgoals, in: Proceedings of the 20th National Conference on Artificial Intelligence(AAAI), Pittsburgh, Pennsylvania, USA, July 9–13 2005, pp. 1235–1241.