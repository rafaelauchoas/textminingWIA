Artificial Intelligence 174 (2010) 410–441Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintDemocratic instance selection: A linear complexity instance selectionalgorithm based on classifier ensemble concepts ✩César García-Osorio a, Aida de Haro-García b, Nicolás García-Pedrajas b,∗a Department of Civil Engineering of the University of Burgos, Spainb Department of Computing and Numerical Analysis of the University of Córdoba, Spaina r t i c l ei n f oa b s t r a c tArticle history:Received 4 November 2008Received in revised form 18 January 2010Accepted 21 January 2010Available online 1 February 2010Keywords:Instance selectionInstance-based learningEnsemblesHuge problemsInstance selection is becoming increasingly relevant due to the huge amount of data thatis constantly being produced in many fields of research. Although current algorithms areuseful for fairly large datasets, scaling problems are found when the number of instancesis in the hundreds of thousands or millions. When we face huge problems, scalabilitybecomes an issue, and most algorithms are not applicable.Thus, paradoxically,instance selection algorithms are for the most part impracticablefor the same problems that would benefit most from their use. This paper presentsa way of avoiding this difficulty using several rounds of instance selection on subsetsof the original dataset. These rounds are combined using a voting scheme to allowgood performance in terms of testing error and storage reduction, while the executiontime of the process is significantly reduced. The method is particularly efficient whenwe use instance selection algorithms that are high in computational cost. The proposedapproach shares the philosophy underlying the construction of ensembles of classifiers. Inan ensemble, several weak learners are combined to form a strong classifier; in our methodseveral weak (in the sense that they are applied to subsets of the data) instance selectionalgorithms are combined to produce a strong and fast instance selection method.An extensive comparison of 30 medium and large datasets from the UCI Machine LearningRepository using 3 different classifiers shows the usefulness of our method. Additionally,the method is applied to 5 huge datasets (from three hundred thousand to more than amillion instances) with good results and fast execution time.© 2010 Elsevier B.V. All rights reserved.1. IntroductionThe overwhelming amount of data that is available nowadays [1] in any field of research poses new problems for datamining and knowledge discovery methods. This huge amount of data makes most of the existing algorithms inapplicable tomany real-world problems. Two approaches have been used to deal with this problem: scaling up data mining algorithms [2]and data reduction. However, scaling up a certain algorithm is not always feasible. On the other hand, data reduction consistsof removing from the data missing, redundant and/or erroneous data to get a tractable amount of data. One common methodfor data reduction is instance selection.✩This work was supported in part by the Project TIN2008-03151 of the Spanish Ministry of Education and Science.* Corresponding author.E-mail addresses: cgosorio@ubu.es (C. García-Osorio), adeharo@uco.es (A. de Haro-García), npedrajas@uco.es (N. García-Pedrajas).URL: http://cibrg.org (N. García-Pedrajas).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.01.001C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441411Instance selection [3] consists of choosing a subset of the total available data to achieve the original purpose of the datamining application as if the whole data were being used. Different variants of instance selection exist. We can distinguishtwo main models [4]: instance selection as a method for prototype selection for algorithms based on prototypes (such ask-nearest neighbors) and instance selection for obtaining the training set for a learning algorithm (such as decision trees orneural networks).The problem of instance selection for instance based learning can be defined as “the isolation of the smallest set ofinstances that enable us to predict the class of a query instance with the same (or higher) accuracy than the originalset” [5].Many widely used instance selection algorithms are, at least, O (n2), n being the number of instances [6]. Althoughmethods for scaling up these learning algorithms have been proposed [7], for many algorithms either these methods arenot applicable or their application is troublesome. For huge problems, with hundreds of thousands or even millions ofinstances, most instance selection methods are not applicable. One natural way of scaling up a certain algorithm is dividingthe original problem into several easier subproblems and applying the algorithm separately to each subproblem. In thisway we might scale up instance selection by dividing the original dataset into several disjoint subsets and performing theinstance selection process separately on each subset. However, this method does not work well, as the application of thealgorithm to a subset suffers from the partial knowledge it has of the dataset. Instance selection algorithms must evaluatethe relevance of each instance to decide whether to remove it. To evaluate the relevance of an instance, the algorithm needsto know the whole dataset, because that relevance depends on all the other instances. Thus, direct application of instanceselection to subsets of the original dataset does not yield a good performance.In this paper we propose a methodology for using this basic idea of applying the instance selection algorithm to subsetsof the original dataset in a way that allows a performance close to the application of the algorithm to the whole dataset,while retaining the advantages of a smaller subset. The underlying idea is based upon the following premises:1. As stated above, a promising way of scaling up instance selection algorithms is using smaller subsets. A simple way ofdoing that is partitioning the dataset into disjoint subsets and applying the instance selection algorithm to each subsetseparately.2. The above solution does not perform well, as each subset is only a partial view of the original dataset. In this way,important instances may be removed and superfluous instances may be kept. In the same sense that we talk of “weaklearners” in a classifier ensemble construction framework, we can consider an instance selection algorithm applied to asubset of the whole dataset as a “weak instance selection algorithm.”3. Following the philosophy of classifier ensembles we conduct several rounds of weak instance selection algorithms andcombine them using a voting scheme. Therefore, our approach is called democratic instance selection, and can be con-sidered a form of extending classifier ensemble philosophy to instance selection.Democratic instance selection is thus based on repeating several rounds of a fast instance selection process. Each roundon its own would not be able to achieve a good performance. However, the combination of several rounds using a votingscheme is able to match the performance of an instance selection algorithm applied to the whole dataset with a largereduction in the time of the algorithm. In a different setup from the case of ensembles of classifiers, we can consider ourmethod a form of “ensembling” instance selection. In classification, several weak learners are combined into an ensemblewhich is able to improve the performance of any of the weak learners alone [8]. In our method, the instance selectionalgorithm applied to a partition into disjoint subsets of the original dataset can be considered a weak instance selector,as it has a partial view of the dataset. The combination of these weak selectors using a voting scheme is similar to thecombination of different learners in an ensemble.The main advantage of our method is that as the instance selection algorithm is applied only to small subsets, the timeis reduced significantly. In fact, as the size of the subset is chosen by the researcher, we can apply the method to anyproblem regardless of the number of instances involved. As for the case of classifier ensembles, where the base learner is aparameter of the algorithm, in our method the instance selection method is a parameter, and any algorithm can be used.This paper is organized as follows: Section 2 presents the proposed model for instance selection based on our approach;Section 3 reviews some related work; Section 4 describes the experimental setup; Section 5 shows the results of the exper-iments; and Section 6 states the conclusions of our work and directions for future research.2. Democratic instance selection methodThe democratic method for instance selection consists of performing r rounds of an instance selection algorithm whichis applied to a number of disjoint subsets of the dataset that constitutes a partition of the available data. For each round,the process consists of dividing the original dataset into several disjoint subsets of approximately the same size. Then, theinstance selection algorithm is applied to each subset separately. The instances that are selected by the algorithm to beremoved receive a vote. Then, a new partition is performed and another round of votes is carried out. After the predefinednumber of rounds is performed, the instances that have received a number of votes above a certain threshold are removed.An outline of the method is shown in Algorithm 1. Each round can be considered to be similar to a classifier in an ensemble,412C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441and the combination process by voting is similar to the combination of base learners in bagging or boosting [9]. Fig. 1 showsan example of the algorithm for 10 rounds of votes and a dataset of 50 instances.Algorithm 1: Democratic instance selection (demoIS.) algorithmData : A training set T = {(x1, y1), . . . , (xn, yn)}, subset size s, and number of rounds r.Result: The set of selected instances S ⊂ T .for k = 1 to r do(cid:2)i ti = T of size sDivide instances into ns disjoint subsets ti :for j = 1 to ns doApply instance selection algorithm to t jStore votes of removed instances from t jendendObtain threshold of votes, v, to remove an instanceS = TRemove from S all instances with a number of votes (cid:2) vreturn S1234567The most important advantage of our method is the large reduction in execution time. The reported experiments willshow a large difference when using standard widely used instance selection algorithms. Additionally, the method is easyto implement in a parallel environment, because the execution of the instance selection algorithm over each subset isperformed independently. Furthermore, as the size of the subsets is a parameter of the algorithm, we can choose thecomplexity of the execution in each of the processors.However, as stated, the method still has two important issues to be addressed before we can obtain a useful algorithm.First, the partition method is not trivial, as a strictly random partition would not perform well. Second, the determinationof the number of votes is problem-dependent. We carried out preliminary experiments using a fixed threshold for differentproblems with poor results. Depending on the problem, a certain threshold may be too low or too high. If we set a certainfixed threshold of votes to remove an instance for any problem, there are datasets for which that threshold means removingalmost all the instances; on the other hand, there are other datasets for which that threshold results in keeping almostall instances. Thus a method must be developed for automatic determination of the number of votes needed to removean instance from the training set. Automatic determination of this threshold has the additional advantage of relieving theresearcher of the duty of setting a difficult parameter of the algorithm. These two issues are discussed in the followingsections. We must also emphasize that our method is applicable to any instance selection algorithm, because the instanceselection algorithm is a parameter of the method.2.1. Partition of the dataset(cid:2)An important step in our method is partitioning the training set into a number of disjoint subsets, ti , which comprisei ti = T . The size of the subsets is fixed by the user. The actual size has no relevant influence overthe whole training set,the results provided it is small enough to avoid large execution time. Furthermore, the time spent by the algorithm dependson the size of the largest subset, so it is important that the partition algorithm produces subsets of approximately equalsize.We need a different partition of the dataset for each round of votes. Otherwise, the votes cast will be the same be-cause most instance selection algorithms are deterministic. The simplest method would be a random partition, where eachinstance is randomly assigned to one of the subsets. Each round of votes will receive a different random partition. Thiswas our first attempt at partition method inspired by [10] where bagging was used to get a sparse but not grandmotherrepresentation for Kernel Principal Component Analysis. However, this method has two problems: k-NN is a local learningalgorithm, so because this partition does not keep, at least partially, the locality of the instances the performance of k-NNwill be greatly affected. Thus, the first goal of our partition method is keeping, as much as possible, a certain locality in thepartition. However, there is an additional important factor for the performance of the method that is subtler.Each partition represents a different optimization problem, and thus a different error surface for the instance selectionalgorithm. If the partitions are very different, these error surfaces will also be very different. In such a case, the votes castby the different rounds are almost randomly distributed, and the obtained performance is poor. Thus, to obtain a goodperformance the partitions of the different rounds of the algorithm must vary smoothly.1These two requirements are met using the theory of Grand Tour [11]. The idea of the Grand Tour method, introducedby Asimov [11] and Buja and Asimov [12], is to generate a continuous sequence of low-dimensional projections of a high-dimensional dataset, based on the premise that to fully understand a subject item, one must examine it from all possible1 In fact, we performed experiments with random partitions with worse results. However, if the different random partitions are varied smoothly, forexample, by performing and initial random partition and then exchanging a few instances between subsets at each round, the performance is clearlyimproved.C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441413Fig. 1. Example of democratic instance selection for a dataset of 50 instances and subsets of 10 instances. The instance selection algorithm can be any oneof the many available.414C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441angles. The method rotates a plane in the high-dimensional space. The data are projected onto this plane for each of itsorientations, and when the sequence of projections is visualized on a computer screen, an animation is obtained, which isuseful for identifying structures in the data set, such as clusters and outliers. Grand Tour shares a common objective withexploratory projection pursuit techniques. In both cases the human ability for visual pattern recognition is exploited.2.1.1. AlgorithmsWhen Grand Tour is used for visualization the sequence of planes must hold two conditions:a. The sequence should be dense in the set of all planes in the high-dimensional space.b. The sequence should be smooth to give a visual impression of the data points moving in a continuous way.The state-of-the-art algorithms for Grand Tour are “guided tours” and “manual tours” [13] and are based on the interpo-lation of a sequence of randomly generated planes. In the context of our algorithm it is enough to use a one-dimensionalGrand Tour, thus, we project the data onto a rotating vector and then use this projection to divide the dataset into thesubsets that we will pass to the underlying instance selection algorithm. In addition, we are more concerned with the sim-plicity of the algorithm. Thus, instead of an interpolation class algorithm, we chose a parametrization class algorithm, thetorus method [11], based on obtaining a sequence of rotation matrices, which leaves us with the problem of how to obtainthese matrices.We want to obtain a generalized rotation matrix Q that we will use to rotate the vector onto the area where we aregoing to project the data. This is implemented by choosing Q as an element of the special orthogonal group, denoted bySO(d), of orthogonal d × d matrices having determinant +1 (a matrix must have these two properties to be a rotationmatrix). So, we need a continuous curve through SO(d). In the torus method this is achieved by obtaining a continuouscurve in a p-dimensional torus (p = (d − 1)d/2, being d the dimension of the dataset) whose points give the angles tocalculate Q . The idea is to get a varying vector of angles α(s) = (θ1,2, θ1,3, . . . , θd−1,d) that we use to generate Q throughthe mapping β : [0, 2π ]p → SO(d) given by:β(θ1,2, θ1,3, . . . , θd−1,d) = R1,2(θ1,2) × R1,3(θ1,3) × · · · × Rd−1,d(θd−1,d)(1)The R i, j(θi, j) are elements of SO(d) that rotate the eie j plane through an angle of θi, j⎡R i, j(θi, j) =⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣1 · · ·.... . .0 · · ·...0 · · ·...0 · · ·0...cos(θi, j)...sin(θi, j)...0· · ·0...0· · · − sin(θi, j). . .· · ·...cos(θi, j)...· · ·⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦· · · 0...· · · 0...· · · 0.... . .· · · 1To summarize, the coordinates of a point of the p-torus give the angles of the rotation matrices R i, j , which are combinedto obtain the rotation matrix Q used for rotating the vector. There are different ways to get the curve through the p-torus[14]: the Asimov–Buja winding algorithm, the random curve algorithm or the fractal curve algorithm. In our experiments weuse the random curve algorithm. First, we randomly use two points si, s j in [0, 2π ]p to create a linear interpolant betweenthe points going from si to s j , then, if needed, we take a third point sk and join it with s j and so on.2.1.2. Some implementation detailsObtaining the curve strictly through the shortest path in the p-torus adds a burden of complexity that does not seemto give any extra advantage to our algorithm. So, instead we just interpolate the points through the hypercube [0, 2π ]p . Ifwe have a point near the p-dimensional point (2π , 2π , . . . , 2π ) and a point near the p-dimensional point (0, 0, . . . , 0), inan actual p-torus these two points are very close to each other and the shortest path should be through the walls of thehypercube [0, 2π ]p . However, in our current implementation, we interpolate the point only using the path strictly inside thep(2π )2). Furthermore, with the interpolation of the first two points in [0, 2π ]phypercube (whose length is approximatelywe usually obtain enough orientations to get all the partitions required by the algorithm.(cid:9)Because the density of the projections in the context of instance selection is not a critical factor (we do not need along tour to get good results and ten to fifteen steps of Grand Tour are usually enough), we can use even simpler ways ofobtaining the sequence of projections. In the case of uni-dimensional projections we could have applied only the pseudoGrand Tour obtained by using Andrews curves [15]. If we want a sequence of bi-dimensional projections we can use theorthogonal vectors given by Wegman curves [16].One concern when Grand Tour is used for dynamic data visualization is that, in general, the mapped curve on SO(d)cannot be uniformly distributed even when the curve on the p-torus is equally distributed. Here, we are also interested inuniformly rotating the projection vector, and we solve this by simply dynamically adapting the interpolation step used toobtain the curve on the p-hypercube whenever the angle changes more than 10% of the previous angle.C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441415Fig. 2. Example of the method for partitioning the dataset with 1500 instances, three classes, and two features, with five rounds of votes. Four subsets arecreated each round.2.2. Partition algorithmFollowing this approach, we obtain the first partition by projecting our dataset onto a random vector and then dividingthe projection into equal sized subsets. The next vector is obtained using the described procedure and a new partitionis made. The procedure is repeated to get the subsequent partitions. Algorithm 2 shows the method for performing thepartition based on this methodology.Algorithm 2: Algorithm for partitioning the training set into disjoint subsets(cid:2)Data : A training set T = {(x1, y1), . . . , (xn, yn)}, and subset size s.Result: The partition into disjoint subsets ti :Get next vector using Grand Tour methodProject all instances into vectorDivide the projected instances into subsets of size s using the linear ordering induced by the projectionAssign ti to each subset(cid:2)Return ti :i ti = T .i ti = T12345An example of a partition performed in an artificial training set of 1500 instances, where the data are divided intofour subsets, is depicted in Fig. 2. The figure shows the original dataset which contains three classes and the five parti-tions performed on five rounds of votes. The figure shows the smooth variation of the subsets as the rounds of votes areperformed.This partition is specifically designed for k-NN instance selection algorithms. If we apply our methodology to otherclassifiers a random partition of the dataset can be used as shown in Section 5.11.416C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–4412.3. Determining the threshold of votesAn important issue in our method is determining the threshold of votes to remove an instance from the training set.Preliminary experiments showed that this number depends on the specific dataset. Thus, it is not possible to set a generalpreestablished value usable in any dataset. On the contrary, we need a way of selecting this value directly from the dataset inrun time. A first natural choice would be the use of a cross-validation procedure. However, this method is time consuming.A less costly method is estimating the best value for the number of votes from the effect on the training set. The choice ofthe number of votes must take into account two criteria: training error, (cid:6)t , and storage, or memory, requirements m. Bothvalues must be minimized. We define a criterion, f (v), which is a combination of these two values:f (v) = α(cid:6)t(v) + (1 − α)m(v)(2)where m is measured as the percentage of instances retained, (cid:6)t is the training error and α is a value in the interval [0, 1]that measures the relative relevance of both values. Because the minimization of the error is usually more important thanstorage reduction, we have used a value of α = 0.75. Different values can be used if the researcher is more interested inreduction than in error. Estimating the training error is time consuming if we have large datasets. To avoid this problem, thetraining error is estimated using only a small percentage of the whole dataset, which is 10% for medium and large datasetsand 0.1% for huge datasets.The process to obtain the threshold is the following: Once we have performed r rounds of the algorithm and stored thenumber of votes received by each instance, we must obtain the threshold of votes, v, to remove an instance. This valuemust be v ∈ [1, r]. We calculate the criterion f (v) (Eq. (2)) for all the possible threshold values from 1 to r and assign v tothe value that minimizes the criterion. After that, we remove the instances whose number of votes is above or equal to theobtained threshold v.2.4. Complexity of our methodologyThe aim of this work is to obtain an instance selection methodology that is able to scale up to large and even hugeproblems. Thus, an analysis of the complexity of the method is essential. In this section, we show how our algorithm islinear in the number of instances, n, of the dataset.We divide the dataset into partitions of disjoint subsets of size s. Thus, the chosen instance selection algorithm is alwaysapplied to a subset of fixed size, s, which is independent of the actual size of the dataset. The complexity of this applicationof the algorithm depends on the base instance selection algorithm we are using, but will always be small because thesize s is always small. Let K be the number of operations needed by the instance selection algorithm to perform its taskin a dataset of size s. For a dataset of n instances we must perform this instance selection process once for each subset,that is, n/s times, spending a time proportional to (n/s)K . The total time needed by the algorithm to perform r roundswill be proportional to r(n/s)K , which is linear in the number of instances, because K is a constant value. Fig. 3 showsthe computational cost, as a function of the number of instances, of a quadratic algorithm and our approach when thatalgorithm is used with subset sizes of s = 100, 1000, 2500 and 5000 instances and 10 rounds of votes. If the complexity ofthe instance selection algorithm is greater, the reduction of the execution will be even better.The method has the additional advantage of allowing an easy parallel implementation. Because the application of theinstance selection algorithm to each subset is independent of all the remaining subsets, all the subsets can be processed atthe same time, even for different rounds of votes. Also, the communication between the nodes of the parallel execution issmall.As we have stated, two additional processes complete the method: the partition of the dataset and the determinationof the number of votes. The determination of the number of votes can be accomplish in different ways. If we consider allthe training instances, the cost of this step would be O (n2). However, to keep the complexity linear, we use a randomsubset of the training set to determine the number of votes, with a limit on the maximum size of this subset that is fixedfor any dataset. In this way, in medium to large datasets we use 10% of the training set, for huge problems 0.1%, and thepercentage is further reduced as the size of the dataset grows. In fact, we have experimentally verified that we can considerany reasonable bound2 in the number of instances without damaging the performance of the algorithm. With this methodthe complexity of this step is O (1) because the number of instances used is bounded regardless of the size of the dataset.Finally, we consider the partition of the dataset apart from the algorithm because many different partition methods canbe devised. The partition described in Section 2.1 can be implemented with a complexity O (n log(n)), using a quicksortalgorithm for sorting the values to make the subsets, or with a complexity O (n) dividing the projection along the vector inequal width intervals. Both methods achieve the same performance as the obtained partition is similar. In our experimentswe have used the latter method to keep the complexity of the whole procedure linear. However, this partition is speciallydesigned for k-NN classifiers. When the method is used with other classifiers, other methods can be used, such as a randompartition, which is also of complexity O (n). In fact, in the experiments reported using decision trees and support vectormachines, we have used a random partition of the dataset.2 This reasonable bound can be from a few hundred to a few thousand instances, even for huge datasets.C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441417Fig. 3. Computational cost of our method and a base instance selection algorithm of O (n2).3. Related workThe usefulness of applying instance selection to disjoint subsets is also shown in [17]. In this work a cooperative evo-lutionary algorithm is used. Several evolutionary algorithms are performed on disjoint subsets of instances and a globalpopulation is used to account for the global view. This method is scalable to medium to large problems but cannot beapplied to huge problems.There are not many previous studies that have dealt with instance selection for huge problems. Cano et al. [18] proposedan evolutionary stratified approach for large problems. Although the algorithm shows good performance, it is still toocomputationally expensive for huge datasets. Kim and Oommen [19] proposed a method based on a recursive application ofinstance selection to smaller datasets.In a recent paper, De Haro-García and García-Pedrajas [20] showed that the application of a recursive divide-and-conquerapproach is able to achieve a good performance while attaining a dramatic reduction in the execution time of the instanceselection process.Domingos and Hulten [21,22] developed a method for scaling up learning algorithms based on Hoeffding bounds [23].The method can be applied to either choosing among a set of discrete models or estimating a continuous parameter. Themethod consists of three steps: first, it must derive an upper bound on the relative loss between using a subset of theavailable data and the whole dataset in each step of the learning algorithm. Then, it must derive an upper bound of the timecomplexity of the learning algorithm as a function of the number of samples used in each step. Finally, it must minimizethe time bound, via the number of samples used in each step, subject to the target limits on the loss of performance ofusing a subset of the dataset. Although the method is able to achieve interesting results, the need to derive these boundsmakes its application troublesome for many algorithms. On the other hand, the advantage or our method with respect tothis approach, is that no modification of the original algorithm is needed. Furthermore, the experiments reported by theauthors [22] show that the dataset size must be several million instances for the method to be worthwhile. The experimentsreported show that our method is able to reduce the execution time of the tested algorithms from a problem size of a fewthousands instances.In a subsequent study [24], Domingos and Hulten developed a method for inductive algorithms based on discrete search.The complexity of the method is independent of the process of generating candidate solutions, but only if in this processthe method does not need to access the data. In that way, it is applicable to randomized search processes. The generalframework proposed [7] has been used for scaling up decision trees, Bayesian network learning, k-means clustering andthe EM algorithm for mixtures of Gaussians. To the best of our knowledge, the approach has not been applied to instanceselection.There is a second advantage to our method. To apply the method of Domingos and Hulten we must derive an upperbound of the time complexity of the learning algorithm as a function of the number of samples used in each step. On theother hand, our proposal uses standard algorithms as black boxes, without any modification. In that way, it is applicable toany existing instance selection algorithm.418C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Table 1Summary of datasets. The features of each dataset can be C (continuous), B (binary) or N (nominal).Data setCasesFeaturesClassesInputs1-NN error123456789101112131415161718192021222324252627282930abaloneadultcargenegermanhypothyroidisoletkrkoptkr vs. kplettermagic04mfeat-facmfeat-foumfeat-karmfeat-mormfeat-pixmfeat-zernurseryoptdigitspage-blockspendigitsphonemesatimagesegmentshuttlesicktexturewaveformyeastzip417748 8421728317510003772779728 056319620 00019 02020002000200020002000200012 9605620547310 99254046435231058 000377255005000148492984. Experimental setup76C––676176–16102167664624047–641016536199740408256B–1––320––34––––––––1–––––––20––––N17660112––––––––––27–––––––––––229243242618226210101010101051051026772113101010105161206129617638161021676646240472364101653619933404082560.80340.20050.15810.27670.31200.06920.14430.43560.08280.04540.20840.03500.20800.04350.29250.02700.21400.25020.02560.03690.00660.09520.09390.03980.00100.04300.01050.28600.48790.0292To make a fair comparison between the standard algorithms and our proposal, we selected 30 problems from the UCIMachine Learning Repository [25]. We selected datasets with, at least, 1000 instances. For estimating the storage reductionand generalization error, we used a k-fold cross-validation (cv) method. In this method the available data is divided into kapproximately equal subsets. Then the method is learned k times, using in turn each one of the k subsets as testing set, andthe remaining k − 1 subsets as training set. The estimated error is the average testing error of the k subsets. A fairly standardvalue for k is k = 10. A summary of these datasets is shown in Table 1. In some figures throughout the paper we use thenumber of order of each dataset to reduce the size needed by the graphs. The table shows the 10-fold cv generalizationerror of a 1-NN classifier without instance selection, which can be considered as a baseline measure of the error of eachdataset. These datasets can be considered representative of medium to large problems.As the main statistical test, we used the Wilcoxon test for comparing pairs of algorithms. We chose this test because itassumes limited commensurability and is safer than parametric tests, because it does not assume normal distributions orhomogeneity of variance. Furthermore, empirical results [26] show that this test is also stronger than other tests.The evaluation of an instance selection algorithm is not a trivial task. We can distinguish two basic approaches: directand indirect evaluation [27]. Direct evaluation evaluates a certain algorithm based exclusively on the data. The objectiveis to measure to what extent the selected instances reflect the information present in the original data. Some proposedmeasures are entropy, moments, and histograms.Indirect methods evaluate the effect of the instance selection algorithm on the learning task. If we are interested in clas-sification, we evaluate the performance of the used classifier when using the reduced set obtained after instance selectionas learning set.Therefore, when evaluating instance selection algorithms for instance-based learning, the usual method for evaluation isestimating the performance of the algorithms on a set of benchmark problems. In those problems several criteria can beconsidered, such as [28] storage reduction, generalization accuracy, noise tolerance, and learning speed. Speed considerationsare difficult to measure, because we are evaluating not only an algorithm but also a certain implementation. However,because the main aim of our work is scaling up instance selection algorithms, execution time is a basic issue. To allowa fair comparison, we performed all the experiments in the same machine, a bi-processor computer with two Intel XeonQuadCore processors at 1.60 GHz. To perform sound experiments the algorithm used for the whole training set and thealgorithm used in our method were exactly the same.C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441419The source code, in C and licensed under the GNU General Public License, used for all methods as well as the partitionsof the datasets are freely available upon request to the authors.4.1. Instance selection algorithmsTo obtain an accurate view of the usefulness of our method, we had to select some of the most widely used instanceselection algorithms. We chose to test our model using several of the most successful state-of-the-art algorithms. Initially,we used the algorithms DROP3 [28], and ICF [5]. DROP3 (Decremental Reduction Optimization Procedure 3) is shown in Algo-rithm 3. This algorithm is an example of a new generation of algorithms that were designed taking into account the effectof the order of removal on the performance of the algorithm. ICF is designed to be insensitive to the order of presentationof the instances. It includes a noise-filtering step using a method similar to Wilson’s Edited Nearest-Neighbor Rule [29]. Then,the instances are ordered by the distance to their nearest neighbor. The instances are removed beginning with the instancesfurthest from its nearest neighbor. This tends to remove the instances farthest from the boundaries first.Algorithm 3: DROP3 algorithmData : A training set T = {(x1, y1), . . . , (xn, yn)}, a selector S = ∅Result: The set of selected instances S ⊂ T .Noise filtering: Remove any instance in T misclassified by its k neighborsS = TSort instances in S by distance to their nearest enemyforeach Instance P ∈ S doFind P .N1..k+1, the k + 1 nearest neighbors of P in SAdd P to each of its neighbors’ list of associatesendforeach Instance P ∈ S doLet with = # of associates of P classified correctly with P as a neighborLet without = # of associates of P classified correctly without Pif without (cid:2) with thenRemove P from Sforeach Associate A of P doRemove P from A’s list of nearest neighborsFind a new nearest neighbor for AAdd A to its new neighbor’s list of associated1234567891011endendendreturn S12ICF (Iterative Case Filtering) is shown in Algorithm 4. For ICF algorithm coverage and reachability are defined as follows:Coverage(c) =Reachable(c) =(cid:10)c(cid:10)c(cid:6) ∈ T : c ∈ LocalSet(cid:6) ∈ T : c(cid:13)(cid:6) ∈ LocalSet(c)(cid:12)(cid:13)(cid:6)(cid:11)c(3)(4)The local set of a case c is defined as “the set of cases contained in the largest hypersphere centered on c such that onlycases in the same class as c are contained in the hypersphere” [5] so that the hypersphere is bounded by the first instanceof a different class. The coverage set of an instance includes the instances that have it as one of their neighbors, and thereachable set is formed by the instances that are its neighbors. The algorithm is based on repeatedly applying a deletingrule to the set of retained instances until no more instances fulfill the deleting rule.In addition to these two methods, it is worth mentioning the Reduced Nearest Neighbor (RNN) rule [30]. This methodis extremely simple, but it also shows a good performance in terms of storage reduction. However, RNN has a seriousdrawback: its computational complexity. Among the standard methods used, RNN shows the worst scalability, taking severalhundreds of hours for the largest problems. Therefore, RNN is the perfect target for our methodology, an instance selectionmethod that is highly efficient but with a serious scalability problem. So, we also tested our approach using RNN, shown inAlgorithm 5, as base instance selection method.We also used one of the most recent algorithms for instance selection, the Modified Selective Subset (MSS) method [31].The procedure is shown in Algorithm 6. We chose this algorithm as an example of a fast algorithm. With MSS we want totest whether our method is also able to improve the execution time of algorithms that are not as time-demanding as theprevious ones.As an alternative to these standard methods, we also applied genetic algorithms to instance selection, considering thistask to be a search problem. The application is easy and straightforward. Each individual is a binary vector that codesa certain sample of the training set. The evaluation is usually made considering both data reduction and classificationaccuracy. Examples of applications of genetic algorithms to instance selection can be found in [32,33] and [34]. Cano et al.[4] performed a comprehensive comparison of the performance of different evolutionary algorithms for instance selection.420C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Algorithm 4: ICF algorithmData : A training set T = {(x1, y1), . . . , (xn, yn)},Result: The set of selected instances S ⊂ T .Noise filtering: Remove any instance in T misclassified by its k neighborsrepeatforall x ∈ T doCompute reachable(x)Compute coverage(x)endprogress = falseforall x ∈ T doif |reachable(x)| > |coverage(x)| thenFlag x for removalprogress = trueendendforall x ∈ T doif x flagged for removal then T = T − {x}enduntil not progressreturn T12345678Algorithm 5: RNN algorithmData : A training set T = {(x1, y1), . . . , (xn, yn)}, a selector S = ∅Result: The set of selected instances S ⊂ T ./* Obtain Condensed Nearest Neighbor setS = {x1}foreach Instance P ∈ T doif P is misclassified using S thenAdd P to SRestartendend/* Obtain Reduced Nearest Neighbor setforeach Instance P ∈ S doRemove P from Sif any instance of T is misclassified using S thenAdd P to Sendendreturn S123456Algorithm 6: MSS algorithmData : A training set T = {(x1, y1), . . . , (xn, yn)}, a selector S = ∅Result: The set of selected instances S ⊂ T .S = ∅Sort instances xi ∈ T by distance, D i , to their nearest enemyfor i = 1 to n doadd = falsefor j = i to n doif x j ∈ T ∧ d(xi , x j ) < D j thenT = T − {x j }add = trueendendif add then S = S ∪ {xi }if T = ∅ then return Sendreturn S12345678*/*/They compared a generational genetic algorithm [35], a steady-state genetic algorithm [36], a CHC genetic algorithm [37],and a population based incremental learning algorithm [38]. They found that evolutionary-based methods were able tooutperform classical algorithms in both classification accuracy and data reduction. Among the evolutionary algorithms, CHCwas able to achieve the best overall performance.Nevertheless, the most critical problem addressed when applying genetic algorithms to instance selection is the scalingof the algorithm. As the number of instances grows, the time needed for the genetic algorithm to reach a good solutionC. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441421increases exponentially, making it totally useless for large problems. Because we are concerned with this problem, we usedas a fifth instance selection method a genetic algorithm using CHC methodology. The execution time of CHC is clearly longerthan the time spent by ICF, DROP3 or MSS, so it gives us a good benchmark to test our methodology on an algorithm that,like RNN, has a scalability problem.5. Experimental resultsThe same parameters were used for the standard version of every algorithm and its application within our methodology.None of the standard methods have relevant parameters. The only value we had to set was k, the number of nearestneighbors. For DROP3, ICF, we used k = 3 neighbors, and k = 1 for RNN and MSS. For CHC we performed 100 generationsof a population with 100 individuals and k = 1. Mutation was applied with a 10% probability. These are fairly standardvalues [4]. Our method has two parameters: subset size, s, and number of rounds, r. For subset size, we have to use a valuelarge enough to allow for a meaningful application of the instance selection algorithm on the subset, and small enoughto allow fast execution because the time used by our method grows with s. As a compromise value, we chose s = 1000,and a minimum of two subsets if the dataset has 1000 or fewer instances. For the number of rounds, we chose a smallvalue to allow for fast execution, r = 10. In Section 5.9 we carry out a study of the effect of these two parameters onthe performance of the algorithm. The application of our method with a certain instance selection algorithm X will benamed demoIS.x. A summary of the results using the five algorithms is shown in Tables 2 and 3, for standard and demoIS.xmethods.5.1. DROP3 vs. demoIS.drop3The results using a standard DROP3 algorithm and our method with DROP3 as base algorithm are plotted in Fig. 4. Thefigure shows results for testing error, storage requirements and execution time. Throughout the paper, we will use a graphicrepresentation based on the kappa-error relative movement diagrams [39]. However, here, instead of the kappa differencevalue, we will use the storage difference. The idea of these diagrams is to represent with an arrow the results of twomethods applied to the same dataset. The arrow starts at the coordinate origin and the coordinates of the tip of the arroware given by the difference between the errors and storages of our method and the standard instance selection algorithm.The numbers indicate the dataset according to Table 1. These graphs are a convenient way of summarizing the results. Forexample, arrows pointing down-left represent datasets for which our method outperformed the standard algorithm in botherror and storage, arrows pointing up-left indicate that our algorithm improved the storage but had a worse testing error,and so on.Numerical results are shown in Tables 2 and 3. In terms of error, our method is able to match the results of the originalDROP3 algorithm, the differences between them being small. In fact, demoIS.drop3 is even able to improve the performanceof DROP3 in some datasets, such as car, mfeat-zer and nursery. In terms of storage reduction, demoIS.drop3 performs betterthan DROP3. Although it achieves worse results than DROP3 in a few problems, demoIS.drop3 is able to obtain a large reduc-tion over the results of DROP3 in abalone, gene, german, isolet, krkopt, waveform and yeast datasets. In terms of executiontime, the advantage of demoIS.drop3 is significant. For small problems, there is a small overload due to the 10 rounds ofvotes performed; however, as the problem grows in size our approach shows a large reduction in the time needed to per-form the instance selection process. In this way, for the most time-consuming problem, adult dataset, demoIS.drop3 needsonly 5% of the time spent by the original DROP3 to achieve a similar error and better storage reduction.5.2. ICF vs. demoIS.icfResults for ICF and demoIS.icf are plotted in Fig. 5, and the numerical results are shown in Tables 2 and 3. In terms oftesting error, demoIS.icf is able to improve, or at least match, the results of ICF in all the datasets, with the only exceptionsbeing gene, krkopt, kr vs. kp and nursery problems. Furthermore, for some problems, such as isolet, letter, mfeat-kar, page-blocks and zip, the test error is clearly better than the error achieved by ICF. In terms of storage reduction, the averageperformance of both algorithms is similar, with a remarkably good performance of demoIS.icf for nursery dataset. In termsof execution time the behavior is similar to the case for DROP3. For complex problems, the advantage of demoIS.icf over ICFis clear.5.3. MSS vs. demoIS.mssResults for MSS and demoIS.mss are plotted in Fig. 6, and the numerical results are shown in Tables 2 and 3. In termsof both testing error and storage reduction, the performances of demoIS.mss and MSS are similar. The relevance of thisexperiment, as we have stated, was to test whether our approach is still able to reduce the execution time of a simpleralgorithm, as was the case with more complex ones, such as DROP3 and ICF. The results show that for large datasets, suchas adult, krkopt, letter and shuttle, improvement in the execution time is significant.Table 2Testing error, storage requirements and execution time (in seconds) for standard instance selection algorithms.DatasetDROP3ICFMSSRNNCHCStorageErrorTime (s)StorageErrorTime (s)StorageErrorTime (s)StorageErrorTime (s)StorageErrorTime (s)abaloneadultcargenegermanhypothyroidisoletkrkoptkr vs. kplettermagic04mfeat-facmfeat-foumfeat-karmfeat-mormfeat-pixmfeat-zernurseryoptdigitspage-blockspendigitsphonemesatimagesegmentshuttlesicktexturewaveformyeastzip0.30690.12480.26680.38770.30730.05140.28520.44310.22290.17440.17890.12080.24730.16550.20620.10950.22310.29340.09110.04300.04510.18520.13660.12190.00280.06250.08780.29610.31930.10400.77820.17140.23780.27760.28700.06100.17700.48030.10160.10370.19780.06000.23200.08350.28850.04800.23750.33270.04200.04370.01680.13830.11010.07840.00160.05090.03290.22760.45000.04971.822 853.91.935.60.911.8208.91533.010.11849.8199.839.35.66.51.476.54.0337.4161.015.7175.011.557.74.17543.414.897.028.80.6601.70.25100.10820.38130.25080.14850.03980.17130.52900.27070.13620.11600.08960.13950.10350.20080.08640.15030.87520.06060.03070.03480.13920.07130.10770.02290.04520.07250.12110.21370.04970.80820.21940.27090.35270.32600.11560.26480.40320.12670.20180.23950.09050.32800.17250.36850.10000.27150.24140.11030.21850.06510.19410.16770.13940.04730.09120.09730.28400.50950.25491.79170.81.126.10.43.7103.11109.85.3760.313817.52.42.50.6271.7287.282.85.670.64.625.41.62640.04.746.8150.3219.80.64350.29500.34240.44420.43090.16750.34140.65650.31920.22650.32040.16720.34530.21590.32530.16610.34880.41600.16630.09910.09000.24330.20320.16280.00780.12400.13350.34350.53390.22830.80530.22810.24240.32740.35500.09950.18710.43230.08430.07490.24400.05550.25150.07150.31700.04200.24750.22490.04250.04320.01350.12910.12120.05410.00120.06080.02130.30520.54120.04401.62990.80.36.90.20.739.5356.71.3266.723.46.21.00.80.48.50.857.221.00.817.81.18.20.3584.70.813.95.30.171.70.00790.03330.09840.04020.02960.03130.04470.04250.05580.05810.02930.03870.04440.05440.02390.04130.03510.05790.03090.01430.01880.04720.02540.04280.00140.02070.03290.01300.02660.03480.79350.19510.24710.39970.29500.06550.26650.56780.14230.14200.18050.09250.31350.12650.31350.08100.30100.28020.08810.05590.02760.17780.13450.08660.00180.05940.05180.31980.52300.08847111.71 896 540.312.41633.228.2168.45950.81 057 715.5128.921 394.050 817.247.7146.231.564.939.4102.75959.7281.499.8289.9485.8976.317.81339.465.8131.82132.249.11420.20.38180.19880.41920.30040.34830.26130.29930.52370.27120.29520.29520.39330.33840.38380.35730.37590.34390.29410.27550.27860.29030.28460.28250.30300.26380.25780.28250.29110.37110.28710.79980.22570.26390.29680.32900.07750.20260.47110.12760.09050.12250.04550.22800.06500.32650.04400.22050.24270.04040.04080.01210.14570.11570.06490.00550.05140.02490.28780.50140.05107722.291 096.05483.67236.6440.17183.910 885.4137 015.07107.551 024.029 327.06518.47026.97010.87054.26441.97076.322 397.37846.47662.811 636.67772.98491.87062.477 089.07179.87876.17926.82981.19748.2422C.García-Osorioetal./ArtificialIntelligence174(2010)410–441Table 3Testing error, storage requirements and execution time (in seconds) for democratic instance selection algorithms.DatasetdemoIS.drop3demoIS.icfdemoIS.mssdemoIS.rnndemoIS.chcStorageErrorTime (s)StorageErrorTime (s)StorageErrorTime (s)StorageErrorTime (s)StorageErrorTime (s)abaloneadultcargenegermanhypothyroidisoletkrkoptkr vs. kplettermagic04mfeat-facmfeat-foumfeat-karmfeat-mormfeat-pixmfeat-zernurseryoptdigitspage-blockspendigitsphonemesatimagesegmentshuttlesicktexturewaveformyeastzip0.08220.08990.32780.18720.20120.06310.16350.26790.23470.22360.11300.14360.22100.19660.14940.17760.17100.22990.10930.05300.08220.17920.12600.15610.01640.08140.12600.11200.14600.11800.77820.18480.21630.27380.28700.06900.18400.49160.10310.12030.20480.06800.24150.08550.28650.04100.22000.23000.04590.04480.02180.14390.11730.07310.00340.05650.04000.23540.45610.06465.11204.58.043.76.826.450.770.428.3140.195.588.821.725.46.087.315.587.069.633.683.121.157.412.1337.129.559.631.42.5106.20.08020.08900.37750.20120.08070.02940.17220.33700.22210.24080.09670.12160.15690.14020.15850.12010.13950.21370.11100.03920.07900.17350.11100.14620.05880.04800.12930.07420.10940.16440.78370.19420.24480.36280.30400.08040.20600.47210.12790.12670.21540.07150.27850.12000.33850.06350.26800.24490.06170.05830.02930.16460.13560.11170.01260.06820.04600.27060.48650.07234.0621.64.923.13.08.324.656.213.381.037.132.69.910.62.840.87.359.028.310.531.78.121.44.8225.69.825.614.51.442.70.50300.34480.36340.35470.43090.17820.27890.54430.31280.28300.31680.18420.33630.23140.36190.16040.35670.41050.12420.08840.09000.29430.19420.17880.02750.15300.15530.23860.51370.14560.79450.20760.25930.32900.32800.07510.19590.41140.08370.08850.22690.05500.24450.07250.33150.05600.22550.23930.05910.04280.02050.14910.11630.08880.00630.04880.03020.27580.50140.06534.01309.01.09.11.21.612.4162.13.958.619.312.74.73.51.318.23.322.610.41.710.32.88.11.619.61.58.16.41.022.00.01670.02380.08440.11610.02960.02580.13350.28890.14710.17750.06120.08040.11840.11570.03920.10930.09310.14400.08610.02150.04900.08270.06970.07960.01380.02040.09700.03810.05670.07730.78730.17460.27970.33090.28600.07050.21090.46340.12010.11520.24690.09550.28600.10250.37750.06000.26400.24170.05500.05960.01970.16810.12360.11390.00580.06100.03710.26900.48040.0639963.412 144.941.01432.9249.1102.61122.75168.2182.61877.21645.0141.8409.5106.6165.5130.4279.9750.6164.253.370.6213.8360.424.219.647.890.3943.9125.6476.80.04250.02030.28930.10010.08040.03060.11120.26780.15380.22440.06140.12730.19620.19020.13230.12530.14910.20170.08100.06090.06560.14620.07890.16120.01300.01070.10230.05920.11240.08020.80840.21140.28260.38040.33900.08220.23810.46910.16840.09580.26200.06800.25000.08550.33300.06600.25450.24390.06190.05940.02460.16830.13300.09260.00480.06740.05870.30120.52840.07151231.58152.1272.4558.3141.5480.01609.27627.5498.94585.82965.8258.6322.7291.9269.8267.1301.72425.0827.5695.31549.5789.4907.2297.07286.7472.5782.9840.5245.51628.0C.García-Osorioetal./ArtificialIntelligence174(2010)410–441423424C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Fig. 4. Storage requirements/testing error (top) and execution time in seconds (bottom – using a logarithmic scale) for standard DROP3 algorithm and ourapproach.5.4. RNN vs. demoIS.rnnThe next experiment was conducted using RNN as the base instance selection algorithm. The results are plotted inFig. 7 with numerical values shown in Tables 2 and 3. As we stated in the previous section, this is a perfect example ofthe potentialities of our approach. In our experiments RNN showed the best performance in terms of storage reduction.However, the algorithm has a serious problem of scalability. As an extreme example, for the adult problem RNN took morethan 500 hours per experiment. This scalability problem prevents its application in those problems where it would bemost useful. The figure shows how demoIS.rnn is able to solve the scalability problem of RNN. In terms of testing error,demoIS.rnn is also able to improve the performance of RNN, with a better performance in 21 of the 30 datasets. In termsof storage reduction our algorithm performs worse than RNN. However, the performance of demoIS.rnn is still good, in fact,better than any other of the previous algorithms. So, our approach is able to scale RNN to complex problems, improving itsresults in terms of testing error, but with worse results in terms of storage reduction. Execution time results are remarkable,and the reduction of the time spent by the selection process is large. The extreme example of these results is the two mosttime-consuming datasets, adult and krkopt, where the speed-up is more than a hundred times.5.5. CHC vs. demoIS.chcFig. 8 plots the results of the CHC algorithm, with numerical values shown in Tables 2 and 3. Due to the high computa-tional cost of CHC we chose for this algorithm a smaller subset size of s = 250. A first interesting result is the problem ofscalability of the CHC algorithm, which is more marked for this algorithm than for the previous ones. In other studies [4,17],C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441425Fig. 5. Storage requirements/testing error (top) and execution time in seconds (bottom – using a logarithmic scale) for standard ICF algorithm and ourapproach.the CHC algorithm was compared with standard methods in small to medium problems. For those problems, the perfor-mance of CHC was better than the performance of other methods. However, because the datasets are larger, the scalabilityproblem of CHC becomes relevant. In our set of problems, CHC clearly performs worse than DROP3, ICF, MSS or RNN interms of storage reduction. We must take into account that for CHC we need one bit in the chromosome for each instancein the dataset. This means that for large problems, such as adult, krkopt, letter, magic or shuttle, the chromosome has morethan 10 000 bits, making the convergence of the algorithm problematic. Thus, CHC is, together with RNN, an excellent ex-ample of the applicability of our approach. For this method, the scaling up of CHC provided by demoIS.chc is evident notonly in terms of running time, with a large reduction in all 30 datasets, but also in terms of storage reduction. demoIS.chcis able to improve the reduction of CHC in all 30 datasets, with an average improvement of more than 20%, from an averagestorage of CHC of 31.83% to an average storage of 11.58%. The negative side effect is a worse testing error, which is, however,compensated by the improvement in running time and storage reduction.5.6. Control experimentsThe previous experiments showed that our method is able to, at least, match the performance of standard methodswith a significant reduction in the execution time. However, it may be argued that this reduction with respect to standardmethods is significant only if the standard methods are useful themselves. In this way, if a simple random sampling is noworse than standard methods, the usefulness of our approach would be partly compromised. In any case, we must not426C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Fig. 6. Storage requirements/testing error (top) and execution time in seconds (bottom – using a logarithmic scale) for standard MSS algorithm and ourapproach.forget that, because random sampling does not determine the number of instances to retain in the subset, it solves onlypart of the problem [28], even in such cases when the random sampling achieves good results.In this section we show the results of a control experiment designed to test whether the simple random approach iscompetitive with respect to standard instance selection methods. For each problem we performed a random sampling witha sampling rate equal to the storage obtained by each algorithm and compared the testing error of each standard methodand random sampling. Table 4 shows the comparison for all methods.The experiment shows interesting results. First, we can see that the most widely used algorithms, Drop3, ICF and RNN,are able to improve the performance of random sampling in a consistent way. All of these algorithms are significantly betterthan random sampling. The same conclusion is valid for their democratic counterparts. This control experiment validates theusefulness of these algorithms. However, the experiment also shows that new algorithms must be compared with randomsampling to assure their viability, because MSS and CHC do not show a significantly better behavior than random sampling.Nevertheless, we must not rule out the use of these algorithms, because the comparison is made using the value obtainedby the corresponding instance selection algorithm as the random sampling rate. If we consider random sampling alone, wewill not be able to determine the percentage of instances to sample. Thus, if instance selection algorithms are not able toimprove the results of random sampling, they are still useful to obtain the sampling rate.5.7. Study of execution timeIn the previous sections we showed that our method’s computational cost is linear in the number of instances. Toillustrate this property, we show the behavior of the standard algorithms and our approach in terms of execution timeC. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441427Fig. 7. Storage requirements/testing error (top) and execution time in seconds (bottom – using a logarithmic scale) for standard RNN algorithm and ourapproach.in function of the number of instances in Fig. 9. We plot the time spent by the algorithms as the number of instancesincreases. A Bezier line is drawn using those points to create a clearer plot. The figure shows that the MSS, ICF and DROP3methods have an execution time that is approximately quadratic with respect to number of instances. RNN and CHC showa worse behavior, with a far longer execution time. Our proposal is approximately linear, allowing the use of the methodseven with hundreds of thousands of instances. This corroborates our theoretical arguments in Section 2.4.5.8. Summary of resultsAs a summary of the previous experiments, Table 5 shows a comparison of our approach when using the five testedinstance selection algorithms averaged over all the datasets shown in previous tables. The table shows the advantage ofusing our approach. In terms of testing error, demoIS is no worse than the standard algorithms in all of the methods withthe exception of CHC. However, although for CHC there is a small increment in testing error, it is coupled with a largedecrement in the storage reduction. In terms of storage reduction, demoIS is no worse in all of the cases with the exceptionof RNN. However, for RNN the reduction in terms of execution time is remarkable, and the storage reduction achieved bydemoIS.rnn is worse than RNN but still better than the other algorithms. In terms of execution time, as showed in Fig. 9,the behavior is excellent for the five algorithms.In Section 3 we discussed a previous method based on a recursive divide-and-conquer approach [20] that was able toobtain good results in terms of execution time and storage reduction. However, the main drawback of that method is in428C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Fig. 8. Storage requirements/testing error (top) and execution time in seconds (bottom – using a logarithmic scale) for standard CHC algorithm and ourapproach.Table 4Summary of the performance of instance selection methods in terms of testing error against a random sample with the same sampling ratio. The tableshows the win/draw/loss record of each algorithm against the random sampling. The row labeled ps is the result of a two-tailed sign test on the win/lossrecord and the row labeled p w shows the result of a Wilcoxon test. Significant differences at a confidence level of 95% using a Wilcoxon test are indicatedwith a (cid:3).Win/draw/losspsp wWin/draw/losspsp wDrop324/0/60.00140.0039(cid:3)Drop325/0/50.00030.0010(cid:3)ICF22/0/80.01610.0012(cid:3)ICF21/0/90.04280.1020Standard methodsMSS18/0/120.36160.2289Democratic methodsMSS18/1/110.26490.3820CHC16/0/140.85550.7971CHC19/0/110.20050.2134RNN27/0/30.00000.0000(cid:3)RNN24/1/50.00050.0009(cid:3)C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441429Fig. 9. Execution time (in seconds) for standard methods and our approach as a function of the number of instances.the testing error, which is worse than that obtained if we apply the original method alone. Fig. 10 shows a comparisonof demoIS. and this method in terms of testing error for DROP3 and ICF as base methods. The figure shows how demoIS.improves the testing error of our previous recursive approach in almost all of the problems. A pairwise comparison of bothalgorithms, for DROP3 and ICF methods separately, shows significant differences using Wilcoxon test at a confidence levelof 99%.430C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Table 5Summary of the performance of our methodology against standard methods in terms of testingerror, storage requirements and execution time. Significant differences, for testing error andstorage reduction, at a confidence level of 95% using Wilcoxon test are indicated by a (cid:3).MethodDemocratic instance selectionDROP3ICFMSSCHCRNNErrorEqualBetter (cid:3)EqualWorse (cid:3)Better (cid:3)StorageBetterBetterEqualBetter (cid:3)WorseTimeBetterBetterBetterBetterBetterFig. 10. Testing error for recursive and democratic instance selection using DROP3 (top) and ICF (bottom) as base methods.5.9. Study of subset size and number of rounds effectWe have stated that the size of the subset is not relevant provided it is kept small, that is, about a few hundreds orthousands instances. Thus, we chose a subset size of 1000 instances as a good compromise between a subset small enoughto obtain a significant reduction of execution time and large enough to allow a meaningful instance selection process. In thissection we study the effect of subset size on the behavior of our method. We performed experiments using DROP3 and ICFand subset sizes of 100, 250, 500, 1000, 2500 and 5000 instances and 10 rounds of votes. Figs. 11 and 12 show the resultsfor testing error, storage requirements and execution time with the six different sizes using DROP3 and ICF respectively.For DROP3 the reduction is kept similar regardless of the subset size. With a larger subset size the reduction is somewhatsmaller, but the differences are not significant. In terms of testing error, the method needs a subset size large enough toform meaningful subsets. In this way, subsets smaller than 1000 instances obtain worse results, but once the minimum sizeof 1000 instances is achieved, there is no longer a decrement in testing error. In fact, the results for subset sizes of 1000,2500 and 5000 instances are almost equal. In terms of execution time we observe a large increment, for example, withDROP3 as the base method, the time grows approximately quadratically as the subset size grows.The behavior for ICF is similar. In this case, there is a more significant reduction in storage requirements as the subsetsize becomes larger. This reduction has the side effect of worsening the testing error for subset sizes of 2500 and 5000instances. The behavior of execution time is the same as for DROP3. As the subset size grows, the execution time grows.As the size becomes larger, the O (n2) of the algorithm begins to be relevant, and the processing of each subset is moreC. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441431Fig. 11. Average testing error (top), storage requirements (middle) and execution time (bottom) as a function of subset size for DROP3 algorithm. The plotsshow relative values with respect to the results using a subset of 1000 instances.important than the reduction of the number of subsets processed. The results corroborate that 1000 instances is a goodcompromise among the sizes that favor storage reduction, testing error and execution time.A similar test was performed to determine the effect of the number of rounds on the performance of the method. Weran the method using 5, 10, 25 and 100 rounds. The results for DROP3 and ICF are shown in Figs. 13 and 14, respectively.Again, similar behavior is observed in both algorithms. As more rounds are added, the reduction in storage decreases. Thiseffect is due to the fact that the threshold for removing an instance is higher and so more rounds must agree to remove it.The testing error is not affected after the first few rounds are added. Inspecting the results, we observed that when manyrounds are used, 25 or more, many of the votes cast are redundant and there is no advantage in having so many rounds. Inthis way, a value measured at around 10 rounds is enough. The effect of adding more voters in the testing error is marginal,and each new round increases execution time. This behavior is similar to the case of classifier ensembles, where little gainis obtained after the first few classifiers are added [8].5.10. Huge problemsIn the previous experiments we have shown the performance of our methodology in problems that can be consid-ered medium to large. In this section we consider huge problems, with a few hundreds thousands to more than a million432C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Fig. 12. Average testing error (top), storage requirements (middle) and execution time (bottom) as a function of subset size for the ICF algorithm. The plotsshow relative values with respect to the results obtained using a subset of 1000 instances.instances, that are shown in Table 6. These datasets will show whether our methodology allows scaling up standard algo-rithms to huge problems. As in the previous experiments, testing error and storage reduction are obtained using 10-foldcross-validation. The size of the datasets prevents the execution of the standard algorithms within a reasonable time, so thevalidity of our approach was tested using the 1-NN 10-fold cv testing error shown in the table. For these problems we useddemoIS.drop3, demoIS.icf and demoIS.rnn.Results are shown in Table 7. The first remarkable result is that our method is able to scale up even to huge problems.In fact, our algorithm makes it possible to do instance selection with datasets whose execution time was prohibitive. In theworst case, in the demoIS.rnn for poker dataset, our approach took 93 hours. This value is good if we take into account thatthe standard RNN took more than 500 hours in adult dataset, a problem with 48 842 instances whereas poker dataset has1 025 010 instances. Regarding the effectiveness of the scalability, the results are good. The achieved testing error is closeto the 1-NN error for all problems and methods, with the only exception being the covtype problem with the demoIS.icfmethod. This testing error comes together with a remarkable reduction in storage size, which for demoIS.rnn is less than 1%of the original dataset for census, kddcup99, kddcup991M and poker. Similarly, demoIS.drop3 and demoIS.icf achieve largereductions for these problems.C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441433Fig. 13. Average testing error (top), storage requirements (middle) and execution time (bottom) as a function of number of rounds for the DROP3 algorithm.The plots show relative values with respect to the results obtained using 10 rounds.5.11. Application to other methods of classificationWe have stated that our approach can be applied to other classifiers as well. Other learners can benefit from the democ-ratization of the instance selection algorithm, as it provides a way to scale up any instance selection algorithm. In this way,classifiers whose complexity is related to the size of the training set, such as decision trees and support vector machines(SVM), can benefit from instance selection because the constructed classifier would be simpler [6,40]. When the instancesselected are used as a training set for an instance-based learner, such as an SVM or a decision tree, the term prototype se-lection is used more often than instance selection. We will use instance selection for k-NN oriented methods, and prototypeselection for methods developed for selecting training instances for an instance-based learner.Our method can be used without any significant modification with any of these classifiers. We just need a prototypeselection algorithm suitable for the used classifier, then we can apply the procedure described in Algorithm 1. As in thedescribed instance selection methods, prototype selection algorithms for decision trees, neural networks or SVMs suffer aproblem of scalability. Thus, our method can contribute to scaling up these algorithms as has been shown for k-NN-basedinstance selection.In this section we present experiments showing the applicability of the democratic algorithm when using decision treesand SVMs as classifiers. We chose these two classifiers because their complexity depends on the quality of the training434C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Fig. 14. Average testing error (top), storage requirements (middle) and execution time (bottom) as a function of number of rounds for the ICF algorithm. Theplots show relative values with respect to the results obtained using 10 rounds.Table 6Summary of datasets. The features of each dataset can be C (continuous), B (binary) or N (nominal). The Inputs column shows the number of input variables.Data setCasesFeaturesClassesInputs1-NN errorcensuscovtypekddcup99kddcup991Mpoker299 285581 012494 0211 000 0001 025 010C75433335B––44–N30–3352723211040954118119250.07430.30240.00060.00020.4975set [40] and also because they are among the most widely used in any machine learning application. As we have stated,the partition method described in Section 2.1 is specially designed for the k-NN method. Thus, for the experiments withdecision trees and SVMs we used a simple random partition of the training set into disjoint subsets of approximately thesame size.C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441435Table 7Testing error, storage requirements and execution time (in seconds) for our approach for hugeproblems.Datasetcensuscovtypekddcup99kddcup991Mpokercensuscovtypekddcup99kddcup991Mpokercensuscovtypekddcup99kddcup991MpokerStoragedemoIS.drop30.02890.16270.01230.01140.0247demoIS.icf0.02960.22500.02660.00970.0483demoIS.rnn0.00060.26530.00630.00260.0001Error0.07710.33330.00660.00190.50090.08180.40030.01120.00720.50990.06230.29550.00360.00370.4990Time20 894.37352.044 198.089 547.06660.06548.03891.34924.115 120.05265.375 181.0190 903.0112 947.0229 273.0335 141.7As the prototype selection algorithm, we can use any of those previously described. However, because these algorithmsare specially designed for k-NN classifiers, their results on other classifiers are rather poor. Thus, we use a method designedfor any type of classifier. This method [41] is a filter approach based on using a set of different classifiers as noise filters.These classifiers should detect the noisy, outliers or mislabeled instances and remove them from the training set. Theprocedure is shown in Algorithm 7. Brodley and Friedl proposed two versions of the method, consensus filter and majorityvote. In consensus filter, a set of classifiers D = {d1, d2, . . . , dk} is available, and each classifier di is trained on the originaltraining set. After that, instances that are misclassified by all classifiers in D are discarded. Then the classifier of our choiceis trained on the remaining instances. In majority vote, the procedure is the same, but instances are discarded if a majorityof the learners misclassify them. In our experiments we used the latter approach because the former resulted in removal ofvery few instances. We will use the term Majority Vote Filter (MVF) algorithm to refer to this method.Algorithm 7: Majority vote filter algorithmData : A training set T = {(x1, y1), . . . , (xn, yn)} and a set of learners DResult: The subset of selected instances Sforeach di ∈ D doTrain di on TendS = Tforeach xi ∈ S doif xi is misclassified by a majority in D thenRemove xi from Sendend123As classifiers in D we chose a 1-NN classifier, a k-NN classifier where k is obtained by cross-validation, a C4.5 decisiontree [42], an SVM with a linear kernel, and an SVM with a Gaussian kernel. Decision trees and SVMs are sensitive toparameters, so we performed our experiments using cross-validation for setting the values of the parameters. For each ofthe classifiers used, we obtained the best parameters from a set of different values. For SVM with a linear kernel we triedC ∈ {0.1, 1, 10}, and for an SVM with a Gaussian kernel we tried C ∈ {0.1, 1, 10} and γ ∈ {0.0001, 0.001, 0.01, 0.1, 1, 10},testing all 18 possible combinations. For C4.5, we tested 1 and 10 trials and softening of thresholds trying all 4 possiblecombinations. Although this method does not assure an optimum set of parameters, at least a good set of parameters isobtained in a reasonable time. The SVM learning algorithm was programmed using functions from the LIBSVM library [43].The experiments were performed with the same experimental setup used previously. There is only one change that mustbe made in the democratic algorithm: the threshold of votes (see Section 2.3) is evaluated using the classifier we are goingto learn with the prototypes selected by the algorithm.The experiments were performed using the standard classifiers, both C4.5 and SVM, on the whole dataset. Then, weapplied the MVF algorithm and trained C4.5 and SVM on the dataset selected by MVF. Finally we performed the sameexperiment using the democratic version of MVF, demoIS.MVF. Results for C4.5 classifier are shown in Table 8 and for SVMare shown in Table 9. These results are plotted in Figs. 15 and 16, respectively.436C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Table 8Testing error, tree size (number of nodes) and execution time (in seconds) for a standard C.45 algorithm, majority vote filtering (MVF) and demoIS.MVF.Datasetabaloneadultcargenegermanhypothyroidisoletkrkoptkr vs. kplettermagic04mfeat-facmfeat-foumfeat-karmfeat-mormfeat-pixmfeat-zernurseryoptdigitspage-blockspendigitsphonemesatimagesegmentshuttlesicktexturewaveformyeastzipC4.5Error0.79140.14700.13310.09460.31700.00560.29970.19330.00630.12210.16610.11500.24150.18250.29550.11900.30880.16780.10820.03310.03480.13260.14460.03070.00020.00980.06660.24740.45270.1340MVF + C4.5demoIS.MVF + C4.5Size2310.21988.56130.2274.4288.227.21368.27527.470.22553719148.8272.2232.2219174.2290.5367439.8121402.4254.8654.489.258.654.2308.4599.8453.6795.4Error0.75680.17350.15820.07290.27200.01140.29880.27390.00810.12190.17630.11000.24050.16450.29330.11300.32000.16830.09980.02690.03590.14110.12710.03290.00070.01960.06450.22880.42090.1324Size187.2257.6102.0163.6127.620.41259.43692.456.82208.6457.6138.8166.8221.297.0164.6267.8310.8383.458.4342.0196.6420.277.047.829.6258.6460.299.2729.8Time (s)62.3387 406.18.3188.73811.3216.11128.03708.740.62089.618 577.2265.7141.7147.82029.3206.8114.8660.9380.32756.41059.556.0747.647.5298 567.8132.6269.2224.26.5916.4Error0.73910.14270.16570.07570.27700.02020.29870.25540.00880.12450.16990.12100.23650.17400.27700.10500.30950.17390.09290.02850.03320.13260.12990.04070.00060.01670.06400.23840.40680.1273Size252.2223.088.0183.6137.418.61253.04320.853.62124.0449.6128.8216.6213.877.4157.2244.8292.8374.646.6336.0227.6402.671.243.631.6257.2544.2101.4721.0Table 9Testing error, size (number of support vectors) and execution time (in seconds) for an SVM, majority vote filtering (MVF) and demoIS.MVF.Datasetabaloneadultcargenegermanhypothyroidisoletkrkoptkr vs. kplettermagic04mfeat-facmfeat-foumfeat-karmfeat-mormfeat-pixmfeat-zernurseryoptdigitspage-blockspendigitsphonemesatimagesegmentshuttlesicktexturewaveformyeastzipSVMError0.73720.15460.14300.07350.24600.02390.05680.16440.00810.01600.31900.01950.16900.03050.26450.01950.16950.12790.01710.03640.00460.10650.07480.04240.00140.03110.00160.14100.41490.0102Size3753.215 175.3539.61720.0549.5261.33362.818 108.7513.07370.04822.7535.11190.0870.0993.9823.4972.12846.61244.1532.41125.52489.91746.9450.0563.6505.9664.02767.61083.61964.6MVF + SVMdemoIS.MVF + SVMError0.75110.15720.10640.07730.26300.03160.06050.25980.00850.03030.19640.02500.15300.02500.32330.01950.19400.10440.01070.02870.00400.11890.08770.04030.00190.04850.00490.13740.42360.0120Size595.21415.5343.51459.4297.5127.73046.117 658.3547.810 112.74517.5328.3734.6700.9334.7678.9402.01177.51281.5157.71062.11714.61927.9593.8627.1149.6817.61508.0341.01146.5Time (s)62.3387 406.18.3188.73811.3216.11128.03708.740.62089.618 577.2265.7141.7147.82029.3206.8114.8660.9380.32756.41059.556.0747.647.5298 567.8132.6269.2224.26.5916.4Error0.74130.15280.14420.07290.26400.05150.05870.24040.00940.03050.15890.02500.16900.02800.26750.02350.17800.14710.01530.03800.00640.12640.08230.04670.00180.03610.00380.13700.40000.0126Size609.12937.5581.31741.1335214.43477.813 472.7372.26952.72399.1612.8955.5901.3234.8805.9755.32975.21161.2294.81085.91293.251295.5395.1767.2365.7737.81828.14242085.1Time (s)30.1520.86.452.87.115.8171.0169.617.9141.5103.769.657.949.710.161.036.867.347.520.350.522.144.110.3193.116.168.551.88.2140.5Time (s)31.62095.011.0119.49.728.4292.6317.541.1664.5259.2105.161.352.58.4119.439.8903.2197.528.5127.027.8124.412.6380.424.596.5119.78.4547.3C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441437Fig. 15. Testing error, relative tree size, measured as the ratio of the number of nodes with respect to C4.5 applied to the whole dataset, and executiontime in seconds (using a logarithmic scale) for standard MVF algorithm and our approach, compared with the C4.5 algorithm applied to the whole dataset.Both the tables and the results show the usefulness of our approach. MVF is able to obtain classifiers, in both cases, thatare simpler than those obtained using all the instances in the training set, and match their testing error. However, as inthe previous methods, MVF has a scalability problem for large datasets. This problem is especially noticeable for adult andshuttle datasets. demoIS.MVF is able to keep the performance of MVF but with a significant reduction in the execution time.As it was the case for the experiments using k-NN, the reduction is more significant when the problem is larger, supportingour claim that the proposed method is able to scale up prototype selection algorithms as well as instance selection methodsefficiently.These results show that our methodology can be applied to different kinds of classifiers provided there is a prototypeselection method for them. Other methods that have reported good results, such as PSRCG [44] and SiS [45], can be used aswell.5.12. Noise toleranceInstance selection algorithms, as any other learning algorithm [46], have degraded performance in the presence of noise.In the field of ensembles of classifiers, Dietterich [47] tested the effect of noise on learning algorithms by introducingartificial noise in the class labels of different datasets. Real-world problems do have noise, thus, it is relevant to study the438C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441Fig. 16. Testing error, relative tree size, measured as the ratio of the number of support vectors with respect to SVM applied to the whole dataset, andexecution time in seconds (using a logarithmic scale) for standard MVF algorithm and our approach, compared with SVM applied to the whole dataset.behavior of any learning algorithm in the presence of noise. In this section, we study the sensitivity of our method to noiseand compare it with standard algorithms.To add noise to the class labels, we follow the method of Dietterich [47]. To add classification noise at a rate ρ, wechose a fraction ρ of the instances and changed their class labels to be incorrect, choosing uniformly from the set ofincorrect labels. We chose all the datasets and three rates of noise, 5%, 10%, and 20%. With these three levels of noise weperformed the experiments using DROP3 and ICF, and their democratic counterparts demoIS.drop3 and demoIS.icf. Fig. 17shows the results for the four methods at noise levels of 5%, 10% and 20%, and using DROP3 and ICF algorithms. The figuredemonstrates the robustness of our method. The degradation of performance is smooth as class label noise is added. It isimportant to note that our method is able to maintain a good performance in the presence of noise because it uses partialviews of the dataset that might be more sensitive to noise. The figures show that our method is able to keep its relativebehavior with respect to the original algorithms as noise is added.6. Conclusions and future workIn this paper we have presented a new method for scaling up instance selection algorithms that is applicable to anyinstance selection method without any modification. The method consists of performing several rounds of applying instanceC. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441439Fig. 17. Comparison between DROP3 and demoIS.drop3 and between ICF and demoIS.icf in presence of noise.selection on disjoint subsets of the original dataset and combining them by means of a voting method. Using five well-known instance selection algorithms, DROP3, ICF, RNN, MSS and a CHC genetic algorithm, we have shown that our methodis able to match the performance of the original algorithms with a considerable reduction in execution time. In termsof reduction of storage requirements and testing error, our approach is even better than the use of the original instance440C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441selection algorithm over the whole dataset for some of the methods. Our method is straightforwardly paralellizable withoutmodifications.Additionally, our method has also been tested using prototype selection algorithms and two additional classifiers, decisiontrees and support vector machines. In both cases it has shown its ability to scale prototype selection algorithms as well asinstance selection algorithms.We have also shown that our approach is able to scale up to huge problems with hundreds of thousands of instances.Using five of those huge datasets our method is able to execute rapidly, achieving a significant reduction of storage whilekeeping the testing error similar to the 1-NN error using whole datasets. We think that the proposed method might bea breakthrough in instance selection algorithms design, because it allows the development of more complex methods forinstance selection. This is due to the relaxation of the constraints on the complexity of the base method through thepossibility of using democratic instance selection.As a principal research approach, we are working on the development of better methods of partitioning the originaldataset that we believe may have a relevant influence on the performance of the method. Additionally, in a recent paper[48] it was shown that instance selection can be used as a mechanism for constructing ensembles of classifiers. The methodpresented in this paper provides a promising way to extend this method to larger datasets.References[1] M. Craven, D. DiPasquoa, D. Freitagb, A. McCalluma, T. Mitchella, K. Nigama, S. Slatterya, Learning to construct knowledge bases from the World WideWeb, Artificial Intelligence 118 (1–2) (2000) 69–113.[2] F.J. Provost, V. Kolluri, A survey of methods for scaling up inductive learning algorithms, Data Mining and Knowledge Discovery 2 (1999) 131–169.[3] H. Liu, H. Motada, L. Yu, A selective sampling approach to active feature selection, Artificial Intelligence 159 (1–2) (2004) 49–74.[4] J.R. Cano, F. Herrera, M. Lozano, Using evolutionary algorithms as instance selection for data reduction in KDD: An experimental study, IEEE Transac-tions on Evolutionary Computation 7 (6) (2003) 561–575.[5] H. Brighton, C. Mellish, Advances in instance selection for instance-based learning algorithms, Data Mining and Knowledge Discovery 6 (2002) 153–172.[6] J.R. Cano, F. Herrera, M. Lozano, Evolutionary stratified training set selection for extracting classification rules with trade off precision-interpretability,Data & Knowledge Engineering 60 (1) (2007) 90–108.[7] P. Domingos, G. Hulten, A general framework for mining massive data streams, Journal of Computational and Graphical Statistics 12 (4) (2003) 945–949.[8] N. García-Pedrajas, C. García-Osorio, C. Fyfe, Nonlinear boosting projections for ensemble construction, Journal of Machine Learning Research 8 (2007)1–33.[9] R.E. Schapire, Y. Freund, P.L. Bartlett, W.S. Lee, Boosting the margin: A new explanation for the effectiveness of voting methods, Annals of Statis-tics 26 (5) (1998) 1651–1686.[10] C. García-Osorio, C. Fyfe, Regaining sparsity in kernel principal components, Neurocomputing 67 (2005) 398–402.[11] D. Asimov, The grand tour: A tool for viewing multidimensional data, SIAM Journal on Scientific and Statistical Computing 6 (1) (1985) 128–143.[12] A. Buja, D. Asimov, Grand Tour methods: An outline, in: D. Allen (Ed.), Computer Science and Statistics: Proceedings of the Seventeenth Symposiumon the Interface, Elsevier Science Publisher B.V., North Holland, Amsterdam, 1986, pp. 63–67.[13] A. Buja, D. Cook, D. Asimov, C. Hurley, Computational Methods for High-Dimensional Rotations in Data Visualization, North-Holland Publishing Co.,2005, Ch. 14, pp. 391–414.[14] E.J. Wegman, J.L. Solka, On some mathematics for visualising high dimensional data, Indian Journal of Statistics (Series A Pt. 2) 64 (2002) 429–452.[15] D.F. Andrews, Plots of high dimensional data, Biometrics 28 (1972) 125–136.[16] E.J. Wegman, J. Shen, Three-dimensional Andrews plots and the grand tour, Computing Science and Statistics 25 (1993) 284–288.[17] N. García-Pedrajas, J.A. Romero del Castillo, D. Ortiz-Boyer, A cooperative coevolutionary algorithm for instance selection for instance-based learning,Machine Learning 78 (3) (2010) 381–420.[18] J.R. Cano, F. Herrera, M. Lozano, Stratification for scaling up evolutionary prototype selection, Pattern Recognition Letters 26 (7) (2005) 953–963.[19] S.-W. Kim, B.J. Oommen, Enhancing prototype reduction schemes with recursion: A method applicable for “large” data sets, IEEE Transactions onSystems, Man, and Cybernetics—Part B: Cybernetics 34 (3) (2004) 1384–1397.[20] A. de Haro-García, N.G. Pedrajas, A divide-and-conquer recursive approach for scaling up instance selection algorithms, Data Mining and KnowledgeDiscovery 18 (3) (2009) 392–418.[21] P. Domingos, G. Hulten, A general method for scaling up machine learning algorithms and its application to clustering, in: Proceedings of the EighteenthInternational Conference on Machine Learning, Morgan Kaufmann, 2001, pp. 106–113.[22] P. Domingos, G. Hulten, Learning from infinite data in finite time, in: Proceedings of Advances in Neural Information Systems, vol. 14, Vancouver,Canada, 2001, pp. 673–680.[23] W. Howffding, Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association 58 (1963) 13–30.[24] G. Hulten, P. Domingos, Mining complex models from arbitrarily large databases in constant time, in: Proceedings of the International Conference onKnowledge Discovery and Data Mining, Edmonton, Canada, 2002, pp. 525–531.[25] S. Hettich, C. Blake, C. Merz, UCI Repository of machine learning databases, http://www.ics.uci.edu/~mlearn/MLRepository.html, 1998.[26] J. Demšar, Statistical comparisons of classifiers over multiple data sets, Journal of Machine Learning Research 7 (2006) 1–30.[27] H. Liu, H. Motoda, On issues of instance selection, Data Mining and Knowledge Discovery 6 (2002) 115–130.[28] D.R. Wilson, T.R. Martinez, Reduction techniques for instance-based learning algorithms, Machine Learning 38 (2000) 257–286.[29] D.L. Wilson, Asymptotic properties of nearest neighbor rules using edited data, IEEE Transactions on Systems, Man, and Cybernetics 2 (3) (1972)408–421.[30] G.W. Gates, The reduced nearest neighbor rule, IEEE Transactions on Information Theory 18 (3) (1972) 431–433.[31] R. Barandela, F.J. Ferri, J.S. Sánchez, Decision boundary preserving prototype selection for nearest neighbor classification, International Journal of PatternRecognition and Artificial Intelligence 19 (6) (2005) 787–806.[32] L. Kuncheva, Editing for the k-nearest neighbors rule by a genetic algorithm, Pattern Recognition Letters 16 (1995) 809–814.[33] H. Ishibuchi, T. Nakashima, Pattern and feature selection by genetic algorithms in nearest neighbor classification, Journal of Advanced ComputationalIntelligence and Intelligent Informatics 4 (2) (2000) 138–145.[34] C.R. Reeves, D.R. Bush, Using genetic algorithms for training data selection in RBF networks, in: H. Liu, H. Motoda (Eds.), Instances Selection andConstruction for Data Mining, Kluwer, Norwell, Massachusetts, USA, 2001, pp. 339–356.[35] D.E. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning, Addison–Wesley, Reading, MA, 1989.C. García-Osorio et al. / Artificial Intelligence 174 (2010) 410–441441[36] D. Whitley, The GENITOR algorithm and selective pressure, in: M.K. Publishers (Ed.), Proc. 3rd International Conf. on Genetic Algorithms, Los Altos, CA,1989, pp. 116–121.[37] L.J. Eshelman, The CHC Adaptive Search Algorithm: How to Have Safe Search when Engaging in Nontraditional Genetic Recombination, Morgan Kauff-man, San Mateo, CA, 1990.[38] S. Baluja, Population-based incremental learning, Tech. Rep. CMU-CS-94-163, Carnegie Mellon University, Pittsburgh, 1994.[39] J. Maudes-Raedo, J.J. Rodríguez-Díez, C. García-Osorio, Disturbing neighbors diversity for decision forest, in: G. Valentini, O. Okun (Eds.), Workshop onSupervised and Unsupervised Ensemble Methods and Their Applications (SUEMA 2008), Patras, Greece, 2008, pp. 67–71.[40] M. Sebban, R. Nock, J.H. Chauchat, R. Rakotomalala, Impact of learning set quality and size on decision tree performances, International Journal ofComputers, Systems and Signals 1 (1) (2000) 85–105.[41] C.E. Brodley, M.A. Friedl, Identifying mislabeled training data, Journal of Artificial Intelligence Research 11 (1999) 131–167.[42] J.R. Quinlan, C4.5: Programs for Machine Learning, Morgan Kaufmann, San Mateo, 1993.[43] C.-C. Chang, C.-J. Lin, LIBSVM: A library for support vector machines, software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm, 2001.[44] M. Sebban, R. Nock, Identifying and eliminating irrelevant instances using information theory, in: H. Hamilton, Q. Yang (Eds.), 13th Biennial Conferenceof the Canadian Society for Computational Studies of Intelligence, AI 2000, Montreal, in: Lecture Notes in Artificial Intelligence, vol. 1822, Springer,2000, pp. 90–101.[45] S.S. Sane, A.A. Ghatol, A novel Supervised Instance Selection algorithm, International Journal on Business Intelligence and Data Mining 2 (4) (2007)471–495.[46] E. Bauer, R. Kohavi, An empirical comparison of voting classification algorithms: Bagging, boosting, and variants, Machine Learning 36 (1/2) (1999)105–142.[47] T.G. Dietterich, An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization,Machine Learning 40 (2000) 139–157.[48] N. García-Pedrajas, Constructing ensembles of classifiers by means of weighted instance selection, IEEE Transactions on Neural Networks 20 (2) (2008)258–277.